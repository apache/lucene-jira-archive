diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.java lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.java
index f39f4ff..611b752 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.java
@@ -30663,7 +30663,7 @@ public final class HTMLStripCharFilter extends BaseCharFilter {
 
   /* user code: */
   private static final Map<String,String> upperCaseVariantsAccepted
-      = new HashMap<String,String>();
+      = new HashMap<>();
   static {
     upperCaseVariantsAccepted.put("quot", "QUOT");
     upperCaseVariantsAccepted.put("copy", "COPY");
@@ -30673,7 +30673,7 @@ public final class HTMLStripCharFilter extends BaseCharFilter {
     upperCaseVariantsAccepted.put("amp", "AMP");
   }
   private static final CharArrayMap<Character> entityValues
-      = new CharArrayMap<Character>(Version.LUCENE_CURRENT, 253, false);
+      = new CharArrayMap<>(Version.LUCENE_CURRENT, 253, false);
   static {
     String[] entities = {
       "AElig", "\u00C6", "Aacute", "\u00C1", "Acirc", "\u00C2",
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/MappingCharFilter.java lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/MappingCharFilter.java
index 5bd456b..095d003 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/MappingCharFilter.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/MappingCharFilter.java
@@ -43,7 +43,7 @@ public class MappingCharFilter extends BaseCharFilter {
   private final FST<CharsRef> map;
   private final FST.BytesReader fstReader;
   private final RollingCharBuffer buffer = new RollingCharBuffer();
-  private final FST.Arc<CharsRef> scratchArc = new FST.Arc<CharsRef>();
+  private final FST.Arc<CharsRef> scratchArc = new FST.Arc<>();
   private final Map<Character,FST.Arc<CharsRef>> cachedRootArcs;
 
   private CharsRef replacement;
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/MappingCharFilterFactory.java lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/MappingCharFilterFactory.java
index 80b7b1f..29c1152 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/MappingCharFilterFactory.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/MappingCharFilterFactory.java
@@ -69,7 +69,7 @@ public class MappingCharFilterFactory extends CharFilterFactory implements
         wlist = getLines(loader, mapping);
       } else {
         List<String> files = splitFileNames(mapping);
-        wlist = new ArrayList<String>();
+        wlist = new ArrayList<>();
         for (String file : files) {
           List<String> lines = getLines(loader, file.trim());
           wlist.addAll(lines);
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/NormalizeCharMap.java lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/NormalizeCharMap.java
index 9203784..499fdf0 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/NormalizeCharMap.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/NormalizeCharMap.java
@@ -40,7 +40,7 @@ import org.apache.lucene.util.fst.Util;
 public class NormalizeCharMap {
 
   final FST<CharsRef> map;
-  final Map<Character,FST.Arc<CharsRef>> cachedRootArcs = new HashMap<Character,FST.Arc<CharsRef>>();
+  final Map<Character,FST.Arc<CharsRef>> cachedRootArcs = new HashMap<>();
 
   // Use the builder to create:
   private NormalizeCharMap(FST<CharsRef> map) {
@@ -48,7 +48,7 @@ public class NormalizeCharMap {
     if (map != null) {
       try {
         // Pre-cache root arcs:
-        final FST.Arc<CharsRef> scratchArc = new FST.Arc<CharsRef>();
+        final FST.Arc<CharsRef> scratchArc = new FST.Arc<>();
         final FST.BytesReader fstReader = map.getBytesReader();
         map.getFirstArc(scratchArc);
         if (FST.targetHasArcs(scratchArc)) {
@@ -78,7 +78,7 @@ public class NormalizeCharMap {
    */
   public static class Builder {
 
-    private final Map<String,String> pendingPairs = new TreeMap<String,String>();
+    private final Map<String,String> pendingPairs = new TreeMap<>();
 
     /** Records a replacement to be applied to the input
      *  stream.  Whenever <code>singleMatch</code> occurs in
@@ -108,7 +108,7 @@ public class NormalizeCharMap {
       final FST<CharsRef> map;
       try {
         final Outputs<CharsRef> outputs = CharSequenceOutputs.getSingleton();
-        final org.apache.lucene.util.fst.Builder<CharsRef> builder = new org.apache.lucene.util.fst.Builder<CharsRef>(FST.INPUT_TYPE.BYTE2, outputs);
+        final org.apache.lucene.util.fst.Builder<CharsRef> builder = new org.apache.lucene.util.fst.Builder<>(FST.INPUT_TYPE.BYTE2, outputs);
         final IntsRef scratch = new IntsRef();
         for(Map.Entry<String,String> ent : pendingPairs.entrySet()) {
           builder.add(Util.toUTF16(ent.getKey(), scratch),
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/CompoundWordTokenFilterBase.java lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/CompoundWordTokenFilterBase.java
index d85b64a..255a95a 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/CompoundWordTokenFilterBase.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/CompoundWordTokenFilterBase.java
@@ -84,7 +84,7 @@ public abstract class CompoundWordTokenFilterBase extends TokenFilter {
   protected CompoundWordTokenFilterBase(Version matchVersion, TokenStream input, CharArraySet dictionary, int minWordSize, int minSubwordSize, int maxSubwordSize, boolean onlyLongestMatch) {
     super(input);
     this.matchVersion = matchVersion;
-    this.tokens=new LinkedList<CompoundToken>();
+    this.tokens=new LinkedList<>();
     if (minWordSize < 0) {
       throw new IllegalArgumentException("minWordSize cannot be negative");
     }
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/hyphenation/HyphenationTree.java lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/hyphenation/HyphenationTree.java
index 119794e..4ba5f27 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/hyphenation/HyphenationTree.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/hyphenation/HyphenationTree.java
@@ -54,7 +54,7 @@ public class HyphenationTree extends TernaryTree implements PatternConsumer {
   private transient TernaryTree ivalues;
 
   public HyphenationTree() {
-    stoplist = new HashMap<String,ArrayList<Object>>(23); // usually a small table
+    stoplist = new HashMap<>(23); // usually a small table
     classmap = new TernaryTree();
     vspace = new ByteVector();
     vspace.alloc(1); // this reserves index 0, which we don't use
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/hyphenation/PatternParser.java lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/hyphenation/PatternParser.java
index 3cf35cb..d9901f1 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/hyphenation/PatternParser.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/hyphenation/PatternParser.java
@@ -188,7 +188,7 @@ public class PatternParser extends DefaultHandler {
   }
 
   protected ArrayList<Object> normalizeException(ArrayList<?> ex) {
-    ArrayList<Object> res = new ArrayList<Object>();
+    ArrayList<Object> res = new ArrayList<>();
     for (int i = 0; i < ex.size(); i++) {
       Object item = ex.get(i);
       if (item instanceof String) {
@@ -287,7 +287,7 @@ public class PatternParser extends DefaultHandler {
       currElement = ELEM_PATTERNS;
     } else if (local.equals("exceptions")) {
       currElement = ELEM_EXCEPTIONS;
-      exception = new ArrayList<Object>();
+      exception = new ArrayList<>();
     } else if (local.equals("hyphen")) {
       if (token.length() > 0) {
         exception.add(token.toString());
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/hyphenation/TernaryTree.java lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/hyphenation/TernaryTree.java
index a48c571..c265134 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/hyphenation/TernaryTree.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/hyphenation/TernaryTree.java
@@ -503,7 +503,7 @@ public class TernaryTree implements Cloneable {
 
     public Iterator() {
       cur = -1;
-      ns = new Stack<Item>();
+      ns = new Stack<>();
       ks = new StringBuilder();
       rewind();
     }
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizerFactory.java lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizerFactory.java
index 790de6c..2d9cf17 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizerFactory.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizerFactory.java
@@ -52,6 +52,6 @@ public class LowerCaseTokenizerFactory extends TokenizerFactory implements Multi
 
   @Override
   public AbstractAnalysisFactory getMultiTermComponent() {
-    return new LowerCaseFilterFactory(new HashMap<String,String>(getOriginalArgs()));
+    return new LowerCaseFilterFactory(new HashMap<>(getOriginalArgs()));
   }
 }
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/core/TypeTokenFilterFactory.java lucene/analysis/common/src/java/org/apache/lucene/analysis/core/TypeTokenFilterFactory.java
index 2dd2fa6..0545d75 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/core/TypeTokenFilterFactory.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/core/TypeTokenFilterFactory.java
@@ -58,7 +58,7 @@ public class TypeTokenFilterFactory extends TokenFilterFactory implements Resour
   public void inform(ResourceLoader loader) throws IOException {
     List<String> files = splitFileNames(stopTypesFiles);
     if (files.size() > 0) {
-      stopTypes = new HashSet<String>();
+      stopTypes = new HashSet<>();
       for (String file : files) {
         List<String> typesLines = getLines(loader, file.trim());
         stopTypes.addAll(typesLines);
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/en/KStemmer.java lucene/analysis/common/src/java/org/apache/lucene/analysis/en/KStemmer.java
index 8f87d91..cdb397b 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/en/KStemmer.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/en/KStemmer.java
@@ -280,7 +280,7 @@ public class KStemmer {
     DictEntry defaultEntry;
     DictEntry entry;
 
-    CharArrayMap<DictEntry> d = new CharArrayMap<DictEntry>(Version.LUCENE_CURRENT, 1000, false);
+    CharArrayMap<DictEntry> d = new CharArrayMap<>(Version.LUCENE_CURRENT, 1000, false);
     for (int i = 0; i < exceptionWords.length; i++) {
       if (!d.containsKey(exceptionWords[i])) {
         entry = new DictEntry(exceptionWords[i], true);
@@ -574,7 +574,7 @@ public class KStemmer {
     return matchedEntry != null;
   }
   
-  // Set<String> lookups = new HashSet<String>();
+  // Set<String> lookups = new HashSet<>();
   
   /* convert past tense (-ed) to present, and `-ied' to `y' */
   private void pastTense() {
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Dictionary.java lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Dictionary.java
index 68a4b45..a4eae51 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Dictionary.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Dictionary.java
@@ -167,7 +167,7 @@ public class Dictionary {
     flagLookup.add(new BytesRef()); // no flags -> ord 0
     stripLookup.add(new BytesRef()); // no strip -> ord 0
     IntSequenceOutputs o = IntSequenceOutputs.getSingleton();
-    Builder<IntsRef> b = new Builder<IntsRef>(FST.INPUT_TYPE.BYTE4, o);
+    Builder<IntsRef> b = new Builder<>(FST.INPUT_TYPE.BYTE4, o);
     readDictionaryFiles(dictionaries, decoder, b);
     words = b.finish();
   }
@@ -438,7 +438,7 @@ public class Dictionary {
       
       List<Character> list = affixes.get(affixArg);
       if (list == null) {
-        list = new ArrayList<Character>();
+        list = new ArrayList<>();
         affixes.put(affixArg, list);
       }
       
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellStemFilterFactory.java lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellStemFilterFactory.java
index 8e97069..a4f24e5 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellStemFilterFactory.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellStemFilterFactory.java
@@ -82,10 +82,10 @@ public class HunspellStemFilterFactory extends TokenFilterFactory implements Res
     String dicts[] = dictionaryFiles.split(",");
 
     InputStream affix = null;
-    List<InputStream> dictionaries = new ArrayList<InputStream>();
+    List<InputStream> dictionaries = new ArrayList<>();
 
     try {
-      dictionaries = new ArrayList<InputStream>();
+      dictionaries = new ArrayList<>();
       for (String file : dicts) {
         dictionaries.add(loader.openResource(file));
       }
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Stemmer.java lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Stemmer.java
index ff6cc0a..22add10 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Stemmer.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Stemmer.java
@@ -84,7 +84,7 @@ final class Stemmer {
       word = scratchBuffer;
     }
     
-    List<CharsRef> stems = new ArrayList<CharsRef>();
+    List<CharsRef> stems = new ArrayList<>();
     IntsRef forms = dictionary.lookupWord(word, 0, length);
     if (forms != null) {
       // TODO: some forms should not be added, e.g. ONLYINCOMPOUND
@@ -158,7 +158,7 @@ final class Stemmer {
   private List<CharsRef> stem(char word[], int length, int previous, int prevFlag, int prefixFlag, int recursionDepth, boolean doPrefix, boolean doSuffix, boolean previousWasPrefix, boolean circumfix) {
     
     // TODO: allow this stuff to be reused by tokenfilter
-    List<CharsRef> stems = new ArrayList<CharsRef>();
+    List<CharsRef> stems = new ArrayList<>();
     
     if (doPrefix) {
       for (int i = length - 1; i >= 0; i--) {
@@ -291,7 +291,7 @@ final class Stemmer {
       return Collections.emptyList();
     }
 
-    List<CharsRef> stems = new ArrayList<CharsRef>();
+    List<CharsRef> stems = new ArrayList<>();
 
     IntsRef forms = dictionary.lookupWord(strippedWord, 0, length);
     if (forms != null) {
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/in/IndicNormalizer.java lucene/analysis/common/src/java/org/apache/lucene/analysis/in/IndicNormalizer.java
index bfef661..81bf4ed 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/in/IndicNormalizer.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/in/IndicNormalizer.java
@@ -43,7 +43,7 @@ public class IndicNormalizer {
   }
   
   private static final IdentityHashMap<Character.UnicodeBlock,ScriptData> scripts = 
-    new IdentityHashMap<Character.UnicodeBlock,ScriptData>(9);
+    new IdentityHashMap<>(9);
   
   private static int flag(Character.UnicodeBlock ub) {
     return scripts.get(ub).flag;
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilterFactory.java lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilterFactory.java
index 7d6ea7a..4c9743c 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilterFactory.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilterFactory.java
@@ -88,7 +88,7 @@ public class CapitalizationFilterFactory extends TokenFilterFactory {
 
     k = getSet(args, OK_PREFIX);
     if (k != null) {
-      okPrefix = new ArrayList<char[]>();
+      okPrefix = new ArrayList<>();
       for (String item : k) {
         okPrefix.add(item.toCharArray());
       }
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PerFieldAnalyzerWrapper.java lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PerFieldAnalyzerWrapper.java
index 2aac745..4badea1 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PerFieldAnalyzerWrapper.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PerFieldAnalyzerWrapper.java
@@ -33,7 +33,7 @@ import java.util.Map;
  * 
  * <pre class="prettyprint">
  * {@code
- * Map<String,Analyzer> analyzerPerField = new HashMap<String,Analyzer>();
+ * Map<String,Analyzer> analyzerPerField = new HashMap<>();
  * analyzerPerField.put("firstname", new KeywordAnalyzer());
  * analyzerPerField.put("lastname", new KeywordAnalyzer());
  *
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/StemmerOverrideFilter.java lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/StemmerOverrideFilter.java
index 2042c84..53b4ecd 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/StemmerOverrideFilter.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/StemmerOverrideFilter.java
@@ -44,7 +44,7 @@ public final class StemmerOverrideFilter extends TokenFilter {
   private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
   private final KeywordAttribute keywordAtt = addAttribute(KeywordAttribute.class);
   private final BytesReader fstReader;
-  private final Arc<BytesRef> scratchArc = new FST.Arc<BytesRef>();
+  private final Arc<BytesRef> scratchArc = new FST.Arc<>();
   private final CharsRef spare = new CharsRef();
   
   /**
@@ -145,7 +145,7 @@ public final class StemmerOverrideFilter extends TokenFilter {
   public static class Builder {
     private final BytesRefHash hash = new BytesRefHash();
     private final BytesRef spare = new BytesRef();
-    private final ArrayList<CharSequence> outputValues = new ArrayList<CharSequence>();
+    private final ArrayList<CharSequence> outputValues = new ArrayList<>();
     private final boolean ignoreCase;
     private final CharsRef charsSpare = new CharsRef();
     
@@ -200,7 +200,7 @@ public final class StemmerOverrideFilter extends TokenFilter {
      */
     public StemmerOverrideMap build() throws IOException {
       ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
-      org.apache.lucene.util.fst.Builder<BytesRef> builder = new org.apache.lucene.util.fst.Builder<BytesRef>(
+      org.apache.lucene.util.fst.Builder<BytesRef> builder = new org.apache.lucene.util.fst.Builder<>(
           FST.INPUT_TYPE.BYTE4, outputs);
       final int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());
       IntsRef intsSpare = new IntsRef();
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilterFactory.java lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilterFactory.java
index b06e53e..cc66970 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilterFactory.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilterFactory.java
@@ -104,7 +104,7 @@ public class WordDelimiterFilterFactory extends TokenFilterFactory implements Re
     }
     if (types != null) {
       List<String> files = splitFileNames( types );
-      List<String> wlist = new ArrayList<String>();
+      List<String> wlist = new ArrayList<>();
       for( String file : files ){
         List<String> lines = getLines(loader, file.trim());
         wlist.addAll( lines );
@@ -124,7 +124,7 @@ public class WordDelimiterFilterFactory extends TokenFilterFactory implements Re
   
   // parses a list of MappingCharFilter style rules into a custom byte[] type table
   private byte[] parseTypes(List<String> rules) {
-    SortedMap<Character,Byte> typeMap = new TreeMap<Character,Byte>();
+    SortedMap<Character,Byte> typeMap = new TreeMap<>();
     for( String rule : rules ){
       Matcher m = typePattern.matcher(rule);
       if( !m.find() )
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java lucene/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java
index 3fbab34..3904919 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java
@@ -82,7 +82,7 @@ public final class DutchAnalyzer extends Analyzer {
         throw new RuntimeException("Unable to load default stopword set");
       }
       
-      DEFAULT_STEM_DICT = new CharArrayMap<String>(Version.LUCENE_CURRENT, 4, false);
+      DEFAULT_STEM_DICT = new CharArrayMap<>(Version.LUCENE_CURRENT, 4, false);
       DEFAULT_STEM_DICT.put("fiets", "fiets"); //otherwise fiet
       DEFAULT_STEM_DICT.put("bromfiets", "bromfiets"); //otherwise bromfiet
       DEFAULT_STEM_DICT.put("ei", "eier");
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/path/ReversePathHierarchyTokenizer.java lucene/analysis/common/src/java/org/apache/lucene/analysis/path/ReversePathHierarchyTokenizer.java
index 5ae5ae2..71db68d 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/path/ReversePathHierarchyTokenizer.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/path/ReversePathHierarchyTokenizer.java
@@ -98,7 +98,7 @@ public class ReversePathHierarchyTokenizer extends Tokenizer {
     this.skip = skip;
     resultToken = new StringBuilder(bufferSize);
     resultTokenBuffer = new char[bufferSize];
-    delimiterPositions = new ArrayList<Integer>(bufferSize/10);
+    delimiterPositions = new ArrayList<>(bufferSize/10);
   }
 
   private static final int DEFAULT_BUFFER_SIZE = 1024;
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/pt/RSLPStemmerBase.java lucene/analysis/common/src/java/org/apache/lucene/analysis/pt/RSLPStemmerBase.java
index 0915d53..f8da034 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/pt/RSLPStemmerBase.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/pt/RSLPStemmerBase.java
@@ -248,7 +248,7 @@ public abstract class RSLPStemmerBase {
     try {
       InputStream is = clazz.getResourceAsStream(resource);
       LineNumberReader r = new LineNumberReader(new InputStreamReader(is, "UTF-8"));
-      Map<String,Step> steps = new HashMap<String,Step>();
+      Map<String,Step> steps = new HashMap<>();
       String step;
       while ((step = readLine(r)) != null) {
         Step s = parseStep(r, step);
@@ -285,7 +285,7 @@ public abstract class RSLPStemmerBase {
   }
   
   private static Rule[] parseRules(LineNumberReader r, int type) throws IOException {
-    List<Rule> rules = new ArrayList<Rule>();
+    List<Rule> rules = new ArrayList<>();
     String line;
     while ((line = readLine(r)) != null) {
       Matcher matcher = stripPattern.matcher(line);
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java
index 9dfdf7d..8a4b8aa 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java
@@ -46,7 +46,7 @@ import org.apache.lucene.util.Version;
 public final class QueryAutoStopWordAnalyzer extends AnalyzerWrapper {
 
   private final Analyzer delegate;
-  private final Map<String, Set<String>> stopWordsPerField = new HashMap<String, Set<String>>();
+  private final Map<String, Set<String>> stopWordsPerField = new HashMap<>();
   //The default maximum percentage (40%) of index documents which
   //can contain a term, after which the term is considered to be a stop word.
   public static final float defaultMaxDocFreqPercent = 0.4f;
@@ -153,7 +153,7 @@ public final class QueryAutoStopWordAnalyzer extends AnalyzerWrapper {
     this.delegate = delegate;
     
     for (String field : fields) {
-      Set<String> stopWords = new HashSet<String>();
+      Set<String> stopWords = new HashSet<>();
       Terms terms = MultiFields.getTerms(indexReader, field);
       CharsRef spare = new CharsRef();
       if (terms != null) {
@@ -204,7 +204,7 @@ public final class QueryAutoStopWordAnalyzer extends AnalyzerWrapper {
    * @return the stop words (as terms)
    */
   public Term[] getStopWords() {
-    List<Term> allStopWords = new ArrayList<Term>();
+    List<Term> allStopWords = new ArrayList<>();
     for (String fieldName : stopWordsPerField.keySet()) {
       Set<String> stopWords = stopWordsPerField.get(fieldName);
       for (String text : stopWords) {
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java lucene/analysis/common/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java
index 507a9d9..fb105f5 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java
@@ -74,7 +74,7 @@ public final class ShingleFilter extends TokenFilter {
    * that will be composed to form output shingles.
    */
   private LinkedList<InputWindowToken> inputWindow
-    = new LinkedList<InputWindowToken>();
+    = new LinkedList<>();
   
   /**
    * The number of input tokens in the next output token.  This is the "n" in
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/sinks/TeeSinkTokenFilter.java lucene/analysis/common/src/java/org/apache/lucene/analysis/sinks/TeeSinkTokenFilter.java
index 80a0ce2..69e90a9 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/sinks/TeeSinkTokenFilter.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/sinks/TeeSinkTokenFilter.java
@@ -75,7 +75,7 @@ sink2.consumeAllTokens();
  * <p>Note, the EntityDetect and URLDetect TokenStreams are for the example and do not currently exist in Lucene.
  */
 public final class TeeSinkTokenFilter extends TokenFilter {
-  private final List<WeakReference<SinkTokenStream>> sinks = new LinkedList<WeakReference<SinkTokenStream>>();
+  private final List<WeakReference<SinkTokenStream>> sinks = new LinkedList<>();
   
   /**
    * Instantiates a new TeeSinkTokenFilter.
@@ -98,7 +98,7 @@ public final class TeeSinkTokenFilter extends TokenFilter {
    */
   public SinkTokenStream newSinkTokenStream(SinkFilter filter) {
     SinkTokenStream sink = new SinkTokenStream(this.cloneAttributes(), filter);
-    this.sinks.add(new WeakReference<SinkTokenStream>(sink));
+    this.sinks.add(new WeakReference<>(sink));
     return sink;
   }
   
@@ -116,7 +116,7 @@ public final class TeeSinkTokenFilter extends TokenFilter {
     for (Iterator<AttributeImpl> it = this.cloneAttributes().getAttributeImplsIterator(); it.hasNext(); ) {
       sink.addAttributeImpl(it.next());
     }
-    this.sinks.add(new WeakReference<SinkTokenStream>(sink));
+    this.sinks.add(new WeakReference<>(sink));
   }
   
   /**
@@ -186,7 +186,7 @@ public final class TeeSinkTokenFilter extends TokenFilter {
    * TokenStream output from a tee with optional filtering.
    */
   public static final class SinkTokenStream extends TokenStream {
-    private final List<AttributeSource.State> cachedStates = new LinkedList<AttributeSource.State>();
+    private final List<AttributeSource.State> cachedStates = new LinkedList<>();
     private AttributeSource.State finalState;
     private Iterator<AttributeSource.State> it = null;
     private SinkFilter filter;
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser.java lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser.java
index 7afa491..1817055 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser.java
@@ -130,7 +130,7 @@ public class SolrSynonymParser extends SynonymMap.Parser {
   }
   
   private static String[] split(String s, String separator) {
-    ArrayList<String> list = new ArrayList<String>(2);
+    ArrayList<String> list = new ArrayList<>(2);
     StringBuilder sb = new StringBuilder();
     int pos=0, end=s.length();
     while (pos < end) {
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java
index a833667..df87946 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java
@@ -282,7 +282,7 @@ public final class SynonymFilter extends TokenFilter {
 
     //System.out.println("FSTFilt maxH=" + synonyms.maxHorizontalContext);
 
-    scratchArc = new FST.Arc<BytesRef>();
+    scratchArc = new FST.Arc<>();
   }
 
   private void capture() {
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilterFactory.java lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilterFactory.java
index 4b962b5..c06b247 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilterFactory.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilterFactory.java
@@ -83,7 +83,7 @@ public class SynonymFilterFactory extends TokenFilterFactory implements Resource
   private final String format;
   private final boolean expand;
   private final String analyzerName;
-  private final Map<String, String> tokArgs = new HashMap<String, String>();
+  private final Map<String, String> tokArgs = new HashMap<>();
 
   private SynonymMap map;
   
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.java lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.java
index 8b455c5..b748c6f 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.java
@@ -67,7 +67,7 @@ public class SynonymMap {
    * @lucene.experimental
    */
   public static class Builder {
-    private final HashMap<CharsRef,MapEntry> workingSet = new HashMap<CharsRef,MapEntry>();
+    private final HashMap<CharsRef,MapEntry> workingSet = new HashMap<>();
     private final BytesRefHash words = new BytesRefHash();
     private final BytesRef utf8Scratch = new BytesRef(8);
     private int maxHorizontalContext;
@@ -82,7 +82,7 @@ public class SynonymMap {
     private static class MapEntry {
       boolean includeOrig;
       // we could sort for better sharing ultimately, but it could confuse people
-      ArrayList<Integer> ords = new ArrayList<Integer>();
+      ArrayList<Integer> ords = new ArrayList<>();
     }
 
     /** Sugar: just joins the provided terms with {@link
@@ -210,7 +210,7 @@ public class SynonymMap {
       ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
       // TODO: are we using the best sharing options?
       org.apache.lucene.util.fst.Builder<BytesRef> builder = 
-        new org.apache.lucene.util.fst.Builder<BytesRef>(FST.INPUT_TYPE.BYTE4, outputs);
+        new org.apache.lucene.util.fst.Builder<>(FST.INPUT_TYPE.BYTE4, outputs);
       
       BytesRef scratch = new BytesRef(64);
       ByteArrayDataOutput scratchOutput = new ByteArrayDataOutput();
@@ -218,7 +218,7 @@ public class SynonymMap {
       final Set<Integer> dedupSet;
 
       if (dedup) {
-        dedupSet = new HashSet<Integer>();
+        dedupSet = new HashSet<>();
       } else {
         dedupSet = null;
       }
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/util/AbstractAnalysisFactory.java lucene/analysis/common/src/java/org/apache/lucene/analysis/util/AbstractAnalysisFactory.java
index 534c166..4ad1473 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/util/AbstractAnalysisFactory.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/util/AbstractAnalysisFactory.java
@@ -65,7 +65,7 @@ public abstract class AbstractAnalysisFactory {
    * Initialize this factory via a set of key-value pairs.
    */
   protected AbstractAnalysisFactory(Map<String,String> args) {
-    originalArgs = Collections.unmodifiableMap(new HashMap<String,String>(args));
+    originalArgs = Collections.unmodifiableMap(new HashMap<>(args));
     String version = get(args, LUCENE_MATCH_VERSION_PARAM);
     luceneMatchVersion = version == null ? null : Version.parseLeniently(version);
     args.remove(CLASS_NAME);  // consume the class arg
@@ -202,7 +202,7 @@ public abstract class AbstractAnalysisFactory {
       Set<String> set = null;
       Matcher matcher = ITEM_PATTERN.matcher(s);
       if (matcher.find()) {
-        set = new HashSet<String>();
+        set = new HashSet<>();
         set.add(matcher.group(0));
         while (matcher.find()) {
           set.add(matcher.group(0));
@@ -296,7 +296,7 @@ public abstract class AbstractAnalysisFactory {
     if (fileNames == null)
       return Collections.<String>emptyList();
 
-    List<String> result = new ArrayList<String>();
+    List<String> result = new ArrayList<>();
     for (String file : fileNames.split("(?<!\\\\),")) {
       result.add(file.replaceAll("\\\\(?=,)", ""));
     }
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/util/AnalysisSPILoader.java lucene/analysis/common/src/java/org/apache/lucene/analysis/util/AnalysisSPILoader.java
index bc25ebb..4b2abad 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/util/AnalysisSPILoader.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/util/AnalysisSPILoader.java
@@ -73,7 +73,7 @@ final class AnalysisSPILoader<S extends AbstractAnalysisFactory> {
    */
   public synchronized void reload(ClassLoader classloader) {
     final LinkedHashMap<String,Class<? extends S>> services =
-      new LinkedHashMap<String,Class<? extends S>>(this.services);
+      new LinkedHashMap<>(this.services);
     final SPIClassIterator<S> loader = SPIClassIterator.get(clazz, classloader);
     while (loader.hasNext()) {
       final Class<? extends S> service = loader.next();
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharArrayMap.java lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharArrayMap.java
index 442bf92..59182d2 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharArrayMap.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharArrayMap.java
@@ -559,7 +559,7 @@ public class CharArrayMap<V> extends AbstractMap<Object,V> {
       return emptyMap();
     if (map instanceof UnmodifiableCharArrayMap)
       return map;
-    return new UnmodifiableCharArrayMap<V>(map);
+    return new UnmodifiableCharArrayMap<>(map);
   }
 
   /**
@@ -595,12 +595,12 @@ public class CharArrayMap<V> extends AbstractMap<Object,V> {
       System.arraycopy(m.keys, 0, keys, 0, keys.length);
       final V[] values = (V[]) new Object[m.values.length];
       System.arraycopy(m.values, 0, values, 0, values.length);
-      m = new CharArrayMap<V>(m);
+      m = new CharArrayMap<>(m);
       m.keys = keys;
       m.values = values;
       return m;
     }
-    return new CharArrayMap<V>(matchVersion, map, false);
+    return new CharArrayMap<>(matchVersion, map, false);
   }
   
   /** Returns an empty, unmodifiable map. */
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharArraySet.java lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharArraySet.java
index 1c73b78..109f247 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharArraySet.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharArraySet.java
@@ -74,7 +74,7 @@ public class CharArraySet extends AbstractSet<Object> {
    *          otherwise <code>true</code>.
    */
   public CharArraySet(Version matchVersion, int startSize, boolean ignoreCase) {
-    this(new CharArrayMap<Object>(matchVersion, startSize, ignoreCase));
+    this(new CharArrayMap<>(matchVersion, startSize, ignoreCase));
   }
 
   /**
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharFilterFactory.java lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharFilterFactory.java
index b2d0799..fb83288 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharFilterFactory.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharFilterFactory.java
@@ -30,7 +30,7 @@ import org.apache.lucene.analysis.CharFilter;
 public abstract class CharFilterFactory extends AbstractAnalysisFactory {
 
   private static final AnalysisSPILoader<CharFilterFactory> loader =
-      new AnalysisSPILoader<CharFilterFactory>(CharFilterFactory.class);
+      new AnalysisSPILoader<>(CharFilterFactory.class);
   
   /** looks up a charfilter by name from context classpath */
   public static CharFilterFactory forName(String name, Map<String,String> args) {
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/util/TokenFilterFactory.java lucene/analysis/common/src/java/org/apache/lucene/analysis/util/TokenFilterFactory.java
index 0015114..12cb556 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/util/TokenFilterFactory.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/util/TokenFilterFactory.java
@@ -29,7 +29,7 @@ import org.apache.lucene.analysis.TokenStream;
 public abstract class TokenFilterFactory extends AbstractAnalysisFactory {
 
   private static final AnalysisSPILoader<TokenFilterFactory> loader =
-      new AnalysisSPILoader<TokenFilterFactory>(TokenFilterFactory.class,
+      new AnalysisSPILoader<>(TokenFilterFactory.class,
           new String[] { "TokenFilterFactory", "FilterFactory" });
   
   /** looks up a tokenfilter by name from context classpath */
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/util/TokenizerFactory.java lucene/analysis/common/src/java/org/apache/lucene/analysis/util/TokenizerFactory.java
index 6d4bbad..3436930 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/util/TokenizerFactory.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/util/TokenizerFactory.java
@@ -31,7 +31,7 @@ import java.util.Set;
 public abstract class TokenizerFactory extends AbstractAnalysisFactory {
 
   private static final AnalysisSPILoader<TokenizerFactory> loader =
-      new AnalysisSPILoader<TokenizerFactory>(TokenizerFactory.class);
+      new AnalysisSPILoader<>(TokenizerFactory.class);
   
   /** looks up a tokenizer by name from context classpath */
   public static TokenizerFactory forName(String name, Map<String,String> args) {
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/util/WordlistLoader.java lucene/analysis/common/src/java/org/apache/lucene/analysis/util/WordlistLoader.java
index 5f97fc7..8fec2c0 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/util/WordlistLoader.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/util/WordlistLoader.java
@@ -219,7 +219,7 @@ public class WordlistLoader {
     try {
       input = getBufferedReader(IOUtils.getDecodingReader(stream, charset));
 
-      lines = new ArrayList<String>();
+      lines = new ArrayList<>();
       for (String word=null; (word=input.readLine())!=null;) {
         // skip initial bom marker
         if (lines.isEmpty() && word.length() > 0 && word.charAt(0) == '\uFEFF')
diff --git lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizer.java lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizer.java
index 6fbd807..b168b8a 100644
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizer.java
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizer.java
@@ -215,7 +215,7 @@ public final class WikipediaTokenizer extends Tokenizer {
     int lastPos = theStart + numAdded;
     int tmpTokType;
     int numSeen = 0;
-    List<AttributeSource.State> tmp = new ArrayList<AttributeSource.State>();
+    List<AttributeSource.State> tmp = new ArrayList<>();
     setupSavedToken(0, type);
     tmp.add(captureState());
     //while we can get a token and that token is the same type and we have not transitioned to a new wiki-item of the same type
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/HTMLStripCharFilterTest.java lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/HTMLStripCharFilterTest.java
index 9c6c730..85e4d69 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/HTMLStripCharFilterTest.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/HTMLStripCharFilterTest.java
@@ -114,7 +114,7 @@ public class HTMLStripCharFilterTest extends BaseTokenStreamTestCase {
   public void testGamma() throws Exception {
     String test = "&Gamma;";
     String gold = "\u0393";
-    Set<String> set = new HashSet<String>();
+    Set<String> set = new HashSet<>();
     set.add("reserved");
     Reader reader = new HTMLStripCharFilter(new StringReader(test), set);
     StringBuilder builder = new StringBuilder();
@@ -129,7 +129,7 @@ public class HTMLStripCharFilterTest extends BaseTokenStreamTestCase {
   public void testEntities() throws Exception {
     String test = "&nbsp; &lt;foo&gt; &Uuml;bermensch &#61; &Gamma; bar &#x393;";
     String gold = "  <foo> \u00DCbermensch = \u0393 bar \u0393";
-    Set<String> set = new HashSet<String>();
+    Set<String> set = new HashSet<>();
     set.add("reserved");
     Reader reader = new HTMLStripCharFilter(new StringReader(test), set);
     StringBuilder builder = new StringBuilder();
@@ -144,7 +144,7 @@ public class HTMLStripCharFilterTest extends BaseTokenStreamTestCase {
   public void testMoreEntities() throws Exception {
     String test = "&nbsp; &lt;junk/&gt; &nbsp; &#33; &#64; and &#8217;";
     String gold = "  <junk/>   ! @ and ’";
-    Set<String> set = new HashSet<String>();
+    Set<String> set = new HashSet<>();
     set.add("reserved");
     Reader reader = new HTMLStripCharFilter(new StringReader(test), set);
     StringBuilder builder = new StringBuilder();
@@ -158,7 +158,7 @@ public class HTMLStripCharFilterTest extends BaseTokenStreamTestCase {
 
   public void testReserved() throws Exception {
     String test = "aaa bbb <reserved ccc=\"ddddd\"> eeee </reserved> ffff <reserved ggg=\"hhhh\"/> <other/>";
-    Set<String> set = new HashSet<String>();
+    Set<String> set = new HashSet<>();
     set.add("reserved");
     Reader reader = new HTMLStripCharFilter(new StringReader(test), set);
     StringBuilder builder = new StringBuilder();
@@ -588,7 +588,7 @@ public class HTMLStripCharFilterTest extends BaseTokenStreamTestCase {
   public void testEscapeScript() throws Exception {
     String test = "one<script no-value-attr>callSomeMethod();</script>two";
     String gold = "one<script no-value-attr></script>two";
-    Set<String> escapedTags = new HashSet<String>(Arrays.asList("SCRIPT"));
+    Set<String> escapedTags = new HashSet<>(Arrays.asList("SCRIPT"));
     Reader reader = new HTMLStripCharFilter
         (new StringReader(test), escapedTags);
     int ch = 0;
@@ -628,7 +628,7 @@ public class HTMLStripCharFilterTest extends BaseTokenStreamTestCase {
   public void testEscapeStyle() throws Exception {
     String test = "one<style type=\"text/css\"> body,font,a { font-family:arial; } </style>two";
     String gold = "one<style type=\"text/css\"></style>two";
-    Set<String> escapedTags = new HashSet<String>(Arrays.asList("STYLE"));
+    Set<String> escapedTags = new HashSet<>(Arrays.asList("STYLE"));
     Reader reader = new HTMLStripCharFilter
         (new StringReader(test), escapedTags);
     int ch = 0;
@@ -668,7 +668,7 @@ public class HTMLStripCharFilterTest extends BaseTokenStreamTestCase {
   public void testEscapeBR() throws Exception {
     String test = "one<BR class='whatever'>two</\nBR\n>";
     String gold = "one<BR class='whatever'>two</\nBR\n>";
-    Set<String> escapedTags = new HashSet<String>(Arrays.asList("BR"));
+    Set<String> escapedTags = new HashSet<>(Arrays.asList("BR"));
     Reader reader = new HTMLStripCharFilter
         (new StringReader(test), escapedTags);
     int ch = 0;
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter.java lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter.java
index 55975a3..e6f5e95 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter.java
@@ -270,7 +270,7 @@ public class TestMappingCharFilter extends BaseTokenStreamTestCase {
     Random random = random();
     NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();
     // we can't add duplicate keys, or NormalizeCharMap gets angry
-    Set<String> keys = new HashSet<String>();
+    Set<String> keys = new HashSet<>();
     int num = random.nextInt(5);
     //System.out.println("NormalizeCharMap=");
     for (int i = 0; i < num; i++) {
@@ -296,7 +296,7 @@ public class TestMappingCharFilter extends BaseTokenStreamTestCase {
 
       final char endLetter = (char) TestUtil.nextInt(random, 'b', 'z');
 
-      final Map<String,String> map = new HashMap<String,String>();
+      final Map<String,String> map = new HashMap<>();
       final NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();
       final int numMappings = atLeast(5);
       if (VERBOSE) {
@@ -333,7 +333,7 @@ public class TestMappingCharFilter extends BaseTokenStreamTestCase {
         final StringBuilder output = new StringBuilder();
 
         // Maps output offset to input offset:
-        final List<Integer> inputOffsets = new ArrayList<Integer>();
+        final List<Integer> inputOffsets = new ArrayList<>();
 
         int cumDiff = 0;
         int charIdx = 0;
@@ -416,7 +416,7 @@ public class TestMappingCharFilter extends BaseTokenStreamTestCase {
         final MappingCharFilter mapFilter = new MappingCharFilter(charMap, new StringReader(content));
 
         final StringBuilder actualBuilder = new StringBuilder();
-        final List<Integer> actualInputOffsets = new ArrayList<Integer>();
+        final List<Integer> actualInputOffsets = new ArrayList<>();
 
         // Now consume the actual mapFilter, somewhat randomly:
         while (true) {
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestAllAnalyzersHaveFactories.java lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestAllAnalyzersHaveFactories.java
index 4c69d00..b410fab 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestAllAnalyzersHaveFactories.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestAllAnalyzersHaveFactories.java
@@ -117,7 +117,7 @@ public class TestAllAnalyzersHaveFactories extends LuceneTestCase {
         continue;
       }
       
-      Map<String,String> args = new HashMap<String,String>();
+      Map<String,String> args = new HashMap<>();
       args.put("luceneMatchVersion", TEST_VERSION_CURRENT.toString());
       
       if (Tokenizer.class.isAssignableFrom(c)) {
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFactories.java lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFactories.java
index 97da57f..f1708ea 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFactories.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFactories.java
@@ -122,7 +122,7 @@ public class TestFactories extends BaseTokenStreamTestCase {
   
   /** tries to initialize a factory with no arguments */
   private AbstractAnalysisFactory initialize(Class<? extends AbstractAnalysisFactory> factoryClazz) throws IOException {
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     args.put("luceneMatchVersion", TEST_VERSION_CURRENT.toString());
     Constructor<? extends AbstractAnalysisFactory> ctor;
     try {
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java
index 617e752..8a91a9e 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java
@@ -110,7 +110,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
     };
   };
 
-  private static final Map<Constructor<?>,Predicate<Object[]>> brokenConstructors = new HashMap<Constructor<?>, Predicate<Object[]>>();
+  private static final Map<Constructor<?>,Predicate<Object[]>> brokenConstructors = new HashMap<>();
   static {
     try {
       brokenConstructors.put(
@@ -158,7 +158,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
 
   // TODO: also fix these and remove (maybe):
   // Classes/options that don't produce consistent graph offsets:
-  private static final Map<Constructor<?>,Predicate<Object[]>> brokenOffsetsConstructors = new HashMap<Constructor<?>, Predicate<Object[]>>();
+  private static final Map<Constructor<?>,Predicate<Object[]>> brokenOffsetsConstructors = new HashMap<>();
   static {
     try {
       for (Class<?> c : Arrays.<Class<?>>asList(
@@ -188,9 +188,9 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
   @BeforeClass
   public static void beforeClass() throws Exception {
     List<Class<?>> analysisClasses = getClassesForPackage("org.apache.lucene.analysis");
-    tokenizers = new ArrayList<Constructor<? extends Tokenizer>>();
-    tokenfilters = new ArrayList<Constructor<? extends TokenFilter>>();
-    charfilters = new ArrayList<Constructor<? extends CharFilter>>();
+    tokenizers = new ArrayList<>();
+    tokenfilters = new ArrayList<>();
+    charfilters = new ArrayList<>();
     for (final Class<?> c : analysisClasses) {
       final int modifiers = c.getModifiers();
       if (
@@ -257,7 +257,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
   }
   
   public static List<Class<?>> getClassesForPackage(String pckgname) throws Exception {
-    final List<Class<?>> classes = new ArrayList<Class<?>>();
+    final List<Class<?>> classes = new ArrayList<>();
     collectClassesForPackage(pckgname, classes);
     assertFalse("No classes found in package '"+pckgname+"'; maybe your test classes are packaged as JAR file?", classes.isEmpty());
     return classes;
@@ -358,7 +358,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
     put(Set.class, new ArgProducer() {
       @Override public Object create(Random random) {
         // TypeTokenFilter
-        Set<String> set = new HashSet<String>();
+        Set<String> set = new HashSet<>();
         int num = random.nextInt(5);
         for (int i = 0; i < num; i++) {
           set.add(StandardTokenizer.TOKEN_TYPES[random.nextInt(StandardTokenizer.TOKEN_TYPES.length)]);
@@ -369,7 +369,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
     put(Collection.class, new ArgProducer() {
       @Override public Object create(Random random) {
         // CapitalizationFilter
-        Collection<char[]> col = new ArrayList<char[]>();
+        Collection<char[]> col = new ArrayList<>();
         int num = random.nextInt(5);
         for (int i = 0; i < num; i++) {
           col.add(TestUtil.randomSimpleString(random).toCharArray());
@@ -459,7 +459,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
       @Override public Object create(Random random) {
         NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();
         // we can't add duplicate keys, or NormalizeCharMap gets angry
-        Set<String> keys = new HashSet<String>();
+        Set<String> keys = new HashSet<>();
         int num = random.nextInt(5);
         //System.out.println("NormalizeCharMap=");
         for (int i = 0; i < num; i++) {
@@ -489,7 +489,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
     put(CharArrayMap.class, new ArgProducer() {
       @Override public Object create(Random random) {
         int num = random.nextInt(10);
-        CharArrayMap<String> map = new CharArrayMap<String>(TEST_VERSION_CURRENT, num, random.nextBoolean());
+        CharArrayMap<String> map = new CharArrayMap<>(TEST_VERSION_CURRENT, num, random.nextBoolean());
         for (int i = 0; i < num; i++) {
           // TODO: make nastier
           map.put(TestUtil.randomSimpleString(random), TestUtil.randomSimpleString(random));
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopAnalyzer.java lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopAnalyzer.java
index 0656c28..9838fe1 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopAnalyzer.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopAnalyzer.java
@@ -32,7 +32,7 @@ import java.util.HashSet;
 public class TestStopAnalyzer extends BaseTokenStreamTestCase {
   
   private StopAnalyzer stop = new StopAnalyzer(TEST_VERSION_CURRENT);
-  private Set<Object> inValidTokens = new HashSet<Object>();
+  private Set<Object> inValidTokens = new HashSet<>();
 
   @Override
   public void setUp() throws Exception {
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopFilter.java lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopFilter.java
index 0d6c26d..8f43c01 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopFilter.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopFilter.java
@@ -59,7 +59,7 @@ public class TestStopFilter extends BaseTokenStreamTestCase {
    */
   public void testStopPositons() throws IOException {
     StringBuilder sb = new StringBuilder();
-    ArrayList<String> a = new ArrayList<String>();
+    ArrayList<String> a = new ArrayList<>();
     for (int i=0; i<20; i++) {
       String w = English.intToEnglish(i).trim();
       sb.append(w).append(" ");
@@ -76,8 +76,8 @@ public class TestStopFilter extends BaseTokenStreamTestCase {
     StopFilter stpf = new StopFilter(Version.LUCENE_40, in, stopSet);
     doTestStopPositons(stpf);
     // with increments, concatenating two stop filters
-    ArrayList<String> a0 = new ArrayList<String>();
-    ArrayList<String> a1 = new ArrayList<String>();
+    ArrayList<String> a0 = new ArrayList<>();
+    ArrayList<String> a1 = new ArrayList<>();
     for (int i=0; i<a.size(); i++) {
       if (i%2==0) { 
         a0.add(a.get(i));
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailTokenizer.java lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailTokenizer.java
index b497f6c..b433d30 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailTokenizer.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailTokenizer.java
@@ -287,7 +287,7 @@ public class TestUAX29URLEmailTokenizer extends BaseTokenStreamTestCase {
     BufferedReader bufferedReader = null;
     String[] urls;
     try {
-      List<String> urlList = new ArrayList<String>();
+      List<String> urlList = new ArrayList<>();
       bufferedReader = new BufferedReader(new InputStreamReader
         (getClass().getResourceAsStream("LuceneResourcesWikiPageURLs.txt"), "UTF-8"));
       String line;
@@ -331,7 +331,7 @@ public class TestUAX29URLEmailTokenizer extends BaseTokenStreamTestCase {
     BufferedReader bufferedReader = null;
     String[] emails;
     try {
-      List<String> emailList = new ArrayList<String>();
+      List<String> emailList = new ArrayList<>();
       bufferedReader = new BufferedReader(new InputStreamReader
         (getClass().getResourceAsStream
           ("email.addresses.from.random.text.with.email.addresses.txt"), "UTF-8"));
@@ -401,7 +401,7 @@ public class TestUAX29URLEmailTokenizer extends BaseTokenStreamTestCase {
     BufferedReader bufferedReader = null;
     String[] urls;
     try {
-      List<String> urlList = new ArrayList<String>();
+      List<String> urlList = new ArrayList<>();
       bufferedReader = new BufferedReader(new InputStreamReader
         (getClass().getResourceAsStream
           ("urls.from.random.text.with.urls.txt"), "UTF-8"));
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestKStemmer.java lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestKStemmer.java
index c982b65..0b07d3c 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestKStemmer.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestKStemmer.java
@@ -73,7 +73,7 @@ public class TestKStemmer extends BaseTokenStreamTestCase {
     // tf = new KStemFilter(tf);
 
     KStemmer kstem = new KStemmer();
-    Map<String,String> map = new TreeMap<String,String>();
+    Map<String,String> map = new TreeMap<>();
     for(;;) {
       Token t = tf.next();
       if (t==null) break;
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestASCIIFoldingFilter.java lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestASCIIFoldingFilter.java
index 81495b1..19ceecc 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestASCIIFoldingFilter.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestASCIIFoldingFilter.java
@@ -1888,8 +1888,8 @@ public class TestASCIIFoldingFilter extends BaseTokenStreamTestCase {
     };
 
     // Construct input text and expected output tokens
-    List<String> expectedUnfoldedTokens = new ArrayList<String>();
-    List<String> expectedFoldedTokens = new ArrayList<String>();
+    List<String> expectedUnfoldedTokens = new ArrayList<>();
+    List<String> expectedFoldedTokens = new ArrayList<>();
     StringBuilder inputText = new StringBuilder();
     for (int n = 0 ; n < foldings.length ; n += 2) {
       if (n > 0) {
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestCapitalizationFilter.java lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestCapitalizationFilter.java
index a6d78e6..00ef72e 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestCapitalizationFilter.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestCapitalizationFilter.java
@@ -78,7 +78,7 @@ public class TestCapitalizationFilter extends BaseTokenStreamTestCase {
         true, keep, true, null, 0, DEFAULT_MAX_WORD_COUNT, DEFAULT_MAX_TOKEN_LENGTH);
     
     // Now try some prefixes
-    List<char[]> okPrefix = new ArrayList<char[]>();
+    List<char[]> okPrefix = new ArrayList<>();
     okPrefix.add("McK".toCharArray());
     
     assertCapitalizesTo("McKinley", 
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeepWordFilter.java lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeepWordFilter.java
index d0df8b8..465c54c 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeepWordFilter.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeepWordFilter.java
@@ -32,7 +32,7 @@ public class TestKeepWordFilter extends BaseTokenStreamTestCase {
   
   public void testStopAndGo() throws Exception 
   {  
-    Set<String> words = new HashSet<String>();
+    Set<String> words = new HashSet<>();
     words.add( "aaa" );
     words.add( "bbb" );
     
@@ -51,7 +51,7 @@ public class TestKeepWordFilter extends BaseTokenStreamTestCase {
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    final Set<String> words = new HashSet<String>();
+    final Set<String> words = new HashSet<>();
     words.add( "a" );
     words.add( "b" );
     
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalyzerWrapper.java lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalyzerWrapper.java
index 0bf5b04..91f929b 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalyzerWrapper.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalyzerWrapper.java
@@ -31,7 +31,7 @@ public class TestPerFieldAnalyzerWrapper extends BaseTokenStreamTestCase {
   public void testPerField() throws Exception {
     String text = "Qwerty";
 
-    Map<String, Analyzer> analyzerPerField = new HashMap<String, Analyzer>();
+    Map<String, Analyzer> analyzerPerField = new HashMap<>();
     analyzerPerField.put("special", new SimpleAnalyzer(TEST_VERSION_CURRENT));
 
     PerFieldAnalyzerWrapper analyzer =
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestStemmerOverrideFilter.java lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestStemmerOverrideFilter.java
index 17f2fbb..a75bfa0 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestStemmerOverrideFilter.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestStemmerOverrideFilter.java
@@ -78,7 +78,7 @@ public class TestStemmerOverrideFilter extends BaseTokenStreamTestCase {
   }
   
   public void testRandomRealisticWhiteSpace() throws IOException {
-    Map<String,String> map = new HashMap<String,String>();
+    Map<String,String> map = new HashMap<>();
     int numTerms = atLeast(50);
     for (int i = 0; i < numTerms; i++) {
       String randomRealisticUnicodeString = TestUtil
@@ -105,7 +105,7 @@ public class TestStemmerOverrideFilter extends BaseTokenStreamTestCase {
     StemmerOverrideFilter.Builder builder = new StemmerOverrideFilter.Builder(random().nextBoolean());
     Set<Entry<String,String>> entrySet = map.entrySet();
     StringBuilder input = new StringBuilder();
-    List<String> output = new ArrayList<String>();
+    List<String> output = new ArrayList<>();
     for (Entry<String,String> entry : entrySet) {
       builder.add(entry.getKey(), entry.getValue());
       if (random().nextBoolean() || output.isEmpty()) {
@@ -121,7 +121,7 @@ public class TestStemmerOverrideFilter extends BaseTokenStreamTestCase {
   }
   
   public void testRandomRealisticKeyword() throws IOException {
-    Map<String,String> map = new HashMap<String,String>();
+    Map<String,String> map = new HashMap<>();
     int numTerms = atLeast(50);
     for (int i = 0; i < numTerms; i++) {
       String randomRealisticUnicodeString = TestUtil
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java
index c1c5829..45ba5d5 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java
@@ -210,7 +210,7 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
   @Test
   public void testPositionIncrements() throws Exception {
     final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;
-    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList("NUTCH")), false);
+    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<>(Arrays.asList("NUTCH")), false);
     
     /* analyzer that uses whitespace + wdf */
     Analyzer a = new Analyzer() {
@@ -332,7 +332,7 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
       final int flags = random().nextInt(512);
       final CharArraySet protectedWords;
       if (random().nextBoolean()) {
-        protectedWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList("a", "b", "cd")), false);
+        protectedWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<>(Arrays.asList("a", "b", "cd")), false);
       } else {
         protectedWords = null;
       }
@@ -355,7 +355,7 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
       final int flags = i;
       final CharArraySet protectedWords;
       if (random.nextBoolean()) {
-        protectedWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList("a", "b", "cd")), false);
+        protectedWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<>(Arrays.asList("a", "b", "cd")), false);
       } else {
         protectedWords = null;
       }
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer.java lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer.java
index e83ecb0..ba1d4a5 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer.java
@@ -78,7 +78,7 @@ public class TestPatternTokenizer extends BaseTokenStreamTestCase
     final String INPUT = "G&uuml;nther G&uuml;nther is here";
 
     // create MappingCharFilter
-    List<String> mappingRules = new ArrayList<String>();
+    List<String> mappingRules = new ArrayList<>();
     mappingRules.add( "\"&uuml;\" => \"ü\"" );
     NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();
     builder.add("&uuml;", "ü");
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymMapFilter.java lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymMapFilter.java
index ceabc4a..b495e8e 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymMapFilter.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymMapFilter.java
@@ -396,8 +396,8 @@ public class TestSynonymMapFilter extends BaseTokenStreamTestCase {
     final int numSyn = atLeast(5);
     //final int numSyn = 2;
 
-    final Map<String,OneSyn> synMap = new HashMap<String,OneSyn>();
-    final List<OneSyn> syns = new ArrayList<OneSyn>();
+    final Map<String,OneSyn> synMap = new HashMap<>();
+    final List<OneSyn> syns = new ArrayList<>();
     final boolean dedup = random().nextBoolean();
     if (VERBOSE) {
       System.out.println("  dedup=" + dedup);
@@ -410,7 +410,7 @@ public class TestSynonymMapFilter extends BaseTokenStreamTestCase {
         s = new OneSyn();
         s.in = synIn;
         syns.add(s);
-        s.out = new ArrayList<String>();
+        s.out = new ArrayList<>();
         synMap.put(synIn, s);
         s.keepOrig = random().nextBoolean();
       }
@@ -453,7 +453,7 @@ public class TestSynonymMapFilter extends BaseTokenStreamTestCase {
   }
 
   private void pruneDups(List<OneSyn> syns) {
-    Set<String> seen = new HashSet<String>();
+    Set<String> seen = new HashSet<>();
     for(OneSyn syn : syns) {
       int idx = 0;
       while(idx < syn.out.size()) {
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/util/BaseTokenStreamFactoryTestCase.java lucene/analysis/common/src/test/org/apache/lucene/analysis/util/BaseTokenStreamFactoryTestCase.java
index 746d1fd..8c36689 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/util/BaseTokenStreamFactoryTestCase.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/util/BaseTokenStreamFactoryTestCase.java
@@ -47,7 +47,7 @@ public abstract class BaseTokenStreamFactoryTestCase extends BaseTokenStreamTest
     if (keysAndValues.length % 2 == 1) {
       throw new IllegalArgumentException("invalid keysAndValues map");
     }
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     for (int i = 0; i < keysAndValues.length; i += 2) {
       String previous = args.put(keysAndValues[i], keysAndValues[i+1]);
       assertNull("duplicate values for key: " + keysAndValues[i], previous);
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArrayMap.java lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArrayMap.java
index ce8db40..9c137c2 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArrayMap.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArrayMap.java
@@ -25,8 +25,8 @@ import org.apache.lucene.util.LuceneTestCase;
 
 public class TestCharArrayMap extends LuceneTestCase {
   public void doRandom(int iter, boolean ignoreCase) {
-    CharArrayMap<Integer> map = new CharArrayMap<Integer>(TEST_VERSION_CURRENT, 1, ignoreCase);
-    HashMap<String,Integer> hmap = new HashMap<String,Integer>();
+    CharArrayMap<Integer> map = new CharArrayMap<>(TEST_VERSION_CURRENT, 1, ignoreCase);
+    HashMap<String,Integer> hmap = new HashMap<>();
 
     char[] key;
     for (int i=0; i<iter; i++) {
@@ -64,8 +64,8 @@ public class TestCharArrayMap extends LuceneTestCase {
   }
 
   public void testMethods() {
-    CharArrayMap<Integer> cm = new CharArrayMap<Integer>(TEST_VERSION_CURRENT, 2, false);
-    HashMap<String,Integer> hm = new HashMap<String,Integer>();
+    CharArrayMap<Integer> cm = new CharArrayMap<>(TEST_VERSION_CURRENT, 2, false);
+    HashMap<String,Integer> hm = new HashMap<>();
     hm.put("foo",1);
     hm.put("bar",2);
     cm.putAll(hm);
@@ -133,7 +133,7 @@ public class TestCharArrayMap extends LuceneTestCase {
   }
 
   public void testModifyOnUnmodifiable(){
-    CharArrayMap<Integer> map = new CharArrayMap<Integer>(TEST_VERSION_CURRENT, 2, false);
+    CharArrayMap<Integer> map = new CharArrayMap<>(TEST_VERSION_CURRENT, 2, false);
     map.put("foo",1);
     map.put("bar",2);
     final int size = map.size();
@@ -230,7 +230,7 @@ public class TestCharArrayMap extends LuceneTestCase {
   }
   
   public void testToString() {
-    CharArrayMap<Integer> cm = new CharArrayMap<Integer>(TEST_VERSION_CURRENT, Collections.singletonMap("test",1), false);
+    CharArrayMap<Integer> cm = new CharArrayMap<>(TEST_VERSION_CURRENT, Collections.singletonMap("test",1), false);
     assertEquals("[test]",cm.keySet().toString());
     assertEquals("[1]",cm.values().toString());
     assertEquals("[test=1]",cm.entrySet().toString());
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArraySet.java lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArraySet.java
index db4d30b..9af7447 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArraySet.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArraySet.java
@@ -256,7 +256,7 @@ public class TestCharArraySet extends LuceneTestCase {
     CharArraySet setCaseSensitive = new CharArraySet(TEST_VERSION_CURRENT, 10, false);
 
     List<String> stopwords = Arrays.asList(TEST_STOP_WORDS);
-    List<String> stopwordsUpper = new ArrayList<String>();
+    List<String> stopwordsUpper = new ArrayList<>();
     for (String string : stopwords) {
       stopwordsUpper.add(string.toUpperCase(Locale.ROOT));
     }
@@ -278,7 +278,7 @@ public class TestCharArraySet extends LuceneTestCase {
       assertFalse(copyCaseSens.contains(string));
     }
     // test adding terms to the copy
-    List<String> newWords = new ArrayList<String>();
+    List<String> newWords = new ArrayList<>();
     for (String string : stopwords) {
       newWords.add(string+"_1");
     }
@@ -303,7 +303,7 @@ public class TestCharArraySet extends LuceneTestCase {
     CharArraySet setCaseSensitive = new CharArraySet(TEST_VERSION_CURRENT, 10, false);
 
     List<String> stopwords = Arrays.asList(TEST_STOP_WORDS);
-    List<String> stopwordsUpper = new ArrayList<String>();
+    List<String> stopwordsUpper = new ArrayList<>();
     for (String string : stopwords) {
       stopwordsUpper.add(string.toUpperCase(Locale.ROOT));
     }
@@ -325,7 +325,7 @@ public class TestCharArraySet extends LuceneTestCase {
       assertFalse(copyCaseSens.contains(string));
     }
     // test adding terms to the copy
-    List<String> newWords = new ArrayList<String>();
+    List<String> newWords = new ArrayList<>();
     for (String string : stopwords) {
       newWords.add(string+"_1");
     }
@@ -346,10 +346,10 @@ public class TestCharArraySet extends LuceneTestCase {
    * Test the static #copy() function with a JDK {@link Set} as a source
    */
   public void testCopyJDKSet() {
-    Set<String> set = new HashSet<String>();
+    Set<String> set = new HashSet<>();
 
     List<String> stopwords = Arrays.asList(TEST_STOP_WORDS);
-    List<String> stopwordsUpper = new ArrayList<String>();
+    List<String> stopwordsUpper = new ArrayList<>();
     for (String string : stopwords) {
       stopwordsUpper.add(string.toUpperCase(Locale.ROOT));
     }
@@ -365,7 +365,7 @@ public class TestCharArraySet extends LuceneTestCase {
       assertFalse(copy.contains(string));
     }
     
-    List<String> newWords = new ArrayList<String>();
+    List<String> newWords = new ArrayList<>();
     for (String string : stopwords) {
       newWords.add(string+"_1");
     }
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestElision.java lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestElision.java
index bca3d3f..1d19d21 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestElision.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestElision.java
@@ -51,7 +51,7 @@ public class TestElision extends BaseTokenStreamTestCase {
   }
 
   private List<String> filter(TokenFilter filter) throws IOException {
-    List<String> tas = new ArrayList<String>();
+    List<String> tas = new ArrayList<>();
     CharTermAttribute termAtt = filter.getAttribute(CharTermAttribute.class);
     filter.reset();
     while (filter.incrementToken()) {
diff --git lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest.java lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest.java
index fe7c916..5459556 100644
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest.java
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest.java
@@ -130,7 +130,7 @@ public class WikipediaTokenizerTest extends BaseTokenStreamTestCase {
   }
 
   public void testLucene1133() throws Exception {
-    Set<String> untoks = new HashSet<String>();
+    Set<String> untoks = new HashSet<>();
     untoks.add(WikipediaTokenizer.CATEGORY);
     untoks.add(WikipediaTokenizer.ITALICS);
     //should be exactly the same, regardless of untoks
@@ -150,7 +150,7 @@ public class WikipediaTokenizerTest extends BaseTokenStreamTestCase {
   }
 
   public void testBoth() throws Exception {
-    Set<String> untoks = new HashSet<String>();
+    Set<String> untoks = new HashSet<>();
     untoks.add(WikipediaTokenizer.CATEGORY);
     untoks.add(WikipediaTokenizer.ITALICS);
     String test = "[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]";
diff --git lucene/analysis/common/src/tools/java/org/apache/lucene/analysis/standard/GenerateJflexTLDMacros.java lucene/analysis/common/src/tools/java/org/apache/lucene/analysis/standard/GenerateJflexTLDMacros.java
index 0cfea3f..69b4b39 100644
--- lucene/analysis/common/src/tools/java/org/apache/lucene/analysis/standard/GenerateJflexTLDMacros.java
+++ lucene/analysis/common/src/tools/java/org/apache/lucene/analysis/standard/GenerateJflexTLDMacros.java
@@ -111,7 +111,7 @@ public class GenerateJflexTLDMacros {
    * @throws java.io.IOException if there is a problem downloading the database 
    */
   private SortedSet<String> getIANARootZoneDatabase() throws IOException {
-    final SortedSet<String> TLDs = new TreeSet<String>();
+    final SortedSet<String> TLDs = new TreeSet<>();
     final URLConnection connection = tldFileURL.openConnection();
     connection.setUseCaches(false);
     connection.addRequestProperty("Cache-Control", "no-cache");
diff --git lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ICUTokenizerFactory.java lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ICUTokenizerFactory.java
index b9c5981..307becc 100644
--- lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ICUTokenizerFactory.java
+++ lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ICUTokenizerFactory.java
@@ -84,7 +84,7 @@ public class ICUTokenizerFactory extends TokenizerFactory implements ResourceLoa
   /** Creates a new ICUTokenizerFactory */
   public ICUTokenizerFactory(Map<String,String> args) {
     super(args);
-    tailored = new HashMap<Integer,String>();
+    tailored = new HashMap<>();
     String rulefilesArg = get(args, RULEFILES);
     if (rulefilesArg != null) {
       List<String> scriptAndResourcePaths = splitFileNames(rulefilesArg);
diff --git lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUTransformFilterFactory.java lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUTransformFilterFactory.java
index 9660be7..720570c 100644
--- lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUTransformFilterFactory.java
+++ lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUTransformFilterFactory.java
@@ -32,7 +32,7 @@ public class TestICUTransformFilterFactory extends BaseTokenStreamTestCase {
   /** ensure the transform is working */
   public void test() throws Exception {
     Reader reader = new StringReader("簡化字");
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     args.put("id", "Traditional-Simplified");
     ICUTransformFilterFactory factory = new ICUTransformFilterFactory(args);
     TokenStream stream = whitespaceMockTokenizer(reader);
@@ -44,7 +44,7 @@ public class TestICUTransformFilterFactory extends BaseTokenStreamTestCase {
   public void testForwardDirection() throws Exception {
     // forward
     Reader reader = new StringReader("Российская Федерация");
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     args.put("id", "Cyrillic-Latin");
     ICUTransformFilterFactory factory = new ICUTransformFilterFactory(args);
     TokenStream stream = whitespaceMockTokenizer(reader);
@@ -55,7 +55,7 @@ public class TestICUTransformFilterFactory extends BaseTokenStreamTestCase {
   public void testReverseDirection() throws Exception {
     // backward (invokes Latin-Cyrillic)
     Reader reader = new StringReader("Rossijskaâ Federaciâ");
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     args.put("id", "Cyrillic-Latin");
     args.put("direction", "reverse");
     ICUTransformFilterFactory factory = new ICUTransformFilterFactory(args);
diff --git lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizerFactory.java lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizerFactory.java
index 1ac528f..033da1c 100644
--- lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizerFactory.java
+++ lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizerFactory.java
@@ -44,7 +44,7 @@ public class TestICUTokenizerFactory extends BaseTokenStreamTestCase {
     // “ U+201C LEFT DOUBLE QUOTATION MARK; ” U+201D RIGHT DOUBLE QUOTATION MARK
     Reader reader = new StringReader
         ("  Don't,break.at?/(punct)!  \u201Cnice\u201D\r\n\r\n85_At:all; `really\" +2=3$5,&813 !@#%$^)(*@#$   ");
-    final Map<String,String> args = new HashMap<String,String>();
+    final Map<String,String> args = new HashMap<>();
     args.put(ICUTokenizerFactory.RULEFILES, "Latn:Latin-break-only-on-whitespace.rbbi");
     ICUTokenizerFactory factory = new ICUTokenizerFactory(args);
     factory.inform(new ClasspathResourceLoader(this.getClass()));
@@ -58,7 +58,7 @@ public class TestICUTokenizerFactory extends BaseTokenStreamTestCase {
   public void testTokenizeLatinDontBreakOnHyphens() throws Exception {
     Reader reader = new StringReader
         ("One-two punch.  Brang-, not brung-it.  This one--not that one--is the right one, -ish.");
-    final Map<String,String> args = new HashMap<String,String>();
+    final Map<String,String> args = new HashMap<>();
     args.put(ICUTokenizerFactory.RULEFILES, "Latn:Latin-dont-break-on-hyphens.rbbi");
     ICUTokenizerFactory factory = new ICUTokenizerFactory(args);
     factory.inform(new ClasspathResourceLoader(getClass()));
@@ -78,7 +78,7 @@ public class TestICUTokenizerFactory extends BaseTokenStreamTestCase {
   public void testKeywordTokenizeCyrillicAndThai() throws Exception {
     Reader reader = new StringReader
         ("Some English.  Немного русский.  ข้อความภาษาไทยเล็ก ๆ น้อย ๆ  More English.");
-    final Map<String,String> args = new HashMap<String,String>();
+    final Map<String,String> args = new HashMap<>();
     args.put(ICUTokenizerFactory.RULEFILES, "Cyrl:KeywordTokenizer.rbbi,Thai:KeywordTokenizer.rbbi");
     ICUTokenizerFactory factory = new ICUTokenizerFactory(args);
     factory.inform(new ClasspathResourceLoader(getClass()));
diff --git lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateHTMLStripCharFilterSupplementaryMacros.java lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateHTMLStripCharFilterSupplementaryMacros.java
index e50abbd..b211cc2 100644
--- lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateHTMLStripCharFilterSupplementaryMacros.java
+++ lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateHTMLStripCharFilterSupplementaryMacros.java
@@ -76,7 +76,7 @@ public class GenerateHTMLStripCharFilterSupplementaryMacros {
       System.out.println("\t  []");
     }
 
-    HashMap<Character,UnicodeSet> utf16ByLead = new HashMap<Character,UnicodeSet>();
+    HashMap<Character,UnicodeSet> utf16ByLead = new HashMap<>();
     for (UnicodeSetIterator it = new UnicodeSetIterator(set); it.next();) {
       char utf16[] = Character.toChars(it.codepoint);
       UnicodeSet trails = utf16ByLead.get(utf16[0]);
@@ -87,7 +87,7 @@ public class GenerateHTMLStripCharFilterSupplementaryMacros {
       trails.add(utf16[1]);
     }
     
-    Map<String,UnicodeSet> utf16ByTrail = new HashMap<String,UnicodeSet>();
+    Map<String,UnicodeSet> utf16ByTrail = new HashMap<>();
     for (Map.Entry<Character,UnicodeSet> entry : utf16ByLead.entrySet()) {
       String trail = entry.getValue().getRegexEquivalent();
       UnicodeSet leads = utf16ByTrail.get(trail);
diff --git lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateJFlexSupplementaryMacros.java lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateJFlexSupplementaryMacros.java
index 2b0ba48..3abcd2c 100644
--- lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateJFlexSupplementaryMacros.java
+++ lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateJFlexSupplementaryMacros.java
@@ -95,7 +95,7 @@ public class GenerateJFlexSupplementaryMacros {
       System.out.println("\t  []");
     }
     
-    HashMap<Character,UnicodeSet> utf16ByLead = new HashMap<Character,UnicodeSet>();
+    HashMap<Character,UnicodeSet> utf16ByLead = new HashMap<>();
     for (UnicodeSetIterator it = new UnicodeSetIterator(set); it.next();) {    
       char utf16[] = Character.toChars(it.codepoint);
       UnicodeSet trails = utf16ByLead.get(utf16[0]);
diff --git lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateUTR30DataFiles.java lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateUTR30DataFiles.java
index 9fb5cee..7f1bdfe 100644
--- lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateUTR30DataFiles.java
+++ lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateUTR30DataFiles.java
@@ -188,7 +188,7 @@ public class GenerateUTR30DataFiles {
         if (matcher.matches()) {
           final String leftHandSide = matcher.group(1);
           final String rightHandSide = matcher.group(2).trim();
-          List<String> diacritics = new ArrayList<String>();
+          List<String> diacritics = new ArrayList<>();
           for (String outputCodePoint : rightHandSide.split("\\s+")) {
             int ch = Integer.parseInt(outputCodePoint, 16);
             if (UCharacter.hasBinaryProperty(ch, UProperty.DIACRITIC)
diff --git lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/GraphvizFormatter.java lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/GraphvizFormatter.java
index cb8999f..ac5ba30 100644
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/GraphvizFormatter.java
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/GraphvizFormatter.java
@@ -48,7 +48,7 @@ public class GraphvizFormatter {
   
   public GraphvizFormatter(ConnectionCosts costs) {
     this.costs = costs;
-    this.bestPathMap = new HashMap<String, String>();
+    this.bestPathMap = new HashMap<>();
     sb.append(formatHeader());
     sb.append("  init [style=invis]\n");
     sb.append("  init -> 0.0 [label=\"" + BOS_LABEL + "\"]\n");
diff --git lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseAnalyzer.java lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseAnalyzer.java
index 0e129ad..f738e4a 100644
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseAnalyzer.java
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseAnalyzer.java
@@ -73,7 +73,7 @@ public class JapaneseAnalyzer extends StopwordAnalyzerBase {
       try {
         DEFAULT_STOP_SET = loadStopwordSet(true, JapaneseAnalyzer.class, "stopwords.txt", "#");  // ignore case
         final CharArraySet tagset = loadStopwordSet(false, JapaneseAnalyzer.class, "stoptags.txt", "#");
-        DEFAULT_STOP_TAGS = new HashSet<String>();
+        DEFAULT_STOP_TAGS = new HashSet<>();
         for (Object element : tagset) {
           char chars[] = (char[]) element;
           DEFAULT_STOP_TAGS.add(new String(chars));
diff --git lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapanesePartOfSpeechStopFilterFactory.java lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapanesePartOfSpeechStopFilterFactory.java
index 2a550ae..18cc27a 100644
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapanesePartOfSpeechStopFilterFactory.java
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapanesePartOfSpeechStopFilterFactory.java
@@ -58,7 +58,7 @@ public class JapanesePartOfSpeechStopFilterFactory extends TokenFilterFactory im
     stopTags = null;
     CharArraySet cas = getWordSet(loader, stopTagFiles, false);
     if (cas != null) {
-      stopTags = new HashSet<String>();
+      stopTags = new HashSet<>();
       for (Object element : cas) {
         char chars[] = (char[]) element;
         stopTags.add(new String(chars));
diff --git lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java
index b99ba31..ede01cc 100644
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java
@@ -132,7 +132,7 @@ public final class JapaneseTokenizer extends Tokenizer {
   private static final int MAX_UNKNOWN_WORD_LENGTH = 1024;
   private static final int MAX_BACKTRACE_GAP = 1024;
 
-  private final EnumMap<Type, Dictionary> dictionaryMap = new EnumMap<Type, Dictionary>(Type.class);
+  private final EnumMap<Type, Dictionary> dictionaryMap = new EnumMap<>(Type.class);
 
   private final TokenInfoFST fst;
   private final TokenInfoDictionary dictionary;
@@ -141,7 +141,7 @@ public final class JapaneseTokenizer extends Tokenizer {
   private final UserDictionary userDictionary;
   private final CharacterDefinition characterDefinition;
 
-  private final FST.Arc<Long> arc = new FST.Arc<Long>();
+  private final FST.Arc<Long> arc = new FST.Arc<>();
   private final FST.BytesReader fstReader;
   private final IntsRef wordIdRef = new IntsRef();
 
@@ -174,7 +174,7 @@ public final class JapaneseTokenizer extends Tokenizer {
   private int pos;
 
   // Already parsed, but not yet passed to caller, tokens:
-  private final List<Token> pending = new ArrayList<Token>();
+  private final List<Token> pending = new ArrayList<>();
 
   private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
   private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
diff --git lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/TokenInfoDictionary.java lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/TokenInfoDictionary.java
index 6edcf34..b628343 100644
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/TokenInfoDictionary.java
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/TokenInfoDictionary.java
@@ -44,7 +44,7 @@ public final class TokenInfoDictionary extends BinaryDictionary {
     try {
       is = getResource(FST_FILENAME_SUFFIX);
       is = new BufferedInputStream(is);
-      fst = new FST<Long>(new InputStreamDataInput(is), PositiveIntOutputs.getSingleton());
+      fst = new FST<>(new InputStreamDataInput(is), PositiveIntOutputs.getSingleton());
     } catch (IOException ioe) {
       priorE = ioe;
     } finally {
diff --git lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/TokenInfoFST.java lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/TokenInfoFST.java
index c386910..dfeae8c 100644
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/TokenInfoFST.java
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/TokenInfoFST.java
@@ -51,9 +51,9 @@ public final class TokenInfoFST {
   @SuppressWarnings({"rawtypes","unchecked"})
   private FST.Arc<Long>[] cacheRootArcs() throws IOException {
     FST.Arc<Long> rootCache[] = new FST.Arc[1+(cacheCeiling-0x3040)];
-    FST.Arc<Long> firstArc = new FST.Arc<Long>();
+    FST.Arc<Long> firstArc = new FST.Arc<>();
     fst.getFirstArc(firstArc);
-    FST.Arc<Long> arc = new FST.Arc<Long>();
+    FST.Arc<Long> arc = new FST.Arc<>();
     final FST.BytesReader fstReader = fst.getBytesReader();
     // TODO: jump to 3040, readNextRealArc to ceiling? (just be careful we don't add bugs)
     for (int i = 0; i < rootCache.length; i++) {
diff --git lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java
index 10df235..2b76d2c 100644
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java
@@ -60,7 +60,7 @@ public final class UserDictionary implements Dictionary {
     BufferedReader br = new BufferedReader(reader);
     String line = null;
     int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;
-    List<String[]> featureEntries = new ArrayList<String[]>();
+    List<String[]> featureEntries = new ArrayList<>();
  
     // text, segmentation, readings, POS
     while ((line = br.readLine()) != null) {
@@ -85,11 +85,11 @@ public final class UserDictionary implements Dictionary {
      }
     });
     
-    List<String> data = new ArrayList<String>(featureEntries.size());
-    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());
+    List<String> data = new ArrayList<>(featureEntries.size());
+    List<int[]> segmentations = new ArrayList<>(featureEntries.size());
     
     PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();
-    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);
+    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, fstOutput);
     IntsRef scratch = new IntsRef();
     long ord = 0;
     
@@ -136,12 +136,12 @@ public final class UserDictionary implements Dictionary {
    */
   public int[][] lookup(char[] chars, int off, int len) throws IOException {
     // TODO: can we avoid this treemap/toIndexArray?
-    TreeMap<Integer, int[]> result = new TreeMap<Integer, int[]>(); // index, [length, length...]
+    TreeMap<Integer, int[]> result = new TreeMap<>(); // index, [length, length...]
     boolean found = false; // true if we found any results
 
     final FST.BytesReader fstReader = fst.getBytesReader();
 
-    FST.Arc<Long> arc = new FST.Arc<Long>();
+    FST.Arc<Long> arc = new FST.Arc<>();
     int end = off + len;
     for (int startOffset = off; startOffset < end; startOffset++) {
       arc = fst.getFirstArc(arc);
@@ -175,7 +175,7 @@ public final class UserDictionary implements Dictionary {
    * @return array of {wordId, index, length}
    */
   private int[][] toIndexArray(Map<Integer, int[]> input) {
-    ArrayList<int[]> result = new ArrayList<int[]>();
+    ArrayList<int[]> result = new ArrayList<>();
     for (int i : input.keySet()) {
       int[] wordIdAndLength = input.get(i);
       int wordId = wordIdAndLength[0];
diff --git lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/util/CSVUtil.java lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/util/CSVUtil.java
index 6e35b02..4ce4fb0 100644
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/util/CSVUtil.java
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/util/CSVUtil.java
@@ -42,7 +42,7 @@ public final class CSVUtil {
    */
   public static String[] parse(String line) {
     boolean insideQuote = false;
-    ArrayList<String> result = new ArrayList<String>();
+    ArrayList<String> result = new ArrayList<>();
     int quoteCount = 0;
     StringBuilder sb = new StringBuilder();
     for(int i = 0; i < line.length(); i++) {
diff --git lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/util/ToStringUtil.java lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/util/ToStringUtil.java
index 1821003..d5a6211 100644
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/util/ToStringUtil.java
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/util/ToStringUtil.java
@@ -26,7 +26,7 @@ import java.util.HashMap;
  */
 public class ToStringUtil {
   // a translation map for parts of speech, only used for reflectWith
-  private static final HashMap<String,String> posTranslations = new HashMap<String,String>();
+  private static final HashMap<String,String> posTranslations = new HashMap<>();
   static {
     posTranslations.put("名詞", "noun");
     posTranslations.put("名詞-一般", "noun-common");
@@ -127,7 +127,7 @@ public class ToStringUtil {
   }
   
   // a translation map for inflection types, only used for reflectWith
-  private static final HashMap<String,String> inflTypeTranslations = new HashMap<String,String>();
+  private static final HashMap<String,String> inflTypeTranslations = new HashMap<>();
   static {
     inflTypeTranslations.put("*", "*");
     inflTypeTranslations.put("形容詞・アウオ段", "adj-group-a-o-u");
@@ -197,7 +197,7 @@ public class ToStringUtil {
   }
 
   // a translation map for inflection forms, only used for reflectWith
-  private static final HashMap<String,String> inflFormTranslations = new HashMap<String,String>();
+  private static final HashMap<String,String> inflFormTranslations = new HashMap<>();
   static {
     inflFormTranslations.put("*", "*");
     inflFormTranslations.put("基本形", "base");
diff --git lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseIterationMarkCharFilterFactory.java lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseIterationMarkCharFilterFactory.java
index 4dbf4f4..9008f86 100644
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseIterationMarkCharFilterFactory.java
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseIterationMarkCharFilterFactory.java
@@ -59,7 +59,7 @@ public class TestJapaneseIterationMarkCharFilterFactory extends BaseTokenStreamT
     JapaneseTokenizerFactory tokenizerFactory = new JapaneseTokenizerFactory(new HashMap<String,String>());
     tokenizerFactory.inform(new StringMockResourceLoader(""));
 
-    Map<String, String> filterArgs = new HashMap<String, String>();
+    Map<String, String> filterArgs = new HashMap<>();
     filterArgs.put("normalizeKanji", "true");
     filterArgs.put("normalizeKana", "false");
     JapaneseIterationMarkCharFilterFactory filterFactory = new JapaneseIterationMarkCharFilterFactory(filterArgs);
@@ -76,7 +76,7 @@ public class TestJapaneseIterationMarkCharFilterFactory extends BaseTokenStreamT
     JapaneseTokenizerFactory tokenizerFactory = new JapaneseTokenizerFactory(new HashMap<String,String>());
     tokenizerFactory.inform(new StringMockResourceLoader(""));
 
-    Map<String, String> filterArgs = new HashMap<String, String>();
+    Map<String, String> filterArgs = new HashMap<>();
     filterArgs.put("normalizeKanji", "false");
     filterArgs.put("normalizeKana", "true");
     JapaneseIterationMarkCharFilterFactory filterFactory = new JapaneseIterationMarkCharFilterFactory(filterArgs);
diff --git lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapanesePartOfSpeechStopFilterFactory.java lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapanesePartOfSpeechStopFilterFactory.java
index aae9e64..23161e1 100644
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapanesePartOfSpeechStopFilterFactory.java
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapanesePartOfSpeechStopFilterFactory.java
@@ -39,7 +39,7 @@ public class TestJapanesePartOfSpeechStopFilterFactory extends BaseTokenStreamTe
     tokenizerFactory.inform(new StringMockResourceLoader(""));
     TokenStream ts = tokenizerFactory.create();
     ((Tokenizer)ts).setReader(new StringReader("私は制限スピードを超える。"));
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     args.put("luceneMatchVersion", TEST_VERSION_CURRENT.toString());
     args.put("tags", "stoptags.txt");
     JapanesePartOfSpeechStopFilterFactory factory = new JapanesePartOfSpeechStopFilterFactory(args);
diff --git lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizerFactory.java lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizerFactory.java
index 8962d6e..48bca55 100644
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizerFactory.java
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizerFactory.java
@@ -60,7 +60,7 @@ public class TestJapaneseTokenizerFactory extends BaseTokenStreamTestCase {
    * Test mode parameter: specifying normal mode
    */
   public void testMode() throws IOException {
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     args.put("mode", "normal");
     JapaneseTokenizerFactory factory = new JapaneseTokenizerFactory(args);
     factory.inform(new StringMockResourceLoader(""));
@@ -81,7 +81,7 @@ public class TestJapaneseTokenizerFactory extends BaseTokenStreamTestCase {
         "関西国際空港,関西 国際 空港,カンサイ コクサイ クウコウ,テスト名詞\n" +
         "# Custom reading for sumo wrestler\n" +
         "朝青龍,朝青龍,アサショウリュウ,カスタム人名\n";
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     args.put("userDictionary", "userdict.txt");
     JapaneseTokenizerFactory factory = new JapaneseTokenizerFactory(args);
     factory.inform(new StringMockResourceLoader(userDict));
@@ -96,7 +96,7 @@ public class TestJapaneseTokenizerFactory extends BaseTokenStreamTestCase {
    * Test preserving punctuation
    */
   public void testPreservePunctuation() throws IOException {
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     args.put("discardPunctuation", "false");
     JapaneseTokenizerFactory factory = new JapaneseTokenizerFactory(args);
     factory.inform(new StringMockResourceLoader(""));
diff --git lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/dict/TestTokenInfoDictionary.java lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/dict/TestTokenInfoDictionary.java
index 77f25be..d08809d 100644
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/dict/TestTokenInfoDictionary.java
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/dict/TestTokenInfoDictionary.java
@@ -37,7 +37,7 @@ public class TestTokenInfoDictionary extends LuceneTestCase {
     TokenInfoDictionary tid = TokenInfoDictionary.getInstance();
     ConnectionCosts matrix = ConnectionCosts.getInstance();
     FST<Long> fst = tid.getFST().getInternalFST();
-    IntsRefFSTEnum<Long> fstEnum = new IntsRefFSTEnum<Long>(fst);
+    IntsRefFSTEnum<Long> fstEnum = new IntsRefFSTEnum<>(fst);
     InputOutput<Long> mapping;
     IntsRef scratch = new IntsRef();
     while ((mapping = fstEnum.next()) != null) {
diff --git lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/BinaryDictionaryWriter.java lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/BinaryDictionaryWriter.java
index 3e5ed72..5c6a260 100644
--- lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/BinaryDictionaryWriter.java
+++ lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/BinaryDictionaryWriter.java
@@ -40,7 +40,7 @@ public abstract class BinaryDictionaryWriter {
   private int targetMapEndOffset = 0, lastWordId = -1, lastSourceId = -1;
   private int[] targetMap = new int[8192];
   private int[] targetMapOffsets = new int[8192];
-  private final ArrayList<String> posDict = new ArrayList<String>();
+  private final ArrayList<String> posDict = new ArrayList<>();
 
   public BinaryDictionaryWriter(Class<? extends BinaryDictionary> implClazz, int size) {
     this.implClazz = implClazz;
diff --git lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder.java lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder.java
index 253bc87..26ed584 100644
--- lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder.java
+++ lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder.java
@@ -69,7 +69,7 @@ public class TokenInfoDictionaryBuilder {
         return name.endsWith(".csv");
       }
     };
-    ArrayList<File> csvFiles = new ArrayList<File>();
+    ArrayList<File> csvFiles = new ArrayList<>();
     for (File file : new File(dirname).listFiles(filter)) {
       csvFiles.add(file);
     }
@@ -82,7 +82,7 @@ public class TokenInfoDictionaryBuilder {
     
     // all lines in the file
     System.out.println("  parse...");
-    List<String[]> lines = new ArrayList<String[]>(400000);
+    List<String[]> lines = new ArrayList<>(400000);
     for (File file : csvFiles){
       FileInputStream inputStream = new FileInputStream(file);
       Charset cs = Charset.forName(encoding);
@@ -132,7 +132,7 @@ public class TokenInfoDictionaryBuilder {
     System.out.println("  encode...");
 
     PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();
-    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true, PackedInts.DEFAULT, true, 15);
+    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true, PackedInts.DEFAULT, true, 15);
     IntsRef scratch = new IntsRef();
     long ord = -1; // first ord will be 0
     String lastValue = null;
diff --git lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/UnknownDictionaryBuilder.java lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/UnknownDictionaryBuilder.java
index 5676799..d1dced9 100644
--- lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/UnknownDictionaryBuilder.java
+++ lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/UnknownDictionaryBuilder.java
@@ -66,7 +66,7 @@ public class UnknownDictionaryBuilder {
     
     dictionary.put(CSVUtil.parse(NGRAM_DICTIONARY_ENTRY));
     
-    List<String[]> lines = new ArrayList<String[]>();
+    List<String[]> lines = new ArrayList<>();
     String line = null;
     while ((line = lineReader.readLine()) != null) {
       // note: unk.def only has 10 fields, it simplifies the writer to just append empty reading and pronunciation,
diff --git lucene/analysis/morfologik/src/java/org/apache/lucene/analysis/morfologik/MorfologikFilter.java lucene/analysis/morfologik/src/java/org/apache/lucene/analysis/morfologik/MorfologikFilter.java
index 5ac14cd..6bb0803 100644
--- lucene/analysis/morfologik/src/java/org/apache/lucene/analysis/morfologik/MorfologikFilter.java
+++ lucene/analysis/morfologik/src/java/org/apache/lucene/analysis/morfologik/MorfologikFilter.java
@@ -56,7 +56,7 @@ public class MorfologikFilter extends TokenFilter {
   private final IStemmer stemmer;
   
   private List<WordData> lemmaList;
-  private final ArrayList<StringBuilder> tagsList = new ArrayList<StringBuilder>();
+  private final ArrayList<StringBuilder> tagsList = new ArrayList<>();
 
   private int lemmaListIndex;
 
diff --git lucene/analysis/morfologik/src/java/org/apache/lucene/analysis/morfologik/MorphosyntacticTagsAttributeImpl.java lucene/analysis/morfologik/src/java/org/apache/lucene/analysis/morfologik/MorphosyntacticTagsAttributeImpl.java
index 700e3f2..aad39bc 100644
--- lucene/analysis/morfologik/src/java/org/apache/lucene/analysis/morfologik/MorphosyntacticTagsAttributeImpl.java
+++ lucene/analysis/morfologik/src/java/org/apache/lucene/analysis/morfologik/MorphosyntacticTagsAttributeImpl.java
@@ -82,7 +82,7 @@ public class MorphosyntacticTagsAttributeImpl extends AttributeImpl
   public void copyTo(AttributeImpl target) {
     List<StringBuilder> cloned = null;
     if (tags != null) {
-      cloned = new ArrayList<StringBuilder>(tags.size());
+      cloned = new ArrayList<>(tags.size());
       for (StringBuilder b : tags) {
         cloned.add(new StringBuilder(b));
       }
diff --git lucene/analysis/morfologik/src/test/org/apache/lucene/analysis/morfologik/TestMorfologikAnalyzer.java lucene/analysis/morfologik/src/test/org/apache/lucene/analysis/morfologik/TestMorfologikAnalyzer.java
index d038c66..3624071 100644
--- lucene/analysis/morfologik/src/test/org/apache/lucene/analysis/morfologik/TestMorfologikAnalyzer.java
+++ lucene/analysis/morfologik/src/test/org/apache/lucene/analysis/morfologik/TestMorfologikAnalyzer.java
@@ -124,8 +124,8 @@ public class TestMorfologikAnalyzer extends BaseTokenStreamTestCase {
     ts.incrementToken();
     assertEquals(term, ts.getAttribute(CharTermAttribute.class).toString());
     
-    TreeSet<String> actual = new TreeSet<String>();
-    TreeSet<String> expected = new TreeSet<String>();
+    TreeSet<String> actual = new TreeSet<>();
+    TreeSet<String> expected = new TreeSet<>();
     for (StringBuilder b : ts.getAttribute(MorphosyntacticTagsAttribute.class).getTags()) {
       actual.add(b.toString());
     }
diff --git lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/DoubleMetaphoneFilter.java lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/DoubleMetaphoneFilter.java
index ef04430..ea6d362 100644
--- lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/DoubleMetaphoneFilter.java
+++ lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/DoubleMetaphoneFilter.java
@@ -32,7 +32,7 @@ public final class DoubleMetaphoneFilter extends TokenFilter {
 
   private static final String TOKEN_TYPE = "DoubleMetaphone";
   
-  private final LinkedList<State> remainingTokens = new LinkedList<State>();
+  private final LinkedList<State> remainingTokens = new LinkedList<>();
   private final DoubleMetaphone encoder = new DoubleMetaphone();
   private final boolean inject;
   private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
diff --git lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/PhoneticFilterFactory.java lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/PhoneticFilterFactory.java
index 4979e52..b2620d9 100644
--- lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/PhoneticFilterFactory.java
+++ lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/PhoneticFilterFactory.java
@@ -73,7 +73,7 @@ public class PhoneticFilterFactory extends TokenFilterFactory implements Resourc
   private static final String PACKAGE_CONTAINING_ENCODERS = "org.apache.commons.codec.language.";
 
   //Effectively constants; uppercase keys
-  private static final Map<String, Class<? extends Encoder>> registry = new HashMap<String, Class<? extends Encoder>>(6);
+  private static final Map<String, Class<? extends Encoder>> registry = new HashMap<>(6);
 
   static {
     registry.put("DoubleMetaphone".toUpperCase(Locale.ROOT), DoubleMetaphone.class);
diff --git lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestBeiderMorseFilterFactory.java lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestBeiderMorseFilterFactory.java
index 17a94a6..b6151d0 100644
--- lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestBeiderMorseFilterFactory.java
+++ lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestBeiderMorseFilterFactory.java
@@ -38,7 +38,7 @@ public class TestBeiderMorseFilterFactory extends BaseTokenStreamTestCase {
   }
   
   public void testLanguageSet() throws Exception {
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     args.put("languageSet", "polish");
     BeiderMorseFilterFactory factory = new BeiderMorseFilterFactory(args);
     TokenStream ts = factory.create(whitespaceMockTokenizer("Weinberg"));
@@ -50,7 +50,7 @@ public class TestBeiderMorseFilterFactory extends BaseTokenStreamTestCase {
   }
   
   public void testOptions() throws Exception {
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     args.put("nameType", "ASHKENAZI");
     args.put("ruleType", "EXACT");
     BeiderMorseFilterFactory factory = new BeiderMorseFilterFactory(args);
diff --git lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestDoubleMetaphoneFilterFactory.java lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestDoubleMetaphoneFilterFactory.java
index 0733408..ca1ba05 100644
--- lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestDoubleMetaphoneFilterFactory.java
+++ lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestDoubleMetaphoneFilterFactory.java
@@ -39,7 +39,7 @@ public class TestDoubleMetaphoneFilterFactory extends BaseTokenStreamTestCase {
   }
 
   public void testSettingSizeAndInject() throws Exception {
-    Map<String,String> parameters = new HashMap<String,String>();
+    Map<String,String> parameters = new HashMap<>();
     parameters.put("inject", "false");
     parameters.put("maxCodeLength", "8");
     DoubleMetaphoneFilterFactory factory = new DoubleMetaphoneFilterFactory(parameters);
diff --git lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestPhoneticFilterFactory.java lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestPhoneticFilterFactory.java
index 83e4e1a..c919da4 100644
--- lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestPhoneticFilterFactory.java
+++ lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestPhoneticFilterFactory.java
@@ -36,7 +36,7 @@ public class TestPhoneticFilterFactory extends BaseTokenStreamTestCase {
    * Case: default
    */
   public void testFactoryDefaults() throws IOException {
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     args.put(PhoneticFilterFactory.ENCODER, "Metaphone");
     PhoneticFilterFactory factory = new PhoneticFilterFactory(args);
     factory.inform(new ClasspathResourceLoader(factory.getClass()));
@@ -45,7 +45,7 @@ public class TestPhoneticFilterFactory extends BaseTokenStreamTestCase {
   }
   
   public void testInjectFalse() throws IOException {
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     args.put(PhoneticFilterFactory.ENCODER, "Metaphone");
     args.put(PhoneticFilterFactory.INJECT, "false");
     PhoneticFilterFactory factory = new PhoneticFilterFactory(args);
@@ -54,7 +54,7 @@ public class TestPhoneticFilterFactory extends BaseTokenStreamTestCase {
   }
   
   public void testMaxCodeLength() throws IOException {
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     args.put(PhoneticFilterFactory.ENCODER, "Metaphone");
     args.put(PhoneticFilterFactory.MAX_CODE_LENGTH, "2");
     PhoneticFilterFactory factory = new PhoneticFilterFactory(args);
@@ -76,7 +76,7 @@ public class TestPhoneticFilterFactory extends BaseTokenStreamTestCase {
   
   public void testUnknownEncoder() throws IOException {
     try {
-      Map<String,String> args = new HashMap<String,String>();
+      Map<String,String> args = new HashMap<>();
       args.put("encoder", "XXX");
       PhoneticFilterFactory factory = new PhoneticFilterFactory(args);
       factory.inform(new ClasspathResourceLoader(factory.getClass()));
@@ -88,7 +88,7 @@ public class TestPhoneticFilterFactory extends BaseTokenStreamTestCase {
   
   public void testUnknownEncoderReflection() throws IOException {
     try {
-      Map<String,String> args = new HashMap<String,String>();
+      Map<String,String> args = new HashMap<>();
       args.put("encoder", "org.apache.commons.codec.language.NonExistence");
       PhoneticFilterFactory factory = new PhoneticFilterFactory(args);
       factory.inform(new ClasspathResourceLoader(factory.getClass()));
@@ -102,7 +102,7 @@ public class TestPhoneticFilterFactory extends BaseTokenStreamTestCase {
    * Case: Reflection
    */
   public void testFactoryReflection() throws IOException {
-    Map<String,String> args = new HashMap<String, String>();
+    Map<String,String> args = new HashMap<>();
     args.put(PhoneticFilterFactory.ENCODER, "org.apache.commons.codec.language.Metaphone");
     PhoneticFilterFactory factory = new PhoneticFilterFactory(args);
     factory.inform(new ClasspathResourceLoader(factory.getClass()));
@@ -115,7 +115,7 @@ public class TestPhoneticFilterFactory extends BaseTokenStreamTestCase {
    * so this effectively tests reflection without package name
    */
   public void testFactoryReflectionCaverphone2() throws IOException {
-    Map<String,String> args = new HashMap<String, String>();
+    Map<String,String> args = new HashMap<>();
     args.put(PhoneticFilterFactory.ENCODER, "Caverphone2");
     PhoneticFilterFactory factory = new PhoneticFilterFactory(args);
     factory.inform(new ClasspathResourceLoader(factory.getClass()));
@@ -124,7 +124,7 @@ public class TestPhoneticFilterFactory extends BaseTokenStreamTestCase {
   }
   
   public void testFactoryReflectionCaverphone() throws IOException {
-    Map<String,String> args = new HashMap<String, String>();
+    Map<String,String> args = new HashMap<>();
     args.put(PhoneticFilterFactory.ENCODER, "Caverphone");
     PhoneticFilterFactory factory = new PhoneticFilterFactory(args);
     factory.inform(new ClasspathResourceLoader(factory.getClass()));
@@ -182,7 +182,7 @@ public class TestPhoneticFilterFactory extends BaseTokenStreamTestCase {
   static void assertAlgorithm(String algName, String inject, String input,
       String[] expected) throws Exception {
     Tokenizer tokenizer = whitespaceMockTokenizer(input);
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     args.put("encoder", algName);
     args.put("inject", inject);
     PhoneticFilterFactory factory = new PhoneticFilterFactory(args);
diff --git lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph.java lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph.java
index e71f20f..c3b9186 100644
--- lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph.java
+++ lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph.java
@@ -34,7 +34,7 @@ import org.apache.lucene.analysis.cn.smart.Utility;
  */
 class BiSegGraph {
 
-  private Map<Integer,ArrayList<SegTokenPair>> tokenPairListTable = new HashMap<Integer,ArrayList<SegTokenPair>>();
+  private Map<Integer,ArrayList<SegTokenPair>> tokenPairListTable = new HashMap<>();
 
   private List<SegToken> segTokenList;
 
@@ -144,7 +144,7 @@ class BiSegGraph {
   public void addSegTokenPair(SegTokenPair tokenPair) {
     int to = tokenPair.to;
     if (!isToExist(to)) {
-      ArrayList<SegTokenPair> newlist = new ArrayList<SegTokenPair>();
+      ArrayList<SegTokenPair> newlist = new ArrayList<>();
       newlist.add(tokenPair);
       tokenPairListTable.put(to, newlist);
     } else {
@@ -168,7 +168,7 @@ class BiSegGraph {
   public List<SegToken> getShortPath() {
     int current;
     int nodeCount = getToCount();
-    List<PathNode> path = new ArrayList<PathNode>();
+    List<PathNode> path = new ArrayList<>();
     PathNode zeroPath = new PathNode();
     zeroPath.weight = 0;
     zeroPath.preNode = 0;
@@ -197,8 +197,8 @@ class BiSegGraph {
     int preNode, lastNode;
     lastNode = path.size() - 1;
     current = lastNode;
-    List<Integer> rpath = new ArrayList<Integer>();
-    List<SegToken> resultPath = new ArrayList<SegToken>();
+    List<Integer> rpath = new ArrayList<>();
+    List<SegToken> resultPath = new ArrayList<>();
 
     rpath.add(current);
     while (current != 0) {
diff --git lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java
index 93b0677..9c1f95a 100644
--- lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java
+++ lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java
@@ -34,7 +34,7 @@ class SegGraph {
   /**
    * Map of start offsets to ArrayList of tokens at that position
    */
-  private Map<Integer,ArrayList<SegToken>> tokenListTable = new HashMap<Integer,ArrayList<SegToken>>();
+  private Map<Integer,ArrayList<SegToken>> tokenListTable = new HashMap<>();
 
   private int maxStart = -1;
 
@@ -72,7 +72,7 @@ class SegGraph {
    * @return a {@link List} of these ordered tokens.
    */
   public List<SegToken> makeIndex() {
-    List<SegToken> result = new ArrayList<SegToken>();
+    List<SegToken> result = new ArrayList<>();
     int s = -1, count = 0, size = tokenListTable.size();
     List<SegToken> tokenList;
     int index = 0;
@@ -98,7 +98,7 @@ class SegGraph {
   public void addToken(SegToken token) {
     int s = token.startOffset;
     if (!isStartExist(s)) {
-      ArrayList<SegToken> newlist = new ArrayList<SegToken>();
+      ArrayList<SegToken> newlist = new ArrayList<>();
       newlist.add(token);
       tokenListTable.put(s, newlist);
     } else {
@@ -115,7 +115,7 @@ class SegGraph {
    * @return {@link List} of all tokens in the map.
    */
   public List<SegToken> toTokenList() {
-    List<SegToken> result = new ArrayList<SegToken>();
+    List<SegToken> result = new ArrayList<>();
     int s = -1, count = 0, size = tokenListTable.size();
     List<SegToken> tokenList;
 
diff --git lucene/analysis/stempel/src/java/org/egothor/stemmer/Gener.java lucene/analysis/stempel/src/java/org/egothor/stemmer/Gener.java
index a073426..983c67f 100644
--- lucene/analysis/stempel/src/java/org/egothor/stemmer/Gener.java
+++ lucene/analysis/stempel/src/java/org/egothor/stemmer/Gener.java
@@ -78,7 +78,7 @@ public class Gener extends Reduce {
   @Override
   public Trie optimize(Trie orig) {
     List<CharSequence> cmds = orig.cmds;
-    List<Row> rows = new ArrayList<Row>();
+    List<Row> rows = new ArrayList<>();
     List<Row> orows = orig.rows;
     int remap[] = new int[orows.size()];
     
diff --git lucene/analysis/stempel/src/java/org/egothor/stemmer/Lift.java lucene/analysis/stempel/src/java/org/egothor/stemmer/Lift.java
index 11c869d..16da8c8 100644
--- lucene/analysis/stempel/src/java/org/egothor/stemmer/Lift.java
+++ lucene/analysis/stempel/src/java/org/egothor/stemmer/Lift.java
@@ -88,7 +88,7 @@ public class Lift extends Reduce {
   @Override
   public Trie optimize(Trie orig) {
     List<CharSequence> cmds = orig.cmds;
-    List<Row> rows = new ArrayList<Row>();
+    List<Row> rows = new ArrayList<>();
     List<Row> orows = orig.rows;
     int remap[] = new int[orows.size()];
     
diff --git lucene/analysis/stempel/src/java/org/egothor/stemmer/MultiTrie.java lucene/analysis/stempel/src/java/org/egothor/stemmer/MultiTrie.java
index e2e9173..e0d9376 100644
--- lucene/analysis/stempel/src/java/org/egothor/stemmer/MultiTrie.java
+++ lucene/analysis/stempel/src/java/org/egothor/stemmer/MultiTrie.java
@@ -70,7 +70,7 @@ public class MultiTrie extends Trie {
   final char EOM = '*';
   final String EOM_NODE = "" + EOM;
   
-  List<Trie> tries = new ArrayList<Trie>();
+  List<Trie> tries = new ArrayList<>();
   
   int BY = 1;
   
@@ -186,7 +186,7 @@ public class MultiTrie extends Trie {
    */
   @Override
   public Trie reduce(Reduce by) {
-    List<Trie> h = new ArrayList<Trie>();
+    List<Trie> h = new ArrayList<>();
     for (Trie trie : tries)
       h.add(trie.reduce(by));
     
diff --git lucene/analysis/stempel/src/java/org/egothor/stemmer/MultiTrie2.java lucene/analysis/stempel/src/java/org/egothor/stemmer/MultiTrie2.java
index be9faa2..cfe3181 100644
--- lucene/analysis/stempel/src/java/org/egothor/stemmer/MultiTrie2.java
+++ lucene/analysis/stempel/src/java/org/egothor/stemmer/MultiTrie2.java
@@ -277,7 +277,7 @@ public class MultiTrie2 extends MultiTrie {
    */
   @Override
   public Trie reduce(Reduce by) {
-    List<Trie> h = new ArrayList<Trie>();
+    List<Trie> h = new ArrayList<>();
     for (Trie trie : tries)
       h.add(trie.reduce(by));
 
diff --git lucene/analysis/stempel/src/java/org/egothor/stemmer/Optimizer.java lucene/analysis/stempel/src/java/org/egothor/stemmer/Optimizer.java
index 3bd612e..25b7235 100644
--- lucene/analysis/stempel/src/java/org/egothor/stemmer/Optimizer.java
+++ lucene/analysis/stempel/src/java/org/egothor/stemmer/Optimizer.java
@@ -81,7 +81,7 @@ public class Optimizer extends Reduce {
   @Override
   public Trie optimize(Trie orig) {
     List<CharSequence> cmds = orig.cmds;
-    List<Row> rows = new ArrayList<Row>();
+    List<Row> rows = new ArrayList<>();
     List<Row> orows = orig.rows;
     int remap[] = new int[orows.size()];
     
diff --git lucene/analysis/stempel/src/java/org/egothor/stemmer/Reduce.java lucene/analysis/stempel/src/java/org/egothor/stemmer/Reduce.java
index 6ebb595..2cd2dec 100644
--- lucene/analysis/stempel/src/java/org/egothor/stemmer/Reduce.java
+++ lucene/analysis/stempel/src/java/org/egothor/stemmer/Reduce.java
@@ -78,7 +78,7 @@ public class Reduce {
    */
   public Trie optimize(Trie orig) {
     List<CharSequence> cmds = orig.cmds;
-    List<Row> rows = new ArrayList<Row>();
+    List<Row> rows = new ArrayList<>();
     List<Row> orows = orig.rows;
     int remap[] = new int[orows.size()];
     
diff --git lucene/analysis/stempel/src/java/org/egothor/stemmer/Row.java lucene/analysis/stempel/src/java/org/egothor/stemmer/Row.java
index 1489a57..600b008 100644
--- lucene/analysis/stempel/src/java/org/egothor/stemmer/Row.java
+++ lucene/analysis/stempel/src/java/org/egothor/stemmer/Row.java
@@ -65,7 +65,7 @@ import java.util.TreeMap;
  * The Row class represents a row in a matrix representation of a trie.
  */
 public class Row {
-  TreeMap<Character,Cell> cells = new TreeMap<Character,Cell>();
+  TreeMap<Character,Cell> cells = new TreeMap<>();
   int uniformCnt = 0;
   int uniformSkip = 0;
   
diff --git lucene/analysis/stempel/src/java/org/egothor/stemmer/Trie.java lucene/analysis/stempel/src/java/org/egothor/stemmer/Trie.java
index b330e83..3830746 100644
--- lucene/analysis/stempel/src/java/org/egothor/stemmer/Trie.java
+++ lucene/analysis/stempel/src/java/org/egothor/stemmer/Trie.java
@@ -70,8 +70,8 @@ import java.util.List;
  * for which a Trie is constructed.
  */
 public class Trie {
-  List<Row> rows = new ArrayList<Row>();
-  List<CharSequence> cmds = new ArrayList<CharSequence>();
+  List<Row> rows = new ArrayList<>();
+  List<CharSequence> cmds = new ArrayList<>();
   int root;
   
   boolean forward = false;
diff --git lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/UIMAAnnotationsTokenizerFactory.java lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/UIMAAnnotationsTokenizerFactory.java
index 2649950..8a1990c 100644
--- lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/UIMAAnnotationsTokenizerFactory.java
+++ lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/UIMAAnnotationsTokenizerFactory.java
@@ -31,7 +31,7 @@ public class UIMAAnnotationsTokenizerFactory extends TokenizerFactory {
 
   private String descriptorPath;
   private String tokenType;
-  private final Map<String,Object> configurationParameters = new HashMap<String,Object>();
+  private final Map<String,Object> configurationParameters = new HashMap<>();
 
   /** Creates a new UIMAAnnotationsTokenizerFactory */
   public UIMAAnnotationsTokenizerFactory(Map<String,String> args) {
diff --git lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/UIMATypeAwareAnnotationsTokenizerFactory.java lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/UIMATypeAwareAnnotationsTokenizerFactory.java
index 674e27a..b787886 100644
--- lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/UIMATypeAwareAnnotationsTokenizerFactory.java
+++ lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/UIMATypeAwareAnnotationsTokenizerFactory.java
@@ -32,7 +32,7 @@ public class UIMATypeAwareAnnotationsTokenizerFactory extends TokenizerFactory {
   private String descriptorPath;
   private String tokenType;
   private String featurePath;
-  private final Map<String,Object> configurationParameters = new HashMap<String,Object>();
+  private final Map<String,Object> configurationParameters = new HashMap<>();
 
   /** Creates a new UIMATypeAwareAnnotationsTokenizerFactory */
   public UIMATypeAwareAnnotationsTokenizerFactory(Map<String,String> args) {
diff --git lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/ae/AEProviderFactory.java lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/ae/AEProviderFactory.java
index d08c850..1a3e2be 100644
--- lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/ae/AEProviderFactory.java
+++ lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/ae/AEProviderFactory.java
@@ -27,7 +27,7 @@ public class AEProviderFactory {
 
   private static final AEProviderFactory instance = new AEProviderFactory();
 
-  private final Map<String, AEProvider> providerCache = new HashMap<String, AEProvider>();
+  private final Map<String, AEProvider> providerCache = new HashMap<>();
 
   private AEProviderFactory() {
     // Singleton
diff --git lucene/analysis/uima/src/test/org/apache/lucene/analysis/uima/UIMABaseAnalyzerTest.java lucene/analysis/uima/src/test/org/apache/lucene/analysis/uima/UIMABaseAnalyzerTest.java
index 702fff4..2b88463 100644
--- lucene/analysis/uima/src/test/org/apache/lucene/analysis/uima/UIMABaseAnalyzerTest.java
+++ lucene/analysis/uima/src/test/org/apache/lucene/analysis/uima/UIMABaseAnalyzerTest.java
@@ -127,7 +127,7 @@ public class UIMABaseAnalyzerTest extends BaseTokenStreamTestCase {
 
   @Test
   public void testRandomStringsWithConfigurationParameters() throws Exception {
-    Map<String, Object> cp = new HashMap<String, Object>();
+    Map<String, Object> cp = new HashMap<>();
     cp.put("line-end", "\r");
     checkRandomData(random(), new UIMABaseAnalyzer("/uima/TestWSTokenizerAE.xml", "org.apache.lucene.uima.ts.TokenAnnotation", cp),
         100 * RANDOM_MULTIPLIER);
diff --git lucene/analysis/uima/src/test/org/apache/lucene/analysis/uima/ae/OverridingParamsAEProviderTest.java lucene/analysis/uima/src/test/org/apache/lucene/analysis/uima/ae/OverridingParamsAEProviderTest.java
index 0922184..ee0f4b5 100644
--- lucene/analysis/uima/src/test/org/apache/lucene/analysis/uima/ae/OverridingParamsAEProviderTest.java
+++ lucene/analysis/uima/src/test/org/apache/lucene/analysis/uima/ae/OverridingParamsAEProviderTest.java
@@ -53,7 +53,7 @@ public class OverridingParamsAEProviderTest {
 
   @Test
   public void testOverridingParamsInitialization() throws Exception {
-    Map<String, Object> runtimeParameters = new HashMap<String, Object>();
+    Map<String, Object> runtimeParameters = new HashMap<>();
     runtimeParameters.put("ngramsize", "3");
     AEProvider aeProvider = new OverridingParamsAEProvider("/uima/AggregateSentenceAE.xml", runtimeParameters);
     AnalysisEngine analysisEngine = aeProvider.getAE();
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/PerfRunData.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/PerfRunData.java
index 5d26625..cf11edc 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/PerfRunData.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/PerfRunData.java
@@ -81,7 +81,7 @@ public class PerfRunData implements Closeable {
   // directory, analyzer, docMaker - created at startup.
   // reader, writer, searcher - maintained by basic tasks. 
   private Directory directory;
-  private Map<String,AnalyzerFactory> analyzerFactories = new HashMap<String,AnalyzerFactory>();
+  private Map<String,AnalyzerFactory> analyzerFactories = new HashMap<>();
   private Analyzer analyzer;
   private DocMaker docMaker;
   private ContentSource contentSource;
@@ -102,7 +102,7 @@ public class PerfRunData implements Closeable {
   private Config config;
   private long startTimeMillis;
 
-  private final HashMap<String, Object> perfObjects = new HashMap<String, Object>();
+  private final HashMap<String, Object> perfObjects = new HashMap<>();
   
   // constructor
   public PerfRunData (Config config) throws Exception {
@@ -125,7 +125,7 @@ public class PerfRunData implements Closeable {
         "org.apache.lucene.benchmark.byTask.feeds.RandomFacetSource")).asSubclass(FacetSource.class).newInstance();
     facetSource.setConfig(config);
     // query makers
-    readTaskQueryMaker = new HashMap<Class<? extends ReadTask>,QueryMaker>();
+    readTaskQueryMaker = new HashMap<>();
     qmkrClass = Class.forName(config.get("query.maker","org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker")).asSubclass(QueryMaker.class);
 
     // index stuff
@@ -147,7 +147,7 @@ public class PerfRunData implements Closeable {
                   docMaker, facetSource, contentSource);
     
     // close all perf objects that are closeable.
-    ArrayList<Closeable> perfObjectsToClose = new ArrayList<Closeable>();
+    ArrayList<Closeable> perfObjectsToClose = new ArrayList<>();
     for (Object obj : perfObjects.values()) {
       if (obj instanceof Closeable) {
         perfObjectsToClose.add((Closeable) obj);
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DemoHTMLParser.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DemoHTMLParser.java
index f2850d7..1f76f8c 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DemoHTMLParser.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DemoHTMLParser.java
@@ -147,7 +147,7 @@ public class DemoHTMLParser implements HTMLParser {
     }
     
     private static final Set<String> createElementNameSet(String... names) {
-      return Collections.unmodifiableSet(new HashSet<String>(Arrays.asList(names)));
+      return Collections.unmodifiableSet(new HashSet<>(Arrays.asList(names)));
     }
     
     /** HTML elements that cause a line break (they are block-elements) */
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DirContentSource.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DirContentSource.java
index 9bf7cc8..031a0f0 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DirContentSource.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DirContentSource.java
@@ -82,7 +82,7 @@ public class DirContentSource extends ContentSource {
 
     int count = 0;
 
-    Stack<File> stack = new Stack<File>();
+    Stack<File> stack = new Stack<>();
 
     /* this seems silly ... there must be a better way ...
        not that this is good, but can it matter? */
@@ -156,7 +156,7 @@ public class DirContentSource extends ContentSource {
 
   }
   
-  private ThreadLocal<DateFormatInfo> dateFormat = new ThreadLocal<DateFormatInfo>();
+  private ThreadLocal<DateFormatInfo> dateFormat = new ThreadLocal<>();
   private File dataDir = null;
   private int iteration = 0;
   private Iterator inputFiles = null;
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DocMaker.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DocMaker.java
index 5e471b1..9676877 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DocMaker.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DocMaker.java
@@ -108,8 +108,8 @@ public class DocMaker implements Closeable {
       this.reuseFields = reuseFields;
       
       if (reuseFields) {
-        fields =  new HashMap<String,Field>();
-        numericFields = new HashMap<String,Field>();
+        fields =  new HashMap<>();
+        numericFields = new HashMap<>();
         
         // Initialize the map with the default fields.
         fields.put(BODY_FIELD, new Field(BODY_FIELD, "", bodyFt));
@@ -192,9 +192,9 @@ public class DocMaker implements Closeable {
   }
 
   // leftovers are thread local, because it is unsafe to share residues between threads
-  private ThreadLocal<LeftOver> leftovr = new ThreadLocal<LeftOver>();
-  private ThreadLocal<DocState> docState = new ThreadLocal<DocState>();
-  private ThreadLocal<DateUtil> dateParsers = new ThreadLocal<DateUtil>();
+  private ThreadLocal<LeftOver> leftovr = new ThreadLocal<>();
+  private ThreadLocal<DocState> docState = new ThreadLocal<>();
+  private ThreadLocal<DateUtil> dateParsers = new ThreadLocal<>();
 
   public static final String BODY_FIELD = "body";
   public static final String TITLE_FIELD = "doctitle";
@@ -459,7 +459,7 @@ public class DocMaker implements Closeable {
     // In a multi-rounds run, it is important to reset DocState since settings
     // of fields may change between rounds, and this is the only way to reset
     // the cache of all threads.
-    docState = new ThreadLocal<DocState>();
+    docState = new ThreadLocal<>();
     
     indexProperties = config.get("doc.index.props", false);
 
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiContentSource.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiContentSource.java
index 8c82628..86a2efa 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiContentSource.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiContentSource.java
@@ -254,7 +254,7 @@ public class EnwikiContentSource extends ContentSource {
 
   }
 
-  private static final Map<String,Integer> ELEMENTS = new HashMap<String,Integer>();
+  private static final Map<String,Integer> ELEMENTS = new HashMap<>();
   private static final int TITLE = 0;
   private static final int DATE = TITLE + 1;
   private static final int BODY = DATE + 1;
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiQueryMaker.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiQueryMaker.java
index aaf12b2..bcc9b60 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiQueryMaker.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiQueryMaker.java
@@ -94,7 +94,7 @@ public class EnwikiQueryMaker extends AbstractQueryMaker implements
    */
   private static Query[] createQueries(List<Object> qs, Analyzer a) {
     QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, DocMaker.BODY_FIELD, a);
-    List<Object> queries = new ArrayList<Object>();
+    List<Object> queries = new ArrayList<>();
     for (int i = 0; i < qs.size(); i++) {
       try {
 
@@ -127,7 +127,7 @@ public class EnwikiQueryMaker extends AbstractQueryMaker implements
     // analyzer (default is standard analyzer)
     Analyzer anlzr = NewAnalyzerTask.createAnalyzer(config.get("analyzer", StandardAnalyzer.class.getName()));
 
-    List<Object> queryList = new ArrayList<Object>(20);
+    List<Object> queryList = new ArrayList<>(20);
     queryList.addAll(Arrays.asList(STANDARD_QUERIES));
     if(!config.get("enwikiQueryMaker.disableSpanQueries", false))
       queryList.addAll(Arrays.asList(getPrebuiltQueries(DocMaker.BODY_FIELD)));
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FileBasedQueryMaker.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FileBasedQueryMaker.java
index 79b25f9..d7e3378 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FileBasedQueryMaker.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FileBasedQueryMaker.java
@@ -54,7 +54,7 @@ public class FileBasedQueryMaker extends AbstractQueryMaker implements QueryMake
     QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, defaultField, anlzr);
     qp.setAllowLeadingWildcard(true);
 
-    List<Query> qq = new ArrayList<Query>();
+    List<Query> qq = new ArrayList<>();
     String fileName = config.get("file.query.maker.file", null);
     if (fileName != null)
     {
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersContentSource.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersContentSource.java
index a6ddcfd..b3106af 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersContentSource.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersContentSource.java
@@ -49,9 +49,9 @@ public class ReutersContentSource extends ContentSource {
     ParsePosition pos;
   }
 
-  private ThreadLocal<DateFormatInfo> dateFormat = new ThreadLocal<DateFormatInfo>();
+  private ThreadLocal<DateFormatInfo> dateFormat = new ThreadLocal<>();
   private File dataDir = null;
-  private ArrayList<File> inputFiles = new ArrayList<File>();
+  private ArrayList<File> inputFiles = new ArrayList<>();
   private int nextFile = 0;
   private int iteration = 0;
   
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersQueryMaker.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersQueryMaker.java
index 2275c28..259928d 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersQueryMaker.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/ReutersQueryMaker.java
@@ -74,7 +74,7 @@ public class ReutersQueryMaker extends AbstractQueryMaker implements QueryMaker
    */
   private static Query[] createQueries(List<Object> qs, Analyzer a) {
     QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, DocMaker.BODY_FIELD, a);
-    List<Object> queries = new ArrayList<Object>();
+    List<Object> queries = new ArrayList<>();
     for (int i = 0; i < qs.size(); i++)  {
       try {
         
@@ -108,7 +108,7 @@ public class ReutersQueryMaker extends AbstractQueryMaker implements QueryMaker
     Analyzer anlzr= NewAnalyzerTask.createAnalyzer(config.get("analyzer",
     "org.apache.lucene.analysis.standard.StandardAnalyzer")); 
     
-    List<Object> queryList = new ArrayList<Object>(20);
+    List<Object> queryList = new ArrayList<>(20);
     queryList.addAll(Arrays.asList(STANDARD_QUERIES));
     queryList.addAll(Arrays.asList(getPrebuiltQueries(DocMaker.BODY_FIELD)));
     return createQueries(queryList, anlzr);
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleQueryMaker.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleQueryMaker.java
index 28aa733..840d2dc 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleQueryMaker.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleQueryMaker.java
@@ -49,7 +49,7 @@ public class SimpleQueryMaker extends AbstractQueryMaker implements QueryMaker {
         "org.apache.lucene.analysis.standard.StandardAnalyzer")); 
     
     QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, DocMaker.BODY_FIELD,anlzr);
-    ArrayList<Query> qq = new ArrayList<Query>();
+    ArrayList<Query> qq = new ArrayList<>();
     Query q1 = new TermQuery(new Term(DocMaker.ID_FIELD,"doc2"));
     qq.add(q1);
     Query q2 = new TermQuery(new Term(DocMaker.BODY_FIELD,"simple"));
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker.java
index c8b6e7d..6a5730c 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker.java
@@ -36,7 +36,7 @@ public class SimpleSloppyPhraseQueryMaker extends SimpleQueryMaker {
   protected Query[] prepareQueries() throws Exception {
     // extract some 100 words from doc text to an array
     String words[];
-    ArrayList<String> w = new ArrayList<String>();
+    ArrayList<String> w = new ArrayList<>();
     StringTokenizer st = new StringTokenizer(SingleDocSource.DOC_TEXT);
     while (st.hasMoreTokens() && w.size()<100) {
       w.add(st.nextToken());
@@ -44,7 +44,7 @@ public class SimpleSloppyPhraseQueryMaker extends SimpleQueryMaker {
     words = w.toArray(new String[0]);
 
     // create queries (that would find stuff) with varying slops
-    ArrayList<Query> queries = new ArrayList<Query>(); 
+    ArrayList<Query> queries = new ArrayList<>();
     for (int slop=0; slop<8; slop++) {
       for (int qlen=2; qlen<6; qlen++) {
         for (int wd=0; wd<words.length-qlen-slop; wd++) {
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SpatialDocMaker.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SpatialDocMaker.java
index 97bff88..a4491b4 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SpatialDocMaker.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SpatialDocMaker.java
@@ -50,7 +50,7 @@ public class SpatialDocMaker extends DocMaker {
   public static final String SPATIAL_FIELD = "spatial";
 
   //cache spatialStrategy by round number
-  private static Map<Integer,SpatialStrategy> spatialStrategyCache = new HashMap<Integer,SpatialStrategy>();
+  private static Map<Integer,SpatialStrategy> spatialStrategyCache = new HashMap<>();
 
   private SpatialStrategy strategy;
   private ShapeConverter shapeConverter;
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/TrecContentSource.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/TrecContentSource.java
index 7d9ce5f..1942684 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/TrecContentSource.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/TrecContentSource.java
@@ -79,10 +79,10 @@ public class TrecContentSource extends ContentSource {
        "hhmm z.z.z. MMM dd, yyyy",       // 0901 u.t.c. April 28, 1994
   };
 
-  private ThreadLocal<DateFormatInfo> dateFormats = new ThreadLocal<DateFormatInfo>();
-  private ThreadLocal<StringBuilder> trecDocBuffer = new ThreadLocal<StringBuilder>();
+  private ThreadLocal<DateFormatInfo> dateFormats = new ThreadLocal<>();
+  private ThreadLocal<StringBuilder> trecDocBuffer = new ThreadLocal<>();
   private File dataDir = null;
-  private ArrayList<File> inputFiles = new ArrayList<File>();
+  private ArrayList<File> inputFiles = new ArrayList<>();
   private int nextFile = 0;
   // Use to synchronize threads on reading from the TREC documents.
   private Object lock = new Object();
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/TrecDocParser.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/TrecDocParser.java
index 8e24f72..24b9801 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/TrecDocParser.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/TrecDocParser.java
@@ -35,7 +35,7 @@ public abstract class TrecDocParser {
   /** trec parser type used for unknown extensions */
   public static final ParsePathType DEFAULT_PATH_TYPE  = ParsePathType.GOV2;
 
-  static final Map<ParsePathType,TrecDocParser> pathType2parser = new HashMap<ParsePathType,TrecDocParser>();
+  static final Map<ParsePathType,TrecDocParser> pathType2parser = new HashMap<>();
   static {
     pathType2parser.put(ParsePathType.GOV2, new TrecGov2Parser());
     pathType2parser.put(ParsePathType.FBIS, new TrecFBISParser());
@@ -44,7 +44,7 @@ public abstract class TrecDocParser {
     pathType2parser.put(ParsePathType.LATIMES, new TrecLATimesParser());
   }
 
-  static final Map<String,ParsePathType> pathName2Type = new HashMap<String,ParsePathType>();
+  static final Map<String,ParsePathType> pathName2Type = new HashMap<>();
   static {
     for (ParsePathType ppt : ParsePathType.values()) {
       pathName2Type.put(ppt.name().toUpperCase(Locale.ROOT),ppt);
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/Points.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/Points.java
index 52cf68b..a12b6ff 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/Points.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/Points.java
@@ -32,7 +32,7 @@ public class Points {
   // stat points ordered by their start time. 
   // for now we collect points as TaskStats objects.
   // later might optimize to collect only native data.
-  private ArrayList<TaskStats> points = new ArrayList<TaskStats>();
+  private ArrayList<TaskStats> points = new ArrayList<>();
 
   private int nextTaskRunNum = 0;
 
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddFacetedDocTask.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddFacetedDocTask.java
index 87f2e12..5063e0a 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddFacetedDocTask.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddFacetedDocTask.java
@@ -74,7 +74,7 @@ public class AddFacetedDocTask extends AddDocTask {
   @Override
   public int doLogic() throws Exception {
     if (config != null) {
-      List<FacetField> facets = new ArrayList<FacetField>();
+      List<FacetField> facets = new ArrayList<>();
       getRunData().getFacetSource().getNextFacets(facets);
       for(FacetField ff : facets) {
         doc.add(ff);
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AnalyzerFactoryTask.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AnalyzerFactoryTask.java
index 9ad49df..4a376ab 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AnalyzerFactoryTask.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AnalyzerFactoryTask.java
@@ -88,9 +88,9 @@ public class AnalyzerFactoryTask extends PerfTask {
   String factoryName = null;
   Integer positionIncrementGap = null;
   Integer offsetGap = null;
-  private List<CharFilterFactory> charFilterFactories = new ArrayList<CharFilterFactory>();
+  private List<CharFilterFactory> charFilterFactories = new ArrayList<>();
   private TokenizerFactory tokenizerFactory = null;
-  private List<TokenFilterFactory> tokenFilterFactories = new ArrayList<TokenFilterFactory>();
+  private List<TokenFilterFactory> tokenFilterFactories = new ArrayList<>();
 
   public AnalyzerFactoryTask(PerfRunData runData) {
     super(runData);
@@ -287,7 +287,7 @@ public class AnalyzerFactoryTask extends PerfTask {
    */
   private void createAnalysisPipelineComponent
       (StreamTokenizer stok, Class<? extends AbstractAnalysisFactory> clazz) {
-    Map<String,String> argMap = new HashMap<String,String>();
+    Map<String,String> argMap = new HashMap<>();
     boolean parenthetical = false;
     try {
       WHILE_LOOP: while (stok.nextToken() != StreamTokenizer.TT_EOF) {
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CommitIndexTask.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CommitIndexTask.java
index ae953c6..d6b2e9e 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CommitIndexTask.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CommitIndexTask.java
@@ -42,7 +42,7 @@ public class CommitIndexTask extends PerfTask {
   @Override
   public void setParams(String params) {
     super.setParams(params);
-    commitUserData = new HashMap<String,String>();
+    commitUserData = new HashMap<>();
     commitUserData.put(OpenReaderTask.USER_DATA, params);
   }
   
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ConsumeContentSourceTask.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ConsumeContentSourceTask.java
index f8c260d..f139519 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ConsumeContentSourceTask.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ConsumeContentSourceTask.java
@@ -25,7 +25,7 @@ import org.apache.lucene.benchmark.byTask.feeds.DocData;
 public class ConsumeContentSourceTask extends PerfTask {
 
   private final ContentSource source;
-  private ThreadLocal<DocData> dd = new ThreadLocal<DocData>();
+  private ThreadLocal<DocData> dd = new ThreadLocal<>();
   
   public ConsumeContentSourceTask(PerfRunData runData) {
     super(runData);
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewAnalyzerTask.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewAnalyzerTask.java
index 0854394..cb8ea74 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewAnalyzerTask.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewAnalyzerTask.java
@@ -39,7 +39,7 @@ public class NewAnalyzerTask extends PerfTask {
 
   public NewAnalyzerTask(PerfRunData runData) {
     super(runData);
-    analyzerNames = new ArrayList<String>();
+    analyzerNames = new ArrayList<>();
   }
   
   public static final Analyzer createAnalyzer(String className) throws Exception{
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java
index c768b98..4480f1c 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java
@@ -300,7 +300,7 @@ public abstract class ReadTask extends PerfTask {
    */
   protected Collection<String> getFieldsToHighlight(StoredDocument document) {
     List<StorableField> fields = document.getFields();
-    Set<String> result = new HashSet<String>(fields.size());
+    Set<String> result = new HashSet<>(fields.size());
     for (final StorableField f : fields) {
       result.add(f.name());
     }
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameRoundTask.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameRoundTask.java
index 2c933f1..731e3ef 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameRoundTask.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameRoundTask.java
@@ -53,7 +53,7 @@ public class RepSumByNameRoundTask extends ReportTask {
    */
   protected Report reportSumByNameRound(List<TaskStats> taskStats) {
     // aggregate by task name and round
-    LinkedHashMap<String,TaskStats> p2 = new LinkedHashMap<String,TaskStats>();
+    LinkedHashMap<String,TaskStats> p2 = new LinkedHashMap<>();
     int reported = 0;
     for (final TaskStats stat1 : taskStats) {
       if (stat1.getElapsed()>=0) { // consider only tasks that ended
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameTask.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameTask.java
index 1aa5c51..2da03ec 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameTask.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByNameTask.java
@@ -54,7 +54,7 @@ public class RepSumByNameTask extends ReportTask {
   protected Report reportSumByName(List<TaskStats> taskStats) {
     // aggregate by task name
     int reported = 0;
-    LinkedHashMap<String,TaskStats> p2 = new LinkedHashMap<String,TaskStats>();
+    LinkedHashMap<String,TaskStats> p2 = new LinkedHashMap<>();
     for (final TaskStats stat1: taskStats) {
       if (stat1.getElapsed()>=0) { // consider only tasks that ended
         reported++;
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefRoundTask.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefRoundTask.java
index e5e047b..7cebb1e 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefRoundTask.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefRoundTask.java
@@ -50,7 +50,7 @@ public class RepSumByPrefRoundTask extends RepSumByPrefTask {
   protected Report reportSumByPrefixRound(List<TaskStats> taskStats) {
     // aggregate by task name and by round
     int reported = 0;
-    LinkedHashMap<String,TaskStats> p2 = new LinkedHashMap<String,TaskStats>();
+    LinkedHashMap<String,TaskStats> p2 = new LinkedHashMap<>();
     for (final TaskStats stat1 : taskStats) {
       if (stat1.getElapsed()>=0 && stat1.getTask().getName().startsWith(prefix)) { // only ended tasks with proper name
         reported++;
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefTask.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefTask.java
index 3a4a961..241f252 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefTask.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/RepSumByPrefTask.java
@@ -52,7 +52,7 @@ public class RepSumByPrefTask extends ReportTask {
   protected Report reportSumByPrefix (List<TaskStats> taskStats) {
     // aggregate by task name
     int reported = 0;
-    LinkedHashMap<String,TaskStats> p2 = new LinkedHashMap<String,TaskStats>();
+    LinkedHashMap<String,TaskStats> p2 = new LinkedHashMap<>();
     for (final TaskStats stat1 : taskStats) {
       if (stat1.getElapsed()>=0 && stat1.getTask().getName().startsWith(prefix)) { // only ended tasks with proper name
         reported++;
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetHighlightTask.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetHighlightTask.java
index 8e3d87e..63c5f33 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetHighlightTask.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetHighlightTask.java
@@ -138,7 +138,7 @@ public class SearchTravRetHighlightTask extends SearchTravTask {
       } else if (splits[i].startsWith("mergeContiguous[") == true){
         mergeContiguous = Boolean.valueOf(splits[i].substring("mergeContiguous[".length(),splits[i].length() - 1)).booleanValue();
       } else if (splits[i].startsWith("fields[") == true){
-        paramFields = new HashSet<String>();
+        paramFields = new HashSet<>();
         String fieldNames = splits[i].substring("fields[".length(), splits[i].length() - 1);
         String [] fieldSplits = fieldNames.split(";");
         for (int j = 0; j < fieldSplits.length; j++) {
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetLoadFieldSelectorTask.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetLoadFieldSelectorTask.java
index 406e994..70167de 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetLoadFieldSelectorTask.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetLoadFieldSelectorTask.java
@@ -68,7 +68,7 @@ public class SearchTravRetLoadFieldSelectorTask extends SearchTravTask {
   @Override
   public void setParams(String params) {
     this.params = params; // cannot just call super.setParams(), b/c it's params differ.
-    fieldsToLoad = new HashSet<String>();
+    fieldsToLoad = new HashSet<>();
     for (StringTokenizer tokenizer = new StringTokenizer(params, ","); tokenizer.hasMoreTokens();) {
       String s = tokenizer.nextToken();
       fieldsToLoad.add(s);
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetVectorHighlightTask.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetVectorHighlightTask.java
index 4993aff..6d9eeb3 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetVectorHighlightTask.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetVectorHighlightTask.java
@@ -135,7 +135,7 @@ public class SearchTravRetVectorHighlightTask extends SearchTravTask {
       } else if (splits[i].startsWith("fragSize[") == true){
         fragSize = (int)Float.parseFloat(splits[i].substring("fragSize[".length(),splits[i].length() - 1));
       } else if (splits[i].startsWith("fields[") == true){
-        paramFields = new HashSet<String>();
+        paramFields = new HashSet<>();
         String fieldNames = splits[i].substring("fields[".length(), splits[i].length() - 1);
         String [] fieldSplits = fieldNames.split(";");
         for (int j = 0; j < fieldSplits.length; j++) {
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/TaskSequence.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/TaskSequence.java
index 1092220..9ee2ab1 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/TaskSequence.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/TaskSequence.java
@@ -58,7 +58,7 @@ public class TaskSequence extends PerfTask {
     setSequenceName();
     this.parent = parent;
     this.parallel = parallel;
-    tasks = new ArrayList<PerfTask>();
+    tasks = new ArrayList<>();
     logByTimeMsec = runData.getConfig().get("report.time.step.msec", 0);
   }
 
@@ -186,7 +186,7 @@ public class TaskSequence extends PerfTask {
         final PerfTask task = tasksArray[l];
         if (task.getRunInBackground()) {
           if (bgTasks == null) {
-            bgTasks = new ArrayList<RunBackgroundTask>();
+            bgTasks = new ArrayList<>();
           }
           RunBackgroundTask bgTask = new RunBackgroundTask(task, letChildReport);
           bgTask.setPriority(task.getBackgroundDeltaPriority() + Thread.currentThread().getPriority());
@@ -518,7 +518,7 @@ public class TaskSequence extends PerfTask {
   @Override
   protected TaskSequence clone() throws CloneNotSupportedException {
     TaskSequence res = (TaskSequence) super.clone();
-    res.tasks = new ArrayList<PerfTask>();
+    res.tasks = new ArrayList<>();
     for (int i = 0; i < tasks.size(); i++) {
       res.tasks.add(tasks.get(i).clone());
     }
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTask.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTask.java
index 6ea37b6..d0f1c52 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTask.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTask.java
@@ -86,8 +86,8 @@ public class WriteLineDocTask extends PerfTask {
   protected final String fname;
   private final PrintWriter lineFileOut;
   private final DocMaker docMaker;
-  private final ThreadLocal<StringBuilder> threadBuffer = new ThreadLocal<StringBuilder>();
-  private final ThreadLocal<Matcher> threadNormalizer = new ThreadLocal<Matcher>();
+  private final ThreadLocal<StringBuilder> threadBuffer = new ThreadLocal<>();
+  private final ThreadLocal<Matcher> threadNormalizer = new ThreadLocal<>();
   private final String[] fieldsToWrite;
   private final boolean[] sufficientFields;
   private final boolean checkSufficientFields;
@@ -122,7 +122,7 @@ public class WriteLineDocTask extends PerfTask {
       checkSufficientFields = false;
     } else {
       checkSufficientFields = true;
-      HashSet<String> sf = new HashSet<String>(Arrays.asList(suff.split(",")));
+      HashSet<String> sf = new HashSet<>(Arrays.asList(suff.split(",")));
       for (int i=0; i<fieldsToWrite.length; i++) {
         if (sf.contains(fieldsToWrite[i])) {
           sufficientFields[i] = true;
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Algorithm.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Algorithm.java
index ef9092c..a0a2d0a 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Algorithm.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Algorithm.java
@@ -293,7 +293,7 @@ public class Algorithm {
     if (alts==null) {
       return new String[]{ dfltPkg };
     }
-    ArrayList<String> pkgs = new ArrayList<String>();
+    ArrayList<String> pkgs = new ArrayList<>();
     pkgs.add(dfltPkg);
     for (String alt : alts.split(",")) {
       pkgs.add(alt);
@@ -339,7 +339,7 @@ public class Algorithm {
    * @return all tasks participating in this algorithm.
    */
   public ArrayList<PerfTask> extractTasks() {
-    ArrayList<PerfTask> res = new ArrayList<PerfTask>();
+    ArrayList<PerfTask> res = new ArrayList<>();
     extractTasks(res, sequence);
     return res;
   }
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Config.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Config.java
index 8d615f9..f9456a3 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Config.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Config.java
@@ -50,8 +50,8 @@ public class Config {
 
   private int roundNumber = 0;
   private Properties props;
-  private HashMap<String, Object> valByRound = new HashMap<String, Object>();
-  private HashMap<String, String> colForValByRound = new HashMap<String, String>();
+  private HashMap<String, Object> valByRound = new HashMap<>();
+  private HashMap<String, String> colForValByRound = new HashMap<>();
   private String algorithmText;
 
   /**
@@ -62,7 +62,7 @@ public class Config {
    */
   public Config(Reader algReader) throws IOException {
     // read alg file to array of lines
-    ArrayList<String> lines = new ArrayList<String>();
+    ArrayList<String> lines = new ArrayList<>();
     BufferedReader r = new BufferedReader(algReader);
     int lastConfigLine = 0;
     for (String line = r.readLine(); line != null; line = r.readLine()) {
@@ -314,7 +314,7 @@ public class Config {
       return new String[]{s};
     }
 
-    ArrayList<String> a = new ArrayList<String>();
+    ArrayList<String> a = new ArrayList<>();
     StringTokenizer st = new StringTokenizer(s, ":");
     while (st.hasMoreTokens()) {
       String t = st.nextToken();
@@ -329,7 +329,7 @@ public class Config {
       return new int[]{Integer.parseInt(s)};
     }
 
-    ArrayList<Integer> a = new ArrayList<Integer>();
+    ArrayList<Integer> a = new ArrayList<>();
     StringTokenizer st = new StringTokenizer(s, ":");
     while (st.hasMoreTokens()) {
       String t = st.nextToken();
@@ -348,7 +348,7 @@ public class Config {
       return new double[]{Double.parseDouble(s)};
     }
 
-    ArrayList<Double> a = new ArrayList<Double>();
+    ArrayList<Double> a = new ArrayList<>();
     StringTokenizer st = new StringTokenizer(s, ":");
     while (st.hasMoreTokens()) {
       String t = st.nextToken();
@@ -367,7 +367,7 @@ public class Config {
       return new boolean[]{Boolean.valueOf(s).booleanValue()};
     }
 
-    ArrayList<Boolean> a = new ArrayList<Boolean>();
+    ArrayList<Boolean> a = new ArrayList<>();
     StringTokenizer st = new StringTokenizer(s, ":");
     while (st.hasMoreTokens()) {
       String t = st.nextToken();
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/StreamUtils.java lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/StreamUtils.java
index 9f20d48..6a3dd3c 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/StreamUtils.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/StreamUtils.java
@@ -71,7 +71,7 @@ public class StreamUtils {
     }
   }
 
-  private static final Map<String,Type> extensionToType = new HashMap<String,Type>();
+  private static final Map<String,Type> extensionToType = new HashMap<>();
   static {
     // these in are lower case, we will lower case at the test as well
     extensionToType.put(".bz2", Type.BZIP2);
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityStats.java lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityStats.java
index dcafbb4..60a031a 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityStats.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityStats.java
@@ -68,7 +68,7 @@ public class QualityStats {
   public QualityStats(double maxGoodPoints, long searchTime) {
     this.maxGoodPoints = maxGoodPoints;
     this.searchTime = searchTime;
-    this.recallPoints = new ArrayList<RecallPoint>();
+    this.recallPoints = new ArrayList<>();
     pAt = new double[MAX_POINTS+1]; // pAt[0] unused. 
   }
 
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/QueryDriver.java lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/QueryDriver.java
index cc4f322..b3bcb55 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/QueryDriver.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/QueryDriver.java
@@ -74,7 +74,7 @@ public class QueryDriver {
     // validate topics & judgments match each other
     judge.validateData(qqs, logger);
 
-    Set<String> fieldSet = new HashSet<String>();
+    Set<String> fieldSet = new HashSet<>();
     if (fieldSpec.indexOf('T') >= 0) fieldSet.add("title");
     if (fieldSpec.indexOf('D') >= 0) fieldSet.add("description");
     if (fieldSpec.indexOf('N') >= 0) fieldSet.add("narrative");
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/Trec1MQReader.java lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/Trec1MQReader.java
index 632d93d..a1346e7 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/Trec1MQReader.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/Trec1MQReader.java
@@ -55,7 +55,7 @@ public class Trec1MQReader {
    * @throws IOException if cannot read the queries.
    */
   public QualityQuery[] readQueries(BufferedReader reader) throws IOException {
-    ArrayList<QualityQuery> res = new ArrayList<QualityQuery>();
+    ArrayList<QualityQuery> res = new ArrayList<>();
     String line;
     try {
       while (null!=(line=reader.readLine())) {
@@ -69,7 +69,7 @@ public class Trec1MQReader {
         // qtext
         String qtext = line.substring(k+1).trim();
         // we got a topic!
-        HashMap<String,String> fields = new HashMap<String,String>();
+        HashMap<String,String> fields = new HashMap<>();
         fields.put(name,qtext);
         //System.out.println("id: "+id+" qtext: "+qtext+"  line: "+line);
         QualityQuery topic = new QualityQuery(id,fields);
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecJudge.java lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecJudge.java
index a82af99..156b0d5 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecJudge.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecJudge.java
@@ -49,7 +49,7 @@ public class TrecJudge implements Judge {
    * @throws IOException If there is a low-level I/O error.
    */
   public TrecJudge (BufferedReader reader) throws IOException {
-    judgements = new HashMap<String,QRelJudgement>();
+    judgements = new HashMap<>();
     QRelJudgement curr = null;
     String zero = "0";
     String line;
@@ -96,7 +96,7 @@ public class TrecJudge implements Judge {
     
     QRelJudgement(String queryID) {
       this.queryID = queryID;
-      relevantDocs = new HashMap<String,String>();
+      relevantDocs = new HashMap<>();
     }
     
     public void addRelevandDoc(String docName) {
@@ -115,8 +115,8 @@ public class TrecJudge implements Judge {
   // inherit javadocs
   @Override
   public boolean validateData(QualityQuery[] qq, PrintWriter logger) {
-    HashMap<String,QRelJudgement> missingQueries = new HashMap<String, QRelJudgement>(judgements);
-    ArrayList<String> missingJudgements = new ArrayList<String>();
+    HashMap<String,QRelJudgement> missingQueries = new HashMap<>(judgements);
+    ArrayList<String> missingJudgements = new ArrayList<>();
     for (int i=0; i<qq.length; i++) {
       String id = qq[i].getQueryID();
       if (missingQueries.containsKey(id)) {
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecTopicsReader.java lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecTopicsReader.java
index 8bee7be..8c478ab 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecTopicsReader.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecTopicsReader.java
@@ -62,11 +62,11 @@ public class TrecTopicsReader {
    * @throws IOException if cannot read the queries.
    */
   public QualityQuery[] readQueries(BufferedReader reader) throws IOException {
-    ArrayList<QualityQuery> res = new ArrayList<QualityQuery>();
+    ArrayList<QualityQuery> res = new ArrayList<>();
     StringBuilder sb;
     try {
       while (null!=(sb=read(reader,"<top>",null,false,false))) {
-        HashMap<String,String> fields = new HashMap<String,String>();
+        HashMap<String,String> fields = new HashMap<>();
         // id
         sb = read(reader,"<num>",null,true,false);
         int k = sb.indexOf(":");
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/DocNameExtractor.java lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/DocNameExtractor.java
index 25b48df..6f133f4 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/DocNameExtractor.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/DocNameExtractor.java
@@ -47,7 +47,7 @@ public class DocNameExtractor {
    * @throws IOException if cannot extract the doc name from the index.
    */
   public String docName(IndexSearcher searcher, int docid) throws IOException {
-    final List<String> name = new ArrayList<String>();
+    final List<String> name = new ArrayList<>();
     searcher.getIndexReader().document(docid, new StoredFieldVisitor() {
         @Override
         public void stringField(FieldInfo fieldInfo, String value) {
diff --git lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java
index 1add4ff..a0b33c5 100644
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java
@@ -35,7 +35,7 @@ public class SimpleQQParser implements QualityQueryParser {
 
   private String qqNames[];
   private String indexField;
-  ThreadLocal<QueryParser> queryParser = new ThreadLocal<QueryParser>();
+  ThreadLocal<QueryParser> queryParser = new ThreadLocal<>();
 
   /**
    * Constructor of a simple qq parser.
diff --git lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/TrecContentSourceTest.java lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/TrecContentSourceTest.java
index 81d2513..006fbba 100644
--- lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/TrecContentSourceTest.java
+++ lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds/TrecContentSourceTest.java
@@ -360,7 +360,7 @@ public class TrecContentSourceTest extends LuceneTestCase {
     DocData dd = new DocData();
     int n = 0;
     boolean gotExpectedException = false;
-    HashSet<ParsePathType> unseenTypes = new HashSet<ParsePathType>(Arrays.asList(ParsePathType.values()));
+    HashSet<ParsePathType> unseenTypes = new HashSet<>(Arrays.asList(ParsePathType.values()));
     try {
       while (n<100) { // arbiterary limit to prevent looping forever in case of test failure
         dd = tcs.getNextDocData(dd);
diff --git lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTaskTest.java lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTaskTest.java
index 439fa96..47aa482 100644
--- lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTaskTest.java
+++ lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTaskTest.java
@@ -344,7 +344,7 @@ public class WriteLineDocTaskTest extends BenchmarkTestCase {
     
     wldt.close();
     
-    Set<String> ids = new HashSet<String>();
+    Set<String> ids = new HashSet<>();
     BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(file), "utf-8"));
     try {
       String line = br.readLine();
diff --git lucene/classification/src/java/org/apache/lucene/classification/KNearestNeighborClassifier.java lucene/classification/src/java/org/apache/lucene/classification/KNearestNeighborClassifier.java
index 22d530a..f4d7347 100644
--- lucene/classification/src/java/org/apache/lucene/classification/KNearestNeighborClassifier.java
+++ lucene/classification/src/java/org/apache/lucene/classification/KNearestNeighborClassifier.java
@@ -97,7 +97,7 @@ public class KNearestNeighborClassifier implements Classifier<BytesRef> {
 
   private ClassificationResult<BytesRef> selectClassFromNeighbors(TopDocs topDocs) throws IOException {
     // TODO : improve the nearest neighbor selection
-    Map<BytesRef, Integer> classCounts = new HashMap<BytesRef, Integer>();
+    Map<BytesRef, Integer> classCounts = new HashMap<>();
     for (ScoreDoc scoreDoc : topDocs.scoreDocs) {
       BytesRef cl = new BytesRef(indexSearcher.doc(scoreDoc.doc).getField(classFieldName).stringValue());
       Integer count = classCounts.get(cl);
@@ -117,7 +117,7 @@ public class KNearestNeighborClassifier implements Classifier<BytesRef> {
       }
     }
     double score = max / (double) k;
-    return new ClassificationResult<BytesRef>(assignedClass, score);
+    return new ClassificationResult<>(assignedClass, score);
   }
 
   /**
diff --git lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier.java lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier.java
index f039e70..cf11e5c 100644
--- lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier.java
+++ lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier.java
@@ -109,7 +109,7 @@ public class SimpleNaiveBayesClassifier implements Classifier<BytesRef> {
   }
 
   private String[] tokenizeDoc(String doc) throws IOException {
-    Collection<String> result = new LinkedList<String>();
+    Collection<String> result = new LinkedList<>();
     for (String textFieldName : textFieldNames) {
       try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {
         CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);
@@ -146,7 +146,7 @@ public class SimpleNaiveBayesClassifier implements Classifier<BytesRef> {
       }
     }
     double score = 10 / Math.abs(max);
-    return new ClassificationResult<BytesRef>(foundClass, score);
+    return new ClassificationResult<>(foundClass, score);
   }
 
 
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.java lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.java
index bb81e8b..0a75f89 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.java
@@ -66,7 +66,7 @@ public class BlockTermsReader extends FieldsProducer {
   // produce DocsEnum on demand
   private final PostingsReaderBase postingsReader;
 
-  private final TreeMap<String,FieldReader> fields = new TreeMap<String,FieldReader>();
+  private final TreeMap<String,FieldReader> fields = new TreeMap<>();
 
   // Reads the terms index
   private TermsIndexReaderBase indexReader;
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsWriter.java lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsWriter.java
index 6ac103b..579cd02 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsWriter.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsWriter.java
@@ -96,7 +96,7 @@ public class BlockTermsWriter extends FieldsConsumer implements Closeable {
     }
   }
 
-  private final List<FieldMetaData> fields = new ArrayList<FieldMetaData>();
+  private final List<FieldMetaData> fields = new ArrayList<>();
 
   // private final String segment;
 
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java
index 621d9ce..b13966b 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java
@@ -61,7 +61,7 @@ public class FixedGapTermsIndexReader extends TermsIndexReaderBase {
   private final PagedBytes termBytes = new PagedBytes(PAGED_BYTES_BITS);
   private PagedBytes.Reader termBytesReader;
 
-  final HashMap<FieldInfo,FieldIndexData> fields = new HashMap<FieldInfo,FieldIndexData>();
+  final HashMap<FieldInfo,FieldIndexData> fields = new HashMap<>();
   
   // start of the field info data
   private long dirOffset;
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexWriter.java lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexWriter.java
index 789300e..4787b7b 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexWriter.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexWriter.java
@@ -58,7 +58,7 @@ public class FixedGapTermsIndexWriter extends TermsIndexWriterBase {
   final private int termIndexInterval;
   public static final int DEFAULT_TERM_INDEX_INTERVAL = 32;
 
-  private final List<SimpleFieldWriter> fields = new ArrayList<SimpleFieldWriter>();
+  private final List<SimpleFieldWriter> fields = new ArrayList<>();
   
   public FixedGapTermsIndexWriter(SegmentWriteState state) throws IOException {
     this(state, DEFAULT_TERM_INDEX_INTERVAL);
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexReader.java lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexReader.java
index 7da8bd9..914d661 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexReader.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexReader.java
@@ -45,7 +45,7 @@ public class VariableGapTermsIndexReader extends TermsIndexReaderBase {
 
   private final PositiveIntOutputs fstOutputs = PositiveIntOutputs.getSingleton();
 
-  final HashMap<FieldInfo,FieldIndexData> fields = new HashMap<FieldInfo,FieldIndexData>();
+  final HashMap<FieldInfo,FieldIndexData> fields = new HashMap<>();
   
   // start of the field info data
   private long dirOffset;
@@ -104,7 +104,7 @@ public class VariableGapTermsIndexReader extends TermsIndexReaderBase {
     private BytesRefFSTEnum.InputOutput<Long> current;
 
     public IndexEnum(FST<Long> fst) {
-      fstEnum = new BytesRefFSTEnum<Long>(fst);
+      fstEnum = new BytesRefFSTEnum<>(fst);
     }
 
     @Override
@@ -158,7 +158,7 @@ public class VariableGapTermsIndexReader extends TermsIndexReaderBase {
     public FieldIndexData(IndexInput in, FieldInfo fieldInfo, long indexStart) throws IOException {
       IndexInput clone = in.clone();
       clone.seek(indexStart);
-      fst = new FST<Long>(clone, fstOutputs);
+      fst = new FST<>(clone, fstOutputs);
       clone.close();
 
       /*
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexWriter.java lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexWriter.java
index 6d3f6ba..4b9be36 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexWriter.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexWriter.java
@@ -55,7 +55,7 @@ public class VariableGapTermsIndexWriter extends TermsIndexWriterBase {
   final static int VERSION_APPEND_ONLY = 1;
   final static int VERSION_CURRENT = VERSION_APPEND_ONLY;
 
-  private final List<FSTFieldWriter> fields = new ArrayList<FSTFieldWriter>();
+  private final List<FSTFieldWriter> fields = new ArrayList<>();
   
   @SuppressWarnings("unused") private final FieldInfos fieldInfos; // unread
   private final IndexTermSelector policy;
@@ -236,7 +236,7 @@ public class VariableGapTermsIndexWriter extends TermsIndexWriterBase {
     public FSTFieldWriter(FieldInfo fieldInfo, long termsFilePointer) throws IOException {
       this.fieldInfo = fieldInfo;
       fstOutputs = PositiveIntOutputs.getSingleton();
-      fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, fstOutputs);
+      fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE1, fstOutputs);
       indexStart = out.getFilePointer();
       ////System.out.println("VGW: field=" + fieldInfo.name);
 
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java
index 6960387..b06c8a2 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java
@@ -150,7 +150,7 @@ public final class BloomFilteringPostingsFormat extends PostingsFormat {
   
   public class BloomFilteredFieldsProducer extends FieldsProducer {
     private FieldsProducer delegateFieldsProducer;
-    HashMap<String,FuzzySet> bloomsByFieldName = new HashMap<String,FuzzySet>();
+    HashMap<String,FuzzySet> bloomsByFieldName = new HashMap<>();
     
     public BloomFilteredFieldsProducer(SegmentReadState state)
         throws IOException {
@@ -394,7 +394,7 @@ public final class BloomFilteringPostingsFormat extends PostingsFormat {
   
   class BloomFilteredFieldsConsumer extends FieldsConsumer {
     private FieldsConsumer delegateFieldsConsumer;
-    private Map<FieldInfo,FuzzySet> bloomFilters = new HashMap<FieldInfo,FuzzySet>();
+    private Map<FieldInfo,FuzzySet> bloomFilters = new HashMap<>();
     private SegmentWriteState state;
     
     public BloomFilteredFieldsConsumer(FieldsConsumer fieldsConsumer,
@@ -454,7 +454,7 @@ public final class BloomFilteringPostingsFormat extends PostingsFormat {
 
     public void close() throws IOException {
       // Now we are done accumulating values for these fields
-      List<Entry<FieldInfo,FuzzySet>> nonSaturatedBlooms = new ArrayList<Map.Entry<FieldInfo,FuzzySet>>();
+      List<Entry<FieldInfo,FuzzySet>> nonSaturatedBlooms = new ArrayList<>();
       
       for (Entry<FieldInfo,FuzzySet> entry : bloomFilters.entrySet()) {
         FuzzySet bloomFilter = entry.getValue();
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesProducer.java lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesProducer.java
index bf7c578..cbca82e 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesProducer.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesProducer.java
@@ -46,22 +46,22 @@ import org.apache.lucene.util.RamUsageEstimator;
 
 class DirectDocValuesProducer extends DocValuesProducer {
   // metadata maps (just file pointers and minimal stuff)
-  private final Map<Integer,NumericEntry> numerics = new HashMap<Integer,NumericEntry>();
-  private final Map<Integer,BinaryEntry> binaries = new HashMap<Integer,BinaryEntry>();
-  private final Map<Integer,SortedEntry> sorteds = new HashMap<Integer,SortedEntry>();
-  private final Map<Integer,SortedSetEntry> sortedSets = new HashMap<Integer,SortedSetEntry>();
+  private final Map<Integer,NumericEntry> numerics = new HashMap<>();
+  private final Map<Integer,BinaryEntry> binaries = new HashMap<>();
+  private final Map<Integer,SortedEntry> sorteds = new HashMap<>();
+  private final Map<Integer,SortedSetEntry> sortedSets = new HashMap<>();
   private final IndexInput data;
   
   // ram instances we have already loaded
   private final Map<Integer,NumericDocValues> numericInstances = 
-      new HashMap<Integer,NumericDocValues>();
+      new HashMap<>();
   private final Map<Integer,BinaryDocValues> binaryInstances =
-      new HashMap<Integer,BinaryDocValues>();
+      new HashMap<>();
   private final Map<Integer,SortedDocValues> sortedInstances =
-      new HashMap<Integer,SortedDocValues>();
+      new HashMap<>();
   private final Map<Integer,SortedSetRawValues> sortedSetInstances =
-      new HashMap<Integer,SortedSetRawValues>();
-  private final Map<Integer,Bits> docsWithFieldInstances = new HashMap<Integer,Bits>();
+      new HashMap<>();
+  private final Map<Integer,Bits> docsWithFieldInstances = new HashMap<>();
   
   private final int maxDoc;
   private final AtomicLong ramBytesUsed;
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.java lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.java
index 0e02e7e..d0e04f2 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.java
@@ -121,7 +121,7 @@ public final class DirectPostingsFormat extends PostingsFormat {
   }
 
   private static final class DirectFields extends FieldsProducer {
-    private final Map<String,DirectField> fields = new TreeMap<String,DirectField>();
+    private final Map<String,DirectField> fields = new TreeMap<>();
 
     public DirectFields(SegmentReadState state, Fields fields, int minSkipCount, int lowFreqCutoff) throws IOException {
       for (String field : fields) {
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsReader.java lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsReader.java
index 4403a30..7a90867 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsReader.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsReader.java
@@ -69,7 +69,7 @@ import org.apache.lucene.codecs.memory.FSTTermsReader.TermsReader;
  */
 public class FSTOrdTermsReader extends FieldsProducer {
   static final int INTERVAL = FSTOrdTermsWriter.SKIP_INTERVAL;
-  final TreeMap<String, TermsReader> fields = new TreeMap<String, TermsReader>();
+  final TreeMap<String, TermsReader> fields = new TreeMap<>();
   final PostingsReaderBase postingsReader;
   IndexInput indexIn = null;
   IndexInput blockIn = null;
@@ -98,7 +98,7 @@ public class FSTOrdTermsReader extends FieldsProducer {
         long sumDocFreq = blockIn.readVLong();
         int docCount = blockIn.readVInt();
         int longsSize = blockIn.readVInt();
-        FST<Long> index = new FST<Long>(indexIn, PositiveIntOutputs.getSingleton());
+        FST<Long> index = new FST<>(indexIn, PositiveIntOutputs.getSingleton());
 
         TermsReader current = new TermsReader(fieldInfo, numTerms, sumTotalTermFreq, sumDocFreq, docCount, longsSize, index);
         TermsReader previous = fields.put(fieldInfo.name, current);
@@ -427,7 +427,7 @@ public class FSTOrdTermsReader extends FieldsProducer {
       boolean seekPending;
 
       SegmentTermsEnum() throws IOException {
-        this.fstEnum = new BytesRefFSTEnum<Long>(index);
+        this.fstEnum = new BytesRefFSTEnum<>(index);
         this.decoded = false;
         this.seekPending = false;
       }
@@ -521,7 +521,7 @@ public class FSTOrdTermsReader extends FieldsProducer {
         int state;
 
         Frame() {
-          this.arc = new FST.Arc<Long>();
+          this.arc = new FST.Arc<>();
           this.state = -1;
         }
 
@@ -781,7 +781,7 @@ public class FSTOrdTermsReader extends FieldsProducer {
   }
 
   static<T> void walk(FST<T> fst) throws IOException {
-    final ArrayList<FST.Arc<T>> queue = new ArrayList<FST.Arc<T>>();
+    final ArrayList<FST.Arc<T>> queue = new ArrayList<>();
     final BitSet seen = new BitSet();
     final FST.BytesReader reader = fst.getBytesReader();
     final FST.Arc<T> startArc = fst.getFirstArc(new FST.Arc<T>());
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsWriter.java lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsWriter.java
index f1eec36..d854c36 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsWriter.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsWriter.java
@@ -154,7 +154,7 @@ public class FSTOrdTermsWriter extends FieldsConsumer {
   final PostingsWriterBase postingsWriter;
   final FieldInfos fieldInfos;
   final int maxDoc;
-  final List<FieldMetaData> fields = new ArrayList<FieldMetaData>();
+  final List<FieldMetaData> fields = new ArrayList<>();
   IndexOutput blockOut = null;
   IndexOutput indexOut = null;
 
@@ -305,7 +305,7 @@ public class FSTOrdTermsWriter extends FieldsConsumer {
       this.fieldInfo = fieldInfo;
       this.longsSize = postingsWriter.setField(fieldInfo);
       this.outputs = PositiveIntOutputs.getSingleton();
-      this.builder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, outputs);
+      this.builder = new Builder<>(FST.INPUT_TYPE.BYTE1, outputs);
 
       this.lastBlockStatsFP = 0;
       this.lastBlockMetaLongsFP = 0;
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsReader.java lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsReader.java
index f88ae9f..cfa4d03 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsReader.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsReader.java
@@ -65,7 +65,7 @@ import org.apache.lucene.codecs.CodecUtil;
  */
 
 public class FSTTermsReader extends FieldsProducer {
-  final TreeMap<String, TermsReader> fields = new TreeMap<String, TermsReader>();
+  final TreeMap<String, TermsReader> fields = new TreeMap<>();
   final PostingsReaderBase postingsReader;
   final IndexInput in;
   //static boolean TEST = false;
@@ -172,7 +172,7 @@ public class FSTTermsReader extends FieldsProducer {
       this.sumDocFreq = sumDocFreq;
       this.docCount = docCount;
       this.longsSize = longsSize;
-      this.dict = new FST<FSTTermOutputs.TermData>(in, new FSTTermOutputs(fieldInfo, longsSize));
+      this.dict = new FST<>(in, new FSTTermOutputs(fieldInfo, longsSize));
     }
 
     @Override
@@ -307,7 +307,7 @@ public class FSTTermsReader extends FieldsProducer {
 
       SegmentTermsEnum() throws IOException {
         super();
-        this.fstEnum = new BytesRefFSTEnum<FSTTermOutputs.TermData>(dict);
+        this.fstEnum = new BytesRefFSTEnum<>(dict);
         this.decoded = false;
         this.seekPending = false;
         this.meta = null;
@@ -411,7 +411,7 @@ public class FSTTermsReader extends FieldsProducer {
         int fsaState;
 
         Frame() {
-          this.fstArc = new FST.Arc<FSTTermOutputs.TermData>();
+          this.fstArc = new FST.Arc<>();
           this.fsaState = -1;
         }
 
@@ -697,7 +697,7 @@ public class FSTTermsReader extends FieldsProducer {
   }
 
   static<T> void walk(FST<T> fst) throws IOException {
-    final ArrayList<FST.Arc<T>> queue = new ArrayList<FST.Arc<T>>();
+    final ArrayList<FST.Arc<T>> queue = new ArrayList<>();
     final BitSet seen = new BitSet();
     final FST.BytesReader reader = fst.getBytesReader();
     final FST.Arc<T> startArc = fst.getFirstArc(new FST.Arc<T>());
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsWriter.java lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsWriter.java
index 4d3b9f9..433a240 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsWriter.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsWriter.java
@@ -130,7 +130,7 @@ public class FSTTermsWriter extends FieldsConsumer {
   final FieldInfos fieldInfos;
   final IndexOutput out;
   final int maxDoc;
-  final List<FieldMetaData> fields = new ArrayList<FieldMetaData>();
+  final List<FieldMetaData> fields = new ArrayList<>();
 
   public FSTTermsWriter(SegmentWriteState state, PostingsWriterBase postingsWriter) throws IOException {
     final String termsFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_EXTENSION);
@@ -259,7 +259,7 @@ public class FSTTermsWriter extends FieldsConsumer {
       this.fieldInfo = fieldInfo;
       this.longsSize = postingsWriter.setField(fieldInfo);
       this.outputs = new FSTTermOutputs(fieldInfo, longsSize);
-      this.builder = new Builder<FSTTermOutputs.TermData>(FST.INPUT_TYPE.BYTE1, outputs);
+      this.builder = new Builder<>(FST.INPUT_TYPE.BYTE1, outputs);
     }
 
     public void finishTerm(BytesRef text, BlockTermState state) throws IOException {
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer.java lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer.java
index 2b3b990..5365a81 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer.java
@@ -158,7 +158,7 @@ class MemoryDocValuesConsumer extends DocValuesConsumer {
       } else {
         meta.writeByte(TABLE_COMPRESSED); // table-compressed
         Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
-        final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();
+        final HashMap<Long,Integer> encode = new HashMap<>();
         data.writeVInt(decode.length);
         for (int i = 0; i < decode.length; i++) {
           data.writeLong(decode[i]);
@@ -281,7 +281,7 @@ class MemoryDocValuesConsumer extends DocValuesConsumer {
     meta.writeByte(FST);
     meta.writeLong(data.getFilePointer());
     PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
-    Builder<Long> builder = new Builder<Long>(INPUT_TYPE.BYTE1, outputs);
+    Builder<Long> builder = new Builder<>(INPUT_TYPE.BYTE1, outputs);
     IntsRef scratch = new IntsRef();
     long ord = 0;
     for (BytesRef v : values) {
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesProducer.java lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesProducer.java
index 0264b1e..4b75e88 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesProducer.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesProducer.java
@@ -68,12 +68,12 @@ class MemoryDocValuesProducer extends DocValuesProducer {
   
   // ram instances we have already loaded
   private final Map<Integer,NumericDocValues> numericInstances = 
-      new HashMap<Integer,NumericDocValues>();
+      new HashMap<>();
   private final Map<Integer,BinaryDocValues> binaryInstances =
-      new HashMap<Integer,BinaryDocValues>();
+      new HashMap<>();
   private final Map<Integer,FST<Long>> fstInstances =
-      new HashMap<Integer,FST<Long>>();
-  private final Map<Integer,Bits> docsWithFieldInstances = new HashMap<Integer,Bits>();
+      new HashMap<>();
+  private final Map<Integer,Bits> docsWithFieldInstances = new HashMap<>();
   
   private final int maxDoc;
   private final AtomicLong ramBytesUsed;
@@ -104,9 +104,9 @@ class MemoryDocValuesProducer extends DocValuesProducer {
       version = CodecUtil.checkHeader(in, metaCodec, 
                                       VERSION_START,
                                       VERSION_CURRENT);
-      numerics = new HashMap<Integer,NumericEntry>();
-      binaries = new HashMap<Integer,BinaryEntry>();
-      fsts = new HashMap<Integer,FSTEntry>();
+      numerics = new HashMap<>();
+      binaries = new HashMap<>();
+      fsts = new HashMap<>();
       readFields(in, state.fieldInfos);
       ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
       success = true;
@@ -314,7 +314,7 @@ class MemoryDocValuesProducer extends DocValuesProducer {
       instance = fstInstances.get(field.number);
       if (instance == null) {
         data.seek(entry.offset);
-        instance = new FST<Long>(data, PositiveIntOutputs.getSingleton());
+        instance = new FST<>(data, PositiveIntOutputs.getSingleton());
         ramBytesUsed.addAndGet(instance.sizeInBytes());
         fstInstances.put(field.number, instance);
       }
@@ -324,10 +324,10 @@ class MemoryDocValuesProducer extends DocValuesProducer {
     
     // per-thread resources
     final BytesReader in = fst.getBytesReader();
-    final Arc<Long> firstArc = new Arc<Long>();
-    final Arc<Long> scratchArc = new Arc<Long>();
+    final Arc<Long> firstArc = new Arc<>();
+    final Arc<Long> scratchArc = new Arc<>();
     final IntsRef scratchInts = new IntsRef();
-    final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<Long>(fst); 
+    final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<>(fst);
     
     return new SortedDocValues() {
       @Override
@@ -389,7 +389,7 @@ class MemoryDocValuesProducer extends DocValuesProducer {
       instance = fstInstances.get(field.number);
       if (instance == null) {
         data.seek(entry.offset);
-        instance = new FST<Long>(data, PositiveIntOutputs.getSingleton());
+        instance = new FST<>(data, PositiveIntOutputs.getSingleton());
         ramBytesUsed.addAndGet(instance.sizeInBytes());
         fstInstances.put(field.number, instance);
       }
@@ -399,10 +399,10 @@ class MemoryDocValuesProducer extends DocValuesProducer {
     
     // per-thread resources
     final BytesReader in = fst.getBytesReader();
-    final Arc<Long> firstArc = new Arc<Long>();
-    final Arc<Long> scratchArc = new Arc<Long>();
+    final Arc<Long> firstArc = new Arc<>();
+    final Arc<Long> scratchArc = new Arc<>();
     final IntsRef scratchInts = new IntsRef();
-    final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<Long>(fst); 
+    final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<>(fst);
     final BytesRef ref = new BytesRef();
     final ByteArrayDataInput input = new ByteArrayDataInput();
     return new SortedSetDocValues() {
@@ -546,14 +546,14 @@ class MemoryDocValuesProducer extends DocValuesProducer {
     // maybe we should add a FSTEnum that supports this operation?
     final FST<Long> fst;
     final FST.BytesReader bytesReader;
-    final Arc<Long> firstArc = new Arc<Long>();
-    final Arc<Long> scratchArc = new Arc<Long>();
+    final Arc<Long> firstArc = new Arc<>();
+    final Arc<Long> scratchArc = new Arc<>();
     final IntsRef scratchInts = new IntsRef();
     final BytesRef scratchBytes = new BytesRef();
     
     FSTTermsEnum(FST<Long> fst) {
       this.fst = fst;
-      in = new BytesRefFSTEnum<Long>(fst);
+      in = new BytesRefFSTEnum<>(fst);
       bytesReader = fst.getBytesReader();
     }
 
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java
index 728e1eb..0614c95 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java
@@ -119,7 +119,7 @@ public final class MemoryPostingsFormat extends PostingsFormat {
       this.field = field;
       this.doPackFST = doPackFST;
       this.acceptableOverheadRatio = acceptableOverheadRatio;
-      builder = new Builder<BytesRef>(FST.INPUT_TYPE.BYTE1, 0, 0, true, true, Integer.MAX_VALUE, outputs, null, doPackFST, acceptableOverheadRatio, true, 15);
+      builder = new Builder<>(FST.INPUT_TYPE.BYTE1, 0, 0, true, true, Integer.MAX_VALUE, outputs, null, doPackFST, acceptableOverheadRatio, true, 15);
     }
 
     private class PostingsWriter {
@@ -740,7 +740,7 @@ public final class MemoryPostingsFormat extends PostingsFormat {
 
     public FSTTermsEnum(FieldInfo field, FST<BytesRef> fst) {
       this.field = field;
-      fstEnum = new BytesRefFSTEnum<BytesRef>(fst);
+      fstEnum = new BytesRefFSTEnum<>(fst);
     }
 
     private void decodeMetaData() {
@@ -895,7 +895,7 @@ public final class MemoryPostingsFormat extends PostingsFormat {
       sumDocFreq = in.readVLong();
       docCount = in.readVInt();
       
-      fst = new FST<BytesRef>(in, outputs);
+      fst = new FST<>(in, outputs);
     }
 
     @Override
@@ -953,7 +953,7 @@ public final class MemoryPostingsFormat extends PostingsFormat {
     final String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, EXTENSION);
     final IndexInput in = state.directory.openInput(fileName, IOContext.READONCE);
 
-    final SortedMap<String,TermsReader> fields = new TreeMap<String,TermsReader>();
+    final SortedMap<String,TermsReader> fields = new TreeMap<>();
 
     try {
       while(true) {
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsReader.java lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsReader.java
index af85c4a..4a2e295 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsReader.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsReader.java
@@ -76,7 +76,7 @@ public class PulsingPostingsReader extends PostingsReaderBase {
         version < PulsingPostingsWriter.VERSION_META_ARRAY) {
       fields = null;
     } else {
-      fields = new TreeMap<Integer, Integer>();
+      fields = new TreeMap<>();
       String summaryFileName = IndexFileNames.segmentFileName(segmentState.segmentInfo.name, segmentState.segmentSuffix, PulsingPostingsWriter.SUMMARY_EXTENSION);
       IndexInput in = null;
       try { 
@@ -628,7 +628,7 @@ public class PulsingPostingsReader extends PostingsReaderBase {
     // another pulsing, because this is just stupid and wasteful. 
     // we still have to be careful in case someone does Pulsing(Stomping(Pulsing(...
     private final Map<PulsingPostingsReader,DocsEnum> enums = 
-      new IdentityHashMap<PulsingPostingsReader,DocsEnum>();
+      new IdentityHashMap<>();
       
     @Override
     public Map<PulsingPostingsReader,DocsEnum> enums() {
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsWriter.java lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsWriter.java
index 4a3c214..c9b8863 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsWriter.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsWriter.java
@@ -125,7 +125,7 @@ public final class PulsingPostingsWriter extends PostingsWriterBase {
    *  for this term) is <= maxPositions, then the postings are
    *  inlined into terms dict */
   public PulsingPostingsWriter(SegmentWriteState state, int maxPositions, PostingsWriterBase wrappedPostingsWriter) {
-    fields = new ArrayList<FieldMetaData>();
+    fields = new ArrayList<>();
     this.maxPositions = maxPositions;
     // We simply wrap another postings writer, but only call
     // on it when tot positions is >= the cutoff:
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesReader.java lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesReader.java
index 37952fb..7d1798b 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesReader.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesReader.java
@@ -67,7 +67,7 @@ class SimpleTextDocValuesReader extends DocValuesProducer {
   final int maxDoc;
   final IndexInput data;
   final BytesRef scratch = new BytesRef();
-  final Map<String,OneField> fields = new HashMap<String,OneField>();
+  final Map<String,OneField> fields = new HashMap<>();
   
   public SimpleTextDocValuesReader(SegmentReadState state, String ext) throws IOException {
     // System.out.println("dir=" + state.directory + " seg=" + state.segmentInfo.name + " file=" + IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, ext));
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesWriter.java lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesWriter.java
index 78c6ea0..70ad897 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesWriter.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesWriter.java
@@ -52,7 +52,7 @@ class SimpleTextDocValuesWriter extends DocValuesConsumer {
   final IndexOutput data;
   final BytesRef scratch = new BytesRef();
   final int numDocs;
-  private final Set<String> fieldsSeen = new HashSet<String>(); // for asserting
+  private final Set<String> fieldsSeen = new HashSet<>(); // for asserting
   
   public SimpleTextDocValuesWriter(SegmentWriteState state, String ext) throws IOException {
     // System.out.println("WRITE: " + IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, ext) + " " + state.segmentInfo.getDocCount() + " docs");
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java
index caa8d39..79206a7 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java
@@ -111,7 +111,7 @@ public class SimpleTextFieldInfosReader extends FieldInfosReader {
         SimpleTextUtil.readLine(input, scratch);
         assert StringHelper.startsWith(scratch, NUM_ATTS);
         int numAtts = Integer.parseInt(readString(NUM_ATTS.length, scratch));
-        Map<String,String> atts = new HashMap<String,String>();
+        Map<String,String> atts = new HashMap<>();
 
         for (int j = 0; j < numAtts; j++) {
           SimpleTextUtil.readLine(input, scratch);
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java
index 9968377..17b6014 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java
@@ -83,7 +83,7 @@ class SimpleTextFieldsReader extends FieldsProducer {
   
   private TreeMap<String,Long> readFields(IndexInput in) throws IOException {
     BytesRef scratch = new BytesRef(10);
-    TreeMap<String,Long> fields = new TreeMap<String,Long>();
+    TreeMap<String,Long> fields = new TreeMap<>();
     
     while (true) {
       SimpleTextUtil.readLine(in, scratch);
@@ -106,7 +106,7 @@ class SimpleTextFieldsReader extends FieldsProducer {
 
     public SimpleTextTermsEnum(FST<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> fst, IndexOptions indexOptions) {
       this.indexOptions = indexOptions;
-      fstEnum = new BytesRefFSTEnum<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>>(fst);
+      fstEnum = new BytesRefFSTEnum<>(fst);
     }
 
     @Override
@@ -513,10 +513,10 @@ class SimpleTextFieldsReader extends FieldsProducer {
     private void loadTerms() throws IOException {
       PositiveIntOutputs posIntOutputs = PositiveIntOutputs.getSingleton();
       final Builder<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> b;
-      final PairOutputs<Long,Long> outputsInner = new PairOutputs<Long,Long>(posIntOutputs, posIntOutputs);
-      final PairOutputs<Long,PairOutputs.Pair<Long,Long>> outputs = new PairOutputs<Long,PairOutputs.Pair<Long,Long>>(posIntOutputs,
+      final PairOutputs<Long,Long> outputsInner = new PairOutputs<>(posIntOutputs, posIntOutputs);
+      final PairOutputs<Long,PairOutputs.Pair<Long,Long>> outputs = new PairOutputs<>(posIntOutputs,
                                                                                                                       outputsInner);
-      b = new Builder<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>>(FST.INPUT_TYPE.BYTE1, outputs);
+      b = new Builder<>(FST.INPUT_TYPE.BYTE1, outputs);
       IndexInput in = SimpleTextFieldsReader.this.in.clone();
       in.seek(termsStart);
       final BytesRef lastTerm = new BytesRef(10);
@@ -633,7 +633,7 @@ class SimpleTextFieldsReader extends FieldsProducer {
     return Collections.unmodifiableSet(fields.keySet()).iterator();
   }
 
-  private final Map<String,SimpleTextTerms> termsCache = new HashMap<String,SimpleTextTerms>();
+  private final Map<String,SimpleTextTerms> termsCache = new HashMap<>();
 
   @Override
   synchronized public Terms terms(String field) throws IOException {
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoReader.java lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoReader.java
index 0bc6c3d..e117155 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoReader.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoReader.java
@@ -72,7 +72,7 @@ public class SimpleTextSegmentInfoReader extends SegmentInfoReader {
       SimpleTextUtil.readLine(input, scratch);
       assert StringHelper.startsWith(scratch, SI_NUM_DIAG);
       int numDiag = Integer.parseInt(readString(SI_NUM_DIAG.length, scratch));
-      Map<String,String> diagnostics = new HashMap<String,String>();
+      Map<String,String> diagnostics = new HashMap<>();
 
       for (int i = 0; i < numDiag; i++) {
         SimpleTextUtil.readLine(input, scratch);
@@ -88,7 +88,7 @@ public class SimpleTextSegmentInfoReader extends SegmentInfoReader {
       SimpleTextUtil.readLine(input, scratch);
       assert StringHelper.startsWith(scratch, SI_NUM_FILES);
       int numFiles = Integer.parseInt(readString(SI_NUM_FILES.length, scratch));
-      Set<String> files = new HashSet<String>();
+      Set<String> files = new HashSet<>();
 
       for (int i = 0; i < numFiles; i++) {
         SimpleTextUtil.readLine(input, scratch);
diff --git lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java
index d415177..c9e9c9e 100644
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java
@@ -96,7 +96,7 @@ public class SimpleTextTermVectorsReader extends TermVectorsReader {
   
   @Override
   public Fields get(int doc) throws IOException {
-    SortedMap<String,SimpleTVTerms> fields = new TreeMap<String,SimpleTVTerms>();
+    SortedMap<String,SimpleTVTerms> fields = new TreeMap<>();
     in.seek(offsets[doc]);
     readLine();
     assert StringHelper.startsWith(scratch, NUMFIELDS);
@@ -261,7 +261,7 @@ public class SimpleTextTermVectorsReader extends TermVectorsReader {
       this.hasOffsets = hasOffsets;
       this.hasPositions = hasPositions;
       this.hasPayloads = hasPayloads;
-      terms = new TreeMap<BytesRef,SimpleTVPostings>();
+      terms = new TreeMap<>();
     }
     
     @Override
diff --git lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java
index a0e3282..b5de916 100644
--- lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java
+++ lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java
@@ -56,7 +56,7 @@ public class TestPulsingReuse extends LuceneTestCase {
     
     AtomicReader segment = getOnlySegmentReader(ir);
     DocsEnum reuse = null;
-    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();
+    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<>();
     TermsEnum te = segment.terms("foo").iterator(null);
     while (te.next() != null) {
       reuse = te.docs(null, reuse, DocsEnum.FLAG_NONE);
@@ -97,7 +97,7 @@ public class TestPulsingReuse extends LuceneTestCase {
     
     AtomicReader segment = getOnlySegmentReader(ir);
     DocsEnum reuse = null;
-    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();
+    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<>();
     TermsEnum te = segment.terms("foo").iterator(null);
     while (te.next() != null) {
       reuse = te.docs(null, reuse, DocsEnum.FLAG_NONE);
diff --git lucene/core/src/java/org/apache/lucene/analysis/Analyzer.java lucene/core/src/java/org/apache/lucene/analysis/Analyzer.java
index a5cf576..da56a5e 100644
--- lucene/core/src/java/org/apache/lucene/analysis/Analyzer.java
+++ lucene/core/src/java/org/apache/lucene/analysis/Analyzer.java
@@ -73,7 +73,7 @@ public abstract class Analyzer implements Closeable {
   private final ReuseStrategy reuseStrategy;
 
   // non final as it gets nulled if closed; pkg private for access by ReuseStrategy's final helper methods:
-  CloseableThreadLocal<Object> storedValue = new CloseableThreadLocal<Object>();
+  CloseableThreadLocal<Object> storedValue = new CloseableThreadLocal<>();
 
   /**
    * Create a new Analyzer, reusing the same set of components per-thread
@@ -417,7 +417,7 @@ public abstract class Analyzer implements Closeable {
     public void setReusableComponents(Analyzer analyzer, String fieldName, TokenStreamComponents components) {
       Map<String, TokenStreamComponents> componentsPerField = (Map<String, TokenStreamComponents>) getStoredValue(analyzer);
       if (componentsPerField == null) {
-        componentsPerField = new HashMap<String, TokenStreamComponents>();
+        componentsPerField = new HashMap<>();
         setStoredValue(analyzer, componentsPerField);
       }
       componentsPerField.put(fieldName, components);
diff --git lucene/core/src/java/org/apache/lucene/analysis/CachingTokenFilter.java lucene/core/src/java/org/apache/lucene/analysis/CachingTokenFilter.java
index e71452a..aef4a70 100644
--- lucene/core/src/java/org/apache/lucene/analysis/CachingTokenFilter.java
+++ lucene/core/src/java/org/apache/lucene/analysis/CachingTokenFilter.java
@@ -51,7 +51,7 @@ public final class CachingTokenFilter extends TokenFilter {
   public final boolean incrementToken() throws IOException {
     if (cache == null) {
       // fill cache lazily
-      cache = new LinkedList<AttributeSource.State>();
+      cache = new LinkedList<>();
       fillCache();
       iterator = cache.iterator();
     }
diff --git lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader.java lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader.java
index 0b41ac3..396315b 100644
--- lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader.java
+++ lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader.java
@@ -96,7 +96,7 @@ public class BlockTreeTermsReader extends FieldsProducer {
   // produce DocsEnum on demand
   private final PostingsReaderBase postingsReader;
 
-  private final TreeMap<String,FieldReader> fields = new TreeMap<String,FieldReader>();
+  private final TreeMap<String,FieldReader> fields = new TreeMap<>();
 
   /** File offset where the directory starts in the terms file. */
   private long dirOffset;
@@ -474,7 +474,7 @@ public class BlockTreeTermsReader extends FieldsProducer {
         final IndexInput clone = indexIn.clone();
         //System.out.println("start=" + indexStartFP + " field=" + fieldInfo.name);
         clone.seek(indexStartFP);
-        index = new FST<BytesRef>(clone, ByteSequenceOutputs.getSingleton());
+        index = new FST<>(clone, ByteSequenceOutputs.getSingleton());
         
         /*
         if (false) {
@@ -848,7 +848,7 @@ public class BlockTreeTermsReader extends FieldsProducer {
           stack[idx] = new Frame(idx);
         }
         for(int arcIdx=0;arcIdx<arcs.length;arcIdx++) {
-          arcs[arcIdx] = new FST.Arc<BytesRef>();
+          arcs[arcIdx] = new FST.Arc<>();
         }
 
         if (index == null) {
@@ -917,7 +917,7 @@ public class BlockTreeTermsReader extends FieldsProducer {
             new FST.Arc[ArrayUtil.oversize(1+ord, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
           System.arraycopy(arcs, 0, next, 0, arcs.length);
           for(int arcOrd=arcs.length;arcOrd<next.length;arcOrd++) {
-            next[arcOrd] = new FST.Arc<BytesRef>();
+            next[arcOrd] = new FST.Arc<>();
           }
           arcs = next;
         }
@@ -1299,7 +1299,7 @@ public class BlockTreeTermsReader extends FieldsProducer {
         // Init w/ root block; don't use index since it may
         // not (and need not) have been loaded
         for(int arcIdx=0;arcIdx<arcs.length;arcIdx++) {
-          arcs[arcIdx] = new FST.Arc<BytesRef>();
+          arcs[arcIdx] = new FST.Arc<>();
         }
 
         currentFrame = staticFrame;
@@ -1441,7 +1441,7 @@ public class BlockTreeTermsReader extends FieldsProducer {
               new FST.Arc[ArrayUtil.oversize(1+ord, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
           System.arraycopy(arcs, 0, next, 0, arcs.length);
           for(int arcOrd=arcs.length;arcOrd<next.length;arcOrd++) {
-            next[arcOrd] = new FST.Arc<BytesRef>();
+            next[arcOrd] = new FST.Arc<>();
           }
           arcs = next;
         }
diff --git lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsWriter.java lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsWriter.java
index 5529001..f363ba0 100644
--- lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsWriter.java
+++ lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsWriter.java
@@ -264,7 +264,7 @@ public class BlockTreeTermsWriter extends FieldsConsumer implements Closeable {
     }
   }
 
-  private final List<FieldMetaData> fields = new ArrayList<FieldMetaData>();
+  private final List<FieldMetaData> fields = new ArrayList<>();
   // private final String segment;
 
   /** Create a new writer.  The number of items (terms or
@@ -462,7 +462,7 @@ public class BlockTreeTermsWriter extends FieldsConsumer implements Closeable {
       }
 
       final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
-      final Builder<BytesRef> indexBuilder = new Builder<BytesRef>(FST.INPUT_TYPE.BYTE1,
+      final Builder<BytesRef> indexBuilder = new Builder<>(FST.INPUT_TYPE.BYTE1,
                                                                    0, 0, true, false, Integer.MAX_VALUE,
                                                                    outputs, null, false,
                                                                    PackedInts.COMPACT, true, 15);
@@ -510,7 +510,7 @@ public class BlockTreeTermsWriter extends FieldsConsumer implements Closeable {
     // Builder?  Takes FST and unions it w/ current
     // FST.
     private void append(Builder<BytesRef> builder, FST<BytesRef> subIndex) throws IOException {
-      final BytesRefFSTEnum<BytesRef> subIndexEnum = new BytesRefFSTEnum<BytesRef>(subIndex);
+      final BytesRefFSTEnum<BytesRef> subIndexEnum = new BytesRefFSTEnum<>(subIndex);
       BytesRefFSTEnum.InputOutput<BytesRef> indexEnt;
       while((indexEnt = subIndexEnum.next()) != null) {
         //if (DEBUG) {
@@ -538,7 +538,7 @@ public class BlockTreeTermsWriter extends FieldsConsumer implements Closeable {
     private final Builder<Object> blockBuilder;
 
     // PendingTerm or PendingBlock:
-    private final List<PendingEntry> pending = new ArrayList<PendingEntry>();
+    private final List<PendingEntry> pending = new ArrayList<>();
 
     // Index into pending of most recently written block
     private int lastBlockIndex = -1;
@@ -593,7 +593,7 @@ public class BlockTreeTermsWriter extends FieldsConsumer implements Closeable {
             // stragglers!  carry count upwards
             node.inputCount = totCount;
           }
-          frontier[idx] = new Builder.UnCompiledNode<Object>(blockBuilder, idx);
+          frontier[idx] = new Builder.UnCompiledNode<>(blockBuilder, idx);
         }
       }
     }
@@ -743,7 +743,7 @@ public class BlockTreeTermsWriter extends FieldsConsumer implements Closeable {
         int curStart = count;
         subCount = 0;
 
-        final List<PendingBlock> floorBlocks = new ArrayList<PendingBlock>();
+        final List<PendingBlock> floorBlocks = new ArrayList<>();
         PendingBlock firstBlock = null;
 
         for(int sub=0;sub<numSubs;sub++) {
@@ -925,7 +925,7 @@ public class BlockTreeTermsWriter extends FieldsConsumer implements Closeable {
         }
         termCount = length;
       } else {
-        subIndices = new ArrayList<FST<BytesRef>>();
+        subIndices = new ArrayList<>();
         termCount = 0;
         for (PendingEntry ent : slice) {
           if (ent.isTerm) {
@@ -1042,7 +1042,7 @@ public class BlockTreeTermsWriter extends FieldsConsumer implements Closeable {
       // This Builder is just used transiently to fragment
       // terms into "good" blocks; we don't save the
       // resulting FST:
-      blockBuilder = new Builder<Object>(FST.INPUT_TYPE.BYTE1,
+      blockBuilder = new Builder<>(FST.INPUT_TYPE.BYTE1,
                                          0, 0, true,
                                          true, Integer.MAX_VALUE,
                                          noOutputs,
diff --git lucene/core/src/java/org/apache/lucene/codecs/Codec.java lucene/core/src/java/org/apache/lucene/codecs/Codec.java
index 72b2e1e..757f343 100644
--- lucene/core/src/java/org/apache/lucene/codecs/Codec.java
+++ lucene/core/src/java/org/apache/lucene/codecs/Codec.java
@@ -39,7 +39,7 @@ import org.apache.lucene.util.NamedSPILoader;
 public abstract class Codec implements NamedSPILoader.NamedSPI {
 
   private static final NamedSPILoader<Codec> loader =
-    new NamedSPILoader<Codec>(Codec.class);
+    new NamedSPILoader<>(Codec.class);
 
   private final String name;
 
diff --git lucene/core/src/java/org/apache/lucene/codecs/DocValuesFormat.java lucene/core/src/java/org/apache/lucene/codecs/DocValuesFormat.java
index 2e47bb1..1b556c0 100644
--- lucene/core/src/java/org/apache/lucene/codecs/DocValuesFormat.java
+++ lucene/core/src/java/org/apache/lucene/codecs/DocValuesFormat.java
@@ -41,7 +41,7 @@ import org.apache.lucene.util.NamedSPILoader;
 public abstract class DocValuesFormat implements NamedSPILoader.NamedSPI {
   
   private static final NamedSPILoader<DocValuesFormat> loader =
-      new NamedSPILoader<DocValuesFormat>(DocValuesFormat.class);
+      new NamedSPILoader<>(DocValuesFormat.class);
   
   /** Unique name that's used to retrieve this format when
    *  reading the index.
diff --git lucene/core/src/java/org/apache/lucene/codecs/PostingsFormat.java lucene/core/src/java/org/apache/lucene/codecs/PostingsFormat.java
index f9b676f..2b145c6 100644
--- lucene/core/src/java/org/apache/lucene/codecs/PostingsFormat.java
+++ lucene/core/src/java/org/apache/lucene/codecs/PostingsFormat.java
@@ -42,7 +42,7 @@ import org.apache.lucene.util.NamedSPILoader;
 public abstract class PostingsFormat implements NamedSPILoader.NamedSPI {
 
   private static final NamedSPILoader<PostingsFormat> loader =
-    new NamedSPILoader<PostingsFormat>(PostingsFormat.class);
+    new NamedSPILoader<>(PostingsFormat.class);
 
   /** Zero-length {@code PostingsFormat} array. */
   public static final PostingsFormat[] EMPTY = new PostingsFormat[0];
diff --git lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java
index b3b4662..9b05c14 100644
--- lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java
+++ lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java
@@ -92,7 +92,7 @@ public final class CompressingTermVectorsWriter extends TermVectorsWriter {
     final int posStart, offStart, payStart;
     DocData(int numFields, int posStart, int offStart, int payStart) {
       this.numFields = numFields;
-      this.fields = new ArrayDeque<FieldData>(numFields);
+      this.fields = new ArrayDeque<>(numFields);
       this.posStart = posStart;
       this.offStart = offStart;
       this.payStart = payStart;
@@ -214,7 +214,7 @@ public final class CompressingTermVectorsWriter extends TermVectorsWriter {
     this.chunkSize = chunkSize;
 
     numDocs = 0;
-    pendingDocs = new ArrayDeque<DocData>();
+    pendingDocs = new ArrayDeque<>();
     termSuffixes = new GrowableByteArrayDataOutput(ArrayUtil.oversize(chunkSize, 1));
     payloadBytes = new GrowableByteArrayDataOutput(ArrayUtil.oversize(1, 1));
     lastTerm = new BytesRef(ArrayUtil.oversize(30, 1));
@@ -393,7 +393,7 @@ public final class CompressingTermVectorsWriter extends TermVectorsWriter {
 
   /** Returns a sorted array containing unique field numbers */
   private int[] flushFieldNums() throws IOException {
-    SortedSet<Integer> fieldNums = new TreeSet<Integer>();
+    SortedSet<Integer> fieldNums = new TreeSet<>();
     for (DocData dd : pendingDocs) {
       for (FieldData fd : dd.fields) {
         fieldNums.add(fd.fieldNum);
diff --git lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java
index 9913da9..a5b5452 100644
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java
@@ -57,11 +57,11 @@ final class Lucene40DocValuesReader extends DocValuesProducer {
 
   // ram instances we have already loaded
   private final Map<Integer,NumericDocValues> numericInstances =
-      new HashMap<Integer,NumericDocValues>();
+      new HashMap<>();
   private final Map<Integer,BinaryDocValues> binaryInstances =
-      new HashMap<Integer,BinaryDocValues>();
+      new HashMap<>();
   private final Map<Integer,SortedDocValues> sortedInstances =
-      new HashMap<Integer,SortedDocValues>();
+      new HashMap<>();
 
   private final AtomicLong ramBytesUsed;
 
diff --git lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java
index 8de01a9..fc830ce 100644
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java
@@ -221,7 +221,7 @@ public class Lucene40TermVectorsReader extends TermVectorsReader implements Clos
   private class TVFields extends Fields {
     private final int[] fieldNumbers;
     private final long[] fieldFPs;
-    private final Map<Integer,Integer> fieldNumberToIndex = new HashMap<Integer,Integer>();
+    private final Map<Integer,Integer> fieldNumberToIndex = new HashMap<>();
 
     public TVFields(int docID) throws IOException {
       seekTvx(docID);
diff --git lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java
index ff55569..2a28eeb 100644
--- lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java
@@ -67,11 +67,11 @@ class Lucene42DocValuesProducer extends DocValuesProducer {
   
   // ram instances we have already loaded
   private final Map<Integer,NumericDocValues> numericInstances = 
-      new HashMap<Integer,NumericDocValues>();
+      new HashMap<>();
   private final Map<Integer,BinaryDocValues> binaryInstances =
-      new HashMap<Integer,BinaryDocValues>();
+      new HashMap<>();
   private final Map<Integer,FST<Long>> fstInstances =
-      new HashMap<Integer,FST<Long>>();
+      new HashMap<>();
   
   private final int maxDoc;
   private final AtomicLong ramBytesUsed;
@@ -103,9 +103,9 @@ class Lucene42DocValuesProducer extends DocValuesProducer {
       version = CodecUtil.checkHeader(in, metaCodec, 
                                       VERSION_START,
                                       VERSION_CURRENT);
-      numerics = new HashMap<Integer,NumericEntry>();
-      binaries = new HashMap<Integer,BinaryEntry>();
-      fsts = new HashMap<Integer,FSTEntry>();
+      numerics = new HashMap<>();
+      binaries = new HashMap<>();
+      fsts = new HashMap<>();
       readFields(in, state.fieldInfos);
 
       success = true;
@@ -297,7 +297,7 @@ class Lucene42DocValuesProducer extends DocValuesProducer {
       instance = fstInstances.get(field.number);
       if (instance == null) {
         data.seek(entry.offset);
-        instance = new FST<Long>(data, PositiveIntOutputs.getSingleton());
+        instance = new FST<>(data, PositiveIntOutputs.getSingleton());
         ramBytesUsed.addAndGet(instance.sizeInBytes());
         fstInstances.put(field.number, instance);
       }
@@ -307,10 +307,10 @@ class Lucene42DocValuesProducer extends DocValuesProducer {
     
     // per-thread resources
     final BytesReader in = fst.getBytesReader();
-    final Arc<Long> firstArc = new Arc<Long>();
-    final Arc<Long> scratchArc = new Arc<Long>();
+    final Arc<Long> firstArc = new Arc<>();
+    final Arc<Long> scratchArc = new Arc<>();
     final IntsRef scratchInts = new IntsRef();
-    final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<Long>(fst); 
+    final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<>(fst);
     
     return new SortedDocValues() {
       @Override
@@ -372,7 +372,7 @@ class Lucene42DocValuesProducer extends DocValuesProducer {
       instance = fstInstances.get(field.number);
       if (instance == null) {
         data.seek(entry.offset);
-        instance = new FST<Long>(data, PositiveIntOutputs.getSingleton());
+        instance = new FST<>(data, PositiveIntOutputs.getSingleton());
         ramBytesUsed.addAndGet(instance.sizeInBytes());
         fstInstances.put(field.number, instance);
       }
@@ -382,10 +382,10 @@ class Lucene42DocValuesProducer extends DocValuesProducer {
     
     // per-thread resources
     final BytesReader in = fst.getBytesReader();
-    final Arc<Long> firstArc = new Arc<Long>();
-    final Arc<Long> scratchArc = new Arc<Long>();
+    final Arc<Long> firstArc = new Arc<>();
+    final Arc<Long> scratchArc = new Arc<>();
     final IntsRef scratchInts = new IntsRef();
-    final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<Long>(fst); 
+    final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<>(fst);
     final BytesRef ref = new BytesRef();
     final ByteArrayDataInput input = new ByteArrayDataInput();
     return new SortedSetDocValues() {
@@ -493,14 +493,14 @@ class Lucene42DocValuesProducer extends DocValuesProducer {
     // maybe we should add a FSTEnum that supports this operation?
     final FST<Long> fst;
     final FST.BytesReader bytesReader;
-    final Arc<Long> firstArc = new Arc<Long>();
-    final Arc<Long> scratchArc = new Arc<Long>();
+    final Arc<Long> firstArc = new Arc<>();
+    final Arc<Long> scratchArc = new Arc<>();
     final IntsRef scratchInts = new IntsRef();
     final BytesRef scratchBytes = new BytesRef();
     
     FSTTermsEnum(FST<Long> fst) {
       this.fst = fst;
-      in = new BytesRefFSTEnum<Long>(fst);
+      in = new BytesRefFSTEnum<>(fst);
       bytesReader = fst.getBytesReader();
     }
 
diff --git lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsConsumer.java lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsConsumer.java
index 797dd80..4c87bce 100644
--- lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsConsumer.java
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsConsumer.java
@@ -131,7 +131,7 @@ class Lucene42NormsConsumer extends DocValuesConsumer {
       } else {
         meta.writeByte(TABLE_COMPRESSED); // table-compressed
         Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
-        final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();
+        final HashMap<Long,Integer> encode = new HashMap<>();
         data.writeVInt(decode.length);
         for (int i = 0; i < decode.length; i++) {
           data.writeLong(decode[i]);
diff --git lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesConsumer.java lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesConsumer.java
index c6652cf..aa6cdb8 100644
--- lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesConsumer.java
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesConsumer.java
@@ -189,7 +189,7 @@ public class Lucene45DocValuesConsumer extends DocValuesConsumer implements Clos
         break;
       case TABLE_COMPRESSED:
         final Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
-        final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();
+        final HashMap<Long,Integer> encode = new HashMap<>();
         meta.writeVInt(decode.length);
         for (int i = 0; i < decode.length; i++) {
           meta.writeLong(decode[i]);
diff --git lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesProducer.java lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesProducer.java
index 25c3842..d1acdaf 100644
--- lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesProducer.java
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesProducer.java
@@ -73,8 +73,8 @@ public class Lucene45DocValuesProducer extends DocValuesProducer implements Clos
   private final int version;
 
   // memory-resident structures
-  private final Map<Integer,MonotonicBlockPackedReader> addressInstances = new HashMap<Integer,MonotonicBlockPackedReader>();
-  private final Map<Integer,MonotonicBlockPackedReader> ordIndexInstances = new HashMap<Integer,MonotonicBlockPackedReader>();
+  private final Map<Integer,MonotonicBlockPackedReader> addressInstances = new HashMap<>();
+  private final Map<Integer,MonotonicBlockPackedReader> ordIndexInstances = new HashMap<>();
   
   /** expert: instantiates a new reader */
   protected Lucene45DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
@@ -87,11 +87,11 @@ public class Lucene45DocValuesProducer extends DocValuesProducer implements Clos
       version = CodecUtil.checkHeader(in, metaCodec, 
                                       Lucene45DocValuesFormat.VERSION_START,
                                       Lucene45DocValuesFormat.VERSION_CURRENT);
-      numerics = new HashMap<Integer,NumericEntry>();
-      ords = new HashMap<Integer,NumericEntry>();
-      ordIndexes = new HashMap<Integer,NumericEntry>();
-      binaries = new HashMap<Integer,BinaryEntry>();
-      sortedSets = new HashMap<Integer,SortedSetEntry>();
+      numerics = new HashMap<>();
+      ords = new HashMap<>();
+      ordIndexes = new HashMap<>();
+      binaries = new HashMap<>();
+      sortedSets = new HashMap<>();
       readFields(in, state.fieldInfos);
 
       success = true;
diff --git lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldDocValuesFormat.java lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldDocValuesFormat.java
index 2021bce..cf960ac 100644
--- lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldDocValuesFormat.java
+++ lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldDocValuesFormat.java
@@ -92,8 +92,8 @@ public abstract class PerFieldDocValuesFormat extends DocValuesFormat {
     
   private class FieldsWriter extends DocValuesConsumer {
 
-    private final Map<DocValuesFormat,ConsumerAndSuffix> formats = new HashMap<DocValuesFormat,ConsumerAndSuffix>();
-    private final Map<String,Integer> suffixes = new HashMap<String,Integer>();
+    private final Map<DocValuesFormat,ConsumerAndSuffix> formats = new HashMap<>();
+    private final Map<String,Integer> suffixes = new HashMap<>();
     
     private final SegmentWriteState segmentWriteState;
     
@@ -209,8 +209,8 @@ public abstract class PerFieldDocValuesFormat extends DocValuesFormat {
 
   private class FieldsReader extends DocValuesProducer {
 
-    private final Map<String,DocValuesProducer> fields = new TreeMap<String,DocValuesProducer>();
-    private final Map<String,DocValuesProducer> formats = new HashMap<String,DocValuesProducer>();
+    private final Map<String,DocValuesProducer> fields = new TreeMap<>();
+    private final Map<String,DocValuesProducer> formats = new HashMap<>();
 
     public FieldsReader(final SegmentReadState readState) throws IOException {
 
@@ -245,7 +245,7 @@ public abstract class PerFieldDocValuesFormat extends DocValuesFormat {
 
     private FieldsReader(FieldsReader other) {
 
-      Map<DocValuesProducer,DocValuesProducer> oldToNew = new IdentityHashMap<DocValuesProducer,DocValuesProducer>();
+      Map<DocValuesProducer,DocValuesProducer> oldToNew = new IdentityHashMap<>();
       // First clone all formats
       for(Map.Entry<String,DocValuesProducer> ent : other.formats.entrySet()) {
         DocValuesProducer values = ent.getValue();
diff --git lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java
index a472af7..0091a9e 100644
--- lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java
+++ lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java
@@ -75,7 +75,7 @@ public abstract class PerFieldPostingsFormat extends PostingsFormat {
 
   /** Group of fields written by one PostingsFormat */
   static class FieldsGroup {
-    final Set<String> fields = new TreeSet<String>();
+    final Set<String> fields = new TreeSet<>();
     int suffix;
 
     /** Custom SegmentWriteState for this group of fields,
@@ -111,10 +111,10 @@ public abstract class PerFieldPostingsFormat extends PostingsFormat {
 
       // Maps a PostingsFormat instance to the suffix it
       // should use
-      Map<PostingsFormat,FieldsGroup> formatToGroups = new HashMap<PostingsFormat,FieldsGroup>();
+      Map<PostingsFormat,FieldsGroup> formatToGroups = new HashMap<>();
 
       // Holds last suffix of each PostingFormat name
-      Map<String,Integer> suffixes = new HashMap<String,Integer>();
+      Map<String,Integer> suffixes = new HashMap<>();
 
       // First pass: assign field -> PostingsFormat
       for(String field : fields) {
@@ -182,8 +182,8 @@ public abstract class PerFieldPostingsFormat extends PostingsFormat {
 
   private class FieldsReader extends FieldsProducer {
 
-    private final Map<String,FieldsProducer> fields = new TreeMap<String,FieldsProducer>();
-    private final Map<String,FieldsProducer> formats = new HashMap<String,FieldsProducer>();
+    private final Map<String,FieldsProducer> fields = new TreeMap<>();
+    private final Map<String,FieldsProducer> formats = new HashMap<>();
 
     public FieldsReader(final SegmentReadState readState) throws IOException {
 
diff --git lucene/core/src/java/org/apache/lucene/document/Document.java lucene/core/src/java/org/apache/lucene/document/Document.java
index 1ce27ac..d63fd09 100644
--- lucene/core/src/java/org/apache/lucene/document/Document.java
+++ lucene/core/src/java/org/apache/lucene/document/Document.java
@@ -44,7 +44,7 @@ import org.apache.lucene.util.FilterIterator;
 
 public final class Document implements IndexDocument {
 
-  private final List<Field> fields = new ArrayList<Field>();
+  private final List<Field> fields = new ArrayList<>();
 
   /** Constructs a new document with no fields. */
   public Document() {}
@@ -140,7 +140,7 @@ public final class Document implements IndexDocument {
   * @return a <code>BytesRef[]</code> of binary field values
   */
   public final BytesRef[] getBinaryValues(String name) {
-    final List<BytesRef> result = new ArrayList<BytesRef>();
+    final List<BytesRef> result = new ArrayList<>();
 
     for (Iterator<StorableField> it = storedFieldsIterator(); it.hasNext(); ) {
       StorableField field = it.next();
@@ -199,7 +199,7 @@ public final class Document implements IndexDocument {
    * @return a <code>Field[]</code> array
    */
   public Field[] getFields(String name) {
-    List<Field> result = new ArrayList<Field>();
+    List<Field> result = new ArrayList<>();
     for (Field field : fields) {
       if (field.name().equals(name)) {
         result.add(field);
@@ -234,7 +234,7 @@ public final class Document implements IndexDocument {
    * @return a <code>String[]</code> of field values
    */
   public final String[] getValues(String name) {
-    List<String> result = new ArrayList<String>();
+    List<String> result = new ArrayList<>();
 
     for (Iterator<StorableField> it = storedFieldsIterator(); it.hasNext(); ) {
       StorableField field = it.next();
diff --git lucene/core/src/java/org/apache/lucene/document/DocumentStoredFieldVisitor.java lucene/core/src/java/org/apache/lucene/document/DocumentStoredFieldVisitor.java
index b9ed951..c01b5ce 100644
--- lucene/core/src/java/org/apache/lucene/document/DocumentStoredFieldVisitor.java
+++ lucene/core/src/java/org/apache/lucene/document/DocumentStoredFieldVisitor.java
@@ -49,7 +49,7 @@ public class DocumentStoredFieldVisitor extends StoredFieldVisitor {
 
   /** Load only fields named in the provided fields. */
   public DocumentStoredFieldVisitor(String... fields) {
-    fieldsToAdd = new HashSet<String>(fields.length);
+    fieldsToAdd = new HashSet<>(fields.length);
     for(String field : fields) {
       fieldsToAdd.add(field);
     }
diff --git lucene/core/src/java/org/apache/lucene/index/BufferedUpdates.java lucene/core/src/java/org/apache/lucene/index/BufferedUpdates.java
index 8600a91..245ff8e 100644
--- lucene/core/src/java/org/apache/lucene/index/BufferedUpdates.java
+++ lucene/core/src/java/org/apache/lucene/index/BufferedUpdates.java
@@ -95,9 +95,9 @@ class BufferedUpdates {
   
   final AtomicInteger numTermDeletes = new AtomicInteger();
   final AtomicInteger numNumericUpdates = new AtomicInteger();
-  final Map<Term,Integer> terms = new HashMap<Term,Integer>();
-  final Map<Query,Integer> queries = new HashMap<Query,Integer>();
-  final List<Integer> docIDs = new ArrayList<Integer>();
+  final Map<Term,Integer> terms = new HashMap<>();
+  final Map<Query,Integer> queries = new HashMap<>();
+  final List<Integer> docIDs = new ArrayList<>();
 
   // Map<dvField,Map<updateTerm,NumericUpdate>>
   // For each field we keep an ordered list of NumericUpdates, key'd by the
@@ -106,7 +106,7 @@ class BufferedUpdates {
   // one that came in wins), and helps us detect faster if the same Term is
   // used to update the same field multiple times (so we later traverse it
   // only once).
-  final Map<String,LinkedHashMap<Term,NumericUpdate>> numericUpdates = new HashMap<String,LinkedHashMap<Term,NumericUpdate>>();
+  final Map<String,LinkedHashMap<Term,NumericUpdate>> numericUpdates = new HashMap<>();
 
   public static final Integer MAX_INT = Integer.valueOf(Integer.MAX_VALUE);
 
@@ -187,7 +187,7 @@ class BufferedUpdates {
   public void addNumericUpdate(NumericUpdate update, int docIDUpto) {
     LinkedHashMap<Term,NumericUpdate> fieldUpdates = numericUpdates.get(update.field);
     if (fieldUpdates == null) {
-      fieldUpdates = new LinkedHashMap<Term,NumericUpdate>();
+      fieldUpdates = new LinkedHashMap<>();
       numericUpdates.put(update.field, fieldUpdates);
       bytesUsed.addAndGet(BYTES_PER_NUMERIC_FIELD_ENTRY);
     }
diff --git lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream.java lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream.java
index 009b982..b20c5a9 100644
--- lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream.java
+++ lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream.java
@@ -54,7 +54,7 @@ import org.apache.lucene.util.InfoStream;
 class BufferedUpdatesStream {
 
   // TODO: maybe linked list?
-  private final List<FrozenBufferedUpdates> updates = new ArrayList<FrozenBufferedUpdates>();
+  private final List<FrozenBufferedUpdates> updates = new ArrayList<>();
 
   // Starts at 1 so that SegmentInfos that have never had
   // deletes applied (whose bufferedDelGen defaults to 0)
@@ -167,7 +167,7 @@ class BufferedUpdatesStream {
 
     final long gen = nextGen++;
 
-    List<SegmentCommitInfo> infos2 = new ArrayList<SegmentCommitInfo>();
+    List<SegmentCommitInfo> infos2 = new ArrayList<>();
     infos2.addAll(infos);
     Collections.sort(infos2, sortSegInfoByDelGen);
 
@@ -240,7 +240,7 @@ class BufferedUpdatesStream {
 
         if (segAllDeletes) {
           if (allDeleted == null) {
-            allDeleted = new ArrayList<SegmentCommitInfo>();
+            allDeleted = new ArrayList<>();
           }
           allDeleted.add(info);
         }
@@ -290,7 +290,7 @@ class BufferedUpdatesStream {
 
           if (segAllDeletes) {
             if (allDeleted == null) {
-              allDeleted = new ArrayList<SegmentCommitInfo>();
+              allDeleted = new ArrayList<>();
             }
             allDeleted.add(info);
           }
diff --git lucene/core/src/java/org/apache/lucene/index/CheckIndex.java lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
index 4b915f1..fc3951a 100644
--- lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
+++ lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
@@ -92,13 +92,13 @@ public class CheckIndex {
 
     /** Empty unless you passed specific segments list to check as optional 3rd argument.
      *  @see CheckIndex#checkIndex(List) */
-    public List<String> segmentsChecked = new ArrayList<String>();
+    public List<String> segmentsChecked = new ArrayList<>();
   
     /** True if the index was created with a newer version of Lucene than the CheckIndex tool. */
     public boolean toolOutOfDate;
 
     /** List of {@link SegmentInfoStatus} instances, detailing status of each segment. */
-    public List<SegmentInfoStatus> segmentInfos = new ArrayList<SegmentInfoStatus>();
+    public List<SegmentInfoStatus> segmentInfos = new ArrayList<>();
   
     /** Directory index is in. */
     public Directory dir;
@@ -1069,7 +1069,7 @@ public class CheckIndex {
           final BlockTreeTermsReader.Stats stats = ((BlockTreeTermsReader.FieldReader) fieldTerms).computeStats();
           assert stats != null;
           if (status.blockTreeStats == null) {
-            status.blockTreeStats = new HashMap<String,BlockTreeTermsReader.Stats>();
+            status.blockTreeStats = new HashMap<>();
           }
           status.blockTreeStats.put(field, stats);
         }
@@ -1831,7 +1831,7 @@ public class CheckIndex {
     boolean doFix = false;
     boolean doCrossCheckTermVectors = false;
     boolean verbose = false;
-    List<String> onlySegments = new ArrayList<String>();
+    List<String> onlySegments = new ArrayList<>();
     String indexPath = null;
     String dirImpl = null;
     int i = 0;
diff --git lucene/core/src/java/org/apache/lucene/index/CoalescedUpdates.java lucene/core/src/java/org/apache/lucene/index/CoalescedUpdates.java
index 687386c..8a0bbd3 100644
--- lucene/core/src/java/org/apache/lucene/index/CoalescedUpdates.java
+++ lucene/core/src/java/org/apache/lucene/index/CoalescedUpdates.java
@@ -28,9 +28,9 @@ import org.apache.lucene.index.BufferedUpdatesStream.QueryAndLimit;
 import org.apache.lucene.util.MergedIterator;
 
 class CoalescedUpdates {
-  final Map<Query,Integer> queries = new HashMap<Query,Integer>();
-  final List<Iterable<Term>> iterables = new ArrayList<Iterable<Term>>();
-  final List<NumericUpdate> numericDVUpdates = new ArrayList<NumericUpdate>();
+  final Map<Query,Integer> queries = new HashMap<>();
+  final List<Iterable<Term>> iterables = new ArrayList<>();
+  final List<NumericUpdate> numericDVUpdates = new ArrayList<>();
   
   @Override
   public String toString() {
@@ -62,7 +62,7 @@ class CoalescedUpdates {
        for (int i = 0; i < iterables.size(); i++) {
          subs[i] = iterables.get(i).iterator();
        }
-       return new MergedIterator<Term>(subs);
+       return new MergedIterator<>(subs);
      }
    };
   }
diff --git lucene/core/src/java/org/apache/lucene/index/CompositeReaderContext.java lucene/core/src/java/org/apache/lucene/index/CompositeReaderContext.java
index 0c51872..d892e18 100644
--- lucene/core/src/java/org/apache/lucene/index/CompositeReaderContext.java
+++ lucene/core/src/java/org/apache/lucene/index/CompositeReaderContext.java
@@ -80,7 +80,7 @@ public final class CompositeReaderContext extends IndexReaderContext {
   
   private static final class Builder {
     private final CompositeReader reader;
-    private final List<AtomicReaderContext> leaves = new ArrayList<AtomicReaderContext>();
+    private final List<AtomicReaderContext> leaves = new ArrayList<>();
     private int leafDocBase = 0;
     
     public Builder(CompositeReader reader) {
diff --git lucene/core/src/java/org/apache/lucene/index/ConcurrentMergeScheduler.java lucene/core/src/java/org/apache/lucene/index/ConcurrentMergeScheduler.java
index 7c015aa..7e66625 100644
--- lucene/core/src/java/org/apache/lucene/index/ConcurrentMergeScheduler.java
+++ lucene/core/src/java/org/apache/lucene/index/ConcurrentMergeScheduler.java
@@ -47,7 +47,7 @@ public class ConcurrentMergeScheduler extends MergeScheduler {
   private int mergeThreadPriority = -1;
 
   /** List of currently active {@link MergeThread}s. */
-  protected List<MergeThread> mergeThreads = new ArrayList<MergeThread>();
+  protected List<MergeThread> mergeThreads = new ArrayList<>();
   
   /** 
    * Default {@code maxThreadCount}.
@@ -171,7 +171,7 @@ public class ConcurrentMergeScheduler extends MergeScheduler {
 
     // Only look at threads that are alive & not in the
     // process of stopping (ie have an active merge):
-    final List<MergeThread> activeMerges = new ArrayList<MergeThread>();
+    final List<MergeThread> activeMerges = new ArrayList<>();
 
     int threadIdx = 0;
     while (threadIdx < mergeThreads.size()) {
@@ -571,7 +571,7 @@ public class ConcurrentMergeScheduler extends MergeScheduler {
     ConcurrentMergeScheduler clone = (ConcurrentMergeScheduler) super.clone();
     clone.writer = null;
     clone.dir = null;
-    clone.mergeThreads = new ArrayList<MergeThread>();
+    clone.mergeThreads = new ArrayList<>();
     return clone;
   }
 }
diff --git lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java
index e2a6121..84c721d 100644
--- lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java
+++ lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java
@@ -224,7 +224,7 @@ public abstract class DirectoryReader extends BaseCompositeReader<AtomicReader>
   public static List<IndexCommit> listCommits(Directory dir) throws IOException {
     final String[] files = dir.listAll();
 
-    List<IndexCommit> commits = new ArrayList<IndexCommit>();
+    List<IndexCommit> commits = new ArrayList<>();
 
     SegmentInfos latest = new SegmentInfos();
     latest.read(dir);
diff --git lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor.java lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor.java
index 23c60a0..0ecfcdb 100644
--- lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor.java
+++ lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor.java
@@ -69,7 +69,7 @@ final class DocFieldProcessor extends DocConsumer {
   @Override
   public void flush(SegmentWriteState state) throws IOException {
 
-    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();
+    Map<String,DocFieldConsumerPerField> childFields = new HashMap<>();
     Collection<DocFieldConsumerPerField> fields = fields();
     for (DocFieldConsumerPerField f : fields) {
       childFields.put(f.getFieldInfo().name, f);
@@ -132,7 +132,7 @@ final class DocFieldProcessor extends DocConsumer {
   }
 
   public Collection<DocFieldConsumerPerField> fields() {
-    Collection<DocFieldConsumerPerField> fields = new HashSet<DocFieldConsumerPerField>();
+    Collection<DocFieldConsumerPerField> fields = new HashSet<>();
     for(int i=0;i<fieldHash.length;i++) {
       DocFieldProcessorPerField field = fieldHash[i];
       while(field != null) {
diff --git lucene/core/src/java/org/apache/lucene/index/DocInverter.java lucene/core/src/java/org/apache/lucene/index/DocInverter.java
index 66eda35..387b863 100644
--- lucene/core/src/java/org/apache/lucene/index/DocInverter.java
+++ lucene/core/src/java/org/apache/lucene/index/DocInverter.java
@@ -41,8 +41,8 @@ final class DocInverter extends DocFieldConsumer {
   @Override
   void flush(Map<String, DocFieldConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {
 
-    Map<String, InvertedDocConsumerPerField> childFieldsToFlush = new HashMap<String, InvertedDocConsumerPerField>();
-    Map<String, InvertedDocEndConsumerPerField> endChildFieldsToFlush = new HashMap<String, InvertedDocEndConsumerPerField>();
+    Map<String, InvertedDocConsumerPerField> childFieldsToFlush = new HashMap<>();
+    Map<String, InvertedDocEndConsumerPerField> endChildFieldsToFlush = new HashMap<>();
 
     for (Map.Entry<String, DocFieldConsumerPerField> fieldToFlush : fieldsToFlush.entrySet()) {
       DocInverterPerField perField = (DocInverterPerField) fieldToFlush.getValue();
diff --git lucene/core/src/java/org/apache/lucene/index/DocTermOrds.java lucene/core/src/java/org/apache/lucene/index/DocTermOrds.java
index 42f1b21..8358312 100644
--- lucene/core/src/java/org/apache/lucene/index/DocTermOrds.java
+++ lucene/core/src/java/org/apache/lucene/index/DocTermOrds.java
@@ -342,7 +342,7 @@ public class DocTermOrds {
         } catch (UnsupportedOperationException uoe) {
           // Reader cannot provide ord support, so we wrap
           // our own support by creating our own terms index:
-          indexedTerms = new ArrayList<BytesRef>();
+          indexedTerms = new ArrayList<>();
           indexedTermsBytes = new PagedBytes(15);
           //System.out.println("NO ORDS");
         }
diff --git lucene/core/src/java/org/apache/lucene/index/DocValuesProcessor.java lucene/core/src/java/org/apache/lucene/index/DocValuesProcessor.java
index cb1b301..29e5570 100644
--- lucene/core/src/java/org/apache/lucene/index/DocValuesProcessor.java
+++ lucene/core/src/java/org/apache/lucene/index/DocValuesProcessor.java
@@ -33,7 +33,7 @@ final class DocValuesProcessor extends StoredFieldsConsumer {
   // TODO: somewhat wasteful we also keep a map here; would
   // be more efficient if we could "reuse" the map/hash
   // lookup DocFieldProcessor already did "above"
-  private final Map<String,DocValuesWriter> writers = new HashMap<String,DocValuesWriter>();
+  private final Map<String,DocValuesWriter> writers = new HashMap<>();
   private final Counter bytesUsed;
 
   public DocValuesProcessor(Counter bytesUsed) {
diff --git lucene/core/src/java/org/apache/lucene/index/DocumentsWriter.java lucene/core/src/java/org/apache/lucene/index/DocumentsWriter.java
index 2c40549..4ad6748 100644
--- lucene/core/src/java/org/apache/lucene/index/DocumentsWriter.java
+++ lucene/core/src/java/org/apache/lucene/index/DocumentsWriter.java
@@ -135,7 +135,7 @@ final class DocumentsWriter {
     this.perThreadPool = config.getIndexerThreadPool();
     flushPolicy = config.getFlushPolicy();
     this.writer = writer;
-    this.events = new ConcurrentLinkedQueue<Event>();
+    this.events = new ConcurrentLinkedQueue<>();
     flushControl = new DocumentsWriterFlushControl(this, config, writer.bufferedUpdatesStream);
   }
   
@@ -207,7 +207,7 @@ final class DocumentsWriter {
   synchronized void abort(IndexWriter writer) {
     assert !Thread.holdsLock(writer) : "IndexWriter lock should never be hold when aborting";
     boolean success = false;
-    final Set<String> newFilesSet = new HashSet<String>();
+    final Set<String> newFilesSet = new HashSet<>();
     try {
       deleteQueue.clear();
       if (infoStream.isEnabled("DW")) {
@@ -243,7 +243,7 @@ final class DocumentsWriter {
     try {
       deleteQueue.clear();
       final int limit = perThreadPool.getMaxThreadStates();
-      final Set<String> newFilesSet = new HashSet<String>();
+      final Set<String> newFilesSet = new HashSet<>();
       for (int i = 0; i < limit; i++) {
         final ThreadState perThread = perThreadPool.getThreadState(i);
         perThread.lock();
diff --git lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushControl.java lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushControl.java
index bd03c4e..d6ec30d 100644
--- lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushControl.java
+++ lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushControl.java
@@ -51,10 +51,10 @@ final class DocumentsWriterFlushControl  {
   private int numDocsSinceStalled = 0; // only with assert
   final AtomicBoolean flushDeletes = new AtomicBoolean(false);
   private boolean fullFlush = false;
-  private final Queue<DocumentsWriterPerThread> flushQueue = new LinkedList<DocumentsWriterPerThread>();
+  private final Queue<DocumentsWriterPerThread> flushQueue = new LinkedList<>();
   // only for safety reasons if a DWPT is close to the RAM limit
-  private final Queue<BlockedFlush> blockedFlushes = new LinkedList<BlockedFlush>();
-  private final IdentityHashMap<DocumentsWriterPerThread, Long> flushingWriters = new IdentityHashMap<DocumentsWriterPerThread, Long>();
+  private final Queue<BlockedFlush> blockedFlushes = new LinkedList<>();
+  private final IdentityHashMap<DocumentsWriterPerThread, Long> flushingWriters = new IdentityHashMap<>();
 
 
   double maxConfiguredRamBuffer = 0;
@@ -531,7 +531,7 @@ final class DocumentsWriterFlushControl  {
     return true;
   }
 
-  private final List<DocumentsWriterPerThread> fullFlushBuffer = new ArrayList<DocumentsWriterPerThread>();
+  private final List<DocumentsWriterPerThread> fullFlushBuffer = new ArrayList<>();
 
   void addFlushableState(ThreadState perThread) {
     if (infoStream.isEnabled("DWFC")) {
diff --git lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue.java lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue.java
index 110ff2a..898f2cd 100644
--- lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue.java
+++ lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue.java
@@ -28,7 +28,7 @@ import org.apache.lucene.index.DocumentsWriterPerThread.FlushedSegment;
  * @lucene.internal 
  */
 class DocumentsWriterFlushQueue {
-  private final Queue<FlushTicket> queue = new LinkedList<FlushTicket>();
+  private final Queue<FlushTicket> queue = new LinkedList<>();
   // we track tickets separately since count must be present even before the ticket is
   // constructed ie. queue.size would not reflect it.
   private final AtomicInteger ticketCount = new AtomicInteger();
diff --git lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
index a50bd1b..9a10843 100644
--- lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
+++ lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
@@ -464,7 +464,7 @@ class DocumentsWriterPerThread {
     try {
       consumer.flush(flushState);
       pendingUpdates.terms.clear();
-      segmentInfo.setFiles(new HashSet<String>(directory.getCreatedFiles()));
+      segmentInfo.setFiles(new HashSet<>(directory.getCreatedFiles()));
 
       final SegmentCommitInfo segmentInfoPerCommit = new SegmentCommitInfo(segmentInfo, 0, -1L, -1L);
       if (infoStream.isEnabled("DWPT")) {
@@ -510,7 +510,7 @@ class DocumentsWriterPerThread {
     }
   }
   
-  private final Set<String> filesToDelete = new HashSet<String>(); 
+  private final Set<String> filesToDelete = new HashSet<>();
   
   public Set<String> pendingFilesToDelete() {
     return filesToDelete;
diff --git lucene/core/src/java/org/apache/lucene/index/DocumentsWriterStallControl.java lucene/core/src/java/org/apache/lucene/index/DocumentsWriterStallControl.java
index cfdb218..9cfe6a5 100644
--- lucene/core/src/java/org/apache/lucene/index/DocumentsWriterStallControl.java
+++ lucene/core/src/java/org/apache/lucene/index/DocumentsWriterStallControl.java
@@ -42,7 +42,7 @@ final class DocumentsWriterStallControl {
   private volatile boolean stalled;
   private int numWaiting; // only with assert
   private boolean wasStalled; // only with assert
-  private final Map<Thread, Boolean> waiting = new IdentityHashMap<Thread, Boolean>(); // only with assert
+  private final Map<Thread, Boolean> waiting = new IdentityHashMap<>(); // only with assert
   
   /**
    * Update the stalled flag status. This method will set the stalled flag to
diff --git lucene/core/src/java/org/apache/lucene/index/FieldInfo.java lucene/core/src/java/org/apache/lucene/index/FieldInfo.java
index 8c156bc..00b5b8b 100644
--- lucene/core/src/java/org/apache/lucene/index/FieldInfo.java
+++ lucene/core/src/java/org/apache/lucene/index/FieldInfo.java
@@ -324,7 +324,7 @@ public final class FieldInfo {
    */
   public String putAttribute(String key, String value) {
     if (attributes == null) {
-      attributes = new HashMap<String,String>();
+      attributes = new HashMap<>();
     }
     return attributes.put(key, value);
   }
diff --git lucene/core/src/java/org/apache/lucene/index/FieldInfos.java lucene/core/src/java/org/apache/lucene/index/FieldInfos.java
index bcee7f2..1159710 100644
--- lucene/core/src/java/org/apache/lucene/index/FieldInfos.java
+++ lucene/core/src/java/org/apache/lucene/index/FieldInfos.java
@@ -41,8 +41,8 @@ public class FieldInfos implements Iterable<FieldInfo> {
   private final boolean hasNorms;
   private final boolean hasDocValues;
   
-  private final SortedMap<Integer,FieldInfo> byNumber = new TreeMap<Integer,FieldInfo>();
-  private final HashMap<String,FieldInfo> byName = new HashMap<String,FieldInfo>();
+  private final SortedMap<Integer,FieldInfo> byNumber = new TreeMap<>();
+  private final HashMap<String,FieldInfo> byName = new HashMap<>();
   private final Collection<FieldInfo> values; // for an unmodifiable iterator
   
   /**
@@ -174,9 +174,9 @@ public class FieldInfos implements Iterable<FieldInfo> {
     private int lowestUnassignedFieldNumber = -1;
     
     FieldNumbers() {
-      this.nameToNumber = new HashMap<String, Integer>();
-      this.numberToName = new HashMap<Integer, String>();
-      this.docValuesType = new HashMap<String,DocValuesType>();
+      this.nameToNumber = new HashMap<>();
+      this.numberToName = new HashMap<>();
+      this.docValuesType = new HashMap<>();
     }
     
     /**
@@ -250,7 +250,7 @@ public class FieldInfos implements Iterable<FieldInfo> {
   }
   
   static final class Builder {
-    private final HashMap<String,FieldInfo> byName = new HashMap<String,FieldInfo>();
+    private final HashMap<String,FieldInfo> byName = new HashMap<>();
     final FieldNumbers globalFieldNumbers;
 
     Builder() {
diff --git lucene/core/src/java/org/apache/lucene/index/FreqProxFields.java lucene/core/src/java/org/apache/lucene/index/FreqProxFields.java
index 605dbfd..84d8c2e 100644
--- lucene/core/src/java/org/apache/lucene/index/FreqProxFields.java
+++ lucene/core/src/java/org/apache/lucene/index/FreqProxFields.java
@@ -35,7 +35,7 @@ import org.apache.lucene.util.BytesRef;
  *  PostingsFormat. */
 
 class FreqProxFields extends Fields {
-  final Map<String,FreqProxTermsWriterPerField> fields = new LinkedHashMap<String,FreqProxTermsWriterPerField>();
+  final Map<String,FreqProxTermsWriterPerField> fields = new LinkedHashMap<>();
 
   public FreqProxFields(List<FreqProxTermsWriterPerField> fieldList) {
     // NOTE: fields are already sorted by field name
diff --git lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter.java lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter.java
index 1df0b10..82d7dc8 100644
--- lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter.java
+++ lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter.java
@@ -36,7 +36,7 @@ final class FreqProxTermsWriter extends TermsHashConsumer {
     // flushed segment:
     if (state.segUpdates != null && state.segUpdates.terms.size() > 0) {
       Map<Term,Integer> segDeletes = state.segUpdates.terms;
-      List<Term> deleteTerms = new ArrayList<Term>(segDeletes.keySet());
+      List<Term> deleteTerms = new ArrayList<>(segDeletes.keySet());
       Collections.sort(deleteTerms);
       String lastField = null;
       TermsEnum termsEnum = null;
@@ -87,7 +87,7 @@ final class FreqProxTermsWriter extends TermsHashConsumer {
 
     // Gather all FieldData's that have postings, across all
     // ThreadStates
-    List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();
+    List<FreqProxTermsWriterPerField> allFields = new ArrayList<>();
 
     for (TermsHashConsumerPerField f : fieldsToFlush.values()) {
       final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;
diff --git lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates.java lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates.java
index c0d52b3..d23139e 100644
--- lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates.java
+++ lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates.java
@@ -83,7 +83,7 @@ class FrozenBufferedUpdates {
     // so that it maps to all fields it affects, sorted by their docUpto, and traverse
     // that Term only once, applying the update to all fields that still need to be
     // updated. 
-    List<NumericUpdate> allUpdates = new ArrayList<NumericUpdate>();
+    List<NumericUpdate> allUpdates = new ArrayList<>();
     int numericUpdatesSize = 0;
     for (LinkedHashMap<Term,NumericUpdate> fieldUpdates : deletes.numericUpdates.values()) {
       for (NumericUpdate update : fieldUpdates.values()) {
diff --git lucene/core/src/java/org/apache/lucene/index/IndexFileDeleter.java lucene/core/src/java/org/apache/lucene/index/IndexFileDeleter.java
index c2f02eb..db7d11f 100644
--- lucene/core/src/java/org/apache/lucene/index/IndexFileDeleter.java
+++ lucene/core/src/java/org/apache/lucene/index/IndexFileDeleter.java
@@ -81,21 +81,21 @@ final class IndexFileDeleter implements Closeable {
   /* Reference count for all files in the index.
    * Counts how many existing commits reference a file.
    **/
-  private Map<String, RefCount> refCounts = new HashMap<String, RefCount>();
+  private Map<String, RefCount> refCounts = new HashMap<>();
 
   /* Holds all commits (segments_N) currently in the index.
    * This will have just 1 commit if you are using the
    * default delete policy (KeepOnlyLastCommitDeletionPolicy).
    * Other policies may leave commit points live for longer
    * in which case this list would be longer than 1: */
-  private List<CommitPoint> commits = new ArrayList<CommitPoint>();
+  private List<CommitPoint> commits = new ArrayList<>();
 
   /* Holds files we had incref'd from the previous
    * non-commit checkpoint: */
-  private final List<String> lastFiles = new ArrayList<String>();
+  private final List<String> lastFiles = new ArrayList<>();
 
   /* Commits that the IndexDeletionPolicy have decided to delete: */
-  private List<CommitPoint> commitsToDelete = new ArrayList<CommitPoint>();
+  private List<CommitPoint> commitsToDelete = new ArrayList<>();
 
   private final InfoStream infoStream;
   private Directory directory;
@@ -597,7 +597,7 @@ final class IndexFileDeleter implements Closeable {
           infoStream.message("IFD", "unable to remove file \"" + fileName + "\": " + e.toString() + "; Will re-try later.");
         }
         if (deletable == null) {
-          deletable = new ArrayList<String>();
+          deletable = new ArrayList<>();
         }
         deletable.add(fileName);                  // add to deletable
       }
diff --git lucene/core/src/java/org/apache/lucene/index/IndexWriter.java lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
index ce54da7..5edd197 100644
--- lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
+++ lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
@@ -236,7 +236,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit{
   final IndexFileDeleter deleter;
 
   // used by forceMerge to note those needing merging
-  private Map<SegmentCommitInfo,Boolean> segmentsToMerge = new HashMap<SegmentCommitInfo,Boolean>();
+  private Map<SegmentCommitInfo,Boolean> segmentsToMerge = new HashMap<>();
   private int mergeMaxNumSegments;
 
   private Lock writeLock;
@@ -246,13 +246,13 @@ public class IndexWriter implements Closeable, TwoPhaseCommit{
 
   // Holds all SegmentInfo instances currently involved in
   // merges
-  private HashSet<SegmentCommitInfo> mergingSegments = new HashSet<SegmentCommitInfo>();
+  private HashSet<SegmentCommitInfo> mergingSegments = new HashSet<>();
 
   private MergePolicy mergePolicy;
   private final MergeScheduler mergeScheduler;
-  private LinkedList<MergePolicy.OneMerge> pendingMerges = new LinkedList<MergePolicy.OneMerge>();
-  private Set<MergePolicy.OneMerge> runningMerges = new HashSet<MergePolicy.OneMerge>();
-  private List<MergePolicy.OneMerge> mergeExceptions = new ArrayList<MergePolicy.OneMerge>();
+  private LinkedList<MergePolicy.OneMerge> pendingMerges = new LinkedList<>();
+  private Set<MergePolicy.OneMerge> runningMerges = new HashSet<>();
+  private List<MergePolicy.OneMerge> mergeExceptions = new ArrayList<>();
   private long mergeGen;
   private boolean stopMerges;
 
@@ -423,7 +423,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit{
 
   class ReaderPool {
     
-    private final Map<SegmentCommitInfo,ReadersAndUpdates> readerMap = new HashMap<SegmentCommitInfo,ReadersAndUpdates>();
+    private final Map<SegmentCommitInfo,ReadersAndUpdates> readerMap = new HashMap<>();
 
     // used only by asserts
     public synchronized boolean infoIsLive(SegmentCommitInfo info) {
@@ -604,7 +604,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit{
     // Make sure that every segment appears only once in the
     // pool:
     private boolean noDups() {
-      Set<String> seen = new HashSet<String>();
+      Set<String> seen = new HashSet<>();
       for(SegmentCommitInfo info : readerMap.keySet()) {
         assert !seen.contains(info.info.name);
         seen.add(info.info.name);
@@ -2344,12 +2344,12 @@ public class IndexWriter implements Closeable, TwoPhaseCommit{
   }
 
   private synchronized void resetMergeExceptions() {
-    mergeExceptions = new ArrayList<MergePolicy.OneMerge>();
+    mergeExceptions = new ArrayList<>();
     mergeGen++;
   }
 
   private void noDupDirs(Directory... dirs) {
-    HashSet<Directory> dups = new HashSet<Directory>();
+    HashSet<Directory> dups = new HashSet<>();
     for(int i=0;i<dirs.length;i++) {
       if (dups.contains(dirs[i]))
         throw new IllegalArgumentException("Directory " + dirs[i] + " appears more than once");
@@ -2363,7 +2363,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit{
    *  to match with a call to {@link IOUtils#close} in a
    *  finally clause. */
   private List<Lock> acquireWriteLocks(Directory... dirs) throws IOException {
-    List<Lock> locks = new ArrayList<Lock>();
+    List<Lock> locks = new ArrayList<>();
     for(int i=0;i<dirs.length;i++) {
       boolean success = false;
       try {
@@ -2442,7 +2442,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit{
 
       flush(false, true);
 
-      List<SegmentCommitInfo> infos = new ArrayList<SegmentCommitInfo>();
+      List<SegmentCommitInfo> infos = new ArrayList<>();
 
       boolean success = false;
       try {
@@ -2565,7 +2565,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit{
       flush(false, true);
 
       String mergedName = newSegmentName();
-      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();
+      final List<AtomicReader> mergeReaders = new ArrayList<>();
       for (IndexReader indexReader : readers) {
         numDocs += indexReader.numDocs();
         for (AtomicReaderContext ctx : indexReader.leaves()) {
@@ -2604,7 +2604,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit{
 
       SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L);
 
-      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));
+      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));
       trackingDir.getCreatedFiles().clear();
                                          
       setDiagnostics(info, SOURCE_ADDINDEXES_READERS);
@@ -2682,7 +2682,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit{
                                           info.info.getDiagnostics());
     SegmentCommitInfo newInfoPerCommit = new SegmentCommitInfo(newInfo, info.getDelCount(), info.getDelGen(), info.getFieldInfosGen());
 
-    Set<String> segFiles = new HashSet<String>();
+    Set<String> segFiles = new HashSet<>();
 
     // Build up new segment's file names.  Must do this
     // before writing SegmentInfo:
@@ -2878,7 +2878,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit{
    * contents after calling this method has no effect.
    */
   public final synchronized void setCommitData(Map<String,String> commitUserData) {
-    segmentInfos.setUserData(new HashMap<String,String>(commitUserData));
+    segmentInfos.setUserData(new HashMap<>(commitUserData));
     ++changeCount;
   }
   
@@ -3201,7 +3201,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit{
     ReadersAndUpdates mergedDeletesAndUpdates = null;
     boolean initWritableLiveDocs = false;
     MergePolicy.DocMap docMap = null;
-    final Map<String,NumericFieldUpdates> mergedFieldUpdates = new HashMap<String,NumericFieldUpdates>();
+    final Map<String,NumericFieldUpdates> mergedFieldUpdates = new HashMap<>();
     
     for (int i = 0; i < sourceSegments.size(); i++) {
       SegmentCommitInfo info = sourceSegments.get(i);
@@ -3855,7 +3855,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit{
     // names.
     final String mergeSegmentName = newSegmentName();
     SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, null);
-    Map<String,String> details = new HashMap<String,String>();
+    Map<String,String> details = new HashMap<>();
     details.put("mergeMaxNumSegments", "" + merge.maxNumSegments);
     details.put("mergeFactor", Integer.toString(merge.segments.size()));
     setDiagnostics(si, SOURCE_MERGE, details);
@@ -3876,7 +3876,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit{
   }
 
   private static void setDiagnostics(SegmentInfo info, String source, Map<String,String> details) {
-    Map<String,String> diagnostics = new HashMap<String,String>();
+    Map<String,String> diagnostics = new HashMap<>();
     diagnostics.put("source", source);
     diagnostics.put("lucene.version", Constants.LUCENE_VERSION);
     diagnostics.put("os", Constants.OS_NAME);
@@ -3970,7 +3970,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit{
       infoStream.message("IW", "merging " + segString(merge.segments));
     }
 
-    merge.readers = new ArrayList<SegmentReader>();
+    merge.readers = new ArrayList<>();
 
     // This is try/finally to make sure merger's readers are
     // closed:
@@ -4067,7 +4067,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit{
         }
       }
       assert mergeState.segmentInfo == merge.info.info;
-      merge.info.info.setFiles(new HashSet<String>(dirWrapper.getCreatedFiles()));
+      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));
 
       // Record which codec was used to write the segment
 
@@ -4314,7 +4314,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit{
   // For infoStream output
   synchronized SegmentInfos toLiveInfos(SegmentInfos sis) {
     final SegmentInfos newSIS = new SegmentInfos();
-    final Map<SegmentCommitInfo,SegmentCommitInfo> liveSIS = new HashMap<SegmentCommitInfo,SegmentCommitInfo>();        
+    final Map<SegmentCommitInfo,SegmentCommitInfo> liveSIS = new HashMap<>();
     for(SegmentCommitInfo info : segmentInfos) {
       liveSIS.put(info, info);
     }
@@ -4606,7 +4606,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit{
     }
 
     // Replace all previous files with the CFS/CFE files:
-    Set<String> siFiles = new HashSet<String>();
+    Set<String> siFiles = new HashSet<>();
     siFiles.add(fileName);
     siFiles.add(IndexFileNames.segmentFileName(info.name, "", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));
     info.setFiles(siFiles);
diff --git lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
index a3a305b..e94e421 100644
--- lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
+++ lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
@@ -130,7 +130,7 @@ public final class IndexWriterConfig extends LiveIndexWriterConfig implements Cl
 
   // indicates whether this config instance is already attached to a writer.
   // not final so that it can be cloned properly.
-  private SetOnce<IndexWriter> writer = new SetOnce<IndexWriter>();
+  private SetOnce<IndexWriter> writer = new SetOnce<>();
   
   /**
    * Sets the {@link IndexWriter} this config is attached to.
diff --git lucene/core/src/java/org/apache/lucene/index/LogMergePolicy.java lucene/core/src/java/org/apache/lucene/index/LogMergePolicy.java
index 1b6b308..b70ef46 100644
--- lucene/core/src/java/org/apache/lucene/index/LogMergePolicy.java
+++ lucene/core/src/java/org/apache/lucene/index/LogMergePolicy.java
@@ -470,7 +470,7 @@ public abstract class LogMergePolicy extends MergePolicy {
 
     // Compute levels, which is just log (base mergeFactor)
     // of the size of each segment
-    final List<SegmentInfoAndLevel> levels = new ArrayList<SegmentInfoAndLevel>();
+    final List<SegmentInfoAndLevel> levels = new ArrayList<>();
     final float norm = (float) Math.log(mergeFactor);
 
     final Collection<SegmentCommitInfo> mergingSegments = writer.get().getMergingSegments();
@@ -572,7 +572,7 @@ public abstract class LogMergePolicy extends MergePolicy {
         } else if (!anyTooLarge) {
           if (spec == null)
             spec = new MergeSpecification();
-          final List<SegmentCommitInfo> mergeInfos = new ArrayList<SegmentCommitInfo>();
+          final List<SegmentCommitInfo> mergeInfos = new ArrayList<>();
           for(int i=start;i<end;i++) {
             mergeInfos.add(levels.get(i).info);
             assert infos.contains(levels.get(i).info);
diff --git lucene/core/src/java/org/apache/lucene/index/MergePolicy.java lucene/core/src/java/org/apache/lucene/index/MergePolicy.java
index 540307d..8010bf0 100644
--- lucene/core/src/java/org/apache/lucene/index/MergePolicy.java
+++ lucene/core/src/java/org/apache/lucene/index/MergePolicy.java
@@ -122,7 +122,7 @@ public abstract class MergePolicy implements java.io.Closeable, Cloneable {
       if (0 == segments.size())
         throw new RuntimeException("segments must include at least one segment");
       // clone the list, as the in list may be based off original SegmentInfos and may be modified
-      this.segments = new ArrayList<SegmentCommitInfo>(segments);
+      this.segments = new ArrayList<>(segments);
       int count = 0;
       for(SegmentCommitInfo info : segments) {
         count += info.info.getDocCount();
@@ -140,7 +140,7 @@ public abstract class MergePolicy implements java.io.Closeable, Cloneable {
       if (readers == null) {
         throw new IllegalStateException("IndexWriter has not initialized readers from the segment infos yet");
       }
-      final List<AtomicReader> readers = new ArrayList<AtomicReader>(this.readers.size());
+      final List<AtomicReader> readers = new ArrayList<>(this.readers.size());
       for (AtomicReader reader : this.readers) {
         if (reader.numDocs() > 0) {
           readers.add(reader);
@@ -295,7 +295,7 @@ public abstract class MergePolicy implements java.io.Closeable, Cloneable {
      * The subset of segments to be included in the primitive merge.
      */
 
-    public final List<OneMerge> merges = new ArrayList<OneMerge>();
+    public final List<OneMerge> merges = new ArrayList<>();
 
     /** Sole constructor.  Use {@link
      *  #add(MergePolicy.OneMerge)} to add merges. */
@@ -393,7 +393,7 @@ public abstract class MergePolicy implements java.io.Closeable, Cloneable {
       // should not happen
       throw new RuntimeException(e);
     }
-    clone.writer = new SetOnce<IndexWriter>();
+    clone.writer = new SetOnce<>();
     return clone;
   }
 
@@ -412,7 +412,7 @@ public abstract class MergePolicy implements java.io.Closeable, Cloneable {
    * defaults than the {@link MergePolicy}
    */
   protected MergePolicy(double defaultNoCFSRatio, long defaultMaxCFSSegmentSize) {
-    writer = new SetOnce<IndexWriter>();
+    writer = new SetOnce<>();
     this.noCFSRatio = defaultNoCFSRatio;
     this.maxCFSSegmentSize = defaultMaxCFSSegmentSize;
   }
diff --git lucene/core/src/java/org/apache/lucene/index/MultiFields.java lucene/core/src/java/org/apache/lucene/index/MultiFields.java
index b25d655..c16738c 100644
--- lucene/core/src/java/org/apache/lucene/index/MultiFields.java
+++ lucene/core/src/java/org/apache/lucene/index/MultiFields.java
@@ -49,7 +49,7 @@ import org.apache.lucene.util.MergedIterator;
 public final class MultiFields extends Fields {
   private final Fields[] subs;
   private final ReaderSlice[] subSlices;
-  private final Map<String,Terms> terms = new ConcurrentHashMap<String,Terms>();
+  private final Map<String,Terms> terms = new ConcurrentHashMap<>();
 
   /** Returns a single {@link Fields} instance for this
    *  reader, merging fields/terms/docs/positions on the
@@ -69,8 +69,8 @@ public final class MultiFields extends Fields {
         // already an atomic reader / reader with one leave
         return leaves.get(0).reader().fields();
       default:
-        final List<Fields> fields = new ArrayList<Fields>();
-        final List<ReaderSlice> slices = new ArrayList<ReaderSlice>();
+        final List<Fields> fields = new ArrayList<>();
+        final List<ReaderSlice> slices = new ArrayList<>();
         for (final AtomicReaderContext ctx : leaves) {
           final AtomicReader r = ctx.reader();
           final Fields f = r.fields();
@@ -203,7 +203,7 @@ public final class MultiFields extends Fields {
     for(int i=0;i<subs.length;i++) {
       subIterators[i] = subs[i].iterator();
     }
-    return new MergedIterator<String>(subIterators);
+    return new MergedIterator<>(subIterators);
   }
 
   @Override
@@ -215,8 +215,8 @@ public final class MultiFields extends Fields {
 
     // Lazy init: first time this field is requested, we
     // create & add to terms:
-    final List<Terms> subs2 = new ArrayList<Terms>();
-    final List<ReaderSlice> slices2 = new ArrayList<ReaderSlice>();
+    final List<Terms> subs2 = new ArrayList<>();
+    final List<ReaderSlice> slices2 = new ArrayList<>();
 
     // Gather all sub-readers that share this field
     for(int i=0;i<subs.length;i++) {
@@ -269,7 +269,7 @@ public final class MultiFields extends Fields {
    *  will be unavailable.
    */
   public static Collection<String> getIndexedFields(IndexReader reader) {
-    final Collection<String> fields = new HashSet<String>();
+    final Collection<String> fields = new HashSet<>();
     for(final FieldInfo fieldInfo : getMergedFieldInfos(reader)) {
       if (fieldInfo.isIndexed()) {
         fields.add(fieldInfo.name);
diff --git lucene/core/src/java/org/apache/lucene/index/MultiTerms.java lucene/core/src/java/org/apache/lucene/index/MultiTerms.java
index 34c5b92..9ad1a1a 100644
--- lucene/core/src/java/org/apache/lucene/index/MultiTerms.java
+++ lucene/core/src/java/org/apache/lucene/index/MultiTerms.java
@@ -70,7 +70,7 @@ public final class MultiTerms extends Terms {
 
   @Override
   public TermsEnum intersect(CompiledAutomaton compiled, BytesRef startTerm) throws IOException {
-    final List<MultiTermsEnum.TermsEnumIndex> termsEnums = new ArrayList<MultiTermsEnum.TermsEnumIndex>();
+    final List<MultiTermsEnum.TermsEnumIndex> termsEnums = new ArrayList<>();
     for(int i=0;i<subs.length;i++) {
       final TermsEnum termsEnum = subs[i].intersect(compiled, startTerm);
       if (termsEnum != null) {
@@ -88,7 +88,7 @@ public final class MultiTerms extends Terms {
   @Override
   public TermsEnum iterator(TermsEnum reuse) throws IOException {
 
-    final List<MultiTermsEnum.TermsEnumIndex> termsEnums = new ArrayList<MultiTermsEnum.TermsEnumIndex>();
+    final List<MultiTermsEnum.TermsEnumIndex> termsEnums = new ArrayList<>();
     for(int i=0;i<subs.length;i++) {
       final TermsEnum termsEnum = subs[i].iterator(null);
       if (termsEnum != null) {
diff --git lucene/core/src/java/org/apache/lucene/index/ParallelAtomicReader.java lucene/core/src/java/org/apache/lucene/index/ParallelAtomicReader.java
index cbc4bbd..f753b3e 100644
--- lucene/core/src/java/org/apache/lucene/index/ParallelAtomicReader.java
+++ lucene/core/src/java/org/apache/lucene/index/ParallelAtomicReader.java
@@ -56,8 +56,8 @@ public class ParallelAtomicReader extends AtomicReader {
   private final boolean closeSubReaders;
   private final int maxDoc, numDocs;
   private final boolean hasDeletions;
-  private final SortedMap<String,AtomicReader> fieldToReader = new TreeMap<String,AtomicReader>();
-  private final SortedMap<String,AtomicReader> tvFieldToReader = new TreeMap<String,AtomicReader>();
+  private final SortedMap<String,AtomicReader> fieldToReader = new TreeMap<>();
+  private final SortedMap<String,AtomicReader> tvFieldToReader = new TreeMap<>();
   
   /** Create a ParallelAtomicReader based on the provided
    *  readers; auto-closes the given readers on {@link #close()}. */
@@ -151,7 +151,7 @@ public class ParallelAtomicReader extends AtomicReader {
   
   // Single instance of this, per ParallelReader instance
   private final class ParallelFields extends Fields {
-    final Map<String,Terms> fields = new TreeMap<String,Terms>();
+    final Map<String,Terms> fields = new TreeMap<>();
     
     ParallelFields() {
     }
diff --git lucene/core/src/java/org/apache/lucene/index/PersistentSnapshotDeletionPolicy.java lucene/core/src/java/org/apache/lucene/index/PersistentSnapshotDeletionPolicy.java
index 052302d..ed56fce 100644
--- lucene/core/src/java/org/apache/lucene/index/PersistentSnapshotDeletionPolicy.java
+++ lucene/core/src/java/org/apache/lucene/index/PersistentSnapshotDeletionPolicy.java
@@ -241,13 +241,13 @@ public class PersistentSnapshotDeletionPolicy extends SnapshotDeletionPolicy {
   private synchronized void loadPriorSnapshots() throws IOException {
     long genLoaded = -1;
     IOException ioe = null;
-    List<String> snapshotFiles = new ArrayList<String>();
+    List<String> snapshotFiles = new ArrayList<>();
     for(String file : dir.listAll()) {
       if (file.startsWith(SNAPSHOTS_PREFIX)) {
         long gen = Long.parseLong(file.substring(SNAPSHOTS_PREFIX.length()));
         if (genLoaded == -1 || gen > genLoaded) {
           snapshotFiles.add(file);
-          Map<Long,Integer> m = new HashMap<Long,Integer>();    
+          Map<Long,Integer> m = new HashMap<>();
           IndexInput in = dir.openInput(file, IOContext.DEFAULT);
           try {
             CodecUtil.checkHeader(in, CODEC_NAME, VERSION_START, VERSION_START);
diff --git lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates.java lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates.java
index 0e68314..23b849e 100644
--- lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates.java
+++ lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates.java
@@ -78,7 +78,7 @@ class ReadersAndUpdates {
   // updates on the merged segment too.
   private boolean isMerging = false;
   
-  private final Map<String,NumericFieldUpdates> mergingNumericUpdates = new HashMap<String,NumericFieldUpdates>();
+  private final Map<String,NumericFieldUpdates> mergingNumericUpdates = new HashMap<>();
   
   public ReadersAndUpdates(IndexWriter writer, SegmentCommitInfo info) {
     this.info = info;
@@ -448,7 +448,7 @@ class ReadersAndUpdates {
     
     // create a new map, keeping only the gens that are in use
     Map<Long,Set<String>> genUpdatesFiles = info.getUpdatesFiles();
-    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<Long,Set<String>>();
+    Map<Long,Set<String>> newGenUpdatesFiles = new HashMap<>();
     final long fieldInfosGen = info.getFieldInfosGen();
     for (FieldInfo fi : fieldInfos) {
       long dvGen = fi.getDocValuesGen();
diff --git lucene/core/src/java/org/apache/lucene/index/SegmentCommitInfo.java lucene/core/src/java/org/apache/lucene/index/SegmentCommitInfo.java
index e2cba48..9b437a2 100644
--- lucene/core/src/java/org/apache/lucene/index/SegmentCommitInfo.java
+++ lucene/core/src/java/org/apache/lucene/index/SegmentCommitInfo.java
@@ -56,7 +56,7 @@ public class SegmentCommitInfo {
   private long nextWriteFieldInfosGen;
 
   // Track the per-generation updates files
-  private final Map<Long,Set<String>> genUpdatesFiles = new HashMap<Long,Set<String>>();
+  private final Map<Long,Set<String>> genUpdatesFiles = new HashMap<>();
   
   private volatile long sizeInBytes = -1;
 
@@ -147,7 +147,7 @@ public class SegmentCommitInfo {
   /** Returns all files in use by this segment. */
   public Collection<String> files() throws IOException {
     // Start from the wrapped info's files:
-    Collection<String> files = new HashSet<String>(info.files());
+    Collection<String> files = new HashSet<>(info.files());
 
     // TODO we could rely on TrackingDir.getCreatedFiles() (like we do for
     // updates) and then maybe even be able to remove LiveDocsFormat.files().
@@ -257,7 +257,7 @@ public class SegmentCommitInfo {
     
     // deep clone
     for (Entry<Long,Set<String>> e : genUpdatesFiles.entrySet()) {
-      other.genUpdatesFiles.put(e.getKey(), new HashSet<String>(e.getValue()));
+      other.genUpdatesFiles.put(e.getKey(), new HashSet<>(e.getValue()));
     }
     
     return other;
diff --git lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
index 057c98d..d6a67df 100644
--- lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
+++ lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
@@ -79,7 +79,7 @@ final class SegmentCoreReaders {
   final CloseableThreadLocal<Map<String,Object>> normsLocal = new CloseableThreadLocal<Map<String,Object>>() {
     @Override
     protected Map<String,Object> initialValue() {
-      return new HashMap<String,Object>();
+      return new HashMap<>();
     }
   };
 
diff --git lucene/core/src/java/org/apache/lucene/index/SegmentDocValues.java lucene/core/src/java/org/apache/lucene/index/SegmentDocValues.java
index 7290c83..4f09296 100644
--- lucene/core/src/java/org/apache/lucene/index/SegmentDocValues.java
+++ lucene/core/src/java/org/apache/lucene/index/SegmentDocValues.java
@@ -35,7 +35,7 @@ import org.apache.lucene.util.RefCount;
  */
 final class SegmentDocValues {
 
-  private final Map<Long,RefCount<DocValuesProducer>> genDVProducers = new HashMap<Long,RefCount<DocValuesProducer>>();
+  private final Map<Long,RefCount<DocValuesProducer>> genDVProducers = new HashMap<>();
 
   private RefCount<DocValuesProducer> newDocValuesProducer(SegmentCommitInfo si, IOContext context, Directory dir,
       DocValuesFormat dvFormat, final Long gen, List<FieldInfo> infos) throws IOException {
diff --git lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java
index c7bbbf5..c993137 100644
--- lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java
+++ lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java
@@ -139,9 +139,9 @@ public final class SegmentInfos implements Cloneable, Iterable<SegmentCommitInfo
                                // there was an IOException that had interrupted a commit
 
   /** Opaque Map&lt;String, String&gt; that user can specify during IndexWriter.commit */
-  public Map<String,String> userData = Collections.<String,String>emptyMap();
+  public Map<String,String> userData = Collections.emptyMap();
   
-  private List<SegmentCommitInfo> segments = new ArrayList<SegmentCommitInfo>();
+  private List<SegmentCommitInfo> segments = new ArrayList<>();
   
   /**
    * If non-null, information about loading segments_N files
@@ -355,7 +355,7 @@ public final class SegmentInfos implements Cloneable, Iterable<SegmentCommitInfo
           if (numGensUpdatesFiles == 0) {
             genUpdatesFiles = Collections.emptyMap();
           } else {
-            genUpdatesFiles = new HashMap<Long,Set<String>>(numGensUpdatesFiles);
+            genUpdatesFiles = new HashMap<>(numGensUpdatesFiles);
             for (int i = 0; i < numGensUpdatesFiles; i++) {
               genUpdatesFiles.put(input.readLong(), input.readStringSet());
             }
@@ -471,13 +471,13 @@ public final class SegmentInfos implements Cloneable, Iterable<SegmentCommitInfo
     try {
       final SegmentInfos sis = (SegmentInfos) super.clone();
       // deep clone, first recreate all collections:
-      sis.segments = new ArrayList<SegmentCommitInfo>(size());
+      sis.segments = new ArrayList<>(size());
       for(final SegmentCommitInfo info : this) {
         assert info.info.getCodec() != null;
         // dont directly access segments, use add method!!!
         sis.add(info.clone());
       }
-      sis.userData = new HashMap<String,String>(userData);
+      sis.userData = new HashMap<>(userData);
       return sis;
     } catch (CloneNotSupportedException e) {
       throw new RuntimeException("should not happen", e);
@@ -832,7 +832,7 @@ public final class SegmentInfos implements Cloneable, Iterable<SegmentCommitInfo
    *  The returned collection is recomputed on each
    *  invocation.  */
   public Collection<String> files(Directory dir, boolean includeSegmentsFile) throws IOException {
-    HashSet<String> files = new HashSet<String>();
+    HashSet<String> files = new HashSet<>();
     if (includeSegmentsFile) {
       final String segmentFileName = getSegmentsFileName();
       if (segmentFileName != null) {
@@ -978,7 +978,7 @@ public final class SegmentInfos implements Cloneable, Iterable<SegmentCommitInfo
   
   /** applies all changes caused by committing a merge to this SegmentInfos */
   void applyMergeChanges(MergePolicy.OneMerge merge, boolean dropSegment) {
-    final Set<SegmentCommitInfo> mergedAway = new HashSet<SegmentCommitInfo>(merge.segments);
+    final Set<SegmentCommitInfo> mergedAway = new HashSet<>(merge.segments);
     boolean inserted = false;
     int newSegIdx = 0;
     for (int segIdx = 0, cnt = segments.size(); segIdx < cnt; segIdx++) {
@@ -1010,7 +1010,7 @@ public final class SegmentInfos implements Cloneable, Iterable<SegmentCommitInfo
   }
 
   List<SegmentCommitInfo> createBackupSegmentInfos() {
-    final List<SegmentCommitInfo> list = new ArrayList<SegmentCommitInfo>(size());
+    final List<SegmentCommitInfo> list = new ArrayList<>(size());
     for(final SegmentCommitInfo info : this) {
       assert info.info.getCodec() != null;
       list.add(info.clone());
diff --git lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
index fec031e..9570c18 100644
--- lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
+++ lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -155,8 +155,8 @@ final class SegmentMerger {
         DocValuesType type = field.getDocValuesType();
         if (type != null) {
           if (type == DocValuesType.NUMERIC) {
-            List<NumericDocValues> toMerge = new ArrayList<NumericDocValues>();
-            List<Bits> docsWithField = new ArrayList<Bits>();
+            List<NumericDocValues> toMerge = new ArrayList<>();
+            List<Bits> docsWithField = new ArrayList<>();
             for (AtomicReader reader : mergeState.readers) {
               NumericDocValues values = reader.getNumericDocValues(field.name);
               Bits bits = reader.getDocsWithField(field.name);
@@ -169,8 +169,8 @@ final class SegmentMerger {
             }
             consumer.mergeNumericField(field, mergeState, toMerge, docsWithField);
           } else if (type == DocValuesType.BINARY) {
-            List<BinaryDocValues> toMerge = new ArrayList<BinaryDocValues>();
-            List<Bits> docsWithField = new ArrayList<Bits>();
+            List<BinaryDocValues> toMerge = new ArrayList<>();
+            List<Bits> docsWithField = new ArrayList<>();
             for (AtomicReader reader : mergeState.readers) {
               BinaryDocValues values = reader.getBinaryDocValues(field.name);
               Bits bits = reader.getDocsWithField(field.name);
@@ -183,7 +183,7 @@ final class SegmentMerger {
             }
             consumer.mergeBinaryField(field, mergeState, toMerge, docsWithField);
           } else if (type == DocValuesType.SORTED) {
-            List<SortedDocValues> toMerge = new ArrayList<SortedDocValues>();
+            List<SortedDocValues> toMerge = new ArrayList<>();
             for (AtomicReader reader : mergeState.readers) {
               SortedDocValues values = reader.getSortedDocValues(field.name);
               if (values == null) {
@@ -193,7 +193,7 @@ final class SegmentMerger {
             }
             consumer.mergeSortedField(field, mergeState, toMerge);
           } else if (type == DocValuesType.SORTED_SET) {
-            List<SortedSetDocValues> toMerge = new ArrayList<SortedSetDocValues>();
+            List<SortedSetDocValues> toMerge = new ArrayList<>();
             for (AtomicReader reader : mergeState.readers) {
               SortedSetDocValues values = reader.getSortedSetDocValues(field.name);
               if (values == null) {
@@ -223,8 +223,8 @@ final class SegmentMerger {
     try {
       for (FieldInfo field : mergeState.fieldInfos) {
         if (field.hasNorms()) {
-          List<NumericDocValues> toMerge = new ArrayList<NumericDocValues>();
-          List<Bits> docsWithField = new ArrayList<Bits>();
+          List<NumericDocValues> toMerge = new ArrayList<>();
+          List<Bits> docsWithField = new ArrayList<>();
           for (AtomicReader reader : mergeState.readers) {
             NumericDocValues norms = reader.getNormValues(field.name);
             if (norms == null) {
@@ -358,8 +358,8 @@ final class SegmentMerger {
 
   private void mergeTerms(SegmentWriteState segmentWriteState) throws IOException {
     
-    final List<Fields> fields = new ArrayList<Fields>();
-    final List<ReaderSlice> slices = new ArrayList<ReaderSlice>();
+    final List<Fields> fields = new ArrayList<>();
+    final List<ReaderSlice> slices = new ArrayList<>();
 
     int docBase = 0;
 
diff --git lucene/core/src/java/org/apache/lucene/index/SegmentReader.java lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
index f148f77..907516e 100644
--- lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
+++ lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
@@ -60,22 +60,22 @@ public final class SegmentReader extends AtomicReader {
   final CloseableThreadLocal<Map<String,Object>> docValuesLocal = new CloseableThreadLocal<Map<String,Object>>() {
     @Override
     protected Map<String,Object> initialValue() {
-      return new HashMap<String,Object>();
+      return new HashMap<>();
     }
   };
 
   final CloseableThreadLocal<Map<String,Bits>> docsWithFieldLocal = new CloseableThreadLocal<Map<String,Bits>>() {
     @Override
     protected Map<String,Bits> initialValue() {
-      return new HashMap<String,Bits>();
+      return new HashMap<>();
     }
   };
 
-  final Map<String,DocValuesProducer> dvProducers = new HashMap<String,DocValuesProducer>();
+  final Map<String,DocValuesProducer> dvProducers = new HashMap<>();
   
   final FieldInfos fieldInfos;
 
-  private final List<Long> dvGens = new ArrayList<Long>();
+  private final List<Long> dvGens = new ArrayList<>();
   
   /**
    * Constructs a new SegmentReader with a new core.
@@ -221,7 +221,7 @@ public final class SegmentReader extends AtomicReader {
   
   // returns a gen->List<FieldInfo> mapping. Fields without DV updates have gen=-1
   private Map<Long,List<FieldInfo>> getGenInfos() {
-    final Map<Long,List<FieldInfo>> genInfos = new HashMap<Long,List<FieldInfo>>();
+    final Map<Long,List<FieldInfo>> genInfos = new HashMap<>();
     for (FieldInfo fi : fieldInfos) {
       if (fi.getDocValuesType() == null) {
         continue;
@@ -229,7 +229,7 @@ public final class SegmentReader extends AtomicReader {
       long gen = fi.getDocValuesGen();
       List<FieldInfo> infos = genInfos.get(gen);
       if (infos == null) {
-        infos = new ArrayList<FieldInfo>();
+        infos = new ArrayList<>();
         genInfos.put(gen, infos);
       }
       infos.add(fi);
diff --git lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
index d81825e..dd56512 100644
--- lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
+++ lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
@@ -180,7 +180,7 @@ public final class SlowCompositeReaderWrapper extends AtomicReader {
   
   // TODO: this could really be a weak map somewhere else on the coreCacheKey,
   // but do we really need to optimize slow-wrapper any more?
-  private final Map<String,OrdinalMap> cachedOrdMaps = new HashMap<String,OrdinalMap>();
+  private final Map<String,OrdinalMap> cachedOrdMaps = new HashMap<>();
 
   @Override
   public NumericDocValues getNormValues(String field) throws IOException {
diff --git lucene/core/src/java/org/apache/lucene/index/SnapshotDeletionPolicy.java lucene/core/src/java/org/apache/lucene/index/SnapshotDeletionPolicy.java
index 6cc7b8c..0f78e3d 100644
--- lucene/core/src/java/org/apache/lucene/index/SnapshotDeletionPolicy.java
+++ lucene/core/src/java/org/apache/lucene/index/SnapshotDeletionPolicy.java
@@ -47,10 +47,10 @@ public class SnapshotDeletionPolicy extends IndexDeletionPolicy {
 
   /** Records how many snapshots are held against each
    *  commit generation */
-  protected Map<Long,Integer> refCounts = new HashMap<Long,Integer>();
+  protected Map<Long,Integer> refCounts = new HashMap<>();
 
   /** Used to map gen to IndexCommit. */
-  protected Map<Long,IndexCommit> indexCommits = new HashMap<Long,IndexCommit>();
+  protected Map<Long,IndexCommit> indexCommits = new HashMap<>();
 
   /** Wrapped {@link IndexDeletionPolicy} */
   private IndexDeletionPolicy primary;
@@ -167,7 +167,7 @@ public class SnapshotDeletionPolicy extends IndexDeletionPolicy {
 
   /** Returns all IndexCommits held by at least one snapshot. */
   public synchronized List<IndexCommit> getSnapshots() {
-    return new ArrayList<IndexCommit>(indexCommits.values());
+    return new ArrayList<>(indexCommits.values());
   }
 
   /** Returns the total number of snapshots currently held. */
@@ -192,15 +192,15 @@ public class SnapshotDeletionPolicy extends IndexDeletionPolicy {
     SnapshotDeletionPolicy other = (SnapshotDeletionPolicy) super.clone();
     other.primary = this.primary.clone();
     other.lastCommit = null;
-    other.refCounts = new HashMap<Long,Integer>(refCounts);
-    other.indexCommits = new HashMap<Long,IndexCommit>(indexCommits);
+    other.refCounts = new HashMap<>(refCounts);
+    other.indexCommits = new HashMap<>(indexCommits);
     return other;
   }
 
   /** Wraps each {@link IndexCommit} as a {@link
    *  SnapshotCommitPoint}. */
   private List<IndexCommit> wrapCommits(List<? extends IndexCommit> commits) {
-    List<IndexCommit> wrappedCommits = new ArrayList<IndexCommit>(commits.size());
+    List<IndexCommit> wrappedCommits = new ArrayList<>(commits.size());
     for (IndexCommit ic : commits) {
       wrappedCommits.add(new SnapshotCommitPoint(ic));
     }
diff --git lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java
index 04d847b..fbe4376 100644
--- lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java
+++ lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java
@@ -78,7 +78,7 @@ final class StandardDirectoryReader extends DirectoryReader {
     // no need to process segments in reverse order
     final int numSegments = infos.size();
 
-    List<SegmentReader> readers = new ArrayList<SegmentReader>();
+    List<SegmentReader> readers = new ArrayList<>();
     final Directory dir = writer.getDirectory();
 
     final SegmentInfos segmentInfos = infos.clone();
@@ -134,7 +134,7 @@ final class StandardDirectoryReader extends DirectoryReader {
 
     // we put the old SegmentReaders in a map, that allows us
     // to lookup a reader using its segment name
-    final Map<String,Integer> segmentReaders = new HashMap<String,Integer>();
+    final Map<String,Integer> segmentReaders = new HashMap<>();
 
     if (oldReaders != null) {
       // create a Map SegmentName->SegmentReader
diff --git lucene/core/src/java/org/apache/lucene/index/StoredDocument.java lucene/core/src/java/org/apache/lucene/index/StoredDocument.java
index b1a9fba..b79c6b6 100644
--- lucene/core/src/java/org/apache/lucene/index/StoredDocument.java
+++ lucene/core/src/java/org/apache/lucene/index/StoredDocument.java
@@ -34,7 +34,7 @@ import org.apache.lucene.util.BytesRef;
 // TODO: shouldn't this really be in the .document package?
 public class StoredDocument implements Iterable<StorableField> {
 
-  private final List<StorableField> fields = new ArrayList<StorableField>();
+  private final List<StorableField> fields = new ArrayList<>();
 
   /** Sole constructor. */
   public StoredDocument() {
@@ -61,7 +61,7 @@ public class StoredDocument implements Iterable<StorableField> {
    * @return a <code>StorableField[]</code> array
    */
   public StorableField[] getFields(String name) {
-    List<StorableField> result = new ArrayList<StorableField>();
+    List<StorableField> result = new ArrayList<>();
     for (StorableField field : fields) {
       if (field.name().equals(name)) {
         result.add(field);
@@ -112,7 +112,7 @@ public class StoredDocument implements Iterable<StorableField> {
    * @return a <code>BytesRef[]</code> of binary field values
    */
    public final BytesRef[] getBinaryValues(String name) {
-     final List<BytesRef> result = new ArrayList<BytesRef>();
+     final List<BytesRef> result = new ArrayList<>();
      for (StorableField field : fields) {
        if (field.name().equals(name)) {
          final BytesRef bytes = field.binaryValue();
@@ -158,7 +158,7 @@ public class StoredDocument implements Iterable<StorableField> {
     * @return a <code>String[]</code> of field values
     */
    public final String[] getValues(String name) {
-     List<String> result = new ArrayList<String>();
+     List<String> result = new ArrayList<>();
      for (StorableField field : fields) {
        if (field.name().equals(name) && field.stringValue() != null) {
          result.add(field.stringValue());
diff --git lucene/core/src/java/org/apache/lucene/index/TermsHash.java lucene/core/src/java/org/apache/lucene/index/TermsHash.java
index bdceb25..ce3887c 100644
--- lucene/core/src/java/org/apache/lucene/index/TermsHash.java
+++ lucene/core/src/java/org/apache/lucene/index/TermsHash.java
@@ -96,11 +96,11 @@ final class TermsHash extends InvertedDocConsumer {
 
   @Override
   void flush(Map<String,InvertedDocConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {
-    Map<String,TermsHashConsumerPerField> childFields = new HashMap<String,TermsHashConsumerPerField>();
+    Map<String,TermsHashConsumerPerField> childFields = new HashMap<>();
     Map<String,InvertedDocConsumerPerField> nextChildFields;
 
     if (nextTermsHash != null) {
-      nextChildFields = new HashMap<String,InvertedDocConsumerPerField>();
+      nextChildFields = new HashMap<>();
     } else {
       nextChildFields = null;
     }
diff --git lucene/core/src/java/org/apache/lucene/index/ThreadAffinityDocumentsWriterThreadPool.java lucene/core/src/java/org/apache/lucene/index/ThreadAffinityDocumentsWriterThreadPool.java
index 82a0477..a1fc118 100644
--- lucene/core/src/java/org/apache/lucene/index/ThreadAffinityDocumentsWriterThreadPool.java
+++ lucene/core/src/java/org/apache/lucene/index/ThreadAffinityDocumentsWriterThreadPool.java
@@ -31,7 +31,7 @@ import org.apache.lucene.index.DocumentsWriterPerThreadPool.ThreadState; //javad
  * minimal contended {@link ThreadState}.
  */
 class ThreadAffinityDocumentsWriterThreadPool extends DocumentsWriterPerThreadPool {
-  private Map<Thread, ThreadState> threadBindings = new ConcurrentHashMap<Thread, ThreadState>();
+  private Map<Thread, ThreadState> threadBindings = new ConcurrentHashMap<>();
   
   /**
    * Creates a new {@link ThreadAffinityDocumentsWriterThreadPool} with a given maximum of {@link ThreadState}s.
@@ -78,7 +78,7 @@ class ThreadAffinityDocumentsWriterThreadPool extends DocumentsWriterPerThreadPo
   @Override
   public ThreadAffinityDocumentsWriterThreadPool clone() {
     ThreadAffinityDocumentsWriterThreadPool clone = (ThreadAffinityDocumentsWriterThreadPool) super.clone();
-    clone.threadBindings = new ConcurrentHashMap<Thread, ThreadState>();
+    clone.threadBindings = new ConcurrentHashMap<>();
     return clone;
   }
 }
diff --git lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java
index 45b8d7e..f1213cb 100644
--- lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java
+++ lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java
@@ -280,9 +280,9 @@ public class TieredMergePolicy extends MergePolicy {
       return null;
     }
     final Collection<SegmentCommitInfo> merging = writer.get().getMergingSegments();
-    final Collection<SegmentCommitInfo> toBeMerged = new HashSet<SegmentCommitInfo>();
+    final Collection<SegmentCommitInfo> toBeMerged = new HashSet<>();
 
-    final List<SegmentCommitInfo> infosSorted = new ArrayList<SegmentCommitInfo>(infos.asList());
+    final List<SegmentCommitInfo> infosSorted = new ArrayList<>(infos.asList());
     Collections.sort(infosSorted, new SegmentByteSizeDescending());
 
     // Compute total index bytes & print details about the index
@@ -341,7 +341,7 @@ public class TieredMergePolicy extends MergePolicy {
       // Gather eligible segments for merging, ie segments
       // not already being merged and not already picked (by
       // prior iteration of this loop) for merging:
-      final List<SegmentCommitInfo> eligible = new ArrayList<SegmentCommitInfo>();
+      final List<SegmentCommitInfo> eligible = new ArrayList<>();
       for(int idx = tooBigCount; idx<infosSorted.size(); idx++) {
         final SegmentCommitInfo info = infosSorted.get(idx);
         if (merging.contains(info)) {
@@ -374,7 +374,7 @@ public class TieredMergePolicy extends MergePolicy {
 
           long totAfterMergeBytes = 0;
 
-          final List<SegmentCommitInfo> candidate = new ArrayList<SegmentCommitInfo>();
+          final List<SegmentCommitInfo> candidate = new ArrayList<>();
           boolean hitTooLarge = false;
           for(int idx = startIdx;idx<eligible.size() && candidate.size() < maxMergeAtOnce;idx++) {
             final SegmentCommitInfo info = eligible.get(idx);
@@ -497,7 +497,7 @@ public class TieredMergePolicy extends MergePolicy {
       message("findForcedMerges maxSegmentCount=" + maxSegmentCount + " infos=" + writer.get().segString(infos) + " segmentsToMerge=" + segmentsToMerge);
     }
 
-    List<SegmentCommitInfo> eligible = new ArrayList<SegmentCommitInfo>();
+    List<SegmentCommitInfo> eligible = new ArrayList<>();
     boolean forceMergeRunning = false;
     final Collection<SegmentCommitInfo> merging = writer.get().getMergingSegments();
     boolean segmentIsOriginal = false;
@@ -568,7 +568,7 @@ public class TieredMergePolicy extends MergePolicy {
     if (verbose()) {
       message("findForcedDeletesMerges infos=" + writer.get().segString(infos) + " forceMergeDeletesPctAllowed=" + forceMergeDeletesPctAllowed);
     }
-    final List<SegmentCommitInfo> eligible = new ArrayList<SegmentCommitInfo>();
+    final List<SegmentCommitInfo> eligible = new ArrayList<>();
     final Collection<SegmentCommitInfo> merging = writer.get().getMergingSegments();
     for(SegmentCommitInfo info : infos) {
       double pctDeletes = 100.*((double) writer.get().numDeletedDocs(info))/info.info.getDocCount();
diff --git lucene/core/src/java/org/apache/lucene/index/UpgradeIndexMergePolicy.java lucene/core/src/java/org/apache/lucene/index/UpgradeIndexMergePolicy.java
index fb08ed6..2561385 100644
--- lucene/core/src/java/org/apache/lucene/index/UpgradeIndexMergePolicy.java
+++ lucene/core/src/java/org/apache/lucene/index/UpgradeIndexMergePolicy.java
@@ -83,7 +83,7 @@ public class UpgradeIndexMergePolicy extends MergePolicy {
   @Override
   public MergeSpecification findForcedMerges(SegmentInfos segmentInfos, int maxSegmentCount, Map<SegmentCommitInfo,Boolean> segmentsToMerge) throws IOException {
     // first find all old segments
-    final Map<SegmentCommitInfo,Boolean> oldSegments = new HashMap<SegmentCommitInfo,Boolean>();
+    final Map<SegmentCommitInfo,Boolean> oldSegments = new HashMap<>();
     for (final SegmentCommitInfo si : segmentInfos) {
       final Boolean v = segmentsToMerge.get(si);
       if (v != null && shouldUpgradeSegment(si)) {
@@ -114,7 +114,7 @@ public class UpgradeIndexMergePolicy extends MergePolicy {
         message("findForcedMerges: " +  base.getClass().getSimpleName() +
         " does not want to merge all old segments, merge remaining ones into new segment: " + oldSegments);
       }
-      final List<SegmentCommitInfo> newInfos = new ArrayList<SegmentCommitInfo>();
+      final List<SegmentCommitInfo> newInfos = new ArrayList<>();
       for (final SegmentCommitInfo si : segmentInfos) {
         if (oldSegments.containsKey(si)) {
           newInfos.add(si);
diff --git lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java
index e6dfa3b..01b4c53 100644
--- lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java
@@ -68,7 +68,7 @@ public class BooleanQuery extends Query implements Iterable<BooleanClause> {
     BooleanQuery.maxClauseCount = maxClauseCount;
   }
 
-  private ArrayList<BooleanClause> clauses = new ArrayList<BooleanClause>();
+  private ArrayList<BooleanClause> clauses = new ArrayList<>();
   private final boolean disableCoord;
 
   /** Constructs an empty boolean query. */
@@ -179,7 +179,7 @@ public class BooleanQuery extends Query implements Iterable<BooleanClause> {
       throws IOException {
       this.similarity = searcher.getSimilarity();
       this.disableCoord = disableCoord;
-      weights = new ArrayList<Weight>(clauses.size());
+      weights = new ArrayList<>(clauses.size());
       for (int i = 0 ; i < clauses.size(); i++) {
         BooleanClause c = clauses.get(i);
         Weight w = c.getQuery().createWeight(searcher);
@@ -308,9 +308,9 @@ public class BooleanQuery extends Query implements Iterable<BooleanClause> {
     public Scorer scorer(AtomicReaderContext context, boolean scoreDocsInOrder,
         boolean topScorer, Bits acceptDocs)
         throws IOException {
-      List<Scorer> required = new ArrayList<Scorer>();
-      List<Scorer> prohibited = new ArrayList<Scorer>();
-      List<Scorer> optional = new ArrayList<Scorer>();
+      List<Scorer> required = new ArrayList<>();
+      List<Scorer> prohibited = new ArrayList<>();
+      List<Scorer> optional = new ArrayList<>();
       Iterator<BooleanClause> cIter = clauses.iterator();
       for (Weight w  : weights) {
         BooleanClause c =  cIter.next();
diff --git lucene/core/src/java/org/apache/lucene/search/BooleanScorer2.java lucene/core/src/java/org/apache/lucene/search/BooleanScorer2.java
index 85fa403..1b8808e 100644
--- lucene/core/src/java/org/apache/lucene/search/BooleanScorer2.java
+++ lucene/core/src/java/org/apache/lucene/search/BooleanScorer2.java
@@ -238,7 +238,7 @@ class BooleanScorer2 extends Scorer {
 
   private Scorer makeCountingSumScorerSomeReq(boolean disableCoord) throws IOException { // At least one required scorer.
     if (optionalScorers.size() == minNrShouldMatch) { // all optional scorers also required.
-      ArrayList<Scorer> allReq = new ArrayList<Scorer>(requiredScorers);
+      ArrayList<Scorer> allReq = new ArrayList<>(requiredScorers);
       allReq.addAll(optionalScorers);
       return addProhibitedScorers(countingConjunctionSumScorer(disableCoord, allReq));
     } else { // optionalScorers.size() > minNrShouldMatch, and at least one required scorer
@@ -335,7 +335,7 @@ class BooleanScorer2 extends Scorer {
 
   @Override
   public Collection<ChildScorer> getChildren() {
-    ArrayList<ChildScorer> children = new ArrayList<ChildScorer>();
+    ArrayList<ChildScorer> children = new ArrayList<>();
     for (Scorer s : optionalScorers) {
       children.add(new ChildScorer(s, "SHOULD"));
     }
diff --git lucene/core/src/java/org/apache/lucene/search/CachingCollector.java lucene/core/src/java/org/apache/lucene/search/CachingCollector.java
index 554da52..23e1590 100644
--- lucene/core/src/java/org/apache/lucene/search/CachingCollector.java
+++ lucene/core/src/java/org/apache/lucene/search/CachingCollector.java
@@ -107,7 +107,7 @@ public abstract class CachingCollector extends Collector {
       super(other, maxRAMMB, true);
 
       cachedScorer = new CachedScorer();
-      cachedScores = new ArrayList<float[]>();
+      cachedScores = new ArrayList<>();
       curScores = new float[INITIAL_ARRAY_SIZE];
       cachedScores.add(curScores);
     }
@@ -116,7 +116,7 @@ public abstract class CachingCollector extends Collector {
       super(other, maxDocsToCache);
 
       cachedScorer = new CachedScorer();
-      cachedScores = new ArrayList<float[]>();
+      cachedScores = new ArrayList<>();
       curScores = new float[INITIAL_ARRAY_SIZE];
       cachedScores.add(curScores);
     }
@@ -315,7 +315,7 @@ public abstract class CachingCollector extends Collector {
   protected final Collector other;
   
   protected final int maxDocsToCache;
-  protected final List<SegStart> cachedSegs = new ArrayList<SegStart>();
+  protected final List<SegStart> cachedSegs = new ArrayList<>();
   protected final List<int[]> cachedDocs;
   
   private AtomicReaderContext lastReaderContext;
@@ -393,7 +393,7 @@ public abstract class CachingCollector extends Collector {
   private CachingCollector(Collector other, double maxRAMMB, boolean cacheScores) {
     this.other = other;
     
-    cachedDocs = new ArrayList<int[]>();
+    cachedDocs = new ArrayList<>();
     curDocs = new int[INITIAL_ARRAY_SIZE];
     cachedDocs.add(curDocs);
 
@@ -407,7 +407,7 @@ public abstract class CachingCollector extends Collector {
   private CachingCollector(Collector other, int maxDocsToCache) {
     this.other = other;
 
-    cachedDocs = new ArrayList<int[]>();
+    cachedDocs = new ArrayList<>();
     curDocs = new int[INITIAL_ARRAY_SIZE];
     cachedDocs.add(curDocs);
     this.maxDocsToCache = maxDocsToCache;
diff --git lucene/core/src/java/org/apache/lucene/search/CachingWrapperFilter.java lucene/core/src/java/org/apache/lucene/search/CachingWrapperFilter.java
index f8b7566..0ad87d5 100644
--- lucene/core/src/java/org/apache/lucene/search/CachingWrapperFilter.java
+++ lucene/core/src/java/org/apache/lucene/search/CachingWrapperFilter.java
@@ -155,7 +155,7 @@ public class CachingWrapperFilter extends Filter {
     // Sync only to pull the current set of values:
     List<DocIdSet> docIdSets;
     synchronized(cache) {
-      docIdSets = new ArrayList<DocIdSet>(cache.values());
+      docIdSets = new ArrayList<>(cache.values());
     }
 
     long total = 0;
diff --git lucene/core/src/java/org/apache/lucene/search/ConjunctionScorer.java lucene/core/src/java/org/apache/lucene/search/ConjunctionScorer.java
index 22476e7..3e81187 100644
--- lucene/core/src/java/org/apache/lucene/search/ConjunctionScorer.java
+++ lucene/core/src/java/org/apache/lucene/search/ConjunctionScorer.java
@@ -122,7 +122,7 @@ class ConjunctionScorer extends Scorer {
 
   @Override
   public Collection<ChildScorer> getChildren() {
-    ArrayList<ChildScorer> children = new ArrayList<ChildScorer>(docsAndFreqs.length);
+    ArrayList<ChildScorer> children = new ArrayList<>(docsAndFreqs.length);
     for (DocsAndFreqs docs : docsAndFreqs) {
       children.add(new ChildScorer(docs.scorer, "MUST"));
     }
diff --git lucene/core/src/java/org/apache/lucene/search/DisjunctionMaxQuery.java lucene/core/src/java/org/apache/lucene/search/DisjunctionMaxQuery.java
index f61a54f..649b0fb 100644
--- lucene/core/src/java/org/apache/lucene/search/DisjunctionMaxQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/DisjunctionMaxQuery.java
@@ -45,7 +45,7 @@ import org.apache.lucene.util.Bits;
 public class DisjunctionMaxQuery extends Query implements Iterable<Query> {
 
   /* The subqueries */
-  private ArrayList<Query> disjuncts = new ArrayList<Query>();
+  private ArrayList<Query> disjuncts = new ArrayList<>();
 
   /* Multiple of the non-max disjunct scores added into our final score.  Non-zero values support tie-breaking. */
   private float tieBreakerMultiplier = 0.0f;
@@ -115,7 +115,7 @@ public class DisjunctionMaxQuery extends Query implements Iterable<Query> {
   protected class DisjunctionMaxWeight extends Weight {
 
     /** The Weights for our subqueries, in 1-1 correspondence with disjuncts */
-    protected ArrayList<Weight> weights = new ArrayList<Weight>();  // The Weight's for our subqueries, in 1-1 correspondence with disjuncts
+    protected ArrayList<Weight> weights = new ArrayList<>();  // The Weight's for our subqueries, in 1-1 correspondence with disjuncts
 
     /** Construct the Weight for this Query searched by searcher.  Recursively construct subquery weights. */
     public DisjunctionMaxWeight(IndexSearcher searcher) throws IOException {
@@ -155,7 +155,7 @@ public class DisjunctionMaxQuery extends Query implements Iterable<Query> {
     @Override
     public Scorer scorer(AtomicReaderContext context, boolean scoreDocsInOrder,
         boolean topScorer, Bits acceptDocs) throws IOException {
-      List<Scorer> scorers = new ArrayList<Scorer>();
+      List<Scorer> scorers = new ArrayList<>();
       for (Weight w : weights) {
         // we will advance() subscorers
         Scorer subScorer = w.scorer(context, true, false, acceptDocs);
diff --git lucene/core/src/java/org/apache/lucene/search/DisjunctionScorer.java lucene/core/src/java/org/apache/lucene/search/DisjunctionScorer.java
index 05522dd..31c1d10 100644
--- lucene/core/src/java/org/apache/lucene/search/DisjunctionScorer.java
+++ lucene/core/src/java/org/apache/lucene/search/DisjunctionScorer.java
@@ -102,7 +102,7 @@ abstract class DisjunctionScorer extends Scorer {
   
   @Override
   public final Collection<ChildScorer> getChildren() {
-    ArrayList<ChildScorer> children = new ArrayList<ChildScorer>(numScorers);
+    ArrayList<ChildScorer> children = new ArrayList<>(numScorers);
     for (int i = 0; i < numScorers; i++) {
       children.add(new ChildScorer(subScorers[i], "SHOULD"));
     }
diff --git lucene/core/src/java/org/apache/lucene/search/Explanation.java lucene/core/src/java/org/apache/lucene/search/Explanation.java
index ce30602..5dad05f 100644
--- lucene/core/src/java/org/apache/lucene/search/Explanation.java
+++ lucene/core/src/java/org/apache/lucene/search/Explanation.java
@@ -76,7 +76,7 @@ public class Explanation {
   /** Adds a sub-node to this explanation node. */
   public void addDetail(Explanation detail) {
     if (details == null)
-      details = new ArrayList<Explanation>();
+      details = new ArrayList<>();
     details.add(detail);
   }
 
diff --git lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.java lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.java
index fcafbbe..9da1de6 100644
--- lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.java
+++ lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.java
@@ -61,7 +61,7 @@ class FieldCacheImpl implements FieldCache {
   }
 
   private synchronized void init() {
-    caches = new HashMap<Class<?>,Cache>(9);
+    caches = new HashMap<>(9);
     caches.put(Integer.TYPE, new IntCache(this));
     caches.put(Float.TYPE, new FloatCache(this));
     caches.put(Long.TYPE, new LongCache(this));
@@ -86,7 +86,7 @@ class FieldCacheImpl implements FieldCache {
 
   @Override
   public synchronized CacheEntry[] getCacheEntries() {
-    List<CacheEntry> result = new ArrayList<CacheEntry>(17);
+    List<CacheEntry> result = new ArrayList<>(17);
     for(final Map.Entry<Class<?>,Cache> cacheEntry: caches.entrySet()) {
       final Cache cache = cacheEntry.getValue();
       final Class<?> cacheType = cacheEntry.getKey();
@@ -149,7 +149,7 @@ class FieldCacheImpl implements FieldCache {
 
     final FieldCacheImpl wrapper;
 
-    final Map<Object,Map<CacheKey,Object>> readerCache = new WeakHashMap<Object,Map<CacheKey,Object>>();
+    final Map<Object,Map<CacheKey,Object>> readerCache = new WeakHashMap<>();
     
     protected abstract Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField)
         throws IOException;
@@ -169,7 +169,7 @@ class FieldCacheImpl implements FieldCache {
         Map<CacheKey,Object> innerCache = readerCache.get(readerKey);
         if (innerCache == null) {
           // First time this reader is using FieldCache
-          innerCache = new HashMap<CacheKey,Object>();
+          innerCache = new HashMap<>();
           readerCache.put(readerKey, innerCache);
           wrapper.initReader(reader);
         }
@@ -190,7 +190,7 @@ class FieldCacheImpl implements FieldCache {
         innerCache = readerCache.get(readerKey);
         if (innerCache == null) {
           // First time this reader is using FieldCache
-          innerCache = new HashMap<CacheKey,Object>();
+          innerCache = new HashMap<>();
           readerCache.put(readerKey, innerCache);
           wrapper.initReader(reader);
           value = null;
@@ -438,7 +438,7 @@ class FieldCacheImpl implements FieldCache {
         return wrapper.getInts(reader, key.field, NUMERIC_UTILS_INT_PARSER, setDocsWithField);
       }
 
-      final HoldsOneThing<GrowableWriterAndMinValue> valuesRef = new HoldsOneThing<GrowableWriterAndMinValue>();
+      final HoldsOneThing<GrowableWriterAndMinValue> valuesRef = new HoldsOneThing<>();
 
       Uninvert u = new Uninvert() {
           private int minValue;
@@ -626,7 +626,7 @@ class FieldCacheImpl implements FieldCache {
         return wrapper.getFloats(reader, key.field, NUMERIC_UTILS_FLOAT_PARSER, setDocsWithField);
       }
 
-      final HoldsOneThing<float[]> valuesRef = new HoldsOneThing<float[]>();
+      final HoldsOneThing<float[]> valuesRef = new HoldsOneThing<>();
 
       Uninvert u = new Uninvert() {
           private float currentValue;
@@ -733,7 +733,7 @@ class FieldCacheImpl implements FieldCache {
         return wrapper.getLongs(reader, key.field, NUMERIC_UTILS_LONG_PARSER, setDocsWithField);
       }
 
-      final HoldsOneThing<GrowableWriterAndMinValue> valuesRef = new HoldsOneThing<GrowableWriterAndMinValue>();
+      final HoldsOneThing<GrowableWriterAndMinValue> valuesRef = new HoldsOneThing<>();
 
       Uninvert u = new Uninvert() {
           private long minValue;
@@ -851,7 +851,7 @@ class FieldCacheImpl implements FieldCache {
         return wrapper.getDoubles(reader, key.field, NUMERIC_UTILS_DOUBLE_PARSER, setDocsWithField);
       }
 
-      final HoldsOneThing<double[]> valuesRef = new HoldsOneThing<double[]>();
+      final HoldsOneThing<double[]> valuesRef = new HoldsOneThing<>();
 
       Uninvert u = new Uninvert() {
           private double currentValue;
diff --git lucene/core/src/java/org/apache/lucene/search/FieldValueHitQueue.java lucene/core/src/java/org/apache/lucene/search/FieldValueHitQueue.java
index a77c9f8..a4a73de 100644
--- lucene/core/src/java/org/apache/lucene/search/FieldValueHitQueue.java
+++ lucene/core/src/java/org/apache/lucene/search/FieldValueHitQueue.java
@@ -168,9 +168,9 @@ public abstract class FieldValueHitQueue<T extends FieldValueHitQueue.Entry> ext
     }
 
     if (fields.length == 1) {
-      return new OneComparatorFieldValueHitQueue<T>(fields, size);
+      return new OneComparatorFieldValueHitQueue<>(fields, size);
     } else {
-      return new MultiComparatorsFieldValueHitQueue<T>(fields, size);
+      return new MultiComparatorsFieldValueHitQueue<>(fields, size);
     }
   }
   
diff --git lucene/core/src/java/org/apache/lucene/search/FuzzyTermsEnum.java lucene/core/src/java/org/apache/lucene/search/FuzzyTermsEnum.java
index 8e2bf8b..6d73239 100644
--- lucene/core/src/java/org/apache/lucene/search/FuzzyTermsEnum.java
+++ lucene/core/src/java/org/apache/lucene/search/FuzzyTermsEnum.java
@@ -405,7 +405,7 @@ public class FuzzyTermsEnum extends TermsEnum {
    * Stores compiled automata as a list (indexed by edit distance)
    * @lucene.internal */
   public static final class LevenshteinAutomataAttributeImpl extends AttributeImpl implements LevenshteinAutomataAttribute {
-    private final List<CompiledAutomaton> automata = new ArrayList<CompiledAutomaton>();
+    private final List<CompiledAutomaton> automata = new ArrayList<>();
       
     @Override
     public List<CompiledAutomaton> automata() {
diff --git lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java
index 266a26e..9a07eea 100644
--- lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java
+++ lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java
@@ -441,7 +441,7 @@ public class IndexSearcher {
     } else {
       final HitQueue hq = new HitQueue(nDocs, false);
       final Lock lock = new ReentrantLock();
-      final ExecutionHelper<TopDocs> runner = new ExecutionHelper<TopDocs>(executor);
+      final ExecutionHelper<TopDocs> runner = new ExecutionHelper<>(executor);
     
       for (int i = 0; i < leafSlices.length; i++) { // search each sub
         runner.submit(new SearcherCallableNoSort(lock, this, leafSlices[i], weight, after, nDocs, hq));
@@ -532,7 +532,7 @@ public class IndexSearcher {
                                                                       false);
 
       final Lock lock = new ReentrantLock();
-      final ExecutionHelper<TopFieldDocs> runner = new ExecutionHelper<TopFieldDocs>(executor);
+      final ExecutionHelper<TopFieldDocs> runner = new ExecutionHelper<>(executor);
       for (int i = 0; i < leafSlices.length; i++) { // search each leaf slice
         runner.submit(
                       new SearcherCallableWithSort(lock, this, leafSlices[i], weight, after, nDocs, topCollector, sort, doDocScores, doMaxScore));
@@ -849,7 +849,7 @@ public class IndexSearcher {
     private int numTasks;
 
     ExecutionHelper(final Executor executor) {
-      this.service = new ExecutorCompletionService<T>(executor);
+      this.service = new ExecutorCompletionService<>(executor);
     }
 
     @Override
diff --git lucene/core/src/java/org/apache/lucene/search/LiveFieldValues.java lucene/core/src/java/org/apache/lucene/search/LiveFieldValues.java
index 76ffae3..ced9066 100644
--- lucene/core/src/java/org/apache/lucene/search/LiveFieldValues.java
+++ lucene/core/src/java/org/apache/lucene/search/LiveFieldValues.java
@@ -35,8 +35,8 @@ import java.util.concurrent.ConcurrentHashMap;
 
 public abstract class LiveFieldValues<S,T> implements ReferenceManager.RefreshListener, Closeable {
 
-  private volatile Map<String,T> current = new ConcurrentHashMap<String,T>();
-  private volatile Map<String,T> old = new ConcurrentHashMap<String,T>();
+  private volatile Map<String,T> current = new ConcurrentHashMap<>();
+  private volatile Map<String,T> old = new ConcurrentHashMap<>();
   private final ReferenceManager<S> mgr;
   private final T missingValue;
 
@@ -58,7 +58,7 @@ public abstract class LiveFieldValues<S,T> implements ReferenceManager.RefreshLi
     // map.  While reopen is running, any lookup will first
     // try this new map, then fallback to old, then to the
     // current searcher:
-    current = new ConcurrentHashMap<String,T>();
+    current = new ConcurrentHashMap<>();
   }
 
   @Override
@@ -69,7 +69,7 @@ public abstract class LiveFieldValues<S,T> implements ReferenceManager.RefreshLi
     // entries in it, which is fine: it means they were
     // actually already included in the previously opened
     // reader.  So we can safely clear old here:
-    old = new ConcurrentHashMap<String,T>();
+    old = new ConcurrentHashMap<>();
   }
 
   /** Call this after you've successfully added a document
diff --git lucene/core/src/java/org/apache/lucene/search/MinShouldMatchSumScorer.java lucene/core/src/java/org/apache/lucene/search/MinShouldMatchSumScorer.java
index 061ebcc..a4a2429 100644
--- lucene/core/src/java/org/apache/lucene/search/MinShouldMatchSumScorer.java
+++ lucene/core/src/java/org/apache/lucene/search/MinShouldMatchSumScorer.java
@@ -119,7 +119,7 @@ class MinShouldMatchSumScorer extends Scorer {
 
   @Override
   public final Collection<ChildScorer> getChildren() {
-    ArrayList<ChildScorer> children = new ArrayList<ChildScorer>(numScorers);
+    ArrayList<ChildScorer> children = new ArrayList<>(numScorers);
     for (int i = 0; i < numScorers; i++) {
       children.add(new ChildScorer(subScorers[i], "SHOULD"));
     }
diff --git lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java
index b48accc..3d15d58 100644
--- lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java
@@ -50,8 +50,8 @@ import org.apache.lucene.util.ToStringUtils;
  */
 public class MultiPhraseQuery extends Query {
   private String field;
-  private ArrayList<Term[]> termArrays = new ArrayList<Term[]>();
-  private ArrayList<Integer> positions = new ArrayList<Integer>();
+  private ArrayList<Term[]> termArrays = new ArrayList<>();
+  private ArrayList<Integer> positions = new ArrayList<>();
 
   private int slop = 0;
 
@@ -141,7 +141,7 @@ public class MultiPhraseQuery extends Query {
   private class MultiPhraseWeight extends Weight {
     private final Similarity similarity;
     private final Similarity.SimWeight stats;
-    private final Map<Term,TermContext> termContexts = new HashMap<Term,TermContext>();
+    private final Map<Term,TermContext> termContexts = new HashMap<>();
 
     public MultiPhraseWeight(IndexSearcher searcher)
       throws IOException {
@@ -149,7 +149,7 @@ public class MultiPhraseQuery extends Query {
       final IndexReaderContext context = searcher.getTopReaderContext();
       
       // compute idf
-      ArrayList<TermStatistics> allTermStats = new ArrayList<TermStatistics>();
+      ArrayList<TermStatistics> allTermStats = new ArrayList<>();
       for(final Term[] terms: termArrays) {
         for (Term term: terms) {
           TermContext termContext = termContexts.get(term);
@@ -480,7 +480,7 @@ class UnionDocsAndPositionsEnum extends DocsAndPositionsEnum {
   private long cost;
 
   public UnionDocsAndPositionsEnum(Bits liveDocs, AtomicReaderContext context, Term[] terms, Map<Term,TermContext> termContexts, TermsEnum termsEnum) throws IOException {
-    List<DocsAndPositionsEnum> docsEnums = new LinkedList<DocsAndPositionsEnum>();
+    List<DocsAndPositionsEnum> docsEnums = new LinkedList<>();
     for (int i = 0; i < terms.length; i++) {
       final Term term = terms[i];
       TermState termState = termContexts.get(term).get(context.ord);
diff --git lucene/core/src/java/org/apache/lucene/search/MultiTermQuery.java lucene/core/src/java/org/apache/lucene/search/MultiTermQuery.java
index a6ed25a..7fb8da6 100644
--- lucene/core/src/java/org/apache/lucene/search/MultiTermQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/MultiTermQuery.java
@@ -92,7 +92,7 @@ public abstract class MultiTermQuery extends Query {
   public static final RewriteMethod CONSTANT_SCORE_FILTER_REWRITE = new RewriteMethod() {
     @Override
     public Query rewrite(IndexReader reader, MultiTermQuery query) {
-      Query result = new ConstantScoreQuery(new MultiTermQueryWrapperFilter<MultiTermQuery>(query));
+      Query result = new ConstantScoreQuery(new MultiTermQueryWrapperFilter<>(query));
       result.setBoost(query.getBoost());
       return result;
     }
diff --git lucene/core/src/java/org/apache/lucene/search/NumericRangeFilter.java lucene/core/src/java/org/apache/lucene/search/NumericRangeFilter.java
index 7fd6699..96fe8a4 100644
--- lucene/core/src/java/org/apache/lucene/search/NumericRangeFilter.java
+++ lucene/core/src/java/org/apache/lucene/search/NumericRangeFilter.java
@@ -61,7 +61,7 @@ public final class NumericRangeFilter<T extends Number> extends MultiTermQueryWr
   public static NumericRangeFilter<Long> newLongRange(final String field, final int precisionStep,
     Long min, Long max, final boolean minInclusive, final boolean maxInclusive
   ) {
-    return new NumericRangeFilter<Long>(
+    return new NumericRangeFilter<>(
       NumericRangeQuery.newLongRange(field, precisionStep, min, max, minInclusive, maxInclusive)
     );
   }
@@ -76,7 +76,7 @@ public final class NumericRangeFilter<T extends Number> extends MultiTermQueryWr
   public static NumericRangeFilter<Long> newLongRange(final String field,
     Long min, Long max, final boolean minInclusive, final boolean maxInclusive
   ) {
-    return new NumericRangeFilter<Long>(
+    return new NumericRangeFilter<>(
       NumericRangeQuery.newLongRange(field, min, max, minInclusive, maxInclusive)
     );
   }
@@ -91,7 +91,7 @@ public final class NumericRangeFilter<T extends Number> extends MultiTermQueryWr
   public static NumericRangeFilter<Integer> newIntRange(final String field, final int precisionStep,
     Integer min, Integer max, final boolean minInclusive, final boolean maxInclusive
   ) {
-    return new NumericRangeFilter<Integer>(
+    return new NumericRangeFilter<>(
       NumericRangeQuery.newIntRange(field, precisionStep, min, max, minInclusive, maxInclusive)
     );
   }
@@ -106,7 +106,7 @@ public final class NumericRangeFilter<T extends Number> extends MultiTermQueryWr
   public static NumericRangeFilter<Integer> newIntRange(final String field,
     Integer min, Integer max, final boolean minInclusive, final boolean maxInclusive
   ) {
-    return new NumericRangeFilter<Integer>(
+    return new NumericRangeFilter<>(
       NumericRangeQuery.newIntRange(field, min, max, minInclusive, maxInclusive)
     );
   }
@@ -123,7 +123,7 @@ public final class NumericRangeFilter<T extends Number> extends MultiTermQueryWr
   public static NumericRangeFilter<Double> newDoubleRange(final String field, final int precisionStep,
     Double min, Double max, final boolean minInclusive, final boolean maxInclusive
   ) {
-    return new NumericRangeFilter<Double>(
+    return new NumericRangeFilter<>(
       NumericRangeQuery.newDoubleRange(field, precisionStep, min, max, minInclusive, maxInclusive)
     );
   }
@@ -140,7 +140,7 @@ public final class NumericRangeFilter<T extends Number> extends MultiTermQueryWr
   public static NumericRangeFilter<Double> newDoubleRange(final String field,
     Double min, Double max, final boolean minInclusive, final boolean maxInclusive
   ) {
-    return new NumericRangeFilter<Double>(
+    return new NumericRangeFilter<>(
       NumericRangeQuery.newDoubleRange(field, min, max, minInclusive, maxInclusive)
     );
   }
@@ -157,7 +157,7 @@ public final class NumericRangeFilter<T extends Number> extends MultiTermQueryWr
   public static NumericRangeFilter<Float> newFloatRange(final String field, final int precisionStep,
     Float min, Float max, final boolean minInclusive, final boolean maxInclusive
   ) {
-    return new NumericRangeFilter<Float>(
+    return new NumericRangeFilter<>(
       NumericRangeQuery.newFloatRange(field, precisionStep, min, max, minInclusive, maxInclusive)
     );
   }
@@ -174,7 +174,7 @@ public final class NumericRangeFilter<T extends Number> extends MultiTermQueryWr
   public static NumericRangeFilter<Float> newFloatRange(final String field,
     Float min, Float max, final boolean minInclusive, final boolean maxInclusive
   ) {
-    return new NumericRangeFilter<Float>(
+    return new NumericRangeFilter<>(
       NumericRangeQuery.newFloatRange(field, min, max, minInclusive, maxInclusive)
     );
   }
diff --git lucene/core/src/java/org/apache/lucene/search/NumericRangeQuery.java lucene/core/src/java/org/apache/lucene/search/NumericRangeQuery.java
index 1ba7030..102523e 100644
--- lucene/core/src/java/org/apache/lucene/search/NumericRangeQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/NumericRangeQuery.java
@@ -191,7 +191,7 @@ public final class NumericRangeQuery<T extends Number> extends MultiTermQuery {
   public static NumericRangeQuery<Long> newLongRange(final String field, final int precisionStep,
     Long min, Long max, final boolean minInclusive, final boolean maxInclusive
   ) {
-    return new NumericRangeQuery<Long>(field, precisionStep, NumericType.LONG, min, max, minInclusive, maxInclusive);
+    return new NumericRangeQuery<>(field, precisionStep, NumericType.LONG, min, max, minInclusive, maxInclusive);
   }
   
   /**
@@ -204,7 +204,7 @@ public final class NumericRangeQuery<T extends Number> extends MultiTermQuery {
   public static NumericRangeQuery<Long> newLongRange(final String field,
     Long min, Long max, final boolean minInclusive, final boolean maxInclusive
   ) {
-    return new NumericRangeQuery<Long>(field, NumericUtils.PRECISION_STEP_DEFAULT, NumericType.LONG, min, max, minInclusive, maxInclusive);
+    return new NumericRangeQuery<>(field, NumericUtils.PRECISION_STEP_DEFAULT, NumericType.LONG, min, max, minInclusive, maxInclusive);
   }
   
   /**
@@ -217,7 +217,7 @@ public final class NumericRangeQuery<T extends Number> extends MultiTermQuery {
   public static NumericRangeQuery<Integer> newIntRange(final String field, final int precisionStep,
     Integer min, Integer max, final boolean minInclusive, final boolean maxInclusive
   ) {
-    return new NumericRangeQuery<Integer>(field, precisionStep, NumericType.INT, min, max, minInclusive, maxInclusive);
+    return new NumericRangeQuery<>(field, precisionStep, NumericType.INT, min, max, minInclusive, maxInclusive);
   }
   
   /**
@@ -230,7 +230,7 @@ public final class NumericRangeQuery<T extends Number> extends MultiTermQuery {
   public static NumericRangeQuery<Integer> newIntRange(final String field,
     Integer min, Integer max, final boolean minInclusive, final boolean maxInclusive
   ) {
-    return new NumericRangeQuery<Integer>(field, NumericUtils.PRECISION_STEP_DEFAULT, NumericType.INT, min, max, minInclusive, maxInclusive);
+    return new NumericRangeQuery<>(field, NumericUtils.PRECISION_STEP_DEFAULT, NumericType.INT, min, max, minInclusive, maxInclusive);
   }
   
   /**
@@ -245,7 +245,7 @@ public final class NumericRangeQuery<T extends Number> extends MultiTermQuery {
   public static NumericRangeQuery<Double> newDoubleRange(final String field, final int precisionStep,
     Double min, Double max, final boolean minInclusive, final boolean maxInclusive
   ) {
-    return new NumericRangeQuery<Double>(field, precisionStep, NumericType.DOUBLE, min, max, minInclusive, maxInclusive);
+    return new NumericRangeQuery<>(field, precisionStep, NumericType.DOUBLE, min, max, minInclusive, maxInclusive);
   }
   
   /**
@@ -260,7 +260,7 @@ public final class NumericRangeQuery<T extends Number> extends MultiTermQuery {
   public static NumericRangeQuery<Double> newDoubleRange(final String field,
     Double min, Double max, final boolean minInclusive, final boolean maxInclusive
   ) {
-    return new NumericRangeQuery<Double>(field, NumericUtils.PRECISION_STEP_DEFAULT, NumericType.DOUBLE, min, max, minInclusive, maxInclusive);
+    return new NumericRangeQuery<>(field, NumericUtils.PRECISION_STEP_DEFAULT, NumericType.DOUBLE, min, max, minInclusive, maxInclusive);
   }
   
   /**
@@ -275,7 +275,7 @@ public final class NumericRangeQuery<T extends Number> extends MultiTermQuery {
   public static NumericRangeQuery<Float> newFloatRange(final String field, final int precisionStep,
     Float min, Float max, final boolean minInclusive, final boolean maxInclusive
   ) {
-    return new NumericRangeQuery<Float>(field, precisionStep, NumericType.FLOAT, min, max, minInclusive, maxInclusive);
+    return new NumericRangeQuery<>(field, precisionStep, NumericType.FLOAT, min, max, minInclusive, maxInclusive);
   }
   
   /**
@@ -290,7 +290,7 @@ public final class NumericRangeQuery<T extends Number> extends MultiTermQuery {
   public static NumericRangeQuery<Float> newFloatRange(final String field,
     Float min, Float max, final boolean minInclusive, final boolean maxInclusive
   ) {
-    return new NumericRangeQuery<Float>(field, NumericUtils.PRECISION_STEP_DEFAULT, NumericType.FLOAT, min, max, minInclusive, maxInclusive);
+    return new NumericRangeQuery<>(field, NumericUtils.PRECISION_STEP_DEFAULT, NumericType.FLOAT, min, max, minInclusive, maxInclusive);
   }
 
   @Override @SuppressWarnings("unchecked")
@@ -390,7 +390,7 @@ public final class NumericRangeQuery<T extends Number> extends MultiTermQuery {
 
     private BytesRef currentLowerBound, currentUpperBound;
 
-    private final LinkedList<BytesRef> rangeBounds = new LinkedList<BytesRef>();
+    private final LinkedList<BytesRef> rangeBounds = new LinkedList<>();
 
     NumericRangeTermsEnum(final TermsEnum tenum) {
       super(tenum);
diff --git lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java
index d8de142..0b84345 100644
--- lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java
@@ -46,8 +46,8 @@ import org.apache.lucene.util.ToStringUtils;
  */
 public class PhraseQuery extends Query {
   private String field;
-  private ArrayList<Term> terms = new ArrayList<Term>(4);
-  private ArrayList<Integer> positions = new ArrayList<Integer>(4);
+  private ArrayList<Term> terms = new ArrayList<>(4);
+  private ArrayList<Integer> positions = new ArrayList<>(4);
   private int maxPosition = 0;
   private int slop = 0;
 
diff --git lucene/core/src/java/org/apache/lucene/search/ReferenceManager.java lucene/core/src/java/org/apache/lucene/search/ReferenceManager.java
index 41383cb..2abb23e 100644
--- lucene/core/src/java/org/apache/lucene/search/ReferenceManager.java
+++ lucene/core/src/java/org/apache/lucene/search/ReferenceManager.java
@@ -47,7 +47,7 @@ public abstract class ReferenceManager<G> implements Closeable {
   
   private final Lock refreshLock = new ReentrantLock();
 
-  private final List<RefreshListener> refreshListeners = new CopyOnWriteArrayList<RefreshListener>();
+  private final List<RefreshListener> refreshListeners = new CopyOnWriteArrayList<>();
 
   private void ensureOpen() {
     if (current == null) {
diff --git lucene/core/src/java/org/apache/lucene/search/ReqOptSumScorer.java lucene/core/src/java/org/apache/lucene/search/ReqOptSumScorer.java
index 4e1c40c..7378e0a 100644
--- lucene/core/src/java/org/apache/lucene/search/ReqOptSumScorer.java
+++ lucene/core/src/java/org/apache/lucene/search/ReqOptSumScorer.java
@@ -94,7 +94,7 @@ class ReqOptSumScorer extends Scorer {
 
   @Override
   public Collection<ChildScorer> getChildren() {
-    ArrayList<ChildScorer> children = new ArrayList<ChildScorer>(2);
+    ArrayList<ChildScorer> children = new ArrayList<>(2);
     children.add(new ChildScorer(reqScorer, "MUST"));
     children.add(new ChildScorer(optScorer, "SHOULD"));
     return children;
diff --git lucene/core/src/java/org/apache/lucene/search/SearcherLifetimeManager.java lucene/core/src/java/org/apache/lucene/search/SearcherLifetimeManager.java
index 3d6cbb3..8a81156 100644
--- lucene/core/src/java/org/apache/lucene/search/SearcherLifetimeManager.java
+++ lucene/core/src/java/org/apache/lucene/search/SearcherLifetimeManager.java
@@ -132,7 +132,7 @@ public class SearcherLifetimeManager implements Closeable {
   // TODO: we could get by w/ just a "set"; need to have
   // Tracker hash by its version and have compareTo(Long)
   // compare to its version
-  private final ConcurrentHashMap<Long,SearcherTracker> searchers = new ConcurrentHashMap<Long,SearcherTracker>();
+  private final ConcurrentHashMap<Long,SearcherTracker> searchers = new ConcurrentHashMap<>();
 
   private void ensureOpen() {
     if (closed) {
@@ -246,7 +246,7 @@ public class SearcherLifetimeManager implements Closeable {
     // (not thread-safe since the values can change while
     // ArrayList is init'ing itself); must instead iterate
     // ourselves:
-    final List<SearcherTracker> trackers = new ArrayList<SearcherTracker>();
+    final List<SearcherTracker> trackers = new ArrayList<>();
     for(SearcherTracker tracker : searchers.values()) {
       trackers.add(tracker);
     }
@@ -285,7 +285,7 @@ public class SearcherLifetimeManager implements Closeable {
   @Override
   public synchronized void close() throws IOException {
     closed = true;
-    final List<SearcherTracker> toClose = new ArrayList<SearcherTracker>(searchers.values());
+    final List<SearcherTracker> toClose = new ArrayList<>(searchers.values());
 
     // Remove up front in case exc below, so we don't
     // over-decRef on double-close:
diff --git lucene/core/src/java/org/apache/lucene/search/SloppyPhraseScorer.java lucene/core/src/java/org/apache/lucene/search/SloppyPhraseScorer.java
index ed7a271..986ab06 100644
--- lucene/core/src/java/org/apache/lucene/search/SloppyPhraseScorer.java
+++ lucene/core/src/java/org/apache/lucene/search/SloppyPhraseScorer.java
@@ -379,7 +379,7 @@ final class SloppyPhraseScorer extends Scorer {
   /** Detect repetition groups. Done once - for first doc */
   private ArrayList<ArrayList<PhrasePositions>> gatherRptGroups(LinkedHashMap<Term,Integer> rptTerms) throws IOException {
     PhrasePositions[] rpp = repeatingPPs(rptTerms); 
-    ArrayList<ArrayList<PhrasePositions>> res = new ArrayList<ArrayList<PhrasePositions>>();
+    ArrayList<ArrayList<PhrasePositions>> res = new ArrayList<>();
     if (!hasMultiTermRpts) {
       // simpler - no multi-terms - can base on positions in first doc
       for (int i=0; i<rpp.length; i++) {
@@ -399,7 +399,7 @@ final class SloppyPhraseScorer extends Scorer {
           if (g < 0) {
             g = res.size();
             pp.rptGroup = g;  
-            ArrayList<PhrasePositions> rl = new ArrayList<PhrasePositions>(2);
+            ArrayList<PhrasePositions> rl = new ArrayList<>(2);
             rl.add(pp);
             res.add(rl); 
           }
@@ -409,11 +409,11 @@ final class SloppyPhraseScorer extends Scorer {
       }
     } else {
       // more involved - has multi-terms
-      ArrayList<HashSet<PhrasePositions>> tmp = new ArrayList<HashSet<PhrasePositions>>();
+      ArrayList<HashSet<PhrasePositions>> tmp = new ArrayList<>();
       ArrayList<FixedBitSet> bb = ppTermsBitSets(rpp, rptTerms);
       unionTermGroups(bb);
       HashMap<Term,Integer> tg = termGroups(rptTerms, bb);
-      HashSet<Integer> distinctGroupIDs = new HashSet<Integer>(tg.values());
+      HashSet<Integer> distinctGroupIDs = new HashSet<>(tg.values());
       for (int i=0; i<distinctGroupIDs.size(); i++) {
         tmp.add(new HashSet<PhrasePositions>());
       }
@@ -428,7 +428,7 @@ final class SloppyPhraseScorer extends Scorer {
         }
       }
       for (HashSet<PhrasePositions> hs : tmp) {
-        res.add(new ArrayList<PhrasePositions>(hs));
+        res.add(new ArrayList<>(hs));
       }
     }
     return res;
@@ -441,8 +441,8 @@ final class SloppyPhraseScorer extends Scorer {
 
   /** find repeating terms and assign them ordinal values */
   private LinkedHashMap<Term,Integer> repeatingTerms() {
-    LinkedHashMap<Term,Integer> tord = new LinkedHashMap<Term,Integer>();
-    HashMap<Term,Integer> tcnt = new HashMap<Term,Integer>();
+    LinkedHashMap<Term,Integer> tord = new LinkedHashMap<>();
+    HashMap<Term,Integer> tcnt = new HashMap<>();
     for (PhrasePositions pp=min,prev=null; prev!=max; pp=(prev=pp).next) { // iterate cyclic list: done once handled max
       for (Term t : pp.terms) {
         Integer cnt0 = tcnt.get(t);
@@ -458,7 +458,7 @@ final class SloppyPhraseScorer extends Scorer {
 
   /** find repeating pps, and for each, if has multi-terms, update this.hasMultiTermRpts */
   private PhrasePositions[] repeatingPPs(HashMap<Term,Integer> rptTerms) {
-    ArrayList<PhrasePositions> rp = new ArrayList<PhrasePositions>(); 
+    ArrayList<PhrasePositions> rp = new ArrayList<>();
     for (PhrasePositions pp=min,prev=null; prev!=max; pp=(prev=pp).next) { // iterate cyclic list: done once handled max
       for (Term t : pp.terms) {
         if (rptTerms.containsKey(t)) {
@@ -473,7 +473,7 @@ final class SloppyPhraseScorer extends Scorer {
   
   /** bit-sets - for each repeating pp, for each of its repeating terms, the term ordinal values is set */
   private ArrayList<FixedBitSet> ppTermsBitSets(PhrasePositions[] rpp, HashMap<Term,Integer> tord) {
-    ArrayList<FixedBitSet> bb = new ArrayList<FixedBitSet>(rpp.length);
+    ArrayList<FixedBitSet> bb = new ArrayList<>(rpp.length);
     for (PhrasePositions pp : rpp) {
       FixedBitSet b = new FixedBitSet(tord.size());
       Integer ord;
@@ -507,7 +507,7 @@ final class SloppyPhraseScorer extends Scorer {
   
   /** map each term to the single group that contains it */ 
   private HashMap<Term,Integer> termGroups(LinkedHashMap<Term,Integer> tord, ArrayList<FixedBitSet> bb) throws IOException {
-    HashMap<Term,Integer> tg = new HashMap<Term,Integer>();
+    HashMap<Term,Integer> tg = new HashMap<>();
     Term[] t = tord.keySet().toArray(new Term[0]);
     for (int i=0; i<bb.size(); i++) { // i is the group no.
       DocIdSetIterator bits = bb.get(i).iterator();
diff --git lucene/core/src/java/org/apache/lucene/search/TopTermsRewrite.java lucene/core/src/java/org/apache/lucene/search/TopTermsRewrite.java
index 0bbf3f3..9cefcc0 100644
--- lucene/core/src/java/org/apache/lucene/search/TopTermsRewrite.java
+++ lucene/core/src/java/org/apache/lucene/search/TopTermsRewrite.java
@@ -62,12 +62,12 @@ public abstract class TopTermsRewrite<Q extends Query> extends TermCollectingRew
   @Override
   public final Q rewrite(final IndexReader reader, final MultiTermQuery query) throws IOException {
     final int maxSize = Math.min(size, getMaxSize());
-    final PriorityQueue<ScoreTerm> stQueue = new PriorityQueue<ScoreTerm>();
+    final PriorityQueue<ScoreTerm> stQueue = new PriorityQueue<>();
     collectTerms(reader, query, new TermCollector() {
       private final MaxNonCompetitiveBoostAttribute maxBoostAtt =
         attributes.addAttribute(MaxNonCompetitiveBoostAttribute.class);
       
-      private final Map<BytesRef,ScoreTerm> visitedTerms = new HashMap<BytesRef,ScoreTerm>();
+      private final Map<BytesRef,ScoreTerm> visitedTerms = new HashMap<>();
       
       private TermsEnum termsEnum;
       private BoostAttribute boostAtt;        
diff --git lucene/core/src/java/org/apache/lucene/search/WildcardQuery.java lucene/core/src/java/org/apache/lucene/search/WildcardQuery.java
index a116ad6..12cd770 100644
--- lucene/core/src/java/org/apache/lucene/search/WildcardQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/WildcardQuery.java
@@ -63,7 +63,7 @@ public class WildcardQuery extends AutomatonQuery {
    */
   @SuppressWarnings("fallthrough")
   public static Automaton toAutomaton(Term wildcardquery) {
-    List<Automaton> automata = new ArrayList<Automaton>();
+    List<Automaton> automata = new ArrayList<>();
     
     String wildcardText = wildcardquery.text();
     
diff --git lucene/core/src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java lucene/core/src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java
index a307598..e48f4f9 100644
--- lucene/core/src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java
+++ lucene/core/src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java
@@ -74,7 +74,7 @@ public class PayloadSpanUtil {
    * @throws IOException if there is a low-level I/O error
    */
   public Collection<byte[]> getPayloadsForQuery(Query query) throws IOException {
-    Collection<byte[]> payloads = new ArrayList<byte[]>();
+    Collection<byte[]> payloads = new ArrayList<>();
     queryToSpanQuery(query, payloads);
     return payloads;
   }
@@ -143,7 +143,7 @@ public class PayloadSpanUtil {
           final Term[] termArray = termArrays.get(i);
           List<Query> disjuncts = disjunctLists[positions[i]];
           if (disjuncts == null) {
-            disjuncts = (disjunctLists[positions[i]] = new ArrayList<Query>(
+            disjuncts = (disjunctLists[positions[i]] = new ArrayList<>(
                 termArray.length));
             ++distinctPositions;
           }
@@ -178,8 +178,8 @@ public class PayloadSpanUtil {
 
   private void getPayloads(Collection<byte []> payloads, SpanQuery query)
       throws IOException {
-    Map<Term,TermContext> termContexts = new HashMap<Term,TermContext>();
-    TreeSet<Term> terms = new TreeSet<Term>();
+    Map<Term,TermContext> termContexts = new HashMap<>();
+    TreeSet<Term> terms = new TreeSet<>();
     query.extractTerms(terms);
     for (Term term : terms) {
       termContexts.put(term, TermContext.build(context, term));
diff --git lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java
index 95451f4..aea50c6 100644
--- lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java
+++ lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java
@@ -103,7 +103,7 @@ public class NearSpansOrdered extends Spans {
     allowedSlop = spanNearQuery.getSlop();
     SpanQuery[] clauses = spanNearQuery.getClauses();
     subSpans = new Spans[clauses.length];
-    matchPayload = new LinkedList<byte[]>();
+    matchPayload = new LinkedList<>();
     subSpansByDoc = new Spans[clauses.length];
     for (int i = 0; i < clauses.length; i++) {
       subSpans[i] = clauses[i].getSpans(context, acceptDocs, termContexts);
@@ -282,7 +282,7 @@ public class NearSpansOrdered extends Spans {
   private boolean shrinkToAfterShortestMatch() throws IOException {
     matchStart = subSpans[subSpans.length - 1].start();
     matchEnd = subSpans[subSpans.length - 1].end();
-    Set<byte[]> possibleMatchPayloads = new HashSet<byte[]>();
+    Set<byte[]> possibleMatchPayloads = new HashSet<>();
     if (subSpans[subSpans.length - 1].isPayloadAvailable()) {
       possibleMatchPayloads.addAll(subSpans[subSpans.length - 1].getPayload());
     }
@@ -296,7 +296,7 @@ public class NearSpansOrdered extends Spans {
       Spans prevSpans = subSpans[i];
       if (collectPayloads && prevSpans.isPayloadAvailable()) {
         Collection<byte[]> payload = prevSpans.getPayload();
-        possiblePayload = new ArrayList<byte[]>(payload.size());
+        possiblePayload = new ArrayList<>(payload.size());
         possiblePayload.addAll(payload);
       }
       
@@ -320,7 +320,7 @@ public class NearSpansOrdered extends Spans {
             prevEnd = ppEnd;
             if (collectPayloads && prevSpans.isPayloadAvailable()) {
               Collection<byte[]> payload = prevSpans.getPayload();
-              possiblePayload = new ArrayList<byte[]>(payload.size());
+              possiblePayload = new ArrayList<>(payload.size());
               possiblePayload.addAll(payload);
             }
           }
diff --git lucene/core/src/java/org/apache/lucene/search/spans/NearSpansUnordered.java lucene/core/src/java/org/apache/lucene/search/spans/NearSpansUnordered.java
index 8c3f989..544d932 100644
--- lucene/core/src/java/org/apache/lucene/search/spans/NearSpansUnordered.java
+++ lucene/core/src/java/org/apache/lucene/search/spans/NearSpansUnordered.java
@@ -40,7 +40,7 @@ import java.util.HashSet;
 public class NearSpansUnordered extends Spans {
   private SpanNearQuery query;
 
-  private List<SpansCell> ordered = new ArrayList<SpansCell>();         // spans in query order
+  private List<SpansCell> ordered = new ArrayList<>();         // spans in query order
   private Spans[] subSpans;  
   private int slop;                               // from query
 
@@ -121,7 +121,7 @@ public class NearSpansUnordered extends Spans {
                     // TODO: Remove warning after API has been finalized
     @Override
     public Collection<byte[]> getPayload() throws IOException {
-      return new ArrayList<byte[]>(spans.getPayload());
+      return new ArrayList<>(spans.getPayload());
     }
 
     // TODO: Remove warning after API has been finalized
@@ -250,7 +250,7 @@ public class NearSpansUnordered extends Spans {
    */
   @Override
   public Collection<byte[]> getPayload() throws IOException {
-    Set<byte[]> matchPayload = new HashSet<byte[]>();
+    Set<byte[]> matchPayload = new HashSet<>();
     for (SpansCell cell = first; cell != null; cell = cell.next) {
       if (cell.isPayloadAvailable()) {
         matchPayload.addAll(cell.getPayload());
diff --git lucene/core/src/java/org/apache/lucene/search/spans/SpanNearQuery.java lucene/core/src/java/org/apache/lucene/search/spans/SpanNearQuery.java
index c412f13..0774d35 100644
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanNearQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanNearQuery.java
@@ -61,7 +61,7 @@ public class SpanNearQuery extends SpanQuery implements Cloneable {
   public SpanNearQuery(SpanQuery[] clauses, int slop, boolean inOrder, boolean collectPayloads) {
 
     // copy clauses array into an ArrayList
-    this.clauses = new ArrayList<SpanQuery>(clauses.length);
+    this.clauses = new ArrayList<>(clauses.length);
     for (int i = 0; i < clauses.length; i++) {
       SpanQuery clause = clauses[i];
       if (field == null) {                               // check field
diff --git lucene/core/src/java/org/apache/lucene/search/spans/SpanNotQuery.java lucene/core/src/java/org/apache/lucene/search/spans/SpanNotQuery.java
index 055ced6..16ae1c6 100644
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanNotQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanNotQuery.java
@@ -175,7 +175,7 @@ public class SpanNotQuery extends SpanQuery implements Cloneable {
       public Collection<byte[]> getPayload() throws IOException {
         ArrayList<byte[]> result = null;
         if (includeSpans.isPayloadAvailable()) {
-          result = new ArrayList<byte[]>(includeSpans.getPayload());
+          result = new ArrayList<>(includeSpans.getPayload());
         }
         return result;
       }
diff --git lucene/core/src/java/org/apache/lucene/search/spans/SpanOrQuery.java lucene/core/src/java/org/apache/lucene/search/spans/SpanOrQuery.java
index d4ab76f..5f8bcbc 100644
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanOrQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanOrQuery.java
@@ -44,7 +44,7 @@ public class SpanOrQuery extends SpanQuery implements Cloneable {
   public SpanOrQuery(SpanQuery... clauses) {
 
     // copy clauses array into an ArrayList
-    this.clauses = new ArrayList<SpanQuery>(clauses.length);
+    this.clauses = new ArrayList<>(clauses.length);
     for (int i = 0; i < clauses.length; i++) {
       addClause(clauses[i]);
     }
@@ -242,7 +242,7 @@ public class SpanOrQuery extends SpanQuery implements Cloneable {
         ArrayList<byte[]> result = null;
         Spans theTop = top();
         if (theTop != null && theTop.isPayloadAvailable()) {
-          result = new ArrayList<byte[]>(theTop.getPayload());
+          result = new ArrayList<>(theTop.getPayload());
         }
         return result;
       }
diff --git lucene/core/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java lucene/core/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java
index edb1e62..7c0994e 100644
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java
@@ -169,7 +169,7 @@ public abstract class SpanPositionCheckQuery extends SpanQuery implements Clonea
     public Collection<byte[]> getPayload() throws IOException {
       ArrayList<byte[]> result = null;
       if (spans.isPayloadAvailable()) {
-        result = new ArrayList<byte[]>(spans.getPayload());
+        result = new ArrayList<>(spans.getPayload());
       }
       return result;//TODO: any way to avoid the new construction?
     }
diff --git lucene/core/src/java/org/apache/lucene/search/spans/SpanWeight.java lucene/core/src/java/org/apache/lucene/search/spans/SpanWeight.java
index fb73721..06d79b1 100644
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanWeight.java
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanWeight.java
@@ -45,8 +45,8 @@ public class SpanWeight extends Weight {
     this.similarity = searcher.getSimilarity();
     this.query = query;
     
-    termContexts = new HashMap<Term,TermContext>();
-    TreeSet<Term> terms = new TreeSet<Term>();
+    termContexts = new HashMap<>();
+    TreeSet<Term> terms = new TreeSet<>();
     query.extractTerms(terms);
     final IndexReaderContext context = searcher.getTopReaderContext();
     final TermStatistics termStats[] = new TermStatistics[terms.size()];
diff --git lucene/core/src/java/org/apache/lucene/store/CompoundFileDirectory.java lucene/core/src/java/org/apache/lucene/store/CompoundFileDirectory.java
index d95dca5..3f7ea84 100644
--- lucene/core/src/java/org/apache/lucene/store/CompoundFileDirectory.java
+++ lucene/core/src/java/org/apache/lucene/store/CompoundFileDirectory.java
@@ -130,7 +130,7 @@ public final class CompoundFileDirectory extends BaseDirectory {
       entriesStream = dir.openInput(entriesFileName, IOContext.READONCE);
       CodecUtil.checkHeader(entriesStream, CompoundFileWriter.ENTRY_CODEC, CompoundFileWriter.VERSION_START, CompoundFileWriter.VERSION_START);
       final int numEntries = entriesStream.readVInt();
-      final Map<String, FileEntry> mapping = new HashMap<String,FileEntry>(numEntries);
+      final Map<String, FileEntry> mapping = new HashMap<>(numEntries);
       for (int i = 0; i < numEntries; i++) {
         final FileEntry fileEntry = new FileEntry();
         final String id = entriesStream.readString();
diff --git lucene/core/src/java/org/apache/lucene/store/CompoundFileWriter.java lucene/core/src/java/org/apache/lucene/store/CompoundFileWriter.java
index 694bc11..f6da132 100644
--- lucene/core/src/java/org/apache/lucene/store/CompoundFileWriter.java
+++ lucene/core/src/java/org/apache/lucene/store/CompoundFileWriter.java
@@ -60,10 +60,10 @@ final class CompoundFileWriter implements Closeable{
   static final String ENTRY_CODEC = "CompoundFileWriterEntries";
 
   private final Directory directory;
-  private final Map<String, FileEntry> entries = new HashMap<String, FileEntry>();
-  private final Set<String> seenIDs = new HashSet<String>();
+  private final Map<String, FileEntry> entries = new HashMap<>();
+  private final Set<String> seenIDs = new HashSet<>();
   // all entries that are written to a sep. file but not yet moved into CFS
-  private final Queue<FileEntry> pendingEntries = new LinkedList<FileEntry>();
+  private final Queue<FileEntry> pendingEntries = new LinkedList<>();
   private boolean closed = false;
   private IndexOutput dataOut;
   private final AtomicBoolean outputTaken = new AtomicBoolean(false);
diff --git lucene/core/src/java/org/apache/lucene/store/DataInput.java lucene/core/src/java/org/apache/lucene/store/DataInput.java
index c5fe95a..6bcf18e 100644
--- lucene/core/src/java/org/apache/lucene/store/DataInput.java
+++ lucene/core/src/java/org/apache/lucene/store/DataInput.java
@@ -212,7 +212,7 @@ public abstract class DataInput implements Cloneable {
   /** Reads a Map&lt;String,String&gt; previously written
    *  with {@link DataOutput#writeStringStringMap(Map)}. */
   public Map<String,String> readStringStringMap() throws IOException {
-    final Map<String,String> map = new HashMap<String,String>();
+    final Map<String,String> map = new HashMap<>();
     final int count = readInt();
     for(int i=0;i<count;i++) {
       final String key = readString();
@@ -226,7 +226,7 @@ public abstract class DataInput implements Cloneable {
   /** Reads a Set&lt;String&gt; previously written
    *  with {@link DataOutput#writeStringSet(Set)}. */
   public Set<String> readStringSet() throws IOException {
-    final Set<String> set = new HashSet<String>();
+    final Set<String> set = new HashSet<>();
     final int count = readInt();
     for(int i=0;i<count;i++) {
       set.add(readString());
diff --git lucene/core/src/java/org/apache/lucene/store/FSDirectory.java lucene/core/src/java/org/apache/lucene/store/FSDirectory.java
index fba28a2..f03a90d 100644
--- lucene/core/src/java/org/apache/lucene/store/FSDirectory.java
+++ lucene/core/src/java/org/apache/lucene/store/FSDirectory.java
@@ -295,7 +295,7 @@ public abstract class FSDirectory extends BaseDirectory {
   @Override
   public void sync(Collection<String> names) throws IOException {
     ensureOpen();
-    Set<String> toSync = new HashSet<String>(names);
+    Set<String> toSync = new HashSet<>(names);
     toSync.retainAll(staleFiles);
 
     for (String name : toSync)
diff --git lucene/core/src/java/org/apache/lucene/store/FileSwitchDirectory.java lucene/core/src/java/org/apache/lucene/store/FileSwitchDirectory.java
index 9816703..bb7a257 100644
--- lucene/core/src/java/org/apache/lucene/store/FileSwitchDirectory.java
+++ lucene/core/src/java/org/apache/lucene/store/FileSwitchDirectory.java
@@ -77,7 +77,7 @@ public class FileSwitchDirectory extends BaseDirectory {
   
   @Override
   public String[] listAll() throws IOException {
-    Set<String> files = new HashSet<String>();
+    Set<String> files = new HashSet<>();
     // LUCENE-3380: either or both of our dirs could be FSDirs,
     // but if one underlying delegate is an FSDir and mkdirs() has not
     // yet been called, because so far everything is written to the other,
@@ -154,8 +154,8 @@ public class FileSwitchDirectory extends BaseDirectory {
 
   @Override
   public void sync(Collection<String> names) throws IOException {
-    List<String> primaryNames = new ArrayList<String>();
-    List<String> secondaryNames = new ArrayList<String>();
+    List<String> primaryNames = new ArrayList<>();
+    List<String> secondaryNames = new ArrayList<>();
 
     for (String name : names)
       if (primaryExtensions.contains(getExtension(name)))
diff --git lucene/core/src/java/org/apache/lucene/store/NRTCachingDirectory.java lucene/core/src/java/org/apache/lucene/store/NRTCachingDirectory.java
index c4c0924..3823d33 100644
--- lucene/core/src/java/org/apache/lucene/store/NRTCachingDirectory.java
+++ lucene/core/src/java/org/apache/lucene/store/NRTCachingDirectory.java
@@ -120,7 +120,7 @@ public class NRTCachingDirectory extends Directory {
 
   @Override
   public synchronized String[] listAll() throws IOException {
-    final Set<String> files = new HashSet<String>();
+    final Set<String> files = new HashSet<>();
     for(String f : cache.listAll()) {
       files.add(f);
     }
diff --git lucene/core/src/java/org/apache/lucene/store/NativeFSLockFactory.java lucene/core/src/java/org/apache/lucene/store/NativeFSLockFactory.java
index 60c6bf58..85c2a11 100644
--- lucene/core/src/java/org/apache/lucene/store/NativeFSLockFactory.java
+++ lucene/core/src/java/org/apache/lucene/store/NativeFSLockFactory.java
@@ -146,7 +146,7 @@ class NativeFSLock extends Lock {
    * (same FileChannel instance or not), so we may want to 
    * change this when Lucene moves to Java 1.6.
    */
-  private static HashSet<String> LOCK_HELD = new HashSet<String>();
+  private static HashSet<String> LOCK_HELD = new HashSet<>();
 
   public NativeFSLock(File lockDir, String lockFileName) {
     this.lockDir = lockDir;
diff --git lucene/core/src/java/org/apache/lucene/store/RAMDirectory.java lucene/core/src/java/org/apache/lucene/store/RAMDirectory.java
index 9ae0d36..cac6224 100644
--- lucene/core/src/java/org/apache/lucene/store/RAMDirectory.java
+++ lucene/core/src/java/org/apache/lucene/store/RAMDirectory.java
@@ -46,7 +46,7 @@ import java.util.concurrent.atomic.AtomicLong;
  * operating system, so copying data to Java heap space is not useful.
  */
 public class RAMDirectory extends BaseDirectory {
-  protected final Map<String,RAMFile> fileMap = new ConcurrentHashMap<String,RAMFile>();
+  protected final Map<String,RAMFile> fileMap = new ConcurrentHashMap<>();
   protected final AtomicLong sizeInBytes = new AtomicLong();
   
   // *****
@@ -113,7 +113,7 @@ public class RAMDirectory extends BaseDirectory {
     // NOTE: fileMap.keySet().toArray(new String[0]) is broken in non Sun JDKs,
     // and the code below is resilient to map changes during the array population.
     Set<String> fileNames = fileMap.keySet();
-    List<String> names = new ArrayList<String>(fileNames.size());
+    List<String> names = new ArrayList<>(fileNames.size());
     for (String name : fileNames) names.add(name);
     return names.toArray(new String[names.size()]);
   }
diff --git lucene/core/src/java/org/apache/lucene/store/RAMFile.java lucene/core/src/java/org/apache/lucene/store/RAMFile.java
index b89d308..1840ac5 100644
--- lucene/core/src/java/org/apache/lucene/store/RAMFile.java
+++ lucene/core/src/java/org/apache/lucene/store/RAMFile.java
@@ -23,7 +23,7 @@ import java.util.ArrayList;
  * Represents a file in RAM as a list of byte[] buffers.
  * @lucene.internal */
 public class RAMFile {
-  protected ArrayList<byte[]> buffers = new ArrayList<byte[]>();
+  protected ArrayList<byte[]> buffers = new ArrayList<>();
   long length;
   RAMDirectory directory;
   protected long sizeInBytes;
diff --git lucene/core/src/java/org/apache/lucene/store/SingleInstanceLockFactory.java lucene/core/src/java/org/apache/lucene/store/SingleInstanceLockFactory.java
index 29a8f19..4bc471e 100644
--- lucene/core/src/java/org/apache/lucene/store/SingleInstanceLockFactory.java
+++ lucene/core/src/java/org/apache/lucene/store/SingleInstanceLockFactory.java
@@ -33,7 +33,7 @@ import java.util.HashSet;
 
 public class SingleInstanceLockFactory extends LockFactory {
 
-  private HashSet<String> locks = new HashSet<String>();
+  private HashSet<String> locks = new HashSet<>();
 
   @Override
   public Lock makeLock(String lockName) {
diff --git lucene/core/src/java/org/apache/lucene/util/AttributeSource.java lucene/core/src/java/org/apache/lucene/util/AttributeSource.java
index 30575cc..83d87fa 100644
--- lucene/core/src/java/org/apache/lucene/util/AttributeSource.java
+++ lucene/core/src/java/org/apache/lucene/util/AttributeSource.java
@@ -145,8 +145,8 @@ public class AttributeSource {
    * An AttributeSource using the supplied {@link AttributeFactory} for creating new {@link Attribute} instances.
    */
   public AttributeSource(AttributeFactory factory) {
-    this.attributes = new LinkedHashMap<Class<? extends Attribute>, AttributeImpl>();
-    this.attributeImpls = new LinkedHashMap<Class<? extends AttributeImpl>, AttributeImpl>();
+    this.attributes = new LinkedHashMap<>();
+    this.attributeImpls = new LinkedHashMap<>();
     this.currentState = new State[1];
     this.factory = factory;
   }
@@ -207,7 +207,7 @@ public class AttributeSource {
     LinkedList<WeakReference<Class<? extends Attribute>>> foundInterfaces = knownImplClasses.get(clazz);
     if (foundInterfaces == null) {
       // we have the slight chance that another thread may do the same, but who cares?
-      foundInterfaces = new LinkedList<WeakReference<Class<? extends Attribute>>>();
+      foundInterfaces = new LinkedList<>();
       // find all interfaces that this attribute instance implements
       // and that extend the Attribute interface
       Class<?> actClazz = clazz;
diff --git lucene/core/src/java/org/apache/lucene/util/CloseableThreadLocal.java lucene/core/src/java/org/apache/lucene/util/CloseableThreadLocal.java
index e4436ce..9681f42 100644
--- lucene/core/src/java/org/apache/lucene/util/CloseableThreadLocal.java
+++ lucene/core/src/java/org/apache/lucene/util/CloseableThreadLocal.java
@@ -55,11 +55,11 @@ import java.util.concurrent.atomic.AtomicInteger;
 
 public class CloseableThreadLocal<T> implements Closeable {
 
-  private ThreadLocal<WeakReference<T>> t = new ThreadLocal<WeakReference<T>>();
+  private ThreadLocal<WeakReference<T>> t = new ThreadLocal<>();
 
   // Use a WeakHashMap so that if a Thread exits and is
   // GC'able, its entry may be removed:
-  private Map<Thread,T> hardRefs = new WeakHashMap<Thread,T>();
+  private Map<Thread,T> hardRefs = new WeakHashMap<>();
   
   // Increase this to decrease frequency of purging in get:
   private static int PURGE_MULTIPLIER = 20;
@@ -92,7 +92,7 @@ public class CloseableThreadLocal<T> implements Closeable {
 
   public void set(T object) {
 
-    t.set(new WeakReference<T>(object));
+    t.set(new WeakReference<>(object));
 
     synchronized(hardRefs) {
       hardRefs.put(Thread.currentThread(), object);
diff --git lucene/core/src/java/org/apache/lucene/util/DoubleBarrelLRUCache.java lucene/core/src/java/org/apache/lucene/util/DoubleBarrelLRUCache.java
index 14f2e44..0011550 100644
--- lucene/core/src/java/org/apache/lucene/util/DoubleBarrelLRUCache.java
+++ lucene/core/src/java/org/apache/lucene/util/DoubleBarrelLRUCache.java
@@ -59,8 +59,8 @@ final public class DoubleBarrelLRUCache<K extends DoubleBarrelLRUCache.Cloneable
   public DoubleBarrelLRUCache(int maxSize) {
     this.maxSize = maxSize;
     countdown = new AtomicInteger(maxSize);
-    cache1 = new ConcurrentHashMap<K,V>();
-    cache2 = new ConcurrentHashMap<K,V>();
+    cache1 = new ConcurrentHashMap<>();
+    cache2 = new ConcurrentHashMap<>();
   }
 
   @SuppressWarnings("unchecked") 
diff --git lucene/core/src/java/org/apache/lucene/util/FieldCacheSanityChecker.java lucene/core/src/java/org/apache/lucene/util/FieldCacheSanityChecker.java
index 5e27861..febc4e3 100644
--- lucene/core/src/java/org/apache/lucene/util/FieldCacheSanityChecker.java
+++ lucene/core/src/java/org/apache/lucene/util/FieldCacheSanityChecker.java
@@ -109,13 +109,13 @@ public final class FieldCacheSanityChecker {
     //
     // maps the (valId) identityhashCode of cache values to 
     // sets of CacheEntry instances
-    final MapOfSets<Integer, CacheEntry> valIdToItems = new MapOfSets<Integer, CacheEntry>(new HashMap<Integer, Set<CacheEntry>>(17));
+    final MapOfSets<Integer, CacheEntry> valIdToItems = new MapOfSets<>(new HashMap<Integer, Set<CacheEntry>>(17));
     // maps ReaderField keys to Sets of ValueIds
-    final MapOfSets<ReaderField, Integer> readerFieldToValIds = new MapOfSets<ReaderField, Integer>(new HashMap<ReaderField, Set<Integer>>(17));
+    final MapOfSets<ReaderField, Integer> readerFieldToValIds = new MapOfSets<>(new HashMap<ReaderField, Set<Integer>>(17));
     //
 
     // any keys that we know result in more then one valId
-    final Set<ReaderField> valMismatchKeys = new HashSet<ReaderField>();
+    final Set<ReaderField> valMismatchKeys = new HashSet<>();
 
     // iterate over all the cacheEntries to get the mappings we'll need
     for (int i = 0; i < cacheEntries.length; i++) {
@@ -144,7 +144,7 @@ public final class FieldCacheSanityChecker {
       }
     }
 
-    final List<Insanity> insanity = new ArrayList<Insanity>(valMismatchKeys.size() * 3);
+    final List<Insanity> insanity = new ArrayList<>(valMismatchKeys.size() * 3);
 
     insanity.addAll(checkValueMismatch(valIdToItems, 
                                        readerFieldToValIds, 
@@ -166,7 +166,7 @@ public final class FieldCacheSanityChecker {
                                         MapOfSets<ReaderField, Integer> readerFieldToValIds,
                                         Set<ReaderField> valMismatchKeys) {
 
-    final List<Insanity> insanity = new ArrayList<Insanity>(valMismatchKeys.size() * 3);
+    final List<Insanity> insanity = new ArrayList<>(valMismatchKeys.size() * 3);
 
     if (! valMismatchKeys.isEmpty() ) { 
       // we have multiple values for some ReaderFields
@@ -174,7 +174,7 @@ public final class FieldCacheSanityChecker {
       final Map<ReaderField, Set<Integer>> rfMap = readerFieldToValIds.getMap();
       final Map<Integer, Set<CacheEntry>> valMap = valIdToItems.getMap();
       for (final ReaderField rf : valMismatchKeys) {
-        final List<CacheEntry> badEntries = new ArrayList<CacheEntry>(valMismatchKeys.size() * 2);
+        final List<CacheEntry> badEntries = new ArrayList<>(valMismatchKeys.size() * 2);
         for(final Integer value: rfMap.get(rf)) {
           for (final CacheEntry cacheEntry : valMap.get(value)) {
             badEntries.add(cacheEntry);
@@ -203,15 +203,15 @@ public final class FieldCacheSanityChecker {
   private Collection<Insanity> checkSubreaders( MapOfSets<Integer, CacheEntry>  valIdToItems,
                                       MapOfSets<ReaderField, Integer> readerFieldToValIds) {
 
-    final List<Insanity> insanity = new ArrayList<Insanity>(23);
+    final List<Insanity> insanity = new ArrayList<>(23);
 
-    Map<ReaderField, Set<ReaderField>> badChildren = new HashMap<ReaderField, Set<ReaderField>>(17);
-    MapOfSets<ReaderField, ReaderField> badKids = new MapOfSets<ReaderField, ReaderField>(badChildren); // wrapper
+    Map<ReaderField, Set<ReaderField>> badChildren = new HashMap<>(17);
+    MapOfSets<ReaderField, ReaderField> badKids = new MapOfSets<>(badChildren); // wrapper
 
     Map<Integer, Set<CacheEntry>> viToItemSets = valIdToItems.getMap();
     Map<ReaderField, Set<Integer>> rfToValIdSets = readerFieldToValIds.getMap();
 
-    Set<ReaderField> seen = new HashSet<ReaderField>(17);
+    Set<ReaderField> seen = new HashSet<>(17);
 
     Set<ReaderField> readerFields = rfToValIdSets.keySet();
     for (final ReaderField rf : readerFields) {
@@ -242,7 +242,7 @@ public final class FieldCacheSanityChecker {
     for (final ReaderField parent : badChildren.keySet()) {
       Set<ReaderField> kids = badChildren.get(parent);
 
-      List<CacheEntry> badEntries = new ArrayList<CacheEntry>(kids.size() * 2);
+      List<CacheEntry> badEntries = new ArrayList<>(kids.size() * 2);
 
       // put parent entr(ies) in first
       {
@@ -277,7 +277,7 @@ public final class FieldCacheSanityChecker {
    * returned by {@code seed.getCoreCacheKey()}
    */
   private List<Object> getAllDescendantReaderKeys(Object seed) {
-    List<Object> all = new ArrayList<Object>(17); // will grow as we iter
+    List<Object> all = new ArrayList<>(17); // will grow as we iter
     all.add(seed);
     for (int i = 0; i < all.size(); i++) {
       final Object obj = all.get(i);
diff --git lucene/core/src/java/org/apache/lucene/util/MapOfSets.java lucene/core/src/java/org/apache/lucene/util/MapOfSets.java
index b08eb76..e814495 100644
--- lucene/core/src/java/org/apache/lucene/util/MapOfSets.java
+++ lucene/core/src/java/org/apache/lucene/util/MapOfSets.java
@@ -55,7 +55,7 @@ public class MapOfSets<K, V> {
     if (theMap.containsKey(key)) {
       theSet = theMap.get(key);
     } else {
-      theSet = new HashSet<V>(23);
+      theSet = new HashSet<>(23);
       theMap.put(key, theSet);
     }
     theSet.add(val);
@@ -72,7 +72,7 @@ public class MapOfSets<K, V> {
     if (theMap.containsKey(key)) {
       theSet = theMap.get(key);
     } else {
-      theSet = new HashSet<V>(23);
+      theSet = new HashSet<>(23);
       theMap.put(key, theSet);
     }
     theSet.addAll(vals);
diff --git lucene/core/src/java/org/apache/lucene/util/MergedIterator.java lucene/core/src/java/org/apache/lucene/util/MergedIterator.java
index 9a2770c..082a68d 100644
--- lucene/core/src/java/org/apache/lucene/util/MergedIterator.java
+++ lucene/core/src/java/org/apache/lucene/util/MergedIterator.java
@@ -59,12 +59,12 @@ public final class MergedIterator<T extends Comparable<T>> implements Iterator<T
   @SuppressWarnings({"unchecked","rawtypes"})
   public MergedIterator(boolean removeDuplicates, Iterator<T>... iterators) {
     this.removeDuplicates = removeDuplicates;
-    queue = new TermMergeQueue<T>(iterators.length);
+    queue = new TermMergeQueue<>(iterators.length);
     top = new SubIterator[iterators.length];
     int index = 0;
     for (Iterator<T> iterator : iterators) {
       if (iterator.hasNext()) {
-        SubIterator<T> sub = new SubIterator<T>();
+        SubIterator<T> sub = new SubIterator<>();
         sub.current = iterator.next();
         sub.iterator = iterator;
         sub.index = index++;
diff --git lucene/core/src/java/org/apache/lucene/util/NamedSPILoader.java lucene/core/src/java/org/apache/lucene/util/NamedSPILoader.java
index 40caaf9..d41cfeb 100644
--- lucene/core/src/java/org/apache/lucene/util/NamedSPILoader.java
+++ lucene/core/src/java/org/apache/lucene/util/NamedSPILoader.java
@@ -59,7 +59,7 @@ public final class NamedSPILoader<S extends NamedSPILoader.NamedSPI> implements
    * of new service providers on the given classpath/classloader!</em>
    */
   public synchronized void reload(ClassLoader classloader) {
-    final LinkedHashMap<String,S> services = new LinkedHashMap<String,S>(this.services);
+    final LinkedHashMap<String,S> services = new LinkedHashMap<>(this.services);
     final SPIClassIterator<S> loader = SPIClassIterator.get(clazz, classloader);
     while (loader.hasNext()) {
       final Class<? extends S> c = loader.next();
diff --git lucene/core/src/java/org/apache/lucene/util/OfflineSorter.java lucene/core/src/java/org/apache/lucene/util/OfflineSorter.java
index 76781f8..3d5119c 100644
--- lucene/core/src/java/org/apache/lucene/util/OfflineSorter.java
+++ lucene/core/src/java/org/apache/lucene/util/OfflineSorter.java
@@ -225,7 +225,7 @@ public final class OfflineSorter {
 
     output.delete();
 
-    ArrayList<File> merges = new ArrayList<File>();
+    ArrayList<File> merges = new ArrayList<>();
     boolean success2 = false;
     try {
       ByteSequencesReader is = new ByteSequencesReader(input);
diff --git lucene/core/src/java/org/apache/lucene/util/PagedBytes.java lucene/core/src/java/org/apache/lucene/util/PagedBytes.java
index 99504e6..5d5f1a1 100644
--- lucene/core/src/java/org/apache/lucene/util/PagedBytes.java
+++ lucene/core/src/java/org/apache/lucene/util/PagedBytes.java
@@ -35,9 +35,9 @@ import org.apache.lucene.store.IndexInput;
 // TODO: refactor this, byteblockpool, fst.bytestore, and any
 // other "shift/mask big arrays". there are too many of these classes!
 public final class PagedBytes {
-  private final List<byte[]> blocks = new ArrayList<byte[]>();
+  private final List<byte[]> blocks = new ArrayList<>();
   // TODO: these are unused?
-  private final List<Integer> blockEnd = new ArrayList<Integer>();
+  private final List<Integer> blockEnd = new ArrayList<>();
   private final int blockSize;
   private final int blockBits;
   private final int blockMask;
diff --git lucene/core/src/java/org/apache/lucene/util/QueryBuilder.java lucene/core/src/java/org/apache/lucene/util/QueryBuilder.java
index aedf14a..9e6ae7d 100644
--- lucene/core/src/java/org/apache/lucene/util/QueryBuilder.java
+++ lucene/core/src/java/org/apache/lucene/util/QueryBuilder.java
@@ -305,7 +305,7 @@ public class QueryBuilder {
           // phrase query:
           MultiPhraseQuery mpq = newMultiPhraseQuery();
           mpq.setSlop(phraseSlop);
-          List<Term> multiTerms = new ArrayList<Term>();
+          List<Term> multiTerms = new ArrayList<>();
           int position = -1;
           for (int i = 0; i < numTokens; i++) {
             int positionIncrement = 1;
diff --git lucene/core/src/java/org/apache/lucene/util/RamUsageEstimator.java lucene/core/src/java/org/apache/lucene/util/RamUsageEstimator.java
index 06cad08..4176539 100644
--- lucene/core/src/java/org/apache/lucene/util/RamUsageEstimator.java
+++ lucene/core/src/java/org/apache/lucene/util/RamUsageEstimator.java
@@ -105,7 +105,7 @@ public final class RamUsageEstimator {
    */
   private static final Map<Class<?>,Integer> primitiveSizes;
   static {
-    primitiveSizes = new IdentityHashMap<Class<?>,Integer>();
+    primitiveSizes = new IdentityHashMap<>();
     primitiveSizes.put(boolean.class, Integer.valueOf(NUM_BYTES_BOOLEAN));
     primitiveSizes.put(byte.class, Integer.valueOf(NUM_BYTES_BYTE));
     primitiveSizes.put(char.class, Integer.valueOf(NUM_BYTES_CHAR));
@@ -403,11 +403,11 @@ public final class RamUsageEstimator {
    */
   private static long measureObjectSize(Object root) {
     // Objects seen so far.
-    final IdentityHashSet<Object> seen = new IdentityHashSet<Object>();
+    final IdentityHashSet<Object> seen = new IdentityHashSet<>();
     // Class cache with reference Field and precalculated shallow size. 
-    final IdentityHashMap<Class<?>, ClassCache> classCache = new IdentityHashMap<Class<?>, ClassCache>();
+    final IdentityHashMap<Class<?>, ClassCache> classCache = new IdentityHashMap<>();
     // Stack of objects pending traversal. Recursion caused stack overflows. 
-    final ArrayList<Object> stack = new ArrayList<Object>();
+    final ArrayList<Object> stack = new ArrayList<>();
     stack.add(root);
 
     long totalSize = 0;
@@ -486,7 +486,7 @@ public final class RamUsageEstimator {
   private static ClassCache createCacheEntry(final Class<?> clazz) {
     ClassCache cachedInfo;
     long shallowInstanceSize = NUM_BYTES_OBJECT_HEADER;
-    final ArrayList<Field> referenceFields = new ArrayList<Field>(32);
+    final ArrayList<Field> referenceFields = new ArrayList<>(32);
     for (Class<?> c = clazz; c != null; c = c.getSuperclass()) {
       final Field[] fields = c.getDeclaredFields();
       for (final Field f : fields) {
diff --git lucene/core/src/java/org/apache/lucene/util/SPIClassIterator.java lucene/core/src/java/org/apache/lucene/util/SPIClassIterator.java
index 0cfa851..764713c 100644
--- lucene/core/src/java/org/apache/lucene/util/SPIClassIterator.java
+++ lucene/core/src/java/org/apache/lucene/util/SPIClassIterator.java
@@ -47,11 +47,11 @@ public final class SPIClassIterator<S> implements Iterator<Class<? extends S>> {
   private Iterator<String> linesIterator;
   
   public static <S> SPIClassIterator<S> get(Class<S> clazz) {
-    return new SPIClassIterator<S>(clazz, Thread.currentThread().getContextClassLoader());
+    return new SPIClassIterator<>(clazz, Thread.currentThread().getContextClassLoader());
   }
   
   public static <S> SPIClassIterator<S> get(Class<S> clazz, ClassLoader loader) {
-    return new SPIClassIterator<S>(clazz, loader);
+    return new SPIClassIterator<>(clazz, loader);
   }
   
   /** Utility method to check if some class loader is a (grand-)parent of or the same as another one.
@@ -84,7 +84,7 @@ public final class SPIClassIterator<S> implements Iterator<Class<? extends S>> {
       if (lines != null) {
         lines.clear();
       } else {
-        lines = new ArrayList<String>();
+        lines = new ArrayList<>();
       }
       final URL url = profilesEnum.nextElement();
       try {
diff --git lucene/core/src/java/org/apache/lucene/util/SetOnce.java lucene/core/src/java/org/apache/lucene/util/SetOnce.java
index 74c2fa8..5b3cfd0 100644
--- lucene/core/src/java/org/apache/lucene/util/SetOnce.java
+++ lucene/core/src/java/org/apache/lucene/util/SetOnce.java
@@ -77,7 +77,7 @@ public final class SetOnce<T> implements Cloneable {
   
   @Override
   public SetOnce<T> clone() {
-    return obj == null ? new SetOnce<T>() : new SetOnce<T>(obj);
+    return obj == null ? new SetOnce<T>() : new SetOnce<>(obj);
   }
   
 }
diff --git lucene/core/src/java/org/apache/lucene/util/WeakIdentityMap.java lucene/core/src/java/org/apache/lucene/util/WeakIdentityMap.java
index d1fbaaf..3ac4297 100644
--- lucene/core/src/java/org/apache/lucene/util/WeakIdentityMap.java
+++ lucene/core/src/java/org/apache/lucene/util/WeakIdentityMap.java
@@ -62,7 +62,7 @@ import java.util.concurrent.ConcurrentHashMap;
  * @lucene.internal
  */
 public final class WeakIdentityMap<K,V> {
-  private final ReferenceQueue<Object> queue = new ReferenceQueue<Object>();
+  private final ReferenceQueue<Object> queue = new ReferenceQueue<>();
   private final Map<IdentityWeakReference, V> backingStore;
   private final boolean reapOnRead;
 
@@ -80,7 +80,7 @@ public final class WeakIdentityMap<K,V> {
    * @param reapOnRead controls if the map <a href="#reapInfo">cleans up the reference queue on every read operation</a>.
    */
   public static <K,V> WeakIdentityMap<K,V> newHashMap(boolean reapOnRead) {
-    return new WeakIdentityMap<K,V>(new HashMap<IdentityWeakReference,V>(), reapOnRead);
+    return new WeakIdentityMap<>(new HashMap<IdentityWeakReference,V>(), reapOnRead);
   }
 
   /**
@@ -96,7 +96,7 @@ public final class WeakIdentityMap<K,V> {
    * @param reapOnRead controls if the map <a href="#reapInfo">cleans up the reference queue on every read operation</a>.
    */
   public static <K,V> WeakIdentityMap<K,V> newConcurrentHashMap(boolean reapOnRead) {
-    return new WeakIdentityMap<K,V>(new ConcurrentHashMap<IdentityWeakReference,V>(), reapOnRead);
+    return new WeakIdentityMap<>(new ConcurrentHashMap<IdentityWeakReference,V>(), reapOnRead);
   }
 
   /** Private only constructor, to create use the static factory methods. */
diff --git lucene/core/src/java/org/apache/lucene/util/automaton/Automaton.java lucene/core/src/java/org/apache/lucene/util/automaton/Automaton.java
index f943489..8ad8bc0 100644
--- lucene/core/src/java/org/apache/lucene/util/automaton/Automaton.java
+++ lucene/core/src/java/org/apache/lucene/util/automaton/Automaton.java
@@ -269,8 +269,8 @@ public class Automaton implements Cloneable {
   public State[] getNumberedStates() {
     if (numberedStates == null) {
       expandSingleton();
-      final Set<State> visited = new HashSet<State>();
-      final LinkedList<State> worklist = new LinkedList<State>();
+      final Set<State> visited = new HashSet<>();
+      final LinkedList<State> worklist = new LinkedList<>();
       numberedStates = new State[4];
       int upto = 0;
       worklist.add(initial);
@@ -333,9 +333,9 @@ public class Automaton implements Cloneable {
    */
   public Set<State> getAcceptStates() {
     expandSingleton();
-    HashSet<State> accepts = new HashSet<State>();
-    HashSet<State> visited = new HashSet<State>();
-    LinkedList<State> worklist = new LinkedList<State>();
+    HashSet<State> accepts = new HashSet<>();
+    HashSet<State> visited = new HashSet<>();
+    LinkedList<State> worklist = new LinkedList<>();
     worklist.add(initial);
     visited.add(initial);
     while (worklist.size() > 0) {
@@ -399,7 +399,7 @@ public class Automaton implements Cloneable {
    */
   int[] getStartPoints() {
     final State[] states = getNumberedStates();
-    Set<Integer> pointset = new HashSet<Integer>();
+    Set<Integer> pointset = new HashSet<>();
     pointset.add(Character.MIN_CODE_POINT);
     for (State s : states) {
       for (Transition t : s.getTransitions()) {
@@ -423,7 +423,7 @@ public class Automaton implements Cloneable {
    */
   private State[] getLiveStates() {
     final State[] states = getNumberedStates();
-    Set<State> live = new HashSet<State>();
+    Set<State> live = new HashSet<>();
     for (State q : states) {
       if (q.isAccept()) {
         live.add(q);
@@ -432,13 +432,13 @@ public class Automaton implements Cloneable {
     // map<state, set<state>>
     @SuppressWarnings({"rawtypes","unchecked"}) Set<State> map[] = new Set[states.length];
     for (int i = 0; i < map.length; i++)
-      map[i] = new HashSet<State>();
+      map[i] = new HashSet<>();
     for (State s : states) {
       for(int i=0;i<s.numTransitions;i++) {
         map[s.transitionsArray[i].to.number].add(s);
       }
     }
-    LinkedList<State> worklist = new LinkedList<State>(live);
+    LinkedList<State> worklist = new LinkedList<>(live);
     while (worklist.size() > 0) {
       State s = worklist.removeFirst();
       for (State p : map[s.number])
@@ -639,7 +639,7 @@ public class Automaton implements Cloneable {
     try {
       Automaton a = (Automaton) super.clone();
       if (!isSingleton()) {
-        HashMap<State,State> m = new HashMap<State,State>();
+        HashMap<State,State> m = new HashMap<>();
         State[] states = getNumberedStates();
         for (State s : states)
           m.put(s, new State());
diff --git lucene/core/src/java/org/apache/lucene/util/automaton/BasicAutomata.java lucene/core/src/java/org/apache/lucene/util/automaton/BasicAutomata.java
index 0a793a6..7f51a37 100644
--- lucene/core/src/java/org/apache/lucene/util/automaton/BasicAutomata.java
+++ lucene/core/src/java/org/apache/lucene/util/automaton/BasicAutomata.java
@@ -216,10 +216,10 @@ final public class BasicAutomata {
       by.append('0');
     by.append(y);
     y = by.toString();
-    Collection<State> initials = new ArrayList<State>();
+    Collection<State> initials = new ArrayList<>();
     a.initial = between(x, y, 0, initials, digits <= 0);
     if (digits <= 0) {
-      ArrayList<StatePair> pairs = new ArrayList<StatePair>();
+      ArrayList<StatePair> pairs = new ArrayList<>();
       for (State p : initials)
         if (a.initial != p) pairs.add(new StatePair(a.initial, p));
       BasicOperations.addEpsilons(a, pairs);
diff --git lucene/core/src/java/org/apache/lucene/util/automaton/BasicOperations.java lucene/core/src/java/org/apache/lucene/util/automaton/BasicOperations.java
index 781a4e2..1d95358 100644
--- lucene/core/src/java/org/apache/lucene/util/automaton/BasicOperations.java
+++ lucene/core/src/java/org/apache/lucene/util/automaton/BasicOperations.java
@@ -106,7 +106,7 @@ final public class BasicOperations {
     } else {
       for (Automaton a : l)
         if (BasicOperations.isEmpty(a)) return BasicAutomata.makeEmpty();
-      Set<Integer> ids = new HashSet<Integer>();
+      Set<Integer> ids = new HashSet<>();
       for (Automaton a : l)
         ids.add(System.identityHashCode(a));
       boolean has_aliases = ids.size() != l.size();
@@ -187,7 +187,7 @@ final public class BasicOperations {
    */
   static public Automaton repeat(Automaton a, int min) {
     if (min == 0) return repeat(a);
-    List<Automaton> as = new ArrayList<Automaton>();
+    List<Automaton> as = new ArrayList<>();
     while (min-- > 0)
       as.add(a);
     as.add(repeat(a));
@@ -210,7 +210,7 @@ final public class BasicOperations {
     if (min == 0) b = BasicAutomata.makeEmptyString();
     else if (min == 1) b = a.clone();
     else {
-      List<Automaton> as = new ArrayList<Automaton>();
+      List<Automaton> as = new ArrayList<>();
       while (min-- > 0)
         as.add(a);
       b = concatenate(as);
@@ -287,8 +287,8 @@ final public class BasicOperations {
     Transition[][] transitions1 = a1.getSortedTransitions();
     Transition[][] transitions2 = a2.getSortedTransitions();
     Automaton c = new Automaton();
-    LinkedList<StatePair> worklist = new LinkedList<StatePair>();
-    HashMap<StatePair,StatePair> newstates = new HashMap<StatePair,StatePair>();
+    LinkedList<StatePair> worklist = new LinkedList<>();
+    HashMap<StatePair,StatePair> newstates = new HashMap<>();
     StatePair p = new StatePair(c.initial, a1.initial, a2.initial);
     worklist.add(p);
     newstates.put(p, p);
@@ -356,8 +356,8 @@ final public class BasicOperations {
     a2.determinize();
     Transition[][] transitions1 = a1.getSortedTransitions();
     Transition[][] transitions2 = a2.getSortedTransitions();
-    LinkedList<StatePair> worklist = new LinkedList<StatePair>();
-    HashSet<StatePair> visited = new HashSet<StatePair>();
+    LinkedList<StatePair> worklist = new LinkedList<>();
+    HashSet<StatePair> visited = new HashSet<>();
     StatePair p = new StatePair(a1.initial, a2.initial);
     worklist.add(p);
     visited.add(p);
@@ -431,7 +431,7 @@ final public class BasicOperations {
    * Complexity: linear in number of states.
    */
   public static Automaton union(Collection<Automaton> l) {
-    Set<Integer> ids = new HashSet<Integer>();
+    Set<Integer> ids = new HashSet<>();
     for (Automaton a : l)
       ids.add(System.identityHashCode(a));
     boolean has_aliases = ids.size() != l.size();
@@ -500,7 +500,7 @@ final public class BasicOperations {
     PointTransitions[] points = new PointTransitions[5];
 
     private final static int HASHMAP_CUTOVER = 30;
-    private final HashMap<Integer,PointTransitions> map = new HashMap<Integer,PointTransitions>();
+    private final HashMap<Integer,PointTransitions> map = new HashMap<>();
     private boolean useHash = false;
 
     private PointTransitions next(int point) {
@@ -597,8 +597,8 @@ final public class BasicOperations {
     a.initial = new State();
     SortedIntSet.FrozenIntSet initialset = new SortedIntSet.FrozenIntSet(initNumber, a.initial);
 
-    LinkedList<SortedIntSet.FrozenIntSet> worklist = new LinkedList<SortedIntSet.FrozenIntSet>();
-    Map<SortedIntSet.FrozenIntSet,State> newstate = new HashMap<SortedIntSet.FrozenIntSet,State>();
+    LinkedList<SortedIntSet.FrozenIntSet> worklist = new LinkedList<>();
+    Map<SortedIntSet.FrozenIntSet,State> newstate = new HashMap<>();
 
     worklist.add(initialset);
 
@@ -713,25 +713,25 @@ final public class BasicOperations {
    */
   public static void addEpsilons(Automaton a, Collection<StatePair> pairs) {
     a.expandSingleton();
-    HashMap<State,HashSet<State>> forward = new HashMap<State,HashSet<State>>();
-    HashMap<State,HashSet<State>> back = new HashMap<State,HashSet<State>>();
+    HashMap<State,HashSet<State>> forward = new HashMap<>();
+    HashMap<State,HashSet<State>> back = new HashMap<>();
     for (StatePair p : pairs) {
       HashSet<State> to = forward.get(p.s1);
       if (to == null) {
-        to = new HashSet<State>();
+        to = new HashSet<>();
         forward.put(p.s1, to);
       }
       to.add(p.s2);
       HashSet<State> from = back.get(p.s2);
       if (from == null) {
-        from = new HashSet<State>();
+        from = new HashSet<>();
         back.put(p.s2, from);
       }
       from.add(p.s1);
     }
     // calculate epsilon closure
-    LinkedList<StatePair> worklist = new LinkedList<StatePair>(pairs);
-    HashSet<StatePair> workset = new HashSet<StatePair>(pairs);
+    LinkedList<StatePair> worklist = new LinkedList<>(pairs);
+    HashSet<StatePair> workset = new HashSet<>(pairs);
     while (!worklist.isEmpty()) {
       StatePair p = worklist.removeFirst();
       workset.remove(p);
@@ -817,12 +817,12 @@ final public class BasicOperations {
       return p.accept;
     } else {
       State[] states = a.getNumberedStates();
-      LinkedList<State> pp = new LinkedList<State>();
-      LinkedList<State> pp_other = new LinkedList<State>();
+      LinkedList<State> pp = new LinkedList<>();
+      LinkedList<State> pp_other = new LinkedList<>();
       BitSet bb = new BitSet(states.length);
       BitSet bb_other = new BitSet(states.length);
       pp.add(a.initial);
-      ArrayList<State> dest = new ArrayList<State>();
+      ArrayList<State> dest = new ArrayList<>();
       boolean accept = a.initial.accept;
       for (int i = 0, c = 0; i < s.length(); i += Character.charCount(c)) {
         c = s.codePointAt(i);
diff --git lucene/core/src/java/org/apache/lucene/util/automaton/CompiledAutomaton.java lucene/core/src/java/org/apache/lucene/util/automaton/CompiledAutomaton.java
index cfc46b1..dcafe70 100644
--- lucene/core/src/java/org/apache/lucene/util/automaton/CompiledAutomaton.java
+++ lucene/core/src/java/org/apache/lucene/util/automaton/CompiledAutomaton.java
@@ -272,7 +272,7 @@ public class CompiledAutomaton {
       }
     }
 
-    final List<Integer> stack = new ArrayList<Integer>();
+    final List<Integer> stack = new ArrayList<>();
 
     int idx = 0;
     while (true) {
diff --git lucene/core/src/java/org/apache/lucene/util/automaton/DaciukMihovAutomatonBuilder.java lucene/core/src/java/org/apache/lucene/util/automaton/DaciukMihovAutomatonBuilder.java
index c6be628..68ce1e9 100644
--- lucene/core/src/java/org/apache/lucene/util/automaton/DaciukMihovAutomatonBuilder.java
+++ lucene/core/src/java/org/apache/lucene/util/automaton/DaciukMihovAutomatonBuilder.java
@@ -187,7 +187,7 @@ final class DaciukMihovAutomatonBuilder {
   /**
    * A "registry" for state interning.
    */
-  private HashMap<State,State> stateRegistry = new HashMap<State,State>();
+  private HashMap<State,State> stateRegistry = new HashMap<>();
   
   /**
    * Root automaton state.
diff --git lucene/core/src/java/org/apache/lucene/util/automaton/LevenshteinAutomata.java lucene/core/src/java/org/apache/lucene/util/automaton/LevenshteinAutomata.java
index 92384c4..869dc3a 100644
--- lucene/core/src/java/org/apache/lucene/util/automaton/LevenshteinAutomata.java
+++ lucene/core/src/java/org/apache/lucene/util/automaton/LevenshteinAutomata.java
@@ -63,7 +63,7 @@ public class LevenshteinAutomata {
     this.alphaMax = alphaMax;
 
     // calculate the alphabet
-    SortedSet<Integer> set = new TreeSet<Integer>();
+    SortedSet<Integer> set = new TreeSet<>();
     for (int i = 0; i < word.length; i++) {
       int v = word[i];
       if (v > alphaMax) {
diff --git lucene/core/src/java/org/apache/lucene/util/automaton/MinimizationOperations.java lucene/core/src/java/org/apache/lucene/util/automaton/MinimizationOperations.java
index b5fd0ca..85f8d58 100644
--- lucene/core/src/java/org/apache/lucene/util/automaton/MinimizationOperations.java
+++ lucene/core/src/java/org/apache/lucene/util/automaton/MinimizationOperations.java
@@ -83,13 +83,13 @@ final public class MinimizationOperations {
     final int[] block = new int[statesLen];
     final StateList[][] active = new StateList[statesLen][sigmaLen];
     final StateListNode[][] active2 = new StateListNode[statesLen][sigmaLen];
-    final LinkedList<IntPair> pending = new LinkedList<IntPair>();
+    final LinkedList<IntPair> pending = new LinkedList<>();
     final BitSet pending2 = new BitSet(sigmaLen*statesLen);
     final BitSet split = new BitSet(statesLen), 
       refine = new BitSet(statesLen), refine2 = new BitSet(statesLen);
     for (int q = 0; q < statesLen; q++) {
-      splitblock[q] = new ArrayList<State>();
-      partition[q] = new HashSet<State>();
+      splitblock[q] = new ArrayList<>();
+      partition[q] = new HashSet<>();
       for (int x = 0; x < sigmaLen; x++) {
         active[q][x] = new StateList();
       }
@@ -104,7 +104,7 @@ final public class MinimizationOperations {
         final ArrayList<State>[] r =
           reverse[qq.step(sigma[x]).number];
         if (r[x] == null)
-          r[x] = new ArrayList<State>();
+          r[x] = new ArrayList<>();
         r[x].add(qq);
       }
     }
diff --git lucene/core/src/java/org/apache/lucene/util/automaton/RegExp.java lucene/core/src/java/org/apache/lucene/util/automaton/RegExp.java
index dec19cb..bf5b4be 100644
--- lucene/core/src/java/org/apache/lucene/util/automaton/RegExp.java
+++ lucene/core/src/java/org/apache/lucene/util/automaton/RegExp.java
@@ -486,14 +486,14 @@ public class RegExp {
     Automaton a = null;
     switch (kind) {
       case REGEXP_UNION:
-        list = new ArrayList<Automaton>();
+        list = new ArrayList<>();
         findLeaves(exp1, Kind.REGEXP_UNION, list, automata, automaton_provider);
         findLeaves(exp2, Kind.REGEXP_UNION, list, automata, automaton_provider);
         a = BasicOperations.union(list);
         MinimizationOperations.minimize(a);
         break;
       case REGEXP_CONCATENATION:
-        list = new ArrayList<Automaton>();
+        list = new ArrayList<>();
         findLeaves(exp1, Kind.REGEXP_CONCATENATION, list, automata,
             automaton_provider);
         findLeaves(exp2, Kind.REGEXP_CONCATENATION, list, automata,
@@ -664,7 +664,7 @@ public class RegExp {
    * Returns set of automaton identifiers that occur in this regular expression.
    */
   public Set<String> getIdentifiers() {
-    HashSet<String> set = new HashSet<String>();
+    HashSet<String> set = new HashSet<>();
     getIdentifiers(set);
     return set;
   }
diff --git lucene/core/src/java/org/apache/lucene/util/automaton/SortedIntSet.java lucene/core/src/java/org/apache/lucene/util/automaton/SortedIntSet.java
index 0cfeae6..546c307 100644
--- lucene/core/src/java/org/apache/lucene/util/automaton/SortedIntSet.java
+++ lucene/core/src/java/org/apache/lucene/util/automaton/SortedIntSet.java
@@ -35,7 +35,7 @@ final class SortedIntSet {
   // O(N^2) linear ops to O(N log(N)) TreeMap
   private final static int TREE_MAP_CUTOVER = 30;
 
-  private final Map<Integer,Integer> map = new TreeMap<Integer,Integer>();
+  private final Map<Integer,Integer> map = new TreeMap<>();
 
   private boolean useTreeMap;
 
diff --git lucene/core/src/java/org/apache/lucene/util/automaton/SpecialOperations.java lucene/core/src/java/org/apache/lucene/util/automaton/SpecialOperations.java
index d479cd7..aff2bea 100644
--- lucene/core/src/java/org/apache/lucene/util/automaton/SpecialOperations.java
+++ lucene/core/src/java/org/apache/lucene/util/automaton/SpecialOperations.java
@@ -95,7 +95,7 @@ final public class SpecialOperations {
   public static String getCommonPrefix(Automaton a) {
     if (a.isSingleton()) return a.singleton;
     StringBuilder b = new StringBuilder();
-    HashSet<State> visited = new HashSet<State>();
+    HashSet<State> visited = new HashSet<>();
     State s = a.initial;
     boolean done;
     do {
@@ -119,7 +119,7 @@ final public class SpecialOperations {
   public static BytesRef getCommonPrefixBytesRef(Automaton a) {
     if (a.isSingleton()) return new BytesRef(a.singleton);
     BytesRef ref = new BytesRef(10);
-    HashSet<State> visited = new HashSet<State>();
+    HashSet<State> visited = new HashSet<>();
     State s = a.initial;
     boolean done;
     do {
@@ -185,9 +185,9 @@ final public class SpecialOperations {
   public static Set<State> reverse(Automaton a) {
     a.expandSingleton();
     // reverse all edges
-    HashMap<State, HashSet<Transition>> m = new HashMap<State, HashSet<Transition>>();
+    HashMap<State, HashSet<Transition>> m = new HashMap<>();
     State[] states = a.getNumberedStates();
-    Set<State> accept = new HashSet<State>();
+    Set<State> accept = new HashSet<>();
     for (State s : states)
       if (s.isAccept())
         accept.add(s);
@@ -223,7 +223,7 @@ final public class SpecialOperations {
    * the limit is infinite.
    */
   public static Set<IntsRef> getFiniteStrings(Automaton a, int limit) {
-    HashSet<IntsRef> strings = new HashSet<IntsRef>();
+    HashSet<IntsRef> strings = new HashSet<>();
     if (a.isSingleton()) {
       if (limit > 0) {
         strings.add(Util.toUTF32(a.singleton, new IntsRef()));
diff --git lucene/core/src/java/org/apache/lucene/util/automaton/UTF32ToUTF8.java lucene/core/src/java/org/apache/lucene/util/automaton/UTF32ToUTF8.java
index 6f52a7a..17be0ec 100644
--- lucene/core/src/java/org/apache/lucene/util/automaton/UTF32ToUTF8.java
+++ lucene/core/src/java/org/apache/lucene/util/automaton/UTF32ToUTF8.java
@@ -261,7 +261,7 @@ public final class UTF32ToUTF8 {
     }
 
     State[] map = new State[utf32.getNumberedStates().length];
-    List<State> pending = new ArrayList<State>();
+    List<State> pending = new ArrayList<>();
     State utf32State = utf32.getInitialState();
     pending.add(utf32State);
     Automaton utf8 = new Automaton();
diff --git lucene/core/src/java/org/apache/lucene/util/fst/Builder.java lucene/core/src/java/org/apache/lucene/util/fst/Builder.java
index 7a2ee75..5502bba 100644
--- lucene/core/src/java/org/apache/lucene/util/fst/Builder.java
+++ lucene/core/src/java/org/apache/lucene/util/fst/Builder.java
@@ -160,9 +160,9 @@ public class Builder<T> {
     this.shareMaxTailLength = shareMaxTailLength;
     this.doPackFST = doPackFST;
     this.acceptableOverheadRatio = acceptableOverheadRatio;
-    fst = new FST<T>(inputType, outputs, doPackFST, acceptableOverheadRatio, allowArrayArcs, bytesPageBits);
+    fst = new FST<>(inputType, outputs, doPackFST, acceptableOverheadRatio, allowArrayArcs, bytesPageBits);
     if (doShareSuffix) {
-      dedupHash = new NodeHash<T>(fst, fst.bytes.getReverseReader(false));
+      dedupHash = new NodeHash<>(fst, fst.bytes.getReverseReader(false));
     } else {
       dedupHash = null;
     }
@@ -172,7 +172,7 @@ public class Builder<T> {
         (UnCompiledNode<T>[]) new UnCompiledNode[10];
     frontier = f;
     for(int idx=0;idx<frontier.length;idx++) {
-      frontier[idx] = new UnCompiledNode<T>(this, idx);
+      frontier[idx] = new UnCompiledNode<>(this, idx);
     }
   }
 
@@ -301,7 +301,7 @@ public class Builder<T> {
             // undecided on whether to prune it.  later, it
             // will be either compiled or pruned, so we must
             // allocate a new node:
-            frontier[idx] = new UnCompiledNode<T>(this, idx);
+            frontier[idx] = new UnCompiledNode<>(this, idx);
           }
         }
       }
@@ -384,7 +384,7 @@ public class Builder<T> {
         new UnCompiledNode[ArrayUtil.oversize(input.length+1, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
       System.arraycopy(frontier, 0, next, 0, frontier.length);
       for(int idx=frontier.length;idx<next.length;idx++) {
-        next[idx] = new UnCompiledNode<T>(this, idx);
+        next[idx] = new UnCompiledNode<>(this, idx);
       }
       frontier = next;
     }
@@ -553,7 +553,7 @@ public class Builder<T> {
     public UnCompiledNode(Builder<T> owner, int depth) {
       this.owner = owner;
       arcs = (Arc<T>[]) new Arc[1];
-      arcs[0] = new Arc<T>();
+      arcs[0] = new Arc<>();
       output = owner.NO_OUTPUT;
       this.depth = depth;
     }
@@ -587,7 +587,7 @@ public class Builder<T> {
           new Arc[ArrayUtil.oversize(numArcs+1, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
         System.arraycopy(arcs, 0, newArcs, 0, arcs.length);
         for(int arcIdx=numArcs;arcIdx<newArcs.length;arcIdx++) {
-          newArcs[arcIdx] = new Arc<T>();
+          newArcs[arcIdx] = new Arc<>();
         }
         arcs = newArcs;
       }
diff --git lucene/core/src/java/org/apache/lucene/util/fst/BytesRefFSTEnum.java lucene/core/src/java/org/apache/lucene/util/fst/BytesRefFSTEnum.java
index baff0b0..53f3585 100644
--- lucene/core/src/java/org/apache/lucene/util/fst/BytesRefFSTEnum.java
+++ lucene/core/src/java/org/apache/lucene/util/fst/BytesRefFSTEnum.java
@@ -30,7 +30,7 @@ import org.apache.lucene.util.BytesRef;
 
 public final class BytesRefFSTEnum<T> extends FSTEnum<T> {
   private final BytesRef current = new BytesRef(10);
-  private final InputOutput<T> result = new InputOutput<T>();
+  private final InputOutput<T> result = new InputOutput<>();
   private BytesRef target;
 
   /** Holds a single input (BytesRef) + output pair. */
diff --git lucene/core/src/java/org/apache/lucene/util/fst/BytesStore.java lucene/core/src/java/org/apache/lucene/util/fst/BytesStore.java
index 4b20947..c7a0c89 100644
--- lucene/core/src/java/org/apache/lucene/util/fst/BytesStore.java
+++ lucene/core/src/java/org/apache/lucene/util/fst/BytesStore.java
@@ -29,7 +29,7 @@ import org.apache.lucene.store.DataOutput;
 
 class BytesStore extends DataOutput {
 
-  private final List<byte[]> blocks = new ArrayList<byte[]>();
+  private final List<byte[]> blocks = new ArrayList<>();
 
   private final int blockSize;
   private final int blockBits;
diff --git lucene/core/src/java/org/apache/lucene/util/fst/FST.java lucene/core/src/java/org/apache/lucene/util/fst/FST.java
index 74e5a0f..f5977e0 100644
--- lucene/core/src/java/org/apache/lucene/util/fst/FST.java
+++ lucene/core/src/java/org/apache/lucene/util/fst/FST.java
@@ -437,7 +437,7 @@ public final class FST<T> {
   }
   
   public void readRootArcs(Arc<T>[] arcs) throws IOException {
-    final Arc<T> arc = new Arc<T>();
+    final Arc<T> arc = new Arc<>();
     getFirstArc(arc);
     final BytesReader in = getBytesReader();
     if (targetHasArcs(arc)) {
@@ -592,7 +592,7 @@ public final class FST<T> {
     InputStream is = new BufferedInputStream(new FileInputStream(file));
     boolean success = false;
     try {
-      FST<T> fst = new FST<T>(new InputStreamDataInput(is), outputs);
+      FST<T> fst = new FST<>(new InputStreamDataInput(is), outputs);
       success = true;
       return fst;
     } finally {
@@ -1349,7 +1349,7 @@ public final class FST<T> {
     // TODO: must assert this FST was built with
     // "willRewrite"
 
-    final List<ArcAndState<T>> queue = new ArrayList<ArcAndState<T>>();
+    final List<ArcAndState<T>> queue = new ArrayList<>();
 
     // TODO: use bitset to not revisit nodes already
     // visited
@@ -1358,7 +1358,7 @@ public final class FST<T> {
     int saved = 0;
 
     queue.add(new ArcAndState<T>(getFirstArc(new Arc<T>()), new IntsRef()));
-    Arc<T> scratchArc = new Arc<T>();
+    Arc<T> scratchArc = new Arc<>();
     while(queue.size() > 0) {
       //System.out.println("cycle size=" + queue.size());
       //for(ArcAndState<T> ent : queue) {
@@ -1499,7 +1499,7 @@ public final class FST<T> {
       throw new IllegalArgumentException("this FST was not built with willPackFST=true");
     }
 
-    Arc<T> arc = new Arc<T>();
+    Arc<T> arc = new Arc<>();
 
     final BytesReader r = getBytesReader();
 
@@ -1526,7 +1526,7 @@ public final class FST<T> {
     // Free up RAM:
     inCounts = null;
 
-    final Map<Integer,Integer> topNodeMap = new HashMap<Integer,Integer>();
+    final Map<Integer,Integer> topNodeMap = new HashMap<>();
     for(int downTo=q.size()-1;downTo>=0;downTo--) {
       NodeAndInCount n = q.pop();
       topNodeMap.put(n.node, downTo);
@@ -1558,7 +1558,7 @@ public final class FST<T> {
       // for assert:
       boolean negDelta = false;
 
-      fst = new FST<T>(inputType, outputs, bytes.getBlockBits());
+      fst = new FST<>(inputType, outputs, bytes.getBlockBits());
       
       final BytesStore writer = fst.bytes;
 
diff --git lucene/core/src/java/org/apache/lucene/util/fst/FSTEnum.java lucene/core/src/java/org/apache/lucene/util/fst/FSTEnum.java
index ea0c68d..60ab642 100644
--- lucene/core/src/java/org/apache/lucene/util/fst/FSTEnum.java
+++ lucene/core/src/java/org/apache/lucene/util/fst/FSTEnum.java
@@ -36,7 +36,7 @@ abstract class FSTEnum<T> {
 
   protected final T NO_OUTPUT;
   protected final FST.BytesReader fstReader;
-  protected final FST.Arc<T> scratchArc = new FST.Arc<T>();
+  protected final FST.Arc<T> scratchArc = new FST.Arc<>();
 
   protected int upto;
   protected int targetLength;
@@ -522,7 +522,7 @@ abstract class FSTEnum<T> {
 
   private FST.Arc<T> getArc(int idx) {
     if (arcs[idx] == null) {
-      arcs[idx] = new FST.Arc<T>();
+      arcs[idx] = new FST.Arc<>();
     }
     return arcs[idx];
   }
diff --git lucene/core/src/java/org/apache/lucene/util/fst/IntsRefFSTEnum.java lucene/core/src/java/org/apache/lucene/util/fst/IntsRefFSTEnum.java
index ff4b80a..d45f5a6 100644
--- lucene/core/src/java/org/apache/lucene/util/fst/IntsRefFSTEnum.java
+++ lucene/core/src/java/org/apache/lucene/util/fst/IntsRefFSTEnum.java
@@ -30,7 +30,7 @@ import java.io.IOException;
 
 public final class IntsRefFSTEnum<T> extends FSTEnum<T> {
   private final IntsRef current = new IntsRef(10);
-  private final InputOutput<T> result = new InputOutput<T>();
+  private final InputOutput<T> result = new InputOutput<>();
   private IntsRef target;
 
   /** Holds a single input (IntsRef) + output pair. */
diff --git lucene/core/src/java/org/apache/lucene/util/fst/NodeHash.java lucene/core/src/java/org/apache/lucene/util/fst/NodeHash.java
index 3212330..04a01c1 100644
--- lucene/core/src/java/org/apache/lucene/util/fst/NodeHash.java
+++ lucene/core/src/java/org/apache/lucene/util/fst/NodeHash.java
@@ -29,7 +29,7 @@ final class NodeHash<T> {
   private long count;
   private long mask;
   private final FST<T> fst;
-  private final FST.Arc<T> scratchArc = new FST.Arc<T>();
+  private final FST.Arc<T> scratchArc = new FST.Arc<>();
   private final FST.BytesReader in;
 
   public NodeHash(FST<T> fst, FST.BytesReader in) {
diff --git lucene/core/src/java/org/apache/lucene/util/fst/PairOutputs.java lucene/core/src/java/org/apache/lucene/util/fst/PairOutputs.java
index e625ca0..b9d5da6 100644
--- lucene/core/src/java/org/apache/lucene/util/fst/PairOutputs.java
+++ lucene/core/src/java/org/apache/lucene/util/fst/PairOutputs.java
@@ -66,7 +66,7 @@ public class PairOutputs<A,B> extends Outputs<PairOutputs.Pair<A,B>> {
   public PairOutputs(Outputs<A> outputs1, Outputs<B> outputs2) {
     this.outputs1 = outputs1;
     this.outputs2 = outputs2;
-    NO_OUTPUT = new Pair<A,B>(outputs1.getNoOutput(), outputs2.getNoOutput());
+    NO_OUTPUT = new Pair<>(outputs1.getNoOutput(), outputs2.getNoOutput());
   }
 
   /** Create a new Pair */
@@ -81,7 +81,7 @@ public class PairOutputs<A,B> extends Outputs<PairOutputs.Pair<A,B>> {
     if (a == outputs1.getNoOutput() && b == outputs2.getNoOutput()) {
       return NO_OUTPUT;
     } else {
-      final Pair<A,B> p = new Pair<A,B>(a, b);
+      final Pair<A,B> p = new Pair<>(a, b);
       assert valid(p);
       return p;
     }
diff --git lucene/core/src/java/org/apache/lucene/util/fst/Util.java lucene/core/src/java/org/apache/lucene/util/fst/Util.java
index 0e20e71..21ea3da 100644
--- lucene/core/src/java/org/apache/lucene/util/fst/Util.java
+++ lucene/core/src/java/org/apache/lucene/util/fst/Util.java
@@ -104,7 +104,7 @@ public final class Util {
     // TODO: would be nice not to alloc this on every lookup
     FST.Arc<Long> arc = fst.getFirstArc(new FST.Arc<Long>());
     
-    FST.Arc<Long> scratchArc = new FST.Arc<Long>();
+    FST.Arc<Long> scratchArc = new FST.Arc<>();
 
     final IntsRef result = new IntsRef();
     
@@ -288,7 +288,7 @@ public final class Util {
     private final int topN;
     private final int maxQueueDepth;
 
-    private final FST.Arc<T> scratchArc = new FST.Arc<T>();
+    private final FST.Arc<T> scratchArc = new FST.Arc<>();
     
     final Comparator<T> comparator;
 
@@ -301,7 +301,7 @@ public final class Util {
       this.maxQueueDepth = maxQueueDepth;
       this.comparator = comparator;
 
-      queue = new TreeSet<FSTPath<T>>(new TieBreakByInputComparator<T>(comparator));
+      queue = new TreeSet<>(new TieBreakByInputComparator<>(comparator));
     }
 
     // If back plus this arc is competitive then add to queue:
@@ -344,7 +344,7 @@ public final class Util {
       System.arraycopy(path.input.ints, 0, newInput.ints, 0, path.input.length);
       newInput.ints[path.input.length] = path.arc.label;
       newInput.length = path.input.length+1;
-      final FSTPath<T> newPath = new FSTPath<T>(cost, path.arc, newInput);
+      final FSTPath<T> newPath = new FSTPath<>(cost, path.arc, newInput);
 
       queue.add(newPath);
 
@@ -362,7 +362,7 @@ public final class Util {
         startOutput = fst.outputs.getNoOutput();
       }
 
-      FSTPath<T> path = new FSTPath<T>(startOutput, node, input);
+      FSTPath<T> path = new FSTPath<>(startOutput, node, input);
       fst.readFirstTargetArc(node, path.arc, bytesReader);
 
       //System.out.println("add start paths");
@@ -381,7 +381,7 @@ public final class Util {
 
     public MinResult<T>[] search() throws IOException {
 
-      final List<MinResult<T>> results = new ArrayList<MinResult<T>>();
+      final List<MinResult<T>> results = new ArrayList<>();
 
       //System.out.println("search topN=" + topN);
 
@@ -422,7 +422,7 @@ public final class Util {
           //System.out.println("    empty string!  cost=" + path.cost);
           // Empty string!
           path.input.length--;
-          results.add(new MinResult<T>(path.input, path.cost));
+          results.add(new MinResult<>(path.input, path.cost));
           continue;
         }
 
@@ -486,7 +486,7 @@ public final class Util {
             T finalOutput = fst.outputs.add(path.cost, path.arc.output);
             if (acceptResult(path.input, finalOutput)) {
               //System.out.println("    add result: " + path);
-              results.add(new MinResult<T>(path.input, finalOutput));
+              results.add(new MinResult<>(path.input, finalOutput));
             } else {
               rejectCount++;
               assert rejectCount + topN <= maxQueueDepth: "maxQueueDepth (" + maxQueueDepth + ") is too small for topN (" + topN + "): rejected " + rejectCount + " paths";
@@ -529,7 +529,7 @@ public final class Util {
 
     // All paths are kept, so we can pass topN for
     // maxQueueDepth and the pruning is admissible:
-    TopNSearcher<T> searcher = new TopNSearcher<T>(fst, topN, topN, comparator);
+    TopNSearcher<T> searcher = new TopNSearcher<>(fst, topN, topN, comparator);
 
     // since this search is initialized with a single start node 
     // it is okay to start with an empty input path here
@@ -578,15 +578,15 @@ public final class Util {
     final FST.Arc<T> startArc = fst.getFirstArc(new FST.Arc<T>());
 
     // A queue of transitions to consider for the next level.
-    final List<FST.Arc<T>> thisLevelQueue = new ArrayList<FST.Arc<T>>();
+    final List<FST.Arc<T>> thisLevelQueue = new ArrayList<>();
 
     // A queue of transitions to consider when processing the next level.
-    final List<FST.Arc<T>> nextLevelQueue = new ArrayList<FST.Arc<T>>();
+    final List<FST.Arc<T>> nextLevelQueue = new ArrayList<>();
     nextLevelQueue.add(startArc);
     //System.out.println("toDot: startArc: " + startArc);
     
     // A list of states on the same level (for ranking).
-    final List<Integer> sameLevelStates = new ArrayList<Integer>();
+    final List<Integer> sameLevelStates = new ArrayList<>();
 
     // A bitset of already seen states (target offset).
     final BitSet seen = new BitSet();
@@ -609,7 +609,7 @@ public final class Util {
     final T NO_OUTPUT = fst.outputs.getNoOutput();
     final BytesReader r = fst.getBytesReader();
 
-    // final FST.Arc<T> scratchArc = new FST.Arc<T>();
+    // final FST.Arc<T> scratchArc = new FST.Arc<>();
 
     {
       final String stateColor;
diff --git lucene/core/src/test/org/apache/lucene/TestSearch.java lucene/core/src/test/org/apache/lucene/TestSearch.java
index 1eaeb1d..cb89201 100644
--- lucene/core/src/test/org/apache/lucene/TestSearch.java
+++ lucene/core/src/test/org/apache/lucene/TestSearch.java
@@ -159,7 +159,7 @@ public class TestSearch extends LuceneTestCase {
   }
 
   private List<Query> buildQueries() {
-    List<Query> queries = new ArrayList<Query>();
+    List<Query> queries = new ArrayList<>();
 
     BooleanQuery booleanAB = new BooleanQuery();
     booleanAB.add(new TermQuery(new Term("contents", "a")), BooleanClause.Occur.SHOULD);
diff --git lucene/core/src/test/org/apache/lucene/analysis/TestGraphTokenizers.java lucene/core/src/test/org/apache/lucene/analysis/TestGraphTokenizers.java
index d215f8e..c49a3ee 100644
--- lucene/core/src/test/org/apache/lucene/analysis/TestGraphTokenizers.java
+++ lucene/core/src/test/org/apache/lucene/analysis/TestGraphTokenizers.java
@@ -118,7 +118,7 @@ public class TestGraphTokenizers extends BaseTokenStreamTestCase {
 
       final String[] parts = sb.toString().split(" ");
 
-      tokens = new ArrayList<Token>();
+      tokens = new ArrayList<>();
       int pos = 0;
       int maxPos = -1;
       int offset = 0;
@@ -454,7 +454,7 @@ public class TestGraphTokenizers extends BaseTokenStreamTestCase {
   private static final Automaton HOLE_A = BasicAutomata.makeChar(TokenStreamToAutomaton.HOLE);
 
   private Automaton join(String ... strings) {
-    List<Automaton> as = new ArrayList<Automaton>();
+    List<Automaton> as = new ArrayList<>();
     for(String s : strings) {
       as.add(BasicAutomata.makeString(s));
       as.add(SEP_A);
diff --git lucene/core/src/test/org/apache/lucene/analysis/TrivialLookaheadFilter.java lucene/core/src/test/org/apache/lucene/analysis/TrivialLookaheadFilter.java
index 4c395b1..cf50927 100644
--- lucene/core/src/test/org/apache/lucene/analysis/TrivialLookaheadFilter.java
+++ lucene/core/src/test/org/apache/lucene/analysis/TrivialLookaheadFilter.java
@@ -78,7 +78,7 @@ final public class TrivialLookaheadFilter extends LookaheadTokenFilter<TestPosit
   }
 
   private void peekSentence() throws IOException {
-    List<String> facts = new ArrayList<String>();
+    List<String> facts = new ArrayList<>();
     boolean haveSentence = false;
     do {
       if (peekToken()) {
diff --git lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsReader.java lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsReader.java
index a18d6cd..a149e3b 100644
--- lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsReader.java
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsReader.java
@@ -112,7 +112,7 @@ public class TestLucene40PostingsReader extends LuceneTestCase {
   }
   
   String fieldValue(int maxTF) {
-    ArrayList<String> shuffled = new ArrayList<String>();
+    ArrayList<String> shuffled = new ArrayList<>();
     StringBuilder sb = new StringBuilder();
     int i = random().nextInt(terms.length);
     while (i < terms.length) {
diff --git lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
index 59bcd81..6712134 100644
--- lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
@@ -62,7 +62,7 @@ public class TestReuseDocsEnum extends LuceneTestCase {
       AtomicReader indexReader = ctx.reader();
       Terms terms = indexReader.terms("body");
       TermsEnum iterator = terms.iterator(null);
-      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();
+      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();
       MatchNoBits bits = new Bits.MatchNoBits(indexReader.maxDoc());
       while ((iterator.next()) != null) {
         DocsEnum docs = iterator.docs(random().nextBoolean() ? bits : new Bits.MatchNoBits(indexReader.maxDoc()), null, random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
@@ -88,7 +88,7 @@ public class TestReuseDocsEnum extends LuceneTestCase {
     for (AtomicReaderContext ctx : open.leaves()) {
       Terms terms = ctx.reader().terms("body");
       TermsEnum iterator = terms.iterator(null);
-      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();
+      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();
       MatchNoBits bits = new Bits.MatchNoBits(open.maxDoc());
       DocsEnum docs = null;
       while ((iterator.next()) != null) {
@@ -139,7 +139,7 @@ public class TestReuseDocsEnum extends LuceneTestCase {
     for (AtomicReaderContext ctx : leaves) {
       Terms terms = ctx.reader().terms("body");
       TermsEnum iterator = terms.iterator(null);
-      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();
+      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();
       MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());
       iterator = terms.iterator(null);
       DocsEnum docs = null;
diff --git lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat3.java lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat3.java
index c4592e9..ee294d2 100644
--- lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat3.java
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat3.java
@@ -201,7 +201,7 @@ public class TestBlockPostingsFormat3 extends LuceneTestCase {
     Random random = random();
     
     // collect this number of terms from the left side
-    HashSet<BytesRef> tests = new HashSet<BytesRef>();
+    HashSet<BytesRef> tests = new HashSet<>();
     int numPasses = 0;
     while (numPasses < 10 && tests.size() < numTests) {
       leftEnum = leftTerms.iterator(leftEnum);
@@ -228,7 +228,7 @@ public class TestBlockPostingsFormat3 extends LuceneTestCase {
       numPasses++;
     }
     
-    ArrayList<BytesRef> shuffledTests = new ArrayList<BytesRef>(tests);
+    ArrayList<BytesRef> shuffledTests = new ArrayList<>(tests);
     Collections.shuffle(shuffledTests, random);
     
     for (BytesRef b : shuffledTests) {
diff --git lucene/core/src/test/org/apache/lucene/index/Test2BTerms.java lucene/core/src/test/org/apache/lucene/index/Test2BTerms.java
index b4249d0..8616615 100644
--- lucene/core/src/test/org/apache/lucene/index/Test2BTerms.java
+++ lucene/core/src/test/org/apache/lucene/index/Test2BTerms.java
@@ -52,7 +52,7 @@ public class Test2BTerms extends LuceneTestCase {
 
     private final int tokensPerDoc;
     private int tokenCount;
-    public final List<BytesRef> savedTerms = new ArrayList<BytesRef>();
+    public final List<BytesRef> savedTerms = new ArrayList<>();
     private int nextSave;
     private final Random random;
 
@@ -204,7 +204,7 @@ public class Test2BTerms extends LuceneTestCase {
       savedTerms = findTerms(r);
     }
     final int numSavedTerms = savedTerms.size();
-    final List<BytesRef> bigOrdTerms = new ArrayList<BytesRef>(savedTerms.subList(numSavedTerms-10, numSavedTerms));
+    final List<BytesRef> bigOrdTerms = new ArrayList<>(savedTerms.subList(numSavedTerms-10, numSavedTerms));
     System.out.println("TEST: test big ord terms...");
     testSavedTerms(r, bigOrdTerms);
     System.out.println("TEST: test all saved terms...");
@@ -223,7 +223,7 @@ public class Test2BTerms extends LuceneTestCase {
   private List<BytesRef> findTerms(IndexReader r) throws IOException {
     System.out.println("TEST: findTerms");
     final TermsEnum termsEnum = MultiFields.getTerms(r, "field").iterator(null);
-    final List<BytesRef> savedTerms = new ArrayList<BytesRef>();
+    final List<BytesRef> savedTerms = new ArrayList<>();
     int nextSave = TestUtil.nextInt(random(), 500000, 1000000);
     BytesRef term;
     while((term = termsEnum.next()) != null) {
diff --git lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
index 31aeac8..27fd97e 100644
--- lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
+++ lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
@@ -653,7 +653,7 @@ public class TestAddIndexes extends LuceneTestCase {
     Directory dir, dir2;
     final static int NUM_INIT_DOCS = 17;
     IndexWriter writer2;
-    final List<Throwable> failures = new ArrayList<Throwable>();
+    final List<Throwable> failures = new ArrayList<>();
     volatile boolean didClose;
     final IndexReader[] readers;
     final int NUM_COPY;
diff --git lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
index ca8817e..efd759f 100644
--- lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
+++ lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
@@ -222,10 +222,10 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
   @BeforeClass
   public static void beforeClass() throws Exception {
     assertFalse("test infra is broken!", LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE);
-    List<String> names = new ArrayList<String>(oldNames.length + oldSingleSegmentNames.length);
+    List<String> names = new ArrayList<>(oldNames.length + oldSingleSegmentNames.length);
     names.addAll(Arrays.asList(oldNames));
     names.addAll(Arrays.asList(oldSingleSegmentNames));
-    oldIndexDirs = new HashMap<String,Directory>();
+    oldIndexDirs = new HashMap<>();
     for (String name : names) {
       File dir = TestUtil.getTempDir(name);
       File dataFile = new File(TestBackwardsCompatibility.class.getResource("index." + name + ".zip").toURI());
@@ -935,7 +935,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
   }
 
   public void testUpgradeOldIndex() throws Exception {
-    List<String> names = new ArrayList<String>(oldNames.length + oldSingleSegmentNames.length);
+    List<String> names = new ArrayList<>(oldNames.length + oldSingleSegmentNames.length);
     names.addAll(Arrays.asList(oldNames));
     names.addAll(Arrays.asList(oldSingleSegmentNames));
     for(String name : names) {
@@ -961,7 +961,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
 
       String path = dir.getAbsolutePath();
       
-      List<String> args = new ArrayList<String>();
+      List<String> args = new ArrayList<>();
       if (random().nextBoolean()) {
         args.add("-verbose");
       }
diff --git lucene/core/src/test/org/apache/lucene/index/TestBagOfPositions.java lucene/core/src/test/org/apache/lucene/index/TestBagOfPositions.java
index 8b3c93b..61b4113 100644
--- lucene/core/src/test/org/apache/lucene/index/TestBagOfPositions.java
+++ lucene/core/src/test/org/apache/lucene/index/TestBagOfPositions.java
@@ -43,7 +43,7 @@ import org.apache.lucene.util.TestUtil;
 @SuppressCodecs({"Direct", "Memory"}) // at night this makes like 200k/300k docs and will make Direct's heart beat!
 public class TestBagOfPositions extends LuceneTestCase {
   public void test() throws Exception {
-    List<String> postingsList = new ArrayList<String>();
+    List<String> postingsList = new ArrayList<>();
     int numTerms = atLeast(300);
     final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);
     boolean isSimpleText = "SimpleText".equals(TestUtil.getPostingsFormat("field"));
@@ -66,7 +66,7 @@ public class TestBagOfPositions extends LuceneTestCase {
     }
     Collections.shuffle(postingsList, random());
 
-    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);
+    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);
 
     Directory dir = newFSDirectory(TestUtil.getTempDir("bagofpositions"));
 
diff --git lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings.java lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings.java
index 5720699..a3067e0 100644
--- lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings.java
+++ lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings.java
@@ -42,7 +42,7 @@ import org.apache.lucene.util.TestUtil;
 @SuppressCodecs({"Direct", "Memory"}) // at night this makes like 200k/300k docs and will make Direct's heart beat!
 public class TestBagOfPostings extends LuceneTestCase {
   public void test() throws Exception {
-    List<String> postingsList = new ArrayList<String>();
+    List<String> postingsList = new ArrayList<>();
     int numTerms = atLeast(300);
     final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);
 
@@ -68,7 +68,7 @@ public class TestBagOfPostings extends LuceneTestCase {
     }
     Collections.shuffle(postingsList, random());
 
-    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);
+    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);
 
     Directory dir = newFSDirectory(TestUtil.getTempDir("bagofpostings"));
     final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
@@ -93,7 +93,7 @@ public class TestBagOfPostings extends LuceneTestCase {
               startingGun.await();
               while (!postings.isEmpty()) {
                 StringBuilder text = new StringBuilder();
-                Set<String> visited = new HashSet<String>();
+                Set<String> visited = new HashSet<>();
                 for (int i = 0; i < maxTermsPerDoc; i++) {
                   String token = postings.poll();
                   if (token == null) {
diff --git lucene/core/src/test/org/apache/lucene/index/TestCheckIndex.java lucene/core/src/test/org/apache/lucene/index/TestCheckIndex.java
index 66542ab..ba2102e 100644
--- lucene/core/src/test/org/apache/lucene/index/TestCheckIndex.java
+++ lucene/core/src/test/org/apache/lucene/index/TestCheckIndex.java
@@ -90,7 +90,7 @@ public class TestCheckIndex extends LuceneTestCase {
     assertEquals(18, seg.termVectorStatus.totVectors);
 
     assertTrue(seg.diagnostics.size() > 0);
-    final List<String> onlySegments = new ArrayList<String>();
+    final List<String> onlySegments = new ArrayList<>();
     onlySegments.add("_0");
     
     assertTrue(checker.checkIndex(onlySegments).clean == true);
diff --git lucene/core/src/test/org/apache/lucene/index/TestCodecs.java lucene/core/src/test/org/apache/lucene/index/TestCodecs.java
index 30a6bd9..5b33629 100644
--- lucene/core/src/test/org/apache/lucene/index/TestCodecs.java
+++ lucene/core/src/test/org/apache/lucene/index/TestCodecs.java
@@ -180,7 +180,7 @@ public class TestCodecs extends LuceneTestCase {
     //final int numTerms = 2;
     final TermData[] terms = new TermData[numTerms];
 
-    final HashSet<String> termsSeen = new HashSet<String>();
+    final HashSet<String> termsSeen = new HashSet<>();
 
     for(int i=0;i<numTerms;i++) {
 
diff --git lucene/core/src/test/org/apache/lucene/index/TestDeletionPolicy.java lucene/core/src/test/org/apache/lucene/index/TestDeletionPolicy.java
index 122a32c..2e8c0fa 100644
--- lucene/core/src/test/org/apache/lucene/index/TestDeletionPolicy.java
+++ lucene/core/src/test/org/apache/lucene/index/TestDeletionPolicy.java
@@ -120,7 +120,7 @@ public class TestDeletionPolicy extends LuceneTestCase {
     int numOnCommit;
     int numToKeep;
     int numDelete;
-    Set<String> seen = new HashSet<String>();
+    Set<String> seen = new HashSet<>();
 
     public KeepLastNDeletionPolicy(int numToKeep) {
       this.numToKeep = numToKeep;
@@ -228,7 +228,7 @@ public class TestDeletionPolicy extends LuceneTestCase {
     mp.setNoCFSRatio(1.0);
     IndexWriter writer = new IndexWriter(dir, conf);
     ExpirationTimeDeletionPolicy policy = (ExpirationTimeDeletionPolicy) writer.getConfig().getIndexDeletionPolicy();
-    Map<String,String> commitData = new HashMap<String,String>();
+    Map<String,String> commitData = new HashMap<>();
     commitData.put("commitTime", String.valueOf(System.currentTimeMillis()));
     writer.setCommitData(commitData);
     writer.commit();
@@ -250,7 +250,7 @@ public class TestDeletionPolicy extends LuceneTestCase {
       for(int j=0;j<17;j++) {
         addDoc(writer);
       }
-      commitData = new HashMap<String,String>();
+      commitData = new HashMap<>();
       commitData.put("commitTime", String.valueOf(System.currentTimeMillis()));
       writer.setCommitData(commitData);
       writer.commit();
diff --git lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
index bc7358f..89fd377 100644
--- lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
+++ lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
@@ -250,10 +250,10 @@ public class TestDirectoryReader extends LuceneTestCase {
       reader = DirectoryReader.open(d);
       fieldInfos = MultiFields.getMergedFieldInfos(reader);
 
-      Collection<String> allFieldNames = new HashSet<String>();
-      Collection<String> indexedFieldNames = new HashSet<String>();
-      Collection<String> notIndexedFieldNames = new HashSet<String>();
-      Collection<String> tvFieldNames = new HashSet<String>();
+      Collection<String> allFieldNames = new HashSet<>();
+      Collection<String> indexedFieldNames = new HashSet<>();
+      Collection<String> notIndexedFieldNames = new HashSet<>();
+      Collection<String> tvFieldNames = new HashSet<>();
 
       for(FieldInfo fieldInfo : fieldInfos) {
         final String name = fieldInfo.name;
@@ -743,7 +743,7 @@ public void testFilesOpenClose() throws IOException {
     Collection<IndexCommit> commits = DirectoryReader.listCommits(dir);
     for (final IndexCommit commit : commits) {
       Collection<String> files = commit.getFileNames();
-      HashSet<String> seen = new HashSet<String>();
+      HashSet<String> seen = new HashSet<>();
       for (final String fileName : files) { 
         assertTrue("file " + fileName + " was duplicated", !seen.contains(fileName));
         seen.add(fileName);
@@ -1079,7 +1079,7 @@ public void testFilesOpenClose() throws IOException {
     writer.addDocument(doc);
     DirectoryReader r = writer.getReader();
     writer.close();
-    Set<String> fieldsToLoad = new HashSet<String>();
+    Set<String> fieldsToLoad = new HashSet<>();
     assertEquals(0, r.document(0, fieldsToLoad).getFields().size());
     fieldsToLoad.add("field1");
     StoredDocument doc2 = r.document(0, fieldsToLoad);
diff --git lucene/core/src/test/org/apache/lucene/index/TestDirectoryReaderReopen.java lucene/core/src/test/org/apache/lucene/index/TestDirectoryReaderReopen.java
index 8938953..a3bb733 100644
--- lucene/core/src/test/org/apache/lucene/index/TestDirectoryReaderReopen.java
+++ lucene/core/src/test/org/apache/lucene/index/TestDirectoryReaderReopen.java
@@ -552,14 +552,14 @@ public class TestDirectoryReaderReopen extends LuceneTestCase {
       Document doc = new Document();
       doc.add(newStringField("id", ""+i, Field.Store.NO));
       writer.addDocument(doc);
-      Map<String,String> data = new HashMap<String,String>();
+      Map<String,String> data = new HashMap<>();
       data.put("index", i+"");
       writer.setCommitData(data);
       writer.commit();
     }
     for(int i=0;i<4;i++) {
       writer.deleteDocuments(new Term("id", ""+i));
-      Map<String,String> data = new HashMap<String,String>();
+      Map<String,String> data = new HashMap<>();
       data.put("index", (4+i)+"");
       writer.setCommitData(data);
       writer.commit();
diff --git lucene/core/src/test/org/apache/lucene/index/TestDoc.java lucene/core/src/test/org/apache/lucene/index/TestDoc.java
index 53ee2cc..1ef079e 100644
--- lucene/core/src/test/org/apache/lucene/index/TestDoc.java
+++ lucene/core/src/test/org/apache/lucene/index/TestDoc.java
@@ -71,7 +71,7 @@ public class TestDoc extends LuceneTestCase {
         Directory directory = newFSDirectory(indexDir);
         directory.close();
 
-        files = new LinkedList<File>();
+        files = new LinkedList<>();
         files.add(createOutput("test.txt",
             "This is the first test file"
         ));
@@ -229,7 +229,7 @@ public class TestDoc extends LuceneTestCase {
       final SegmentInfo info = new SegmentInfo(si1.info.dir, Constants.LUCENE_MAIN_VERSION, merged,
                                                si1.info.getDocCount() + si2.info.getDocCount(),
                                                false, codec, null);
-      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));
+      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));
       
       if (useCompoundFile) {
         Collection<String> filesToDelete = IndexWriter.createCompoundFile(InfoStream.getDefault(), dir, MergeState.CheckAbort.NONE, info, newIOContext(random()));
diff --git lucene/core/src/test/org/apache/lucene/index/TestDocTermOrds.java lucene/core/src/test/org/apache/lucene/index/TestDocTermOrds.java
index d557859..de4534f 100644
--- lucene/core/src/test/org/apache/lucene/index/TestDocTermOrds.java
+++ lucene/core/src/test/org/apache/lucene/index/TestDocTermOrds.java
@@ -95,7 +95,7 @@ public class TestDocTermOrds extends LuceneTestCase {
     Directory dir = newDirectory();
 
     final int NUM_TERMS = atLeast(20);
-    final Set<BytesRef> terms = new HashSet<BytesRef>();
+    final Set<BytesRef> terms = new HashSet<>();
     while(terms.size() < NUM_TERMS) {
       final String s = TestUtil.randomRealisticUnicodeString(random());
       //final String s = _TestUtil.randomSimpleString(random);
@@ -120,7 +120,7 @@ public class TestDocTermOrds extends LuceneTestCase {
     final RandomIndexWriter w = new RandomIndexWriter(random(), dir, conf);
 
     final int[][] idToOrds = new int[NUM_DOCS][];
-    final Set<Integer> ordsForDocSet = new HashSet<Integer>();
+    final Set<Integer> ordsForDocSet = new HashSet<>();
 
     for(int id=0;id<NUM_DOCS;id++) {
       Document doc = new Document();
@@ -181,7 +181,7 @@ public class TestDocTermOrds extends LuceneTestCase {
   public void testRandomWithPrefix() throws Exception {
     Directory dir = newDirectory();
 
-    final Set<String> prefixes = new HashSet<String>();
+    final Set<String> prefixes = new HashSet<>();
     final int numPrefix = TestUtil.nextInt(random(), 2, 7);
     if (VERBOSE) {
       System.out.println("TEST: use " + numPrefix + " prefixes");
@@ -193,7 +193,7 @@ public class TestDocTermOrds extends LuceneTestCase {
     final String[] prefixesArray = prefixes.toArray(new String[prefixes.size()]);
 
     final int NUM_TERMS = atLeast(20);
-    final Set<BytesRef> terms = new HashSet<BytesRef>();
+    final Set<BytesRef> terms = new HashSet<>();
     while(terms.size() < NUM_TERMS) {
       final String s = prefixesArray[random().nextInt(prefixesArray.length)] + TestUtil.randomRealisticUnicodeString(random());
       //final String s = prefixesArray[random.nextInt(prefixesArray.length)] + _TestUtil.randomSimpleString(random);
@@ -217,7 +217,7 @@ public class TestDocTermOrds extends LuceneTestCase {
     final RandomIndexWriter w = new RandomIndexWriter(random(), dir, conf);
 
     final int[][] idToOrds = new int[NUM_DOCS][];
-    final Set<Integer> ordsForDocSet = new HashSet<Integer>();
+    final Set<Integer> ordsForDocSet = new HashSet<>();
 
     for(int id=0;id<NUM_DOCS;id++) {
       Document doc = new Document();
@@ -262,7 +262,7 @@ public class TestDocTermOrds extends LuceneTestCase {
       final int[][] idToOrdsPrefix = new int[NUM_DOCS][];
       for(int id=0;id<NUM_DOCS;id++) {
         final int[] docOrds = idToOrds[id];
-        final List<Integer> newOrds = new ArrayList<Integer>();
+        final List<Integer> newOrds = new ArrayList<>();
         for(int ord : idToOrds[id]) {
           if (StringHelper.startsWith(termsArray[ord], prefixRef)) {
             newOrds.add(ord);
diff --git lucene/core/src/test/org/apache/lucene/index/TestDocValuesWithThreads.java lucene/core/src/test/org/apache/lucene/index/TestDocValuesWithThreads.java
index 0ba4897..b2188bf 100644
--- lucene/core/src/test/org/apache/lucene/index/TestDocValuesWithThreads.java
+++ lucene/core/src/test/org/apache/lucene/index/TestDocValuesWithThreads.java
@@ -43,9 +43,9 @@ public class TestDocValuesWithThreads extends LuceneTestCase {
     Directory dir = newDirectory();
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
 
-    final List<Long> numbers = new ArrayList<Long>();
-    final List<BytesRef> binary = new ArrayList<BytesRef>();
-    final List<BytesRef> sorted = new ArrayList<BytesRef>();
+    final List<Long> numbers = new ArrayList<>();
+    final List<BytesRef> binary = new ArrayList<>();
+    final List<BytesRef> sorted = new ArrayList<>();
     final int numDocs = atLeast(100);
     for(int i=0;i<numDocs;i++) {
       Document d = new Document();
@@ -69,7 +69,7 @@ public class TestDocValuesWithThreads extends LuceneTestCase {
     final AtomicReader ar = r.leaves().get(0).reader();
 
     int numThreads = TestUtil.nextInt(random(), 2, 5);
-    List<Thread> threads = new ArrayList<Thread>();
+    List<Thread> threads = new ArrayList<>();
     final CountDownLatch startingGun = new CountDownLatch(1);
     for(int t=0;t<numThreads;t++) {
       final Random threadRandom = new Random(random().nextLong());
@@ -133,12 +133,12 @@ public class TestDocValuesWithThreads extends LuceneTestCase {
     final Directory dir = newDirectory();
     final RandomIndexWriter writer = new RandomIndexWriter(random, dir);
     final boolean allowDups = random.nextBoolean();
-    final Set<String> seen = new HashSet<String>();
+    final Set<String> seen = new HashSet<>();
     if (VERBOSE) {
       System.out.println("TEST: NUM_DOCS=" + NUM_DOCS + " allowDups=" + allowDups);
     }
     int numDocs = 0;
-    final List<BytesRef> docValues = new ArrayList<BytesRef>();
+    final List<BytesRef> docValues = new ArrayList<>();
 
     // TODO: deletions
     while (numDocs < NUM_DOCS) {
diff --git lucene/core/src/test/org/apache/lucene/index/TestDocsAndPositions.java lucene/core/src/test/org/apache/lucene/index/TestDocsAndPositions.java
index 3bd8797..52eee95 100644
--- lucene/core/src/test/org/apache/lucene/index/TestDocsAndPositions.java
+++ lucene/core/src/test/org/apache/lucene/index/TestDocsAndPositions.java
@@ -120,7 +120,7 @@ public class TestDocsAndPositions extends LuceneTestCase {
     customType.setOmitNorms(true);
     for (int i = 0; i < numDocs; i++) {
       Document doc = new Document();
-      ArrayList<Integer> positions = new ArrayList<Integer>();
+      ArrayList<Integer> positions = new ArrayList<>();
       StringBuilder builder = new StringBuilder();
       int num = atLeast(131);
       for (int j = 0; j < num; j++) {
diff --git lucene/core/src/test/org/apache/lucene/index/TestDocumentsWriterDeleteQueue.java lucene/core/src/test/org/apache/lucene/index/TestDocumentsWriterDeleteQueue.java
index 714638c..dcc6119 100644
--- lucene/core/src/test/org/apache/lucene/index/TestDocumentsWriterDeleteQueue.java
+++ lucene/core/src/test/org/apache/lucene/index/TestDocumentsWriterDeleteQueue.java
@@ -47,7 +47,7 @@ public class TestDocumentsWriterDeleteQueue extends LuceneTestCase {
     BufferedUpdates bd2 = new BufferedUpdates();
     int last1 = 0;
     int last2 = 0;
-    Set<Term> uniqueValues = new HashSet<Term>();
+    Set<Term> uniqueValues = new HashSet<>();
     for (int j = 0; j < ids.length; j++) {
       Integer i = ids[j];
       // create an array here since we compare identity below against tailItem
@@ -72,7 +72,7 @@ public class TestDocumentsWriterDeleteQueue extends LuceneTestCase {
     }
     assertEquals(uniqueValues, bd1.terms.keySet());
     assertEquals(uniqueValues, bd2.terms.keySet());
-    HashSet<Term> frozenSet = new HashSet<Term>();
+    HashSet<Term> frozenSet = new HashSet<>();
     for (Term t : queue.freezeGlobalBuffer(null).termsIterable()) {
       BytesRef bytesRef = new BytesRef();
       bytesRef.copyBytes(t.bytes);
@@ -173,7 +173,7 @@ public class TestDocumentsWriterDeleteQueue extends LuceneTestCase {
 
   public void testStressDeleteQueue() throws InterruptedException {
     DocumentsWriterDeleteQueue queue = new DocumentsWriterDeleteQueue();
-    Set<Term> uniqueValues = new HashSet<Term>();
+    Set<Term> uniqueValues = new HashSet<>();
     final int size = 10000 + random().nextInt(500) * RANDOM_MULTIPLIER;
     Integer[] ids = new Integer[size];
     for (int i = 0; i < ids.length; i++) {
@@ -201,7 +201,7 @@ public class TestDocumentsWriterDeleteQueue extends LuceneTestCase {
       assertEquals(uniqueValues, deletes.terms.keySet());
     }
     queue.tryApplyGlobalSlice();
-    Set<Term> frozenSet = new HashSet<Term>();
+    Set<Term> frozenSet = new HashSet<>();
     for (Term t : queue.freezeGlobalBuffer(null).termsIterable()) {
       BytesRef bytesRef = new BytesRef();
       bytesRef.copyBytes(t.bytes);
diff --git lucene/core/src/test/org/apache/lucene/index/TestFlushByRamOrCountsPolicy.java lucene/core/src/test/org/apache/lucene/index/TestFlushByRamOrCountsPolicy.java
index 7960b0c..0f8d05f 100644
--- lucene/core/src/test/org/apache/lucene/index/TestFlushByRamOrCountsPolicy.java
+++ lucene/core/src/test/org/apache/lucene/index/TestFlushByRamOrCountsPolicy.java
@@ -342,8 +342,8 @@ public class TestFlushByRamOrCountsPolicy extends LuceneTestCase {
 
     @Override
     public void onDelete(DocumentsWriterFlushControl control, ThreadState state) {
-      final ArrayList<ThreadState> pending = new ArrayList<DocumentsWriterPerThreadPool.ThreadState>();
-      final ArrayList<ThreadState> notPending = new ArrayList<DocumentsWriterPerThreadPool.ThreadState>();
+      final ArrayList<ThreadState> pending = new ArrayList<>();
+      final ArrayList<ThreadState> notPending = new ArrayList<>();
       findPending(control, pending, notPending);
       final boolean flushCurrent = state.flushPending;
       final ThreadState toFlush;
@@ -374,8 +374,8 @@ public class TestFlushByRamOrCountsPolicy extends LuceneTestCase {
 
     @Override
     public void onInsert(DocumentsWriterFlushControl control, ThreadState state) {
-      final ArrayList<ThreadState> pending = new ArrayList<DocumentsWriterPerThreadPool.ThreadState>();
-      final ArrayList<ThreadState> notPending = new ArrayList<DocumentsWriterPerThreadPool.ThreadState>();
+      final ArrayList<ThreadState> pending = new ArrayList<>();
+      final ArrayList<ThreadState> notPending = new ArrayList<>();
       findPending(control, pending, notPending);
       final boolean flushCurrent = state.flushPending;
       long activeBytes = control.activeBytes();
diff --git lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter.java lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
index a34d2f8..81c3461 100644
--- lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
@@ -141,9 +141,9 @@ public class TestIndexFileDeleter extends LuceneTestCase {
   }
 
   private static Set<String> difFiles(String[] files1, String[] files2) {
-    Set<String> set1 = new HashSet<String>();
-    Set<String> set2 = new HashSet<String>();
-    Set<String> extra = new HashSet<String>();
+    Set<String> set1 = new HashSet<>();
+    Set<String> set2 = new HashSet<>();
+    Set<String> extra = new HashSet<>();
     
     for (int x=0; x < files1.length; x++) {
       set1.add(files1[x]);
diff --git lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
index 3284dde..da4a0c7 100644
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -1361,7 +1361,7 @@ public class TestIndexWriter extends LuceneTestCase {
         r = DirectoryReader.open(dir);
       }
 
-      List<String> files = new ArrayList<String>(Arrays.asList(dir.listAll()));
+      List<String> files = new ArrayList<>(Arrays.asList(dir.listAll()));
 
       // RAMDir won't have a write.lock, but fs dirs will:
       files.remove("write.lock");
@@ -1820,7 +1820,7 @@ public class TestIndexWriter extends LuceneTestCase {
     IndexWriter w = new IndexWriter(dir,
                                     new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
 
-    final List<Document> docs = new ArrayList<Document>();
+    final List<Document> docs = new ArrayList<>();
     docs.add(new Document());
     w.updateDocuments(new Term("foo", "bar"),
                       docs);
@@ -2135,9 +2135,9 @@ public class TestIndexWriter extends LuceneTestCase {
     int iters = atLeast(100);
     int docCount = 0;
     int docId = 0;
-    Set<String> liveIds = new HashSet<String>();
+    Set<String> liveIds = new HashSet<>();
     for (int i = 0; i < iters; i++) {
-      List<Document> docs = new ArrayList<Document>();
+      List<Document> docs = new ArrayList<>();
       FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);
       FieldType idFt = new FieldType(TextField.TYPE_STORED);
       
@@ -2316,7 +2316,7 @@ public class TestIndexWriter extends LuceneTestCase {
   public void testMergeAllDeleted() throws IOException {
     Directory dir = newDirectory();
     IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    final SetOnce<IndexWriter> iwRef = new SetOnce<IndexWriter>();
+    final SetOnce<IndexWriter> iwRef = new SetOnce<>();
     iwc.setInfoStream(new RandomIndexWriter.TestPointInfoStream(iwc.getInfoStream(), new RandomIndexWriter.TestPoint() {
       @Override
       public void apply(String message) {
diff --git lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit.java lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit.java
index a36cbc3..9bb2c5c 100644
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit.java
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit.java
@@ -425,7 +425,7 @@ public class TestIndexWriterCommit extends LuceneTestCase {
     w.addDocument(doc);
 
     // commit to "first"
-    Map<String,String> commitData = new HashMap<String,String>();
+    Map<String,String> commitData = new HashMap<>();
     commitData.put("tag", "first");
     w.setCommitData(commitData);
     w.commit();
@@ -633,7 +633,7 @@ public class TestIndexWriterCommit extends LuceneTestCase {
     w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2));
     for(int j=0;j<17;j++)
       TestIndexWriter.addDoc(w);
-    Map<String,String> data = new HashMap<String,String>();
+    Map<String,String> data = new HashMap<>();
     data.put("label", "test1");
     w.setCommitData(data);
     w.close();
diff --git lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java
index bdd83a7..8df58c9 100644
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java
@@ -82,7 +82,7 @@ public class TestIndexWriterConfig extends LuceneTestCase {
     assertEquals(InfoStream.getDefault(), conf.getInfoStream());
     assertEquals(IndexWriterConfig.DEFAULT_USE_COMPOUND_FILE_SYSTEM, conf.getUseCompoundFile());
     // Sanity check - validate that all getters are covered.
-    Set<String> getters = new HashSet<String>();
+    Set<String> getters = new HashSet<>();
     getters.add("getAnalyzer");
     getters.add("getIndexCommit");
     getters.add("getIndexDeletionPolicy");
@@ -117,8 +117,8 @@ public class TestIndexWriterConfig extends LuceneTestCase {
   @Test
   public void testSettersChaining() throws Exception {
     // Ensures that every setter returns IndexWriterConfig to allow chaining.
-    HashSet<String> liveSetters = new HashSet<String>();
-    HashSet<String> allSetters = new HashSet<String>();
+    HashSet<String> liveSetters = new HashSet<>();
+    HashSet<String> allSetters = new HashSet<>();
     for (Method m : IndexWriterConfig.class.getDeclaredMethods()) {
       if (m.getName().startsWith("set") && !Modifier.isStatic(m.getModifiers())) {
         allSetters.add(m.getName());
@@ -175,7 +175,7 @@ public class TestIndexWriterConfig extends LuceneTestCase {
     // Test that IndexWriterConfig overrides all getters, so that javadocs
     // contain all methods for the users. Also, ensures that IndexWriterConfig
     // doesn't declare getters that are not declared on LiveIWC.
-    HashSet<String> liveGetters = new HashSet<String>();
+    HashSet<String> liveGetters = new HashSet<>();
     for (Method m : LiveIndexWriterConfig.class.getDeclaredMethods()) {
       if (m.getName().startsWith("get") && !Modifier.isStatic(m.getModifiers())) {
         liveGetters.add(m.getName());
diff --git lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
index 36af529..445c4af 100644
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
@@ -939,7 +939,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
     final Directory dir = newDirectory();
     RandomIndexWriter w = new RandomIndexWriter(random(), dir);
     final int NUM_DOCS = atLeast(1000);
-    final List<Integer> ids = new ArrayList<Integer>(NUM_DOCS);
+    final List<Integer> ids = new ArrayList<>(NUM_DOCS);
     for(int id=0;id<NUM_DOCS;id++) {
       ids.add(id);
     }
diff --git lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
index 39762ae..1689080 100644
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
@@ -215,7 +215,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
     }
   }
 
-  ThreadLocal<Thread> doFail = new ThreadLocal<Thread>();
+  ThreadLocal<Thread> doFail = new ThreadLocal<>();
 
   private class TestPoint1 implements RandomIndexWriter.TestPoint {
     Random r = new Random(random().nextLong());
@@ -1355,7 +1355,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       w.addDocument(doc);
     }
     
-    final List<Document> docs = new ArrayList<Document>();
+    final List<Document> docs = new ArrayList<>();
     for(int docCount=0;docCount<7;docCount++) {
       Document doc = new Document();
       docs.add(doc);
@@ -1415,7 +1415,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
     }
 
     // Use addDocs (no exception) to get docs in the index:
-    final List<Document> docs = new ArrayList<Document>();
+    final List<Document> docs = new ArrayList<>();
     final int numDocs2 = random().nextInt(25);
     for(int docCount=0;docCount<numDocs2;docCount++) {
       Document doc = new Document();
@@ -1575,8 +1575,8 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       // try to boost with norms omitted
       IndexDocument docList = new IndexDocument() {
         
-        List<IndexableField> list = new ArrayList<IndexableField>();
-        List<StorableField> storedList = new ArrayList<StorableField>();
+        List<IndexableField> list = new ArrayList<>();
+        List<StorableField> storedList = new ArrayList<>();
         
         @Override
         public Iterable<IndexableField> indexableFields() {
diff --git lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java
index 5e87625..6100b9d 100644
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java
@@ -399,7 +399,7 @@ public class TestIndexWriterMerging extends LuceneTestCase
         ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(2);
 
         final IndexWriter finalWriter = writer;
-        final ArrayList<Throwable> failure = new ArrayList<Throwable>();
+        final ArrayList<Throwable> failure = new ArrayList<>();
         Thread t1 = new Thread() {
             @Override
             public void run() {
diff --git lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnJRECrash.java lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnJRECrash.java
index 3c6dd3e..9620ed7 100644
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnJRECrash.java
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnJRECrash.java
@@ -88,7 +88,7 @@ public class TestIndexWriterOnJRECrash extends TestNRTThreads {
   
   /** fork ourselves in a new jvm. sets -Dtests.crashmode=true */
   public void forkTest() throws Exception {
-    List<String> cmd = new ArrayList<String>();
+    List<String> cmd = new ArrayList<>();
     cmd.add(System.getProperty("java.home") 
         + System.getProperty("file.separator")
         + "bin"
diff --git lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOutOfFileDescriptors.java lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOutOfFileDescriptors.java
index 9e2c458..d5cd794 100644
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOutOfFileDescriptors.java
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOutOfFileDescriptors.java
@@ -132,7 +132,7 @@ public class TestIndexWriterOutOfFileDescriptors extends LuceneTestCase {
         dir.setRandomIOExceptionRateOnOpen(0.0);
         r = DirectoryReader.open(dir);
         dirCopy = newMockFSDirectory(TestUtil.getTempDir("TestIndexWriterOutOfFileDescriptors.copy"));
-        Set<String> files = new HashSet<String>();
+        Set<String> files = new HashSet<>();
         for (String file : dir.listAll()) {
           dir.copy(dirCopy, file, file, IOContext.DEFAULT);
           files.add(file);
diff --git lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java
index 5fedf2b..80b77a5 100644
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java
@@ -402,7 +402,7 @@ public class TestIndexWriterReader extends LuceneTestCase {
     int numDirs;
     final Thread[] threads = new Thread[numThreads];
     IndexWriter mainWriter;
-    final List<Throwable> failures = new ArrayList<Throwable>();
+    final List<Throwable> failures = new ArrayList<>();
     IndexReader[] readers;
     boolean didClose = false;
     AtomicInteger count = new AtomicInteger(0);
diff --git lucene/core/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java lucene/core/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java
index b0f44f0..be3170d 100644
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java
@@ -137,7 +137,7 @@ public class TestIndexWriterUnicode extends LuceneTestCase {
 
     BytesRef last = new BytesRef();
 
-    Set<String> seenTerms = new HashSet<String>();
+    Set<String> seenTerms = new HashSet<>();
 
     while(true) {
       final BytesRef term = terms.next();
@@ -279,7 +279,7 @@ public class TestIndexWriterUnicode extends LuceneTestCase {
     Field f = newStringField("f", "", Field.Store.NO);
     d.add(f);
     char[] chars = new char[2];
-    final Set<String> allTerms = new HashSet<String>();
+    final Set<String> allTerms = new HashSet<>();
 
     int num = atLeast(200);
     for (int i = 0; i < num; i++) {
diff --git lucene/core/src/test/org/apache/lucene/index/TestIndexWriterWithThreads.java lucene/core/src/test/org/apache/lucene/index/TestIndexWriterWithThreads.java
index 00ebb55..f905695 100644
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterWithThreads.java
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterWithThreads.java
@@ -547,7 +547,7 @@ public class TestIndexWriterWithThreads extends LuceneTestCase {
 
     final int threadCount = TestUtil.nextInt(random(), 2, 6);
 
-    final AtomicReference<IndexWriter> writerRef = new AtomicReference<IndexWriter>();
+    final AtomicReference<IndexWriter> writerRef = new AtomicReference<>();
     MockAnalyzer analyzer = new MockAnalyzer(random());
     analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));
 
diff --git lucene/core/src/test/org/apache/lucene/index/TestIntBlockPool.java lucene/core/src/test/org/apache/lucene/index/TestIntBlockPool.java
index 8fb87b2..199c3a7 100644
--- lucene/core/src/test/org/apache/lucene/index/TestIntBlockPool.java
+++ lucene/core/src/test/org/apache/lucene/index/TestIntBlockPool.java
@@ -63,7 +63,7 @@ public class TestIntBlockPool extends LuceneTestCase {
     Counter bytesUsed = Counter.newCounter();
     IntBlockPool pool = new IntBlockPool(new ByteTrackingAllocator(bytesUsed));
     for (int j = 0; j < 2; j++) {
-      List<StartEndAndValues> holders = new ArrayList<TestIntBlockPool.StartEndAndValues>();
+      List<StartEndAndValues> holders = new ArrayList<>();
       int num = atLeast(4);
       for (int i = 0; i < num; i++) {
         holders.add(new StartEndAndValues(random().nextInt(1000)));
diff --git lucene/core/src/test/org/apache/lucene/index/TestMaxTermFrequency.java lucene/core/src/test/org/apache/lucene/index/TestMaxTermFrequency.java
index 8441185..078d7c9 100644
--- lucene/core/src/test/org/apache/lucene/index/TestMaxTermFrequency.java
+++ lucene/core/src/test/org/apache/lucene/index/TestMaxTermFrequency.java
@@ -39,7 +39,7 @@ public class TestMaxTermFrequency extends LuceneTestCase {
   Directory dir;
   IndexReader reader;
   /* expected maxTermFrequency values for our documents */
-  ArrayList<Integer> expected = new ArrayList<Integer>();
+  ArrayList<Integer> expected = new ArrayList<>();
   
   @Override
   public void setUp() throws Exception {
@@ -81,7 +81,7 @@ public class TestMaxTermFrequency extends LuceneTestCase {
    * puts the max-frequency term into expected, to be checked against the norm.
    */
   private String addValue() {
-    List<String> terms = new ArrayList<String>();
+    List<String> terms = new ArrayList<>();
     int maxCeiling = TestUtil.nextInt(random(), 0, 255);
     int max = 0;
     for (char ch = 'a'; ch <= 'z'; ch++) {
diff --git lucene/core/src/test/org/apache/lucene/index/TestMixedCodecs.java lucene/core/src/test/org/apache/lucene/index/TestMixedCodecs.java
index 9cb54b1..708c160 100644
--- lucene/core/src/test/org/apache/lucene/index/TestMixedCodecs.java
+++ lucene/core/src/test/org/apache/lucene/index/TestMixedCodecs.java
@@ -70,7 +70,7 @@ public class TestMixedCodecs extends LuceneTestCase {
     }
 
     // Random delete half the docs:
-    final Set<Integer> deleted = new HashSet<Integer>();
+    final Set<Integer> deleted = new HashSet<>();
     while(deleted.size() < NUM_DOCS/2) {
       final Integer toDelete = random().nextInt(NUM_DOCS);
       if (!deleted.contains(toDelete)) {
diff --git lucene/core/src/test/org/apache/lucene/index/TestMultiDocValues.java lucene/core/src/test/org/apache/lucene/index/TestMultiDocValues.java
index 72d546f..6794588 100644
--- lucene/core/src/test/org/apache/lucene/index/TestMultiDocValues.java
+++ lucene/core/src/test/org/apache/lucene/index/TestMultiDocValues.java
@@ -241,7 +241,7 @@ public class TestMultiDocValues extends LuceneTestCase {
       // check ord list
       for (int i = 0; i < numDocs; i++) {
         single.setDocument(i);
-        ArrayList<Long> expectedList = new ArrayList<Long>();
+        ArrayList<Long> expectedList = new ArrayList<>();
         long ord;
         while ((ord = single.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {
           expectedList.add(ord);
@@ -306,7 +306,7 @@ public class TestMultiDocValues extends LuceneTestCase {
       // check ord list
       for (int i = 0; i < numDocs; i++) {
         single.setDocument(i);
-        ArrayList<Long> expectedList = new ArrayList<Long>();
+        ArrayList<Long> expectedList = new ArrayList<>();
         long ord;
         while ((ord = single.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {
           expectedList.add(ord);
diff --git lucene/core/src/test/org/apache/lucene/index/TestMultiFields.java lucene/core/src/test/org/apache/lucene/index/TestMultiFields.java
index 6c162fe..ec45a2d 100644
--- lucene/core/src/test/org/apache/lucene/index/TestMultiFields.java
+++ lucene/core/src/test/org/apache/lucene/index/TestMultiFields.java
@@ -40,9 +40,9 @@ public class TestMultiFields extends LuceneTestCase {
       // we can do this because we use NoMergePolicy (and dont merge to "nothing")
       w.setKeepFullyDeletedSegments(true);
 
-      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();
-      Set<Integer> deleted = new HashSet<Integer>();
-      List<BytesRef> terms = new ArrayList<BytesRef>();
+      Map<BytesRef,List<Integer>> docs = new HashMap<>();
+      Set<Integer> deleted = new HashSet<>();
+      List<BytesRef> terms = new ArrayList<>();
 
       int numDocs = TestUtil.nextInt(random(), 1, 100 * RANDOM_MULTIPLIER);
       Document doc = new Document();
@@ -55,7 +55,7 @@ public class TestMultiFields extends LuceneTestCase {
       if (VERBOSE) {
         System.out.println("TEST: onlyUniqueTerms=" + onlyUniqueTerms + " numDocs=" + numDocs);
       }
-      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();
+      Set<BytesRef> uniqueTerms = new HashSet<>();
       for(int i=0;i<numDocs;i++) {
 
         if (!onlyUniqueTerms && random().nextBoolean() && terms.size() > 0) {
@@ -90,7 +90,7 @@ public class TestMultiFields extends LuceneTestCase {
       }
 
       if (VERBOSE) {
-        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);
+        List<BytesRef> termsList = new ArrayList<>(uniqueTerms);
         Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());
         System.out.println("TEST: terms in UTF16 order:");
         for(BytesRef b : termsList) {
diff --git lucene/core/src/test/org/apache/lucene/index/TestNeverDelete.java lucene/core/src/test/org/apache/lucene/index/TestNeverDelete.java
index 9f45e21..fc0df11 100644
--- lucene/core/src/test/org/apache/lucene/index/TestNeverDelete.java
+++ lucene/core/src/test/org/apache/lucene/index/TestNeverDelete.java
@@ -80,7 +80,7 @@ public class TestNeverDelete extends LuceneTestCase {
       indexThreads[x].start();
     }
 
-    final Set<String> allFiles = new HashSet<String>();
+    final Set<String> allFiles = new HashSet<>();
 
     DirectoryReader r = DirectoryReader.open(d);
     while(System.currentTimeMillis() < stopTime) {
diff --git lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
index 4d81ba5..1491d37 100644
--- lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
+++ lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
@@ -1202,7 +1202,7 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
     
     final int numDocs = atLeast(50);
     final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);
-    Set<String> randomTerms = new HashSet<String>();
+    Set<String> randomTerms = new HashSet<>();
     while (randomTerms.size() < numTerms) {
       randomTerms.add(TestUtil.randomSimpleString(random()));
     }
@@ -1300,7 +1300,7 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
     final int numDocs = atLeast(20000);
     final int numNumericFields = atLeast(5);
     final int numTerms = TestUtil.nextInt(random, 10, 100); // terms should affect many docs
-    Set<String> updateTerms = new HashSet<String>();
+    Set<String> updateTerms = new HashSet<>();
     while (updateTerms.size() < numTerms) {
       updateTerms.add(TestUtil.randomSimpleString(random));
     }
diff --git lucene/core/src/test/org/apache/lucene/index/TestPayloads.java lucene/core/src/test/org/apache/lucene/index/TestPayloads.java
index dda5ab3..9e21e7c 100644
--- lucene/core/src/test/org/apache/lucene/index/TestPayloads.java
+++ lucene/core/src/test/org/apache/lucene/index/TestPayloads.java
@@ -359,7 +359,7 @@ public class TestPayloads extends LuceneTestCase {
      * This Analyzer uses an WhitespaceTokenizer and PayloadFilter.
      */
     private static class PayloadAnalyzer extends Analyzer {
-        Map<String,PayloadData> fieldToData = new HashMap<String,PayloadData>();
+        Map<String,PayloadData> fieldToData = new HashMap<>();
 
         public PayloadAnalyzer() {
           super(PER_FIELD_REUSE_STRATEGY);
@@ -539,7 +539,7 @@ public class TestPayloads extends LuceneTestCase {
         private List<byte[]> pool;
         
         ByteArrayPool(int capacity, int size) {
-            pool = new ArrayList<byte[]>();
+            pool = new ArrayList<>();
             for (int i = 0; i < capacity; i++) {
                 pool.add(new byte[size]);
             }
diff --git lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java
index c504542..0fded22 100644
--- lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java
+++ lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java
@@ -234,7 +234,7 @@ public class TestPerSegmentDeletes extends LuceneTestCase {
   }
 
   public static int[] toArray(DocsEnum docsEnum) throws IOException {
-    List<Integer> docs = new ArrayList<Integer>();
+    List<Integer> docs = new ArrayList<>();
     while (docsEnum.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
       int docID = docsEnum.docID();
       docs.add(docID);
diff --git lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java
index c460af4..f7238a5 100644
--- lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java
+++ lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java
@@ -220,7 +220,7 @@ public class TestPostingsOffsets extends LuceneTestCase {
 
   public void testRandom() throws Exception {
     // token -> docID -> tokens
-    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();
+    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();
 
     Directory dir = newDirectory();
     RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
@@ -242,7 +242,7 @@ public class TestPostingsOffsets extends LuceneTestCase {
     for(int docCount=0;docCount<numDocs;docCount++) {
       Document doc = new Document();
       doc.add(new IntField("id", docCount, Field.Store.YES));
-      List<Token> tokens = new ArrayList<Token>();
+      List<Token> tokens = new ArrayList<>();
       final int numTokens = atLeast(100);
       //final int numTokens = atLeast(20);
       int pos = -1;
diff --git lucene/core/src/test/org/apache/lucene/index/TestPrefixCodedTerms.java lucene/core/src/test/org/apache/lucene/index/TestPrefixCodedTerms.java
index 11d2c23..61dafa9 100644
--- lucene/core/src/test/org/apache/lucene/index/TestPrefixCodedTerms.java
+++ lucene/core/src/test/org/apache/lucene/index/TestPrefixCodedTerms.java
@@ -46,7 +46,7 @@ public class TestPrefixCodedTerms extends LuceneTestCase {
   }
   
   public void testRandom() {
-    Set<Term> terms = new TreeSet<Term>();
+    Set<Term> terms = new TreeSet<>();
     int nterms = atLeast(10000);
     for (int i = 0; i < nterms; i++) {
       Term term = new Term(TestUtil.randomUnicodeString(random(), 2), TestUtil.randomUnicodeString(random()));
@@ -79,7 +79,7 @@ public class TestPrefixCodedTerms extends LuceneTestCase {
     b2.add(t2);
     PrefixCodedTerms pb2 = b2.finish();
     
-    Iterator<Term> merged = new MergedIterator<Term>(pb1.iterator(), pb2.iterator());
+    Iterator<Term> merged = new MergedIterator<>(pb1.iterator(), pb2.iterator());
     assertTrue(merged.hasNext());
     assertEquals(t1, merged.next());
     assertTrue(merged.hasNext());
@@ -89,10 +89,10 @@ public class TestPrefixCodedTerms extends LuceneTestCase {
   @SuppressWarnings({"unchecked","rawtypes"})
   public void testMergeRandom() {
     PrefixCodedTerms pb[] = new PrefixCodedTerms[TestUtil.nextInt(random(), 2, 10)];
-    Set<Term> superSet = new TreeSet<Term>();
+    Set<Term> superSet = new TreeSet<>();
     
     for (int i = 0; i < pb.length; i++) {
-      Set<Term> terms = new TreeSet<Term>();
+      Set<Term> terms = new TreeSet<>();
       int nterms = TestUtil.nextInt(random(), 0, 10000);
       for (int j = 0; j < nterms; j++) {
         Term term = new Term(TestUtil.randomUnicodeString(random(), 2), TestUtil.randomUnicodeString(random(), 4));
@@ -107,7 +107,7 @@ public class TestPrefixCodedTerms extends LuceneTestCase {
       pb[i] = b.finish();
     }
     
-    List<Iterator<Term>> subs = new ArrayList<Iterator<Term>>();
+    List<Iterator<Term>> subs = new ArrayList<>();
     for (int i = 0; i < pb.length; i++) {
       subs.add(pb[i].iterator());
     }
diff --git lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java
index 11e1ab1..f9f1f49 100644
--- lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java
+++ lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java
@@ -75,11 +75,11 @@ public class TestSegmentReader extends LuceneTestCase {
   }
   
   public void testGetFieldNameVariations() {
-    Collection<String> allFieldNames = new HashSet<String>();
-    Collection<String> indexedFieldNames = new HashSet<String>();
-    Collection<String> notIndexedFieldNames = new HashSet<String>();
-    Collection<String> tvFieldNames = new HashSet<String>();
-    Collection<String> noTVFieldNames = new HashSet<String>();
+    Collection<String> allFieldNames = new HashSet<>();
+    Collection<String> indexedFieldNames = new HashSet<>();
+    Collection<String> notIndexedFieldNames = new HashSet<>();
+    Collection<String> tvFieldNames = new HashSet<>();
+    Collection<String> noTVFieldNames = new HashSet<>();
 
     for(FieldInfo fieldInfo : reader.getFieldInfos()) {
       final String name = fieldInfo.name;
diff --git lucene/core/src/test/org/apache/lucene/index/TestSnapshotDeletionPolicy.java lucene/core/src/test/org/apache/lucene/index/TestSnapshotDeletionPolicy.java
index 84fd32e..599a64d 100644
--- lucene/core/src/test/org/apache/lucene/index/TestSnapshotDeletionPolicy.java
+++ lucene/core/src/test/org/apache/lucene/index/TestSnapshotDeletionPolicy.java
@@ -63,7 +63,7 @@ public class TestSnapshotDeletionPolicy extends LuceneTestCase {
     }
   }
 
-  protected List<IndexCommit> snapshots = new ArrayList<IndexCommit>();
+  protected List<IndexCommit> snapshots = new ArrayList<>();
 
   protected void prepareIndexAndSnapshots(SnapshotDeletionPolicy sdp,
       IndexWriter writer, int numSnapshots)
diff --git lucene/core/src/test/org/apache/lucene/index/TestStressAdvance.java lucene/core/src/test/org/apache/lucene/index/TestStressAdvance.java
index c24b171..bd35b50 100644
--- lucene/core/src/test/org/apache/lucene/index/TestStressAdvance.java
+++ lucene/core/src/test/org/apache/lucene/index/TestStressAdvance.java
@@ -35,7 +35,7 @@ public class TestStressAdvance extends LuceneTestCase {
       }
       Directory dir = newDirectory();
       RandomIndexWriter w = new RandomIndexWriter(random(), dir);
-      final Set<Integer> aDocs = new HashSet<Integer>();
+      final Set<Integer> aDocs = new HashSet<>();
       final Document doc = new Document();
       final Field f = newStringField("field", "", Field.Store.NO);
       doc.add(f);
@@ -61,8 +61,8 @@ public class TestStressAdvance extends LuceneTestCase {
 
       w.forceMerge(1);
 
-      final List<Integer> aDocIDs = new ArrayList<Integer>();
-      final List<Integer> bDocIDs = new ArrayList<Integer>();
+      final List<Integer> aDocIDs = new ArrayList<>();
+      final List<Integer> bDocIDs = new ArrayList<>();
 
       final DirectoryReader r = w.getReader();
       final int[] idToDocID = new int[r.maxDoc()];
diff --git lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java
index 9c154cb..199a484 100644
--- lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java
+++ lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java
@@ -145,7 +145,7 @@ public class TestStressIndexing2 extends LuceneTestCase {
   }
   
   public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {
-    Map<String,Document> docs = new HashMap<String,Document>();
+    Map<String,Document> docs = new HashMap<>();
     IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.CREATE).setRAMBufferSizeMB(
             0.1).setMaxBufferedDocs(maxBufferedDocs).setMergePolicy(newLogMergePolicy()), new YieldTestPoint());
@@ -196,7 +196,7 @@ public class TestStressIndexing2 extends LuceneTestCase {
   
   public Map<String,Document> indexRandom(int nThreads, int iterations, int range, Directory dir, int maxThreadStates,
                                           boolean doReaderPooling) throws IOException, InterruptedException {
-    Map<String,Document> docs = new HashMap<String,Document>();
+    Map<String,Document> docs = new HashMap<>();
     IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.CREATE)
              .setRAMBufferSizeMB(0.1).setMaxBufferedDocs(maxBufferedDocs).setIndexerThreadPool(new ThreadAffinityDocumentsWriterThreadPool(maxThreadStates))
@@ -246,7 +246,7 @@ public class TestStressIndexing2 extends LuceneTestCase {
     Iterator<Document> iter = docs.values().iterator();
     while (iter.hasNext()) {
       Document d = iter.next();
-      ArrayList<Field> fields = new ArrayList<Field>();
+      ArrayList<Field> fields = new ArrayList<>();
       fields.addAll(d.getFields());
       // put fields in same order each time
       Collections.sort(fields, fieldNameComparator);
@@ -690,7 +690,7 @@ public class TestStressIndexing2 extends LuceneTestCase {
     int base;
     int range;
     int iterations;
-    Map<String,Document> docs = new HashMap<String,Document>();  
+    Map<String,Document> docs = new HashMap<>();
     Random r;
 
     public int nextInt(int lim) {
@@ -768,7 +768,7 @@ public class TestStressIndexing2 extends LuceneTestCase {
       customType1.setTokenized(false);
       customType1.setOmitNorms(true);
       
-      ArrayList<Field> fields = new ArrayList<Field>();      
+      ArrayList<Field> fields = new ArrayList<>();
       String idString = getIdString();
       Field idField =  newField("id", idString, customType1);
       fields.add(idField);
diff --git lucene/core/src/test/org/apache/lucene/index/TestStressNRT.java lucene/core/src/test/org/apache/lucene/index/TestStressNRT.java
index fa3b909..c891c03 100644
--- lucene/core/src/test/org/apache/lucene/index/TestStressNRT.java
+++ lucene/core/src/test/org/apache/lucene/index/TestStressNRT.java
@@ -42,8 +42,8 @@ import org.apache.lucene.util.TestUtil;
 public class TestStressNRT extends LuceneTestCase {
   volatile DirectoryReader reader;
 
-  final ConcurrentHashMap<Integer,Long> model = new ConcurrentHashMap<Integer,Long>();
-  Map<Integer,Long> committedModel = new HashMap<Integer,Long>();
+  final ConcurrentHashMap<Integer,Long> model = new ConcurrentHashMap<>();
+  Map<Integer,Long> committedModel = new HashMap<>();
   long snapshotCount;
   long committedModelClock;
   volatile int lastId;
@@ -102,7 +102,7 @@ public class TestStressNRT extends LuceneTestCase {
 
     final AtomicInteger numCommitting = new AtomicInteger();
 
-    List<Thread> threads = new ArrayList<Thread>();
+    List<Thread> threads = new ArrayList<>();
 
     Directory dir = newDirectory();
 
@@ -128,7 +128,7 @@ public class TestStressNRT extends LuceneTestCase {
                   DirectoryReader oldReader;
 
                   synchronized(TestStressNRT.this) {
-                    newCommittedModel = new HashMap<Integer,Long>(model);  // take a snapshot
+                    newCommittedModel = new HashMap<>(model);  // take a snapshot
                     version = snapshotCount++;
                     oldReader = reader;
                     oldReader.incRef();  // increment the reference since we will use this for reopening
diff --git lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java
index 1385f3e..9e07167 100644
--- lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java
+++ lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java
@@ -54,7 +54,7 @@ public class TestTermsEnum extends LuceneTestCase {
     final IndexReader r = w.getReader();
     w.close();
 
-    final List<BytesRef> terms = new ArrayList<BytesRef>();
+    final List<BytesRef> terms = new ArrayList<>();
     final TermsEnum termsEnum = MultiFields.getTerms(r, "body").iterator(null);
     BytesRef term;
     while((term = termsEnum.next()) != null) {
@@ -188,9 +188,9 @@ public class TestTermsEnum extends LuceneTestCase {
     final int numTerms = atLeast(300);
     //final int numTerms = 50;
 
-    final Set<String> terms = new HashSet<String>();
-    final Collection<String> pendingTerms = new ArrayList<String>();
-    final Map<BytesRef,Integer> termToID = new HashMap<BytesRef,Integer>();
+    final Set<String> terms = new HashSet<>();
+    final Collection<String> pendingTerms = new ArrayList<>();
+    final Map<BytesRef,Integer> termToID = new HashMap<>();
     int id = 0;
     while(terms.size() != numTerms) {
       final String s = getRandomString();
@@ -205,7 +205,7 @@ public class TestTermsEnum extends LuceneTestCase {
     addDoc(w, pendingTerms, termToID, id++);
 
     final BytesRef[] termsArray = new BytesRef[terms.size()];
-    final Set<BytesRef> termsSet = new HashSet<BytesRef>();
+    final Set<BytesRef> termsSet = new HashSet<>();
     {
       int upto = 0;
       for(String s : terms) {
@@ -235,8 +235,8 @@ public class TestTermsEnum extends LuceneTestCase {
 
       // From the random terms, pick some ratio and compile an
       // automaton:
-      final Set<String> acceptTerms = new HashSet<String>();
-      final TreeSet<BytesRef> sortedAcceptTerms = new TreeSet<BytesRef>();
+      final Set<String> acceptTerms = new HashSet<>();
+      final TreeSet<BytesRef> sortedAcceptTerms = new TreeSet<>();
       final double keepPct = random().nextDouble();
       Automaton a;
       if (iter == 0) {
@@ -271,7 +271,7 @@ public class TestTermsEnum extends LuceneTestCase {
       final CompiledAutomaton c = new CompiledAutomaton(a, true, false);
 
       final BytesRef[] acceptTermsArray = new BytesRef[acceptTerms.size()];
-      final Set<BytesRef> acceptTermsSet = new HashSet<BytesRef>();
+      final Set<BytesRef> acceptTermsSet = new HashSet<>();
       int upto = 0;
       for(String s : acceptTerms) {
         final BytesRef b = new BytesRef(s);
@@ -531,7 +531,7 @@ public class TestTermsEnum extends LuceneTestCase {
 
   public void testRandomTerms() throws Exception {
     final String[] terms = new String[TestUtil.nextInt(random(), 1, atLeast(1000))];
-    final Set<String> seen = new HashSet<String>();
+    final Set<String> seen = new HashSet<>();
 
     final boolean allowEmptyString = random().nextBoolean();
 
@@ -622,7 +622,7 @@ public class TestTermsEnum extends LuceneTestCase {
 
     final int END_LOC = -validTerms.length-1;
     
-    final List<TermAndState> termStates = new ArrayList<TermAndState>();
+    final List<TermAndState> termStates = new ArrayList<>();
 
     for(int iter=0;iter<100*RANDOM_MULTIPLIER;iter++) {
 
diff --git lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2.java lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2.java
index fb1356e..af87fb3 100644
--- lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2.java
+++ lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2.java
@@ -58,7 +58,7 @@ public class TestTermsEnum2 extends LuceneTestCase {
     Document doc = new Document();
     Field field = newStringField("field", "", Field.Store.YES);
     doc.add(field);
-    terms = new TreeSet<BytesRef>();
+    terms = new TreeSet<>();
  
     int num = atLeast(200);
     for (int i = 0; i < num; i++) {
@@ -87,7 +87,7 @@ public class TestTermsEnum2 extends LuceneTestCase {
     for (int i = 0; i < numIterations; i++) {
       String reg = AutomatonTestUtil.randomRegexp(random());
       Automaton automaton = new RegExp(reg, RegExp.NONE).toAutomaton();
-      final List<BytesRef> matchedTerms = new ArrayList<BytesRef>();
+      final List<BytesRef> matchedTerms = new ArrayList<>();
       for(BytesRef t : terms) {
         if (BasicOperations.run(automaton, t.utf8ToString())) {
           matchedTerms.add(t);
@@ -110,7 +110,7 @@ public class TestTermsEnum2 extends LuceneTestCase {
       String reg = AutomatonTestUtil.randomRegexp(random());
       Automaton automaton = new RegExp(reg, RegExp.NONE).toAutomaton();
       TermsEnum te = MultiFields.getTerms(reader, "field").iterator(null);
-      ArrayList<BytesRef> unsortedTerms = new ArrayList<BytesRef>(terms);
+      ArrayList<BytesRef> unsortedTerms = new ArrayList<>(terms);
       Collections.shuffle(unsortedTerms, random());
 
       for (BytesRef term : unsortedTerms) {
@@ -156,7 +156,7 @@ public class TestTermsEnum2 extends LuceneTestCase {
       CompiledAutomaton ca = new CompiledAutomaton(automaton, SpecialOperations.isFinite(automaton), false);
       TermsEnum te = MultiFields.getTerms(reader, "field").intersect(ca, null);
       Automaton expected = BasicOperations.intersection(termsAutomaton, automaton);
-      TreeSet<BytesRef> found = new TreeSet<BytesRef>();
+      TreeSet<BytesRef> found = new TreeSet<>();
       while (te.next() != null) {
         found.add(BytesRef.deepCopyOf(te.term()));
       }
diff --git lucene/core/src/test/org/apache/lucene/index/TestTransactionRollback.java lucene/core/src/test/org/apache/lucene/index/TestTransactionRollback.java
index ef926db..2c2b869 100644
--- lucene/core/src/test/org/apache/lucene/index/TestTransactionRollback.java
+++ lucene/core/src/test/org/apache/lucene/index/TestTransactionRollback.java
@@ -69,7 +69,7 @@ public class TestTransactionRollback extends LuceneTestCase {
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random())).setIndexDeletionPolicy(
         new RollbackDeletionPolicy(id)).setIndexCommit(last));
-    Map<String,String> data = new HashMap<String,String>();
+    Map<String,String> data = new HashMap<>();
     data.put("index", "Rolled back to 1-"+id);
     w.setCommitData(data);
     w.close();
@@ -139,7 +139,7 @@ public class TestTransactionRollback extends LuceneTestCase {
       w.addDocument(doc);
 
       if (currentRecordId%10 == 0) {
-        Map<String,String> data = new HashMap<String,String>();
+        Map<String,String> data = new HashMap<>();
         data.put("index", "records 1-"+currentRecordId);
         w.setCommitData(data);
         w.commit();
diff --git lucene/core/src/test/org/apache/lucene/index/TestUniqueTermCount.java lucene/core/src/test/org/apache/lucene/index/TestUniqueTermCount.java
index 5d87e9f..bf3f4d5 100644
--- lucene/core/src/test/org/apache/lucene/index/TestUniqueTermCount.java
+++ lucene/core/src/test/org/apache/lucene/index/TestUniqueTermCount.java
@@ -39,7 +39,7 @@ public class TestUniqueTermCount extends LuceneTestCase {
   Directory dir;
   IndexReader reader;
   /* expected uniqueTermCount values for our documents */
-  ArrayList<Integer> expected = new ArrayList<Integer>();
+  ArrayList<Integer> expected = new ArrayList<>();
   
   @Override
   public void setUp() throws Exception {
@@ -82,7 +82,7 @@ public class TestUniqueTermCount extends LuceneTestCase {
    */
   private String addValue() {
     StringBuilder sb = new StringBuilder();
-    HashSet<String> terms = new HashSet<String>();
+    HashSet<String> terms = new HashSet<>();
     int num = TestUtil.nextInt(random(), 0, 255);
     for (int i = 0; i < num; i++) {
       sb.append(' ');
diff --git lucene/core/src/test/org/apache/lucene/search/TestBooleanQuery.java lucene/core/src/test/org/apache/lucene/search/TestBooleanQuery.java
index faa779a..de736e4 100644
--- lucene/core/src/test/org/apache/lucene/search/TestBooleanQuery.java
+++ lucene/core/src/test/org/apache/lucene/search/TestBooleanQuery.java
@@ -217,7 +217,7 @@ public class TestBooleanQuery extends LuceneTestCase {
       if (VERBOSE) {
         System.out.println("iter=" + iter);
       }
-      final List<String> terms = new ArrayList<String>(Arrays.asList("a", "b", "c", "d", "e", "f"));
+      final List<String> terms = new ArrayList<>(Arrays.asList("a", "b", "c", "d", "e", "f"));
       final int numTerms = TestUtil.nextInt(random(), 1, terms.size());
       while(terms.size() > numTerms) {
         terms.remove(random().nextInt(terms.size()));
@@ -238,7 +238,7 @@ public class TestBooleanQuery extends LuceneTestCase {
                                           true, false, null);
 
       // First pass: just use .nextDoc() to gather all hits
-      final List<ScoreDoc> hits = new ArrayList<ScoreDoc>();
+      final List<ScoreDoc> hits = new ArrayList<>();
       while(scorer.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
         hits.add(new ScoreDoc(scorer.docID(), scorer.score()));
       }
diff --git lucene/core/src/test/org/apache/lucene/search/TestBooleanQueryVisitSubscorers.java lucene/core/src/test/org/apache/lucene/search/TestBooleanQueryVisitSubscorers.java
index 744ab68..175061a 100644
--- lucene/core/src/test/org/apache/lucene/search/TestBooleanQueryVisitSubscorers.java
+++ lucene/core/src/test/org/apache/lucene/search/TestBooleanQueryVisitSubscorers.java
@@ -126,8 +126,8 @@ public class TestBooleanQueryVisitSubscorers extends LuceneTestCase {
     private TopDocsCollector<ScoreDoc> collector;
     private int docBase;
 
-    public final Map<Integer,Integer> docCounts = new HashMap<Integer,Integer>();
-    private final Set<Scorer> tqsSet = new HashSet<Scorer>();
+    public final Map<Integer,Integer> docCounts = new HashMap<>();
+    private final Set<Scorer> tqsSet = new HashSet<>();
     
     MyCollector() {
       collector = TopScoreDocCollector.create(10, true);
diff --git lucene/core/src/test/org/apache/lucene/search/TestBooleanScorer.java lucene/core/src/test/org/apache/lucene/search/TestBooleanScorer.java
index ffeac52..733a74a 100644
--- lucene/core/src/test/org/apache/lucene/search/TestBooleanScorer.java
+++ lucene/core/src/test/org/apache/lucene/search/TestBooleanScorer.java
@@ -100,7 +100,7 @@ public class TestBooleanScorer extends LuceneTestCase
     
     BooleanScorer bs = new BooleanScorer(weight, false, 1, Arrays.asList(scorers), null, scorers.length);
 
-    final List<Integer> hits = new ArrayList<Integer>();
+    final List<Integer> hits = new ArrayList<>();
     bs.score(new Collector() {
       int docBase;
       @Override
diff --git lucene/core/src/test/org/apache/lucene/search/TestControlledRealTimeReopenThread.java lucene/core/src/test/org/apache/lucene/search/TestControlledRealTimeReopenThread.java
index 0959d51..28877c6 100644
--- lucene/core/src/test/org/apache/lucene/search/TestControlledRealTimeReopenThread.java
+++ lucene/core/src/test/org/apache/lucene/search/TestControlledRealTimeReopenThread.java
@@ -65,7 +65,7 @@ public class TestControlledRealTimeReopenThread extends ThreadedIndexingAndSearc
   private ControlledRealTimeReopenThread<IndexSearcher> nrtDeletesThread;
   private ControlledRealTimeReopenThread<IndexSearcher> nrtNoDeletesThread;
 
-  private final ThreadLocal<Long> lastGens = new ThreadLocal<Long>();
+  private final ThreadLocal<Long> lastGens = new ThreadLocal<>();
   private boolean warmCalled;
 
   public void testControlledRealTimeReopenThread() throws Exception {
@@ -232,13 +232,13 @@ public class TestControlledRealTimeReopenThread extends ThreadedIndexingAndSearc
     nrtNoDeletes = new SearcherManager(writer, false, sf);
     nrtDeletes = new SearcherManager(writer, true, sf);
                          
-    nrtDeletesThread = new ControlledRealTimeReopenThread<IndexSearcher>(genWriter, nrtDeletes, maxReopenSec, minReopenSec);
+    nrtDeletesThread = new ControlledRealTimeReopenThread<>(genWriter, nrtDeletes, maxReopenSec, minReopenSec);
     nrtDeletesThread.setName("NRTDeletes Reopen Thread");
     nrtDeletesThread.setPriority(Math.min(Thread.currentThread().getPriority()+2, Thread.MAX_PRIORITY));
     nrtDeletesThread.setDaemon(true);
     nrtDeletesThread.start();
 
-    nrtNoDeletesThread = new ControlledRealTimeReopenThread<IndexSearcher>(genWriter, nrtNoDeletes, maxReopenSec, minReopenSec);
+    nrtNoDeletesThread = new ControlledRealTimeReopenThread<>(genWriter, nrtNoDeletes, maxReopenSec, minReopenSec);
     nrtNoDeletesThread.setName("NRTNoDeletes Reopen Thread");
     nrtNoDeletesThread.setPriority(Math.min(Thread.currentThread().getPriority()+2, Thread.MAX_PRIORITY));
     nrtNoDeletesThread.setDaemon(true);
@@ -343,7 +343,7 @@ public class TestControlledRealTimeReopenThread extends ThreadedIndexingAndSearc
     } finally {
       manager.release(searcher);
     }
-    final ControlledRealTimeReopenThread<IndexSearcher> thread = new ControlledRealTimeReopenThread<IndexSearcher>(writer, manager, 0.01, 0.01);
+    final ControlledRealTimeReopenThread<IndexSearcher> thread = new ControlledRealTimeReopenThread<>(writer, manager, 0.01, 0.01);
     thread.start(); // start reopening
     if (VERBOSE) {
       System.out.println("waiting now for generation " + lastGen);
@@ -482,12 +482,12 @@ public class TestControlledRealTimeReopenThread extends ThreadedIndexingAndSearc
     SearcherManager sm = new SearcherManager(iw, true, new SearcherFactory());
     final TrackingIndexWriter tiw = new TrackingIndexWriter(iw);
     ControlledRealTimeReopenThread<IndexSearcher> controlledRealTimeReopenThread =
-      new ControlledRealTimeReopenThread<IndexSearcher>(tiw, sm, maxStaleSecs, 0);
+      new ControlledRealTimeReopenThread<>(tiw, sm, maxStaleSecs, 0);
 
     controlledRealTimeReopenThread.setDaemon(true);
     controlledRealTimeReopenThread.start();
 
-    List<Thread> commitThreads = new ArrayList<Thread>();
+    List<Thread> commitThreads = new ArrayList<>();
 
     for (int i = 0; i < 500; i++) {
       if (i > 0 && i % 50 == 0) {
diff --git lucene/core/src/test/org/apache/lucene/search/TestCustomSearcherSort.java lucene/core/src/test/org/apache/lucene/search/TestCustomSearcherSort.java
index 3c872a5..a42c8f8 100644
--- lucene/core/src/test/org/apache/lucene/search/TestCustomSearcherSort.java
+++ lucene/core/src/test/org/apache/lucene/search/TestCustomSearcherSort.java
@@ -112,7 +112,7 @@ public class TestCustomSearcherSort extends LuceneTestCase {
     // make a query without sorting first
     ScoreDoc[] hitsByRank = searcher.search(query, null, Integer.MAX_VALUE).scoreDocs;
     checkHits(hitsByRank, "Sort by rank: "); // check for duplicates
-    Map<Integer,Integer> resultMap = new TreeMap<Integer,Integer>();
+    Map<Integer,Integer> resultMap = new TreeMap<>();
     // store hits in TreeMap - TreeMap does not allow duplicates; existing
     // entries are silently overwritten
     for (int hitid = 0; hitid < hitsByRank.length; ++hitid) {
@@ -155,7 +155,7 @@ public class TestCustomSearcherSort extends LuceneTestCase {
    */
   private void checkHits(ScoreDoc[] hits, String prefix) {
     if (hits != null) {
-      Map<Integer,Integer> idMap = new TreeMap<Integer,Integer>();
+      Map<Integer,Integer> idMap = new TreeMap<>();
       for (int docnum = 0; docnum < hits.length; ++docnum) {
         Integer luceneId = null;
         
diff --git lucene/core/src/test/org/apache/lucene/search/TestDocIdSet.java lucene/core/src/test/org/apache/lucene/search/TestDocIdSet.java
index c0fc918..4f23f88 100644
--- lucene/core/src/test/org/apache/lucene/search/TestDocIdSet.java
+++ lucene/core/src/test/org/apache/lucene/search/TestDocIdSet.java
@@ -77,7 +77,7 @@ public class TestDocIdSet extends LuceneTestCase {
       };
 
     DocIdSetIterator iter = filteredSet.iterator();
-    ArrayList<Integer> list = new ArrayList<Integer>();
+    ArrayList<Integer> list = new ArrayList<>();
     int doc = iter.advance(3);
     if (doc != DocIdSetIterator.NO_MORE_DOCS) {
       list.add(Integer.valueOf(doc));
diff --git lucene/core/src/test/org/apache/lucene/search/TestDocTermOrdsRangeFilter.java lucene/core/src/test/org/apache/lucene/search/TestDocTermOrdsRangeFilter.java
index fc11cb0..8fe3858 100644
--- lucene/core/src/test/org/apache/lucene/search/TestDocTermOrdsRangeFilter.java
+++ lucene/core/src/test/org/apache/lucene/search/TestDocTermOrdsRangeFilter.java
@@ -54,7 +54,7 @@ public class TestDocTermOrdsRangeFilter extends LuceneTestCase {
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir, 
         newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))
         .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));
-    List<String> terms = new ArrayList<String>();
+    List<String> terms = new ArrayList<>();
     int num = atLeast(200);
     for (int i = 0; i < num; i++) {
       Document doc = new Document();
diff --git lucene/core/src/test/org/apache/lucene/search/TestDocTermOrdsRewriteMethod.java lucene/core/src/test/org/apache/lucene/search/TestDocTermOrdsRewriteMethod.java
index 64c5382..31d973b 100644
--- lucene/core/src/test/org/apache/lucene/search/TestDocTermOrdsRewriteMethod.java
+++ lucene/core/src/test/org/apache/lucene/search/TestDocTermOrdsRewriteMethod.java
@@ -56,7 +56,7 @@ public class TestDocTermOrdsRewriteMethod extends LuceneTestCase {
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir, 
         newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))
         .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));
-    List<String> terms = new ArrayList<String>();
+    List<String> terms = new ArrayList<>();
     int num = atLeast(200);
     for (int i = 0; i < num; i++) {
       Document doc = new Document();
diff --git lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java
index 082884a..816e582 100644
--- lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java
+++ lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java
@@ -32,7 +32,7 @@ import java.util.Map;
 
 public class TestElevationComparator extends LuceneTestCase {
 
-  private final Map<BytesRef,Integer> priority = new HashMap<BytesRef,Integer>();
+  private final Map<BytesRef,Integer> priority = new HashMap<>();
 
   //@Test
   public void testSorting() throws Throwable {
diff --git lucene/core/src/test/org/apache/lucene/search/TestFieldCache.java lucene/core/src/test/org/apache/lucene/search/TestFieldCache.java
index 8c2148a..f78f6f8 100644
--- lucene/core/src/test/org/apache/lucene/search/TestFieldCache.java
+++ lucene/core/src/test/org/apache/lucene/search/TestFieldCache.java
@@ -278,7 +278,7 @@ public class TestFieldCache extends LuceneTestCase {
     for (int i = 0; i < NUM_DOCS; i++) {
       termOrds.setDocument(i);
       // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId
-      List<BytesRef> values = new ArrayList<BytesRef>(new LinkedHashSet<BytesRef>(Arrays.asList(multiValued[i])));
+      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));
       for (BytesRef v : values) {
         if (v == null) {
           // why does this test use null values... instead of an empty list: confusing
diff --git lucene/core/src/test/org/apache/lucene/search/TestFieldCacheTermsFilter.java lucene/core/src/test/org/apache/lucene/search/TestFieldCacheTermsFilter.java
index bd6a78e..8e022b6 100644
--- lucene/core/src/test/org/apache/lucene/search/TestFieldCacheTermsFilter.java
+++ lucene/core/src/test/org/apache/lucene/search/TestFieldCacheTermsFilter.java
@@ -52,17 +52,17 @@ public class TestFieldCacheTermsFilter extends LuceneTestCase {
     ScoreDoc[] results;
     MatchAllDocsQuery q = new MatchAllDocsQuery();
 
-    List<String> terms = new ArrayList<String>();
+    List<String> terms = new ArrayList<>();
     terms.add("5");
     results = searcher.search(q, new FieldCacheTermsFilter(fieldName,  terms.toArray(new String[0])), numDocs).scoreDocs;
     assertEquals("Must match nothing", 0, results.length);
 
-    terms = new ArrayList<String>();
+    terms = new ArrayList<>();
     terms.add("10");
     results = searcher.search(q, new FieldCacheTermsFilter(fieldName,  terms.toArray(new String[0])), numDocs).scoreDocs;
     assertEquals("Must match 1", 1, results.length);
 
-    terms = new ArrayList<String>();
+    terms = new ArrayList<>();
     terms.add("10");
     terms.add("20");
     results = searcher.search(q, new FieldCacheTermsFilter(fieldName,  terms.toArray(new String[0])), numDocs).scoreDocs;
diff --git lucene/core/src/test/org/apache/lucene/search/TestLiveFieldValues.java lucene/core/src/test/org/apache/lucene/search/TestLiveFieldValues.java
index 2da514f..6a4f295 100644
--- lucene/core/src/test/org/apache/lucene/search/TestLiveFieldValues.java
+++ lucene/core/src/test/org/apache/lucene/search/TestLiveFieldValues.java
@@ -79,7 +79,7 @@ public class TestLiveFieldValues extends LuceneTestCase {
     }
 
     final CountDownLatch startingGun = new CountDownLatch(1);
-    List<Thread> threads = new ArrayList<Thread>();
+    List<Thread> threads = new ArrayList<>();
 
     final int iters = atLeast(1000);
     final int idCount = TestUtil.nextInt(random(), 100, 10000);
@@ -96,7 +96,7 @@ public class TestLiveFieldValues extends LuceneTestCase {
           @Override
           public void run() {
             try {
-              Map<String,Integer> values = new HashMap<String,Integer>();
+              Map<String,Integer> values = new HashMap<>();
               List<String> allIDs = Collections.synchronizedList(new ArrayList<String>());
 
               startingGun.await();
diff --git lucene/core/src/test/org/apache/lucene/search/TestMinShouldMatch2.java lucene/core/src/test/org/apache/lucene/search/TestMinShouldMatch2.java
index 4ca3722..6b51665 100644
--- lucene/core/src/test/org/apache/lucene/search/TestMinShouldMatch2.java
+++ lucene/core/src/test/org/apache/lucene/search/TestMinShouldMatch2.java
@@ -194,7 +194,7 @@ public class TestMinShouldMatch2 extends LuceneTestCase {
   
   /** test next with giant bq of all terms with varying minShouldMatch */
   public void testNextAllTerms() throws Exception {
-    List<String> termsList = new ArrayList<String>();
+    List<String> termsList = new ArrayList<>();
     termsList.addAll(Arrays.asList(commonTerms));
     termsList.addAll(Arrays.asList(mediumTerms));
     termsList.addAll(Arrays.asList(rareTerms));
@@ -209,7 +209,7 @@ public class TestMinShouldMatch2 extends LuceneTestCase {
   
   /** test advance with giant bq of all terms with varying minShouldMatch */
   public void testAdvanceAllTerms() throws Exception {
-    List<String> termsList = new ArrayList<String>();
+    List<String> termsList = new ArrayList<>();
     termsList.addAll(Arrays.asList(commonTerms));
     termsList.addAll(Arrays.asList(mediumTerms));
     termsList.addAll(Arrays.asList(rareTerms));
@@ -226,7 +226,7 @@ public class TestMinShouldMatch2 extends LuceneTestCase {
   
   /** test next with varying numbers of terms with varying minShouldMatch */
   public void testNextVaryingNumberOfTerms() throws Exception {
-    List<String> termsList = new ArrayList<String>();
+    List<String> termsList = new ArrayList<>();
     termsList.addAll(Arrays.asList(commonTerms));
     termsList.addAll(Arrays.asList(mediumTerms));
     termsList.addAll(Arrays.asList(rareTerms));
@@ -243,7 +243,7 @@ public class TestMinShouldMatch2 extends LuceneTestCase {
   
   /** test advance with varying numbers of terms with varying minShouldMatch */
   public void testAdvanceVaryingNumberOfTerms() throws Exception {
-    List<String> termsList = new ArrayList<String>();
+    List<String> termsList = new ArrayList<>();
     termsList.addAll(Arrays.asList(commonTerms));
     termsList.addAll(Arrays.asList(mediumTerms));
     termsList.addAll(Arrays.asList(rareTerms));
@@ -273,7 +273,7 @@ public class TestMinShouldMatch2 extends LuceneTestCase {
     final SortedSetDocValues dv;
     final int maxDoc;
 
-    final Set<Long> ords = new HashSet<Long>();
+    final Set<Long> ords = new HashSet<>();
     final SimScorer[] sims;
     final int minNrShouldMatch;
     
diff --git lucene/core/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java lucene/core/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java
index 392dc6b..1b352a1 100644
--- lucene/core/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java
+++ lucene/core/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java
@@ -70,7 +70,7 @@ public class TestMultiPhraseQuery extends LuceneTestCase {
     query1.add(new Term("body", "blueberry"));
     query2.add(new Term("body", "strawberry"));
     
-    LinkedList<Term> termsWithPrefix = new LinkedList<Term>();
+    LinkedList<Term> termsWithPrefix = new LinkedList<>();
     
     // this TermEnum gives "piccadilly", "pie" and "pizza".
     String prefix = "pi";
diff --git lucene/core/src/test/org/apache/lucene/search/TestPhrasePrefixQuery.java lucene/core/src/test/org/apache/lucene/search/TestPhrasePrefixQuery.java
index 8fe492e..26cf76a 100644
--- lucene/core/src/test/org/apache/lucene/search/TestPhrasePrefixQuery.java
+++ lucene/core/src/test/org/apache/lucene/search/TestPhrasePrefixQuery.java
@@ -69,7 +69,7 @@ public class TestPhrasePrefixQuery extends LuceneTestCase {
     query1.add(new Term("body", "blueberry"));
     query2.add(new Term("body", "strawberry"));
     
-    LinkedList<Term> termsWithPrefix = new LinkedList<Term>();
+    LinkedList<Term> termsWithPrefix = new LinkedList<>();
     
     // this TermEnum gives "piccadilly", "pie" and "pizza".
     String prefix = "pi";
diff --git lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery.java lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery.java
index d96dad8..ca0a33f 100644
--- lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery.java
+++ lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery.java
@@ -590,7 +590,7 @@ public class TestPhraseQuery extends LuceneTestCase {
     Analyzer analyzer = new MockAnalyzer(random());
 
     RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));
-    List<List<String>> docs = new ArrayList<List<String>>();
+    List<List<String>> docs = new ArrayList<>();
     Document d = new Document();
     Field f = newTextField("f", "", Field.Store.NO);
     d.add(f);
@@ -602,7 +602,7 @@ public class TestPhraseQuery extends LuceneTestCase {
       // must be > 4096 so it spans multiple chunks
       int termCount = TestUtil.nextInt(random(), 4097, 8200);
 
-      List<String> doc = new ArrayList<String>();
+      List<String> doc = new ArrayList<>();
 
       StringBuilder sb = new StringBuilder();
       while(doc.size() < termCount) {
diff --git lucene/core/src/test/org/apache/lucene/search/TestQueryWrapperFilter.java lucene/core/src/test/org/apache/lucene/search/TestQueryWrapperFilter.java
index dd6bef5..d727aec 100644
--- lucene/core/src/test/org/apache/lucene/search/TestQueryWrapperFilter.java
+++ lucene/core/src/test/org/apache/lucene/search/TestQueryWrapperFilter.java
@@ -88,7 +88,7 @@ public class TestQueryWrapperFilter extends LuceneTestCase {
     final RandomIndexWriter w = new RandomIndexWriter(random(), d);
     w.w.getConfig().setMaxBufferedDocs(17);
     final int numDocs = atLeast(100);
-    final Set<String> aDocs = new HashSet<String>();
+    final Set<String> aDocs = new HashSet<>();
     for(int i=0;i<numDocs;i++) {
       final Document doc = new Document();
       final String v;
diff --git lucene/core/src/test/org/apache/lucene/search/TestRegexpRandom2.java lucene/core/src/test/org/apache/lucene/search/TestRegexpRandom2.java
index 582e00f..ba12fa2 100644
--- lucene/core/src/test/org/apache/lucene/search/TestRegexpRandom2.java
+++ lucene/core/src/test/org/apache/lucene/search/TestRegexpRandom2.java
@@ -66,7 +66,7 @@ public class TestRegexpRandom2 extends LuceneTestCase {
     Document doc = new Document();
     Field field = newStringField(fieldName, "", Field.Store.NO);
     doc.add(field);
-    List<String> terms = new ArrayList<String>();
+    List<String> terms = new ArrayList<>();
     int num = atLeast(200);
     for (int i = 0; i < num; i++) {
       String s = TestUtil.randomUnicodeString(random());
diff --git lucene/core/src/test/org/apache/lucene/search/TestSameScoresWithThreads.java lucene/core/src/test/org/apache/lucene/search/TestSameScoresWithThreads.java
index ba8059f..3abd501 100644
--- lucene/core/src/test/org/apache/lucene/search/TestSameScoresWithThreads.java
+++ lucene/core/src/test/org/apache/lucene/search/TestSameScoresWithThreads.java
@@ -72,7 +72,7 @@ public class TestSameScoresWithThreads extends LuceneTestCase {
     // Target ~10 terms to search:
     double chance = 10.0 / termCount;
     termsEnum = terms.iterator(termsEnum);
-    final Map<BytesRef,TopDocs> answers = new HashMap<BytesRef,TopDocs>();
+    final Map<BytesRef,TopDocs> answers = new HashMap<>();
     while(termsEnum.next() != null) {
       if (random().nextDouble() <= chance) {
         BytesRef term = BytesRef.deepCopyOf(termsEnum.term());
@@ -92,7 +92,7 @@ public class TestSameScoresWithThreads extends LuceneTestCase {
               try {
                 startingGun.await();
                 for(int i=0;i<20;i++) {
-                  List<Map.Entry<BytesRef,TopDocs>> shuffled = new ArrayList<Map.Entry<BytesRef,TopDocs>>(answers.entrySet());
+                  List<Map.Entry<BytesRef,TopDocs>> shuffled = new ArrayList<>(answers.entrySet());
                   Collections.shuffle(shuffled);
                   for(Map.Entry<BytesRef,TopDocs> ent : shuffled) {
                     TopDocs actual = s.search(new TermQuery(new Term("body", ent.getKey())), 100);
diff --git lucene/core/src/test/org/apache/lucene/search/TestSearchAfter.java lucene/core/src/test/org/apache/lucene/search/TestSearchAfter.java
index 38e8914..efb96e0 100644
--- lucene/core/src/test/org/apache/lucene/search/TestSearchAfter.java
+++ lucene/core/src/test/org/apache/lucene/search/TestSearchAfter.java
@@ -56,7 +56,7 @@ public class TestSearchAfter extends LuceneTestCase {
   public void setUp() throws Exception {
     super.setUp();
 
-    allSortFields = new ArrayList<SortField>(Arrays.asList(new SortField[] {
+    allSortFields = new ArrayList<>(Arrays.asList(new SortField[] {
           new SortField("int", SortField.Type.INT, false),
           new SortField("long", SortField.Type.LONG, false),
           new SortField("float", SortField.Type.FLOAT, false),
@@ -123,7 +123,7 @@ public class TestSearchAfter extends LuceneTestCase {
     RandomIndexWriter iw = new RandomIndexWriter(random(), dir);
     int numDocs = atLeast(200);
     for (int i = 0; i < numDocs; i++) {
-      List<Field> fields = new ArrayList<Field>();
+      List<Field> fields = new ArrayList<>();
       fields.add(newTextField("english", English.intToEnglish(i), Field.Store.NO));
       fields.add(newTextField("oddeven", (i % 2 == 0) ? "even" : "odd", Field.Store.NO));
       fields.add(newStringField("byte", "" + ((byte) random().nextInt()), Field.Store.NO));
diff --git lucene/core/src/test/org/apache/lucene/search/TestSearcherManager.java lucene/core/src/test/org/apache/lucene/search/TestSearcherManager.java
index 963ec79..cfe72c4 100644
--- lucene/core/src/test/org/apache/lucene/search/TestSearcherManager.java
+++ lucene/core/src/test/org/apache/lucene/search/TestSearcherManager.java
@@ -67,7 +67,7 @@ public class TestSearcherManager extends ThreadedIndexingAndSearchingTestCase {
 
   private SearcherManager mgr;
   private SearcherLifetimeManager lifetimeMGR;
-  private final List<Long> pastSearchers = new ArrayList<Long>();
+  private final List<Long> pastSearchers = new ArrayList<>();
   private boolean isNRT;
 
   @Override
diff --git lucene/core/src/test/org/apache/lucene/search/TestShardSearching.java lucene/core/src/test/org/apache/lucene/search/TestShardSearching.java
index 7d2c585..ea54c26 100644
--- lucene/core/src/test/org/apache/lucene/search/TestShardSearching.java
+++ lucene/core/src/test/org/apache/lucene/search/TestShardSearching.java
@@ -82,7 +82,7 @@ public class TestShardSearching extends ShardSearchingTestBase {
           maxSearcherAgeSeconds
           );
 
-    final List<PreviousSearchState> priorSearches = new ArrayList<PreviousSearchState>();
+    final List<PreviousSearchState> priorSearches = new ArrayList<>();
     List<BytesRef> terms = null;
     while (System.nanoTime() < endTimeNanos) {
 
@@ -175,7 +175,7 @@ public class TestShardSearching extends ShardSearchingTestBase {
             // TODO: try to "focus" on high freq terms sometimes too
             // TODO: maybe also periodically reset the terms...?
             final TermsEnum termsEnum = MultiFields.getTerms(mockReader, "body").iterator(null);
-            terms = new ArrayList<BytesRef>();
+            terms = new ArrayList<>();
             while(termsEnum.next() != null) {
               terms.add(BytesRef.deepCopyOf(termsEnum.term()));
             }
diff --git lucene/core/src/test/org/apache/lucene/search/TestSortRandom.java lucene/core/src/test/org/apache/lucene/search/TestSortRandom.java
index bfa3dd7..faa6560 100644
--- lucene/core/src/test/org/apache/lucene/search/TestSortRandom.java
+++ lucene/core/src/test/org/apache/lucene/search/TestSortRandom.java
@@ -53,14 +53,14 @@ public class TestSortRandom extends LuceneTestCase {
     final Directory dir = newDirectory();
     final RandomIndexWriter writer = new RandomIndexWriter(random, dir);
     final boolean allowDups = random.nextBoolean();
-    final Set<String> seen = new HashSet<String>();
+    final Set<String> seen = new HashSet<>();
     final int maxLength = TestUtil.nextInt(random, 5, 100);
     if (VERBOSE) {
       System.out.println("TEST: NUM_DOCS=" + NUM_DOCS + " maxLength=" + maxLength + " allowDups=" + allowDups);
     }
 
     int numDocs = 0;
-    final List<BytesRef> docValues = new ArrayList<BytesRef>();
+    final List<BytesRef> docValues = new ArrayList<>();
     // TODO: deletions
     while (numDocs < NUM_DOCS) {
       final Document doc = new Document();
diff --git lucene/core/src/test/org/apache/lucene/search/TestSubScorerFreqs.java lucene/core/src/test/org/apache/lucene/search/TestSubScorerFreqs.java
index f82caa8..063d26b 100644
--- lucene/core/src/test/org/apache/lucene/search/TestSubScorerFreqs.java
+++ lucene/core/src/test/org/apache/lucene/search/TestSubScorerFreqs.java
@@ -69,13 +69,13 @@ public class TestSubScorerFreqs extends LuceneTestCase {
     private final Collector other;
     private int docBase;
 
-    public final Map<Integer, Map<Query, Float>> docCounts = new HashMap<Integer, Map<Query, Float>>();
+    public final Map<Integer, Map<Query, Float>> docCounts = new HashMap<>();
 
-    private final Map<Query, Scorer> subScorers = new HashMap<Query, Scorer>();
+    private final Map<Query, Scorer> subScorers = new HashMap<>();
     private final Set<String> relationships;
 
     public CountingCollector(Collector other) {
-      this(other, new HashSet<String>(Arrays.asList("MUST", "SHOULD", "MUST_NOT")));
+      this(other, new HashSet<>(Arrays.asList("MUST", "SHOULD", "MUST_NOT")));
     }
 
     public CountingCollector(Collector other, Set<String> relationships) {
@@ -101,7 +101,7 @@ public class TestSubScorerFreqs extends LuceneTestCase {
 
     @Override
     public void collect(int doc) throws IOException {
-      final Map<Query, Float> freqs = new HashMap<Query, Float>();
+      final Map<Query, Float> freqs = new HashMap<>();
       for (Map.Entry<Query, Scorer> ent : subScorers.entrySet()) {
         Scorer value = ent.getValue();
         int matchId = value.docID();
@@ -165,7 +165,7 @@ public class TestSubScorerFreqs extends LuceneTestCase {
     // see http://docs.oracle.com/javase/7/docs/api/java/lang/SafeVarargs.html
     @SuppressWarnings("unchecked") final Iterable<Set<String>> occurList = Arrays.asList(
         Collections.singleton("MUST"), 
-        new HashSet<String>(Arrays.asList("MUST", "SHOULD"))
+        new HashSet<>(Arrays.asList("MUST", "SHOULD"))
     );
     
     for (final Set<String> occur : occurList) {
diff --git lucene/core/src/test/org/apache/lucene/search/TestTermScorer.java lucene/core/src/test/org/apache/lucene/search/TestTermScorer.java
index 0c4e229..2deb259 100644
--- lucene/core/src/test/org/apache/lucene/search/TestTermScorer.java
+++ lucene/core/src/test/org/apache/lucene/search/TestTermScorer.java
@@ -81,7 +81,7 @@ public class TestTermScorer extends LuceneTestCase {
     Scorer ts = weight.scorer(context, true, true, context.reader().getLiveDocs());
     // we have 2 documents with the term all in them, one document for all the
     // other values
-    final List<TestHit> docs = new ArrayList<TestHit>();
+    final List<TestHit> docs = new ArrayList<>();
     // must call next first
     
     ts.score(new Collector() {
diff --git lucene/core/src/test/org/apache/lucene/search/TestTopDocsMerge.java lucene/core/src/test/org/apache/lucene/search/TestTopDocsMerge.java
index 1dcd43d..ef89211 100644
--- lucene/core/src/test/org/apache/lucene/search/TestTopDocsMerge.java
+++ lucene/core/src/test/org/apache/lucene/search/TestTopDocsMerge.java
@@ -145,7 +145,7 @@ public class TestTopDocsMerge extends LuceneTestCase {
       }
     }
 
-    final List<SortField> sortFields = new ArrayList<SortField>();
+    final List<SortField> sortFields = new ArrayList<>();
     sortFields.add(new SortField("string", SortField.Type.STRING, true));
     sortFields.add(new SortField("string", SortField.Type.STRING, false));
     sortFields.add(new SortField("int", SortField.Type.INT, true));
diff --git lucene/core/src/test/org/apache/lucene/search/similarities/TestSimilarity2.java lucene/core/src/test/org/apache/lucene/search/similarities/TestSimilarity2.java
index 730877a..abe5a6d 100644
--- lucene/core/src/test/org/apache/lucene/search/similarities/TestSimilarity2.java
+++ lucene/core/src/test/org/apache/lucene/search/similarities/TestSimilarity2.java
@@ -48,7 +48,7 @@ public class TestSimilarity2 extends LuceneTestCase {
   @Override
   public void setUp() throws Exception {
     super.setUp();
-    sims = new ArrayList<Similarity>();
+    sims = new ArrayList<>();
     sims.add(new DefaultSimilarity());
     sims.add(new BM25Similarity());
     // TODO: not great that we dup this all with TestSimilarityBase
diff --git lucene/core/src/test/org/apache/lucene/search/similarities/TestSimilarityBase.java lucene/core/src/test/org/apache/lucene/search/similarities/TestSimilarityBase.java
index f526c6b..91861f9 100644
--- lucene/core/src/test/org/apache/lucene/search/similarities/TestSimilarityBase.java
+++ lucene/core/src/test/org/apache/lucene/search/similarities/TestSimilarityBase.java
@@ -121,7 +121,7 @@ public class TestSimilarityBase extends LuceneTestCase {
     searcher = newSearcher(reader);
     writer.close();
     
-    sims = new ArrayList<SimilarityBase>();
+    sims = new ArrayList<>();
     for (BasicModel basicModel : BASIC_MODELS) {
       for (AfterEffect afterEffect : AFTER_EFFECTS) {
         for (Normalization normalization : NORMALIZATIONS) {
diff --git lucene/core/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java lucene/core/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java
index fcab7ae..a07db83 100644
--- lucene/core/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java
+++ lucene/core/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java
@@ -56,8 +56,8 @@ public class MultiSpansWrapper extends Spans { // can't be package private due t
   }
   
   public static Spans wrap(IndexReaderContext topLevelReaderContext, SpanQuery query) throws IOException {
-    Map<Term,TermContext> termContexts = new HashMap<Term,TermContext>();
-    TreeSet<Term> terms = new TreeSet<Term>();
+    Map<Term,TermContext> termContexts = new HashMap<>();
+    TreeSet<Term> terms = new TreeSet<>();
     query.extractTerms(terms);
     for (Term term : terms) {
       termContexts.put(term, TermContext.build(topLevelReaderContext, term));
diff --git lucene/core/src/test/org/apache/lucene/search/spans/TestBasics.java lucene/core/src/test/org/apache/lucene/search/spans/TestBasics.java
index 2dfeb1c..53314e8 100644
--- lucene/core/src/test/org/apache/lucene/search/spans/TestBasics.java
+++ lucene/core/src/test/org/apache/lucene/search/spans/TestBasics.java
@@ -499,7 +499,7 @@ public class TestBasics extends LuceneTestCase {
     snq = new SpanNearQuery(clauses, 0, true);
     pay = new BytesRef(("pos: " + 0).getBytes("UTF-8"));
     pay2 = new BytesRef(("pos: " + 1).getBytes("UTF-8"));
-    list = new ArrayList<byte[]>();
+    list = new ArrayList<>();
     list.add(pay.bytes);
     list.add(pay2.bytes);
     query = new SpanNearPayloadCheckQuery(snq, list);
@@ -513,7 +513,7 @@ public class TestBasics extends LuceneTestCase {
     pay = new BytesRef(("pos: " + 0).getBytes("UTF-8"));
     pay2 = new BytesRef(("pos: " + 1).getBytes("UTF-8"));
     BytesRef pay3 = new BytesRef(("pos: " + 2).getBytes("UTF-8"));
-    list = new ArrayList<byte[]>();
+    list = new ArrayList<>();
     list.add(pay.bytes);
     list.add(pay2.bytes);
     list.add(pay3.bytes);
@@ -540,7 +540,7 @@ public class TestBasics extends LuceneTestCase {
     query = new SpanPositionRangeQuery(oneThousHunThree, 0, 6);
     checkHits(query, new int[]{1103, 1203,1303,1403,1503,1603,1703,1803,1903});
 
-    Collection<byte[]> payloads = new ArrayList<byte[]>();
+    Collection<byte[]> payloads = new ArrayList<>();
     BytesRef pay = new BytesRef(("pos: " + 0).getBytes("UTF-8"));
     BytesRef pay2 = new BytesRef(("pos: " + 1).getBytes("UTF-8"));
     BytesRef pay3 = new BytesRef(("pos: " + 3).getBytes("UTF-8"));
diff --git lucene/core/src/test/org/apache/lucene/search/spans/TestFieldMaskingSpanQuery.java lucene/core/src/test/org/apache/lucene/search/spans/TestFieldMaskingSpanQuery.java
index 0971b20..d5b55c6 100644
--- lucene/core/src/test/org/apache/lucene/search/spans/TestFieldMaskingSpanQuery.java
+++ lucene/core/src/test/org/apache/lucene/search/spans/TestFieldMaskingSpanQuery.java
@@ -138,7 +138,7 @@ public class TestFieldMaskingSpanQuery extends LuceneTestCase {
 
     QueryUtils.checkEqual(q, qr);
 
-    Set<Term> terms = new HashSet<Term>();
+    Set<Term> terms = new HashSet<>();
     qr.extractTerms(terms);
     assertEquals(1, terms.size());
   }
@@ -158,7 +158,7 @@ public class TestFieldMaskingSpanQuery extends LuceneTestCase {
 
     QueryUtils.checkUnequal(q, qr);
 
-    Set<Term> terms = new HashSet<Term>();
+    Set<Term> terms = new HashSet<>();
     qr.extractTerms(terms);
     assertEquals(2, terms.size());
   }
@@ -172,7 +172,7 @@ public class TestFieldMaskingSpanQuery extends LuceneTestCase {
 
     QueryUtils.checkEqual(q, qr);
 
-    HashSet<Term> set = new HashSet<Term>();
+    HashSet<Term> set = new HashSet<>();
     qr.extractTerms(set);
     assertEquals(2, set.size());
   }
diff --git lucene/core/src/test/org/apache/lucene/search/spans/TestPayloadSpans.java lucene/core/src/test/org/apache/lucene/search/spans/TestPayloadSpans.java
index 9f5481c..485430b 100644
--- lucene/core/src/test/org/apache/lucene/search/spans/TestPayloadSpans.java
+++ lucene/core/src/test/org/apache/lucene/search/spans/TestPayloadSpans.java
@@ -270,7 +270,7 @@ public class TestPayloadSpans extends LuceneTestCase {
     Spans spans = MultiSpansWrapper.wrap(is.getTopReaderContext(), snq);
 
     TopDocs topDocs = is.search(snq, 1);
-    Set<String> payloadSet = new HashSet<String>();
+    Set<String> payloadSet = new HashSet<>();
     for (int i = 0; i < topDocs.scoreDocs.length; i++) {
       while (spans.next()) {
         Collection<byte[]> payloads = spans.getPayload();
@@ -306,7 +306,7 @@ public class TestPayloadSpans extends LuceneTestCase {
     Spans spans =  MultiSpansWrapper.wrap(is.getTopReaderContext(), snq);
 
     TopDocs topDocs = is.search(snq, 1);
-    Set<String> payloadSet = new HashSet<String>();
+    Set<String> payloadSet = new HashSet<>();
     for (int i = 0; i < topDocs.scoreDocs.length; i++) {
       while (spans.next()) {
         Collection<byte[]> payloads = spans.getPayload();
@@ -341,7 +341,7 @@ public class TestPayloadSpans extends LuceneTestCase {
     Spans spans =  MultiSpansWrapper.wrap(is.getTopReaderContext(), snq);
 
     TopDocs topDocs = is.search(snq, 1);
-    Set<String> payloadSet = new HashSet<String>();
+    Set<String> payloadSet = new HashSet<>();
     for (int i = 0; i < topDocs.scoreDocs.length; i++) {
       while (spans.next()) {
         Collection<byte[]> payloads = spans.getPayload();
@@ -479,8 +479,8 @@ public class TestPayloadSpans extends LuceneTestCase {
   }
 
   final class PayloadFilter extends TokenFilter {
-    Set<String> entities = new HashSet<String>();
-    Set<String> nopayload = new HashSet<String>();
+    Set<String> entities = new HashSet<>();
+    Set<String> nopayload = new HashSet<>();
     int pos;
     PayloadAttribute payloadAtt;
     CharTermAttribute termAtt;
diff --git lucene/core/src/test/org/apache/lucene/search/spans/TestSpanMultiTermQueryWrapper.java lucene/core/src/test/org/apache/lucene/search/spans/TestSpanMultiTermQueryWrapper.java
index 4dad293..c5023c7 100644
--- lucene/core/src/test/org/apache/lucene/search/spans/TestSpanMultiTermQueryWrapper.java
+++ lucene/core/src/test/org/apache/lucene/search/spans/TestSpanMultiTermQueryWrapper.java
@@ -67,7 +67,7 @@ public class TestSpanMultiTermQueryWrapper extends LuceneTestCase {
   
   public void testWildcard() throws Exception {
     WildcardQuery wq = new WildcardQuery(new Term("field", "bro?n"));
-    SpanQuery swq = new SpanMultiTermQueryWrapper<WildcardQuery>(wq);
+    SpanQuery swq = new SpanMultiTermQueryWrapper<>(wq);
     // will only match quick brown fox
     SpanFirstQuery sfq = new SpanFirstQuery(swq, 2);
     assertEquals(1, searcher.search(sfq, 10).totalHits);
@@ -75,7 +75,7 @@ public class TestSpanMultiTermQueryWrapper extends LuceneTestCase {
   
   public void testPrefix() throws Exception {
     WildcardQuery wq = new WildcardQuery(new Term("field", "extrem*"));
-    SpanQuery swq = new SpanMultiTermQueryWrapper<WildcardQuery>(wq);
+    SpanQuery swq = new SpanMultiTermQueryWrapper<>(wq);
     // will only match "jumps over extremely very lazy broxn dog"
     SpanFirstQuery sfq = new SpanFirstQuery(swq, 3);
     assertEquals(1, searcher.search(sfq, 10).totalHits);
@@ -83,7 +83,7 @@ public class TestSpanMultiTermQueryWrapper extends LuceneTestCase {
   
   public void testFuzzy() throws Exception {
     FuzzyQuery fq = new FuzzyQuery(new Term("field", "broan"));
-    SpanQuery sfq = new SpanMultiTermQueryWrapper<FuzzyQuery>(fq);
+    SpanQuery sfq = new SpanMultiTermQueryWrapper<>(fq);
     // will not match quick brown fox
     SpanPositionRangeQuery sprq = new SpanPositionRangeQuery(sfq, 3, 6);
     assertEquals(2, searcher.search(sprq, 10).totalHits);
@@ -92,7 +92,7 @@ public class TestSpanMultiTermQueryWrapper extends LuceneTestCase {
   public void testFuzzy2() throws Exception {
     // maximum of 1 term expansion
     FuzzyQuery fq = new FuzzyQuery(new Term("field", "broan"), 1, 0, 1, false);
-    SpanQuery sfq = new SpanMultiTermQueryWrapper<FuzzyQuery>(fq);
+    SpanQuery sfq = new SpanMultiTermQueryWrapper<>(fq);
     // will only match jumps over lazy broun dog
     SpanPositionRangeQuery sprq = new SpanPositionRangeQuery(sfq, 0, 100);
     assertEquals(1, searcher.search(sprq, 10).totalHits);
@@ -100,7 +100,7 @@ public class TestSpanMultiTermQueryWrapper extends LuceneTestCase {
   public void testNoSuchMultiTermsInNear() throws Exception {
     //test to make sure non existent multiterms aren't throwing null pointer exceptions  
     FuzzyQuery fuzzyNoSuch = new FuzzyQuery(new Term("field", "noSuch"), 1, 0, 1, false);
-    SpanQuery spanNoSuch = new SpanMultiTermQueryWrapper<FuzzyQuery>(fuzzyNoSuch);
+    SpanQuery spanNoSuch = new SpanMultiTermQueryWrapper<>(fuzzyNoSuch);
     SpanQuery term = new SpanTermQuery(new Term("field", "brown"));
     SpanQuery near = new SpanNearQuery(new SpanQuery[]{term, spanNoSuch}, 1, true);
     assertEquals(0, searcher.search(near, 10).totalHits);
@@ -109,17 +109,17 @@ public class TestSpanMultiTermQueryWrapper extends LuceneTestCase {
     assertEquals(0, searcher.search(near, 10).totalHits);
     
     WildcardQuery wcNoSuch = new WildcardQuery(new Term("field", "noSuch*"));
-    SpanQuery spanWCNoSuch = new SpanMultiTermQueryWrapper<WildcardQuery>(wcNoSuch);
+    SpanQuery spanWCNoSuch = new SpanMultiTermQueryWrapper<>(wcNoSuch);
     near = new SpanNearQuery(new SpanQuery[]{term, spanWCNoSuch}, 1, true);
     assertEquals(0, searcher.search(near, 10).totalHits);
   
     RegexpQuery rgxNoSuch = new RegexpQuery(new Term("field", "noSuch"));
-    SpanQuery spanRgxNoSuch = new SpanMultiTermQueryWrapper<RegexpQuery>(rgxNoSuch);
+    SpanQuery spanRgxNoSuch = new SpanMultiTermQueryWrapper<>(rgxNoSuch);
     near = new SpanNearQuery(new SpanQuery[]{term, spanRgxNoSuch}, 1, true);
     assertEquals(0, searcher.search(near, 10).totalHits);
     
     PrefixQuery prfxNoSuch = new PrefixQuery(new Term("field", "noSuch"));
-    SpanQuery spanPrfxNoSuch = new SpanMultiTermQueryWrapper<PrefixQuery>(prfxNoSuch);
+    SpanQuery spanPrfxNoSuch = new SpanMultiTermQueryWrapper<>(prfxNoSuch);
     near = new SpanNearQuery(new SpanQuery[]{term, spanPrfxNoSuch}, 1, true);
     assertEquals(0, searcher.search(near, 10).totalHits);
 
@@ -136,7 +136,7 @@ public class TestSpanMultiTermQueryWrapper extends LuceneTestCase {
   public void testNoSuchMultiTermsInNotNear() throws Exception {
     //test to make sure non existent multiterms aren't throwing non-matching field exceptions  
     FuzzyQuery fuzzyNoSuch = new FuzzyQuery(new Term("field", "noSuch"), 1, 0, 1, false);
-    SpanQuery spanNoSuch = new SpanMultiTermQueryWrapper<FuzzyQuery>(fuzzyNoSuch);
+    SpanQuery spanNoSuch = new SpanMultiTermQueryWrapper<>(fuzzyNoSuch);
     SpanQuery term = new SpanTermQuery(new Term("field", "brown"));
     SpanNotQuery notNear = new SpanNotQuery(term, spanNoSuch, 0,0);
     assertEquals(1, searcher.search(notNear, 10).totalHits);
@@ -150,17 +150,17 @@ public class TestSpanMultiTermQueryWrapper extends LuceneTestCase {
     assertEquals(0, searcher.search(notNear, 10).totalHits);
 
     WildcardQuery wcNoSuch = new WildcardQuery(new Term("field", "noSuch*"));
-    SpanQuery spanWCNoSuch = new SpanMultiTermQueryWrapper<WildcardQuery>(wcNoSuch);
+    SpanQuery spanWCNoSuch = new SpanMultiTermQueryWrapper<>(wcNoSuch);
     notNear = new SpanNotQuery(term, spanWCNoSuch, 0,0);
     assertEquals(1, searcher.search(notNear, 10).totalHits);
   
     RegexpQuery rgxNoSuch = new RegexpQuery(new Term("field", "noSuch"));
-    SpanQuery spanRgxNoSuch = new SpanMultiTermQueryWrapper<RegexpQuery>(rgxNoSuch);
+    SpanQuery spanRgxNoSuch = new SpanMultiTermQueryWrapper<>(rgxNoSuch);
     notNear = new SpanNotQuery(term, spanRgxNoSuch, 1, 1);
     assertEquals(1, searcher.search(notNear, 10).totalHits);
     
     PrefixQuery prfxNoSuch = new PrefixQuery(new Term("field", "noSuch"));
-    SpanQuery spanPrfxNoSuch = new SpanMultiTermQueryWrapper<PrefixQuery>(prfxNoSuch);
+    SpanQuery spanPrfxNoSuch = new SpanMultiTermQueryWrapper<>(prfxNoSuch);
     notNear = new SpanNotQuery(term, spanPrfxNoSuch, 1, 1);
     assertEquals(1, searcher.search(notNear, 10).totalHits);
     
@@ -169,7 +169,7 @@ public class TestSpanMultiTermQueryWrapper extends LuceneTestCase {
   public void testNoSuchMultiTermsInOr() throws Exception {
     //test to make sure non existent multiterms aren't throwing null pointer exceptions  
     FuzzyQuery fuzzyNoSuch = new FuzzyQuery(new Term("field", "noSuch"), 1, 0, 1, false);
-    SpanQuery spanNoSuch = new SpanMultiTermQueryWrapper<FuzzyQuery>(fuzzyNoSuch);
+    SpanQuery spanNoSuch = new SpanMultiTermQueryWrapper<>(fuzzyNoSuch);
     SpanQuery term = new SpanTermQuery(new Term("field", "brown"));
     SpanOrQuery near = new SpanOrQuery(new SpanQuery[]{term, spanNoSuch});
     assertEquals(1, searcher.search(near, 10).totalHits);
@@ -180,17 +180,17 @@ public class TestSpanMultiTermQueryWrapper extends LuceneTestCase {
 
     
     WildcardQuery wcNoSuch = new WildcardQuery(new Term("field", "noSuch*"));
-    SpanQuery spanWCNoSuch = new SpanMultiTermQueryWrapper<WildcardQuery>(wcNoSuch);
+    SpanQuery spanWCNoSuch = new SpanMultiTermQueryWrapper<>(wcNoSuch);
     near = new SpanOrQuery(new SpanQuery[]{term, spanWCNoSuch});
     assertEquals(1, searcher.search(near, 10).totalHits);
   
     RegexpQuery rgxNoSuch = new RegexpQuery(new Term("field", "noSuch"));
-    SpanQuery spanRgxNoSuch = new SpanMultiTermQueryWrapper<RegexpQuery>(rgxNoSuch);
+    SpanQuery spanRgxNoSuch = new SpanMultiTermQueryWrapper<>(rgxNoSuch);
     near = new SpanOrQuery(new SpanQuery[]{term, spanRgxNoSuch});
     assertEquals(1, searcher.search(near, 10).totalHits);
     
     PrefixQuery prfxNoSuch = new PrefixQuery(new Term("field", "noSuch"));
-    SpanQuery spanPrfxNoSuch = new SpanMultiTermQueryWrapper<PrefixQuery>(prfxNoSuch);
+    SpanQuery spanPrfxNoSuch = new SpanMultiTermQueryWrapper<>(prfxNoSuch);
     near = new SpanOrQuery(new SpanQuery[]{term, spanPrfxNoSuch});
     assertEquals(1, searcher.search(near, 10).totalHits);
     
@@ -206,23 +206,23 @@ public class TestSpanMultiTermQueryWrapper extends LuceneTestCase {
   public void testNoSuchMultiTermsInSpanFirst() throws Exception {
     //this hasn't been a problem  
     FuzzyQuery fuzzyNoSuch = new FuzzyQuery(new Term("field", "noSuch"), 1, 0, 1, false);
-    SpanQuery spanNoSuch = new SpanMultiTermQueryWrapper<FuzzyQuery>(fuzzyNoSuch);
+    SpanQuery spanNoSuch = new SpanMultiTermQueryWrapper<>(fuzzyNoSuch);
     SpanQuery spanFirst = new SpanFirstQuery(spanNoSuch, 10);
  
     assertEquals(0, searcher.search(spanFirst, 10).totalHits);
     
     WildcardQuery wcNoSuch = new WildcardQuery(new Term("field", "noSuch*"));
-    SpanQuery spanWCNoSuch = new SpanMultiTermQueryWrapper<WildcardQuery>(wcNoSuch);
+    SpanQuery spanWCNoSuch = new SpanMultiTermQueryWrapper<>(wcNoSuch);
     spanFirst = new SpanFirstQuery(spanWCNoSuch, 10);
     assertEquals(0, searcher.search(spanFirst, 10).totalHits);
   
     RegexpQuery rgxNoSuch = new RegexpQuery(new Term("field", "noSuch"));
-    SpanQuery spanRgxNoSuch = new SpanMultiTermQueryWrapper<RegexpQuery>(rgxNoSuch);
+    SpanQuery spanRgxNoSuch = new SpanMultiTermQueryWrapper<>(rgxNoSuch);
     spanFirst = new SpanFirstQuery(spanRgxNoSuch, 10);
     assertEquals(0, searcher.search(spanFirst, 10).totalHits);
     
     PrefixQuery prfxNoSuch = new PrefixQuery(new Term("field", "noSuch"));
-    SpanQuery spanPrfxNoSuch = new SpanMultiTermQueryWrapper<PrefixQuery>(prfxNoSuch);
+    SpanQuery spanPrfxNoSuch = new SpanMultiTermQueryWrapper<>(prfxNoSuch);
     spanFirst = new SpanFirstQuery(spanPrfxNoSuch, 10);
     assertEquals(0, searcher.search(spanFirst, 10).totalHits);
   }
diff --git lucene/core/src/test/org/apache/lucene/store/TestBufferedIndexInput.java lucene/core/src/test/org/apache/lucene/store/TestBufferedIndexInput.java
index 349e8b9..5c99fa2 100644
--- lucene/core/src/test/org/apache/lucene/store/TestBufferedIndexInput.java
+++ lucene/core/src/test/org/apache/lucene/store/TestBufferedIndexInput.java
@@ -281,7 +281,7 @@ public class TestBufferedIndexInput extends LuceneTestCase {
 
     private static class MockFSDirectory extends BaseDirectory {
 
-      List<IndexInput> allIndexInputs = new ArrayList<IndexInput>();
+      List<IndexInput> allIndexInputs = new ArrayList<>();
 
       Random rand;
 
diff --git lucene/core/src/test/org/apache/lucene/store/TestFileSwitchDirectory.java lucene/core/src/test/org/apache/lucene/store/TestFileSwitchDirectory.java
index bebc675..0d91b58 100644
--- lucene/core/src/test/org/apache/lucene/store/TestFileSwitchDirectory.java
+++ lucene/core/src/test/org/apache/lucene/store/TestFileSwitchDirectory.java
@@ -40,7 +40,7 @@ public class TestFileSwitchDirectory extends LuceneTestCase {
    * Test if writing doc stores to disk and everything else to ram works.
    */
   public void testBasic() throws IOException {
-    Set<String> fileExtensions = new HashSet<String>();
+    Set<String> fileExtensions = new HashSet<>();
     fileExtensions.add(Lucene40StoredFieldsWriter.FIELDS_EXTENSION);
     fileExtensions.add(Lucene40StoredFieldsWriter.FIELDS_INDEX_EXTENSION);
     
diff --git lucene/core/src/test/org/apache/lucene/store/TestFilterDirectory.java lucene/core/src/test/org/apache/lucene/store/TestFilterDirectory.java
index 3a90c9a..577315b 100644
--- lucene/core/src/test/org/apache/lucene/store/TestFilterDirectory.java
+++ lucene/core/src/test/org/apache/lucene/store/TestFilterDirectory.java
@@ -30,7 +30,7 @@ public class TestFilterDirectory extends LuceneTestCase {
   public void testOverrides() throws Exception {
     // verify that all methods of Directory are overridden by FilterDirectory,
     // except those under the 'exclude' list
-    Set<String> exclude = new HashSet<String>();
+    Set<String> exclude = new HashSet<>();
     exclude.add("copy");
     exclude.add("createSlicer");
     for (Method m : FilterDirectory.class.getMethods()) {
diff --git lucene/core/src/test/org/apache/lucene/store/TestHugeRamFile.java lucene/core/src/test/org/apache/lucene/store/TestHugeRamFile.java
index 6840f51..01c7ed1 100644
--- lucene/core/src/test/org/apache/lucene/store/TestHugeRamFile.java
+++ lucene/core/src/test/org/apache/lucene/store/TestHugeRamFile.java
@@ -31,7 +31,7 @@ public class TestHugeRamFile extends LuceneTestCase {
    * buffers under maxint. */
   private static class DenseRAMFile extends RAMFile {
     private long capacity = 0;
-    private HashMap<Integer,byte[]> singleBuffers = new HashMap<Integer,byte[]>();
+    private HashMap<Integer,byte[]> singleBuffers = new HashMap<>();
     @Override
     protected byte[] newBuffer(int size) {
       capacity += size;
diff --git lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java
index f5b3e02..5102c76 100644
--- lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java
+++ lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java
@@ -55,7 +55,7 @@ public class TestNRTCachingDirectory extends LuceneTestCase {
       System.out.println("TEST: numDocs=" + numDocs);
     }
 
-    final List<BytesRef> ids = new ArrayList<BytesRef>();
+    final List<BytesRef> ids = new ArrayList<>();
     DirectoryReader r = null;
     for(int docCount=0;docCount<numDocs;docCount++) {
       final Document doc = docs.nextDoc();
diff --git lucene/core/src/test/org/apache/lucene/util/StressRamUsageEstimator.java lucene/core/src/test/org/apache/lucene/util/StressRamUsageEstimator.java
index 820230f..7dc8ec8 100644
--- lucene/core/src/test/org/apache/lucene/util/StressRamUsageEstimator.java
+++ lucene/core/src/test/org/apache/lucene/util/StressRamUsageEstimator.java
@@ -149,11 +149,11 @@ public class StressRamUsageEstimator extends LuceneTestCase {
    */
   private void causeGc() {
     List<GarbageCollectorMXBean> garbageCollectorMXBeans = ManagementFactory.getGarbageCollectorMXBeans();
-    List<Long> ccounts = new ArrayList<Long>();
+    List<Long> ccounts = new ArrayList<>();
     for (GarbageCollectorMXBean g : garbageCollectorMXBeans) {
       ccounts.add(g.getCollectionCount());
     }
-    List<Long> ccounts2 = new ArrayList<Long>();
+    List<Long> ccounts2 = new ArrayList<>();
     do {
       System.gc();
       ccounts.clear();
diff --git lucene/core/src/test/org/apache/lucene/util/TestByteBlockPool.java lucene/core/src/test/org/apache/lucene/util/TestByteBlockPool.java
index 8b439f8..3b845bc 100644
--- lucene/core/src/test/org/apache/lucene/util/TestByteBlockPool.java
+++ lucene/core/src/test/org/apache/lucene/util/TestByteBlockPool.java
@@ -29,7 +29,7 @@ public class TestByteBlockPool extends LuceneTestCase {
     boolean reuseFirst = random().nextBoolean();
     for (int j = 0; j < 2; j++) {
         
-      List<BytesRef> list = new ArrayList<BytesRef>();
+      List<BytesRef> list = new ArrayList<>();
       int maxLength = atLeast(500);
       final int numValues = atLeast(100);
       BytesRef ref = new BytesRef();
diff --git lucene/core/src/test/org/apache/lucene/util/TestBytesRefArray.java lucene/core/src/test/org/apache/lucene/util/TestBytesRefArray.java
index 9fcd6a1..fa691fb 100644
--- lucene/core/src/test/org/apache/lucene/util/TestBytesRefArray.java
+++ lucene/core/src/test/org/apache/lucene/util/TestBytesRefArray.java
@@ -31,7 +31,7 @@ public class TestBytesRefArray extends LuceneTestCase {
   public void testAppend() throws IOException {
     Random random = random();
     BytesRefArray list = new BytesRefArray(Counter.newCounter());
-    List<String> stringList = new ArrayList<String>();
+    List<String> stringList = new ArrayList<>();
     for (int j = 0; j < 2; j++) {
       if (j > 0 && random.nextBoolean()) {
         list.clear();
@@ -73,7 +73,7 @@ public class TestBytesRefArray extends LuceneTestCase {
   public void testSort() throws IOException {
     Random random = random();
     BytesRefArray list = new BytesRefArray(Counter.newCounter());
-    List<String> stringList = new ArrayList<String>();
+    List<String> stringList = new ArrayList<>();
 
     for (int j = 0; j < 2; j++) {
       if (j > 0 && random.nextBoolean()) {
diff --git lucene/core/src/test/org/apache/lucene/util/TestBytesRefHash.java lucene/core/src/test/org/apache/lucene/util/TestBytesRefHash.java
index f71f3c7..52fd568 100644
--- lucene/core/src/test/org/apache/lucene/util/TestBytesRefHash.java
+++ lucene/core/src/test/org/apache/lucene/util/TestBytesRefHash.java
@@ -95,7 +95,7 @@ public class TestBytesRefHash extends LuceneTestCase {
     BytesRef scratch = new BytesRef();
     int num = atLeast(2);
     for (int j = 0; j < num; j++) {
-      Map<String, Integer> strings = new HashMap<String, Integer>();
+      Map<String, Integer> strings = new HashMap<>();
       int uniqueCount = 0;
       for (int i = 0; i < 797; i++) {
         String str;
@@ -175,7 +175,7 @@ public class TestBytesRefHash extends LuceneTestCase {
     BytesRef ref = new BytesRef();
     int num = atLeast(2);
     for (int j = 0; j < num; j++) {
-      SortedSet<String> strings = new TreeSet<String>();
+      SortedSet<String> strings = new TreeSet<>();
       for (int i = 0; i < 797; i++) {
         String str;
         do {
@@ -213,7 +213,7 @@ public class TestBytesRefHash extends LuceneTestCase {
     BytesRef scratch = new BytesRef();
     int num = atLeast(2);
     for (int j = 0; j < num; j++) {
-      Set<String> strings = new HashSet<String>();
+      Set<String> strings = new HashSet<>();
       int uniqueCount = 0;
       for (int i = 0; i < 797; i++) {
         String str;
@@ -250,7 +250,7 @@ public class TestBytesRefHash extends LuceneTestCase {
     BytesRef scratch = new BytesRef();
     int num = atLeast(2);
     for (int j = 0; j < num; j++) {
-      Set<String> strings = new HashSet<String>();
+      Set<String> strings = new HashSet<>();
       int uniqueCount = 0;
       for (int i = 0; i < 797; i++) {
         String str;
@@ -313,7 +313,7 @@ public class TestBytesRefHash extends LuceneTestCase {
     BytesRefHash offsetHash = newHash(pool);
     int num = atLeast(2);
     for (int j = 0; j < num; j++) {
-      Set<String> strings = new HashSet<String>();
+      Set<String> strings = new HashSet<>();
       int uniqueCount = 0;
       for (int i = 0; i < 797; i++) {
         String str;
diff --git lucene/core/src/test/org/apache/lucene/util/TestCloseableThreadLocal.java lucene/core/src/test/org/apache/lucene/util/TestCloseableThreadLocal.java
index c1931ce..c093722 100644
--- lucene/core/src/test/org/apache/lucene/util/TestCloseableThreadLocal.java
+++ lucene/core/src/test/org/apache/lucene/util/TestCloseableThreadLocal.java
@@ -29,7 +29,7 @@ public class TestCloseableThreadLocal extends LuceneTestCase {
   public void testNullValue() throws Exception {
     // Tests that null can be set as a valid value (LUCENE-1805). This
     // previously failed in get().
-    CloseableThreadLocal<Object> ctl = new CloseableThreadLocal<Object>();
+    CloseableThreadLocal<Object> ctl = new CloseableThreadLocal<>();
     ctl.set(null);
     assertNull(ctl.get());
   }
@@ -37,7 +37,7 @@ public class TestCloseableThreadLocal extends LuceneTestCase {
   public void testDefaultValueWithoutSetting() throws Exception {
     // LUCENE-1805: make sure default get returns null,
     // twice in a row
-    CloseableThreadLocal<Object> ctl = new CloseableThreadLocal<Object>();
+    CloseableThreadLocal<Object> ctl = new CloseableThreadLocal<>();
     assertNull(ctl.get());
   }
 
diff --git lucene/core/src/test/org/apache/lucene/util/TestCollectionUtil.java lucene/core/src/test/org/apache/lucene/util/TestCollectionUtil.java
index 3655856..013e854 100644
--- lucene/core/src/test/org/apache/lucene/util/TestCollectionUtil.java
+++ lucene/core/src/test/org/apache/lucene/util/TestCollectionUtil.java
@@ -37,13 +37,13 @@ public class TestCollectionUtil extends LuceneTestCase {
   
   public void testIntroSort() {
     for (int i = 0, c = atLeast(500); i < c; i++) {
-      List<Integer> list1 = createRandomList(2000), list2 = new ArrayList<Integer>(list1);
+      List<Integer> list1 = createRandomList(2000), list2 = new ArrayList<>(list1);
       CollectionUtil.introSort(list1);
       Collections.sort(list2);
       assertEquals(list2, list1);
       
       list1 = createRandomList(2000);
-      list2 = new ArrayList<Integer>(list1);
+      list2 = new ArrayList<>(list1);
       CollectionUtil.introSort(list1, Collections.reverseOrder());
       Collections.sort(list2, Collections.reverseOrder());
       assertEquals(list2, list1);
@@ -56,13 +56,13 @@ public class TestCollectionUtil extends LuceneTestCase {
 
   public void testTimSort() {
     for (int i = 0, c = atLeast(500); i < c; i++) {
-      List<Integer> list1 = createRandomList(2000), list2 = new ArrayList<Integer>(list1);
+      List<Integer> list1 = createRandomList(2000), list2 = new ArrayList<>(list1);
       CollectionUtil.timSort(list1);
       Collections.sort(list2);
       assertEquals(list2, list1);
       
       list1 = createRandomList(2000);
-      list2 = new ArrayList<Integer>(list1);
+      list2 = new ArrayList<>(list1);
       CollectionUtil.timSort(list1, Collections.reverseOrder());
       Collections.sort(list2, Collections.reverseOrder());
       assertEquals(list2, list1);
@@ -82,7 +82,7 @@ public class TestCollectionUtil extends LuceneTestCase {
     CollectionUtil.timSort(list, Collections.reverseOrder());
     
     // check that empty non-random access lists pass sorting without ex (as sorting is not needed)
-    list = new LinkedList<Integer>();
+    list = new LinkedList<>();
     CollectionUtil.introSort(list);
     CollectionUtil.timSort(list);
     CollectionUtil.introSort(list, Collections.reverseOrder());
@@ -91,7 +91,7 @@ public class TestCollectionUtil extends LuceneTestCase {
   
   public void testOneElementListSort() {
     // check that one-element non-random access lists pass sorting without ex (as sorting is not needed)
-    List<Integer> list = new LinkedList<Integer>();
+    List<Integer> list = new LinkedList<>();
     list.add(1);
     CollectionUtil.introSort(list);
     CollectionUtil.timSort(list);
diff --git lucene/core/src/test/org/apache/lucene/util/TestDoubleBarrelLRUCache.java lucene/core/src/test/org/apache/lucene/util/TestDoubleBarrelLRUCache.java
index b84ecf5..ffc65dc 100644
--- lucene/core/src/test/org/apache/lucene/util/TestDoubleBarrelLRUCache.java
+++ lucene/core/src/test/org/apache/lucene/util/TestDoubleBarrelLRUCache.java
@@ -118,7 +118,7 @@ public class TestDoubleBarrelLRUCache extends LuceneTestCase {
     final int CACHE_SIZE = 512;
     final int OBJ_COUNT = 3*CACHE_SIZE;
 
-    DoubleBarrelLRUCache<CloneableObject,Object> c = new DoubleBarrelLRUCache<CloneableObject,Object>(1024);
+    DoubleBarrelLRUCache<CloneableObject,Object> c = new DoubleBarrelLRUCache<>(1024);
 
     CloneableObject[] objs = new CloneableObject[OBJ_COUNT];
     for(int i=0;i<OBJ_COUNT;i++) {
diff --git lucene/core/src/test/org/apache/lucene/util/TestFilterIterator.java lucene/core/src/test/org/apache/lucene/util/TestFilterIterator.java
index 66613e4..a5486e1 100644
--- lucene/core/src/test/org/apache/lucene/util/TestFilterIterator.java
+++ lucene/core/src/test/org/apache/lucene/util/TestFilterIterator.java
@@ -25,7 +25,7 @@ import java.util.NoSuchElementException;
 
 public class TestFilterIterator extends LuceneTestCase {
 
-  private static final Set<String> set = new TreeSet<String>(Arrays.asList("a", "b", "c"));
+  private static final Set<String> set = new TreeSet<>(Arrays.asList("a", "b", "c"));
 
   private static void assertNoMore(Iterator<?> it) {
     assertFalse(it.hasNext());
diff --git lucene/core/src/test/org/apache/lucene/util/TestIdentityHashSet.java lucene/core/src/test/org/apache/lucene/util/TestIdentityHashSet.java
index 49e6b84..434978d 100644
--- lucene/core/src/test/org/apache/lucene/util/TestIdentityHashSet.java
+++ lucene/core/src/test/org/apache/lucene/util/TestIdentityHashSet.java
@@ -28,7 +28,7 @@ public class TestIdentityHashSet extends LuceneTestCase {
     Random rnd = random();
     Set<Object> jdk = Collections.newSetFromMap(
         new IdentityHashMap<Object,Boolean>());
-    RamUsageEstimator.IdentityHashSet<Object> us = new RamUsageEstimator.IdentityHashSet<Object>();
+    RamUsageEstimator.IdentityHashSet<Object> us = new RamUsageEstimator.IdentityHashSet<>();
 
     int max = 100000;
     int threshold = 256;
diff --git lucene/core/src/test/org/apache/lucene/util/TestInPlaceMergeSorter.java lucene/core/src/test/org/apache/lucene/util/TestInPlaceMergeSorter.java
index 6c1fe57..d6e5f46 100644
--- lucene/core/src/test/org/apache/lucene/util/TestInPlaceMergeSorter.java
+++ lucene/core/src/test/org/apache/lucene/util/TestInPlaceMergeSorter.java
@@ -30,7 +30,7 @@ public class TestInPlaceMergeSorter extends BaseSortTestCase {
 
   @Override
   public Sorter newSorter(Entry[] arr) {
-    return new ArrayInPlaceMergeSorter<Entry>(arr, ArrayUtil.<Entry>naturalComparator());
+    return new ArrayInPlaceMergeSorter<>(arr, ArrayUtil.<Entry>naturalComparator());
   }
 
 }
diff --git lucene/core/src/test/org/apache/lucene/util/TestIntroSorter.java lucene/core/src/test/org/apache/lucene/util/TestIntroSorter.java
index 63e9f95..eb4316e 100644
--- lucene/core/src/test/org/apache/lucene/util/TestIntroSorter.java
+++ lucene/core/src/test/org/apache/lucene/util/TestIntroSorter.java
@@ -26,7 +26,7 @@ public class TestIntroSorter extends BaseSortTestCase {
 
   @Override
   public Sorter newSorter(Entry[] arr) {
-    return new ArrayIntroSorter<Entry>(arr, ArrayUtil.<Entry>naturalComparator());
+    return new ArrayIntroSorter<>(arr, ArrayUtil.<Entry>naturalComparator());
   }
 
 }
diff --git lucene/core/src/test/org/apache/lucene/util/TestMergedIterator.java lucene/core/src/test/org/apache/lucene/util/TestMergedIterator.java
index e9e37a7..9867098 100644
--- lucene/core/src/test/org/apache/lucene/util/TestMergedIterator.java
+++ lucene/core/src/test/org/apache/lucene/util/TestMergedIterator.java
@@ -30,17 +30,17 @@ public class TestMergedIterator extends LuceneTestCase {
 
   @SuppressWarnings({"rawtypes", "unchecked"})
   public void testMergeEmpty() {
-    Iterator<Integer> merged = new MergedIterator<Integer>();
+    Iterator<Integer> merged = new MergedIterator<>();
     assertFalse(merged.hasNext());
 
-    merged = new MergedIterator<Integer>(new ArrayList<Integer>().iterator());
+    merged = new MergedIterator<>(new ArrayList<Integer>().iterator());
     assertFalse(merged.hasNext());
 
     Iterator<Integer>[] itrs = new Iterator[random().nextInt(100)];
     for (int i = 0; i < itrs.length; i++) {
       itrs[i] = new ArrayList<Integer>().iterator();
     }
-    merged = new MergedIterator<Integer>( itrs );
+    merged = new MergedIterator<>( itrs );
     assertFalse(merged.hasNext());
   }
 
@@ -106,13 +106,13 @@ public class TestMergedIterator extends LuceneTestCase {
 
   private void testCase(int itrsWithVal, int specifiedValsOnItr, boolean removeDups) {
     // Build a random number of lists
-    List<Integer> expected = new ArrayList<Integer>();
+    List<Integer> expected = new ArrayList<>();
     Random random = new Random(random().nextLong());
     int numLists = itrsWithVal + random.nextInt(1000 - itrsWithVal);
     @SuppressWarnings({"rawtypes", "unchecked"})
     List<Integer>[] lists = new List[numLists];
     for (int i = 0; i < numLists; i++) {
-      lists[i] = new ArrayList<Integer>();
+      lists[i] = new ArrayList<>();
     }
     int start = random.nextInt(1000000);
     int end = start + VALS_TO_MERGE / itrsWithVal / Math.abs(specifiedValsOnItr);
@@ -143,7 +143,7 @@ public class TestMergedIterator extends LuceneTestCase {
       itrs[i] = lists[i].iterator();
     }
     
-    MergedIterator<Integer> mergedItr = new MergedIterator<Integer>(removeDups, itrs);
+    MergedIterator<Integer> mergedItr = new MergedIterator<>(removeDups, itrs);
     Iterator<Integer> expectedItr = expected.iterator();
     while (expectedItr.hasNext()) {
       assertTrue(mergedItr.hasNext());
diff --git lucene/core/src/test/org/apache/lucene/util/TestOfflineSorter.java lucene/core/src/test/org/apache/lucene/util/TestOfflineSorter.java
index b7f14d0..273f59f 100644
--- lucene/core/src/test/org/apache/lucene/util/TestOfflineSorter.java
+++ lucene/core/src/test/org/apache/lucene/util/TestOfflineSorter.java
@@ -86,7 +86,7 @@ public class TestOfflineSorter extends LuceneTestCase {
   }
 
   private byte[][] generateRandom(int howMuchData) {
-    ArrayList<byte[]> data = new ArrayList<byte[]>(); 
+    ArrayList<byte[]> data = new ArrayList<>();
     while (howMuchData > 0) {
       byte [] current = new byte [random().nextInt(256)];
       random().nextBytes(current);
diff --git lucene/core/src/test/org/apache/lucene/util/TestRecyclingByteBlockAllocator.java lucene/core/src/test/org/apache/lucene/util/TestRecyclingByteBlockAllocator.java
index 8d81f65..db1044f 100644
--- lucene/core/src/test/org/apache/lucene/util/TestRecyclingByteBlockAllocator.java
+++ lucene/core/src/test/org/apache/lucene/util/TestRecyclingByteBlockAllocator.java
@@ -45,7 +45,7 @@ public class TestRecyclingByteBlockAllocator extends LuceneTestCase {
   @Test
   public void testAllocate() {
     RecyclingByteBlockAllocator allocator = newAllocator();
-    HashSet<byte[]> set = new HashSet<byte[]>();
+    HashSet<byte[]> set = new HashSet<>();
     byte[] block = allocator.getByteBlock();
     set.add(block);
     assertNotNull(block);
@@ -65,7 +65,7 @@ public class TestRecyclingByteBlockAllocator extends LuceneTestCase {
   @Test
   public void testAllocateAndRecycle() {
     RecyclingByteBlockAllocator allocator = newAllocator();
-    HashSet<byte[]> allocated = new HashSet<byte[]>();
+    HashSet<byte[]> allocated = new HashSet<>();
 
     byte[] block = allocator.getByteBlock();
     allocated.add(block);
@@ -86,7 +86,7 @@ public class TestRecyclingByteBlockAllocator extends LuceneTestCase {
       byte[][] array = allocated.toArray(new byte[0][]);
       int begin = random().nextInt(array.length);
       int end = begin + random().nextInt(array.length - begin);
-      List<byte[]> selected = new ArrayList<byte[]>();
+      List<byte[]> selected = new ArrayList<>();
       for (int j = begin; j < end; j++) {
         selected.add(array[j]);
       }
@@ -102,7 +102,7 @@ public class TestRecyclingByteBlockAllocator extends LuceneTestCase {
   @Test
   public void testAllocateAndFree() {
     RecyclingByteBlockAllocator allocator = newAllocator();
-    HashSet<byte[]> allocated = new HashSet<byte[]>();
+    HashSet<byte[]> allocated = new HashSet<>();
     int freeButAllocated = 0;
     byte[] block = allocator.getByteBlock();
     allocated.add(block);
diff --git lucene/core/src/test/org/apache/lucene/util/TestRecyclingIntBlockAllocator.java lucene/core/src/test/org/apache/lucene/util/TestRecyclingIntBlockAllocator.java
index 46ad8cd..1ea6702 100644
--- lucene/core/src/test/org/apache/lucene/util/TestRecyclingIntBlockAllocator.java
+++ lucene/core/src/test/org/apache/lucene/util/TestRecyclingIntBlockAllocator.java
@@ -45,7 +45,7 @@ public class TestRecyclingIntBlockAllocator extends LuceneTestCase {
   @Test
   public void testAllocate() {
     RecyclingIntBlockAllocator allocator = newAllocator();
-    HashSet<int[]> set = new HashSet<int[]>();
+    HashSet<int[]> set = new HashSet<>();
     int[] block = allocator.getIntBlock();
     set.add(block);
     assertNotNull(block);
@@ -65,7 +65,7 @@ public class TestRecyclingIntBlockAllocator extends LuceneTestCase {
   @Test
   public void testAllocateAndRecycle() {
     RecyclingIntBlockAllocator allocator = newAllocator();
-    HashSet<int[]> allocated = new HashSet<int[]>();
+    HashSet<int[]> allocated = new HashSet<>();
 
     int[] block = allocator.getIntBlock();
     allocated.add(block);
@@ -86,7 +86,7 @@ public class TestRecyclingIntBlockAllocator extends LuceneTestCase {
       int[][] array = allocated.toArray(new int[0][]);
       int begin = random().nextInt(array.length);
       int end = begin + random().nextInt(array.length - begin);
-      List<int[]> selected = new ArrayList<int[]>();
+      List<int[]> selected = new ArrayList<>();
       for (int j = begin; j < end; j++) {
         selected.add(array[j]);
       }
@@ -102,7 +102,7 @@ public class TestRecyclingIntBlockAllocator extends LuceneTestCase {
   @Test
   public void testAllocateAndFree() {
     RecyclingIntBlockAllocator allocator = newAllocator();
-    HashSet<int[]> allocated = new HashSet<int[]>();
+    HashSet<int[]> allocated = new HashSet<>();
     int freeButAllocated = 0;
     int[] block = allocator.getIntBlock();
     allocated.add(block);
diff --git lucene/core/src/test/org/apache/lucene/util/TestSentinelIntSet.java lucene/core/src/test/org/apache/lucene/util/TestSentinelIntSet.java
index 796c723..00a5bb8 100644
--- lucene/core/src/test/org/apache/lucene/util/TestSentinelIntSet.java
+++ lucene/core/src/test/org/apache/lucene/util/TestSentinelIntSet.java
@@ -56,7 +56,7 @@ public class TestSentinelIntSet extends LuceneTestCase {
       int num = random().nextInt(30);
       int maxVal = (random().nextBoolean() ? random().nextInt(50) : random().nextInt(Integer.MAX_VALUE)) + 1;
 
-      HashSet<Integer> a = new HashSet<Integer>(initSz);
+      HashSet<Integer> a = new HashSet<>(initSz);
       SentinelIntSet b = new SentinelIntSet(initSz, -1);
       
       for (int j=0; j<num; j++) {
diff --git lucene/core/src/test/org/apache/lucene/util/TestSetOnce.java lucene/core/src/test/org/apache/lucene/util/TestSetOnce.java
index bfaad09..1ee0e6d 100644
--- lucene/core/src/test/org/apache/lucene/util/TestSetOnce.java
+++ lucene/core/src/test/org/apache/lucene/util/TestSetOnce.java
@@ -51,20 +51,20 @@ public class TestSetOnce extends LuceneTestCase {
   
   @Test
   public void testEmptyCtor() throws Exception {
-    SetOnce<Integer> set = new SetOnce<Integer>();
+    SetOnce<Integer> set = new SetOnce<>();
     assertNull(set.get());
   }
   
   @Test(expected=AlreadySetException.class)
   public void testSettingCtor() throws Exception {
-    SetOnce<Integer> set = new SetOnce<Integer>(new Integer(5));
+    SetOnce<Integer> set = new SetOnce<>(new Integer(5));
     assertEquals(5, set.get().intValue());
     set.set(new Integer(7));
   }
   
   @Test(expected=AlreadySetException.class)
   public void testSetOnce() throws Exception {
-    SetOnce<Integer> set = new SetOnce<Integer>();
+    SetOnce<Integer> set = new SetOnce<>();
     set.set(new Integer(5));
     assertEquals(5, set.get().intValue());
     set.set(new Integer(7));
@@ -72,7 +72,7 @@ public class TestSetOnce extends LuceneTestCase {
   
   @Test
   public void testSetMultiThreaded() throws Exception {
-    final SetOnce<Integer> set = new SetOnce<Integer>();
+    final SetOnce<Integer> set = new SetOnce<>();
     SetOnceThread[] threads = new SetOnceThread[10];
     for (int i = 0; i < threads.length; i++) {
       threads[i] = new SetOnceThread(random());
diff --git lucene/core/src/test/org/apache/lucene/util/TestTimSorter.java lucene/core/src/test/org/apache/lucene/util/TestTimSorter.java
index 8037e3b..064f5d7 100644
--- lucene/core/src/test/org/apache/lucene/util/TestTimSorter.java
+++ lucene/core/src/test/org/apache/lucene/util/TestTimSorter.java
@@ -25,7 +25,7 @@ public class TestTimSorter extends BaseSortTestCase {
 
   @Override
   public Sorter newSorter(Entry[] arr) {
-    return new ArrayTimSorter<Entry>(arr, ArrayUtil.<Entry>naturalComparator(), TestUtil.nextInt(random(), 0, arr.length));
+    return new ArrayTimSorter<>(arr, ArrayUtil.<Entry>naturalComparator(), TestUtil.nextInt(random(), 0, arr.length));
   }
 
 }
diff --git lucene/core/src/test/org/apache/lucene/util/TestVirtualMethod.java lucene/core/src/test/org/apache/lucene/util/TestVirtualMethod.java
index fffa0e8..1f365f2 100644
--- lucene/core/src/test/org/apache/lucene/util/TestVirtualMethod.java
+++ lucene/core/src/test/org/apache/lucene/util/TestVirtualMethod.java
@@ -20,9 +20,9 @@ package org.apache.lucene.util;
 public class TestVirtualMethod extends LuceneTestCase {
 
   private static final VirtualMethod<TestVirtualMethod> publicTestMethod =
-    new VirtualMethod<TestVirtualMethod>(TestVirtualMethod.class, "publicTest", String.class);
+    new VirtualMethod<>(TestVirtualMethod.class, "publicTest", String.class);
   private static final VirtualMethod<TestVirtualMethod> protectedTestMethod =
-    new VirtualMethod<TestVirtualMethod>(TestVirtualMethod.class, "protectedTest", int.class);
+    new VirtualMethod<>(TestVirtualMethod.class, "protectedTest", int.class);
 
   public void publicTest(String test) {}
   protected void protectedTest(int test) {}
@@ -80,14 +80,14 @@ public class TestVirtualMethod extends LuceneTestCase {
     }
     
     try {
-      new VirtualMethod<TestVirtualMethod>(TestVirtualMethod.class, "bogus");
+      new VirtualMethod<>(TestVirtualMethod.class, "bogus");
       fail("Method bogus() does not exist, so IAE should be thrown");
     } catch (IllegalArgumentException arg) {
       // pass
     }
     
     try {
-      new VirtualMethod<TestClass2>(TestClass2.class, "publicTest", String.class);
+      new VirtualMethod<>(TestClass2.class, "publicTest", String.class);
       fail("Method publicTest(String) is not declared in TestClass2, so IAE should be thrown");
     } catch (IllegalArgumentException arg) {
       // pass
@@ -95,7 +95,7 @@ public class TestVirtualMethod extends LuceneTestCase {
 
     try {
       // try to create a second instance of the same baseClass / method combination
-      new VirtualMethod<TestVirtualMethod>(TestVirtualMethod.class, "publicTest", String.class);
+      new VirtualMethod<>(TestVirtualMethod.class, "publicTest", String.class);
       fail("Violating singleton status succeeded");
     } catch (UnsupportedOperationException arg) {
       // pass
diff --git lucene/core/src/test/org/apache/lucene/util/TestWAH8DocIdSet.java lucene/core/src/test/org/apache/lucene/util/TestWAH8DocIdSet.java
index de135db..1689d02 100644
--- lucene/core/src/test/org/apache/lucene/util/TestWAH8DocIdSet.java
+++ lucene/core/src/test/org/apache/lucene/util/TestWAH8DocIdSet.java
@@ -44,11 +44,11 @@ public class TestWAH8DocIdSet extends BaseDocIdSetTestCase<WAH8DocIdSet> {
   public void testUnion() throws IOException {
     final int numBits = TestUtil.nextInt(random(), 100, 1 << 20);
     final int numDocIdSets = TestUtil.nextInt(random(), 0, 4);
-    final List<BitSet> fixedSets = new ArrayList<BitSet>(numDocIdSets);
+    final List<BitSet> fixedSets = new ArrayList<>(numDocIdSets);
     for (int i = 0; i < numDocIdSets; ++i) {
       fixedSets.add(randomSet(numBits, random().nextFloat() / 16));
     }
-    final List<WAH8DocIdSet> compressedSets = new ArrayList<WAH8DocIdSet>(numDocIdSets);
+    final List<WAH8DocIdSet> compressedSets = new ArrayList<>(numDocIdSets);
     for (BitSet set : fixedSets) {
       compressedSets.add(copyOf(set, numBits));
     }
@@ -66,11 +66,11 @@ public class TestWAH8DocIdSet extends BaseDocIdSetTestCase<WAH8DocIdSet> {
   public void testIntersection() throws IOException {
     final int numBits = TestUtil.nextInt(random(), 100, 1 << 20);
     final int numDocIdSets = TestUtil.nextInt(random(), 1, 4);
-    final List<BitSet> fixedSets = new ArrayList<BitSet>(numDocIdSets);
+    final List<BitSet> fixedSets = new ArrayList<>(numDocIdSets);
     for (int i = 0; i < numDocIdSets; ++i) {
       fixedSets.add(randomSet(numBits, random().nextFloat()));
     }
-    final List<WAH8DocIdSet> compressedSets = new ArrayList<WAH8DocIdSet>(numDocIdSets);
+    final List<WAH8DocIdSet> compressedSets = new ArrayList<>(numDocIdSets);
     for (BitSet set : fixedSets) {
       compressedSets.add(copyOf(set, numBits));
     }
diff --git lucene/core/src/test/org/apache/lucene/util/TestWeakIdentityMap.java lucene/core/src/test/org/apache/lucene/util/TestWeakIdentityMap.java
index db8c632..0173e0a 100644
--- lucene/core/src/test/org/apache/lucene/util/TestWeakIdentityMap.java
+++ lucene/core/src/test/org/apache/lucene/util/TestWeakIdentityMap.java
@@ -167,7 +167,7 @@ public class TestWeakIdentityMap extends LuceneTestCase {
       WeakIdentityMap.newConcurrentHashMap(random().nextBoolean());
     // we keep strong references to the keys,
     // so WeakIdentityMap will not forget about them:
-    final AtomicReferenceArray<Object> keys = new AtomicReferenceArray<Object>(keyCount);
+    final AtomicReferenceArray<Object> keys = new AtomicReferenceArray<>(keyCount);
     for (int j = 0; j < keyCount; j++) {
       keys.set(j, new Object());
     }
diff --git lucene/core/src/test/org/apache/lucene/util/automaton/TestBasicOperations.java lucene/core/src/test/org/apache/lucene/util/automaton/TestBasicOperations.java
index 622aa9d..1b2e62d 100644
--- lucene/core/src/test/org/apache/lucene/util/automaton/TestBasicOperations.java
+++ lucene/core/src/test/org/apache/lucene/util/automaton/TestBasicOperations.java
@@ -26,7 +26,7 @@ import com.carrotsearch.randomizedtesting.generators.RandomInts;
 public class TestBasicOperations extends LuceneTestCase {
   /** Test string union. */
   public void testStringUnion() {
-    List<BytesRef> strings = new ArrayList<BytesRef>();
+    List<BytesRef> strings = new ArrayList<>();
     for (int i = RandomInts.randomIntBetween(random(), 0, 1000); --i >= 0;) {
       strings.add(new BytesRef(TestUtil.randomUnicodeString(random())));
     }
diff --git lucene/core/src/test/org/apache/lucene/util/automaton/TestCompiledAutomaton.java lucene/core/src/test/org/apache/lucene/util/automaton/TestCompiledAutomaton.java
index b45ee8f..7370f34 100644
--- lucene/core/src/test/org/apache/lucene/util/automaton/TestCompiledAutomaton.java
+++ lucene/core/src/test/org/apache/lucene/util/automaton/TestCompiledAutomaton.java
@@ -31,7 +31,7 @@ import org.apache.lucene.util.TestUtil;
 public class TestCompiledAutomaton extends LuceneTestCase {
 
   private CompiledAutomaton build(String... strings) {
-    final List<BytesRef> terms = new ArrayList<BytesRef>();
+    final List<BytesRef> terms = new ArrayList<>();
     for(String s : strings) {
       terms.add(new BytesRef(s));
     }
@@ -95,7 +95,7 @@ public class TestCompiledAutomaton extends LuceneTestCase {
 
   public void testRandom() throws Exception {
     final int numTerms = atLeast(400);
-    final Set<String> terms = new HashSet<String>();
+    final Set<String> terms = new HashSet<>();
     while(terms.size() != numTerms) {
       terms.add(randomString());
     }
diff --git lucene/core/src/test/org/apache/lucene/util/automaton/TestDeterminizeLexicon.java lucene/core/src/test/org/apache/lucene/util/automaton/TestDeterminizeLexicon.java
index 1bf01e7..e435fe6 100644
--- lucene/core/src/test/org/apache/lucene/util/automaton/TestDeterminizeLexicon.java
+++ lucene/core/src/test/org/apache/lucene/util/automaton/TestDeterminizeLexicon.java
@@ -29,8 +29,8 @@ import org.apache.lucene.util.TestUtil;
  * somewhat randomly, by determinizing a huge random lexicon.
  */
 public class TestDeterminizeLexicon extends LuceneTestCase {
-  private List<Automaton> automata = new ArrayList<Automaton>();
-  private List<String> terms = new ArrayList<String>();
+  private List<Automaton> automata = new ArrayList<>();
+  private List<String> terms = new ArrayList<>();
   
   public void testLexicon() throws Exception {
     int num = atLeast(1);
diff --git lucene/core/src/test/org/apache/lucene/util/automaton/TestLevenshteinAutomata.java lucene/core/src/test/org/apache/lucene/util/automaton/TestLevenshteinAutomata.java
index d0c01b0..c301493 100644
--- lucene/core/src/test/org/apache/lucene/util/automaton/TestLevenshteinAutomata.java
+++ lucene/core/src/test/org/apache/lucene/util/automaton/TestLevenshteinAutomata.java
@@ -141,7 +141,7 @@ public class TestLevenshteinAutomata extends LuceneTestCase {
    * one character)
    */
   private Automaton insertionsOf(String s) {
-    List<Automaton> list = new ArrayList<Automaton>();
+    List<Automaton> list = new ArrayList<>();
     
     for (int i = 0; i <= s.length(); i++) {
       Automaton a = BasicAutomata.makeString(s.substring(0, i));
@@ -161,7 +161,7 @@ public class TestLevenshteinAutomata extends LuceneTestCase {
    * one character).
    */
   private Automaton deletionsOf(String s) {
-    List<Automaton> list = new ArrayList<Automaton>();
+    List<Automaton> list = new ArrayList<>();
     
     for (int i = 0; i < s.length(); i++) {
       Automaton a = BasicAutomata.makeString(s.substring(0, i));
@@ -181,7 +181,7 @@ public class TestLevenshteinAutomata extends LuceneTestCase {
    * (replacing one character)
    */
   private Automaton substitutionsOf(String s) {
-    List<Automaton> list = new ArrayList<Automaton>();
+    List<Automaton> list = new ArrayList<>();
     
     for (int i = 0; i < s.length(); i++) {
       Automaton a = BasicAutomata.makeString(s.substring(0, i));
@@ -203,7 +203,7 @@ public class TestLevenshteinAutomata extends LuceneTestCase {
   private Automaton transpositionsOf(String s) {
     if (s.length() < 2)
       return BasicAutomata.makeEmpty();
-    List<Automaton> list = new ArrayList<Automaton>();
+    List<Automaton> list = new ArrayList<>();
     for (int i = 0; i < s.length()-1; i++) {
       StringBuilder sb = new StringBuilder();
       sb.append(s.substring(0, i));
diff --git lucene/core/src/test/org/apache/lucene/util/fst/Test2BFST.java lucene/core/src/test/org/apache/lucene/util/fst/Test2BFST.java
index 21feb91..9628875 100644
--- lucene/core/src/test/org/apache/lucene/util/fst/Test2BFST.java
+++ lucene/core/src/test/org/apache/lucene/util/fst/Test2BFST.java
@@ -55,7 +55,7 @@ public class Test2BFST extends LuceneTestCase {
         System.out.println("\nTEST: 3B nodes; doPack=false output=NO_OUTPUTS");
         Outputs<Object> outputs = NoOutputs.getSingleton();
         Object NO_OUTPUT = outputs.getNoOutput();
-        final Builder<Object> b = new Builder<Object>(FST.INPUT_TYPE.BYTE1, 0, 0, true, true, Integer.MAX_VALUE, outputs,
+        final Builder<Object> b = new Builder<>(FST.INPUT_TYPE.BYTE1, 0, 0, true, true, Integer.MAX_VALUE, outputs,
                                                       null, doPack, PackedInts.COMPACT, true, 15);
 
         int count = 0;
@@ -98,7 +98,7 @@ public class Test2BFST extends LuceneTestCase {
           }
 
           System.out.println("\nTEST: enum all input/outputs");
-          IntsRefFSTEnum<Object> fstEnum = new IntsRefFSTEnum<Object>(fst);
+          IntsRefFSTEnum<Object> fstEnum = new IntsRefFSTEnum<>(fst);
 
           Arrays.fill(ints2, 0);
           r = new Random(seed);
@@ -124,7 +124,7 @@ public class Test2BFST extends LuceneTestCase {
             fst.save(out);
             out.close();
             IndexInput in = dir.openInput("fst", IOContext.DEFAULT);
-            fst = new FST<Object>(in, outputs);
+            fst = new FST<>(in, outputs);
             in.close();
           } else {
             dir.deleteFile("fst");
@@ -137,7 +137,7 @@ public class Test2BFST extends LuceneTestCase {
       {
         System.out.println("\nTEST: 3 GB size; doPack=" + doPack + " outputs=bytes");
         Outputs<BytesRef> outputs = ByteSequenceOutputs.getSingleton();
-        final Builder<BytesRef> b = new Builder<BytesRef>(FST.INPUT_TYPE.BYTE1, 0, 0, true, true, Integer.MAX_VALUE, outputs,
+        final Builder<BytesRef> b = new Builder<>(FST.INPUT_TYPE.BYTE1, 0, 0, true, true, Integer.MAX_VALUE, outputs,
                                                           null, doPack, PackedInts.COMPACT, true, 15);
 
         byte[] outputBytes = new byte[20];
@@ -177,7 +177,7 @@ public class Test2BFST extends LuceneTestCase {
           }
 
           System.out.println("\nTEST: enum all input/outputs");
-          IntsRefFSTEnum<BytesRef> fstEnum = new IntsRefFSTEnum<BytesRef>(fst);
+          IntsRefFSTEnum<BytesRef> fstEnum = new IntsRefFSTEnum<>(fst);
 
           Arrays.fill(ints, 0);
           r = new Random(seed);
@@ -201,7 +201,7 @@ public class Test2BFST extends LuceneTestCase {
             fst.save(out);
             out.close();
             IndexInput in = dir.openInput("fst", IOContext.DEFAULT);
-            fst = new FST<BytesRef>(in, outputs);
+            fst = new FST<>(in, outputs);
             in.close();
           } else {
             dir.deleteFile("fst");
@@ -214,7 +214,7 @@ public class Test2BFST extends LuceneTestCase {
       {
         System.out.println("\nTEST: 3 GB size; doPack=" + doPack + " outputs=long");
         Outputs<Long> outputs = PositiveIntOutputs.getSingleton();
-        final Builder<Long> b = new Builder<Long>(FST.INPUT_TYPE.BYTE1, 0, 0, true, true, Integer.MAX_VALUE, outputs,
+        final Builder<Long> b = new Builder<>(FST.INPUT_TYPE.BYTE1, 0, 0, true, true, Integer.MAX_VALUE, outputs,
                                                   null, doPack, PackedInts.COMPACT, true, 15);
 
         long output = 1;
@@ -260,7 +260,7 @@ public class Test2BFST extends LuceneTestCase {
           }
 
           System.out.println("\nTEST: enum all input/outputs");
-          IntsRefFSTEnum<Long> fstEnum = new IntsRefFSTEnum<Long>(fst);
+          IntsRefFSTEnum<Long> fstEnum = new IntsRefFSTEnum<>(fst);
 
           Arrays.fill(ints, 0);
           r = new Random(seed);
@@ -285,7 +285,7 @@ public class Test2BFST extends LuceneTestCase {
             fst.save(out);
             out.close();
             IndexInput in = dir.openInput("fst", IOContext.DEFAULT);
-            fst = new FST<Long>(in, outputs);
+            fst = new FST<>(in, outputs);
             in.close();
           } else {
             dir.deleteFile("fst");
diff --git lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java
index e720d79..1d24cf9 100644
--- lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java
+++ lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java
@@ -114,11 +114,11 @@ public class TestFSTs extends LuceneTestCase {
       {
         final Outputs<Object> outputs = NoOutputs.getSingleton();
         final Object NO_OUTPUT = outputs.getNoOutput();      
-        final List<FSTTester.InputOutput<Object>> pairs = new ArrayList<FSTTester.InputOutput<Object>>(terms2.length);
+        final List<FSTTester.InputOutput<Object>> pairs = new ArrayList<>(terms2.length);
         for(IntsRef term : terms2) {
-          pairs.add(new FSTTester.InputOutput<Object>(term, NO_OUTPUT));
+          pairs.add(new FSTTester.InputOutput<>(term, NO_OUTPUT));
         }
-        FST<Object> fst = new FSTTester<Object>(random(), dir, inputMode, pairs, outputs, false).doTest(0, 0, false);
+        FST<Object> fst = new FSTTester<>(random(), dir, inputMode, pairs, outputs, false).doTest(0, 0, false);
         assertNotNull(fst);
         assertEquals(22, fst.getNodeCount());
         assertEquals(27, fst.getArcCount());
@@ -127,11 +127,11 @@ public class TestFSTs extends LuceneTestCase {
       // FST ord pos int
       {
         final PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
-        final List<FSTTester.InputOutput<Long>> pairs = new ArrayList<FSTTester.InputOutput<Long>>(terms2.length);
+        final List<FSTTester.InputOutput<Long>> pairs = new ArrayList<>(terms2.length);
         for(int idx=0;idx<terms2.length;idx++) {
-          pairs.add(new FSTTester.InputOutput<Long>(terms2[idx], (long) idx));
+          pairs.add(new FSTTester.InputOutput<>(terms2[idx], (long) idx));
         }
-        final FST<Long> fst = new FSTTester<Long>(random(), dir, inputMode, pairs, outputs, true).doTest(0, 0, false);
+        final FST<Long> fst = new FSTTester<>(random(), dir, inputMode, pairs, outputs, true).doTest(0, 0, false);
         assertNotNull(fst);
         assertEquals(22, fst.getNodeCount());
         assertEquals(27, fst.getArcCount());
@@ -141,12 +141,12 @@ public class TestFSTs extends LuceneTestCase {
       {
         final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
         final BytesRef NO_OUTPUT = outputs.getNoOutput();      
-        final List<FSTTester.InputOutput<BytesRef>> pairs = new ArrayList<FSTTester.InputOutput<BytesRef>>(terms2.length);
+        final List<FSTTester.InputOutput<BytesRef>> pairs = new ArrayList<>(terms2.length);
         for(int idx=0;idx<terms2.length;idx++) {
           final BytesRef output = random().nextInt(30) == 17 ? NO_OUTPUT : new BytesRef(Integer.toString(idx));
-          pairs.add(new FSTTester.InputOutput<BytesRef>(terms2[idx], output));
+          pairs.add(new FSTTester.InputOutput<>(terms2[idx], output));
         }
-        final FST<BytesRef> fst = new FSTTester<BytesRef>(random(), dir, inputMode, pairs, outputs, false).doTest(0, 0, false);
+        final FST<BytesRef> fst = new FSTTester<>(random(), dir, inputMode, pairs, outputs, false).doTest(0, 0, false);
         assertNotNull(fst);
         assertEquals(24, fst.getNodeCount());
         assertEquals(30, fst.getArcCount());
@@ -162,78 +162,78 @@ public class TestFSTs extends LuceneTestCase {
     {
       final Outputs<Object> outputs = NoOutputs.getSingleton();
       final Object NO_OUTPUT = outputs.getNoOutput();      
-      final List<FSTTester.InputOutput<Object>> pairs = new ArrayList<FSTTester.InputOutput<Object>>(terms.length);
+      final List<FSTTester.InputOutput<Object>> pairs = new ArrayList<>(terms.length);
       for(IntsRef term : terms) {
-        pairs.add(new FSTTester.InputOutput<Object>(term, NO_OUTPUT));
+        pairs.add(new FSTTester.InputOutput<>(term, NO_OUTPUT));
       }
-      new FSTTester<Object>(random(), dir, inputMode, pairs, outputs, false).doTest(true);
+      new FSTTester<>(random(), dir, inputMode, pairs, outputs, false).doTest(true);
     }
 
     // PositiveIntOutput (ord)
     {
       final PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
-      final List<FSTTester.InputOutput<Long>> pairs = new ArrayList<FSTTester.InputOutput<Long>>(terms.length);
+      final List<FSTTester.InputOutput<Long>> pairs = new ArrayList<>(terms.length);
       for(int idx=0;idx<terms.length;idx++) {
-        pairs.add(new FSTTester.InputOutput<Long>(terms[idx], (long) idx));
+        pairs.add(new FSTTester.InputOutput<>(terms[idx], (long) idx));
       }
-      new FSTTester<Long>(random(), dir, inputMode, pairs, outputs, true).doTest(true);
+      new FSTTester<>(random(), dir, inputMode, pairs, outputs, true).doTest(true);
     }
 
     // PositiveIntOutput (random monotonically increasing positive number)
     {
       final PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
-      final List<FSTTester.InputOutput<Long>> pairs = new ArrayList<FSTTester.InputOutput<Long>>(terms.length);
+      final List<FSTTester.InputOutput<Long>> pairs = new ArrayList<>(terms.length);
       long lastOutput = 0;
       for(int idx=0;idx<terms.length;idx++) {
         final long value = lastOutput + TestUtil.nextInt(random(), 1, 1000);
         lastOutput = value;
-        pairs.add(new FSTTester.InputOutput<Long>(terms[idx], value));
+        pairs.add(new FSTTester.InputOutput<>(terms[idx], value));
       }
-      new FSTTester<Long>(random(), dir, inputMode, pairs, outputs, true).doTest(true);
+      new FSTTester<>(random(), dir, inputMode, pairs, outputs, true).doTest(true);
     }
 
     // PositiveIntOutput (random positive number)
     {
       final PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
-      final List<FSTTester.InputOutput<Long>> pairs = new ArrayList<FSTTester.InputOutput<Long>>(terms.length);
+      final List<FSTTester.InputOutput<Long>> pairs = new ArrayList<>(terms.length);
       for(int idx=0;idx<terms.length;idx++) {
-        pairs.add(new FSTTester.InputOutput<Long>(terms[idx], TestUtil.nextLong(random(), 0, Long.MAX_VALUE)));
+        pairs.add(new FSTTester.InputOutput<>(terms[idx], TestUtil.nextLong(random(), 0, Long.MAX_VALUE)));
       }
-      new FSTTester<Long>(random(), dir, inputMode, pairs, outputs, false).doTest(true);
+      new FSTTester<>(random(), dir, inputMode, pairs, outputs, false).doTest(true);
     }
 
     // Pair<ord, (random monotonically increasing positive number>
     {
       final PositiveIntOutputs o1 = PositiveIntOutputs.getSingleton();
       final PositiveIntOutputs o2 = PositiveIntOutputs.getSingleton();
-      final PairOutputs<Long,Long> outputs = new PairOutputs<Long,Long>(o1, o2);
-      final List<FSTTester.InputOutput<PairOutputs.Pair<Long,Long>>> pairs = new ArrayList<FSTTester.InputOutput<PairOutputs.Pair<Long,Long>>>(terms.length);
+      final PairOutputs<Long,Long> outputs = new PairOutputs<>(o1, o2);
+      final List<FSTTester.InputOutput<PairOutputs.Pair<Long,Long>>> pairs = new ArrayList<>(terms.length);
       long lastOutput = 0;
       for(int idx=0;idx<terms.length;idx++) {
         final long value = lastOutput + TestUtil.nextInt(random(), 1, 1000);
         lastOutput = value;
-        pairs.add(new FSTTester.InputOutput<PairOutputs.Pair<Long,Long>>(terms[idx],
+        pairs.add(new FSTTester.InputOutput<>(terms[idx],
                                                                          outputs.newPair((long) idx, value)));
       }
-      new FSTTester<PairOutputs.Pair<Long,Long>>(random(), dir, inputMode, pairs, outputs, false).doTest(true);
+      new FSTTester<>(random(), dir, inputMode, pairs, outputs, false).doTest(true);
     }
 
     // Sequence-of-bytes
     {
       final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
       final BytesRef NO_OUTPUT = outputs.getNoOutput();      
-      final List<FSTTester.InputOutput<BytesRef>> pairs = new ArrayList<FSTTester.InputOutput<BytesRef>>(terms.length);
+      final List<FSTTester.InputOutput<BytesRef>> pairs = new ArrayList<>(terms.length);
       for(int idx=0;idx<terms.length;idx++) {
         final BytesRef output = random().nextInt(30) == 17 ? NO_OUTPUT : new BytesRef(Integer.toString(idx));
-        pairs.add(new FSTTester.InputOutput<BytesRef>(terms[idx], output));
+        pairs.add(new FSTTester.InputOutput<>(terms[idx], output));
       }
-      new FSTTester<BytesRef>(random(), dir, inputMode, pairs, outputs, false).doTest(true);
+      new FSTTester<>(random(), dir, inputMode, pairs, outputs, false).doTest(true);
     }
 
     // Sequence-of-ints
     {
       final IntSequenceOutputs outputs = IntSequenceOutputs.getSingleton();
-      final List<FSTTester.InputOutput<IntsRef>> pairs = new ArrayList<FSTTester.InputOutput<IntsRef>>(terms.length);
+      final List<FSTTester.InputOutput<IntsRef>> pairs = new ArrayList<>(terms.length);
       for(int idx=0;idx<terms.length;idx++) {
         final String s = Integer.toString(idx);
         final IntsRef output = new IntsRef(s.length());
@@ -241,9 +241,9 @@ public class TestFSTs extends LuceneTestCase {
         for(int idx2=0;idx2<output.length;idx2++) {
           output.ints[idx2] = s.charAt(idx2);
         }
-        pairs.add(new FSTTester.InputOutput<IntsRef>(terms[idx], output));
+        pairs.add(new FSTTester.InputOutput<>(terms[idx], output));
       }
-      new FSTTester<IntsRef>(random(), dir, inputMode, pairs, outputs, false).doTest(true);
+      new FSTTester<>(random(), dir, inputMode, pairs, outputs, false).doTest(true);
     }
 
   }
@@ -270,7 +270,7 @@ public class TestFSTs extends LuceneTestCase {
       }
       for(int inputMode=0;inputMode<2;inputMode++) {
         final int numWords = random.nextInt(maxNumWords+1);
-        Set<IntsRef> termsSet = new HashSet<IntsRef>();
+        Set<IntsRef> termsSet = new HashSet<>();
         IntsRef[] terms = new IntsRef[numWords];
         while(termsSet.size() < numWords) {
           final String term = getRandomString(random);
@@ -312,7 +312,7 @@ public class TestFSTs extends LuceneTestCase {
 
     final boolean doRewrite = random().nextBoolean();
 
-    Builder<Long> builder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, 0, 0, true, true, Integer.MAX_VALUE, outputs, null, doRewrite, PackedInts.DEFAULT, true, 15);
+    Builder<Long> builder = new Builder<>(FST.INPUT_TYPE.BYTE1, 0, 0, true, true, Integer.MAX_VALUE, outputs, null, doRewrite, PackedInts.DEFAULT, true, 15);
 
     boolean storeOrd = random().nextBoolean();
     if (VERBOSE) {
@@ -373,7 +373,7 @@ public class TestFSTs extends LuceneTestCase {
         final Random random = new Random(random().nextLong());
         // Now confirm BytesRefFSTEnum and TermsEnum act the
         // same:
-        final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<Long>(fst);
+        final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<>(fst);
         int num = atLeast(1000);
         for(int iter=0;iter<num;iter++) {
           final BytesRef randomTerm = new BytesRef(getRandomString(random));
@@ -455,7 +455,7 @@ public class TestFSTs extends LuceneTestCase {
       this.outputs = outputs;
       this.doPack = doPack;
 
-      builder = new Builder<T>(inputMode == 0 ? FST.INPUT_TYPE.BYTE1 : FST.INPUT_TYPE.BYTE4, 0, prune, prune == 0, true, Integer.MAX_VALUE, outputs, null, doPack, PackedInts.DEFAULT, !noArcArrays, 15);
+      builder = new Builder<>(inputMode == 0 ? FST.INPUT_TYPE.BYTE1 : FST.INPUT_TYPE.BYTE4, 0, prune, prune == 0, true, Integer.MAX_VALUE, outputs, null, doPack, PackedInts.DEFAULT, !noArcArrays, 15);
     }
 
     protected abstract T getOutput(IntsRef input, int ord) throws IOException;
@@ -657,7 +657,7 @@ public class TestFSTs extends LuceneTestCase {
       // Store both ord & docFreq:
       final PositiveIntOutputs o1 = PositiveIntOutputs.getSingleton();
       final PositiveIntOutputs o2 = PositiveIntOutputs.getSingleton();
-      final PairOutputs<Long,Long> outputs = new PairOutputs<Long,Long>(o1, o2);
+      final PairOutputs<Long,Long> outputs = new PairOutputs<>(o1, o2);
       new VisitTerms<PairOutputs.Pair<Long,Long>>(dirOut, wordsFileIn, inputMode, prune, outputs, doPack, noArcArrays) {
         Random rand;
         @Override
@@ -706,9 +706,9 @@ public class TestFSTs extends LuceneTestCase {
 
   public void testSingleString() throws Exception {
     final Outputs<Object> outputs = NoOutputs.getSingleton();
-    final Builder<Object> b = new Builder<Object>(FST.INPUT_TYPE.BYTE1, outputs);
+    final Builder<Object> b = new Builder<>(FST.INPUT_TYPE.BYTE1, outputs);
     b.add(Util.toIntsRef(new BytesRef("foobar"), new IntsRef()), outputs.getNoOutput());
-    final BytesRefFSTEnum<Object> fstEnum = new BytesRefFSTEnum<Object>(b.finish());
+    final BytesRefFSTEnum<Object> fstEnum = new BytesRefFSTEnum<>(b.finish());
     assertNull(fstEnum.seekFloor(new BytesRef("foo")));
     assertNull(fstEnum.seekCeil(new BytesRef("foobaz")));
   }
@@ -717,7 +717,7 @@ public class TestFSTs extends LuceneTestCase {
   public void testDuplicateFSAString() throws Exception {
     String str = "foobar";
     final Outputs<Object> outputs = NoOutputs.getSingleton();
-    final Builder<Object> b = new Builder<Object>(FST.INPUT_TYPE.BYTE1, outputs);
+    final Builder<Object> b = new Builder<>(FST.INPUT_TYPE.BYTE1, outputs);
     IntsRef ints = new IntsRef();
     for(int i=0; i<10; i++) {
       b.add(Util.toIntsRef(new BytesRef(str), ints), outputs.getNoOutput());
@@ -726,7 +726,7 @@ public class TestFSTs extends LuceneTestCase {
     
     // count the input paths
     int count = 0; 
-    final BytesRefFSTEnum<Object> fstEnum = new BytesRefFSTEnum<Object>(fst);
+    final BytesRefFSTEnum<Object> fstEnum = new BytesRefFSTEnum<>(fst);
     while(fstEnum.next()!=null) {
       count++;  
     }
@@ -786,7 +786,7 @@ public class TestFSTs extends LuceneTestCase {
     final PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
 
     // Build an FST mapping BytesRef -> Long
-    final Builder<Long> builder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, outputs);
+    final Builder<Long> builder = new Builder<>(FST.INPUT_TYPE.BYTE1, outputs);
 
     final BytesRef a = new BytesRef("a");
     final BytesRef b = new BytesRef("b");
@@ -802,7 +802,7 @@ public class TestFSTs extends LuceneTestCase {
     assertEquals(42, (long) Util.get(fst, b));
     assertEquals(17, (long) Util.get(fst, a));
 
-    BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<Long>(fst);
+    BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<>(fst);
     BytesRefFSTEnum.InputOutput<Long> seekResult;
     seekResult = fstEnum.seekFloor(a);
     assertNotNull(seekResult);
@@ -846,7 +846,7 @@ public class TestFSTs extends LuceneTestCase {
       if (VERBOSE) {
         System.out.println("TEST: NUM_IDS=" + NUM_IDS);
       }
-      final Set<String> allIDs = new HashSet<String>();
+      final Set<String> allIDs = new HashSet<>();
       for(int id=0;id<NUM_IDS;id++) {
         String idString;
         if (cycle == 0) {
@@ -873,12 +873,12 @@ public class TestFSTs extends LuceneTestCase {
       final IndexSearcher s = newSearcher(r);
       w.close();
 
-      final List<String> allIDsList = new ArrayList<String>(allIDs);
-      final List<String> sortedAllIDsList = new ArrayList<String>(allIDsList);
+      final List<String> allIDsList = new ArrayList<>(allIDs);
+      final List<String> sortedAllIDsList = new ArrayList<>(allIDsList);
       Collections.sort(sortedAllIDsList);
 
       // Sprinkle in some non-existent PKs:
-      Set<String> outOfBounds = new HashSet<String>();
+      Set<String> outOfBounds = new HashSet<>();
       for(int idx=0;idx<NUM_IDS/10;idx++) {
         String idString;
         if (cycle == 0) {
@@ -976,7 +976,7 @@ public class TestFSTs extends LuceneTestCase {
       System.out.println("TEST: NUM_TERMS=" + NUM_TERMS);
     }
 
-    final Set<String> allTerms = new HashSet<String>();
+    final Set<String> allTerms = new HashSet<>();
     while(allTerms.size() < NUM_TERMS) {
       allTerms.add(simpleRandomString(random()));
     }
@@ -997,7 +997,7 @@ public class TestFSTs extends LuceneTestCase {
     IndexSearcher s = newSearcher(r);
     w.close();
 
-    final List<String> allTermsList = new ArrayList<String>(allTerms);
+    final List<String> allTermsList = new ArrayList<>(allTerms);
     Collections.shuffle(allTermsList, random());
 
     // verify exact lookup
@@ -1024,7 +1024,7 @@ public class TestFSTs extends LuceneTestCase {
       FST<Object> compile(String[] lines) throws IOException {
         final NoOutputs outputs = NoOutputs.getSingleton();
         final Object nothing = outputs.getNoOutput();
-        final Builder<Object> b = new Builder<Object>(FST.INPUT_TYPE.BYTE1, outputs);
+        final Builder<Object> b = new Builder<>(FST.INPUT_TYPE.BYTE1, outputs);
 
         int line = 0;
         final BytesRef term = new BytesRef();
@@ -1064,7 +1064,7 @@ public class TestFSTs extends LuceneTestCase {
                arc = fst.readNextArc(arc, fstReader), childCount++)
           {
             boolean expanded = fst.isExpandedTarget(arc, fstReader);
-            int children = verifyStateAndBelow(fst, new FST.Arc<Object>().copyFrom(arc), depth + 1);
+            int children = verifyStateAndBelow(fst, new FST.Arc<>().copyFrom(arc), depth + 1);
 
             assertEquals(
                 expanded,
@@ -1086,20 +1086,20 @@ public class TestFSTs extends LuceneTestCase {
 
     SyntheticData s = new SyntheticData();
 
-    ArrayList<String> out = new ArrayList<String>();
+    ArrayList<String> out = new ArrayList<>();
     StringBuilder b = new StringBuilder();
     s.generate(out, b, 'a', 'i', 10);
     String[] input = out.toArray(new String[out.size()]);
     Arrays.sort(input);
     FST<Object> fst = s.compile(input);
-    FST.Arc<Object> arc = fst.getFirstArc(new FST.Arc<Object>());
+    FST.Arc<Object> arc = fst.getFirstArc(new FST.Arc<>());
     s.verifyStateAndBelow(fst, arc, 1);
   }
 
   public void testFinalOutputOnEndState() throws Exception {
     final PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
 
-    final Builder<Long> builder = new Builder<Long>(FST.INPUT_TYPE.BYTE4, 2, 0, true, true, Integer.MAX_VALUE, outputs, null, random().nextBoolean(), PackedInts.DEFAULT, true, 15);
+    final Builder<Long> builder = new Builder<>(FST.INPUT_TYPE.BYTE4, 2, 0, true, true, Integer.MAX_VALUE, outputs, null, random().nextBoolean(), PackedInts.DEFAULT, true, 15);
     builder.add(Util.toUTF32("stat", new IntsRef()), 17L);
     builder.add(Util.toUTF32("station", new IntsRef()), 10L);
     final FST<Long> fst = builder.finish();
@@ -1114,7 +1114,7 @@ public class TestFSTs extends LuceneTestCase {
   public void testInternalFinalState() throws Exception {
     final PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
     final boolean willRewrite = random().nextBoolean();
-    final Builder<Long> builder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, 0, 0, true, true, Integer.MAX_VALUE, outputs, null, willRewrite, PackedInts.DEFAULT, true, 15);
+    final Builder<Long> builder = new Builder<>(FST.INPUT_TYPE.BYTE1, 0, 0, true, true, Integer.MAX_VALUE, outputs, null, willRewrite, PackedInts.DEFAULT, true, 15);
     builder.add(Util.toIntsRef(new BytesRef("stat"), new IntsRef()), outputs.getNoOutput());
     builder.add(Util.toIntsRef(new BytesRef("station"), new IntsRef()), outputs.getNoOutput());
     final FST<Long> fst = builder.finish();
@@ -1135,15 +1135,15 @@ public class TestFSTs extends LuceneTestCase {
   public void testNonFinalStopNode() throws Exception {
     final PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
     final Long nothing = outputs.getNoOutput();
-    final Builder<Long> b = new Builder<Long>(FST.INPUT_TYPE.BYTE1, outputs);
+    final Builder<Long> b = new Builder<>(FST.INPUT_TYPE.BYTE1, outputs);
 
-    final FST<Long> fst = new FST<Long>(FST.INPUT_TYPE.BYTE1, outputs, false, PackedInts.COMPACT, true, 15);
+    final FST<Long> fst = new FST<>(FST.INPUT_TYPE.BYTE1, outputs, false, PackedInts.COMPACT, true, 15);
 
-    final Builder.UnCompiledNode<Long> rootNode = new Builder.UnCompiledNode<Long>(b, 0);
+    final Builder.UnCompiledNode<Long> rootNode = new Builder.UnCompiledNode<>(b, 0);
 
     // Add final stop node
     {
-      final Builder.UnCompiledNode<Long> node = new Builder.UnCompiledNode<Long>(b, 0);
+      final Builder.UnCompiledNode<Long> node = new Builder.UnCompiledNode<>(b, 0);
       node.isFinal = true;
       rootNode.addArc('a', node);
       final Builder.CompiledNode frozen = new Builder.CompiledNode();
@@ -1156,7 +1156,7 @@ public class TestFSTs extends LuceneTestCase {
 
     // Add non-final stop node
     {
-      final Builder.UnCompiledNode<Long> node = new Builder.UnCompiledNode<Long>(b, 0);
+      final Builder.UnCompiledNode<Long> node = new Builder.UnCompiledNode<>(b, 0);
       rootNode.addArc('b', node);
       final Builder.CompiledNode frozen = new Builder.CompiledNode();
       frozen.node = fst.addNode(node);
@@ -1181,7 +1181,7 @@ public class TestFSTs extends LuceneTestCase {
     out.close();
 
     IndexInput in = dir.openInput("fst", IOContext.DEFAULT);
-    final FST<Long> fst2 = new FST<Long>(in, outputs);
+    final FST<Long> fst2 = new FST<>(in, outputs);
     checkStopNodes(fst2, outputs);
     in.close();
     dir.close();
@@ -1214,7 +1214,7 @@ public class TestFSTs extends LuceneTestCase {
 
   public void testShortestPaths() throws Exception {
     final PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
-    final Builder<Long> builder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, outputs);
+    final Builder<Long> builder = new Builder<>(FST.INPUT_TYPE.BYTE1, outputs);
 
     final IntsRef scratch = new IntsRef();
     builder.add(Util.toIntsRef(new BytesRef("aab"), scratch), 22L);
@@ -1254,12 +1254,12 @@ public class TestFSTs extends LuceneTestCase {
   /** like testShortestPaths, but uses pairoutputs so we have both a weight and an output */
   public void testShortestPathsWFST() throws Exception {
 
-    PairOutputs<Long,Long> outputs = new PairOutputs<Long,Long>(
+    PairOutputs<Long,Long> outputs = new PairOutputs<>(
         PositiveIntOutputs.getSingleton(), // weight
         PositiveIntOutputs.getSingleton()  // output
     );
     
-    final Builder<Pair<Long,Long>> builder = new Builder<Pair<Long,Long>>(FST.INPUT_TYPE.BYTE1, outputs);
+    final Builder<Pair<Long,Long>> builder = new Builder<>(FST.INPUT_TYPE.BYTE1, outputs);
 
     final IntsRef scratch = new IntsRef();
     builder.add(Util.toIntsRef(new BytesRef("aab"), scratch), outputs.newPair(22L, 57L));
@@ -1295,11 +1295,11 @@ public class TestFSTs extends LuceneTestCase {
     final Random random = random();
     int numWords = atLeast(1000);
     
-    final TreeMap<String,Long> slowCompletor = new TreeMap<String,Long>();
-    final TreeSet<String> allPrefixes = new TreeSet<String>();
+    final TreeMap<String,Long> slowCompletor = new TreeMap<>();
+    final TreeSet<String> allPrefixes = new TreeSet<>();
     
     final PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
-    final Builder<Long> builder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, outputs);
+    final Builder<Long> builder = new Builder<>(FST.INPUT_TYPE.BYTE1, outputs);
     final IntsRef scratch = new IntsRef();
     
     for (int i = 0; i < numWords; i++) {
@@ -1350,19 +1350,19 @@ public class TestFSTs extends LuceneTestCase {
       Util.MinResult<Long>[] r = Util.shortestPaths(fst, arc, fst.outputs.getNoOutput(), minLongComparator, topN, true);
 
       // 2. go thru whole treemap (slowCompletor) and check its actually the best suggestion
-      final List<Util.MinResult<Long>> matches = new ArrayList<Util.MinResult<Long>>();
+      final List<Util.MinResult<Long>> matches = new ArrayList<>();
 
       // TODO: could be faster... but its slowCompletor for a reason
       for (Map.Entry<String,Long> e : slowCompletor.entrySet()) {
         if (e.getKey().startsWith(prefix)) {
           //System.out.println("  consider " + e.getKey());
-          matches.add(new Util.MinResult<Long>(Util.toIntsRef(new BytesRef(e.getKey().substring(prefix.length())), new IntsRef()),
+          matches.add(new Util.MinResult<>(Util.toIntsRef(new BytesRef(e.getKey().substring(prefix.length())), new IntsRef()),
                                          e.getValue() - prefixOutput));
         }
       }
 
       assertTrue(matches.size() > 0);
-      Collections.sort(matches, new TieBreakByInputComparator<Long>(minLongComparator));
+      Collections.sort(matches, new TieBreakByInputComparator<>(minLongComparator));
       if (matches.size() > topN) {
         matches.subList(topN, matches.size()).clear();
       }
@@ -1409,14 +1409,14 @@ public class TestFSTs extends LuceneTestCase {
   public void testShortestPathsWFSTRandom() throws Exception {
     int numWords = atLeast(1000);
     
-    final TreeMap<String,TwoLongs> slowCompletor = new TreeMap<String,TwoLongs>();
-    final TreeSet<String> allPrefixes = new TreeSet<String>();
+    final TreeMap<String,TwoLongs> slowCompletor = new TreeMap<>();
+    final TreeSet<String> allPrefixes = new TreeSet<>();
     
-    PairOutputs<Long,Long> outputs = new PairOutputs<Long,Long>(
+    PairOutputs<Long,Long> outputs = new PairOutputs<>(
         PositiveIntOutputs.getSingleton(), // weight
         PositiveIntOutputs.getSingleton()  // output
     );
-    final Builder<Pair<Long,Long>> builder = new Builder<Pair<Long,Long>>(FST.INPUT_TYPE.BYTE1, outputs);
+    final Builder<Pair<Long,Long>> builder = new Builder<>(FST.INPUT_TYPE.BYTE1, outputs);
     final IntsRef scratch = new IntsRef();
     
     Random random = random();
@@ -1471,19 +1471,19 @@ public class TestFSTs extends LuceneTestCase {
       Util.MinResult<Pair<Long,Long>>[] r = Util.shortestPaths(fst, arc, fst.outputs.getNoOutput(), minPairWeightComparator, topN, true);
 
       // 2. go thru whole treemap (slowCompletor) and check its actually the best suggestion
-      final List<Util.MinResult<Pair<Long,Long>>> matches = new ArrayList<Util.MinResult<Pair<Long,Long>>>();
+      final List<Util.MinResult<Pair<Long,Long>>> matches = new ArrayList<>();
 
       // TODO: could be faster... but its slowCompletor for a reason
       for (Map.Entry<String,TwoLongs> e : slowCompletor.entrySet()) {
         if (e.getKey().startsWith(prefix)) {
           //System.out.println("  consider " + e.getKey());
-          matches.add(new Util.MinResult<Pair<Long,Long>>(Util.toIntsRef(new BytesRef(e.getKey().substring(prefix.length())), new IntsRef()),
+          matches.add(new Util.MinResult<>(Util.toIntsRef(new BytesRef(e.getKey().substring(prefix.length())), new IntsRef()),
                                                           outputs.newPair(e.getValue().a - prefixOutput.output1, e.getValue().b - prefixOutput.output2)));
         }
       }
 
       assertTrue(matches.size() > 0);
-      Collections.sort(matches, new TieBreakByInputComparator<Pair<Long,Long>>(minPairWeightComparator));
+      Collections.sort(matches, new TieBreakByInputComparator<>(minPairWeightComparator));
       if (matches.size() > topN) {
         matches.subList(topN, matches.size()).clear();
       }
@@ -1500,7 +1500,7 @@ public class TestFSTs extends LuceneTestCase {
 
   public void testLargeOutputsOnArrayArcs() throws Exception {
     final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
-    final Builder<BytesRef> builder = new Builder<BytesRef>(FST.INPUT_TYPE.BYTE1, outputs);
+    final Builder<BytesRef> builder = new Builder<>(FST.INPUT_TYPE.BYTE1, outputs);
 
     final byte[] bytes = new byte[300];
     final IntsRef input = new IntsRef();
diff --git lucene/core/src/test/org/apache/lucene/util/junitcompat/TestExceptionInBeforeClassHooks.java lucene/core/src/test/org/apache/lucene/util/junitcompat/TestExceptionInBeforeClassHooks.java
index 4457b04..18af7af 100644
--- lucene/core/src/test/org/apache/lucene/util/junitcompat/TestExceptionInBeforeClassHooks.java
+++ lucene/core/src/test/org/apache/lucene/util/junitcompat/TestExceptionInBeforeClassHooks.java
@@ -117,7 +117,7 @@ public class TestExceptionInBeforeClassHooks extends WithNestedTests {
     Assert.assertEquals(3, runClasses.getFailureCount());
     Assert.assertEquals(3, runClasses.getRunCount());
     
-    ArrayList<String> foobars = new ArrayList<String>();
+    ArrayList<String> foobars = new ArrayList<>();
     for (Failure f : runClasses.getFailures()) {
       Matcher m = Pattern.compile("foobar[0-9]+").matcher(f.getTrace());
       while (m.find()) {
diff --git lucene/core/src/test/org/apache/lucene/util/junitcompat/TestJUnitRuleOrder.java lucene/core/src/test/org/apache/lucene/util/junitcompat/TestJUnitRuleOrder.java
index 875879f..9d29c9f 100644
--- lucene/core/src/test/org/apache/lucene/util/junitcompat/TestJUnitRuleOrder.java
+++ lucene/core/src/test/org/apache/lucene/util/junitcompat/TestJUnitRuleOrder.java
@@ -75,7 +75,7 @@ public class TestJUnitRuleOrder extends WithNestedTests {
 
     @BeforeClass
     public static void beforeClassCleanup() {
-      stack = new Stack<String>();
+      stack = new Stack<>();
     }
 
     @AfterClass
diff --git lucene/core/src/test/org/apache/lucene/util/packed/TestPackedInts.java lucene/core/src/test/org/apache/lucene/util/packed/TestPackedInts.java
index d997f38..db9e917 100644
--- lucene/core/src/test/org/apache/lucene/util/packed/TestPackedInts.java
+++ lucene/core/src/test/org/apache/lucene/util/packed/TestPackedInts.java
@@ -355,7 +355,7 @@ public class TestPackedInts extends LuceneTestCase {
 
   private static List<PackedInts.Mutable> createPackedInts(
           int valueCount, int bitsPerValue) {
-    List<PackedInts.Mutable> packedInts = new ArrayList<PackedInts.Mutable>();
+    List<PackedInts.Mutable> packedInts = new ArrayList<>();
     if (bitsPerValue <= 8) {
       packedInts.add(new Direct8(valueCount));
     }
diff --git lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java
index 8b17e3c..7d492d7 100644
--- lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java
+++ lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java
@@ -106,7 +106,7 @@ public class AssociationsFacetsExample {
     Facets genre = new TaxonomyFacetSumFloatAssociations("$genre", taxoReader, config, fc);
 
     // Retrieve results
-    List<FacetResult> results = new ArrayList<FacetResult>();
+    List<FacetResult> results = new ArrayList<>();
     results.add(tags.getTopChildren(10, "tags"));
     results.add(genre.getTopChildren(10, "genre"));
 
diff --git lucene/demo/src/java/org/apache/lucene/demo/facet/MultiCategoryListsFacetsExample.java lucene/demo/src/java/org/apache/lucene/demo/facet/MultiCategoryListsFacetsExample.java
index a638312..0bc2bbb 100644
--- lucene/demo/src/java/org/apache/lucene/demo/facet/MultiCategoryListsFacetsExample.java
+++ lucene/demo/src/java/org/apache/lucene/demo/facet/MultiCategoryListsFacetsExample.java
@@ -105,7 +105,7 @@ public class MultiCategoryListsFacetsExample {
     FacetsCollector.search(searcher, new MatchAllDocsQuery(), 10, fc);
 
     // Retrieve results
-    List<FacetResult> results = new ArrayList<FacetResult>();
+    List<FacetResult> results = new ArrayList<>();
 
     // Count both "Publish Date" and "Author" dimensions
     Facets author = new FastTaxonomyFacetCounts("author", taxoReader, config, fc);
diff --git lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleFacetsExample.java lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleFacetsExample.java
index eec15d7..0e1c4a6 100644
--- lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleFacetsExample.java
+++ lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleFacetsExample.java
@@ -104,7 +104,7 @@ public class SimpleFacetsExample {
     FacetsCollector.search(searcher, new MatchAllDocsQuery(), 10, fc);
 
     // Retrieve results
-    List<FacetResult> results = new ArrayList<FacetResult>();
+    List<FacetResult> results = new ArrayList<>();
 
     // Count both "Publish Date" and "Author" dimensions
     Facets facets = new FastTaxonomyFacetCounts(taxoReader, config, fc);
@@ -131,7 +131,7 @@ public class SimpleFacetsExample {
     searcher.search(new MatchAllDocsQuery(), null /*Filter */, fc);
 
     // Retrieve results
-    List<FacetResult> results = new ArrayList<FacetResult>();
+    List<FacetResult> results = new ArrayList<>();
 
     // Count both "Publish Date" and "Author" dimensions
     Facets facets = new FastTaxonomyFacetCounts(taxoReader, config, fc);
diff --git lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java
index 7fc1a20..46054f6 100644
--- lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java
+++ lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java
@@ -102,7 +102,7 @@ public class SimpleSortedSetFacetsExample {
     // Retrieve results
     Facets facets = new SortedSetDocValuesFacetCounts(state, fc);
 
-    List<FacetResult> results = new ArrayList<FacetResult>();
+    List<FacetResult> results = new ArrayList<>();
     results.add(facets.getTopChildren(10, "Author"));
     results.add(facets.getTopChildren(10, "Publish Year"));
     indexReader.close();
diff --git lucene/facet/src/java/org/apache/lucene/facet/DrillDownQuery.java lucene/facet/src/java/org/apache/lucene/facet/DrillDownQuery.java
index 8a39034..29b4068 100644
--- lucene/facet/src/java/org/apache/lucene/facet/DrillDownQuery.java
+++ lucene/facet/src/java/org/apache/lucene/facet/DrillDownQuery.java
@@ -56,7 +56,7 @@ public final class DrillDownQuery extends Query {
 
   private final FacetsConfig config;
   private final BooleanQuery query;
-  private final Map<String,Integer> drillDownDims = new LinkedHashMap<String,Integer>();
+  private final Map<String,Integer> drillDownDims = new LinkedHashMap<>();
 
   /** Used by clone() */
   DrillDownQuery(FacetsConfig config, BooleanQuery query, Map<String,Integer> drillDownDims) {
@@ -233,8 +233,8 @@ public final class DrillDownQuery extends Query {
       return new MatchAllDocsQuery();
     }
 
-    List<Filter> filters = new ArrayList<Filter>();
-    List<Query> queries = new ArrayList<Query>();
+    List<Filter> filters = new ArrayList<>();
+    List<Query> queries = new ArrayList<>();
     List<BooleanClause> clauses = query.clauses();
     Query baseQuery;
     int startIndex;
diff --git lucene/facet/src/java/org/apache/lucene/facet/DrillSideways.java lucene/facet/src/java/org/apache/lucene/facet/DrillSideways.java
index e8741eb..01b3b6c 100644
--- lucene/facet/src/java/org/apache/lucene/facet/DrillSideways.java
+++ lucene/facet/src/java/org/apache/lucene/facet/DrillSideways.java
@@ -101,7 +101,7 @@ public class DrillSideways {
   protected Facets buildFacetsResult(FacetsCollector drillDowns, FacetsCollector[] drillSideways, String[] drillSidewaysDims) throws IOException {
 
     Facets drillDownFacets;
-    Map<String,Facets> drillSidewaysFacets = new HashMap<String,Facets>();
+    Map<String,Facets> drillSidewaysFacets = new HashMap<>();
 
     if (taxoReader != null) {
       drillDownFacets = new FastTaxonomyFacetCounts(taxoReader, config, drillDowns);
diff --git lucene/facet/src/java/org/apache/lucene/facet/FacetsCollector.java lucene/facet/src/java/org/apache/lucene/facet/FacetsCollector.java
index bf02d62..912725d 100644
--- lucene/facet/src/java/org/apache/lucene/facet/FacetsCollector.java
+++ lucene/facet/src/java/org/apache/lucene/facet/FacetsCollector.java
@@ -54,7 +54,7 @@ public class FacetsCollector extends Collector {
   private int totalHits;
   private float[] scores;
   private final boolean keepScores;
-  private final List<MatchingDocs> matchingDocs = new ArrayList<MatchingDocs>();
+  private final List<MatchingDocs> matchingDocs = new ArrayList<>();
   private Docs docs;
   
   /**
diff --git lucene/facet/src/java/org/apache/lucene/facet/FacetsConfig.java lucene/facet/src/java/org/apache/lucene/facet/FacetsConfig.java
index 12351a1..e72f9c4 100644
--- lucene/facet/src/java/org/apache/lucene/facet/FacetsConfig.java
+++ lucene/facet/src/java/org/apache/lucene/facet/FacetsConfig.java
@@ -62,11 +62,11 @@ public class FacetsConfig {
    *  doc values). */
   public static final String DEFAULT_INDEX_FIELD_NAME = "$facets";
 
-  private final Map<String,DimConfig> fieldTypes = new ConcurrentHashMap<String,DimConfig>();
+  private final Map<String,DimConfig> fieldTypes = new ConcurrentHashMap<>();
 
   // Used only for best-effort detection of app mixing
   // int/float/bytes in a single indexed field:
-  private final Map<String,String> assocDimTypes = new ConcurrentHashMap<String,String>();
+  private final Map<String,String> assocDimTypes = new ConcurrentHashMap<>();
 
   /** Holds the configuration for one dimension
    *
@@ -198,15 +198,15 @@ public class FacetsConfig {
    */
   public Document build(TaxonomyWriter taxoWriter, Document doc) throws IOException {
     // Find all FacetFields, collated by the actual field:
-    Map<String,List<FacetField>> byField = new HashMap<String,List<FacetField>>();
+    Map<String,List<FacetField>> byField = new HashMap<>();
 
     // ... and also all SortedSetDocValuesFacetFields:
-    Map<String,List<SortedSetDocValuesFacetField>> dvByField = new HashMap<String,List<SortedSetDocValuesFacetField>>();
+    Map<String,List<SortedSetDocValuesFacetField>> dvByField = new HashMap<>();
 
     // ... and also all AssociationFacetFields
-    Map<String,List<AssociationFacetField>> assocByField = new HashMap<String,List<AssociationFacetField>>();
+    Map<String,List<AssociationFacetField>> assocByField = new HashMap<>();
 
-    Set<String> seenDims = new HashSet<String>();
+    Set<String> seenDims = new HashSet<>();
 
     for (IndexableField field : doc.indexableFields()) {
       if (field.fieldType() == FacetField.TYPE) {
@@ -218,7 +218,7 @@ public class FacetsConfig {
         String indexFieldName = dimConfig.indexFieldName;
         List<FacetField> fields = byField.get(indexFieldName);
         if (fields == null) {
-          fields = new ArrayList<FacetField>();
+          fields = new ArrayList<>();
           byField.put(indexFieldName, fields);
         }
         fields.add(facetField);
@@ -233,7 +233,7 @@ public class FacetsConfig {
         String indexFieldName = dimConfig.indexFieldName;
         List<SortedSetDocValuesFacetField> fields = dvByField.get(indexFieldName);
         if (fields == null) {
-          fields = new ArrayList<SortedSetDocValuesFacetField>();
+          fields = new ArrayList<>();
           dvByField.put(indexFieldName, fields);
         }
         fields.add(facetField);
@@ -255,7 +255,7 @@ public class FacetsConfig {
         String indexFieldName = dimConfig.indexFieldName;
         List<AssociationFacetField> fields = assocByField.get(indexFieldName);
         if (fields == null) {
-          fields = new ArrayList<AssociationFacetField>();
+          fields = new ArrayList<>();
           assocByField.put(indexFieldName, fields);
         }
         fields.add(facetField);
@@ -514,7 +514,7 @@ public class FacetsConfig {
    *  #pathToString}) back into the original {@code
    *  String[]}. */
   public static String[] stringToPath(String s) {
-    List<String> parts = new ArrayList<String>();
+    List<String> parts = new ArrayList<>();
     int length = s.length();
     if (length == 0) {
       return new String[0];
diff --git lucene/facet/src/java/org/apache/lucene/facet/range/LongRangeCounter.java lucene/facet/src/java/org/apache/lucene/facet/range/LongRangeCounter.java
index 3e9db73..0f047b2 100644
--- lucene/facet/src/java/org/apache/lucene/facet/range/LongRangeCounter.java
+++ lucene/facet/src/java/org/apache/lucene/facet/range/LongRangeCounter.java
@@ -44,7 +44,7 @@ final class LongRangeCounter {
     // track the start vs end case separately because if a
     // given point is both, then it must be its own
     // elementary interval:
-    Map<Long,Integer> endsMap = new HashMap<Long,Integer>();
+    Map<Long,Integer> endsMap = new HashMap<>();
 
     endsMap.put(Long.MIN_VALUE, 1);
     endsMap.put(Long.MAX_VALUE, 2);
@@ -64,11 +64,11 @@ final class LongRangeCounter {
       }
     }
 
-    List<Long> endsList = new ArrayList<Long>(endsMap.keySet());
+    List<Long> endsList = new ArrayList<>(endsMap.keySet());
     Collections.sort(endsList);
 
     // Build elementaryIntervals (a 1D Venn diagram):
-    List<InclusiveRange> elementaryIntervals = new ArrayList<InclusiveRange>();
+    List<InclusiveRange> elementaryIntervals = new ArrayList<>();
     int upto0 = 1;
     long v = endsList.get(0);
     long prev;
@@ -281,7 +281,7 @@ final class LongRangeCounter {
         // Our range is fully included in the incoming
         // range; add to our output list:
         if (outputs == null) {
-          outputs = new ArrayList<Integer>();
+          outputs = new ArrayList<>();
         }
         outputs.add(index);
       } else if (left != null) {
diff --git lucene/facet/src/java/org/apache/lucene/facet/sortedset/DefaultSortedSetDocValuesReaderState.java lucene/facet/src/java/org/apache/lucene/facet/sortedset/DefaultSortedSetDocValuesReaderState.java
index 9665b28..8f38619 100644
--- lucene/facet/src/java/org/apache/lucene/facet/sortedset/DefaultSortedSetDocValuesReaderState.java
+++ lucene/facet/src/java/org/apache/lucene/facet/sortedset/DefaultSortedSetDocValuesReaderState.java
@@ -42,7 +42,7 @@ public class DefaultSortedSetDocValuesReaderState extends SortedSetDocValuesRead
   /** {@link IndexReader} passed to the constructor. */
   public final IndexReader origReader;
 
-  private final Map<String,OrdRange> prefixToOrdRange = new HashMap<String,OrdRange>();
+  private final Map<String,OrdRange> prefixToOrdRange = new HashMap<>();
 
   /** Creates this, pulling doc values from the default {@link
    *  FacetsConfig#DEFAULT_INDEX_FIELD_NAME}. */ 
diff --git lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts.java lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts.java
index 76c7205..9210748 100644
--- lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts.java
+++ lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts.java
@@ -266,7 +266,7 @@ public class SortedSetDocValuesFacetCounts extends Facets {
   @Override
   public List<FacetResult> getAllDims(int topN) throws IOException {
 
-    List<FacetResult> results = new ArrayList<FacetResult>();
+    List<FacetResult> results = new ArrayList<>();
     for(Map.Entry<String,OrdRange> ent : state.getPrefixToOrdRange().entrySet()) {
       FacetResult fr = getDim(ent.getKey(), ent.getValue(), topN);
       if (fr != null) {
diff --git lucene/facet/src/java/org/apache/lucene/facet/taxonomy/CachedOrdinalsReader.java lucene/facet/src/java/org/apache/lucene/facet/taxonomy/CachedOrdinalsReader.java
index e6ada5f..bbd3250 100644
--- lucene/facet/src/java/org/apache/lucene/facet/taxonomy/CachedOrdinalsReader.java
+++ lucene/facet/src/java/org/apache/lucene/facet/taxonomy/CachedOrdinalsReader.java
@@ -57,7 +57,7 @@ public class CachedOrdinalsReader extends OrdinalsReader {
 
   private final OrdinalsReader source;
 
-  private final Map<Object,CachedOrds> ordsCache = new WeakHashMap<Object,CachedOrds>();
+  private final Map<Object,CachedOrds> ordsCache = new WeakHashMap<>();
 
   /** Sole constructor. */
   public CachedOrdinalsReader(OrdinalsReader source) {
diff --git lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetSumValueSource.java lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetSumValueSource.java
index bb04db1..954dd3b 100644
--- lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetSumValueSource.java
+++ lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetSumValueSource.java
@@ -74,7 +74,7 @@ public class TaxonomyFacetSumValueSource extends FloatTaxonomyFacets {
 
   private final void sumValues(List<MatchingDocs> matchingDocs, boolean keepScores, ValueSource valueSource) throws IOException {
     final FakeScorer scorer = new FakeScorer();
-    Map<String, Scorer> context = new HashMap<String, Scorer>();
+    Map<String, Scorer> context = new HashMap<>();
     if (keepScores) {
       context.put("scorer", scorer);
     }
diff --git lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacets.java lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacets.java
index 0c4d586..d1d9e11 100644
--- lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacets.java
+++ lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacets.java
@@ -85,7 +85,7 @@ public abstract class TaxonomyFacets extends Facets {
   @Override
   public List<FacetResult> getAllDims(int topN) throws IOException {
     int ord = children[TaxonomyReader.ROOT_ORDINAL];
-    List<FacetResult> results = new ArrayList<FacetResult>();
+    List<FacetResult> results = new ArrayList<>();
     while (ord != TaxonomyReader.INVALID_ORDINAL) {
       String dim = taxoReader.getPath(ord).components[0];
       FacetsConfig.DimConfig dimConfig = config.getDimConfig(dim);
diff --git lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java
index 0d4d6ce..a084278 100644
--- lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java
+++ lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java
@@ -102,8 +102,8 @@ public class DirectoryTaxonomyReader extends TaxonomyReader {
 
     // These are the default cache sizes; they can be configured after
     // construction with the cache's setMaxSize() method
-    ordinalCache = new LRUHashMap<FacetLabel, Integer>(DEFAULT_CACHE_VALUE);
-    categoryCache = new LRUHashMap<Integer, FacetLabel>(DEFAULT_CACHE_VALUE);
+    ordinalCache = new LRUHashMap<>(DEFAULT_CACHE_VALUE);
+    categoryCache = new LRUHashMap<>(DEFAULT_CACHE_VALUE);
   }
   
   /**
@@ -121,8 +121,8 @@ public class DirectoryTaxonomyReader extends TaxonomyReader {
     
     // These are the default cache sizes; they can be configured after
     // construction with the cache's setMaxSize() method
-    ordinalCache = new LRUHashMap<FacetLabel, Integer>(DEFAULT_CACHE_VALUE);
-    categoryCache = new LRUHashMap<Integer, FacetLabel>(DEFAULT_CACHE_VALUE);
+    ordinalCache = new LRUHashMap<>(DEFAULT_CACHE_VALUE);
+    categoryCache = new LRUHashMap<>(DEFAULT_CACHE_VALUE);
   }
   
   private synchronized void initTaxoArrays() throws IOException {
diff --git lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
index 9d5b24d..55d8384 100644
--- lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
+++ lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
@@ -620,7 +620,7 @@ public class DirectoryTaxonomyWriter implements TaxonomyWriter {
 
   /** Combine original user data with the taxonomy epoch. */
   private Map<String,String> combinedCommitData(Map<String,String> commitData) {
-    Map<String,String> m = new HashMap<String, String>();
+    Map<String,String> m = new HashMap<>();
     if (commitData != null) {
       m.putAll(commitData);
     }
diff --git lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CharBlockArray.java lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CharBlockArray.java
index 5c7d46f..d182f8b 100644
--- lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CharBlockArray.java
+++ lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CharBlockArray.java
@@ -60,7 +60,7 @@ class CharBlockArray implements Appendable, Serializable, CharSequence {
   }
 
   CharBlockArray(int blockSize) {
-    this.blocks = new ArrayList<Block>();
+    this.blocks = new ArrayList<>();
     this.blockSize = blockSize;
     addBlock();
   }
diff --git lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/NameIntCacheLRU.java lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/NameIntCacheLRU.java
index 1626ada..b425496 100644
--- lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/NameIntCacheLRU.java
+++ lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/NameIntCacheLRU.java
@@ -54,9 +54,9 @@ class NameIntCacheLRU {
 
   private void createCache (int maxSize) {
     if (maxSize<Integer.MAX_VALUE) {
-      cache = new LinkedHashMap<Object, Integer>(1000,(float)0.7,true); //for LRU
+      cache = new LinkedHashMap<>(1000,(float)0.7,true); //for LRU
     } else {
-      cache = new HashMap<Object, Integer>(1000,(float)0.7); //no need for LRU
+      cache = new HashMap<>(1000,(float)0.7); //no need for LRU
     }
   }
 
diff --git lucene/facet/src/test/org/apache/lucene/facet/AssertingSubDocsAtOnceCollector.java lucene/facet/src/test/org/apache/lucene/facet/AssertingSubDocsAtOnceCollector.java
index 1caad40..644b3ad 100644
--- lucene/facet/src/test/org/apache/lucene/facet/AssertingSubDocsAtOnceCollector.java
+++ lucene/facet/src/test/org/apache/lucene/facet/AssertingSubDocsAtOnceCollector.java
@@ -36,7 +36,7 @@ class AssertingSubDocsAtOnceCollector extends Collector {
   @Override
   public void setScorer(Scorer s) {
     // Gathers all scorers, including s and "under":
-    allScorers = new ArrayList<Scorer>();
+    allScorers = new ArrayList<>();
     allScorers.add(s);
     int upto = 0;
     while(upto < allScorers.size()) {
diff --git lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java
index 80849a9..1622e3d 100644
--- lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java
+++ lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java
@@ -177,14 +177,14 @@ public abstract class FacetTestCase extends LuceneTestCase {
   protected void assertFloatValuesEquals(List<FacetResult> a, List<FacetResult> b) {
     assertEquals(a.size(), b.size());
     float lastValue = Float.POSITIVE_INFINITY;
-    Map<String,FacetResult> aByDim = new HashMap<String,FacetResult>();
+    Map<String,FacetResult> aByDim = new HashMap<>();
     for(int i=0;i<a.size();i++) {
       assertTrue(a.get(i).value.floatValue() <= lastValue);
       lastValue = a.get(i).value.floatValue();
       aByDim.put(a.get(i).dim, a.get(i));
     }
     lastValue = Float.POSITIVE_INFINITY;
-    Map<String,FacetResult> bByDim = new HashMap<String,FacetResult>();
+    Map<String,FacetResult> bByDim = new HashMap<>();
     for(int i=0;i<b.size();i++) {
       bByDim.put(b.get(i).dim, b.get(i));
       assertTrue(b.get(i).value.floatValue() <= lastValue);
diff --git lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java
index 1120a99..5850ee5 100644
--- lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java
+++ lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java
@@ -422,7 +422,7 @@ public class TestDrillSideways extends FacetTestCase {
     int valueCount = 2;
 
     for(int dim=0;dim<numDims;dim++) {
-      Set<String> values = new HashSet<String>();
+      Set<String> values = new HashSet<>();
       while (values.size() < valueCount) {
         String s = TestUtil.randomRealisticUnicodeString(random());
         //String s = _TestUtil.randomString(random());
@@ -434,7 +434,7 @@ public class TestDrillSideways extends FacetTestCase {
       valueCount *= 2;
     }
 
-    List<Doc> docs = new ArrayList<Doc>();
+    List<Doc> docs = new ArrayList<>();
     for(int i=0;i<numDocs;i++) {
       Doc doc = new Doc();
       doc.id = ""+i;
@@ -710,7 +710,7 @@ public class TestDrillSideways extends FacetTestCase {
         ds = new DrillSideways(s, config, tr) {
             @Override
             protected Facets buildFacetsResult(FacetsCollector drillDowns, FacetsCollector[] drillSideways, String[] drillSidewaysDims) throws IOException {
-              Map<String,Facets> drillSidewaysFacets = new HashMap<String,Facets>();
+              Map<String,Facets> drillSidewaysFacets = new HashMap<>();
               Facets drillDownFacets = getTaxonomyFacetCounts(taxoReader, config, drillDowns);
               if (drillSideways != null) {
                 for(int i=0;i<drillSideways.length;i++) {
@@ -733,7 +733,7 @@ public class TestDrillSideways extends FacetTestCase {
       DrillSidewaysResult actual = ds.search(ddq, filter, null, numDocs, sort, true, true);
 
       TopDocs hits = s.search(baseQuery, numDocs);
-      Map<String,Float> scores = new HashMap<String,Float>();
+      Map<String,Float> scores = new HashMap<>();
       for(ScoreDoc sd : hits.scoreDocs) {
         scores.put(s.doc(sd.doc).get("id"), sd.score);
       }
@@ -847,7 +847,7 @@ public class TestDrillSideways extends FacetTestCase {
                                                         String[][] dimValues, Filter onlyEven) throws Exception {
     int numDims = dimValues.length;
 
-    List<Doc> hits = new ArrayList<Doc>();
+    List<Doc> hits = new ArrayList<>();
     Counters drillDownCounts = new Counters(dimValues);
     Counters[] drillSidewaysCounts = new Counters[dimValues.length];
     for(int dim=0;dim<numDims;dim++) {
@@ -909,7 +909,7 @@ public class TestDrillSideways extends FacetTestCase {
       }
     }
 
-    Map<String,Integer> idToDocID = new HashMap<String,Integer>();
+    Map<String,Integer> idToDocID = new HashMap<>();
     for(int i=0;i<s.getIndexReader().maxDoc();i++) {
       idToDocID.put(s.doc(i).get("id"), i);
     }
@@ -964,7 +964,7 @@ public class TestDrillSideways extends FacetTestCase {
       }
 
       int idx = 0;
-      Map<String,Integer> actualValues = new HashMap<String,Integer>();
+      Map<String,Integer> actualValues = new HashMap<>();
 
       if (fr != null) {
         for(LabelAndValue labelValue : fr.labelValues) {
diff --git lucene/facet/src/test/org/apache/lucene/facet/TestMultipleIndexFields.java lucene/facet/src/test/org/apache/lucene/facet/TestMultipleIndexFields.java
index db25349..d9888e5 100644
--- lucene/facet/src/test/org/apache/lucene/facet/TestMultipleIndexFields.java
+++ lucene/facet/src/test/org/apache/lucene/facet/TestMultipleIndexFields.java
@@ -120,7 +120,7 @@ public class TestMultipleIndexFields extends FacetTestCase {
 
     FacetsCollector sfc = performSearch(tr, ir, searcher);
 
-    Map<String,Facets> facetsMap = new HashMap<String,Facets>();
+    Map<String,Facets> facetsMap = new HashMap<>();
     facetsMap.put("Author", getTaxonomyFacetCounts(tr, config, sfc, "$author"));
     Facets facets = new MultiFacets(facetsMap, getTaxonomyFacetCounts(tr, config, sfc));
 
@@ -160,7 +160,7 @@ public class TestMultipleIndexFields extends FacetTestCase {
 
     FacetsCollector sfc = performSearch(tr, ir, searcher);
 
-    Map<String,Facets> facetsMap = new HashMap<String,Facets>();
+    Map<String,Facets> facetsMap = new HashMap<>();
     Facets facets2 = getTaxonomyFacetCounts(tr, config, sfc, "$music");
     facetsMap.put("Band", facets2);
     facetsMap.put("Composer", facets2);
@@ -213,7 +213,7 @@ public class TestMultipleIndexFields extends FacetTestCase {
 
     FacetsCollector sfc = performSearch(tr, ir, searcher);
 
-    Map<String,Facets> facetsMap = new HashMap<String,Facets>();
+    Map<String,Facets> facetsMap = new HashMap<>();
     facetsMap.put("Band", getTaxonomyFacetCounts(tr, config, sfc, "$bands"));
     facetsMap.put("Composer", getTaxonomyFacetCounts(tr, config, sfc, "$composers"));
     Facets facets = new MultiFacets(facetsMap, getTaxonomyFacetCounts(tr, config, sfc));
@@ -255,7 +255,7 @@ public class TestMultipleIndexFields extends FacetTestCase {
 
     FacetsCollector sfc = performSearch(tr, ir, searcher);
 
-    Map<String,Facets> facetsMap = new HashMap<String,Facets>();
+    Map<String,Facets> facetsMap = new HashMap<>();
     Facets facets2 = getTaxonomyFacetCounts(tr, config, sfc, "$music");
     facetsMap.put("Band", facets2);
     facetsMap.put("Composer", facets2);
diff --git lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeFacetCounts.java lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeFacetCounts.java
index 0dc324b..238af98 100644
--- lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeFacetCounts.java
+++ lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeFacetCounts.java
@@ -260,7 +260,7 @@ public class TestRangeFacetCounts extends FacetTestCase {
             }
           }
 
-          Map<String,Facets> byDim = new HashMap<String,Facets>();
+          Map<String,Facets> byDim = new HashMap<>();
           byDim.put("field",
                     new LongRangeFacetCounts("field", fieldFC,
                           new LongRange("less than 10", 0L, true, 10L, false),
diff --git lucene/facet/src/test/org/apache/lucene/facet/sortedset/TestSortedSetDocValuesFacets.java lucene/facet/src/test/org/apache/lucene/facet/sortedset/TestSortedSetDocValuesFacets.java
index 4210ba7..6d478cd 100644
--- lucene/facet/src/test/org/apache/lucene/facet/sortedset/TestSortedSetDocValuesFacets.java
+++ lucene/facet/src/test/org/apache/lucene/facet/sortedset/TestSortedSetDocValuesFacets.java
@@ -310,7 +310,7 @@ public class TestSortedSetDocValuesFacets extends FacetTestCase {
       // Slow, yet hopefully bug-free, faceting:
       @SuppressWarnings({"rawtypes","unchecked"}) Map<String,Integer>[] expectedCounts = new HashMap[numDims];
       for(int i=0;i<numDims;i++) {
-        expectedCounts[i] = new HashMap<String,Integer>();
+        expectedCounts[i] = new HashMap<>();
       }
 
       for(TestDoc doc : testDocs) {
@@ -328,9 +328,9 @@ public class TestSortedSetDocValuesFacets extends FacetTestCase {
         }
       }
 
-      List<FacetResult> expected = new ArrayList<FacetResult>();
+      List<FacetResult> expected = new ArrayList<>();
       for(int i=0;i<numDims;i++) {
-        List<LabelAndValue> labelValues = new ArrayList<LabelAndValue>();
+        List<LabelAndValue> labelValues = new ArrayList<>();
         int totCount = 0;
         for(Map.Entry<String,Integer> ent : expectedCounts[i].entrySet()) {
           labelValues.add(new LabelAndValue(ent.getKey(), ent.getValue()));
diff --git lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestLRUHashMap.java lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestLRUHashMap.java
index 2607a1e..f86a7a7 100644
--- lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestLRUHashMap.java
+++ lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestLRUHashMap.java
@@ -27,7 +27,7 @@ public class TestLRUHashMap extends FacetTestCase {
   // recently used
   @Test
   public void testLRU() throws Exception {
-    LRUHashMap<String, String> lru = new LRUHashMap<String, String>(3);
+    LRUHashMap<String, String> lru = new LRUHashMap<>(3);
     assertEquals(0, lru.size());
     lru.put("one", "Hello world");
     assertEquals(1, lru.size());
diff --git lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestSearcherTaxonomyManager.java lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestSearcherTaxonomyManager.java
index 9482176..daf9672 100644
--- lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestSearcherTaxonomyManager.java
+++ lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestSearcherTaxonomyManager.java
@@ -67,8 +67,8 @@ public class TestSearcherTaxonomyManager extends FacetTestCase {
     @Override
     public void run() {
       try {
-        Set<String> seen = new HashSet<String>();
-        List<String> paths = new ArrayList<String>();
+        Set<String> seen = new HashSet<>();
+        List<String> paths = new ArrayList<>();
         while (true) {
           Document doc = new Document();
           int numPaths = TestUtil.nextInt(random(), 1, 5);
diff --git lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java
index a230d4f..60aa0fe 100644
--- lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java
+++ lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java
@@ -534,7 +534,7 @@ public class TestTaxonomyCombined extends FacetTestCase {
     for (int i=0; i<expectedCategories.length; i++) {
       // find expected children by looking at all expectedCategories
       // for children
-      ArrayList<Integer> expectedChildren = new ArrayList<Integer>();
+      ArrayList<Integer> expectedChildren = new ArrayList<>();
       for (int j=expectedCategories.length-1; j>=0; j--) {
         if (expectedCategories[j].length != expectedCategories[i].length+1) {
           continue; // not longer by 1, so can't be a child
diff --git lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetCounts.java lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetCounts.java
index ec4abc3..a63a27a 100644
--- lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetCounts.java
+++ lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetCounts.java
@@ -450,7 +450,7 @@ public class TestTaxonomyFacetCounts extends FacetTestCase {
 
     FacetResult result = facets.getTopChildren(Integer.MAX_VALUE, "dim");
     assertEquals(numLabels, result.labelValues.length);
-    Set<String> allLabels = new HashSet<String>();
+    Set<String> allLabels = new HashSet<>();
     for (LabelAndValue labelValue : result.labelValues) {
       allLabels.add(labelValue.label);
       assertEquals(1, labelValue.value.intValue());
@@ -710,7 +710,7 @@ public class TestTaxonomyFacetCounts extends FacetTestCase {
       // Slow, yet hopefully bug-free, faceting:
       @SuppressWarnings({"rawtypes","unchecked"}) Map<String,Integer>[] expectedCounts = new HashMap[numDims];
       for(int i=0;i<numDims;i++) {
-        expectedCounts[i] = new HashMap<String,Integer>();
+        expectedCounts[i] = new HashMap<>();
       }
 
       for(TestDoc doc : testDocs) {
@@ -728,9 +728,9 @@ public class TestTaxonomyFacetCounts extends FacetTestCase {
         }
       }
 
-      List<FacetResult> expected = new ArrayList<FacetResult>();
+      List<FacetResult> expected = new ArrayList<>();
       for(int i=0;i<numDims;i++) {
-        List<LabelAndValue> labelValues = new ArrayList<LabelAndValue>();
+        List<LabelAndValue> labelValues = new ArrayList<>();
         int totCount = 0;
         for(Map.Entry<String,Integer> ent : expectedCounts[i].entrySet()) {
           labelValues.add(new LabelAndValue(ent.getKey(), ent.getValue()));
diff --git lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetCounts2.java lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetCounts2.java
index f60dfad..d7f5940 100644
--- lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetCounts2.java
+++ lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetCounts2.java
@@ -101,14 +101,14 @@ public class TestTaxonomyFacetCounts2 extends FacetTestCase {
     // category is not added twice.
     int numFacetsA = random.nextInt(3) + 1; // 1-3
     int numFacetsB = random.nextInt(2) + 1; // 1-2
-    ArrayList<FacetField> categories_a = new ArrayList<FacetField>();
+    ArrayList<FacetField> categories_a = new ArrayList<>();
     categories_a.addAll(Arrays.asList(CATEGORIES_A));
-    ArrayList<FacetField> categories_b = new ArrayList<FacetField>();
+    ArrayList<FacetField> categories_b = new ArrayList<>();
     categories_b.addAll(Arrays.asList(CATEGORIES_B));
     Collections.shuffle(categories_a, random);
     Collections.shuffle(categories_b, random);
 
-    ArrayList<FacetField> categories = new ArrayList<FacetField>();
+    ArrayList<FacetField> categories = new ArrayList<>();
     categories.addAll(categories_a.subList(0, numFacetsA));
     categories.addAll(categories_b.subList(0, numFacetsB));
     
@@ -210,7 +210,7 @@ public class TestTaxonomyFacetCounts2 extends FacetTestCase {
   
   // initialize expectedCounts w/ 0 for all categories
   private static Map<String,Integer> newCounts() {
-    Map<String,Integer> counts = new HashMap<String,Integer>();
+    Map<String,Integer> counts = new HashMap<>();
     counts.put(CP_A, 0);
     counts.put(CP_B, 0);
     counts.put(CP_C, 0);
diff --git lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetSumValueSource.java lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetSumValueSource.java
index 6735905..316e5f6 100644
--- lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetSumValueSource.java
+++ lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetSumValueSource.java
@@ -464,7 +464,7 @@ public class TestTaxonomyFacetSumValueSource extends FacetTestCase {
       // Slow, yet hopefully bug-free, faceting:
       @SuppressWarnings({"rawtypes","unchecked"}) Map<String,Float>[] expectedValues = new HashMap[numDims];
       for(int i=0;i<numDims;i++) {
-        expectedValues[i] = new HashMap<String,Float>();
+        expectedValues[i] = new HashMap<>();
       }
 
       for(TestDoc doc : testDocs) {
@@ -482,9 +482,9 @@ public class TestTaxonomyFacetSumValueSource extends FacetTestCase {
         }
       }
 
-      List<FacetResult> expected = new ArrayList<FacetResult>();
+      List<FacetResult> expected = new ArrayList<>();
       for(int i=0;i<numDims;i++) {
-        List<LabelAndValue> labelValues = new ArrayList<LabelAndValue>();
+        List<LabelAndValue> labelValues = new ArrayList<>();
         double totValue = 0;
         for(Map.Entry<String,Float> ent : expectedValues[i].entrySet()) {
           labelValues.add(new LabelAndValue(ent.getKey(), ent.getValue()));
diff --git lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestAddTaxonomy.java lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestAddTaxonomy.java
index f61e273..fdf492b 100644
--- lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestAddTaxonomy.java
+++ lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestAddTaxonomy.java
@@ -229,7 +229,7 @@ public class TestAddTaxonomy extends FacetTestCase {
     DirectoryTaxonomyReader dtr = new DirectoryTaxonomyReader(dest);
     // +2 to account for the root category + "a"
     assertEquals(numCategories + 2, dtr.getSize());
-    HashSet<FacetLabel> categories = new HashSet<FacetLabel>();
+    HashSet<FacetLabel> categories = new HashSet<>();
     for (int i = 1; i < dtr.getSize(); i++) {
       FacetLabel cat = dtr.getPath(i);
       assertTrue("category " + cat + " already existed", categories.add(cat));
diff --git lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestConcurrentFacetedIndexing.java lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestConcurrentFacetedIndexing.java
index d28e7b7..30168b4 100644
--- lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestConcurrentFacetedIndexing.java
+++ lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestConcurrentFacetedIndexing.java
@@ -82,7 +82,7 @@ public class TestConcurrentFacetedIndexing extends FacetTestCase {
     final AtomicInteger numDocs = new AtomicInteger(atLeast(10000));
     final Directory indexDir = newDirectory();
     final Directory taxoDir = newDirectory();
-    final ConcurrentHashMap<String,String> values = new ConcurrentHashMap<String,String>();
+    final ConcurrentHashMap<String,String> values = new ConcurrentHashMap<>();
     final IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
     final DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE, newTaxoWriterCache(numDocs.get()));
     final Thread[] indexThreads = new Thread[atLeast(4)];
diff --git lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyReader.java lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyReader.java
index fe5b09c..623af36 100644
--- lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyReader.java
+++ lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyReader.java
@@ -504,7 +504,7 @@ public class TestDirectoryTaxonomyReader extends FacetTestCase {
     assertEquals(TaxonomyReader.INVALID_ORDINAL, it.next());
 
     // root's children
-    Set<String> roots = new HashSet<String>(Arrays.asList("a", "b", "c"));
+    Set<String> roots = new HashSet<>(Arrays.asList("a", "b", "c"));
     it = taxoReader.getChildren(TaxonomyReader.ROOT_ORDINAL);
     while (!roots.isEmpty()) {
       FacetLabel root = taxoReader.getPath(it.next());
diff --git lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java
index a676b28..c79292a 100644
--- lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java
+++ lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java
@@ -93,7 +93,7 @@ public class TestDirectoryTaxonomyWriter extends FacetTestCase {
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(dir, OpenMode.CREATE_OR_APPEND, NO_OP_CACHE);
     taxoWriter.addCategory(new FacetLabel("a"));
     taxoWriter.addCategory(new FacetLabel("b"));
-    Map<String, String> userCommitData = new HashMap<String, String>();
+    Map<String, String> userCommitData = new HashMap<>();
     userCommitData.put("testing", "1 2 3");
     taxoWriter.setCommitData(userCommitData);
     taxoWriter.close();
@@ -243,7 +243,7 @@ public class TestDirectoryTaxonomyWriter extends FacetTestCase {
     final int range = ncats * 3; // affects the categories selection
     final AtomicInteger numCats = new AtomicInteger(ncats);
     final Directory dir = newDirectory();
-    final ConcurrentHashMap<String,String> values = new ConcurrentHashMap<String,String>();
+    final ConcurrentHashMap<String,String> values = new ConcurrentHashMap<>();
     final double d = random().nextDouble();
     final TaxonomyWriterCache cache;
     if (d < 0.7) {
diff --git lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/TestCompactLabelToOrdinal.java lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/TestCompactLabelToOrdinal.java
index 715a80e..eddcb1b 100644
--- lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/TestCompactLabelToOrdinal.java
+++ lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/TestCompactLabelToOrdinal.java
@@ -118,7 +118,7 @@ public class TestCompactLabelToOrdinal extends FacetTestCase {
   }
 
   private static class LabelToOrdinalMap extends LabelToOrdinal {
-    private Map<FacetLabel, Integer> map = new HashMap<FacetLabel, Integer>();
+    private Map<FacetLabel, Integer> map = new HashMap<>();
 
     LabelToOrdinalMap() { }
     
diff --git lucene/grouping/src/java/org/apache/lucene/search/grouping/SearchGroup.java lucene/grouping/src/java/org/apache/lucene/search/grouping/SearchGroup.java
index 8aab580..032abbb 100644
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/SearchGroup.java
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/SearchGroup.java
@@ -101,7 +101,7 @@ public class SearchGroup<GROUP_VALUE_TYPE> {
     public final T groupValue;
 
     public Object[] topValues;
-    public final List<ShardIter<T>> shards = new ArrayList<ShardIter<T>>();
+    public final List<ShardIter<T>> shards = new ArrayList<>();
     public int minShardIndex;
     public boolean processed;
     public boolean inQueue;
@@ -214,7 +214,7 @@ public class SearchGroup<GROUP_VALUE_TYPE> {
         if (isNew) {
           // Start a new group:
           //System.out.println("      new");
-          mergedGroup = new MergedGroup<T>(group.groupValue);
+          mergedGroup = new MergedGroup<>(group.groupValue);
           mergedGroup.minShardIndex = shard.shardIndex;
           assert group.sortValues != null;
           mergedGroup.topValues = group.sortValues;
@@ -281,12 +281,12 @@ public class SearchGroup<GROUP_VALUE_TYPE> {
         final Collection<SearchGroup<T>> shard = shards.get(shardIDX);
         if (!shard.isEmpty()) {
           //System.out.println("  insert shard=" + shardIDX);
-          updateNextGroup(maxQueueSize, new ShardIter<T>(shard, shardIDX));
+          updateNextGroup(maxQueueSize, new ShardIter<>(shard, shardIDX));
         }
       }
 
       // Pull merged topN groups:
-      final List<SearchGroup<T>> newTopGroups = new ArrayList<SearchGroup<T>>();
+      final List<SearchGroup<T>> newTopGroups = new ArrayList<>();
 
       int count = 0;
 
@@ -295,7 +295,7 @@ public class SearchGroup<GROUP_VALUE_TYPE> {
         group.processed = true;
         //System.out.println("  pop: shards=" + group.shards + " group=" + (group.groupValue == null ? "null" : (((BytesRef) group.groupValue).utf8ToString())) + " sortValues=" + Arrays.toString(group.topValues));
         if (count++ >= offset) {
-          final SearchGroup<T> newGroup = new SearchGroup<T>();
+          final SearchGroup<T> newGroup = new SearchGroup<>();
           newGroup.groupValue = group.groupValue;
           newGroup.sortValues = group.topValues;
           newTopGroups.add(newGroup);
diff --git lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java
index 72b8246..c778162 100644
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java
@@ -45,7 +45,7 @@ public class FunctionAllGroupsCollector extends AbstractAllGroupsCollector<Mutab
 
   private final Map<?, ?> vsContext;
   private final ValueSource groupBy;
-  private final SortedSet<MutableValue> groups = new TreeSet<MutableValue>();
+  private final SortedSet<MutableValue> groups = new TreeSet<>();
 
   private FunctionValues.ValueFiller filler;
   private MutableValue mval;
diff --git lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter.java lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter.java
index e59d430..758e35f 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter.java
@@ -156,7 +156,7 @@ public class Highlighter
     TextFragment[] frag =getBestTextFragments(tokenStream,text, true,maxNumFragments);
 
     //Get text
-    ArrayList<String> fragTexts = new ArrayList<String>();
+    ArrayList<String> fragTexts = new ArrayList<>();
     for (int i = 0; i < frag.length; i++)
     {
       if ((frag[i] != null) && (frag[i].getScore() > 0))
@@ -182,7 +182,7 @@ public class Highlighter
     int maxNumFragments)
     throws IOException, InvalidTokenOffsetsException
   {
-    ArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();
+    ArrayList<TextFragment> docFrags = new ArrayList<>();
     StringBuilder newText=new StringBuilder();
 
     CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);
@@ -327,7 +327,7 @@ public class Highlighter
       if(mergeContiguousFragments)
       {
         mergeContiguousFragments(frag);
-        ArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();
+        ArrayList<TextFragment> fragTexts = new ArrayList<>();
         for (int i = 0; i < frag.length; i++)
         {
           if ((frag[i] != null) && (frag[i].getScore() > 0))
diff --git lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryScorer.java lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryScorer.java
index 09806fc..38dd0e5 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryScorer.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryScorer.java
@@ -102,7 +102,7 @@ public class QueryScorer implements Scorer {
    * @param weightedTerms an array of pre-created {@link WeightedSpanTerm}s
    */
   public QueryScorer(WeightedSpanTerm[] weightedTerms) {
-    this.fieldWeightedSpanTerms = new HashMap<String,WeightedSpanTerm>(weightedTerms.length);
+    this.fieldWeightedSpanTerms = new HashMap<>(weightedTerms.length);
 
     for (int i = 0; i < weightedTerms.length; i++) {
       WeightedSpanTerm existingTerm = fieldWeightedSpanTerms.get(weightedTerms[i].term);
@@ -239,7 +239,7 @@ public class QueryScorer implements Scorer {
    */
   @Override
   public void startFragment(TextFragment newFragment) {
-    foundTerms = new HashSet<String>();
+    foundTerms = new HashSet<>();
     totalScore = 0;
   }
   
diff --git lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryTermExtractor.java lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryTermExtractor.java
index 2fe6aa5..59ceb9c 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryTermExtractor.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryTermExtractor.java
@@ -88,7 +88,7 @@ public final class QueryTermExtractor
    */
   public static final WeightedTerm[] getTerms(Query query, boolean prohibited, String fieldName)
   {
-    HashSet<WeightedTerm> terms=new HashSet<WeightedTerm>();
+    HashSet<WeightedTerm> terms=new HashSet<>();
     getTerms(query,terms,prohibited,fieldName);
     return terms.toArray(new WeightedTerm[0]);
   }
@@ -112,7 +112,7 @@ public final class QueryTermExtractor
       else if (query instanceof FilteredQuery)
         getTermsFromFilteredQuery((FilteredQuery) query, terms, prohibited, fieldName);
       else {
-        HashSet<Term> nonWeightedTerms = new HashSet<Term>();
+        HashSet<Term> nonWeightedTerms = new HashSet<>();
         query.extractTerms(nonWeightedTerms);
         for (Iterator<Term> iter = nonWeightedTerms.iterator(); iter.hasNext(); ) {
           Term term = iter.next();
diff --git lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryTermScorer.java lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryTermScorer.java
index 17ba516..69f93ec 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryTermScorer.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryTermScorer.java
@@ -77,7 +77,7 @@ public class QueryTermScorer implements Scorer {
   }
 
   public QueryTermScorer(WeightedTerm[] weightedTerms) {
-    termsToFind = new HashMap<String,WeightedTerm>();
+    termsToFind = new HashMap<>();
     for (int i = 0; i < weightedTerms.length; i++) {
       WeightedTerm existingTerm = termsToFind
           .get(weightedTerms[i].term);
@@ -109,7 +109,7 @@ public class QueryTermScorer implements Scorer {
    */
   @Override
   public void startFragment(TextFragment newFragment) {
-    uniqueTermsInFragment = new HashSet<String>();
+    uniqueTermsInFragment = new HashSet<>();
     currentTextFragment = newFragment;
     totalScore = 0;
 
diff --git lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java
index 14a8f7f..68433a9 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java
@@ -257,7 +257,7 @@ public class TokenSources {
           // tokens NOT stored with positions or not guaranteed contiguous - must
           // add to list and sort later
           if (unsortedTokens == null) {
-            unsortedTokens = new ArrayList<Token>();
+            unsortedTokens = new ArrayList<>();
           }
           unsortedTokens.add(token);
         }
diff --git lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector.java lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector.java
index 42db712..bd87206 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector.java
@@ -39,7 +39,7 @@ import org.apache.lucene.util.CollectionUtil;
  */
 public final class TokenStreamFromTermPositionVector extends TokenStream {
 
-  private final List<Token> positionedTokens = new ArrayList<Token>();
+  private final List<Token> positionedTokens = new ArrayList<>();
 
   private Iterator<Token> tokensAtCurrentPosition;
 
diff --git lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTerm.java lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTerm.java
index ff39546..6a06683 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTerm.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTerm.java
@@ -28,11 +28,11 @@ import java.util.List;
  */
 public class WeightedSpanTerm extends WeightedTerm{
   boolean positionSensitive;
-  private List<PositionSpan> positionSpans = new ArrayList<PositionSpan>();
+  private List<PositionSpan> positionSpans = new ArrayList<>();
 
   public WeightedSpanTerm(float weight, String term) {
     super(weight, term);
-    this.positionSpans = new ArrayList<PositionSpan>();
+    this.positionSpans = new ArrayList<>();
   }
 
   public WeightedSpanTerm(float weight, String term, boolean positionSensitive) {
diff --git lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java
index a2b1bd4..f38f74d 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java
@@ -175,7 +175,7 @@ public class WeightedSpanTermExtractor {
           final Term[] termArray = termArrays.get(i);
           List<SpanQuery> disjuncts = disjunctLists[positions[i]];
           if (disjuncts == null) {
-            disjuncts = (disjunctLists[positions[i]] = new ArrayList<SpanQuery>(termArray.length));
+            disjuncts = (disjunctLists[positions[i]] = new ArrayList<>(termArray.length));
             ++distinctPositions;
           }
           for (int j = 0; j < termArray.length; ++j) {
@@ -243,10 +243,10 @@ public class WeightedSpanTermExtractor {
     Set<String> fieldNames;
 
     if (fieldName == null) {
-      fieldNames = new HashSet<String>();
+      fieldNames = new HashSet<>();
       collectSpanQueryFields(spanQuery, fieldNames);
     } else {
-      fieldNames = new HashSet<String>(1);
+      fieldNames = new HashSet<>(1);
       fieldNames.add(fieldName);
     }
     // To support the use of the default field name
@@ -254,9 +254,9 @@ public class WeightedSpanTermExtractor {
       fieldNames.add(defaultField);
     }
     
-    Map<String, SpanQuery> queries = new HashMap<String, SpanQuery>();
+    Map<String, SpanQuery> queries = new HashMap<>();
  
-    Set<Term> nonWeightedTerms = new HashSet<Term>();
+    Set<Term> nonWeightedTerms = new HashSet<>();
     final boolean mustRewriteQuery = mustRewriteQuery(spanQuery);
     if (mustRewriteQuery) {
       for (final String field : fieldNames) {
@@ -268,7 +268,7 @@ public class WeightedSpanTermExtractor {
       spanQuery.extractTerms(nonWeightedTerms);
     }
 
-    List<PositionSpan> spanPositions = new ArrayList<PositionSpan>();
+    List<PositionSpan> spanPositions = new ArrayList<>();
 
     for (final String field : fieldNames) {
       final SpanQuery q;
@@ -278,8 +278,8 @@ public class WeightedSpanTermExtractor {
         q = spanQuery;
       }
       AtomicReaderContext context = getLeafContext();
-      Map<Term,TermContext> termContexts = new HashMap<Term,TermContext>();
-      TreeSet<Term> extractedTerms = new TreeSet<Term>();
+      Map<Term,TermContext> termContexts = new HashMap<>();
+      TreeSet<Term> extractedTerms = new TreeSet<>();
       q.extractTerms(extractedTerms);
       for (Term term : extractedTerms) {
         termContexts.put(term, TermContext.build(context, term));
@@ -328,7 +328,7 @@ public class WeightedSpanTermExtractor {
    * @throws IOException If there is a low-level I/O error
    */
   protected void extractWeightedTerms(Map<String,WeightedSpanTerm> terms, Query query) throws IOException {
-    Set<Term> nonWeightedTerms = new HashSet<Term>();
+    Set<Term> nonWeightedTerms = new HashSet<>();
     query.extractTerms(nonWeightedTerms);
 
     for (final Term queryTerm : nonWeightedTerms) {
@@ -468,7 +468,7 @@ public class WeightedSpanTermExtractor {
       this.fieldName = null;
     }
 
-    Map<String,WeightedSpanTerm> terms = new PositionCheckingMap<String>();
+    Map<String,WeightedSpanTerm> terms = new PositionCheckingMap<>();
     this.tokenStream = tokenStream;
     try {
       extract(query, terms);
@@ -505,7 +505,7 @@ public class WeightedSpanTermExtractor {
     }
     this.tokenStream = tokenStream;
 
-    Map<String,WeightedSpanTerm> terms = new PositionCheckingMap<String>();
+    Map<String,WeightedSpanTerm> terms = new PositionCheckingMap<>();
     extract(query, terms);
 
     int totalNumDocs = reader.maxDoc();
diff --git lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter.java lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter.java
index ccfe124..e55ae36 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter.java
@@ -299,7 +299,7 @@ public class PostingsHighlighter {
    *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}
    */
   public Map<String,String[]> highlightFields(String fieldsIn[], Query query, IndexSearcher searcher, int[] docidsIn, int maxPassagesIn[]) throws IOException {
-    Map<String,String[]> snippets = new HashMap<String,String[]>();
+    Map<String,String[]> snippets = new HashMap<>();
     for(Map.Entry<String,Object[]> ent : highlightFieldsAsObjects(fieldsIn, query, searcher, docidsIn, maxPassagesIn).entrySet()) {
       Object[] snippetObjects = ent.getValue();
       String[] snippetStrings = new String[snippetObjects.length];
@@ -346,7 +346,7 @@ public class PostingsHighlighter {
     }
     final IndexReader reader = searcher.getIndexReader();
     Query rewritten = rewrite(query);
-    SortedSet<Term> queryTerms = new TreeSet<Term>();
+    SortedSet<Term> queryTerms = new TreeSet<>();
     rewritten.extractTerms(queryTerms);
 
     IndexReaderContext readerContext = reader.getContext();
@@ -384,7 +384,7 @@ public class PostingsHighlighter {
     // pull stored data:
     String[][] contents = loadFieldValues(searcher, fields, docids, maxLength);
     
-    Map<String,Object[]> highlights = new HashMap<String,Object[]>();
+    Map<String,Object[]> highlights = new HashMap<>();
     for (int i = 0; i < fields.length; i++) {
       String field = fields[i];
       int numPassages = maxPassages[i];
@@ -454,7 +454,7 @@ public class PostingsHighlighter {
   }
     
   private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<AtomicReaderContext> leaves, int maxPassages, Query query) throws IOException {  
-    Map<Integer,Object> highlights = new HashMap<Integer,Object>();
+    Map<Integer,Object> highlights = new HashMap<>();
     
     PassageFormatter fieldFormatter = getFormatter(field);
     if (fieldFormatter == null) {
@@ -539,7 +539,7 @@ public class PostingsHighlighter {
     if (scorer == null) {
       throw new NullPointerException("PassageScorer cannot be null");
     }
-    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();
+    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<>();
     float weights[] = new float[terms.length];
     // initialize postings
     for (int i = 0; i < terms.length; i++) {
@@ -574,7 +574,7 @@ public class PostingsHighlighter {
     
     pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination
     
-    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {
+    PriorityQueue<Passage> passageQueue = new PriorityQueue<>(n, new Comparator<Passage>() {
       @Override
       public int compare(Passage left, Passage right) {
         if (left.score < right.score) {
@@ -678,7 +678,7 @@ public class PostingsHighlighter {
    *  to customize. */
   protected Passage[] getEmptyHighlight(String fieldName, BreakIterator bi, int maxPassages) {
     // BreakIterator should be un-next'd:
-    List<Passage> passages = new ArrayList<Passage>();
+    List<Passage> passages = new ArrayList<>();
     int pos = bi.current();
     assert pos == 0;
     while (passages.size() < maxPassages) {
diff --git lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragListBuilder.java lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragListBuilder.java
index 6699f6e..058b578 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragListBuilder.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragListBuilder.java
@@ -50,8 +50,8 @@ public abstract class BaseFragListBuilder implements FragListBuilder {
     if( fragCharSize < minFragCharSize )
       throw new IllegalArgumentException( "fragCharSize(" + fragCharSize + ") is too small. It must be " + minFragCharSize + " or higher." );
     
-    List<WeightedPhraseInfo> wpil = new ArrayList<WeightedPhraseInfo>();
-    IteratorQueue<WeightedPhraseInfo> queue = new IteratorQueue<WeightedPhraseInfo>(fieldPhraseList.getPhraseList().iterator());
+    List<WeightedPhraseInfo> wpil = new ArrayList<>();
+    IteratorQueue<WeightedPhraseInfo> queue = new IteratorQueue<>(fieldPhraseList.getPhraseList().iterator());
     WeightedPhraseInfo phraseInfo = null;
     int startOffset = 0;
     while((phraseInfo = queue.top()) != null){
diff --git lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragmentsBuilder.java lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragmentsBuilder.java
index 8ee06dc..f0ec6fd 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragmentsBuilder.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragmentsBuilder.java
@@ -135,7 +135,7 @@ public abstract class BaseFragmentsBuilder implements FragmentsBuilder {
 
     fragInfos = getWeightedFragInfoList(fragInfos);
     int limitFragments = maxNumFragments < fragInfos.size() ? maxNumFragments : fragInfos.size();
-    List<String> fragments = new ArrayList<String>( limitFragments );
+    List<String> fragments = new ArrayList<>( limitFragments );
 
     StringBuilder buffer = new StringBuilder();
     int[] nextValueIndex = { 0 };
@@ -148,7 +148,7 @@ public abstract class BaseFragmentsBuilder implements FragmentsBuilder {
   
   protected Field[] getFields( IndexReader reader, int docId, final String fieldName) throws IOException {
     // according to javadoc, doc.getFields(fieldName) cannot be used with lazy loaded field???
-    final List<Field> fields = new ArrayList<Field>();
+    final List<Field> fields = new ArrayList<>();
     reader.document(docId, new StoredFieldVisitor() {
         
         @Override
@@ -215,7 +215,7 @@ public abstract class BaseFragmentsBuilder implements FragmentsBuilder {
   }
 
   protected List<WeightedFragInfo> discreteMultiValueHighlighting(List<WeightedFragInfo> fragInfos, Field[] fields) {
-    Map<String, List<WeightedFragInfo>> fieldNameToFragInfos = new HashMap<String, List<WeightedFragInfo>>();
+    Map<String, List<WeightedFragInfo>> fieldNameToFragInfos = new HashMap<>();
     for (Field field : fields) {
       fieldNameToFragInfos.put(field.name(), new ArrayList<WeightedFragInfo>());
     }
@@ -257,12 +257,12 @@ public abstract class BaseFragmentsBuilder implements FragmentsBuilder {
         }
 
 
-        List<SubInfo> subInfos = new ArrayList<SubInfo>();
+        List<SubInfo> subInfos = new ArrayList<>();
         Iterator<SubInfo> subInfoIterator = fragInfo.getSubInfos().iterator();
         float boost = 0.0f;  //  The boost of the new info will be the sum of the boosts of its SubInfos
         while (subInfoIterator.hasNext()) {
           SubInfo subInfo = subInfoIterator.next();
-          List<Toffs> toffsList = new ArrayList<Toffs>();
+          List<Toffs> toffsList = new ArrayList<>();
           Iterator<Toffs> toffsIterator = subInfo.getTermsOffsets().iterator();
           while (toffsIterator.hasNext()) {
             Toffs toffs = toffsIterator.next();
@@ -286,7 +286,7 @@ public abstract class BaseFragmentsBuilder implements FragmentsBuilder {
       }
     }
 
-    List<WeightedFragInfo> result = new ArrayList<WeightedFragInfo>();
+    List<WeightedFragInfo> result = new ArrayList<>();
     for (List<WeightedFragInfo> weightedFragInfos : fieldNameToFragInfos.values()) {
       result.addAll(weightedFragInfos);
     }
diff --git lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldFragList.java lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldFragList.java
index 81afd4e..119ff43 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldFragList.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldFragList.java
@@ -29,7 +29,7 @@ import java.util.List;
  */
 public abstract class FieldFragList {
 
-  private List<WeightedFragInfo> fragInfos = new ArrayList<WeightedFragInfo>();
+  private List<WeightedFragInfo> fragInfos = new ArrayList<>();
 
   /**
    * a constructor.
diff --git lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldPhraseList.java lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldPhraseList.java
index d46b4d2..1696702 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldPhraseList.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldPhraseList.java
@@ -34,7 +34,7 @@ public class FieldPhraseList {
   /**
    * List of non-overlapping WeightedPhraseInfo objects.
    */
-  LinkedList<WeightedPhraseInfo> phraseList = new LinkedList<WeightedPhraseInfo>();
+  LinkedList<WeightedPhraseInfo> phraseList = new LinkedList<>();
   
   /**
    * create a FieldPhraseList that has no limit on the number of phrases to analyze
@@ -65,7 +65,7 @@ public class FieldPhraseList {
   public FieldPhraseList( FieldTermStack fieldTermStack, FieldQuery fieldQuery, int phraseLimit ){
     final String field = fieldTermStack.getFieldName();
 
-    LinkedList<TermInfo> phraseCandidate = new LinkedList<TermInfo>();
+    LinkedList<TermInfo> phraseCandidate = new LinkedList<>();
     QueryPhraseMap currMap = null;
     QueryPhraseMap nextMap = null;
     while( !fieldTermStack.isEmpty() && (phraseList.size() < phraseLimit) )
@@ -125,13 +125,13 @@ public class FieldPhraseList {
     for ( FieldPhraseList fplToMerge : toMerge ) {
       allInfos[ index++ ] = fplToMerge.phraseList.iterator();
     }
-    MergedIterator< WeightedPhraseInfo > itr = new MergedIterator< WeightedPhraseInfo >( false, allInfos );
+    MergedIterator< WeightedPhraseInfo > itr = new MergedIterator<>( false, allInfos );
     // Step 2.  Walk the sorted list merging infos that overlap
-    phraseList = new LinkedList< WeightedPhraseInfo >();
+    phraseList = new LinkedList<>();
     if ( !itr.hasNext() ) {
       return;
     }
-    List< WeightedPhraseInfo > work = new ArrayList< WeightedPhraseInfo >();
+    List< WeightedPhraseInfo > work = new ArrayList<>();
     WeightedPhraseInfo first = itr.next();
     work.add( first );
     int workEndOffset = first.getEndOffset();
@@ -225,9 +225,9 @@ public class FieldPhraseList {
       this.seqnum = seqnum;
       
       // We keep TermInfos for further operations
-      termsInfos = new ArrayList<TermInfo>( terms );
+      termsInfos = new ArrayList<>( terms );
       
-      termsOffsets = new ArrayList<Toffs>( terms.size() );
+      termsOffsets = new ArrayList<>( terms.size() );
       TermInfo ti = terms.get( 0 );
       termsOffsets.add( new Toffs( ti.getStartOffset(), ti.getEndOffset() ) );
       if( terms.size() == 1 ){
@@ -261,7 +261,7 @@ public class FieldPhraseList {
       WeightedPhraseInfo first = toMergeItr.next();
       @SuppressWarnings( { "rawtypes", "unchecked" } )
       Iterator< Toffs >[] allToffs = new Iterator[ toMerge.size() ];
-      termsInfos = new ArrayList< TermInfo >();
+      termsInfos = new ArrayList<>();
       seqnum = first.seqnum;
       boost = first.boost;
       allToffs[ 0 ] = first.termsOffsets.iterator();
@@ -273,8 +273,8 @@ public class FieldPhraseList {
         allToffs[ index++ ] = info.termsOffsets.iterator();
       }
       // Step 2.  Walk the sorted list merging overlaps
-      MergedIterator< Toffs > itr = new MergedIterator< Toffs >( false, allToffs );
-      termsOffsets = new ArrayList< Toffs >();
+      MergedIterator< Toffs > itr = new MergedIterator<>( false, allToffs );
+      termsOffsets = new ArrayList<>();
       if ( !itr.hasNext() ) {
         return;
       }
diff --git lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldQuery.java lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldQuery.java
index 9d125c6..dbf72a1 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldQuery.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldQuery.java
@@ -50,11 +50,11 @@ public class FieldQuery {
 
   // fieldMatch==true,  Map<fieldName,QueryPhraseMap>
   // fieldMatch==false, Map<null,QueryPhraseMap>
-  Map<String, QueryPhraseMap> rootMaps = new HashMap<String, QueryPhraseMap>();
+  Map<String, QueryPhraseMap> rootMaps = new HashMap<>();
 
   // fieldMatch==true,  Map<fieldName,setOfTermsInQueries>
   // fieldMatch==false, Map<null,setOfTermsInQueries>
-  Map<String, Set<String>> termSetMap = new HashMap<String, Set<String>>();
+  Map<String, Set<String>> termSetMap = new HashMap<>();
 
   int termOrPhraseNumber; // used for colored tag support
 
@@ -63,7 +63,7 @@ public class FieldQuery {
 
   FieldQuery( Query query, IndexReader reader, boolean phraseHighlight, boolean fieldMatch ) throws IOException {
     this.fieldMatch = fieldMatch;
-    Set<Query> flatQueries = new LinkedHashSet<Query>();
+    Set<Query> flatQueries = new LinkedHashSet<>();
     flatten( query, reader, flatQueries );
     saveTerms( flatQueries, reader );
     Collection<Query> expandQueries = expand( flatQueries );
@@ -169,7 +169,7 @@ public class FieldQuery {
    *      => expandQueries={a,"b c","c d","b c d"}
    */
   Collection<Query> expand( Collection<Query> flatQueries ){
-    Set<Query> expandQueries = new LinkedHashSet<Query>();
+    Set<Query> expandQueries = new LinkedHashSet<>();
     for( Iterator<Query> i = flatQueries.iterator(); i.hasNext(); ){
       Query query = i.next();
       i.remove();
@@ -316,7 +316,7 @@ public class FieldQuery {
     String key = getKey( query );
     Set<String> set = termSetMap.get( key );
     if( set == null ){
-      set = new HashSet<String>();
+      set = new HashSet<>();
       termSetMap.put( key, set );
     }
     return set;
@@ -364,7 +364,7 @@ public class FieldQuery {
     float boost;  // valid if terminal == true
     int termOrPhraseNumber;   // valid if terminal == true
     FieldQuery fieldQuery;
-    Map<String, QueryPhraseMap> subMap = new HashMap<String, QueryPhraseMap>();
+    Map<String, QueryPhraseMap> subMap = new HashMap<>();
     
     public QueryPhraseMap( FieldQuery fieldQuery ){
       this.fieldQuery = fieldQuery;
diff --git lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java
index 7c4534e..db5ecc6 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java
@@ -38,7 +38,7 @@ import org.apache.lucene.util.UnicodeUtil;
 public class FieldTermStack {
   
   private final String fieldName;
-  LinkedList<TermInfo> termList = new LinkedList<TermInfo>();
+  LinkedList<TermInfo> termList = new LinkedList<>();
   
   //public static void main( String[] args ) throws Exception {
   //  Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
diff --git lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleBoundaryScanner.java lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleBoundaryScanner.java
index bca20c4..2d6b468 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleBoundaryScanner.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleBoundaryScanner.java
@@ -47,7 +47,7 @@ public class SimpleBoundaryScanner implements BoundaryScanner {
   
   public SimpleBoundaryScanner( int maxScan, Character[] boundaryChars ){
     this.maxScan = maxScan;
-    this.boundaryChars = new HashSet<Character>();
+    this.boundaryChars = new HashSet<>();
     this.boundaryChars.addAll(Arrays.asList(boundaryChars));
   }
   
diff --git lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFieldFragList.java lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFieldFragList.java
index 93d1140..09d29e6 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFieldFragList.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFieldFragList.java
@@ -43,7 +43,7 @@ public class SimpleFieldFragList extends FieldFragList {
   @Override
   public void add( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList ) {
     float totalBoost = 0;
-    List<SubInfo> subInfos = new ArrayList<SubInfo>();
+    List<SubInfo> subInfos = new ArrayList<>();
     for( WeightedPhraseInfo phraseInfo : phraseInfoList ){
       subInfos.add( new SubInfo( phraseInfo.getText(), phraseInfo.getTermsOffsets(), phraseInfo.getSeqnum(), phraseInfo.getBoost() ) );
       totalBoost += phraseInfo.getBoost();
diff --git lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SingleFragListBuilder.java lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SingleFragListBuilder.java
index 2d2d10c..2051860 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SingleFragListBuilder.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SingleFragListBuilder.java
@@ -41,7 +41,7 @@ public class SingleFragListBuilder implements FragListBuilder {
 
     FieldFragList ffl = new SimpleFieldFragList( fragCharSize );
 
-    List<WeightedPhraseInfo> wpil = new ArrayList<WeightedPhraseInfo>();
+    List<WeightedPhraseInfo> wpil = new ArrayList<>();
     Iterator<WeightedPhraseInfo> ite = fieldPhraseList.phraseList.iterator();
     WeightedPhraseInfo phraseInfo = null;
     while( true ){
diff --git lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/WeightedFieldFragList.java lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/WeightedFieldFragList.java
index e542f6d..9af3ca6 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/WeightedFieldFragList.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/WeightedFieldFragList.java
@@ -44,9 +44,9 @@ public class WeightedFieldFragList extends FieldFragList {
    */ 
   @Override
   public void add( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList ) {
-    List<SubInfo> tempSubInfos = new ArrayList<SubInfo>();
-    List<SubInfo> realSubInfos = new ArrayList<SubInfo>();
-    HashSet<String> distinctTerms = new HashSet<String>();   
+    List<SubInfo> tempSubInfos = new ArrayList<>();
+    List<SubInfo> realSubInfos = new ArrayList<>();
+    HashSet<String> distinctTerms = new HashSet<>();
     int length = 0;
 
     for( WeightedPhraseInfo phraseInfo : phraseInfoList ){
diff --git lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java
index 2569254..706fcc6 100644
--- lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java
+++ lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java
@@ -419,7 +419,7 @@ public class HighlighterTest extends BaseTokenStreamTestCase implements Formatte
   }
   
   public void testSpanRegexQuery() throws Exception {
-    query = new SpanOrQuery(new SpanMultiTermQueryWrapper<RegexpQuery>(new RegexpQuery(new Term(FIELD_NAME, "ken.*"))));
+    query = new SpanOrQuery(new SpanMultiTermQueryWrapper<>(new RegexpQuery(new Term(FIELD_NAME, "ken.*"))));
     searcher = newSearcher(reader);
     hits = searcher.search(query, 100);
     int maxNumFragmentsRequired = 2;
@@ -1173,12 +1173,12 @@ public class HighlighterTest extends BaseTokenStreamTestCase implements Formatte
         WeightedSpanTerm[] wTerms = new WeightedSpanTerm[2];
         wTerms[0] = new WeightedSpanTerm(10f, "hello");
 
-        List<PositionSpan> positionSpans = new ArrayList<PositionSpan>();
+        List<PositionSpan> positionSpans = new ArrayList<>();
         positionSpans.add(new PositionSpan(0, 0));
         wTerms[0].addPositionSpans(positionSpans);
 
         wTerms[1] = new WeightedSpanTerm(1f, "kennedy");
-        positionSpans = new ArrayList<PositionSpan>();
+        positionSpans = new ArrayList<>();
         positionSpans.add(new PositionSpan(14, 14));
         wTerms[1].addPositionSpans(positionSpans);
 
@@ -1216,7 +1216,7 @@ public class HighlighterTest extends BaseTokenStreamTestCase implements Formatte
 
       @Override
       public void run() throws Exception {
-        HashMap<String,String> synonyms = new HashMap<String,String>();
+        HashMap<String,String> synonyms = new HashMap<>();
         synonyms.put("football", "soccer,footie");
         Analyzer analyzer = new SynonymAnalyzer(synonyms);
 
@@ -1578,7 +1578,7 @@ public class HighlighterTest extends BaseTokenStreamTestCase implements Formatte
       private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
       private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
       {
-        lst = new ArrayList<Token>();
+        lst = new ArrayList<>();
         Token t;
         t = createToken("hi", 0, 2);
         t.setPositionIncrement(1);
@@ -1629,7 +1629,7 @@ public class HighlighterTest extends BaseTokenStreamTestCase implements Formatte
       private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
       private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
       {
-        lst = new ArrayList<Token>();
+        lst = new ArrayList<>();
         Token t;
         t = createToken("hispeed", 0, 8);
         t.setPositionIncrement(1);
diff --git lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestMultiTermHighlighting.java lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestMultiTermHighlighting.java
index 4b1ce87..29210a0 100644
--- lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestMultiTermHighlighting.java
+++ lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestMultiTermHighlighting.java
@@ -511,7 +511,7 @@ public class TestMultiTermHighlighting extends LuceneTestCase {
         return analyzer;
       }
     };
-    Query query = new SpanMultiTermQueryWrapper<WildcardQuery>(new WildcardQuery(new Term("body", "te*")));
+    Query query = new SpanMultiTermQueryWrapper<>(new WildcardQuery(new Term("body", "te*")));
     TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
     assertEquals(2, topDocs.totalHits);
     String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
@@ -552,7 +552,7 @@ public class TestMultiTermHighlighting extends LuceneTestCase {
         return analyzer;
       }
     };
-    SpanQuery childQuery = new SpanMultiTermQueryWrapper<WildcardQuery>(new WildcardQuery(new Term("body", "te*")));
+    SpanQuery childQuery = new SpanMultiTermQueryWrapper<>(new WildcardQuery(new Term("body", "te*")));
     Query query = new SpanOrQuery(new SpanQuery[] { childQuery });
     TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
     assertEquals(2, topDocs.totalHits);
@@ -594,7 +594,7 @@ public class TestMultiTermHighlighting extends LuceneTestCase {
         return analyzer;
       }
     };
-    SpanQuery childQuery = new SpanMultiTermQueryWrapper<WildcardQuery>(new WildcardQuery(new Term("body", "te*")));
+    SpanQuery childQuery = new SpanMultiTermQueryWrapper<>(new WildcardQuery(new Term("body", "te*")));
     Query query = new SpanNearQuery(new SpanQuery[] { childQuery }, 0, true);
     TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
     assertEquals(2, topDocs.totalHits);
@@ -636,7 +636,7 @@ public class TestMultiTermHighlighting extends LuceneTestCase {
         return analyzer;
       }
     };
-    SpanQuery include = new SpanMultiTermQueryWrapper<WildcardQuery>(new WildcardQuery(new Term("body", "te*")));
+    SpanQuery include = new SpanMultiTermQueryWrapper<>(new WildcardQuery(new Term("body", "te*")));
     SpanQuery exclude = new SpanTermQuery(new Term("body", "bogus"));
     Query query = new SpanNotQuery(include, exclude);
     TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
@@ -679,7 +679,7 @@ public class TestMultiTermHighlighting extends LuceneTestCase {
         return analyzer;
       }
     };
-    SpanQuery childQuery = new SpanMultiTermQueryWrapper<WildcardQuery>(new WildcardQuery(new Term("body", "te*")));
+    SpanQuery childQuery = new SpanMultiTermQueryWrapper<>(new WildcardQuery(new Term("body", "te*")));
     Query query = new SpanFirstQuery(childQuery, 1000000);
     TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
     assertEquals(2, topDocs.totalHits);
diff --git lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking.java lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking.java
index 284c2ce..3af9eb1 100644
--- lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking.java
+++ lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking.java
@@ -163,7 +163,7 @@ public class TestPostingsHighlighterRanking extends LuceneTestCase {
    * instead it just collects them for asserts!
    */
   static class FakePassageFormatter extends PassageFormatter {
-    HashSet<Pair> seen = new HashSet<Pair>();
+    HashSet<Pair> seen = new HashSet<>();
     
     @Override
     public String format(Passage passages[], String content) {
diff --git lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java
index fcd1a38..b5f33e5 100644
--- lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java
+++ lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java
@@ -168,7 +168,7 @@ public abstract class AbstractTestCase extends LuceneTestCase {
   }
 
   protected List<BytesRef> analyze(String text, String field, Analyzer analyzer) throws IOException {
-    List<BytesRef> bytesRefs = new ArrayList<BytesRef>();
+    List<BytesRef> bytesRefs = new ArrayList<>();
 
     try (TokenStream tokenStream = analyzer.tokenStream(field, text)) {
       TermToBytesRefAttribute termAttribute = tokenStream.getAttribute(TermToBytesRefAttribute.class);
diff --git lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest.java lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest.java
index de23634..cbff751 100644
--- lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest.java
+++ lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest.java
@@ -533,7 +533,7 @@ public class FastVectorHighlighterTest extends LuceneTestCase {
           token( "red", 0, 0, 3 )
         ), matched ) );
 
-    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<String, Analyzer>();
+    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<>();
     fieldAnalyzers.put( "field", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );
     fieldAnalyzers.put( "field_exact", new MockAnalyzer( random() ) );
     fieldAnalyzers.put( "field_super_exact", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );
@@ -566,7 +566,7 @@ public class FastVectorHighlighterTest extends LuceneTestCase {
     FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );
     String[] bestFragments;
     if ( useMatchedFields ) {
-      Set< String > matchedFields = new HashSet< String >();
+      Set< String > matchedFields = new HashSet<>();
       matchedFields.add( "field" );
       matchedFields.add( "field_exact" );
       matchedFields.add( "field_super_exact" );
diff --git lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldPhraseListTest.java lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldPhraseListTest.java
index e0f5ad1..a0bfc39 100644
--- lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldPhraseListTest.java
+++ lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldPhraseListTest.java
@@ -269,7 +269,7 @@ public class FieldPhraseListTest extends AbstractTestCase {
   }
 
   private WeightedPhraseInfo newInfo( int startOffset, int endOffset, float boost ) {
-    LinkedList< TermInfo > infos = new LinkedList< TermInfo >();
+    LinkedList< TermInfo > infos = new LinkedList<>();
     infos.add( new TermInfo( TestUtil.randomUnicodeString(random()), startOffset, endOffset, 0, 0 ) );
     return new WeightedPhraseInfo( infos, boost );
   }
diff --git lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java
index ac67674..a0a9233 100644
--- lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java
+++ lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java
@@ -67,7 +67,7 @@ public class FieldQueryTest extends AbstractTestCase {
     booleanQuery.add(innerQuery, Occur.MUST_NOT);
 
     FieldQuery fq = new FieldQuery(booleanQuery, true, true );
-    Set<Query> flatQueries = new HashSet<Query>();
+    Set<Query> flatQueries = new HashSet<>();
     fq.flatten(booleanQuery, reader, flatQueries);
     assertCollectionQueries( flatQueries, tq( boost, "A" ), tq( boost, "B" ), tq( boost, "C" ) );
   }
@@ -77,7 +77,7 @@ public class FieldQueryTest extends AbstractTestCase {
     Query query = dmq( tq( "A" ), tq( "B" ), pqF( "C", "D" ) );
     query.setBoost( boost );
     FieldQuery fq = new FieldQuery( query, true, true );
-    Set<Query> flatQueries = new HashSet<Query>();
+    Set<Query> flatQueries = new HashSet<>();
     fq.flatten( query, reader, flatQueries );
     assertCollectionQueries( flatQueries, tq( boost, "A" ), tq( boost, "B" ), pqF( boost, "C", "D" ) );
   }
@@ -90,7 +90,7 @@ public class FieldQueryTest extends AbstractTestCase {
     booleanQuery.add(pqF("B", "C"), Occur.MUST);
 
     FieldQuery fq = new FieldQuery(booleanQuery, true, true );
-    Set<Query> flatQueries = new HashSet<Query>();
+    Set<Query> flatQueries = new HashSet<>();
     fq.flatten(booleanQuery, reader, flatQueries);
     assertCollectionQueries( flatQueries, tq( boost, "A" ), pqF( boost, "B", "C" ) );
   }
@@ -102,7 +102,7 @@ public class FieldQueryTest extends AbstractTestCase {
     query.add(toPhraseQuery(analyze("EFGH", F, analyzerB), F), Occur.SHOULD);
 
     FieldQuery fq = new FieldQuery( query, true, true );
-    Set<Query> flatQueries = new HashSet<Query>();
+    Set<Query> flatQueries = new HashSet<>();
     fq.flatten( query, reader, flatQueries );
     assertCollectionQueries( flatQueries, tq( "AA" ), pqF( "BC", "CD" ), pqF( "EF", "FG", "GH" ) );
   }
@@ -110,7 +110,7 @@ public class FieldQueryTest extends AbstractTestCase {
   public void testFlatten1TermPhrase() throws Exception {
     Query query = pqF( "A" );
     FieldQuery fq = new FieldQuery( query, true, true );
-    Set<Query> flatQueries = new HashSet<Query>();
+    Set<Query> flatQueries = new HashSet<>();
     fq.flatten( query, reader, flatQueries );
     assertCollectionQueries( flatQueries, tq( "A" ) );
   }
@@ -120,56 +120,56 @@ public class FieldQueryTest extends AbstractTestCase {
     FieldQuery fq = new FieldQuery( dummy, true, true );
 
     // "a b","b c" => "a b","b c","a b c"
-    Set<Query> flatQueries = new HashSet<Query>();
+    Set<Query> flatQueries = new HashSet<>();
     flatQueries.add( pqF( "a", "b" ) );
     flatQueries.add( pqF( "b", "c" ) );
     assertCollectionQueries( fq.expand( flatQueries ),
         pqF( "a", "b" ), pqF( "b", "c" ), pqF( "a", "b", "c" ) );
 
     // "a b","b c d" => "a b","b c d","a b c d"
-    flatQueries = new HashSet<Query>();
+    flatQueries = new HashSet<>();
     flatQueries.add( pqF( "a", "b" ) );
     flatQueries.add( pqF( "b", "c", "d" ) );
     assertCollectionQueries( fq.expand( flatQueries ),
         pqF( "a", "b" ), pqF( "b", "c", "d" ), pqF( "a", "b", "c", "d" ) );
 
     // "a b c","b c d" => "a b c","b c d","a b c d"
-    flatQueries = new HashSet<Query>();
+    flatQueries = new HashSet<>();
     flatQueries.add( pqF( "a", "b", "c" ) );
     flatQueries.add( pqF( "b", "c", "d" ) );
     assertCollectionQueries( fq.expand( flatQueries ),
         pqF( "a", "b", "c" ), pqF( "b", "c", "d" ), pqF( "a", "b", "c", "d" ) );
 
     // "a b c","c d e" => "a b c","c d e","a b c d e"
-    flatQueries = new HashSet<Query>();
+    flatQueries = new HashSet<>();
     flatQueries.add( pqF( "a", "b", "c" ) );
     flatQueries.add( pqF( "c", "d", "e" ) );
     assertCollectionQueries( fq.expand( flatQueries ),
         pqF( "a", "b", "c" ), pqF( "c", "d", "e" ), pqF( "a", "b", "c", "d", "e" ) );
 
     // "a b c d","b c" => "a b c d","b c"
-    flatQueries = new HashSet<Query>();
+    flatQueries = new HashSet<>();
     flatQueries.add( pqF( "a", "b", "c", "d" ) );
     flatQueries.add( pqF( "b", "c" ) );
     assertCollectionQueries( fq.expand( flatQueries ),
         pqF( "a", "b", "c", "d" ), pqF( "b", "c" ) );
 
     // "a b b","b c" => "a b b","b c","a b b c"
-    flatQueries = new HashSet<Query>();
+    flatQueries = new HashSet<>();
     flatQueries.add( pqF( "a", "b", "b" ) );
     flatQueries.add( pqF( "b", "c" ) );
     assertCollectionQueries( fq.expand( flatQueries ),
         pqF( "a", "b", "b" ), pqF( "b", "c" ), pqF( "a", "b", "b", "c" ) );
 
     // "a b","b a" => "a b","b a","a b a", "b a b"
-    flatQueries = new HashSet<Query>();
+    flatQueries = new HashSet<>();
     flatQueries.add( pqF( "a", "b" ) );
     flatQueries.add( pqF( "b", "a" ) );
     assertCollectionQueries( fq.expand( flatQueries ),
         pqF( "a", "b" ), pqF( "b", "a" ), pqF( "a", "b", "a" ), pqF( "b", "a", "b" ) );
 
     // "a b","a b c" => "a b","a b c"
-    flatQueries = new HashSet<Query>();
+    flatQueries = new HashSet<>();
     flatQueries.add( pqF( "a", "b" ) );
     flatQueries.add( pqF( "a", "b", "c" ) );
     assertCollectionQueries( fq.expand( flatQueries ),
@@ -181,42 +181,42 @@ public class FieldQueryTest extends AbstractTestCase {
     FieldQuery fq = new FieldQuery( dummy, true, true );
 
     // "a b","c d" => "a b","c d"
-    Set<Query> flatQueries = new HashSet<Query>();
+    Set<Query> flatQueries = new HashSet<>();
     flatQueries.add( pqF( "a", "b" ) );
     flatQueries.add( pqF( "c", "d" ) );
     assertCollectionQueries( fq.expand( flatQueries ),
         pqF( "a", "b" ), pqF( "c", "d" ) );
 
     // "a","a b" => "a", "a b"
-    flatQueries = new HashSet<Query>();
+    flatQueries = new HashSet<>();
     flatQueries.add( tq( "a" ) );
     flatQueries.add( pqF( "a", "b" ) );
     assertCollectionQueries( fq.expand( flatQueries ),
         tq( "a" ), pqF( "a", "b" ) );
 
     // "a b","b" => "a b", "b"
-    flatQueries = new HashSet<Query>();
+    flatQueries = new HashSet<>();
     flatQueries.add( pqF( "a", "b" ) );
     flatQueries.add( tq( "b" ) );
     assertCollectionQueries( fq.expand( flatQueries ),
         pqF( "a", "b" ), tq( "b" ) );
 
     // "a b c","b c" => "a b c","b c"
-    flatQueries = new HashSet<Query>();
+    flatQueries = new HashSet<>();
     flatQueries.add( pqF( "a", "b", "c" ) );
     flatQueries.add( pqF( "b", "c" ) );
     assertCollectionQueries( fq.expand( flatQueries ),
         pqF( "a", "b", "c" ), pqF( "b", "c" ) );
 
     // "a b","a b c" => "a b","a b c"
-    flatQueries = new HashSet<Query>();
+    flatQueries = new HashSet<>();
     flatQueries.add( pqF( "a", "b" ) );
     flatQueries.add( pqF( "a", "b", "c" ) );
     assertCollectionQueries( fq.expand( flatQueries ),
         pqF( "a", "b" ), pqF( "a", "b", "c" ) );
 
     // "a b c","b d e" => "a b c","b d e"
-    flatQueries = new HashSet<Query>();
+    flatQueries = new HashSet<>();
     flatQueries.add( pqF( "a", "b", "c" ) );
     flatQueries.add( pqF( "b", "d", "e" ) );
     assertCollectionQueries( fq.expand( flatQueries ),
@@ -228,7 +228,7 @@ public class FieldQueryTest extends AbstractTestCase {
     FieldQuery fq = new FieldQuery( dummy, true, false );
 
     // f1:"a b",f2:"b c" => f1:"a b",f2:"b c",f1:"a b c"
-    Set<Query> flatQueries = new HashSet<Query>();
+    Set<Query> flatQueries = new HashSet<>();
     flatQueries.add( pq( F1, "a", "b" ) );
     flatQueries.add( pq( F2, "b", "c" ) );
     assertCollectionQueries( fq.expand( flatQueries ),
@@ -826,7 +826,7 @@ public class FieldQueryTest extends AbstractTestCase {
     FieldQuery fq = new FieldQuery( query, true, true );
     
     // "a"
-    List<TermInfo> phraseCandidate = new ArrayList<TermInfo>();
+    List<TermInfo> phraseCandidate = new ArrayList<>();
     phraseCandidate.add( new TermInfo( "a", 0, 1, 0, 1 ) );
     assertNull( fq.searchPhrase( F, phraseCandidate ) );
     // "a b"
@@ -868,7 +868,7 @@ public class FieldQueryTest extends AbstractTestCase {
     FieldQuery fq = new FieldQuery( query, true, true );
     
     // "a b c" w/ position-gap = 2
-    List<TermInfo> phraseCandidate = new ArrayList<TermInfo>();
+    List<TermInfo> phraseCandidate = new ArrayList<>();
     phraseCandidate.add( new TermInfo( "a", 0, 1, 0, 1 ) );
     phraseCandidate.add( new TermInfo( "b", 2, 3, 2, 1 ) );
     phraseCandidate.add( new TermInfo( "c", 4, 5, 4, 1 ) );
@@ -917,7 +917,7 @@ public class FieldQueryTest extends AbstractTestCase {
     QueryPhraseMap qpm = fq.getFieldTermMap(F, "defg");
     assertNotNull (qpm);
     assertNull (fq.getFieldTermMap(F, "dog"));
-    List<TermInfo> phraseCandidate = new ArrayList<TermInfo>();
+    List<TermInfo> phraseCandidate = new ArrayList<>();
     phraseCandidate.add( new TermInfo( "defg", 0, 12, 0, 1 ) );
     assertNotNull (fq.searchPhrase(F, phraseCandidate));
   }
@@ -947,7 +947,7 @@ public class FieldQueryTest extends AbstractTestCase {
     });
     query.setBoost(boost);
     FieldQuery fq = new FieldQuery( query, true, true );
-    Set<Query> flatQueries = new HashSet<Query>();
+    Set<Query> flatQueries = new HashSet<>();
     fq.flatten( query, reader, flatQueries );
     assertCollectionQueries( flatQueries, tq( boost, "A" ) );
   }
@@ -957,7 +957,7 @@ public class FieldQueryTest extends AbstractTestCase {
     Query query = new ConstantScoreQuery(pqF( "A" ));
     query.setBoost(boost);
     FieldQuery fq = new FieldQuery( query, true, true );
-    Set<Query> flatQueries = new HashSet<Query>();
+    Set<Query> flatQueries = new HashSet<>();
     fq.flatten( query, reader, flatQueries );
     assertCollectionQueries( flatQueries, tq( boost, "A" ) );
   }
diff --git lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.java lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.java
index b09b0dc..1c495c6 100644
--- lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.java
+++ lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.java
@@ -48,7 +48,7 @@ public class IndexTimeSynonymTest extends AbstractTestCase {
     FieldQuery fq = new FieldQuery( bq, true, true );
     FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
     assertEquals( 2, stack.termList.size() );
-    Set<String> expectedSet = new HashSet<String>();
+    Set<String> expectedSet = new HashSet<>();
     expectedSet.add( "Mac(11,20,3)" );
     expectedSet.add( "MacBook(11,20,3)" );
     assertTrue( expectedSet.contains( stack.pop().toString() ) );
@@ -92,7 +92,7 @@ public class IndexTimeSynonymTest extends AbstractTestCase {
     FieldQuery fq = new FieldQuery( bq, true, true );
     FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
     assertEquals( 3, stack.termList.size() );
-    Set<String> expectedSet = new HashSet<String>();
+    Set<String> expectedSet = new HashSet<>();
     expectedSet.add( "pc(3,5,1)" );
     expectedSet.add( "personal(3,5,1)" );
     assertTrue( expectedSet.contains( stack.pop().toString() ) );
@@ -137,7 +137,7 @@ public class IndexTimeSynonymTest extends AbstractTestCase {
     FieldQuery fq = new FieldQuery( bq, true, true );
     FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
     assertEquals( 3, stack.termList.size() );
-    Set<String> expectedSet = new HashSet<String>();
+    Set<String> expectedSet = new HashSet<>();
     expectedSet.add( "pc(3,20,1)" );
     expectedSet.add( "personal(3,20,1)" );
     assertTrue( expectedSet.contains( stack.pop().toString() ) );
diff --git lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilderTest.java lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilderTest.java
index e15b32a..ee4cf65 100644
--- lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilderTest.java
+++ lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilderTest.java
@@ -248,9 +248,9 @@ public class SimpleFragmentsBuilderTest extends AbstractTestCase {
     int numDocs = randomValues.length * 5;
     int numFields = 2 + random().nextInt(5);
     int numTerms = 2 + random().nextInt(3);
-    List<Doc> docs = new ArrayList<Doc>(numDocs);
-    List<Document> documents = new ArrayList<Document>(numDocs);
-    Map<String, Set<Integer>> valueToDocId = new HashMap<String, Set<Integer>>();
+    List<Doc> docs = new ArrayList<>(numDocs);
+    List<Document> documents = new ArrayList<>(numDocs);
+    Map<String, Set<Integer>> valueToDocId = new HashMap<>();
     for (int i = 0; i < numDocs; i++) {
       Document document = new Document();
       String[][] fields = new String[numFields][numTerms];
@@ -277,7 +277,7 @@ public class SimpleFragmentsBuilderTest extends AbstractTestCase {
       for (int highlightIter = 0; highlightIter < highlightIters; highlightIter++) {
         String queryTerm = randomValues[random().nextInt(randomValues.length)];
         int randomHit = valueToDocId.get(queryTerm).iterator().next();
-        List<StringBuilder> builders = new ArrayList<StringBuilder>();
+        List<StringBuilder> builders = new ArrayList<>();
         for (String[] fieldValues : docs.get(randomHit).fieldValues) {
           StringBuilder builder = new StringBuilder();
           boolean hit = false;
diff --git lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinCollector.java lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinCollector.java
index 5e68f5e..1171af0 100644
--- lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinCollector.java
+++ lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinCollector.java
@@ -309,7 +309,7 @@ public class ToParentBlockJoinCollector extends Collector {
     }
     Arrays.fill(joinScorers, null);
 
-    Queue<Scorer> queue = new LinkedList<Scorer>();
+    Queue<Scorer> queue = new LinkedList<>();
     //System.out.println("\nqueue: add top scorer=" + scorer);
     queue.add(scorer);
     while ((scorer = queue.poll()) != null) {
diff --git lucene/join/src/test/org/apache/lucene/search/join/TestBlockJoin.java lucene/join/src/test/org/apache/lucene/search/join/TestBlockJoin.java
index c7ddcdd..c3bafea 100644
--- lucene/join/src/test/org/apache/lucene/search/join/TestBlockJoin.java
+++ lucene/join/src/test/org/apache/lucene/search/join/TestBlockJoin.java
@@ -69,7 +69,7 @@ public class TestBlockJoin extends LuceneTestCase {
     // we don't want to merge - since we rely on certain segment setup
     final IndexWriter w = new IndexWriter(dir, config);
 
-    final List<Document> docs = new ArrayList<Document>();
+    final List<Document> docs = new ArrayList<>();
 
     docs.add(makeJob("java", 2007));
     docs.add(makeJob("python", 2010));
@@ -127,7 +127,7 @@ public class TestBlockJoin extends LuceneTestCase {
     final Directory dir = newDirectory();
     final RandomIndexWriter w = new RandomIndexWriter(random(), dir);
 
-    final List<Document> docs = new ArrayList<Document>();
+    final List<Document> docs = new ArrayList<>();
 
     docs.add(makeJob("java", 2007));
     docs.add(makeJob("python", 2010));
@@ -217,7 +217,7 @@ public class TestBlockJoin extends LuceneTestCase {
     final Directory dir = newDirectory();
     final RandomIndexWriter w = new RandomIndexWriter(random(), dir);
 
-    final List<Document> docs = new ArrayList<Document>();
+    final List<Document> docs = new ArrayList<>();
 
     for (int i=0;i<10;i++) {
       docs.clear();
@@ -1266,7 +1266,7 @@ public class TestBlockJoin extends LuceneTestCase {
     parent.add(newTextField("parentText", "text", Field.Store.NO));
     parent.add(newStringField("isParent", "yes", Field.Store.NO));
 
-    List<Document> docs = new ArrayList<Document>();
+    List<Document> docs = new ArrayList<>();
 
     Document child = new Document();
     docs.add(child);
@@ -1331,7 +1331,7 @@ public class TestBlockJoin extends LuceneTestCase {
     parent.add(newTextField("parentText", "text", Field.Store.NO));
     parent.add(newStringField("isParent", "yes", Field.Store.NO));
 
-    List<Document> docs = new ArrayList<Document>();
+    List<Document> docs = new ArrayList<>();
 
     Document child = new Document();
     docs.add(child);
@@ -1397,7 +1397,7 @@ public class TestBlockJoin extends LuceneTestCase {
     parent.add(newTextField("parentText", "text", Field.Store.NO));
     parent.add(newStringField("isParent", "yes", Field.Store.NO));
 
-    List<Document> docs = new ArrayList<Document>();
+    List<Document> docs = new ArrayList<>();
 
     Document child = new Document();
     docs.add(child);
diff --git lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
index f6b30bb..87f33dc 100644
--- lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
+++ lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
@@ -195,7 +195,7 @@ import org.apache.lucene.util.RecyclingIntBlockAllocator;
 public class MemoryIndex {
 
   /** info for each field: Map<String fieldName, Info field> */
-  private final HashMap<String,Info> fields = new HashMap<String,Info>();
+  private final HashMap<String,Info> fields = new HashMap<>();
   
   /** fields sorted ascending by fieldName; lazily computed on demand */
   private transient Map.Entry<String,Info>[] sortedFields; 
@@ -209,7 +209,7 @@ public class MemoryIndex {
 //  private final IntBlockPool.SliceReader postingsReader;
   private final IntBlockPool.SliceWriter postingsWriter;
   
-  private HashMap<String,FieldInfo> fieldInfos = new HashMap<String,FieldInfo>();
+  private HashMap<String,FieldInfo> fieldInfos = new HashMap<>();
 
   private Counter bytesUsed;
   
diff --git lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
index 2f09347..093c0c3 100644
--- lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
+++ lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
@@ -83,7 +83,7 @@ import static org.hamcrest.CoreMatchers.equalTo;
  * returning the same results for queries on some randomish indexes.
  */
 public class MemoryIndexTest extends BaseTokenStreamTestCase {
-  private Set<String> queries = new HashSet<String>();
+  private Set<String> queries = new HashSet<>();
   
   public static final int ITERATIONS = 100 * RANDOM_MULTIPLIER;
 
@@ -98,7 +98,7 @@ public class MemoryIndexTest extends BaseTokenStreamTestCase {
    * read a set of queries from a resource file
    */
   private Set<String> readQueries(String resource) throws IOException {
-    Set<String> queries = new HashSet<String>();
+    Set<String> queries = new HashSet<>();
     InputStream stream = getClass().getResourceAsStream(resource);
     BufferedReader reader = new BufferedReader(new InputStreamReader(stream, "UTF-8"));
     String line = null;
@@ -376,7 +376,7 @@ public class MemoryIndexTest extends BaseTokenStreamTestCase {
   // LUCENE-3831
   public void testNullPointerException() throws IOException {
     RegexpQuery regex = new RegexpQuery(new Term("field", "worl."));
-    SpanQuery wrappedquery = new SpanMultiTermQueryWrapper<RegexpQuery>(regex);
+    SpanQuery wrappedquery = new SpanMultiTermQueryWrapper<>(regex);
         
     MemoryIndex mindex = new MemoryIndex(random().nextBoolean(),  random().nextInt(50) * 1024 * 1024);
     mindex.addField("field", new MockAnalyzer(random()).tokenStream("field", "hello there"));
@@ -388,7 +388,7 @@ public class MemoryIndexTest extends BaseTokenStreamTestCase {
   // LUCENE-3831
   public void testPassesIfWrapped() throws IOException {
     RegexpQuery regex = new RegexpQuery(new Term("field", "worl."));
-    SpanQuery wrappedquery = new SpanOrQuery(new SpanMultiTermQueryWrapper<RegexpQuery>(regex));
+    SpanQuery wrappedquery = new SpanOrQuery(new SpanMultiTermQueryWrapper<>(regex));
 
     MemoryIndex mindex = new MemoryIndex(random().nextBoolean(),  random().nextInt(50) * 1024 * 1024);
     mindex.addField("field", new MockAnalyzer(random()).tokenStream("field", "hello there"));
diff --git lucene/misc/src/java/org/apache/lucene/document/LazyDocument.java lucene/misc/src/java/org/apache/lucene/document/LazyDocument.java
index fc34460..784db2c 100644
--- lucene/misc/src/java/org/apache/lucene/document/LazyDocument.java
+++ lucene/misc/src/java/org/apache/lucene/document/LazyDocument.java
@@ -45,8 +45,8 @@ public class LazyDocument {
   // null until first field is loaded
   private StoredDocument doc;
 
-  private Map<Integer,List<LazyField>> fields = new HashMap<Integer,List<LazyField>>();
-  private Set<String> fieldNames = new HashSet<String>();
+  private Map<Integer,List<LazyField>> fields = new HashMap<>();
+  private Set<String> fieldNames = new HashSet<>();
 
   public LazyDocument(IndexReader reader, int docID) {
     this.reader = reader;
@@ -73,7 +73,7 @@ public class LazyDocument {
     fieldNames.add(fieldInfo.name);
     List<LazyField> values = fields.get(fieldInfo.number);
     if (null == values) {
-      values = new ArrayList<LazyField>();
+      values = new ArrayList<>();
       fields.put(fieldInfo.number, values);
     } 
 
diff --git lucene/misc/src/java/org/apache/lucene/index/IndexSplitter.java lucene/misc/src/java/org/apache/lucene/index/IndexSplitter.java
index 4d8eec4..5a3508d 100644
--- lucene/misc/src/java/org/apache/lucene/index/IndexSplitter.java
+++ lucene/misc/src/java/org/apache/lucene/index/IndexSplitter.java
@@ -73,14 +73,14 @@ public class IndexSplitter {
     if (args[1].equals("-l")) {
       is.listSegments();
     } else if (args[1].equals("-d")) {
-      List<String> segs = new ArrayList<String>();
+      List<String> segs = new ArrayList<>();
       for (int x = 2; x < args.length; x++) {
         segs.add(args[x]);
       }
       is.remove(segs.toArray(new String[0]));
     } else {
       File targetDir = new File(args[1]);
-      List<String> segs = new ArrayList<String>();
+      List<String> segs = new ArrayList<>();
       for (int x = 2; x < args.length; x++) {
         segs.add(args[x]);
       }
diff --git lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java
index 3b9ee54..65a5744 100644
--- lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java
+++ lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java
@@ -118,7 +118,7 @@ public class MultiPassIndexSplitter {
       System.err.println("\t-seq\tsequential docid-range split (default is round-robin)");
       System.exit(-1);
     }
-    ArrayList<IndexReader> indexes = new ArrayList<IndexReader>();
+    ArrayList<IndexReader> indexes = new ArrayList<>();
     String outDir = null;
     int numParts = -1;
     boolean seq = false;
diff --git lucene/misc/src/java/org/apache/lucene/util/fst/ListOfOutputs.java lucene/misc/src/java/org/apache/lucene/util/fst/ListOfOutputs.java
index 99b2f3f..b4f41c3 100644
--- lucene/misc/src/java/org/apache/lucene/util/fst/ListOfOutputs.java
+++ lucene/misc/src/java/org/apache/lucene/util/fst/ListOfOutputs.java
@@ -90,7 +90,7 @@ public final class ListOfOutputs<T> extends Outputs<Object> {
       return outputs.add((T) prefix, (T) output);
     } else {
       List<T> outputList = (List<T>) output;
-      List<T> addedList = new ArrayList<T>(outputList.size());
+      List<T> addedList = new ArrayList<>(outputList.size());
       for(T _output : outputList) {
         addedList.add(outputs.add((T) prefix, _output));
       }
@@ -129,7 +129,7 @@ public final class ListOfOutputs<T> extends Outputs<Object> {
     if (count == 1) {
       return outputs.read(in);
     } else {
-      List<T> outputList = new ArrayList<T>(count);
+      List<T> outputList = new ArrayList<>(count);
       for(int i=0;i<count;i++) {
         outputList.add(outputs.read(in));
       }
@@ -165,7 +165,7 @@ public final class ListOfOutputs<T> extends Outputs<Object> {
 
   @Override
   public Object merge(Object first, Object second) {
-    List<T> outputList = new ArrayList<T>();
+    List<T> outputList = new ArrayList<>();
     if (!(first instanceof List)) {
       outputList.add((T) first);
     } else {
@@ -188,7 +188,7 @@ public final class ListOfOutputs<T> extends Outputs<Object> {
 
   public List<T> asList(Object output) { 
     if (!(output instanceof List)) {
-      List<T> result = new ArrayList<T>(1);
+      List<T> result = new ArrayList<>(1);
       result.add((T) output);
       return result;
     } else {
diff --git lucene/misc/src/test/org/apache/lucene/document/TestLazyDocument.java lucene/misc/src/test/org/apache/lucene/document/TestLazyDocument.java
index e25326f..0427b8d 100644
--- lucene/misc/src/test/org/apache/lucene/document/TestLazyDocument.java
+++ lucene/misc/src/test/org/apache/lucene/document/TestLazyDocument.java
@@ -94,7 +94,7 @@ public class TestLazyDocument extends LuceneTestCase {
       StoredDocument d = visitor.doc;
 
       int numFieldValues = 0;
-      Map<String,Integer> fieldValueCounts = new HashMap<String,Integer>();
+      Map<String,Integer> fieldValueCounts = new HashMap<>();
 
       // at this point, all FIELDS should be Lazy and unrealized
       for (StorableField f : d) {
@@ -195,7 +195,7 @@ public class TestLazyDocument extends LuceneTestCase {
 
     LazyTestingStoredFieldVisitor(LazyDocument l, String... fields) {
       lazyDoc = l;
-      lazyFieldNames = new HashSet<String>(Arrays.asList(fields));
+      lazyFieldNames = new HashSet<>(Arrays.asList(fields));
     }
 
     @Override
diff --git lucene/misc/src/test/org/apache/lucene/index/sorter/IndexSortingTest.java lucene/misc/src/test/org/apache/lucene/index/sorter/IndexSortingTest.java
index 16d4adf..3838b58 100644
--- lucene/misc/src/test/org/apache/lucene/index/sorter/IndexSortingTest.java
+++ lucene/misc/src/test/org/apache/lucene/index/sorter/IndexSortingTest.java
@@ -43,7 +43,7 @@ public class IndexSortingTest extends SorterTestBase {
     // only read the values of the undeleted documents, since after addIndexes,
     // the deleted ones will be dropped from the index.
     Bits liveDocs = reader.getLiveDocs();
-    List<Integer> values = new ArrayList<Integer>();
+    List<Integer> values = new ArrayList<>();
     for (int i = 0; i < reader.maxDoc(); i++) {
       if (liveDocs == null || liveDocs.get(i)) {
         values.add(Integer.valueOf(reader.document(i).get(ID_FIELD)));
diff --git lucene/misc/src/test/org/apache/lucene/index/sorter/SorterTestBase.java lucene/misc/src/test/org/apache/lucene/index/sorter/SorterTestBase.java
index 372a119..49c036d 100644
--- lucene/misc/src/test/org/apache/lucene/index/sorter/SorterTestBase.java
+++ lucene/misc/src/test/org/apache/lucene/index/sorter/SorterTestBase.java
@@ -194,7 +194,7 @@ public abstract class SorterTestBase extends LuceneTestCase {
 
   /** Creates an index for sorting. */
   public static void createIndex(Directory dir, int numDocs, Random random) throws IOException {
-    List<Integer> ids = new ArrayList<Integer>();
+    List<Integer> ids = new ArrayList<>();
     for (int i = 0; i < numDocs; i++) {
       ids.add(Integer.valueOf(i * 10));
     }
diff --git lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter.java lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter.java
index aa8e77e..ea732b2 100644
--- lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter.java
+++ lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter.java
@@ -73,7 +73,7 @@ public class TestBlockJoinSorter extends LuceneTestCase {
     final StringField parent = new StringField("parent", "true", Store.YES);
     parentDoc.add(parent);
     for (int i = 0; i < numParents; ++i) {
-      List<Document> documents = new ArrayList<Document>();
+      List<Document> documents = new ArrayList<>();
       final int numChildren = random().nextInt(10);
       for (int j = 0; j < numChildren; ++j) {
         final Document childDoc = new Document();
diff --git lucene/misc/src/test/org/apache/lucene/index/sorter/TestEarlyTermination.java lucene/misc/src/test/org/apache/lucene/index/sorter/TestEarlyTermination.java
index 9601dc9..f64f56d 100644
--- lucene/misc/src/test/org/apache/lucene/index/sorter/TestEarlyTermination.java
+++ lucene/misc/src/test/org/apache/lucene/index/sorter/TestEarlyTermination.java
@@ -73,11 +73,11 @@ public class TestEarlyTermination extends LuceneTestCase {
     dir = newDirectory();
     numDocs = atLeast(150);
     final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);
-    Set<String> randomTerms = new HashSet<String>();
+    Set<String> randomTerms = new HashSet<>();
     while (randomTerms.size() < numTerms) {
       randomTerms.add(TestUtil.randomSimpleString(random()));
     }
-    terms = new ArrayList<String>(randomTerms);
+    terms = new ArrayList<>(randomTerms);
     final long seed = random().nextLong();
     final IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(new Random(seed)));
     iwc.setMergePolicy(TestSortingMergePolicy.newSortingMergePolicy(sort));
diff --git lucene/misc/src/test/org/apache/lucene/index/sorter/TestSortingMergePolicy.java lucene/misc/src/test/org/apache/lucene/index/sorter/TestSortingMergePolicy.java
index 5095aec..3d4edbb 100644
--- lucene/misc/src/test/org/apache/lucene/index/sorter/TestSortingMergePolicy.java
+++ lucene/misc/src/test/org/apache/lucene/index/sorter/TestSortingMergePolicy.java
@@ -93,11 +93,11 @@ public class TestSortingMergePolicy extends LuceneTestCase {
     dir2 = newDirectory();
     final int numDocs = atLeast(150);
     final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);
-    Set<String> randomTerms = new HashSet<String>();
+    Set<String> randomTerms = new HashSet<>();
     while (randomTerms.size() < numTerms) {
       randomTerms.add(TestUtil.randomSimpleString(random()));
     }
-    terms = new ArrayList<String>(randomTerms);
+    terms = new ArrayList<>(randomTerms);
     final long seed = random().nextLong();
     final IndexWriterConfig iwc1 = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(new Random(seed)));
     final IndexWriterConfig iwc2 = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(new Random(seed)));
diff --git lucene/misc/src/test/org/apache/lucene/util/fst/TestFSTsMisc.java lucene/misc/src/test/org/apache/lucene/util/fst/TestFSTsMisc.java
index caa5b5f..c0341a9 100644
--- lucene/misc/src/test/org/apache/lucene/util/fst/TestFSTsMisc.java
+++ lucene/misc/src/test/org/apache/lucene/util/fst/TestFSTsMisc.java
@@ -67,7 +67,7 @@ public class TestFSTsMisc extends LuceneTestCase {
       }
       for(int inputMode=0;inputMode<2;inputMode++) {
         final int numWords = random.nextInt(maxNumWords+1);
-        Set<IntsRef> termsSet = new HashSet<IntsRef>();
+        Set<IntsRef> termsSet = new HashSet<>();
         IntsRef[] terms = new IntsRef[numWords];
         while(termsSet.size() < numWords) {
           final String term = getRandomString(random);
@@ -88,7 +88,7 @@ public class TestFSTsMisc extends LuceneTestCase {
         System.out.println("TEST: now test UpToTwoPositiveIntOutputs");
       }
       final UpToTwoPositiveIntOutputs outputs = UpToTwoPositiveIntOutputs.getSingleton(true);
-      final List<FSTTester.InputOutput<Object>> pairs = new ArrayList<FSTTester.InputOutput<Object>>(terms.length);
+      final List<FSTTester.InputOutput<Object>> pairs = new ArrayList<>(terms.length);
       long lastOutput = 0;
       for(int idx=0;idx<terms.length;idx++) {
         // Sometimes go backwards
@@ -102,14 +102,14 @@ public class TestFSTsMisc extends LuceneTestCase {
           while(value2 < 0) {
             value2 = lastOutput + TestUtil.nextInt(random(), -100, 1000);
           }
-          List<Long> values = new ArrayList<Long>();
+          List<Long> values = new ArrayList<>();
           values.add(value);
           values.add(value2);
           output = values;
         } else {
           output = outputs.get(value);
         }
-        pairs.add(new FSTTester.InputOutput<Object>(terms[idx], output));
+        pairs.add(new FSTTester.InputOutput<>(terms[idx], output));
       }
       new FSTTester<Object>(random(), dir, inputMode, pairs, outputs, false) {
         @Override
@@ -133,13 +133,13 @@ public class TestFSTsMisc extends LuceneTestCase {
         System.out.println("TEST: now test OneOrMoreOutputs");
       }
       final PositiveIntOutputs _outputs = PositiveIntOutputs.getSingleton();
-      final ListOfOutputs<Long> outputs = new ListOfOutputs<Long>(_outputs);
-      final List<FSTTester.InputOutput<Object>> pairs = new ArrayList<FSTTester.InputOutput<Object>>(terms.length);
+      final ListOfOutputs<Long> outputs = new ListOfOutputs<>(_outputs);
+      final List<FSTTester.InputOutput<Object>> pairs = new ArrayList<>(terms.length);
       long lastOutput = 0;
       for(int idx=0;idx<terms.length;idx++) {
         
         int outputCount = TestUtil.nextInt(random(), 1, 7);
-        List<Long> values = new ArrayList<Long>();
+        List<Long> values = new ArrayList<>();
         for(int i=0;i<outputCount;i++) {
           // Sometimes go backwards
           long value = lastOutput + TestUtil.nextInt(random(), -100, 1000);
@@ -157,16 +157,16 @@ public class TestFSTsMisc extends LuceneTestCase {
           output = values;
         }
 
-        pairs.add(new FSTTester.InputOutput<Object>(terms[idx], output));
+        pairs.add(new FSTTester.InputOutput<>(terms[idx], output));
       }
-      new FSTTester<Object>(random(), dir, inputMode, pairs, outputs, false).doTest(false);
+      new FSTTester<>(random(), dir, inputMode, pairs, outputs, false).doTest(false);
     }
   }
 
   public void testListOfOutputs() throws Exception {
     PositiveIntOutputs _outputs = PositiveIntOutputs.getSingleton();
-    ListOfOutputs<Long> outputs = new ListOfOutputs<Long>(_outputs);
-    final Builder<Object> builder = new Builder<Object>(FST.INPUT_TYPE.BYTE1, outputs);
+    ListOfOutputs<Long> outputs = new ListOfOutputs<>(_outputs);
+    final Builder<Object> builder = new Builder<>(FST.INPUT_TYPE.BYTE1, outputs);
 
     final IntsRef scratch = new IntsRef();
     // Add the same input more than once and the outputs
@@ -194,8 +194,8 @@ public class TestFSTsMisc extends LuceneTestCase {
 
   public void testListOfOutputsEmptyString() throws Exception {
     PositiveIntOutputs _outputs = PositiveIntOutputs.getSingleton();
-    ListOfOutputs<Long> outputs = new ListOfOutputs<Long>(_outputs);
-    final Builder<Object> builder = new Builder<Object>(FST.INPUT_TYPE.BYTE1, outputs);
+    ListOfOutputs<Long> outputs = new ListOfOutputs<>(_outputs);
+    final Builder<Object> builder = new Builder<>(FST.INPUT_TYPE.BYTE1, outputs);
 
     final IntsRef scratch = new IntsRef();
     builder.add(scratch, 0L);
diff --git lucene/queries/src/java/org/apache/lucene/queries/BooleanFilter.java lucene/queries/src/java/org/apache/lucene/queries/BooleanFilter.java
index d050e0f..ce2497e 100644
--- lucene/queries/src/java/org/apache/lucene/queries/BooleanFilter.java
+++ lucene/queries/src/java/org/apache/lucene/queries/BooleanFilter.java
@@ -43,7 +43,7 @@ import org.apache.lucene.util.FixedBitSet;
  */
 public class BooleanFilter extends Filter implements Iterable<FilterClause> {
 
-  private final List<FilterClause> clauses = new ArrayList<FilterClause>();
+  private final List<FilterClause> clauses = new ArrayList<>();
 
   /**
    * Returns the a DocIdSetIterator representing the Boolean composition
diff --git lucene/queries/src/java/org/apache/lucene/queries/CommonTermsQuery.java lucene/queries/src/java/org/apache/lucene/queries/CommonTermsQuery.java
index 2e04ca4..a78cef3 100644
--- lucene/queries/src/java/org/apache/lucene/queries/CommonTermsQuery.java
+++ lucene/queries/src/java/org/apache/lucene/queries/CommonTermsQuery.java
@@ -67,7 +67,7 @@ public class CommonTermsQuery extends Query {
    * rewrite to dismax rather than boolean. Yet, this can already be subclassed
    * to do so.
    */
-  protected final List<Term> terms = new ArrayList<Term>();
+  protected final List<Term> terms = new ArrayList<>();
   protected final boolean disableCoord;
   protected final float maxTermFrequency;
   protected final Occur lowFreqOccur;
diff --git lucene/queries/src/java/org/apache/lucene/queries/TermsFilter.java lucene/queries/src/java/org/apache/lucene/queries/TermsFilter.java
index 4dc8822..32c4653 100644
--- lucene/queries/src/java/org/apache/lucene/queries/TermsFilter.java
+++ lucene/queries/src/java/org/apache/lucene/queries/TermsFilter.java
@@ -129,7 +129,7 @@ public final class TermsFilter extends Filter {
     this.offsets = new int[length+1];
     int lastEndOffset = 0;
     int index = 0;
-    ArrayList<TermsAndField> termsAndFields = new ArrayList<TermsAndField>();
+    ArrayList<TermsAndField> termsAndFields = new ArrayList<>();
     TermsAndField lastTermsAndField = null;
     BytesRef previousTerm = null;
     String previousField = null;
diff --git lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java
index 9e2ceee..94eecce 100644
--- lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java
+++ lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java
@@ -706,7 +706,7 @@ public final class MoreLikeThis {
    * @param docNum the id of the lucene document from which to find terms
    */
   public PriorityQueue<Object[]> retrieveTerms(int docNum) throws IOException {
-    Map<String, Int> termFreqMap = new HashMap<String, Int>();
+    Map<String, Int> termFreqMap = new HashMap<>();
     for (String fieldName : fieldNames) {
       final Fields vectors = ir.getTermVectors(docNum);
       final Terms vector;
@@ -846,7 +846,7 @@ public final class MoreLikeThis {
    * @see #retrieveInterestingTerms
    */
   public PriorityQueue<Object[]> retrieveTerms(Reader r, String fieldName) throws IOException {
-    Map<String, Int> words = new HashMap<String, Int>();
+    Map<String, Int> words = new HashMap<>();
     addTermFrequencies(r, words, fieldName);
     return createQueue(words);
   }
@@ -855,7 +855,7 @@ public final class MoreLikeThis {
    * @see #retrieveInterestingTerms(java.io.Reader, String)
    */
   public String[] retrieveInterestingTerms(int docNum) throws IOException {
-    ArrayList<Object> al = new ArrayList<Object>(maxQueryTerms);
+    ArrayList<Object> al = new ArrayList<>(maxQueryTerms);
     PriorityQueue<Object[]> pq = retrieveTerms(docNum);
     Object cur;
     int lim = maxQueryTerms; // have to be careful, retrieveTerms returns all words but that's probably not useful to our caller...
@@ -879,7 +879,7 @@ public final class MoreLikeThis {
    * @see #setMaxQueryTerms
    */
   public String[] retrieveInterestingTerms(Reader r, String fieldName) throws IOException {
-    ArrayList<Object> al = new ArrayList<Object>(maxQueryTerms);
+    ArrayList<Object> al = new ArrayList<>(maxQueryTerms);
     PriorityQueue<Object[]> pq = retrieveTerms(r, fieldName);
     Object cur;
     int lim = maxQueryTerms; // have to be careful, retrieveTerms returns all words but that's probably not useful to our caller...
diff --git lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java
index 0d22c61..728ac59 100644
--- lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java
+++ lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java
@@ -477,7 +477,7 @@ public class CommonTermsQueryTest extends LuceneTestCase {
       
       TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());
       assertEquals(verifySearch.totalHits, cqSearch.totalHits);
-      Set<Integer> hits = new HashSet<Integer>();
+      Set<Integer> hits = new HashSet<>();
       for (ScoreDoc doc : verifySearch.scoreDocs) {
         hits.add(doc.doc);
       }
@@ -508,7 +508,7 @@ public class CommonTermsQueryTest extends LuceneTestCase {
   }
   
   private static List<TermAndFreq> queueToList(PriorityQueue<TermAndFreq> queue) {
-    List<TermAndFreq> terms = new ArrayList<CommonTermsQueryTest.TermAndFreq>();
+    List<TermAndFreq> terms = new ArrayList<>();
     while (queue.size() > 0) {
       terms.add(queue.pop());
     }
diff --git lucene/queries/src/test/org/apache/lucene/queries/TermFilterTest.java lucene/queries/src/test/org/apache/lucene/queries/TermFilterTest.java
index e46dd70..41e30db 100644
--- lucene/queries/src/test/org/apache/lucene/queries/TermFilterTest.java
+++ lucene/queries/src/test/org/apache/lucene/queries/TermFilterTest.java
@@ -45,7 +45,7 @@ public class TermFilterTest extends LuceneTestCase {
 
   public void testCachability() throws Exception {
     TermFilter a = termFilter("field1", "a");
-    HashSet<Filter> cachedFilters = new HashSet<Filter>();
+    HashSet<Filter> cachedFilters = new HashSet<>();
     cachedFilters.add(a);
     assertTrue("Must be cached", cachedFilters.contains(termFilter("field1", "a")));
     assertFalse("Must not be cached", cachedFilters.contains(termFilter("field1", "b")));
@@ -84,7 +84,7 @@ public class TermFilterTest extends LuceneTestCase {
     Directory dir = newDirectory();
     RandomIndexWriter w = new RandomIndexWriter(random(), dir);
     int num = atLeast(100);
-    List<Term> terms = new ArrayList<Term>();
+    List<Term> terms = new ArrayList<>();
     for (int i = 0; i < num; i++) {
       String field = "field" + i;
       String string = TestUtil.randomRealisticUnicodeString(random());
diff --git lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest.java lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest.java
index 2a07118..c36e829 100644
--- lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest.java
+++ lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest.java
@@ -56,7 +56,7 @@ public class TermsFilterTest extends LuceneTestCase {
 
   public void testCachability() throws Exception {
     TermsFilter a = termsFilter(random().nextBoolean(), new Term("field1", "a"), new Term("field1", "b"));
-    HashSet<Filter> cachedFilters = new HashSet<Filter>();
+    HashSet<Filter> cachedFilters = new HashSet<>();
     cachedFilters.add(a);
     TermsFilter b = termsFilter(random().nextBoolean(), new Term("field1", "b"), new Term("field1", "a"));
     assertTrue("Must be cached", cachedFilters.contains(b));
@@ -80,7 +80,7 @@ public class TermsFilterTest extends LuceneTestCase {
     AtomicReaderContext context = (AtomicReaderContext) reader.getContext();
     w.close();
 
-    List<Term> terms = new ArrayList<Term>();
+    List<Term> terms = new ArrayList<>();
     terms.add(new Term(fieldName, "19"));
     FixedBitSet bits = (FixedBitSet) termsFilter(random().nextBoolean(), terms).getDocIdSet(context, context.reader().getLiveDocs());
     assertNull("Must match nothing", bits);
@@ -143,7 +143,7 @@ public class TermsFilterTest extends LuceneTestCase {
     RandomIndexWriter w = new RandomIndexWriter(random(), dir);
     int num = atLeast(3);
     int skip = random().nextInt(num);
-    List<Term> terms = new ArrayList<Term>();
+    List<Term> terms = new ArrayList<>();
     for (int i = 0; i < num; i++) {
       terms.add(new Term("field" + i, "content1"));
       Document doc = new Document();
@@ -174,7 +174,7 @@ public class TermsFilterTest extends LuceneTestCase {
     Directory dir = newDirectory();
     RandomIndexWriter w = new RandomIndexWriter(random(), dir);
     int num = atLeast(10);
-    Set<Term> terms = new HashSet<Term>();
+    Set<Term> terms = new HashSet<>();
     for (int i = 0; i < num; i++) {
       String field = "field" + random().nextInt(100);
       terms.add(new Term(field, "content1"));
@@ -198,7 +198,7 @@ public class TermsFilterTest extends LuceneTestCase {
     w.close();
     assertEquals(1, reader.leaves().size());
     AtomicReaderContext context = reader.leaves().get(0);
-    TermsFilter tf = new TermsFilter(new ArrayList<Term>(terms));
+    TermsFilter tf = new TermsFilter(new ArrayList<>(terms));
 
     FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());
     assertEquals(context.reader().numDocs(), bits.cardinality());  
@@ -211,7 +211,7 @@ public class TermsFilterTest extends LuceneTestCase {
     RandomIndexWriter w = new RandomIndexWriter(random(), dir);
     int num = atLeast(100);
     final boolean singleField = random().nextBoolean();
-    List<Term> terms = new ArrayList<Term>();
+    List<Term> terms = new ArrayList<>();
     for (int i = 0; i < num; i++) {
       String field = "field" + (singleField ? "1" : random().nextInt(100));
       String string = TestUtil.randomRealisticUnicodeString(random());
@@ -256,10 +256,10 @@ public class TermsFilterTest extends LuceneTestCase {
 
   private TermsFilter termsFilter(boolean singleField, Collection<Term> termList) {
     if (!singleField) {
-      return new TermsFilter(new ArrayList<Term>(termList));
+      return new TermsFilter(new ArrayList<>(termList));
     }
     final TermsFilter filter;
-    List<BytesRef> bytes = new ArrayList<BytesRef>();
+    List<BytesRef> bytes = new ArrayList<>();
     String field = null;
     for (Term term : termList) {
         bytes.add(term.bytes());
@@ -276,8 +276,8 @@ public class TermsFilterTest extends LuceneTestCase {
   public void testHashCodeAndEquals() {
     int num = atLeast(100);
     final boolean singleField = random().nextBoolean();
-    List<Term> terms = new ArrayList<Term>();
-    Set<Term> uniqueTerms = new HashSet<Term>();
+    List<Term> terms = new ArrayList<>();
+    Set<Term> uniqueTerms = new HashSet<>();
     for (int i = 0; i < num; i++) {
       String field = "field" + (singleField ? "1" : random().nextInt(100));
       String string = TestUtil.randomRealisticUnicodeString(random());
@@ -289,7 +289,7 @@ public class TermsFilterTest extends LuceneTestCase {
       assertEquals(right, left);
       assertEquals(right.hashCode(), left.hashCode());
       if (uniqueTerms.size() > 1) {
-        List<Term> asList = new ArrayList<Term>(uniqueTerms);
+        List<Term> asList = new ArrayList<>(uniqueTerms);
         asList.remove(0);
         TermsFilter notEqual = termsFilter(singleField ? random().nextBoolean() : false, asList);
         assertFalse(left.equals(notEqual));
diff --git lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java
index d976d80..7210104 100644
--- lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java
+++ lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java
@@ -343,7 +343,7 @@ public class TestCustomScoreQuery extends FunctionTestSetup {
   // since custom scoring modifies the order of docs, map results 
   // by doc ids so that we can later compare/verify them 
   private Map<Integer,Float> topDocsToMap(TopDocs td) {
-    Map<Integer,Float> h = new HashMap<Integer,Float>();
+    Map<Integer,Float> h = new HashMap<>();
     for (int i=0; i<td.totalHits; i++) {
       h.put(td.scoreDocs[i].doc, td.scoreDocs[i].score);
     }
diff --git lucene/queries/src/test/org/apache/lucene/queries/mlt/TestMoreLikeThis.java lucene/queries/src/test/org/apache/lucene/queries/mlt/TestMoreLikeThis.java
index 524f7df..dc9bf0e 100644
--- lucene/queries/src/test/org/apache/lucene/queries/mlt/TestMoreLikeThis.java
+++ lucene/queries/src/test/org/apache/lucene/queries/mlt/TestMoreLikeThis.java
@@ -108,7 +108,7 @@ public class TestMoreLikeThis extends LuceneTestCase {
   }
   
   private Map<String,Float> getOriginalValues() throws IOException {
-    Map<String,Float> originalValues = new HashMap<String,Float>();
+    Map<String,Float> originalValues = new HashMap<>();
     MoreLikeThis mlt = new MoreLikeThis(reader);
     mlt.setAnalyzer(new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false));
     mlt.setMinDocFreq(1);
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/MultiFieldQueryParser.java lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/MultiFieldQueryParser.java
index 9b6e196..5626cdf 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/MultiFieldQueryParser.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/MultiFieldQueryParser.java
@@ -98,7 +98,7 @@ public class MultiFieldQueryParser extends QueryParser
   @Override
   protected Query getFieldQuery(String field, String queryText, int slop) throws ParseException {
     if (field == null) {
-      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+      List<BooleanClause> clauses = new ArrayList<>();
       for (int i = 0; i < fields.length; i++) {
         Query q = super.getFieldQuery(fields[i], queryText, true);
         if (q != null) {
@@ -135,7 +135,7 @@ public class MultiFieldQueryParser extends QueryParser
   @Override
   protected Query getFieldQuery(String field, String queryText, boolean quoted) throws ParseException {
     if (field == null) {
-      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+      List<BooleanClause> clauses = new ArrayList<>();
       for (int i = 0; i < fields.length; i++) {
         Query q = super.getFieldQuery(fields[i], queryText, quoted);
         if (q != null) {
@@ -163,7 +163,7 @@ public class MultiFieldQueryParser extends QueryParser
   protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException
   {
     if (field == null) {
-      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+      List<BooleanClause> clauses = new ArrayList<>();
       for (int i = 0; i < fields.length; i++) {
         clauses.add(new BooleanClause(getFuzzyQuery(fields[i], termStr, minSimilarity),
             BooleanClause.Occur.SHOULD));
@@ -177,7 +177,7 @@ public class MultiFieldQueryParser extends QueryParser
   protected Query getPrefixQuery(String field, String termStr) throws ParseException
   {
     if (field == null) {
-      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+      List<BooleanClause> clauses = new ArrayList<>();
       for (int i = 0; i < fields.length; i++) {
         clauses.add(new BooleanClause(getPrefixQuery(fields[i], termStr),
             BooleanClause.Occur.SHOULD));
@@ -190,7 +190,7 @@ public class MultiFieldQueryParser extends QueryParser
   @Override
   protected Query getWildcardQuery(String field, String termStr) throws ParseException {
     if (field == null) {
-      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+      List<BooleanClause> clauses = new ArrayList<>();
       for (int i = 0; i < fields.length; i++) {
         clauses.add(new BooleanClause(getWildcardQuery(fields[i], termStr),
             BooleanClause.Occur.SHOULD));
@@ -204,7 +204,7 @@ public class MultiFieldQueryParser extends QueryParser
   @Override
   protected Query getRangeQuery(String field, String part1, String part2, boolean startInclusive, boolean endInclusive) throws ParseException {
     if (field == null) {
-      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+      List<BooleanClause> clauses = new ArrayList<>();
       for (int i = 0; i < fields.length; i++) {
         clauses.add(new BooleanClause(getRangeQuery(fields[i], part1, part2, startInclusive, endInclusive),
             BooleanClause.Occur.SHOULD));
@@ -220,7 +220,7 @@ public class MultiFieldQueryParser extends QueryParser
   protected Query getRegexpQuery(String field, String termStr)
       throws ParseException {
     if (field == null) {
-      List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+      List<BooleanClause> clauses = new ArrayList<>();
       for (int i = 0; i < fields.length; i++) {
         clauses.add(new BooleanClause(getRegexpQuery(fields[i], termStr),
             BooleanClause.Occur.SHOULD));
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.java lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.java
index 273ed8c..2d7e29b 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.java
@@ -174,7 +174,7 @@ public class QueryParser extends QueryParserBase implements QueryParserConstants
   }
 
   final public Query Query(String field) throws ParseException {
-  List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+  List<BooleanClause> clauses = new ArrayList<>();
   Query q, firstQuery=null;
   int conj, mods;
     mods = Modifiers();
@@ -640,7 +640,7 @@ public class QueryParser extends QueryParserBase implements QueryParserConstants
       return (jj_ntk = jj_nt.kind);
   }
 
-  private java.util.List<int[]> jj_expentries = new java.util.ArrayList<int[]>();
+  private java.util.List<int[]> jj_expentries = new java.util.ArrayList<>();
   private int[] jj_expentry;
   private int jj_kind = -1;
   private int[] jj_lasttokens = new int[100];
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java
index 4e0d5a6..0b8d803 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java
@@ -351,7 +351,7 @@ public abstract class QueryParserBase extends QueryBuilder implements CommonQuer
 
     if (fieldToDateResolution == null) {
       // lazily initialize HashMap
-      fieldToDateResolution = new HashMap<String,DateTools.Resolution>();
+      fieldToDateResolution = new HashMap<>();
     }
 
     fieldToDateResolution.put(fieldName, dateResolution);
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/complexPhrase/ComplexPhraseQueryParser.java lucene/queryparser/src/java/org/apache/lucene/queryparser/complexPhrase/ComplexPhraseQueryParser.java
index 1adabae..0f63fb7 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/complexPhrase/ComplexPhraseQueryParser.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/complexPhrase/ComplexPhraseQueryParser.java
@@ -104,7 +104,7 @@ public class ComplexPhraseQueryParser extends QueryParser {
 
     // First pass - parse the top-level query recording any PhraseQuerys
     // which will need to be resolved
-    complexPhrases = new ArrayList<ComplexPhraseQuery>();
+    complexPhrases = new ArrayList<>();
     Query q = super.parse(query);
 
     // Perform second pass, using this QueryParser to parse any nested
@@ -254,7 +254,7 @@ public class ComplexPhraseQueryParser extends QueryParser {
         }
 
         if (qc instanceof BooleanQuery) {
-          ArrayList<SpanQuery> sc = new ArrayList<SpanQuery>();
+          ArrayList<SpanQuery> sc = new ArrayList<>();
           addComplexPhraseClause(sc, (BooleanQuery) qc);
           if (sc.size() > 0) {
             allSpanClauses[i] = sc.get(0);
@@ -285,7 +285,7 @@ public class ComplexPhraseQueryParser extends QueryParser {
       // Complex case - we have mixed positives and negatives in the
       // sequence.
       // Need to return a SpanNotQuery
-      ArrayList<SpanQuery> positiveClauses = new ArrayList<SpanQuery>();
+      ArrayList<SpanQuery> positiveClauses = new ArrayList<>();
       for (int j = 0; j < allSpanClauses.length; j++) {
         if (!bclauses[j].getOccur().equals(BooleanClause.Occur.MUST_NOT)) {
           positiveClauses.add(allSpanClauses[j]);
@@ -312,8 +312,8 @@ public class ComplexPhraseQueryParser extends QueryParser {
     }
 
     private void addComplexPhraseClause(List<SpanQuery> spanClauses, BooleanQuery qc) {
-      ArrayList<SpanQuery> ors = new ArrayList<SpanQuery>();
-      ArrayList<SpanQuery> nots = new ArrayList<SpanQuery>();
+      ArrayList<SpanQuery> ors = new ArrayList<>();
+      ArrayList<SpanQuery> nots = new ArrayList<>();
       BooleanClause[] bclauses = qc.getClauses();
 
       // For all clauses e.g. one* two~
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/ext/Extensions.java lucene/queryparser/src/java/org/apache/lucene/queryparser/ext/Extensions.java
index 1a82e98..86e616b 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/ext/Extensions.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/ext/Extensions.java
@@ -42,7 +42,7 @@ import java.util.Map;
  * @see ParserExtension
  */
 public class Extensions {
-  private final Map<String,ParserExtension> extensions = new HashMap<String,ParserExtension>();
+  private final Map<String,ParserExtension> extensions = new HashMap<>();
   private final char extensionFieldDelimiter;
   /**
    * The default extension field delimiter character. This constant is set to
@@ -122,11 +122,11 @@ public class Extensions {
       String field) {
     int indexOf = field.indexOf(this.extensionFieldDelimiter);
     if (indexOf < 0)
-      return new Pair<String,String>(field, null);
+      return new Pair<>(field, null);
     final String indexField = indexOf == 0 ? defaultField : field.substring(0,
         indexOf);
     final String extensionKey = field.substring(indexOf + 1);
-    return new Pair<String,String>(indexField, extensionKey);
+    return new Pair<>(indexField, extensionKey);
 
   }
 
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/builders/QueryTreeBuilder.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/builders/QueryTreeBuilder.java
index 7fc916e..567a326 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/builders/QueryTreeBuilder.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/builders/QueryTreeBuilder.java
@@ -79,7 +79,7 @@ public class QueryTreeBuilder implements QueryBuilder {
   public void setBuilder(CharSequence fieldName, QueryBuilder builder) {
 
     if (this.fieldNameBuilders == null) {
-      this.fieldNameBuilders = new HashMap<String, QueryBuilder>();
+      this.fieldNameBuilders = new HashMap<>();
     }
 
     this.fieldNameBuilders.put(fieldName.toString(), builder);
@@ -97,7 +97,7 @@ public class QueryTreeBuilder implements QueryBuilder {
       QueryBuilder builder) {
 
     if (this.queryNodeBuilders == null) {
-      this.queryNodeBuilders = new HashMap<Class<? extends QueryNode>, QueryBuilder>();
+      this.queryNodeBuilders = new HashMap<>();
     }
 
     this.queryNodeBuilders.put(queryNodeClass, builder);
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/config/AbstractQueryConfig.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/config/AbstractQueryConfig.java
index 0276e33..f3f6c7e 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/config/AbstractQueryConfig.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/config/AbstractQueryConfig.java
@@ -33,7 +33,7 @@ import java.util.HashMap;
  */
 public abstract class AbstractQueryConfig {
   
-  final private HashMap<ConfigurationKey<?>, Object> configMap = new HashMap<ConfigurationKey<?>, Object>();
+  final private HashMap<ConfigurationKey<?>, Object> configMap = new HashMap<>();
   
   AbstractQueryConfig() {
     // although this class is public, it can only be constructed from package
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/config/ConfigurationKey.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/config/ConfigurationKey.java
index 1ced76d..5689a9a 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/config/ConfigurationKey.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/config/ConfigurationKey.java
@@ -36,7 +36,7 @@ final public class ConfigurationKey<T> {
    * @return a new instance
    */
   public static <T> ConfigurationKey<T> newInstance() {
-    return new ConfigurationKey<T>();
+    return new ConfigurationKey<>();
   }
   
 }
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/config/QueryConfigHandler.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/config/QueryConfigHandler.java
index 03e6110..3efb435 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/config/QueryConfigHandler.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/config/QueryConfigHandler.java
@@ -40,7 +40,7 @@ import org.apache.lucene.queryparser.flexible.core.util.StringUtils;
  */
 public abstract class QueryConfigHandler extends AbstractQueryConfig {
   
-  final private LinkedList<FieldConfigListener> listeners = new LinkedList<FieldConfigListener>();
+  final private LinkedList<FieldConfigListener> listeners = new LinkedList<>();
 
   /**
    * Returns an implementation of
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/GroupQueryNode.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/GroupQueryNode.java
index 2214467..0eff69a 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/GroupQueryNode.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/GroupQueryNode.java
@@ -73,7 +73,7 @@ public class GroupQueryNode extends QueryNodeImpl {
   }
 
   public void setChild(QueryNode child) {
-    List<QueryNode> list = new ArrayList<QueryNode>();
+    List<QueryNode> list = new ArrayList<>();
     list.add(child);
     this.set(list);
   }
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/ModifierQueryNode.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/ModifierQueryNode.java
index 9791cf6..a88667e 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/ModifierQueryNode.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/ModifierQueryNode.java
@@ -151,7 +151,7 @@ public class ModifierQueryNode extends QueryNodeImpl {
   }
 
   public void setChild(QueryNode child) {
-    List<QueryNode> list = new ArrayList<QueryNode>();
+    List<QueryNode> list = new ArrayList<>();
     list.add(child);
     this.set(list);
   }
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/PathQueryNode.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/PathQueryNode.java
index fd54d4b..2235ad2 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/PathQueryNode.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/PathQueryNode.java
@@ -162,7 +162,7 @@ public class PathQueryNode extends QueryNodeImpl {
    * @return a List QueryText element from position startIndex
    */
   public List<QueryText> getPathElements(int startIndex) {
-    List<PathQueryNode.QueryText> rValues = new ArrayList<PathQueryNode.QueryText>();
+    List<PathQueryNode.QueryText> rValues = new ArrayList<>();
     for (int i = startIndex; i < this.values.size(); i++) {
       try {
         rValues.add(this.values.get(i).clone());
@@ -209,7 +209,7 @@ public class PathQueryNode extends QueryNodeImpl {
 
     // copy children
     if (this.values != null) {
-      List<QueryText> localValues = new ArrayList<QueryText>();
+      List<QueryText> localValues = new ArrayList<>();
       for (QueryText value : this.values) {
         localValues.add(value.clone());
       }
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/QueryNodeImpl.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/QueryNodeImpl.java
index 81fd9bb..6e3de27 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/QueryNodeImpl.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/QueryNodeImpl.java
@@ -41,14 +41,14 @@ public abstract class QueryNodeImpl implements QueryNode, Cloneable {
 
   private boolean isLeaf = true;
 
-  private Hashtable<String, Object> tags = new Hashtable<String, Object>();
+  private Hashtable<String, Object> tags = new Hashtable<>();
 
   private List<QueryNode> clauses = null;
 
   protected void allocate() {
 
     if (this.clauses == null) {
-      this.clauses = new ArrayList<QueryNode>();
+      this.clauses = new ArrayList<>();
 
     } else {
       this.clauses.clear();
@@ -106,7 +106,7 @@ public abstract class QueryNodeImpl implements QueryNode, Cloneable {
       child.removeFromParent();
     }
     
-    ArrayList<QueryNode> existingChildren = new ArrayList<QueryNode>(getChildren());
+    ArrayList<QueryNode> existingChildren = new ArrayList<>(getChildren());
     for (QueryNode existingChild : existingChildren) {
       existingChild.removeFromParent();
     }
@@ -124,11 +124,11 @@ public abstract class QueryNodeImpl implements QueryNode, Cloneable {
     clone.isLeaf = this.isLeaf;
 
     // Reset all tags
-    clone.tags = new Hashtable<String, Object>();
+    clone.tags = new Hashtable<>();
 
     // copy children
     if (this.clauses != null) {
-      List<QueryNode> localClauses = new ArrayList<QueryNode>();
+      List<QueryNode> localClauses = new ArrayList<>();
       for (QueryNode clause : this.clauses) {
         localClauses.add(clause.cloneTree());
       }
@@ -156,7 +156,7 @@ public abstract class QueryNodeImpl implements QueryNode, Cloneable {
     if (isLeaf() || this.clauses == null) {
       return null;
     }
-    return new ArrayList<QueryNode>(this.clauses);
+    return new ArrayList<>(this.clauses);
   }
 
   @Override
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/processors/QueryNodeProcessorImpl.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/processors/QueryNodeProcessorImpl.java
index 965fb51..92f0d6e 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/processors/QueryNodeProcessorImpl.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/processors/QueryNodeProcessorImpl.java
@@ -73,7 +73,7 @@ import org.apache.lucene.queryparser.flexible.core.nodes.QueryNode;
  */
 public abstract class QueryNodeProcessorImpl implements QueryNodeProcessor {
 
-  private ArrayList<ChildrenList> childrenListPool = new ArrayList<ChildrenList>();
+  private ArrayList<ChildrenList> childrenListPool = new ArrayList<>();
 
   private QueryConfigHandler queryConfig;
 
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/processors/QueryNodeProcessorPipeline.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/processors/QueryNodeProcessorPipeline.java
index 9b15207..b439798 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/processors/QueryNodeProcessorPipeline.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/processors/QueryNodeProcessorPipeline.java
@@ -40,7 +40,7 @@ import org.apache.lucene.queryparser.flexible.core.nodes.QueryNode;
 public class QueryNodeProcessorPipeline implements QueryNodeProcessor,
     List<QueryNodeProcessor> {
 
-  private LinkedList<QueryNodeProcessor> processors = new LinkedList<QueryNodeProcessor>();
+  private LinkedList<QueryNodeProcessor> processors = new LinkedList<>();
 
   private QueryConfigHandler queryConfig;
 
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/util/QueryNodeOperation.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/util/QueryNodeOperation.java
index 1c91c30..fc1f097 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/util/QueryNodeOperation.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/util/QueryNodeOperation.java
@@ -66,7 +66,7 @@ public final class QueryNodeOperation {
       QueryNode result = null;
       switch (op) {
       case NONE:
-        List<QueryNode> children = new ArrayList<QueryNode>();
+        List<QueryNode> children = new ArrayList<>();
         children.add(q1.cloneTree());
         children.add(q2.cloneTree());
         result = new AndQueryNode(children);
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/messages/NLS.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/messages/NLS.java
index df2f5f9..06b65ca 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/messages/NLS.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/messages/NLS.java
@@ -46,7 +46,7 @@ import java.util.ResourceBundle;
 public class NLS {
 
   private static Map<String, Class<? extends NLS>> bundles = 
-    new HashMap<String, Class<? extends NLS>>(0);
+    new HashMap<>(0);
 
   protected NLS() {
     // Do not instantiate
@@ -129,7 +129,7 @@ public class NLS {
 
     // build a map of field names to Field objects
     final int len = fieldArray.length;
-    Map<String, Field> fields = new HashMap<String, Field>(len * 2);
+    Map<String, Field> fields = new HashMap<>(len * 2);
     for (int i = 0; i < len; i++) {
       fields.put(fieldArray[i].getName(), fieldArray[i]);
       loadfieldValue(fieldArray[i], isFieldAccessible, clazz);
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/precedence/processors/BooleanModifiersQueryNodeProcessor.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/precedence/processors/BooleanModifiersQueryNodeProcessor.java
index fe4fb13..85cdae9 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/precedence/processors/BooleanModifiersQueryNodeProcessor.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/precedence/processors/BooleanModifiersQueryNodeProcessor.java
@@ -46,7 +46,7 @@ import org.apache.lucene.queryparser.flexible.core.nodes.ModifierQueryNode.Modif
  */
 public class BooleanModifiersQueryNodeProcessor extends QueryNodeProcessorImpl {
 
-  private ArrayList<QueryNode> childrenBuffer = new ArrayList<QueryNode>();
+  private ArrayList<QueryNode> childrenBuffer = new ArrayList<>();
 
   private Boolean usingAnd = false;
 
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/MultiPhraseQueryNodeBuilder.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/MultiPhraseQueryNodeBuilder.java
index 89923d7..7a695d9 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/MultiPhraseQueryNodeBuilder.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/MultiPhraseQueryNodeBuilder.java
@@ -49,7 +49,7 @@ public class MultiPhraseQueryNodeBuilder implements StandardQueryBuilder {
     List<QueryNode> children = phraseNode.getChildren();
 
     if (children != null) {
-      TreeMap<Integer, List<Term>> positionTermMap = new TreeMap<Integer, List<Term>>();
+      TreeMap<Integer, List<Term>> positionTermMap = new TreeMap<>();
 
       for (QueryNode child : children) {
         FieldQueryNode termNode = (FieldQueryNode) child;
@@ -59,7 +59,7 @@ public class MultiPhraseQueryNodeBuilder implements StandardQueryBuilder {
             .getPositionIncrement());
 
         if (termList == null) {
-          termList = new LinkedList<Term>();
+          termList = new LinkedList<>();
           positionTermMap.put(termNode.getPositionIncrement(), termList);
 
         }
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/nodes/AbstractRangeQueryNode.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/nodes/AbstractRangeQueryNode.java
index 935db13..8e83fe0 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/nodes/AbstractRangeQueryNode.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/nodes/AbstractRangeQueryNode.java
@@ -163,7 +163,7 @@ public class AbstractRangeQueryNode<T extends FieldValuePairQueryNode<?>>
       this.lowerInclusive = lowerInclusive;
       this.upperInclusive = upperInclusive;
       
-      ArrayList<QueryNode> children = new ArrayList<QueryNode>(2);
+      ArrayList<QueryNode> children = new ArrayList<>(2);
       children.add(lower);
       children.add(upper);
       
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/StandardSyntaxParser.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/StandardSyntaxParser.java
index 4844ec8..5001f63 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/StandardSyntaxParser.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/StandardSyntaxParser.java
@@ -185,7 +185,7 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
       }
       c = DisjQuery(field);
        if (clauses == null) {
-           clauses = new Vector<QueryNode>();
+           clauses = new Vector<>();
            clauses.addElement(first);
         }
         clauses.addElement(c);
@@ -215,7 +215,7 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
       jj_consume_token(OR);
       c = ConjQuery(field);
      if (clauses == null) {
-         clauses = new Vector<QueryNode>();
+         clauses = new Vector<>();
          clauses.addElement(first);
      }
      clauses.addElement(c);
@@ -245,7 +245,7 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
       jj_consume_token(AND);
       c = ModClause(field);
      if (clauses == null) {
-         clauses = new Vector<QueryNode>();
+         clauses = new Vector<>();
          clauses.addElement(first);
      }
      clauses.addElement(c);
@@ -986,7 +986,7 @@ public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserC
       return (jj_ntk = jj_nt.kind);
   }
 
-  private java.util.List<int[]> jj_expentries = new java.util.ArrayList<int[]>();
+  private java.util.List<int[]> jj_expentries = new java.util.ArrayList<>();
   private int[] jj_expentry;
   private int jj_kind = -1;
   private int[] jj_lasttokens = new int[100];
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AnalyzerQueryNodeProcessor.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AnalyzerQueryNodeProcessor.java
index c0b3e72..ef68066 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AnalyzerQueryNodeProcessor.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AnalyzerQueryNodeProcessor.java
@@ -193,7 +193,7 @@ public class AnalyzerQueryNodeProcessor extends QueryNodeProcessorImpl {
             
             if (positionCount == 1) {
               // simple case: only one position, with synonyms
-              LinkedList<QueryNode> children = new LinkedList<QueryNode>();
+              LinkedList<QueryNode> children = new LinkedList<>();
               
               for (int i = 0; i < numTokens; i++) {
                 String term = null;
@@ -257,7 +257,7 @@ public class AnalyzerQueryNodeProcessor extends QueryNodeProcessorImpl {
             // phrase query:
             MultiPhraseQueryNode mpq = new MultiPhraseQueryNode();
   
-            List<FieldQueryNode> multiTerms = new ArrayList<FieldQueryNode>();
+            List<FieldQueryNode> multiTerms = new ArrayList<>();
             int position = -1;
             int i = 0;
             int termGroupCount = 0;
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/BooleanQuery2ModifierNodeProcessor.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/BooleanQuery2ModifierNodeProcessor.java
index 7f4d727..db8f308 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/BooleanQuery2ModifierNodeProcessor.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/BooleanQuery2ModifierNodeProcessor.java
@@ -68,7 +68,7 @@ public class BooleanQuery2ModifierNodeProcessor implements QueryNodeProcessor {
   
   QueryConfigHandler queryConfigHandler;
   
-  private final ArrayList<QueryNode> childrenBuffer = new ArrayList<QueryNode>();
+  private final ArrayList<QueryNode> childrenBuffer = new ArrayList<>();
   
   private Boolean usingAnd = false;
   
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/GroupQueryNodeProcessor.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/GroupQueryNodeProcessor.java
index 38deff0..ef1228a 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/GroupQueryNodeProcessor.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/GroupQueryNodeProcessor.java
@@ -80,7 +80,7 @@ public class GroupQueryNodeProcessor implements QueryNodeProcessor {
       queryTree = ((GroupQueryNode) queryTree).getChild();
     }
 
-    this.queryNodeList = new ArrayList<QueryNode>();
+    this.queryNodeList = new ArrayList<>();
     this.latestNodeVerified = false;
     readTree(queryTree);
 
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/MultiFieldQueryNodeProcessor.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/MultiFieldQueryNodeProcessor.java
index 99a121c..44f74e8 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/MultiFieldQueryNodeProcessor.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/MultiFieldQueryNodeProcessor.java
@@ -93,7 +93,7 @@ public class MultiFieldQueryNodeProcessor extends QueryNodeProcessorImpl {
             return fieldNode;
 
           } else {
-            LinkedList<QueryNode> children = new LinkedList<QueryNode>();
+            LinkedList<QueryNode> children = new LinkedList<>();
             children.add(fieldNode);
 
             for (int i = 1; i < fields.length; i++) {
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/RemoveEmptyNonLeafQueryNodeProcessor.java lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/RemoveEmptyNonLeafQueryNodeProcessor.java
index addee51..0365d35 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/RemoveEmptyNonLeafQueryNodeProcessor.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/RemoveEmptyNonLeafQueryNodeProcessor.java
@@ -42,7 +42,7 @@ import org.apache.lucene.queryparser.flexible.core.processors.QueryNodeProcessor
 public class RemoveEmptyNonLeafQueryNodeProcessor extends
     QueryNodeProcessorImpl {
 
-  private LinkedList<QueryNode> childrenBuffer = new LinkedList<QueryNode>();
+  private LinkedList<QueryNode> childrenBuffer = new LinkedList<>();
 
   public RemoveEmptyNonLeafQueryNodeProcessor() {
     // empty constructor
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/QueryParser.java lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/QueryParser.java
index 41ed459..1c429d6 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/QueryParser.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/QueryParser.java
@@ -185,7 +185,7 @@ public class QueryParser implements QueryParserConstants {
           fieldName = jj_consume_token(TERM);
       jj_consume_token(COLON);
       if (fieldNames == null) {
-        fieldNames = new ArrayList<String>();
+        fieldNames = new ArrayList<>();
       }
       fieldNames.add(fieldName.image);
     }
@@ -211,7 +211,7 @@ public class QueryParser implements QueryParserConstants {
       oprt = jj_consume_token(OR);
                   /* keep only last used operator */
       if (queries == null) {
-        queries = new ArrayList<SrndQuery>();
+        queries = new ArrayList<>();
         queries.add(q);
       }
       q = AndQuery();
@@ -239,7 +239,7 @@ public class QueryParser implements QueryParserConstants {
       oprt = jj_consume_token(AND);
                    /* keep only last used operator */
       if (queries == null) {
-        queries = new ArrayList<SrndQuery>();
+        queries = new ArrayList<>();
         queries.add(q);
       }
       q = NotQuery();
@@ -267,7 +267,7 @@ public class QueryParser implements QueryParserConstants {
       oprt = jj_consume_token(NOT);
                     /* keep only last used operator */
       if (queries == null) {
-        queries = new ArrayList<SrndQuery>();
+        queries = new ArrayList<>();
         queries.add(q);
       }
       q = NQuery();
@@ -293,7 +293,7 @@ public class QueryParser implements QueryParserConstants {
         break label_5;
       }
       dt = jj_consume_token(N);
-      queries = new ArrayList<SrndQuery>();
+      queries = new ArrayList<>();
       queries.add(q); /* left associative */
 
       q = WQuery();
@@ -320,7 +320,7 @@ public class QueryParser implements QueryParserConstants {
         break label_6;
       }
       wt = jj_consume_token(W);
-      queries = new ArrayList<SrndQuery>();
+      queries = new ArrayList<>();
       queries.add(q); /* left associative */
 
       q = PrimaryQuery();
@@ -401,7 +401,7 @@ public class QueryParser implements QueryParserConstants {
 
   final public List<SrndQuery> FieldsQueryList() throws ParseException {
   SrndQuery q;
-  ArrayList<SrndQuery> queries = new ArrayList<SrndQuery>();
+  ArrayList<SrndQuery> queries = new ArrayList<>();
     jj_consume_token(LPAREN);
     q = FieldsQuery();
                      queries.add(q);
@@ -644,7 +644,7 @@ public class QueryParser implements QueryParserConstants {
       return (jj_ntk = jj_nt.kind);
   }
 
-  private java.util.List<int[]> jj_expentries = new java.util.ArrayList<int[]>();
+  private java.util.List<int[]> jj_expentries = new java.util.ArrayList<>();
   private int[] jj_expentry;
   private int jj_kind = -1;
   private int[] jj_lasttokens = new int[100];
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/ComposedQuery.java lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/ComposedQuery.java
index 1ec65c8..0874a1f 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/ComposedQuery.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/ComposedQuery.java
@@ -51,7 +51,7 @@ public abstract class ComposedQuery extends SrndQuery {
   public boolean isOperatorInfix() { return operatorInfix; } /* else prefix operator */
   
   public List<Query> makeLuceneSubQueriesField(String fn, BasicQueryFactory qf) {
-    List<Query> luceneSubQueries = new ArrayList<Query>();
+    List<Query> luceneSubQueries = new ArrayList<>();
     Iterator<SrndQuery> sqi = getSubQueriesIterator();
     while (sqi.hasNext()) {
       luceneSubQueries.add( (sqi.next()).makeLuceneQueryField(fn, qf));
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/FieldsQuery.java lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/FieldsQuery.java
index 7582711..e0dfe74 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/FieldsQuery.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/FieldsQuery.java
@@ -39,7 +39,7 @@ public class FieldsQuery extends SrndQuery { /* mostly untested */
   
   public FieldsQuery(SrndQuery q, String fieldName, char fieldOp) {
     this.q = q;
-    fieldNames = new ArrayList<String>();
+    fieldNames = new ArrayList<>();
     fieldNames.add(fieldName);
     this.fieldOp = fieldOp;
   }
@@ -53,7 +53,7 @@ public class FieldsQuery extends SrndQuery { /* mostly untested */
     if (fieldNames.size() == 1) { /* single field name: no new queries needed */
       return q.makeLuceneQueryFieldNoBoost(fieldNames.get(0), qf);
     } else { /* OR query over the fields */
-      List<SrndQuery> queries = new ArrayList<SrndQuery>();
+      List<SrndQuery> queries = new ArrayList<>();
       Iterator<String> fni = getFieldNames().listIterator();
       SrndQuery qc;
       while (fni.hasNext()) {
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SimpleTermRewriteQuery.java lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SimpleTermRewriteQuery.java
index c3529b6..f7255cb 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SimpleTermRewriteQuery.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SimpleTermRewriteQuery.java
@@ -35,7 +35,7 @@ class SimpleTermRewriteQuery extends RewriteQuery<SimpleTerm> {
 
   @Override
   public Query rewrite(IndexReader reader) throws IOException {
-    final List<Query> luceneSubQueries = new ArrayList<Query>();
+    final List<Query> luceneSubQueries = new ArrayList<>();
     srndQuery.visitMatchingTerms(reader, fieldName,
     new SimpleTerm.MatchingTermVisitor() {
       @Override
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SpanNearClauseFactory.java lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SpanNearClauseFactory.java
index ddbb61a..bbc7ad1 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SpanNearClauseFactory.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SpanNearClauseFactory.java
@@ -71,7 +71,7 @@ public class SpanNearClauseFactory { // FIXME: rename to SpanClauseFactory
   public SpanNearClauseFactory(IndexReader reader, String fieldName, BasicQueryFactory qf) {
     this.reader = reader;
     this.fieldName = fieldName;
-    this.weightBySpanQuery = new HashMap<SpanQuery, Float>(); 
+    this.weightBySpanQuery = new HashMap<>();
     this.qf = qf;
   }
   private IndexReader reader;
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/FilterBuilderFactory.java lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/FilterBuilderFactory.java
index 1d804e1..1ab1bb8 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/FilterBuilderFactory.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/FilterBuilderFactory.java
@@ -29,7 +29,7 @@ import java.util.HashMap;
  */
 public class FilterBuilderFactory implements FilterBuilder {
 
-  HashMap<String, FilterBuilder> builders = new HashMap<String, FilterBuilder>();
+  HashMap<String, FilterBuilder> builders = new HashMap<>();
 
   @Override
   public Filter getFilter(Element n) throws ParserException {
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryBuilderFactory.java lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryBuilderFactory.java
index 23cc6e8..94ab8f8 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryBuilderFactory.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryBuilderFactory.java
@@ -29,7 +29,7 @@ import java.util.HashMap;
  */
 public class QueryBuilderFactory implements QueryBuilder {
 
-  HashMap<String, QueryBuilder> builders = new HashMap<String, QueryBuilder>();
+  HashMap<String, QueryBuilder> builders = new HashMap<>();
 
   @Override
   public Query getQuery(Element n) throws ParserException {
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryTemplateManager.java lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryTemplateManager.java
index 4abc2fd..3abdd27 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryTemplateManager.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryTemplateManager.java
@@ -48,7 +48,7 @@ public class QueryTemplateManager {
   static final DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();
   static final TransformerFactory tFactory = TransformerFactory.newInstance();
 
-  HashMap<String, Templates> compiledTemplatesCache = new HashMap<String, Templates>();
+  HashMap<String, Templates> compiledTemplatesCache = new HashMap<>();
   Templates defaultCompiledTemplates = null;
 
 
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/CachedFilterBuilder.java lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/CachedFilterBuilder.java
index b101ad5..da01dc9 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/CachedFilterBuilder.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/CachedFilterBuilder.java
@@ -65,7 +65,7 @@ public class CachedFilterBuilder implements FilterBuilder {
     Element childElement = DOMUtils.getFirstChildOrFail(e);
 
     if (filterCache == null) {
-      filterCache = new LRUCache<Object, Filter>(cacheSize);
+      filterCache = new LRUCache<>(cacheSize);
     }
 
     // Test to see if child Element is a query or filter that needs to be
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/LikeThisQueryBuilder.java lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/LikeThisQueryBuilder.java
index 1156504..d63f61a 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/LikeThisQueryBuilder.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/LikeThisQueryBuilder.java
@@ -71,7 +71,7 @@ public class LikeThisQueryBuilder implements QueryBuilder {
     String stopWords = e.getAttribute("stopWords");
     Set<String> stopWordsSet = null;
     if ((stopWords != null) && (fields != null)) {
-      stopWordsSet = new HashSet<String>();
+      stopWordsSet = new HashSet<>();
       for (String field : fields) {
         try (TokenStream ts = analyzer.tokenStream(field, stopWords)) {
           CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanNearBuilder.java lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanNearBuilder.java
index 548844e..e26735f 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanNearBuilder.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanNearBuilder.java
@@ -42,7 +42,7 @@ public class SpanNearBuilder extends SpanBuilderBase {
     String slopString = DOMUtils.getAttributeOrFail(e, "slop");
     int slop = Integer.parseInt(slopString);
     boolean inOrder = DOMUtils.getAttribute(e, "inOrder", false);
-    List<SpanQuery> spans = new ArrayList<SpanQuery>();
+    List<SpanQuery> spans = new ArrayList<>();
     for (Node kid = e.getFirstChild(); kid != null; kid = kid.getNextSibling()) {
       if (kid.getNodeType() == Node.ELEMENT_NODE) {
         spans.add(factory.getSpanQuery((Element) kid));
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanOrBuilder.java lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanOrBuilder.java
index 54d0618..ce48d00 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanOrBuilder.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanOrBuilder.java
@@ -39,7 +39,7 @@ public class SpanOrBuilder extends SpanBuilderBase {
 
   @Override
   public SpanQuery getSpanQuery(Element e) throws ParserException {
-    List<SpanQuery> clausesList = new ArrayList<SpanQuery>();
+    List<SpanQuery> clausesList = new ArrayList<>();
     for (Node kid = e.getFirstChild(); kid != null; kid = kid.getNextSibling()) {
       if (kid.getNodeType() == Node.ELEMENT_NODE) {
         SpanQuery clause = factory.getSpanQuery((Element) kid);
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanOrTermsBuilder.java lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanOrTermsBuilder.java
index 5e316f5..e7978d1 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanOrTermsBuilder.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanOrTermsBuilder.java
@@ -49,7 +49,7 @@ public class SpanOrTermsBuilder extends SpanBuilderBase {
     String fieldName = DOMUtils.getAttributeWithInheritanceOrFail(e, "fieldName");
     String value = DOMUtils.getNonBlankTextOrFail(e);
 
-    List<SpanQuery> clausesList = new ArrayList<SpanQuery>();
+    List<SpanQuery> clausesList = new ArrayList<>();
 
     try (TokenStream ts = analyzer.tokenStream(fieldName, value)) {
       TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanQueryBuilderFactory.java lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanQueryBuilderFactory.java
index 4c7f200..69fd7ba 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanQueryBuilderFactory.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanQueryBuilderFactory.java
@@ -29,7 +29,7 @@ import java.util.Map;
  */
 public class SpanQueryBuilderFactory implements SpanQueryBuilder {
 
-  private final Map<String, SpanQueryBuilder> builders = new HashMap<String, SpanQueryBuilder>();
+  private final Map<String, SpanQueryBuilder> builders = new HashMap<>();
 
   @Override
   public Query getQuery(Element e) throws ParserException {
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/TermsFilterBuilder.java lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/TermsFilterBuilder.java
index 6b97f72..59f424d 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/TermsFilterBuilder.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/TermsFilterBuilder.java
@@ -50,7 +50,7 @@ public class TermsFilterBuilder implements FilterBuilder {
     */
   @Override
   public Filter getFilter(Element e) throws ParserException {
-    List<BytesRef> terms = new ArrayList<BytesRef>();
+    List<BytesRef> terms = new ArrayList<>();
     String text = DOMUtils.getNonBlankTextOrFail(e);
     String fieldName = DOMUtils.getAttributeWithInheritanceOrFail(e, "fieldName");
 
diff --git lucene/queryparser/src/test/org/apache/lucene/queryparser/analyzing/TestAnalyzingQueryParser.java lucene/queryparser/src/test/org/apache/lucene/queryparser/analyzing/TestAnalyzingQueryParser.java
index cf21eb2..126e07e 100644
--- lucene/queryparser/src/test/org/apache/lucene/queryparser/analyzing/TestAnalyzingQueryParser.java
+++ lucene/queryparser/src/test/org/apache/lucene/queryparser/analyzing/TestAnalyzingQueryParser.java
@@ -61,8 +61,8 @@ public class TestAnalyzingQueryParser extends LuceneTestCase {
   private String[] fuzzyInput;
   private String[] fuzzyExpected;
 
-  private Map<String, String> wildcardEscapeHits = new TreeMap<String, String>();
-  private Map<String, String> wildcardEscapeMisses = new TreeMap<String, String>();
+  private Map<String, String> wildcardEscapeHits = new TreeMap<>();
+  private Map<String, String> wildcardEscapeMisses = new TreeMap<>();
 
   @Override
   public void setUp() throws Exception {
diff --git lucene/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiFieldQueryParser.java lucene/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiFieldQueryParser.java
index 5a24316..38ba07a 100644
--- lucene/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiFieldQueryParser.java
+++ lucene/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiFieldQueryParser.java
@@ -132,7 +132,7 @@ public class TestMultiFieldQueryParser extends LuceneTestCase {
   }
   
   public void testBoostsSimple() throws Exception {
-      Map<String,Float> boosts = new HashMap<String,Float>();
+      Map<String,Float> boosts = new HashMap<>();
       boosts.put("b", Float.valueOf(5));
       boosts.put("t", Float.valueOf(10));
       String[] fields = {"b", "t"};
diff --git lucene/queryparser/src/test/org/apache/lucene/queryparser/complexPhrase/TestComplexPhraseQuery.java lucene/queryparser/src/test/org/apache/lucene/queryparser/complexPhrase/TestComplexPhraseQuery.java
index 37ec316..ec7c20f 100644
--- lucene/queryparser/src/test/org/apache/lucene/queryparser/complexPhrase/TestComplexPhraseQuery.java
+++ lucene/queryparser/src/test/org/apache/lucene/queryparser/complexPhrase/TestComplexPhraseQuery.java
@@ -91,7 +91,7 @@ public class TestComplexPhraseQuery extends LuceneTestCase {
 
     Query q = qp.parse(qString);
 
-    HashSet<String> expecteds = new HashSet<String>();
+    HashSet<String> expecteds = new HashSet<>();
     String[] vals = expectedVals.split(",");
     for (int i = 0; i < vals.length; i++) {
       if (vals[i].length() > 0)
diff --git lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/precedence/TestPrecedenceQueryParser.java lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/precedence/TestPrecedenceQueryParser.java
index 91cc1ae..a758a6f 100644
--- lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/precedence/TestPrecedenceQueryParser.java
+++ lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/precedence/TestPrecedenceQueryParser.java
@@ -431,7 +431,7 @@ public class TestPrecedenceQueryParser extends LuceneTestCase {
     final String hourField = "hour";
     PrecedenceQueryParser qp = new PrecedenceQueryParser(new MockAnalyzer(random()));
 
-    Map<CharSequence, DateTools.Resolution> fieldMap = new HashMap<CharSequence,DateTools.Resolution>();
+    Map<CharSequence, DateTools.Resolution> fieldMap = new HashMap<>();
     // set a field specific date resolution
     fieldMap.put(monthField, DateTools.Resolution.MONTH);
     qp.setDateResolution(fieldMap);
diff --git lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestMultiFieldQPHelper.java lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestMultiFieldQPHelper.java
index db15646..debb484 100644
--- lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestMultiFieldQPHelper.java
+++ lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestMultiFieldQPHelper.java
@@ -145,7 +145,7 @@ public class TestMultiFieldQPHelper extends LuceneTestCase {
   }
 
   public void testBoostsSimple() throws Exception {
-    Map<String,Float> boosts = new HashMap<String,Float>();
+    Map<String,Float> boosts = new HashMap<>();
     boosts.put("b", Float.valueOf(5));
     boosts.put("t", Float.valueOf(10));
     String[] fields = { "b", "t" };
diff --git lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestNumericQueryParser.java lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestNumericQueryParser.java
index a209ce2..7dc8aeb 100644
--- lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestNumericQueryParser.java
+++ lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestNumericQueryParser.java
@@ -103,7 +103,7 @@ public class TestNumericQueryParser extends LuceneTestCase {
     
     qp = new StandardQueryParser(ANALYZER);
     
-    final HashMap<String,Number> randomNumberMap = new HashMap<String,Number>();
+    final HashMap<String,Number> randomNumberMap = new HashMap<>();
     
     SimpleDateFormat dateFormat;
     long randomDate;
@@ -194,8 +194,8 @@ public class TestNumericQueryParser extends LuceneTestCase {
             .setMergePolicy(newLogMergePolicy()));
     
     Document doc = new Document();
-    HashMap<String,NumericConfig> numericConfigMap = new HashMap<String,NumericConfig>();
-    HashMap<String,Field> numericFieldMap = new HashMap<String,Field>();
+    HashMap<String,NumericConfig> numericConfigMap = new HashMap<>();
+    HashMap<String,Field> numericFieldMap = new HashMap<>();
     qp.setNumericConfigMap(numericConfigMap);
     
     for (NumericType type : NumericType.values()) {
diff --git lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestQPHelper.java lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestQPHelper.java
index 177e6e1..ab9d1dd 100644
--- lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestQPHelper.java
+++ lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/standard/TestQPHelper.java
@@ -741,7 +741,7 @@ public class TestQPHelper extends LuceneTestCase {
     final String hourField = "hour";
     StandardQueryParser qp = new StandardQueryParser();
 
-    Map<CharSequence, DateTools.Resolution> dateRes =  new HashMap<CharSequence, DateTools.Resolution>();
+    Map<CharSequence, DateTools.Resolution> dateRes =  new HashMap<>();
     
     // set a field specific date resolution    
     dateRes.put(monthField, DateTools.Resolution.MONTH);
diff --git lucene/replicator/src/java/org/apache/lucene/replicator/IndexAndTaxonomyRevision.java lucene/replicator/src/java/org/apache/lucene/replicator/IndexAndTaxonomyRevision.java
index 125a66c..dcecf9f 100644
--- lucene/replicator/src/java/org/apache/lucene/replicator/IndexAndTaxonomyRevision.java
+++ lucene/replicator/src/java/org/apache/lucene/replicator/IndexAndTaxonomyRevision.java
@@ -116,7 +116,7 @@ public class IndexAndTaxonomyRevision implements Revision {
   /** Returns a singleton map of the revision files from the given {@link IndexCommit}. */
   public static Map<String, List<RevisionFile>> revisionFiles(IndexCommit indexCommit, IndexCommit taxoCommit)
       throws IOException {
-    HashMap<String,List<RevisionFile>> files = new HashMap<String,List<RevisionFile>>();
+    HashMap<String,List<RevisionFile>> files = new HashMap<>();
     files.put(INDEX_SOURCE, IndexRevision.revisionFiles(indexCommit).values().iterator().next());
     files.put(TAXONOMY_SOURCE, IndexRevision.revisionFiles(taxoCommit).values().iterator().next());
     return files;
diff --git lucene/replicator/src/java/org/apache/lucene/replicator/IndexReplicationHandler.java lucene/replicator/src/java/org/apache/lucene/replicator/IndexReplicationHandler.java
index 325c96d..299a666 100644
--- lucene/replicator/src/java/org/apache/lucene/replicator/IndexReplicationHandler.java
+++ lucene/replicator/src/java/org/apache/lucene/replicator/IndexReplicationHandler.java
@@ -150,7 +150,7 @@ public class IndexReplicationHandler implements ReplicationHandler {
       // if there were any IO errors reading the expected commit point (i.e.
       // segments files mismatch), then ignore that commit either.
       if (commit != null && commit.getSegmentsFileName().equals(segmentsFile)) {
-        Set<String> commitFiles = new HashSet<String>();
+        Set<String> commitFiles = new HashSet<>();
         commitFiles.addAll(commit.getFileNames());
         commitFiles.add(IndexFileNames.SEGMENTS_GEN);
         Matcher matcher = IndexFileNames.CODEC_FILE_PATTERN.matcher("");
diff --git lucene/replicator/src/java/org/apache/lucene/replicator/IndexRevision.java lucene/replicator/src/java/org/apache/lucene/replicator/IndexRevision.java
index d135a3d..c43d331 100644
--- lucene/replicator/src/java/org/apache/lucene/replicator/IndexRevision.java
+++ lucene/replicator/src/java/org/apache/lucene/replicator/IndexRevision.java
@@ -69,7 +69,7 @@ public class IndexRevision implements Revision {
   /** Returns a singleton map of the revision files from the given {@link IndexCommit}. */
   public static Map<String,List<RevisionFile>> revisionFiles(IndexCommit commit) throws IOException {
     Collection<String> commitFiles = commit.getFileNames();
-    List<RevisionFile> revisionFiles = new ArrayList<RevisionFile>(commitFiles.size());
+    List<RevisionFile> revisionFiles = new ArrayList<>(commitFiles.size());
     String segmentsFile = commit.getSegmentsFileName();
     Directory dir = commit.getDirectory();
     
diff --git lucene/replicator/src/java/org/apache/lucene/replicator/LocalReplicator.java lucene/replicator/src/java/org/apache/lucene/replicator/LocalReplicator.java
index 4ab746c..8dd9087 100644
--- lucene/replicator/src/java/org/apache/lucene/replicator/LocalReplicator.java
+++ lucene/replicator/src/java/org/apache/lucene/replicator/LocalReplicator.java
@@ -108,11 +108,11 @@ public class LocalReplicator implements Replicator {
   private volatile boolean closed = false;
   
   private final AtomicInteger sessionToken = new AtomicInteger(0);
-  private final Map<String, ReplicationSession> sessions = new HashMap<String, ReplicationSession>();
+  private final Map<String, ReplicationSession> sessions = new HashMap<>();
   
   private void checkExpiredSessions() throws IOException {
     // make a "to-delete" list so we don't risk deleting from the map while iterating it
-    final ArrayList<ReplicationSession> toExpire = new ArrayList<ReplicationSession>();
+    final ArrayList<ReplicationSession> toExpire = new ArrayList<>();
     for (ReplicationSession token : sessions.values()) {
       if (token.isExpired(expirationThresholdMilllis)) {
         toExpire.add(token);
diff --git lucene/replicator/src/java/org/apache/lucene/replicator/ReplicationClient.java lucene/replicator/src/java/org/apache/lucene/replicator/ReplicationClient.java
index 9397351..b747853 100644
--- lucene/replicator/src/java/org/apache/lucene/replicator/ReplicationClient.java
+++ lucene/replicator/src/java/org/apache/lucene/replicator/ReplicationClient.java
@@ -188,8 +188,8 @@ public class ReplicationClient implements Closeable {
   
   private void doUpdate() throws IOException {
     SessionToken session = null;
-    final Map<String,Directory> sourceDirectory = new HashMap<String,Directory>();
-    final Map<String,List<String>> copiedFiles = new HashMap<String,List<String>>();
+    final Map<String,Directory> sourceDirectory = new HashMap<>();
+    final Map<String,List<String>> copiedFiles = new HashMap<>();
     boolean notify = false;
     try {
       final String version = handler.currentVersion();
@@ -209,7 +209,7 @@ public class ReplicationClient implements Closeable {
         String source = e.getKey();
         Directory dir = factory.getDirectory(session.id, source);
         sourceDirectory.put(source, dir);
-        List<String> cpFiles = new ArrayList<String>();
+        List<String> cpFiles = new ArrayList<>();
         copiedFiles.put(source, cpFiles);
         for (RevisionFile file : e.getValue()) {
           if (closed) {
@@ -298,16 +298,16 @@ public class ReplicationClient implements Closeable {
       return newRevisionFiles;
     }
     
-    Map<String,List<RevisionFile>> requiredFiles = new HashMap<String,List<RevisionFile>>();
+    Map<String,List<RevisionFile>> requiredFiles = new HashMap<>();
     for (Entry<String,List<RevisionFile>> e : handlerRevisionFiles.entrySet()) {
       // put the handler files in a Set, for faster contains() checks later
-      Set<String> handlerFiles = new HashSet<String>();
+      Set<String> handlerFiles = new HashSet<>();
       for (RevisionFile file : e.getValue()) {
         handlerFiles.add(file.fileName);
       }
       
       // make sure to preserve revisionFiles order
-      ArrayList<RevisionFile> res = new ArrayList<RevisionFile>();
+      ArrayList<RevisionFile> res = new ArrayList<>();
       String source = e.getKey();
       assert newRevisionFiles.containsKey(source) : "source not found in newRevisionFiles: " + newRevisionFiles;
       for (RevisionFile file : newRevisionFiles.get(source)) {
diff --git lucene/replicator/src/java/org/apache/lucene/replicator/SessionToken.java lucene/replicator/src/java/org/apache/lucene/replicator/SessionToken.java
index 90b6e41..955736e 100644
--- lucene/replicator/src/java/org/apache/lucene/replicator/SessionToken.java
+++ lucene/replicator/src/java/org/apache/lucene/replicator/SessionToken.java
@@ -60,12 +60,12 @@ public final class SessionToken {
   public SessionToken(DataInput in) throws IOException {
     this.id = in.readUTF();
     this.version = in.readUTF();
-    this.sourceFiles = new HashMap<String,List<RevisionFile>>();
+    this.sourceFiles = new HashMap<>();
     int numSources = in.readInt();
     while (numSources > 0) {
       String source = in.readUTF();
       int numFiles = in.readInt();
-      List<RevisionFile> files = new ArrayList<RevisionFile>(numFiles);
+      List<RevisionFile> files = new ArrayList<>(numFiles);
       for (int i = 0; i < numFiles; i++) {
         String fileName = in.readUTF();
         RevisionFile file = new RevisionFile(fileName);
diff --git lucene/replicator/src/java/org/apache/lucene/replicator/http/ReplicationService.java lucene/replicator/src/java/org/apache/lucene/replicator/http/ReplicationService.java
index 4f62490..3815f8d 100644
--- lucene/replicator/src/java/org/apache/lucene/replicator/http/ReplicationService.java
+++ lucene/replicator/src/java/org/apache/lucene/replicator/http/ReplicationService.java
@@ -106,7 +106,7 @@ public class ReplicationService {
     // than using String.split() since the latter may return empty elements in
     // the array
     StringTokenizer stok = new StringTokenizer(path.substring(startIdx), "/");
-    ArrayList<String> elements = new ArrayList<String>();
+    ArrayList<String> elements = new ArrayList<>();
     while (stok.hasMoreTokens()) {
       elements.add(stok.nextToken());
     }
diff --git lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery.java lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery.java
index 6d9f5a3..845ac62 100644
--- lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery.java
+++ lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery.java
@@ -60,7 +60,7 @@ public class FuzzyLikeThisQuery extends Query
     // provided to TermQuery, so that the general idea is agnostic to any scoring system...
     static TFIDFSimilarity sim=new DefaultSimilarity();
     Query rewrittenQuery=null;
-    ArrayList<FieldVals> fieldVals=new ArrayList<FieldVals>();
+    ArrayList<FieldVals> fieldVals=new ArrayList<>();
     Analyzer analyzer;
     
     ScoreTermQueue q;
@@ -201,7 +201,7 @@ public class FuzzyLikeThisQuery extends Query
       CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
 
       int corpusNumDocs = reader.numDocs();
-      HashSet<String> processedTerms = new HashSet<String>();
+      HashSet<String> processedTerms = new HashSet<>();
       ts.reset();
       while (ts.incrementToken()) {
         String term = termAtt.toString();
@@ -277,7 +277,7 @@ public class FuzzyLikeThisQuery extends Query
         //create BooleanQueries to hold the variants for each token/field pair and ensure it
         // has no coord factor
         //Step 1: sort the termqueries by term/field
-        HashMap<Term,ArrayList<ScoreTerm>> variantQueries=new HashMap<Term,ArrayList<ScoreTerm>>();
+        HashMap<Term,ArrayList<ScoreTerm>> variantQueries=new HashMap<>();
         int size = q.size();
         for(int i = 0; i < size; i++)
         {
@@ -285,7 +285,7 @@ public class FuzzyLikeThisQuery extends Query
           ArrayList<ScoreTerm> l= variantQueries.get(st.fuzziedSourceTerm);
           if(l==null)
           {
-              l=new ArrayList<ScoreTerm>();
+              l=new ArrayList<>();
               variantQueries.put(st.fuzziedSourceTerm,l);
           }
           l.add(st);
diff --git lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/DuplicateFilterTest.java lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/DuplicateFilterTest.java
index 260e85b..aa8cb87 100644
--- lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/DuplicateFilterTest.java
+++ lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/DuplicateFilterTest.java
@@ -83,7 +83,7 @@ public class DuplicateFilterTest extends LuceneTestCase {
 
   public void testDefaultFilter() throws Throwable {
     DuplicateFilter df = new DuplicateFilter(KEY_FIELD);
-    HashSet<String> results = new HashSet<String>();
+    HashSet<String> results = new HashSet<>();
     ScoreDoc[] hits = searcher.search(tq, df, 1000).scoreDocs;
 
     for (ScoreDoc hit : hits) {
@@ -95,7 +95,7 @@ public class DuplicateFilterTest extends LuceneTestCase {
   }
 
   public void testNoFilter() throws Throwable {
-    HashSet<String> results = new HashSet<String>();
+    HashSet<String> results = new HashSet<>();
     ScoreDoc[] hits = searcher.search(tq, null, 1000).scoreDocs;
     assertTrue("Default searching should have found some matches", hits.length > 0);
     boolean dupsFound = false;
@@ -113,7 +113,7 @@ public class DuplicateFilterTest extends LuceneTestCase {
   public void testFastFilter() throws Throwable {
     DuplicateFilter df = new DuplicateFilter(KEY_FIELD);
     df.setProcessingMode(DuplicateFilter.ProcessingMode.PM_FAST_INVALIDATION);
-    HashSet<String> results = new HashSet<String>();
+    HashSet<String> results = new HashSet<>();
     ScoreDoc[] hits = searcher.search(tq, df, 1000).scoreDocs;
     assertTrue("Filtered searching should have found some matches", hits.length > 0);
 
diff --git lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/FuzzyLikeThisQueryTest.java lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/FuzzyLikeThisQueryTest.java
index 2b7070a..edfc963 100644
--- lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/FuzzyLikeThisQueryTest.java
+++ lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/FuzzyLikeThisQueryTest.java
@@ -81,7 +81,7 @@ public class FuzzyLikeThisQueryTest extends LuceneTestCase {
     FuzzyLikeThisQuery flt = new FuzzyLikeThisQuery(10, analyzer);
     flt.addTerms("smith", "name", 0.3f, 1);
     Query q = flt.rewrite(searcher.getIndexReader());
-    HashSet<Term> queryTerms = new HashSet<Term>();
+    HashSet<Term> queryTerms = new HashSet<>();
     q.extractTerms(queryTerms);
     assertTrue("Should have variant smythe", queryTerms.contains(new Term("name", "smythe")));
     assertTrue("Should have variant smith", queryTerms.contains(new Term("name", "smith")));
@@ -98,7 +98,7 @@ public class FuzzyLikeThisQueryTest extends LuceneTestCase {
     FuzzyLikeThisQuery flt = new FuzzyLikeThisQuery(10, analyzer);
     flt.addTerms("jonathin smoth", "name", 0.3f, 1);
     Query q = flt.rewrite(searcher.getIndexReader());
-    HashSet<Term> queryTerms = new HashSet<Term>();
+    HashSet<Term> queryTerms = new HashSet<>();
     q.extractTerms(queryTerms);
     assertTrue("Should have variant jonathan", queryTerms.contains(new Term("name", "jonathan")));
     assertTrue("Should have variant smith", queryTerms.contains(new Term("name", "smith")));
@@ -116,7 +116,7 @@ public class FuzzyLikeThisQueryTest extends LuceneTestCase {
     flt.addTerms("jonathin smoth", "this field does not exist", 0.3f, 1);
     // don't fail here just because the field doesn't exits
     Query q = flt.rewrite(searcher.getIndexReader());
-    HashSet<Term> queryTerms = new HashSet<Term>();
+    HashSet<Term> queryTerms = new HashSet<>();
     q.extractTerms(queryTerms);
     assertTrue("Should have variant jonathan", queryTerms.contains(new Term("name", "jonathan")));
     assertTrue("Should have variant smith", queryTerms.contains(new Term("name", "smith")));
@@ -133,7 +133,7 @@ public class FuzzyLikeThisQueryTest extends LuceneTestCase {
     FuzzyLikeThisQuery flt = new FuzzyLikeThisQuery(10, analyzer);
     flt.addTerms("fernando smith", "name", 0.3f, 1);
     Query q = flt.rewrite(searcher.getIndexReader());
-    HashSet<Term> queryTerms = new HashSet<Term>();
+    HashSet<Term> queryTerms = new HashSet<>();
     q.extractTerms(queryTerms);
     assertTrue("Should have variant smith", queryTerms.contains(new Term("name", "smith")));
     TopDocs topDocs = searcher.search(flt, 1);
diff --git lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestRegexQuery.java lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestRegexQuery.java
index 7ae01e2..a2e517b 100644
--- lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestRegexQuery.java
+++ lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestRegexQuery.java
@@ -73,8 +73,8 @@ public class TestRegexQuery extends LuceneTestCase {
   }
 
   private int  spanRegexQueryNrHits(String regex1, String regex2, int slop, boolean ordered) throws Exception {
-    SpanQuery srq1 = new SpanMultiTermQueryWrapper<RegexQuery>(new RegexQuery(newTerm(regex1)));
-    SpanQuery srq2 = new SpanMultiTermQueryWrapper<RegexQuery>(new RegexQuery(newTerm(regex2)));
+    SpanQuery srq1 = new SpanMultiTermQueryWrapper<>(new RegexQuery(newTerm(regex1)));
+    SpanQuery srq2 = new SpanMultiTermQueryWrapper<>(new RegexQuery(newTerm(regex2)));
     SpanNearQuery query = new SpanNearQuery( new SpanQuery[]{srq1, srq2}, slop, ordered);
 
     return searcher.search(query, null, 1000).totalHits;
diff --git lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestSpanRegexQuery.java lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestSpanRegexQuery.java
index 36e3a0c..0680686 100644
--- lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestSpanRegexQuery.java
+++ lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestSpanRegexQuery.java
@@ -70,7 +70,7 @@ public class TestSpanRegexQuery extends LuceneTestCase {
 
     IndexReader reader = DirectoryReader.open(directory);
     IndexSearcher searcher = newSearcher(reader);
-    SpanQuery srq = new SpanMultiTermQueryWrapper<RegexQuery>(new RegexQuery(new Term("field", "aut.*")));
+    SpanQuery srq = new SpanMultiTermQueryWrapper<>(new RegexQuery(new Term("field", "aut.*")));
     SpanFirstQuery sfq = new SpanFirstQuery(srq, 1);
     // SpanNearQuery query = new SpanNearQuery(new SpanQuery[] {srq, stq}, 6,
     // true);
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/prefix/PrefixTreeStrategy.java lucene/spatial/src/java/org/apache/lucene/spatial/prefix/PrefixTreeStrategy.java
index 297889c..31b9a85 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/prefix/PrefixTreeStrategy.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/prefix/PrefixTreeStrategy.java
@@ -77,7 +77,7 @@ import java.util.concurrent.ConcurrentHashMap;
  */
 public abstract class PrefixTreeStrategy extends SpatialStrategy {
   protected final SpatialPrefixTree grid;
-  private final Map<String, PointPrefixTreeFieldCacheProvider> provider = new ConcurrentHashMap<String, PointPrefixTreeFieldCacheProvider>();
+  private final Map<String, PointPrefixTreeFieldCacheProvider> provider = new ConcurrentHashMap<>();
   protected final boolean simplifyIndexedCells;
   protected int defaultFieldValuesArrayLen = 2;
   protected double distErrPct = SpatialArgs.DEFAULT_DISTERRPCT;// [ 0 TO 0.5 ]
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/Cell.java lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/Cell.java
index b5bcb3e..592f815 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/Cell.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/Cell.java
@@ -174,7 +174,7 @@ public abstract class Cell implements Comparable<Cell> {
     }
 
     //TODO change API to return a filtering iterator
-    List<Cell> copy = new ArrayList<Cell>(cells.size());
+    List<Cell> copy = new ArrayList<>(cells.size());
     for (Cell cell : cells) {
       SpatialRelation rel = cell.getShape().relate(shapeFilter);
       if (rel == SpatialRelation.DISJOINT)
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/GeohashPrefixTree.java lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/GeohashPrefixTree.java
index 7085213..5f2ca6d 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/GeohashPrefixTree.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/GeohashPrefixTree.java
@@ -111,7 +111,7 @@ public class GeohashPrefixTree extends SpatialPrefixTree {
     @Override
     public Collection<Cell> getSubCells() {
       String[] hashes = GeohashUtils.getSubGeohashes(getGeohash());//sorted
-      List<Cell> cells = new ArrayList<Cell>(hashes.length);
+      List<Cell> cells = new ArrayList<>(hashes.length);
       for (String hash : hashes) {
         cells.add(new GhCell(hash));
       }
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/QuadPrefixTree.java lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/QuadPrefixTree.java
index 6330605..d2e16a1 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/QuadPrefixTree.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/QuadPrefixTree.java
@@ -141,7 +141,7 @@ public class QuadPrefixTree extends SpatialPrefixTree {
 
   @Override
   public Cell getCell(Point p, int level) {
-    List<Cell> cells = new ArrayList<Cell>(1);
+    List<Cell> cells = new ArrayList<>(1);
     build(xmid, ymid, 0, cells, new StringBuilder(), ctx.makePoint(p.getX(),p.getY()), level);
     return cells.get(0);//note cells could be longer if p on edge
   }
@@ -240,7 +240,7 @@ public class QuadPrefixTree extends SpatialPrefixTree {
 
     @Override
     public Collection<Cell> getSubCells() {
-      List<Cell> cells = new ArrayList<Cell>(4);
+      List<Cell> cells = new ArrayList<>(4);
       cells.add(new QuadCell(getTokenString()+"A"));
       cells.add(new QuadCell(getTokenString()+"B"));
       cells.add(new QuadCell(getTokenString()+"C"));
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/SpatialPrefixTree.java lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/SpatialPrefixTree.java
index 1b8261a..1fbd396 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/SpatialPrefixTree.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/SpatialPrefixTree.java
@@ -164,7 +164,7 @@ public abstract class SpatialPrefixTree {
     if (shape instanceof Point) {
       return getCells((Point) shape, detailLevel, inclParents);
     }
-    List<Cell> cells = new ArrayList<Cell>(inclParents ? 4096 : 2048);
+    List<Cell> cells = new ArrayList<>(inclParents ? 4096 : 2048);
     recursiveGetCells(getWorldCell(), shape, detailLevel, inclParents, simplify, cells);
     return cells;
   }
@@ -226,7 +226,7 @@ public abstract class SpatialPrefixTree {
 
     String endToken = cell.getTokenString();
     assert endToken.length() == detailLevel;
-    List<Cell> cells = new ArrayList<Cell>(detailLevel);
+    List<Cell> cells = new ArrayList<>(detailLevel);
     for (int i = 1; i < detailLevel; i++) {
       cells.add(getCell(endToken.substring(0, i)));
     }
@@ -238,7 +238,7 @@ public abstract class SpatialPrefixTree {
    * Will add the trailing leaf byte for leaves. This isn't particularly efficient.
    */
   public static List<String> cellsToTokenStrings(Collection<Cell> cells) {
-    List<String> tokens = new ArrayList<String>((cells.size()));
+    List<String> tokens = new ArrayList<>((cells.size()));
     for (Cell cell : cells) {
       final String token = cell.getTokenString();
       if (cell.isLeaf()) {
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/query/SpatialArgsParser.java lucene/spatial/src/java/org/apache/lucene/spatial/query/SpatialArgsParser.java
index 4ebf2e8..363ee13 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/query/SpatialArgsParser.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/query/SpatialArgsParser.java
@@ -129,7 +129,7 @@ public class SpatialArgsParser {
   /** Parses "a=b c=d f" (whitespace separated) into name-value pairs. If there
    * is no '=' as in 'f' above then it's short for f=f. */
   protected static Map<String, String> parseMap(String body) {
-    Map<String, String> map = new HashMap<String, String>();
+    Map<String, String> map = new HashMap<>();
     StringTokenizer st = new StringTokenizer(body, " \n\t");
     while (st.hasMoreTokens()) {
       String a = st.nextToken();
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/query/SpatialOperation.java lucene/spatial/src/java/org/apache/lucene/spatial/query/SpatialOperation.java
index 7e772a9..7166649 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/query/SpatialOperation.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/query/SpatialOperation.java
@@ -40,8 +40,8 @@ import java.util.Map;
  */
 public abstract class SpatialOperation implements Serializable {
   // Private registry
-  private static final Map<String, SpatialOperation> registry = new HashMap<String, SpatialOperation>();
-  private static final List<SpatialOperation> list = new ArrayList<SpatialOperation>();
+  private static final Map<String, SpatialOperation> registry = new HashMap<>();
+  private static final List<SpatialOperation> list = new ArrayList<>();
 
   // Geometry Operations
 
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/util/CachingDoubleValueSource.java lucene/spatial/src/java/org/apache/lucene/spatial/util/CachingDoubleValueSource.java
index fa9fb85..dd2b411 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/util/CachingDoubleValueSource.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/util/CachingDoubleValueSource.java
@@ -38,7 +38,7 @@ public class CachingDoubleValueSource extends ValueSource {
   public CachingDoubleValueSource( ValueSource source )
   {
     this.source = source;
-    cache = new HashMap<Integer, Double>();
+    cache = new HashMap<>();
   }
 
   @Override
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCache.java lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCache.java
index 92c30b6..07e1ab1 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCache.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCache.java
@@ -44,7 +44,7 @@ public class ShapeFieldCache<T extends Shape> {
   public void add( int docid, T s ) {
     List<T> list = cache[docid];
     if( list == null ) {
-      list = cache[docid] = new ArrayList<T>(defaultLength);
+      list = cache[docid] = new ArrayList<>(defaultLength);
     }
     list.add( s );
   }
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCacheProvider.java lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCacheProvider.java
index c78ddd9..9458ff1 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCacheProvider.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCacheProvider.java
@@ -39,7 +39,7 @@ public abstract class ShapeFieldCacheProvider<T extends Shape> {
   private Logger log = Logger.getLogger(getClass().getName());
 
   // it may be a List<T> or T
-  WeakHashMap<IndexReader, ShapeFieldCache<T>> sidx = new WeakHashMap<IndexReader, ShapeFieldCache<T>>();
+  WeakHashMap<IndexReader, ShapeFieldCache<T>> sidx = new WeakHashMap<>();
 
   protected final int defaultSize;
   protected final String shapeField;
@@ -59,7 +59,7 @@ public abstract class ShapeFieldCacheProvider<T extends Shape> {
     long startTime = System.currentTimeMillis();
 
     log.fine("Building Cache [" + reader.maxDoc() + "]");
-    idx = new ShapeFieldCache<T>(reader.maxDoc(),defaultSize);
+    idx = new ShapeFieldCache<>(reader.maxDoc(),defaultSize);
     int count = 0;
     DocsEnum docs = null;
     Terms terms = reader.terms(shapeField);
diff --git lucene/spatial/src/test/org/apache/lucene/spatial/DistanceStrategyTest.java lucene/spatial/src/test/org/apache/lucene/spatial/DistanceStrategyTest.java
index 30bad06..9f52dc2 100644
--- lucene/spatial/src/test/org/apache/lucene/spatial/DistanceStrategyTest.java
+++ lucene/spatial/src/test/org/apache/lucene/spatial/DistanceStrategyTest.java
@@ -42,7 +42,7 @@ public class DistanceStrategyTest extends StrategyTestCase {
 
   @ParametersFactory
   public static Iterable<Object[]> parameters() {
-    List<Object[]> ctorArgs = new ArrayList<Object[]>();
+    List<Object[]> ctorArgs = new ArrayList<>();
 
     SpatialContext ctx = SpatialContext.GEO;
     SpatialPrefixTree grid;
diff --git lucene/spatial/src/test/org/apache/lucene/spatial/PortedSolr3Test.java lucene/spatial/src/test/org/apache/lucene/spatial/PortedSolr3Test.java
index 6619d17..cc3fb02 100644
--- lucene/spatial/src/test/org/apache/lucene/spatial/PortedSolr3Test.java
+++ lucene/spatial/src/test/org/apache/lucene/spatial/PortedSolr3Test.java
@@ -48,7 +48,7 @@ public class PortedSolr3Test extends StrategyTestCase {
 
   @ParametersFactory
   public static Iterable<Object[]> parameters() {
-    List<Object[]> ctorArgs = new ArrayList<Object[]>();
+    List<Object[]> ctorArgs = new ArrayList<>();
 
     SpatialContext ctx = SpatialContext.GEO;
     SpatialPrefixTree grid;
@@ -176,7 +176,7 @@ public class PortedSolr3Test extends StrategyTestCase {
     SearchResults results = executeQuery(query, 100);
     assertEquals(""+shape,assertNumFound,results.numFound);
     if (assertIds != null) {
-      Set<Integer> resultIds = new HashSet<Integer>();
+      Set<Integer> resultIds = new HashSet<>();
       for (SearchResult result : results.results) {
         resultIds.add(Integer.valueOf(result.document.get("id")));
       }
diff --git lucene/spatial/src/test/org/apache/lucene/spatial/QueryEqualsHashCodeTest.java lucene/spatial/src/test/org/apache/lucene/spatial/QueryEqualsHashCodeTest.java
index 913e88f..3198e1b 100644
--- lucene/spatial/src/test/org/apache/lucene/spatial/QueryEqualsHashCodeTest.java
+++ lucene/spatial/src/test/org/apache/lucene/spatial/QueryEqualsHashCodeTest.java
@@ -45,7 +45,7 @@ public class QueryEqualsHashCodeTest extends LuceneTestCase {
     final SpatialPrefixTree gridQuad = new QuadPrefixTree(ctx,10);
     final SpatialPrefixTree gridGeohash = new GeohashPrefixTree(ctx,10);
 
-    Collection<SpatialStrategy> strategies = new ArrayList<SpatialStrategy>();
+    Collection<SpatialStrategy> strategies = new ArrayList<>();
     strategies.add(new RecursivePrefixTreeStrategy(gridGeohash, "recursive_geohash"));
     strategies.add(new TermQueryPrefixTreeStrategy(gridQuad, "termquery_quad"));
     strategies.add(new PointVectorStrategy(ctx, "pointvector"));
diff --git lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestCase.java lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestCase.java
index 51e4bc6..603f05e 100644
--- lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestCase.java
+++ lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestCase.java
@@ -119,7 +119,7 @@ public abstract class SpatialTestCase extends LuceneTestCase {
     try {
       TopDocs topDocs = indexSearcher.search(query, numDocs);
 
-      List<SearchResult> results = new ArrayList<SearchResult>();
+      List<SearchResult> results = new ArrayList<>();
       for (ScoreDoc scoreDoc : topDocs.scoreDocs) {
         results.add(new SearchResult(scoreDoc.score, indexSearcher.doc(scoreDoc.doc)));
       }
diff --git lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestData.java lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestData.java
index 8087c92..eeaea30 100644
--- lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestData.java
+++ lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestData.java
@@ -42,7 +42,7 @@ public class SpatialTestData {
    * The stream is closed.
    */
   public static Iterator<SpatialTestData> getTestData(InputStream in, SpatialContext ctx) throws IOException {
-    List<SpatialTestData> results = new ArrayList<SpatialTestData>();
+    List<SpatialTestData> results = new ArrayList<>();
     BufferedReader bufInput = new BufferedReader(new InputStreamReader(in,"UTF-8"));
     try {
       String line;
diff --git lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestQuery.java lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestQuery.java
index 81705ec..4ccf469c 100644
--- lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestQuery.java
+++ lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestQuery.java
@@ -38,7 +38,7 @@ public class SpatialTestQuery {
   public String line;
   public int lineNumber = -1;
   public SpatialArgs args;
-  public List<String> ids = new ArrayList<String>();
+  public List<String> ids = new ArrayList<>();
 
   /**
    * Get Test Queries.  The InputStream is closed.
@@ -49,7 +49,7 @@ public class SpatialTestQuery {
       final String name,
       final InputStream in ) throws IOException {
 
-    List<SpatialTestQuery> results = new ArrayList<SpatialTestQuery>();
+    List<SpatialTestQuery> results = new ArrayList<>();
 
     BufferedReader bufInput = new BufferedReader(new InputStreamReader(in,"UTF-8"));
     try {
diff --git lucene/spatial/src/test/org/apache/lucene/spatial/StrategyTestCase.java lucene/spatial/src/test/org/apache/lucene/spatial/StrategyTestCase.java
index 9cdab4e..52b0882 100644
--- lucene/spatial/src/test/org/apache/lucene/spatial/StrategyTestCase.java
+++ lucene/spatial/src/test/org/apache/lucene/spatial/StrategyTestCase.java
@@ -89,7 +89,7 @@ public abstract class StrategyTestCase extends SpatialTestCase {
   }
 
   protected List<Document> getDocuments(Iterator<SpatialTestData> sampleData) {
-    List<Document> documents = new ArrayList<Document>();
+    List<Document> documents = new ArrayList<>();
     while (sampleData.hasNext()) {
       SpatialTestData data = sampleData.next();
       Document document = new Document();
@@ -161,7 +161,7 @@ public abstract class StrategyTestCase extends SpatialTestCase {
     } else {
       // We are looking at how the results overlap
       if (concern.resultsAreSuperset) {
-        Set<String> found = new HashSet<String>();
+        Set<String> found = new HashSet<>();
         for (SearchResult r : got.results) {
           found.add(r.document.get("id"));
         }
@@ -171,7 +171,7 @@ public abstract class StrategyTestCase extends SpatialTestCase {
           }
         }
       } else {
-        List<String> found = new ArrayList<String>();
+        List<String> found = new ArrayList<>();
         for (SearchResult r : got.results) {
           found.add(r.document.get("id"));
         }
@@ -237,7 +237,7 @@ public abstract class StrategyTestCase extends SpatialTestCase {
   protected void assertOperation(Map<String,Shape> indexedDocs,
                                  SpatialOperation operation, Shape queryShape) {
     //Generate truth via brute force
-    Set<String> expectedIds = new HashSet<String>();
+    Set<String> expectedIds = new HashSet<>();
     for (Map.Entry<String, Shape> stringShapeEntry : indexedDocs.entrySet()) {
       if (operation.evaluate(stringShapeEntry.getValue(), queryShape))
         expectedIds.add(stringShapeEntry.getKey());
@@ -245,7 +245,7 @@ public abstract class StrategyTestCase extends SpatialTestCase {
 
     SpatialTestQuery testQuery = new SpatialTestQuery();
     testQuery.args = new SpatialArgs(operation, queryShape);
-    testQuery.ids = new ArrayList<String>(expectedIds);
+    testQuery.ids = new ArrayList<>(expectedIds);
     runTestQuery(SpatialMatchConcern.FILTER, testQuery);
   }
 
diff --git lucene/spatial/src/test/org/apache/lucene/spatial/TestTestFramework.java lucene/spatial/src/test/org/apache/lucene/spatial/TestTestFramework.java
index b33a24d..82c5aa8 100644
--- lucene/spatial/src/test/org/apache/lucene/spatial/TestTestFramework.java
+++ lucene/spatial/src/test/org/apache/lucene/spatial/TestTestFramework.java
@@ -45,7 +45,7 @@ public class TestTestFramework extends LuceneTestCase {
     SpatialContext ctx = SpatialContext.GEO;
     Iterator<SpatialTestQuery> iter = SpatialTestQuery.getTestQueries(
         new SpatialArgsParser(), ctx, name, in );//closes the InputStream
-    List<SpatialTestQuery> tests = new ArrayList<SpatialTestQuery>();
+    List<SpatialTestQuery> tests = new ArrayList<>();
     while( iter.hasNext() ) {
       tests.add( iter.next() );
     }
diff --git lucene/spatial/src/test/org/apache/lucene/spatial/prefix/JtsPolygonTest.java lucene/spatial/src/test/org/apache/lucene/spatial/prefix/JtsPolygonTest.java
index 1eccf87..6dbaa9b 100644
--- lucene/spatial/src/test/org/apache/lucene/spatial/prefix/JtsPolygonTest.java
+++ lucene/spatial/src/test/org/apache/lucene/spatial/prefix/JtsPolygonTest.java
@@ -44,7 +44,7 @@ public class JtsPolygonTest extends StrategyTestCase {
 
   public JtsPolygonTest() {
     try {
-      HashMap<String, String> args = new HashMap<String, String>();
+      HashMap<String, String> args = new HashMap<>();
       args.put("spatialContextFactory",
           "com.spatial4j.core.context.jts.JtsSpatialContextFactory");
       ctx = SpatialContextFactory.makeSpatialContext(args, getClass().getClassLoader());
diff --git lucene/spatial/src/test/org/apache/lucene/spatial/prefix/SpatialOpRecursivePrefixTreeTest.java lucene/spatial/src/test/org/apache/lucene/spatial/prefix/SpatialOpRecursivePrefixTreeTest.java
index 7dcdac4..070c63d 100644
--- lucene/spatial/src/test/org/apache/lucene/spatial/prefix/SpatialOpRecursivePrefixTreeTest.java
+++ lucene/spatial/src/test/org/apache/lucene/spatial/prefix/SpatialOpRecursivePrefixTreeTest.java
@@ -199,8 +199,8 @@ public class SpatialOpRecursivePrefixTreeTest extends StrategyTestCase {
 
     final boolean biasContains = (operation == SpatialOperation.Contains);
 
-    Map<String, Shape> indexedShapes = new LinkedHashMap<String, Shape>();
-    Map<String, Shape> indexedShapesGS = new LinkedHashMap<String, Shape>();//grid snapped
+    Map<String, Shape> indexedShapes = new LinkedHashMap<>();
+    Map<String, Shape> indexedShapesGS = new LinkedHashMap<>();//grid snapped
     final int numIndexedShapes = randomIntBetween(1, 6);
     for (int i = 0; i < numIndexedShapes; i++) {
       String id = "" + i;
@@ -257,8 +257,8 @@ public class SpatialOpRecursivePrefixTreeTest extends StrategyTestCase {
       // We ensure true-positive matches (if the predicate on the raw shapes match
       //  then the search should find those same matches).
       // approximations, false-positive matches
-      Set<String> expectedIds = new LinkedHashSet<String>();//true-positives
-      Set<String> secondaryIds = new LinkedHashSet<String>();//false-positives (unless disjoint)
+      Set<String> expectedIds = new LinkedHashSet<>();//true-positives
+      Set<String> secondaryIds = new LinkedHashSet<>();//false-positives (unless disjoint)
       for (Map.Entry<String, Shape> entry : indexedShapes.entrySet()) {
         String id = entry.getKey();
         Shape indexedShapeCompare = entry.getValue();
@@ -297,7 +297,7 @@ public class SpatialOpRecursivePrefixTreeTest extends StrategyTestCase {
       SpatialArgs args = new SpatialArgs(operation, queryShape);
       Query query = strategy.makeQuery(args);
       SearchResults got = executeQuery(query, 100);
-      Set<String> remainingExpectedIds = new LinkedHashSet<String>(expectedIds);
+      Set<String> remainingExpectedIds = new LinkedHashSet<>(expectedIds);
       for (SearchResult result : got.results) {
         String id = result.getId();
         boolean removed = remainingExpectedIds.remove(id);
@@ -334,11 +334,11 @@ public class SpatialOpRecursivePrefixTreeTest extends StrategyTestCase {
     List<Cell> cells = grid.getCells(snapMe, detailLevel, false, true);
 
     //calc bounding box of cells.
-    List<Shape> cellShapes = new ArrayList<Shape>(cells.size());
+    List<Shape> cellShapes = new ArrayList<>(cells.size());
     for (Cell cell : cells) {
       cellShapes.add(cell.getShape());
     }
-    return new ShapeCollection<Shape>(cellShapes, ctx).getBoundingBox();
+    return new ShapeCollection<>(cellShapes, ctx).getBoundingBox();
   }
 
   /**
diff --git lucene/spatial/src/test/org/apache/lucene/spatial/prefix/TestRecursivePrefixTreeStrategy.java lucene/spatial/src/test/org/apache/lucene/spatial/prefix/TestRecursivePrefixTreeStrategy.java
index 4b52e3a..4bd7f77 100644
--- lucene/spatial/src/test/org/apache/lucene/spatial/prefix/TestRecursivePrefixTreeStrategy.java
+++ lucene/spatial/src/test/org/apache/lucene/spatial/prefix/TestRecursivePrefixTreeStrategy.java
@@ -123,7 +123,7 @@ public class TestRecursivePrefixTreeStrategy extends StrategyTestCase {
       for (double radiusDeg : radiusDegs) {
         //3. Index random points in this cluster circle
         deleteAll();
-        List<Point> points = new ArrayList<Point>();
+        List<Point> points = new ArrayList<>();
         for(int i = 0; i < 20; i++) {
           //Note that this will not result in randomly distributed points in the
           // circle, they will be concentrated towards the center a little. But
@@ -184,7 +184,7 @@ public class TestRecursivePrefixTreeStrategy extends StrategyTestCase {
     SearchResults got = executeQuery(strategy.makeQuery(args), 100);
     assertEquals("" + args, assertNumFound, got.numFound);
     if (assertIds != null) {
-      Set<Integer> gotIds = new HashSet<Integer>();
+      Set<Integer> gotIds = new HashSet<>();
       for (SearchResult result : got.results) {
         gotIds.add(Integer.valueOf(result.document.get("id")));
       }
diff --git lucene/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java lucene/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java
index 4df757b..adb453d 100644
--- lucene/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java
+++ lucene/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java
@@ -354,7 +354,7 @@ public class DirectSpellChecker {
     // try ed=1 first, in case we get lucky
     terms = suggestSimilar(term, inspections, ir, docfreq, 1, accuracy, spare);
     if (maxEdits > 1 && terms.size() < inspections) {
-      HashSet<ScoreTerm> moreTerms = new HashSet<ScoreTerm>();
+      HashSet<ScoreTerm> moreTerms = new HashSet<>();
       moreTerms.addAll(terms);
       moreTerms.addAll(suggestSimilar(term, inspections, ir, docfreq, maxEdits, accuracy, spare));
       terms = moreTerms;
@@ -409,7 +409,7 @@ public class DirectSpellChecker {
       return Collections.emptyList();
     }
     FuzzyTermsEnum e = new FuzzyTermsEnum(terms, atts, term, editDistance, Math.max(minPrefix, editDistance-1), true);
-    final PriorityQueue<ScoreTerm> stQueue = new PriorityQueue<ScoreTerm>();
+    final PriorityQueue<ScoreTerm> stQueue = new PriorityQueue<>();
     
     BytesRef queryTerm = new BytesRef(term.text());
     BytesRef candidateTerm;
diff --git lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
index e61a287..27e720e 100644
--- lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
+++ lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
@@ -498,7 +498,7 @@ public class SpellChecker implements java.io.Closeable {
       final Directory dir = this.spellIndex;
       final IndexWriter writer = new IndexWriter(dir, config);
       IndexSearcher indexSearcher = obtainSearcher();
-      final List<TermsEnum> termsEnums = new ArrayList<TermsEnum>();
+      final List<TermsEnum> termsEnums = new ArrayList<>();
 
       final IndexReader reader = searcher.getIndexReader();
       if (reader.maxDoc() > 0) {
diff --git lucene/suggest/src/java/org/apache/lucene/search/spell/WordBreakSpellChecker.java lucene/suggest/src/java/org/apache/lucene/search/spell/WordBreakSpellChecker.java
index 30f26c9..e4309a7 100644
--- lucene/suggest/src/java/org/apache/lucene/search/spell/WordBreakSpellChecker.java
+++ lucene/suggest/src/java/org/apache/lucene/search/spell/WordBreakSpellChecker.java
@@ -106,7 +106,7 @@ public class WordBreakSpellChecker {
     int queueInitialCapacity = maxSuggestions > 10 ? 10 : maxSuggestions;
     Comparator<SuggestWordArrayWrapper> queueComparator = sortMethod == BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY ? new LengthThenMaxFreqComparator()
         : new LengthThenSumFreqComparator();
-    Queue<SuggestWordArrayWrapper> suggestions = new PriorityQueue<SuggestWordArrayWrapper>(
+    Queue<SuggestWordArrayWrapper> suggestions = new PriorityQueue<>(
         queueInitialCapacity, queueComparator);
     
     int origFreq = ir.docFreq(term);
@@ -176,7 +176,7 @@ public class WordBreakSpellChecker {
     
     int queueInitialCapacity = maxSuggestions > 10 ? 10 : maxSuggestions;
     Comparator<CombineSuggestionWrapper> queueComparator = new CombinationsThenFreqComparator();
-    Queue<CombineSuggestionWrapper> suggestions = new PriorityQueue<CombineSuggestionWrapper>(
+    Queue<CombineSuggestionWrapper> suggestions = new PriorityQueue<>(
         queueInitialCapacity, queueComparator);
     
     int thisTimeEvaluations = 0;
diff --git lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java
index df3aa04..9333f7a 100644
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java
@@ -375,7 +375,7 @@ public class AnalyzingInfixSuggester extends Lookup implements Closeable {
     }
 
     BooleanQuery query;
-    Set<String> matchedTokens = new HashSet<String>();
+    Set<String> matchedTokens = new HashSet<>();
     String prefixToken = null;
 
     try (TokenStream ts = queryAnalyzer.tokenStream("", new StringReader(key.toString()))) {
@@ -386,7 +386,7 @@ public class AnalyzingInfixSuggester extends Lookup implements Closeable {
       String lastToken = null;
       query = new BooleanQuery();
       int maxEndOffset = -1;
-      matchedTokens = new HashSet<String>();
+      matchedTokens = new HashSet<>();
       while (ts.incrementToken()) {
         if (lastToken != null) {  
           matchedTokens.add(lastToken);
@@ -475,7 +475,7 @@ public class AnalyzingInfixSuggester extends Lookup implements Closeable {
     // This will just be null if app didn't pass payloads to build():
     // TODO: maybe just stored fields?  they compress...
     BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), "payloads");
-    List<LookupResult> results = new ArrayList<LookupResult>();
+    List<LookupResult> results = new ArrayList<>();
     BytesRef scratch = new BytesRef();
     for (int i=0;i<hits.scoreDocs.length;i++) {
       FieldDoc fd = (FieldDoc) hits.scoreDocs[i];
diff --git lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java
index 5dad351..2438ccc 100644
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java
@@ -273,7 +273,7 @@ public class AnalyzingSuggester extends Lookup {
     // make one pass:
     for(int stateNumber=states.length-1;stateNumber >=0;stateNumber--) {
       final State state = states[stateNumber];
-      List<Transition> newTransitions = new ArrayList<Transition>();
+      List<Transition> newTransitions = new ArrayList<>();
       for(Transition t : state.getTransitions()) {
         assert t.getMin() == t.getMax();
         if (t.getMin() == TokenStreamToAutomaton.POS_SEP) {
@@ -470,8 +470,8 @@ public class AnalyzingSuggester extends Lookup {
 
       reader = new OfflineSorter.ByteSequencesReader(tempSorted);
      
-      PairOutputs<Long,BytesRef> outputs = new PairOutputs<Long,BytesRef>(PositiveIntOutputs.getSingleton(), ByteSequenceOutputs.getSingleton());
-      Builder<Pair<Long,BytesRef>> builder = new Builder<Pair<Long,BytesRef>>(FST.INPUT_TYPE.BYTE1, outputs);
+      PairOutputs<Long,BytesRef> outputs = new PairOutputs<>(PositiveIntOutputs.getSingleton(), ByteSequenceOutputs.getSingleton());
+      Builder<Pair<Long,BytesRef>> builder = new Builder<>(FST.INPUT_TYPE.BYTE1, outputs);
 
       // Build FST:
       BytesRef previousAnalyzed = null;
@@ -484,7 +484,7 @@ public class AnalyzingSuggester extends Lookup {
       // still index the hightest-weight one).  We clear
       // this when we see a new analyzed form, so it cannot
       // grow unbounded (at most 256 entries):
-      Set<BytesRef> seenSurfaceForms = new HashSet<BytesRef>();
+      Set<BytesRef> seenSurfaceForms = new HashSet<>();
 
       int dedup = 0;
       while (reader.read(scratch)) {
@@ -587,7 +587,7 @@ public class AnalyzingSuggester extends Lookup {
   @Override
   public boolean load(DataInput input) throws IOException {
     count = input.readVLong();
-    this.fst = new FST<Pair<Long,BytesRef>>(input, new PairOutputs<Long,BytesRef>(PositiveIntOutputs.getSingleton(), ByteSequenceOutputs.getSingleton()));
+    this.fst = new FST<>(input, new PairOutputs<>(PositiveIntOutputs.getSingleton(), ByteSequenceOutputs.getSingleton()));
     maxAnalyzedPathsForOneInput = input.readVInt();
     hasPayloads = input.readByte() == 1;
     return true;
@@ -674,9 +674,9 @@ public class AnalyzingSuggester extends Lookup {
 
       BytesReader bytesReader = fst.getBytesReader();
 
-      FST.Arc<Pair<Long,BytesRef>> scratchArc = new FST.Arc<Pair<Long,BytesRef>>();
+      FST.Arc<Pair<Long,BytesRef>> scratchArc = new FST.Arc<>();
 
-      final List<LookupResult> results = new ArrayList<LookupResult>();
+      final List<LookupResult> results = new ArrayList<>();
 
       List<FSTUtil.Path<Pair<Long,BytesRef>>> prefixPaths = FSTUtil.intersectPrefixPaths(convertAutomaton(lookupAutomaton), fst);
 
@@ -694,7 +694,7 @@ public class AnalyzingSuggester extends Lookup {
         // Searcher just to find the single exact only
         // match, if present:
         Util.TopNSearcher<Pair<Long,BytesRef>> searcher;
-        searcher = new Util.TopNSearcher<Pair<Long,BytesRef>>(fst, count * maxSurfaceFormsPerAnalyzedForm, count * maxSurfaceFormsPerAnalyzedForm, weightComparator);
+        searcher = new Util.TopNSearcher<>(fst, count * maxSurfaceFormsPerAnalyzedForm, count * maxSurfaceFormsPerAnalyzedForm, weightComparator);
 
         // NOTE: we could almost get away with only using
         // the first start node.  The only catch is if
@@ -742,7 +742,7 @@ public class AnalyzingSuggester extends Lookup {
                                                             num - results.size(),
                                                             num * maxAnalyzedPathsForOneInput,
                                                             weightComparator) {
-        private final Set<BytesRef> seen = new HashSet<BytesRef>();
+        private final Set<BytesRef> seen = new HashSet<>();
 
         @Override
         protected boolean acceptResult(IntsRef input, Pair<Long,BytesRef> output) {
diff --git lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FSTUtil.java lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FSTUtil.java
index 3bb21c2..b9e886f 100644
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FSTUtil.java
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FSTUtil.java
@@ -70,13 +70,13 @@ public class FSTUtil {
   public static <T> List<Path<T>> intersectPrefixPaths(Automaton a, FST<T> fst)
       throws IOException {
     assert a.isDeterministic();
-    final List<Path<T>> queue = new ArrayList<Path<T>>();
-    final List<Path<T>> endNodes = new ArrayList<Path<T>>();
-    queue.add(new Path<T>(a.getInitialState(), fst
+    final List<Path<T>> queue = new ArrayList<>();
+    final List<Path<T>> endNodes = new ArrayList<>();
+    queue.add(new Path<>(a.getInitialState(), fst
         .getFirstArc(new FST.Arc<T>()), fst.outputs.getNoOutput(),
         new IntsRef()));
     
-    final FST.Arc<T> scratchArc = new FST.Arc<T>();
+    final FST.Arc<T> scratchArc = new FST.Arc<>();
     final FST.BytesReader fstReader = fst.getBytesReader();
     
     while (queue.size() != 0) {
@@ -100,7 +100,7 @@ public class FSTUtil {
             newInput.copyInts(currentInput);
             newInput.ints[currentInput.length] = t.getMin();
             newInput.length = currentInput.length + 1;
-            queue.add(new Path<T>(t.getDest(), new FST.Arc<T>()
+            queue.add(new Path<>(t.getDest(), new FST.Arc<T>()
                 .copyFrom(nextArc), fst.outputs
                 .add(path.output, nextArc.output), newInput));
           }
@@ -122,7 +122,7 @@ public class FSTUtil {
             newInput.copyInts(currentInput);
             newInput.ints[currentInput.length] = nextArc.label;
             newInput.length = currentInput.length + 1;
-            queue.add(new Path<T>(t.getDest(), new FST.Arc<T>()
+            queue.add(new Path<>(t.getDest(), new FST.Arc<T>()
                 .copyFrom(nextArc), fst.outputs
                 .add(path.output, nextArc.output), newInput));
             final int label = nextArc.label; // used in assert
diff --git lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester.java lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester.java
index f425235..d2c2f61 100644
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester.java
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester.java
@@ -343,7 +343,7 @@ public class FreeTextSuggester extends Lookup {
       TermsEnum termsEnum = terms.iterator(null);
 
       Outputs<Long> outputs = PositiveIntOutputs.getSingleton();
-      Builder<Long> builder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, outputs);
+      Builder<Long> builder = new Builder<>(FST.INPUT_TYPE.BYTE1, outputs);
 
       IntsRef scratchInts = new IntsRef();
       while (true) {
@@ -424,7 +424,7 @@ public class FreeTextSuggester extends Lookup {
     }
     totTokens = input.readVLong();
 
-    fst = new FST<Long>(input, PositiveIntOutputs.getSingleton());
+    fst = new FST<>(input, PositiveIntOutputs.getSingleton());
 
     return true;
   }
@@ -527,7 +527,7 @@ public class FreeTextSuggester extends Lookup {
         lastTokens[0] = new BytesRef();
       }
       
-      Arc<Long> arc = new Arc<Long>();
+      Arc<Long> arc = new Arc<>();
       
       BytesReader bytesReader = fst.getBytesReader();
       
@@ -535,12 +535,12 @@ public class FreeTextSuggester extends Lookup {
       // results, return that; else, fallback:
       double backoff = 1.0;
       
-      List<LookupResult> results = new ArrayList<LookupResult>(num);
+      List<LookupResult> results = new ArrayList<>(num);
       
       // We only add a given suffix once, from the highest
       // order model that saw it; for subsequent lower order
       // models we skip it:
-      final Set<BytesRef> seen = new HashSet<BytesRef>();
+      final Set<BytesRef> seen = new HashSet<>();
       
       for(int gram=grams-1;gram>=0;gram--) {
         BytesRef token = lastTokens[gram];
diff --git lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTCompletion.java lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTCompletion.java
index eea1042..51b8c18 100644
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTCompletion.java
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTCompletion.java
@@ -73,7 +73,7 @@ public class FSTCompletion {
    * An empty result. Keep this an {@link ArrayList} to keep all the returned
    * lists of single type (monomorphic calls).
    */
-  private static final ArrayList<Completion> EMPTY_RESULT = new ArrayList<Completion>();
+  private static final ArrayList<Completion> EMPTY_RESULT = new ArrayList<>();
 
   /**
    * Finite state automaton encoding all the lookup terms. See class notes for
@@ -137,12 +137,12 @@ public class FSTCompletion {
   @SuppressWarnings({"unchecked","rawtypes"})
   private static Arc<Object>[] cacheRootArcs(FST<Object> automaton) {
     try {
-      List<Arc<Object>> rootArcs = new ArrayList<Arc<Object>>();
-      Arc<Object> arc = automaton.getFirstArc(new Arc<Object>());
+      List<Arc<Object>> rootArcs = new ArrayList<>();
+      Arc<Object> arc = automaton.getFirstArc(new Arc<>());
       FST.BytesReader fstReader = automaton.getBytesReader();
       automaton.readFirstTargetArc(arc, arc, fstReader);
       while (true) {
-        rootArcs.add(new Arc<Object>().copyFrom(arc));
+        rootArcs.add(new Arc<>().copyFrom(arc));
         if (arc.isLast()) break;
         automaton.readNextArc(arc, fstReader);
       }
@@ -172,7 +172,7 @@ public class FSTCompletion {
       int rootArcIndex, BytesRef utf8) {
     // Get the UTF-8 bytes representation of the input key.
     try {
-      final FST.Arc<Object> scratch = new FST.Arc<Object>();
+      final FST.Arc<Object> scratch = new FST.Arc<>();
       FST.BytesReader fstReader = automaton.getBytesReader();
       for (; rootArcIndex < rootArcs.length; rootArcIndex++) {
         final FST.Arc<Object> rootArc = rootArcs[rootArcIndex];
@@ -261,12 +261,12 @@ public class FSTCompletion {
     // Don't overallocate the results buffers. This also serves the purpose of
     // allowing the user of this class to request all matches using Integer.MAX_VALUE as
     // the number of results.
-    final ArrayList<Completion> res = new ArrayList<Completion>(Math.min(10, num));
+    final ArrayList<Completion> res = new ArrayList<>(Math.min(10, num));
 
     final BytesRef output = BytesRef.deepCopyOf(key);
     for (int i = 0; i < rootArcs.length; i++) {
       final FST.Arc<Object> rootArc = rootArcs[i];
-      final FST.Arc<Object> arc = new FST.Arc<Object>().copyFrom(rootArc);
+      final FST.Arc<Object> arc = new FST.Arc<>().copyFrom(rootArc);
 
       // Descend into the automaton using the key as prefix.
       if (descendWithPrefix(arc, key)) {
@@ -370,7 +370,7 @@ public class FSTCompletion {
         if (res.size() >= num) return true;
       } else {
         int save = output.length;
-        if (collect(res, num, bucket, output, new Arc<Object>().copyFrom(arc))) {
+        if (collect(res, num, bucket, output, new Arc<>().copyFrom(arc))) {
           return true;
         }
         output.length = save;
diff --git lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTCompletionBuilder.java lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTCompletionBuilder.java
index 8029cfd97..dbfb3ce 100644
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTCompletionBuilder.java
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTCompletionBuilder.java
@@ -237,7 +237,7 @@ public class FSTCompletionBuilder {
     // Build the automaton.
     final Outputs<Object> outputs = NoOutputs.getSingleton();
     final Object empty = outputs.getNoOutput();
-    final Builder<Object> builder = new Builder<Object>(
+    final Builder<Object> builder = new Builder<>(
         FST.INPUT_TYPE.BYTE1, 0, 0, true, true, 
         shareMaxTailLength, outputs, null, false, 
         PackedInts.DEFAULT, true, 15);
diff --git lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTCompletionLookup.java lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTCompletionLookup.java
index 0f410c6..a24d316 100644
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTCompletionLookup.java
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTCompletionLookup.java
@@ -251,7 +251,7 @@ public class FSTCompletionLookup extends Lookup {
       completions = normalCompletion.lookup(key, num);
     }
     
-    final ArrayList<LookupResult> results = new ArrayList<LookupResult>(completions.size());
+    final ArrayList<LookupResult> results = new ArrayList<>(completions.size());
     CharsRef spare = new CharsRef();
     for (Completion c : completions) {
       spare.grow(c.utf8.length);
@@ -283,7 +283,7 @@ public class FSTCompletionLookup extends Lookup {
   @Override
   public synchronized boolean load(DataInput input) throws IOException {
     count = input.readVLong();
-    this.higherWeightsCompletion = new FSTCompletion(new FST<Object>(
+    this.higherWeightsCompletion = new FSTCompletion(new FST<>(
         input, NoOutputs.getSingleton()));
     this.normalCompletion = new FSTCompletion(
         higherWeightsCompletion.getFST(), false, exactMatchFirst);
diff --git lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/WFSTCompletionLookup.java lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/WFSTCompletionLookup.java
index d654f18..be4213d 100644
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/WFSTCompletionLookup.java
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/WFSTCompletionLookup.java
@@ -102,7 +102,7 @@ public class WFSTCompletionLookup extends Lookup {
     IntsRef scratchInts = new IntsRef();
     BytesRef previous = null;
     PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
-    Builder<Long> builder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, outputs);
+    Builder<Long> builder = new Builder<>(FST.INPUT_TYPE.BYTE1, outputs);
     while ((scratch = iter.next()) != null) {
       long cost = iter.weight();
       
@@ -134,7 +134,7 @@ public class WFSTCompletionLookup extends Lookup {
   @Override
   public boolean load(DataInput input) throws IOException {
     count = input.readVLong();
-    this.fst = new FST<Long>(input, PositiveIntOutputs.getSingleton());
+    this.fst = new FST<>(input, PositiveIntOutputs.getSingleton());
     return true;
   }
 
@@ -152,7 +152,7 @@ public class WFSTCompletionLookup extends Lookup {
 
     BytesRef scratch = new BytesRef(key);
     int prefixLength = scratch.length;
-    Arc<Long> arc = new Arc<Long>();
+    Arc<Long> arc = new Arc<>();
     
     // match the prefix portion exactly
     Long prefixOutput = null;
@@ -161,10 +161,10 @@ public class WFSTCompletionLookup extends Lookup {
     } catch (IOException bogus) { throw new RuntimeException(bogus); }
     
     if (prefixOutput == null) {
-      return Collections.<LookupResult>emptyList();
+      return Collections.emptyList();
     }
     
-    List<LookupResult> results = new ArrayList<LookupResult>(num);
+    List<LookupResult> results = new ArrayList<>(num);
     CharsRef spare = new CharsRef();
     if (exactFirst && arc.isFinal()) {
       spare.grow(scratch.length);
@@ -225,7 +225,7 @@ public class WFSTCompletionLookup extends Lookup {
     if (fst == null) {
       return null;
     }
-    Arc<Long> arc = new Arc<Long>();
+    Arc<Long> arc = new Arc<>();
     Long result = null;
     try {
       result = lookupPrefix(new BytesRef(key), arc);
diff --git lucene/suggest/src/java/org/apache/lucene/search/suggest/jaspell/JaspellLookup.java lucene/suggest/src/java/org/apache/lucene/search/suggest/jaspell/JaspellLookup.java
index 941df30..28cc39a 100644
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/jaspell/JaspellLookup.java
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/jaspell/JaspellLookup.java
@@ -96,7 +96,7 @@ public class JaspellLookup extends Lookup {
 
   @Override
   public List<LookupResult> lookup(CharSequence key, boolean onlyMorePopular, int num) {
-    List<LookupResult> res = new ArrayList<LookupResult>();
+    List<LookupResult> res = new ArrayList<>();
     List<String> list;
     int count = onlyMorePopular ? num * 2 : num;
     if (usePrefix) {
diff --git lucene/suggest/src/java/org/apache/lucene/search/suggest/jaspell/JaspellTernarySearchTrie.java lucene/suggest/src/java/org/apache/lucene/search/suggest/jaspell/JaspellTernarySearchTrie.java
index 474f5da..2bffc11 100644
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/jaspell/JaspellTernarySearchTrie.java
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/jaspell/JaspellTernarySearchTrie.java
@@ -682,7 +682,7 @@ public class JaspellTernarySearchTrie {
    *@return A <code>List</code> with the results
    */
   public List<String> matchPrefix(CharSequence prefix, int numReturnValues) {
-    Vector<String> sortKeysResult = new Vector<String>();
+    Vector<String> sortKeysResult = new Vector<>();
     TSTNode startNode = getNode(prefix);
     if (startNode == null) {
       return sortKeysResult;
diff --git lucene/suggest/src/java/org/apache/lucene/search/suggest/tst/TSTAutocomplete.java lucene/suggest/src/java/org/apache/lucene/search/suggest/tst/TSTAutocomplete.java
index d188402..ae454b7 100644
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/tst/TSTAutocomplete.java
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/tst/TSTAutocomplete.java
@@ -114,7 +114,7 @@ public class TSTAutocomplete {
           CharSequence s, int x) {
 
     TernaryTreeNode p = root;
-    ArrayList<TernaryTreeNode> suggest = new ArrayList<TernaryTreeNode>();
+    ArrayList<TernaryTreeNode> suggest = new ArrayList<>();
 
     while (p != null) {
       if (s.charAt(x) < p.splitchar) {
@@ -143,7 +143,7 @@ public class TSTAutocomplete {
     }
     p = p.eqKid;
 
-    Stack<TernaryTreeNode> st = new Stack<TernaryTreeNode>();
+    Stack<TernaryTreeNode> st = new Stack<>();
     st.push(p);
     while (!st.empty()) {
       TernaryTreeNode top = st.peek();
diff --git lucene/suggest/src/java/org/apache/lucene/search/suggest/tst/TSTLookup.java lucene/suggest/src/java/org/apache/lucene/search/suggest/tst/TSTLookup.java
index f47c808..4b4d61e 100644
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/tst/TSTLookup.java
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/tst/TSTLookup.java
@@ -60,8 +60,8 @@ public class TSTLookup extends Lookup {
     // make sure it's sorted and the comparator uses UTF16 sort order
     iterator = new SortedInputIterator(iterator, BytesRef.getUTF8SortedAsUTF16Comparator());
     count = 0;
-    ArrayList<String> tokens = new ArrayList<String>();
-    ArrayList<Number> vals = new ArrayList<Number>();
+    ArrayList<String> tokens = new ArrayList<>();
+    ArrayList<Number> vals = new ArrayList<>();
     BytesRef spare;
     CharsRef charsSpare = new CharsRef();
     while ((spare = iterator.next()) != null) {
@@ -119,7 +119,7 @@ public class TSTLookup extends Lookup {
   @Override
   public List<LookupResult> lookup(CharSequence key, boolean onlyMorePopular, int num) {
     List<TernaryTreeNode> list = autocomplete.prefixCompletion(root, key, 0);
-    List<LookupResult> res = new ArrayList<LookupResult>();
+    List<LookupResult> res = new ArrayList<>();
     if (list == null || list.size() == 0) {
       return res;
     }
diff --git lucene/suggest/src/test/org/apache/lucene/search/spell/TestWordBreakSpellChecker.java lucene/suggest/src/test/org/apache/lucene/search/spell/TestWordBreakSpellChecker.java
index e5dfc45..de79db1 100644
--- lucene/suggest/src/test/org/apache/lucene/search/spell/TestWordBreakSpellChecker.java
+++ lucene/suggest/src/test/org/apache/lucene/search/spell/TestWordBreakSpellChecker.java
@@ -272,8 +272,8 @@ public class TestWordBreakSpellChecker extends LuceneTestCase {
       writer = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(),
           MockTokenizer.WHITESPACE, false));
       int maxLength = TestUtil.nextInt(random(), 5, 50);
-      List<String> originals = new ArrayList<String>(numDocs);
-      List<String[]> breaks = new ArrayList<String[]>(numDocs);
+      List<String> originals = new ArrayList<>(numDocs);
+      List<String[]> breaks = new ArrayList<>(numDocs);
       for (int i = 0; i < numDocs; i++) {
         String orig = "";
         if (random().nextBoolean()) {
diff --git lucene/suggest/src/test/org/apache/lucene/search/suggest/DocumentDictionaryTest.java lucene/suggest/src/test/org/apache/lucene/search/suggest/DocumentDictionaryTest.java
index c0a5185..b78c856 100644
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/DocumentDictionaryTest.java
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/DocumentDictionaryTest.java
@@ -95,7 +95,7 @@ public class DocumentDictionaryTest extends LuceneTestCase {
       
       docs.put(term, doc);
     }
-    return new SimpleEntry<List<String>, Map<String, Document>>(invalidDocTerms, docs);
+    return new SimpleEntry<>(invalidDocTerms, docs);
   }
   
   @Test
diff --git lucene/suggest/src/test/org/apache/lucene/search/suggest/FileDictionaryTest.java lucene/suggest/src/test/org/apache/lucene/search/suggest/FileDictionaryTest.java
index ba4e675..2d880f5 100644
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/FileDictionaryTest.java
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/FileDictionaryTest.java
@@ -53,7 +53,7 @@ public class FileDictionaryTest extends LuceneTestCase {
       entryValues.add(payload);
     }
     sb.append("\n");
-    return new SimpleEntry<List<String>, String>(entryValues, sb.toString());
+    return new SimpleEntry<>(entryValues, sb.toString());
   }
   
   private Map.Entry<List<List<String>>,String> generateFileInput(int count, String fieldDelimiter, boolean hasWeights, boolean hasPayloads) {
@@ -68,7 +68,7 @@ public class FileDictionaryTest extends LuceneTestCase {
       entries.add(entrySet.getKey());
       sb.append(entrySet.getValue());
     }
-    return new SimpleEntry<List<List<String>>, String>(entries, sb.toString());
+    return new SimpleEntry<>(entries, sb.toString());
   }
   
   @Test
diff --git lucene/suggest/src/test/org/apache/lucene/search/suggest/LookupBenchmarkTest.java lucene/suggest/src/test/org/apache/lucene/search/suggest/LookupBenchmarkTest.java
index b2471ef..7d99824 100644
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/LookupBenchmarkTest.java
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/LookupBenchmarkTest.java
@@ -98,7 +98,7 @@ public class LookupBenchmarkTest extends LuceneTestCase {
    * Collect the multilingual input for benchmarks/ tests.
    */
   public static List<Input> readTop50KWiki() throws Exception {
-    List<Input> input = new ArrayList<Input>();
+    List<Input> input = new ArrayList<>();
     URL resource = LookupBenchmarkTest.class.getResource("Top50KWiki.utf8");
     assert resource != null : "Resource missing: Top50KWiki.utf8";
 
@@ -211,7 +211,7 @@ public class LookupBenchmarkTest extends LuceneTestCase {
     for (Class<? extends Lookup> cls : benchmarkClasses) {
       final Lookup lookup = buildLookup(cls, dictionaryInput);
 
-      final List<String> input = new ArrayList<String>(benchmarkInput.size());
+      final List<String> input = new ArrayList<>(benchmarkInput.size());
       for (Input tf : benchmarkInput) {
         String s = tf.term.utf8ToString();
         String sub = s.substring(0, Math.min(s.length(), 
@@ -246,7 +246,7 @@ public class LookupBenchmarkTest extends LuceneTestCase {
     final double NANOS_PER_MS = 1000000;
 
     try {
-      List<Double> times = new ArrayList<Double>();
+      List<Double> times = new ArrayList<>();
       for (int i = 0; i < warmup + rounds; i++) {
           final long start = System.nanoTime();
           guard = callable.call().intValue();
diff --git lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest.java lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest.java
index 147ee3b..86da9e0 100644
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest.java
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest.java
@@ -148,7 +148,7 @@ public class AnalyzingInfixSuggesterTest extends LuceneTestCase {
             CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
             OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);
             ts.reset();
-            List<LookupHighlightFragment> fragments = new ArrayList<LookupHighlightFragment>();
+            List<LookupHighlightFragment> fragments = new ArrayList<>();
             int upto = 0;
             while (ts.incrementToken()) {
               String token = termAtt.toString();
@@ -492,11 +492,11 @@ public class AnalyzingInfixSuggesterTest extends LuceneTestCase {
     int iters = atLeast(1000);
     int visibleUpto = 0;
 
-    Set<Long> usedWeights = new HashSet<Long>();
-    Set<String> usedKeys = new HashSet<String>();
+    Set<Long> usedWeights = new HashSet<>();
+    Set<String> usedKeys = new HashSet<>();
 
-    List<Input> inputs = new ArrayList<Input>();
-    List<Update> pendingUpdates = new ArrayList<Update>();
+    List<Input> inputs = new ArrayList<>();
+    List<Update> pendingUpdates = new ArrayList<>();
 
     for(int iter=0;iter<iters;iter++) {
       String text;
@@ -590,7 +590,7 @@ public class AnalyzingInfixSuggesterTest extends LuceneTestCase {
         }
 
         // Stupid slow but hopefully correct matching:
-        List<Input> expected = new ArrayList<Input>();
+        List<Input> expected = new ArrayList<>();
         for(int i=0;i<visibleUpto;i++) {
           Input input = inputs.get(i);
           String[] inputTerms = input.term.utf8ToString().split("\\s");
diff --git lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest.java lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest.java
index ce8fbea..43a979a 100644
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest.java
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest.java
@@ -648,9 +648,9 @@ public class AnalyzingSuggesterTest extends LuceneTestCase {
 
     int numQueries = atLeast(1000);
     
-    final List<TermFreq2> slowCompletor = new ArrayList<TermFreq2>();
-    final TreeSet<String> allPrefixes = new TreeSet<String>();
-    final Set<String> seen = new HashSet<String>();
+    final List<TermFreq2> slowCompletor = new ArrayList<>();
+    final TreeSet<String> allPrefixes = new TreeSet<>();
+    final Set<String> seen = new HashSet<>();
     
     boolean doPayloads = random().nextBoolean();
 
@@ -742,7 +742,7 @@ public class AnalyzingSuggesterTest extends LuceneTestCase {
     if (VERBOSE) {
       // Don't just sort original list, to avoid VERBOSE
       // altering the test:
-      List<TermFreq2> sorted = new ArrayList<TermFreq2>(slowCompletor);
+      List<TermFreq2> sorted = new ArrayList<>(slowCompletor);
       Collections.sort(sorted);
       for(TermFreq2 ent : sorted) {
         System.out.println("  surface='" + ent.surfaceForm + "' analyzed='" + ent.analyzedForm + "' weight=" + ent.weight);
@@ -768,7 +768,7 @@ public class AnalyzingSuggesterTest extends LuceneTestCase {
       List<LookupResult> r = suggester.lookup(TestUtil.stringToCharSequence(prefix, random()), false, topN);
 
       // 2. go thru whole set to find suggestions:
-      List<TermFreq2> matches = new ArrayList<TermFreq2>();
+      List<TermFreq2> matches = new ArrayList<>();
 
       // "Analyze" the key:
       String[] tokens = prefix.split(" ");
@@ -1194,7 +1194,7 @@ public class AnalyzingSuggesterTest extends LuceneTestCase {
 
   @SafeVarargs
   public final <T> Iterable<T> shuffle(T...values) {
-    final List<T> asList = new ArrayList<T>(values.length);
+    final List<T> asList = new ArrayList<>(values.length);
     for (T value : values) {
       asList.add(value);
     }
diff --git lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/FuzzySuggesterTest.java lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/FuzzySuggesterTest.java
index daac881..6a0a58c 100644
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/FuzzySuggesterTest.java
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/FuzzySuggesterTest.java
@@ -54,7 +54,7 @@ import org.apache.lucene.util.fst.Util;
 public class FuzzySuggesterTest extends LuceneTestCase {
   
   public void testRandomEdits() throws IOException {
-    List<Input> keys = new ArrayList<Input>();
+    List<Input> keys = new ArrayList<>();
     int numTerms = atLeast(100);
     for (int i = 0; i < numTerms; i++) {
       keys.add(new Input("boo" + TestUtil.randomSimpleString(random()), 1 + random().nextInt(100)));
@@ -75,7 +75,7 @@ public class FuzzySuggesterTest extends LuceneTestCase {
   }
   
   public void testNonLatinRandomEdits() throws IOException {
-    List<Input> keys = new ArrayList<Input>();
+    List<Input> keys = new ArrayList<>();
     int numTerms = atLeast(100);
     for (int i = 0; i < numTerms; i++) {
       keys.add(new Input("буу" + TestUtil.randomSimpleString(random()), 1 + random().nextInt(100)));
@@ -596,9 +596,9 @@ public class FuzzySuggesterTest extends LuceneTestCase {
 
     int numQueries = atLeast(100);
     
-    final List<TermFreqPayload2> slowCompletor = new ArrayList<TermFreqPayload2>();
-    final TreeSet<String> allPrefixes = new TreeSet<String>();
-    final Set<String> seen = new HashSet<String>();
+    final List<TermFreqPayload2> slowCompletor = new ArrayList<>();
+    final TreeSet<String> allPrefixes = new TreeSet<>();
+    final Set<String> seen = new HashSet<>();
     
     Input[] keys = new Input[numQueries];
 
@@ -674,7 +674,7 @@ public class FuzzySuggesterTest extends LuceneTestCase {
     if (VERBOSE) {
       // Don't just sort original list, to avoid VERBOSE
       // altering the test:
-      List<TermFreqPayload2> sorted = new ArrayList<TermFreqPayload2>(slowCompletor);
+      List<TermFreqPayload2> sorted = new ArrayList<>(slowCompletor);
       Collections.sort(sorted);
       for(TermFreqPayload2 ent : sorted) {
         System.out.println("  surface='" + ent.surfaceForm + " analyzed='" + ent.analyzedForm + "' weight=" + ent.weight);
@@ -696,7 +696,7 @@ public class FuzzySuggesterTest extends LuceneTestCase {
       List<LookupResult> r = suggester.lookup(TestUtil.stringToCharSequence(prefix, random()), false, topN);
 
       // 2. go thru whole set to find suggestions:
-      List<LookupResult> matches = new ArrayList<LookupResult>();
+      List<LookupResult> matches = new ArrayList<>();
 
       // "Analyze" the key:
       String[] tokens = prefix.split(" ");
@@ -929,8 +929,8 @@ public class FuzzySuggesterTest extends LuceneTestCase {
 
   public void testRandom2() throws Throwable {
     final int NUM = atLeast(200);
-    final List<Input> answers = new ArrayList<Input>();
-    final Set<String> seen = new HashSet<String>();
+    final List<Input> answers = new ArrayList<>();
+    final Set<String> seen = new HashSet<>();
     for(int i=0;i<NUM;i++) {
       final String s = randomSimpleString(8);
       if (!seen.contains(s)) {
@@ -1005,7 +1005,7 @@ public class FuzzySuggesterTest extends LuceneTestCase {
   }
 
   private List<LookupResult> slowFuzzyMatch(int prefixLen, int maxEdits, boolean allowTransposition, List<Input> answers, String frag) {
-    final List<LookupResult> results = new ArrayList<LookupResult>();
+    final List<LookupResult> results = new ArrayList<>();
     final int fragLen = frag.length();
     for(Input tf : answers) {
       //System.out.println("  check s=" + tf.term.utf8ToString());
diff --git lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester.java lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester.java
index 6ca17de..151d465 100644
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester.java
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester.java
@@ -293,7 +293,7 @@ public class TestFreeTextSuggester extends LuceneTestCase {
 
   public void testRandom() throws IOException {
     String[] terms = new String[TestUtil.nextInt(random(), 2, 10)];
-    Set<String> seen = new HashSet<String>();
+    Set<String> seen = new HashSet<>();
     while (seen.size() < terms.length) {
       String token = TestUtil.randomSimpleString(random(), 1, 5);
       if (!seen.contains(token)) {
@@ -367,12 +367,12 @@ public class TestFreeTextSuggester extends LuceneTestCase {
       });
 
     // Build inefficient but hopefully correct model:
-    List<Map<String,Integer>> gramCounts = new ArrayList<Map<String,Integer>>(grams);
+    List<Map<String,Integer>> gramCounts = new ArrayList<>(grams);
     for(int gram=0;gram<grams;gram++) {
       if (VERBOSE) {
         System.out.println("TEST: build model for gram=" + gram);
       }
-      Map<String,Integer> model = new HashMap<String,Integer>();
+      Map<String,Integer> model = new HashMap<>();
       gramCounts.add(model);
       for(String[] doc : docs) {
         for(int i=0;i<doc.length-gram;i++) {
@@ -429,9 +429,9 @@ public class TestFreeTextSuggester extends LuceneTestCase {
       }
 
       // Expected:
-      List<LookupResult> expected = new ArrayList<LookupResult>();
+      List<LookupResult> expected = new ArrayList<>();
       double backoff = 1.0;
-      seen = new HashSet<String>();
+      seen = new HashSet<>();
 
       if (VERBOSE) {
         System.out.println("  compute expected");
@@ -494,7 +494,7 @@ public class TestFreeTextSuggester extends LuceneTestCase {
         if (VERBOSE) {
           System.out.println("      find terms w/ prefix=" + tokens[tokens.length-1]);
         }
-        List<LookupResult> tmp = new ArrayList<LookupResult>();
+        List<LookupResult> tmp = new ArrayList<>();
         for(String term : terms) {
           if (term.startsWith(tokens[tokens.length-1])) {
             if (VERBOSE) {
@@ -587,7 +587,7 @@ public class TestFreeTextSuggester extends LuceneTestCase {
 
   @SafeVarargs
   private final <T> Iterable<T> shuffle(T...values) {
-    final List<T> asList = new ArrayList<T>(values.length);
+    final List<T> asList = new ArrayList<>(values.length);
     for (T value : values) {
       asList.add(value);
     }
diff --git lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/FSTCompletionTest.java lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/FSTCompletionTest.java
index dc3f24c..1b02794 100644
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/FSTCompletionTest.java
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/FSTCompletionTest.java
@@ -157,7 +157,7 @@ public class FSTCompletionTest extends LuceneTestCase {
     FSTCompletionLookup lookup = new FSTCompletionLookup(10, true);
     
     Random r = random();
-    List<Input> keys = new ArrayList<Input>();
+    List<Input> keys = new ArrayList<>();
     for (int i = 0; i < 5000; i++) {
       keys.add(new Input(TestUtil.randomSimpleString(r), -1));
     }
@@ -199,7 +199,7 @@ public class FSTCompletionTest extends LuceneTestCase {
   }
 
   public void testRandom() throws Exception {
-    List<Input> freqs = new ArrayList<Input>();
+    List<Input> freqs = new ArrayList<>();
     Random rnd = random();
     for (int i = 0; i < 2500 + rnd.nextInt(2500); i++) {
       int weight = rnd.nextInt(100); 
diff --git lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/WFSTCompletionTest.java lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/WFSTCompletionTest.java
index 9439a3a..81babbd 100644
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/WFSTCompletionTest.java
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/WFSTCompletionTest.java
@@ -128,8 +128,8 @@ public class WFSTCompletionTest extends LuceneTestCase {
   public void testRandom() throws Exception {
     int numWords = atLeast(1000);
     
-    final TreeMap<String,Long> slowCompletor = new TreeMap<String,Long>();
-    final TreeSet<String> allPrefixes = new TreeSet<String>();
+    final TreeMap<String,Long> slowCompletor = new TreeMap<>();
+    final TreeSet<String> allPrefixes = new TreeSet<>();
     
     Input[] keys = new Input[numWords];
     
@@ -163,7 +163,7 @@ public class WFSTCompletionTest extends LuceneTestCase {
       List<LookupResult> r = suggester.lookup(TestUtil.stringToCharSequence(prefix, random), false, topN);
 
       // 2. go thru whole treemap (slowCompletor) and check its actually the best suggestion
-      final List<LookupResult> matches = new ArrayList<LookupResult>();
+      final List<LookupResult> matches = new ArrayList<>();
 
       // TODO: could be faster... but its slowCompletor for a reason
       for (Map.Entry<String,Long> e : slowCompletor.entrySet()) {
diff --git lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
index 7a8ca28..d62a425 100644
--- lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
+++ lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
@@ -154,8 +154,8 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
     }
     
     // Maps position to the start/end offset:
-    final Map<Integer,Integer> posToStartOffset = new HashMap<Integer,Integer>();
-    final Map<Integer,Integer> posToEndOffset = new HashMap<Integer,Integer>();
+    final Map<Integer,Integer> posToStartOffset = new HashMap<>();
+    final Map<Integer,Integer> posToEndOffset = new HashMap<>();
 
     ts.reset();
     int pos = -1;
@@ -682,12 +682,12 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
     PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;
     PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;
     TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;
-    List<String> tokens = new ArrayList<String>();
-    List<String> types = new ArrayList<String>();
-    List<Integer> positions = new ArrayList<Integer>();
-    List<Integer> positionLengths = new ArrayList<Integer>();
-    List<Integer> startOffsets = new ArrayList<Integer>();
-    List<Integer> endOffsets = new ArrayList<Integer>();
+    List<String> tokens = new ArrayList<>();
+    List<String> types = new ArrayList<>();
+    List<Integer> positions = new ArrayList<>();
+    List<Integer> positionLengths = new ArrayList<>();
+    List<Integer> startOffsets = new ArrayList<>();
+    List<Integer> endOffsets = new ArrayList<>();
     ts.reset();
 
     // First pass: save away "correct" tokens
diff --git lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase.java lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase.java
index 3a1c5b3..1bee019 100644
--- lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase.java
+++ lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase.java
@@ -251,7 +251,7 @@ public abstract class CollationTestBase extends LuceneTestCase {
   public void assertThreadSafe(final Analyzer analyzer) throws Exception {
     int numTestPoints = 100;
     int numThreads = TestUtil.nextInt(random(), 3, 5);
-    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();
+    final HashMap<String,BytesRef> map = new HashMap<>();
     
     // create a map<String,SortKey> up front.
     // then with multiple threads, generate sort keys for all the keys in the map
diff --git lucene/test-framework/src/java/org/apache/lucene/analysis/LookaheadTokenFilter.java lucene/test-framework/src/java/org/apache/lucene/analysis/LookaheadTokenFilter.java
index b963dd3..e664026 100644
--- lucene/test-framework/src/java/org/apache/lucene/analysis/LookaheadTokenFilter.java
+++ lucene/test-framework/src/java/org/apache/lucene/analysis/LookaheadTokenFilter.java
@@ -59,7 +59,7 @@ public abstract class LookaheadTokenFilter<T extends LookaheadTokenFilter.Positi
    *  to record other state at each position. */ 
   protected static class Position implements RollingBuffer.Resettable {
     // Buffered input tokens at this position:
-    public final List<AttributeSource.State> inputTokens = new ArrayList<AttributeSource.State>();
+    public final List<AttributeSource.State> inputTokens = new ArrayList<>();
 
     // Next buffered token to be returned to consumer:
     public int nextRead;
diff --git lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer.java lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer.java
index 8fe142d..1ab9ef0 100644
--- lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer.java
+++ lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer.java
@@ -48,7 +48,7 @@ public final class MockAnalyzer extends Analyzer {
   private int positionIncrementGap;
   private Integer offsetGap;
   private final Random random;
-  private Map<String,Integer> previousMappings = new HashMap<String,Integer>();
+  private Map<String,Integer> previousMappings = new HashMap<>();
   private boolean enableChecks = true;
   private int maxTokenLength = MockTokenizer.DEFAULT_MAX_TOKEN_LENGTH;
 
diff --git lucene/test-framework/src/java/org/apache/lucene/analysis/MockCharFilter.java lucene/test-framework/src/java/org/apache/lucene/analysis/MockCharFilter.java
index fc62028..5e88a60 100644
--- lucene/test-framework/src/java/org/apache/lucene/analysis/MockCharFilter.java
+++ lucene/test-framework/src/java/org/apache/lucene/analysis/MockCharFilter.java
@@ -100,5 +100,5 @@ public class MockCharFilter extends CharFilter {
     corrections.put(off, cumulativeDiff);
   }
   
-  TreeMap<Integer,Integer> corrections = new TreeMap<Integer,Integer>();
+  TreeMap<Integer,Integer> corrections = new TreeMap<>();
 }
diff --git lucene/test-framework/src/java/org/apache/lucene/analysis/ValidatingTokenFilter.java lucene/test-framework/src/java/org/apache/lucene/analysis/ValidatingTokenFilter.java
index a7d6456..bd40643 100644
--- lucene/test-framework/src/java/org/apache/lucene/analysis/ValidatingTokenFilter.java
+++ lucene/test-framework/src/java/org/apache/lucene/analysis/ValidatingTokenFilter.java
@@ -44,8 +44,8 @@ public final class ValidatingTokenFilter extends TokenFilter {
   private int lastStartOffset;
 
   // Maps position to the start/end offset:
-  private final Map<Integer,Integer> posToStartOffset = new HashMap<Integer,Integer>();
-  private final Map<Integer,Integer> posToEndOffset = new HashMap<Integer,Integer>();
+  private final Map<Integer,Integer> posToStartOffset = new HashMap<>();
+  private final Map<Integer,Integer> posToEndOffset = new HashMap<>();
 
   private final PositionIncrementAttribute posIncAtt = getAttrIfExists(PositionIncrementAttribute.class);
   private final PositionLengthAttribute posLenAtt = getAttrIfExists(PositionLengthAttribute.class);
diff --git lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java
index 90c161c..2a30039 100644
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java
@@ -155,7 +155,7 @@ class Lucene40DocValuesWriter extends DocValuesConsumer {
   @Override
   public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException {
     // examine the values to determine best type to use
-    HashSet<BytesRef> uniqueValues = new HashSet<BytesRef>();
+    HashSet<BytesRef> uniqueValues = new HashSet<>();
     int minLength = Integer.MAX_VALUE;
     int maxLength = Integer.MIN_VALUE;
     for (BytesRef b : values) {
@@ -314,7 +314,7 @@ class Lucene40DocValuesWriter extends DocValuesConsumer {
                           Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_CURRENT);
     
     // deduplicate
-    TreeSet<BytesRef> dictionary = new TreeSet<BytesRef>();
+    TreeSet<BytesRef> dictionary = new TreeSet<>();
     for (BytesRef v : values) {
       dictionary.add(v == null ? new BytesRef() : BytesRef.deepCopyOf(v));
     }
@@ -354,7 +354,7 @@ class Lucene40DocValuesWriter extends DocValuesConsumer {
                           Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_CURRENT);
     
     // deduplicate
-    TreeSet<BytesRef> dictionary = new TreeSet<BytesRef>();
+    TreeSet<BytesRef> dictionary = new TreeSet<>();
     for (BytesRef v : values) {
       dictionary.add(v == null ? new BytesRef() : BytesRef.deepCopyOf(v));
     }
@@ -362,7 +362,7 @@ class Lucene40DocValuesWriter extends DocValuesConsumer {
     /* values */
     long startPosition = data.getFilePointer();
     long currentAddress = 0;
-    HashMap<BytesRef,Long> valueToAddress = new HashMap<BytesRef,Long>();
+    HashMap<BytesRef,Long> valueToAddress = new HashMap<>();
     for (BytesRef v : dictionary) {
       currentAddress = data.getFilePointer() - startPosition;
       valueToAddress.put(v, currentAddress);
diff --git lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java
index ac8aeaf..78680f1 100644
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java
@@ -144,7 +144,7 @@ class Lucene42DocValuesConsumer extends DocValuesConsumer {
       } else {
         meta.writeByte(TABLE_COMPRESSED); // table-compressed
         Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
-        final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();
+        final HashMap<Long,Integer> encode = new HashMap<>();
         data.writeVInt(decode.length);
         for (int i = 0; i < decode.length; i++) {
           data.writeLong(decode[i]);
@@ -252,7 +252,7 @@ class Lucene42DocValuesConsumer extends DocValuesConsumer {
     meta.writeByte(FST);
     meta.writeLong(data.getFilePointer());
     PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
-    Builder<Long> builder = new Builder<Long>(INPUT_TYPE.BYTE1, outputs);
+    Builder<Long> builder = new Builder<>(INPUT_TYPE.BYTE1, outputs);
     IntsRef scratch = new IntsRef();
     long ord = 0;
     for (BytesRef v : values) {
diff --git lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
index eacde8e..f761565 100644
--- lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
@@ -96,7 +96,7 @@ public final class MockRandomPostingsFormat extends PostingsFormat {
   // Chooses random IntStreamFactory depending on file's extension
   private static class MockIntStreamFactory extends IntStreamFactory {
     private final int salt;
-    private final List<IntStreamFactory> delegates = new ArrayList<IntStreamFactory>();
+    private final List<IntStreamFactory> delegates = new ArrayList<>();
 
     public MockIntStreamFactory(Random random) {
       salt = random.nextInt();
diff --git lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java
index 9fcaab7..a83b5b6 100644
--- lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java
@@ -65,7 +65,7 @@ public final class RAMOnlyPostingsFormat extends PostingsFormat {
     
   // Postings state:
   static class RAMPostings extends FieldsProducer {
-    final Map<String,RAMField> fieldToTerms = new TreeMap<String,RAMField>();
+    final Map<String,RAMField> fieldToTerms = new TreeMap<>();
 
     @Override
     public Terms terms(String field) {
@@ -98,7 +98,7 @@ public final class RAMOnlyPostingsFormat extends PostingsFormat {
 
   static class RAMField extends Terms {
     final String field;
-    final SortedMap<String,RAMTerm> termToDocs = new TreeMap<String,RAMTerm>();
+    final SortedMap<String,RAMTerm> termToDocs = new TreeMap<>();
     long sumTotalTermFreq;
     long sumDocFreq;
     int docCount;
@@ -167,7 +167,7 @@ public final class RAMOnlyPostingsFormat extends PostingsFormat {
   static class RAMTerm {
     final String term;
     long totalTermFreq;
-    final List<RAMDoc> docs = new ArrayList<RAMDoc>();
+    final List<RAMDoc> docs = new ArrayList<>();
     public RAMTerm(String term) {
       this.term = term;
     }
@@ -599,7 +599,7 @@ public final class RAMOnlyPostingsFormat extends PostingsFormat {
   }
 
   // Holds all indexes created, keyed by the ID assigned in fieldsConsumer
-  private final Map<Integer,RAMPostings> state = new HashMap<Integer,RAMPostings>();
+  private final Map<Integer,RAMPostings> state = new HashMap<>();
 
   private final AtomicInteger nextID = new AtomicInteger();
 
diff --git lucene/test-framework/src/java/org/apache/lucene/index/BaseCompressingDocValuesFormatTestCase.java lucene/test-framework/src/java/org/apache/lucene/index/BaseCompressingDocValuesFormatTestCase.java
index 119dcb9..6a2c117 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/BaseCompressingDocValuesFormatTestCase.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/BaseCompressingDocValuesFormatTestCase.java
@@ -48,7 +48,7 @@ public abstract class BaseCompressingDocValuesFormatTestCase extends BaseDocValu
     final IndexWriter iwriter = new IndexWriter(dir, iwc);
 
     final int uniqueValueCount = TestUtil.nextInt(random(), 1, 256);
-    final List<Long> values = new ArrayList<Long>();
+    final List<Long> values = new ArrayList<>();
 
     final Document doc = new Document();
     final NumericDocValuesField dvf = new NumericDocValuesField("dv", 0);
diff --git lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java
index aab13f0..422eb1d 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java
@@ -1138,7 +1138,7 @@ public abstract class BaseDocValuesFormatTestCase extends LuceneTestCase {
     RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);
     int numDocs = atLeast(100);
     BytesRefHash hash = new BytesRefHash();
-    Map<String, String> docToString = new HashMap<String, String>();
+    Map<String, String> docToString = new HashMap<>();
     int maxLength = TestUtil.nextInt(random(), 1, 50);
     for (int i = 0; i < numDocs; i++) {
       Document doc = new Document();
@@ -2086,7 +2086,7 @@ public abstract class BaseDocValuesFormatTestCase extends LuceneTestCase {
       }
       int numValues = TestUtil.nextInt(random(), 0, maxValuesPerDoc);
       // create a random set of strings
-      Set<String> values = new TreeSet<String>();
+      Set<String> values = new TreeSet<>();
       for (int v = 0; v < numValues; v++) {
         values.add(TestUtil.randomSimpleString(random(), length));
       }
@@ -2097,7 +2097,7 @@ public abstract class BaseDocValuesFormatTestCase extends LuceneTestCase {
       }
 
       // add in any order to the dv field
-      ArrayList<String> unordered = new ArrayList<String>(values);
+      ArrayList<String> unordered = new ArrayList<>(values);
       Collections.shuffle(unordered, random());
       for (String v : unordered) {
         doc.add(new SortedSetDocValuesField("dv", new BytesRef(v)));
@@ -2303,20 +2303,20 @@ public abstract class BaseDocValuesFormatTestCase extends LuceneTestCase {
       }
       int numValues = random().nextInt(17);
       // create a random list of strings
-      List<String> values = new ArrayList<String>();
+      List<String> values = new ArrayList<>();
       for (int v = 0; v < numValues; v++) {
         values.add(TestUtil.randomSimpleString(random(), length));
       }
       
       // add in any order to the indexed field
-      ArrayList<String> unordered = new ArrayList<String>(values);
+      ArrayList<String> unordered = new ArrayList<>(values);
       Collections.shuffle(unordered, random());
       for (String v : values) {
         doc.add(newStringField("indexed", v, Field.Store.NO));
       }
 
       // add in any order to the dv field
-      ArrayList<String> unordered2 = new ArrayList<String>(values);
+      ArrayList<String> unordered2 = new ArrayList<>(values);
       Collections.shuffle(unordered2, random());
       for (String v : unordered2) {
         doc.add(new SortedSetDocValuesField("dv", new BytesRef(v)));
@@ -2628,7 +2628,7 @@ public abstract class BaseDocValuesFormatTestCase extends LuceneTestCase {
       numDocs = TestUtil.nextInt(random(), 100, 200);
     }
     IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
-    List<byte[]> docBytes = new ArrayList<byte[]>();
+    List<byte[]> docBytes = new ArrayList<>();
     long totalBytes = 0;
     for(int docID=0;docID<numDocs;docID++) {
       // we don't use RandomIndexWriter because it might add
@@ -2726,7 +2726,7 @@ public abstract class BaseDocValuesFormatTestCase extends LuceneTestCase {
       numDocs = TestUtil.nextInt(random(), 100, 200);
     }
     IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
-    List<byte[]> docBytes = new ArrayList<byte[]>();
+    List<byte[]> docBytes = new ArrayList<>();
     long totalBytes = 0;
     for(int docID=0;docID<numDocs;docID++) {
       // we don't use RandomIndexWriter because it might add
@@ -2905,7 +2905,7 @@ public abstract class BaseDocValuesFormatTestCase extends LuceneTestCase {
         doc.add(dvNumericField);
       }
       int numSortedSetFields = random().nextInt(3);
-      Set<String> values = new TreeSet<String>();
+      Set<String> values = new TreeSet<>();
       for (int j = 0; j < numSortedSetFields; j++) {
         values.add(TestUtil.randomSimpleString(random()));
       }
diff --git lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java
index d34ce2f..a0550c7 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java
@@ -341,7 +341,7 @@ public abstract class BasePostingsFormatTestCase extends LuceneTestCase {
   public static void createPostings() throws IOException {
     totalPostings = 0;
     totalPayloadBytes = 0;
-    fields = new TreeMap<String,SortedMap<BytesRef,Long>>();
+    fields = new TreeMap<>();
 
     final int numFields = TestUtil.nextInt(random(), 1, 5);
     if (VERBOSE) {
@@ -362,9 +362,9 @@ public abstract class BasePostingsFormatTestCase extends LuceneTestCase {
                                                 null, DocValuesType.NUMERIC, null);
       fieldUpto++;
 
-      SortedMap<BytesRef,Long> postings = new TreeMap<BytesRef,Long>();
+      SortedMap<BytesRef,Long> postings = new TreeMap<>();
       fields.put(field, postings);
-      Set<String> seenTerms = new HashSet<String>();
+      Set<String> seenTerms = new HashSet<>();
 
       int numTerms;
       if (random().nextInt(10) == 7) {
@@ -422,7 +422,7 @@ public abstract class BasePostingsFormatTestCase extends LuceneTestCase {
       }
     }
 
-    allTerms = new ArrayList<FieldAndTerm>();
+    allTerms = new ArrayList<>();
     for(Map.Entry<String,SortedMap<BytesRef,Long>> fieldEnt : fields.entrySet()) {
       String field = fieldEnt.getKey();
       for(Map.Entry<BytesRef,Long> termEnt : fieldEnt.getValue().entrySet()) {
@@ -1103,8 +1103,8 @@ public abstract class BasePostingsFormatTestCase extends LuceneTestCase {
     ThreadState threadState = new ThreadState();
 
     // Test random terms/fields:
-    List<TermState> termStates = new ArrayList<TermState>();
-    List<FieldAndTerm> termStateTerms = new ArrayList<FieldAndTerm>();
+    List<TermState> termStates = new ArrayList<>();
+    List<FieldAndTerm> termStateTerms = new ArrayList<>();
     
     Collections.shuffle(allTerms, random());
     int upto = 0;
@@ -1387,7 +1387,7 @@ public abstract class BasePostingsFormatTestCase extends LuceneTestCase {
     // while up to one thread flushes, and each of those
     // threads iterates over the map while the flushing
     // thread might be adding to it:
-    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<String,TermFreqs>();
+    final Map<String,TermFreqs> termFreqs = new ConcurrentHashMap<>();
 
     final AtomicLong sumDocFreq = new AtomicLong();
     final AtomicLong sumTotalTermFreq = new AtomicLong();
diff --git lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java
index 1c9a826..a2d9234 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java
@@ -99,7 +99,7 @@ public abstract class BaseStoredFieldsFormatTestCase extends LuceneTestCase {
     final int docCount = atLeast(200);
     final int fieldCount = TestUtil.nextInt(rand, 1, 5);
 
-    final List<Integer> fieldIDs = new ArrayList<Integer>();
+    final List<Integer> fieldIDs = new ArrayList<>();
 
     FieldType customType = new FieldType(TextField.TYPE_STORED);
     customType.setTokenized(false);
@@ -109,7 +109,7 @@ public abstract class BaseStoredFieldsFormatTestCase extends LuceneTestCase {
       fieldIDs.add(i);
     }
 
-    final Map<String,Document> docs = new HashMap<String,Document>();
+    final Map<String,Document> docs = new HashMap<>();
 
     if (VERBOSE) {
       System.out.println("TEST: build index docCount=" + docCount);
@@ -439,8 +439,8 @@ public abstract class BaseStoredFieldsFormatTestCase extends LuceneTestCase {
     final IndexSearcher searcher = new IndexSearcher(rd);
     final int concurrentReads = atLeast(5);
     final int readsPerThread = atLeast(50);
-    final List<Thread> readThreads = new ArrayList<Thread>();
-    final AtomicReference<Exception> ex = new AtomicReference<Exception>();
+    final List<Thread> readThreads = new ArrayList<>();
+    final AtomicReference<Exception> ex = new AtomicReference<>();
     for (int i = 0; i < concurrentReads; ++i) {
       readThreads.add(new Thread() {
 
diff --git lucene/test-framework/src/java/org/apache/lucene/index/BaseTermVectorsFormatTestCase.java lucene/test-framework/src/java/org/apache/lucene/index/BaseTermVectorsFormatTestCase.java
index 5b0bdd0..76760d0 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/BaseTermVectorsFormatTestCase.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/BaseTermVectorsFormatTestCase.java
@@ -102,7 +102,7 @@ public abstract class BaseTermVectorsFormatTestCase extends LuceneTestCase {
   }
 
   protected Options randomOptions() {
-    return RandomPicks.randomFrom(random(), new ArrayList<Options>(validOptions()));
+    return RandomPicks.randomFrom(random(), new ArrayList<>(validOptions()));
   }
 
   protected FieldType fieldType(Options options) {
@@ -245,8 +245,8 @@ public abstract class BaseTermVectorsFormatTestCase extends LuceneTestCase {
         }
       }
 
-      positionToTerms = new HashMap<Integer, Set<Integer>>(len);
-      startOffsetToTerms = new HashMap<Integer, Set<Integer>>(len);
+      positionToTerms = new HashMap<>(len);
+      startOffsetToTerms = new HashMap<>(len);
       for (int i = 0; i < len; ++i) {
         if (!positionToTerms.containsKey(positions[i])) {
           positionToTerms.put(positions[i], new HashSet<Integer>(1));
@@ -258,7 +258,7 @@ public abstract class BaseTermVectorsFormatTestCase extends LuceneTestCase {
         startOffsetToTerms.get(startOffsets[i]).add(i);
       }
 
-      freqs = new HashMap<String, Integer>();
+      freqs = new HashMap<>();
       for (String term : terms) {
         if (freqs.containsKey(term)) {
           freqs.put(term, freqs.get(term) + 1);
@@ -314,7 +314,7 @@ public abstract class BaseTermVectorsFormatTestCase extends LuceneTestCase {
       fieldTypes = new FieldType[fieldCount];
       tokenStreams = new RandomTokenStream[fieldCount];
       Arrays.fill(fieldTypes, fieldType(options));
-      final Set<String> usedFileNames = new HashSet<String>();
+      final Set<String> usedFileNames = new HashSet<>();
       for (int i = 0; i < fieldCount; ++i) {
         do {
           this.fieldNames[i] = RandomPicks.randomFrom(random(), fieldNames);
@@ -341,7 +341,7 @@ public abstract class BaseTermVectorsFormatTestCase extends LuceneTestCase {
     private final BytesRef[] termBytes;
 
     protected RandomDocumentFactory(int distinctFieldNames, int disctinctTerms) {
-      final Set<String> fieldNames = new HashSet<String>();
+      final Set<String> fieldNames = new HashSet<>();
       while (fieldNames.size() < distinctFieldNames) {
         fieldNames.add(TestUtil.randomSimpleString(random()));
         fieldNames.remove("id");
@@ -365,8 +365,8 @@ public abstract class BaseTermVectorsFormatTestCase extends LuceneTestCase {
     // compare field names
     assertEquals(doc == null, fields == null);
     assertEquals(doc.fieldNames.length, fields.size());
-    final Set<String> fields1 = new HashSet<String>();
-    final Set<String> fields2 = new HashSet<String>();
+    final Set<String> fields1 = new HashSet<>();
+    final Set<String> fields2 = new HashSet<>();
     for (int i = 0; i < doc.fieldNames.length; ++i) {
       fields1.add(doc.fieldNames[i]);
     }
@@ -389,19 +389,19 @@ public abstract class BaseTermVectorsFormatTestCase extends LuceneTestCase {
   }
 
   // to test reuse
-  private final ThreadLocal<TermsEnum> termsEnum = new ThreadLocal<TermsEnum>();
-  private final ThreadLocal<DocsEnum> docsEnum = new ThreadLocal<DocsEnum>();
-  private final ThreadLocal<DocsAndPositionsEnum> docsAndPositionsEnum = new ThreadLocal<DocsAndPositionsEnum>();
+  private final ThreadLocal<TermsEnum> termsEnum = new ThreadLocal<>();
+  private final ThreadLocal<DocsEnum> docsEnum = new ThreadLocal<>();
+  private final ThreadLocal<DocsAndPositionsEnum> docsAndPositionsEnum = new ThreadLocal<>();
 
   protected void assertEquals(RandomTokenStream tk, FieldType ft, Terms terms) throws IOException {
     assertEquals(1, terms.getDocCount());
-    final int termCount = new HashSet<String>(Arrays.asList(tk.terms)).size();
+    final int termCount = new HashSet<>(Arrays.asList(tk.terms)).size();
     assertEquals(termCount, terms.size());
     assertEquals(termCount, terms.getSumDocFreq());
     assertEquals(ft.storeTermVectorPositions(), terms.hasPositions());
     assertEquals(ft.storeTermVectorOffsets(), terms.hasOffsets());
     assertEquals(ft.storeTermVectorPayloads() && tk.hasPayloads(), terms.hasPayloads());
-    final Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();
+    final Set<BytesRef> uniqueTerms = new HashSet<>();
     for (String term : tk.freqs.keySet()) {
       uniqueTerms.add(new BytesRef(term));
     }
@@ -638,7 +638,7 @@ public abstract class BaseTermVectorsFormatTestCase extends LuceneTestCase {
     final RandomDocumentFactory docFactory = new RandomDocumentFactory(5, 20);
     final int numDocs = atLeast(100);
     final int numDeletes = random().nextInt(numDocs);
-    final Set<Integer> deletes = new HashSet<Integer>();
+    final Set<Integer> deletes = new HashSet<>();
     while (deletes.size() < numDeletes) {
       deletes.add(random().nextInt(numDocs));
     }
@@ -694,7 +694,7 @@ public abstract class BaseTermVectorsFormatTestCase extends LuceneTestCase {
         assertEquals(docs[i], reader.getTermVectors(docID));
       }
 
-      final AtomicReference<Throwable> exception = new AtomicReference<Throwable>();
+      final AtomicReference<Throwable> exception = new AtomicReference<>();
       final Thread[] threads = new Thread[2];
       for (int i = 0; i < threads.length; ++i) {
         threads[i] = new Thread() {
diff --git lucene/test-framework/src/java/org/apache/lucene/index/DocHelper.java lucene/test-framework/src/java/org/apache/lucene/index/DocHelper.java
index ca741f2..01588ed 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/DocHelper.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/DocHelper.java
@@ -175,16 +175,16 @@ class DocHelper {
     largeLazyField//placeholder for large field, since this is null.  It must always be last
   };
 
-  public static Map<String,IndexableField> all     =new HashMap<String,IndexableField>();
-  public static Map<String,IndexableField> indexed =new HashMap<String,IndexableField>();
-  public static Map<String,IndexableField> stored  =new HashMap<String,IndexableField>();
-  public static Map<String,IndexableField> unstored=new HashMap<String,IndexableField>();
-  public static Map<String,IndexableField> unindexed=new HashMap<String,IndexableField>();
-  public static Map<String,IndexableField> termvector=new HashMap<String,IndexableField>();
-  public static Map<String,IndexableField> notermvector=new HashMap<String,IndexableField>();
-  public static Map<String,IndexableField> lazy= new HashMap<String,IndexableField>();
-  public static Map<String,IndexableField> noNorms=new HashMap<String,IndexableField>();
-  public static Map<String,IndexableField> noTf=new HashMap<String,IndexableField>();
+  public static Map<String,IndexableField> all     =new HashMap<>();
+  public static Map<String,IndexableField> indexed =new HashMap<>();
+  public static Map<String,IndexableField> stored  =new HashMap<>();
+  public static Map<String,IndexableField> unstored=new HashMap<>();
+  public static Map<String,IndexableField> unindexed=new HashMap<>();
+  public static Map<String,IndexableField> termvector=new HashMap<>();
+  public static Map<String,IndexableField> notermvector=new HashMap<>();
+  public static Map<String,IndexableField> lazy= new HashMap<>();
+  public static Map<String,IndexableField> noNorms=new HashMap<>();
+  public static Map<String,IndexableField> noTf=new HashMap<>();
 
   static {
     //Initialize the large Lazy Field
@@ -227,7 +227,7 @@ class DocHelper {
 
   static
   {
-    nameValues = new HashMap<String,Object>();
+    nameValues = new HashMap<>();
     nameValues.put(TEXT_FIELD_1_KEY, FIELD_1_TEXT);
     nameValues.put(TEXT_FIELD_2_KEY, FIELD_2_TEXT);
     nameValues.put(TEXT_FIELD_3_KEY, FIELD_3_TEXT);
diff --git lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterAtomicReader.java lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterAtomicReader.java
index 66c2487..5f073e8 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterAtomicReader.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterAtomicReader.java
@@ -39,7 +39,7 @@ public final class FieldFilterAtomicReader extends FilterAtomicReader {
     super(in);
     this.fields = fields;
     this.negate = negate;
-    ArrayList<FieldInfo> filteredInfos = new ArrayList<FieldInfo>();
+    ArrayList<FieldInfo> filteredInfos = new ArrayList<>();
     for (FieldInfo fi : in.getFieldInfos()) {
       if (hasField(fi.name)) {
         filteredInfos.add(fi);
diff --git lucene/test-framework/src/java/org/apache/lucene/index/MockRandomMergePolicy.java lucene/test-framework/src/java/org/apache/lucene/index/MockRandomMergePolicy.java
index d4ec400..9d8e0ee 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/MockRandomMergePolicy.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/MockRandomMergePolicy.java
@@ -46,7 +46,7 @@ public class MockRandomMergePolicy extends MergePolicy {
 
     int numSegments = segmentInfos.size();
 
-    List<SegmentCommitInfo> segments = new ArrayList<SegmentCommitInfo>();
+    List<SegmentCommitInfo> segments = new ArrayList<>();
     final Collection<SegmentCommitInfo> merging = writer.get().getMergingSegments();
 
     for(SegmentCommitInfo sipc : segmentInfos) {
@@ -75,7 +75,7 @@ public class MockRandomMergePolicy extends MergePolicy {
        SegmentInfos segmentInfos, int maxSegmentCount, Map<SegmentCommitInfo,Boolean> segmentsToMerge)
     throws IOException {
 
-    final List<SegmentCommitInfo> eligibleSegments = new ArrayList<SegmentCommitInfo>();
+    final List<SegmentCommitInfo> eligibleSegments = new ArrayList<>();
     for(SegmentCommitInfo info : segmentInfos) {
       if (segmentsToMerge.containsKey(info)) {
         eligibleSegments.add(info);
diff --git lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java
index 5e3911f..8da536d 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java
@@ -68,16 +68,16 @@ import org.apache.lucene.util.TestUtil;
  */
 public class RandomCodec extends Lucene46Codec {
   /** Shuffled list of postings formats to use for new mappings */
-  private List<PostingsFormat> formats = new ArrayList<PostingsFormat>();
+  private List<PostingsFormat> formats = new ArrayList<>();
   
   /** Shuffled list of docvalues formats to use for new mappings */
-  private List<DocValuesFormat> dvFormats = new ArrayList<DocValuesFormat>();
+  private List<DocValuesFormat> dvFormats = new ArrayList<>();
   
   /** unique set of format names this codec knows about */
-  public Set<String> formatNames = new HashSet<String>();
+  public Set<String> formatNames = new HashSet<>();
   
   /** unique set of docvalues format names this codec knows about */
-  public Set<String> dvFormatNames = new HashSet<String>();
+  public Set<String> dvFormatNames = new HashSet<>();
 
   /** memorized field->postingsformat mappings */
   // note: we have to sync this map even though its just for debugging/toString, 
diff --git lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java
index bbf00a5..8beff38 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java
@@ -125,8 +125,8 @@ public abstract class ThreadedIndexingAndSearchingTestCase extends LuceneTestCas
           @Override
           public void run() {
             // TODO: would be better if this were cross thread, so that we make sure one thread deleting anothers added docs works:
-            final List<String> toDeleteIDs = new ArrayList<String>();
-            final List<SubDocs> toDeleteSubDocs = new ArrayList<SubDocs>();
+            final List<String> toDeleteIDs = new ArrayList<>();
+            final List<SubDocs> toDeleteSubDocs = new ArrayList<>();
             while(System.currentTimeMillis() < stopTime && !failed.get()) {
               try {
 
@@ -180,9 +180,9 @@ public abstract class ThreadedIndexingAndSearchingTestCase extends LuceneTestCas
                     }
 
                     final Field packIDField = newStringField("packID", packID, Field.Store.YES);
-                    final List<String> docIDs = new ArrayList<String>();
+                    final List<String> docIDs = new ArrayList<>();
                     final SubDocs subDocs = new SubDocs(packID, docIDs);
-                    final List<Document> docsList = new ArrayList<Document>();
+                    final List<Document> docsList = new ArrayList<>();
 
                     allSubDocs.add(subDocs);
                     doc.add(packIDField);
diff --git lucene/test-framework/src/java/org/apache/lucene/search/AssertingScorer.java lucene/test-framework/src/java/org/apache/lucene/search/AssertingScorer.java
index a296988..5756124 100644
--- lucene/test-framework/src/java/org/apache/lucene/search/AssertingScorer.java
+++ lucene/test-framework/src/java/org/apache/lucene/search/AssertingScorer.java
@@ -36,8 +36,8 @@ public class AssertingScorer extends Scorer {
     YES, NO, UNKNOWN;
   }
 
-  private static final VirtualMethod<Scorer> SCORE_COLLECTOR = new VirtualMethod<Scorer>(Scorer.class, "score", Collector.class);
-  private static final VirtualMethod<Scorer> SCORE_COLLECTOR_RANGE = new VirtualMethod<Scorer>(Scorer.class, "score", Collector.class, int.class, int.class);
+  private static final VirtualMethod<Scorer> SCORE_COLLECTOR = new VirtualMethod<>(Scorer.class, "score", Collector.class);
+  private static final VirtualMethod<Scorer> SCORE_COLLECTOR_RANGE = new VirtualMethod<>(Scorer.class, "score", Collector.class, int.class, int.class);
 
   // we need to track scorers using a weak hash map because otherwise we
   // could loose references because of eg.
@@ -49,7 +49,7 @@ public class AssertingScorer extends Scorer {
       return other;
     }
     final AssertingScorer assertScorer = new AssertingScorer(random, other, topScorer, inOrder);
-    ASSERTING_INSTANCES.put(other, new WeakReference<AssertingScorer>(assertScorer));
+    ASSERTING_INSTANCES.put(other, new WeakReference<>(assertScorer));
     return assertScorer;
   }
 
diff --git lucene/test-framework/src/java/org/apache/lucene/search/CheckHits.java lucene/test-framework/src/java/org/apache/lucene/search/CheckHits.java
index 730f253..034396c 100644
--- lucene/test-framework/src/java/org/apache/lucene/search/CheckHits.java
+++ lucene/test-framework/src/java/org/apache/lucene/search/CheckHits.java
@@ -59,7 +59,7 @@ public class CheckHits {
     throws IOException {
 
     String d = q.toString(defaultFieldName);
-    Set<Integer> ignore = new TreeSet<Integer>();
+    Set<Integer> ignore = new TreeSet<>();
     for (int i = 0; i < results.length; i++) {
       ignore.add(Integer.valueOf(results[i]));
     }
@@ -98,11 +98,11 @@ public class CheckHits {
 
     QueryUtils.check(random,query,searcher);
     
-    Set<Integer> correct = new TreeSet<Integer>();
+    Set<Integer> correct = new TreeSet<>();
     for (int i = 0; i < results.length; i++) {
       correct.add(Integer.valueOf(results[i]));
     }
-    final Set<Integer> actual = new TreeSet<Integer>();
+    final Set<Integer> actual = new TreeSet<>();
     final Collector c = new SetCollector(actual);
 
     searcher.search(query, c);
@@ -168,12 +168,12 @@ public class CheckHits {
 
     ScoreDoc[] hits = searcher.search(query, 1000).scoreDocs;
 
-    Set<Integer> correct = new TreeSet<Integer>();
+    Set<Integer> correct = new TreeSet<>();
     for (int i = 0; i < results.length; i++) {
       correct.add(Integer.valueOf(results[i]));
     }
 
-    Set<Integer> actual = new TreeSet<Integer>();
+    Set<Integer> actual = new TreeSet<>();
     for (int i = 0; i < hits.length; i++) {
       actual.add(Integer.valueOf(hits[i].doc));
     }
diff --git lucene/test-framework/src/java/org/apache/lucene/search/RandomSimilarityProvider.java lucene/test-framework/src/java/org/apache/lucene/search/RandomSimilarityProvider.java
index d7aca2d..5f68765 100644
--- lucene/test-framework/src/java/org/apache/lucene/search/RandomSimilarityProvider.java
+++ lucene/test-framework/src/java/org/apache/lucene/search/RandomSimilarityProvider.java
@@ -65,7 +65,7 @@ import org.apache.lucene.search.similarities.Similarity;
 public class RandomSimilarityProvider extends PerFieldSimilarityWrapper {
   final DefaultSimilarity defaultSim = new DefaultSimilarity();
   final List<Similarity> knownSims;
-  Map<String,Similarity> previousMappings = new HashMap<String,Similarity>();
+  Map<String,Similarity> previousMappings = new HashMap<>();
   final int perFieldSeed;
   final int coordType; // 0 = no coord, 1 = coord, 2 = crazy coord
   final boolean shouldQueryNorm;
@@ -74,7 +74,7 @@ public class RandomSimilarityProvider extends PerFieldSimilarityWrapper {
     perFieldSeed = random.nextInt();
     coordType = random.nextInt(3);
     shouldQueryNorm = random.nextBoolean();
-    knownSims = new ArrayList<Similarity>(allSims);
+    knownSims = new ArrayList<>(allSims);
     Collections.shuffle(knownSims, random);
   }
   
@@ -138,7 +138,7 @@ public class RandomSimilarityProvider extends PerFieldSimilarityWrapper {
   };
   static List<Similarity> allSims;
   static {
-    allSims = new ArrayList<Similarity>();
+    allSims = new ArrayList<>();
     allSims.add(new DefaultSimilarity());
     allSims.add(new BM25Similarity());
     for (BasicModel basicModel : BASIC_MODELS) {
diff --git lucene/test-framework/src/java/org/apache/lucene/search/ShardSearchingTestBase.java lucene/test-framework/src/java/org/apache/lucene/search/ShardSearchingTestBase.java
index b0f46f4..43b33e9 100644
--- lucene/test-framework/src/java/org/apache/lucene/search/ShardSearchingTestBase.java
+++ lucene/test-framework/src/java/org/apache/lucene/search/ShardSearchingTestBase.java
@@ -178,7 +178,7 @@ public abstract class ShardSearchingTestBase extends LuceneTestCase {
   // term stats from remote node
   Map<Term,TermStatistics> getNodeTermStats(Set<Term> terms, int nodeID, long version) throws IOException {
     final NodeState node = nodes[nodeID];
-    final Map<Term,TermStatistics> stats = new HashMap<Term,TermStatistics>();
+    final Map<Term,TermStatistics> stats = new HashMap<>();
     final IndexSearcher s = node.searchers.acquire(version);
     if (s == null) {
       throw new SearcherExpiredException("node=" + nodeID + " version=" + version);
@@ -207,8 +207,8 @@ public abstract class ShardSearchingTestBase extends LuceneTestCase {
     // local cache...?  And still LRU otherwise (for the
     // still-live searchers).
 
-    private final Map<FieldAndShardVersion,CollectionStatistics> collectionStatsCache = new ConcurrentHashMap<FieldAndShardVersion,CollectionStatistics>();
-    private final Map<TermAndShardVersion,TermStatistics> termStatsCache = new ConcurrentHashMap<TermAndShardVersion,TermStatistics>();
+    private final Map<FieldAndShardVersion,CollectionStatistics> collectionStatsCache = new ConcurrentHashMap<>();
+    private final Map<TermAndShardVersion,TermStatistics> termStatsCache = new ConcurrentHashMap<>();
 
     /** Matches docs in the local shard but scores based on
      *  aggregated stats ("mock distributed scoring") from all
@@ -229,7 +229,7 @@ public abstract class ShardSearchingTestBase extends LuceneTestCase {
       @Override
       public Query rewrite(Query original) throws IOException {
         final Query rewritten = super.rewrite(original);
-        final Set<Term> terms = new HashSet<Term>();
+        final Set<Term> terms = new HashSet<>();
         rewritten.extractTerms(terms);
 
         // Make a single request to remote nodes for term
@@ -239,7 +239,7 @@ public abstract class ShardSearchingTestBase extends LuceneTestCase {
             continue;
           }
 
-          final Set<Term> missing = new HashSet<Term>();
+          final Set<Term> missing = new HashSet<>();
           for(Term term : terms) {
             final TermAndShardVersion key = new TermAndShardVersion(nodeID, nodeVersions[nodeID], term);
             if (!termStatsCache.containsKey(key)) {
diff --git lucene/test-framework/src/java/org/apache/lucene/store/MockDirectoryWrapper.java lucene/test-framework/src/java/org/apache/lucene/store/MockDirectoryWrapper.java
index e8eb029..2f2acac 100644
--- lucene/test-framework/src/java/org/apache/lucene/store/MockDirectoryWrapper.java
+++ lucene/test-framework/src/java/org/apache/lucene/store/MockDirectoryWrapper.java
@@ -76,7 +76,7 @@ public class MockDirectoryWrapper extends BaseDirectoryWrapper {
   boolean wrapLockFactory = true;
   private Set<String> unSyncedFiles;
   private Set<String> createdFiles;
-  private Set<String> openFilesForWrite = new HashSet<String>();
+  private Set<String> openFilesForWrite = new HashSet<>();
   Set<String> openLocks = Collections.synchronizedSet(new HashSet<String>());
   volatile boolean crashed;
   private ThrottledIndexOutput throttledOutput;
@@ -101,14 +101,14 @@ public class MockDirectoryWrapper extends BaseDirectoryWrapper {
 
   private synchronized void init() {
     if (openFiles == null) {
-      openFiles = new HashMap<String,Integer>();
-      openFilesDeleted = new HashSet<String>();
+      openFiles = new HashMap<>();
+      openFilesDeleted = new HashSet<>();
     }
 
     if (createdFiles == null)
-      createdFiles = new HashSet<String>();
+      createdFiles = new HashSet<>();
     if (unSyncedFiles == null)
-      unSyncedFiles = new HashSet<String>();
+      unSyncedFiles = new HashSet<>();
   }
 
   public MockDirectoryWrapper(Random random, Directory delegate) {
@@ -207,14 +207,14 @@ public class MockDirectoryWrapper extends BaseDirectoryWrapper {
    *  unsynced files. */
   public synchronized void crash() throws IOException {
     crashed = true;
-    openFiles = new HashMap<String,Integer>();
-    openFilesForWrite = new HashSet<String>();
-    openFilesDeleted = new HashSet<String>();
+    openFiles = new HashMap<>();
+    openFilesForWrite = new HashSet<>();
+    openFilesDeleted = new HashSet<>();
     Iterator<String> it = unSyncedFiles.iterator();
-    unSyncedFiles = new HashSet<String>();
+    unSyncedFiles = new HashSet<>();
     // first force-close all files, so we can corrupt on windows etc.
     // clone the file map, as these guys want to remove themselves on close.
-    Map<Closeable,Exception> m = new IdentityHashMap<Closeable,Exception>(openFileHandles);
+    Map<Closeable,Exception> m = new IdentityHashMap<>(openFileHandles);
     for (Closeable f : m.keySet()) {
       try {
         f.close();
@@ -441,7 +441,7 @@ public class MockDirectoryWrapper extends BaseDirectoryWrapper {
   }
 
   public synchronized Set<String> getOpenDeletedFiles() {
-    return new HashSet<String>(openFilesDeleted);
+    return new HashSet<>(openFilesDeleted);
   }
 
   private boolean failOnCreateOutput = true;
@@ -629,11 +629,11 @@ public class MockDirectoryWrapper extends BaseDirectoryWrapper {
   public synchronized void close() throws IOException {
     // files that we tried to delete, but couldn't because readers were open.
     // all that matters is that we tried! (they will eventually go away)
-    Set<String> pendingDeletions = new HashSet<String>(openFilesDeleted);
+    Set<String> pendingDeletions = new HashSet<>(openFilesDeleted);
     maybeYield();
     if (openFiles == null) {
-      openFiles = new HashMap<String,Integer>();
-      openFilesDeleted = new HashSet<String>();
+      openFiles = new HashMap<>();
+      openFilesDeleted = new HashSet<>();
     }
     if (openFiles.size() > 0) {
       // print the first one as its very verbose otherwise
@@ -666,7 +666,7 @@ public class MockDirectoryWrapper extends BaseDirectoryWrapper {
         // TODO: factor this out / share w/ TestIW.assertNoUnreferencedFiles
         if (assertNoUnreferencedFilesOnClose) {
           // now look for unreferenced files: discount ones that we tried to delete but could not
-          Set<String> allFiles = new HashSet<String>(Arrays.asList(listAll()));
+          Set<String> allFiles = new HashSet<>(Arrays.asList(listAll()));
           allFiles.removeAll(pendingDeletions);
           String[] startFiles = allFiles.toArray(new String[0]);
           IndexWriterConfig iwc = new IndexWriterConfig(LuceneTestCase.TEST_VERSION_CURRENT, null);
@@ -674,8 +674,8 @@ public class MockDirectoryWrapper extends BaseDirectoryWrapper {
           new IndexWriter(in, iwc).rollback();
           String[] endFiles = in.listAll();
 
-          Set<String> startSet = new TreeSet<String>(Arrays.asList(startFiles));
-          Set<String> endSet = new TreeSet<String>(Arrays.asList(endFiles));
+          Set<String> startSet = new TreeSet<>(Arrays.asList(startFiles));
+          Set<String> endSet = new TreeSet<>(Arrays.asList(endFiles));
           
           if (pendingDeletions.contains("segments.gen") && endSet.contains("segments.gen")) {
             // this is possible if we hit an exception while writing segments.gen, we try to delete it
@@ -703,7 +703,7 @@ public class MockDirectoryWrapper extends BaseDirectoryWrapper {
               }
 
               try {
-                Set<String> ghosts = new HashSet<String>(sis.files(in, false));
+                Set<String> ghosts = new HashSet<>(sis.files(in, false));
                 for (String s : ghosts) {
                   if (endSet.contains(s) && !startSet.contains(s)) {
                     assert pendingDeletions.contains(s);
@@ -725,14 +725,14 @@ public class MockDirectoryWrapper extends BaseDirectoryWrapper {
           endFiles = endSet.toArray(new String[0]);
 
           if (!Arrays.equals(startFiles, endFiles)) {
-            List<String> removed = new ArrayList<String>();
+            List<String> removed = new ArrayList<>();
             for(String fileName : startFiles) {
               if (!endSet.contains(fileName)) {
                 removed.add(fileName);
               }
             }
 
-            List<String> added = new ArrayList<String>();
+            List<String> added = new ArrayList<>();
             for(String fileName : endFiles) {
               if (!startSet.contains(fileName)) {
                 added.add(fileName);
@@ -841,7 +841,7 @@ public class MockDirectoryWrapper extends BaseDirectoryWrapper {
    */
   synchronized public void failOn(Failure fail) {
     if (failures == null) {
-      failures = new ArrayList<Failure>();
+      failures = new ArrayList<>();
     }
     failures.add(fail);
   }
diff --git lucene/test-framework/src/java/org/apache/lucene/util/AbstractBeforeAfterRule.java lucene/test-framework/src/java/org/apache/lucene/util/AbstractBeforeAfterRule.java
index 881da27..fc82bac 100644
--- lucene/test-framework/src/java/org/apache/lucene/util/AbstractBeforeAfterRule.java
+++ lucene/test-framework/src/java/org/apache/lucene/util/AbstractBeforeAfterRule.java
@@ -39,7 +39,7 @@ abstract class AbstractBeforeAfterRule implements TestRule {
     return new Statement() {
       @Override
       public void evaluate() throws Throwable {
-        final ArrayList<Throwable> errors = new ArrayList<Throwable>();
+        final ArrayList<Throwable> errors = new ArrayList<>();
 
         try {
           before();
diff --git lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java
index 063dec3..33d9d1d 100644
--- lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java
+++ lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java
@@ -197,7 +197,7 @@ public class LineFileDocs implements Closeable {
     }
   }
 
-  private final ThreadLocal<DocState> threadDocs = new ThreadLocal<DocState>();
+  private final ThreadLocal<DocState> threadDocs = new ThreadLocal<>();
 
   /** Note: Document instance is re-used per-thread */
   public Document nextDoc() throws IOException {
diff --git lucene/test-framework/src/java/org/apache/lucene/util/LuceneJUnit3MethodProvider.java lucene/test-framework/src/java/org/apache/lucene/util/LuceneJUnit3MethodProvider.java
index bbc9831..47ba67f 100644
--- lucene/test-framework/src/java/org/apache/lucene/util/LuceneJUnit3MethodProvider.java
+++ lucene/test-framework/src/java/org/apache/lucene/util/LuceneJUnit3MethodProvider.java
@@ -34,7 +34,7 @@ public final class LuceneJUnit3MethodProvider implements TestMethodProvider {
   @Override
   public Collection<Method> getTestMethods(Class<?> suiteClass, ClassModel classModel) {
     Map<Method,MethodModel> methods = classModel.getMethods();
-    ArrayList<Method> result = new ArrayList<Method>();
+    ArrayList<Method> result = new ArrayList<>();
     for (MethodModel mm : methods.values()) {
       // Skip any methods that have overrieds/ shadows.
       if (mm.getDown() != null) continue;
diff --git lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
index 52e8032..153211a 100644
--- lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
+++ lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
@@ -319,11 +319,11 @@ public abstract class LuceneTestCase extends Assert {
   /** All {@link Directory} implementations. */
   private static final List<String> CORE_DIRECTORIES;
   static {
-    CORE_DIRECTORIES = new ArrayList<String>(FS_DIRECTORIES);
+    CORE_DIRECTORIES = new ArrayList<>(FS_DIRECTORIES);
     CORE_DIRECTORIES.add("RAMDirectory");
   };
   
-  protected static final Set<String> doesntSupportOffsets = new HashSet<String>(Arrays.asList( 
+  protected static final Set<String> doesntSupportOffsets = new HashSet<>(Arrays.asList(
     "Lucene3x",
     "MockFixedIntBlock",
     "MockVariableIntBlock",
@@ -392,7 +392,7 @@ public abstract class LuceneTestCase extends Assert {
     }
 
     ignoreAfterMaxFailuresDelegate = 
-        new AtomicReference<TestRuleIgnoreAfterMaxFailures>(
+        new AtomicReference<>(
             new TestRuleIgnoreAfterMaxFailures(maxFailures));
     ignoreAfterMaxFailures = TestRuleDelegate.of(ignoreAfterMaxFailuresDelegate);
   }
@@ -414,7 +414,7 @@ public abstract class LuceneTestCase extends Assert {
 
   /** By-name list of ignored types like loggers etc. */
   private final static Set<String> STATIC_LEAK_IGNORED_TYPES = 
-      Collections.unmodifiableSet(new HashSet<String>(Arrays.asList(
+      Collections.unmodifiableSet(new HashSet<>(Arrays.asList(
       "org.slf4j.Logger",
       "org.apache.solr.SolrLogFormatter",
       EnumSet.class.getName())));
@@ -691,7 +691,7 @@ public abstract class LuceneTestCase extends Assert {
    */
   @SafeVarargs @SuppressWarnings("varargs")
   public static <T> Set<T> asSet(T... args) {
-    return new HashSet<T>(Arrays.asList(args));
+    return new HashSet<>(Arrays.asList(args));
   }
 
   /**
@@ -1203,13 +1203,13 @@ public abstract class LuceneTestCase extends Assert {
             break;
           case 3:
             final AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);
-            final List<String> allFields = new ArrayList<String>();
+            final List<String> allFields = new ArrayList<>();
             for (FieldInfo fi : ar.getFieldInfos()) {
               allFields.add(fi.name);
             }
             Collections.shuffle(allFields, random);
             final int end = allFields.isEmpty() ? 0 : random.nextInt(allFields.size());
-            final Set<String> fields = new HashSet<String>(allFields.subList(0, end));
+            final Set<String> fields = new HashSet<>(allFields.subList(0, end));
             // will create no FC insanity as ParallelAtomicReader has own cache key:
             r = new ParallelAtomicReader(
               new FieldFilterAtomicReader(ar, fields, false),
@@ -1735,7 +1735,7 @@ public abstract class LuceneTestCase extends Assert {
     Random random = random();
 
     // collect this number of terms from the left side
-    HashSet<BytesRef> tests = new HashSet<BytesRef>();
+    HashSet<BytesRef> tests = new HashSet<>();
     int numPasses = 0;
     while (numPasses < 10 && tests.size() < numTests) {
       leftEnum = leftTerms.iterator(leftEnum);
@@ -1778,7 +1778,7 @@ public abstract class LuceneTestCase extends Assert {
 
     rightEnum = rightTerms.iterator(rightEnum);
 
-    ArrayList<BytesRef> shuffledTests = new ArrayList<BytesRef>(tests);
+    ArrayList<BytesRef> shuffledTests = new ArrayList<>(tests);
     Collections.shuffle(shuffledTests, random);
 
     for (BytesRef b : shuffledTests) {
@@ -1897,7 +1897,7 @@ public abstract class LuceneTestCase extends Assert {
   }
 
   private static Set<String> getDVFields(IndexReader reader) {
-    Set<String> fields = new HashSet<String>();
+    Set<String> fields = new HashSet<>();
     for(FieldInfo fi : MultiFields.getMergedFieldInfos(reader)) {
       if (fi.hasDocValues()) {
         fields.add(fi.name);
@@ -2050,8 +2050,8 @@ public abstract class LuceneTestCase extends Assert {
     FieldInfos rightInfos = MultiFields.getMergedFieldInfos(rightReader);
     
     // TODO: would be great to verify more than just the names of the fields!
-    TreeSet<String> left = new TreeSet<String>();
-    TreeSet<String> right = new TreeSet<String>();
+    TreeSet<String> left = new TreeSet<>();
+    TreeSet<String> right = new TreeSet<>();
     
     for (FieldInfo fi : leftInfos) {
       left.add(fi.name);
diff --git lucene/test-framework/src/java/org/apache/lucene/util/RunListenerPrintReproduceInfo.java lucene/test-framework/src/java/org/apache/lucene/util/RunListenerPrintReproduceInfo.java
index 32f2cc0..6041d22e 100644
--- lucene/test-framework/src/java/org/apache/lucene/util/RunListenerPrintReproduceInfo.java
+++ lucene/test-framework/src/java/org/apache/lucene/util/RunListenerPrintReproduceInfo.java
@@ -42,7 +42,7 @@ public final class RunListenerPrintReproduceInfo extends RunListener {
    * A list of all test suite classes executed so far in this JVM (ehm, 
    * under this class's classloader).
    */
-  private static List<String> testClassesRun = new ArrayList<String>();
+  private static List<String> testClassesRun = new ArrayList<>();
 
   /**
    * The currently executing scope.
diff --git lucene/test-framework/src/java/org/apache/lucene/util/TestRuleDelegate.java lucene/test-framework/src/java/org/apache/lucene/util/TestRuleDelegate.java
index 08d969a..53d1d43 100644
--- lucene/test-framework/src/java/org/apache/lucene/util/TestRuleDelegate.java
+++ lucene/test-framework/src/java/org/apache/lucene/util/TestRuleDelegate.java
@@ -40,6 +40,6 @@ final class TestRuleDelegate<T extends TestRule> implements TestRule {
   }
 
   static <T extends TestRule> TestRuleDelegate<T> of(AtomicReference<T> delegate) {
-    return new TestRuleDelegate<T>(delegate);
+    return new TestRuleDelegate<>(delegate);
   }
 }
diff --git lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java
index 5d61527..121c1c0 100644
--- lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java
+++ lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java
@@ -72,7 +72,7 @@ final class TestRuleSetupAndRestoreClassEnv extends AbstractBeforeAfterRule {
   /**
    * Restore these system property values.
    */
-  private HashMap<String, String> restoreProperties = new HashMap<String,String>();
+  private HashMap<String, String> restoreProperties = new HashMap<>();
 
   private Codec savedCodec;
   private Locale savedLocale;
@@ -149,7 +149,7 @@ final class TestRuleSetupAndRestoreClassEnv extends AbstractBeforeAfterRule {
     }
 
     Class<?> targetClass = RandomizedContext.current().getTargetClass();
-    avoidCodecs = new HashSet<String>();
+    avoidCodecs = new HashSet<>();
     if (targetClass.isAnnotationPresent(SuppressCodecs.class)) {
       SuppressCodecs a = targetClass.getAnnotation(SuppressCodecs.class);
       avoidCodecs.addAll(Arrays.asList(a.value()));
diff --git lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java
index 0f4337a..e7718da 100644
--- lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java
+++ lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java
@@ -799,7 +799,7 @@ public final class TestUtil {
    * @param reflectedValues contains a map with "AttributeClass#key" as values
    */
   public static <T> void assertAttributeReflection(final AttributeImpl att, Map<String,T> reflectedValues) {
-    final Map<String,Object> map = new HashMap<String,Object>();
+    final Map<String,Object> map = new HashMap<>();
     att.reflectWith(new AttributeReflector() {
       @Override
       public void reflect(Class<? extends Attribute> attClass, String key, Object value) {
diff --git lucene/test-framework/src/java/org/apache/lucene/util/automaton/AutomatonTestUtil.java lucene/test-framework/src/java/org/apache/lucene/util/automaton/AutomatonTestUtil.java
index 67f5eaa..e55d8ed 100644
--- lucene/test-framework/src/java/org/apache/lucene/util/automaton/AutomatonTestUtil.java
+++ lucene/test-framework/src/java/org/apache/lucene/util/automaton/AutomatonTestUtil.java
@@ -157,11 +157,11 @@ public class AutomatonTestUtil {
 
       // must use IdentityHashmap because two Transitions w/
       // different start nodes can be considered the same
-      leadsToAccept = new IdentityHashMap<Transition,Boolean>();
-      final Map<State,List<ArrivingTransition>> allArriving = new HashMap<State,List<ArrivingTransition>>();
+      leadsToAccept = new IdentityHashMap<>();
+      final Map<State,List<ArrivingTransition>> allArriving = new HashMap<>();
 
-      final LinkedList<State> q = new LinkedList<State>();
-      final Set<State> seen = new HashSet<State>();
+      final LinkedList<State> q = new LinkedList<>();
+      final Set<State> seen = new HashSet<>();
 
       // reverse map the transitions, so we can quickly look
       // up all arriving transitions to a given state
@@ -170,7 +170,7 @@ public class AutomatonTestUtil {
           final Transition t = s.transitionsArray[i];
           List<ArrivingTransition> tl = allArriving.get(t.to);
           if (tl == null) {
-            tl = new ArrayList<ArrivingTransition>();
+            tl = new ArrayList<>();
             allArriving.put(t.to, tl);
           }
           tl.add(new ArrivingTransition(s, t));
@@ -201,7 +201,7 @@ public class AutomatonTestUtil {
 
     public int[] getRandomAcceptedString(Random r) {
 
-      final List<Integer> soFar = new ArrayList<Integer>();
+      final List<Integer> soFar = new ArrayList<>();
       if (a.isSingleton()) {
         // accepts only one
         final String s = a.singleton;
@@ -239,7 +239,7 @@ public class AutomatonTestUtil {
           if (cheat) {
             // pick a transition that we know is the fastest
             // path to an accept state
-            List<Transition> toAccept = new ArrayList<Transition>();
+            List<Transition> toAccept = new ArrayList<>();
             for(int i=0;i<s.numTransitions;i++) {
               final Transition t0 = s.transitionsArray[i];
               if (leadsToAccept.containsKey(t0)) {
@@ -334,7 +334,7 @@ public class AutomatonTestUtil {
   public static void determinizeSimple(Automaton a) {
     if (a.deterministic || a.isSingleton())
       return;
-    Set<State> initialset = new HashSet<State>();
+    Set<State> initialset = new HashSet<>();
     initialset.add(a.initial);
     determinizeSimple(a, initialset);
   }
@@ -346,9 +346,9 @@ public class AutomatonTestUtil {
   public static void determinizeSimple(Automaton a, Set<State> initialset) {
     int[] points = a.getStartPoints();
     // subset construction
-    Map<Set<State>, Set<State>> sets = new HashMap<Set<State>, Set<State>>();
-    LinkedList<Set<State>> worklist = new LinkedList<Set<State>>();
-    Map<Set<State>, State> newstate = new HashMap<Set<State>, State>();
+    Map<Set<State>, Set<State>> sets = new HashMap<>();
+    LinkedList<Set<State>> worklist = new LinkedList<>();
+    Map<Set<State>, State> newstate = new HashMap<>();
     sets.put(initialset, initialset);
     worklist.add(initialset);
     a.initial = new State();
@@ -362,7 +362,7 @@ public class AutomatonTestUtil {
           break;
         }
       for (int n = 0; n < points.length; n++) {
-        Set<State> p = new HashSet<State>();
+        Set<State> p = new HashSet<>();
         for (State q : s)
           for (Transition t : q.getTransitions())
             if (t.min <= points[n] && points[n] <= t.max)
diff --git lucene/test-framework/src/java/org/apache/lucene/util/fst/FSTTester.java lucene/test-framework/src/java/org/apache/lucene/util/fst/FSTTester.java
index bf5c8ac..bb72ab0 100644
--- lucene/test-framework/src/java/org/apache/lucene/util/fst/FSTTester.java
+++ lucene/test-framework/src/java/org/apache/lucene/util/fst/FSTTester.java
@@ -236,7 +236,7 @@ public class FSTTester<T> {
   private T randomAcceptedWord(FST<T> fst, IntsRef in) throws IOException {
     FST.Arc<T> arc = fst.getFirstArc(new FST.Arc<T>());
 
-    final List<FST.Arc<T>> arcs = new ArrayList<FST.Arc<T>>();
+    final List<FST.Arc<T>> arcs = new ArrayList<>();
     in.length = 0;
     in.offset = 0;
     final T NO_OUTPUT = fst.outputs.getNoOutput();
@@ -281,7 +281,7 @@ public class FSTTester<T> {
 
     final boolean willRewrite = random.nextBoolean();
 
-    final Builder<T> builder = new Builder<T>(inputMode == 0 ? FST.INPUT_TYPE.BYTE1 : FST.INPUT_TYPE.BYTE4,
+    final Builder<T> builder = new Builder<>(inputMode == 0 ? FST.INPUT_TYPE.BYTE1 : FST.INPUT_TYPE.BYTE4,
                                               prune1, prune2,
                                               prune1==0 && prune2==0,
                                               allowRandomSuffixSharing ? random.nextBoolean() : true,
@@ -320,7 +320,7 @@ public class FSTTester<T> {
       out.close();
       IndexInput in = dir.openInput("fst.bin", context);
       try {
-        fst = new FST<T>(in, outputs);
+        fst = new FST<>(in, outputs);
       } finally {
         in.close();
         dir.deleteFile("fst.bin");
@@ -366,7 +366,7 @@ public class FSTTester<T> {
     if (doReverseLookup) {
       @SuppressWarnings("unchecked") FST<Long> fstLong0 = (FST<Long>) fst;
       fstLong = fstLong0;
-      validOutputs = new HashSet<Long>();
+      validOutputs = new HashSet<>();
       for(InputOutput<T> pair: pairs) {
         Long output = (Long) pair.output;
         maxLong = Math.max(maxLong, output);
@@ -402,7 +402,7 @@ public class FSTTester<T> {
       System.out.println("TEST: check valid terms/next()");
     }
     {
-      IntsRefFSTEnum<T> fstEnum = new IntsRefFSTEnum<T>(fst);
+      IntsRefFSTEnum<T> fstEnum = new IntsRefFSTEnum<>(fst);
       for(InputOutput<T> pair : pairs) {
         IntsRef term = pair.input;
         if (LuceneTestCase.VERBOSE) {
@@ -421,7 +421,7 @@ public class FSTTester<T> {
       assertNull(fstEnum.next());
     }
 
-    final Map<IntsRef,T> termsMap = new HashMap<IntsRef,T>();
+    final Map<IntsRef,T> termsMap = new HashMap<>();
     for(InputOutput<T> pair : pairs) {
       termsMap.put(pair.input, pair.output);
     }
@@ -464,7 +464,7 @@ public class FSTTester<T> {
     if (LuceneTestCase.VERBOSE) {
       System.out.println("TEST: verify seek");
     }
-    IntsRefFSTEnum<T> fstEnum = new IntsRefFSTEnum<T>(fst);
+    IntsRefFSTEnum<T> fstEnum = new IntsRefFSTEnum<>(fst);
     num = LuceneTestCase.atLeast(random, 100);
     for(int iter=0;iter<num;iter++) {
       if (LuceneTestCase.VERBOSE) {
@@ -556,7 +556,7 @@ public class FSTTester<T> {
         System.out.println("TEST: iter " + iter);
       }
       // reset:
-      fstEnum = new IntsRefFSTEnum<T>(fst);
+      fstEnum = new IntsRefFSTEnum<>(fst);
       int upto = -1;
       while(true) {
         boolean isDone = false;
@@ -682,7 +682,7 @@ public class FSTTester<T> {
     //System.out.println("TEST: tally prefixes");
 
     // build all prefixes
-    final Map<IntsRef,CountMinOutput<T>> prefixes = new HashMap<IntsRef,CountMinOutput<T>>();
+    final Map<IntsRef,CountMinOutput<T>> prefixes = new HashMap<>();
     final IntsRef scratch = new IntsRef(10);
     for(InputOutput<T> pair: pairs) {
       scratch.copyInts(pair.input);
@@ -690,7 +690,7 @@ public class FSTTester<T> {
         scratch.length = idx;
         CountMinOutput<T> cmo = prefixes.get(scratch);
         if (cmo == null) {
-          cmo = new CountMinOutput<T>();
+          cmo = new CountMinOutput<>();
           cmo.count = 1;
           cmo.output = pair.output;
           prefixes.put(IntsRef.deepCopyOf(scratch), cmo);
@@ -787,7 +787,7 @@ public class FSTTester<T> {
     if (LuceneTestCase.VERBOSE) {
       System.out.println("TEST: check pruned enum");
     }
-    IntsRefFSTEnum<T> fstEnum = new IntsRefFSTEnum<T>(fst);
+    IntsRefFSTEnum<T> fstEnum = new IntsRefFSTEnum<>(fst);
     IntsRefFSTEnum.InputOutput<T> current;
     while((current = fstEnum.next()) != null) {
       if (LuceneTestCase.VERBOSE) {
diff --git lucene/tools/src/java/org/apache/lucene/dependencies/GetMavenDependenciesTask.java lucene/tools/src/java/org/apache/lucene/dependencies/GetMavenDependenciesTask.java
index 553d73b..33402d0 100644
--- lucene/tools/src/java/org/apache/lucene/dependencies/GetMavenDependenciesTask.java
+++ lucene/tools/src/java/org/apache/lucene/dependencies/GetMavenDependenciesTask.java
@@ -89,9 +89,9 @@ public class GetMavenDependenciesTask extends Task {
   private static final String DEPENDENCY_MANAGEMENT_PROPERTY = "lucene.solr.dependency.management";
   private static final String IVY_USER_DIR_PROPERTY = "ivy.default.ivy.user.dir";
   private static final Properties allProperties = new Properties();
-  private static final Set<String> modulesWithSeparateCompileAndTestPOMs = new HashSet<String>();
+  private static final Set<String> modulesWithSeparateCompileAndTestPOMs = new HashSet<>();
 
-  private static final Set<String>  optionalExternalDependencies = new HashSet<String>();
+  private static final Set<String>  optionalExternalDependencies = new HashSet<>();
   static {
     // Add modules here that have split compile and test POMs
     // - they need compile-scope deps to also be test-scope deps.
@@ -106,13 +106,13 @@ public class GetMavenDependenciesTask extends Task {
 
   private final XPath xpath = XPathFactory.newInstance().newXPath();
   private final SortedMap<String,SortedSet<String>> internalCompileScopeDependencies
-      = new TreeMap<String,SortedSet<String>>();
-  private final Set<String> nonJarDependencies = new HashSet<String>();
-  private final Map<String,Set<String>> dependencyClassifiers = new HashMap<String,Set<String>>();
-  private final Map<String,Set<String>> interModuleExternalCompileScopeDependencies = new HashMap<String,Set<String>>();
-  private final Map<String,Set<String>> interModuleExternalTestScopeDependencies = new HashMap<String,Set<String>>();
+      = new TreeMap<>();
+  private final Set<String> nonJarDependencies = new HashSet<>();
+  private final Map<String,Set<String>> dependencyClassifiers = new HashMap<>();
+  private final Map<String,Set<String>> interModuleExternalCompileScopeDependencies = new HashMap<>();
+  private final Map<String,Set<String>> interModuleExternalTestScopeDependencies = new HashMap<>();
   private final Map<String,SortedSet<ExternalDependency>> allExternalDependencies
-     = new HashMap<String,SortedSet<ExternalDependency>>();
+     = new HashMap<>();
   private final DocumentBuilder documentBuilder;
   private File ivyCacheDir;
   private Pattern internalJarPattern;
@@ -257,7 +257,7 @@ public class GetMavenDependenciesTask extends Task {
   private void addSharedExternalDependencies() {
     // Delay adding shared compile-scope dependencies until after all have been processed,
     // so dependency sharing is limited to a depth of one.
-    Map<String,SortedSet<ExternalDependency>> sharedDependencies = new HashMap<String,SortedSet<ExternalDependency>>();
+    Map<String,SortedSet<ExternalDependency>> sharedDependencies = new HashMap<>();
     for (String artifactId : interModuleExternalCompileScopeDependencies.keySet()) {
       TreeSet<ExternalDependency> deps = new TreeSet<>();
       sharedDependencies.put(artifactId, deps);
@@ -278,7 +278,7 @@ public class GetMavenDependenciesTask extends Task {
     for (String artifactId : interModuleExternalTestScopeDependencies.keySet()) {
       SortedSet<ExternalDependency> deps = sharedDependencies.get(artifactId);
       if (null == deps) {
-        deps = new TreeSet<ExternalDependency>();
+        deps = new TreeSet<>();
         sharedDependencies.put(artifactId, deps);
       }
       Set<String> moduleDependencies = interModuleExternalTestScopeDependencies.get(artifactId);
@@ -311,7 +311,7 @@ public class GetMavenDependenciesTask extends Task {
     for (String artifactId : sharedDependencies.keySet()) {
       SortedSet<ExternalDependency> deps = allExternalDependencies.get(artifactId);
       if (null == deps) {
-        deps = new TreeSet<ExternalDependency>();
+        deps = new TreeSet<>();
         allExternalDependencies.put(artifactId, deps);
       }
       deps.addAll(sharedDependencies.get(artifactId));
@@ -360,7 +360,7 @@ public class GetMavenDependenciesTask extends Task {
   private void setGrandparentDependencyManagementProperty() {
     StringBuilder builder = new StringBuilder();
     appendAllInternalDependencies(builder);
-    Map<String,String> versionsMap = new HashMap<String,String>();
+    Map<String,String> versionsMap = new HashMap<>();
     appendAllExternalDependencies(builder, versionsMap);
     builder.setLength(builder.length() - 1); // drop trailing newline
     allProperties.setProperty(DEPENDENCY_MANAGEMENT_PROPERTY, builder.toString());
@@ -377,7 +377,7 @@ public class GetMavenDependenciesTask extends Task {
    */
   private void appendAllInternalDependencies(StringBuilder builder) {
     for (String artifactId : internalCompileScopeDependencies.keySet()) {
-      List<String> exclusions = new ArrayList<String>();
+      List<String> exclusions = new ArrayList<>();
       exclusions.addAll(internalCompileScopeDependencies.get(artifactId));
       SortedSet<ExternalDependency> extDeps = allExternalDependencies.get(artifactId);
       if (null != extDeps) {
@@ -427,7 +427,7 @@ public class GetMavenDependenciesTask extends Task {
     log("Loading centralized ivy versions from: " + centralizedVersionsFile, verboseLevel);
     ivyCacheDir = getIvyCacheDir();
     Properties versions = loadPropertiesFile(centralizedVersionsFile);
-    SortedSet<Map.Entry> sortedEntries = new TreeSet<Map.Entry>(new Comparator<Map.Entry>() {
+    SortedSet<Map.Entry> sortedEntries = new TreeSet<>(new Comparator<Map.Entry>() {
       @Override public int compare(Map.Entry o1, Map.Entry o2) {
         return ((String)o1.getKey()).compareTo((String)o2.getKey());
       }
@@ -465,7 +465,7 @@ public class GetMavenDependenciesTask extends Task {
    */
   private Collection<String> getTransitiveDependenciesFromIvyCache
   (String groupId, String artifactId, String version) {
-    SortedSet<String> transitiveDependencies = new TreeSet<String>();
+    SortedSet<String> transitiveDependencies = new TreeSet<>();
     //                                      E.g. ~/.ivy2/cache/xerces/xercesImpl/ivy-2.9.1.xml
     File ivyXmlFile = new File(new File(new File(ivyCacheDir, groupId), artifactId), "ivy-" + version + ".xml");
     if ( ! ivyXmlFile.exists()) {
@@ -500,8 +500,8 @@ public class GetMavenDependenciesTask extends Task {
   private void  setInternalDependencyProperties() {
     log("Loading module dependencies from: " + moduleDependenciesPropertiesFile, verboseLevel);
     Properties moduleDependencies = loadPropertiesFile(moduleDependenciesPropertiesFile);
-    Map<String,SortedSet<String>> testScopeDependencies = new HashMap<String,SortedSet<String>>();
-    Map<String, String> testScopePropertyKeys = new HashMap<String,String>();
+    Map<String,SortedSet<String>> testScopeDependencies = new HashMap<>();
+    Map<String, String> testScopePropertyKeys = new HashMap<>();
     for (Map.Entry entry : moduleDependencies.entrySet()) {
       String newPropertyKey = (String)entry.getKey();
       StringBuilder newPropertyValue = new StringBuilder();
@@ -527,7 +527,7 @@ public class GetMavenDependenciesTask extends Task {
         String origModuleDir = antProjectName.replace("analyzers-", "analysis/");
         Pattern unwantedInternalDependencies = Pattern.compile
             ("(?:lucene/build/|solr/build/(?:contrib/)?)" + origModuleDir + "|" + UNWANTED_INTERNAL_DEPENDENCIES);
-        SortedSet<String> sortedDeps = new TreeSet<String>();
+        SortedSet<String> sortedDeps = new TreeSet<>();
         for (String dependency : value.split(",")) {
           matcher = SHARED_EXTERNAL_DEPENDENCIES_PATTERN.matcher(dependency);
           if (matcher.find()) {
@@ -542,7 +542,7 @@ public class GetMavenDependenciesTask extends Task {
                   = isTest ? interModuleExternalTestScopeDependencies : interModuleExternalCompileScopeDependencies;
               Set<String> sharedSet = sharedDeps.get(artifactName);
               if (null == sharedSet) {
-                sharedSet = new HashSet<String>();
+                sharedSet = new HashSet<>();
                 sharedDeps.put(artifactName, sharedSet);
               }
               if (isTestScope) {
@@ -675,7 +675,7 @@ public class GetMavenDependenciesTask extends Task {
       boolean isOptional = optionalExternalDependencies.contains(dependencyCoordinate);
       SortedSet<ExternalDependency> deps = allExternalDependencies.get(module);
       if (null == deps) {
-        deps = new TreeSet<ExternalDependency>();
+        deps = new TreeSet<>();
         allExternalDependencies.put(module, deps);
       }
       NodeList artifacts = null;
diff --git lucene/tools/src/java/org/apache/lucene/validation/LibVersionsCheckTask.java lucene/tools/src/java/org/apache/lucene/validation/LibVersionsCheckTask.java
index f5f234b..7741fb6 100644
--- lucene/tools/src/java/org/apache/lucene/validation/LibVersionsCheckTask.java
+++ lucene/tools/src/java/org/apache/lucene/validation/LibVersionsCheckTask.java
@@ -98,7 +98,7 @@ public class LibVersionsCheckTask extends Task {
    * All /org/name version keys found in ivy-versions.properties, and whether they
    * are referenced in any ivy.xml file.
    */
-  private Map<String,Boolean> referencedCoordinateKeys = new LinkedHashMap<String,Boolean>();
+  private Map<String,Boolean> referencedCoordinateKeys = new LinkedHashMap<>();
 
   /**
    * Adds a set of ivy.xml resources to check.
@@ -320,7 +320,7 @@ public class LibVersionsCheckTask extends Task {
 
   private class DependencyRevChecker extends DefaultHandler {
     private final File ivyXmlFile;
-    private final Stack<String> tags = new Stack<String>();
+    private final Stack<String> tags = new Stack<>();
     
     public boolean fail = false;
 
diff --git lucene/tools/src/java/org/apache/lucene/validation/LicenseCheckTask.java lucene/tools/src/java/org/apache/lucene/validation/LicenseCheckTask.java
index 21494de..7ee4a6a 100644
--- lucene/tools/src/java/org/apache/lucene/validation/LicenseCheckTask.java
+++ lucene/tools/src/java/org/apache/lucene/validation/LicenseCheckTask.java
@@ -235,8 +235,8 @@ public class LicenseCheckTask extends Task {
     }
     
     // Get the expected license path base from the mapper and search for license files.
-    Map<File, LicenseType> foundLicenses = new LinkedHashMap<File, LicenseType>();
-    List<File> expectedLocations = new ArrayList<File>();
+    Map<File, LicenseType> foundLicenses = new LinkedHashMap<>();
+    List<File> expectedLocations = new ArrayList<>();
 outer:
     for (String mappedPath : licenseMapper.mapFileName(jarFile.getName())) {
       for (LicenseType licenseType : LicenseType.values()) {
diff --git solr/contrib/analysis-extras/src/java/org/apache/solr/schema/ICUCollationField.java solr/contrib/analysis-extras/src/java/org/apache/solr/schema/ICUCollationField.java
index c1b2ce7..bed79e6 100644
--- solr/contrib/analysis-extras/src/java/org/apache/solr/schema/ICUCollationField.java
+++ solr/contrib/analysis-extras/src/java/org/apache/solr/schema/ICUCollationField.java
@@ -287,7 +287,7 @@ public class ICUCollationField extends FieldType {
   @Override
   public List<StorableField> createFields(SchemaField field, Object value, float boost) {
     if (field.hasDocValues()) {
-      List<StorableField> fields = new ArrayList<StorableField>();
+      List<StorableField> fields = new ArrayList<>();
       fields.add(createField(field, value, boost));
       final BytesRef bytes = getCollationKey(field.getName(), value.toString());
       if (field.multiValued()) {
diff --git solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/ClusteringComponent.java solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/ClusteringComponent.java
index 9d04543..1ed20f3 100644
--- solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/ClusteringComponent.java
+++ solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/ClusteringComponent.java
@@ -210,7 +210,7 @@ public class ClusteringComponent extends SearchComponent implements SolrCoreAwar
       if( fields == null || fields.size() == 0 ) return;
       StringBuilder sb = new StringBuilder();
       String[] flparams = fl.split( "[,\\s]+" );
-      Set<String> flParamSet = new HashSet<String>(flparams.length);
+      Set<String> flParamSet = new HashSet<>(flparams.length);
       for( String flparam : flparams ){
         // no need trim() because of split() by \s+
         flParamSet.add(flparam);
diff --git solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/carrot2/CarrotClusteringEngine.java solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/carrot2/CarrotClusteringEngine.java
index 6147fb4..3440ae6 100644
--- solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/carrot2/CarrotClusteringEngine.java
+++ solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/carrot2/CarrotClusteringEngine.java
@@ -123,7 +123,7 @@ public class CarrotClusteringEngine extends SearchClusteringEngine {
     final SolrParams initParams = SolrParams.toSolrParams(config);
 
     // Initialization attributes for Carrot2 controller.
-    HashMap<String, Object> initAttributes = new HashMap<String, Object>();
+    HashMap<String, Object> initAttributes = new HashMap<>();
 
     // Customize Carrot2's resource lookup to first look for resources
     // using Solr's resource loader. If that fails, try loading from the classpath.
@@ -221,7 +221,7 @@ public class CarrotClusteringEngine extends SearchClusteringEngine {
       Map<SolrDocument, Integer> docIds, SolrQueryRequest sreq) {
     try {
       // Prepare attributes for Carrot2 clustering call
-      Map<String, Object> attributes = new HashMap<String, Object>();
+      Map<String, Object> attributes = new HashMap<>();
       List<Document> documents = getDocuments(solrDocList, docIds, query, sreq);
       attributes.put(AttributeNames.DOCUMENTS, documents);
       attributes.put(AttributeNames.QUERY, query.toString());
@@ -350,7 +350,7 @@ public class CarrotClusteringEngine extends SearchClusteringEngine {
     }
 
     Iterator<SolrDocument> docsIter = solrDocList.iterator();
-    List<Document> result = new ArrayList<Document>(solrDocList.size());
+    List<Document> result = new ArrayList<>(solrDocList.size());
 
     float[] scores = {1.0f};
     int[] docsHolder = new int[1];
@@ -500,7 +500,7 @@ public class CarrotClusteringEngine extends SearchClusteringEngine {
   private void clustersToNamedList(List<Cluster> outputClusters,
                                    List<NamedList<Object>> parent, boolean outputSubClusters, int maxLabels) {
     for (Cluster outCluster : outputClusters) {
-      NamedList<Object> cluster = new SimpleOrderedMap<Object>();
+      NamedList<Object> cluster = new SimpleOrderedMap<>();
       parent.add(cluster);
 
       // Add labels
diff --git solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/carrot2/LuceneCarrot2StemmerFactory.java solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/carrot2/LuceneCarrot2StemmerFactory.java
index d0e7abd..82887a1 100644
--- solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/carrot2/LuceneCarrot2StemmerFactory.java
+++ solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/carrot2/LuceneCarrot2StemmerFactory.java
@@ -84,7 +84,7 @@ public class LuceneCarrot2StemmerFactory implements IStemmerFactory {
      */
     private static HashMap<LanguageCode, Class<? extends SnowballProgram>> snowballStemmerClasses;
     static {
-      snowballStemmerClasses = new HashMap<LanguageCode, Class<? extends SnowballProgram>>();
+      snowballStemmerClasses = new HashMap<>();
       snowballStemmerClasses.put(LanguageCode.DANISH, DanishStemmer.class);
       snowballStemmerClasses.put(LanguageCode.DUTCH, DutchStemmer.class);
       snowballStemmerClasses.put(LanguageCode.ENGLISH, EnglishStemmer.class);
diff --git solr/contrib/clustering/src/test/org/apache/solr/handler/clustering/ClusteringComponentTest.java solr/contrib/clustering/src/test/org/apache/solr/handler/clustering/ClusteringComponentTest.java
index e671d77..791fc05 100644
--- solr/contrib/clustering/src/test/org/apache/solr/handler/clustering/ClusteringComponentTest.java
+++ solr/contrib/clustering/src/test/org/apache/solr/handler/clustering/ClusteringComponentTest.java
@@ -52,7 +52,7 @@ public class ClusteringComponentTest extends AbstractClusteringTestCase {
     SolrRequestHandler handler = core.getRequestHandler("standard");
     SolrQueryResponse rsp;
     rsp = new SolrQueryResponse();
-    rsp.add("responseHeader", new SimpleOrderedMap<Object>());
+    rsp.add("responseHeader", new SimpleOrderedMap<>());
     SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
     handler.handleRequest(req, rsp);
     NamedList<?> values = rsp.getValues();
@@ -70,7 +70,7 @@ public class ClusteringComponentTest extends AbstractClusteringTestCase {
     handler = core.getRequestHandler("docClustering");
 
     rsp = new SolrQueryResponse();
-    rsp.add("responseHeader", new SimpleOrderedMap<Object>());
+    rsp.add("responseHeader", new SimpleOrderedMap<>());
     req = new LocalSolrQueryRequest(core, params);
     handler.handleRequest(req, rsp);
     values = rsp.getValues();
diff --git solr/contrib/clustering/src/test/org/apache/solr/handler/clustering/carrot2/CarrotClusteringEngineTest.java solr/contrib/clustering/src/test/org/apache/solr/handler/clustering/carrot2/CarrotClusteringEngineTest.java
index 6334d63..2a55f65 100644
--- solr/contrib/clustering/src/test/org/apache/solr/handler/clustering/carrot2/CarrotClusteringEngineTest.java
+++ solr/contrib/clustering/src/test/org/apache/solr/handler/clustering/carrot2/CarrotClusteringEngineTest.java
@@ -472,7 +472,7 @@ public class CarrotClusteringEngineTest extends AbstractClusteringTestCase {
 
       // Perform clustering
       LocalSolrQueryRequest req = new LocalSolrQueryRequest(h.getCore(), solrParams);
-      Map<SolrDocument,Integer> docIds = new HashMap<SolrDocument, Integer>(docList.size());
+      Map<SolrDocument,Integer> docIds = new HashMap<>(docList.size());
       SolrDocumentList solrDocList = SolrPluginUtils.docListToSolrDocumentList( docList, searcher, engine.getFieldsToLoad(req), docIds );
 
       @SuppressWarnings("unchecked")
diff --git solr/contrib/dataimporthandler-extras/src/java/org/apache/solr/handler/dataimport/MailEntityProcessor.java solr/contrib/dataimporthandler-extras/src/java/org/apache/solr/handler/dataimport/MailEntityProcessor.java
index 1c4d021..756b764 100644
--- solr/contrib/dataimporthandler-extras/src/java/org/apache/solr/handler/dataimport/MailEntityProcessor.java
+++ solr/contrib/dataimporthandler-extras/src/java/org/apache/solr/handler/dataimport/MailEntityProcessor.java
@@ -140,7 +140,7 @@ public class MailEntityProcessor extends EntityProcessorBase {
   }
 
   private Map<String, Object> getDocumentFromMail(Message mail) {
-    Map<String, Object> row = new HashMap<String, Object>();
+    Map<String, Object> row = new HashMap<>();
     try {
       addPartToDocument(mail, row, true);
       return row;
@@ -201,7 +201,7 @@ public class MailEntityProcessor extends EntityProcessorBase {
     if ((adresses = mail.getFrom()) != null && adresses.length > 0)
       row.put(FROM, adresses[0].toString());
 
-    List<String> to = new ArrayList<String>();
+    List<String> to = new ArrayList<>();
     if ((adresses = mail.getRecipients(Message.RecipientType.TO)) != null)
       addAddressToList(adresses, to);
     if ((adresses = mail.getRecipients(Message.RecipientType.CC)) != null)
@@ -219,7 +219,7 @@ public class MailEntityProcessor extends EntityProcessorBase {
       row.put(SENT_DATE, d);
     }
 
-    List<String> flags = new ArrayList<String>();
+    List<String> flags = new ArrayList<>();
     for (Flags.Flag flag : mail.getFlags().getSystemFlags()) {
       if (flag == Flags.Flag.ANSWERED)
         flags.add(FLAG_ANSWERED);
@@ -319,7 +319,7 @@ public class MailEntityProcessor extends EntityProcessorBase {
 
     public FolderIterator(Store mailBox) {
       this.mailbox = mailBox;
-      folders = new ArrayList<Folder>();
+      folders = new ArrayList<>();
       getTopLevelFolders(mailBox);
     }
 
@@ -529,8 +529,8 @@ public class MailEntityProcessor extends EntityProcessorBase {
   private String protocol;
 
   private String folderNames;
-  private List<String> exclude = new ArrayList<String>();
-  private List<String> include = new ArrayList<String>();
+  private List<String> exclude = new ArrayList<>();
+  private List<String> include = new ArrayList<>();
   private boolean recurse;
 
   private int batchSize;
@@ -550,7 +550,7 @@ public class MailEntityProcessor extends EntityProcessorBase {
   private boolean connected = false;
   private FolderIterator folderIter;
   private MessageIterator msgIter;
-  private List<CustomFilter> filters = new ArrayList<CustomFilter>();
+  private List<CustomFilter> filters = new ArrayList<>();
   private static FetchProfile fp = new FetchProfile();
   private static final Logger LOG = LoggerFactory.getLogger(DataImporter.class);
 
diff --git solr/contrib/dataimporthandler-extras/src/java/org/apache/solr/handler/dataimport/TikaEntityProcessor.java solr/contrib/dataimporthandler-extras/src/java/org/apache/solr/handler/dataimport/TikaEntityProcessor.java
index 3e957ca..20121d4 100644
--- solr/contrib/dataimporthandler-extras/src/java/org/apache/solr/handler/dataimport/TikaEntityProcessor.java
+++ solr/contrib/dataimporthandler-extras/src/java/org/apache/solr/handler/dataimport/TikaEntityProcessor.java
@@ -107,7 +107,7 @@ public class TikaEntityProcessor extends EntityProcessorBase {
   @Override
   public Map<String, Object> nextRow() {
     if(done) return null;
-    Map<String, Object> row = new HashMap<String, Object>();
+    Map<String, Object> row = new HashMap<>();
     DataSource<InputStream> dataSource = context.getDataSource();
     InputStream is = dataSource.getData(context.getResolvedEntityAttribute(URL));
     ContentHandler contentHandler = null;
diff --git solr/contrib/dataimporthandler-extras/src/test/org/apache/solr/handler/dataimport/TestMailEntityProcessor.java solr/contrib/dataimporthandler-extras/src/test/org/apache/solr/handler/dataimport/TestMailEntityProcessor.java
index a6273c0..e595c1e 100644
--- solr/contrib/dataimporthandler-extras/src/test/org/apache/solr/handler/dataimport/TestMailEntityProcessor.java
+++ solr/contrib/dataimporthandler-extras/src/test/org/apache/solr/handler/dataimport/TestMailEntityProcessor.java
@@ -53,7 +53,7 @@ public class TestMailEntityProcessor extends AbstractDataImportHandlerTestCase {
   private static final String host = "host";
   private static final String protocol = "imaps";
 
-  private static Map<String, String> paramMap = new HashMap<String, String>();
+  private static Map<String, String> paramMap = new HashMap<>();
 
   @Test
   @Ignore("Needs a Mock Mail Server to work")
@@ -172,7 +172,7 @@ public class TestMailEntityProcessor extends AbstractDataImportHandlerTestCase {
   }
 
   static class SolrWriterImpl extends SolrWriter {
-    List<SolrInputDocument> docs = new ArrayList<SolrInputDocument>();
+    List<SolrInputDocument> docs = new ArrayList<>();
     Boolean deleteAllCalled;
     Boolean commitCalled;
 
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/ClobTransformer.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/ClobTransformer.java
index 690e9db..ac0d393 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/ClobTransformer.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/ClobTransformer.java
@@ -47,7 +47,7 @@ public class ClobTransformer extends Transformer {
       Object o = aRow.get(srcCol);
       if (o instanceof List) {
         List<Clob> inputs = (List<Clob>) o;
-        List<String> results = new ArrayList<String>();
+        List<String> results = new ArrayList<>();
         for (Object input : inputs) {
           if (input instanceof Clob) {
             Clob clob = (Clob) input;
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/ConfigParseUtil.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/ConfigParseUtil.java
index 70a4df9..179df23 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/ConfigParseUtil.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/ConfigParseUtil.java
@@ -34,7 +34,7 @@ public class ConfigParseUtil {
   }
 
   public static HashMap<String, String> getAllAttributes(Element e) {
-    HashMap<String, String> m = new HashMap<String, String>();
+    HashMap<String, String> m = new HashMap<>();
     NamedNodeMap nnm = e.getAttributes();
     for (int i = 0; i < nnm.getLength(); i++) {
       m.put(nnm.item(i).getNodeName(), nnm.item(i).getNodeValue());
@@ -61,7 +61,7 @@ public class ConfigParseUtil {
   }
 
   public static List<Element> getChildNodes(Element e, String byName) {
-    List<Element> result = new ArrayList<Element>();
+    List<Element> result = new ArrayList<>();
     NodeList l = e.getChildNodes();
     for (int i = 0; i < l.getLength(); i++) {
       if (e.equals(l.item(i).getParentNode())
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/ContextImpl.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/ContextImpl.java
index c06401b..b233b7b 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/ContextImpl.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/ContextImpl.java
@@ -138,7 +138,7 @@ public class ContextImpl extends Context {
     }
     if (Context.SCOPE_ENTITY.equals(scope)) {
       if (entitySession == null) {
-        entitySession = new HashMap<String, Object>();
+        entitySession = new HashMap<>();
       }
       entitySession.put(name, val);
     } else if (Context.SCOPE_GLOBAL.equals(scope)) {
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHCacheSupport.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHCacheSupport.java
index caa75d9..d5e7385 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHCacheSupport.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHCacheSupport.java
@@ -36,7 +36,7 @@ public class DIHCacheSupport {
       .getLogger(DIHCacheSupport.class);
   private String cacheForeignKey;
   private String cacheImplName;
-  private Map<String,DIHCache> queryVsCache = new HashMap<String,DIHCache>();
+  private Map<String,DIHCache> queryVsCache = new HashMap<>();
   private Map<String,Iterator<Map<String,Object>>> queryVsCacheIterator;
   private Iterator<Map<String,Object>> dataSourceRowCache;
   private boolean cacheDoKeyLookup;
@@ -94,7 +94,7 @@ public class DIHCacheSupport {
   
   public void initNewParent(Context context) {
     dataSourceRowCache = null;
-    queryVsCacheIterator = new HashMap<String,Iterator<Map<String,Object>>>();
+    queryVsCacheIterator = new HashMap<>();
     for (Map.Entry<String,DIHCache> entry : queryVsCache.entrySet()) {
       queryVsCacheIterator.put(entry.getKey(), entry.getValue().iterator());
     }
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHWriterBase.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHWriterBase.java
index 12cd563..a33a202 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHWriterBase.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHWriterBase.java
@@ -27,7 +27,7 @@ public abstract class DIHWriterBase implements DIHWriter {
   
   @Override
   public void setDeltaKeys(Set<Map<String,Object>> passedInDeltaKeys) {
-    deltaKeys = new HashSet<Object>();
+    deltaKeys = new HashSet<>();
     for (Map<String,Object> aMap : passedInDeltaKeys) {
       if (aMap.size() > 0) {
         Object key = null;
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java
index 4171bb6..e5b74f3 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java
@@ -210,7 +210,7 @@ public class DataImportHandler extends RequestHandlerBase implements
 
   private Map<String, Object> getParamsMap(SolrParams params) {
     Iterator<String> names = params.getParameterNamesIterator();
-    Map<String, Object> result = new HashMap<String, Object>();
+    Map<String, Object> result = new HashMap<>();
     while (names.hasNext()) {
       String s = names.next();
       String[] val = params.getParams(s);
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImporter.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImporter.java
index 9862ba4..72512cd5 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImporter.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImporter.java
@@ -80,12 +80,12 @@ public class DataImporter {
   private DIHConfiguration config;
   private Date indexStartTime;
   private Properties store = new Properties();
-  private Map<String, Map<String,String>> requestLevelDataSourceProps = new HashMap<String, Map<String,String>>();
+  private Map<String, Map<String,String>> requestLevelDataSourceProps = new HashMap<>();
   private IndexSchema schema;
   public DocBuilder docBuilder;
   public DocBuilder.Statistics cumulativeStatistics = new DocBuilder.Statistics();
   private SolrCore core;  
-  private Map<String, Object> coreScopeSession = new ConcurrentHashMap<String,Object>();
+  private Map<String, Object> coreScopeSession = new ConcurrentHashMap<>();
   private ReentrantLock importLock = new ReentrantLock();
   private boolean isDeltaImportSupported = false;  
   private final String handlerName;  
@@ -131,7 +131,7 @@ public class DataImporter {
           success = true;
         }      
         
-        Map<String,Map<String,String>> dsProps = new HashMap<String,Map<String,String>>();
+        Map<String,Map<String,String>> dsProps = new HashMap<>();
         if(defaultParams!=null) {
           int position = 0;
           while (position < defaultParams.size()) {
@@ -143,7 +143,7 @@ public class DataImporter {
               success = true;
               NamedList dsConfig = (NamedList) defaultParams.getVal(position);
               LOG.info("Getting configuration for Global Datasource...");              
-              Map<String,String> props = new HashMap<String,String>();
+              Map<String,String> props = new HashMap<>();
               for (int i = 0; i < dsConfig.size(); i++) {
                 props.put(dsConfig.getName(i), dsConfig.getVal(i).toString());
               }
@@ -225,9 +225,9 @@ public class DataImporter {
   
   public DIHConfiguration readFromXml(Document xmlDocument) {
     DIHConfiguration config;
-    List<Map<String, String >> functions = new ArrayList<Map<String ,String>>();
+    List<Map<String, String >> functions = new ArrayList<>();
     Script script = null;
-    Map<String, Map<String,String>> dataSources = new HashMap<String, Map<String,String>>();
+    Map<String, Map<String,String>> dataSources = new HashMap<>();
     
     NodeList dataConfigTags = xmlDocument.getElementsByTagName("dataConfig");
     if(dataConfigTags == null || dataConfigTags.getLength() == 0) {
@@ -263,7 +263,7 @@ public class DataImporter {
     List<Element> dataSourceTags = ConfigParseUtil.getChildNodes(e, ConfigNameConstants.DATA_SRC);
     if (!dataSourceTags.isEmpty()) {
       for (Element element : dataSourceTags) {
-        Map<String,String> p = new HashMap<String,String>();
+        Map<String,String> p = new HashMap<>();
         HashMap<String, String> attrs = ConfigParseUtil.getAllAttributes(element);
         for (Map.Entry<String, String> entry : attrs.entrySet()) {
           p.put(entry.getKey(), entry.getValue());
@@ -294,7 +294,7 @@ public class DataImporter {
     } else {
       Element pwElement = propertyWriterTags.get(0);
       String type = null;
-      Map<String,String> params = new HashMap<String,String>();
+      Map<String,String> params = new HashMap<>();
       for (Map.Entry<String,String> entry : ConfigParseUtil.getAllAttributes(
           pwElement).entrySet()) {
         if (TYPE.equals(entry.getKey())) {
@@ -494,7 +494,7 @@ public class DataImporter {
     //this map object is a Collections.synchronizedMap(new LinkedHashMap()). if we
     // synchronize on the object it must be safe to iterate through the map
     Map statusMessages = (Map) retrieve(STATUS_MSGS);
-    Map<String, String> result = new LinkedHashMap<String, String>();
+    Map<String, String> result = new LinkedHashMap<>();
     if (statusMessages != null) {
       synchronized (statusMessages) {
         for (Object o : statusMessages.entrySet()) {
@@ -520,7 +520,7 @@ public class DataImporter {
    * used by tests.
    */
   Map<String, Evaluator> getEvaluators(List<Map<String,String>> fn) {
-    Map<String, Evaluator> evaluators = new HashMap<String, Evaluator>();
+    Map<String, Evaluator> evaluators = new HashMap<>();
     evaluators.put(Evaluator.DATE_FORMAT_EVALUATOR, new DateFormatEvaluator());
     evaluators.put(Evaluator.SQL_ESCAPE_EVALUATOR, new SqlEscapingEvaluator());
     evaluators.put(Evaluator.URL_ENCODE_EVALUATOR, new UrlEvaluator());
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DateFormatEvaluator.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DateFormatEvaluator.java
index a1f03fd..84bab52 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DateFormatEvaluator.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DateFormatEvaluator.java
@@ -52,9 +52,9 @@ import org.apache.solr.util.DateMathParser;
 public class DateFormatEvaluator extends Evaluator {
   
   public static final String DEFAULT_DATE_FORMAT = "yyyy-MM-dd HH:mm:ss";
-  Map<DateFormatCacheKey, SimpleDateFormat> cache = new WeakHashMap<DateFormatCacheKey, SimpleDateFormat>();
-  Map<String, Locale> availableLocales = new HashMap<String, Locale>();
-  Set<String> availableTimezones = new HashSet<String>();
+  Map<DateFormatCacheKey, SimpleDateFormat> cache = new WeakHashMap<>();
+  Map<String, Locale> availableLocales = new HashMap<>();
+  Set<String> availableTimezones = new HashSet<>();
   
   class DateFormatCacheKey {
     DateFormatCacheKey(Locale l, TimeZone tz, String df) {
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DateFormatTransformer.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DateFormatTransformer.java
index f67dbfa..dfc30e5 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DateFormatTransformer.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DateFormatTransformer.java
@@ -40,7 +40,7 @@ import org.slf4j.LoggerFactory;
  * @since solr 1.3
  */
 public class DateFormatTransformer extends Transformer {
-  private Map<String, SimpleDateFormat> fmtCache = new HashMap<String, SimpleDateFormat>();
+  private Map<String, SimpleDateFormat> fmtCache = new HashMap<>();
   private static final Logger LOG = LoggerFactory
           .getLogger(DateFormatTransformer.class);
 
@@ -66,7 +66,7 @@ public class DateFormatTransformer extends Transformer {
         Object o = aRow.get(srcCol);
         if (o instanceof List) {
           List inputs = (List) o;
-          List<Date> results = new ArrayList<Date>();
+          List<Date> results = new ArrayList<>();
           for (Object input : inputs) {
             results.add(process(input, fmt, locale));
           }
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DebugInfo.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DebugInfo.java
index b35fe91..f58dc6e 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DebugInfo.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DebugInfo.java
@@ -26,12 +26,12 @@ import org.apache.solr.common.util.NamedList;
 import org.apache.solr.common.util.StrUtils;
 
 public class DebugInfo {
-  public List<SolrInputDocument> debugDocuments = new ArrayList<SolrInputDocument>(0);
+  public List<SolrInputDocument> debugDocuments = new ArrayList<>(0);
   public NamedList<String> debugVerboseOutput = null;
   public boolean verbose;
   
   public DebugInfo(Map<String,Object> requestParams) {
     verbose = StrUtils.parseBool((String) requestParams.get("verbose"), false);
-    debugVerboseOutput = new NamedList<String>();
+    debugVerboseOutput = new NamedList<>();
   }
 }
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DocBuilder.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DocBuilder.java
index e41d513..1d01a6e 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DocBuilder.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DocBuilder.java
@@ -74,9 +74,9 @@ public class DocBuilder {
 
   boolean verboseDebug = false;
 
-  Map<String, Object> session = new HashMap<String, Object>();
+  Map<String, Object> session = new HashMap<>();
 
-  static final ThreadLocal<DocBuilder> INSTANCE = new ThreadLocal<DocBuilder>();
+  static final ThreadLocal<DocBuilder> INSTANCE = new ThreadLocal<>();
   private Map<String, Object> persistedProperties;
   
   private DIHProperties propWriter;
@@ -132,7 +132,7 @@ public class DocBuilder {
         resolver = new VariableResolver();
       }
       resolver.setEvaluators(dataImporter.getEvaluators());
-      Map<String, Object> indexerNamespace = new HashMap<String, Object>();
+      Map<String, Object> indexerNamespace = new HashMap<>();
       if (persistedProperties.get(LAST_INDEX_TIME) != null) {
         indexerNamespace.put(LAST_INDEX_TIME, persistedProperties.get(LAST_INDEX_TIME));
       } else  {
@@ -140,9 +140,9 @@ public class DocBuilder {
         indexerNamespace.put(LAST_INDEX_TIME, epoch);
       }
       indexerNamespace.put(INDEX_START_TIME, dataImporter.getIndexStartTime());
-      indexerNamespace.put("request", new HashMap<String,Object>(reqParams.getRawParams()));
+      indexerNamespace.put("request", new HashMap<>(reqParams.getRawParams()));
       for (Entity entity : dataImporter.getConfig().getEntities()) {
-        Map<String, Object> entityNamespace = new HashMap<String, Object>();        
+        Map<String, Object> entityNamespace = new HashMap<>();
         String key = SolrWriter.LAST_INDEX_KEY;
         Object lastIndex = persistedProperties.get(entity.getName() + "." + key);
         if (lastIndex != null) {
@@ -213,10 +213,10 @@ public class DocBuilder {
       }
       AtomicBoolean fullCleanDone = new AtomicBoolean(false);
       //we must not do a delete of *:* multiple times if there are multiple root entities to be run
-      Map<String,Object> lastIndexTimeProps = new HashMap<String,Object>();
+      Map<String,Object> lastIndexTimeProps = new HashMap<>();
       lastIndexTimeProps.put(LAST_INDEX_KEY, dataImporter.getIndexStartTime());
 
-      epwList = new ArrayList<EntityProcessorWrapper>(config.getEntities().size());
+      epwList = new ArrayList<>(config.getEntities().size());
       for (Entity e : config.getEntities()) {
         epwList.add(getEntityProcessorWrapper(e));
       }
@@ -342,7 +342,7 @@ public class DocBuilder {
 
     addStatusMessage("Identifying Delta");
     LOG.info("Starting delta collection.");
-    Set<Map<String, Object>> deletedKeys = new HashSet<Map<String, Object>>();
+    Set<Map<String, Object>> deletedKeys = new HashSet<>();
     Set<Map<String, Object>> allPks = collectDelta(currentEntityProcessorWrapper, resolver, deletedKeys);
     if (stop.get())
       return;
@@ -411,7 +411,7 @@ public class DocBuilder {
   private void buildDocument(VariableResolver vr, DocWrapper doc,
       Map<String,Object> pk, EntityProcessorWrapper epw, boolean isRoot,
       ContextImpl parentCtx) {
-    List<EntityProcessorWrapper> entitiesToDestroy = new ArrayList<EntityProcessorWrapper>();
+    List<EntityProcessorWrapper> entitiesToDestroy = new ArrayList<>();
     try {
       buildDocument(vr, doc, pk, epw, isRoot, parentCtx, entitiesToDestroy);
     } catch (Exception e) {
@@ -565,7 +565,7 @@ public class DocBuilder {
     Map<String ,Object> session;
 
     public void setSessionAttribute(String key, Object val){
-      if(session == null) session = new HashMap<String, Object>();
+      if(session == null) session = new HashMap<>();
       session.put(key, val);
     }
 
@@ -768,7 +768,7 @@ public class DocBuilder {
     ContextImpl context1 = new ContextImpl(epw, resolver, null, Context.FIND_DELTA, session, null, this);
     epw.init(context1);
 
-    Set<Map<String, Object>> myModifiedPks = new HashSet<Map<String, Object>>();
+    Set<Map<String, Object>> myModifiedPks = new HashSet<>();
 
    
 
@@ -781,7 +781,7 @@ public class DocBuilder {
     }
     
     // identifying the modified rows for this entity
-    Map<String, Map<String, Object>> deltaSet = new HashMap<String, Map<String, Object>>();
+    Map<String, Map<String, Object>> deltaSet = new HashMap<>();
     LOG.info("Running ModifiedRowKey() for Entity: " + epw.getEntity().getName());
     //get the modified rows in this entity
     String pk = epw.getEntity().getPk();
@@ -804,7 +804,7 @@ public class DocBuilder {
         return new HashSet();
     }
     //get the deleted rows for this entity
-    Set<Map<String, Object>> deletedSet = new HashSet<Map<String, Object>>();
+    Set<Map<String, Object>> deletedSet = new HashSet<>();
     while (true) {
       Map<String, Object> row = epw.nextDeletedRowKey();
       if (row == null)
@@ -834,7 +834,7 @@ public class DocBuilder {
     LOG.info("Completed DeletedRowKey for Entity: " + epw.getEntity().getName() + " rows obtained : " + deletedSet.size());
 
     myModifiedPks.addAll(deltaSet.values());
-    Set<Map<String, Object>> parentKeyList = new HashSet<Map<String, Object>>();
+    Set<Map<String, Object>> parentKeyList = new HashSet<>();
     //all that we have captured is useless (in a sub-entity) if no rows in the parent is modified because of these
     //propogate up the changes in the chain
     if (epw.getEntity().getParentEntity() != null) {
@@ -862,7 +862,7 @@ public class DocBuilder {
 
     // Do not use entity.isDocRoot here because one of descendant entities may set rootEntity="true"
     return epw.getEntity().getParentEntity() == null ?
-        myModifiedPks : new HashSet<Map<String, Object>>(parentKeyList);
+        myModifiedPks : new HashSet<>(parentKeyList);
   }
 
   private void getModifiedParentRows(VariableResolver resolver,
@@ -949,7 +949,7 @@ public class DocBuilder {
     }
 
     public Map<String, Object> getStatsSnapshot() {
-      Map<String, Object> result = new HashMap<String, Object>();
+      Map<String, Object> result = new HashMap<>();
       result.put("docCount", docCount.get());
       result.put("deletedDocCount", deletedDocCount.get());
       result.put("rowCount", rowsCount.get());
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/EntityProcessorWrapper.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/EntityProcessorWrapper.java
index 682e08a..a09efb1 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/EntityProcessorWrapper.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/EntityProcessorWrapper.java
@@ -43,7 +43,7 @@ public class EntityProcessorWrapper extends EntityProcessor {
   private EntityProcessor delegate;
   private Entity entity;
   private DataSource datasource;
-  private List<EntityProcessorWrapper> children = new ArrayList<EntityProcessorWrapper>();
+  private List<EntityProcessorWrapper> children = new ArrayList<>();
   private DocBuilder docBuilder;
   private boolean initalized;
   private String onError;
@@ -176,7 +176,7 @@ public class EntityProcessorWrapper extends EntityProcessor {
       if (stopTransform) break;
       try {
         if (rows != null) {
-          List<Map<String, Object>> tmpRows = new ArrayList<Map<String, Object>>();
+          List<Map<String, Object>> tmpRows = new ArrayList<>();
           for (Map<String, Object> map : rows) {
             resolver.addNamespace(entityName, map);
             Object o = t.transformRow(map, context);
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/Evaluator.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/Evaluator.java
index 2801167..4516895 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/Evaluator.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/Evaluator.java
@@ -71,7 +71,7 @@ public abstract class Evaluator {
    * @return a List of objects which can either be a string, number or a variable wrapper
    */
   List<Object> parseParams(String expression, VariableResolver vr) {
-    List<Object> result = new ArrayList<Object>();
+    List<Object> result = new ArrayList<>();
     expression = expression.trim();
     String[] ss = expression.split(",");
     for (int i = 0; i < ss.length; i++) {
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/FileListEntityProcessor.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/FileListEntityProcessor.java
index 4a63e87..2ea0a4f 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/FileListEntityProcessor.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/FileListEntityProcessor.java
@@ -200,7 +200,7 @@ public class FileListEntityProcessor extends EntityProcessorBase {
   public Map<String, Object> nextRow() {
     if (rowIterator != null)
       return getNext();
-    List<Map<String, Object>> fileDetails = new ArrayList<Map<String, Object>>();
+    List<Map<String, Object>> fileDetails = new ArrayList<>();
     File dir = new File(baseDir);
 
     String dateStr = context.getEntityAttribute(NEWER_THAN);
@@ -243,7 +243,7 @@ public class FileListEntityProcessor extends EntityProcessorBase {
   }
 
   private void addDetails(List<Map<String, Object>> files, File dir, String name) {
-    Map<String, Object> details = new HashMap<String, Object>();
+    Map<String, Object> details = new HashMap<>();
     File aFile = new File(dir, name);
     if (aFile.isDirectory()) return;
     long sz = aFile.length();
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/JdbcDataSource.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/JdbcDataSource.java
index c42a201..f6eb5ef 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/JdbcDataSource.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/JdbcDataSource.java
@@ -49,7 +49,7 @@ public class JdbcDataSource extends
 
   private Connection conn;
 
-  private Map<String, Integer> fieldNameVsType = new HashMap<String, Integer>();
+  private Map<String, Integer> fieldNameVsType = new HashMap<>();
 
   private boolean convertType = false;
 
@@ -245,7 +245,7 @@ public class JdbcDataSource extends
 
   private List<String> readFieldNames(ResultSetMetaData metaData)
           throws SQLException {
-    List<String> colNames = new ArrayList<String>();
+    List<String> colNames = new ArrayList<>();
     int count = metaData.getColumnCount();
     for (int i = 0; i < count; i++) {
       colNames.add(metaData.getColumnLabel(i + 1));
@@ -309,7 +309,7 @@ public class JdbcDataSource extends
     private Map<String, Object> getARow() {
       if (resultSet == null)
         return null;
-      Map<String, Object> result = new HashMap<String, Object>();
+      Map<String, Object> result = new HashMap<>();
       for (String colName : colNames) {
         try {
           if (!convertType) {
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/LineEntityProcessor.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/LineEntityProcessor.java
index 5b919bb..99bbea1 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/LineEntityProcessor.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/LineEntityProcessor.java
@@ -125,7 +125,7 @@ public class LineEntityProcessor extends EntityProcessorBase {
       if (acceptLineRegex != null && ! acceptLineRegex.matcher(line).find()) continue;
       if (skipLineRegex != null &&   skipLineRegex.matcher(line).find()) continue;
       // Contruct the 'row' of fields
-      Map<String, Object> row = new HashMap<String, Object>();
+      Map<String, Object> row = new HashMap<>();
       row.put("rawLine", line);
       return row;
     }
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/MockDataSource.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/MockDataSource.java
index 469f582..5036a50 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/MockDataSource.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/MockDataSource.java
@@ -33,7 +33,7 @@ import java.util.Properties;
 public class MockDataSource extends
         DataSource<Iterator<Map<String, Object>>> {
 
-  private static Map<String, Iterator<Map<String, Object>>> cache = new HashMap<String, Iterator<Map<String, Object>>>();
+  private static Map<String, Iterator<Map<String, Object>>> cache = new HashMap<>();
 
   public static void setIterator(String query,
                                  Iterator<Map<String, Object>> iter) {
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/PlainTextEntityProcessor.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/PlainTextEntityProcessor.java
index 857865c..ecc9aab 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/PlainTextEntityProcessor.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/PlainTextEntityProcessor.java
@@ -69,7 +69,7 @@ public class PlainTextEntityProcessor extends EntityProcessorBase {
       if (len <= 0) break;
       sw.append(new String(buf, 0, len));
     }
-    Map<String, Object> row = new HashMap<String, Object>();
+    Map<String, Object> row = new HashMap<>();
     row.put(PLAIN_TEXT, sw.toString());
     ended = true;
     IOUtils.closeQuietly(r);
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/RegexTransformer.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/RegexTransformer.java
index babd3e3..24bf9df 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/RegexTransformer.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/RegexTransformer.java
@@ -75,7 +75,7 @@ public class RegexTransformer extends Transformer {
                   Map.Entry<String ,Object> entry = (Map.Entry<String, Object>) e;
                   List l = results;
                   if(!col.equals(entry.getKey())){
-                    if(otherVars == null) otherVars = new HashMap<String, List>();
+                    if(otherVars == null) otherVars = new HashMap<>();
                     l = otherVars.get(entry.getKey());
                     if(l == null){
                       l = new ArrayList();
@@ -131,7 +131,7 @@ public class RegexTransformer extends Transformer {
   @SuppressWarnings("unchecked")
   private List<String> readBySplit(String splitBy, String value) {
     String[] vals = value.split(splitBy);
-    List<String> l = new ArrayList<String>();
+    List<String> l = new ArrayList<>();
     l.addAll(Arrays.asList(vals));
     return l;
   }
@@ -151,7 +151,7 @@ public class RegexTransformer extends Transformer {
         if(groupNames == null){
           l = new ArrayList();
         } else {
-          map =  new HashMap<String, String>();
+          map =  new HashMap<>();
         }
         for (int i = 1; i <= m.groupCount(); i++) {
           try {
@@ -186,7 +186,7 @@ public class RegexTransformer extends Transformer {
     return result;
   }
 
-  private HashMap<String, Pattern> PATTERN_CACHE = new HashMap<String, Pattern>();
+  private HashMap<String, Pattern> PATTERN_CACHE = new HashMap<>();
 
   public static final String REGEX = "regex";
 
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/RequestInfo.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/RequestInfo.java
index 6a45438..d3f1a56 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/RequestInfo.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/RequestInfo.java
@@ -91,11 +91,11 @@ public class RequestInfo {
     List<String> modifiableEntities = null;
     if(o != null) {
       if (o instanceof String) {
-        modifiableEntities = new ArrayList<String>();
+        modifiableEntities = new ArrayList<>();
         modifiableEntities.add((String) o);
       } else if (o instanceof List<?>) {
         @SuppressWarnings("unchecked")
-        List<String> modifiableEntities1 = new ArrayList<String>((List<String>) o);
+        List<String> modifiableEntities1 = new ArrayList<>((List<String>) o);
         modifiableEntities = modifiableEntities1;
       } 
       entitiesToRun = Collections.unmodifiableList(modifiableEntities);
@@ -110,7 +110,7 @@ public class RequestInfo {
       dataConfigParam = null;
     }
     dataConfig = dataConfigParam;
-    this.rawParams = Collections.unmodifiableMap(new HashMap<String,Object>(requestParams));
+    this.rawParams = Collections.unmodifiableMap(new HashMap<>(requestParams));
   }
 
   public String getCommand() {
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SimplePropertiesWriter.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SimplePropertiesWriter.java
index 7967558..08209f2 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SimplePropertiesWriter.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SimplePropertiesWriter.java
@@ -171,7 +171,7 @@ public class SimplePropertiesWriter extends DIHProperties {
    * already converted them.
    */
   protected Map<String,Object> propertiesToMap(Properties p) {
-    Map<String,Object> theMap = new HashMap<String,Object>();
+    Map<String,Object> theMap = new HashMap<>();
     for(Map.Entry<Object,Object> entry : p.entrySet()) {
       String key = entry.getKey().toString();
       Object val = entry.getValue().toString();
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SolrEntityProcessor.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SolrEntityProcessor.java
index f2b5e93..2684f72 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SolrEntityProcessor.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SolrEntityProcessor.java
@@ -222,7 +222,7 @@ public class SolrEntityProcessor extends EntityProcessorBase {
     public Map<String,Object> next() {
       SolrDocument solrDocument = solrDocumentIterator.next();
       
-      HashMap<String,Object> map = new HashMap<String,Object>();
+      HashMap<String,Object> map = new HashMap<>();
       Collection<String> fields = solrDocument.getFieldNames();
       for (String field : fields) {
         Object fieldValue = solrDocument.getFieldValue(field);
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SortedMapBackedCache.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SortedMapBackedCache.java
index 109b6d5..aaf38d7 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SortedMapBackedCache.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SortedMapBackedCache.java
@@ -60,7 +60,7 @@ public class SortedMapBackedCache implements DIHCache {
     }
     List<Map<String,Object>> thisKeysRecs = theMap.get(pk);
     if (thisKeysRecs == null) {
-      thisKeysRecs = new ArrayList<Map<String,Object>>();
+      thisKeysRecs = new ArrayList<>();
       theMap.put(pk, thisKeysRecs);
     }
     thisKeysRecs.add(rec);
@@ -131,7 +131,7 @@ public class SortedMapBackedCache implements DIHCache {
       return null;
     }
     if(key instanceof Iterable<?>) {
-      List<Map<String,Object>> vals = new ArrayList<Map<String,Object>>();
+      List<Map<String,Object>> vals = new ArrayList<>();
       Iterator<?> iter = ((Iterable<?>) key).iterator();
       while(iter.hasNext()) {
         List<Map<String,Object>> val = theMap.get(iter.next());
@@ -220,7 +220,7 @@ public class SortedMapBackedCache implements DIHCache {
     checkOpen(false);
     isOpen = true;
     if (theMap == null) {
-      theMap = new TreeMap<Object,List<Map<String,Object>>>();
+      theMap = new TreeMap<>();
     }
     
     String pkName = CachePropertyUtil.getAttributeValueAsString(context,
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/TemplateTransformer.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/TemplateTransformer.java
index 4b7ca30..7d83ea3 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/TemplateTransformer.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/TemplateTransformer.java
@@ -49,7 +49,7 @@ import org.slf4j.LoggerFactory;
 public class TemplateTransformer extends Transformer {
 
   private static final Logger LOG = LoggerFactory.getLogger(TemplateTransformer.class);
-  private Map<String ,List<String>> templateVsVars = new HashMap<String, List<String>>();
+  private Map<String ,List<String>> templateVsVars = new HashMap<>();
 
   @Override
   @SuppressWarnings("unchecked")
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/VariableResolver.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/VariableResolver.java
index f594118..76930e2 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/VariableResolver.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/VariableResolver.java
@@ -54,30 +54,30 @@ public class VariableResolver {
       .compile("^(\\w*?)\\((.*?)\\)$");
   private Map<String,Object> rootNamespace;
   private Map<String,Evaluator> evaluators;
-  private Map<String,Resolved> cache = new WeakHashMap<String,Resolved>();
+  private Map<String,Resolved> cache = new WeakHashMap<>();
   
   class Resolved {
-    List<Integer> startIndexes = new ArrayList<Integer>(2);
-    List<Integer> endOffsets = new ArrayList<Integer>(2);
-    List<String> variables = new ArrayList<String>(2);
+    List<Integer> startIndexes = new ArrayList<>(2);
+    List<Integer> endOffsets = new ArrayList<>(2);
+    List<String> variables = new ArrayList<>(2);
   }
   
   public static final String FUNCTIONS_NAMESPACE = "dataimporter.functions.";
   public static final String FUNCTIONS_NAMESPACE_SHORT = "dih.functions.";
   
   public VariableResolver() {
-    rootNamespace = new HashMap<String,Object>();
+    rootNamespace = new HashMap<>();
   }
   
   public VariableResolver(Properties defaults) {
-    rootNamespace = new HashMap<String,Object>();
+    rootNamespace = new HashMap<>();
     for (Map.Entry<Object,Object> entry : defaults.entrySet()) {
       rootNamespace.put(entry.getKey().toString(), entry.getValue());
     }
   }
   
   public VariableResolver(Map<String,Object> defaults) {
-    rootNamespace = new HashMap<String,Object>(defaults);
+    rootNamespace = new HashMap<>(defaults);
   }
   
   /**
@@ -184,7 +184,7 @@ public class VariableResolver {
     if (r == null) {
       return Collections.emptyList();
     }
-    return new ArrayList<String>(r.variables);
+    return new ArrayList<>(r.variables);
   }
   
   public void addNamespace(String name, Map<String,Object> newMap) {
@@ -221,7 +221,7 @@ public class VariableResolver {
       Object o = currentLevel.get(keyParts[i]);
       if (o == null) {
         if(i == j-1) {
-          Map<String,Object> nextLevel = new HashMap<String,Object>();
+          Map<String,Object> nextLevel = new HashMap<>();
           currentLevel.put(keyParts[i], nextLevel);
           currentLevel = nextLevel;
         } else {
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor.java
index 4819e75..b50cdae 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathEntityProcessor.java
@@ -57,7 +57,7 @@ public class XPathEntityProcessor extends EntityProcessorBase {
   private static final Logger LOG = LoggerFactory.getLogger(XPathEntityProcessor.class);
   private static final XMLErrorLogger xmllog = new XMLErrorLogger(LOG);
 
-  private static final Map<String, Object> END_MARKER = new HashMap<String, Object>();
+  private static final Map<String, Object> END_MARKER = new HashMap<>();
   
   protected List<String> placeHolderVariables;
 
@@ -179,14 +179,14 @@ public class XPathEntityProcessor extends EntityProcessorBase {
     for (String s : l) {
       if (s.startsWith(entityName + ".")) {
         if (placeHolderVariables == null)
-          placeHolderVariables = new ArrayList<String>();
+          placeHolderVariables = new ArrayList<>();
         placeHolderVariables.add(s.substring(entityName.length() + 1));
       }
     }
     for (Map<String, String> fld : context.getAllEntityFields()) {
       if (fld.get(COMMON_FIELD) != null && "true".equals(fld.get(COMMON_FIELD))) {
         if (commonFields == null)
-          commonFields = new ArrayList<String>();
+          commonFields = new ArrayList<>();
         commonFields.add(fld.get(DataImporter.COLUMN));
       }
     }
@@ -249,8 +249,8 @@ public class XPathEntityProcessor extends EntityProcessorBase {
   }
 
   private void addNamespace() {
-    Map<String, Object> namespace = new HashMap<String, Object>();
-    Set<String> allNames = new HashSet<String>();
+    Map<String, Object> namespace = new HashMap<>();
+    Set<String> allNames = new HashSet<>();
     if (commonFields != null) allNames.addAll(commonFields);
     if (placeHolderVariables != null) allNames.addAll(placeHolderVariables);
     if(allNames.isEmpty()) return;
@@ -278,7 +278,7 @@ public class XPathEntityProcessor extends EntityProcessorBase {
   private void initQuery(String s) {
     Reader data = null;
     try {
-      final List<Map<String, Object>> rows = new ArrayList<Map<String, Object>>();
+      final List<Map<String, Object>> rows = new ArrayList<>();
       try {
         data = dataSource.getData(s);
       } catch (Exception e) {
@@ -329,7 +329,7 @@ public class XPathEntityProcessor extends EntityProcessorBase {
             wrapAndThrow(SEVERE, e, msg);
           } else if (SKIP.equals(onError)) {
             LOG.warn(msg, e);
-            Map<String, Object> map = new HashMap<String, Object>();
+            Map<String, Object> map = new HashMap<>();
             map.put(DocBuilder.SKIP_DOC, Boolean.TRUE);
             rows.add(map);
           } else if (CONTINUE.equals(onError)) {
@@ -357,7 +357,7 @@ public class XPathEntityProcessor extends EntityProcessorBase {
     if (useSolrAddXml) {
       List<String> names = (List<String>) record.get("name");
       List<String> values = (List<String>) record.get("value");
-      Map<String, Object> row = new HashMap<String, Object>();
+      Map<String, Object> row = new HashMap<>();
       for (int i = 0; i < names.size() && i < values.size(); i++) {
         if (row.containsKey(names.get(i))) {
           Object existing = row.get(names.get(i));
@@ -417,8 +417,8 @@ public class XPathEntityProcessor extends EntityProcessorBase {
 
   private Iterator<Map<String, Object>> getRowIterator(final Reader data, final String s) {
     //nothing atomic about it. I just needed a StongReference
-    final AtomicReference<Exception> exp = new AtomicReference<Exception>();
-    final BlockingQueue<Map<String, Object>> blockingQueue = new ArrayBlockingQueue<Map<String, Object>>(blockingQueueSize);
+    final AtomicReference<Exception> exp = new AtomicReference<>();
+    final BlockingQueue<Map<String, Object>> blockingQueue = new ArrayBlockingQueue<>(blockingQueueSize);
     final AtomicBoolean isEnd = new AtomicBoolean(false);
     final AtomicBoolean throwExp = new AtomicBoolean(true);
     publisherThread = new Thread() {
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathRecordReader.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathRecordReader.java
index 23f4755..8d2af7e 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathRecordReader.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/XPathRecordReader.java
@@ -162,7 +162,7 @@ public class XPathRecordReader {
    * @return results a List of emitted records
    */
   public List<Map<String, Object>> getAllRecords(Reader r) {
-    final List<Map<String, Object>> results = new ArrayList<Map<String, Object>>();
+    final List<Map<String, Object>> results = new ArrayList<>();
     streamRecords(r, new Handler() {
       @Override
       public void handle(Map<String, Object> record, String s) {
@@ -249,7 +249,7 @@ public class XPathRecordReader {
         // prepare for the clean up that will occurr when the record
         // is emitted after its END_ELEMENT is matched 
         recordStarted = true;
-        valuesAddedinThisFrame = new HashSet<String>();
+        valuesAddedinThisFrame = new HashSet<>();
         stack.push(valuesAddedinThisFrame);
       } else if (recordStarted) {
         // This node is a child of some parent which matched against forEach 
@@ -274,7 +274,7 @@ public class XPathRecordReader {
           }
         }
 
-        Set<Node> childrenFound = new HashSet<Node>();
+        Set<Node> childrenFound = new HashSet<>();
         int event = -1;
         int flattenedStarts=0; // our tag depth when flattening elements
         StringBuilder text = new StringBuilder();
@@ -342,7 +342,7 @@ public class XPathRecordReader {
                                     Stack<Set<String>> stack, boolean recordStarted)
             throws IOException, XMLStreamException {
       Node n = getMatchingNode(parser,childNodes);
-      Map<String, Object> decends=new HashMap<String, Object>();
+      Map<String, Object> decends=new HashMap<>();
       if (n != null) {
         childrenFound.add(n);
         n.parse(parser, handler, values, stack, recordStarted);
@@ -466,7 +466,7 @@ public class XPathRecordReader {
       if (multiValued) {
         List<String> v = (List<String>) values.get(fieldName);
         if (v == null) {
-          v = new ArrayList<String>();
+          v = new ArrayList<>();
           values.put(fieldName, v);
         }
         v.add(value);
@@ -510,7 +510,7 @@ public class XPathRecordReader {
         // we have reached end of element portion of Xpath and can now only
         // have an element attribute. Add it to this nodes list of attributes
         if (attributes == null) {
-          attributes = new ArrayList<Node>();
+          attributes = new ArrayList<>();
         }
         xpseg = xpseg.substring(1); // strip the '@'
         attributes.add(new Node(xpseg, fieldName, multiValued));
@@ -518,7 +518,7 @@ public class XPathRecordReader {
       else if ( xpseg.length() == 0) {
         // we have a '//' selector for all decendents of the current nodes
         xpseg = paths.remove(0); // shift out next Xpath segment
-        if (wildCardNodes == null) wildCardNodes = new ArrayList<Node>();
+        if (wildCardNodes == null) wildCardNodes = new ArrayList<>();
         Node n = getOrAddNode(xpseg, wildCardNodes);
         if (paths.isEmpty()) {
           // We are current a leaf node.
@@ -535,7 +535,7 @@ public class XPathRecordReader {
       }
       else {
         if (childNodes == null)
-          childNodes = new ArrayList<Node>();
+          childNodes = new ArrayList<>();
         // does this "name" already exist as a child node.
         Node n = getOrAddNode(xpseg,childNodes);
         if (paths.isEmpty()) {
@@ -572,13 +572,13 @@ public class XPathRecordReader {
         n.name = m.group(1);
         int start = m.start(2);
         while (true) {
-          HashMap<String, String> attribs = new HashMap<String, String>();
+          HashMap<String, String> attribs = new HashMap<>();
           if (!m.find(start))
             break;
           attribs.put(m.group(3), m.group(5));
           start = m.end(6);
           if (n.attribAndValues == null)
-            n.attribAndValues = new ArrayList<Map.Entry<String, String>>();
+            n.attribAndValues = new ArrayList<>();
           n.attribAndValues.addAll(attribs.entrySet());
         }
       }
@@ -592,7 +592,7 @@ public class XPathRecordReader {
      * deep-copied for thread safety
      */
     private static Map<String, Object> getDeepCopy(Map<String, Object> values) {
-      Map<String, Object> result = new HashMap<String, Object>();
+      Map<String, Object> result = new HashMap<>();
       for (Map.Entry<String, Object> entry : values.entrySet()) {
         if (entry.getValue() instanceof List) {
           result.put(entry.getKey(), new ArrayList((List) entry.getValue()));
@@ -616,7 +616,7 @@ public class XPathRecordReader {
    * seperator or if a sequence of multiple seperator's appear. 
    */
   private static List<String> splitEscapeQuote(String str) {
-    List<String> result = new LinkedList<String>();
+    List<String> result = new LinkedList<>();
     String[] ss = str.split("/");
     for (int i=0; i<ss.length; i++) { // i=1: skip seperator at start of string
       StringBuilder sb = new StringBuilder();
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/ConfigNameConstants.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/ConfigNameConstants.java
index ad18e3a..d959e75 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/ConfigNameConstants.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/ConfigNameConstants.java
@@ -49,7 +49,7 @@ public class ConfigNameConstants {
 
   public static final Set<String> RESERVED_WORDS;
   static{
-    Set<String> rw =  new HashSet<String>();
+    Set<String> rw =  new HashSet<>();
     rw.add(IMPORTER_NS);
     rw.add(IMPORTER_NS_SHORT);
     rw.add("request");
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/ConfigParseUtil.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/ConfigParseUtil.java
index 96be947..5c833fa 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/ConfigParseUtil.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/ConfigParseUtil.java
@@ -41,7 +41,7 @@ public class ConfigParseUtil {
   }
   
   public static HashMap<String,String> getAllAttributes(Element e) {
-    HashMap<String,String> m = new HashMap<String,String>();
+    HashMap<String,String> m = new HashMap<>();
     NamedNodeMap nnm = e.getAttributes();
     for (int i = 0; i < nnm.getLength(); i++) {
       m.put(nnm.item(i).getNodeName(), nnm.item(i).getNodeValue());
@@ -68,7 +68,7 @@ public class ConfigParseUtil {
   }
   
   public static List<Element> getChildNodes(Element e, String byName) {
-    List<Element> result = new ArrayList<Element>();
+    List<Element> result = new ArrayList<>();
     NodeList l = e.getChildNodes();
     for (int i = 0; i < l.getLength(); i++) {
       if (e.equals(l.item(i).getParentNode())
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/DIHConfiguration.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/DIHConfiguration.java
index bf07e40..ce302f9 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/DIHConfiguration.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/DIHConfiguration.java
@@ -71,7 +71,7 @@ public class DIHConfiguration {
     this.deleteQuery = ConfigParseUtil.getStringAttribute(element, "deleteQuery", null);
     this.onImportStart = ConfigParseUtil.getStringAttribute(element, "onImportStart", null);
     this.onImportEnd = ConfigParseUtil.getStringAttribute(element, "onImportEnd", null);
-    List<Entity> modEntities = new ArrayList<Entity>();
+    List<Entity> modEntities = new ArrayList<>();
     List<Element> l = ConfigParseUtil.getChildNodes(element, "entity");
     boolean docRootFound = false;
     for (Element e : l) {
@@ -84,7 +84,7 @@ public class DIHConfiguration {
     if(functions==null) {
       functions = Collections.emptyList();
     }
-    List<Map<String, String>> modFunc = new ArrayList<Map<String, String>>(functions.size());
+    List<Map<String, String>> modFunc = new ArrayList<>(functions.size());
     for(Map<String, String> f : functions) {
       modFunc.add(Collections.unmodifiableMap(f));
     }
@@ -119,7 +119,7 @@ public class DIHConfiguration {
   }
 
   private Map<String,EntityField> gatherAllFields(DataImporter di, Entity e) {
-    Map<String,EntityField> fields = new HashMap<String,EntityField>();
+    Map<String,EntityField> fields = new HashMap<>();
     if (e.getFields() != null) {
       for (EntityField f : e.getFields()) {
         fields.put(f.getName(), f);
@@ -132,7 +132,7 @@ public class DIHConfiguration {
   }
 
   private Map<String,SchemaField> loadSchemaFieldMap() {
-    Map<String, SchemaField> modLnvsf = new HashMap<String, SchemaField>();
+    Map<String, SchemaField> modLnvsf = new HashMap<>();
     for (Map.Entry<String, SchemaField> entry : schema.getFields().entrySet()) {
       modLnvsf.put(entry.getKey().toLowerCase(Locale.ROOT), entry.getValue());
     }
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/Entity.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/Entity.java
index 14726fb..6d66064 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/Entity.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/Entity.java
@@ -83,9 +83,9 @@ public class Entity {
     this.allAttributes = Collections.unmodifiableMap(modAttributes);
     
     List<Element> n = ConfigParseUtil.getChildNodes(element, "field");
-    List<EntityField> modFields = new ArrayList<EntityField>(n.size());
-    Map<String,Set<EntityField>> modColNameVsField = new HashMap<String,Set<EntityField>>();
-    List<Map<String,String>> modAllFieldAttributes = new ArrayList<Map<String,String>>();
+    List<EntityField> modFields = new ArrayList<>(n.size());
+    Map<String,Set<EntityField>> modColNameVsField = new HashMap<>();
+    List<Map<String,String>> modAllFieldAttributes = new ArrayList<>();
     for (Element elem : n) {
       EntityField.Builder fieldBuilder = new EntityField.Builder(elem);
       if (config.getSchema() != null) {
@@ -115,7 +115,7 @@ public class Entity {
       }
       Set<EntityField> fieldSet = modColNameVsField.get(fieldBuilder.column);
       if (fieldSet == null) {
-        fieldSet = new HashSet<EntityField>();
+        fieldSet = new HashSet<>();
         modColNameVsField.put(fieldBuilder.column, fieldSet);
       }
       fieldBuilder.allAttributes.put("boost", Float
@@ -128,7 +128,7 @@ public class Entity {
       fieldSet.add(field);
       modFields.add(field);
     }
-    Map<String,Set<EntityField>> modColNameVsField1 = new HashMap<String,Set<EntityField>>();
+    Map<String,Set<EntityField>> modColNameVsField1 = new HashMap<>();
     for (Map.Entry<String,Set<EntityField>> entry : modColNameVsField
         .entrySet()) {
       if (entry.getValue().size() > 0) {
@@ -161,7 +161,7 @@ public class Entity {
     }
     pkMappingFromSchema = modPkMappingFromSchema;
     n = ConfigParseUtil.getChildNodes(element, "entity");
-    List<Entity> modEntities = new ArrayList<Entity>();
+    List<Entity> modEntities = new ArrayList<>();
     for (Element elem : n) {
       modEntities.add(new Entity((docRootFound || this.docRoot), elem, di, config, this));
     }
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/EntityField.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/EntityField.java
index 806150c..adef412 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/EntityField.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/EntityField.java
@@ -46,7 +46,7 @@ public class EntityField {
     this.multiValued = b.multiValued;
     this.dynamicName = b.dynamicName;
     this.entity = b.entity;
-    this.allAttributes = Collections.unmodifiableMap(new HashMap<String,String>(b.allAttributes));
+    this.allAttributes = Collections.unmodifiableMap(new HashMap<>(b.allAttributes));
   }
 
   public String getName() {
@@ -89,7 +89,7 @@ public class EntityField {
     public boolean multiValued = false;
     public boolean dynamicName = false;
     public Entity entity;
-    public Map<String, String> allAttributes = new HashMap<String,String>();
+    public Map<String, String> allAttributes = new HashMap<>();
     
     public Builder(Element e) {
       this.name = ConfigParseUtil.getStringAttribute(e, DataImporter.NAME, null);
@@ -98,7 +98,7 @@ public class EntityField {
         throw new DataImportHandlerException(SEVERE, "Field must have a column attribute");
       }
       this.boost = Float.parseFloat(ConfigParseUtil.getStringAttribute(e, "boost", "1.0f"));
-      this.allAttributes = new HashMap<String, String>(ConfigParseUtil.getAllAttributes(e));
+      this.allAttributes = new HashMap<>(ConfigParseUtil.getAllAttributes(e));
     }
     
     public String getNameOrColumn() {
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/Field.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/Field.java
index da0851d..4331811 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/Field.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/Field.java
@@ -46,7 +46,7 @@ public class Field {
     this.multiValued = b.multiValued;
     this.dynamicName = b.dynamicName;
     this.entity = b.entity;
-    this.allAttributes = Collections.unmodifiableMap(new HashMap<String,String>(b.allAttributes));
+    this.allAttributes = Collections.unmodifiableMap(new HashMap<>(b.allAttributes));
   }
 
   public String getName() {
@@ -89,7 +89,7 @@ public class Field {
     public boolean multiValued = false;
     public boolean dynamicName;
     public Entity entity;
-    public Map<String, String> allAttributes = new HashMap<String,String>();
+    public Map<String, String> allAttributes = new HashMap<>();
     
     public Builder(Element e) {
       this.name = ConfigParseUtil.getStringAttribute(e, DataImporter.NAME, null);
@@ -98,7 +98,7 @@ public class Field {
         throw new DataImportHandlerException(SEVERE, "Field must have a column attribute");
       }
       this.boost = Float.parseFloat(ConfigParseUtil.getStringAttribute(e, "boost", "1.0f"));
-      this.allAttributes = new HashMap<String, String>(ConfigParseUtil.getAllAttributes(e));
+      this.allAttributes = new HashMap<>(ConfigParseUtil.getAllAttributes(e));
     }
     
     public String getNameOrColumn() {
diff --git solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/PropertyWriter.java solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/PropertyWriter.java
index 5925131..811cce0 100644
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/PropertyWriter.java
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/config/PropertyWriter.java
@@ -27,7 +27,7 @@ public class PropertyWriter {
   
   public PropertyWriter(String type, Map<String,String> parameters) {
     this.type = type;
-    this.parameters = Collections.unmodifiableMap(new HashMap<String,String>(parameters));
+    this.parameters = Collections.unmodifiableMap(new HashMap<>(parameters));
   }
 
   public Map<String,String> getParameters() {
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractDIHCacheTestCase.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractDIHCacheTestCase.java
index 6c290f2..88e1ed8 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractDIHCacheTestCase.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractDIHCacheTestCase.java
@@ -42,7 +42,7 @@ public class AbstractDIHCacheTestCase {
   protected static final Date Feb21_2011 = new Date(1298268000000l);
   protected final String[] fieldTypes = { "INTEGER", "BIGDECIMAL", "STRING", "STRING",   "FLOAT",   "DATE",   "CLOB" };
   protected final String[] fieldNames = { "a_id",    "PI",         "letter", "examples", "a_float", "a_date", "DESCRIPTION" };
-  protected List<ControlData> data = new ArrayList<ControlData>();
+  protected List<ControlData> data = new ArrayList<>();
   protected Clob APPLE = null;
 
   @Before
@@ -55,7 +55,7 @@ public class AbstractDIHCacheTestCase {
 
     // The first row needs to have all non-null fields,
     // otherwise we would have to always send the fieldTypes & fieldNames as CacheProperties when building.
-    data = new ArrayList<ControlData>();
+    data = new ArrayList<>();
     data.add(new ControlData(new Object[] { new Integer(1), new BigDecimal(Math.PI), "A", "Apple", new Float(1.11), Feb21_2011, APPLE }));
     data.add(new ControlData(new Object[] { new Integer(2), new BigDecimal(Math.PI), "B", "Ball", new Float(2.22), Feb21_2011, null }));
     data.add(new ControlData(new Object[] { new Integer(4), new BigDecimal(Math.PI), "D", "Dog", new Float(4.44), Feb21_2011, null }));
@@ -102,7 +102,7 @@ public class AbstractDIHCacheTestCase {
   }
 
   protected List<ControlData> extractDataInKeyOrder(DIHCache cache, String[] theFieldNames) {
-    List<Object[]> data = new ArrayList<Object[]>();
+    List<Object[]> data = new ArrayList<>();
     Iterator<Map<String, Object>> cacheIter = cache.iterator();
     while (cacheIter.hasNext()) {
       data.add(mapToObjectArray(cacheIter.next(), theFieldNames));
@@ -114,7 +114,7 @@ public class AbstractDIHCacheTestCase {
   //It will look for id's sequentially until one is skipped, then will stop.
   protected List<ControlData> extractDataByKeyLookup(DIHCache cache, String[] theFieldNames) {
     int recId = 1;
-    List<Object[]> data = new ArrayList<Object[]>();
+    List<Object[]> data = new ArrayList<>();
     while (true) {
       Iterator<Map<String, Object>> listORecs = cache.iterator(recId);
       if (listORecs == null) {
@@ -130,7 +130,7 @@ public class AbstractDIHCacheTestCase {
   }
 
   protected List<ControlData> listToControlData(List<Object[]> data) {
-    List<ControlData> returnData = new ArrayList<ControlData>(data.size());
+    List<ControlData> returnData = new ArrayList<>(data.size());
     for (int i = 0; i < data.size(); i++) {
       returnData.add(new ControlData(data.get(i)));
     }
@@ -147,7 +147,7 @@ public class AbstractDIHCacheTestCase {
 
   protected void compareData(List<ControlData> theControl, List<ControlData> test) {
     // The test data should come back primarily in Key order and secondarily in insertion order.
-    List<ControlData> control = new ArrayList<ControlData>(theControl);
+    List<ControlData> control = new ArrayList<>(theControl);
     Collections.sort(control);
 
     StringBuilder errors = new StringBuilder();
@@ -189,9 +189,9 @@ public class AbstractDIHCacheTestCase {
   protected Map<String, Object> controlDataToMap(ControlData cd, String[] theFieldNames, boolean keepOrdered) {
     Map<String, Object> rec = null;
     if (keepOrdered) {
-      rec = new LinkedHashMap<String, Object>();
+      rec = new LinkedHashMap<>();
     } else {
-      rec = new HashMap<String, Object>();
+      rec = new HashMap<>();
     }
     for (int i = 0; i < cd.data.length; i++) {
       String fieldName = theFieldNames[i];
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractDataImportHandlerTestCase.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractDataImportHandlerTestCase.java
index a39d14d..3f3d596 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractDataImportHandlerTestCase.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractDataImportHandlerTestCase.java
@@ -113,7 +113,7 @@ public abstract class AbstractDataImportHandlerTestCase extends
    * @throws Exception in case of any error
    */
   protected void runFullImport(String dataConfig, Map<String, String> extraParams) throws Exception {
-    HashMap<String, String> params = new HashMap<String, String>();
+    HashMap<String, String> params = new HashMap<>();
     params.put("command", "full-import");
     params.put("debug", "on");
     params.put("dataConfig", dataConfig);
@@ -174,7 +174,7 @@ public abstract class AbstractDataImportHandlerTestCase extends
   
   public static Map<String, String> getField(String col, String type,
                                              String re, String srcCol, String splitBy) {
-    HashMap<String, String> vals = new HashMap<String, String>();
+    HashMap<String, String> vals = new HashMap<>();
     vals.put("column", col);
     vals.put("type", type);
     vals.put("regex", re);
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractSqlEntityProcessorTestCase.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractSqlEntityProcessorTestCase.java
index 94543e3..0077005 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractSqlEntityProcessorTestCase.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractSqlEntityProcessorTestCase.java
@@ -81,7 +81,7 @@ public abstract class AbstractSqlEntityProcessorTestCase extends
   }
   
   protected void logPropertiesFile() {
-    Map<String,String> init = new HashMap<String,String>();
+    Map<String,String> init = new HashMap<>();
     init.put("filename", fileName);
     init.put("directory", fileLocation);
     SimplePropertiesWriter spw = new SimplePropertiesWriter();
@@ -339,7 +339,7 @@ public abstract class AbstractSqlEntityProcessorTestCase extends
       conn = newConnection();
       s = conn.createStatement();
       rs = s.executeQuery(query);
-      List<String> results = new ArrayList<String>();
+      List<String> results = new ArrayList<>();
       while (rs.next()) {
         results.add(rs.getString(1));
       }
@@ -409,9 +409,9 @@ public abstract class AbstractSqlEntityProcessorTestCase extends
   public IntChanges modifySomePeople() throws Exception {
     underlyingDataModified = true;
     int numberToChange = random().nextInt(people.length + 1);
-    Set<Integer> changeSet = new HashSet<Integer>();
-    Set<Integer> deleteSet = new HashSet<Integer>();
-    Set<Integer> addSet = new HashSet<Integer>();
+    Set<Integer> changeSet = new HashSet<>();
+    Set<Integer> deleteSet = new HashSet<>();
+    Set<Integer> addSet = new HashSet<>();
     Connection conn = null;
     PreparedStatement change = null;
     PreparedStatement delete = null;
@@ -484,7 +484,7 @@ public abstract class AbstractSqlEntityProcessorTestCase extends
   public String[] modifySomeCountries() throws Exception {
     underlyingDataModified = true;
     int numberToChange = random().nextInt(countries.length + 1);
-    Set<String> changeSet = new HashSet<String>();
+    Set<String> changeSet = new HashSet<>();
     Connection conn = null;
     PreparedStatement change = null;
     // One second in the future ensures a change time after the last import (DIH
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/DestroyCountCache.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/DestroyCountCache.java
index 8b83f5f..2cd6e8a 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/DestroyCountCache.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/DestroyCountCache.java
@@ -25,7 +25,7 @@ import java.util.Map;
 import org.junit.Assert;
 
 public class DestroyCountCache extends SortedMapBackedCache {
-  static Map<DIHCache,DIHCache> destroyed = new IdentityHashMap<DIHCache,DIHCache>();
+  static Map<DIHCache,DIHCache> destroyed = new IdentityHashMap<>();
   
   @Override
   public void destroy() {
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/MockInitialContextFactory.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/MockInitialContextFactory.java
index dc44d02..ca6f13c 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/MockInitialContextFactory.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/MockInitialContextFactory.java
@@ -28,7 +28,7 @@ import org.easymock.IAnswer;
 import org.easymock.IMocksControl;
 
 public class MockInitialContextFactory implements InitialContextFactory {
-  private static final Map<String, Object> objects = new HashMap<String, Object>();
+  private static final Map<String, Object> objects = new HashMap<>();
   private final IMocksControl mockControl;
   private final javax.naming.Context context;
 
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestBuiltInEvaluators.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestBuiltInEvaluators.java
index f07d4ab..7b8d632 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestBuiltInEvaluators.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestBuiltInEvaluators.java
@@ -44,7 +44,7 @@ public class TestBuiltInEvaluators extends AbstractDataImportHandlerTestCase {
     super.setUp();
     resolver = new VariableResolver();
 
-    sqlTests = new HashMap<String, String>();
+    sqlTests = new HashMap<>();
 
     sqlTests.put("foo\"", "foo\"\"");
     sqlTests.put("foo\\", "foo\\\\");
@@ -53,7 +53,7 @@ public class TestBuiltInEvaluators extends AbstractDataImportHandlerTestCase {
     sqlTests.put("'foo\"", "''foo\"\"");
     sqlTests.put("\"Albert D'souza\"", "\"\"Albert D''souza\"\"");
 
-    urlTests = new HashMap<String, String>();
+    urlTests = new HashMap<>();
 
     urlTests.put("*:*", URLEncoder.encode("*:*", ENCODING));
     urlTests.put("price:[* TO 200]", URLEncoder.encode("price:[* TO 200]",
@@ -78,7 +78,7 @@ public class TestBuiltInEvaluators extends AbstractDataImportHandlerTestCase {
 
   @Test
   public void parseParams() {
-    Map<String,Object> m = new HashMap<String,Object>();
+    Map<String,Object> m = new HashMap<>();
     m.put("b","B");
     VariableResolver vr = new VariableResolver();
     vr.addNamespace("a",m);
@@ -97,7 +97,7 @@ public class TestBuiltInEvaluators extends AbstractDataImportHandlerTestCase {
   @Test
   public void testEscapeSolrQueryFunction() {
     final VariableResolver resolver = new VariableResolver();    
-    Map<String,Object> m= new HashMap<String,Object>();
+    Map<String,Object> m= new HashMap<>();
     m.put("query","c:t");
     resolver.setEvaluators(new DataImporter().getEvaluators(Collections.<Map<String,String>>emptyList()));
     
@@ -147,7 +147,7 @@ public class TestBuiltInEvaluators extends AbstractDataImportHandlerTestCase {
     }
    
     Date d = new Date();    
-    Map<String,Object> map = new HashMap<String,Object>();
+    Map<String,Object> map = new HashMap<>();
     map.put("key", d);
     resolver.addNamespace("A", map);
         
@@ -174,7 +174,7 @@ public class TestBuiltInEvaluators extends AbstractDataImportHandlerTestCase {
   private void runTests(Map<String, String> tests, Evaluator evaluator) {
     ContextImpl ctx = new ContextImpl(null, resolver, null, Context.FULL_DUMP, Collections.<String, Object>emptyMap(), null, null);    
     for (Map.Entry<String, String> entry : tests.entrySet()) {
-      Map<String, Object> values = new HashMap<String, Object>();
+      Map<String, Object> values = new HashMap<>();
       values.put("key", entry.getKey());
       resolver.addNamespace("A", values);
 
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestClobTransformer.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestClobTransformer.java
index d391931..fe00d49 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestClobTransformer.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestClobTransformer.java
@@ -35,8 +35,8 @@ import java.util.*;
 public class TestClobTransformer extends AbstractDataImportHandlerTestCase {
   @Test
   public void simple() throws Exception {
-    List<Map<String, String>> flds = new ArrayList<Map<String, String>>();
-    Map<String, String> f = new HashMap<String, String>();
+    List<Map<String, String>> flds = new ArrayList<>();
+    Map<String, String> f = new HashMap<>();
     // <field column="dsc" clob="true" name="description" />
     f.put(DataImporter.COLUMN, "dsc");
     f.put(ClobTransformer.CLOB, "true");
@@ -44,7 +44,7 @@ public class TestClobTransformer extends AbstractDataImportHandlerTestCase {
     flds.add(f);
     Context ctx = getContext(null, new VariableResolver(), null, Context.FULL_DUMP, flds, Collections.EMPTY_MAP);
     Transformer t = new ClobTransformer();
-    Map<String, Object> row = new HashMap<String, Object>();
+    Map<String, Object> row = new HashMap<>();
     Clob clob = (Clob) Proxy.newProxyInstance(this.getClass().getClassLoader(), new Class[]{Clob.class}, new InvocationHandler() {
       @Override
       public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestDateFormatTransformer.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestDateFormatTransformer.java
index 769ef5a..717ecce 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestDateFormatTransformer.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestDateFormatTransformer.java
@@ -34,7 +34,7 @@ public class TestDateFormatTransformer extends AbstractDataImportHandlerTestCase
   @Test
   @SuppressWarnings("unchecked")
   public void testTransformRow_SingleRow() throws Exception {
-    List<Map<String, String>> fields = new ArrayList<Map<String, String>>();
+    List<Map<String, String>> fields = new ArrayList<>();
     fields.add(createMap(DataImporter.COLUMN, "lastModified"));
     fields.add(createMap(DataImporter.COLUMN,
             "dateAdded", RegexTransformer.SRC_COL_NAME, "lastModified",
@@ -57,7 +57,7 @@ public class TestDateFormatTransformer extends AbstractDataImportHandlerTestCase
   @Test
   @SuppressWarnings("unchecked")
   public void testTransformRow_MultipleRows() throws Exception {
-    List<Map<String, String>> fields = new ArrayList<Map<String, String>>();
+    List<Map<String, String>> fields = new ArrayList<>();
     fields.add(createMap(DataImporter.COLUMN, "lastModified"));
     fields.add(createMap(DataImporter.COLUMN,
             "dateAdded", RegexTransformer.SRC_COL_NAME, "lastModified",
@@ -67,8 +67,8 @@ public class TestDateFormatTransformer extends AbstractDataImportHandlerTestCase
     Date now1 = format.parse(format.format(new Date()));
     Date now2 = format.parse(format.format(new Date()));
 
-    Map<String,Object> row = new HashMap<String,Object>();
-    List<String> list = new ArrayList<String>();
+    Map<String,Object> row = new HashMap<>();
+    List<String> list = new ArrayList<>();
     list.add(format.format(now1));
     list.add(format.format(now2));
     row.put("lastModified", list);
@@ -79,7 +79,7 @@ public class TestDateFormatTransformer extends AbstractDataImportHandlerTestCase
     Context context = getContext(null, resolver,
             null, Context.FULL_DUMP, fields, null);
     new DateFormatTransformer().transformRow(row, context);
-    List<Object> output = new ArrayList<Object>();
+    List<Object> output = new ArrayList<>();
     output.add(now1);
     output.add(now2);
     assertEquals(output, row.get("dateAdded"));
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestDocBuilder.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestDocBuilder.java
index 4023fa2..d40d41a 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestDocBuilder.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestDocBuilder.java
@@ -95,7 +95,7 @@ public class TestDocBuilder extends AbstractDataImportHandlerTestCase {
     di.loadAndInit(dc_singleEntity);
     DIHConfiguration cfg = di.getConfig();
     Entity ent = cfg.getEntities().get(0);
-    List<Map<String, Object>> l = new ArrayList<Map<String, Object>>();
+    List<Map<String, Object>> l = new ArrayList<>();
     l.add(createMap("id", 1, "desc", "one"));
     MockDataSource.setIterator("select * from x", l.iterator());
     RequestInfo rp = new RequestInfo(null, createMap("command", "full-import"), null);
@@ -124,7 +124,7 @@ public class TestDocBuilder extends AbstractDataImportHandlerTestCase {
     di.loadAndInit(dc_singleEntity);
     DIHConfiguration cfg = di.getConfig();
     Entity ent = cfg.getEntities().get(0);
-    List<Map<String, Object>> l = new ArrayList<Map<String, Object>>();
+    List<Map<String, Object>> l = new ArrayList<>();
     l.add(createMap("id", 1, "desc", "one"));
     MockDataSource.setIterator("select * from x", l.iterator());
     RequestInfo rp = new RequestInfo(null, createMap("command", "import"), null);
@@ -154,7 +154,7 @@ public class TestDocBuilder extends AbstractDataImportHandlerTestCase {
     DIHConfiguration cfg = di.getConfig();
     Entity ent = cfg.getEntities().get(0);
     RequestInfo rp = new RequestInfo(null, createMap("command", "full-import"), null);
-    List<Map<String, Object>> l = new ArrayList<Map<String, Object>>();
+    List<Map<String, Object>> l = new ArrayList<>();
     l.add(createMap("id", 1, "desc", "one"));
     l.add(createMap("id", 2, "desc", "two"));
     l.add(createMap("id", 3, "desc", "three"));
@@ -180,7 +180,7 @@ public class TestDocBuilder extends AbstractDataImportHandlerTestCase {
   }
 
   static class SolrWriterImpl extends SolrWriter {
-    List<SolrInputDocument> docs = new ArrayList<SolrInputDocument>();
+    List<SolrInputDocument> docs = new ArrayList<>();
 
     Boolean deleteAllCalled = Boolean.FALSE;
 
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestEntityProcessorBase.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestEntityProcessorBase.java
index bf16bbc..623e49e 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestEntityProcessorBase.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestEntityProcessorBase.java
@@ -35,8 +35,8 @@ public class TestEntityProcessorBase extends AbstractDataImportHandlerTestCase {
 
   @Test
   public void multiTransformer() {
-    List<Map<String, String>> fields = new ArrayList<Map<String, String>>();
-    Map<String, String> entity = new HashMap<String, String>();
+    List<Map<String, String>> fields = new ArrayList<>();
+    Map<String, String> entity = new HashMap<>();
     entity.put("transformer", T1.class.getName() + "," + T2.class.getName()
             + "," + T3.class.getName());
     fields.add(getField("A", null, null, null, null));
@@ -44,7 +44,7 @@ public class TestEntityProcessorBase extends AbstractDataImportHandlerTestCase {
 
     Context context = getContext(null, null, new MockDataSource(), Context.FULL_DUMP,
             fields, entity);
-    Map<String, Object> src = new HashMap<String, Object>();
+    Map<String, Object> src = new HashMap<>();
     src.put("A", "NA");
     src.put("B", "NA");
     EntityProcessorWrapper sep = new EntityProcessorWrapper(new SqlEntityProcessor(), null, null);
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestErrorHandling.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestErrorHandling.java
index 5edc9d8..bbf66e3 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestErrorHandling.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestErrorHandling.java
@@ -83,7 +83,7 @@ public class TestErrorHandling extends AbstractDataImportHandlerTestCase {
 
   public void testTransformerErrorContinue() throws Exception {
     StringDataSource.xml = wellformedXml;
-    List<Map<String, Object>> rows = new ArrayList<Map<String, Object>>();
+    List<Map<String, Object>> rows = new ArrayList<>();
     rows.add(createMap("id", "3", "desc", "exception-transformer"));
     MockDataSource.setIterator("select * from foo", rows.iterator());
     runFullImport(dataConfigWithTransformer);
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestFieldReader.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestFieldReader.java
index 06f6a83..cac2c28 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestFieldReader.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestFieldReader.java
@@ -37,7 +37,7 @@ public class TestFieldReader extends AbstractDataImportHandlerTestCase {
     di.loadAndInit(config);
     TestDocBuilder.SolrWriterImpl sw = new TestDocBuilder.SolrWriterImpl();
     RequestInfo rp = new RequestInfo(null, createMap("command", "full-import"), null);
-    List<Map<String, Object>> l = new ArrayList<Map<String, Object>>();
+    List<Map<String, Object>> l = new ArrayList<>();
     l.add(createMap("xml", xml));
     MockDataSource.setIterator("select * from a", l.iterator());
     di.runCmd(rp, sw);
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestFileListEntityProcessor.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestFileListEntityProcessor.java
index 91ebd1a..6540655 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestFileListEntityProcessor.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestFileListEntityProcessor.java
@@ -51,7 +51,7 @@ public class TestFileListEntityProcessor extends AbstractDataImportHandlerTestCa
             new VariableResolver(), null, Context.FULL_DUMP, Collections.EMPTY_LIST, attrs);
     FileListEntityProcessor fileListEntityProcessor = new FileListEntityProcessor();
     fileListEntityProcessor.init(c);
-    List<String> fList = new ArrayList<String>();
+    List<String> fList = new ArrayList<>();
     while (true) {
       Map<String, Object> f = fileListEntityProcessor.nextRow();
       if (f == null)
@@ -93,10 +93,10 @@ public class TestFileListEntityProcessor extends AbstractDataImportHandlerTestCa
             FileListEntityProcessor.BIGGER_THAN, String.valueOf(minLength));
     List<String> fList = getFiles(null, attrs);
     assertEquals(2, fList.size());
-    Set<String> l = new HashSet<String>();
+    Set<String> l = new HashSet<>();
     l.add(new File(tmpdir, "a.xml").getAbsolutePath());
     l.add(new File(tmpdir, "b.xml").getAbsolutePath());
-    assertEquals(l, new HashSet<String>(fList));
+    assertEquals(l, new HashSet<>(fList));
     attrs = createMap(
             FileListEntityProcessor.FILE_NAME, ".*",
             FileListEntityProcessor.BASE_DIR, tmpdir.getAbsolutePath(),
@@ -104,7 +104,7 @@ public class TestFileListEntityProcessor extends AbstractDataImportHandlerTestCa
     fList = getFiles(null, attrs);
     l.clear();
     l.add(new File(tmpdir, smallestFile).getAbsolutePath());
-    assertEquals(l, new HashSet<String>(fList));
+    assertEquals(l, new HashSet<>(fList));
     attrs = createMap(
             FileListEntityProcessor.FILE_NAME, ".*",
             FileListEntityProcessor.BASE_DIR, tmpdir.getAbsolutePath(),
@@ -112,7 +112,7 @@ public class TestFileListEntityProcessor extends AbstractDataImportHandlerTestCa
     VariableResolver resolver = new VariableResolver();
     resolver.addNamespace("a", createMap("x", "4"));
     fList = getFiles(resolver, attrs);
-    assertEquals(l, new HashSet<String>(fList));
+    assertEquals(l, new HashSet<>(fList));
   }
 
   @SuppressWarnings("unchecked")
@@ -121,7 +121,7 @@ public class TestFileListEntityProcessor extends AbstractDataImportHandlerTestCa
             resolver, null, Context.FULL_DUMP, Collections.EMPTY_LIST, attrs);
     FileListEntityProcessor fileListEntityProcessor = new FileListEntityProcessor();
     fileListEntityProcessor.init(c);
-    List<String> fList = new ArrayList<String>();
+    List<String> fList = new ArrayList<>();
     while (true) {
       Map<String, Object> f = fileListEntityProcessor.nextRow();
       if (f == null)
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestJdbcDataSource.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestJdbcDataSource.java
index da3c0a5..2725b77 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestJdbcDataSource.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestJdbcDataSource.java
@@ -49,7 +49,7 @@ public class TestJdbcDataSource extends AbstractDataImportHandlerTestCase {
   private Connection connection;
   private IMocksControl mockControl;
   private JdbcDataSource jdbcDataSource = new JdbcDataSource();
-  List<Map<String, String>> fields = new ArrayList<Map<String, String>>();
+  List<Map<String, String>> fields = new ArrayList<>();
 
   Context context = AbstractDataImportHandlerTestCase.getContext(null, null,
           jdbcDataSource, Context.FULL_DUMP, fields, null);
@@ -207,12 +207,12 @@ public class TestJdbcDataSource extends AbstractDataImportHandlerTestCase {
     p.put("user", "root");
     p.put("password", "");
 
-    List<Map<String, String>> flds = new ArrayList<Map<String, String>>();
-    Map<String, String> f = new HashMap<String, String>();
+    List<Map<String, String>> flds = new ArrayList<>();
+    Map<String, String> f = new HashMap<>();
     f.put("column", "trim_id");
     f.put("type", "long");
     flds.add(f);
-    f = new HashMap<String, String>();
+    f = new HashMap<>();
     f.put("column", "msrp");
     f.put("type", "float");
     flds.add(f);
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestLineEntityProcessor.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestLineEntityProcessor.java
index f79ed9a..0cb07d3 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestLineEntityProcessor.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestLineEntityProcessor.java
@@ -62,7 +62,7 @@ public class TestLineEntityProcessor extends AbstractDataImportHandlerTestCase {
 
     /// call the entity processor to the list of lines
     if (VERBOSE) System.out.print("\n");
-    List<String> fList = new ArrayList<String>();
+    List<String> fList = new ArrayList<>();
     while (true) {
       Map<String, Object> f = ep.nextRow();
       if (f == null) break;
@@ -101,7 +101,7 @@ public class TestLineEntityProcessor extends AbstractDataImportHandlerTestCase {
     ep.init(c);
 
     /// call the entity processor to the list of lines
-    List<String> fList = new ArrayList<String>();
+    List<String> fList = new ArrayList<>();
     while (true) {
       Map<String, Object> f = ep.nextRow();
       if (f == null) break;
@@ -139,7 +139,7 @@ public class TestLineEntityProcessor extends AbstractDataImportHandlerTestCase {
     ep.init(c);
 
     /// call the entity processor to walk the directory
-    List<String> fList = new ArrayList<String>();
+    List<String> fList = new ArrayList<>();
     while (true) {
       Map<String, Object> f = ep.nextRow();
       if (f == null) break;
@@ -175,7 +175,7 @@ public class TestLineEntityProcessor extends AbstractDataImportHandlerTestCase {
     ep.init(c);
 
     /// call the entity processor to walk the directory
-    List<String> fList = new ArrayList<String>();
+    List<String> fList = new ArrayList<>();
     while (true) {
       Map<String, Object> f = ep.nextRow();
       if (f == null) break;
@@ -195,7 +195,7 @@ public class TestLineEntityProcessor extends AbstractDataImportHandlerTestCase {
           String rw,  // DIH regex attribute 'replaceWith'
           String gn    // DIH regex attribute 'groupNames'
   ) {
-    HashMap<String, String> vals = new HashMap<String, String>();
+    HashMap<String, String> vals = new HashMap<>();
     vals.put("column", col);
     vals.put("type", type);
     vals.put("sourceColName", srcCol);
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestNumberFormatTransformer.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestNumberFormatTransformer.java
index 723b5c2..6f76fb8 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestNumberFormatTransformer.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestNumberFormatTransformer.java
@@ -40,7 +40,7 @@ public class TestNumberFormatTransformer extends AbstractDataImportHandlerTestCa
   @Test
   public void testTransformRow_SingleNumber() {
     char GERMAN_GROUPING_SEP = new DecimalFormatSymbols(Locale.GERMANY).getGroupingSeparator();
-    List<Map<String, String>> l = new ArrayList<Map<String, String>>();
+    List<Map<String, String>> l = new ArrayList<>();
     l.add(createMap("column", "num",
             NumberFormatTransformer.FORMAT_STYLE, NumberFormatTransformer.NUMBER));
     l.add(createMap("column", "localizedNum",
@@ -55,13 +55,13 @@ public class TestNumberFormatTransformer extends AbstractDataImportHandlerTestCa
   @Test
   @SuppressWarnings("unchecked")
   public void testTransformRow_MultipleNumbers() throws Exception {
-    List<Map<String, String>> fields = new ArrayList<Map<String, String>>();
+    List<Map<String, String>> fields = new ArrayList<>();
     fields.add(createMap(DataImporter.COLUMN, "inputs"));
     fields.add(createMap(DataImporter.COLUMN,
             "outputs", RegexTransformer.SRC_COL_NAME, "inputs",
             NumberFormatTransformer.FORMAT_STYLE, NumberFormatTransformer.NUMBER));
 
-    List<String> inputs = new ArrayList<String>();
+    List<String> inputs = new ArrayList<>();
     inputs.add("123" + GROUPING_SEP + "567");
     inputs.add("245" + GROUPING_SEP + "678");
     Map<String, Object> row = createMap("inputs", inputs);
@@ -72,7 +72,7 @@ public class TestNumberFormatTransformer extends AbstractDataImportHandlerTestCa
     Context context = getContext(null, resolver, null, Context.FULL_DUMP, fields, null);
     new NumberFormatTransformer().transformRow(row, context);
 
-    List<Long> output = new ArrayList<Long>();
+    List<Long> output = new ArrayList<>();
     output.add(new Long(123567));
     output.add(new Long(245678));
     Map<String, Object> outputRow = createMap("inputs", inputs, "outputs", output);
@@ -83,7 +83,7 @@ public class TestNumberFormatTransformer extends AbstractDataImportHandlerTestCa
   @Test(expected = DataImportHandlerException.class)
   @SuppressWarnings("unchecked")
   public void testTransformRow_InvalidInput1_Number() {
-    List<Map<String, String>> l = new ArrayList<Map<String, String>>();
+    List<Map<String, String>> l = new ArrayList<>();
     l.add(createMap("column", "num",
             NumberFormatTransformer.FORMAT_STYLE, NumberFormatTransformer.NUMBER));
     Context c = getContext(null, null, null, Context.FULL_DUMP, l, null);
@@ -94,7 +94,7 @@ public class TestNumberFormatTransformer extends AbstractDataImportHandlerTestCa
   @Test(expected = DataImportHandlerException.class)
   @SuppressWarnings("unchecked")
   public void testTransformRow_InvalidInput2_Number() {
-    List<Map<String, String>> l = new ArrayList<Map<String, String>>();
+    List<Map<String, String>> l = new ArrayList<>();
     l.add(createMap("column", "num",
             NumberFormatTransformer.FORMAT_STYLE, NumberFormatTransformer.NUMBER));
     Context c = getContext(null, null, null, Context.FULL_DUMP, l, null);
@@ -105,7 +105,7 @@ public class TestNumberFormatTransformer extends AbstractDataImportHandlerTestCa
   @Test(expected = DataImportHandlerException.class)
   @SuppressWarnings("unchecked")
   public void testTransformRow_InvalidInput2_Currency() {
-    List<Map<String, String>> l = new ArrayList<Map<String, String>>();
+    List<Map<String, String>> l = new ArrayList<>();
     l.add(createMap("column", "num",
             NumberFormatTransformer.FORMAT_STYLE, NumberFormatTransformer.CURRENCY));
     Context c = getContext(null, null, null, Context.FULL_DUMP, l, null);
@@ -116,7 +116,7 @@ public class TestNumberFormatTransformer extends AbstractDataImportHandlerTestCa
   @Test(expected = DataImportHandlerException.class)
   @SuppressWarnings("unchecked")
   public void testTransformRow_InvalidInput1_Percent() {
-    List<Map<String, String>> l = new ArrayList<Map<String, String>>();
+    List<Map<String, String>> l = new ArrayList<>();
     l.add(createMap("column", "num",
             NumberFormatTransformer.FORMAT_STYLE, NumberFormatTransformer.PERCENT));
     Context c = getContext(null, null, null, Context.FULL_DUMP, l, null);
@@ -127,7 +127,7 @@ public class TestNumberFormatTransformer extends AbstractDataImportHandlerTestCa
   @Test(expected = DataImportHandlerException.class)
   @SuppressWarnings("unchecked")
   public void testTransformRow_InvalidInput3_Currency() {
-    List<Map<String, String>> l = new ArrayList<Map<String, String>>();
+    List<Map<String, String>> l = new ArrayList<>();
     l.add(createMap("column", "num",
             NumberFormatTransformer.FORMAT_STYLE, NumberFormatTransformer.CURRENCY));
     Context c = getContext(null, null, null, Context.FULL_DUMP, l, null);
@@ -138,7 +138,7 @@ public class TestNumberFormatTransformer extends AbstractDataImportHandlerTestCa
   @Test(expected = DataImportHandlerException.class)
   @SuppressWarnings("unchecked")
   public void testTransformRow_InvalidInput3_Number() {
-    List<Map<String, String>> l = new ArrayList<Map<String, String>>();
+    List<Map<String, String>> l = new ArrayList<>();
     l.add(createMap("column", "num",
             NumberFormatTransformer.FORMAT_STYLE, NumberFormatTransformer.NUMBER));
     Context c = getContext(null, null, null, Context.FULL_DUMP, l, null);
@@ -149,7 +149,7 @@ public class TestNumberFormatTransformer extends AbstractDataImportHandlerTestCa
   @Test
   @SuppressWarnings("unchecked")
   public void testTransformRow_MalformedInput_Number() {
-    List<Map<String, String>> l = new ArrayList<Map<String, String>>();
+    List<Map<String, String>> l = new ArrayList<>();
     l.add(createMap("column", "num",
             NumberFormatTransformer.FORMAT_STYLE, NumberFormatTransformer.NUMBER));
     Context c = getContext(null, null, null, Context.FULL_DUMP, l, null);
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestRegexTransformer.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestRegexTransformer.java
index bd7eccf..50da4f1 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestRegexTransformer.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestRegexTransformer.java
@@ -38,12 +38,12 @@ public class TestRegexTransformer extends AbstractDataImportHandlerTestCase {
 
   @Test
   public void testCommaSeparated() {
-    List<Map<String, String>> fields = new ArrayList<Map<String, String>>();
+    List<Map<String, String>> fields = new ArrayList<>();
     // <field column="col1" sourceColName="a" splitBy="," />
     fields.add(getField("col1", "string", null, "a", ","));
     Context context = getContext(null, null, null, Context.FULL_DUMP, fields, null);
 
-    Map<String, Object> src = new HashMap<String, Object>();
+    Map<String, Object> src = new HashMap<>();
     src.put("a", "a,bb,cc,d");
 
     Map<String, Object> result = new RegexTransformer().transformRow(src, context);
@@ -54,21 +54,21 @@ public class TestRegexTransformer extends AbstractDataImportHandlerTestCase {
 
   @Test
   public void testGroupNames() {
-    List<Map<String, String>> fields = new ArrayList<Map<String, String>>();
+    List<Map<String, String>> fields = new ArrayList<>();
     // <field column="col1" regex="(\w*)(\w*) (\w*)" groupNames=",firstName,lastName"/>
-    Map<String ,String > m = new HashMap<String, String>();
+    Map<String ,String > m = new HashMap<>();
     m.put(COLUMN,"fullName");
     m.put(GROUP_NAMES,",firstName,lastName");
     m.put(REGEX,"(\\w*) (\\w*) (\\w*)");
     fields.add(m);
     Context context = getContext(null, null, null, Context.FULL_DUMP, fields, null);
-    Map<String, Object> src = new HashMap<String, Object>();
+    Map<String, Object> src = new HashMap<>();
     src.put("fullName", "Mr Noble Paul");
 
     Map<String, Object> result = new RegexTransformer().transformRow(src, context);
     assertEquals("Noble", result.get("firstName"));
     assertEquals("Paul", result.get("lastName"));
-    src= new HashMap<String, Object>();
+    src= new HashMap<>();
     List<String> l= new ArrayList();
     l.add("Mr Noble Paul") ;
     l.add("Mr Shalin Mangar") ;
@@ -84,14 +84,14 @@ public class TestRegexTransformer extends AbstractDataImportHandlerTestCase {
 
   @Test
   public void testReplaceWith() {
-    List<Map<String, String>> fields = new ArrayList<Map<String, String>>();
+    List<Map<String, String>> fields = new ArrayList<>();
     // <field column="name" regexp="'" replaceWith="''" />
     Map<String, String> fld = getField("name", "string", "'", null, null);
     fld.put(REPLACE_WITH, "''");
     fields.add(fld);
     Context context = getContext(null, null, null, Context.FULL_DUMP, fields, null);
 
-    Map<String, Object> src = new HashMap<String, Object>();
+    Map<String, Object> src = new HashMap<>();
     String s = "D'souza";
     src.put("name", s);
 
@@ -130,7 +130,7 @@ public class TestRegexTransformer extends AbstractDataImportHandlerTestCase {
     fld.put(GROUP_NAMES,"t4,t5");
     fields.add(fld);
 
-    Map<String, Object> row = new HashMap<String, Object>();
+    Map<String, Object> row = new HashMap<>();
     String s = "Fuel Economy Range: 26 mpg Hwy, 19 mpg City";
     row.put("rowdata", s);
 
@@ -150,14 +150,14 @@ public class TestRegexTransformer extends AbstractDataImportHandlerTestCase {
 
   @Test
   public void testMultiValuedRegex(){
-      List<Map<String, String>> fields = new ArrayList<Map<String, String>>();
+      List<Map<String, String>> fields = new ArrayList<>();
 //    <field column="participant" sourceColName="person" regex="(.*)" />
     Map<String, String> fld = getField("participant", null, "(.*)", "person", null);
     fields.add(fld);
     Context context = getContext(null, null,
             null, Context.FULL_DUMP, fields, null);
 
-    ArrayList<String> strings = new ArrayList<String>();
+    ArrayList<String> strings = new ArrayList<>();
     strings.add("hello");
     strings.add("world");
     Map<String, Object> result = new RegexTransformer().transformRow(createMap("person", strings), context);
@@ -165,7 +165,7 @@ public class TestRegexTransformer extends AbstractDataImportHandlerTestCase {
   }
 
   public static List<Map<String, String>> getFields() {
-    List<Map<String, String>> fields = new ArrayList<Map<String, String>>();
+    List<Map<String, String>> fields = new ArrayList<>();
 
     // <field column="city_mileage" sourceColName="rowdata" regexp=
     //    "Fuel Economy Range:\\s*?\\d*?\\s*?mpg Hwy,\\s*?(\\d*?)\\s*?mpg City"
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestScriptTransformer.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestScriptTransformer.java
index cf08bb9..1f01774 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestScriptTransformer.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestScriptTransformer.java
@@ -48,7 +48,7 @@ public class TestScriptTransformer extends AbstractDataImportHandlerTestCase {
       String script = "function f1(row,context){"
               + "row.put('name','Hello ' + row.get('name'));" + "return row;\n" + "}";
       Context context = getContext("f1", script);
-      Map<String, Object> map = new HashMap<String, Object>();
+      Map<String, Object> map = new HashMap<>();
       map.put("name", "Scott");
       EntityProcessorWrapper sep = new EntityProcessorWrapper(new SqlEntityProcessor(), null, null);
       sep.init(context);
@@ -62,8 +62,8 @@ public class TestScriptTransformer extends AbstractDataImportHandlerTestCase {
   }
 
   private Context getContext(String funcName, String script) {
-    List<Map<String, String>> fields = new ArrayList<Map<String, String>>();
-    Map<String, String> entity = new HashMap<String, String>();
+    List<Map<String, String>> fields = new ArrayList<>();
+    Map<String, String> entity = new HashMap<>();
     entity.put("name", "hello");
     entity.put("transformer", "script:" + funcName);
 
@@ -81,7 +81,7 @@ public class TestScriptTransformer extends AbstractDataImportHandlerTestCase {
               + "row.put('name','Hello ' + row.get('name'));" + "return row;\n" + "}";
 
       Context context = getContext("f1", script);
-      Map<String, Object> map = new HashMap<String, Object>();
+      Map<String, Object> map = new HashMap<>();
       map.put("name", "Scott");
       EntityProcessorWrapper sep = new EntityProcessorWrapper(new SqlEntityProcessor(), null, null);
       sep.init(context);
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSimplePropertiesWriter.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSimplePropertiesWriter.java
index 1aed814..3cf21a7 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSimplePropertiesWriter.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSimplePropertiesWriter.java
@@ -84,13 +84,13 @@ public class TestSimplePropertiesWriter extends AbstractDIHJdbcTestCase {
       SimpleDateFormat df = new SimpleDateFormat(dateFormat, Locale.ROOT);
       Date oneSecondAgo = new Date(System.currentTimeMillis() - 1000);
       
-      Map<String,String> init = new HashMap<String,String>();
+      Map<String,String> init = new HashMap<>();
       init.put("dateFormat", dateFormat);
       init.put("filename", fileName);
       init.put("directory", fileLocation);
       SimplePropertiesWriter spw = new SimplePropertiesWriter();
       spw.init(new DataImporter(), init);
-      Map<String, Object> props = new HashMap<String,Object>();
+      Map<String, Object> props = new HashMap<>();
       props.put("SomeDates.last_index_time", oneSecondAgo);
       props.put("last_index_time", oneSecondAgo);
       spw.persist(props);
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSolrEntityProcessorEndToEnd.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSolrEntityProcessorEndToEnd.java
index 92a0b9c..5aa55e7 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSolrEntityProcessorEndToEnd.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSolrEntityProcessorEndToEnd.java
@@ -53,17 +53,17 @@ public class TestSolrEntityProcessorEndToEnd extends AbstractDataImportHandlerTe
 
   private static final String DEAD_SOLR_SERVER = "http://[ff01::114]:33332/solr";
   
-  private static final List<Map<String,Object>> DB_DOCS = new ArrayList<Map<String,Object>>();
-  private static final List<Map<String,Object>> SOLR_DOCS = new ArrayList<Map<String,Object>>();
+  private static final List<Map<String,Object>> DB_DOCS = new ArrayList<>();
+  private static final List<Map<String,Object>> SOLR_DOCS = new ArrayList<>();
   
   static {
     // dynamic fields in the destination schema
-    Map<String,Object> dbDoc = new HashMap<String,Object>();
+    Map<String,Object> dbDoc = new HashMap<>();
     dbDoc.put("dbid_s", "1");
     dbDoc.put("dbdesc_s", "DbDescription");
     DB_DOCS.add(dbDoc);
 
-    Map<String,Object> solrDoc = new HashMap<String,Object>();
+    Map<String,Object> solrDoc = new HashMap<>();
     solrDoc.put("id", "1");
     solrDoc.put("desc", "SolrDescription");
     SOLR_DOCS.add(solrDoc);
@@ -161,7 +161,7 @@ public class TestSolrEntityProcessorEndToEnd extends AbstractDataImportHandlerTe
     
     try {
       addDocumentsToSolr(generateSolrDocuments(30));
-      Map<String,String> map = new HashMap<String,String>();
+      Map<String,String> map = new HashMap<>();
       map.put("rows", "50");
       runFullImport(generateDIHConfig("query='*:*' fq='desc:Description1*,desc:Description*2' rows='2'", false), map);
     } catch (Exception e) {
@@ -202,15 +202,15 @@ public class TestSolrEntityProcessorEndToEnd extends AbstractDataImportHandlerTe
     assertQ(req("*:*"), "//result[@numFound='0']");
     
     try {
-      List<Map<String,Object>> DOCS = new ArrayList<Map<String,Object>>(DB_DOCS);
-      Map<String, Object> doc = new HashMap<String, Object>();
+      List<Map<String,Object>> DOCS = new ArrayList<>(DB_DOCS);
+      Map<String, Object> doc = new HashMap<>();
       doc.put("dbid_s", "2");
       doc.put("dbdesc_s", "DbDescription2");
       DOCS.add(doc);
       MockDataSource.setIterator("select * from x", DOCS.iterator());
 
-      DOCS = new ArrayList<Map<String,Object>>(SOLR_DOCS);
-      Map<String,Object> solrDoc = new HashMap<String,Object>();
+      DOCS = new ArrayList<>(SOLR_DOCS);
+      Map<String,Object> solrDoc = new HashMap<>();
       solrDoc.put("id", "2");
       solrDoc.put("desc", "SolrDescription2");
       DOCS.add(solrDoc);
@@ -261,9 +261,9 @@ public class TestSolrEntityProcessorEndToEnd extends AbstractDataImportHandlerTe
   }
     
   private static List<Map<String,Object>> generateSolrDocuments(int num) {
-    List<Map<String,Object>> docList = new ArrayList<Map<String,Object>>();
+    List<Map<String,Object>> docList = new ArrayList<>();
     for (int i = 1; i <= num; i++) {
-      Map<String,Object> map = new HashMap<String,Object>();
+      Map<String,Object> map = new HashMap<>();
       map.put("id", i);
       map.put("desc", "Description" + i);
       docList.add(map);
@@ -272,7 +272,7 @@ public class TestSolrEntityProcessorEndToEnd extends AbstractDataImportHandlerTe
   }
   
   private void addDocumentsToSolr(List<Map<String,Object>> docs) throws SolrServerException, IOException {
-    List<SolrInputDocument> sidl = new ArrayList<SolrInputDocument>();
+    List<SolrInputDocument> sidl = new ArrayList<>();
     for (Map<String,Object> doc : docs) {
       SolrInputDocument sd = new SolrInputDocument();
       for (Entry<String,Object> entry : doc.entrySet()) {
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSolrEntityProcessorUnit.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSolrEntityProcessorUnit.java
index afd9450..cab6241 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSolrEntityProcessorUnit.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSolrEntityProcessorUnit.java
@@ -46,8 +46,8 @@ public class TestSolrEntityProcessorUnit extends AbstractDataImportHandlerTestCa
   }
 
   public void testMultiValuedFields() {
-    List<Doc> docs = new ArrayList<Doc>();
-    List<FldType> types = new ArrayList<FldType>();
+    List<Doc> docs = new ArrayList<>();
+    List<FldType> types = new ArrayList<>();
     types.add(new FldType(ID, ONE_ONE, new SVal('A', 'Z', 4, 4)));
     types.add(new FldType("description", new IRange(3, 3), new SVal('a', 'c', 1, 1)));
     Doc testDoc = createDoc(types);
@@ -66,12 +66,12 @@ public class TestSolrEntityProcessorUnit extends AbstractDataImportHandlerTestCa
   }
 
   private List<Doc> generateUniqueDocs(int numDocs) {
-    List<FldType> types = new ArrayList<FldType>();
+    List<FldType> types = new ArrayList<>();
     types.add(new FldType(ID, ONE_ONE, new SVal('A', 'Z', 4, 40)));
     types.add(new FldType("description", new IRange(1, 3), new SVal('a', 'c', 1, 1)));
 
-    Set<Comparable> previousIds = new HashSet<Comparable>();
-    List<Doc> docs = new ArrayList<Doc>(numDocs);
+    Set<Comparable> previousIds = new HashSet<>();
+    List<Doc> docs = new ArrayList<>(numDocs);
     for (int i = 0; i < numDocs; i++) {
       Doc doc = createDoc(types);
       while (previousIds.contains(doc.id)) {
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSortedMapBackedCache.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSortedMapBackedCache.java
index 023831a..29e1df6 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSortedMapBackedCache.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSortedMapBackedCache.java
@@ -79,11 +79,11 @@ public class TestSortedMapBackedCache extends AbstractDIHCacheTestCase {
     DIHCache cache = null;
     try {
       cache = new SortedMapBackedCache();
-      Map<String, String> cacheProps = new HashMap<String, String>();
+      Map<String, String> cacheProps = new HashMap<>();
       cacheProps.put(DIHCacheSupport.CACHE_PRIMARY_KEY, "a_id");
       cache.open(getContext(cacheProps));
       
-      Map<String,Object> data = new HashMap<String,Object>();
+      Map<String,Object> data = new HashMap<>();
       data.put("a_id", null);
       data.put("bogus", "data");
       cache.add(data);
@@ -108,7 +108,7 @@ public class TestSortedMapBackedCache extends AbstractDIHCacheTestCase {
   public void testCacheReopensWithUpdate() {
     DIHCache cache = null;
     try {      
-      Map<String, String> cacheProps = new HashMap<String, String>();
+      Map<String, String> cacheProps = new HashMap<>();
       cacheProps.put(DIHCacheSupport.CACHE_PRIMARY_KEY, "a_id");
       
       cache = new SortedMapBackedCache();
@@ -120,7 +120,7 @@ public class TestSortedMapBackedCache extends AbstractDIHCacheTestCase {
       // Close the cache.
       cache.close();
 
-      List<ControlData> newControlData = new ArrayList<ControlData>();
+      List<ControlData> newControlData = new ArrayList<>();
       Object[] newIdEqualsThree = null;
       int j = 0;
       for (int i = 0; i < data.size(); i++) {
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestURLDataSource.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestURLDataSource.java
index 7d8f1b8..c1acc54 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestURLDataSource.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestURLDataSource.java
@@ -25,7 +25,7 @@ import java.util.Properties;
 import org.junit.Test;
 
 public class TestURLDataSource extends AbstractDataImportHandlerTestCase {
-  private List<Map<String, String>> fields = new ArrayList<Map<String, String>>();
+  private List<Map<String, String>> fields = new ArrayList<>();
   private URLDataSource dataSource = new URLDataSource();
   private VariableResolver variableResolver = new VariableResolver();
   private Context context = AbstractDataImportHandlerTestCase.getContext(null, variableResolver,
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestVariableResolver.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestVariableResolver.java
index 2766f82..e7ff2e6 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestVariableResolver.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestVariableResolver.java
@@ -36,7 +36,7 @@ public class TestVariableResolver extends AbstractDataImportHandlerTestCase {
   @Test
   public void testSimpleNamespace() {
     VariableResolver vri = new VariableResolver();
-    Map<String,Object> ns = new HashMap<String,Object>();
+    Map<String,Object> ns = new HashMap<>();
     ns.put("world", "WORLD");
     vri.addNamespace("hello", ns);
     assertEquals("WORLD", vri.resolve("hello.world"));
@@ -61,10 +61,10 @@ public class TestVariableResolver extends AbstractDataImportHandlerTestCase {
   @Test
   public void testNestedNamespace() {
     VariableResolver vri = new VariableResolver();
-    Map<String,Object> ns = new HashMap<String,Object>();
+    Map<String,Object> ns = new HashMap<>();
     ns.put("world", "WORLD");
     vri.addNamespace("hello", ns);
-    ns = new HashMap<String,Object>();
+    ns = new HashMap<>();
     ns.put("world1", "WORLD1");
     vri.addNamespace("hello.my", ns);
     assertEquals("WORLD1", vri.resolve("hello.my.world1"));
@@ -73,10 +73,10 @@ public class TestVariableResolver extends AbstractDataImportHandlerTestCase {
   @Test
   public void test3LevelNestedNamespace() {
     VariableResolver vri = new VariableResolver();
-    Map<String,Object> ns = new HashMap<String,Object>();
+    Map<String,Object> ns = new HashMap<>();
     ns.put("world", "WORLD");
     vri.addNamespace("hello", ns);
-    ns = new HashMap<String,Object>();
+    ns = new HashMap<>();
     ns.put("world1", "WORLD1");
     vri.addNamespace("hello.my.new", ns);
     assertEquals("WORLD1", vri.resolve("hello.my.new.world1"));
@@ -87,7 +87,7 @@ public class TestVariableResolver extends AbstractDataImportHandlerTestCase {
     VariableResolver vri = new VariableResolver();
     vri.setEvaluators(new DataImporter().getEvaluators(Collections
         .<Map<String,String>> emptyList()));
-    Map<String,Object> ns = new HashMap<String,Object>();
+    Map<String,Object> ns = new HashMap<>();
     Date d = new Date();
     ns.put("dt", d);
     vri.addNamespace("A", ns);
@@ -115,7 +115,7 @@ public class TestVariableResolver extends AbstractDataImportHandlerTestCase {
   @Test
   public void testDefaultNamespace() {
     VariableResolver vri = new VariableResolver();
-    Map<String,Object> ns = new HashMap<String,Object>();
+    Map<String,Object> ns = new HashMap<>();
     ns.put("world", "WORLD");
     vri.addNamespace(null, ns);
     assertEquals("WORLD", vri.resolve("world"));
@@ -124,7 +124,7 @@ public class TestVariableResolver extends AbstractDataImportHandlerTestCase {
   @Test
   public void testDefaultNamespace1() {
     VariableResolver vri = new VariableResolver();
-    Map<String,Object> ns = new HashMap<String,Object>();
+    Map<String,Object> ns = new HashMap<>();
     ns.put("world", "WORLD");
     vri.addNamespace(null, ns);
     assertEquals("WORLD", vri.resolve("world"));
@@ -133,8 +133,8 @@ public class TestVariableResolver extends AbstractDataImportHandlerTestCase {
   @Test
   public void testFunctionNamespace1() throws Exception {
     VariableResolver resolver = new VariableResolver();
-    final List<Map<String,String>> l = new ArrayList<Map<String,String>>();
-    Map<String,String> m = new HashMap<String,String>();
+    final List<Map<String,String>> l = new ArrayList<>();
+    Map<String,String> m = new HashMap<>();
     m.put("name", "test");
     m.put("class", E.class.getName());
     l.add(m);
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestXPathEntityProcessor.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestXPathEntityProcessor.java
index 3d105f6..dfbdcbf 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestXPathEntityProcessor.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestXPathEntityProcessor.java
@@ -57,7 +57,7 @@ public class TestXPathEntityProcessor extends AbstractDataImportHandlerTestCase
             new VariableResolver(), getDataSource(cdData), Context.FULL_DUMP, fields, entityAttrs);
     XPathEntityProcessor xPathEntityProcessor = new XPathEntityProcessor();
     xPathEntityProcessor.init(c);
-    List<Map<String, Object>> result = new ArrayList<Map<String, Object>>();
+    List<Map<String, Object>> result = new ArrayList<>();
     while (true) {
       Map<String, Object> row = xPathEntityProcessor.nextRow();
       if (row == null)
@@ -80,7 +80,7 @@ public class TestXPathEntityProcessor extends AbstractDataImportHandlerTestCase
             new VariableResolver(), getDataSource(testXml), Context.FULL_DUMP, fields, entityAttrs);
     XPathEntityProcessor xPathEntityProcessor = new XPathEntityProcessor();
     xPathEntityProcessor.init(c);
-    List<Map<String, Object>> result = new ArrayList<Map<String, Object>>();
+    List<Map<String, Object>> result = new ArrayList<>();
     while (true) {
       Map<String, Object> row = xPathEntityProcessor.nextRow();
       if (row == null)
@@ -109,7 +109,7 @@ public class TestXPathEntityProcessor extends AbstractDataImportHandlerTestCase
             new VariableResolver(), getDataSource(textMultipleDocuments), Context.FULL_DUMP, fields, entityAttrs);
     XPathEntityProcessor xPathEntityProcessor = new XPathEntityProcessor();
     xPathEntityProcessor.init(c);
-    List<Map<String, Object>> result = new ArrayList<Map<String, Object>>();
+    List<Map<String, Object>> result = new ArrayList<>();
     while (true) {
       Map<String, Object> row = xPathEntityProcessor.nextRow();
       if (row == null)
@@ -276,7 +276,7 @@ public class TestXPathEntityProcessor extends AbstractDataImportHandlerTestCase
     xPathEntityProcessor.blockingQueueTimeOutUnits = TimeUnit.MICROSECONDS;
     
     xPathEntityProcessor.init(c);
-    List<Map<String, Object>> result = new ArrayList<Map<String, Object>>();
+    List<Map<String, Object>> result = new ArrayList<>();
     while (true) {
       if (rowsToRead >= 0 && result.size() >= rowsToRead) {
         Thread.currentThread().interrupt();
@@ -346,7 +346,7 @@ public class TestXPathEntityProcessor extends AbstractDataImportHandlerTestCase
             new VariableResolver(), getDataSource(cdData), Context.FULL_DUMP, null, entityAttrs);
     XPathEntityProcessor xPathEntityProcessor = new XPathEntityProcessor();
     xPathEntityProcessor.init(c);
-    List<Map<String, Object>> result = new ArrayList<Map<String, Object>>();
+    List<Map<String, Object>> result = new ArrayList<>();
     while (true) {
       Map<String, Object> row = xPathEntityProcessor.nextRow();
       if (row == null)
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestXPathRecordReader.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestXPathRecordReader.java
index 7eabcc9..f3a2f5a 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestXPathRecordReader.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestXPathRecordReader.java
@@ -136,8 +136,8 @@ public class TestXPathRecordReader extends AbstractDataImportHandlerTestCase {
     rr.addField("a", "/root/x/b/@a", false);
     rr.addField("b", "/root/x/b/@b", false);
 
-    final List<Map<String, Object>> a = new ArrayList<Map<String, Object>>();
-    final List<Map<String, Object>> x = new ArrayList<Map<String, Object>>();
+    final List<Map<String, Object>> a = new ArrayList<>();
+    final List<Map<String, Object>> x = new ArrayList<>();
     rr.streamRecords(new StringReader(xml), new XPathRecordReader.Handler() {
       @Override
       public void handle(Map<String, Object> record, String xpath) {
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestZKPropertiesWriter.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestZKPropertiesWriter.java
index 7947431..cc96407 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestZKPropertiesWriter.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestZKPropertiesWriter.java
@@ -106,11 +106,11 @@ public class TestZKPropertiesWriter extends AbstractDataImportHandlerTestCase {
     SimpleDateFormat df = new SimpleDateFormat(dateFormat, Locale.ROOT);
     Date oneSecondAgo = new Date(System.currentTimeMillis() - 1000);
 
-    Map<String, String> init = new HashMap<String, String>();
+    Map<String, String> init = new HashMap<>();
     init.put("dateFormat", dateFormat);
     ZKPropertiesWriter spw = new ZKPropertiesWriter();
     spw.init(new DataImporter(h.getCore(), "dataimport"), init);
-    Map<String, Object> props = new HashMap<String, Object>();
+    Map<String, Object> props = new HashMap<>();
     props.put("SomeDates.last_index_time", oneSecondAgo);
     props.put("last_index_time", oneSecondAgo);
     spw.persist(props);
diff --git solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TripleThreatTransformer.java solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TripleThreatTransformer.java
index 380ef92..63f3e25 100644
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TripleThreatTransformer.java
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TripleThreatTransformer.java
@@ -36,17 +36,17 @@ import java.util.Map;
  */
 public class TripleThreatTransformer {
   public Object transformRow(Map<String, Object> row) {
-    List<Map<String, Object>> rows = new ArrayList<Map<String, Object>>(3);
+    List<Map<String, Object>> rows = new ArrayList<>(3);
     rows.add(row);
     rows.add(addDuplicateBackwardsValues(row));
-    rows.add(new LinkedHashMap<String,Object>(row));
+    rows.add(new LinkedHashMap<>(row));
     rows.get(2).put("AddAColumn_s", "Added");
     modifyIdColumn(rows.get(1), 1);
     modifyIdColumn(rows.get(2), 2);
     return rows;
   }
   private LinkedHashMap<String,Object> addDuplicateBackwardsValues(Map<String, Object> row) {
-    LinkedHashMap<String,Object> n = new LinkedHashMap<String,Object>();
+    LinkedHashMap<String,Object> n = new LinkedHashMap<>();
     for(Map.Entry<String,Object> entry : row.entrySet()) {
       String key = entry.getKey();
       if(!"id".equalsIgnoreCase(key)) {
diff --git solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java
index 360259a..172d3b1 100644
--- solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java
+++ solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java
@@ -82,7 +82,7 @@ public class ExtractingRequestHandler extends ContentStreamHandlerBase implement
       }
       NamedList configDateFormats = (NamedList) initArgs.get(DATE_FORMATS);
       if (configDateFormats != null && configDateFormats.size() > 0) {
-        dateFormats = new HashSet<String>();
+        dateFormats = new HashSet<>();
         Iterator<Map.Entry> it = configDateFormats.iterator();
         while (it.hasNext()) {
           String format = (String) it.next().getValue();
diff --git solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/RegexRulesPasswordProvider.java solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/RegexRulesPasswordProvider.java
index 76b8262..35b8f3c 100644
--- solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/RegexRulesPasswordProvider.java
+++ solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/RegexRulesPasswordProvider.java
@@ -41,7 +41,7 @@ import org.slf4j.LoggerFactory;
 public class RegexRulesPasswordProvider implements PasswordProvider {
   private static final Logger log = LoggerFactory.getLogger(RegexRulesPasswordProvider.class);
   
-  private LinkedHashMap<Pattern,String> passwordMap = new LinkedHashMap<Pattern,String>(); 
+  private LinkedHashMap<Pattern,String> passwordMap = new LinkedHashMap<>();
   private String explicitPassword; 
   
   @Override
@@ -72,7 +72,7 @@ public class RegexRulesPasswordProvider implements PasswordProvider {
    * @param is input stream for the file
    */
   public static LinkedHashMap<Pattern,String> parseRulesFile(InputStream is) {
-    LinkedHashMap<Pattern,String> rules = new LinkedHashMap<Pattern,String>();
+    LinkedHashMap<Pattern,String> rules = new LinkedHashMap<>();
     BufferedReader br = new BufferedReader(IOUtils.getDecodingReader(is, IOUtils.CHARSET_UTF_8));
     String line;
     try {
diff --git solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/SolrContentHandler.java solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/SolrContentHandler.java
index f0cc5d0..e4bd7e0 100644
--- solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/SolrContentHandler.java
+++ solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/SolrContentHandler.java
@@ -57,7 +57,7 @@ public class SolrContentHandler extends DefaultHandler implements ExtractingPara
   protected StringBuilder catchAllBuilder = new StringBuilder(2048);
   protected IndexSchema schema;
   protected Map<String, StringBuilder> fieldBuilders = Collections.emptyMap();
-  private LinkedList<StringBuilder> bldrStack = new LinkedList<StringBuilder>();
+  private LinkedList<StringBuilder> bldrStack = new LinkedList<>();
 
   protected boolean captureAttribs;
   protected boolean lowerNames;
@@ -89,7 +89,7 @@ public class SolrContentHandler extends DefaultHandler implements ExtractingPara
     this.defaultField = params.get(DEFAULT_FIELD, "");
     String[] captureFields = params.getParams(CAPTURE_ELEMENTS);
     if (captureFields != null && captureFields.length > 0) {
-      fieldBuilders = new HashMap<String, StringBuilder>();
+      fieldBuilders = new HashMap<>();
       for (int i = 0; i < captureFields.length; i++) {
         fieldBuilders.put(captureFields[i], new StringBuilder());
       }
@@ -158,7 +158,7 @@ public class SolrContentHandler extends DefaultHandler implements ExtractingPara
    */
   protected void addLiterals() {
     Iterator<String> paramNames = params.getParameterNamesIterator();
-    literalFieldNames = new HashSet<String>();
+    literalFieldNames = new HashSet<>();
     while (paramNames.hasNext()) {
       String pname = paramNames.next();
       if (!pname.startsWith(LITERALS_PREFIX)) continue;
diff --git solr/contrib/extraction/src/test/org/apache/solr/handler/extraction/ExtractingRequestHandlerTest.java solr/contrib/extraction/src/test/org/apache/solr/handler/extraction/ExtractingRequestHandlerTest.java
index 2f59ec6..4bc476a 100644
--- solr/contrib/extraction/src/test/org/apache/solr/handler/extraction/ExtractingRequestHandlerTest.java
+++ solr/contrib/extraction/src/test/org/apache/solr/handler/extraction/ExtractingRequestHandlerTest.java
@@ -616,7 +616,7 @@ public class ExtractingRequestHandlerTest extends SolrTestCaseJ4 {
     try {
       // TODO: stop using locally defined streams once stream.file and
       // stream.body work everywhere
-      List<ContentStream> cs = new ArrayList<ContentStream>();
+      List<ContentStream> cs = new ArrayList<>();
       cs.add(new ContentStreamBase.FileStream(getFile(filename)));
       req.setContentStreams(cs);
       return h.queryAndResponse("/update/extract", req);
diff --git solr/contrib/langid/src/java/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessor.java solr/contrib/langid/src/java/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessor.java
index 83e949c..2b6d121 100644
--- solr/contrib/langid/src/java/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessor.java
+++ solr/contrib/langid/src/java/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessor.java
@@ -53,7 +53,7 @@ public class LangDetectLanguageIdentifierUpdateProcessor extends LanguageIdentif
       Detector detector = DetectorFactory.create();
       detector.append(content);
       ArrayList<Language> langlist = detector.getProbabilities();
-      ArrayList<DetectedLanguage> solrLangList = new ArrayList<DetectedLanguage>();
+      ArrayList<DetectedLanguage> solrLangList = new ArrayList<>();
       for (Language l: langlist) {
         solrLangList.add(new DetectedLanguage(l.lang, l.prob));
       }
diff --git solr/contrib/langid/src/java/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessorFactory.java solr/contrib/langid/src/java/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessorFactory.java
index 2b20143..9a50840 100644
--- solr/contrib/langid/src/java/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessorFactory.java
+++ solr/contrib/langid/src/java/org/apache/solr/update/processor/LangDetectLanguageIdentifierUpdateProcessorFactory.java
@@ -125,7 +125,7 @@ public class LangDetectLanguageIdentifierUpdateProcessorFactory extends
       return;
     }
     loaded = true;
-    List<String> profileData = new ArrayList<String>();
+    List<String> profileData = new ArrayList<>();
     Charset encoding = Charset.forName("UTF-8");
     for (String language : languages) {
       InputStream stream = LangDetectLanguageIdentifierUpdateProcessor.class.getResourceAsStream("langdetect-profiles/" + language);
diff --git solr/contrib/langid/src/java/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessor.java solr/contrib/langid/src/java/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessor.java
index 1717c48..2fcd02f 100644
--- solr/contrib/langid/src/java/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessor.java
+++ solr/contrib/langid/src/java/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessor.java
@@ -107,7 +107,7 @@ public abstract class LanguageIdentifierUpdateProcessor extends UpdateRequestPro
         fallbackFields = params.get(FALLBACK_FIELDS).split(",");
       }
       overwrite = params.getBool(OVERWRITE, false);
-      langWhitelist = new HashSet<String>();
+      langWhitelist = new HashSet<>();
       threshold = params.getDouble(THRESHOLD, DOCID_THRESHOLD_DEFAULT);
       if(params.get(LANG_WHITELIST, "").length() > 0) {
         for(String lang : params.get(LANG_WHITELIST, "").split(",")) {
@@ -133,15 +133,15 @@ public abstract class LanguageIdentifierUpdateProcessor extends UpdateRequestPro
       } else {
         mapIndividualFields = mapFields;
       }
-      mapIndividualFieldsSet = new HashSet<String>(Arrays.asList(mapIndividualFields));
+      mapIndividualFieldsSet = new HashSet<>(Arrays.asList(mapIndividualFields));
       // Compile a union of the lists of fields to map
-      allMapFieldsSet = new HashSet<String>(Arrays.asList(mapFields));
+      allMapFieldsSet = new HashSet<>(Arrays.asList(mapFields));
       if(Arrays.equals(mapFields, mapIndividualFields)) {
         allMapFieldsSet.addAll(mapIndividualFieldsSet);
       }
 
       // Normalize detected langcode onto normalized langcode
-      lcMap = new HashMap<String,String>();
+      lcMap = new HashMap<>();
       if(params.get(LCMAP) != null) {
         for(String mapping : params.get(LCMAP).split("[, ]")) {
           String[] keyVal = mapping.split(":");
@@ -154,7 +154,7 @@ public abstract class LanguageIdentifierUpdateProcessor extends UpdateRequestPro
       }
 
       // Language Code mapping
-      mapLcMap = new HashMap<String,String>();
+      mapLcMap = new HashMap<>();
       if(params.get(MAP_LCMAP) != null) {
         for(String mapping : params.get(MAP_LCMAP).split("[, ]")) {
           String[] keyVal = mapping.split(":");
@@ -199,7 +199,7 @@ public abstract class LanguageIdentifierUpdateProcessor extends UpdateRequestPro
    */
   protected SolrInputDocument process(SolrInputDocument doc) {
     String docLang = null;
-    HashSet<String> docLangs = new HashSet<String>();
+    HashSet<String> docLangs = new HashSet<>();
     String fallbackLang = getFallbackLang(doc, fallbackFields, fallbackValue);
 
     if(langField == null || !doc.containsKey(langField) || (doc.containsKey(langField) && overwrite)) {
@@ -323,7 +323,7 @@ public abstract class LanguageIdentifierUpdateProcessor extends UpdateRequestPro
    * @return a string of the chosen language
    */
   protected String resolveLanguage(String language, String fallbackLang) {
-    List<DetectedLanguage> l = new ArrayList<DetectedLanguage>();
+    List<DetectedLanguage> l = new ArrayList<>();
     l.add(new DetectedLanguage(language, 1.0));
     return resolveLanguage(l, fallbackLang);
   }
diff --git solr/contrib/langid/src/java/org/apache/solr/update/processor/TikaLanguageIdentifierUpdateProcessor.java solr/contrib/langid/src/java/org/apache/solr/update/processor/TikaLanguageIdentifierUpdateProcessor.java
index 4ba326d..5399cf4 100644
--- solr/contrib/langid/src/java/org/apache/solr/update/processor/TikaLanguageIdentifierUpdateProcessor.java
+++ solr/contrib/langid/src/java/org/apache/solr/update/processor/TikaLanguageIdentifierUpdateProcessor.java
@@ -41,7 +41,7 @@ public class TikaLanguageIdentifierUpdateProcessor extends LanguageIdentifierUpd
   
   @Override
   protected List<DetectedLanguage> detectLanguage(String content) {
-    List<DetectedLanguage> languages = new ArrayList<DetectedLanguage>();
+    List<DetectedLanguage> languages = new ArrayList<>();
     if(content.trim().length() != 0) { 
       LanguageIdentifier identifier = new LanguageIdentifier(content);
       // FIXME: Hack - we get the distance from toString and calculate our own certainty score
diff --git solr/contrib/langid/src/test/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessorFactoryTestCase.java solr/contrib/langid/src/test/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessorFactoryTestCase.java
index d845d18..46bfa4f 100644
--- solr/contrib/langid/src/test/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessorFactoryTestCase.java
+++ solr/contrib/langid/src/test/org/apache/solr/update/processor/LanguageIdentifierUpdateProcessorFactoryTestCase.java
@@ -124,7 +124,7 @@ public abstract class LanguageIdentifierUpdateProcessorFactoryTestCase extends S
     assertEquals("zh", liProcessor.resolveLanguage("zh_cn", "NA"));
     assertEquals("zh", liProcessor.resolveLanguage("zh_tw", "NA"));
     assertEquals("no", liProcessor.resolveLanguage("no", "NA"));
-    List<DetectedLanguage> langs = new ArrayList<DetectedLanguage>();
+    List<DetectedLanguage> langs = new ArrayList<>();
     langs.add(new DetectedLanguage("zh_cn", 0.8));
     assertEquals("zh", liProcessor.resolveLanguage(langs, "NA"));
   }
@@ -246,7 +246,7 @@ public abstract class LanguageIdentifierUpdateProcessorFactoryTestCase extends S
     liProcessor = createLangIdProcessor(parameters);
 
     // No detected languages
-    langs = new ArrayList<DetectedLanguage>();
+    langs = new ArrayList<>();
     assertEquals("", liProcessor.resolveLanguage(langs, null));
     assertEquals("fallback", liProcessor.resolveLanguage(langs, "fallback"));
 
@@ -255,7 +255,7 @@ public abstract class LanguageIdentifierUpdateProcessorFactoryTestCase extends S
     assertEquals("one", liProcessor.resolveLanguage(langs, "fallback"));    
 
     // One detected language under default threshold
-    langs = new ArrayList<DetectedLanguage>();
+    langs = new ArrayList<>();
     langs.add(new DetectedLanguage("under", 0.1));
     assertEquals("fallback", liProcessor.resolveLanguage(langs, "fallback"));    
   }
diff --git solr/contrib/uima/src/java/org/apache/solr/uima/processor/SolrUIMAConfigurationReader.java solr/contrib/uima/src/java/org/apache/solr/uima/processor/SolrUIMAConfigurationReader.java
index e8e252a..68fdc48 100644
--- solr/contrib/uima/src/java/org/apache/solr/uima/processor/SolrUIMAConfigurationReader.java
+++ solr/contrib/uima/src/java/org/apache/solr/uima/processor/SolrUIMAConfigurationReader.java
@@ -65,7 +65,7 @@ public class SolrUIMAConfigurationReader {
 
   @SuppressWarnings("rawtypes")
   private Map<String, Map<String, MapField>> readTypesFeaturesFieldsMapping() {
-    Map<String, Map<String, MapField>> map = new HashMap<String, Map<String, MapField>>();
+    Map<String, Map<String, MapField>> map = new HashMap<>();
 
     NamedList fieldMappings = (NamedList) args.get("fieldMappings");
     /* iterate over UIMA types */
@@ -73,7 +73,7 @@ public class SolrUIMAConfigurationReader {
       NamedList type = (NamedList) fieldMappings.get("type", i);
       String typeName = (String)type.get("name");
 
-      Map<String, MapField> subMap = new HashMap<String, MapField>();
+      Map<String, MapField> subMap = new HashMap<>();
       /* iterate over mapping definitions */
       for(int j = 0; j < type.size() - 1; j++){
         NamedList mapping = (NamedList) type.get("mapping", j + 1);
@@ -96,7 +96,7 @@ public class SolrUIMAConfigurationReader {
 
   @SuppressWarnings("rawtypes")
   private Map<String, Object> readAEOverridingParameters() {
-    Map<String, Object> runtimeParameters = new HashMap<String, Object>();
+    Map<String, Object> runtimeParameters = new HashMap<>();
     NamedList runtimeParams = (NamedList) args.get("runtimeParameters");
     for (int i = 0; i < runtimeParams.size(); i++) {
       String name = runtimeParams.getName(i);
diff --git solr/contrib/uima/src/test/org/apache/solr/uima/processor/UIMAUpdateRequestProcessorTest.java solr/contrib/uima/src/test/org/apache/solr/uima/processor/UIMAUpdateRequestProcessorTest.java
index 3f4a8f2..af41d24 100644
--- solr/contrib/uima/src/test/org/apache/solr/uima/processor/UIMAUpdateRequestProcessorTest.java
+++ solr/contrib/uima/src/test/org/apache/solr/uima/processor/UIMAUpdateRequestProcessorTest.java
@@ -192,7 +192,7 @@ public class UIMAUpdateRequestProcessorTest extends SolrTestCaseJ4 {
   }
 
   private void addDoc(String chain, String doc) throws Exception {
-    Map<String, String[]> params = new HashMap<String, String[]>();
+    Map<String, String[]> params = new HashMap<>();
     params.put(UpdateParams.UPDATE_CHAIN, new String[] { chain });
     MultiMapSolrParams mmparams = new MultiMapSolrParams(params);
     SolrQueryRequestBase req = new SolrQueryRequestBase(h.getCore(), (SolrParams) mmparams) {
@@ -200,7 +200,7 @@ public class UIMAUpdateRequestProcessorTest extends SolrTestCaseJ4 {
 
     UpdateRequestHandler handler = new UpdateRequestHandler();
     handler.init(null);
-    ArrayList<ContentStream> streams = new ArrayList<ContentStream>(2);
+    ArrayList<ContentStream> streams = new ArrayList<>(2);
     streams.add(new ContentStreamBase.StringStream(doc));
     req.setContentStreams(streams);
     handler.handleRequestBody(req, new SolrQueryResponse());
diff --git solr/contrib/velocity/src/java/org/apache/solr/response/SolrParamResourceLoader.java solr/contrib/velocity/src/java/org/apache/solr/response/SolrParamResourceLoader.java
index ab486bb..54d86dd 100644
--- solr/contrib/velocity/src/java/org/apache/solr/response/SolrParamResourceLoader.java
+++ solr/contrib/velocity/src/java/org/apache/solr/response/SolrParamResourceLoader.java
@@ -31,7 +31,7 @@ import java.util.Iterator;
 import java.util.Map;
 
 public class SolrParamResourceLoader extends ResourceLoader {
-  private Map<String,String> templates = new HashMap<String,String>();
+  private Map<String,String> templates = new HashMap<>();
   public SolrParamResourceLoader(SolrQueryRequest request) {
     super();
 
diff --git solr/core/src/java/org/apache/solr/SolrLogFormatter.java solr/core/src/java/org/apache/solr/SolrLogFormatter.java
index 55c01e1..ff1cf22 100644
--- solr/core/src/java/org/apache/solr/SolrLogFormatter.java
+++ solr/core/src/java/org/apache/solr/SolrLogFormatter.java
@@ -47,7 +47,7 @@ public class SolrLogFormatter extends Formatter {
 
   long startTime = System.currentTimeMillis();
   long lastTime = startTime;
-  Map<Method, String> methodAlias = new HashMap<Method, String>();
+  Map<Method, String> methodAlias = new HashMap<>();
   
   public static class Method {
     public String className;
@@ -106,9 +106,9 @@ public class SolrLogFormatter extends Formatter {
     Map<String, Object> coreProps;
   }
 
-  Map<SolrCore, CoreInfo> coreInfoMap = new WeakHashMap<SolrCore, CoreInfo>();    // TODO: use something that survives across a core reload?
+  Map<SolrCore, CoreInfo> coreInfoMap = new WeakHashMap<>();    // TODO: use something that survives across a core reload?
 
-  public Map<String,String> classAliases = new HashMap<String, String>();
+  public Map<String,String> classAliases = new HashMap<>();
 
   @Override
   public String format(LogRecord record) {
@@ -379,7 +379,7 @@ sb.append("(group_name=").append(tg.getName()).append(")");
 
 
 
-  static ThreadLocal<String> threadLocal = new ThreadLocal<String>();
+  static ThreadLocal<String> threadLocal = new ThreadLocal<>();
   
   public static void main(String[] args) throws Exception {
 
diff --git solr/core/src/java/org/apache/solr/analysis/LegacyHTMLStripCharFilter.java solr/core/src/java/org/apache/solr/analysis/LegacyHTMLStripCharFilter.java
index b648142..515f8b7 100644
--- solr/core/src/java/org/apache/solr/analysis/LegacyHTMLStripCharFilter.java
+++ solr/core/src/java/org/apache/solr/analysis/LegacyHTMLStripCharFilter.java
@@ -775,7 +775,7 @@ public class LegacyHTMLStripCharFilter extends BaseCharFilter {
 
   private static final HashMap<String,Character> entityTable;
   static {
-    entityTable = new HashMap<String,Character>();
+    entityTable = new HashMap<>();
     // entityName and entityVal generated from the python script
     // included in comments at the end of this file.
     final String[] entityName={ "zwnj","aring","gt","yen","ograve","Chi","delta","rang","sup","trade","Ntilde","xi","upsih","nbsp","Atilde","radic","otimes","aelig","oelig","equiv","ni","infin","Psi","auml","cup","Epsilon","otilde","lt","Icirc","Eacute","Lambda","sbquo","Prime","prime","psi","Kappa","rsaquo","Tau","uacute","ocirc","lrm","zwj","cedil","Alpha","not","amp","AElig","oslash","acute","lceil","alefsym","laquo","shy","loz","ge","Igrave","nu","Ograve","lsaquo","sube","euro","rarr","sdot","rdquo","Yacute","lfloor","lArr","Auml","Dagger","brvbar","Otilde","szlig","clubs","diams","agrave","Ocirc","Iota","Theta","Pi","zeta","Scaron","frac14","egrave","sub","iexcl","frac12","ordf","sum","prop","Uuml","ntilde","atilde","asymp","uml","prod","nsub","reg","rArr","Oslash","emsp","THORN","yuml","aacute","Mu","hArr","le","thinsp","dArr","ecirc","bdquo","Sigma","Aring","tilde","nabla","mdash","uarr","times","Ugrave","Eta","Agrave","chi","real","circ","eth","rceil","iuml","gamma","lambda","harr","Egrave","frac34","dagger","divide","Ouml","image","ndash","hellip","igrave","Yuml","ang","alpha","frasl","ETH","lowast","Nu","plusmn","bull","sup1","sup2","sup3","Aacute","cent","oline","Beta","perp","Delta","there4","pi","iota","empty","euml","notin","iacute","para","epsilon","weierp","OElig","uuml","larr","icirc","Upsilon","omicron","upsilon","copy","Iuml","Oacute","Xi","kappa","ccedil","Ucirc","cap","mu","scaron","lsquo","isin","Zeta","minus","deg","and","tau","pound","curren","int","ucirc","rfloor","ensp","crarr","ugrave","exist","cong","theta","oplus","permil","Acirc","piv","Euml","Phi","Iacute","quot","Uacute","Omicron","ne","iquest","eta","rsquo","yacute","Rho","darr","Ecirc","Omega","acirc","sim","phi","sigmaf","macr","thetasym","Ccedil","ordm","uArr","forall","beta","fnof","rho","micro","eacute","omega","middot","Gamma","rlm","lang","spades","supe","thorn","ouml","or","raquo","part","sect","ldquo","hearts","sigma","oacute"};
diff --git solr/core/src/java/org/apache/solr/analytics/accumulator/BasicAccumulator.java solr/core/src/java/org/apache/solr/analytics/accumulator/BasicAccumulator.java
index 6a9232f..1e3a2db 100644
--- solr/core/src/java/org/apache/solr/analytics/accumulator/BasicAccumulator.java
+++ solr/core/src/java/org/apache/solr/analytics/accumulator/BasicAccumulator.java
@@ -103,7 +103,7 @@ public class BasicAccumulator extends ValueAccumulator {
   }
   
   public NamedList<?> export(){
-    NamedList<Object> base = new NamedList<Object>();
+    NamedList<Object> base = new NamedList<>();
     for (int count = 0; count < expressions.length; count++) {
       if (!hiddenExpressions.contains(expressionNames[count])) {
         base.add(expressionNames[count], expressions[count].getValue());
diff --git solr/core/src/java/org/apache/solr/analytics/accumulator/FacetingAccumulator.java solr/core/src/java/org/apache/solr/analytics/accumulator/FacetingAccumulator.java
index f22c345..c23e633 100644
--- solr/core/src/java/org/apache/solr/analytics/accumulator/FacetingAccumulator.java
+++ solr/core/src/java/org/apache/solr/analytics/accumulator/FacetingAccumulator.java
@@ -98,14 +98,14 @@ public class FacetingAccumulator extends BasicAccumulator implements FacetValueA
     List<RangeFacetRequest> rangeFreqs = request.getRangeFacets();
     List<QueryFacetRequest> queryFreqs = request.getQueryFacets();
 
-    this.fieldFacetExpressions = new LinkedHashMap<String,Map<String,Expression[]>>(fieldFreqs.size());
-    this.rangeFacetExpressions = new LinkedHashMap<String,Map<String,Expression[]>>(rangeFreqs.size());
-    this.queryFacetExpressions = new LinkedHashMap<String,Map<String,Expression[]>>(queryFreqs.size());
-    this.fieldFacetCollectors = new LinkedHashMap<String,Map<String,StatsCollector[]>>(fieldFreqs.size());
-    this.rangeFacetCollectors = new LinkedHashMap<String,Map<String,StatsCollector[]>>(rangeFreqs.size());
-    this.queryFacetCollectors = new LinkedHashMap<String,Map<String,StatsCollector[]>>(queryFreqs.size());
-    this.facetAccumulators = new ArrayList<FieldFacetAccumulator>();
-    this.hiddenFieldFacets = new HashSet<String>();
+    this.fieldFacetExpressions = new LinkedHashMap<>(fieldFreqs.size());
+    this.rangeFacetExpressions = new LinkedHashMap<>(rangeFreqs.size());
+    this.queryFacetExpressions = new LinkedHashMap<>(queryFreqs.size());
+    this.fieldFacetCollectors = new LinkedHashMap<>(fieldFreqs.size());
+    this.rangeFacetCollectors = new LinkedHashMap<>(rangeFreqs.size());
+    this.queryFacetCollectors = new LinkedHashMap<>(queryFreqs.size());
+    this.facetAccumulators = new ArrayList<>();
+    this.hiddenFieldFacets = new HashSet<>();
     
     /**
      * For each field facet request add a bucket to the {@link Expression} map and {@link StatsCollector} map.
@@ -130,13 +130,13 @@ public class FacetingAccumulator extends BasicAccumulator implements FacetValueA
      * are not created initially.
      */
     for( RangeFacetRequest freq : rangeFreqs ){
-      if( rangeFacets == null ) rangeFacets = new ArrayList<RangeFacetRequest>();
+      if( rangeFacets == null ) rangeFacets = new ArrayList<>();
       rangeFacets.add(freq);
       rangeFacetExpressions.put(freq.getName(), new LinkedHashMap<String,Expression[]>() );
       rangeFacetCollectors.put(freq.getName(), new LinkedHashMap<String,StatsCollector[]>());
     }
     for( QueryFacetRequest freq : queryFreqs ){
-      if( queryFacets == null ) queryFacets = new ArrayList<QueryFacetRequest>();
+      if( queryFacets == null ) queryFacets = new ArrayList<>();
       queryFacets.add(freq);
       queryFacetExpressions.put(freq.getName(), new LinkedHashMap<String,Expression[]>() );
       queryFacetCollectors.put(freq.getName(), new LinkedHashMap<String,StatsCollector[]>());
@@ -442,7 +442,7 @@ public class FacetingAccumulator extends BasicAccumulator implements FacetValueA
   @SuppressWarnings("unchecked")
   public NamedList<?> export() {
     final NamedList<Object> base = (NamedList<Object>)super.export();
-    NamedList<NamedList<?>> facetList = new NamedList<NamedList<?>>();
+    NamedList<NamedList<?>> facetList = new NamedList<>();
     
     // Add the field facet buckets to the output
     base.add("fieldFacets",facetList);
@@ -452,7 +452,7 @@ public class FacetingAccumulator extends BasicAccumulator implements FacetValueA
         continue;
       }
       final Map<String,Expression[]> buckets = fieldFacetExpressions.get(name);
-      final NamedList<Object> bucketBase = new NamedList<Object>();
+      final NamedList<Object> bucketBase = new NamedList<>();
 
       Iterable<Entry<String,Expression[]>> iter = buckets.entrySet();
       
@@ -471,7 +471,7 @@ public class FacetingAccumulator extends BasicAccumulator implements FacetValueA
         final Expression first = buckets.values().iterator().next()[sortPlace];
         final Comparator<Expression> comp = (Comparator<Expression>) first.comparator(sort.getDirection());
         
-        final List<Entry<String,Expression[]>> sorted = new ArrayList<Entry<String,Expression[]>>(buckets.size());
+        final List<Entry<String,Expression[]>> sorted = new ArrayList<>(buckets.size());
         Iterables.addAll(sorted, iter);
         Collections.sort(sorted, new EntryComparator(comp,sortPlace));
         iter = sorted;
@@ -493,12 +493,12 @@ public class FacetingAccumulator extends BasicAccumulator implements FacetValueA
     }
 
     // Add the range facet buckets to the output
-    facetList = new NamedList<NamedList<?>>();
+    facetList = new NamedList<>();
     base.add("rangeFacets",facetList);
     for( RangeFacetRequest freq : request.getRangeFacets() ){
       final String name = freq.getName();
       final Map<String,Expression[]> buckets = rangeFacetExpressions.get(name);
-      final NamedList<Object> bucketBase = new NamedList<Object>();
+      final NamedList<Object> bucketBase = new NamedList<>();
 
       Iterable<Entry<String,Expression[]>> iter = buckets.entrySet();
       
@@ -510,12 +510,12 @@ public class FacetingAccumulator extends BasicAccumulator implements FacetValueA
     }
     
     // Add the query facet buckets to the output
-    facetList = new NamedList<NamedList<?>>();
+    facetList = new NamedList<>();
     base.add("queryFacets",facetList);
     for( QueryFacetRequest freq : request.getQueryFacets() ){
       final String name = freq.getName();
       final Map<String,Expression[]> buckets = queryFacetExpressions.get(name);
-      final NamedList<Object> bucketBase = new NamedList<Object>();
+      final NamedList<Object> bucketBase = new NamedList<>();
 
       Iterable<Entry<String,Expression[]>> iter = buckets.entrySet();
       
@@ -535,7 +535,7 @@ public class FacetingAccumulator extends BasicAccumulator implements FacetValueA
    * @return named list of expressions
    */
   public NamedList<?> export(Expression[] expressionArr) {
-    NamedList<Object> base = new NamedList<Object>();
+    NamedList<Object> base = new NamedList<>();
     for (int count = 0; count < expressionArr.length; count++) {
       if (!hiddenExpressions.contains(expressionNames[count])) {
         base.add(expressionNames[count], expressionArr[count].getValue());
diff --git solr/core/src/java/org/apache/solr/analytics/expression/ExpressionFactory.java solr/core/src/java/org/apache/solr/analytics/expression/ExpressionFactory.java
index 5da5fb0..0fd9db0 100644
--- solr/core/src/java/org/apache/solr/analytics/expression/ExpressionFactory.java
+++ solr/core/src/java/org/apache/solr/analytics/expression/ExpressionFactory.java
@@ -158,7 +158,7 @@ public class ExpressionFactory {
     String[] strings = new String[1];
     int stack = 0;
     int start = 0;
-    List<String> arguments = new ArrayList<String>();
+    List<String> arguments = new ArrayList<>();
     char[] chars = expression.toCharArray();
     for (int count = 0; count < expression.length(); count++) {
       char c = chars[count];
diff --git solr/core/src/java/org/apache/solr/analytics/plugin/AnalyticsStatisticsCollector.java solr/core/src/java/org/apache/solr/analytics/plugin/AnalyticsStatisticsCollector.java
index 74db91d..a57c546 100644
--- solr/core/src/java/org/apache/solr/analytics/plugin/AnalyticsStatisticsCollector.java
+++ solr/core/src/java/org/apache/solr/analytics/plugin/AnalyticsStatisticsCollector.java
@@ -88,7 +88,7 @@ public class AnalyticsStatisticsCollector {
   }
 
   public NamedList<Object> getStatistics() {
-    NamedList<Object> lst = new SimpleOrderedMap<Object>();
+    NamedList<Object> lst = new SimpleOrderedMap<>();
     Snapshot snapshot = requestTimes.getSnapshot();
     lst.add("requests", numRequests.longValue());
     lst.add("analyticsRequests", numAnalyticsRequests.longValue());
diff --git solr/core/src/java/org/apache/solr/analytics/request/AnalyticsContentHandler.java solr/core/src/java/org/apache/solr/analytics/request/AnalyticsContentHandler.java
index 1f038ba..db21094 100644
--- solr/core/src/java/org/apache/solr/analytics/request/AnalyticsContentHandler.java
+++ solr/core/src/java/org/apache/solr/analytics/request/AnalyticsContentHandler.java
@@ -177,28 +177,28 @@ public class AnalyticsContentHandler implements ContentHandler {
           }
           
           // Initiate Range Facet classes
-          gaps = new ArrayList<String>();
+          gaps = new ArrayList<>();
           includeBoundaries = EnumSet.noneOf(FacetRangeInclude.class);
           otherRanges = EnumSet.noneOf(FacetRangeOther.class);
           inRangeFacet = true;
         } else if (localName.equals(QUERY_FACET)) {
           // Start a Query Facet Request
-          queries = new ArrayList<String>();
+          queries = new ArrayList<>();
           inQueryFacet = true;
         }
       } else if (localName.equals(ANALYTICS_REQUEST)){
         // Start an Analytics Request
         
         // Renew each list.
-        fieldFacetList = new ArrayList<FieldFacetRequest>();
-        rangeFacetList = new ArrayList<RangeFacetRequest>();
-        queryFacetList = new ArrayList<QueryFacetRequest>();
-        expressionList = new ArrayList<ExpressionRequest>();
+        fieldFacetList = new ArrayList<>();
+        rangeFacetList = new ArrayList<>();
+        queryFacetList = new ArrayList<>();
+        expressionList = new ArrayList<>();
         inRequest = true;
       }
     } else if (localName.equals(ANALYTICS_REQUEST_ENVELOPE)){
       //Begin the parsing of the Analytics Requests
-      requests = new ArrayList<AnalyticsRequest>();
+      requests = new ArrayList<>();
       inEnvelope = true;
     }
   }
diff --git solr/core/src/java/org/apache/solr/analytics/request/AnalyticsRequest.java solr/core/src/java/org/apache/solr/analytics/request/AnalyticsRequest.java
index 8e8282c..2f24999 100644
--- solr/core/src/java/org/apache/solr/analytics/request/AnalyticsRequest.java
+++ solr/core/src/java/org/apache/solr/analytics/request/AnalyticsRequest.java
@@ -38,11 +38,11 @@ public class AnalyticsRequest {
   
   public AnalyticsRequest(String name) {
     this.name = name;
-    expressions = new ArrayList<ExpressionRequest>();
-    hiddenExpressions = new HashSet<String>();
-    fieldFacets = new ArrayList<FieldFacetRequest>();
-    rangeFacets = new ArrayList<RangeFacetRequest>();
-    queryFacets = new ArrayList<QueryFacetRequest>();
+    expressions = new ArrayList<>();
+    hiddenExpressions = new HashSet<>();
+    fieldFacets = new ArrayList<>();
+    rangeFacets = new ArrayList<>();
+    queryFacets = new ArrayList<>();
   }
   
   public String getName() {
diff --git solr/core/src/java/org/apache/solr/analytics/request/AnalyticsRequestFactory.java solr/core/src/java/org/apache/solr/analytics/request/AnalyticsRequestFactory.java
index 62fa734..3e2e994 100644
--- solr/core/src/java/org/apache/solr/analytics/request/AnalyticsRequestFactory.java
+++ solr/core/src/java/org/apache/solr/analytics/request/AnalyticsRequestFactory.java
@@ -51,14 +51,14 @@ public class AnalyticsRequestFactory implements AnalyticsParams {
   public static final Pattern queryFacetParamPattern = Pattern.compile("^o(?:lap)?\\.([^\\.]+)\\.(?:"+QUERY_FACET+")\\.([^\\.]+)\\.("+QUERY+"|"+DEPENDENCY+")$", Pattern.CASE_INSENSITIVE);
   
   public static List<AnalyticsRequest> parse(IndexSchema schema, SolrParams params) {
-    Map<String, AnalyticsRequest> requestMap = new HashMap<String, AnalyticsRequest>();
-    Map<String, Map<String,FieldFacetRequest>> fieldFacetMap = new HashMap<String, Map<String,FieldFacetRequest>>();
-    Map<String, Set<String>> fieldFacetSet = new HashMap<String,Set<String>>();
-    Map<String, Map<String,RangeFacetRequest>> rangeFacetMap = new HashMap<String, Map<String,RangeFacetRequest>>();
-    Map<String, Set<String>> rangeFacetSet = new HashMap<String,Set<String>>();
-    Map<String, Map<String,QueryFacetRequest>> queryFacetMap = new HashMap<String, Map<String,QueryFacetRequest>>();
-    Map<String, Set<String>> queryFacetSet = new HashMap<String,Set<String>>();
-    List<AnalyticsRequest> requestList = new ArrayList<AnalyticsRequest>();
+    Map<String, AnalyticsRequest> requestMap = new HashMap<>();
+    Map<String, Map<String,FieldFacetRequest>> fieldFacetMap = new HashMap<>();
+    Map<String, Set<String>> fieldFacetSet = new HashMap<>();
+    Map<String, Map<String,RangeFacetRequest>> rangeFacetMap = new HashMap<>();
+    Map<String, Set<String>> rangeFacetSet = new HashMap<>();
+    Map<String, Map<String,QueryFacetRequest>> queryFacetMap = new HashMap<>();
+    Map<String, Set<String>> queryFacetSet = new HashMap<>();
+    List<AnalyticsRequest> requestList = new ArrayList<>();
     
     Iterator<String> paramsIterator = params.getParameterNamesIterator();
     while (paramsIterator.hasNext()) {
@@ -115,7 +115,7 @@ public class AnalyticsRequestFactory implements AnalyticsParams {
     }
     for (String reqName : requestMap.keySet()) {
       AnalyticsRequest ar = requestMap.get(reqName);
-      List<FieldFacetRequest> ffrs = new ArrayList<FieldFacetRequest>();
+      List<FieldFacetRequest> ffrs = new ArrayList<>();
       if (fieldFacetSet.get(reqName)!=null) {
         for (String field : fieldFacetSet.get(reqName)) {
           ffrs.add(fieldFacetMap.get(reqName).get(field));
@@ -123,7 +123,7 @@ public class AnalyticsRequestFactory implements AnalyticsParams {
       }
       ar.setFieldFacets(ffrs);
       
-      List<RangeFacetRequest> rfrs = new ArrayList<RangeFacetRequest>();
+      List<RangeFacetRequest> rfrs = new ArrayList<>();
       if (rangeFacetSet.get(reqName)!=null) {
         for (String field : rangeFacetSet.get(reqName)) {
           RangeFacetRequest rfr = rangeFacetMap.get(reqName).get(field);
@@ -134,7 +134,7 @@ public class AnalyticsRequestFactory implements AnalyticsParams {
       }
       ar.setRangeFacets(rfrs);
       
-      List<QueryFacetRequest> qfrs = new ArrayList<QueryFacetRequest>();
+      List<QueryFacetRequest> qfrs = new ArrayList<>();
       if (queryFacetSet.get(reqName)!=null) {
         for (String name : queryFacetSet.get(reqName)) {
           QueryFacetRequest qfr = queryFacetMap.get(reqName).get(name);
@@ -157,12 +157,12 @@ public class AnalyticsRequestFactory implements AnalyticsParams {
   private static void makeFieldFacet(IndexSchema schema, Map<String, Map<String, FieldFacetRequest>> fieldFacetMap, Map<String, Set<String>> fieldFacetSet, String requestName, String[] fields) {
     Map<String, FieldFacetRequest> facetMap = fieldFacetMap.get(requestName);
     if (facetMap == null) {
-      facetMap = new HashMap<String, FieldFacetRequest>();
+      facetMap = new HashMap<>();
       fieldFacetMap.put(requestName, facetMap);
     }
     Set<String> set = fieldFacetSet.get(requestName);
     if (set == null) {
-      set = new HashSet<String>();
+      set = new HashSet<>();
       fieldFacetSet.put(requestName, set);
     }
     for (String field : fields) {
@@ -176,7 +176,7 @@ public class AnalyticsRequestFactory implements AnalyticsParams {
   private static void setFieldFacetParam(IndexSchema schema, Map<String, Map<String, FieldFacetRequest>> fieldFacetMap, String requestName, String field, String paramType, String[] params) {
     Map<String, FieldFacetRequest> facetMap = fieldFacetMap.get(requestName);
     if (facetMap == null) {
-      facetMap = new HashMap<String, FieldFacetRequest>();
+      facetMap = new HashMap<>();
       fieldFacetMap.put(requestName, facetMap);
     }
     FieldFacetRequest fr = facetMap.get(field);
@@ -202,7 +202,7 @@ public class AnalyticsRequestFactory implements AnalyticsParams {
   private static void makeRangeFacet(IndexSchema schema, Map<String, Set<String>> rangeFacetSet, String requestName, String[] fields) {
     Set<String> set = rangeFacetSet.get(requestName);
     if (set == null) {
-      set = new HashSet<String>();
+      set = new HashSet<>();
       rangeFacetSet.put(requestName, set);
     }
     for (String field : fields) {
@@ -213,7 +213,7 @@ public class AnalyticsRequestFactory implements AnalyticsParams {
   private static void setRangeFacetParam(IndexSchema schema, Map<String, Map<String, RangeFacetRequest>> rangeFacetMap, String requestName, String field, String paramType, String[] params) {
     Map<String, RangeFacetRequest> facetMap = rangeFacetMap.get(requestName);
     if (facetMap == null) {
-      facetMap = new HashMap<String, RangeFacetRequest>();
+      facetMap = new HashMap<>();
       rangeFacetMap.put(requestName, facetMap);
     }
     RangeFacetRequest rr = facetMap.get(field);
@@ -243,7 +243,7 @@ public class AnalyticsRequestFactory implements AnalyticsParams {
   private static void makeQueryFacet(IndexSchema schema,Map<String, Set<String>> queryFacetSet, String requestName, String[] names) {
     Set<String> set = queryFacetSet.get(requestName);
     if (set == null) {
-      set = new HashSet<String>();
+      set = new HashSet<>();
       queryFacetSet.put(requestName, set);
     }
     for (String name : names) {
@@ -254,7 +254,7 @@ public class AnalyticsRequestFactory implements AnalyticsParams {
   private static void setQueryFacetParam(IndexSchema schema, Map<String, Map<String, QueryFacetRequest>> queryFacetMap, String requestName, String name, String paramType, String[] params) {
     Map<String, QueryFacetRequest> facetMap = queryFacetMap.get(requestName);
     if (facetMap == null) {
-      facetMap = new HashMap<String, QueryFacetRequest>();
+      facetMap = new HashMap<>();
       queryFacetMap.put(requestName, facetMap);
     }
     QueryFacetRequest qr = facetMap.get(name);
diff --git solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats.java solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats.java
index a740fad..e019569 100644
--- solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats.java
+++ solr/core/src/java/org/apache/solr/analytics/request/AnalyticsStats.java
@@ -60,7 +60,7 @@ public class AnalyticsStats {
    */
   public NamedList<?> execute() throws IOException {
     statsCollector.startRequest();
-    NamedList<Object> res = new NamedList<Object>();
+    NamedList<Object> res = new NamedList<>();
     List<AnalyticsRequest> requests;
     
     requests = AnalyticsRequestFactory.parse(searcher.getSchema(), params);
diff --git solr/core/src/java/org/apache/solr/analytics/request/QueryFacetRequest.java solr/core/src/java/org/apache/solr/analytics/request/QueryFacetRequest.java
index 6d36d58..b02740c 100644
--- solr/core/src/java/org/apache/solr/analytics/request/QueryFacetRequest.java
+++ solr/core/src/java/org/apache/solr/analytics/request/QueryFacetRequest.java
@@ -31,13 +31,13 @@ public class QueryFacetRequest implements FacetRequest {
   private Set<String> dependencies;
   
   public QueryFacetRequest() {
-    dependencies = new HashSet<String>();
+    dependencies = new HashSet<>();
   }
 
   public QueryFacetRequest(String name) {
     this.name = name;
-    this.queries = new ArrayList<String>();
-    dependencies = new HashSet<String>();
+    this.queries = new ArrayList<>();
+    dependencies = new HashSet<>();
   }
  
   public List<String> getQueries() {
diff --git solr/core/src/java/org/apache/solr/analytics/statistics/MedianStatsCollector.java solr/core/src/java/org/apache/solr/analytics/statistics/MedianStatsCollector.java
index c8f9ee0..8095536 100644
--- solr/core/src/java/org/apache/solr/analytics/statistics/MedianStatsCollector.java
+++ solr/core/src/java/org/apache/solr/analytics/statistics/MedianStatsCollector.java
@@ -28,7 +28,7 @@ import org.apache.solr.analytics.util.MedianCalculator;
  */
 public class MedianStatsCollector extends AbstractDelegatingStatsCollector{
 
-  private final List<Double> values = new ArrayList<Double>();
+  private final List<Double> values = new ArrayList<>();
   protected double median;
   
   public MedianStatsCollector(StatsCollector delegate) {
diff --git solr/core/src/java/org/apache/solr/analytics/statistics/PercentileStatsCollector.java solr/core/src/java/org/apache/solr/analytics/statistics/PercentileStatsCollector.java
index 88e1c74..2ddfb99 100644
--- solr/core/src/java/org/apache/solr/analytics/statistics/PercentileStatsCollector.java
+++ solr/core/src/java/org/apache/solr/analytics/statistics/PercentileStatsCollector.java
@@ -30,7 +30,7 @@ import com.google.common.collect.Iterables;
  */
 @SuppressWarnings("rawtypes")
 public class PercentileStatsCollector extends AbstractDelegatingStatsCollector{
-  public final List<Comparable> values = new ArrayList<Comparable>();
+  public final List<Comparable> values = new ArrayList<>();
   public static final Pattern PERCENTILE_PATTERN = Pattern.compile("perc(?:entile)?_(\\d+)",Pattern.CASE_INSENSITIVE);
   protected final double[] percentiles;
   protected final String[] percentileNames;
diff --git solr/core/src/java/org/apache/solr/analytics/statistics/StatsCollectorSupplierFactory.java solr/core/src/java/org/apache/solr/analytics/statistics/StatsCollectorSupplierFactory.java
index c4dea1b..7b2d14b 100644
--- solr/core/src/java/org/apache/solr/analytics/statistics/StatsCollectorSupplierFactory.java
+++ solr/core/src/java/org/apache/solr/analytics/statistics/StatsCollectorSupplierFactory.java
@@ -88,9 +88,9 @@ public class StatsCollectorSupplierFactory {
    */
   @SuppressWarnings("unchecked")
   public static Supplier<StatsCollector[]> create(IndexSchema schema, AnalyticsRequest request) {
-    final Map<String, Set<String>> collectorStats =  new HashMap<String, Set<String>>();
-    final Map<String, Set<Integer>> collectorPercs =  new HashMap<String, Set<Integer>>();
-    final Map<String, ValueSource> collectorSources =  new HashMap<String, ValueSource>();
+    final Map<String, Set<String>> collectorStats =  new HashMap<>();
+    final Map<String, Set<Integer>> collectorPercs =  new HashMap<>();
+    final Map<String, ValueSource> collectorSources =  new HashMap<>();
     
     // Iterate through all expression request to make a list of ValueSource strings
     // and statistics that need to be calculated on those ValueSources.
@@ -121,7 +121,7 @@ public class StatsCollectorSupplierFactory {
           source = arguments[1];
           Set<Integer> percs = collectorPercs.get(source);
           if (percs == null) {
-            percs = new HashSet<Integer>();
+            percs = new HashSet<>();
             collectorPercs.put(source, percs);
           }
           try {
@@ -143,7 +143,7 @@ public class StatsCollectorSupplierFactory {
         // each ValueSource, even across different expression requests
         Set<String> stats = collectorStats.get(source);
         if (stats == null) {
-          stats = new HashSet<String>();
+          stats = new HashSet<>();
           collectorStats.put(source, stats);
         }
         stats.add(stat);
@@ -244,7 +244,7 @@ public class StatsCollectorSupplierFactory {
    * @return The set of statistics (sum, mean, median, etc.) found in the expression
    */
   public static Set<String> getStatistics(String expression) {
-    HashSet<String> set = new HashSet<String>();
+    HashSet<String> set = new HashSet<>();
     int firstParen = expression.indexOf('(');
     if (firstParen>0) {
       String topOperation = expression.substring(0,firstParen).trim();
@@ -511,7 +511,7 @@ public class StatsCollectorSupplierFactory {
     } else if (operation.equals(AnalyticsParams.FILTER)) {
       return buildFilterSource(schema, operands, NUMBER_TYPE);
     }
-    List<ValueSource> subExpressions = new ArrayList<ValueSource>();
+    List<ValueSource> subExpressions = new ArrayList<>();
     for (String argument : arguments) {
       ValueSource argSource = buildNumericSource(schema, argument);
       if (argSource == null) {
@@ -577,7 +577,7 @@ public class StatsCollectorSupplierFactory {
       return buildFilterSource(schema, operands, DATE_TYPE);
     }
     if (operation.equals(AnalyticsParams.DATE_MATH)) {
-      List<ValueSource> subExpressions = new ArrayList<ValueSource>();
+      List<ValueSource> subExpressions = new ArrayList<>();
       boolean first = true;
       for (String argument : arguments) {
         ValueSource argSource;
@@ -632,7 +632,7 @@ public class StatsCollectorSupplierFactory {
       }
       return new ReverseStringFunction(buildStringSource(schema, operands));
     }
-    List<ValueSource> subExpressions = new ArrayList<ValueSource>();
+    List<ValueSource> subExpressions = new ArrayList<>();
     for (String argument : arguments) {
       subExpressions.add(buildSourceTree(schema, argument));
     }
diff --git solr/core/src/java/org/apache/solr/analytics/statistics/UniqueStatsCollector.java solr/core/src/java/org/apache/solr/analytics/statistics/UniqueStatsCollector.java
index ca8d2ab..a06a093 100644
--- solr/core/src/java/org/apache/solr/analytics/statistics/UniqueStatsCollector.java
+++ solr/core/src/java/org/apache/solr/analytics/statistics/UniqueStatsCollector.java
@@ -24,7 +24,7 @@ import java.util.Set;
  * <code>UniqueValueCounter</code> computes the number of unique values.
  */
 public class UniqueStatsCollector extends AbstractDelegatingStatsCollector{
-  private final Set<Object> uniqueValues = new HashSet<Object>();
+  private final Set<Object> uniqueValues = new HashSet<>();
   
   public UniqueStatsCollector(StatsCollector delegate) {
     super(delegate);
diff --git solr/core/src/java/org/apache/solr/analytics/util/PercentileCalculator.java solr/core/src/java/org/apache/solr/analytics/util/PercentileCalculator.java
index a98ed0c..714575e 100644
--- solr/core/src/java/org/apache/solr/analytics/util/PercentileCalculator.java
+++ solr/core/src/java/org/apache/solr/analytics/util/PercentileCalculator.java
@@ -46,7 +46,7 @@ public class PercentileCalculator {
       throw new IllegalArgumentException();
     }
 
-    List<T> results = new ArrayList<T>(percs.length);
+    List<T> results = new ArrayList<>(percs.length);
 
     distributeAndFind(list, percentiles, 0, percentiles.length - 1);
 
diff --git solr/core/src/java/org/apache/solr/analytics/util/RangeEndpointCalculator.java solr/core/src/java/org/apache/solr/analytics/util/RangeEndpointCalculator.java
index 50e45c0..83c9b7c 100644
--- solr/core/src/java/org/apache/solr/analytics/util/RangeEndpointCalculator.java
+++ solr/core/src/java/org/apache/solr/analytics/util/RangeEndpointCalculator.java
@@ -151,7 +151,7 @@ public abstract class RangeEndpointCalculator<T extends Comparable<T>> {
         
     T low = start;
     
-    List<FacetRange> ranges = new ArrayList<FacetRange>();
+    List<FacetRange> ranges = new ArrayList<>();
     
     int gapCounter = 0;
     
diff --git solr/core/src/java/org/apache/solr/client/solrj/embedded/JettySolrRunner.java solr/core/src/java/org/apache/solr/client/solrj/embedded/JettySolrRunner.java
index d2fffb6..bc298e9 100644
--- solr/core/src/java/org/apache/solr/client/solrj/embedded/JettySolrRunner.java
+++ solr/core/src/java/org/apache/solr/client/solrj/embedded/JettySolrRunner.java
@@ -90,7 +90,7 @@ public class JettySolrRunner {
   private String coreNodeName;
 
   /** Maps servlet holders (i.e. factories: class + init params) to path specs */
-  private SortedMap<ServletHolder,String> extraServlets = new TreeMap<ServletHolder,String>();
+  private SortedMap<ServletHolder,String> extraServlets = new TreeMap<>();
   private SortedMap<Class,String> extraRequestFilters;
   private LinkedList<FilterHolder> extraFilters;
 
@@ -106,7 +106,7 @@ public class JettySolrRunner {
     }
 
     // TODO: keep track of certain number of last requests
-    private LinkedList<HttpServletRequest> requests = new LinkedList<HttpServletRequest>();
+    private LinkedList<HttpServletRequest> requests = new LinkedList<>();
 
 
     @Override
@@ -188,7 +188,7 @@ public class JettySolrRunner {
       SortedMap<Class,String> extraRequestFilters) {
     if (null != extraServlets) { this.extraServlets.putAll(extraServlets); }
     if (null != extraRequestFilters) {
-      this.extraRequestFilters = new TreeMap<Class,String>(extraRequestFilters.comparator());
+      this.extraRequestFilters = new TreeMap<>(extraRequestFilters.comparator());
       this.extraRequestFilters.putAll(extraRequestFilters);
     }
     this.solrConfigFilename = solrConfigFilename;
@@ -316,7 +316,7 @@ public class JettySolrRunner {
 //        FilterHolder fh = new FilterHolder(filter);
         debugFilter = root.addFilter(DebugFilter.class, "*", EnumSet.of(DispatcherType.REQUEST) );
         if (extraRequestFilters != null) {
-          extraFilters = new LinkedList<FilterHolder>();
+          extraFilters = new LinkedList<>();
           for (Class filterClass : extraRequestFilters.keySet()) {
             extraFilters.add(root.addFilter(filterClass, extraRequestFilters.get(filterClass),
               EnumSet.of(DispatcherType.REQUEST)));
diff --git solr/core/src/java/org/apache/solr/cloud/Assign.java solr/core/src/java/org/apache/solr/cloud/Assign.java
index 62f62fc..e231e04 100644
--- solr/core/src/java/org/apache/solr/cloud/Assign.java
+++ solr/core/src/java/org/apache/solr/cloud/Assign.java
@@ -86,7 +86,7 @@ public class Assign {
       return "shard1";
     }
 
-    List<String> shardIdNames = new ArrayList<String>(sliceMap.keySet());
+    List<String> shardIdNames = new ArrayList<>(sliceMap.keySet());
 
     if (shardIdNames.size() < numShards) {
       return "shard" + (shardIdNames.size() + 1);
@@ -95,7 +95,7 @@ public class Assign {
     // TODO: don't need to sort to find shard with fewest replicas!
 
     // else figure out which shard needs more replicas
-    final Map<String, Integer> map = new HashMap<String, Integer>();
+    final Map<String, Integer> map = new HashMap<>();
     for (String shardId : shardIdNames) {
       int cnt = sliceMap.get(shardId).getReplicasMap().size();
       map.put(shardId, cnt);
@@ -135,12 +135,12 @@ public class Assign {
 
     Set<String> nodes = clusterState.getLiveNodes();
 
-    List<String> nodeList = new ArrayList<String>(nodes.size());
+    List<String> nodeList = new ArrayList<>(nodes.size());
     nodeList.addAll(nodes);
     if (createNodeList != null) nodeList.retainAll(createNodeList);
 
 
-    HashMap<String,Node> nodeNameVsShardCount =  new HashMap<String, Node>();
+    HashMap<String,Node> nodeNameVsShardCount =  new HashMap<>();
     for (String s : nodeList) nodeNameVsShardCount.put(s,new Node(s));
     for (String s : clusterState.getCollections()) {
       DocCollection c = clusterState.getCollection(s);
diff --git solr/core/src/java/org/apache/solr/cloud/DistributedQueue.java solr/core/src/java/org/apache/solr/cloud/DistributedQueue.java
index f763b02..d6fc0aa 100644
--- solr/core/src/java/org/apache/solr/cloud/DistributedQueue.java
+++ solr/core/src/java/org/apache/solr/cloud/DistributedQueue.java
@@ -82,7 +82,7 @@ public class DistributedQueue {
    */
   private TreeMap<Long,String> orderedChildren(Watcher watcher)
       throws KeeperException, InterruptedException {
-    TreeMap<Long,String> orderedChildren = new TreeMap<Long,String>();
+    TreeMap<Long,String> orderedChildren = new TreeMap<>();
     
     List<String> childNames = null;
     try {
diff --git solr/core/src/java/org/apache/solr/cloud/LeaderElector.java solr/core/src/java/org/apache/solr/cloud/LeaderElector.java
index 488ae8f..d1f5f96 100644
--- solr/core/src/java/org/apache/solr/cloud/LeaderElector.java
+++ solr/core/src/java/org/apache/solr/cloud/LeaderElector.java
@@ -212,7 +212,7 @@ public  class LeaderElector {
    * @return int seqs
    */
   private List<Integer> getSeqs(List<String> seqs) {
-    List<Integer> intSeqs = new ArrayList<Integer>(seqs.size());
+    List<Integer> intSeqs = new ArrayList<>(seqs.size());
     for (String seq : seqs) {
       intSeqs.add(getSeq(seq));
     }
diff --git solr/core/src/java/org/apache/solr/cloud/Overseer.java solr/core/src/java/org/apache/solr/cloud/Overseer.java
index 0704ccc..ffc182d 100644
--- solr/core/src/java/org/apache/solr/cloud/Overseer.java
+++ solr/core/src/java/org/apache/solr/cloud/Overseer.java
@@ -337,7 +337,7 @@ public class Overseer {
         return clusterState;
       }
 
-      ArrayList<String> shardNames = new ArrayList<String>();
+      ArrayList<String> shardNames = new ArrayList<>();
 
       if(ImplicitDocRouter.NAME.equals( message.getStr("router.name",DocRouter.DEFAULT_NAME))){
         getShardNames(shardNames,message.getStr("shards",DocRouter.DEFAULT_NAME));
@@ -392,10 +392,10 @@ public class Overseer {
 
       Map<String, RoutingRule> routingRules = slice.getRoutingRules();
       if (routingRules == null)
-        routingRules = new HashMap<String, RoutingRule>();
+        routingRules = new HashMap<>();
       RoutingRule r = routingRules.get(routeKey);
       if (r == null) {
-        Map<String, Object> map = new HashMap<String, Object>();
+        Map<String, Object> map = new HashMap<>();
         map.put("routeRanges", range);
         map.put("targetCollection", targetCollection);
         map.put("expireAt", expireAt);
@@ -463,7 +463,7 @@ public class Overseer {
       Slice slice = clusterState.getSlice(collection, shardId);
       if (slice == null)  {
         Map<String, Replica> replicas = Collections.EMPTY_MAP;
-        Map<String, Object> sliceProps = new HashMap<String, Object>();
+        Map<String, Object> sliceProps = new HashMap<>();
         String shardRange = message.getStr(ZkStateReader.SHARD_RANGE_PROP);
         String shardState = message.getStr(ZkStateReader.SHARD_STATE_PROP);
         String shardParent = message.getStr(ZkStateReader.SHARD_PARENT_PROP);
@@ -530,7 +530,7 @@ public class Overseer {
         Integer numShards = message.getInt(ZkStateReader.NUM_SHARDS_PROP, null);
         log.info("Update state numShards={} message={}", numShards, message);
 
-        List<String> shardNames  = new ArrayList<String>();
+        List<String> shardNames  = new ArrayList<>();
 
         //collection does not yet exist, create placeholders if num shards is specified
         boolean collectionExists = clusterState.hasCollection(collection);
@@ -574,7 +574,7 @@ public class Overseer {
 
         Slice slice = clusterState.getSlice(collection, sliceName);
         
-        Map<String,Object> replicaProps = new LinkedHashMap<String,Object>();
+        Map<String,Object> replicaProps = new LinkedHashMap<>();
 
         replicaProps.putAll(message.getProperties());
         // System.out.println("########## UPDATE MESSAGE: " + JSONUtil.toJSON(message));
@@ -594,7 +594,7 @@ public class Overseer {
           
           // remove any props with null values
           Set<Entry<String,Object>> entrySet = replicaProps.entrySet();
-          List<String> removeKeys = new ArrayList<String>();
+          List<String> removeKeys = new ArrayList<>();
           for (Entry<String,Object> entry : entrySet) {
             if (entry.getValue() == null) {
               removeKeys.add(entry.getKey());
@@ -624,8 +624,8 @@ public class Overseer {
             sliceProps = slice.getProperties();
             replicas = slice.getReplicasCopy();
           } else {
-            replicas = new HashMap<String, Replica>(1);
-            sliceProps = new HashMap<String, Object>();
+            replicas = new HashMap<>(1);
+            sliceProps = new HashMap<>();
             sliceProps.put(Slice.RANGE, shardRange);
             sliceProps.put(Slice.STATE, shardState);
             sliceProps.put(Slice.PARENT, shardParent);
@@ -661,8 +661,8 @@ public class Overseer {
           if (allActive)  {
             log.info("Shard: {} - all replicas are active. Finding status of fellow sub-shards", sliceName);
             // find out about other sub shards
-            Map<String, Slice> allSlicesCopy = new HashMap<String, Slice>(state.getSlicesMap(collection));
-            List<Slice> subShardSlices = new ArrayList<Slice>();
+            Map<String, Slice> allSlicesCopy = new HashMap<>(state.getSlicesMap(collection));
+            List<Slice> subShardSlices = new ArrayList<>();
             outer:
             for (Entry<String, Slice> entry : allSlicesCopy.entrySet()) {
               if (sliceName.equals(entry.getKey()))
@@ -688,7 +688,7 @@ public class Overseer {
               log.info("Shard: {} - All replicas across all fellow sub-shards are now ACTIVE. Preparing to switch shard states.", sliceName);
               String parentSliceName = (String) sliceProps.remove(Slice.PARENT);
 
-              Map<String, Object> propMap = new HashMap<String, Object>();
+              Map<String, Object> propMap = new HashMap<>();
               propMap.put(Overseer.QUEUE_OPERATION, "updateshardstate");
               propMap.put(parentSliceName, Slice.INACTIVE);
               propMap.put(sliceName, Slice.ACTIVE);
@@ -717,7 +717,7 @@ public class Overseer {
 //        Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>();
 
 
-        Map<String, Slice> newSlices = new LinkedHashMap<String,Slice>();
+        Map<String, Slice> newSlices = new LinkedHashMap<>();
 //        newCollections.putAll(state.getCollectionStates());
         for (int i = 0; i < shards.size(); i++) {
           String sliceName = shards.get(i);
@@ -725,14 +725,14 @@ public class Overseer {
         for (int i = 0; i < numShards; i++) {
           final String sliceName = "shard" + (i+1);*/
 
-          Map<String, Object> sliceProps = new LinkedHashMap<String, Object>(1);
+          Map<String, Object> sliceProps = new LinkedHashMap<>(1);
           sliceProps.put(Slice.RANGE, ranges == null? null: ranges.get(i));
 
           newSlices.put(sliceName, new Slice(sliceName, null, sliceProps));
         }
 
         // TODO: fill in with collection properties read from the /collections/<collectionName> node
-        Map<String,Object> collectionProps = new HashMap<String,Object>();
+        Map<String,Object> collectionProps = new HashMap<>();
 
         for (Entry<String, Object> e : OverseerCollectionProcessor.COLL_PROPS.entrySet()) {
           Object val = message.get(e.getKey());
@@ -791,7 +791,7 @@ public class Overseer {
       private ClusterState updateSlice(ClusterState state, String collectionName, Slice slice) {
         // System.out.println("###!!!### OLD CLUSTERSTATE: " + JSONUtil.toJSON(state.getCollectionStates()));
         // System.out.println("Updating slice:" + slice);
-        Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(state.getCollectionStates());  // make a shallow copy
+        Map<String, DocCollection> newCollections = new LinkedHashMap<>(state.getCollectionStates());  // make a shallow copy
         DocCollection coll = newCollections.get(collectionName);
         Map<String,Slice> slices;
         Map<String,Object> props;
@@ -800,14 +800,14 @@ public class Overseer {
         if (coll == null) {
           //  when updateSlice is called on a collection that doesn't exist, it's currently when a core is publishing itself
           // without explicitly creating a collection.  In this current case, we assume custom sharding with an "implicit" router.
-          slices = new HashMap<String, Slice>(1);
-          props = new HashMap<String,Object>(1);
+          slices = new HashMap<>(1);
+          props = new HashMap<>(1);
           props.put(DocCollection.DOC_ROUTER, ZkNodeProps.makeMap("name",ImplicitDocRouter.NAME));
           router = new ImplicitDocRouter();
         } else {
           props = coll.getProperties();
           router = coll.getRouter();
-          slices = new LinkedHashMap<String, Slice>(coll.getSlicesMap()); // make a shallow copy
+          slices = new LinkedHashMap<>(coll.getSlicesMap()); // make a shallow copy
         }
         slices.put(slice.getName(), slice);
         DocCollection newCollection = new DocCollection(collectionName, slices, props, router);
@@ -820,7 +820,7 @@ public class Overseer {
       
       private ClusterState setShardLeader(ClusterState state, String collectionName, String sliceName, String leaderUrl) {
 
-        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(state.getCollectionStates());
+        final Map<String, DocCollection> newCollections = new LinkedHashMap<>(state.getCollectionStates());
         DocCollection coll = newCollections.get(collectionName);
         if(coll == null) {
           log.error("Could not mark shard leader for non existing collection:" + collectionName);
@@ -829,7 +829,7 @@ public class Overseer {
 
         Map<String, Slice> slices = coll.getSlicesMap();
         // make a shallow copy and add it to the new collection
-        slices = new LinkedHashMap<String,Slice>(slices);
+        slices = new LinkedHashMap<>(slices);
 
         Slice slice = slices.get(sliceName);
         if (slice == null) {
@@ -844,7 +844,7 @@ public class Overseer {
 
           Replica oldLeader = slice.getLeader();
 
-          final Map<String,Replica> newReplicas = new LinkedHashMap<String,Replica>();
+          final Map<String,Replica> newReplicas = new LinkedHashMap<>();
 
           for (Replica replica : slice.getReplicas()) {
 
@@ -852,11 +852,11 @@ public class Overseer {
             String coreURL = ZkCoreNodeProps.getCoreUrl(replica.getStr(ZkStateReader.BASE_URL_PROP), replica.getStr(ZkStateReader.CORE_NAME_PROP));
 
             if (replica == oldLeader && !coreURL.equals(leaderUrl)) {
-              Map<String,Object> replicaProps = new LinkedHashMap<String,Object>(replica.getProperties());
+              Map<String,Object> replicaProps = new LinkedHashMap<>(replica.getProperties());
               replicaProps.remove(Slice.LEADER);
               replica = new Replica(replica.getName(), replicaProps);
             } else if (coreURL.equals(leaderUrl)) {
-              Map<String,Object> replicaProps = new LinkedHashMap<String,Object>(replica.getProperties());
+              Map<String,Object> replicaProps = new LinkedHashMap<>(replica.getProperties());
               replicaProps.put(Slice.LEADER, "true");  // TODO: allow booleans instead of strings
               replica = new Replica(replica.getName(), replicaProps);
             }
@@ -901,7 +901,7 @@ public class Overseer {
 
       DocCollection coll = clusterState.getCollection(collection);
 
-      Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>(coll.getSlicesMap());
+      Map<String, Slice> newSlices = new LinkedHashMap<>(coll.getSlicesMap());
       newSlices.remove(sliceId);
 
       DocCollection newCollection = new DocCollection(coll.getName(), newSlices, coll.getProperties(), coll.getRouter());
@@ -916,7 +916,7 @@ public class Overseer {
         final String collection = message.getStr(ZkStateReader.COLLECTION_PROP);
         if (!checkCollectionKeyExistence(message)) return clusterState;
 
-//        final Map<String, DocCollection> newCollections = new LinkedHashMap<String,DocCollection>(clusterState.getCollectionStates()); // shallow copy
+//        final Map<String, DocCollection> newCollections = new LinkedHashMap<>(clusterState.getCollectionStates()); // shallow copy
 //        DocCollection coll = newCollections.get(collection);
         DocCollection coll = clusterState.getCollectionOrNull(collection) ;
         if (coll == null) {
@@ -933,7 +933,7 @@ public class Overseer {
           return clusterState;
         }
 
-        Map<String, Slice> newSlices = new LinkedHashMap<String, Slice>();
+        Map<String, Slice> newSlices = new LinkedHashMap<>();
         boolean lastSlice = false;
         for (Slice slice : coll.getSlices()) {
           Replica replica = slice.getReplica(cnn);
diff --git solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java
index bee9e18..b36f47f 100644
--- solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java
+++ solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java
@@ -245,7 +245,7 @@ public class OverseerCollectionProcessor implements Runnable, ClosableThread {
 //
     ArrayList<String> nodesTobePushedBack =  new ArrayList<>();
     //ensure that the node right behind the leader , i.r at position 1 is a Overseer
-    List<String> availableDesignates = new ArrayList<String>();
+    List<String> availableDesignates = new ArrayList<>();
 
     log.info("sorted nodes {}", nodeNames);//TODO to be removed
     for (int i = 0; i < nodeNames.size(); i++) {
@@ -322,7 +322,7 @@ public class OverseerCollectionProcessor implements Runnable, ClosableThread {
       children = zk.getChildren(OverseerElectionContext.PATH + LeaderElector.ELECTION_NODE, null, true);
     } catch (Exception e) {
       log.warn("error ", e);
-      return new ArrayList<String>();
+      return new ArrayList<>();
     }
     LeaderElector.sortSeqs(children);
     ArrayList<String> nodeNames = new ArrayList<>(children.size());
@@ -487,7 +487,7 @@ public class OverseerCollectionProcessor implements Runnable, ClosableThread {
     }
     Replica replica = slice.getReplica(replicaName);
     if(replica == null){
-      ArrayList<String> l = new ArrayList<String>();
+      ArrayList<String> l = new ArrayList<>();
       for (Replica r : slice.getReplicas()) l.add(r.getName());
       throw new SolrException(ErrorCode.BAD_REQUEST, "Invalid replica : " + replicaName + " in shard/collection : "
           + shard + "/"+ collectionName + " available replicas are "+ StrUtils.join(l,','));
@@ -607,8 +607,8 @@ public class OverseerCollectionProcessor implements Runnable, ClosableThread {
     String aliasName = message.getStr("name");
     String collections = message.getStr("collections");
     
-    Map<String,Map<String,String>> newAliasesMap = new HashMap<String,Map<String,String>>();
-    Map<String,String> newCollectionAliasesMap = new HashMap<String,String>();
+    Map<String,Map<String,String>> newAliasesMap = new HashMap<>();
+    Map<String,String> newCollectionAliasesMap = new HashMap<>();
     Map<String,String> prevColAliases = aliases.getCollectionAliasMap();
     if (prevColAliases != null) {
       newCollectionAliasesMap.putAll(prevColAliases);
@@ -678,8 +678,8 @@ public class OverseerCollectionProcessor implements Runnable, ClosableThread {
   private void deleteAlias(Aliases aliases, ZkNodeProps message) {
     String aliasName = message.getStr("name");
 
-    Map<String,Map<String,String>> newAliasesMap = new HashMap<String,Map<String,String>>();
-    Map<String,String> newCollectionAliasesMap = new HashMap<String,String>();
+    Map<String,Map<String,String>> newAliasesMap = new HashMap<>();
+    Map<String,String> newCollectionAliasesMap = new HashMap<>();
     newCollectionAliasesMap.putAll(aliases.getCollectionAliasMap());
     newCollectionAliasesMap.remove(aliasName);
     newAliasesMap.put("collection", newCollectionAliasesMap);
@@ -839,7 +839,7 @@ public class OverseerCollectionProcessor implements Runnable, ClosableThread {
       if (ranges.length == 0 || ranges.length == 1) {
         throw new SolrException(ErrorCode.BAD_REQUEST, "There must be at least two ranges specified to split a shard");
       } else  {
-        subRanges = new ArrayList<DocRouter.Range>(ranges.length);
+        subRanges = new ArrayList<>(ranges.length);
         for (int i = 0; i < ranges.length; i++) {
           String r = ranges[i];
           try {
@@ -852,7 +852,7 @@ public class OverseerCollectionProcessor implements Runnable, ClosableThread {
                 "Specified hash range: " + r + " is not a subset of parent shard's range: " + range.toString());
           }
         }
-        List<DocRouter.Range> temp = new ArrayList<DocRouter.Range>(subRanges); // copy to preserve original order
+        List<DocRouter.Range> temp = new ArrayList<>(subRanges); // copy to preserve original order
         Collections.sort(temp);
         if (!range.equals(new DocRouter.Range(temp.get(0).min, temp.get(temp.size() - 1).max)))  {
           throw new SolrException(ErrorCode.BAD_REQUEST,
@@ -894,8 +894,8 @@ public class OverseerCollectionProcessor implements Runnable, ClosableThread {
     }
 
     try {
-      List<String> subSlices = new ArrayList<String>(subRanges.size());
-      List<String> subShardNames = new ArrayList<String>(subRanges.size());
+      List<String> subSlices = new ArrayList<>(subRanges.size());
+      List<String> subShardNames = new ArrayList<>(subRanges.size());
       String nodeName = parentShardLeader.getNodeName();
       for (int i = 0; i < subRanges.size(); i++) {
         String subSlice = slice + "_" + i;
@@ -911,7 +911,7 @@ public class OverseerCollectionProcessor implements Runnable, ClosableThread {
             // delete the shards
             for (String sub : subSlices) {
               log.info("Sub-shard: {} already exists therefore requesting its deletion", sub);
-              Map<String, Object> propMap = new HashMap<String, Object>();
+              Map<String, Object> propMap = new HashMap<>();
               propMap.put(Overseer.QUEUE_OPERATION, "deleteshard");
               propMap.put(COLLECTION_PROP, collectionName);
               propMap.put(SHARD_ID_PROP, sub);
@@ -940,7 +940,7 @@ public class OverseerCollectionProcessor implements Runnable, ClosableThread {
             + subSlice + " of collection " + collectionName + " on "
             + nodeName);
 
-        Map<String, Object> propMap = new HashMap<String, Object>();
+        Map<String, Object> propMap = new HashMap<>();
         propMap.put(Overseer.QUEUE_OPERATION, "createshard");
         propMap.put(ZkStateReader.SHARD_ID_PROP, subSlice);
         propMap.put(ZkStateReader.COLLECTION_PROP, collectionName);
@@ -1037,7 +1037,7 @@ public class OverseerCollectionProcessor implements Runnable, ClosableThread {
       // node?
       // for now we just go random
       Set<String> nodes = clusterState.getLiveNodes();
-      List<String> nodeList = new ArrayList<String>(nodes.size());
+      List<String> nodeList = new ArrayList<>(nodes.size());
       nodeList.addAll(nodes);
       
       Collections.shuffle(nodeList);
@@ -1101,7 +1101,7 @@ public class OverseerCollectionProcessor implements Runnable, ClosableThread {
         // switch sub shard states to 'active'
         log.info("Replication factor is 1 so switching shard states");
         DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());
-        Map<String, Object> propMap = new HashMap<String, Object>();
+        Map<String, Object> propMap = new HashMap<>();
         propMap.put(Overseer.QUEUE_OPERATION, "updateshardstate");
         propMap.put(slice, Slice.INACTIVE);
         for (String subSlice : subSlices) {
@@ -1113,7 +1113,7 @@ public class OverseerCollectionProcessor implements Runnable, ClosableThread {
       } else  {
         log.info("Requesting shard state be set to 'recovery'");
         DistributedQueue inQueue = Overseer.getInQueue(zkStateReader.getZkClient());
-        Map<String, Object> propMap = new HashMap<String, Object>();
+        Map<String, Object> propMap = new HashMap<>();
         propMap.put(Overseer.QUEUE_OPERATION, "updateshardstate");
         for (String subSlice : subSlices) {
           propMap.put(subSlice, Slice.RECOVERY);
@@ -1576,7 +1576,7 @@ public class OverseerCollectionProcessor implements Runnable, ClosableThread {
       // node?
       // for now we just go random
       Set<String> nodes = clusterState.getLiveNodes();
-      List<String> nodeList = new ArrayList<String>(nodes.size());
+      List<String> nodeList = new ArrayList<>(nodes.size());
       nodeList.addAll(nodes);
       if (createNodeList != null) nodeList.retainAll(createNodeList);
       Collections.shuffle(nodeList);
@@ -1628,7 +1628,7 @@ public class OverseerCollectionProcessor implements Runnable, ClosableThread {
         throw new SolrException(ErrorCode.SERVER_ERROR, "Could not fully createcollection: " + message.getStr("name"));
 
       log.info("Creating SolrCores for new collection, shardNames {} , replicationFactor : {}", shardNames, repFactor);
-      Map<String ,ShardRequest> coresToCreate = new LinkedHashMap<String, ShardRequest>();
+      Map<String ,ShardRequest> coresToCreate = new LinkedHashMap<>();
       for (int i = 1; i <= shardNames.size(); i++) {
         String sliceName = shardNames.get(i-1);
         for (int j = 1; j <= repFactor; j++) {
@@ -1708,7 +1708,7 @@ public class OverseerCollectionProcessor implements Runnable, ClosableThread {
   }
 
   private Map<String, Replica> waitToSeeReplicasInState(String collectionName, Collection<String> coreNames) throws InterruptedException {
-    Map<String, Replica> result = new HashMap<String, Replica>();
+    Map<String, Replica> result = new HashMap<>();
     long endTime = System.nanoTime() + TimeUnit.NANOSECONDS.convert(30, TimeUnit.SECONDS);
     while (true) {
       DocCollection coll = zkStateReader.getClusterState().getCollection(
diff --git solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy.java solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy.java
index 805fb75..cbff15e 100644
--- solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy.java
+++ solr/core/src/java/org/apache/solr/cloud/RecoveryStrategy.java
@@ -284,7 +284,7 @@ public class RecoveryStrategy extends Thread implements ClosableThread {
       recentVersions = recentUpdates.getVersions(ulog.numRecordsToKeep);
     } catch (Exception e) {
       SolrException.log(log, "Corrupt tlog - ignoring. core=" + coreName, e);
-      recentVersions = new ArrayList<Long>(0);
+      recentVersions = new ArrayList<>(0);
     } finally {
       if (recentUpdates != null) {
         recentUpdates.close();
@@ -313,7 +313,7 @@ public class RecoveryStrategy extends Thread implements ClosableThread {
         log.info("###### startupVersions=" + startingVersions);
       } catch (Exception e) {
         SolrException.log(log, "Error getting recent versions. core=" + coreName, e);
-        recentVersions = new ArrayList<Long>(0);
+        recentVersions = new ArrayList<>(0);
       }
     }
 
diff --git solr/core/src/java/org/apache/solr/cloud/SyncStrategy.java solr/core/src/java/org/apache/solr/cloud/SyncStrategy.java
index 270bb0a..54ae5f6 100644
--- solr/core/src/java/org/apache/solr/cloud/SyncStrategy.java
+++ solr/core/src/java/org/apache/solr/cloud/SyncStrategy.java
@@ -158,7 +158,7 @@ public class SyncStrategy {
       return true;
     }
     
-    List<String> syncWith = new ArrayList<String>();
+    List<String> syncWith = new ArrayList<>();
     for (ZkCoreNodeProps node : nodes) {
       syncWith.add(node.getCoreUrl());
     }
diff --git solr/core/src/java/org/apache/solr/cloud/ZkController.java solr/core/src/java/org/apache/solr/cloud/ZkController.java
index 185a50c..b0781d3 100644
--- solr/core/src/java/org/apache/solr/cloud/ZkController.java
+++ solr/core/src/java/org/apache/solr/cloud/ZkController.java
@@ -578,7 +578,7 @@ public final class ZkController {
     
     ClusterState clusterState = zkStateReader.getClusterState();
     Set<String> collections = clusterState.getCollections();
-    List<String> updatedNodes = new ArrayList<String>();
+    List<String> updatedNodes = new ArrayList<>();
     for (String collectionName : collections) {
       DocCollection collection = clusterState.getCollection(collectionName);
       Collection<Slice> slices = collection.getSlices();
@@ -755,7 +755,7 @@ public final class ZkController {
     
     String shardId = cloudDesc.getShardId();
 
-    Map<String,Object> props = new HashMap<String,Object>();
+    Map<String,Object> props = new HashMap<>();
  // we only put a subset of props into the leader node
     props.put(ZkStateReader.BASE_URL_PROP, baseUrl);
     props.put(ZkStateReader.CORE_NAME_PROP, coreName);
@@ -948,7 +948,7 @@ public final class ZkController {
     
     String shardId = cd.getCloudDescriptor().getShardId();
     
-    Map<String,Object> props = new HashMap<String,Object>();
+    Map<String,Object> props = new HashMap<>();
     // we only put a subset of props into the leader node
     props.put(ZkStateReader.BASE_URL_PROP, getBaseUrl());
     props.put(ZkStateReader.CORE_NAME_PROP, cd.getName());
@@ -1131,7 +1131,7 @@ public final class ZkController {
        SolrParams params = cd.getParams();
 
         try {
-          Map<String,Object> collectionProps = new HashMap<String,Object>();
+          Map<String,Object> collectionProps = new HashMap<>();
 
           // TODO: if collection.configName isn't set, and there isn't already a conf in zk, just use that?
           String defaultConfigName = System.getProperty(COLLECTION_PARAM_PREFIX+CONFIGNAME_PROP, collection);
@@ -1540,7 +1540,7 @@ public final class ZkController {
     ZkNodeProps props = null;
     if(data != null) {
       props = ZkNodeProps.load(data);
-      Map<String,Object> newProps = new HashMap<String,Object>();
+      Map<String,Object> newProps = new HashMap<>();
       newProps.putAll(props.getProperties());
       newProps.put(CONFIGNAME_PROP, confSetName);
       props = new ZkNodeProps(newProps);
diff --git solr/core/src/java/org/apache/solr/core/CachingDirectoryFactory.java solr/core/src/java/org/apache/solr/core/CachingDirectoryFactory.java
index 5cbcdbe..c77af49 100644
--- solr/core/src/java/org/apache/solr/core/CachingDirectoryFactory.java
+++ solr/core/src/java/org/apache/solr/core/CachingDirectoryFactory.java
@@ -76,8 +76,8 @@ public abstract class CachingDirectoryFactory extends DirectoryFactory {
     public boolean closeCacheValueCalled = false;
     public boolean doneWithDir = false;
     private boolean deleteAfterCoreClose = false;
-    public Set<CacheValue> removeEntries = new HashSet<CacheValue>();
-    public Set<CacheValue> closeEntries = new HashSet<CacheValue>();
+    public Set<CacheValue> removeEntries = new HashSet<>();
+    public Set<CacheValue> closeEntries = new HashSet<>();
 
     public void setDeleteOnClose(boolean deleteOnClose, boolean deleteAfterCoreClose) {
       if (deleteOnClose) {
@@ -96,13 +96,13 @@ public abstract class CachingDirectoryFactory extends DirectoryFactory {
   private static Logger log = LoggerFactory
       .getLogger(CachingDirectoryFactory.class);
   
-  protected Map<String,CacheValue> byPathCache = new HashMap<String,CacheValue>();
+  protected Map<String,CacheValue> byPathCache = new HashMap<>();
   
-  protected Map<Directory,CacheValue> byDirectoryCache = new IdentityHashMap<Directory,CacheValue>();
+  protected Map<Directory,CacheValue> byDirectoryCache = new IdentityHashMap<>();
   
-  protected Map<Directory,List<CloseListener>> closeListeners = new HashMap<Directory,List<CloseListener>>();
+  protected Map<Directory,List<CloseListener>> closeListeners = new HashMap<>();
   
-  protected Set<CacheValue> removeEntries = new HashSet<CacheValue>();
+  protected Set<CacheValue> removeEntries = new HashSet<>();
 
   private Double maxWriteMBPerSecFlush;
 
@@ -129,7 +129,7 @@ public abstract class CachingDirectoryFactory extends DirectoryFactory {
       }
       List<CloseListener> listeners = closeListeners.get(dir);
       if (listeners == null) {
-        listeners = new ArrayList<CloseListener>();
+        listeners = new ArrayList<>();
         closeListeners.put(dir, listeners);
       }
       listeners.add(closeListener);
@@ -192,7 +192,7 @@ public abstract class CachingDirectoryFactory extends DirectoryFactory {
       }
       
       values = byDirectoryCache.values();
-      Set<CacheValue> closedDirs = new HashSet<CacheValue>();
+      Set<CacheValue> closedDirs = new HashSet<>();
       for (CacheValue val : values) {
         try {
           for (CacheValue v : val.closeEntries) {
@@ -248,7 +248,7 @@ public abstract class CachingDirectoryFactory extends DirectoryFactory {
       // see if we are a subpath
       Collection<CacheValue> values = byPathCache.values();
       
-      Collection<CacheValue> cacheValues = new ArrayList<CacheValue>(values);
+      Collection<CacheValue> cacheValues = new ArrayList<>(values);
       cacheValues.remove(cacheValue);
       for (CacheValue otherCacheValue : cacheValues) {
         // if we are a parent path and a sub path is not already closed, get a sub path to close us later
@@ -556,7 +556,7 @@ public abstract class CachingDirectoryFactory extends DirectoryFactory {
    * @lucene.internal
    */
   public synchronized Set<String> getLivePaths() {
-    HashSet<String> livePaths = new HashSet<String>();
+    HashSet<String> livePaths = new HashSet<>();
     for (CacheValue val : byPathCache.values()) {
       if (!val.doneWithDir) {
         livePaths.add(val.path);
diff --git solr/core/src/java/org/apache/solr/core/Config.java solr/core/src/java/org/apache/solr/core/Config.java
index 2e79b31..8d1ef49 100644
--- solr/core/src/java/org/apache/solr/core/Config.java
+++ solr/core/src/java/org/apache/solr/core/Config.java
@@ -301,14 +301,14 @@ public class Config {
    * or null if all attributes are known.
    */
   public Set<String> getUnknownAttributes(Element element, String... knownAttributes) {
-    Set<String> knownAttributeSet = new HashSet<String>(Arrays.asList(knownAttributes));
+    Set<String> knownAttributeSet = new HashSet<>(Arrays.asList(knownAttributes));
     Set<String> unknownAttributeSet = null;
     NamedNodeMap attributes = element.getAttributes();
     for (int i = 0 ; i < attributes.getLength() ; ++i) {
       final String attributeName = attributes.item(i).getNodeName();
       if ( ! knownAttributeSet.contains(attributeName)) {
         if (null == unknownAttributeSet) {
-          unknownAttributeSet = new HashSet<String>();
+          unknownAttributeSet = new HashSet<>();
         }
         unknownAttributeSet.add(attributeName);
       }
@@ -321,7 +321,7 @@ public class Config {
    * contains an attribute name that is not among knownAttributes. 
    */
   public void complainAboutUnknownAttributes(String elementXpath, String... knownAttributes) {
-    SortedMap<String,SortedSet<String>> problems = new TreeMap<String,SortedSet<String>>(); 
+    SortedMap<String,SortedSet<String>> problems = new TreeMap<>();
     NodeList nodeList = getNodeList(elementXpath, false);
     for (int i = 0 ; i < nodeList.getLength() ; ++i) {
       Element element = (Element)nodeList.item(i);
@@ -330,7 +330,7 @@ public class Config {
         String elementName = element.getNodeName();
         SortedSet<String> allUnknownAttributes = problems.get(elementName);
         if (null == allUnknownAttributes) {
-          allUnknownAttributes = new TreeSet<String>();
+          allUnknownAttributes = new TreeSet<>();
           problems.put(elementName, allUnknownAttributes);
         }
         allUnknownAttributes.addAll(unknownAttributes);
diff --git solr/core/src/java/org/apache/solr/core/ConfigSolr.java solr/core/src/java/org/apache/solr/core/ConfigSolr.java
index e4e0b25..893cc2a 100644
--- solr/core/src/java/org/apache/solr/core/ConfigSolr.java
+++ solr/core/src/java/org/apache/solr/core/ConfigSolr.java
@@ -269,7 +269,7 @@ public abstract class ConfigSolr {
   }
 
   protected Config config;
-  protected Map<CfgProp, String> propMap = new HashMap<CfgProp, String>();
+  protected Map<CfgProp, String> propMap = new HashMap<>();
 
   public ConfigSolr(Config config) {
     this.config = config;
diff --git solr/core/src/java/org/apache/solr/core/ConfigSolrXmlOld.java solr/core/src/java/org/apache/solr/core/ConfigSolrXmlOld.java
index f33bf62..0b0dbbf 100644
--- solr/core/src/java/org/apache/solr/core/ConfigSolrXmlOld.java
+++ solr/core/src/java/org/apache/solr/core/ConfigSolrXmlOld.java
@@ -186,8 +186,8 @@ public class ConfigSolrXmlOld extends ConfigSolr {
     coreNodes = (NodeList) config.evaluate("solr/cores/core",
         XPathConstants.NODESET);
     // Check a couple of error conditions
-    Set<String> names = new HashSet<String>(); // for duplicate names
-    Map<String,String> dirs = new HashMap<String,String>(); // for duplicate
+    Set<String> names = new HashSet<>(); // for duplicate names
+    Map<String,String> dirs = new HashMap<>(); // for duplicate
                                                             // data dirs.
     
     for (int idx = 0; idx < coreNodes.getLength(); ++idx) {
@@ -236,7 +236,7 @@ public class ConfigSolrXmlOld extends ConfigSolr {
   }
 
   public List<String> getAllCoreNames() {
-    List<String> ret = new ArrayList<String>();
+    List<String> ret = new ArrayList<>();
     
     synchronized (coreNodes) {
       for (int idx = 0; idx < coreNodes.getLength(); ++idx) {
diff --git solr/core/src/java/org/apache/solr/core/CoreContainer.java solr/core/src/java/org/apache/solr/core/CoreContainer.java
index be8ecba..ccf4405 100644
--- solr/core/src/java/org/apache/solr/core/CoreContainer.java
+++ solr/core/src/java/org/apache/solr/core/CoreContainer.java
@@ -206,7 +206,7 @@ public class CoreContainer {
     shareSchema = cfg.hasSchemaCache();
 
     if (shareSchema) {
-      indexSchemaCache = new ConcurrentHashMap<String,IndexSchema>();
+      indexSchemaCache = new ConcurrentHashMap<>();
     }
     
     hostName = cfg.getHost();
@@ -227,10 +227,10 @@ public class CoreContainer {
         new DefaultSolrThreadFactory("coreLoadExecutor") );
 
     try {
-      CompletionService<SolrCore> completionService = new ExecutorCompletionService<SolrCore>(
+      CompletionService<SolrCore> completionService = new ExecutorCompletionService<>(
           coreLoadExecutor);
 
-      Set<Future<SolrCore>> pending = new HashSet<Future<SolrCore>>();
+      Set<Future<SolrCore>> pending = new HashSet<>();
 
       List<CoreDescriptor> cds = coresLocator.discover(this);
       checkForDuplicateCoreNames(cds);
@@ -657,7 +657,7 @@ public class CoreContainer {
    */
   public Map<String,Exception> getCoreInitFailures() {
     synchronized ( coreInitFailures ) {
-      return Collections.unmodifiableMap(new LinkedHashMap<String,Exception>
+      return Collections.unmodifiableMap(new LinkedHashMap<>
                                          (coreInitFailures));
     }
   }
diff --git solr/core/src/java/org/apache/solr/core/IndexDeletionPolicyWrapper.java solr/core/src/java/org/apache/solr/core/IndexDeletionPolicyWrapper.java
index 8e7013a..f48c9da 100644
--- solr/core/src/java/org/apache/solr/core/IndexDeletionPolicyWrapper.java
+++ solr/core/src/java/org/apache/solr/core/IndexDeletionPolicyWrapper.java
@@ -44,10 +44,10 @@ import java.util.concurrent.atomic.AtomicInteger;
  */
 public final class IndexDeletionPolicyWrapper extends IndexDeletionPolicy {
   private final IndexDeletionPolicy deletionPolicy;
-  private volatile Map<Long, IndexCommit> solrVersionVsCommits = new ConcurrentHashMap<Long, IndexCommit>();
-  private final Map<Long, Long> reserves = new ConcurrentHashMap<Long,Long>();
+  private volatile Map<Long, IndexCommit> solrVersionVsCommits = new ConcurrentHashMap<>();
+  private final Map<Long, Long> reserves = new ConcurrentHashMap<>();
   private volatile IndexCommit latestCommit;
-  private final ConcurrentHashMap<Long, AtomicInteger> savedCommits = new ConcurrentHashMap<Long, AtomicInteger>();
+  private final ConcurrentHashMap<Long, AtomicInteger> savedCommits = new ConcurrentHashMap<>();
 
   public IndexDeletionPolicyWrapper(IndexDeletionPolicy deletionPolicy) {
     this.deletionPolicy = deletionPolicy;
@@ -102,7 +102,7 @@ public final class IndexDeletionPolicyWrapper extends IndexDeletionPolicy {
   }
 
   private List<IndexCommitWrapper> wrap(List<? extends IndexCommit> list) {
-    List<IndexCommitWrapper> result = new ArrayList<IndexCommitWrapper>();
+    List<IndexCommitWrapper> result = new ArrayList<>();
     for (IndexCommit indexCommit : list) result.add(new IndexCommitWrapper(indexCommit));
     return result;
   }
@@ -232,7 +232,7 @@ public final class IndexDeletionPolicyWrapper extends IndexDeletionPolicy {
   }
 
   private void updateCommitPoints(List<IndexCommitWrapper> list) {
-    Map<Long, IndexCommit> map = new ConcurrentHashMap<Long, IndexCommit>();
+    Map<Long, IndexCommit> map = new ConcurrentHashMap<>();
     for (IndexCommitWrapper wrapper : list) {
       if (!wrapper.isDeleted())
         map.put(wrapper.delegate.getGeneration(), wrapper.delegate);
diff --git solr/core/src/java/org/apache/solr/core/JmxMonitoredMap.java solr/core/src/java/org/apache/solr/core/JmxMonitoredMap.java
index 384fe08..7a96d62 100644
--- solr/core/src/java/org/apache/solr/core/JmxMonitoredMap.java
+++ solr/core/src/java/org/apache/solr/core/JmxMonitoredMap.java
@@ -182,7 +182,7 @@ public class JmxMonitoredMap<K, V> extends
 
   private ObjectName getObjectName(String key, SolrInfoMBean infoBean)
           throws MalformedObjectNameException {
-    Hashtable<String, String> map = new Hashtable<String, String>();
+    Hashtable<String, String> map = new Hashtable<>();
     map.put("type", key);
     if (infoBean.getName() != null && !"".equals(infoBean.getName())) {
       map.put("id", infoBean.getName());
@@ -208,7 +208,7 @@ public class JmxMonitoredMap<K, V> extends
 
     public SolrDynamicMBean(String coreHashCode, SolrInfoMBean managedResource) {
       this.infoBean = managedResource;
-      staticStats = new HashSet<String>();
+      staticStats = new HashSet<>();
 
       // For which getters are already available in SolrInfoMBean
       staticStats.add("name");
@@ -221,7 +221,7 @@ public class JmxMonitoredMap<K, V> extends
 
     @Override
     public MBeanInfo getMBeanInfo() {
-      ArrayList<MBeanAttributeInfo> attrInfoList = new ArrayList<MBeanAttributeInfo>();
+      ArrayList<MBeanAttributeInfo> attrInfoList = new ArrayList<>();
 
       for (String stat : staticStats) {
         attrInfoList.add(new MBeanAttributeInfo(stat, String.class.getName(),
diff --git solr/core/src/java/org/apache/solr/core/PluginInfo.java solr/core/src/java/org/apache/solr/core/PluginInfo.java
index 2ecb617..19bbf90 100644
--- solr/core/src/java/org/apache/solr/core/PluginInfo.java
+++ solr/core/src/java/org/apache/solr/core/PluginInfo.java
@@ -55,7 +55,7 @@ public class PluginInfo {
   }
 
   private List<PluginInfo> loadSubPlugins(Node node) {
-    List<PluginInfo> children = new ArrayList<PluginInfo>();
+    List<PluginInfo> children = new ArrayList<>();
     //if there is another sub tag with a non namedlist tag that has to be another plugin
     NodeList nlst = node.getChildNodes();
     for (int i = 0; i < nlst.getLength(); i++) {
@@ -99,13 +99,13 @@ public class PluginInfo {
    */
   public List<PluginInfo> getChildren(String type){
     if(children.isEmpty()) return children;
-    List<PluginInfo> result = new ArrayList<PluginInfo>();
+    List<PluginInfo> result = new ArrayList<>();
     for (PluginInfo child : children) if(type.equals(child.type)) result.add(child);
     return result;
   }
   public static final PluginInfo EMPTY_INFO = new PluginInfo("",Collections.<String,String>emptyMap(), new NamedList(),Collections.<PluginInfo>emptyList());
 
-  private static final HashSet<String> NL_TAGS = new HashSet<String>
+  private static final HashSet<String> NL_TAGS = new HashSet<>
     (Arrays.asList("lst", "arr",
                    "bool",
                    "str",
diff --git solr/core/src/java/org/apache/solr/core/RequestHandlers.java solr/core/src/java/org/apache/solr/core/RequestHandlers.java
index 63844cc..90fac1b 100644
--- solr/core/src/java/org/apache/solr/core/RequestHandlers.java
+++ solr/core/src/java/org/apache/solr/core/RequestHandlers.java
@@ -45,7 +45,7 @@ public final class RequestHandlers {
   // Use a synchronized map - since the handlers can be changed at runtime, 
   // the map implementation should be thread safe
   private final Map<String, SolrRequestHandler> handlers =
-      new ConcurrentHashMap<String,SolrRequestHandler>() ;
+      new ConcurrentHashMap<>() ;
 
   /**
    * Trim the trailing '/' if its there, and convert null to empty string.
@@ -80,7 +80,7 @@ public final class RequestHandlers {
    * @return a Map of all registered handlers of the specified type.
    */
   public <T extends SolrRequestHandler> Map<String,T> getAll(Class<T> clazz) {
-    Map<String,T> result = new HashMap<String,T>(7);
+    Map<String,T> result = new HashMap<>(7);
     for (Map.Entry<String,SolrRequestHandler> e : handlers.entrySet()) {
       if(clazz.isInstance(e.getValue())) result.put(e.getKey(), clazz.cast(e.getValue()));
     }
@@ -137,7 +137,7 @@ public final class RequestHandlers {
 
   void initHandlersFromConfig(SolrConfig config ){
     // use link map so we iterate in the same order
-    Map<PluginInfo,SolrRequestHandler> handlers = new LinkedHashMap<PluginInfo,SolrRequestHandler>();
+    Map<PluginInfo,SolrRequestHandler> handlers = new LinkedHashMap<>();
     for (PluginInfo info : config.getPluginInfos(SolrRequestHandler.class.getName())) {
       try {
         SolrRequestHandler requestHandler;
@@ -317,7 +317,7 @@ public final class RequestHandlers {
       if( _handler != null ) {
         return _handler.getStatistics();
       }
-      NamedList<String> lst = new SimpleOrderedMap<String>();
+      NamedList<String> lst = new SimpleOrderedMap<>();
       lst.add("note", "not initialized yet" );
       return lst;
     }
diff --git solr/core/src/java/org/apache/solr/core/SolrConfig.java solr/core/src/java/org/apache/solr/core/SolrConfig.java
index 45dcfee..5681667 100644
--- solr/core/src/java/org/apache/solr/core/SolrConfig.java
+++ solr/core/src/java/org/apache/solr/core/SolrConfig.java
@@ -193,7 +193,7 @@ public class SolrConfig extends Config {
     documentCacheConfig = CacheConfig.getConfig(this, "query/documentCache");
     CacheConfig conf = CacheConfig.getConfig(this, "query/fieldValueCache");
     if (conf == null) {
-      Map<String,String> args = new HashMap<String,String>();
+      Map<String,String> args = new HashMap<>();
       args.put("name","fieldValueCache");
       args.put("size","10000");
       args.put("initialSize","10");
@@ -318,7 +318,7 @@ public class SolrConfig extends Config {
   }
 
   public List<PluginInfo> readPluginInfos(String tag, boolean requireName, boolean requireClass) {
-    ArrayList<PluginInfo> result = new ArrayList<PluginInfo>();
+    ArrayList<PluginInfo> result = new ArrayList<>();
     NodeList nodes = (NodeList) evaluate(tag, XPathConstants.NODESET);
     for (int i=0; i<nodes.getLength(); i++) {
       PluginInfo pluginInfo = new PluginInfo(nodes.item(i), "[solrconfig.xml] " + tag, requireName, requireClass);
@@ -362,7 +362,7 @@ public class SolrConfig extends Config {
 
   protected UpdateHandlerInfo updateHandlerInfo ;
 
-  private Map<String, List<PluginInfo>> pluginStore = new LinkedHashMap<String, List<PluginInfo>>();
+  private Map<String, List<PluginInfo>> pluginStore = new LinkedHashMap<>();
 
   public final int maxWarmingSearchers;
   public final boolean unlockOnStartup;
diff --git solr/core/src/java/org/apache/solr/core/SolrCore.java solr/core/src/java/org/apache/solr/core/SolrCore.java
index a091273..a87e2c3 100644
--- solr/core/src/java/org/apache/solr/core/SolrCore.java
+++ solr/core/src/java/org/apache/solr/core/SolrCore.java
@@ -360,8 +360,8 @@ public final class SolrCore implements SolrInfoMBean {
     }
   }
 
-  final List<SolrEventListener> firstSearcherListeners = new ArrayList<SolrEventListener>();
-  final List<SolrEventListener> newSearcherListeners = new ArrayList<SolrEventListener>();
+  final List<SolrEventListener> firstSearcherListeners = new ArrayList<>();
+  final List<SolrEventListener> newSearcherListeners = new ArrayList<>();
 
   /**
    * NOTE: this function is not thread safe.  However, it is safe to call within the
@@ -464,7 +464,7 @@ public final class SolrCore implements SolrInfoMBean {
   }
   
   // protect via synchronized(SolrCore.class)
-  private static Set<String> dirs = new HashSet<String>();
+  private static Set<String> dirs = new HashSet<>();
 
   void initIndex(boolean reload) throws IOException {
  
@@ -716,7 +716,7 @@ public final class SolrCore implements SolrInfoMBean {
       infoRegistry = new JmxMonitoredMap<String, SolrInfoMBean>(name, String.valueOf(this.hashCode()), config.jmxConfig);
     } else  {
       log.info("JMX monitoring not detected for core: " + name);
-      infoRegistry = new ConcurrentHashMap<String, SolrInfoMBean>();
+      infoRegistry = new ConcurrentHashMap<>();
     }
 
     infoRegistry.put("fieldCache", new SolrFieldCacheMBean());
@@ -923,7 +923,7 @@ public final class SolrCore implements SolrInfoMBean {
    * Load the request processors
    */
    private Map<String,UpdateRequestProcessorChain> loadUpdateProcessorChains() {
-    Map<String, UpdateRequestProcessorChain> map = new HashMap<String, UpdateRequestProcessorChain>();
+    Map<String, UpdateRequestProcessorChain> map = new HashMap<>();
     UpdateRequestProcessorChain def = initPlugins(map,UpdateRequestProcessorChain.class, UpdateRequestProcessorChain.class.getName());
     if(def == null){
       def = map.get(null);
@@ -1141,7 +1141,7 @@ public final class SolrCore implements SolrInfoMBean {
    public void addCloseHook( CloseHook hook )
    {
      if( closeHooks == null ) {
-       closeHooks = new ArrayList<CloseHook>();
+       closeHooks = new ArrayList<>();
      }
      closeHooks.add( hook );
    }
@@ -1218,7 +1218,7 @@ public final class SolrCore implements SolrInfoMBean {
    */
   private Map<String, SearchComponent> loadSearchComponents()
   {
-    Map<String, SearchComponent> components = new HashMap<String, SearchComponent>();
+    Map<String, SearchComponent> components = new HashMap<>();
     initPlugins(components,SearchComponent.class);
     for (Map.Entry<String, SearchComponent> e : components.entrySet()) {
       SearchComponent c = e.getValue();
@@ -1303,8 +1303,8 @@ public final class SolrCore implements SolrInfoMBean {
 
   // All of the normal open searchers.  Don't access this directly.
   // protected by synchronizing on searcherLock.
-  private final LinkedList<RefCounted<SolrIndexSearcher>> _searchers = new LinkedList<RefCounted<SolrIndexSearcher>>();
-  private final LinkedList<RefCounted<SolrIndexSearcher>> _realtimeSearchers = new LinkedList<RefCounted<SolrIndexSearcher>>();
+  private final LinkedList<RefCounted<SolrIndexSearcher>> _searchers = new LinkedList<>();
+  private final LinkedList<RefCounted<SolrIndexSearcher>> _realtimeSearchers = new LinkedList<>();
 
   final ExecutorService searcherExecutor = Executors.newSingleThreadExecutor(
       new DefaultSolrThreadFactory("searcherExecutor"));
@@ -1940,7 +1940,7 @@ public final class SolrCore implements SolrInfoMBean {
 
   public static void preDecorateResponse(SolrQueryRequest req, SolrQueryResponse rsp) {
     // setup response header
-    final NamedList<Object> responseHeader = new SimpleOrderedMap<Object>();
+    final NamedList<Object> responseHeader = new SimpleOrderedMap<>();
     rsp.add("responseHeader", responseHeader);
 
     // toLog is a local ref to the same NamedList used by the response
@@ -2010,10 +2010,10 @@ public final class SolrCore implements SolrInfoMBean {
   
   
   private QueryResponseWriter defaultResponseWriter;
-  private final Map<String, QueryResponseWriter> responseWriters = new HashMap<String, QueryResponseWriter>();
+  private final Map<String, QueryResponseWriter> responseWriters = new HashMap<>();
   public static final Map<String ,QueryResponseWriter> DEFAULT_RESPONSE_WRITERS ;
   static{
-    HashMap<String, QueryResponseWriter> m= new HashMap<String, QueryResponseWriter>();
+    HashMap<String, QueryResponseWriter> m= new HashMap<>();
     m.put("xml", new XMLResponseWriter());
     m.put("standard", m.get("xml"));
     m.put("json", new JSONResponseWriter());
@@ -2032,7 +2032,7 @@ public final class SolrCore implements SolrInfoMBean {
    * writers may also be configured. */
   private void initWriters() {
     // use link map so we iterate in the same order
-    Map<PluginInfo,QueryResponseWriter> writers = new LinkedHashMap<PluginInfo,QueryResponseWriter>();
+    Map<PluginInfo,QueryResponseWriter> writers = new LinkedHashMap<>();
     for (PluginInfo info : solrConfig.getPluginInfos(QueryResponseWriter.class.getName())) {
       try {
         QueryResponseWriter writer;
@@ -2112,7 +2112,7 @@ public final class SolrCore implements SolrInfoMBean {
     return getQueryResponseWriter(request.getParams().get(CommonParams.WT)); 
   }
 
-  private final Map<String, QParserPlugin> qParserPlugins = new HashMap<String, QParserPlugin>();
+  private final Map<String, QParserPlugin> qParserPlugins = new HashMap<>();
 
   /** Configure the query parsers. */
   private void initQParsers() {
@@ -2140,7 +2140,7 @@ public final class SolrCore implements SolrInfoMBean {
     throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, "Unknown query parser '"+parserName+"'");
   }
   
-  private final HashMap<String, ValueSourceParser> valueSourceParsers = new HashMap<String, ValueSourceParser>();
+  private final HashMap<String, ValueSourceParser> valueSourceParsers = new HashMap<>();
   
   /** Configure the ValueSource (function) plugins */
   private void initValueSourceParsers() {
@@ -2161,7 +2161,7 @@ public final class SolrCore implements SolrInfoMBean {
   }
   
 
-  private final HashMap<String, TransformerFactory> transformerFactories = new HashMap<String, TransformerFactory>();
+  private final HashMap<String, TransformerFactory> transformerFactories = new HashMap<>();
   
   /** Configure the TransformerFactory plugins */
   private void initTransformerFactories() {
@@ -2220,7 +2220,7 @@ public final class SolrCore implements SolrInfoMBean {
    */
   public <T> List<T> initPlugins(List<PluginInfo> pluginInfos, Class<T> type, String defClassName) {
     if(pluginInfos.isEmpty()) return Collections.emptyList();
-    List<T> result = new ArrayList<T>();
+    List<T> result = new ArrayList<>();
     for (PluginInfo info : pluginInfos) result.add(createInitInstance(info,type, type.getSimpleName(), defClassName));
     return result;
   }
@@ -2251,10 +2251,10 @@ public final class SolrCore implements SolrInfoMBean {
           "solrconfig.xml uses deprecated <admin/gettableFiles>, Please "+
           "update your config to use the ShowFileRequestHandler." );
       if( getRequestHandler( "/admin/file" ) == null ) {
-        NamedList<String> invariants = new NamedList<String>();
+        NamedList<String> invariants = new NamedList<>();
         
         // Hide everything...
-        Set<String> hide = new HashSet<String>();
+        Set<String> hide = new HashSet<>();
 
         for (String file : solrConfig.getResourceLoader().listConfigDir()) {
           hide.add(file.toUpperCase(Locale.ROOT));
@@ -2269,7 +2269,7 @@ public final class SolrCore implements SolrInfoMBean {
           invariants.add( ShowFileRequestHandler.HIDDEN, s );
         }
         
-        NamedList<Object> args = new NamedList<Object>();
+        NamedList<Object> args = new NamedList<>();
         args.add( "invariants", invariants );
         ShowFileRequestHandler handler = new ShowFileRequestHandler();
         handler.init( args );
@@ -2330,7 +2330,7 @@ public final class SolrCore implements SolrInfoMBean {
 
   @Override
   public NamedList getStatistics() {
-    NamedList<Object> lst = new SimpleOrderedMap<Object>();
+    NamedList<Object> lst = new SimpleOrderedMap<>();
     lst.add("coreName", name==null ? "(null)" : name);
     lst.add("startTime", new Date(startTime));
     lst.add("refCount", getOpenCount());
diff --git solr/core/src/java/org/apache/solr/core/SolrCores.java solr/core/src/java/org/apache/solr/core/SolrCores.java
index a22c1c4..dc7c232 100644
--- solr/core/src/java/org/apache/solr/core/SolrCores.java
+++ solr/core/src/java/org/apache/solr/core/SolrCores.java
@@ -36,16 +36,16 @@ import java.util.concurrent.ConcurrentHashMap;
 class SolrCores {
 
   private static Object modifyLock = new Object(); // for locking around manipulating any of the core maps.
-  private final Map<String, SolrCore> cores = new LinkedHashMap<String, SolrCore>(); // For "permanent" cores
+  private final Map<String, SolrCore> cores = new LinkedHashMap<>(); // For "permanent" cores
 
   //WARNING! The _only_ place you put anything into the list of transient cores is with the putTransientCore method!
-  private Map<String, SolrCore> transientCores = new LinkedHashMap<String, SolrCore>(); // For "lazily loaded" cores
+  private Map<String, SolrCore> transientCores = new LinkedHashMap<>(); // For "lazily loaded" cores
 
-  private final Map<String, CoreDescriptor> dynamicDescriptors = new LinkedHashMap<String, CoreDescriptor>();
+  private final Map<String, CoreDescriptor> dynamicDescriptors = new LinkedHashMap<>();
 
-  private final Map<String, SolrCore> createdCores = new LinkedHashMap<String, SolrCore>();
+  private final Map<String, SolrCore> createdCores = new LinkedHashMap<>();
 
-  private Map<SolrCore, String> coreToOrigName = new ConcurrentHashMap<SolrCore, String>();
+  private Map<SolrCore, String> coreToOrigName = new ConcurrentHashMap<>();
 
   private final CoreContainer container;
 
@@ -53,11 +53,11 @@ class SolrCores {
 
   // This map will hold objects that are being currently operated on. The core (value) may be null in the case of
   // initial load. The rule is, never to any operation on a core that is currently being operated upon.
-  private static final Set<String> pendingCoreOps = new HashSet<String>();
+  private static final Set<String> pendingCoreOps = new HashSet<>();
 
   // Due to the fact that closes happen potentially whenever anything is _added_ to the transient core list, we need
   // to essentially queue them up to be handled via pendingCoreOps.
-  private static final List<SolrCore> pendingCloses = new ArrayList<SolrCore>();
+  private static final List<SolrCore> pendingCloses = new ArrayList<>();
 
   SolrCores(CoreContainer container) {
     this.container = container;
@@ -95,7 +95,7 @@ class SolrCores {
   // We are shutting down. You can't hold the lock on the various lists of cores while they shut down, so we need to
   // make a temporary copy of the names and shut them down outside the lock.
   protected void close() {
-    Collection<SolrCore> coreList = new ArrayList<SolrCore>();
+    Collection<SolrCore> coreList = new ArrayList<>();
 
     // It might be possible for one of the cores to move from one list to another while we're closing them. So
     // loop through the lists until they're all empty. In particular, the core could have moved from the transient
@@ -145,7 +145,7 @@ class SolrCores {
   }
 
   List<SolrCore> getCores() {
-    List<SolrCore> lst = new ArrayList<SolrCore>();
+    List<SolrCore> lst = new ArrayList<>();
 
     synchronized (modifyLock) {
       lst.addAll(cores.values());
@@ -154,7 +154,7 @@ class SolrCores {
   }
 
   Set<String> getCoreNames() {
-    Set<String> set = new TreeSet<String>();
+    Set<String> set = new TreeSet<>();
 
     synchronized (modifyLock) {
       set.addAll(cores.keySet());
@@ -164,7 +164,7 @@ class SolrCores {
   }
 
   List<String> getCoreNames(SolrCore core) {
-    List<String> lst = new ArrayList<String>();
+    List<String> lst = new ArrayList<>();
 
     synchronized (modifyLock) {
       for (Map.Entry<String, SolrCore> entry : cores.entrySet()) {
@@ -187,7 +187,7 @@ class SolrCores {
    * @return all cores names, whether loaded or unloaded.
    */
   public Collection<String> getAllCoreNames() {
-    Set<String> set = new TreeSet<String>();
+    Set<String> set = new TreeSet<>();
     synchronized (modifyLock) {
       set.addAll(cores.keySet());
       set.addAll(transientCores.keySet());
diff --git solr/core/src/java/org/apache/solr/core/SolrResourceLoader.java solr/core/src/java/org/apache/solr/core/SolrResourceLoader.java
index 5d6e289..df5921e 100644
--- solr/core/src/java/org/apache/solr/core/SolrResourceLoader.java
+++ solr/core/src/java/org/apache/solr/core/SolrResourceLoader.java
@@ -390,7 +390,7 @@ public class SolrResourceLoader implements ResourceLoader,Closeable
   /*
    * A static map of short class name to fully qualified class name 
    */
-  private static final Map<String, String> classNameCache = new ConcurrentHashMap<String, String>();
+  private static final Map<String, String> classNameCache = new ConcurrentHashMap<>();
 
   // Using this pattern, legacy analysis components from previous Solr versions are identified and delegated to SPI loader:
   private static final Pattern legacyAnalysisPattern = 
@@ -742,7 +742,7 @@ public class SolrResourceLoader implements ResourceLoader,Closeable
    */
   private static final Map<Class, Class[]> awareCompatibility;
   static {
-    awareCompatibility = new HashMap<Class, Class[]>();
+    awareCompatibility = new HashMap<>();
     awareCompatibility.put( 
       SolrCoreAware.class, new Class[] {
         CodecFactory.class,
diff --git solr/core/src/java/org/apache/solr/core/SolrXMLCoresLocator.java solr/core/src/java/org/apache/solr/core/SolrXMLCoresLocator.java
index 632b4d6..bd59ad8 100644
--- solr/core/src/java/org/apache/solr/core/SolrXMLCoresLocator.java
+++ solr/core/src/java/org/apache/solr/core/SolrXMLCoresLocator.java
@@ -143,7 +143,7 @@ public class SolrXMLCoresLocator implements CoresLocator {
 
   @Override
   public synchronized final void persist(CoreContainer cc, CoreDescriptor... coreDescriptors) {
-    List<CoreDescriptor> cds = new ArrayList<CoreDescriptor>(cc.getCoreDescriptors().size() + coreDescriptors.length);
+    List<CoreDescriptor> cds = new ArrayList<>(cc.getCoreDescriptors().size() + coreDescriptors.length);
     
     cds.addAll(cc.getCoreDescriptors());
     cds.addAll(Arrays.asList(coreDescriptors));
diff --git solr/core/src/java/org/apache/solr/core/ZkContainer.java solr/core/src/java/org/apache/solr/core/ZkContainer.java
index e0bcd45..e992aaf 100644
--- solr/core/src/java/org/apache/solr/core/ZkContainer.java
+++ solr/core/src/java/org/apache/solr/core/ZkContainer.java
@@ -141,7 +141,7 @@ public class ZkContainer {
 
               @Override
               public List<CoreDescriptor> getCurrentDescriptors() {
-                List<CoreDescriptor> descriptors = new ArrayList<CoreDescriptor>(
+                List<CoreDescriptor> descriptors = new ArrayList<>(
                     cc.getCoreNames().size());
                 Collection<SolrCore> cores = cc.getCores();
                 for (SolrCore core : cores) {
diff --git solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase.java solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase.java
index ad81138..a2cd9ed 100644
--- solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase.java
+++ solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase.java
@@ -87,7 +87,7 @@ public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
     if (!TokenizerChain.class.isInstance(analyzer)) {
 
       try (TokenStream tokenStream = analyzer.tokenStream(context.getFieldName(), value)) {
-        NamedList<List<NamedList>> namedList = new NamedList<List<NamedList>>();
+        NamedList<List<NamedList>> namedList = new NamedList<>();
         namedList.add(tokenStream.getClass().getName(), convertTokensToNamedLists(analyzeTokenStream(tokenStream), context));
         return namedList;
       } catch (IOException e) {
@@ -100,7 +100,7 @@ public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
     TokenizerFactory tfac = tokenizerChain.getTokenizerFactory();
     TokenFilterFactory[] filtfacs = tokenizerChain.getTokenFilterFactories();
 
-    NamedList<Object> namedList = new NamedList<Object>();
+    NamedList<Object> namedList = new NamedList<>();
 
     if( cfiltfacs != null ){
       String source = value;
@@ -144,7 +144,7 @@ public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
    */
   protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {
     try (TokenStream tokenStream = analyzer.tokenStream("", query)){
-      final Set<BytesRef> tokens = new HashSet<BytesRef>();
+      final Set<BytesRef> tokens = new HashSet<>();
       final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);
       final BytesRef bytes = bytesAtt.getBytesRef();
 
@@ -170,7 +170,7 @@ public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
    * @return List of tokens produced from the TokenStream
    */
   private List<AttributeSource> analyzeTokenStream(TokenStream tokenStream) {
-    final List<AttributeSource> tokens = new ArrayList<AttributeSource>();
+    final List<AttributeSource> tokens = new ArrayList<>();
     final PositionIncrementAttribute posIncrAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);
     final TokenTrackingAttribute trackerAtt = tokenStream.addAttribute(TokenTrackingAttribute.class);
     // for backwards compatibility, add all "common" attributes
@@ -212,7 +212,7 @@ public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
    * @return List of NamedLists containing the relevant information taken from the tokens
    */
   private List<NamedList> convertTokensToNamedLists(final List<AttributeSource> tokenList, AnalysisContext context) {
-    final List<NamedList> tokensNamedLists = new ArrayList<NamedList>();
+    final List<NamedList> tokensNamedLists = new ArrayList<>();
     final FieldType fieldType = context.getFieldType();
     final AttributeSource[] tokens = tokenList.toArray(new AttributeSource[tokenList.size()]);
     
@@ -241,7 +241,7 @@ public abstract class AnalysisRequestHandlerBase extends RequestHandlerBase {
 
     for (int i = 0; i < tokens.length; i++) {
       AttributeSource token = tokens[i];
-      final NamedList<Object> tokenNamedList = new SimpleOrderedMap<Object>();
+      final NamedList<Object> tokenNamedList = new SimpleOrderedMap<>();
       final TermToBytesRefAttribute termAtt = token.getAttribute(TermToBytesRefAttribute.class);
       BytesRef rawBytes = termAtt.getBytesRef();
       termAtt.fillBytesRef();
diff --git solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java
index 6e8c53a..a75857e 100644
--- solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java
+++ solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java
@@ -198,11 +198,11 @@ public class DocumentAnalysisRequestHandler extends AnalysisRequestHandlerBase {
   NamedList<Object> handleAnalysisRequest(DocumentAnalysisRequest request, IndexSchema schema) {
 
     SchemaField uniqueKeyField = schema.getUniqueKeyField();
-    NamedList<Object> result = new SimpleOrderedMap<Object>();
+    NamedList<Object> result = new SimpleOrderedMap<>();
 
     for (SolrInputDocument document : request.getDocuments()) {
 
-      NamedList<NamedList> theTokens = new SimpleOrderedMap<NamedList>();
+      NamedList<NamedList> theTokens = new SimpleOrderedMap<>();
       result.add(document.getFieldValue(uniqueKeyField.getName()).toString(), theTokens);
       for (String name : document.getFieldNames()) {
 
@@ -212,7 +212,7 @@ public class DocumentAnalysisRequestHandler extends AnalysisRequestHandlerBase {
           continue;
         }
 
-        NamedList<Object> fieldTokens = new SimpleOrderedMap<Object>();
+        NamedList<Object> fieldTokens = new SimpleOrderedMap<>();
         theTokens.add(name, fieldTokens);
 
         FieldType fieldType = schema.getFieldType(name);
@@ -241,7 +241,7 @@ public class DocumentAnalysisRequestHandler extends AnalysisRequestHandlerBase {
         AnalysisContext analysisContext = new AnalysisContext(fieldType, analyzer, termsToMatch);
         Collection<Object> fieldValues = document.getFieldValues(name);
         NamedList<NamedList<? extends Object>> indexTokens 
-          = new SimpleOrderedMap<NamedList<? extends Object>>();
+          = new SimpleOrderedMap<>();
         for (Object fieldValue : fieldValues) {
           indexTokens.add(String.valueOf(fieldValue), 
                           analyzeValue(fieldValue.toString(), analysisContext));
diff --git solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java
index 7556c30..a539d7c 100644
--- solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java
+++ solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java
@@ -38,10 +38,10 @@ public class DumpRequestHandler extends RequestHandlerBase
         
     // Write the streams...
     if( req.getContentStreams() != null ) {
-      ArrayList<NamedList<Object>> streams = new ArrayList<NamedList<Object>>();
+      ArrayList<NamedList<Object>> streams = new ArrayList<>();
       // Cycle through each stream
       for( ContentStream content : req.getContentStreams() ) {
-        NamedList<Object> stream = new SimpleOrderedMap<Object>();
+        NamedList<Object> stream = new SimpleOrderedMap<>();
         stream.add( "name", content.getName() );
         stream.add( "sourceInfo", content.getSourceInfo() );
         stream.add( "size", content.getSize() );
diff --git solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java
index 5774aa3..7bd82a5 100644
--- solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java
+++ solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java
@@ -173,9 +173,9 @@ public class FieldAnalysisRequestHandler extends AnalysisRequestHandlerBase {
    * @return The analysis breakdown as a named list.
    */
   protected NamedList<NamedList> handleAnalysisRequest(FieldAnalysisRequest request, IndexSchema schema) {
-    NamedList<NamedList> analysisResults = new SimpleOrderedMap<NamedList>();
+    NamedList<NamedList> analysisResults = new SimpleOrderedMap<>();
 
-    NamedList<NamedList> fieldTypeAnalysisResults = new SimpleOrderedMap<NamedList>();
+    NamedList<NamedList> fieldTypeAnalysisResults = new SimpleOrderedMap<>();
     if (request.getFieldTypes() != null)  {
       for (String fieldTypeName : request.getFieldTypes()) {
         FieldType fieldType = schema.getFieldTypes().get(fieldTypeName);
@@ -183,7 +183,7 @@ public class FieldAnalysisRequestHandler extends AnalysisRequestHandlerBase {
       }
     }
 
-    NamedList<NamedList> fieldNameAnalysisResults = new SimpleOrderedMap<NamedList>();
+    NamedList<NamedList> fieldNameAnalysisResults = new SimpleOrderedMap<>();
     if (request.getFieldNames() != null)  {
       for (String fieldName : request.getFieldNames()) {
         FieldType fieldType = schema.getFieldType(fieldName);
@@ -215,7 +215,7 @@ public class FieldAnalysisRequestHandler extends AnalysisRequestHandlerBase {
       ? getQueryTokenSet(queryValue, fieldType.getQueryAnalyzer())
       : EMPTY_BYTES_SET;
 
-    NamedList<NamedList> analyzeResults = new SimpleOrderedMap<NamedList>();
+    NamedList<NamedList> analyzeResults = new SimpleOrderedMap<>();
     if (analysisRequest.getFieldValue() != null) {
       AnalysisContext context = new AnalysisContext(fieldName, fieldType, fieldType.getAnalyzer(), termsToMatch);
       NamedList analyzedTokens = analyzeValue(analysisRequest.getFieldValue(), context);
diff --git solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java
index 763f426..280e6b9 100644
--- solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java
+++ solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java
@@ -98,7 +98,7 @@ public class MoreLikeThisHandler extends RequestHandlerBase
 
       String[] fqs = req.getParams().getParams(CommonParams.FQ);
       if (fqs!=null && fqs.length!=0) {
-          filters = new ArrayList<Query>();
+          filters = new ArrayList<>();
         for (String fq : fqs) {
           if (fq != null && fq.trim().length()!=0) {
             QParser fqp = QParser.getParser(fq, null, req);
@@ -186,14 +186,14 @@ public class MoreLikeThisHandler extends RequestHandlerBase
   
     if( interesting != null ) {
       if( termStyle == TermStyle.DETAILS ) {
-        NamedList<Float> it = new NamedList<Float>();
+        NamedList<Float> it = new NamedList<>();
         for( InterestingTerm t : interesting ) {
           it.add( t.term.toString(), t.boost );
         }
         rsp.add( "interestingTerms", it );
       }
       else {
-        List<String> it = new ArrayList<String>( interesting.size() );
+        List<String> it = new ArrayList<>( interesting.size() );
         for( InterestingTerm t : interesting ) {
           it.add( t.term.text());
         }
@@ -236,7 +236,7 @@ public class MoreLikeThisHandler extends RequestHandlerBase
         if (null != dbgInfo) {
           if (null != filters) {
             dbgInfo.add("filter_queries",req.getParams().getParams(CommonParams.FQ));
-            List<String> fqs = new ArrayList<String>(filters.size());
+            List<String> fqs = new ArrayList<>(filters.size());
             for (Query fq : filters) {
               fqs.add(QueryParsing.toString(fq, req.getSchema()));
             }
@@ -388,7 +388,7 @@ public class MoreLikeThisHandler extends RequestHandlerBase
     public NamedList<DocList> getMoreLikeThese( DocList docs, int rows, int flags ) throws IOException
     {
       IndexSchema schema = searcher.getSchema();
-      NamedList<DocList> mlt = new SimpleOrderedMap<DocList>();
+      NamedList<DocList> mlt = new SimpleOrderedMap<>();
       DocIterator iterator = docs.iterator();
       while( iterator.hasNext() ) {
         int id = iterator.nextDoc();
@@ -404,7 +404,7 @@ public class MoreLikeThisHandler extends RequestHandlerBase
     public NamedList<BooleanQuery> getMoreLikeTheseQuery(DocList docs)
         throws IOException {
       IndexSchema schema = searcher.getSchema();
-      NamedList<BooleanQuery> result = new NamedList<BooleanQuery>();
+      NamedList<BooleanQuery> result = new NamedList<>();
       DocIterator iterator = docs.iterator();
       while (iterator.hasNext()) {
         int id = iterator.nextDoc();
diff --git solr/core/src/java/org/apache/solr/handler/RealTimeGetHandler.java solr/core/src/java/org/apache/solr/handler/RealTimeGetHandler.java
index 03ff291..76fc29a 100644
--- solr/core/src/java/org/apache/solr/handler/RealTimeGetHandler.java
+++ solr/core/src/java/org/apache/solr/handler/RealTimeGetHandler.java
@@ -28,7 +28,7 @@ public class RealTimeGetHandler extends SearchHandler {
   @Override
   protected List<String> getDefaultComponents()
   {
-    List<String> names = new ArrayList<String>(1);
+    List<String> names = new ArrayList<>(1);
     names.add(RealTimeGetComponent.COMPONENT_NAME);
     return names;
   }
diff --git solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java
index fbd8a38..03bcc8e 100644
--- solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java
+++ solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java
@@ -140,7 +140,7 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
 
   private String includeConfFiles;
 
-  private NamedList<String> confFileNameAlias = new NamedList<String>();
+  private NamedList<String> confFileNameAlias = new NamedList<>();
 
   private boolean isMaster = false;
 
@@ -156,7 +156,7 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
 
   private int numTimesReplicated = 0;
 
-  private final Map<String, FileInfo> confFileInfoCache = new HashMap<String, FileInfo>();
+  private final Map<String, FileInfo> confFileInfoCache = new HashMap<>();
 
   private Integer reserveCommitDuration = SnapPuller.readInterval("00:00:10");
 
@@ -270,14 +270,14 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
 
   private List<NamedList<Object>> getCommits() {
     Map<Long, IndexCommit> commits = core.getDeletionPolicy().getCommits();
-    List<NamedList<Object>> l = new ArrayList<NamedList<Object>>();
+    List<NamedList<Object>> l = new ArrayList<>();
 
     for (IndexCommit c : commits.values()) {
       try {
-        NamedList<Object> nl = new NamedList<Object>();
+        NamedList<Object> nl = new NamedList<>();
         nl.add("indexVersion", IndexDeletionPolicyWrapper.getCommitTimestamp(c));
         nl.add(GENERATION, c.getGeneration());
-        List<String> commitList = new ArrayList<String>(c.getFileNames().size());
+        List<String> commitList = new ArrayList<>(c.getFileNames().size());
         commitList.addAll(c.getFileNames());
         Collections.sort(commitList);
         nl.add(CMD_GET_FILE_LIST, commitList);
@@ -404,19 +404,19 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
     }
     // reserve the indexcommit for sometime
     core.getDeletionPolicy().setReserveDuration(gen, reserveCommitDuration);
-    List<Map<String, Object>> result = new ArrayList<Map<String, Object>>();
+    List<Map<String, Object>> result = new ArrayList<>();
     Directory dir = null;
     try {
       // get all the files in the commit
       // use a set to workaround possible Lucene bug which returns same file
       // name multiple times
-      Collection<String> files = new HashSet<String>(commit.getFileNames());
+      Collection<String> files = new HashSet<>(commit.getFileNames());
       dir = core.getDirectoryFactory().get(core.getNewIndexDir(), DirContext.DEFAULT, core.getSolrConfig().indexConfig.lockType);
       try {
         
         for (String fileName : files) {
           if (fileName.endsWith(".lock")) continue;
-          Map<String,Object> fileMeta = new HashMap<String,Object>();
+          Map<String,Object> fileMeta = new HashMap<>();
           fileMeta.put(NAME, fileName);
           fileMeta.put(SIZE, dir.fileLength(fileName));
           result.add(fileMeta);
@@ -446,7 +446,7 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
    */
   List<Map<String, Object>> getConfFileInfoFromCache(NamedList<String> nameAndAlias,
                                                      final Map<String, FileInfo> confFileInfoCache) {
-    List<Map<String, Object>> confFiles = new ArrayList<Map<String, Object>>();
+    List<Map<String, Object>> confFiles = new ArrayList<>();
     synchronized (confFileInfoCache) {
       File confDir = new File(core.getResourceLoader().getConfigDir());
       Checksum checksum = null;
@@ -482,7 +482,7 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
     }
 
     Map<String, Object> getAsMap() {
-      Map<String, Object> map = new HashMap<String, Object>();
+      Map<String, Object> map = new HashMap<>();
       map.put(NAME, name);
       map.put(SIZE, size);
       map.put(CHECKSUM, checksum);
@@ -608,9 +608,9 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
    * Used for showing statistics and progress information.
    */
   private NamedList<Object> getReplicationDetails(boolean showSlaveDetails) {
-    NamedList<Object> details = new SimpleOrderedMap<Object>();
-    NamedList<Object> master = new SimpleOrderedMap<Object>();
-    NamedList<Object> slave = new SimpleOrderedMap<Object>();
+    NamedList<Object> details = new SimpleOrderedMap<>();
+    NamedList<Object> master = new SimpleOrderedMap<>();
+    NamedList<Object> slave = new SimpleOrderedMap<>();
 
     details.add("indexSize", NumberUtils.readableSize(getIndexSize()));
     details.add("indexPath", core.getIndexDir());
@@ -677,7 +677,7 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
       if (isReplicating) {
         try {
           long bytesToDownload = 0;
-          List<String> filesToDownload = new ArrayList<String>();
+          List<String> filesToDownload = new ArrayList<>();
           for (Map<String, Object> file : snapPuller.getFilesToDownload()) {
             filesToDownload.add((String) file.get(NAME));
             bytesToDownload += (Long) file.get(SIZE);
@@ -694,7 +694,7 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
           slave.add("bytesToDownload", NumberUtils.readableSize(bytesToDownload));
 
           long bytesDownloaded = 0;
-          List<String> filesDownloaded = new ArrayList<String>();
+          List<String> filesDownloaded = new ArrayList<>();
           for (Map<String, Object> file : snapPuller.getFilesDownloaded()) {
             filesDownloaded.add((String) file.get(NAME));
             bytesDownloaded += (Long) file.get(SIZE);
@@ -776,7 +776,7 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
       } catch (NumberFormatException e) {/*no op*/ }
     } else if (clzz == List.class) {
       String ss[] = s.split(",");
-      List<String> l = new ArrayList<String>();
+      List<String> l = new ArrayList<>();
       for (int i = 0; i < ss.length; i++) {
         l.add(new Date(Long.valueOf(ss[i])).toString());
       }
@@ -788,7 +788,7 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
   }
 
   private List<String> getReplicateAfterStrings() {
-    List<String> replicateAfter = new ArrayList<String>();
+    List<String> replicateAfter = new ArrayList<>();
     if (replicateOnCommit)
       replicateAfter.add("commit");
     if (replicateOnOptimize)
diff --git solr/core/src/java/org/apache/solr/handler/RequestHandlerBase.java solr/core/src/java/org/apache/solr/handler/RequestHandlerBase.java
index 774ed18..dfe22a6 100644
--- solr/core/src/java/org/apache/solr/handler/RequestHandlerBase.java
+++ solr/core/src/java/org/apache/solr/handler/RequestHandlerBase.java
@@ -196,7 +196,7 @@ public abstract class RequestHandlerBase implements SolrRequestHandler, SolrInfo
 
   @Override
   public NamedList<Object> getStatistics() {
-    NamedList<Object> lst = new SimpleOrderedMap<Object>();
+    NamedList<Object> lst = new SimpleOrderedMap<>();
     Snapshot snapshot = requestTimes.getSnapshot();
     lst.add("handlerStart",handlerStart);
     lst.add("requests", numRequests.longValue());
diff --git solr/core/src/java/org/apache/solr/handler/RequestHandlerUtils.java solr/core/src/java/org/apache/solr/handler/RequestHandlerUtils.java
index fb5e8e4..2266b71 100644
--- solr/core/src/java/org/apache/solr/handler/RequestHandlerUtils.java
+++ solr/core/src/java/org/apache/solr/handler/RequestHandlerUtils.java
@@ -75,7 +75,7 @@ public class RequestHandlerUtils
   }
 
   
-  private static Set<String> commitParams = new HashSet<String>(Arrays.asList(new String[]{UpdateParams.OPEN_SEARCHER, UpdateParams.WAIT_SEARCHER, UpdateParams.SOFT_COMMIT, UpdateParams.EXPUNGE_DELETES, UpdateParams.MAX_OPTIMIZE_SEGMENTS, UpdateParams.PREPARE_COMMIT}));
+  private static Set<String> commitParams = new HashSet<>(Arrays.asList(new String[]{UpdateParams.OPEN_SEARCHER, UpdateParams.WAIT_SEARCHER, UpdateParams.SOFT_COMMIT, UpdateParams.EXPUNGE_DELETES, UpdateParams.MAX_OPTIMIZE_SEGMENTS, UpdateParams.PREPARE_COMMIT}));
 
   public static void validateCommitParams(SolrParams params) {
     Iterator<String> i = params.getParameterNamesIterator();
diff --git solr/core/src/java/org/apache/solr/handler/SnapPuller.java solr/core/src/java/org/apache/solr/handler/SnapPuller.java
index e9ee9ae..b4d080c 100644
--- solr/core/src/java/org/apache/solr/handler/SnapPuller.java
+++ solr/core/src/java/org/apache/solr/handler/SnapPuller.java
@@ -558,7 +558,7 @@ public class SnapPuller {
    * @throws IOException on IO error
    */
   private void logReplicationTimeAndConfFiles(Collection<Map<String, Object>> modifiedConfFiles, boolean successfulInstall) throws IOException {
-    List<String> confFiles = new ArrayList<String>();
+    List<String> confFiles = new ArrayList<>();
     if (modifiedConfFiles != null && !modifiedConfFiles.isEmpty())
       for (Map<String, Object> map1 : modifiedConfFiles)
         confFiles.add((String) map1.get(NAME));
@@ -641,7 +641,7 @@ public class SnapPuller {
 
   private StringBuilder readToStringBuilder(long replicationTime, String str) {
     StringBuilder sb = new StringBuilder();
-    List<String> l = new ArrayList<String>();
+    List<String> l = new ArrayList<>();
     if (str != null && str.length() != 0) {
       String[] ss = str.split(",");
       for (int i = 0; i < ss.length; i++) {
@@ -737,7 +737,7 @@ public class SnapPuller {
         localFileFetcher = new LocalFsFileFetcher(tmpconfDir, file, saveAs, true, latestGeneration);
         currentFile = file;
         localFileFetcher.fetchFile();
-        confFilesDownloaded.add(new HashMap<String, Object>(file));
+        confFilesDownloaded.add(new HashMap<>(file));
       }
       // this is called before copying the files to the original conf dir
       // so that if there is an exception avoid corrupting the original files.
@@ -769,7 +769,7 @@ public class SnapPuller {
             (String) file.get(NAME), false, latestGeneration);
         currentFile = file;
         dirFileFetcher.fetchFile();
-        filesDownloaded.add(new HashMap<String,Object>(file));
+        filesDownloaded.add(new HashMap<>(file));
       } else {
         LOG.info("Skipping download for " + file.get(NAME)
             + " because it already exists");
@@ -836,7 +836,7 @@ public class SnapPuller {
       }
     }
     String segmentsFile = null;
-    List<String> movedfiles = new ArrayList<String>();
+    List<String> movedfiles = new ArrayList<>();
     for (Map<String, Object> f : filesDownloaded) {
       String fname = (String) f.get(NAME);
       // the segments file must be copied last
@@ -973,7 +973,7 @@ public class SnapPuller {
     
   }
 
-  private final Map<String, FileInfo> confFileInfoCache = new HashMap<String, FileInfo>();
+  private final Map<String, FileInfo> confFileInfoCache = new HashMap<>();
 
   /**
    * The local conf files are compared with the conf files in the master. If they are same (by checksum) do not copy.
@@ -986,7 +986,7 @@ public class SnapPuller {
     if (confFilesToDownload == null || confFilesToDownload.isEmpty())
       return Collections.EMPTY_LIST;
     //build a map with alias/name as the key
-    Map<String, Map<String, Object>> nameVsFile = new HashMap<String, Map<String, Object>>();
+    Map<String, Map<String, Object>> nameVsFile = new HashMap<>();
     NamedList names = new NamedList();
     for (Map<String, Object> map : confFilesToDownload) {
       //if alias is present that is the name the file may have in the slave
@@ -1063,25 +1063,25 @@ public class SnapPuller {
     //make a copy first because it can be null later
     List<Map<String, Object>> tmp = confFilesToDownload;
     //create a new instance. or else iterator may fail
-    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<Map<String, Object>>(tmp);
+    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<>(tmp);
   }
 
   List<Map<String, Object>> getConfFilesDownloaded() {
     //make a copy first because it can be null later
     List<Map<String, Object>> tmp = confFilesDownloaded;
     // NOTE: it's safe to make a copy of a SynchronizedCollection(ArrayList)
-    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<Map<String, Object>>(tmp);
+    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<>(tmp);
   }
 
   List<Map<String, Object>> getFilesToDownload() {
     //make a copy first because it can be null later
     List<Map<String, Object>> tmp = filesToDownload;
-    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<Map<String, Object>>(tmp);
+    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<>(tmp);
   }
 
   List<Map<String, Object>> getFilesDownloaded() {
     List<Map<String, Object>> tmp = filesDownloaded;
-    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<Map<String, Object>>(tmp);
+    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<>(tmp);
   }
 
   // TODO: currently does not reflect conf files
@@ -1090,7 +1090,7 @@ public class SnapPuller {
     DirectoryFileFetcher tmpFileFetcher = dirFileFetcher;
     if (tmp == null)
       return null;
-    tmp = new HashMap<String, Object>(tmp);
+    tmp = new HashMap<>(tmp);
     if (tmpFileFetcher != null)
       tmp.put("bytesDownloaded", tmpFileFetcher.bytesDownloaded);
     return tmp;
diff --git solr/core/src/java/org/apache/solr/handler/SnapShooter.java solr/core/src/java/org/apache/solr/handler/SnapShooter.java
index ba88a3f..14df3d3 100644
--- solr/core/src/java/org/apache/solr/handler/SnapShooter.java
+++ solr/core/src/java/org/apache/solr/handler/SnapShooter.java
@@ -82,7 +82,7 @@ public class SnapShooter {
 
   void createSnapshot(final IndexCommit indexCommit, int numberToKeep, ReplicationHandler replicationHandler) {
     LOG.info("Creating backup snapshot...");
-    NamedList<Object> details = new NamedList<Object>();
+    NamedList<Object> details = new NamedList<>();
     details.add("startTime", new Date().toString());
     File snapShotDir = null;
     String directoryName = null;
@@ -131,7 +131,7 @@ public class SnapShooter {
   }
   private void deleteOldBackups(int numberToKeep) {
     File[] files = new File(snapDir).listFiles();
-    List<OldBackupDirectory> dirs = new ArrayList<OldBackupDirectory>();
+    List<OldBackupDirectory> dirs = new ArrayList<>();
     for(File f : files) {
       OldBackupDirectory obd = new OldBackupDirectory(f);
       if(obd.dir != null) {
diff --git solr/core/src/java/org/apache/solr/handler/UpdateRequestHandler.java solr/core/src/java/org/apache/solr/handler/UpdateRequestHandler.java
index aa46b2e..ed193bf 100644
--- solr/core/src/java/org/apache/solr/handler/UpdateRequestHandler.java
+++ solr/core/src/java/org/apache/solr/handler/UpdateRequestHandler.java
@@ -98,7 +98,7 @@ public class UpdateRequestHandler extends ContentStreamHandlerBase {
         String wt = loader.getDefaultWT();
         // Make sure it is a valid writer
         if(req.getCore().getQueryResponseWriter(wt)!=null) {
-          Map<String,String> map = new HashMap<String,String>(1);
+          Map<String,String> map = new HashMap<>(1);
           map.put(CommonParams.WT, wt);
           req.setParams(SolrParams.wrapDefaults(params, 
               new MapSolrParams(map)));
@@ -117,7 +117,7 @@ public class UpdateRequestHandler extends ContentStreamHandlerBase {
   
   protected void setAssumeContentType(String ct) {
     if(invariants==null) {
-      Map<String,String> map = new HashMap<String,String>();
+      Map<String,String> map = new HashMap<>();
       map.put(UpdateParams.ASSUME_CONTENT_TYPE,ct);
       invariants = new MapSolrParams(map);
     }
@@ -133,7 +133,7 @@ public class UpdateRequestHandler extends ContentStreamHandlerBase {
     if(args!=null) {
       p = SolrParams.toSolrParams(args);
     }
-    Map<String,ContentStreamLoader> registry = new HashMap<String,ContentStreamLoader>();
+    Map<String,ContentStreamLoader> registry = new HashMap<>();
     registry.put("application/xml", new XMLLoader().init(p) );
     registry.put("application/json", new JsonLoader().init(p) );
     registry.put("application/csv", new CSVLoader().init(p) );
diff --git solr/core/src/java/org/apache/solr/handler/admin/CollectionsHandler.java solr/core/src/java/org/apache/solr/handler/admin/CollectionsHandler.java
index e387a71..6326a1c 100644
--- solr/core/src/java/org/apache/solr/handler/admin/CollectionsHandler.java
+++ solr/core/src/java/org/apache/solr/handler/admin/CollectionsHandler.java
@@ -401,7 +401,7 @@ public class CollectionsHandler extends RequestHandlerBase {
   }
 
   private static void copyIfNotNull(SolrParams params, Map<String, Object> props, String... keys) {
-    ArrayList<String> prefixes = new ArrayList<String>(1);
+    ArrayList<String> prefixes = new ArrayList<>(1);
     if(keys !=null){
       for (String key : keys) {
         if(key.endsWith(".")) {
@@ -444,7 +444,7 @@ public class CollectionsHandler extends RequestHandlerBase {
     String name = req.getParams().required().get(ZkStateReader.COLLECTION_PROP);
     String shard = req.getParams().required().get(ZkStateReader.SHARD_ID_PROP);
     
-    Map<String,Object> props = new HashMap<String,Object>();
+    Map<String,Object> props = new HashMap<>();
     props.put(ZkStateReader.COLLECTION_PROP, name);
     props.put(Overseer.QUEUE_OPERATION, OverseerCollectionProcessor.DELETESHARD);
     props.put(ZkStateReader.SHARD_ID_PROP, shard);
@@ -473,7 +473,7 @@ public class CollectionsHandler extends RequestHandlerBase {
           "Only one of 'ranges' or 'split.key' should be specified");
     }
 
-    Map<String,Object> props = new HashMap<String,Object>();
+    Map<String,Object> props = new HashMap<>();
     props.put(Overseer.QUEUE_OPERATION, OverseerCollectionProcessor.SPLITSHARD);
     props.put("collection", name);
     if (shard != null)  {
@@ -495,7 +495,7 @@ public class CollectionsHandler extends RequestHandlerBase {
   private void handleMigrate(SolrQueryRequest req, SolrQueryResponse rsp) throws KeeperException, InterruptedException {
     log.info("Migrate action invoked: " + req.getParamString());
     req.getParams().required().check("collection", "split.key", "target.collection");
-    Map<String,Object> props = new HashMap<String,Object>();
+    Map<String,Object> props = new HashMap<>();
     props.put(Overseer.QUEUE_OPERATION, OverseerCollectionProcessor.MIGRATE);
     copyIfNotNull(req.getParams(), props, "collection", "split.key", "target.collection", "forward.timeout");
     ZkNodeProps m = new ZkNodeProps(props);
@@ -504,7 +504,7 @@ public class CollectionsHandler extends RequestHandlerBase {
 
   private void handleAddReplica(SolrQueryRequest req, SolrQueryResponse rsp) throws KeeperException, InterruptedException  {
     log.info("Add replica action invoked: " + req.getParamString());
-    Map<String,Object> props = new HashMap<String,Object>();
+    Map<String,Object> props = new HashMap<>();
     props.put(Overseer.QUEUE_OPERATION, CollectionAction.ADDREPLICA.toString());
     copyIfNotNull(req.getParams(), props, COLLECTION_PROP, "node", SHARD_ID_PROP, ShardParams._ROUTE_,
         CoreAdminParams.NAME, CoreAdminParams.INSTANCE_DIR, CoreAdminParams.DATA_DIR);
diff --git solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java
index 456dbb9..50165d2 100644
--- solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java
+++ solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java
@@ -255,7 +255,7 @@ public class CoreAdminHandler extends RequestHandlerBase {
       if (rangesArr.length == 0) {
         throw new SolrException(ErrorCode.BAD_REQUEST, "There must be at least one range specified to split an index");
       } else  {
-        ranges = new ArrayList<DocRouter.Range>(rangesArr.length);
+        ranges = new ArrayList<>(rangesArr.length);
         for (String r : rangesArr) {
           try {
             ranges.add(DocRouter.DEFAULT.fromString(r));
@@ -304,7 +304,7 @@ public class CoreAdminHandler extends RequestHandlerBase {
       }
 
       if (pathsArr == null) {
-        newCores = new ArrayList<SolrCore>(partitions);
+        newCores = new ArrayList<>(partitions);
         for (String newCoreName : newCoreNames) {
           SolrCore newcore = coreContainer.getCore(newCoreName);
           if (newcore != null) {
@@ -705,7 +705,7 @@ public class CoreAdminHandler extends RequestHandlerBase {
     String indexInfo = params.get(CoreAdminParams.INDEX_INFO);
     boolean isIndexInfoNeeded = Boolean.parseBoolean(null == indexInfo ? "true" : indexInfo);
     boolean doPersist = false;
-    NamedList<Object> status = new SimpleOrderedMap<Object>();
+    NamedList<Object> status = new SimpleOrderedMap<>();
     Map<String,Exception> allFailures = coreContainer.getCoreInitFailures();
     try {
       if (cname == null) {
@@ -832,7 +832,7 @@ public class CoreAdminHandler extends RequestHandlerBase {
       if (core != null) {
         syncStrategy = new SyncStrategy(core.getCoreDescriptor().getCoreContainer());
         
-        Map<String,Object> props = new HashMap<String,Object>();
+        Map<String,Object> props = new HashMap<>();
         props.put(ZkStateReader.BASE_URL_PROP, zkController.getBaseUrl());
         props.put(ZkStateReader.CORE_NAME_PROP, cname);
         props.put(ZkStateReader.NODE_NAME_PROP, zkController.getNodeName());
@@ -1077,7 +1077,7 @@ public class CoreAdminHandler extends RequestHandlerBase {
    * @throws IOException - LukeRequestHandler can throw an I/O exception
    */
   protected NamedList<Object> getCoreStatus(CoreContainer cores, String cname, boolean isIndexInfoNeeded)  throws IOException {
-    NamedList<Object> info = new SimpleOrderedMap<Object>();
+    NamedList<Object> info = new SimpleOrderedMap<>();
 
     if (!cores.isLoaded(cname)) { // Lazily-loaded core, fill in what we can.
       // It would be a real mistake to load the cores just to get the status
diff --git solr/core/src/java/org/apache/solr/handler/admin/LoggingHandler.java solr/core/src/java/org/apache/solr/handler/admin/LoggingHandler.java
index 6e26e86..4c005d4 100644
--- solr/core/src/java/org/apache/solr/handler/admin/LoggingHandler.java
+++ solr/core/src/java/org/apache/solr/handler/admin/LoggingHandler.java
@@ -118,7 +118,7 @@ public class LoggingHandler extends RequestHandlerBase implements SolrCoreAware
         return;
       }
       else {
-        SimpleOrderedMap<Object> info = new SimpleOrderedMap<Object>();
+        SimpleOrderedMap<Object> info = new SimpleOrderedMap<>();
         if(time>0) {
           info.add("since", time);
           info.add("found", found);
@@ -140,7 +140,7 @@ public class LoggingHandler extends RequestHandlerBase implements SolrCoreAware
       List<LoggerInfo> loggers = new ArrayList<LoggerInfo>(watcher.getAllLoggers());
       Collections.sort(loggers);
   
-      List<SimpleOrderedMap<?>> info = new ArrayList<SimpleOrderedMap<?>>();
+      List<SimpleOrderedMap<?>> info = new ArrayList<>();
       for(LoggerInfo wrap:loggers) {
         info.add(wrap.getInfo());
       }
diff --git solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java
index d521cb2..af4ebe6 100644
--- solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java
+++ solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java
@@ -147,7 +147,7 @@ public class LukeRequestHandler extends RequestHandlerBase
 
       SimpleOrderedMap<Object> info = getDocumentFieldsInfo( doc, docId, reader, schema );
 
-      SimpleOrderedMap<Object> docinfo = new SimpleOrderedMap<Object>();
+      SimpleOrderedMap<Object> docinfo = new SimpleOrderedMap<>();
       docinfo.add( "docId", docId );
       docinfo.add( "lucene", info );
       docinfo.add( "solr", doc );
@@ -161,7 +161,7 @@ public class LukeRequestHandler extends RequestHandlerBase
     }
 
     // Add some generally helpful information
-    NamedList<Object> info = new SimpleOrderedMap<Object>();
+    NamedList<Object> info = new SimpleOrderedMap<>();
     info.add( "key", getFieldFlagsKey() );
     info.add( "NOTE", "Document Frequency (df) is not updated when a document is marked for deletion.  df values include deleted documents." );
     rsp.add( "info", info );
@@ -241,7 +241,7 @@ public class LukeRequestHandler extends RequestHandlerBase
    * @return a key to what each character means
    */
   public static SimpleOrderedMap<String> getFieldFlagsKey() {
-    SimpleOrderedMap<String> key = new SimpleOrderedMap<String>();
+    SimpleOrderedMap<String> key = new SimpleOrderedMap<>();
     for (FieldFlag f : FieldFlag.values()) {
       key.add(String.valueOf(f.getAbbreviation()), f.getDisplay() );
     }
@@ -252,10 +252,10 @@ public class LukeRequestHandler extends RequestHandlerBase
                                                                  IndexSchema schema ) throws IOException
   {
     final CharsRef spare = new CharsRef();
-    SimpleOrderedMap<Object> finfo = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> finfo = new SimpleOrderedMap<>();
     for( Object o : doc.getFields() ) {
       Field field = (Field)o;
-      SimpleOrderedMap<Object> f = new SimpleOrderedMap<Object>();
+      SimpleOrderedMap<Object> f = new SimpleOrderedMap<>();
 
       SchemaField sfield = schema.getFieldOrNull( field.name() );
       FieldType ftype = (sfield==null)?null:sfield.getType();
@@ -283,7 +283,7 @@ public class LukeRequestHandler extends RequestHandlerBase
         try {
           Terms v = reader.getTermVector( docId, field.name() );
           if( v != null ) {
-            SimpleOrderedMap<Integer> tfv = new SimpleOrderedMap<Integer>();
+            SimpleOrderedMap<Integer> tfv = new SimpleOrderedMap<>();
             final TermsEnum termsEnum = v.iterator(null);
             BytesRef text;
             while((text = termsEnum.next()) != null) {
@@ -313,27 +313,27 @@ public class LukeRequestHandler extends RequestHandlerBase
     Set<String> fields = null;
     String fl = params.get(CommonParams.FL);
     if (fl != null) {
-      fields = new TreeSet<String>(Arrays.asList(fl.split( "[,\\s]+" )));
+      fields = new TreeSet<>(Arrays.asList(fl.split( "[,\\s]+" )));
     }
 
     AtomicReader reader = searcher.getAtomicReader();
     IndexSchema schema = searcher.getSchema();
 
     // Don't be tempted to put this in the loop below, the whole point here is to alphabetize the fields!
-    Set<String> fieldNames = new TreeSet<String>();
+    Set<String> fieldNames = new TreeSet<>();
     for(FieldInfo fieldInfo : reader.getFieldInfos()) {
       fieldNames.add(fieldInfo.name);
     }
 
     // Walk the term enum and keep a priority queue for each map in our set
-    SimpleOrderedMap<Object> finfo = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> finfo = new SimpleOrderedMap<>();
 
     for (String fieldName : fieldNames) {
       if (fields != null && ! fields.contains(fieldName) && ! fields.contains("*")) {
         continue; //we're not interested in this field Still an issue here
       }
 
-      SimpleOrderedMap<Object> fieldMap = new SimpleOrderedMap<Object>();
+      SimpleOrderedMap<Object> fieldMap = new SimpleOrderedMap<>();
 
       SchemaField sfield = schema.getFieldOrNull( fieldName );
       FieldType ftype = (sfield==null)?null:sfield.getType();
@@ -408,21 +408,21 @@ public class LukeRequestHandler extends RequestHandlerBase
    * Return info from the index
    */
   private static SimpleOrderedMap<Object> getSchemaInfo( IndexSchema schema ) {
-    Map<String, List<String>> typeusemap = new TreeMap<String, List<String>>();
-    Map<String, Object> fields = new TreeMap<String, Object>();
+    Map<String, List<String>> typeusemap = new TreeMap<>();
+    Map<String, Object> fields = new TreeMap<>();
     SchemaField uniqueField = schema.getUniqueKeyField();
     for( SchemaField f : schema.getFields().values() ) {
       populateFieldInfo(schema, typeusemap, fields, uniqueField, f);
     }
 
-    Map<String, Object> dynamicFields = new TreeMap<String, Object>();
+    Map<String, Object> dynamicFields = new TreeMap<>();
     for (SchemaField f : schema.getDynamicFieldPrototypes()) {
       populateFieldInfo(schema, typeusemap, dynamicFields, uniqueField, f);
     }
-    SimpleOrderedMap<Object> types = new SimpleOrderedMap<Object>();
-    Map<String, FieldType> sortedTypes = new TreeMap<String, FieldType>(schema.getFieldTypes());
+    SimpleOrderedMap<Object> types = new SimpleOrderedMap<>();
+    Map<String, FieldType> sortedTypes = new TreeMap<>(schema.getFieldTypes());
     for( FieldType ft : sortedTypes.values() ) {
-      SimpleOrderedMap<Object> field = new SimpleOrderedMap<Object>();
+      SimpleOrderedMap<Object> field = new SimpleOrderedMap<>();
       field.add("fields", typeusemap.get( ft.getTypeName() ) );
       field.add("tokenized", ft.isTokenized() );
       field.add("className", ft.getClass().getName());
@@ -433,15 +433,15 @@ public class LukeRequestHandler extends RequestHandlerBase
     }
 
     // Must go through this to maintain binary compatbility. Putting a TreeMap into a resp leads to casting errors
-    SimpleOrderedMap<Object> finfo = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> finfo = new SimpleOrderedMap<>();
 
-    SimpleOrderedMap<Object> fieldsSimple = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> fieldsSimple = new SimpleOrderedMap<>();
     for (Map.Entry<String, Object> ent : fields.entrySet()) {
       fieldsSimple.add(ent.getKey(), ent.getValue());
     }
     finfo.add("fields", fieldsSimple);
 
-    SimpleOrderedMap<Object> dynamicSimple = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> dynamicSimple = new SimpleOrderedMap<>();
     for (Map.Entry<String, Object> ent : dynamicFields.entrySet()) {
       dynamicSimple.add(ent.getKey(), ent.getValue());
     }
@@ -455,7 +455,7 @@ public class LukeRequestHandler extends RequestHandlerBase
   }
 
   private static SimpleOrderedMap<Object> getSimilarityInfo(Similarity similarity) {
-    SimpleOrderedMap<Object> toReturn = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> toReturn = new SimpleOrderedMap<>();
     if (similarity != null) {
       toReturn.add("className", similarity.getClass().getName());
       toReturn.add("details", similarity.toString());
@@ -464,16 +464,16 @@ public class LukeRequestHandler extends RequestHandlerBase
   }
 
   private static SimpleOrderedMap<Object> getAnalyzerInfo(Analyzer analyzer) {
-    SimpleOrderedMap<Object> aninfo = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> aninfo = new SimpleOrderedMap<>();
     aninfo.add("className", analyzer.getClass().getName());
     if (analyzer instanceof TokenizerChain) {
 
       TokenizerChain tchain = (TokenizerChain)analyzer;
 
       CharFilterFactory[] cfiltfacs = tchain.getCharFilterFactories();
-      SimpleOrderedMap<Map<String, Object>> cfilters = new SimpleOrderedMap<Map<String, Object>>();
+      SimpleOrderedMap<Map<String, Object>> cfilters = new SimpleOrderedMap<>();
       for (CharFilterFactory cfiltfac : cfiltfacs) {
-        Map<String, Object> tok = new HashMap<String, Object>();
+        Map<String, Object> tok = new HashMap<>();
         String className = cfiltfac.getClass().getName();
         tok.put("className", className);
         tok.put("args", cfiltfac.getOriginalArgs());
@@ -483,16 +483,16 @@ public class LukeRequestHandler extends RequestHandlerBase
         aninfo.add("charFilters", cfilters);
       }
 
-      SimpleOrderedMap<Object> tokenizer = new SimpleOrderedMap<Object>();
+      SimpleOrderedMap<Object> tokenizer = new SimpleOrderedMap<>();
       TokenizerFactory tfac = tchain.getTokenizerFactory();
       tokenizer.add("className", tfac.getClass().getName());
       tokenizer.add("args", tfac.getOriginalArgs());
       aninfo.add("tokenizer", tokenizer);
 
       TokenFilterFactory[] filtfacs = tchain.getTokenFilterFactories();
-      SimpleOrderedMap<Map<String, Object>> filters = new SimpleOrderedMap<Map<String, Object>>();
+      SimpleOrderedMap<Map<String, Object>> filters = new SimpleOrderedMap<>();
       for (TokenFilterFactory filtfac : filtfacs) {
-        Map<String, Object> tok = new HashMap<String, Object>();
+        Map<String, Object> tok = new HashMap<>();
         String className = filtfac.getClass().getName();
         tok.put("className", className);
         tok.put("args", filtfac.getOriginalArgs());
@@ -509,7 +509,7 @@ public class LukeRequestHandler extends RequestHandlerBase
                                         Map<String, List<String>> typeusemap, Map<String, Object> fields,
                                         SchemaField uniqueField, SchemaField f) {
     FieldType ft = f.getType();
-    SimpleOrderedMap<Object> field = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> field = new SimpleOrderedMap<>();
     field.add( "type", ft.getTypeName() );
     field.add( "flags", getFieldFlags(f) );
     if( f.isRequired() ) {
@@ -532,7 +532,7 @@ public class LukeRequestHandler extends RequestHandlerBase
 
     List<String> v = typeusemap.get( ft.getTypeName() );
     if( v == null ) {
-      v = new ArrayList<String>();
+      v = new ArrayList<>();
     }
     v.add( f.getName() );
     typeusemap.put( ft.getTypeName(), v );
@@ -550,7 +550,7 @@ public class LukeRequestHandler extends RequestHandlerBase
 
   public static SimpleOrderedMap<Object> getIndexInfo(DirectoryReader reader) throws IOException {
     Directory dir = reader.directory();
-    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<>();
 
     indexInfo.add("numDocs", reader.numDocs());
     indexInfo.add("maxDoc", reader.maxDoc());
@@ -638,14 +638,14 @@ public class LukeRequestHandler extends RequestHandlerBase
   }
 
   private static List<String> toListOfStrings(SchemaField[] raw) {
-    List<String> result = new ArrayList<String>(raw.length);
+    List<String> result = new ArrayList<>(raw.length);
     for (SchemaField f : raw) {
       result.add(f.getName());
     }
     return result;
   }
   private static List<String> toListOfStringDests(List<CopyField> raw) {
-    List<String> result = new ArrayList<String>(raw.size());
+    List<String> result = new ArrayList<>(raw.size());
     for (CopyField f : raw) {
       result.add(f.getDestination().getName());
     }
@@ -689,7 +689,7 @@ public class LukeRequestHandler extends RequestHandlerBase
     // TODO? should this be a list or a map?
     public NamedList<Integer> toNamedList()
     {
-      NamedList<Integer> nl = new NamedList<Integer>();
+      NamedList<Integer> nl = new NamedList<>();
       for( int bucket = 0; bucket <= _maxBucket; bucket++ ) {
         nl.add( ""+ (1 << bucket), _buckets[bucket] );
       }
@@ -733,12 +733,12 @@ public class LukeRequestHandler extends RequestHandlerBase
     public NamedList<Integer> toNamedList( IndexSchema schema )
     {
       // reverse the list..
-      List<TermInfo> aslist = new LinkedList<TermInfo>();
+      List<TermInfo> aslist = new LinkedList<>();
       while( size() > 0 ) {
         aslist.add( 0, (TermInfo)pop() );
       }
 
-      NamedList<Integer> list = new NamedList<Integer>();
+      NamedList<Integer> list = new NamedList<>();
       for (TermInfo i : aslist) {
         String txt = i.term.text();
         SchemaField ft = schema.getFieldOrNull( i.term.field() );
diff --git solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java
index c8a64f9..3a1f3e6 100644
--- solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java
+++ solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java
@@ -46,10 +46,10 @@ public class PluginInfoHandler extends RequestHandlerBase
   
   private static SimpleOrderedMap<Object> getSolrInfoBeans( SolrCore core, boolean stats )
   {
-    SimpleOrderedMap<Object> list = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> list = new SimpleOrderedMap<>();
     for (SolrInfoMBean.Category cat : SolrInfoMBean.Category.values()) 
     {
-      SimpleOrderedMap<Object> category = new SimpleOrderedMap<Object>();
+      SimpleOrderedMap<Object> category = new SimpleOrderedMap<>();
       list.add( cat.name(), category );
       Map<String, SolrInfoMBean> reg = core.getInfoRegistry();
       for (Map.Entry<String,SolrInfoMBean> entry : reg.entrySet()) {
@@ -57,7 +57,7 @@ public class PluginInfoHandler extends RequestHandlerBase
         if (m.getCategory() != cat) continue;
 
         String na = "Not Declared";
-        SimpleOrderedMap<Object> info = new SimpleOrderedMap<Object>();
+        SimpleOrderedMap<Object> info = new SimpleOrderedMap<>();
         category.add( entry.getKey(), info );
 
         info.add( "name",        (m.getName()       !=null ? m.getName()        : na) );
@@ -67,7 +67,7 @@ public class PluginInfoHandler extends RequestHandlerBase
 
         URL[] urls = m.getDocs();
         if ((urls != null) && (urls.length > 0)) {
-          ArrayList<String> docs = new ArrayList<String>(urls.length);
+          ArrayList<String> docs = new ArrayList<>(urls.length);
           for( URL u : urls ) {
             docs.add( u.toExternalForm() );
           }
diff --git solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java
index b8a7890..b6eb2b1 100644
--- solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java
+++ solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java
@@ -36,7 +36,7 @@ public class PropertiesRequestHandler extends RequestHandlerBase
     Object props = null;
     String name = req.getParams().get( "name" );
     if( name != null ) {
-      NamedList<String> p = new SimpleOrderedMap<String>();
+      NamedList<String> p = new SimpleOrderedMap<>();
       p.add( name, System.getProperty(name) );
       props = p;
     }
diff --git solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java
index d831425..74d965f 100644
--- solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java
+++ solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java
@@ -113,7 +113,7 @@ public class ShowFileRequestHandler extends RequestHandlerBase
 
   public static Set<String> initHidden(SolrParams invariants) {
 
-    Set<String> hiddenRet = new HashSet<String>();
+    Set<String> hiddenRet = new HashSet<>();
     // Build a list of hidden files
     if (invariants != null) {
       String[] hidden = invariants.getParams(HIDDEN);
@@ -155,13 +155,13 @@ public class ShowFileRequestHandler extends RequestHandlerBase
     List<String> children = zkClient.getChildren(adminFile, null, true);
     if (children.size() > 0) {
       
-      NamedList<SimpleOrderedMap<Object>> files = new SimpleOrderedMap<SimpleOrderedMap<Object>>();
+      NamedList<SimpleOrderedMap<Object>> files = new SimpleOrderedMap<>();
       for (String f : children) {
         if (isHiddenFile(req, rsp, f, false, hiddenFiles)) {
           continue;
         }
 
-        SimpleOrderedMap<Object> fileInfo = new SimpleOrderedMap<Object>();
+        SimpleOrderedMap<Object> fileInfo = new SimpleOrderedMap<>();
         files.add(f, fileInfo);
         List<String> fchildren = zkClient.getChildren(adminFile + "/" + f, null, true);
         if (fchildren.size() > 0) {
@@ -216,7 +216,7 @@ public class ShowFileRequestHandler extends RequestHandlerBase
     if( adminFile.isDirectory() ) {
       // it's really a directory, just go for it.
       int basePath = adminFile.getAbsolutePath().length() + 1;
-      NamedList<SimpleOrderedMap<Object>> files = new SimpleOrderedMap<SimpleOrderedMap<Object>>();
+      NamedList<SimpleOrderedMap<Object>> files = new SimpleOrderedMap<>();
       for( File f : adminFile.listFiles() ) {
         String path = f.getAbsolutePath().substring( basePath );
         path = path.replace( '\\', '/' ); // normalize slashes
@@ -225,7 +225,7 @@ public class ShowFileRequestHandler extends RequestHandlerBase
           continue;
         }
 
-        SimpleOrderedMap<Object> fileInfo = new SimpleOrderedMap<Object>();
+        SimpleOrderedMap<Object> fileInfo = new SimpleOrderedMap<>();
         files.add( path, fileInfo );
         if( f.isDirectory() ) {
           fileInfo.add( "directory", true ); 
diff --git solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java
index 33fc21c..3698440 100644
--- solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java
+++ solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java
@@ -52,7 +52,7 @@ public class SolrInfoMBeanHandler extends RequestHandlerBase {
    * Set is guarantee to never be null (but may be empty)
    */
   private Set<String> arrayToSet(Object[] arr) {
-    HashSet<String> r = new HashSet<String>();
+    HashSet<String> r = new HashSet<>();
     if (null == arr) return r;
     for (Object o : arr) {
       if (null != o) r.add(o.toString());
@@ -114,7 +114,7 @@ public class SolrInfoMBeanHandler extends RequestHandlerBase {
   
   protected NamedList<NamedList<NamedList<Object>>> getMBeanInfo(SolrQueryRequest req) {
 
-    NamedList<NamedList<NamedList<Object>>> cats = new NamedList<NamedList<NamedList<Object>>>();
+    NamedList<NamedList<NamedList<Object>>> cats = new NamedList<>();
     
     String[] requestedCats = req.getParams().getParams("cat");
     if (null == requestedCats || 0 == requestedCats.length) {
@@ -139,7 +139,7 @@ public class SolrInfoMBeanHandler extends RequestHandlerBase {
       NamedList<NamedList<Object>> catInfo = cats.get(m.getCategory().name());
       if ( null == catInfo ) continue;
 
-      NamedList<Object> mBeanInfo = new SimpleOrderedMap<Object>();
+      NamedList<Object> mBeanInfo = new SimpleOrderedMap<>();
       mBeanInfo.add("class", m.getName());
       mBeanInfo.add("version", m.getVersion());
       mBeanInfo.add("description", m.getDescription());
@@ -148,7 +148,7 @@ public class SolrInfoMBeanHandler extends RequestHandlerBase {
       // Use an external form
       URL[] urls = m.getDocs();
       if(urls!=null) {
-        List<String> docs = new ArrayList<String>(urls.length);
+        List<String> docs = new ArrayList<>(urls.length);
         for(URL url : urls) {
           docs.add(url.toExternalForm());
         }
@@ -168,7 +168,7 @@ public class SolrInfoMBeanHandler extends RequestHandlerBase {
       NamedList<NamedList<NamedList<Object>>> now,
       boolean includeAll ) {
     
-    NamedList<NamedList<NamedList<Object>>> changed = new NamedList<NamedList<NamedList<Object>>>();
+    NamedList<NamedList<NamedList<Object>>> changed = new NamedList<>();
     
     // Cycle through each category
     for(int i=0;i<ref.size();i++) {
@@ -182,7 +182,7 @@ public class SolrInfoMBeanHandler extends RequestHandlerBase {
           // Something in the category changed
           // Now iterate the real beans
           
-          NamedList<NamedList<Object>> cat = new SimpleOrderedMap<NamedList<Object>>();
+          NamedList<NamedList<Object>> cat = new SimpleOrderedMap<>();
           for(int j=0;j<ref_cat.size();j++) {
             String name = ref_cat.getName(j);
             NamedList<Object> ref_bean = ref_cat.get(name);
diff --git solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java
index 929b307..b7367b2 100644
--- solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java
+++ solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java
@@ -112,7 +112,7 @@ public class SystemInfoHandler extends RequestHandlerBase
    * Get system info
    */
   private SimpleOrderedMap<Object> getCoreInfo( SolrCore core, IndexSchema schema ) {
-    SimpleOrderedMap<Object> info = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> info = new SimpleOrderedMap<>();
     
     info.add( "schema", schema != null ? schema.getSchemaName():"no schema!" );
     
@@ -126,7 +126,7 @@ public class SystemInfoHandler extends RequestHandlerBase
     info.add( "start", new Date(core.getStartTime()) );
 
     // Solr Home
-    SimpleOrderedMap<Object> dirs = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> dirs = new SimpleOrderedMap<>();
     dirs.add( "cwd" , new File( System.getProperty("user.dir")).getAbsolutePath() );
     dirs.add( "instance", new File( core.getResourceLoader().getInstanceDir() ).getAbsolutePath() );
     try {
@@ -150,7 +150,7 @@ public class SystemInfoHandler extends RequestHandlerBase
    * Get system info
    */
   public static SimpleOrderedMap<Object> getSystemInfo() {
-    SimpleOrderedMap<Object> info = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> info = new SimpleOrderedMap<>();
     
     OperatingSystemMXBean os = ManagementFactory.getOperatingSystemMXBean();
     info.add( "name", os.getName() );
@@ -240,7 +240,7 @@ public class SystemInfoHandler extends RequestHandlerBase
    */
   public static SimpleOrderedMap<Object> getJvmInfo()
   {
-    SimpleOrderedMap<Object> jvm = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> jvm = new SimpleOrderedMap<>();
 
     final String javaVersion = System.getProperty("java.specification.version", "unknown"); 
     final String javaVendor = System.getProperty("java.specification.vendor", "unknown"); 
@@ -256,16 +256,16 @@ public class SystemInfoHandler extends RequestHandlerBase
     jvm.add( "name", jreVendor + " " + vmName );
     
     // details
-    SimpleOrderedMap<Object> java = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> java = new SimpleOrderedMap<>();
     java.add( "vendor", javaVendor );
     java.add( "name", javaName );
     java.add( "version", javaVersion );
     jvm.add( "spec", java );
-    SimpleOrderedMap<Object> jre = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> jre = new SimpleOrderedMap<>();
     jre.add( "vendor", jreVendor );
     jre.add( "version", jreVersion );
     jvm.add( "jre", jre );
-    SimpleOrderedMap<Object> vm = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> vm = new SimpleOrderedMap<>();
     vm.add( "vendor", vmVendor );
     vm.add( "name", vmName );
     vm.add( "version", vmVersion );
@@ -278,8 +278,8 @@ public class SystemInfoHandler extends RequestHandlerBase
     // not thread safe, but could be thread local
     DecimalFormat df = new DecimalFormat("#.#", DecimalFormatSymbols.getInstance(Locale.ROOT));
 
-    SimpleOrderedMap<Object> mem = new SimpleOrderedMap<Object>();
-    SimpleOrderedMap<Object> raw = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> mem = new SimpleOrderedMap<>();
+    SimpleOrderedMap<Object> raw = new SimpleOrderedMap<>();
     long free = runtime.freeMemory();
     long max = runtime.maxMemory();
     long total = runtime.totalMemory();
@@ -300,7 +300,7 @@ public class SystemInfoHandler extends RequestHandlerBase
     jvm.add("memory", mem);
 
     // JMX properties -- probably should be moved to a different handler
-    SimpleOrderedMap<Object> jmx = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> jmx = new SimpleOrderedMap<>();
     try{
       RuntimeMXBean mx = ManagementFactory.getRuntimeMXBean();
       jmx.add( "bootclasspath", mx.getBootClassPath());
@@ -322,7 +322,7 @@ public class SystemInfoHandler extends RequestHandlerBase
   }
   
   private static SimpleOrderedMap<Object> getLuceneInfo() {
-    SimpleOrderedMap<Object> info = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> info = new SimpleOrderedMap<>();
 
     Package p = SolrCore.class.getPackage();
 
diff --git solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java
index 1af19bf..997ac53 100644
--- solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java
+++ solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java
@@ -38,13 +38,13 @@ public class ThreadDumpHandler extends RequestHandlerBase
   @Override
   public void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throws IOException 
   {    
-    SimpleOrderedMap<Object> system = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> system = new SimpleOrderedMap<>();
     rsp.add( "system", system );
 
     ThreadMXBean tmbean = ManagementFactory.getThreadMXBean();
     
     // Thread Count
-    SimpleOrderedMap<Object> nl = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> nl = new SimpleOrderedMap<>();
     nl.add( "current",tmbean.getThreadCount() );
     nl.add( "peak", tmbean.getPeakThreadCount() );
     nl.add( "daemon", tmbean.getDaemonThreadCount() );
@@ -55,7 +55,7 @@ public class ThreadDumpHandler extends RequestHandlerBase
     long[] tids = tmbean.findMonitorDeadlockedThreads();
     if (tids != null) {
       tinfos = tmbean.getThreadInfo(tids, Integer.MAX_VALUE);
-      NamedList<SimpleOrderedMap<Object>> lst = new NamedList<SimpleOrderedMap<Object>>();
+      NamedList<SimpleOrderedMap<Object>> lst = new NamedList<>();
       for (ThreadInfo ti : tinfos) {
         if (ti != null) {
           lst.add( "thread", getThreadInfo( ti, tmbean ) );
@@ -67,7 +67,7 @@ public class ThreadDumpHandler extends RequestHandlerBase
     // Now show all the threads....
     tids = tmbean.getAllThreadIds();
     tinfos = tmbean.getThreadInfo(tids, Integer.MAX_VALUE);
-    NamedList<SimpleOrderedMap<Object>> lst = new NamedList<SimpleOrderedMap<Object>>();
+    NamedList<SimpleOrderedMap<Object>> lst = new NamedList<>();
     for (ThreadInfo ti : tinfos) {
       if (ti != null) {
         lst.add( "thread", getThreadInfo( ti, tmbean ) );
@@ -81,7 +81,7 @@ public class ThreadDumpHandler extends RequestHandlerBase
   //--------------------------------------------------------------------------------
   
   private static SimpleOrderedMap<Object> getThreadInfo( ThreadInfo ti, ThreadMXBean tmbean ) {
-    SimpleOrderedMap<Object> info = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> info = new SimpleOrderedMap<>();
     long tid = ti.getThreadId();
 
     info.add( "id", tid );
@@ -104,7 +104,7 @@ public class ThreadDumpHandler extends RequestHandlerBase
     }
 
     if (ti.getLockOwnerName() != null) {
-      SimpleOrderedMap<Object> owner = new SimpleOrderedMap<Object>();
+      SimpleOrderedMap<Object> owner = new SimpleOrderedMap<>();
       owner.add( "name", ti.getLockOwnerName() );
       owner.add( "id", ti.getLockOwnerId() );
     }
diff --git solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java
index 5a35a03..b755e8e 100644
--- solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java
+++ solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java
@@ -106,7 +106,7 @@ public class DebugComponent extends SearchComponent
       if (null != rb.getDebugInfo() ) {
         if (rb.isDebugQuery() && null != rb.getFilters() ) {
           info.add("filter_queries",rb.req.getParams().getParams(FQ));
-          List<String> fqs = new ArrayList<String>(rb.getFilters().size());
+          List<String> fqs = new ArrayList<>(rb.getFilters().size());
           for (Query fq : rb.getFilters()) {
             fqs.add(QueryParsing.toString(fq, rb.req.getSchema()));
           }
@@ -175,7 +175,7 @@ public class DebugComponent extends SearchComponent
       @SuppressWarnings("unchecked")
       NamedList<Object> stageList = (NamedList<Object>) ((NamedList<Object>)rb.getDebugInfo().get("track")).get(stages.get(rb.stage));
       if(stageList == null) {
-        stageList = new NamedList<Object>();
+        stageList = new NamedList<>();
         rb.addDebug(stageList, "track", stages.get(rb.stage));
       }
       for(ShardResponse response: sreq.responses) {
@@ -184,7 +184,7 @@ public class DebugComponent extends SearchComponent
     }
   }
 
-  private Set<String> excludeSet = new HashSet<String>(Arrays.asList("explain"));
+  private Set<String> excludeSet = new HashSet<>(Arrays.asList("explain"));
 
   @Override
   public void finishStage(ResponseBuilder rb) {
@@ -207,19 +207,19 @@ public class DebugComponent extends SearchComponent
               // TODO: lookup won't work for non-string ids... String vs Float
               ShardDoc sdoc = rb.resultIds.get(id);
               int idx = sdoc.positionInResponse;
-              arr[idx] = new NamedList.NamedListEntry<Object>(id, sexplain.getVal(i));
+              arr[idx] = new NamedList.NamedListEntry<>(id, sexplain.getVal(i));
             }
           }
         }
       }
 
       if (rb.isDebugResults()) {
-        explain = SolrPluginUtils.removeNulls(new SimpleOrderedMap<Object>(arr));
+        explain = SolrPluginUtils.removeNulls(new SimpleOrderedMap<>(arr));
       }
 
       if (info == null) {
         // No responses were received from shards. Show local query info.
-        info = new SimpleOrderedMap<Object>();
+        info = new SimpleOrderedMap<>();
         SolrPluginUtils.doStandardQueryDebug(
                 rb.req, rb.getQueryString(),  rb.getQuery(), rb.isDebugQuery(), info);
         if (rb.isDebugQuery() && rb.getQparser() != null) {
@@ -243,7 +243,7 @@ public class DebugComponent extends SearchComponent
 
 
   private NamedList<String> getTrackResponse(ShardResponse shardResponse) {
-    NamedList<String> namedList = new NamedList<String>();
+    NamedList<String> namedList = new NamedList<>();
     NamedList<Object> responseNL = shardResponse.getSolrResponse().getResponse();
     @SuppressWarnings("unchecked")
     NamedList<Object> responseHeader = (NamedList<Object>)responseNL.get("responseHeader");
@@ -295,7 +295,7 @@ public class DebugComponent extends SearchComponent
 
 
     if (source instanceof NamedList && dest instanceof NamedList) {
-      NamedList<Object> tmp = new NamedList<Object>();
+      NamedList<Object> tmp = new NamedList<>();
       @SuppressWarnings("unchecked")
       NamedList<Object> sl = (NamedList<Object>)source;
       @SuppressWarnings("unchecked")
@@ -329,7 +329,7 @@ public class DebugComponent extends SearchComponent
     }
 
     // merge unlike elements in a list
-    List<Object> t = new ArrayList<Object>();
+    List<Object> t = new ArrayList<>();
     t.add(dest);
     t.add(source);
     return t;
diff --git solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java
index cb952f6..841540a 100644
--- solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java
+++ solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java
@@ -141,7 +141,7 @@ public class FacetComponent extends SearchComponent
           }
 
           if (refinements == null) {
-            refinements = new ArrayList<String>();
+            refinements = new ArrayList<>();
           }
 
           refinements.add(facetCommand);
@@ -470,7 +470,7 @@ public class FacetComponent extends SearchComponent
               dff.needRefinements = true;
               List<String> lst = dff._toRefine[shardNum];
               if (lst == null) {
-                lst = dff._toRefine[shardNum] = new ArrayList<String>();
+                lst = dff._toRefine[shardNum] = new ArrayList<>();
               }
               lst.add(sfc.name);
             }
@@ -526,19 +526,19 @@ public class FacetComponent extends SearchComponent
 
     FacetInfo fi = rb._facetInfo;
 
-    NamedList<Object> facet_counts = new SimpleOrderedMap<Object>();
+    NamedList<Object> facet_counts = new SimpleOrderedMap<>();
 
-    NamedList<Number> facet_queries = new SimpleOrderedMap<Number>();
+    NamedList<Number> facet_queries = new SimpleOrderedMap<>();
     facet_counts.add("facet_queries",facet_queries);
     for (QueryFacet qf : fi.queryFacets.values()) {
       facet_queries.add(qf.getKey(), num(qf.count));
     }
 
-    NamedList<Object> facet_fields = new SimpleOrderedMap<Object>();
+    NamedList<Object> facet_fields = new SimpleOrderedMap<>();
     facet_counts.add("facet_fields", facet_fields);
 
     for (DistribFieldFacet dff : fi.facets.values()) {
-      NamedList<Object> fieldCounts = new NamedList<Object>(); // order is more important for facets
+      NamedList<Object> fieldCounts = new NamedList<>(); // order is more important for facets
       facet_fields.add(dff.getKey(), fieldCounts);
 
       ShardFacetCount[] counts;
@@ -634,13 +634,13 @@ public class FacetComponent extends SearchComponent
     public LinkedHashMap<String,QueryFacet> queryFacets;
     public LinkedHashMap<String,DistribFieldFacet> facets;
     public SimpleOrderedMap<SimpleOrderedMap<Object>> dateFacets
-      = new SimpleOrderedMap<SimpleOrderedMap<Object>>();
+      = new SimpleOrderedMap<>();
     public SimpleOrderedMap<SimpleOrderedMap<Object>> rangeFacets
-      = new SimpleOrderedMap<SimpleOrderedMap<Object>>();
+      = new SimpleOrderedMap<>();
 
     void parse(SolrParams params, ResponseBuilder rb) {
-      queryFacets = new LinkedHashMap<String,QueryFacet>();
-      facets = new LinkedHashMap<String,DistribFieldFacet>();
+      queryFacets = new LinkedHashMap<>();
+      facets = new LinkedHashMap<>();
 
       String[] facetQs = params.getParams(FacetParams.FACET_QUERY);
       if (facetQs != null) {
@@ -766,7 +766,7 @@ public class FacetComponent extends SearchComponent
     // the max possible count for a missing term for each shard (indexed by shardNum)
     public long[] missingMax;
     public FixedBitSet[] counted; // a bitset for each shard, keeping track of which terms seen
-    public HashMap<String,ShardFacetCount> counts = new HashMap<String,ShardFacetCount>(128);
+    public HashMap<String,ShardFacetCount> counts = new HashMap<>(128);
     public int termNum;
 
     public int initialLimit;     // how many terms requested in first phase
diff --git solr/core/src/java/org/apache/solr/handler/component/FieldFacetStats.java solr/core/src/java/org/apache/solr/handler/component/FieldFacetStats.java
index 0a8703e..70d0f38 100644
--- solr/core/src/java/org/apache/solr/handler/component/FieldFacetStats.java
+++ solr/core/src/java/org/apache/solr/handler/component/FieldFacetStats.java
@@ -73,8 +73,8 @@ public class FieldFacetStats {
     topLevelReader = searcher.getAtomicReader();
     valueSource = facet_sf.getType().getValueSource(facet_sf, null);
 
-    facetStatsValues = new HashMap<String, StatsValues>();
-    facetStatsTerms = new ArrayList<HashMap<String, Integer>>();
+    facetStatsValues = new HashMap<>();
+    facetStatsTerms = new ArrayList<>();
   }
 
   private StatsValues getStatsValues(String key) throws IOException {
diff --git solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java
index 9e17ab5..fea94e6 100644
--- solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java
+++ solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java
@@ -182,7 +182,7 @@ public class HighlightComponent extends SearchComponent implements PluginInfoIni
             String id = hl.getName(i);
             ShardDoc sdoc = rb.resultIds.get(id);
             int idx = sdoc.positionInResponse;
-            arr[idx] = new NamedList.NamedListEntry<Object>(id, hl.getVal(i));
+            arr[idx] = new NamedList.NamedListEntry<>(id, hl.getVal(i));
           }
         }
       }
diff --git solr/core/src/java/org/apache/solr/handler/component/HttpShardHandler.java solr/core/src/java/org/apache/solr/handler/component/HttpShardHandler.java
index 7362daa..509b2d8 100644
--- solr/core/src/java/org/apache/solr/handler/component/HttpShardHandler.java
+++ solr/core/src/java/org/apache/solr/handler/component/HttpShardHandler.java
@@ -68,12 +68,12 @@ public class HttpShardHandler extends ShardHandler {
     this.httpClient = httpClient;
     this.httpShardHandlerFactory = httpShardHandlerFactory;
     completionService = httpShardHandlerFactory.newCompletionService();
-    pending = new HashSet<Future<ShardResponse>>();
+    pending = new HashSet<>();
 
     // maps "localhost:8983|localhost:7574" to a shuffled List("http://localhost:8983","http://localhost:7574")
     // This is primarily to keep track of what order we should use to query the replicas of a shard
     // so that we use the same replica for all phases of a distributed request.
-    shardToURLs = new HashMap<String,List<String>>();
+    shardToURLs = new HashMap<>();
 
   }
 
@@ -285,7 +285,7 @@ public class HttpShardHandler extends ShardHandler {
         if(shardKeys == null) shardKeys = params.get(ShardParams.SHARD_KEYS);//eprecated
 
         // This will be the complete list of slices we need to query for this request.
-        slices = new HashMap<String,Slice>();
+        slices = new HashMap<>();
 
         // we need to find out what collections this request is for.
 
diff --git solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java
index 4764cff..c53b542 100644
--- solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java
+++ solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java
@@ -100,7 +100,7 @@ public class MoreLikeThisComponent extends SearchComponent {
           NamedList<BooleanQuery> bQuery = mlt.getMoreLikeTheseQuery(rb
               .getResults().docList);
           
-          NamedList<String> temp = new NamedList<String>();
+          NamedList<String> temp = new NamedList<>();
           Iterator<Entry<String,BooleanQuery>> idToQueryIt = bQuery.iterator();
 
           
@@ -164,7 +164,7 @@ public class MoreLikeThisComponent extends SearchComponent {
     // segment ahead of result/response.
     if (rb.stage == ResponseBuilder.STAGE_GET_FIELDS
         && rb.req.getParams().getBool(COMPONENT_NAME, false)) {
-      Map<Object,SolrDocumentList> tempResults = new LinkedHashMap<Object,SolrDocumentList>();
+      Map<Object,SolrDocumentList> tempResults = new LinkedHashMap<>();
       
       int mltcount = rb.req.getParams().getInt(MoreLikeThisParams.DOC_COUNT, 5);
       String keyName = rb.req.getSchema().getUniqueKeyField().getName();
@@ -221,8 +221,8 @@ public class MoreLikeThisComponent extends SearchComponent {
    */
   NamedList<SolrDocumentList> buildMoreLikeThisNamed(
       Map<Object,SolrDocumentList> allMlt, Map<Object,ShardDoc> resultIds) {
-    NamedList<SolrDocumentList> result = new NamedList<SolrDocumentList>();
-    TreeMap<Integer,Object> sortingMap = new TreeMap<Integer,Object>();
+    NamedList<SolrDocumentList> result = new NamedList<>();
+    TreeMap<Integer,Object> sortingMap = new TreeMap<>();
     for (Entry<Object,ShardDoc> next : resultIds.entrySet()) {
       sortingMap.put(next.getValue().positionInResponse, next.getKey());
     }
@@ -241,10 +241,10 @@ public class MoreLikeThisComponent extends SearchComponent {
   public SolrDocumentList mergeSolrDocumentList(SolrDocumentList one,
       SolrDocumentList two, int maxSize, String idField) {
 
-    List<SolrDocument> l = new ArrayList<SolrDocument>();
+    List<SolrDocument> l = new ArrayList<>();
     
     // De-dup records sets. Shouldn't happen if indexed correctly.
-    Map<String,SolrDocument> map = new HashMap<String,SolrDocument>();
+    Map<String,SolrDocument> map = new HashMap<>();
     for (SolrDocument doc : one) {
       Object id = doc.getFieldValue(idField);
       assert id != null : doc.toString();
@@ -254,7 +254,7 @@ public class MoreLikeThisComponent extends SearchComponent {
       map.put(doc.getFieldValue(idField).toString(), doc);
     }
     
-    l = new ArrayList<SolrDocument>(map.values());
+    l = new ArrayList<>(map.values());
     
     // Comparator to sort docs based on score. null scores/docs are set to 0.
     
@@ -352,12 +352,12 @@ public class MoreLikeThisComponent extends SearchComponent {
     IndexSchema schema = searcher.getSchema();
     MoreLikeThisHandler.MoreLikeThisHelper mltHelper = new MoreLikeThisHandler.MoreLikeThisHelper(
         p, searcher);
-    NamedList<DocList> mlt = new SimpleOrderedMap<DocList>();
+    NamedList<DocList> mlt = new SimpleOrderedMap<>();
     DocIterator iterator = docs.iterator();
     
     SimpleOrderedMap<Object> dbg = null;
     if (rb.isDebug()) {
-      dbg = new SimpleOrderedMap<Object>();
+      dbg = new SimpleOrderedMap<>();
     }
     
     while (iterator.hasNext()) {
@@ -369,12 +369,12 @@ public class MoreLikeThisComponent extends SearchComponent {
       mlt.add(name, sim.docList);
       
       if (dbg != null) {
-        SimpleOrderedMap<Object> docDbg = new SimpleOrderedMap<Object>();
+        SimpleOrderedMap<Object> docDbg = new SimpleOrderedMap<>();
         docDbg.add("rawMLTQuery", mltHelper.getRawMLTQuery().toString());
         docDbg
             .add("boostedMLTQuery", mltHelper.getBoostedMLTQuery().toString());
         docDbg.add("realMLTQuery", mltHelper.getRealMLTQuery().toString());
-        SimpleOrderedMap<Object> explains = new SimpleOrderedMap<Object>();
+        SimpleOrderedMap<Object> explains = new SimpleOrderedMap<>();
         DocIterator mltIte = sim.docList.iterator();
         while (mltIte.hasNext()) {
           int mltid = mltIte.nextDoc();
diff --git solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java
index 7606adc..d13eefc 100644
--- solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java
+++ solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java
@@ -60,7 +60,7 @@ public class PivotFacetHelper extends SimpleFacets
     if (!rb.doFacets || pivots == null) 
       return null;
 
-    SimpleOrderedMap<List<NamedList<Object>>> pivotResponse = new SimpleOrderedMap<List<NamedList<Object>>>();
+    SimpleOrderedMap<List<NamedList<Object>>> pivotResponse = new SimpleOrderedMap<>();
     for (String pivot : pivots) {
       //ex: pivot == "features,cat" or even "{!ex=mytag}features,cat"
       try {
@@ -79,7 +79,7 @@ public class PivotFacetHelper extends SimpleFacets
 
       String field = fields[0];
       String subField = fields[1];
-      Deque<String> fnames = new LinkedList<String>();
+      Deque<String> fnames = new LinkedList<>();
       for( int i=fields.length-1; i>1; i-- ) {
         fnames.push( fields[i] );
       }
@@ -106,7 +106,7 @@ public class PivotFacetHelper extends SimpleFacets
 
     String nextField = fnames.poll();
 
-    List<NamedList<Object>> values = new ArrayList<NamedList<Object>>( superFacets.size() );
+    List<NamedList<Object>> values = new ArrayList<>( superFacets.size() );
     for (Map.Entry<String, Integer> kv : superFacets) {
       // Only sub-facet if parent facet has positive count - still may not be any values for the sub-field though
       if (kv.getValue() >= minMatch) {
@@ -118,7 +118,7 @@ public class PivotFacetHelper extends SimpleFacets
         // constructing Term objects used in TermQueries that may be cached.
         BytesRef termval = null;
 
-        SimpleOrderedMap<Object> pivot = new SimpleOrderedMap<Object>();
+        SimpleOrderedMap<Object> pivot = new SimpleOrderedMap<>();
         pivot.add( "field", field );
         if (null == fieldValue) {
           pivot.add( "value", null );
diff --git solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java
index e2c5ba8..e20d5d8 100644
--- solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java
+++ solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java
@@ -163,7 +163,7 @@ public class QueryComponent extends SearchComponent
       if (fqs!=null && fqs.length!=0) {
         List<Query> filters = rb.getFilters();
         // if filters already exists, make a copy instead of modifying the original
-        filters = filters == null ? new ArrayList<Query>(fqs.length) : new ArrayList<Query>(filters);
+        filters = filters == null ? new ArrayList<Query>(fqs.length) : new ArrayList<>(filters);
         for (String fq : fqs) {
           if (fq != null && fq.trim().length()!=0) {
             QParser fqp = QParser.getParser(fq, null, req);
@@ -291,7 +291,7 @@ public class QueryComponent extends SearchComponent
       res.docList = new DocSlice(0, docs, luceneIds, null, docs, 0);
       if (rb.isNeedDocSet()) {
         // TODO: create a cache for this!
-        List<Query> queries = new ArrayList<Query>();
+        List<Query> queries = new ArrayList<>();
         queries.add(rb.getQuery());
         List<Query> filters = rb.getFilters();
         if (filters != null) queries.addAll(filters);
@@ -353,9 +353,9 @@ public class QueryComponent extends SearchComponent
               topGroupsParam = new String[0];
             }
 
-            List<SearchGroup<BytesRef>> topGroups = new ArrayList<SearchGroup<BytesRef>>(topGroupsParam.length);
+            List<SearchGroup<BytesRef>> topGroups = new ArrayList<>(topGroupsParam.length);
             for (String topGroup : topGroupsParam) {
-              SearchGroup<BytesRef> searchGroup = new SearchGroup<BytesRef>();
+              SearchGroup<BytesRef> searchGroup = new SearchGroup<>();
               if (!topGroup.equals(TopGroupsShardRequestFactory.GROUP_NULL_VALUE)) {
                 searchGroup.groupValue = new BytesRef(searcher.getSchema().getField(field).getType().readableToIndexed(topGroup));
               }
@@ -488,7 +488,7 @@ public class QueryComponent extends SearchComponent
     // TODO: See SOLR-5595
     boolean fsv = req.getParams().getBool(ResponseBuilder.FIELD_SORT_VALUES,false);
     if(fsv){
-      NamedList<Object[]> sortVals = new NamedList<Object[]>(); // order is important for the sort fields
+      NamedList<Object[]> sortVals = new NamedList<>(); // order is important for the sort fields
       IndexReaderContext topReaderContext = searcher.getTopReaderContext();
       List<AtomicReaderContext> leaves = topReaderContext.leaves();
       AtomicReaderContext currentLeaf = null;
@@ -714,7 +714,7 @@ public class QueryComponent extends SearchComponent
       for (String field : groupSpec.getFields()) {
         rb.mergedTopGroups.put(field, new TopGroups(null, null, 0, 0, new GroupDocs[]{}, Float.NaN));
       }
-      rb.resultIds = new HashMap<Object, ShardDoc>();
+      rb.resultIds = new HashMap<>();
     }
 
     EndResultTransformer.SolrDocumentSource solrDocumentSource = new EndResultTransformer.SolrDocumentSource() {
@@ -736,7 +736,7 @@ public class QueryComponent extends SearchComponent
     } else {
       return;
     }
-    Map<String, Object> combinedMap = new LinkedHashMap<String, Object>();
+    Map<String, Object> combinedMap = new LinkedHashMap<>();
     combinedMap.putAll(rb.mergedTopGroups);
     combinedMap.putAll(rb.mergedQueryCommandResults);
     endResultTransformer.transform(combinedMap, rb, solrDocumentSource);
@@ -835,7 +835,7 @@ public class QueryComponent extends SearchComponent
 
 
       // id to shard mapping, to eliminate any accidental dups
-      HashMap<Object,String> uniqueDoc = new HashMap<Object,String>();    
+      HashMap<Object,String> uniqueDoc = new HashMap<>();
 
       // Merge the docs via a priority queue so we don't have to sort *all* of the
       // documents... we only need to order the top (rows+start)
@@ -844,7 +844,7 @@ public class QueryComponent extends SearchComponent
 
       NamedList<Object> shardInfo = null;
       if(rb.req.getParams().getBool(ShardParams.SHARDS_INFO, false)) {
-        shardInfo = new SimpleOrderedMap<Object>();
+        shardInfo = new SimpleOrderedMap<>();
         rb.rsp.getValues().add(ShardParams.SHARDS_INFO,shardInfo);
       }
       
@@ -855,7 +855,7 @@ public class QueryComponent extends SearchComponent
         SolrDocumentList docs = null;
 
         if(shardInfo!=null) {
-          SimpleOrderedMap<Object> nl = new SimpleOrderedMap<Object>();
+          SimpleOrderedMap<Object> nl = new SimpleOrderedMap<>();
           
           if (srsp.getException() != null) {
             Throwable t = srsp.getException();
@@ -952,7 +952,7 @@ public class QueryComponent extends SearchComponent
       int resultSize = queue.size() - ss.getOffset();
       resultSize = Math.max(0, resultSize);  // there may not be any docs in range
 
-      Map<Object,ShardDoc> resultIds = new HashMap<Object,ShardDoc>();
+      Map<Object,ShardDoc> resultIds = new HashMap<>();
       for (int i=resultSize-1; i>=0; i--) {
         ShardDoc shardDoc = queue.pop();
         shardDoc.positionInResponse = i;
@@ -1021,7 +1021,7 @@ public class QueryComponent extends SearchComponent
       }
     }
     SortField[] sortFields = lastCursorMark.getSortSpec().getSort().getSort();
-    List<Object> nextCursorMarkValues = new ArrayList<Object>(sortFields.length);
+    List<Object> nextCursorMarkValues = new ArrayList<>(sortFields.length);
     for (SortField sf : sortFields) {
       if (sf.getType().equals(SortField.Type.SCORE)) {
         assert null != lastDoc.score : "lastDoc has null score";
@@ -1084,11 +1084,11 @@ public class QueryComponent extends SearchComponent
     // unless those requests always go to the final destination shard
 
     // for each shard, collect the documents for that shard.
-    HashMap<String, Collection<ShardDoc>> shardMap = new HashMap<String,Collection<ShardDoc>>();
+    HashMap<String, Collection<ShardDoc>> shardMap = new HashMap<>();
     for (ShardDoc sdoc : rb.resultIds.values()) {
       Collection<ShardDoc> shardDocs = shardMap.get(sdoc.shard);
       if (shardDocs == null) {
-        shardDocs = new ArrayList<ShardDoc>();
+        shardDocs = new ArrayList<>();
         shardMap.put(sdoc.shard, shardDocs);
       }
       shardDocs.add(sdoc);
@@ -1119,7 +1119,7 @@ public class QueryComponent extends SearchComponent
         sreq.params.add(CommonParams.FL, uniqueField.getName());
       }
     
-      ArrayList<String> ids = new ArrayList<String>(shardDocs.size());
+      ArrayList<String> ids = new ArrayList<>(shardDocs.size());
       for (ShardDoc shardDoc : shardDocs) {
         // TODO: depending on the type, we may need more tha a simple toString()?
         ids.add(shardDoc.id.toString());
diff --git solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java
index 4254368..3d82d49 100644
--- solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java
+++ solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java
@@ -113,7 +113,7 @@ public class QueryElevationComponent extends SearchComponent implements SolrCore
   // The key is null if loaded from the config directory, and
   // is never re-loaded.
   final Map<IndexReader, Map<String, ElevationObj>> elevationCache =
-      new WeakHashMap<IndexReader, Map<String, ElevationObj>>();
+      new WeakHashMap<>();
 
   class ElevationObj {
     final String text;
@@ -127,12 +127,12 @@ public class QueryElevationComponent extends SearchComponent implements SolrCore
     ElevationObj(String qstr, List<String> elevate, List<String> exclude) throws IOException {
       this.text = qstr;
       this.analyzed = getAnalyzedQuery(this.text);
-      this.ids = new HashSet<String>();
-      this.excludeIds = new HashSet<String>();
+      this.ids = new HashSet<>();
+      this.excludeIds = new HashSet<>();
 
       this.include = new BooleanQuery();
       this.include.setBoost(0);
-      this.priority = new HashMap<BytesRef, Integer>();
+      this.priority = new HashMap<>();
       int max = elevate.size() + 5;
       for (String id : elevate) {
         id = idSchemaFT.readableToIndexed(id);
@@ -279,7 +279,7 @@ public class QueryElevationComponent extends SearchComponent implements SolrCore
   //load up the elevation map
   private Map<String, ElevationObj> loadElevationMap(Config cfg) throws IOException {
     XPath xpath = XPathFactory.newInstance().newXPath();
-    Map<String, ElevationObj> map = new HashMap<String, ElevationObj>();
+    Map<String, ElevationObj> map = new HashMap<>();
     NodeList nodes = (NodeList) cfg.evaluate("elevate/query", XPathConstants.NODESET);
     for (int i = 0; i < nodes.getLength(); i++) {
       Node node = nodes.item(i);
@@ -293,8 +293,8 @@ public class QueryElevationComponent extends SearchComponent implements SolrCore
             "query requires '<doc .../>' child");
       }
 
-      ArrayList<String> include = new ArrayList<String>();
-      ArrayList<String> exclude = new ArrayList<String>();
+      ArrayList<String> include = new ArrayList<>();
+      ArrayList<String> exclude = new ArrayList<>();
       for (int j = 0; j < children.getLength(); j++) {
         Node child = children.item(j);
         String id = DOMUtil.getAttr(child, "id", "missing 'id'");
@@ -333,7 +333,7 @@ public class QueryElevationComponent extends SearchComponent implements SolrCore
 
     Map<String, ElevationObj> elev = elevationCache.get(reader);
     if (elev == null) {
-      elev = new HashMap<String, ElevationObj>();
+      elev = new HashMap<>();
       elevationCache.put(reader, elev);
     }
     ElevationObj obj = new ElevationObj(query, Arrays.asList(ids), Arrays.asList(ex));
@@ -463,14 +463,14 @@ public class QueryElevationComponent extends SearchComponent implements SolrCore
       List<String> match = null;
       if (booster != null) {
         // Extract the elevated terms into a list
-        match = new ArrayList<String>(booster.priority.size());
+        match = new ArrayList<>(booster.priority.size());
         for (Object o : booster.include.clauses()) {
           TermQuery tq = (TermQuery) ((BooleanClause) o).getQuery();
           match.add(tq.getTerm().text());
         }
       }
 
-      SimpleOrderedMap<Object> dbg = new SimpleOrderedMap<Object>();
+      SimpleOrderedMap<Object> dbg = new SimpleOrderedMap<>();
       dbg.add("q", qstr);
       dbg.add("match", match);
       if (rb.isDebugQuery()) {
@@ -490,8 +490,8 @@ public class QueryElevationComponent extends SearchComponent implements SolrCore
     SortField[] currentSorts = current.getSort().getSort();
     List<SchemaField> currentFields = current.getSchemaFields();
 
-    ArrayList<SortField> sorts = new ArrayList<SortField>(currentSorts.length + 1);
-    List<SchemaField> fields = new ArrayList<SchemaField>(currentFields.size() + 1);
+    ArrayList<SortField> sorts = new ArrayList<>(currentSorts.length + 1);
+    List<SchemaField> fields = new ArrayList<>(currentFields.size() + 1);
 
     // Perhaps force it to always sort by score
     if (force && currentSorts[0].getType() != SortField.Type.SCORE) {
@@ -568,7 +568,7 @@ public class QueryElevationComponent extends SearchComponent implements SolrCore
       private int topVal;
       private TermsEnum termsEnum;
       private DocsEnum docsEnum;
-      Set<String> seen = new HashSet<String>(elevations.ids.size());
+      Set<String> seen = new HashSet<>(elevations.ids.size());
 
       @Override
       public int compare(int slot1, int slot2) {
diff --git solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java
index 84093da..4ca1c05 100644
--- solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java
+++ solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java
@@ -111,7 +111,7 @@ public class RealTimeGetComponent extends SearchComponent
     String[] allIds = id==null ? new String[0] : id;
 
     if (ids != null) {
-      List<String> lst = new ArrayList<String>();
+      List<String> lst = new ArrayList<>();
       for (String s : allIds) {
         lst.add(s);
       }
@@ -305,7 +305,7 @@ public class RealTimeGetComponent extends SearchComponent
         if (sf != null && schema.isCopyFieldTarget(sf)) continue;
 
         if (sf != null && sf.multiValued()) {
-          List<Object> vals = new ArrayList<Object>();
+          List<Object> vals = new ArrayList<>();
           vals.add( f );
           out.setField( f.name(), vals );
         }
@@ -354,7 +354,7 @@ public class RealTimeGetComponent extends SearchComponent
       return ResponseBuilder.STAGE_DONE;
     }
 
-    List<String> allIds = new ArrayList<String>();
+    List<String> allIds = new ArrayList<>();
     if (id1 != null) {
       for (String s : id1) {
         allIds.add(s);
@@ -379,13 +379,13 @@ public class RealTimeGetComponent extends SearchComponent
       DocCollection coll = clusterState.getCollection(collection);
 
 
-      Map<String, List<String>> sliceToId = new HashMap<String, List<String>>();
+      Map<String, List<String>> sliceToId = new HashMap<>();
       for (String id : allIds) {
         Slice slice = coll.getRouter().getTargetSlice(id, null, params, coll);
 
         List<String> idsForShard = sliceToId.get(slice.getName());
         if (idsForShard == null) {
-          idsForShard = new ArrayList<String>(2);
+          idsForShard = new ArrayList<>(2);
           sliceToId.put(slice.getName(), idsForShard);
         }
         idsForShard.add(id);
@@ -582,7 +582,7 @@ public class RealTimeGetComponent extends SearchComponent
     List<String> versions = StrUtils.splitSmart(versionsStr, ",", true);
 
 
-    List<Object> updates = new ArrayList<Object>(versions.size());
+    List<Object> updates = new ArrayList<>(versions.size());
 
     long minVersion = Long.MAX_VALUE;
 
diff --git solr/core/src/java/org/apache/solr/handler/component/ResponseBuilder.java solr/core/src/java/org/apache/solr/handler/component/ResponseBuilder.java
index ac9bf6f..3c5b5d6 100644
--- solr/core/src/java/org/apache/solr/handler/component/ResponseBuilder.java
+++ solr/core/src/java/org/apache/solr/handler/component/ResponseBuilder.java
@@ -168,12 +168,12 @@ public class ResponseBuilder
   SimpleOrderedMap<List<NamedList<Object>>> _pivots;
 
   // Context fields for grouping
-  public final Map<String, Collection<SearchGroup<BytesRef>>> mergedSearchGroups = new HashMap<String, Collection<SearchGroup<BytesRef>>>();
-  public final Map<String, Integer> mergedGroupCounts = new HashMap<String, Integer>();
-  public final Map<String, Map<SearchGroup<BytesRef>, Set<String>>> searchGroupToShards = new HashMap<String, Map<SearchGroup<BytesRef>, Set<String>>>();
-  public final Map<String, TopGroups<BytesRef>> mergedTopGroups = new HashMap<String, TopGroups<BytesRef>>();
-  public final Map<String, QueryCommandResult> mergedQueryCommandResults = new HashMap<String, QueryCommandResult>();
-  public final Map<Object, SolrDocument> retrievedDocuments = new HashMap<Object, SolrDocument>();
+  public final Map<String, Collection<SearchGroup<BytesRef>>> mergedSearchGroups = new HashMap<>();
+  public final Map<String, Integer> mergedGroupCounts = new HashMap<>();
+  public final Map<String, Map<SearchGroup<BytesRef>, Set<String>>> searchGroupToShards = new HashMap<>();
+  public final Map<String, TopGroups<BytesRef>> mergedTopGroups = new HashMap<>();
+  public final Map<String, QueryCommandResult> mergedQueryCommandResults = new HashMap<>();
+  public final Map<Object, SolrDocument> retrievedDocuments = new HashMap<>();
   public int totalHitCount; // Hit count used when distributed grouping is performed.
   // Used for timeAllowed parameter. First phase elapsed time is subtracted from the time allowed for the second phase.
   public int firstPhaseElapsedTime;
@@ -185,14 +185,14 @@ public class ResponseBuilder
   public void addDebugInfo( String name, Object val )
   {
     if( debugInfo == null ) {
-      debugInfo = new SimpleOrderedMap<Object>();
+      debugInfo = new SimpleOrderedMap<>();
     }
     debugInfo.add( name, val );
   }
 
   public void addDebug(Object val, String... path) {
     if( debugInfo == null ) {
-      debugInfo = new SimpleOrderedMap<Object>();
+      debugInfo = new SimpleOrderedMap<>();
     }
 
     NamedList<Object> target = debugInfo;
@@ -200,7 +200,7 @@ public class ResponseBuilder
       String elem = path[i];
       NamedList<Object> newTarget = (NamedList<Object>)debugInfo.get(elem);
       if (newTarget == null) {
-        newTarget = new SimpleOrderedMap<Object>();
+        newTarget = new SimpleOrderedMap<>();
         target.add(elem, newTarget);
       }
       target = newTarget;
diff --git solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java
index 67c55a1..8ce9ef9 100644
--- solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java
+++ solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java
@@ -64,7 +64,7 @@ public class SearchHandler extends RequestHandlerBase implements SolrCoreAware ,
 
   protected List<String> getDefaultComponents()
   {
-    ArrayList<String> names = new ArrayList<String>(6);
+    ArrayList<String> names = new ArrayList<>(6);
     names.add( QueryComponent.COMPONENT_NAME );
     names.add( FacetComponent.COMPONENT_NAME );
     names.add( MoreLikeThisComponent.COMPONENT_NAME );
@@ -126,7 +126,7 @@ public class SearchHandler extends RequestHandlerBase implements SolrCoreAware ,
     }
 
     // Build the component list
-    components = new ArrayList<SearchComponent>( list.size() );
+    components = new ArrayList<>( list.size() );
     DebugComponent dbgCmp = null;
     for(String c : list){
       SearchComponent comp = core.getSearchComponent( c );
@@ -237,9 +237,9 @@ public class SearchHandler extends RequestHandlerBase implements SolrCoreAware ,
       // a distributed request
 
       if (rb.outgoing == null) {
-        rb.outgoing = new LinkedList<ShardRequest>();
+        rb.outgoing = new LinkedList<>();
       }
-      rb.finished = new ArrayList<ShardRequest>();
+      rb.finished = new ArrayList<>();
 
       int nextStage = 0;
       do {
@@ -263,7 +263,7 @@ public class SearchHandler extends RequestHandlerBase implements SolrCoreAware ,
             if (sreq.actualShards==ShardRequest.ALL_SHARDS) {
               sreq.actualShards = rb.shards;
             }
-            sreq.responses = new ArrayList<ShardResponse>();
+            sreq.responses = new ArrayList<>();
 
             // TODO: map from shard to address[]
             for (String shard : sreq.actualShards) {
diff --git solr/core/src/java/org/apache/solr/handler/component/ShardDoc.java solr/core/src/java/org/apache/solr/handler/component/ShardDoc.java
index 603f262..aeae8d6 100644
--- solr/core/src/java/org/apache/solr/handler/component/ShardDoc.java
+++ solr/core/src/java/org/apache/solr/handler/component/ShardDoc.java
@@ -110,7 +110,7 @@ class ShardFieldSortedHitQueue extends PriorityQueue<ShardDoc> {
   protected SortField[] fields;
 
   /** The order of these fieldNames should correspond to the order of sort field values retrieved from the shard */
-  protected List<String> fieldNames = new ArrayList<String>();
+  protected List<String> fieldNames = new ArrayList<>();
 
   public ShardFieldSortedHitQueue(SortField[] fields, int size, IndexSearcher searcher) {
     super(size);
diff --git solr/core/src/java/org/apache/solr/handler/component/ShardRequest.java solr/core/src/java/org/apache/solr/handler/component/ShardRequest.java
index 57ceb43..53e319a 100644
--- solr/core/src/java/org/apache/solr/handler/component/ShardRequest.java
+++ solr/core/src/java/org/apache/solr/handler/component/ShardRequest.java
@@ -48,7 +48,7 @@ public class ShardRequest {
 
 
   /** list of responses... filled out by framework */
-  public List<ShardResponse> responses = new ArrayList<ShardResponse>();
+  public List<ShardResponse> responses = new ArrayList<>();
 
   /** actual shards to send the request to, filled out by framework */
   public String[] actualShards;
diff --git solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
index a2c609e..3544630 100644
--- solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
+++ solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
@@ -100,7 +100,7 @@ public class SpellCheckComponent extends SearchComponent implements SolrCoreAwar
   /**
    * Key is the dictionary, value is the SpellChecker for that dictionary name
    */
-  protected Map<String, SolrSpellChecker> spellCheckers = new ConcurrentHashMap<String, SolrSpellChecker>();
+  protected Map<String, SolrSpellChecker> spellCheckers = new ConcurrentHashMap<>();
 
   protected QueryConverter queryConverter;
 
@@ -380,7 +380,7 @@ public class SpellCheckComponent extends SearchComponent implements SolrCoreAwar
       mergeData.origVsSuggestion.put(suggestion.getToken(), suggestion);
       HashSet<String> suggested = mergeData.origVsSuggested.get(suggestion.getToken());
       if (suggested == null) {
-        suggested = new HashSet<String>();
+        suggested = new HashSet<>();
         mergeData.origVsSuggested.put(suggestion.getToken(), suggested);
       }
 
@@ -470,7 +470,7 @@ public class SpellCheckComponent extends SearchComponent implements SolrCoreAwar
   }
 
   private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {
-    Collection<Token> result = new ArrayList<Token>();
+    Collection<Token> result = new ArrayList<>();
     assert analyzer != null;
     try (TokenStream ts = analyzer.tokenStream("", q)) {
       ts.reset();
@@ -555,7 +555,7 @@ public class SpellCheckComponent extends SearchComponent implements SolrCoreAwar
       Token inputToken = entry.getKey();
       String tokenString = new String(inputToken.buffer(), 0, inputToken
           .length());
-      Map<String,Integer> theSuggestions = new LinkedHashMap<String,Integer>(
+      Map<String,Integer> theSuggestions = new LinkedHashMap<>(
           entry.getValue());
       Iterator<String> sugIter = theSuggestions.keySet().iterator();
       while (sugIter.hasNext()) {
@@ -585,7 +585,7 @@ public class SpellCheckComponent extends SearchComponent implements SolrCoreAwar
           suggestionList.add("origFreq", spellingResult
               .getTokenFrequency(inputToken));
           
-          ArrayList<SimpleOrderedMap> sugs = new ArrayList<SimpleOrderedMap>();
+          ArrayList<SimpleOrderedMap> sugs = new ArrayList<>();
           suggestionList.add("suggestion", sugs);
           for (Map.Entry<String,Integer> suggEntry : theSuggestions.entrySet()) {
             SimpleOrderedMap sugEntry = new SimpleOrderedMap();
@@ -660,7 +660,7 @@ public class SpellCheckComponent extends SearchComponent implements SolrCoreAwar
         }
      }
 
-      Map<String, QueryConverter> queryConverters = new HashMap<String, QueryConverter>();
+      Map<String, QueryConverter> queryConverters = new HashMap<>();
       core.initPlugins(queryConverters,QueryConverter.class);
 
       //ensure that there is at least one query converter defined
diff --git solr/core/src/java/org/apache/solr/handler/component/SpellCheckMergeData.java solr/core/src/java/org/apache/solr/handler/component/SpellCheckMergeData.java
index b41ab05..cfdab0d 100644
--- solr/core/src/java/org/apache/solr/handler/component/SpellCheckMergeData.java
+++ solr/core/src/java/org/apache/solr/handler/component/SpellCheckMergeData.java
@@ -28,16 +28,16 @@ import org.apache.solr.spelling.SpellCheckCollation;
 
 public class SpellCheckMergeData {
   //original token -> corresponding Suggestion object (keep track of start,end)
-  public Map<String, SpellCheckResponse.Suggestion> origVsSuggestion = new HashMap<String, SpellCheckResponse.Suggestion>();
+  public Map<String, SpellCheckResponse.Suggestion> origVsSuggestion = new HashMap<>();
   // original token string -> summed up frequency
-  public Map<String, Integer> origVsFreq = new HashMap<String, Integer>();
+  public Map<String, Integer> origVsFreq = new HashMap<>();
   // original token string -> # of shards reporting it as misspelled
-  public Map<String, Integer> origVsShards = new HashMap<String, Integer>();
+  public Map<String, Integer> origVsShards = new HashMap<>();
   // original token string -> set of alternatives
   // must preserve order because collation algorithm can only work in-order
-  public Map<String, HashSet<String>> origVsSuggested = new LinkedHashMap<String, HashSet<String>>();
+  public Map<String, HashSet<String>> origVsSuggested = new LinkedHashMap<>();
   // alternative string -> corresponding SuggestWord object
-  public Map<String, SuggestWord> suggestedVsWord = new HashMap<String, SuggestWord>();
-  public Map<String, SpellCheckCollation> collations = new HashMap<String, SpellCheckCollation>();
+  public Map<String, SuggestWord> suggestedVsWord = new HashMap<>();
+  public Map<String, SpellCheckCollation> collations = new HashMap<>();
   public int totalNumberShardResponses = 0;
 }
diff --git solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java
index 3460180..6eb8395 100644
--- solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java
+++ solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java
@@ -133,8 +133,8 @@ public class StatsComponent extends SearchComponent {
 
     StatsInfo si = rb._statsInfo;
 
-    NamedList<NamedList<Object>> stats = new SimpleOrderedMap<NamedList<Object>>();
-    NamedList<Object> stats_fields = new SimpleOrderedMap<Object>();
+    NamedList<NamedList<Object>> stats = new SimpleOrderedMap<>();
+    NamedList<Object> stats_fields = new SimpleOrderedMap<>();
     stats.add("stats_fields", stats_fields);
     for (String field : si.statsFields.keySet()) {
       NamedList stv = si.statsFields.get(field).getStatsValues();
@@ -171,7 +171,7 @@ class StatsInfo {
   Map<String, StatsValues> statsFields;
 
   void parse(SolrParams params, ResponseBuilder rb) {
-    statsFields = new HashMap<String, StatsValues>();
+    statsFields = new HashMap<>();
 
     String[] statsFs = params.getParams(StatsParams.STATS_FIELD);
     if (statsFs != null) {
@@ -205,13 +205,13 @@ class SimpleStats {
   }
 
   public NamedList<Object> getStatsCounts() throws IOException {
-    NamedList<Object> res = new SimpleOrderedMap<Object>();
+    NamedList<Object> res = new SimpleOrderedMap<>();
     res.add("stats_fields", getStatsFields());
     return res;
   }
 
   public NamedList<Object> getStatsFields() throws IOException {
-    NamedList<Object> res = new SimpleOrderedMap<Object>();
+    NamedList<Object> res = new SimpleOrderedMap<>();
     String[] statsFs = params.getParams(StatsParams.STATS_FIELD);
     boolean isShard = params.getBool(ShardParams.IS_SHARD, false);
     if (null != statsFs) {
@@ -249,7 +249,7 @@ class SimpleStats {
 
     final StatsValues allstats = StatsValuesFactory.createStatsValues(sf, calcDistinct);
 
-    List<FieldFacetStats> facetStats = new ArrayList<FieldFacetStats>();
+    List<FieldFacetStats> facetStats = new ArrayList<>();
     for( String facetField : facet ) {
       SchemaField fsf = schema.getField(facetField);
 
diff --git solr/core/src/java/org/apache/solr/handler/component/StatsValuesFactory.java solr/core/src/java/org/apache/solr/handler/component/StatsValuesFactory.java
index 706bcd3..f5f1b62d 100644
--- solr/core/src/java/org/apache/solr/handler/component/StatsValuesFactory.java
+++ solr/core/src/java/org/apache/solr/handler/component/StatsValuesFactory.java
@@ -88,12 +88,12 @@ abstract class AbstractStatsValues<T> implements StatsValues {
   protected boolean calcDistinct = false;
   
   // facetField   facetValue
-  protected Map<String, Map<String, StatsValues>> facets = new HashMap<String, Map<String, StatsValues>>();
+  protected Map<String, Map<String, StatsValues>> facets = new HashMap<>();
 
   protected AbstractStatsValues(SchemaField sf, boolean calcDistinct) {
     this.sf = sf;
     this.ft = sf.getType();
-    this.distinctValues = new TreeSet<T>();
+    this.distinctValues = new TreeSet<>();
     this.calcDistinct = calcDistinct;
   }
 
@@ -122,7 +122,7 @@ abstract class AbstractStatsValues<T> implements StatsValues {
       NamedList vals = (NamedList) f.getVal(i);
       Map<String, StatsValues> addTo = facets.get(field);
       if (addTo == null) {
-        addTo = new HashMap<String, StatsValues>();
+        addTo = new HashMap<>();
         facets.put(field, addTo);
       }
       for (int j = 0; j < vals.size(); j++) {
@@ -185,7 +185,7 @@ abstract class AbstractStatsValues<T> implements StatsValues {
    */
   @Override
   public NamedList<?> getStatsValues() {
-    NamedList<Object> res = new SimpleOrderedMap<Object>();
+    NamedList<Object> res = new SimpleOrderedMap<>();
 
     res.add("min", min);
     res.add("max", max);
@@ -199,9 +199,9 @@ abstract class AbstractStatsValues<T> implements StatsValues {
     addTypeSpecificStats(res);
 
      // add the facet stats
-    NamedList<NamedList<?>> nl = new SimpleOrderedMap<NamedList<?>>();
+    NamedList<NamedList<?>> nl = new SimpleOrderedMap<>();
     for (Map.Entry<String, Map<String, StatsValues>> entry : facets.entrySet()) {
-      NamedList<NamedList<?>> nl2 = new SimpleOrderedMap<NamedList<?>>();
+      NamedList<NamedList<?>> nl2 = new SimpleOrderedMap<>();
       nl.add(entry.getKey(), nl2);
       for (Map.Entry<String, StatsValues> e2 : entry.getValue().entrySet()) {
         nl2.add(e2.getKey(), e2.getValue().getStatsValues());
diff --git solr/core/src/java/org/apache/solr/handler/component/SuggestComponent.java solr/core/src/java/org/apache/solr/handler/component/SuggestComponent.java
index 31d2925..2024cb0 100644
--- solr/core/src/java/org/apache/solr/handler/component/SuggestComponent.java
+++ solr/core/src/java/org/apache/solr/handler/component/SuggestComponent.java
@@ -80,7 +80,7 @@ public class SuggestComponent extends SearchComponent implements SolrCoreAware,
   /**
    * Key is the dictionary name used in SolrConfig, value is the corresponding {@link SolrSuggester}
    */
-  protected Map<String, SolrSuggester> suggesters = new ConcurrentHashMap<String, SolrSuggester>();
+  protected Map<String, SolrSuggester> suggesters = new ConcurrentHashMap<>();
   
   /** Container for various labels used in the responses generated by this component */
   private static class SuggesterResultLabels {
@@ -211,7 +211,7 @@ public class SuggestComponent extends SearchComponent implements SolrCoreAware,
       if (!buildAll && !reloadAll) {
         throw ex;
       } else {
-        querySuggesters = new HashSet<SolrSuggester>();
+        querySuggesters = new HashSet<>();
       }
     }
     
@@ -227,7 +227,7 @@ public class SuggestComponent extends SearchComponent implements SolrCoreAware,
       int count = params.getInt(SUGGEST_COUNT, 1);
       SuggesterOptions options = new SuggesterOptions(new CharsRef(query), count);
       Map<String, SimpleOrderedMap<NamedList<Object>>> namedListResults = 
-          new HashMap<String, SimpleOrderedMap<NamedList<Object>>>();
+          new HashMap<>();
       for (SolrSuggester suggester : querySuggesters) {
         SuggesterResult suggesterResult = suggester.getSuggestions(options);
         toNamedList(suggesterResult, namedListResults);
@@ -247,7 +247,7 @@ public class SuggestComponent extends SearchComponent implements SolrCoreAware,
       return;
     int count = params.getInt(SUGGEST_COUNT, 1);
     
-    List<SuggesterResult> suggesterResults = new ArrayList<SuggesterResult>();
+    List<SuggesterResult> suggesterResults = new ArrayList<>();
     
     // Collect Shard responses
     for (ShardRequest sreq : rb.finished) {
@@ -266,7 +266,7 @@ public class SuggestComponent extends SearchComponent implements SolrCoreAware,
     // Merge Shard responses
     SuggesterResult suggesterResult = merge(suggesterResults, count);
     Map<String, SimpleOrderedMap<NamedList<Object>>> namedListResults = 
-        new HashMap<String, SimpleOrderedMap<NamedList<Object>>>();
+        new HashMap<>();
     toNamedList(suggesterResult, namedListResults);
     
     rb.rsp.add(SuggesterResultLabels.SUGGEST, namedListResults);
@@ -280,8 +280,8 @@ public class SuggestComponent extends SearchComponent implements SolrCoreAware,
    * */
   private static SuggesterResult merge(List<SuggesterResult> suggesterResults, int count) {
     SuggesterResult result = new SuggesterResult();
-    Set<String> allTokens = new HashSet<String>();
-    Set<String> suggesterNames = new HashSet<String>();
+    Set<String> allTokens = new HashSet<>();
+    Set<String> suggesterNames = new HashSet<>();
     
     // collect all tokens
     for (SuggesterResult shardResult : suggesterResults) {
@@ -305,7 +305,7 @@ public class SuggestComponent extends SearchComponent implements SolrCoreAware,
             resultQueue.insertWithOverflow(res);
           }
         }
-        List<LookupResult> sortedSuggests = new LinkedList<LookupResult>();
+        List<LookupResult> sortedSuggests = new LinkedList<>();
         Collections.addAll(sortedSuggests, resultQueue.getResults());
         result.add(suggesterName, token, sortedSuggests);
       }
@@ -325,7 +325,7 @@ public class SuggestComponent extends SearchComponent implements SolrCoreAware,
   
   @Override
   public NamedList getStatistics() {
-    NamedList<String> stats = new SimpleOrderedMap<String>();
+    NamedList<String> stats = new SimpleOrderedMap<>();
     stats.add("totalSizeInBytes", String.valueOf(sizeInBytes()));
     for (Map.Entry<String, SolrSuggester> entry : suggesters.entrySet()) {
       SolrSuggester suggester = entry.getValue();
@@ -344,7 +344,7 @@ public class SuggestComponent extends SearchComponent implements SolrCoreAware,
   }
   
   private Set<SolrSuggester> getSuggesters(SolrParams params) {
-    Set<SolrSuggester> solrSuggesters = new HashSet<SolrSuggester>();
+    Set<SolrSuggester> solrSuggesters = new HashSet<>();
     for(String suggesterName : getSuggesterNames(params)) {
       SolrSuggester curSuggester = suggesters.get(suggesterName);
       if (curSuggester != null) {
@@ -361,7 +361,7 @@ public class SuggestComponent extends SearchComponent implements SolrCoreAware,
   }
   
   private Set<String> getSuggesterNames(SolrParams params) {
-    Set<String> suggesterNames = new HashSet<String>();
+    Set<String> suggesterNames = new HashSet<>();
     String[] suggesterNamesFromParams = params.getParams(SUGGEST_DICT);
     if (suggesterNamesFromParams == null) {
       suggesterNames.add(DEFAULT_DICT_NAME);
@@ -376,12 +376,12 @@ public class SuggestComponent extends SearchComponent implements SolrCoreAware,
   /** Convert {@link SuggesterResult} to NamedList for constructing responses */
   private void toNamedList(SuggesterResult suggesterResult, Map<String, SimpleOrderedMap<NamedList<Object>>> resultObj) {
     for(String suggesterName : suggesterResult.getSuggesterNames()) {
-      SimpleOrderedMap<NamedList<Object>> results = new SimpleOrderedMap<NamedList<Object>>();
+      SimpleOrderedMap<NamedList<Object>> results = new SimpleOrderedMap<>();
       for (String token : suggesterResult.getTokens(suggesterName)) {
-        SimpleOrderedMap<Object> suggestionBody = new SimpleOrderedMap<Object>();
+        SimpleOrderedMap<Object> suggestionBody = new SimpleOrderedMap<>();
         List<LookupResult> lookupResults = suggesterResult.getLookupResult(suggesterName, token);
         suggestionBody.add(SuggesterResultLabels.SUGGESTION_NUM_FOUND, lookupResults.size());
-        List<SimpleOrderedMap<Object>> suggestEntriesNamedList = new ArrayList<SimpleOrderedMap<Object>>();
+        List<SimpleOrderedMap<Object>> suggestEntriesNamedList = new ArrayList<>();
         for (LookupResult lookupResult : lookupResults) {
           String suggestionString = lookupResult.key.toString();
           long weight = lookupResult.value;
@@ -389,7 +389,7 @@ public class SuggestComponent extends SearchComponent implements SolrCoreAware,
               lookupResult.payload.utf8ToString()
               : "";
           
-          SimpleOrderedMap<Object> suggestEntryNamedList = new SimpleOrderedMap<Object>();
+          SimpleOrderedMap<Object> suggestEntryNamedList = new SimpleOrderedMap<>();
           suggestEntryNamedList.add(SuggesterResultLabels.SUGGESTION_TERM, suggestionString);
           suggestEntryNamedList.add(SuggesterResultLabels.SUGGESTION_WEIGHT, weight);
           suggestEntryNamedList.add(SuggesterResultLabels.SUGGESTION_PAYLOAD, payload);
@@ -415,7 +415,7 @@ public class SuggestComponent extends SearchComponent implements SolrCoreAware,
       for (Iterator<Map.Entry<String, NamedList<Object>>> suggestionsIter = entry.getValue().iterator(); suggestionsIter.hasNext();) {
         Map.Entry<String, NamedList<Object>> suggestions = suggestionsIter.next(); 
         String tokenString = suggestions.getKey();
-        List<LookupResult> lookupResults = new ArrayList<LookupResult>();
+        List<LookupResult> lookupResults = new ArrayList<>();
         NamedList<Object> suggestion = suggestions.getValue();
         // for each suggestion
         for (int j = 0; j < suggestion.size(); j++) {
diff --git solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java
index 3378847..4e66e65 100644
--- solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java
+++ solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java
@@ -127,7 +127,7 @@ public class TermVectorComponent extends SearchComponent implements SolrCoreAwar
     }
 
     // otherwise us the raw fldList as is, no special parsing or globs
-    Set<String> fieldNames = new LinkedHashSet<String>();
+    Set<String> fieldNames = new LinkedHashSet<>();
     for (String fl : fldLst) {
       fieldNames.addAll(Arrays.asList(SolrPluginUtils.split(fl)));
     }
@@ -141,7 +141,7 @@ public class TermVectorComponent extends SearchComponent implements SolrCoreAwar
       return;
     }
 
-    NamedList<Object> termVectors = new NamedList<Object>();
+    NamedList<Object> termVectors = new NamedList<>();
     rb.rsp.add(TERM_VECTORS, termVectors);
 
     IndexSchema schema = rb.req.getSchema();
@@ -170,11 +170,11 @@ public class TermVectorComponent extends SearchComponent implements SolrCoreAwar
     }
 
     //Build up our per field mapping
-    Map<String, FieldOptions> fieldOptions = new HashMap<String, FieldOptions>();
-    NamedList<List<String>> warnings = new NamedList<List<String>>();
-    List<String>  noTV = new ArrayList<String>();
-    List<String>  noPos = new ArrayList<String>();
-    List<String>  noOff = new ArrayList<String>();
+    Map<String, FieldOptions> fieldOptions = new HashMap<>();
+    NamedList<List<String>> warnings = new NamedList<>();
+    List<String>  noTV = new ArrayList<>();
+    List<String>  noPos = new ArrayList<>();
+    List<String>  noOff = new ArrayList<>();
 
     Set<String> fields = getFields(rb);
     if ( null != fields ) {
@@ -261,7 +261,7 @@ public class TermVectorComponent extends SearchComponent implements SolrCoreAwar
 
     final String finalUniqFieldName = uniqFieldName;
 
-    final List<String> uniqValues = new ArrayList<String>();
+    final List<String> uniqValues = new ArrayList<>();
     
     // TODO: is this required to be single-valued? if so, we should STOP
     // once we find it...
@@ -291,7 +291,7 @@ public class TermVectorComponent extends SearchComponent implements SolrCoreAwar
 
     while (iter.hasNext()) {
       Integer docId = iter.next();
-      NamedList<Object> docNL = new NamedList<Object>();
+      NamedList<Object> docNL = new NamedList<>();
 
       if (keyField != null) {
         reader.document(docId, getUniqValue);
@@ -331,14 +331,14 @@ public class TermVectorComponent extends SearchComponent implements SolrCoreAwar
   }
 
   private void mapOneVector(NamedList<Object> docNL, FieldOptions fieldOptions, IndexReader reader, int docID, TermsEnum termsEnum, String field) throws IOException {
-    NamedList<Object> fieldNL = new NamedList<Object>();
+    NamedList<Object> fieldNL = new NamedList<>();
     docNL.add(field, fieldNL);
 
     BytesRef text;
     DocsAndPositionsEnum dpEnum = null;
     while((text = termsEnum.next()) != null) {
       String term = text.utf8ToString();
-      NamedList<Object> termInfo = new NamedList<Object>();
+      NamedList<Object> termInfo = new NamedList<>();
       fieldNL.add(term, termInfo);
       final int freq = (int) termsEnum.totalTermFreq();
       if (fieldOptions.termFreq == true) {
@@ -362,7 +362,7 @@ public class TermVectorComponent extends SearchComponent implements SolrCoreAwar
           final int pos = dpEnum.nextPosition();
           if (usePositions && pos >= 0) {
             if (positionsNL == null) {
-              positionsNL = new NamedList<Integer>();
+              positionsNL = new NamedList<>();
               termInfo.add("positions", positionsNL);
             }
             positionsNL.add("position", pos);
@@ -372,7 +372,7 @@ public class TermVectorComponent extends SearchComponent implements SolrCoreAwar
             if (dpEnum.startOffset() == -1) {
               useOffsets = false;
             } else {
-              theOffsets = new NamedList<Number>();
+              theOffsets = new NamedList<>();
               termInfo.add("offsets", theOffsets);
             }
           }
@@ -404,7 +404,7 @@ public class TermVectorComponent extends SearchComponent implements SolrCoreAwar
   private List<Integer> getInts(String[] vals) {
     List<Integer> result = null;
     if (vals != null && vals.length > 0) {
-      result = new ArrayList<Integer>(vals.length);
+      result = new ArrayList<>(vals.length);
       for (int i = 0; i < vals.length; i++) {
         try {
           result.add(new Integer(vals[i]));
@@ -444,13 +444,13 @@ public class TermVectorComponent extends SearchComponent implements SolrCoreAwar
               }
             } else {
               int idx = sdoc.positionInResponse;
-              arr[idx] = new NamedList.NamedListEntry<Object>(key, nl.getVal(i));
+              arr[idx] = new NamedList.NamedListEntry<>(key, nl.getVal(i));
             }
           }
         }
       }
       // remove nulls in case not all docs were able to be retrieved
-      termVectors.addAll(SolrPluginUtils.removeNulls(new NamedList<Object>(arr)));
+      termVectors.addAll(SolrPluginUtils.removeNulls(new NamedList<>(arr)));
       rb.rsp.add(TERM_VECTORS, termVectors);
     }
   }
diff --git solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java
index 1fe142d..3e6ba01 100644
--- solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java
+++ solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java
@@ -89,7 +89,7 @@ public class TermsComponent extends SearchComponent {
 
     String[] fields = params.getParams(TermsParams.TERMS_FIELD);
 
-    NamedList<Object> termsResult = new SimpleOrderedMap<Object>();
+    NamedList<Object> termsResult = new SimpleOrderedMap<>();
     rb.rsp.add("terms", termsResult);
 
     if (fields == null || fields.length==0) return;
@@ -121,7 +121,7 @@ public class TermsComponent extends SearchComponent {
     Fields lfields = indexReader.fields();
 
     for (String field : fields) {
-      NamedList<Integer> fieldTerms = new NamedList<Integer>();
+      NamedList<Integer> fieldTerms = new NamedList<>();
       termsResult.add(field, fieldTerms);
 
       Terms terms = lfields == null ? null : lfields.terms(field);
@@ -208,7 +208,7 @@ public class TermsComponent extends SearchComponent {
         if (docFreq >= freqmin && docFreq <= freqmax) {
           // add the term to the list
           if (sort) {
-            queue.add(new CountPair<BytesRef, Integer>(BytesRef.deepCopyOf(term), docFreq));
+            queue.add(new CountPair<>(BytesRef.deepCopyOf(term), docFreq));
           } else {
 
             // TODO: handle raw somehow
@@ -326,7 +326,7 @@ public class TermsComponent extends SearchComponent {
     private SolrParams params;
 
     public TermsHelper() {
-      fieldmap = new HashMap<String, HashMap<String, TermsResponse.Term>>(5);
+      fieldmap = new HashMap<>(5);
     }
 
     public void init(SolrParams params) {
@@ -374,7 +374,7 @@ public class TermsComponent extends SearchComponent {
     }
 
     public NamedList buildResponse() {
-      NamedList<Object> response = new SimpleOrderedMap<Object>();
+      NamedList<Object> response = new SimpleOrderedMap<>();
 
       // determine if we are going index or count sort
       boolean sort = !TermsParams.TERMS_SORT_INDEX.equals(params.get(
@@ -403,7 +403,7 @@ public class TermsComponent extends SearchComponent {
 
       // loop though each field we want terms from
       for (String key : fieldmap.keySet()) {
-        NamedList<Number> fieldterms = new SimpleOrderedMap<Number>();
+        NamedList<Number> fieldterms = new SimpleOrderedMap<>();
         TermsResponse.Term[] data = null;
         if (sort) {
           data = getCountSorted(fieldmap.get(key));
diff --git solr/core/src/java/org/apache/solr/handler/loader/CSVLoaderBase.java solr/core/src/java/org/apache/solr/handler/loader/CSVLoaderBase.java
index f525a12..3a50431 100644
--- solr/core/src/java/org/apache/solr/handler/loader/CSVLoaderBase.java
+++ solr/core/src/java/org/apache/solr/handler/loader/CSVLoaderBase.java
@@ -164,7 +164,7 @@ abstract class CSVLoaderBase extends ContentStreamLoader {
   CSVLoaderBase(SolrQueryRequest req, UpdateRequestProcessor processor) {
     this.processor = processor;
     this.params = req.getParams();
-    this.literals = new HashMap<String, String>();
+    this.literals = new HashMap<>();
 
     templateAdd = new AddUpdateCommand(req);
     templateAdd.overwrite=params.getBool(OVERWRITE,true);
diff --git solr/core/src/java/org/apache/solr/handler/loader/JsonLoader.java solr/core/src/java/org/apache/solr/handler/loader/JsonLoader.java
index 948708b..8c9bd18 100644
--- solr/core/src/java/org/apache/solr/handler/loader/JsonLoader.java
+++ solr/core/src/java/org/apache/solr/handler/loader/JsonLoader.java
@@ -480,7 +480,7 @@ public class JsonLoader extends ContentStreamLoader {
             } else {
               // If we encounter other unknown map keys, then use a map
               if (extendedInfo == null) {
-                extendedInfo = new HashMap<String, Object>(2);
+                extendedInfo = new HashMap<>(2);
               }
               // for now, the only extended info will be field values
               // we could either store this as an Object or a SolrInputField
diff --git solr/core/src/java/org/apache/solr/handler/loader/XMLLoader.java solr/core/src/java/org/apache/solr/handler/loader/XMLLoader.java
index a9374be..d2798fb 100644
--- solr/core/src/java/org/apache/solr/handler/loader/XMLLoader.java
+++ solr/core/src/java/org/apache/solr/handler/loader/XMLLoader.java
@@ -409,10 +409,10 @@ public class XMLLoader extends ContentStreamLoader {
             // should I warn in some text has been found too
             Object v = isNull ? null : text.toString();
             if (update != null) {
-              if (updateMap == null) updateMap = new HashMap<String, Map<String, Object>>();
+              if (updateMap == null) updateMap = new HashMap<>();
               Map<String, Object> extendedValues = updateMap.get(name);
               if (extendedValues == null) {
-                extendedValues = new HashMap<String, Object>(1);
+                extendedValues = new HashMap<>(1);
                 updateMap.put(name, extendedValues);
               }
               Object val = extendedValues.get(update);
@@ -424,7 +424,7 @@ public class XMLLoader extends ContentStreamLoader {
                   List list = (List) val;
                   list.add(v);
                 } else {
-                  List<Object> values = new ArrayList<Object>();
+                  List<Object> values = new ArrayList<>();
                   values.add(val);
                   values.add(v);
                   extendedValues.put(update, values);
diff --git solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java
index 7bd9a07..ae9809e 100644
--- solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java
+++ solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java
@@ -70,27 +70,27 @@ public class DefaultSolrHighlighter extends SolrHighlighter implements PluginInf
 
   // Thread safe registry
   protected final Map<String,SolrFormatter> formatters =
-    new HashMap<String, SolrFormatter>();
+    new HashMap<>();
 
   // Thread safe registry
   protected final Map<String,SolrEncoder> encoders =
-    new HashMap<String, SolrEncoder>();
+    new HashMap<>();
 
   // Thread safe registry
   protected final Map<String,SolrFragmenter> fragmenters =
-    new HashMap<String, SolrFragmenter>() ;
+    new HashMap<>() ;
 
   // Thread safe registry
   protected final Map<String, SolrFragListBuilder> fragListBuilders =
-    new HashMap<String, SolrFragListBuilder>() ;
+    new HashMap<>() ;
 
   // Thread safe registry
   protected final Map<String, SolrFragmentsBuilder> fragmentsBuilders =
-    new HashMap<String, SolrFragmentsBuilder>() ;
+    new HashMap<>() ;
 
   // Thread safe registry
   protected final Map<String, SolrBoundaryScanner> boundaryScanners =
-    new HashMap<String, SolrBoundaryScanner>() ;
+    new HashMap<>() ;
 
   @Override
   public void init(PluginInfo info) {
@@ -382,7 +382,7 @@ public class DefaultSolrHighlighter extends SolrHighlighter implements PluginInf
     IndexSchema schema = searcher.getSchema();
     NamedList fragments = new SimpleOrderedMap();
     String[] fieldNames = getHighlightFields(query, req, defaultFields);
-    Set<String> fset = new HashSet<String>();
+    Set<String> fset = new HashSet<>();
      
     {
       // pre-fetch documents using the Searcher's doc cache
@@ -466,7 +466,7 @@ public class DefaultSolrHighlighter extends SolrHighlighter implements PluginInf
     boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);
 
     String[] summaries = null;
-    List<TextFragment> frags = new ArrayList<TextFragment>();
+    List<TextFragment> frags = new ArrayList<>();
 
     TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization
     TokenStream tvStream = TokenSources.getTokenStreamWithOffsets(searcher.getIndexReader(), docId, fieldName);
@@ -555,7 +555,7 @@ public class DefaultSolrHighlighter extends SolrHighlighter implements PluginInf
      // convert fragments back into text
      // TODO: we can include score and position information in output as snippet attributes
     if (frags.size() > 0) {
-      ArrayList<String> fragTexts = new ArrayList<String>();
+      ArrayList<String> fragTexts = new ArrayList<>();
       for (TextFragment fragment: frags) {
         if (preserveMulti) {
           if (fragment != null) {
@@ -606,7 +606,7 @@ public class DefaultSolrHighlighter extends SolrHighlighter implements PluginInf
         // The alternate field did not exist, treat the original field as fallback instead
         docFields = doc.getFields(fieldName);
       }
-      List<String> listFields = new ArrayList<String>();
+      List<String> listFields = new ArrayList<>();
       for (StorableField field : docFields) {
         if (field.binaryValue() == null)
           listFields.add(field.stringValue());
@@ -617,7 +617,7 @@ public class DefaultSolrHighlighter extends SolrHighlighter implements PluginInf
       if (altTexts != null && altTexts.length > 0){
         Encoder encoder = getEncoder(fieldName, params);
         int alternateFieldLen = params.getFieldInt(fieldName, HighlightParams.ALTERNATE_FIELD_LENGTH,0);
-        List<String> altList = new ArrayList<String>();
+        List<String> altList = new ArrayList<>();
         int len = 0;
         for( String altText: altTexts ){
           if( alternateFieldLen <= 0 ){
@@ -653,7 +653,7 @@ public class DefaultSolrHighlighter extends SolrHighlighter implements PluginInf
  */
 final class TokenOrderingFilter extends TokenFilter {
   private final int windowSize;
-  private final LinkedList<OrderedToken> queue = new LinkedList<OrderedToken>();
+  private final LinkedList<OrderedToken> queue = new LinkedList<>();
   private boolean done=false;
   private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
   
diff --git solr/core/src/java/org/apache/solr/highlight/HighlightingPluginBase.java solr/core/src/java/org/apache/solr/highlight/HighlightingPluginBase.java
index 8f6717f..9ac35e6 100644
--- solr/core/src/java/org/apache/solr/highlight/HighlightingPluginBase.java
+++ solr/core/src/java/org/apache/solr/highlight/HighlightingPluginBase.java
@@ -72,7 +72,7 @@ public abstract class HighlightingPluginBase implements SolrInfoMBean
 
   @Override
   public NamedList getStatistics() {
-    NamedList<Long> lst = new SimpleOrderedMap<Long>();
+    NamedList<Long> lst = new SimpleOrderedMap<>();
     lst.add("requests", numRequests);
     return lst;
   }
diff --git solr/core/src/java/org/apache/solr/highlight/PostingsSolrHighlighter.java solr/core/src/java/org/apache/solr/highlight/PostingsSolrHighlighter.java
index a11e1a0..0a02444 100644
--- solr/core/src/java/org/apache/solr/highlight/PostingsSolrHighlighter.java
+++ solr/core/src/java/org/apache/solr/highlight/PostingsSolrHighlighter.java
@@ -209,9 +209,9 @@ public class PostingsSolrHighlighter extends SolrHighlighter implements PluginIn
    * @return encoded namedlist of summaries
    */
   protected NamedList<Object> encodeSnippets(String[] keys, String[] fieldNames, Map<String,String[]> snippets) {
-    NamedList<Object> list = new SimpleOrderedMap<Object>();
+    NamedList<Object> list = new SimpleOrderedMap<>();
     for (int i = 0; i < keys.length; i++) {
-      NamedList<Object> summary = new SimpleOrderedMap<Object>();
+      NamedList<Object> summary = new SimpleOrderedMap<>();
       for (String field : fieldNames) {
         String snippet = snippets.get(field)[i];
         // box in an array to match the format of existing highlighters, 
diff --git solr/core/src/java/org/apache/solr/highlight/RegexFragmenter.java solr/core/src/java/org/apache/solr/highlight/RegexFragmenter.java
index 01b7f0c..cc5bd8d 100644
--- solr/core/src/java/org/apache/solr/highlight/RegexFragmenter.java
+++ solr/core/src/java/org/apache/solr/highlight/RegexFragmenter.java
@@ -203,7 +203,7 @@ class LuceneRegexFragmenter implements Fragmenter
 
   protected void addHotSpots(String text) {
     //System.out.println("hot spotting");
-    ArrayList<Integer> temphs = new ArrayList<Integer>(
+    ArrayList<Integer> temphs = new ArrayList<>(
                               text.length() / targetFragChars);
     Matcher match = textRE.matcher(text);
     int cur = 0;
diff --git solr/core/src/java/org/apache/solr/highlight/SolrHighlighter.java solr/core/src/java/org/apache/solr/highlight/SolrHighlighter.java
index 1a41af9..ac4e3d3 100644
--- solr/core/src/java/org/apache/solr/highlight/SolrHighlighter.java
+++ solr/core/src/java/org/apache/solr/highlight/SolrHighlighter.java
@@ -77,7 +77,7 @@ public abstract class SolrHighlighter
         // create a Java regular expression from the wildcard string
         String fieldRegex = fields[0].replaceAll("\\*", ".*");
         Collection<String> storedHighlightFieldNames = request.getSearcher().getStoredHighlightFieldNames();
-        List<String> storedFieldsToHighlight = new ArrayList<String>();
+        List<String> storedFieldsToHighlight = new ArrayList<>();
         for (String storedFieldName: storedHighlightFieldNames) {
             if (storedFieldName.matches(fieldRegex)) {
               storedFieldsToHighlight.add(storedFieldName);
diff --git solr/core/src/java/org/apache/solr/logging/CircularList.java solr/core/src/java/org/apache/solr/logging/CircularList.java
index 6e4641e..e9dc561 100644
--- solr/core/src/java/org/apache/solr/logging/CircularList.java
+++ solr/core/src/java/org/apache/solr/logging/CircularList.java
@@ -108,7 +108,7 @@ public class CircularList<T> implements Iterable<T>
 
   public List<T> toList()
   {
-    ArrayList<T> list = new ArrayList<T>( size );
+    ArrayList<T> list = new ArrayList<>( size );
     for( int i=0; i<size; i++ ) {
       list.add( data[convert(i)] );
     }
diff --git solr/core/src/java/org/apache/solr/logging/LoggerInfo.java solr/core/src/java/org/apache/solr/logging/LoggerInfo.java
index 2151b32..9dbc48b 100644
--- solr/core/src/java/org/apache/solr/logging/LoggerInfo.java
+++ solr/core/src/java/org/apache/solr/logging/LoggerInfo.java
@@ -43,7 +43,7 @@ public abstract class LoggerInfo implements Comparable<LoggerInfo> {
   public abstract boolean isSet();
 
   public SimpleOrderedMap<?> getInfo() {
-    SimpleOrderedMap<Object> info = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> info = new SimpleOrderedMap<>();
     info.add("name", getName());
     info.add("level", getLevel());
     info.add("set", isSet());
diff --git solr/core/src/java/org/apache/solr/logging/jul/JulWatcher.java solr/core/src/java/org/apache/solr/logging/jul/JulWatcher.java
index 6d20d1c..ccf31b3 100644
--- solr/core/src/java/org/apache/solr/logging/jul/JulWatcher.java
+++ solr/core/src/java/org/apache/solr/logging/jul/JulWatcher.java
@@ -88,7 +88,7 @@ public class JulWatcher extends LogWatcher<LogRecord> {
     LogManager manager = LogManager.getLogManager();
 
     Logger root = manager.getLogger("");
-    Map<String,LoggerInfo> map = new HashMap<String,LoggerInfo>();
+    Map<String,LoggerInfo> map = new HashMap<>();
     Enumeration<String> names = manager.getLoggerNames();
     while (names.hasMoreElements()) {
       String name = names.nextElement();
@@ -133,7 +133,7 @@ public class JulWatcher extends LogWatcher<LogRecord> {
     if(history!=null) {
       throw new IllegalStateException("History already registered");
     }
-    history = new CircularList<LogRecord>(cfg.size);
+    history = new CircularList<>(cfg.size);
     handler = new RecordHandler(this);
     if(cfg.threshold != null) {
       handler.setLevel(Level.parse(cfg.threshold));
diff --git solr/core/src/java/org/apache/solr/logging/log4j/Log4jWatcher.java solr/core/src/java/org/apache/solr/logging/log4j/Log4jWatcher.java
index b906127..c137b65 100644
--- solr/core/src/java/org/apache/solr/logging/log4j/Log4jWatcher.java
+++ solr/core/src/java/org/apache/solr/logging/log4j/Log4jWatcher.java
@@ -83,7 +83,7 @@ public class Log4jWatcher extends LogWatcher<LoggingEvent> {
   @Override
   public Collection<LoggerInfo> getAllLoggers() {
     org.apache.log4j.Logger root = org.apache.log4j.LogManager.getRootLogger();
-    Map<String,LoggerInfo> map = new HashMap<String,LoggerInfo>();
+    Map<String,LoggerInfo> map = new HashMap<>();
     Enumeration<?> loggers = org.apache.log4j.LogManager.getCurrentLoggers();
     while (loggers.hasMoreElements()) {
       org.apache.log4j.Logger logger = (org.apache.log4j.Logger)loggers.nextElement();
@@ -128,7 +128,7 @@ public class Log4jWatcher extends LogWatcher<LoggingEvent> {
     if(history!=null) {
       throw new IllegalStateException("History already registered");
     }
-    history = new CircularList<LoggingEvent>(cfg.size);
+    history = new CircularList<>(cfg.size);
 
     appender = new EventAppender(this);
     if(cfg.threshold != null) {
diff --git solr/core/src/java/org/apache/solr/parser/QueryParser.java solr/core/src/java/org/apache/solr/parser/QueryParser.java
index 709ad00..7624462 100644
--- solr/core/src/java/org/apache/solr/parser/QueryParser.java
+++ solr/core/src/java/org/apache/solr/parser/QueryParser.java
@@ -100,7 +100,7 @@ public class QueryParser extends SolrQueryParserBase implements QueryParserConst
   }
 
   final public Query Query(String field) throws ParseException, SyntaxError {
-  List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+  List<BooleanClause> clauses = new ArrayList<>();
   Query q, firstQuery=null;
   int conj, mods;
     mods = Modifiers();
@@ -581,7 +581,7 @@ public class QueryParser extends SolrQueryParserBase implements QueryParserConst
       return (jj_ntk = jj_nt.kind);
   }
 
-  private java.util.List<int[]> jj_expentries = new java.util.ArrayList<int[]>();
+  private java.util.List<int[]> jj_expentries = new java.util.ArrayList<>();
   private int[] jj_expentry;
   private int jj_kind = -1;
   private int[] jj_lasttokens = new int[100];
diff --git solr/core/src/java/org/apache/solr/parser/SolrQueryParserBase.java solr/core/src/java/org/apache/solr/parser/SolrQueryParserBase.java
index 7239e67..7726de2 100644
--- solr/core/src/java/org/apache/solr/parser/SolrQueryParserBase.java
+++ solr/core/src/java/org/apache/solr/parser/SolrQueryParserBase.java
@@ -114,7 +114,7 @@ public abstract class SolrQueryParserBase extends QueryBuilder {
       return field;
     }
     private final static Map<String,MagicFieldName> lookup
-        = new HashMap<String,MagicFieldName>();
+        = new HashMap<>();
     static {
       for(MagicFieldName s : EnumSet.allOf(MagicFieldName.class))
         lookup.put(s.toString(), s);
@@ -671,7 +671,7 @@ public abstract class SolrQueryParserBase extends QueryBuilder {
 
 
   protected ReversedWildcardFilterFactory getReversedWildcardFilterFactory(FieldType fieldType) {
-    if (leadingWildcards == null) leadingWildcards = new HashMap<FieldType, ReversedWildcardFilterFactory>();
+    if (leadingWildcards == null) leadingWildcards = new HashMap<>();
     ReversedWildcardFilterFactory fac = leadingWildcards.get(fieldType);
     if (fac != null || leadingWildcards.containsKey(fac)) {
       return fac;
diff --git solr/core/src/java/org/apache/solr/request/DocValuesFacets.java solr/core/src/java/org/apache/solr/request/DocValuesFacets.java
index 81c04f8..9b3db15 100644
--- solr/core/src/java/org/apache/solr/request/DocValuesFacets.java
+++ solr/core/src/java/org/apache/solr/request/DocValuesFacets.java
@@ -59,7 +59,7 @@ public class DocValuesFacets {
   public static NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {
     SchemaField schemaField = searcher.getSchema().getField(fieldName);
     FieldType ft = schemaField.getType();
-    NamedList<Integer> res = new NamedList<Integer>();
+    NamedList<Integer> res = new NamedList<>();
 
     final SortedSetDocValues si; // for term lookups only
     OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones
diff --git solr/core/src/java/org/apache/solr/request/LocalSolrQueryRequest.java solr/core/src/java/org/apache/solr/request/LocalSolrQueryRequest.java
index fd601ef..e5730e6 100644
--- solr/core/src/java/org/apache/solr/request/LocalSolrQueryRequest.java
+++ solr/core/src/java/org/apache/solr/request/LocalSolrQueryRequest.java
@@ -37,7 +37,7 @@ public class LocalSolrQueryRequest extends SolrQueryRequestBase {
   public final static Map emptyArgs = new HashMap(0,1);
 
   protected static SolrParams makeParams(String query, String qtype, int start, int limit, Map args) {
-    Map<String,String[]> map = new HashMap<String,String[]>();
+    Map<String,String[]> map = new HashMap<>();
     for (Iterator iter = args.entrySet().iterator(); iter.hasNext();) {
       Map.Entry e = (Map.Entry)iter.next();
       String k = e.getKey().toString();
diff --git solr/core/src/java/org/apache/solr/request/NumericFacets.java solr/core/src/java/org/apache/solr/request/NumericFacets.java
index 62950e2..d88fecf 100644
--- solr/core/src/java/org/apache/solr/request/NumericFacets.java
+++ solr/core/src/java/org/apache/solr/request/NumericFacets.java
@@ -235,13 +235,13 @@ final class NumericFacets {
 
     // 4. build the NamedList
     final ValueSource vs = ft.getValueSource(sf, null);
-    final NamedList<Integer> result = new NamedList<Integer>();
+    final NamedList<Integer> result = new NamedList<>();
 
     // This stuff is complicated because if facet.mincount=0, the counts needs
     // to be merged with terms from the terms dict
     if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {
       // Only keep items we're interested in
-      final Deque<Entry> counts = new ArrayDeque<Entry>();
+      final Deque<Entry> counts = new ArrayDeque<>();
       while (pq.size() > offset) {
         counts.addFirst(pq.pop());
       }
@@ -258,7 +258,7 @@ final class NumericFacets {
           throw new IllegalStateException("Cannot use " + FacetParams.FACET_MINCOUNT + "=0 on field " + sf.getName() + " which is not indexed");
         }
         // Add zeros until there are limit results
-        final Set<String> alreadySeen = new HashSet<String>();
+        final Set<String> alreadySeen = new HashSet<>();
         while (pq.size() > 0) {
           Entry entry = pq.pop();
           final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);
@@ -314,7 +314,7 @@ final class NumericFacets {
       if (!sf.indexed()) {
         throw new IllegalStateException("Cannot use " + FacetParams.FACET_SORT + "=" + FacetParams.FACET_SORT_INDEX + " on a field which is not indexed");
       }
-      final Map<String, Integer> counts = new HashMap<String, Integer>();
+      final Map<String, Integer> counts = new HashMap<>();
       while (pq.size() > 0) {
         final Entry entry = pq.pop();
         final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);
diff --git solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java
index 7573a1b..6ad399c 100644
--- solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java
+++ solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java
@@ -78,7 +78,7 @@ class PerSegmentSingleValuedFaceting {
 
   NamedList<Integer> getFacetCounts(Executor executor) throws IOException {
 
-    CompletionService<SegFacet> completionService = new ExecutorCompletionService<SegFacet>(executor);
+    CompletionService<SegFacet> completionService = new ExecutorCompletionService<>(executor);
 
     // reuse the translation logic to go from top level set to per-segment set
     baseSet = docs.getTopFilter();
@@ -87,7 +87,7 @@ class PerSegmentSingleValuedFaceting {
     // The list of pending tasks that aren't immediately submitted
     // TODO: Is there a completion service, or a delegating executor that can
     // limit the number of concurrent tasks submitted to a bigger executor?
-    LinkedList<Callable<SegFacet>> pending = new LinkedList<Callable<SegFacet>>();
+    LinkedList<Callable<SegFacet>> pending = new LinkedList<>();
 
     int threads = nThreads <= 0 ? Integer.MAX_VALUE : nThreads;
 
@@ -308,7 +308,7 @@ class CountSortedFacetCollector extends FacetCollector {
     this.offset = offset;
     this.limit = limit;
     maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;
-    queue = new BoundedTreeSet<SimpleFacets.CountPair<String,Integer>>(maxsize);
+    queue = new BoundedTreeSet<>(maxsize);
     min=mincount-1;  // the smallest value in the top 'N' values
   }
 
@@ -319,7 +319,7 @@ class CountSortedFacetCollector extends FacetCollector {
       // index order, so we already know that the keys are ordered.  This can be very
       // important if a lot of the counts are repeated (like zero counts would be).
       UnicodeUtil.UTF8toUTF16(term, spare);
-      queue.add(new SimpleFacets.CountPair<String,Integer>(spare.toString(), count));
+      queue.add(new SimpleFacets.CountPair<>(spare.toString(), count));
       if (queue.size()>=maxsize) min=queue.last().val;
     }
     return false;
@@ -327,7 +327,7 @@ class CountSortedFacetCollector extends FacetCollector {
 
   @Override
   public NamedList<Integer> getFacetCounts() {
-    NamedList<Integer> res = new NamedList<Integer>();
+    NamedList<Integer> res = new NamedList<>();
     int off=offset;
     int lim=limit>=0 ? limit : Integer.MAX_VALUE;
      // now select the right page from the results
@@ -347,7 +347,7 @@ class IndexSortedFacetCollector extends FacetCollector {
   int offset;
   int limit;
   final int mincount;
-  final NamedList<Integer> res = new NamedList<Integer>();
+  final NamedList<Integer> res = new NamedList<>();
 
   public IndexSortedFacetCollector(int offset, int limit, int mincount) {
     this.offset = offset;
diff --git solr/core/src/java/org/apache/solr/request/SimpleFacets.java solr/core/src/java/org/apache/solr/request/SimpleFacets.java
index 9325c5b..e247ff4 100644
--- solr/core/src/java/org/apache/solr/request/SimpleFacets.java
+++ solr/core/src/java/org/apache/solr/request/SimpleFacets.java
@@ -184,7 +184,7 @@ public class SimpleFacets {
     if (tagMap != null && rb != null) {
       List<String> excludeTagList = StrUtils.splitSmart(excludeStr,',');
 
-      IdentityHashMap<Query,Boolean> excludeSet = new IdentityHashMap<Query,Boolean>();
+      IdentityHashMap<Query,Boolean> excludeSet = new IdentityHashMap<>();
       for (String excludeTag : excludeTagList) {
         Object olst = tagMap.get(excludeTag);
         // tagMap has entries of List<String,List<QParser>>, but subject to change in the future
@@ -197,7 +197,7 @@ public class SimpleFacets {
       }
       if (excludeSet.size() == 0) return;
 
-      List<Query> qlist = new ArrayList<Query>();
+      List<Query> qlist = new ArrayList<>();
 
       // add the base query
       if (!excludeSet.containsKey(rb.getQuery())) {
@@ -254,7 +254,7 @@ public class SimpleFacets {
     if (!params.getBool(FacetParams.FACET,true))
       return null;
 
-    facetResponse = new SimpleOrderedMap<Object>();
+    facetResponse = new SimpleOrderedMap<>();
     try {
       facetResponse.add("facet_queries", getFacetQueryCounts());
       facetResponse.add("facet_fields", getFacetFieldCounts());
@@ -277,7 +277,7 @@ public class SimpleFacets {
    */
   public NamedList<Integer> getFacetQueryCounts() throws IOException,SyntaxError {
 
-    NamedList<Integer> res = new SimpleOrderedMap<Integer>();
+    NamedList<Integer> res = new SimpleOrderedMap<>();
 
     /* Ignore CommonParams.DF - could have init param facet.query assuming
      * the schema default with query param DF intented to only affect Q.
@@ -341,7 +341,7 @@ public class SimpleFacets {
   public NamedList<Integer> getTermCounts(String field, DocSet base) throws IOException {
     int offset = params.getFieldInt(field, FacetParams.FACET_OFFSET, 0);
     int limit = params.getFieldInt(field, FacetParams.FACET_LIMIT, 100);
-    if (limit == 0) return new NamedList<Integer>();
+    if (limit == 0) return new NamedList<>();
     Integer mincount = params.getFieldInt(field, FacetParams.FACET_MINCOUNT);
     if (mincount==null) {
       Boolean zeros = params.getFieldBool(field, FacetParams.FACET_ZEROS);
@@ -481,7 +481,7 @@ public class SimpleFacets {
 
     CharsRef charsRef = new CharsRef();
     FieldType facetFieldType = searcher.getSchema().getFieldType(field);
-    NamedList<Integer> facetCounts = new NamedList<Integer>();
+    NamedList<Integer> facetCounts = new NamedList<>();
     List<TermGroupFacetCollector.FacetEntry> scopedEntries 
       = result.getFacetEntries(offset, limit < 0 ? Integer.MAX_VALUE : limit);
     for (TermGroupFacetCollector.FacetEntry facetEntry : scopedEntries) {
@@ -524,7 +524,7 @@ public class SimpleFacets {
   public NamedList<Object> getFacetFieldCounts()
       throws IOException, SyntaxError {
 
-    NamedList<Object> res = new SimpleOrderedMap<Object>();
+    NamedList<Object> res = new SimpleOrderedMap<>();
     String[] facetFs = params.getParams(FacetParams.FACET_FIELD);
     if (null == facetFs) {
       return res;
@@ -536,7 +536,7 @@ public class SimpleFacets {
     int maxThreads = req.getParams().getInt(FacetParams.FACET_THREADS, 0);
     Executor executor = maxThreads == 0 ? directExecutor : facetExecutor;
     final Semaphore semaphore = new Semaphore((maxThreads <= 0) ? Integer.MAX_VALUE : maxThreads);
-    List<Future<NamedList>> futures = new ArrayList<Future<NamedList>>(facetFs.length);
+    List<Future<NamedList>> futures = new ArrayList<>(facetFs.length);
 
     try {
       //Loop over fields; submit to executor, keeping the future
@@ -550,7 +550,7 @@ public class SimpleFacets {
           @Override
           public NamedList call() throws Exception {
             try {
-              NamedList<Object> result = new SimpleOrderedMap<Object>();
+              NamedList<Object> result = new SimpleOrderedMap<>();
               if(termList != null) {
                 result.add(workerKey, getListedTermCounts(workerFacetValue, termList, workerBase));
               } else {
@@ -568,7 +568,7 @@ public class SimpleFacets {
           }
         };
 
-        RunnableFuture<NamedList> runnableFuture = new FutureTask<NamedList>(callable);
+        RunnableFuture<NamedList> runnableFuture = new FutureTask<>(callable);
         semaphore.acquire();//may block and/or interrupt
         executor.execute(runnableFuture);//releases semaphore when done
         futures.add(runnableFuture);
@@ -602,7 +602,7 @@ public class SimpleFacets {
   private NamedList getListedTermCounts(String field, String termList, DocSet base) throws IOException {
     FieldType ft = searcher.getSchema().getFieldType(field);
     List<String> terms = StrUtils.splitSmart(termList, ",", true);
-    NamedList<Integer> res = new NamedList<Integer>();
+    NamedList<Integer> res = new NamedList<>();
     for (String term : terms) {
       String internal = ft.toInternal(term);
       int count = searcher.numDocs(new TermQuery(new Term(field, internal)), base);
@@ -646,7 +646,7 @@ public class SimpleFacets {
     // trying to pass all the various params around.
 
     FieldType ft = searcher.getSchema().getFieldType(fieldName);
-    NamedList<Integer> res = new NamedList<Integer>();
+    NamedList<Integer> res = new NamedList<>();
 
     SortedDocValues si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);
 
@@ -808,7 +808,7 @@ public class SimpleFacets {
     boolean sortByCount = sort.equals("count") || sort.equals("true");
     final int maxsize = limit>=0 ? offset+limit : Integer.MAX_VALUE-1;
     final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = sortByCount ? new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize) : null;
-    final NamedList<Integer> res = new NamedList<Integer>();
+    final NamedList<Integer> res = new NamedList<>();
 
     int min=mincount-1;  // the smallest value in the top 'N' values    
     int off=offset;
@@ -908,7 +908,7 @@ public class SimpleFacets {
           if (sortByCount) {
             if (c>min) {
               BytesRef termCopy = BytesRef.deepCopyOf(term);
-              queue.add(new CountPair<BytesRef,Integer>(termCopy, c));
+              queue.add(new CountPair<>(termCopy, c));
               if (queue.size()>=maxsize) min=queue.last().val;
             }
           } else {
@@ -952,7 +952,7 @@ public class SimpleFacets {
   public NamedList<Object> getFacetDateCounts()
     throws IOException, SyntaxError {
 
-    final NamedList<Object> resOuter = new SimpleOrderedMap<Object>();
+    final NamedList<Object> resOuter = new SimpleOrderedMap<>();
     final String[] fields = params.getParams(FacetParams.FACET_DATE);
 
     if (null == fields || 0 == fields.length) return resOuter;
@@ -977,7 +977,7 @@ public class SimpleFacets {
     String f = facetValue;
 
 
-    final NamedList<Object> resInner = new SimpleOrderedMap<Object>();
+    final NamedList<Object> resInner = new SimpleOrderedMap<>();
     resOuter.add(key, resInner);
     final SchemaField sf = schema.getField(f);
     if (! (sf.getType() instanceof DateField)) {
@@ -1133,7 +1133,7 @@ public class SimpleFacets {
    */
 
   public NamedList<Object> getFacetRangeCounts() throws IOException, SyntaxError {
-    final NamedList<Object> resOuter = new SimpleOrderedMap<Object>();
+    final NamedList<Object> resOuter = new SimpleOrderedMap<>();
     final String[] fields = params.getParams(FacetParams.FACET_RANGE);
 
     if (null == fields || 0 == fields.length) return resOuter;
@@ -1203,8 +1203,8 @@ public class SimpleFacets {
      final RangeEndpointCalculator<T> calc) throws IOException {
     
     final String f = sf.getName();
-    final NamedList<Object> res = new SimpleOrderedMap<Object>();
-    final NamedList<Integer> counts = new NamedList<Integer>();
+    final NamedList<Object> res = new SimpleOrderedMap<>();
+    final NamedList<Integer> counts = new NamedList<>();
     res.add("counts", counts);
 
     final T start = calc.getValue(required.getFieldParam(f,FacetParams.FACET_RANGE_START));
diff --git solr/core/src/java/org/apache/solr/request/SolrQueryRequestBase.java solr/core/src/java/org/apache/solr/request/SolrQueryRequestBase.java
index 1ad75bb..b243ccf 100644
--- solr/core/src/java/org/apache/solr/request/SolrQueryRequestBase.java
+++ solr/core/src/java/org/apache/solr/request/SolrQueryRequestBase.java
@@ -57,7 +57,7 @@ public abstract class SolrQueryRequestBase implements SolrQueryRequest {
   @Override
   public Map<Object,Object> getContext() {
     // SolrQueryRequest as a whole isn't thread safe, and this isn't either.
-    if (context==null) context = new HashMap<Object,Object>();
+    if (context==null) context = new HashMap<>();
     return context;
   }
 
diff --git solr/core/src/java/org/apache/solr/request/SolrRequestInfo.java solr/core/src/java/org/apache/solr/request/SolrRequestInfo.java
index bbea9b2..eaef45a 100644
--- solr/core/src/java/org/apache/solr/request/SolrRequestInfo.java
+++ solr/core/src/java/org/apache/solr/request/SolrRequestInfo.java
@@ -32,7 +32,7 @@ import java.util.List;
 
 
 public class SolrRequestInfo {
-  protected final static ThreadLocal<SolrRequestInfo> threadLocal = new ThreadLocal<SolrRequestInfo>();
+  protected final static ThreadLocal<SolrRequestInfo> threadLocal = new ThreadLocal<>();
 
   protected SolrQueryRequest req;
   protected SolrQueryResponse rsp;
@@ -132,7 +132,7 @@ public class SolrRequestInfo {
     // is this better here, or on SolrQueryRequest?
     synchronized (this) {
       if (closeHooks == null) {
-        closeHooks = new LinkedList<Closeable>();
+        closeHooks = new LinkedList<>();
       }
       closeHooks.add(hook);
     }
diff --git solr/core/src/java/org/apache/solr/request/UnInvertedField.java solr/core/src/java/org/apache/solr/request/UnInvertedField.java
index a7d6230..4e591ba 100644
--- solr/core/src/java/org/apache/solr/request/UnInvertedField.java
+++ solr/core/src/java/org/apache/solr/request/UnInvertedField.java
@@ -100,7 +100,7 @@ public class UnInvertedField extends DocTermOrds {
 
   int[] maxTermCounts = new int[1024];
 
-  final Map<Integer,TopTerm> bigTerms = new LinkedHashMap<Integer,TopTerm>();
+  final Map<Integer,TopTerm> bigTerms = new LinkedHashMap<>();
 
   private SolrIndexSearcher.DocsEnumState deState;
   private final SolrIndexSearcher searcher;
@@ -220,7 +220,7 @@ public class UnInvertedField extends DocTermOrds {
 
     FieldType ft = searcher.getSchema().getFieldType(field);
 
-    NamedList<Integer> res = new NamedList<Integer>();  // order is important
+    NamedList<Integer> res = new NamedList<>();  // order is important
 
     DocSet docs = baseDocs;
     int baseSize = docs.size();
diff --git solr/core/src/java/org/apache/solr/response/BinaryResponseWriter.java solr/core/src/java/org/apache/solr/response/BinaryResponseWriter.java
index 3ef0112..3ec8dde 100644
--- solr/core/src/java/org/apache/solr/response/BinaryResponseWriter.java
+++ solr/core/src/java/org/apache/solr/response/BinaryResponseWriter.java
@@ -40,7 +40,7 @@ import org.slf4j.LoggerFactory;
 
 public class BinaryResponseWriter implements BinaryQueryResponseWriter {
   private static final Logger LOG = LoggerFactory.getLogger(BinaryResponseWriter.class);
-  public static final Set<Class> KNOWN_TYPES = new HashSet<Class>();
+  public static final Set<Class> KNOWN_TYPES = new HashSet<>();
 
   @Override
   public void write(OutputStream out, SolrQueryRequest req, SolrQueryResponse response) throws IOException {
diff --git solr/core/src/java/org/apache/solr/response/CSVResponseWriter.java solr/core/src/java/org/apache/solr/response/CSVResponseWriter.java
index e49b078..3ef75c9 100644
--- solr/core/src/java/org/apache/solr/response/CSVResponseWriter.java
+++ solr/core/src/java/org/apache/solr/response/CSVResponseWriter.java
@@ -146,12 +146,12 @@ class CSVWriter extends TextResponseWriter {
     CSVSharedBufPrinter mvPrinter;  // printer used to encode multiple values in a single CSV value
 
     // used to collect values
-    List<IndexableField> values = new ArrayList<IndexableField>(1);  // low starting amount in case there are many fields
+    List<IndexableField> values = new ArrayList<>(1);  // low starting amount in case there are many fields
     int tmp;
   }
 
   int pass;
-  Map<String,CSVField> csvFields = new LinkedHashMap<String,CSVField>();
+  Map<String,CSVField> csvFields = new LinkedHashMap<>();
 
   Calendar cal;  // for formatting date objects
 
@@ -242,7 +242,7 @@ class CSVWriter extends TextResponseWriter {
       if (responseObj instanceof SolrDocumentList) {
         // get the list of fields from the SolrDocumentList
         if(fields==null) {
-          fields = new LinkedHashSet<String>();
+          fields = new LinkedHashSet<>();
         }
         for (SolrDocument sdoc: (SolrDocumentList)responseObj) {
           fields.addAll(sdoc.getFieldNames());
diff --git solr/core/src/java/org/apache/solr/response/JSONResponseWriter.java solr/core/src/java/org/apache/solr/response/JSONResponseWriter.java
index 301ad3f..d48d31e 100644
--- solr/core/src/java/org/apache/solr/response/JSONResponseWriter.java
+++ solr/core/src/java/org/apache/solr/response/JSONResponseWriter.java
@@ -124,7 +124,7 @@ class JSONWriter extends TextResponseWriter {
     // Disad: this is ambiguous with a real single value that happens to be an array
     //
     // Both of these mappings have ambiguities.
-    HashMap<String,Integer> repeats = new HashMap<String,Integer>(4);
+    HashMap<String,Integer> repeats = new HashMap<>(4);
 
     boolean first=true;
     for (int i=0; i<sz; i++) {
@@ -314,7 +314,7 @@ class JSONWriter extends TextResponseWriter {
     final ArrayList<IndexableField> fields;
     MultiValueField(SchemaField sfield, IndexableField firstVal) {
       this.sfield = sfield;
-      this.fields = new ArrayList<IndexableField>(4);
+      this.fields = new ArrayList<>(4);
       this.fields.add(firstVal);
     }
   }
diff --git solr/core/src/java/org/apache/solr/response/PHPSerializedResponseWriter.java solr/core/src/java/org/apache/solr/response/PHPSerializedResponseWriter.java
index 5e0f3f4..ba1933a 100644
--- solr/core/src/java/org/apache/solr/response/PHPSerializedResponseWriter.java
+++ solr/core/src/java/org/apache/solr/response/PHPSerializedResponseWriter.java
@@ -121,8 +121,8 @@ class PHPSerializedWriter extends JSONWriter {
   {
     writeKey(idx, false);
     
-    LinkedHashMap <String,Object> single = new LinkedHashMap<String, Object>();
-    LinkedHashMap <String,Object> multi = new LinkedHashMap<String, Object>();
+    LinkedHashMap <String,Object> single = new LinkedHashMap<>();
+    LinkedHashMap <String,Object> multi = new LinkedHashMap<>();
 
     for (String fname : doc.getFieldNames()) {
       if(!returnFields.wantsField(fname)){
diff --git solr/core/src/java/org/apache/solr/response/SolrQueryResponse.java solr/core/src/java/org/apache/solr/response/SolrQueryResponse.java
index 88955c9..5d51ebd 100644
--- solr/core/src/java/org/apache/solr/response/SolrQueryResponse.java
+++ solr/core/src/java/org/apache/solr/response/SolrQueryResponse.java
@@ -68,13 +68,13 @@ public class SolrQueryResponse {
    * @see #setAllValues
    * @see <a href="#returnable_data">Note on Returnable Data</a>
    */
-  protected NamedList<Object> values = new SimpleOrderedMap<Object>();
+  protected NamedList<Object> values = new SimpleOrderedMap<>();
   
   
 /**
    * Container for storing information that should be logged by Solr before returning.
    */
-  protected NamedList<Object> toLog = new SimpleOrderedMap<Object>();
+  protected NamedList<Object> toLog = new SimpleOrderedMap<>();
 
   protected ReturnFields returnFields;
 
diff --git solr/core/src/java/org/apache/solr/response/TextResponseWriter.java solr/core/src/java/org/apache/solr/response/TextResponseWriter.java
index 08630c6..513ffd6 100644
--- solr/core/src/java/org/apache/solr/response/TextResponseWriter.java
+++ solr/core/src/java/org/apache/solr/response/TextResponseWriter.java
@@ -235,7 +235,7 @@ public abstract class TextResponseWriter {
       if (existing == null) {
         SchemaField sf = schema.getFieldOrNull(f.name());
         if (sf != null && sf.multiValued()) {
-          List<Object> vals = new ArrayList<Object>();
+          List<Object> vals = new ArrayList<>();
           vals.add( f );
           out.setField( f.name(), vals );
         } 
diff --git solr/core/src/java/org/apache/solr/response/transform/DocTransformers.java solr/core/src/java/org/apache/solr/response/transform/DocTransformers.java
index 950b1cc..579e7fe 100644
--- solr/core/src/java/org/apache/solr/response/transform/DocTransformers.java
+++ solr/core/src/java/org/apache/solr/response/transform/DocTransformers.java
@@ -32,7 +32,7 @@ import org.apache.solr.request.SolrQueryRequest;
  */
 public class DocTransformers extends DocTransformer
 {
-  final List<DocTransformer> children = new ArrayList<DocTransformer>();
+  final List<DocTransformer> children = new ArrayList<>();
 
   @Override
   public String getName()
diff --git solr/core/src/java/org/apache/solr/response/transform/TransformerFactory.java solr/core/src/java/org/apache/solr/response/transform/TransformerFactory.java
index 0b389b1..44d46bc 100644
--- solr/core/src/java/org/apache/solr/response/transform/TransformerFactory.java
+++ solr/core/src/java/org/apache/solr/response/transform/TransformerFactory.java
@@ -41,7 +41,7 @@ public abstract class TransformerFactory implements NamedListInitializedPlugin
 
   public abstract DocTransformer create(String field, SolrParams params, SolrQueryRequest req);
 
-  public static final Map<String,TransformerFactory> defaultFactories = new HashMap<String,TransformerFactory>();
+  public static final Map<String,TransformerFactory> defaultFactories = new HashMap<>();
   static {
     defaultFactories.put( "explain", new ExplainAugmenterFactory() );
     defaultFactories.put( "value", new ValueAugmenterFactory() );
diff --git solr/core/src/java/org/apache/solr/rest/schema/BaseFieldResource.java solr/core/src/java/org/apache/solr/rest/schema/BaseFieldResource.java
index b8a093d..337f50d 100644
--- solr/core/src/java/org/apache/solr/rest/schema/BaseFieldResource.java
+++ solr/core/src/java/org/apache/solr/rest/schema/BaseFieldResource.java
@@ -64,7 +64,7 @@ abstract class BaseFieldResource extends BaseSchemaResource {
       if (null != flParam) {
         String[] fields = flParam.trim().split("[,\\s]+");
         if (fields.length > 0) {
-          requestedFields = new LinkedHashSet<String>();
+          requestedFields = new LinkedHashSet<>();
           for (String field : fields) {
             if ( ! field.trim().isEmpty()) {
               requestedFields.add(field.trim());
diff --git solr/core/src/java/org/apache/solr/rest/schema/CopyFieldCollectionResource.java solr/core/src/java/org/apache/solr/rest/schema/CopyFieldCollectionResource.java
index 8c8f8e9..a90391d 100644
--- solr/core/src/java/org/apache/solr/rest/schema/CopyFieldCollectionResource.java
+++ solr/core/src/java/org/apache/solr/rest/schema/CopyFieldCollectionResource.java
@@ -77,7 +77,7 @@ public class CopyFieldCollectionResource extends BaseFieldResource implements GE
       if (null != sourceFieldListParam) {
         String[] fields = sourceFieldListParam.trim().split("[,\\s]+");
         if (fields.length > 0) {
-          requestedSourceFields = new HashSet<String>(Arrays.asList(fields));
+          requestedSourceFields = new HashSet<>(Arrays.asList(fields));
           requestedSourceFields.remove(""); // Remove empty values, if any
         }
       }
@@ -85,7 +85,7 @@ public class CopyFieldCollectionResource extends BaseFieldResource implements GE
       if (null != destinationFieldListParam) {
         String[] fields = destinationFieldListParam.trim().split("[,\\s]+");
         if (fields.length > 0) {
-          requestedDestinationFields = new HashSet<String>(Arrays.asList(fields));
+          requestedDestinationFields = new HashSet<>(Arrays.asList(fields));
           requestedDestinationFields.remove(""); // Remove empty values, if any
         }
       }
diff --git solr/core/src/java/org/apache/solr/rest/schema/DynamicFieldCollectionResource.java solr/core/src/java/org/apache/solr/rest/schema/DynamicFieldCollectionResource.java
index 1078311..cc89d11 100644
--- solr/core/src/java/org/apache/solr/rest/schema/DynamicFieldCollectionResource.java
+++ solr/core/src/java/org/apache/solr/rest/schema/DynamicFieldCollectionResource.java
@@ -56,7 +56,7 @@ public class DynamicFieldCollectionResource extends BaseFieldResource implements
   public Representation get() {
     
     try {
-      List<SimpleOrderedMap<Object>> props = new ArrayList<SimpleOrderedMap<Object>>();
+      List<SimpleOrderedMap<Object>> props = new ArrayList<>();
       if (null == getRequestedFields()) {
         for (IndexSchema.DynamicField dynamicField : getSchema().getDynamicFields()) {
           if ( ! dynamicField.getRegex().startsWith(IndexSchema.INTERNAL_POLY_FIELD_PREFIX)) { // omit internal polyfields
@@ -68,7 +68,7 @@ public class DynamicFieldCollectionResource extends BaseFieldResource implements
           String message = "Empty " + CommonParams.FL + " parameter value";
           throw new SolrException(ErrorCode.BAD_REQUEST, message);
         }
-        Map<String,SchemaField> dynamicFieldsByName = new HashMap<String,SchemaField>();
+        Map<String,SchemaField> dynamicFieldsByName = new HashMap<>();
         for (IndexSchema.DynamicField dynamicField : getSchema().getDynamicFields()) {
           dynamicFieldsByName.put(dynamicField.getRegex(), dynamicField.getPrototype());
         }
diff --git solr/core/src/java/org/apache/solr/rest/schema/FieldCollectionResource.java solr/core/src/java/org/apache/solr/rest/schema/FieldCollectionResource.java
index e9a4254..cb83e70 100644
--- solr/core/src/java/org/apache/solr/rest/schema/FieldCollectionResource.java
+++ solr/core/src/java/org/apache/solr/rest/schema/FieldCollectionResource.java
@@ -79,9 +79,9 @@ public class FieldCollectionResource extends BaseFieldResource implements GETabl
   @Override
   public Representation get() {
     try {
-      final List<SimpleOrderedMap<Object>> props = new ArrayList<SimpleOrderedMap<Object>>();
+      final List<SimpleOrderedMap<Object>> props = new ArrayList<>();
       if (null == getRequestedFields()) {
-        SortedSet<String> fieldNames = new TreeSet<String>(getSchema().getFields().keySet());
+        SortedSet<String> fieldNames = new TreeSet<>(getSchema().getFields().keySet());
         for (String fieldName : fieldNames) {
           props.add(getFieldProperties(getSchema().getFields().get(fieldName)));
         }
@@ -138,7 +138,7 @@ public class FieldCollectionResource extends BaseFieldResource implements GETabl
             throw new SolrException(ErrorCode.BAD_REQUEST, message);
           } else {
             List<Map<String, Object>> list = (List<Map<String, Object>>) object;
-            List<SchemaField> newFields = new ArrayList<SchemaField>();
+            List<SchemaField> newFields = new ArrayList<>();
             IndexSchema oldSchema = getSchema();
             Map<String, Collection<String>> copyFields = new HashMap<>();
             Set<String> malformed = new HashSet<>();
diff --git solr/core/src/java/org/apache/solr/rest/schema/FieldTypeCollectionResource.java solr/core/src/java/org/apache/solr/rest/schema/FieldTypeCollectionResource.java
index 01aa815..5693e80 100644
--- solr/core/src/java/org/apache/solr/rest/schema/FieldTypeCollectionResource.java
+++ solr/core/src/java/org/apache/solr/rest/schema/FieldTypeCollectionResource.java
@@ -60,8 +60,8 @@ public class FieldTypeCollectionResource extends BaseFieldTypeResource implement
   @Override
   public Representation get() {
     try {
-      List<SimpleOrderedMap<Object>> props = new ArrayList<SimpleOrderedMap<Object>>();
-      Map<String,FieldType> sortedFieldTypes = new TreeMap<String, FieldType>(getSchema().getFieldTypes());
+      List<SimpleOrderedMap<Object>> props = new ArrayList<>();
+      Map<String,FieldType> sortedFieldTypes = new TreeMap<>(getSchema().getFieldTypes());
       for (FieldType fieldType : sortedFieldTypes.values()) {
         props.add(getFieldTypeProperties(fieldType));
       }
@@ -99,12 +99,12 @@ public class FieldTypeCollectionResource extends BaseFieldTypeResource implement
    * The map only includes field types that are used by at least one field.  
    */
   private Map<String,List<String>> getFieldsByFieldType() {
-    Map<String,List<String>> fieldsByFieldType = new HashMap<String,List<String>>();
+    Map<String,List<String>> fieldsByFieldType = new HashMap<>();
     for (SchemaField schemaField : getSchema().getFields().values()) {
       final String fieldType = schemaField.getType().getTypeName();
       List<String> fields = fieldsByFieldType.get(fieldType);
       if (null == fields) {
-        fields = new ArrayList<String>();
+        fields = new ArrayList<>();
         fieldsByFieldType.put(fieldType, fields);
       }
       fields.add(schemaField.getName());
@@ -120,12 +120,12 @@ public class FieldTypeCollectionResource extends BaseFieldTypeResource implement
    * The map only includes field types that are used by at least one dynamic field.  
    */
   private Map<String,List<String>> getDynamicFieldsByFieldType() {
-    Map<String,List<String>> dynamicFieldsByFieldType = new HashMap<String,List<String>>();
+    Map<String,List<String>> dynamicFieldsByFieldType = new HashMap<>();
     for (SchemaField schemaField : getSchema().getDynamicFieldPrototypes()) {
       final String fieldType = schemaField.getType().getTypeName();
       List<String> dynamicFields = dynamicFieldsByFieldType.get(fieldType);
       if (null == dynamicFields) {
-        dynamicFields = new ArrayList<String>();
+        dynamicFields = new ArrayList<>();
         dynamicFieldsByFieldType.put(fieldType, dynamicFields);
       }
       dynamicFields.add(schemaField.getName());
diff --git solr/core/src/java/org/apache/solr/rest/schema/FieldTypeResource.java solr/core/src/java/org/apache/solr/rest/schema/FieldTypeResource.java
index 353aa12..aab8532 100644
--- solr/core/src/java/org/apache/solr/rest/schema/FieldTypeResource.java
+++ solr/core/src/java/org/apache/solr/rest/schema/FieldTypeResource.java
@@ -88,7 +88,7 @@ public class FieldTypeResource extends BaseFieldTypeResource implements GETable
    */
   @Override
   protected List<String> getFieldsWithFieldType(FieldType fieldType) {
-    List<String> fields = new ArrayList<String>();
+    List<String> fields = new ArrayList<>();
     for (SchemaField schemaField : getSchema().getFields().values()) {
       if (schemaField.getType().getTypeName().equals(fieldType.getTypeName())) {
         fields.add(schemaField.getName());
@@ -104,7 +104,7 @@ public class FieldTypeResource extends BaseFieldTypeResource implements GETable
    */
   @Override
   protected List<String> getDynamicFieldsWithFieldType(FieldType fieldType) {
-    List<String> dynamicFields = new ArrayList<String>();
+    List<String> dynamicFields = new ArrayList<>();
     for (SchemaField prototype : getSchema().getDynamicFieldPrototypes()) {
       if (prototype.getType().getTypeName().equals(fieldType.getTypeName())) {
         dynamicFields.add(prototype.getName());
diff --git solr/core/src/java/org/apache/solr/rest/schema/SolrQueryParserResource.java solr/core/src/java/org/apache/solr/rest/schema/SolrQueryParserResource.java
index 967759c..5103d11 100644
--- solr/core/src/java/org/apache/solr/rest/schema/SolrQueryParserResource.java
+++ solr/core/src/java/org/apache/solr/rest/schema/SolrQueryParserResource.java
@@ -43,7 +43,7 @@ public class SolrQueryParserResource extends BaseSchemaResource implements GETab
   @Override
   public Representation get() {
     try {
-      SimpleOrderedMap<Object> props = new SimpleOrderedMap<Object>();
+      SimpleOrderedMap<Object> props = new SimpleOrderedMap<>();
       props.add(IndexSchema.DEFAULT_OPERATOR, getSchema().getQueryParserDefaultOperator());
       getSolrResponse().add(IndexSchema.SOLR_QUERY_PARSER, props);
     } catch (Exception e) {
diff --git solr/core/src/java/org/apache/solr/schema/AbstractSpatialFieldType.java solr/core/src/java/org/apache/solr/schema/AbstractSpatialFieldType.java
index f913fbb..3b1ddd1 100644
--- solr/core/src/java/org/apache/solr/schema/AbstractSpatialFieldType.java
+++ solr/core/src/java/org/apache/solr/schema/AbstractSpatialFieldType.java
@@ -103,7 +103,7 @@ public abstract class AbstractSpatialFieldType<T extends SpatialStrategy> extend
     }
 
     //Solr expects us to remove the parameters we've used.
-    MapListener<String, String> argsWrap = new MapListener<String, String>(args);
+    MapListener<String, String> argsWrap = new MapListener<>(args);
     ctx = SpatialContextFactory.makeSpatialContext(argsWrap, schema.getResourceLoader().getClassLoader());
     args.keySet().removeAll(argsWrap.getSeenKeys());
 
@@ -143,7 +143,7 @@ public abstract class AbstractSpatialFieldType<T extends SpatialStrategy> extend
       return Collections.emptyList();
     }
 
-    List<StorableField> result = new ArrayList<StorableField>();
+    List<StorableField> result = new ArrayList<>();
     if (field.indexed()) {
       T strategy = getStrategy(field.getName());
       result.addAll(Arrays.asList(strategy.createIndexableFields(shape)));
diff --git solr/core/src/java/org/apache/solr/schema/AbstractSpatialPrefixTreeFieldType.java solr/core/src/java/org/apache/solr/schema/AbstractSpatialPrefixTreeFieldType.java
index 59efa75..8af7e46 100644
--- solr/core/src/java/org/apache/solr/schema/AbstractSpatialPrefixTreeFieldType.java
+++ solr/core/src/java/org/apache/solr/schema/AbstractSpatialPrefixTreeFieldType.java
@@ -43,7 +43,7 @@ public abstract class AbstractSpatialPrefixTreeFieldType<T extends PrefixTreeStr
     super.init(schema, args);
 
     //Solr expects us to remove the parameters we've used.
-    MapListener<String, String> argsWrap = new MapListener<String, String>(args);
+    MapListener<String, String> argsWrap = new MapListener<>(args);
     grid = SpatialPrefixTreeFactory.makeSPT(argsWrap, schema.getResourceLoader().getClassLoader(), ctx);
     args.keySet().removeAll(argsWrap.getSeenKeys());
 
diff --git solr/core/src/java/org/apache/solr/schema/AbstractSubTypeFieldType.java solr/core/src/java/org/apache/solr/schema/AbstractSubTypeFieldType.java
index 36b8af9..bdac406 100644
--- solr/core/src/java/org/apache/solr/schema/AbstractSubTypeFieldType.java
+++ solr/core/src/java/org/apache/solr/schema/AbstractSubTypeFieldType.java
@@ -83,7 +83,7 @@ public abstract class AbstractSubTypeFieldType extends FieldType implements Sche
 
   static SchemaField registerPolyFieldDynamicPrototype(IndexSchema schema, FieldType type) {
     String name = "*" + FieldType.POLY_FIELD_SEPARATOR + type.typeName;
-    Map<String, String> props = new HashMap<String, String>();
+    Map<String, String> props = new HashMap<>();
     //Just set these, delegate everything else to the field type
     props.put("indexed", "true");
     props.put("stored", "false");
diff --git solr/core/src/java/org/apache/solr/schema/CollationField.java solr/core/src/java/org/apache/solr/schema/CollationField.java
index 891e673..13f35a8 100644
--- solr/core/src/java/org/apache/solr/schema/CollationField.java
+++ solr/core/src/java/org/apache/solr/schema/CollationField.java
@@ -262,7 +262,7 @@ public class CollationField extends FieldType {
   @Override
   public List<StorableField> createFields(SchemaField field, Object value, float boost) {
     if (field.hasDocValues()) {
-      List<StorableField> fields = new ArrayList<StorableField>();
+      List<StorableField> fields = new ArrayList<>();
       fields.add(createField(field, value, boost));
       final BytesRef bytes = getCollationKey(field.getName(), value.toString());
       if (field.multiValued()) {
diff --git solr/core/src/java/org/apache/solr/schema/CurrencyField.java solr/core/src/java/org/apache/solr/schema/CurrencyField.java
index ab4e839..7379e0f 100644
--- solr/core/src/java/org/apache/solr/schema/CurrencyField.java
+++ solr/core/src/java/org/apache/solr/schema/CurrencyField.java
@@ -128,7 +128,7 @@ public class CurrencyField extends FieldType implements SchemaAware, ResourceLoa
     // Initialize field type for amount
     fieldTypeAmountRaw = new TrieLongField();
     fieldTypeAmountRaw.setTypeName("amount_raw_type_tlong");
-    Map<String,String> map = new HashMap<String,String>(1);
+    Map<String,String> map = new HashMap<>(1);
     map.put("precisionStep", precisionStepString);
     fieldTypeAmountRaw.init(schema, map);
     
@@ -169,7 +169,7 @@ public class CurrencyField extends FieldType implements SchemaAware, ResourceLoa
   public List<StorableField> createFields(SchemaField field, Object externalVal, float boost) {
     CurrencyValue value = CurrencyValue.parse(externalVal.toString(), defaultCurrency);
 
-    List<StorableField> f = new ArrayList<StorableField>();
+    List<StorableField> f = new ArrayList<>();
     SchemaField amountField = getAmountField(field);
     f.add(amountField.createField(String.valueOf(value.getAmount()), amountField.indexed() && !amountField.omitNorms() ? boost : 1F));
     SchemaField currencyField = getCurrencyField(field);
@@ -199,7 +199,7 @@ public class CurrencyField extends FieldType implements SchemaAware, ResourceLoa
 
   private void createDynamicCurrencyField(String suffix, FieldType type) {
     String name = "*" + POLY_FIELD_SEPARATOR + suffix;
-    Map<String, String> props = new HashMap<String, String>();
+    Map<String, String> props = new HashMap<>();
     props.put("indexed", "true");
     props.put("stored", "false");
     props.put("multiValued", "false");
@@ -665,7 +665,7 @@ class FileExchangeRateProvider implements ExchangeRateProvider {
   protected static final String PARAM_CURRENCY_CONFIG       = "currencyConfig";
 
   // Exchange rate map, maps Currency Code -> Currency Code -> Rate
-  private Map<String, Map<String, Double>> rates = new HashMap<String, Map<String, Double>>();
+  private Map<String, Map<String, Double>> rates = new HashMap<>();
 
   private String currencyConfigFile;
   private ResourceLoader loader;
@@ -734,7 +734,7 @@ class FileExchangeRateProvider implements ExchangeRateProvider {
     Map<String, Double> rhs = ratesMap.get(sourceCurrencyCode);
 
     if (rhs == null) {
-      rhs = new HashMap<String, Double>();
+      rhs = new HashMap<>();
       ratesMap.put(sourceCurrencyCode, rhs);
     }
 
@@ -763,7 +763,7 @@ class FileExchangeRateProvider implements ExchangeRateProvider {
 
   @Override
   public Set<String> listAvailableCurrencies() {
-    Set<String> currencies = new HashSet<String>();
+    Set<String> currencies = new HashSet<>();
     for(String from : rates.keySet()) {
       currencies.add(from);
       for(String to : rates.get(from).keySet()) {
@@ -776,7 +776,7 @@ class FileExchangeRateProvider implements ExchangeRateProvider {
   @Override
   public boolean reload() throws SolrException {
     InputStream is = null;
-    Map<String, Map<String, Double>> tmpRates = new HashMap<String, Map<String, Double>>();
+    Map<String, Map<String, Double>> tmpRates = new HashMap<>();
     try {
       log.info("Reloading exchange rates from file "+this.currencyConfigFile);
 
diff --git solr/core/src/java/org/apache/solr/schema/EnumField.java solr/core/src/java/org/apache/solr/schema/EnumField.java
index 845b75f..3a9e121 100644
--- solr/core/src/java/org/apache/solr/schema/EnumField.java
+++ solr/core/src/java/org/apache/solr/schema/EnumField.java
@@ -61,8 +61,8 @@ public class EnumField extends PrimitiveFieldType {
   protected static final Integer DEFAULT_VALUE = -1;
   protected static final int DEFAULT_PRECISION_STEP = Integer.MAX_VALUE;
 
-  protected Map<String, Integer> enumStringToIntMap = new HashMap<String, Integer>();
-  protected Map<Integer, String> enumIntToStringMap = new HashMap<Integer, String>();
+  protected Map<String, Integer> enumStringToIntMap = new HashMap<>();
+  protected Map<Integer, String> enumIntToStringMap = new HashMap<>();
 
   protected String enumsConfigFile;
   protected String enumName;
diff --git solr/core/src/java/org/apache/solr/schema/ExternalFileFieldReloader.java solr/core/src/java/org/apache/solr/schema/ExternalFileFieldReloader.java
index 6fcd6d5..a2d6d4e 100644
--- solr/core/src/java/org/apache/solr/schema/ExternalFileFieldReloader.java
+++ solr/core/src/java/org/apache/solr/schema/ExternalFileFieldReloader.java
@@ -51,7 +51,7 @@ import java.util.List;
 public class ExternalFileFieldReloader extends AbstractSolrEventListener {
 
   private String datadir;
-  private List<FileFloatSource> fieldSources = new ArrayList<FileFloatSource>();
+  private List<FileFloatSource> fieldSources = new ArrayList<>();
 
   private static final Logger log = LoggerFactory.getLogger(ExternalFileFieldReloader.class);
 
diff --git solr/core/src/java/org/apache/solr/schema/FieldProperties.java solr/core/src/java/org/apache/solr/schema/FieldProperties.java
index 3dab1d7..a560fac 100644
--- solr/core/src/java/org/apache/solr/schema/FieldProperties.java
+++ solr/core/src/java/org/apache/solr/schema/FieldProperties.java
@@ -61,7 +61,7 @@ public abstract class FieldProperties {
           "storeOffsetsWithPositions", "docValues"
   };
 
-  static final Map<String,Integer> propertyMap = new HashMap<String,Integer>();
+  static final Map<String,Integer> propertyMap = new HashMap<>();
   static {
     for (String prop : propertyNames) {
       propertyMap.put(prop, propertyNameToInt(prop, true));
diff --git solr/core/src/java/org/apache/solr/schema/FieldType.java solr/core/src/java/org/apache/solr/schema/FieldType.java
index e7e518c..4d4ae5a 100644
--- solr/core/src/java/org/apache/solr/schema/FieldType.java
+++ solr/core/src/java/org/apache/solr/schema/FieldType.java
@@ -151,7 +151,7 @@ public abstract class FieldType extends FieldProperties {
     }
 
     this.args = Collections.unmodifiableMap(args);
-    Map<String,String> initArgs = new HashMap<String,String>(args);
+    Map<String,String> initArgs = new HashMap<>(args);
     initArgs.remove(CLASS_NAME); // consume the class arg 
 
     trueProperties = FieldProperties.parseProperties(initArgs,true,false);
@@ -785,7 +785,7 @@ public abstract class FieldType extends FieldProperties {
    * @param showDefaults if true, include default properties.
    */
   public SimpleOrderedMap<Object> getNamedPropertyValues(boolean showDefaults) {
-    SimpleOrderedMap<Object> namedPropertyValues = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> namedPropertyValues = new SimpleOrderedMap<>();
     namedPropertyValues.add(TYPE_NAME, getTypeName());
     namedPropertyValues.add(CLASS_NAME, getClassArg());
     if (showDefaults) {
@@ -829,7 +829,7 @@ public abstract class FieldType extends FieldProperties {
         namedPropertyValues.add(DOC_VALUES_FORMAT, getDocValuesFormat());
       }
     } else { // Don't show defaults
-      Set<String> fieldProperties = new HashSet<String>();
+      Set<String> fieldProperties = new HashSet<>();
       for (String propertyName : FieldProperties.propertyNames) {
         fieldProperties.add(propertyName);
       }
@@ -861,7 +861,7 @@ public abstract class FieldType extends FieldProperties {
 
   /** Returns args to this field type that aren't standard field properties */
   protected Map<String,String> getNonFieldPropertyArgs() {
-    Map<String,String> initArgs =  new HashMap<String,String>(args);
+    Map<String,String> initArgs =  new HashMap<>(args);
     for (String prop : FieldProperties.propertyNames) {
       initArgs.remove(prop);
     }
@@ -874,16 +874,16 @@ public abstract class FieldType extends FieldProperties {
    * name and args.
    */
   protected static SimpleOrderedMap<Object> getAnalyzerProperties(Analyzer analyzer) {
-    SimpleOrderedMap<Object> analyzerProps = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> analyzerProps = new SimpleOrderedMap<>();
 
     if (analyzer instanceof TokenizerChain) {
       Map<String,String> factoryArgs;
       TokenizerChain tokenizerChain = (TokenizerChain)analyzer;
       CharFilterFactory[] charFilterFactories = tokenizerChain.getCharFilterFactories();
       if (null != charFilterFactories && charFilterFactories.length > 0) {
-        List<SimpleOrderedMap<Object>> charFilterProps = new ArrayList<SimpleOrderedMap<Object>>();
+        List<SimpleOrderedMap<Object>> charFilterProps = new ArrayList<>();
         for (CharFilterFactory charFilterFactory : charFilterFactories) {
-          SimpleOrderedMap<Object> props = new SimpleOrderedMap<Object>();
+          SimpleOrderedMap<Object> props = new SimpleOrderedMap<>();
           props.add(CLASS_NAME, charFilterFactory.getClassArg());
           factoryArgs = charFilterFactory.getOriginalArgs();
           if (null != factoryArgs) {
@@ -904,7 +904,7 @@ public abstract class FieldType extends FieldProperties {
         analyzerProps.add(CHAR_FILTERS, charFilterProps);
       }
 
-      SimpleOrderedMap<Object> tokenizerProps = new SimpleOrderedMap<Object>();
+      SimpleOrderedMap<Object> tokenizerProps = new SimpleOrderedMap<>();
       TokenizerFactory tokenizerFactory = tokenizerChain.getTokenizerFactory();
       tokenizerProps.add(CLASS_NAME, tokenizerFactory.getClassArg());
       factoryArgs = tokenizerFactory.getOriginalArgs();
@@ -925,9 +925,9 @@ public abstract class FieldType extends FieldProperties {
 
       TokenFilterFactory[] filterFactories = tokenizerChain.getTokenFilterFactories();
       if (null != filterFactories && filterFactories.length > 0) {
-        List<SimpleOrderedMap<Object>> filterProps = new ArrayList<SimpleOrderedMap<Object>>();
+        List<SimpleOrderedMap<Object>> filterProps = new ArrayList<>();
         for (TokenFilterFactory filterFactory : filterFactories) {
-          SimpleOrderedMap<Object> props = new SimpleOrderedMap<Object>();
+          SimpleOrderedMap<Object> props = new SimpleOrderedMap<>();
           props.add(CLASS_NAME, filterFactory.getClassArg());
           factoryArgs = filterFactory.getOriginalArgs();
           if (null != factoryArgs) {
diff --git solr/core/src/java/org/apache/solr/schema/FieldTypePluginLoader.java solr/core/src/java/org/apache/solr/schema/FieldTypePluginLoader.java
index 95b9555..7f618d5 100644
--- solr/core/src/java/org/apache/solr/schema/FieldTypePluginLoader.java
+++ solr/core/src/java/org/apache/solr/schema/FieldTypePluginLoader.java
@@ -185,7 +185,7 @@ public final class FieldTypePluginLoader
     static final KeywordTokenizerFactory keyFactory = new KeywordTokenizerFactory(new HashMap<String,String>());
 
     ArrayList<CharFilterFactory> charFilters = null;
-    ArrayList<TokenFilterFactory> filters = new ArrayList<TokenFilterFactory>(2);
+    ArrayList<TokenFilterFactory> filters = new ArrayList<>(2);
     TokenizerFactory tokenizer = keyFactory;
 
     public void add(Object current) {
@@ -193,14 +193,14 @@ public final class FieldTypePluginLoader
       AbstractAnalysisFactory newComponent = ((MultiTermAwareComponent)current).getMultiTermComponent();
       if (newComponent instanceof TokenFilterFactory) {
         if (filters == null) {
-          filters = new ArrayList<TokenFilterFactory>(2);
+          filters = new ArrayList<>(2);
         }
         filters.add((TokenFilterFactory)newComponent);
       } else if (newComponent instanceof TokenizerFactory) {
         tokenizer = (TokenizerFactory)newComponent;
       } else if (newComponent instanceof CharFilterFactory) {
         if (charFilters == null) {
-          charFilters = new ArrayList<CharFilterFactory>(1);
+          charFilters = new ArrayList<>(1);
         }
         charFilters.add( (CharFilterFactory)newComponent);
 
@@ -293,7 +293,7 @@ public final class FieldTypePluginLoader
     // Load the CharFilters
 
     final ArrayList<CharFilterFactory> charFilters 
-      = new ArrayList<CharFilterFactory>();
+      = new ArrayList<>();
     AbstractPluginLoader<CharFilterFactory> charFilterLoader =
       new AbstractPluginLoader<CharFilterFactory>
       ("[schema.xml] analyzer/charFilter", CharFilterFactory.class, false, false) {
@@ -329,7 +329,7 @@ public final class FieldTypePluginLoader
     // the configuration is ok
 
     final ArrayList<TokenizerFactory> tokenizers 
-      = new ArrayList<TokenizerFactory>(1);
+      = new ArrayList<>(1);
     AbstractPluginLoader<TokenizerFactory> tokenizerLoader =
       new AbstractPluginLoader<TokenizerFactory>
       ("[schema.xml] analyzer/tokenizer", TokenizerFactory.class, false, false) {
@@ -369,7 +369,7 @@ public final class FieldTypePluginLoader
     // Load the Filters
 
     final ArrayList<TokenFilterFactory> filters 
-      = new ArrayList<TokenFilterFactory>();
+      = new ArrayList<>();
 
     AbstractPluginLoader<TokenFilterFactory> filterLoader = 
       new AbstractPluginLoader<TokenFilterFactory>("[schema.xml] analyzer/filter", TokenFilterFactory.class, false, false)
diff --git solr/core/src/java/org/apache/solr/schema/IndexSchema.java solr/core/src/java/org/apache/solr/schema/IndexSchema.java
index 4a9e454..0263d7a 100644
--- solr/core/src/java/org/apache/solr/schema/IndexSchema.java
+++ solr/core/src/java/org/apache/solr/schema/IndexSchema.java
@@ -120,25 +120,25 @@ public class IndexSchema {
   protected float version;
   protected final SolrResourceLoader loader;
 
-  protected Map<String,SchemaField> fields = new HashMap<String,SchemaField>();
-  protected Map<String,FieldType> fieldTypes = new HashMap<String,FieldType>();
+  protected Map<String,SchemaField> fields = new HashMap<>();
+  protected Map<String,FieldType> fieldTypes = new HashMap<>();
 
-  protected List<SchemaField> fieldsWithDefaultValue = new ArrayList<SchemaField>();
-  protected Collection<SchemaField> requiredFields = new HashSet<SchemaField>();
+  protected List<SchemaField> fieldsWithDefaultValue = new ArrayList<>();
+  protected Collection<SchemaField> requiredFields = new HashSet<>();
   protected volatile DynamicField[] dynamicFields;
   public DynamicField[] getDynamicFields() { return dynamicFields; }
 
   private Analyzer analyzer;
   private Analyzer queryAnalyzer;
 
-  protected List<SchemaAware> schemaAware = new ArrayList<SchemaAware>();
+  protected List<SchemaAware> schemaAware = new ArrayList<>();
 
   protected String defaultSearchFieldName=null;
   protected String queryParserDefaultOperator = "OR";
   protected boolean isExplicitQueryParserDefaultOperator = false;
 
 
-  protected Map<String, List<CopyField>> copyFieldsMap = new HashMap<String, List<CopyField>>();
+  protected Map<String, List<CopyField>> copyFieldsMap = new HashMap<>();
   public Map<String,List<CopyField>> getCopyFieldsMap() { return Collections.unmodifiableMap(copyFieldsMap); }
   
   protected DynamicCopy[] dynamicCopyFields;
@@ -148,7 +148,7 @@ public class IndexSchema {
    * keys are all fields copied to, count is num of copyField
    * directives that target them.
    */
-  protected Map<SchemaField, Integer> copyFieldTargetCounts = new HashMap<SchemaField, Integer>();
+  protected Map<SchemaField, Integer> copyFieldTargetCounts = new HashMap<>();
 
     /**
    * Constructs a schema using the specified resource name and stream.
@@ -386,7 +386,7 @@ public class IndexSchema {
     }
 
     protected HashMap<String, Analyzer> analyzerCache() {
-      HashMap<String, Analyzer> cache = new HashMap<String, Analyzer>();
+      HashMap<String, Analyzer> cache = new HashMap<>();
       for (SchemaField f : getFields().values()) {
         Analyzer analyzer = f.getType().getAnalyzer();
         cache.put(f.getName(), analyzer);
@@ -407,7 +407,7 @@ public class IndexSchema {
 
     @Override
     protected HashMap<String, Analyzer> analyzerCache() {
-      HashMap<String, Analyzer> cache = new HashMap<String, Analyzer>();
+      HashMap<String, Analyzer> cache = new HashMap<>();
        for (SchemaField f : getFields().values()) {
         Analyzer analyzer = f.getType().getQueryAnalyzer();
         cache.put(f.getName(), analyzer);
@@ -633,9 +633,9 @@ public class IndexSchema {
    */ 
   protected synchronized Map<String,Boolean> loadFields(Document document, XPath xpath) throws XPathExpressionException {
     // Hang on to the fields that say if they are required -- this lets us set a reasonable default for the unique key
-    Map<String,Boolean> explicitRequiredProp = new HashMap<String,Boolean>();
+    Map<String,Boolean> explicitRequiredProp = new HashMap<>();
     
-    ArrayList<DynamicField> dFields = new ArrayList<DynamicField>();
+    ArrayList<DynamicField> dFields = new ArrayList<>();
 
     //                  /schema/fields/field | /schema/fields/dynamicField
     String expression = stepsToPath(SCHEMA, FIELDS, FIELD)
@@ -756,7 +756,7 @@ public class IndexSchema {
    * @param fields The sequence of {@link org.apache.solr.schema.SchemaField}
    */
   public void registerDynamicFields(SchemaField... fields) {
-    List<DynamicField> dynFields = new ArrayList<DynamicField>(Arrays.asList(dynamicFields));
+    List<DynamicField> dynFields = new ArrayList<>(Arrays.asList(dynamicFields));
     for (SchemaField field : fields) {
       if (isDuplicateDynField(dynFields, field)) {
         log.debug("dynamic field already exists: dynamic field: [" + field.getName() + "]");
@@ -889,7 +889,7 @@ public class IndexSchema {
       } else {                        // source & dest: explicit fields 
         List<CopyField> copyFieldList = copyFieldsMap.get(source);
         if (copyFieldList == null) {
-          copyFieldList = new ArrayList<CopyField>();
+          copyFieldList = new ArrayList<>();
           copyFieldsMap.put(source, copyFieldList);
         }
         copyFieldList.add(new CopyField(sourceSchemaField, destSchemaField, maxChars));
@@ -1261,7 +1261,7 @@ public class IndexSchema {
     if (!isCopyFieldTarget(f)) {
       return Collections.emptyList();
     }
-    List<String> fieldNames = new ArrayList<String>();
+    List<String> fieldNames = new ArrayList<>();
     for (Map.Entry<String, List<CopyField>> cfs : copyFieldsMap.entrySet()) {
       for (CopyField copyField : cfs.getValue()) {
         if (copyField.getDestination().getName().equals(destField)) {
@@ -1285,7 +1285,7 @@ public class IndexSchema {
    */
   // This is useful when we need the maxSize param of each CopyField
   public List<CopyField> getCopyFieldsList(final String sourceField){
-    final List<CopyField> result = new ArrayList<CopyField>();
+    final List<CopyField> result = new ArrayList<>();
     for (DynamicCopy dynamicCopy : dynamicCopyFields) {
       if (dynamicCopy.matches(sourceField)) {
         result.add(new CopyField(getField(sourceField), dynamicCopy.getTargetField(sourceField), dynamicCopy.maxChars));
@@ -1312,7 +1312,7 @@ public class IndexSchema {
    * Get a map of property name -> value for the whole schema.
    */
   public SimpleOrderedMap<Object> getNamedPropertyValues() {
-    SimpleOrderedMap<Object> topLevel = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> topLevel = new SimpleOrderedMap<>();
     topLevel.add(NAME, getSchemaName());
     topLevel.add(VERSION, getVersion());
     if (null != uniqueKeyFieldName) {
@@ -1322,26 +1322,26 @@ public class IndexSchema {
       topLevel.add(DEFAULT_SEARCH_FIELD, defaultSearchFieldName);
     }
     if (isExplicitQueryParserDefaultOperator) {
-      SimpleOrderedMap<Object> solrQueryParserProperties = new SimpleOrderedMap<Object>();
+      SimpleOrderedMap<Object> solrQueryParserProperties = new SimpleOrderedMap<>();
       solrQueryParserProperties.add(DEFAULT_OPERATOR, queryParserDefaultOperator);
       topLevel.add(SOLR_QUERY_PARSER, solrQueryParserProperties);
     }
     if (isExplicitSimilarity) {
       topLevel.add(SIMILARITY, similarityFactory.getNamedPropertyValues());
     }
-    List<SimpleOrderedMap<Object>> fieldTypeProperties = new ArrayList<SimpleOrderedMap<Object>>();
-    SortedMap<String,FieldType> sortedFieldTypes = new TreeMap<String,FieldType>(fieldTypes);
+    List<SimpleOrderedMap<Object>> fieldTypeProperties = new ArrayList<>();
+    SortedMap<String,FieldType> sortedFieldTypes = new TreeMap<>(fieldTypes);
     for (FieldType fieldType : sortedFieldTypes.values()) {
       fieldTypeProperties.add(fieldType.getNamedPropertyValues(false));
     }
     topLevel.add(FIELD_TYPES, fieldTypeProperties);  
-    List<SimpleOrderedMap<Object>> fieldProperties = new ArrayList<SimpleOrderedMap<Object>>();
-    SortedSet<String> fieldNames = new TreeSet<String>(fields.keySet());
+    List<SimpleOrderedMap<Object>> fieldProperties = new ArrayList<>();
+    SortedSet<String> fieldNames = new TreeSet<>(fields.keySet());
     for (String fieldName : fieldNames) {
       fieldProperties.add(fields.get(fieldName).getNamedPropertyValues(false));
     }
     topLevel.add(FIELDS, fieldProperties);
-    List<SimpleOrderedMap<Object>> dynamicFieldProperties = new ArrayList<SimpleOrderedMap<Object>>();
+    List<SimpleOrderedMap<Object>> dynamicFieldProperties = new ArrayList<>();
     for (IndexSchema.DynamicField dynamicField : dynamicFields) {
       if ( ! dynamicField.getRegex().startsWith(INTERNAL_POLY_FIELD_PREFIX)) { // omit internal polyfields
         dynamicFieldProperties.add(dynamicField.getPrototype().getNamedPropertyValues(false));
@@ -1366,8 +1366,8 @@ public class IndexSchema {
    */
   public List<SimpleOrderedMap<Object>> getCopyFieldProperties
       (boolean showDetails, Set<String> requestedSourceFields, Set<String> requestedDestinationFields) {
-    List<SimpleOrderedMap<Object>> copyFieldProperties = new ArrayList<SimpleOrderedMap<Object>>();
-    SortedMap<String,List<CopyField>> sortedCopyFields = new TreeMap<String,List<CopyField>>(copyFieldsMap);
+    List<SimpleOrderedMap<Object>> copyFieldProperties = new ArrayList<>();
+    SortedMap<String,List<CopyField>> sortedCopyFields = new TreeMap<>(copyFieldsMap);
     for (List<CopyField> copyFields : sortedCopyFields.values()) {
       Collections.sort(copyFields, new Comparator<CopyField>() {
         @Override
@@ -1381,7 +1381,7 @@ public class IndexSchema {
         final String destination = copyField.getDestination().getName();
         if (   (null == requestedSourceFields      || requestedSourceFields.contains(source))
             && (null == requestedDestinationFields || requestedDestinationFields.contains(destination))) {
-          SimpleOrderedMap<Object> props = new SimpleOrderedMap<Object>();
+          SimpleOrderedMap<Object> props = new SimpleOrderedMap<>();
           props.add(SOURCE, source);
           props.add(DESTINATION, destination);
             if (0 != copyField.getMaxChars()) {
@@ -1396,7 +1396,7 @@ public class IndexSchema {
       final String destination = dynamicCopy.getDestFieldName();
       if (   (null == requestedSourceFields      || requestedSourceFields.contains(source))
           && (null == requestedDestinationFields || requestedDestinationFields.contains(destination))) {
-        SimpleOrderedMap<Object> dynamicCopyProps = new SimpleOrderedMap<Object>();
+        SimpleOrderedMap<Object> dynamicCopyProps = new SimpleOrderedMap<>();
 
         dynamicCopyProps.add(SOURCE, dynamicCopy.getRegex());
         if (showDetails) {
@@ -1404,7 +1404,7 @@ public class IndexSchema {
           if (null != sourceDynamicBase) {
             dynamicCopyProps.add(SOURCE_DYNAMIC_BASE, sourceDynamicBase.getRegex());
           } else if (source.contains("*")) {
-            List<String> sourceExplicitFields = new ArrayList<String>();
+            List<String> sourceExplicitFields = new ArrayList<>();
             Pattern pattern = Pattern.compile(source.replace("*", ".*"));   // glob->regex
             for (String field : fields.keySet()) {
               if (pattern.matcher(field).matches()) {
diff --git solr/core/src/java/org/apache/solr/schema/JsonPreAnalyzedParser.java solr/core/src/java/org/apache/solr/schema/JsonPreAnalyzedParser.java
index 77b6c04..accce1d 100644
--- solr/core/src/java/org/apache/solr/schema/JsonPreAnalyzedParser.java
+++ solr/core/src/java/org/apache/solr/schema/JsonPreAnalyzedParser.java
@@ -208,7 +208,7 @@ public class JsonPreAnalyzedParser implements PreAnalyzedParser {
 
   @Override
   public String toFormattedString(Field f) throws IOException {
-    Map<String,Object> map = new LinkedHashMap<String,Object>();
+    Map<String,Object> map = new LinkedHashMap<>();
     map.put(VERSION_KEY, VERSION);
     if (f.fieldType().stored()) {
       String stringValue = f.stringValue();
@@ -222,12 +222,12 @@ public class JsonPreAnalyzedParser implements PreAnalyzedParser {
     }
     TokenStream ts = f.tokenStreamValue();
     if (ts != null) {
-      List<Map<String,Object>> tokens = new LinkedList<Map<String,Object>>();
+      List<Map<String,Object>> tokens = new LinkedList<>();
       while (ts.incrementToken()) {
         Iterator<Class<? extends Attribute>> it = ts.getAttributeClassesIterator();
         String cTerm = null;
         String tTerm = null;
-        Map<String,Object> tok = new TreeMap<String,Object>();
+        Map<String,Object> tok = new TreeMap<>();
         while (it.hasNext()) {
           Class<? extends Attribute> cl = it.next();
           if (!ts.hasAttribute(cl)) {
diff --git solr/core/src/java/org/apache/solr/schema/LatLonType.java solr/core/src/java/org/apache/solr/schema/LatLonType.java
index 51b186b..548cdbf 100644
--- solr/core/src/java/org/apache/solr/schema/LatLonType.java
+++ solr/core/src/java/org/apache/solr/schema/LatLonType.java
@@ -72,7 +72,7 @@ public class LatLonType extends AbstractSubTypeFieldType implements SpatialQuery
   public List<StorableField> createFields(SchemaField field, Object value, float boost) {
     String externalVal = value.toString();
     //we could have 3 fields (two for the lat & lon, one for storage)
-    List<StorableField> f = new ArrayList<StorableField>(3);
+    List<StorableField> f = new ArrayList<>(3);
     if (field.indexed()) {
       Point point = SpatialUtils.parsePointSolrException(externalVal, SpatialContext.GEO);
       //latitude
@@ -216,7 +216,7 @@ public class LatLonType extends AbstractSubTypeFieldType implements SpatialQuery
 
   @Override
   public ValueSource getValueSource(SchemaField field, QParser parser) {
-    ArrayList<ValueSource> vs = new ArrayList<ValueSource>(2);
+    ArrayList<ValueSource> vs = new ArrayList<>(2);
     for (int i = 0; i < 2; i++) {
       SchemaField sub = subField(field, i, parser.getReq().getSchema());
       vs.add(sub.getType().getValueSource(sub, parser));
diff --git solr/core/src/java/org/apache/solr/schema/OpenExchangeRatesOrgProvider.java solr/core/src/java/org/apache/solr/schema/OpenExchangeRatesOrgProvider.java
index 0945d41..c82266c 100644
--- solr/core/src/java/org/apache/solr/schema/OpenExchangeRatesOrgProvider.java
+++ solr/core/src/java/org/apache/solr/schema/OpenExchangeRatesOrgProvider.java
@@ -203,7 +203,7 @@ public class OpenExchangeRatesOrgProvider implements ExchangeRateProvider {
     
     public OpenExchangeRates(InputStream ratesStream) throws IOException {
       parser = new JSONParser(new InputStreamReader(ratesStream, IOUtils.CHARSET_UTF_8));
-      rates = new HashMap<String, Double>();
+      rates = new HashMap<>();
       
       int ev;
       do {
diff --git solr/core/src/java/org/apache/solr/schema/PointType.java solr/core/src/java/org/apache/solr/schema/PointType.java
index 9bc9c61..3f6ffa5 100644
--- solr/core/src/java/org/apache/solr/schema/PointType.java
+++ solr/core/src/java/org/apache/solr/schema/PointType.java
@@ -70,7 +70,7 @@ public class PointType extends CoordinateFieldType implements SpatialQueryable {
     String[] point = parseCommaSeparatedList(externalVal, dimension);
 
     // TODO: this doesn't currently support polyFields as sub-field types
-    List<StorableField> f = new ArrayList<StorableField>(dimension+1);
+    List<StorableField> f = new ArrayList<>(dimension+1);
 
     if (field.indexed()) {
       for (int i=0; i<dimension; i++) {
@@ -91,7 +91,7 @@ public class PointType extends CoordinateFieldType implements SpatialQueryable {
 
   @Override
   public ValueSource getValueSource(SchemaField field, QParser parser) {
-    ArrayList<ValueSource> vs = new ArrayList<ValueSource>(dimension);
+    ArrayList<ValueSource> vs = new ArrayList<>(dimension);
     for (int i=0; i<dimension; i++) {
       SchemaField sub = subField(field, i, schema);
       vs.add(sub.getType().getValueSource(sub, parser));
diff --git solr/core/src/java/org/apache/solr/schema/PreAnalyzedField.java solr/core/src/java/org/apache/solr/schema/PreAnalyzedField.java
index 62722ab..3e49574 100644
--- solr/core/src/java/org/apache/solr/schema/PreAnalyzedField.java
+++ solr/core/src/java/org/apache/solr/schema/PreAnalyzedField.java
@@ -168,7 +168,7 @@ public class PreAnalyzedField extends FieldType {
   public static class ParseResult {
     public String str;
     public byte[] bin;
-    public List<State> states = new LinkedList<State>();
+    public List<State> states = new LinkedList<>();
   }
   
   /**
@@ -247,7 +247,7 @@ public class PreAnalyzedField extends FieldType {
    * Token stream that works from a list of saved states.
    */
   private static class PreAnalyzedTokenizer extends Tokenizer {
-    private final List<AttributeSource.State> cachedStates = new LinkedList<AttributeSource.State>();
+    private final List<AttributeSource.State> cachedStates = new LinkedList<>();
     private Iterator<AttributeSource.State> it = null;
     private String stringValue = null;
     private byte[] binaryValue = null;
diff --git solr/core/src/java/org/apache/solr/schema/SchemaField.java solr/core/src/java/org/apache/solr/schema/SchemaField.java
index ca6bd90..5add53f 100644
--- solr/core/src/java/org/apache/solr/schema/SchemaField.java
+++ solr/core/src/java/org/apache/solr/schema/SchemaField.java
@@ -313,7 +313,7 @@ public final class SchemaField extends FieldProperties {
    * not overridden in the field declaration).
    */
   public SimpleOrderedMap<Object> getNamedPropertyValues(boolean showDefaults) {
-    SimpleOrderedMap<Object> properties = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> properties = new SimpleOrderedMap<>();
     properties.add(FIELD_NAME, getName());
     properties.add(TYPE_NAME, getType().getTypeName());
     if (showDefaults) {
diff --git solr/core/src/java/org/apache/solr/schema/SimilarityFactory.java solr/core/src/java/org/apache/solr/schema/SimilarityFactory.java
index 0f1b89d..7e32e71 100644
--- solr/core/src/java/org/apache/solr/schema/SimilarityFactory.java
+++ solr/core/src/java/org/apache/solr/schema/SimilarityFactory.java
@@ -51,7 +51,7 @@ public abstract class SimilarityFactory {
 
   /** Returns a serializable description of this similarity(factory) */
   public SimpleOrderedMap<Object> getNamedPropertyValues() {
-    SimpleOrderedMap<Object> props = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> props = new SimpleOrderedMap<>();
     props.add(CLASS_NAME, getClassArg());
     if (null != params) {
       Iterator<String> iter = params.getParameterNamesIterator();
diff --git solr/core/src/java/org/apache/solr/schema/SimplePreAnalyzedParser.java solr/core/src/java/org/apache/solr/schema/SimplePreAnalyzedParser.java
index 0ab959a..1e58824 100644
--- solr/core/src/java/org/apache/solr/schema/SimplePreAnalyzedParser.java
+++ solr/core/src/java/org/apache/solr/schema/SimplePreAnalyzedParser.java
@@ -141,7 +141,7 @@ public final class SimplePreAnalyzedParser implements PreAnalyzedParser {
   
   private static class Tok {
     StringBuilder token = new StringBuilder();
-    Map<String, String> attr = new HashMap<String, String>();
+    Map<String, String> attr = new HashMap<>();
     
     public boolean isEmpty() {
       return token.length() == 0 && attr.size() == 0;
diff --git solr/core/src/java/org/apache/solr/schema/SpatialPointVectorFieldType.java solr/core/src/java/org/apache/solr/schema/SpatialPointVectorFieldType.java
index 6c008dc..2f93dc1 100644
--- solr/core/src/java/org/apache/solr/schema/SpatialPointVectorFieldType.java
+++ solr/core/src/java/org/apache/solr/schema/SpatialPointVectorFieldType.java
@@ -65,7 +65,7 @@ public class SpatialPointVectorFieldType extends AbstractSpatialFieldType<PointV
 
     //Just set these, delegate everything else to the field type
     final int p = (INDEXED | TOKENIZED | OMIT_NORMS | OMIT_TF_POSITIONS);
-    List<SchemaField> newFields = new ArrayList<SchemaField>();
+    List<SchemaField> newFields = new ArrayList<>();
     for( SchemaField sf : schema.getFields().values() ) {
       if( sf.getType() == this ) {
         String name = sf.getName();
diff --git solr/core/src/java/org/apache/solr/schema/StrField.java solr/core/src/java/org/apache/solr/schema/StrField.java
index 15060b9..e39d7e6 100644
--- solr/core/src/java/org/apache/solr/schema/StrField.java
+++ solr/core/src/java/org/apache/solr/schema/StrField.java
@@ -45,7 +45,7 @@ public class StrField extends PrimitiveFieldType {
   public List<StorableField> createFields(SchemaField field, Object value,
       float boost) {
     if (field.hasDocValues()) {
-      List<StorableField> fields = new ArrayList<StorableField>();
+      List<StorableField> fields = new ArrayList<>();
       fields.add(createField(field, value, boost));
       final BytesRef bytes = new BytesRef(value.toString());
       if (field.multiValued()) {
diff --git solr/core/src/java/org/apache/solr/schema/TrieField.java solr/core/src/java/org/apache/solr/schema/TrieField.java
index 4859e12..a0aca98 100644
--- solr/core/src/java/org/apache/solr/schema/TrieField.java
+++ solr/core/src/java/org/apache/solr/schema/TrieField.java
@@ -634,7 +634,7 @@ public class TrieField extends PrimitiveFieldType {
   @Override
   public List<StorableField> createFields(SchemaField sf, Object value, float boost) {
     if (sf.hasDocValues()) {
-      List<StorableField> fields = new ArrayList<StorableField>();
+      List<StorableField> fields = new ArrayList<>();
       final StorableField field = createField(sf, value, boost);
       fields.add(field);
       
diff --git solr/core/src/java/org/apache/solr/search/CursorMark.java solr/core/src/java/org/apache/solr/search/CursorMark.java
index 2c64514..13bdc93 100644
--- solr/core/src/java/org/apache/solr/search/CursorMark.java
+++ solr/core/src/java/org/apache/solr/search/CursorMark.java
@@ -160,7 +160,7 @@ public final class CursorMark {
     } else {
       assert input.size() == sortSpec.getSort().getSort().length;
       // defensive copy
-      this.values = new ArrayList<Object>(input);
+      this.values = new ArrayList<>(input);
     }
   }
 
@@ -170,7 +170,7 @@ public final class CursorMark {
    */
   public List<Object> getSortValues() {
     // defensive copy
-    return null == this.values ? null : new ArrayList<Object>(this.values);
+    return null == this.values ? null : new ArrayList<>(this.values);
   }
 
   /**
@@ -218,7 +218,7 @@ public final class CursorMark {
     }
 
 
-    this.values = new ArrayList<Object>(sortFields.length);
+    this.values = new ArrayList<>(sortFields.length);
 
     final BytesRef tmpBytes = new BytesRef();
     for (int i = 0; i < sortFields.length; i++) {
@@ -248,7 +248,7 @@ public final class CursorMark {
     }
 
     final List<SchemaField> schemaFields = sortSpec.getSchemaFields();
-    final ArrayList<Object> marshalledValues = new ArrayList<Object>(values.size()+1);
+    final ArrayList<Object> marshalledValues = new ArrayList<>(values.size()+1);
     for (int i = 0; i < schemaFields.size(); i++) {
       SchemaField fld = schemaFields.get(i);
       Object safeValue = values.get(i);
diff --git solr/core/src/java/org/apache/solr/search/DisMaxQParser.java solr/core/src/java/org/apache/solr/search/DisMaxQParser.java
index 8444849..748b176 100644
--- solr/core/src/java/org/apache/solr/search/DisMaxQParser.java
+++ solr/core/src/java/org/apache/solr/search/DisMaxQParser.java
@@ -137,7 +137,7 @@ public class DisMaxQParser extends QParser {
     //List<Query> boostQueries = SolrPluginUtils.parseQueryStrings(req, boostParams);
     boostQueries = null;
     if (boostParams != null && boostParams.length > 0) {
-      boostQueries = new ArrayList<Query>();
+      boostQueries = new ArrayList<>();
       for (String qs : boostParams) {
         if (qs.trim().length() == 0) continue;
         Query q = subQuery(qs, null).getQuery();
diff --git solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.java solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.java
index aecfabe..df106cc 100644
--- solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.java
+++ solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.java
@@ -204,7 +204,7 @@ public class ExtendedDismaxQParser extends QParser {
     
     if (allPhraseFields.size() > 0) {
       // find non-field clauses
-      List<Clause> normalClauses = new ArrayList<Clause>(clauses.size());
+      List<Clause> normalClauses = new ArrayList<>(clauses.size());
       for (Clause clause : clauses) {
         if (clause.field != null || clause.isPhrase) continue;
         // check for keywords "AND,OR,TO"
@@ -218,7 +218,7 @@ public class ExtendedDismaxQParser extends QParser {
       
       // full phrase and shingles
       for (FieldParams phraseField: allPhraseFields) {
-        Map<String,Float> pf = new HashMap<String,Float>(1);
+        Map<String,Float> pf = new HashMap<>(1);
         pf.put(phraseField.getField(),phraseField.getBoost());
         addShingledPhraseQueries(query, normalClauses, pf,   
             phraseField.getWordGrams(),config.tiebreaker, phraseField.getSlop());
@@ -407,7 +407,7 @@ public class ExtendedDismaxQParser extends QParser {
    * Parses all multiplicative boosts
    */
   protected List<ValueSource> getMultiplicativeBoosts() throws SyntaxError {
-    List<ValueSource> boosts = new ArrayList<ValueSource>();
+    List<ValueSource> boosts = new ArrayList<>();
     if (config.hasMultiplicativeBoosts()) {
       for (String boostStr : config.multBoosts) {
         if (boostStr==null || boostStr.length()==0) continue;
@@ -428,7 +428,7 @@ public class ExtendedDismaxQParser extends QParser {
    * Parses all function queries
    */
   protected List<Query> getBoostFunctions() throws SyntaxError {
-    List<Query> boostFunctions = new LinkedList<Query>();
+    List<Query> boostFunctions = new LinkedList<>();
     if (config.hasBoostFunctions()) {
       for (String boostFunc : config.boostFuncs) {
         if(null == boostFunc || "".equals(boostFunc)) continue;
@@ -450,7 +450,7 @@ public class ExtendedDismaxQParser extends QParser {
    * Parses all boost queries
    */
   protected List<Query> getBoostQueries() throws SyntaxError {
-    List<Query> boostQueries = new LinkedList<Query>();
+    List<Query> boostQueries = new LinkedList<>();
     if (config.hasBoostParams()) {
       for (String qs : config.boostParams) {
         if (qs.trim().length()==0) continue;
@@ -676,7 +676,7 @@ public class ExtendedDismaxQParser extends QParser {
   }
   
   public List<Clause> splitIntoClauses(String s, boolean ignoreQuote) {
-    ArrayList<Clause> lst = new ArrayList<Clause>(4);
+    ArrayList<Clause> lst = new ArrayList<>(4);
     Clause clause;
     
     int pos=0;
@@ -859,7 +859,7 @@ public class ExtendedDismaxQParser extends QParser {
   }
   
   public static List<String> split(String s, boolean ignoreQuote) {
-    ArrayList<String> lst = new ArrayList<String>(4);
+    ArrayList<String> lst = new ArrayList<>(4);
     int pos=0, start=0, end=s.length();
     char inString=0;
     char ch=0;
@@ -937,7 +937,7 @@ public class ExtendedDismaxQParser extends QParser {
      * string, to Alias object containing the fields to use in our
      * DisjunctionMaxQuery and the tiebreaker to use.
      */
-    protected Map<String,Alias> aliases = new HashMap<String,Alias>(3);
+    protected Map<String,Alias> aliases = new HashMap<>(3);
     
     private QType type;
     private String field;
@@ -1029,7 +1029,7 @@ public class ExtendedDismaxQParser extends QParser {
       Analyzer actualAnalyzer;
       if (removeStopFilter) {
         if (nonStopFilterAnalyzerPerField == null) {
-          nonStopFilterAnalyzerPerField = new HashMap<String, Analyzer>();
+          nonStopFilterAnalyzerPerField = new HashMap<>();
         }
         actualAnalyzer = nonStopFilterAnalyzerPerField.get(field);
         if (actualAnalyzer == null) {
@@ -1127,7 +1127,7 @@ public class ExtendedDismaxQParser extends QParser {
      * Validate there is no cyclic referencing in the aliasing
      */
     private void validateCyclicAliasing(String field) throws SyntaxError {
-      Set<String> set = new HashSet<String>();
+      Set<String> set = new HashSet<>();
       set.add(field);
       if(validateField(field, set)) {
         throw new SyntaxError("Field aliases lead to a cycle");
@@ -1155,7 +1155,7 @@ public class ExtendedDismaxQParser extends QParser {
     protected List<Query> getQueries(Alias a) throws SyntaxError {
       if (a == null) return null;
       if (a.fields.size()==0) return null;
-      List<Query> lst= new ArrayList<Query>(4);
+      List<Query> lst= new ArrayList<>(4);
       
       for (String f : a.fields.keySet()) {
         this.field = f;
@@ -1289,8 +1289,8 @@ public class ExtendedDismaxQParser extends QParser {
       }
       
       // Process dynamic patterns in userFields
-      ArrayList<DynamicField> dynUserFields = new ArrayList<DynamicField>();
-      ArrayList<DynamicField> negDynUserFields = new ArrayList<DynamicField>();
+      ArrayList<DynamicField> dynUserFields = new ArrayList<>();
+      ArrayList<DynamicField> negDynUserFields = new ArrayList<>();
       for(String f : userFieldsMap.keySet()) {
         if(f.contains("*")) {
           if(f.startsWith("-"))
@@ -1459,7 +1459,7 @@ public class ExtendedDismaxQParser extends QParser {
       List<FieldParams> phraseFields2 = U.parseFieldBoostsAndSlop(solrParams.getParams(DMP.PF2),2,pslop[2]);
       List<FieldParams> phraseFields3 = U.parseFieldBoostsAndSlop(solrParams.getParams(DMP.PF3),3,pslop[3]);
       
-      allPhraseFields = new ArrayList<FieldParams>(phraseFields.size() + phraseFields2.size() + phraseFields3.size());
+      allPhraseFields = new ArrayList<>(phraseFields.size() + phraseFields2.size() + phraseFields3.size());
       allPhraseFields.addAll(phraseFields);
       allPhraseFields.addAll(phraseFields2);
       allPhraseFields.addAll(phraseFields3);
diff --git solr/core/src/java/org/apache/solr/search/FastLRUCache.java solr/core/src/java/org/apache/solr/search/FastLRUCache.java
index dfd7000..a9abf14 100644
--- solr/core/src/java/org/apache/solr/search/FastLRUCache.java
+++ solr/core/src/java/org/apache/solr/search/FastLRUCache.java
@@ -85,7 +85,7 @@ public class FastLRUCache<K,V> extends SolrCacheBase implements SolrCache<K,V> {
     str = (String) args.get("showItems");
     showItems = str == null ? 0 : Integer.parseInt(str);
     description = generateDescription(limit, initialSize, minLimit, acceptableLimit, newThread);
-    cache = new ConcurrentLRUCache<K,V>(limit, minLimit, acceptableLimit, initialSize, newThread, false, null);
+    cache = new ConcurrentLRUCache<>(limit, minLimit, acceptableLimit, initialSize, newThread, false, null);
     cache.setAlive(false);
 
     statsList = (List<ConcurrentLRUCache.Stats>) persistence;
@@ -93,7 +93,7 @@ public class FastLRUCache<K,V> extends SolrCacheBase implements SolrCache<K,V> {
       // must be the first time a cache of this type is being created
       // Use a CopyOnWriteArrayList since puts are very rare and iteration may be a frequent operation
       // because it is used in getStatistics()
-      statsList = new CopyOnWriteArrayList<ConcurrentLRUCache.Stats>();
+      statsList = new CopyOnWriteArrayList<>();
 
       // the first entry will be for cumulative stats of caches that have been closed.
       statsList.add(new ConcurrentLRUCache.Stats());
@@ -197,7 +197,7 @@ public class FastLRUCache<K,V> extends SolrCacheBase implements SolrCache<K,V> {
 
   @Override
   public NamedList getStatistics() {
-    NamedList<Serializable> lst = new SimpleOrderedMap<Serializable>();
+    NamedList<Serializable> lst = new SimpleOrderedMap<>();
     if (cache == null)  return lst;
     ConcurrentLRUCache.Stats stats = cache.getStats();
     long lookups = stats.getCumulativeLookups();
diff --git solr/core/src/java/org/apache/solr/search/FunctionQParser.java solr/core/src/java/org/apache/solr/search/FunctionQParser.java
index 53744ea..1403999 100644
--- solr/core/src/java/org/apache/solr/search/FunctionQParser.java
+++ solr/core/src/java/org/apache/solr/search/FunctionQParser.java
@@ -84,7 +84,7 @@ public class FunctionQParser extends QParser {
       consumeArgumentDelimiter();
 
       if (lst == null) {
-        lst = new ArrayList<ValueSource>(2);
+        lst = new ArrayList<>(2);
         lst.add(valsource);
       }
     }
@@ -208,7 +208,7 @@ public class FunctionQParser extends QParser {
    * @return List&lt;ValueSource&gt;
    */
   public List<ValueSource> parseValueSourceList() throws SyntaxError {
-    List<ValueSource> sources = new ArrayList<ValueSource>(3);
+    List<ValueSource> sources = new ArrayList<>(3);
     while (hasMoreArguments()) {
       sources.add(parseValueSource(true));
     }
diff --git solr/core/src/java/org/apache/solr/search/Grouping.java solr/core/src/java/org/apache/solr/search/Grouping.java
index 024eb3f..88066de 100644
--- solr/core/src/java/org/apache/solr/search/Grouping.java
+++ solr/core/src/java/org/apache/solr/search/Grouping.java
@@ -83,7 +83,7 @@ public class Grouping {
   private final SolrIndexSearcher searcher;
   private final SolrIndexSearcher.QueryResult qr;
   private final SolrIndexSearcher.QueryCommand cmd;
-  private final List<Command> commands = new ArrayList<Command>();
+  private final List<Command> commands = new ArrayList<>();
   private final boolean main;
   private final boolean cacheSecondPassSearch;
   private final int maxDocsPercentageToCache;
@@ -105,7 +105,7 @@ public class Grouping {
   private DocSet filter;
   private Filter luceneFilter;
   private NamedList grouped = new SimpleOrderedMap();
-  private Set<Integer> idSet = new LinkedHashSet<Integer>();  // used for tracking unique docs when we need a doclist
+  private Set<Integer> idSet = new LinkedHashSet<>();  // used for tracking unique docs when we need a doclist
   private int maxMatches;  // max number of matches from any grouping command
   private float maxScore = Float.NEGATIVE_INFINITY;  // max score seen in any doclist
   private boolean signalCacheWarning = false;
@@ -331,7 +331,7 @@ public class Grouping {
     }
 
     AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;
-    List<Collector> collectors = new ArrayList<Collector>(commands.size());
+    List<Collector> collectors = new ArrayList<>(commands.size());
     for (Command cmd : commands) {
       Collector collector = cmd.createFirstPassCollector();
       if (collector != null) {
@@ -649,8 +649,8 @@ public class Grouping {
     protected DocList createSimpleResponse() {
       GroupDocs[] groups = result != null ? result.groups : new GroupDocs[0];
 
-      List<Integer> ids = new ArrayList<Integer>();
-      List<Float> scores = new ArrayList<Float>();
+      List<Integer> ids = new ArrayList<>();
+      List<Float> scores = new ArrayList<>();
       int docsToGather = getMax(offset, numGroups, maxDoc);
       int docsGathered = 0;
       float maxScore = Float.NEGATIVE_INFINITY;
@@ -888,7 +888,7 @@ public class Grouping {
     protected void finish() throws IOException {
       TopDocsCollector topDocsCollector = (TopDocsCollector) collector.getDelegate();
       TopDocs topDocs = topDocsCollector.topDocs();
-      GroupDocs<String> groupDocs = new GroupDocs<String>(Float.NaN, topDocs.getMaxScore(), topDocs.totalHits, topDocs.scoreDocs, query.toString(), null);
+      GroupDocs<String> groupDocs = new GroupDocs<>(Float.NaN, topDocs.getMaxScore(), topDocs.totalHits, topDocs.scoreDocs, query.toString(), null);
       if (main) {
         mainResult = getDocList(groupDocs);
       } else {
diff --git solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
index 6c451fd..bdc8ef0 100644
--- solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
+++ solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
@@ -241,7 +241,7 @@ class JoinQuery extends Query {
         long end = debug ? System.currentTimeMillis() : 0;
 
         if (debug) {
-          SimpleOrderedMap<Object> dbg = new SimpleOrderedMap<Object>();
+          SimpleOrderedMap<Object> dbg = new SimpleOrderedMap<>();
           dbg.add("time", (end-start));
           dbg.add("fromSetSize", fromSetSize);  // the input
           dbg.add("toSetSize", resultSet.size());    // the output
@@ -296,7 +296,7 @@ class JoinQuery extends Query {
       DocSet fromSet = fromSearcher.getDocSet(q);
       fromSetSize = fromSet.size();
 
-      List<DocSet> resultList = new ArrayList<DocSet>(10);
+      List<DocSet> resultList = new ArrayList<>(10);
 
       // make sure we have a set that is fast for random access, if we will use it for that
       DocSet fastForRandomSet = fromSet;
diff --git solr/core/src/java/org/apache/solr/search/LFUCache.java solr/core/src/java/org/apache/solr/search/LFUCache.java
index 0093d47..aadae66 100644
--- solr/core/src/java/org/apache/solr/search/LFUCache.java
+++ solr/core/src/java/org/apache/solr/search/LFUCache.java
@@ -109,7 +109,7 @@ public class LFUCache<K, V> implements SolrCache<K, V> {
     }
     description += ')';
 
-    cache = new ConcurrentLFUCache<K, V>(limit, minLimit, acceptableSize, initialSize, newThread, false, null, timeDecay);
+    cache = new ConcurrentLFUCache<>(limit, minLimit, acceptableSize, initialSize, newThread, false, null, timeDecay);
     cache.setAlive(false);
 
     statsList = (List<ConcurrentLFUCache.Stats>) persistence;
@@ -117,7 +117,7 @@ public class LFUCache<K, V> implements SolrCache<K, V> {
       // must be the first time a cache of this type is being created
       // Use a CopyOnWriteArrayList since puts are very rare and iteration may be a frequent operation
       // because it is used in getStatistics()
-      statsList = new CopyOnWriteArrayList<ConcurrentLFUCache.Stats>();
+      statsList = new CopyOnWriteArrayList<>();
 
       // the first entry will be for cumulative stats of caches that have been closed.
       statsList.add(new ConcurrentLFUCache.Stats());
@@ -242,7 +242,7 @@ public class LFUCache<K, V> implements SolrCache<K, V> {
 
   @Override
   public NamedList getStatistics() {
-    NamedList<Serializable> lst = new SimpleOrderedMap<Serializable>();
+    NamedList<Serializable> lst = new SimpleOrderedMap<>();
     if (cache == null) return lst;
     ConcurrentLFUCache.Stats stats = cache.getStats();
     long lookups = stats.getCumulativeLookups();
diff --git solr/core/src/java/org/apache/solr/search/MaxScoreQParser.java solr/core/src/java/org/apache/solr/search/MaxScoreQParser.java
index d105ab1..e58c793 100644
--- solr/core/src/java/org/apache/solr/search/MaxScoreQParser.java
+++ solr/core/src/java/org/apache/solr/search/MaxScoreQParser.java
@@ -58,8 +58,8 @@ public class MaxScoreQParser extends LuceneQParser {
       return q;
     }
     BooleanQuery obq = (BooleanQuery)q;
-    Collection<Query> should = new ArrayList<Query>();
-    Collection<BooleanClause> prohibOrReq = new ArrayList<BooleanClause>();
+    Collection<Query> should = new ArrayList<>();
+    Collection<BooleanClause> prohibOrReq = new ArrayList<>();
     BooleanQuery newq = new BooleanQuery();
 
     for (BooleanClause clause : obq.getClauses()) {
diff --git solr/core/src/java/org/apache/solr/search/QParser.java solr/core/src/java/org/apache/solr/search/QParser.java
index 0b415dd..0054dba 100644
--- solr/core/src/java/org/apache/solr/search/QParser.java
+++ solr/core/src/java/org/apache/solr/search/QParser.java
@@ -66,7 +66,7 @@ public abstract class QParser {
         @SuppressWarnings("unchecked")
         Map<Object,Collection<Object>> tagMap = (Map<Object, Collection<Object>>)req.getContext().get("tags");
         if (tagMap == null) {
-          tagMap = new HashMap<Object,Collection<Object>>();
+          tagMap = new HashMap<>();
           context.put("tags", tagMap);          
         }
         if (tagStr.indexOf(',') >= 0) {
@@ -88,7 +88,7 @@ public abstract class QParser {
   private static void addTag(Map<Object,Collection<Object>> tagMap, Object key, Object val) {
     Collection<Object> lst = tagMap.get(key);
     if (lst == null) {
-      lst = new ArrayList<Object>(2);
+      lst = new ArrayList<>(2);
       tagMap.put(key, lst);
     }
     lst.add(val);
@@ -283,7 +283,7 @@ public abstract class QParser {
     int localParamsEnd = -1;
 
     if (qstr != null && qstr.startsWith(QueryParsing.LOCALPARAM_START)) {
-      Map<String, String> localMap = new HashMap<String, String>();
+      Map<String, String> localMap = new HashMap<>();
       localParamsEnd = QueryParsing.parseLocalParams(qstr, 0, localMap, globalParams);
 
       String val = localMap.get(QueryParsing.V);
diff --git solr/core/src/java/org/apache/solr/search/QueryParsing.java solr/core/src/java/org/apache/solr/search/QueryParsing.java
index 75d5b52..03389b4 100644
--- solr/core/src/java/org/apache/solr/search/QueryParsing.java
+++ solr/core/src/java/org/apache/solr/search/QueryParsing.java
@@ -207,7 +207,7 @@ public class QueryParsing {
     if (txt == null || !txt.startsWith(LOCALPARAM_START)) {
       return null;
     }
-    Map<String, String> localParams = new HashMap<String, String>();
+    Map<String, String> localParams = new HashMap<>();
     int start = QueryParsing.parseLocalParams(txt, 0, localParams, params);
 
     String val = localParams.get(V);
@@ -255,8 +255,8 @@ public class QueryParsing {
   public static SortSpec parseSortSpec(String sortSpec, SolrQueryRequest req) {
     if (sortSpec == null || sortSpec.length() == 0) return newEmptySortSpec();
 
-    List<SortField> sorts = new ArrayList<SortField>(4);
-    List<SchemaField> fields = new ArrayList<SchemaField>(4);
+    List<SortField> sorts = new ArrayList<>(4);
+    List<SchemaField> fields = new ArrayList<>(4);
 
     try {
 
@@ -920,7 +920,7 @@ public class QueryParsing {
    * Builds a list of String which are stringified versions of a list of Queries
    */
   public static List<String> toString(List<Query> queries, IndexSchema schema) {
-    List<String> out = new ArrayList<String>(queries.size());
+    List<String> out = new ArrayList<>(queries.size());
     for (Query q : queries) {
       out.add(QueryParsing.toString(q, schema));
     }
diff --git solr/core/src/java/org/apache/solr/search/QueryResultKey.java solr/core/src/java/org/apache/solr/search/QueryResultKey.java
index e0ac6d9..893ead7 100644
--- solr/core/src/java/org/apache/solr/search/QueryResultKey.java
+++ solr/core/src/java/org/apache/solr/search/QueryResultKey.java
@@ -144,7 +144,7 @@ public final class QueryResultKey {
     // (And of course: if the SolrIndexSearcher / QueryCommmand was ever changed to
     // sort the filter query list, then this whole method could be eliminated).
 
-    final ArrayList<Query> set2 = new ArrayList<Query>(fqList2.subList(start, sz));
+    final ArrayList<Query> set2 = new ArrayList<>(fqList2.subList(start, sz));
     for (int i = start; i < sz; i++) {
       Query q1 = fqList1.get(i);
       if ( ! set2.remove(q1) ) {
diff --git solr/core/src/java/org/apache/solr/search/SimpleQParserPlugin.java solr/core/src/java/org/apache/solr/search/SimpleQParserPlugin.java
index 54e5ee4..feaa24a 100644
--- solr/core/src/java/org/apache/solr/search/SimpleQParserPlugin.java
+++ solr/core/src/java/org/apache/solr/search/SimpleQParserPlugin.java
@@ -72,7 +72,7 @@ public class SimpleQParserPlugin extends QParserPlugin {
   public static final String NAME = "simple";
 
   /** Map of string operators to their int counterparts in SimpleQueryParser. */
-  private static final Map<String, Integer> OPERATORS = new HashMap<String, Integer>();
+  private static final Map<String, Integer> OPERATORS = new HashMap<>();
 
   /* Setup the map of possible operators. */
   static {
diff --git solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
index 22a7360..976a027 100644
--- solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
+++ solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
@@ -153,7 +153,7 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
   
   // map of generic caches - not synchronized since it's read-only after the constructor.
   private final HashMap<String, SolrCache> cacheMap;
-  private static final HashMap<String, SolrCache> noGenericCaches=new HashMap<String,SolrCache>(0);
+  private static final HashMap<String, SolrCache> noGenericCaches=new HashMap<>(0);
 
   // list of all caches associated with this searcher.
   private final SolrCache[] cacheList;
@@ -225,7 +225,7 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
     
     cachingEnabled=enableCache;
     if (cachingEnabled) {
-      ArrayList<SolrCache> clist = new ArrayList<SolrCache>();
+      ArrayList<SolrCache> clist = new ArrayList<>();
       fieldValueCache = solrConfig.fieldValueCacheConfig==null ? null : solrConfig.fieldValueCacheConfig.newInstance();
       if (fieldValueCache!=null) clist.add(fieldValueCache);
       filterCache= solrConfig.filterCacheConfig==null ? null : solrConfig.filterCacheConfig.newInstance();
@@ -238,7 +238,7 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
       if (solrConfig.userCacheConfigs == null) {
         cacheMap = noGenericCaches;
       } else {
-        cacheMap = new HashMap<String,SolrCache>(solrConfig.userCacheConfigs.length);
+        cacheMap = new HashMap<>(solrConfig.userCacheConfigs.length);
         for (CacheConfig userCacheConfig : solrConfig.userCacheConfigs) {
           SolrCache cache = null;
           if (userCacheConfig != null) cache = userCacheConfig.newInstance();
@@ -263,7 +263,7 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
 //    optimizer = solrConfig.filtOptEnabled ? new LuceneQueryOptimizer(solrConfig.filtOptCacheSize,solrConfig.filtOptThreshold) : null;
     optimizer = null;
     
-    fieldNames = new HashSet<String>();
+    fieldNames = new HashSet<>();
     fieldInfos = atomicReader.getFieldInfos();
     for(FieldInfo fieldInfo : fieldInfos) {
       fieldNames.add(fieldInfo.name);
@@ -388,7 +388,7 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
   public Collection<String> getStoredHighlightFieldNames() {
     synchronized (this) {
       if (storedHighlightFieldNames == null) {
-        storedHighlightFieldNames = new LinkedList<String>();
+        storedHighlightFieldNames = new LinkedList<>();
         for (String fieldName : fieldNames) {
           try {
             SchemaField field = schema.getField(fieldName);
@@ -980,10 +980,10 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
         ExtendedQuery eq = (ExtendedQuery)q;
         if (!eq.getCache()) {
           if (eq.getCost() >= 100 && eq instanceof PostFilter) {
-            if (postFilters == null) postFilters = new ArrayList<Query>(sets.length-end);
+            if (postFilters == null) postFilters = new ArrayList<>(sets.length-end);
             postFilters.add(q);
           } else {
-            if (notCached == null) notCached = new ArrayList<Query>(sets.length-end);
+            if (notCached == null) notCached = new ArrayList<>(sets.length-end);
             notCached.add(q);
           }
           continue;
@@ -992,7 +992,7 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
       
       if (filterCache == null) {
         // there is no cache: don't pull bitsets
-        if (notCached == null) notCached = new ArrayList<Query>(sets.length-end);
+        if (notCached == null) notCached = new ArrayList<>(sets.length-end);
         WrappedQuery uncached = new WrappedQuery(q);
         uncached.setCache(false);
         notCached.add(uncached);
@@ -1036,7 +1036,7 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
 
     if (notCached != null) {
       Collections.sort(notCached, sortByCost);
-      List<Weight> weights = new ArrayList<Weight>(notCached.size());
+      List<Weight> weights = new ArrayList<>(notCached.size());
       for (Query q : notCached) {
         Query qq = QueryUtils.makeQueryable(q);
         weights.add(createNormalizedWeight(qq));
@@ -1323,7 +1323,7 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
               if (cmd.getFilterList()==null) {
                 out.docSet = getDocSet(cmd.getQuery());
               } else {
-                List<Query> newList = new ArrayList<Query>(cmd.getFilterList().size()+1);
+                List<Query> newList = new ArrayList<>(cmd.getFilterList().size()+1);
                 newList.add(cmd.getQuery());
                 newList.addAll(cmd.getFilterList());
                 out.docSet = getDocSet(newList);
@@ -2241,7 +2241,7 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
 
   @Override
   public NamedList<Object> getStatistics() {
-    NamedList<Object> lst = new SimpleOrderedMap<Object>();
+    NamedList<Object> lst = new SimpleOrderedMap<>();
     lst.add("searcherName", name);
     lst.add("caching", cachingEnabled);
     lst.add("numDocs", reader.numDocs());
@@ -2319,7 +2319,7 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
       }
       filterList = null;
       if (f != null) {
-        filterList = new ArrayList<Query>(2);
+        filterList = new ArrayList<>(2);
         filterList.add(f);
       }
       return this;
@@ -2465,7 +2465,7 @@ class FilterImpl extends Filter {
 
     @Override
     public DocIdSetIterator iterator() throws IOException {
-      List<DocIdSetIterator> iterators = new ArrayList<DocIdSetIterator>(weights.size()+1);
+      List<DocIdSetIterator> iterators = new ArrayList<>(weights.size()+1);
       if (docIdSet != null) {
         DocIdSetIterator iter = docIdSet.iterator();
         if (iter == null) return null;
diff --git solr/core/src/java/org/apache/solr/search/SolrReturnFields.java solr/core/src/java/org/apache/solr/search/SolrReturnFields.java
index 03289ba..d7c6964 100644
--- solr/core/src/java/org/apache/solr/search/SolrReturnFields.java
+++ solr/core/src/java/org/apache/solr/search/SolrReturnFields.java
@@ -49,14 +49,14 @@ public class SolrReturnFields extends ReturnFields {
   // Special Field Keys
   public static final String SCORE = "score";
 
-  private final List<String> globs = new ArrayList<String>(1);
+  private final List<String> globs = new ArrayList<>(1);
 
   // The lucene field names to request from the SolrIndexSearcher
-  private final Set<String> fields = new HashSet<String>();
+  private final Set<String> fields = new HashSet<>();
 
   // Field names that are OK to include in the response.
   // This will include pseudo fields, lucene fields, and matching globs
-  private Set<String> okFieldNames = new HashSet<String>();
+  private Set<String> okFieldNames = new HashSet<>();
 
   // The list of explicitly requested fields
   // Order is important for CSVResponseWriter
@@ -106,7 +106,7 @@ public class SolrReturnFields extends ReturnFields {
       return;
     }
 
-    NamedList<String> rename = new NamedList<String>();
+    NamedList<String> rename = new NamedList<>();
     DocTransformers augmenters = new DocTransformers();
     for (String fieldList : fl) {
       add(fieldList,rename,augmenters,req);
@@ -122,7 +122,7 @@ public class SolrReturnFields extends ReturnFields {
           if(from.equals(rename.getName(j))) {
             rename.setName(j, to); // copy from the current target
             if(reqFieldNames==null) {
-              reqFieldNames = new LinkedHashSet<String>();
+              reqFieldNames = new LinkedHashSet<>();
             }
             reqFieldNames.add(to); // don't rename our current target
           }
@@ -247,7 +247,7 @@ public class SolrReturnFields extends ReturnFields {
         // This is identical to localParams syntax except it uses [] instead of {!}
 
         if (funcStr.startsWith("[")) {
-          Map<String,String> augmenterArgs = new HashMap<String,String>();
+          Map<String,String> augmenterArgs = new HashMap<>();
           int end = QueryParsing.parseLocalParams(funcStr, 0, augmenterArgs, req.getParams(), "[", ']');
           sp.pos += end;
 
@@ -356,7 +356,7 @@ public class SolrReturnFields extends ReturnFields {
   private void addField(String field, String key, DocTransformers augmenters, boolean isPseudoField)
   {
     if(reqFieldNames==null) {
-      reqFieldNames = new LinkedHashSet<String>();
+      reqFieldNames = new LinkedHashSet<>();
     }
     
     if(key==null) {
diff --git solr/core/src/java/org/apache/solr/search/ValueSourceParser.java solr/core/src/java/org/apache/solr/search/ValueSourceParser.java
index 6386968..a2b567e 100644
--- solr/core/src/java/org/apache/solr/search/ValueSourceParser.java
+++ solr/core/src/java/org/apache/solr/search/ValueSourceParser.java
@@ -65,7 +65,7 @@ public abstract class ValueSourceParser implements NamedListInitializedPlugin {
   public abstract ValueSource parse(FunctionQParser fp) throws SyntaxError;
 
   /* standard functions */
-  public static Map<String, ValueSourceParser> standardValueSourceParsers = new HashMap<String, ValueSourceParser>();
+  public static Map<String, ValueSourceParser> standardValueSourceParsers = new HashMap<>();
 
   /** Adds a new parser for the name and returns any existing one that was overriden.
    *  This is not thread safe.
@@ -849,8 +849,8 @@ public abstract class ValueSourceParser implements NamedListInitializedPlugin {
       }
     } else {
       int dim = sources.size() / 2;
-      List<ValueSource> sources1 = new ArrayList<ValueSource>(dim);
-      List<ValueSource> sources2 = new ArrayList<ValueSource>(dim);
+      List<ValueSource> sources1 = new ArrayList<>(dim);
+      List<ValueSource> sources2 = new ArrayList<>(dim);
       //Get dim value sources for the first vector
       splitSources(dim, sources, sources1, sources2);
       mvr.mv1 = new VectorValueSource(sources1);
diff --git solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java
index b6bb66b..a2c1acc 100644
--- solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java
+++ solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java
@@ -261,7 +261,7 @@ public class FileFloatSource extends ValueSource {
     // because of this, simply ask the reader for a new termEnum rather than
     // trying to use skipTo()
 
-    List<String> notFound = new ArrayList<String>();
+    List<String> notFound = new ArrayList<>();
     int notFoundCount=0;
     int otherErrors=0;
 
diff --git solr/core/src/java/org/apache/solr/search/grouping/CommandHandler.java solr/core/src/java/org/apache/solr/search/grouping/CommandHandler.java
index 9659472..4d10c93 100644
--- solr/core/src/java/org/apache/solr/search/grouping/CommandHandler.java
+++ solr/core/src/java/org/apache/solr/search/grouping/CommandHandler.java
@@ -53,7 +53,7 @@ public class CommandHandler {
   public static class Builder {
 
     private SolrIndexSearcher.QueryCommand queryCommand;
-    private List<Command> commands = new ArrayList<Command>();
+    private List<Command> commands = new ArrayList<>();
     private SolrIndexSearcher searcher;
     private boolean needDocSet = false;
     private boolean truncateGroups = false;
@@ -137,7 +137,7 @@ public class CommandHandler {
   @SuppressWarnings("unchecked")
   public void execute() throws IOException {
     final int nrOfCommands = commands.size();
-    List<Collector> collectors = new ArrayList<Collector>(nrOfCommands);
+    List<Collector> collectors = new ArrayList<>(nrOfCommands);
     for (Command command : commands) {
       collectors.addAll(command.create());
     }
diff --git solr/core/src/java/org/apache/solr/search/grouping/distributed/command/SearchGroupsFieldCommand.java solr/core/src/java/org/apache/solr/search/grouping/distributed/command/SearchGroupsFieldCommand.java
index 37b66a8..b079847 100644
--- solr/core/src/java/org/apache/solr/search/grouping/distributed/command/SearchGroupsFieldCommand.java
+++ solr/core/src/java/org/apache/solr/search/grouping/distributed/command/SearchGroupsFieldCommand.java
@@ -88,7 +88,7 @@ public class SearchGroupsFieldCommand implements Command<Pair<Integer, Collectio
 
   @Override
   public List<Collector> create() throws IOException {
-    List<Collector> collectors = new ArrayList<Collector>();
+    List<Collector> collectors = new ArrayList<>();
     if (topNGroups > 0) {
       firstPassGroupingCollector = new TermFirstPassGroupingCollector(field.getName(), groupSort, topNGroups);
       collectors.add(firstPassGroupingCollector);
@@ -114,7 +114,7 @@ public class SearchGroupsFieldCommand implements Command<Pair<Integer, Collectio
     } else {
       groupCount = null;
     }
-    return new Pair<Integer, Collection<SearchGroup<BytesRef>>>(groupCount, topGroups);
+    return new Pair<>(groupCount, topGroups);
   }
 
   @Override
diff --git solr/core/src/java/org/apache/solr/search/grouping/distributed/command/TopGroupsFieldCommand.java solr/core/src/java/org/apache/solr/search/grouping/distributed/command/TopGroupsFieldCommand.java
index 256f427..ad027cf 100644
--- solr/core/src/java/org/apache/solr/search/grouping/distributed/command/TopGroupsFieldCommand.java
+++ solr/core/src/java/org/apache/solr/search/grouping/distributed/command/TopGroupsFieldCommand.java
@@ -125,7 +125,7 @@ public class TopGroupsFieldCommand implements Command<TopGroups<BytesRef>> {
       return Collections.emptyList();
     }
 
-    List<Collector> collectors = new ArrayList<Collector>();
+    List<Collector> collectors = new ArrayList<>();
     secondPassCollector = new TermSecondPassGroupingCollector(
           field.getName(), firstPhaseGroups, groupSort, sortWithinGroup, maxDocPerGroup, needScores, needMaxScore, true
     );
diff --git solr/core/src/java/org/apache/solr/search/grouping/distributed/requestfactory/StoredFieldsShardRequestFactory.java solr/core/src/java/org/apache/solr/search/grouping/distributed/requestfactory/StoredFieldsShardRequestFactory.java
index f2032f5..ba09068 100644
--- solr/core/src/java/org/apache/solr/search/grouping/distributed/requestfactory/StoredFieldsShardRequestFactory.java
+++ solr/core/src/java/org/apache/solr/search/grouping/distributed/requestfactory/StoredFieldsShardRequestFactory.java
@@ -42,7 +42,7 @@ public class StoredFieldsShardRequestFactory implements ShardRequestFactory {
 
   @Override
   public ShardRequest[] constructRequest(ResponseBuilder rb) {
-    HashMap<String, Set<ShardDoc>> shardMap = new HashMap<String,Set<ShardDoc>>();
+    HashMap<String, Set<ShardDoc>> shardMap = new HashMap<>();
     for (TopGroups<BytesRef> topGroups : rb.mergedTopGroups.values()) {
       for (GroupDocs<BytesRef> group : topGroups.groups) {
         mapShardToDocs(shardMap, group.scoreDocs);
@@ -75,7 +75,7 @@ public class StoredFieldsShardRequestFactory implements ShardRequestFactory {
          }
       }
 
-      List<String> ids = new ArrayList<String>(shardDocs.size());
+      List<String> ids = new ArrayList<>(shardDocs.size());
       for (ShardDoc shardDoc : shardDocs) {
         ids.add(shardDoc.id.toString());
       }
@@ -91,7 +91,7 @@ public class StoredFieldsShardRequestFactory implements ShardRequestFactory {
       ShardDoc solrDoc = (ShardDoc) scoreDoc;
       Set<ShardDoc> shardDocs = shardMap.get(solrDoc.shard);
       if (shardDocs == null) {
-        shardMap.put(solrDoc.shard, shardDocs = new HashSet<ShardDoc>());
+        shardMap.put(solrDoc.shard, shardDocs = new HashSet<>());
       }
       shardDocs.add(solrDoc);
     }
diff --git solr/core/src/java/org/apache/solr/search/grouping/distributed/requestfactory/TopGroupsShardRequestFactory.java solr/core/src/java/org/apache/solr/search/grouping/distributed/requestfactory/TopGroupsShardRequestFactory.java
index a356455..d7eba2e 100644
--- solr/core/src/java/org/apache/solr/search/grouping/distributed/requestfactory/TopGroupsShardRequestFactory.java
+++ solr/core/src/java/org/apache/solr/search/grouping/distributed/requestfactory/TopGroupsShardRequestFactory.java
@@ -67,7 +67,7 @@ public class TopGroupsShardRequestFactory implements ShardRequestFactory {
 
   private ShardRequest[] createRequestForSpecificShards(ResponseBuilder rb) {
     // Determine all unique shards to query for TopGroups
-    Set<String> uniqueShards = new HashSet<String>();
+    Set<String> uniqueShards = new HashSet<>();
     for (String command : rb.searchGroupToShards.keySet()) {
       Map<SearchGroup<BytesRef>, Set<String>> groupsToShard = rb.searchGroupToShards.get(command);
       for (Set<String> shards : groupsToShard.values()) {
diff --git solr/core/src/java/org/apache/solr/search/grouping/distributed/responseprocessor/SearchGroupShardResponseProcessor.java solr/core/src/java/org/apache/solr/search/grouping/distributed/responseprocessor/SearchGroupShardResponseProcessor.java
index af75df7..d5cefc1 100644
--- solr/core/src/java/org/apache/solr/search/grouping/distributed/responseprocessor/SearchGroupShardResponseProcessor.java
+++ solr/core/src/java/org/apache/solr/search/grouping/distributed/responseprocessor/SearchGroupShardResponseProcessor.java
@@ -52,8 +52,8 @@ public class SearchGroupShardResponseProcessor implements ShardResponseProcessor
     Sort groupSort = rb.getGroupingSpec().getGroupSort();
     String[] fields = rb.getGroupingSpec().getFields();
 
-    Map<String, List<Collection<SearchGroup<BytesRef>>>> commandSearchGroups = new HashMap<String, List<Collection<SearchGroup<BytesRef>>>>();
-    Map<String, Map<SearchGroup<BytesRef>, Set<String>>> tempSearchGroupToShards = new HashMap<String, Map<SearchGroup<BytesRef>, Set<String>>>();
+    Map<String, List<Collection<SearchGroup<BytesRef>>>> commandSearchGroups = new HashMap<>();
+    Map<String, Map<SearchGroup<BytesRef>, Set<String>>> tempSearchGroupToShards = new HashMap<>();
     for (String field : fields) {
       commandSearchGroups.put(field, new ArrayList<Collection<SearchGroup<BytesRef>>>(shardRequest.responses.size()));
       tempSearchGroupToShards.put(field, new HashMap<SearchGroup<BytesRef>, Set<String>>());
@@ -69,13 +69,13 @@ public class SearchGroupShardResponseProcessor implements ShardResponseProcessor
 
       NamedList<Object> shardInfo = null;
       if (rb.req.getParams().getBool(ShardParams.SHARDS_INFO, false)) {
-        shardInfo = new SimpleOrderedMap<Object>();
+        shardInfo = new SimpleOrderedMap<>();
         rb.rsp.getValues().add(ShardParams.SHARDS_INFO + ".firstPhase", shardInfo);
       }
 
       for (ShardResponse srsp : shardRequest.responses) {
         if (shardInfo != null) {
-          SimpleOrderedMap<Object> nl = new SimpleOrderedMap<Object>();
+          SimpleOrderedMap<Object> nl = new SimpleOrderedMap<>();
 
           if (srsp.getException() != null) {
             Throwable t = srsp.getException();
@@ -126,7 +126,7 @@ public class SearchGroupShardResponseProcessor implements ShardResponseProcessor
             Map<SearchGroup<BytesRef>, java.util.Set<String>> map = tempSearchGroupToShards.get(field);
             Set<String> shards = map.get(searchGroup);
             if (shards == null) {
-              shards = new HashSet<String>();
+              shards = new HashSet<>();
               map.put(searchGroup, shards);
             }
             shards.add(srsp.getShard());
diff --git solr/core/src/java/org/apache/solr/search/grouping/distributed/responseprocessor/TopGroupsShardResponseProcessor.java solr/core/src/java/org/apache/solr/search/grouping/distributed/responseprocessor/TopGroupsShardResponseProcessor.java
index f7a22f0..2d2c146 100644
--- solr/core/src/java/org/apache/solr/search/grouping/distributed/responseprocessor/TopGroupsShardResponseProcessor.java
+++ solr/core/src/java/org/apache/solr/search/grouping/distributed/responseprocessor/TopGroupsShardResponseProcessor.java
@@ -70,12 +70,12 @@ public class TopGroupsShardResponseProcessor implements ShardResponseProcessor {
     }
     int docsPerGroupDefault = rb.getGroupingSpec().getGroupLimit();
 
-    Map<String, List<TopGroups<BytesRef>>> commandTopGroups = new HashMap<String, List<TopGroups<BytesRef>>>();
+    Map<String, List<TopGroups<BytesRef>>> commandTopGroups = new HashMap<>();
     for (String field : fields) {
       commandTopGroups.put(field, new ArrayList<TopGroups<BytesRef>>());
     }
 
-    Map<String, List<QueryCommandResult>> commandTopDocs = new HashMap<String, List<QueryCommandResult>>();
+    Map<String, List<QueryCommandResult>> commandTopDocs = new HashMap<>();
     for (String query : queries) {
       commandTopDocs.put(query, new ArrayList<QueryCommandResult>());
     }
@@ -84,14 +84,14 @@ public class TopGroupsShardResponseProcessor implements ShardResponseProcessor {
 
     NamedList<Object> shardInfo = null;
     if (rb.req.getParams().getBool(ShardParams.SHARDS_INFO, false)) {
-      shardInfo = new SimpleOrderedMap<Object>();
+      shardInfo = new SimpleOrderedMap<>();
       rb.rsp.getValues().add(ShardParams.SHARDS_INFO, shardInfo);
     }
 
     for (ShardResponse srsp : shardRequest.responses) {
       SimpleOrderedMap<Object> individualShardInfo = null;
       if (shardInfo != null) {
-        individualShardInfo = new SimpleOrderedMap<Object>();
+        individualShardInfo = new SimpleOrderedMap<>();
 
         if (srsp.getException() != null) {
           Throwable t = srsp.getException();
@@ -161,7 +161,7 @@ public class TopGroupsShardResponseProcessor implements ShardResponseProcessor {
 
       for (String query : commandTopDocs.keySet()) {
         List<QueryCommandResult> queryCommandResults = commandTopDocs.get(query);
-        List<TopDocs> topDocs = new ArrayList<TopDocs>(queryCommandResults.size());
+        List<TopDocs> topDocs = new ArrayList<>(queryCommandResults.size());
         int mergedMatches = 0;
         for (QueryCommandResult queryCommandResult : queryCommandResults) {
           topDocs.add(queryCommandResult.getTopDocs());
@@ -173,7 +173,7 @@ public class TopGroupsShardResponseProcessor implements ShardResponseProcessor {
         rb.mergedQueryCommandResults.put(query, new QueryCommandResult(mergedTopDocs, mergedMatches));
       }
 
-      Map<Object, ShardDoc> resultIds = new HashMap<Object, ShardDoc>();
+      Map<Object, ShardDoc> resultIds = new HashMap<>();
       int i = 0;
       for (TopGroups<BytesRef> topGroups : rb.mergedTopGroups.values()) {
         for (GroupDocs<BytesRef> group : topGroups.groups) {
diff --git solr/core/src/java/org/apache/solr/search/grouping/distributed/shardresultserializer/SearchGroupsResultTransformer.java solr/core/src/java/org/apache/solr/search/grouping/distributed/shardresultserializer/SearchGroupsResultTransformer.java
index 33d3c6b..efa7824 100644
--- solr/core/src/java/org/apache/solr/search/grouping/distributed/shardresultserializer/SearchGroupsResultTransformer.java
+++ solr/core/src/java/org/apache/solr/search/grouping/distributed/shardresultserializer/SearchGroupsResultTransformer.java
@@ -49,9 +49,9 @@ public class SearchGroupsResultTransformer implements ShardResultTransformer<Lis
    */
   @Override
   public NamedList transform(List<Command> data) throws IOException {
-    NamedList<NamedList> result = new NamedList<NamedList>();
+    NamedList<NamedList> result = new NamedList<>();
     for (Command command : data) {
-      final NamedList<Object> commandResult = new NamedList<Object>();
+      final NamedList<Object> commandResult = new NamedList<>();
       if (SearchGroupsFieldCommand.class.isInstance(command)) {
         SearchGroupsFieldCommand fieldCommand = (SearchGroupsFieldCommand) command;
         Pair<Integer, Collection<SearchGroup<BytesRef>>> pair = fieldCommand.result();
@@ -77,15 +77,15 @@ public class SearchGroupsResultTransformer implements ShardResultTransformer<Lis
    */
   @Override
   public Map<String, Pair<Integer, Collection<SearchGroup<BytesRef>>>> transformToNative(NamedList<NamedList> shardResponse, Sort groupSort, Sort sortWithinGroup, String shard) {
-    Map<String, Pair<Integer, Collection<SearchGroup<BytesRef>>>> result = new HashMap<String, Pair<Integer, Collection<SearchGroup<BytesRef>>>>();
+    Map<String, Pair<Integer, Collection<SearchGroup<BytesRef>>>> result = new HashMap<>();
     for (Map.Entry<String, NamedList> command : shardResponse) {
-      List<SearchGroup<BytesRef>> searchGroups = new ArrayList<SearchGroup<BytesRef>>();
+      List<SearchGroup<BytesRef>> searchGroups = new ArrayList<>();
       NamedList topGroupsAndGroupCount = command.getValue();
       @SuppressWarnings("unchecked")
       NamedList<List<Comparable>> rawSearchGroups = (NamedList<List<Comparable>>) topGroupsAndGroupCount.get("topGroups");
       if (rawSearchGroups != null) {
         for (Map.Entry<String, List<Comparable>> rawSearchGroup : rawSearchGroups){
-          SearchGroup<BytesRef> searchGroup = new SearchGroup<BytesRef>();
+          SearchGroup<BytesRef> searchGroup = new SearchGroup<>();
           searchGroup.groupValue = rawSearchGroup.getKey() != null ? new BytesRef(rawSearchGroup.getKey()) : null;
           searchGroup.sortValues = rawSearchGroup.getValue().toArray(new Comparable[rawSearchGroup.getValue().size()]);
           for (int i = 0; i < searchGroup.sortValues.length; i++) {
@@ -108,7 +108,7 @@ public class SearchGroupsResultTransformer implements ShardResultTransformer<Lis
   }
 
   private NamedList serializeSearchGroup(Collection<SearchGroup<BytesRef>> data, Sort groupSort) {
-    NamedList<Object[]> result = new NamedList<Object[]>();
+    NamedList<Object[]> result = new NamedList<>();
 
     for (SearchGroup<BytesRef> searchGroup : data) {
       Object[] convertedSortValues = new Object[searchGroup.sortValues.length];
diff --git solr/core/src/java/org/apache/solr/search/grouping/distributed/shardresultserializer/TopGroupsResultTransformer.java solr/core/src/java/org/apache/solr/search/grouping/distributed/shardresultserializer/TopGroupsResultTransformer.java
index 1513d09..23bca60 100644
--- solr/core/src/java/org/apache/solr/search/grouping/distributed/shardresultserializer/TopGroupsResultTransformer.java
+++ solr/core/src/java/org/apache/solr/search/grouping/distributed/shardresultserializer/TopGroupsResultTransformer.java
@@ -66,7 +66,7 @@ public class TopGroupsResultTransformer implements ShardResultTransformer<List<C
    */
   @Override
   public NamedList transform(List<Command> data) throws IOException {
-    NamedList<NamedList> result = new NamedList<NamedList>();
+    NamedList<NamedList> result = new NamedList<>();
     final IndexSchema schema = rb.req.getSearcher().getSchema();
     for (Command command : data) {
       NamedList commandResult;
@@ -91,7 +91,7 @@ public class TopGroupsResultTransformer implements ShardResultTransformer<List<C
    */
   @Override
   public Map<String, ?> transformToNative(NamedList<NamedList> shardResponse, Sort groupSort, Sort sortWithinGroup, String shard) {
-    Map<String, Object> result = new HashMap<String, Object>();
+    Map<String, Object> result = new HashMap<>();
 
     final IndexSchema schema = rb.req.getSearcher().getSchema();
 
@@ -147,7 +147,7 @@ public class TopGroupsResultTransformer implements ShardResultTransformer<List<C
 
       Integer totalHitCount = (Integer) commandResult.get("totalHitCount");
 
-      List<GroupDocs<BytesRef>> groupDocs = new ArrayList<GroupDocs<BytesRef>>();
+      List<GroupDocs<BytesRef>> groupDocs = new ArrayList<>();
       for (int i = 2; i < commandResult.size(); i++) {
         String groupValue = commandResult.getName(i);
         @SuppressWarnings("unchecked")
@@ -182,12 +182,12 @@ public class TopGroupsResultTransformer implements ShardResultTransformer<List<C
         }
 
         BytesRef groupValueRef = groupValue != null ? new BytesRef(groupValue) : null;
-        groupDocs.add(new GroupDocs<BytesRef>(Float.NaN, maxScore, totalGroupHits, scoreDocs, groupValueRef, null));
+        groupDocs.add(new GroupDocs<>(Float.NaN, maxScore, totalGroupHits, scoreDocs, groupValueRef, null));
       }
 
       @SuppressWarnings("unchecked")
       GroupDocs<BytesRef>[] groupDocsArr = groupDocs.toArray(new GroupDocs[groupDocs.size()]);
-      TopGroups<BytesRef> topGroups = new TopGroups<BytesRef>(
+      TopGroups<BytesRef> topGroups = new TopGroups<>(
            groupSort.getSort(), sortWithinGroup.getSort(), totalHitCount, totalGroupedHitCount, groupDocsArr, Float.NaN
       );
 
@@ -198,7 +198,7 @@ public class TopGroupsResultTransformer implements ShardResultTransformer<List<C
   }
 
   protected NamedList serializeTopGroups(TopGroups<BytesRef> data, SchemaField groupField) throws IOException {
-    NamedList<Object> result = new NamedList<Object>();
+    NamedList<Object> result = new NamedList<>();
     result.add("totalGroupedHitCount", data.totalGroupedHitCount);
     result.add("totalHitCount", data.totalHitCount);
     if (data.totalGroupCount != null) {
@@ -209,15 +209,15 @@ public class TopGroupsResultTransformer implements ShardResultTransformer<List<C
     final IndexSchema schema = rb.req.getSearcher().getSchema();
     SchemaField uniqueField = schema.getUniqueKeyField();
     for (GroupDocs<BytesRef> searchGroup : data.groups) {
-      NamedList<Object> groupResult = new NamedList<Object>();
+      NamedList<Object> groupResult = new NamedList<>();
       groupResult.add("totalHits", searchGroup.totalHits);
       if (!Float.isNaN(searchGroup.maxScore)) {
         groupResult.add("maxScore", searchGroup.maxScore);
       }
 
-      List<NamedList<Object>> documents = new ArrayList<NamedList<Object>>();
+      List<NamedList<Object>> documents = new ArrayList<>();
       for (int i = 0; i < searchGroup.scoreDocs.length; i++) {
-        NamedList<Object> document = new NamedList<Object>();
+        NamedList<Object> document = new NamedList<>();
         documents.add(document);
 
         StoredDocument doc = retrieveDocument(uniqueField, searchGroup.scoreDocs[i].doc);
@@ -254,20 +254,20 @@ public class TopGroupsResultTransformer implements ShardResultTransformer<List<C
   }
 
   protected NamedList serializeTopDocs(QueryCommandResult result) throws IOException {
-    NamedList<Object> queryResult = new NamedList<Object>();
+    NamedList<Object> queryResult = new NamedList<>();
     queryResult.add("matches", result.getMatches());
     queryResult.add("totalHits", result.getTopDocs().totalHits);
     if (rb.getGroupingSpec().isNeedScore()) {
       queryResult.add("maxScore", result.getTopDocs().getMaxScore());
     }
-    List<NamedList> documents = new ArrayList<NamedList>();
+    List<NamedList> documents = new ArrayList<>();
     queryResult.add("documents", documents);
 
     final IndexSchema schema = rb.req.getSearcher().getSchema();
     SchemaField uniqueField = schema.getUniqueKeyField();
     CharsRef spare = new CharsRef();
     for (ScoreDoc scoreDoc : result.getTopDocs().scoreDocs) {
-      NamedList<Object> document = new NamedList<Object>();
+      NamedList<Object> document = new NamedList<>();
       documents.add(document);
 
       StoredDocument doc = retrieveDocument(uniqueField, scoreDoc.doc);
diff --git solr/core/src/java/org/apache/solr/search/grouping/endresulttransformer/GroupedEndResultTransformer.java solr/core/src/java/org/apache/solr/search/grouping/endresulttransformer/GroupedEndResultTransformer.java
index 0414aff..a1707d0 100644
--- solr/core/src/java/org/apache/solr/search/grouping/endresulttransformer/GroupedEndResultTransformer.java
+++ solr/core/src/java/org/apache/solr/search/grouping/endresulttransformer/GroupedEndResultTransformer.java
@@ -50,24 +50,24 @@ public class GroupedEndResultTransformer implements EndResultTransformer {
    */
   @Override
   public void transform(Map<String, ?> result, ResponseBuilder rb, SolrDocumentSource solrDocumentSource) {
-    NamedList<Object> commands = new SimpleOrderedMap<Object>();
+    NamedList<Object> commands = new SimpleOrderedMap<>();
     for (Map.Entry<String, ?> entry : result.entrySet()) {
       Object value = entry.getValue();
       if (TopGroups.class.isInstance(value)) {
         @SuppressWarnings("unchecked")
         TopGroups<BytesRef> topGroups = (TopGroups<BytesRef>) value;
-        NamedList<Object> command = new SimpleOrderedMap<Object>();
+        NamedList<Object> command = new SimpleOrderedMap<>();
         command.add("matches", rb.totalHitCount);
         Integer totalGroupCount = rb.mergedGroupCounts.get(entry.getKey());
         if (totalGroupCount != null) {
           command.add("ngroups", totalGroupCount);
         }
 
-        List<NamedList> groups = new ArrayList<NamedList>();
+        List<NamedList> groups = new ArrayList<>();
         SchemaField groupField = searcher.getSchema().getField(entry.getKey());
         FieldType groupFieldType = groupField.getType();
         for (GroupDocs<BytesRef> group : topGroups.groups) {
-          SimpleOrderedMap<Object> groupResult = new SimpleOrderedMap<Object>();
+          SimpleOrderedMap<Object> groupResult = new SimpleOrderedMap<>();
           if (group.groupValue != null) {
             groupResult.add(
                 "groupValue", groupFieldType.toObject(groupField.createField(group.groupValue.utf8ToString(), 1.0f))
@@ -91,7 +91,7 @@ public class GroupedEndResultTransformer implements EndResultTransformer {
         commands.add(entry.getKey(), command);
       } else if (QueryCommandResult.class.isInstance(value)) {
         QueryCommandResult queryCommandResult = (QueryCommandResult) value;
-        NamedList<Object> command = new SimpleOrderedMap<Object>();
+        NamedList<Object> command = new SimpleOrderedMap<>();
         command.add("matches", queryCommandResult.getMatches());
         SolrDocumentList docList = new SolrDocumentList();
         docList.setNumFound(queryCommandResult.getTopDocs().totalHits);
diff --git solr/core/src/java/org/apache/solr/search/grouping/endresulttransformer/SimpleEndResultTransformer.java solr/core/src/java/org/apache/solr/search/grouping/endresulttransformer/SimpleEndResultTransformer.java
index ff866ae..5667181 100644
--- solr/core/src/java/org/apache/solr/search/grouping/endresulttransformer/SimpleEndResultTransformer.java
+++ solr/core/src/java/org/apache/solr/search/grouping/endresulttransformer/SimpleEndResultTransformer.java
@@ -38,13 +38,13 @@ public class SimpleEndResultTransformer implements EndResultTransformer {
    */
   @Override
   public void transform(Map<String, ?> result, ResponseBuilder rb, SolrDocumentSource solrDocumentSource) {
-    NamedList<Object> commands = new SimpleOrderedMap<Object>();
+    NamedList<Object> commands = new SimpleOrderedMap<>();
     for (Map.Entry<String, ?> entry : result.entrySet()) {
       Object value = entry.getValue();
       if (TopGroups.class.isInstance(value)) {
         @SuppressWarnings("unchecked")
         TopGroups<BytesRef> topGroups = (TopGroups<BytesRef>) value;
-        NamedList<Object> command = new SimpleOrderedMap<Object>();
+        NamedList<Object> command = new SimpleOrderedMap<>();
         command.add("matches", rb.totalHitCount);
         if (topGroups.totalGroupCount != null) {
           command.add("ngroups", topGroups.totalGroupCount);
diff --git solr/core/src/java/org/apache/solr/servlet/DirectSolrConnection.java solr/core/src/java/org/apache/solr/servlet/DirectSolrConnection.java
index c97b91d..f2073da 100644
--- solr/core/src/java/org/apache/solr/servlet/DirectSolrConnection.java
+++ solr/core/src/java/org/apache/solr/servlet/DirectSolrConnection.java
@@ -118,7 +118,7 @@ public class DirectSolrConnection
       params = new MapSolrParams( new HashMap<String, String>() );
 
     // Make a stream for the 'body' content
-    List<ContentStream> streams = new ArrayList<ContentStream>( 1 );
+    List<ContentStream> streams = new ArrayList<>( 1 );
     if( body != null && body.length() > 0 ) {
       streams.add( new ContentStreamBase.StringStream( body ) );
     }
diff --git solr/core/src/java/org/apache/solr/servlet/SolrDispatchFilter.java solr/core/src/java/org/apache/solr/servlet/SolrDispatchFilter.java
index a7c4a0f..a8a2ae2 100644
--- solr/core/src/java/org/apache/solr/servlet/SolrDispatchFilter.java
+++ solr/core/src/java/org/apache/solr/servlet/SolrDispatchFilter.java
@@ -479,7 +479,7 @@ public class SolrDispatchFilter implements Filter
       collectionsList = StrUtils.splitSmart(collection, ",", true);
     }
     if (collectionsList != null) {
-      Set<String> newCollectionsList = new HashSet<String>(
+      Set<String> newCollectionsList = new HashSet<>(
           collectionsList.size());
       for (String col : collectionsList) {
         String al = aliases.getCollectionAlias(col);
@@ -601,7 +601,7 @@ public class SolrDispatchFilter implements Filter
     boolean byCoreName = false;
     
     if (slices == null) {
-      slices = new ArrayList<Slice>();
+      slices = new ArrayList<>();
       // look by core name
       byCoreName = true;
       slices = getSlicesForCollections(clusterState, slices, true);
diff --git solr/core/src/java/org/apache/solr/servlet/SolrRequestParsers.java solr/core/src/java/org/apache/solr/servlet/SolrRequestParsers.java
index f8bba33..5518ac8 100644
--- solr/core/src/java/org/apache/solr/servlet/SolrRequestParsers.java
+++ solr/core/src/java/org/apache/solr/servlet/SolrRequestParsers.java
@@ -76,7 +76,7 @@ public class SolrRequestParsers
   private static final byte[] INPUT_ENCODING_BYTES = INPUT_ENCODING_KEY.getBytes(CHARSET_US_ASCII);
 
   private final HashMap<String, SolrRequestParser> parsers =
-      new HashMap<String, SolrRequestParser>();
+      new HashMap<>();
   private final boolean enableRemoteStreams;
   private StandardRequestParser standard;
   private boolean handleSelect = true;
@@ -141,7 +141,7 @@ public class SolrRequestParsers
     // TODO -- in the future, we could pick a different parser based on the request
     
     // Pick the parser from the request...
-    ArrayList<ContentStream> streams = new ArrayList<ContentStream>(1);
+    ArrayList<ContentStream> streams = new ArrayList<>(1);
     SolrParams params = parser.parseParamsAndFillStreams( req, streams );
     SolrQueryRequest sreq = buildRequestFrom( core, params, streams );
 
@@ -213,7 +213,7 @@ public class SolrRequestParsers
    * Given a url-encoded query string (UTF-8), map it into solr params
    */
   public static MultiMapSolrParams parseQueryString(String queryString) {
-    Map<String,String[]> map = new HashMap<String, String[]>();
+    Map<String,String[]> map = new HashMap<>();
     parseQueryString(queryString, map);
     return new MultiMapSolrParams(map);
   }
@@ -261,7 +261,7 @@ public class SolrRequestParsers
   @SuppressWarnings({"fallthrough", "resource"})
   static long parseFormDataContent(final InputStream postContent, final long maxLen, Charset charset, final Map<String,String[]> map, boolean supportCharsetParam) throws IOException {
     CharsetDecoder charsetDecoder = supportCharsetParam ? null : getCharsetDecoder(charset);
-    final LinkedList<Object> buffer = supportCharsetParam ? new LinkedList<Object>() : null;
+    final LinkedList<Object> buffer = supportCharsetParam ? new LinkedList<>() : null;
     long len = 0L, keyPos = 0L, valuePos = 0L;
     final ByteArrayOutputStream keyStream = new ByteArrayOutputStream(),
       valueStream = new ByteArrayOutputStream();
@@ -580,7 +580,7 @@ public class SolrRequestParsers
         throw new SolrException( ErrorCode.BAD_REQUEST, "Not application/x-www-form-urlencoded content: "+req.getContentType() );
       }
 
-      final Map<String,String[]> map = new HashMap<String, String[]>();
+      final Map<String,String[]> map = new HashMap<>();
       
       // also add possible URL parameters and include into the map (parsed using UTF-8):
       final String qs = req.getQueryString();
diff --git solr/core/src/java/org/apache/solr/servlet/cache/HttpCacheHeaderUtil.java solr/core/src/java/org/apache/solr/servlet/cache/HttpCacheHeaderUtil.java
index 1e6b6b8..dc976b2 100644
--- solr/core/src/java/org/apache/solr/servlet/cache/HttpCacheHeaderUtil.java
+++ solr/core/src/java/org/apache/solr/servlet/cache/HttpCacheHeaderUtil.java
@@ -56,7 +56,7 @@ public final class HttpCacheHeaderUtil {
    * @see #calcEtag
    */
   private static Map<SolrCore, EtagCacheVal> etagCoreCache
-    = new WeakHashMap<SolrCore, EtagCacheVal>();
+    = new WeakHashMap<>();
 
   /** @see #etagCoreCache */
   private static class EtagCacheVal {
diff --git solr/core/src/java/org/apache/solr/spelling/ConjunctionSolrSpellChecker.java solr/core/src/java/org/apache/solr/spelling/ConjunctionSolrSpellChecker.java
index d6efcfb..e1e55f0 100644
--- solr/core/src/java/org/apache/solr/spelling/ConjunctionSolrSpellChecker.java
+++ solr/core/src/java/org/apache/solr/spelling/ConjunctionSolrSpellChecker.java
@@ -45,7 +45,7 @@ public class ConjunctionSolrSpellChecker extends SolrSpellChecker {
   private Float accuracy = null;
   private String dictionaryName = null;
   private Analyzer queryAnalyzer = null;
-  private List<SolrSpellChecker> checkers = new ArrayList<SolrSpellChecker>();
+  private List<SolrSpellChecker> checkers = new ArrayList<>();
   private boolean initalized = false;
   
   public void addChecker(SolrSpellChecker checker) {
@@ -136,8 +136,8 @@ public class ConjunctionSolrSpellChecker extends SolrSpellChecker {
   //TODO: This just interleaves the results.  In the future, we might want to let users give each checker its
   //      own weight and use that in combination to score & frequency to sort the results ?
   private SpellingResult mergeCheckers(SpellingResult[] results, int numSug) {
-    Map<Token, Integer> combinedTokenFrequency = new HashMap<Token, Integer>();
-    Map<Token, List<LinkedHashMap<String, Integer>>> allSuggestions = new LinkedHashMap<Token, List<LinkedHashMap<String, Integer>>>();
+    Map<Token, Integer> combinedTokenFrequency = new HashMap<>();
+    Map<Token, List<LinkedHashMap<String, Integer>>> allSuggestions = new LinkedHashMap<>();
     for(SpellingResult result : results) {
       if(result.getTokenFrequency()!=null) {
         combinedTokenFrequency.putAll(result.getTokenFrequency());
@@ -145,7 +145,7 @@ public class ConjunctionSolrSpellChecker extends SolrSpellChecker {
       for(Map.Entry<Token, LinkedHashMap<String, Integer>> entry : result.getSuggestions().entrySet()) {
         List<LinkedHashMap<String, Integer>> allForThisToken = allSuggestions.get(entry.getKey());
         if(allForThisToken==null) {
-          allForThisToken = new ArrayList<LinkedHashMap<String, Integer>>();
+          allForThisToken = new ArrayList<>();
           allSuggestions.put(entry.getKey(), allForThisToken);
         }
         allForThisToken.add(entry.getValue());
@@ -154,7 +154,7 @@ public class ConjunctionSolrSpellChecker extends SolrSpellChecker {
     SpellingResult combinedResult = new SpellingResult();    
     for(Map.Entry<Token, List<LinkedHashMap<String, Integer>>> entry : allSuggestions.entrySet()) {
       Token original = entry.getKey();      
-      List<Iterator<Map.Entry<String,Integer>>> corrIters = new ArrayList<Iterator<Map.Entry<String,Integer>>>(entry.getValue().size());
+      List<Iterator<Map.Entry<String,Integer>>> corrIters = new ArrayList<>(entry.getValue().size());
       for(LinkedHashMap<String, Integer> corrections : entry.getValue()) {
         corrIters.add(corrections.entrySet().iterator());
       }        
diff --git solr/core/src/java/org/apache/solr/spelling/PossibilityIterator.java solr/core/src/java/org/apache/solr/spelling/PossibilityIterator.java
index a2f8d8b..0ea275b 100644
--- solr/core/src/java/org/apache/solr/spelling/PossibilityIterator.java
+++ solr/core/src/java/org/apache/solr/spelling/PossibilityIterator.java
@@ -44,7 +44,7 @@ import org.apache.lucene.analysis.Token;
  */
 public class PossibilityIterator implements
     Iterator<PossibilityIterator.RankedSpellPossibility> {
-  private List<List<SpellCheckCorrection>> possibilityList = new ArrayList<List<SpellCheckCorrection>>();
+  private List<List<SpellCheckCorrection>> possibilityList = new ArrayList<>();
   private Iterator<RankedSpellPossibility> rankedPossibilityIterator = null;
   private int correctionIndex[];
   private boolean done = false;
@@ -74,7 +74,7 @@ public class PossibilityIterator implements
       if (entry.getValue().size() == 0) {
         continue;
       }
-      List<SpellCheckCorrection> possibleCorrections = new ArrayList<SpellCheckCorrection>();
+      List<SpellCheckCorrection> possibleCorrections = new ArrayList<>();
       for (Map.Entry<String,Integer> entry1 : entry.getValue().entrySet()) {
         SpellCheckCorrection correction = new SpellCheckCorrection();
         correction.setOriginal(token);
@@ -99,11 +99,11 @@ public class PossibilityIterator implements
         correctionIndex[i] = 0;
       }
     }
-    PriorityQueue<RankedSpellPossibility> rankedPossibilities = new PriorityQueue<RankedSpellPossibility>(
+    PriorityQueue<RankedSpellPossibility> rankedPossibilities = new PriorityQueue<>(
         11, new RankComparator());
     Set<RankedSpellPossibility> removeDuplicates = null;
     if (suggestionsMayOverlap) {
-      removeDuplicates = new HashSet<RankedSpellPossibility>();
+      removeDuplicates = new HashSet<>();
     }
     long numEvaluations = 0;
     while (numEvaluations < maxEvaluations && internalHasNext()) {
@@ -206,7 +206,7 @@ public class PossibilityIterator implements
       if (done) {
         throw new NoSuchElementException();
       }
-      possibleCorrection = new ArrayList<SpellCheckCorrection>();
+      possibleCorrection = new ArrayList<>();
       List<List<SpellCheckCorrection>> possibleCorrections = null;
       int rank = 0;
       while (!done
@@ -244,7 +244,7 @@ public class PossibilityIterator implements
         if (suggestionsMayOverlap) {
           possibleCorrections = separateOverlappingTokens(possibleCorrection);
         } else {
-          possibleCorrections = new ArrayList<List<SpellCheckCorrection>>(1);
+          possibleCorrections = new ArrayList<>(1);
           possibleCorrections.add(possibleCorrection);
         }
       }
@@ -258,11 +258,11 @@ public class PossibilityIterator implements
       List<SpellCheckCorrection> possibleCorrection) {
     List<List<SpellCheckCorrection>> ret = null;
     if (possibleCorrection.size() == 1) {
-      ret = new ArrayList<List<SpellCheckCorrection>>(1);
+      ret = new ArrayList<>(1);
       ret.add(possibleCorrection);
       return ret;
     }
-    ret = new ArrayList<List<SpellCheckCorrection>>();
+    ret = new ArrayList<>();
     for (int i = 0; i < possibleCorrection.size(); i++) {
       List<SpellCheckCorrection> c = compatible(possibleCorrection, i);
       ret.add(c);
@@ -274,7 +274,7 @@ public class PossibilityIterator implements
       int pos) {
     List<SpellCheckCorrection> priorPassCompatibles = null;
     {
-      List<SpellCheckCorrection> firstPassCompatibles = new ArrayList<SpellCheckCorrection>(
+      List<SpellCheckCorrection> firstPassCompatibles = new ArrayList<>(
           all.size());
       SpellCheckCorrection sacred = all.get(pos);
       firstPassCompatibles.add(sacred);
@@ -303,7 +303,7 @@ public class PossibilityIterator implements
         if (pos == priorPassCompatibles.size() - 1) {
           return priorPassCompatibles;
         }
-        List<SpellCheckCorrection> subsequentPassCompatibles = new ArrayList<SpellCheckCorrection>(
+        List<SpellCheckCorrection> subsequentPassCompatibles = new ArrayList<>(
             priorPassCompatibles.size());
         SpellCheckCorrection sacred = null;
         for (int i = 0; i <= pos; i++) {
diff --git solr/core/src/java/org/apache/solr/spelling/SolrSpellChecker.java solr/core/src/java/org/apache/solr/spelling/SolrSpellChecker.java
index 38e2543..01f01a3 100644
--- solr/core/src/java/org/apache/solr/spelling/SolrSpellChecker.java
+++ solr/core/src/java/org/apache/solr/spelling/SolrSpellChecker.java
@@ -138,7 +138,7 @@ public abstract class SolrSpellChecker {
         for (SuggestWord word : suggestions)
           result.add(token, word.string, word.freq);
       } else {
-        List<String> words = new ArrayList<String>(sugQueue.size());
+        List<String> words = new ArrayList<>(sugQueue.size());
         for (SuggestWord word : suggestions) words.add(word.string);
         result.add(token, words);
       }
diff --git solr/core/src/java/org/apache/solr/spelling/SpellCheckCollator.java solr/core/src/java/org/apache/solr/spelling/SpellCheckCollator.java
index 16955d6..e422d6c 100644
--- solr/core/src/java/org/apache/solr/spelling/SpellCheckCollator.java
+++ solr/core/src/java/org/apache/solr/spelling/SpellCheckCollator.java
@@ -50,7 +50,7 @@ public class SpellCheckCollator {
 
   public List<SpellCheckCollation> collate(SpellingResult result,
       String originalQuery, ResponseBuilder ultimateResponse) {
-  List<SpellCheckCollation> collations = new ArrayList<SpellCheckCollation>();
+  List<SpellCheckCollation> collations = new ArrayList<>();
 
     QueryComponent queryComponent = null;
     if (ultimateResponse.components != null) {
@@ -169,7 +169,7 @@ public class SpellCheckCollator {
         collation.setHits(hits);
         collation.setInternalRank(suggestionsMayOverlap ? ((possibility.rank * 1000) + possibility.index) : possibility.rank);
 
-        NamedList<String> misspellingsAndCorrections = new NamedList<String>();
+        NamedList<String> misspellingsAndCorrections = new NamedList<>();
         for (SpellCheckCorrection corr : possibility.corrections) {
           misspellingsAndCorrections.add(corr.getOriginal().toString(), corr.getCorrection());
         }
diff --git solr/core/src/java/org/apache/solr/spelling/SpellingQueryConverter.java solr/core/src/java/org/apache/solr/spelling/SpellingQueryConverter.java
index 359cc69..8b19e83 100644
--- solr/core/src/java/org/apache/solr/spelling/SpellingQueryConverter.java
+++ solr/core/src/java/org/apache/solr/spelling/SpellingQueryConverter.java
@@ -104,7 +104,7 @@ public class SpellingQueryConverter extends QueryConverter  {
     if (original == null) { // this can happen with q.alt = and no query
       return Collections.emptyList();
     }
-    Collection<Token> result = new ArrayList<Token>();
+    Collection<Token> result = new ArrayList<>();
     Matcher matcher = QUERY_REGEX.matcher(original);
     String nextWord = null;
     int nextStartIndex = 0;
diff --git solr/core/src/java/org/apache/solr/spelling/SpellingResult.java solr/core/src/java/org/apache/solr/spelling/SpellingResult.java
index 06816ba..f69fb69 100644
--- solr/core/src/java/org/apache/solr/spelling/SpellingResult.java
+++ solr/core/src/java/org/apache/solr/spelling/SpellingResult.java
@@ -37,7 +37,7 @@ public class SpellingResult {
    * Key == token
    * Value = Map  -> key is the suggestion, value is the frequency of the token in the collection
    */
-  private Map<Token, LinkedHashMap<String, Integer>> suggestions = new LinkedHashMap<Token, LinkedHashMap<String, Integer>>();
+  private Map<Token, LinkedHashMap<String, Integer>> suggestions = new LinkedHashMap<>();
   private Map<Token, Integer> tokenFrequency;
   public static final int NO_FREQUENCY_INFO = -1;
 
@@ -58,7 +58,7 @@ public class SpellingResult {
   public void add(Token token, List<String> suggestions) {
     LinkedHashMap<String, Integer> map = this.suggestions.get(token);
     if (map == null ) {
-      map = new LinkedHashMap<String, Integer>();
+      map = new LinkedHashMap<>();
       this.suggestions.put(token, map);
     }
     for (String suggestion : suggestions) {
@@ -74,7 +74,7 @@ public class SpellingResult {
    */
   public void addFrequency(Token token, int docFreq) {
     if (tokenFrequency == null) {
-      tokenFrequency = new LinkedHashMap<Token, Integer>();
+      tokenFrequency = new LinkedHashMap<>();
     }
     tokenFrequency.put(token, docFreq);
   }
@@ -89,7 +89,7 @@ public class SpellingResult {
     LinkedHashMap<String, Integer> map = this.suggestions.get(token);
     //Don't bother adding if we already have this token
     if (map == null) {
-      map = new LinkedHashMap<String, Integer>();
+      map = new LinkedHashMap<>();
       this.suggestions.put(token, map);
     }
     map.put(suggestion, docFreq);
diff --git solr/core/src/java/org/apache/solr/spelling/SuggestQueryConverter.java solr/core/src/java/org/apache/solr/spelling/SuggestQueryConverter.java
index c16f6c6..a1014c0 100644
--- solr/core/src/java/org/apache/solr/spelling/SuggestQueryConverter.java
+++ solr/core/src/java/org/apache/solr/spelling/SuggestQueryConverter.java
@@ -35,7 +35,7 @@ public class SuggestQueryConverter extends SpellingQueryConverter {
       return Collections.emptyList();
     }
 
-    Collection<Token> result = new ArrayList<Token>();
+    Collection<Token> result = new ArrayList<>();
     try {
       analyze(result, original, 0, 0);
     } catch (IOException e) {
diff --git solr/core/src/java/org/apache/solr/spelling/WordBreakSolrSpellChecker.java solr/core/src/java/org/apache/solr/spelling/WordBreakSolrSpellChecker.java
index 243ff4f..ad01005 100644
--- solr/core/src/java/org/apache/solr/spelling/WordBreakSolrSpellChecker.java
+++ solr/core/src/java/org/apache/solr/spelling/WordBreakSolrSpellChecker.java
@@ -199,9 +199,9 @@ public class WordBreakSolrSpellChecker extends SolrSpellChecker {
     
     StringBuilder sb = new StringBuilder();
     Token[] tokenArr = options.tokens.toArray(new Token[options.tokens.size()]);
-    List<Term> termArr = new ArrayList<Term>(options.tokens.size() + 2);
+    List<Term> termArr = new ArrayList<>(options.tokens.size() + 2);
     
-    List<ResultEntry> breakSuggestionList = new ArrayList<ResultEntry>();
+    List<ResultEntry> breakSuggestionList = new ArrayList<>();
     boolean lastOneProhibited = false;
     boolean lastOneRequired = false;
     boolean lastOneprocedesNewBooleanOp = false;
@@ -253,7 +253,7 @@ public class WordBreakSolrSpellChecker extends SolrSpellChecker {
     CombineSuggestion[] combineSuggestions = wbsp.suggestWordCombinations(
         termArr.toArray(new Term[termArr.size()]), numSuggestions, ir, options.suggestMode);
     if (combineWords) {
-      combineSuggestionList = new ArrayList<ResultEntry>(
+      combineSuggestionList = new ArrayList<>(
           combineSuggestions.length);
       for (CombineSuggestion cs : combineSuggestions) {
         int firstTermIndex = cs.originalTermIndexes[0];
diff --git solr/core/src/java/org/apache/solr/spelling/suggest/DocumentExpressionDictionaryFactory.java solr/core/src/java/org/apache/solr/spelling/suggest/DocumentExpressionDictionaryFactory.java
index a511d90..cb44d63 100644
--- solr/core/src/java/org/apache/solr/spelling/suggest/DocumentExpressionDictionaryFactory.java
+++ solr/core/src/java/org/apache/solr/spelling/suggest/DocumentExpressionDictionaryFactory.java
@@ -68,7 +68,7 @@ public class DocumentExpressionDictionaryFactory extends DictionaryFactory {
     String field = (String) params.get(FIELD);
     String payloadField = (String) params.get(PAYLOAD_FIELD);
     String weightExpression = (String) params.get(WEIGHT_EXPRESSION);
-    Set<SortField> sortFields = new HashSet<SortField>();
+    Set<SortField> sortFields = new HashSet<>();
     
     if (field == null) {
       throw new IllegalArgumentException(FIELD + " is a mandatory parameter");
diff --git solr/core/src/java/org/apache/solr/spelling/suggest/SuggesterResult.java solr/core/src/java/org/apache/solr/spelling/suggest/SuggesterResult.java
index 0ab357c..3017349 100644
--- solr/core/src/java/org/apache/solr/spelling/suggest/SuggesterResult.java
+++ solr/core/src/java/org/apache/solr/spelling/suggest/SuggesterResult.java
@@ -35,7 +35,7 @@ public class SuggesterResult {
   
   /** token -> lookup results mapping*/
   private Map<String, Map<String, List<LookupResult>>> suggestionsMap = 
-      new HashMap<String, Map<String, List<LookupResult>>>();
+      new HashMap<>();
 
   /** Add suggestion results for <code>token</code> */
   public void add(String suggesterName, String token, List<LookupResult> results) {
diff --git solr/core/src/java/org/apache/solr/store/blockcache/BlockDirectoryCache.java solr/core/src/java/org/apache/solr/store/blockcache/BlockDirectoryCache.java
index 592831b..b1b827b 100644
--- solr/core/src/java/org/apache/solr/store/blockcache/BlockDirectoryCache.java
+++ solr/core/src/java/org/apache/solr/store/blockcache/BlockDirectoryCache.java
@@ -27,7 +27,7 @@ import java.util.concurrent.atomic.AtomicInteger;
 public class BlockDirectoryCache implements Cache {
   private final BlockCache blockCache;
   private AtomicInteger counter = new AtomicInteger();
-  private Map<String,Integer> names = new ConcurrentHashMap<String,Integer>();
+  private Map<String,Integer> names = new ConcurrentHashMap<>();
   private String path;
   private Metrics metrics;
   
diff --git solr/core/src/java/org/apache/solr/store/blockcache/BufferStore.java solr/core/src/java/org/apache/solr/store/blockcache/BufferStore.java
index f54b275..ad10357 100644
--- solr/core/src/java/org/apache/solr/store/blockcache/BufferStore.java
+++ solr/core/src/java/org/apache/solr/store/blockcache/BufferStore.java
@@ -39,7 +39,7 @@ public class BufferStore implements Store {
     }
   };
 
-  private final static ConcurrentMap<Integer, BufferStore> bufferStores = new ConcurrentHashMap<Integer, BufferStore>();
+  private final static ConcurrentMap<Integer, BufferStore> bufferStores = new ConcurrentHashMap<>();
 
   private final BlockingQueue<byte[]> buffers;
 
@@ -66,7 +66,7 @@ public class BufferStore implements Store {
   }
 
   private static BlockingQueue<byte[]> setupBuffers(int bufferSize, int count) {
-    BlockingQueue<byte[]> queue = new ArrayBlockingQueue<byte[]>(count);
+    BlockingQueue<byte[]> queue = new ArrayBlockingQueue<>(count);
     for (int i = 0; i < count; i++) {
       queue.add(new byte[bufferSize]);
     }
diff --git solr/core/src/java/org/apache/solr/store/blockcache/Metrics.java solr/core/src/java/org/apache/solr/store/blockcache/Metrics.java
index 052e704..04e9867 100644
--- solr/core/src/java/org/apache/solr/store/blockcache/Metrics.java
+++ solr/core/src/java/org/apache/solr/store/blockcache/Metrics.java
@@ -53,7 +53,7 @@ public class Metrics implements Updater {
   public AtomicLong shardBuffercacheAllocate8192 = new AtomicLong(0);
   public AtomicLong shardBuffercacheAllocateOther = new AtomicLong(0);
   public AtomicLong shardBuffercacheLost = new AtomicLong(0);
-  public Map<String,MethodCall> methodCalls = new ConcurrentHashMap<String, MethodCall>();
+  public Map<String,MethodCall> methodCalls = new ConcurrentHashMap<>();
   
   public AtomicLong tableCount = new AtomicLong(0);
   public AtomicLong rowCount = new AtomicLong(0);
diff --git solr/core/src/java/org/apache/solr/store/hdfs/HdfsDirectory.java solr/core/src/java/org/apache/solr/store/hdfs/HdfsDirectory.java
index 3e5d771..468f56c 100644
--- solr/core/src/java/org/apache/solr/store/hdfs/HdfsDirectory.java
+++ solr/core/src/java/org/apache/solr/store/hdfs/HdfsDirectory.java
@@ -163,7 +163,7 @@ public class HdfsDirectory extends BaseDirectory {
   @Override
   public String[] listAll() throws IOException {
     FileStatus[] listStatus = getFileSystem().listStatus(hdfsDirPath);
-    List<String> files = new ArrayList<String>();
+    List<String> files = new ArrayList<>();
     if (listStatus == null) {
       return new String[] {};
     }
diff --git solr/core/src/java/org/apache/solr/update/AddUpdateCommand.java solr/core/src/java/org/apache/solr/update/AddUpdateCommand.java
index 6def045..607ed34 100644
--- solr/core/src/java/org/apache/solr/update/AddUpdateCommand.java
+++ solr/core/src/java/org/apache/solr/update/AddUpdateCommand.java
@@ -193,7 +193,7 @@ public class AddUpdateCommand extends UpdateCommand implements Iterable<IndexDoc
   }
 
   private List<SolrInputDocument> flatten(SolrInputDocument root) {
-    List<SolrInputDocument> unwrappedDocs = new ArrayList<SolrInputDocument>();
+    List<SolrInputDocument> unwrappedDocs = new ArrayList<>();
     recUnwrapp(unwrappedDocs, root);
     return unwrappedDocs;
   }
diff --git solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java
index 864dc83..ec03bca 100644
--- solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java
+++ solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java
@@ -201,7 +201,7 @@ public class DirectUpdateHandler2 extends UpdateHandler implements SolrCoreState
           if (deletesAfter != null) {
             log.info("Reordered DBQs detected.  Update=" + cmd + " DBQs="
                 + deletesAfter);
-            List<Query> dbqList = new ArrayList<Query>(deletesAfter.size());
+            List<Query> dbqList = new ArrayList<>(deletesAfter.size());
             for (UpdateLog.DBQ dbq : deletesAfter) {
               try {
                 DeleteUpdateCommand tmpDel = new DeleteUpdateCommand(cmd.req);
@@ -493,7 +493,7 @@ public class DirectUpdateHandler2 extends UpdateHandler implements SolrCoreState
       log.info("start "+cmd);
       RefCounted<IndexWriter> iw = solrCoreState.getIndexWriter(core);
       try {
-        final Map<String,String> commitData = new HashMap<String,String>();
+        final Map<String,String> commitData = new HashMap<>();
         commitData.put(SolrIndexWriter.COMMIT_TIME_MSEC_KEY,
             String.valueOf(System.currentTimeMillis()));
         iw.get().setCommitData(commitData);
@@ -571,7 +571,7 @@ public class DirectUpdateHandler2 extends UpdateHandler implements SolrCoreState
           // SolrCore.verbose("writer.commit() start writer=",writer);
 
           if (writer.hasUncommittedChanges()) {
-            final Map<String,String> commitData = new HashMap<String,String>();
+            final Map<String,String> commitData = new HashMap<>();
             commitData.put(SolrIndexWriter.COMMIT_TIME_MSEC_KEY,
                 String.valueOf(System.currentTimeMillis()));
             writer.setCommitData(commitData);
@@ -759,7 +759,7 @@ public class DirectUpdateHandler2 extends UpdateHandler implements SolrCoreState
           }
 
           // todo: refactor this shared code (or figure out why a real CommitUpdateCommand can't be used)
-          final Map<String,String> commitData = new HashMap<String,String>();
+          final Map<String,String> commitData = new HashMap<>();
           commitData.put(SolrIndexWriter.COMMIT_TIME_MSEC_KEY, String.valueOf(System.currentTimeMillis()));
           writer.setCommitData(commitData);
           writer.commit();
diff --git solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.java solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.java
index 935774d..4b0a7ce 100644
--- solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.java
+++ solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.java
@@ -191,7 +191,7 @@ public class HdfsTransactionLog extends TransactionLog {
 
     synchronized (this) {
       globalStringList = (List<String>)header.get("strings");
-      globalStringMap = new HashMap<String, Integer>(globalStringList.size());
+      globalStringMap = new HashMap<>(globalStringList.size());
       for (int i=0; i<globalStringList.size(); i++) {
         globalStringMap.put( globalStringList.get(i), i+1);
       }
diff --git solr/core/src/java/org/apache/solr/update/HdfsUpdateLog.java solr/core/src/java/org/apache/solr/update/HdfsUpdateLog.java
index 6db18c4..433064c 100644
--- solr/core/src/java/org/apache/solr/update/HdfsUpdateLog.java
+++ solr/core/src/java/org/apache/solr/update/HdfsUpdateLog.java
@@ -329,7 +329,7 @@ public class HdfsUpdateLog extends UpdateLog {
         return name.getName().startsWith(prefix);
       }
     });
-    List<String> fileList = new ArrayList<String>(files.length);
+    List<String> fileList = new ArrayList<>(files.length);
     for (FileStatus file : files) {
       fileList.add(file.getPath().getName());
     }
diff --git solr/core/src/java/org/apache/solr/update/MemOutputStream.java solr/core/src/java/org/apache/solr/update/MemOutputStream.java
index 32b459e..fbc3d45 100644
--- solr/core/src/java/org/apache/solr/update/MemOutputStream.java
+++ solr/core/src/java/org/apache/solr/update/MemOutputStream.java
@@ -25,7 +25,7 @@ import java.util.List;
 
 /** @lucene.internal */
 public class MemOutputStream extends FastOutputStream {
-  public List<byte[]> buffers = new LinkedList<byte[]>();
+  public List<byte[]> buffers = new LinkedList<>();
   public MemOutputStream(byte[] tempBuffer) {
     super(null, tempBuffer, 0);
   }
diff --git solr/core/src/java/org/apache/solr/update/PeerSync.java solr/core/src/java/org/apache/solr/update/PeerSync.java
index f94a4a3..4f7b5d2 100644
--- solr/core/src/java/org/apache/solr/update/PeerSync.java
+++ solr/core/src/java/org/apache/solr/update/PeerSync.java
@@ -227,7 +227,7 @@ public class PeerSync  {
       }
 
       // let's merge the lists
-      List<Long> newList = new ArrayList<Long>(ourUpdates);
+      List<Long> newList = new ArrayList<>(ourUpdates);
       for (Long ver : startingVersions) {
         if (Math.abs(ver) < smallestNewUpdate) {
           newList.add(ver);
@@ -248,8 +248,8 @@ public class PeerSync  {
       }
     }
 
-    ourUpdateSet = new HashSet<Long>(ourUpdates);
-    requestedUpdateSet = new HashSet<Long>(ourUpdates);
+    ourUpdateSet = new HashSet<>(ourUpdates);
+    requestedUpdateSet = new HashSet<>(ourUpdates);
 
     for(;;) {
       ShardResponse srsp = shardHandler.takeCompletedOrError();
@@ -390,7 +390,7 @@ public class PeerSync  {
       return true;
     }
     
-    List<Long> toRequest = new ArrayList<Long>();
+    List<Long> toRequest = new ArrayList<>();
     for (Long otherVersion : otherVersions) {
       // stop when the entries get old enough that reorders may lead us to see updates we don't need
       if (!completeList && Math.abs(otherVersion) < ourLowThreshold) break;
diff --git solr/core/src/java/org/apache/solr/update/SolrCmdDistributor.java solr/core/src/java/org/apache/solr/update/SolrCmdDistributor.java
index 88d6e19..5243539 100644
--- solr/core/src/java/org/apache/solr/update/SolrCmdDistributor.java
+++ solr/core/src/java/org/apache/solr/update/SolrCmdDistributor.java
@@ -47,8 +47,8 @@ public class SolrCmdDistributor {
   private int retryPause = 500;
   private int maxRetriesOnForward = MAX_RETRIES_ON_FORWARD;
   
-  private List<Error> allErrors = new ArrayList<Error>();
-  private List<Error> errors = new ArrayList<Error>();
+  private List<Error> allErrors = new ArrayList<>();
+  private List<Error> errors = new ArrayList<>();
   
   public static interface AbortCheck {
     public boolean abortCheck();
@@ -76,9 +76,9 @@ public class SolrCmdDistributor {
   private void doRetriesIfNeeded() {
     // NOTE: retries will be forwards to a single url
     
-    List<Error> errors = new ArrayList<Error>(this.errors);
+    List<Error> errors = new ArrayList<>(this.errors);
     errors.addAll(servers.getErrors());
-    List<Error> resubmitList = new ArrayList<Error>();
+    List<Error> resubmitList = new ArrayList<>();
 
     for (Error err : errors) {
       try {
@@ -266,7 +266,7 @@ public class SolrCmdDistributor {
 
   
   public static class Response {
-    public List<Error> errors = new ArrayList<Error>();
+    public List<Error> errors = new ArrayList<>();
   }
   
   public static class Error {
diff --git solr/core/src/java/org/apache/solr/update/SolrIndexSplitter.java solr/core/src/java/org/apache/solr/update/SolrIndexSplitter.java
index 50ce0da..543e956 100644
--- solr/core/src/java/org/apache/solr/update/SolrIndexSplitter.java
+++ solr/core/src/java/org/apache/solr/update/SolrIndexSplitter.java
@@ -90,7 +90,7 @@ public class SolrIndexSplitter {
   public void split() throws IOException {
 
     List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();
-    List<FixedBitSet[]> segmentDocSets = new ArrayList<FixedBitSet[]>(leaves.size());
+    List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());
 
     log.info("SolrIndexSplitter: partitions=" + numPieces + " segments="+leaves.size());
 
diff --git solr/core/src/java/org/apache/solr/update/StreamingSolrServers.java solr/core/src/java/org/apache/solr/update/StreamingSolrServers.java
index 4bd8e1a..261dd42 100644
--- solr/core/src/java/org/apache/solr/update/StreamingSolrServers.java
+++ solr/core/src/java/org/apache/solr/update/StreamingSolrServers.java
@@ -43,7 +43,7 @@ public class StreamingSolrServers {
   
   private HttpClient httpClient;
   
-  private Map<String,ConcurrentUpdateSolrServer> solrServers = new HashMap<String,ConcurrentUpdateSolrServer>();
+  private Map<String,ConcurrentUpdateSolrServer> solrServers = new HashMap<>();
   private List<Error> errors = Collections.synchronizedList(new ArrayList<Error>());
 
   private ExecutorService updateExecutor;
@@ -82,7 +82,7 @@ public class StreamingSolrServers {
       server.setParser(new BinaryResponseParser());
       server.setRequestWriter(new BinaryRequestWriter());
       server.setPollQueueTime(0);
-      Set<String> queryParams = new HashSet<String>(2);
+      Set<String> queryParams = new HashSet<>(2);
       queryParams.add(DistributedUpdateProcessor.DISTRIB_FROM);
       queryParams.add(DistributingUpdateProcessorFactory.DISTRIB_UPDATE_PARAM);
       server.setQueryParams(queryParams);
diff --git solr/core/src/java/org/apache/solr/update/TransactionLog.java solr/core/src/java/org/apache/solr/update/TransactionLog.java
index a9cefc6..08ba6c4 100644
--- solr/core/src/java/org/apache/solr/update/TransactionLog.java
+++ solr/core/src/java/org/apache/solr/update/TransactionLog.java
@@ -77,8 +77,8 @@ public class TransactionLog {
   protected volatile boolean deleteOnClose = true;  // we can delete old tlogs since they are currently only used for real-time-get (and in the future, recovery)
 
   AtomicInteger refcount = new AtomicInteger(1);
-  Map<String,Integer> globalStringMap = new HashMap<String, Integer>();
-  List<String> globalStringList = new ArrayList<String>();
+  Map<String,Integer> globalStringMap = new HashMap<>();
+  List<String> globalStringList = new ArrayList<>();
 
   long snapshot_size;
   int snapshot_numRecords;
@@ -271,7 +271,7 @@ public class TransactionLog {
 
     synchronized (this) {
       globalStringList = (List<String>)header.get("strings");
-      globalStringMap = new HashMap<String, Integer>(globalStringList.size());
+      globalStringMap = new HashMap<>(globalStringList.size());
       for (int i=0; i<globalStringList.size(); i++) {
         globalStringMap.put( globalStringList.get(i), i+1);
       }
@@ -295,7 +295,7 @@ public class TransactionLog {
 
   Collection<String> getGlobalStrings() {
     synchronized (this) {
-      return new ArrayList<String>(globalStringList);
+      return new ArrayList<>(globalStringList);
     }
   }
 
diff --git solr/core/src/java/org/apache/solr/update/UpdateHandler.java solr/core/src/java/org/apache/solr/update/UpdateHandler.java
index 03c5a48..b9af14a 100644
--- solr/core/src/java/org/apache/solr/update/UpdateHandler.java
+++ solr/core/src/java/org/apache/solr/update/UpdateHandler.java
@@ -49,9 +49,9 @@ public abstract class UpdateHandler implements SolrInfoMBean {
   protected final SchemaField idField;
   protected final FieldType idFieldType;
 
-  protected Vector<SolrEventListener> commitCallbacks = new Vector<SolrEventListener>();
-  protected Vector<SolrEventListener> softCommitCallbacks = new Vector<SolrEventListener>();
-  protected Vector<SolrEventListener> optimizeCallbacks = new Vector<SolrEventListener>();
+  protected Vector<SolrEventListener> commitCallbacks = new Vector<>();
+  protected Vector<SolrEventListener> softCommitCallbacks = new Vector<>();
+  protected Vector<SolrEventListener> optimizeCallbacks = new Vector<>();
 
   protected final UpdateLog ulog;
 
diff --git solr/core/src/java/org/apache/solr/update/UpdateLog.java solr/core/src/java/org/apache/solr/update/UpdateLog.java
index 7353bfa..a65012c 100644
--- solr/core/src/java/org/apache/solr/update/UpdateLog.java
+++ solr/core/src/java/org/apache/solr/update/UpdateLog.java
@@ -128,11 +128,11 @@ public class UpdateLog implements PluginInfoInitialized {
 
   protected TransactionLog tlog;
   protected TransactionLog prevTlog;
-  protected Deque<TransactionLog> logs = new LinkedList<TransactionLog>();  // list of recent logs, newest first
-  protected LinkedList<TransactionLog> newestLogsOnStartup = new LinkedList<TransactionLog>();
+  protected Deque<TransactionLog> logs = new LinkedList<>();  // list of recent logs, newest first
+  protected LinkedList<TransactionLog> newestLogsOnStartup = new LinkedList<>();
   protected int numOldRecords;  // number of records in the recent logs
 
-  protected Map<BytesRef,LogPtr> map = new HashMap<BytesRef, LogPtr>();
+  protected Map<BytesRef,LogPtr> map = new HashMap<>();
   protected Map<BytesRef,LogPtr> prevMap;  // used while committing/reopening is happening
   protected Map<BytesRef,LogPtr> prevMap2;  // used while committing/reopening is happening
   protected TransactionLog prevMapLog;  // the transaction log used to look up entries found in prevMap
@@ -160,7 +160,7 @@ public class UpdateLog implements PluginInfoInitialized {
     }
   }
 
-  protected LinkedList<DBQ> deleteByQueries = new LinkedList<DBQ>();
+  protected LinkedList<DBQ> deleteByQueries = new LinkedList<>();
 
   protected String[] tlogFiles;
   protected File tlogDir;
@@ -566,7 +566,7 @@ public class UpdateLog implements PluginInfoInitialized {
         return null;
       }
 
-      List<DBQ> dbqList = new ArrayList<DBQ>();
+      List<DBQ> dbqList = new ArrayList<>();
       for (DBQ dbq : deleteByQueries) {
         if (dbq.version <= version) break;
         dbqList.add(dbq);
@@ -582,7 +582,7 @@ public class UpdateLog implements PluginInfoInitialized {
     prevMap = map;
     prevMapLog = tlog;
 
-    map = new HashMap<BytesRef, LogPtr>();
+    map = new HashMap<>();
   }
 
   private void clearOldMaps() {
@@ -656,7 +656,7 @@ public class UpdateLog implements PluginInfoInitialized {
       // any added documents will make it into this commit or not.
       // But we do know that any updates already added will definitely
       // show up in the latest reader after the commit succeeds.
-      map = new HashMap<BytesRef, LogPtr>();
+      map = new HashMap<>();
 
       if (debug) {
         log.debug("TLOG: preSoftCommit: prevMap="+ System.identityHashCode(prevMap) + " new map=" + System.identityHashCode(map));
@@ -792,7 +792,7 @@ public class UpdateLog implements PluginInfoInitialized {
   public Future<RecoveryInfo> recoverFromLog() {
     recoveryInfo = new RecoveryInfo();
 
-    List<TransactionLog> recoverLogs = new ArrayList<TransactionLog>(1);
+    List<TransactionLog> recoverLogs = new ArrayList<>(1);
     for (TransactionLog ll : newestLogsOnStartup) {
       if (!ll.try_incref()) continue;
 
@@ -812,7 +812,7 @@ public class UpdateLog implements PluginInfoInitialized {
 
     if (recoverLogs.isEmpty()) return null;
 
-    ExecutorCompletionService<RecoveryInfo> cs = new ExecutorCompletionService<RecoveryInfo>(recoveryExecutor);
+    ExecutorCompletionService<RecoveryInfo> cs = new ExecutorCompletionService<>(recoveryExecutor);
     LogReplayer replayer = new LogReplayer(recoverLogs, false);
 
     versionInfo.blockUpdates();
@@ -925,7 +925,7 @@ public class UpdateLog implements PluginInfoInitialized {
 
     /** Returns the list of deleteByQueries that happened after the given version */
     public List<Object> getDeleteByQuery(long afterVersion) {
-      List<Object> result = new ArrayList<Object>(deleteByQueryList.size());
+      List<Object> result = new ArrayList<>(deleteByQueryList.size());
       for (Update update : deleteByQueryList) {
         if (Math.abs(update.version) > afterVersion) {
           Object dbq = update.log.lookup(update.pointer);
@@ -942,13 +942,13 @@ public class UpdateLog implements PluginInfoInitialized {
 
     private void update() {
       int numUpdates = 0;
-      updateList = new ArrayList<List<Update>>(logList.size());
-      deleteByQueryList = new ArrayList<Update>();
-      deleteList = new ArrayList<DeleteUpdate>();
-      updates = new HashMap<Long,Update>(numRecordsToKeep);
+      updateList = new ArrayList<>(logList.size());
+      deleteByQueryList = new ArrayList<>();
+      deleteList = new ArrayList<>();
+      updates = new HashMap<>(numRecordsToKeep);
 
       for (TransactionLog oldLog : logList) {
-        List<Update> updatesForLog = new ArrayList<Update>();
+        List<Update> updatesForLog = new ArrayList<>();
 
         TransactionLog.ReverseReader reader = null;
         try {
@@ -1030,7 +1030,7 @@ public class UpdateLog implements PluginInfoInitialized {
   public RecentUpdates getRecentUpdates() {
     Deque<TransactionLog> logList;
     synchronized (this) {
-      logList = new LinkedList<TransactionLog>(logs);
+      logList = new LinkedList<>(logs);
       for (TransactionLog log : logList) {
         log.incref();
       }
@@ -1156,7 +1156,7 @@ public class UpdateLog implements PluginInfoInitialized {
       tlog.decref();
       throw new RuntimeException("executor is not running...");
     }
-    ExecutorCompletionService<RecoveryInfo> cs = new ExecutorCompletionService<RecoveryInfo>(recoveryExecutor);
+    ExecutorCompletionService<RecoveryInfo> cs = new ExecutorCompletionService<>(recoveryExecutor);
     LogReplayer replayer = new LogReplayer(Arrays.asList(new TransactionLog[]{tlog}), true);
     return cs.submit(replayer, recoveryInfo);
   }
@@ -1188,7 +1188,7 @@ public class UpdateLog implements PluginInfoInitialized {
     boolean debug = loglog.isDebugEnabled();
 
     public LogReplayer(List<TransactionLog> translogs, boolean activeLog) {
-      this.translogs = new LinkedList<TransactionLog>();
+      this.translogs = new LinkedList<>();
       this.translogs.addAll(translogs);
       this.activeLog = activeLog;
     }
diff --git solr/core/src/java/org/apache/solr/update/processor/AddSchemaFieldsUpdateProcessorFactory.java solr/core/src/java/org/apache/solr/update/processor/AddSchemaFieldsUpdateProcessorFactory.java
index d901249..9d49cd8 100644
--- solr/core/src/java/org/apache/solr/update/processor/AddSchemaFieldsUpdateProcessorFactory.java
+++ solr/core/src/java/org/apache/solr/update/processor/AddSchemaFieldsUpdateProcessorFactory.java
@@ -126,7 +126,7 @@ public class AddSchemaFieldsUpdateProcessorFactory extends UpdateRequestProcesso
   
   private List<TypeMapping> typeMappings = Collections.emptyList();
   private SelectorParams inclusions = new SelectorParams();
-  private Collection<SelectorParams> exclusions = new ArrayList<SelectorParams>();
+  private Collection<SelectorParams> exclusions = new ArrayList<>();
   private FieldNameSelector selector = null;
   private String defaultFieldType;
 
@@ -191,7 +191,7 @@ public class AddSchemaFieldsUpdateProcessorFactory extends UpdateRequestProcesso
   }
 
   private static List<TypeMapping> parseTypeMappings(NamedList args) {
-    List<TypeMapping> typeMappings = new ArrayList<TypeMapping>();
+    List<TypeMapping> typeMappings = new ArrayList<>();
     List<Object> typeMappingsParams = args.getAll(TYPE_MAPPING_PARAM);
     for (Object typeMappingObj : typeMappingsParams) {
       if (null == typeMappingObj) {
@@ -262,7 +262,7 @@ public class AddSchemaFieldsUpdateProcessorFactory extends UpdateRequestProcesso
       if (null == schema.getFieldTypeByName(fieldTypeName)) {
         throw new SolrException(SERVER_ERROR, "fieldType '" + fieldTypeName + "' not found in the schema");
       }
-      valueClasses = new HashSet<Class<?>>();
+      valueClasses = new HashSet<>();
       for (String valueClassName : valueClassNames) {
         try {
           valueClasses.add(loader.loadClass(valueClassName));
@@ -289,7 +289,7 @@ public class AddSchemaFieldsUpdateProcessorFactory extends UpdateRequestProcesso
       final SolrCore core = cmd.getReq().getCore();
       for (;;) {
         final IndexSchema oldSchema = core.getLatestSchema();
-        List<SchemaField> newFields = new ArrayList<SchemaField>();
+        List<SchemaField> newFields = new ArrayList<>();
         for (final String fieldName : doc.getFieldNames()) {
           if (selector.shouldMutate(fieldName)) {
             String fieldTypeName = mapValueClassesToFieldType(doc.getField(fieldName));
diff --git solr/core/src/java/org/apache/solr/update/processor/AllValuesOrNoneFieldMutatingUpdateProcessor.java solr/core/src/java/org/apache/solr/update/processor/AllValuesOrNoneFieldMutatingUpdateProcessor.java
index 58969b0..a548244 100644
--- solr/core/src/java/org/apache/solr/update/processor/AllValuesOrNoneFieldMutatingUpdateProcessor.java
+++ solr/core/src/java/org/apache/solr/update/processor/AllValuesOrNoneFieldMutatingUpdateProcessor.java
@@ -87,7 +87,7 @@ public abstract class AllValuesOrNoneFieldMutatingUpdateProcessor extends FieldM
       if (DELETE_VALUE_SINGLETON == destVal) {
         if (log.isDebugEnabled()) {
           if (null == messages) {
-            messages = new ArrayList<String>();
+            messages = new ArrayList<>();
           }
           messages.add(String.format(Locale.ROOT, "removing value from field '%s': %s '%s'", 
                                      srcField.getName(), srcVal.getClass().getSimpleName(), srcVal));
@@ -95,7 +95,7 @@ public abstract class AllValuesOrNoneFieldMutatingUpdateProcessor extends FieldM
       } else {
         if (log.isDebugEnabled()) {
           if (null == messages) {
-            messages = new ArrayList<String>();
+            messages = new ArrayList<>();
           }
           messages.add(String.format(Locale.ROOT, "replace value from field '%s': %s '%s' with %s '%s'", 
                                      srcField.getName(), srcVal.getClass().getSimpleName(), srcVal, 
diff --git solr/core/src/java/org/apache/solr/update/processor/CloneFieldUpdateProcessorFactory.java solr/core/src/java/org/apache/solr/update/processor/CloneFieldUpdateProcessorFactory.java
index 2f77947..05250e0 100644
--- solr/core/src/java/org/apache/solr/update/processor/CloneFieldUpdateProcessorFactory.java
+++ solr/core/src/java/org/apache/solr/update/processor/CloneFieldUpdateProcessorFactory.java
@@ -108,7 +108,7 @@ public class CloneFieldUpdateProcessorFactory
   
   private SelectorParams srcInclusions = new SelectorParams();
   private Collection<SelectorParams> srcExclusions 
-    = new ArrayList<SelectorParams>();
+    = new ArrayList<>();
 
   private FieldNameSelector srcSelector = null;
   private String dest = null;
diff --git solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor.java solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor.java
index 765e892..37c935e 100644
--- solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor.java
+++ solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor.java
@@ -283,13 +283,13 @@ public class DistributedUpdateProcessor extends UpdateRequestProcessor {
 
           if (replicaProps != null) {
             if (nodes == null)  {
-            nodes = new ArrayList<Node>(replicaProps.size());
+            nodes = new ArrayList<>(replicaProps.size());
             }
             // check for test param that lets us miss replicas
             String[] skipList = req.getParams().getParams(TEST_DISTRIB_SKIP_SERVERS);
             Set<String> skipListSet = null;
             if (skipList != null) {
-              skipListSet = new HashSet<String>(skipList.length);
+              skipListSet = new HashSet<>(skipList.length);
               skipListSet.addAll(Arrays.asList(skipList));
               log.info("test.distrib.skip.servers was found and contains:" + skipListSet);
             }
@@ -309,7 +309,7 @@ public class DistributedUpdateProcessor extends UpdateRequestProcessor {
 
         } else {
           // I need to forward onto the leader...
-          nodes = new ArrayList<Node>(1);
+          nodes = new ArrayList<>(1);
           nodes.add(new RetryNode(new ZkCoreNodeProps(leaderReplica), zkController.getZkStateReader(), collection, shardId));
           forwardToLeader = true;
         }
@@ -373,7 +373,7 @@ public class DistributedUpdateProcessor extends UpdateRequestProcessor {
           Replica sliceLeader = aslice.getLeader();
           // slice leader can be null because node/shard is created zk before leader election
           if (sliceLeader != null && zkController.getClusterState().liveNodesContain(sliceLeader.getNodeName()))  {
-            if (nodes == null) nodes = new ArrayList<Node>();
+            if (nodes == null) nodes = new ArrayList<>();
             ZkCoreNodeProps nodeProps = new ZkCoreNodeProps(sliceLeader);
             nodes.add(new StdNode(nodeProps));
           }
@@ -400,7 +400,7 @@ public class DistributedUpdateProcessor extends UpdateRequestProcessor {
             Collection<Slice> activeSlices = cstate.getActiveSlices(targetCollectionName);
             if (activeSlices != null && !activeSlices.isEmpty()) {
               Slice any = activeSlices.iterator().next();
-              if (nodes == null) nodes = new ArrayList<Node>();
+              if (nodes == null) nodes = new ArrayList<>();
               nodes.add(new StdNode(new ZkCoreNodeProps(any.getLeader())));
             }
           }
@@ -418,7 +418,7 @@ public class DistributedUpdateProcessor extends UpdateRequestProcessor {
                 int hash = compositeIdRouter.sliceHash(id, doc, null, coll);
                 for (DocRouter.Range range : ranges) {
                   if (range.includes(hash)) {
-                    if (nodes == null) nodes = new ArrayList<Node>();
+                    if (nodes == null) nodes = new ArrayList<>();
                     DocCollection targetColl = cstate.getCollection(rule.getTargetCollectionName());
                     Collection<Slice> activeSlices = targetColl.getRouter().getSearchSlicesSingle(id, null, targetColl);
                     if (activeSlices == null || activeSlices.isEmpty()) {
@@ -526,7 +526,7 @@ public class DistributedUpdateProcessor extends UpdateRequestProcessor {
           .getReplicaProps(collection, shardId, leaderReplica.getName(),
               req.getCore().getName());
       if (replicaProps != null) {
-        nodes = new ArrayList<Node>(replicaProps.size());
+        nodes = new ArrayList<>(replicaProps.size());
         for (ZkCoreNodeProps props : replicaProps) {
           nodes.add(new StdNode(props));
         }
@@ -1100,7 +1100,7 @@ public class DistributedUpdateProcessor extends UpdateRequestProcessor {
       if(route == null) route = params.get(ShardParams.SHARD_KEYS);// deprecated . kept for backcompat
       Collection<Slice> slices = coll.getRouter().getSearchSlices(route, params, coll);
 
-      List<Node> leaders =  new ArrayList<Node>(slices.size());
+      List<Node> leaders =  new ArrayList<>(slices.size());
       for (Slice slice : slices) {
         String sliceName = slice.getName();
         Replica leader;
@@ -1224,7 +1224,7 @@ public class DistributedUpdateProcessor extends UpdateRequestProcessor {
               .getReplicaProps(collection, myShardId, leaderReplica.getName(),
                   req.getCore().getName(), null, ZkStateReader.DOWN);
           if (replicaProps != null) {
-            List<Node> myReplicas = new ArrayList<Node>();
+            List<Node> myReplicas = new ArrayList<>();
             for (ZkCoreNodeProps replicaProp : replicaProps) {
               myReplicas.add(new StdNode(replicaProp));
             }
@@ -1479,7 +1479,7 @@ public class DistributedUpdateProcessor extends UpdateRequestProcessor {
   private List<Node> getCollectionUrls(SolrQueryRequest req, String collection) {
     ClusterState clusterState = req.getCore().getCoreDescriptor()
         .getCoreContainer().getZkController().getClusterState();
-    List<Node> urls = new ArrayList<Node>();
+    List<Node> urls = new ArrayList<>();
     Map<String,Slice> slices = clusterState.getSlicesMap(collection);
     if (slices == null) {
       throw new ZooKeeperException(ErrorCode.BAD_REQUEST,
diff --git solr/core/src/java/org/apache/solr/update/processor/FieldMutatingUpdateProcessor.java solr/core/src/java/org/apache/solr/update/processor/FieldMutatingUpdateProcessor.java
index 92fc82b..042d387 100644
--- solr/core/src/java/org/apache/solr/update/processor/FieldMutatingUpdateProcessor.java
+++ solr/core/src/java/org/apache/solr/update/processor/FieldMutatingUpdateProcessor.java
@@ -84,7 +84,7 @@ public abstract class FieldMutatingUpdateProcessor
 
     // make a copy we can iterate over while mutating the doc
     final Collection<String> fieldNames 
-      = new ArrayList<String>(doc.getFieldNames());
+      = new ArrayList<>(doc.getFieldNames());
 
     for (final String fname : fieldNames) {
 
@@ -217,7 +217,7 @@ public abstract class FieldMutatingUpdateProcessor
       this.core = core;
       this.params = params;
 
-      final Collection<Class> classes = new ArrayList<Class>(params.typeClass.size());
+      final Collection<Class> classes = new ArrayList<>(params.typeClass.size());
 
       for (String t : params.typeClass) {
         try {
diff --git solr/core/src/java/org/apache/solr/update/processor/FieldMutatingUpdateProcessorFactory.java solr/core/src/java/org/apache/solr/update/processor/FieldMutatingUpdateProcessorFactory.java
index 9375885..9683cfc 100644
--- solr/core/src/java/org/apache/solr/update/processor/FieldMutatingUpdateProcessorFactory.java
+++ solr/core/src/java/org/apache/solr/update/processor/FieldMutatingUpdateProcessorFactory.java
@@ -125,7 +125,7 @@ public abstract class FieldMutatingUpdateProcessorFactory
 
   private SelectorParams inclusions = new SelectorParams();
   private Collection<SelectorParams> exclusions 
-    = new ArrayList<SelectorParams>();
+    = new ArrayList<>();
 
   private FieldMutatingUpdateProcessor.FieldNameSelector selector = null;
   
@@ -145,7 +145,7 @@ public abstract class FieldMutatingUpdateProcessorFactory
     // we can compile the patterns now
     Collection<String> patterns = args.removeConfigArgs("fieldRegex");
     if (! patterns.isEmpty()) {
-      params.fieldRegex = new ArrayList<Pattern>(patterns.size());
+      params.fieldRegex = new ArrayList<>(patterns.size());
       for (String s : patterns) {
         try {
           params.fieldRegex.add(Pattern.compile(s));
@@ -167,7 +167,7 @@ public abstract class FieldMutatingUpdateProcessorFactory
   }
                                
   public static Collection<SelectorParams> parseSelectorExclusionParams(NamedList args) {
-    Collection<SelectorParams> exclusions = new ArrayList<SelectorParams>();
+    Collection<SelectorParams> exclusions = new ArrayList<>();
     List<Object> excList = args.getAll("exclude");
     for (Object excObj : excList) {
       if (null == excObj) {
diff --git solr/core/src/java/org/apache/solr/update/processor/LogUpdateProcessorFactory.java solr/core/src/java/org/apache/solr/update/processor/LogUpdateProcessorFactory.java
index c9d9737..38e70fe 100644
--- solr/core/src/java/org/apache/solr/update/processor/LogUpdateProcessorFactory.java
+++ solr/core/src/java/org/apache/solr/update/processor/LogUpdateProcessorFactory.java
@@ -89,7 +89,7 @@ class LogUpdateProcessor extends UpdateRequestProcessor {
     // TODO: make log level configurable as well, or is that overkill?
     // (ryan) maybe?  I added it mostly to show that it *can* be configurable
 
-    this.toLog = new SimpleOrderedMap<Object>();
+    this.toLog = new SimpleOrderedMap<>();
   }
   
   @Override
@@ -101,7 +101,7 @@ class LogUpdateProcessor extends UpdateRequestProcessor {
 
     // Add a list of added id's to the response
     if (adds == null) {
-      adds = new ArrayList<String>();
+      adds = new ArrayList<>();
       toLog.add("add",adds);
     }
 
@@ -122,7 +122,7 @@ class LogUpdateProcessor extends UpdateRequestProcessor {
 
     if (cmd.isDeleteById()) {
       if (deletes == null) {
-        deletes = new ArrayList<String>();
+        deletes = new ArrayList<>();
         toLog.add("delete",deletes);
       }
       if (deletes.size() < maxNumToLog) {
diff --git solr/core/src/java/org/apache/solr/update/processor/ParseBooleanFieldUpdateProcessorFactory.java solr/core/src/java/org/apache/solr/update/processor/ParseBooleanFieldUpdateProcessorFactory.java
index 7be8e7f..e0d2444 100644
--- solr/core/src/java/org/apache/solr/update/processor/ParseBooleanFieldUpdateProcessorFactory.java
+++ solr/core/src/java/org/apache/solr/update/processor/ParseBooleanFieldUpdateProcessorFactory.java
@@ -72,8 +72,8 @@ public class ParseBooleanFieldUpdateProcessorFactory extends FieldMutatingUpdate
   private static final String FALSE_VALUES_PARAM = "falseValue";
   private static final String CASE_SENSITIVE_PARAM = "caseSensitive";
   
-  private Set<String> trueValues = new HashSet<String>(Arrays.asList(new String[] { "true" }));
-  private Set<String> falseValues = new HashSet<String>(Arrays.asList(new String[] { "false" }));
+  private Set<String> trueValues = new HashSet<>(Arrays.asList(new String[] { "true" }));
+  private Set<String> falseValues = new HashSet<>(Arrays.asList(new String[] { "false" }));
   private boolean caseSensitive = false;
 
   @Override
diff --git solr/core/src/java/org/apache/solr/update/processor/ParseDateFieldUpdateProcessorFactory.java solr/core/src/java/org/apache/solr/update/processor/ParseDateFieldUpdateProcessorFactory.java
index 5fa9bc2..2314952 100644
--- solr/core/src/java/org/apache/solr/update/processor/ParseDateFieldUpdateProcessorFactory.java
+++ solr/core/src/java/org/apache/solr/update/processor/ParseDateFieldUpdateProcessorFactory.java
@@ -102,7 +102,7 @@ public class ParseDateFieldUpdateProcessorFactory extends FieldMutatingUpdatePro
   private static final String DEFAULT_TIME_ZONE_PARAM = "defaultTimeZone";
   private static final String LOCALE_PARAM = "locale";
 
-  private Map<String,DateTimeFormatter> formats = new LinkedHashMap<String,DateTimeFormatter>();
+  private Map<String,DateTimeFormatter> formats = new LinkedHashMap<>();
 
   @Override
   public UpdateRequestProcessor getInstance(SolrQueryRequest req,
diff --git solr/core/src/java/org/apache/solr/update/processor/PreAnalyzedUpdateProcessorFactory.java solr/core/src/java/org/apache/solr/update/processor/PreAnalyzedUpdateProcessorFactory.java
index 56d9af7..f276206 100644
--- solr/core/src/java/org/apache/solr/update/processor/PreAnalyzedUpdateProcessorFactory.java
+++ solr/core/src/java/org/apache/solr/update/processor/PreAnalyzedUpdateProcessorFactory.java
@@ -121,7 +121,7 @@ public class PreAnalyzedUpdateProcessorFactory extends FieldMutatingUpdateProces
   public void inform(SolrCore core) {
     super.inform(core);
     parser = new PreAnalyzedField();
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     if (parserImpl != null) {
       args.put(PreAnalyzedField.PARSER_IMPL, parserImpl);
     }
diff --git solr/core/src/java/org/apache/solr/update/processor/RegexpBoostProcessor.java solr/core/src/java/org/apache/solr/update/processor/RegexpBoostProcessor.java
index 62aae60..29a7acb 100644
--- solr/core/src/java/org/apache/solr/update/processor/RegexpBoostProcessor.java
+++ solr/core/src/java/org/apache/solr/update/processor/RegexpBoostProcessor.java
@@ -66,7 +66,7 @@ public class RegexpBoostProcessor extends UpdateRequestProcessor {
   private String inputFieldname = DEFAULT_INPUT_FIELDNAME;
   private String boostFieldname = DEFAULT_BOOST_FIELDNAME;
   private String boostFilename;
-  private List<BoostEntry> boostEntries = new ArrayList<BoostEntry>();
+  private List<BoostEntry> boostEntries = new ArrayList<>();
   private static final String BOOST_ENTRIES_CACHE_KEY = "boost-entries";
 
   RegexpBoostProcessor(SolrParams parameters,
@@ -119,7 +119,7 @@ public class RegexpBoostProcessor extends UpdateRequestProcessor {
   }
 
   private List<BoostEntry> initBoostEntries(InputStream is) throws IOException {
-    List<BoostEntry> newBoostEntries = new ArrayList<BoostEntry>();
+    List<BoostEntry> newBoostEntries = new ArrayList<>();
     
     BufferedReader reader = new BufferedReader(new InputStreamReader(is, Charset.forName("UTF-8")));
     try {
diff --git solr/core/src/java/org/apache/solr/update/processor/RegexpBoostProcessorFactory.java solr/core/src/java/org/apache/solr/update/processor/RegexpBoostProcessorFactory.java
index 5bb22ba..a2660c1 100644
--- solr/core/src/java/org/apache/solr/update/processor/RegexpBoostProcessorFactory.java
+++ solr/core/src/java/org/apache/solr/update/processor/RegexpBoostProcessorFactory.java
@@ -33,7 +33,7 @@ import org.apache.solr.response.SolrQueryResponse;
 public class RegexpBoostProcessorFactory extends UpdateRequestProcessorFactory {
 
     private SolrParams params;
-    private final Map<Object, Object> sharedObjectCache = new HashMap<Object, Object>();
+    private final Map<Object, Object> sharedObjectCache = new HashMap<>();
 
     @Override
     public void init(@SuppressWarnings("rawtypes") final NamedList args) {
diff --git solr/core/src/java/org/apache/solr/update/processor/SignatureUpdateProcessorFactory.java solr/core/src/java/org/apache/solr/update/processor/SignatureUpdateProcessorFactory.java
index a6c38ed..4618aad 100644
--- solr/core/src/java/org/apache/solr/update/processor/SignatureUpdateProcessorFactory.java
+++ solr/core/src/java/org/apache/solr/update/processor/SignatureUpdateProcessorFactory.java
@@ -142,7 +142,7 @@ public class SignatureUpdateProcessorFactory
                     "Can't use SignatureUpdateProcessor with partial updates on signature fields");
           }
           Collection<String> docFields = doc.getFieldNames();
-          currDocSigFields = new ArrayList<String>(docFields.size());
+          currDocSigFields = new ArrayList<>(docFields.size());
           currDocSigFields.addAll(docFields);
           Collections.sort(currDocSigFields);
         } else {
diff --git solr/core/src/java/org/apache/solr/update/processor/StatelessScriptUpdateProcessorFactory.java solr/core/src/java/org/apache/solr/update/processor/StatelessScriptUpdateProcessorFactory.java
index 8d14dc7..f330e63 100644
--- solr/core/src/java/org/apache/solr/update/processor/StatelessScriptUpdateProcessorFactory.java
+++ solr/core/src/java/org/apache/solr/update/processor/StatelessScriptUpdateProcessorFactory.java
@@ -179,7 +179,7 @@ public class StatelessScriptUpdateProcessorFactory extends UpdateRequestProcesso
                               "StatelessScriptUpdateProcessorFactory must be " +
                               "initialized with at least one " + SCRIPT_ARG);
     }
-    scriptFiles = new ArrayList<ScriptFile>();
+    scriptFiles = new ArrayList<>();
     for (String script : scripts) {
       scriptFiles.add(new ScriptFile(script));
     }
@@ -251,7 +251,7 @@ public class StatelessScriptUpdateProcessorFactory extends UpdateRequestProcesso
                                        SolrQueryResponse rsp) 
     throws SolrException {
     
-    List<EngineInfo> scriptEngines = new ArrayList<EngineInfo>();
+    List<EngineInfo> scriptEngines = new ArrayList<>();
 
     ScriptEngineManager scriptEngineManager 
       = new ScriptEngineManager(resourceLoader.getClassLoader());
@@ -338,7 +338,7 @@ public class StatelessScriptUpdateProcessorFactory extends UpdateRequestProcesso
       List<ScriptEngineFactory> factories = mgr.getEngineFactories();
       if (null == factories) return result;
 
-      Set<String> engines = new LinkedHashSet<String>(factories.size());
+      Set<String> engines = new LinkedHashSet<>(factories.size());
       for (ScriptEngineFactory f : factories) {
         if (ext) {
           engines.addAll(f.getExtensions());
diff --git solr/core/src/java/org/apache/solr/update/processor/TextProfileSignature.java solr/core/src/java/org/apache/solr/update/processor/TextProfileSignature.java
index 7a24e8d..8a4bd2a 100644
--- solr/core/src/java/org/apache/solr/update/processor/TextProfileSignature.java
+++ solr/core/src/java/org/apache/solr/update/processor/TextProfileSignature.java
@@ -65,7 +65,7 @@ public class TextProfileSignature extends MD5Signature {
 
   @Override
   public void add(String content) {
-    HashMap<String, Token> tokens = new HashMap<String, Token>();
+    HashMap<String, Token> tokens = new HashMap<>();
 
     StringBuilder curToken = new StringBuilder();
     int maxFreq = 0;
@@ -105,7 +105,7 @@ public class TextProfileSignature extends MD5Signature {
         maxFreq = tok.cnt;
     }
     Iterator<Token> it = tokens.values().iterator();
-    ArrayList<Token> profile = new ArrayList<Token>();
+    ArrayList<Token> profile = new ArrayList<>();
     // calculate the QUANT value
     int quant = Math.round(maxFreq * quantRate);
     if (quant < 2) {
diff --git solr/core/src/java/org/apache/solr/update/processor/UniqFieldsUpdateProcessorFactory.java solr/core/src/java/org/apache/solr/update/processor/UniqFieldsUpdateProcessorFactory.java
index 13d497b..17aaf78 100644
--- solr/core/src/java/org/apache/solr/update/processor/UniqFieldsUpdateProcessorFactory.java
+++ solr/core/src/java/org/apache/solr/update/processor/UniqFieldsUpdateProcessorFactory.java
@@ -70,8 +70,8 @@ public class UniqFieldsUpdateProcessorFactory extends FieldValueSubsetUpdateProc
   @Override
   @SuppressWarnings("unchecked")
   public Collection pickSubset(Collection values) {
-    Set<Object> uniqs = new HashSet<Object>();
-    List<Object> result = new ArrayList<Object>(values.size());
+    Set<Object> uniqs = new HashSet<>();
+    List<Object> result = new ArrayList<>(values.size());
     for (Object o : values) {
       if (!uniqs.contains(o)) {
         uniqs.add(o);
diff --git solr/core/src/java/org/apache/solr/util/ConcurrentLFUCache.java solr/core/src/java/org/apache/solr/util/ConcurrentLFUCache.java
index 6d6f84c..f0b031f 100644
--- solr/core/src/java/org/apache/solr/util/ConcurrentLFUCache.java
+++ solr/core/src/java/org/apache/solr/util/ConcurrentLFUCache.java
@@ -62,7 +62,7 @@ public class ConcurrentLFUCache<K, V> {
     if (upperWaterMark < 1) throw new IllegalArgumentException("upperWaterMark must be > 0");
     if (lowerWaterMark >= upperWaterMark)
       throw new IllegalArgumentException("lowerWaterMark must be  < upperWaterMark");
-    map = new ConcurrentHashMap<Object, CacheEntry<K, V>>(initialSize);
+    map = new ConcurrentHashMap<>(initialSize);
     newThreadForCleanup = runNewThreadForCleanup;
     this.upperWaterMark = upperWaterMark;
     this.lowerWaterMark = lowerWaterMark;
@@ -108,7 +108,7 @@ public class ConcurrentLFUCache<K, V> {
 
   public V put(K key, V val) {
     if (val == null) return null;
-    CacheEntry<K, V> e = new CacheEntry<K, V>(key, val, stats.accessCounter.incrementAndGet());
+    CacheEntry<K, V> e = new CacheEntry<>(key, val, stats.accessCounter.incrementAndGet());
     CacheEntry<K, V> oldCacheEntry = map.put(key, e);
     int currentSize;
     if (oldCacheEntry == null) {
@@ -171,7 +171,7 @@ public class ConcurrentLFUCache<K, V> {
 
       int wantToRemove = sz - lowerWaterMark;
 
-      TreeSet<CacheEntry> tree = new TreeSet<CacheEntry>();
+      TreeSet<CacheEntry> tree = new TreeSet<>();
 
       for (CacheEntry<K, V> ce : map.values()) {
         // set hitsCopy to avoid later Atomic reads
@@ -223,10 +223,10 @@ public class ConcurrentLFUCache<K, V> {
    * @return a LinkedHashMap containing 'n' or less than 'n' entries
    */
   public Map<K, V> getLeastUsedItems(int n) {
-    Map<K, V> result = new LinkedHashMap<K, V>();
+    Map<K, V> result = new LinkedHashMap<>();
     if (n <= 0)
       return result;
-    TreeSet<CacheEntry> tree = new TreeSet<CacheEntry>();
+    TreeSet<CacheEntry> tree = new TreeSet<>();
     // we need to grab the lock since we are changing the copy variables
     markAndSweepLock.lock();
     try {
@@ -267,10 +267,10 @@ public class ConcurrentLFUCache<K, V> {
    * @return a LinkedHashMap containing 'n' or less than 'n' entries
    */
   public Map<K, V> getMostUsedItems(int n) {
-    Map<K, V> result = new LinkedHashMap<K, V>();
+    Map<K, V> result = new LinkedHashMap<>();
     if (n <= 0)
       return result;
-    TreeSet<CacheEntry> tree = new TreeSet<CacheEntry>();
+    TreeSet<CacheEntry> tree = new TreeSet<>();
     // we need to grab the lock since we are changing the copy variables
     markAndSweepLock.lock();
     try {
@@ -427,7 +427,7 @@ public class ConcurrentLFUCache<K, V> {
     private boolean stop = false;
 
     public CleanupThread(ConcurrentLFUCache c) {
-      cache = new WeakReference<ConcurrentLFUCache>(c);
+      cache = new WeakReference<>(c);
     }
 
     @Override
diff --git solr/core/src/java/org/apache/solr/util/ConcurrentLRUCache.java solr/core/src/java/org/apache/solr/util/ConcurrentLRUCache.java
index a977419..5b5f7df 100644
--- solr/core/src/java/org/apache/solr/util/ConcurrentLRUCache.java
+++ solr/core/src/java/org/apache/solr/util/ConcurrentLRUCache.java
@@ -64,7 +64,7 @@ public class ConcurrentLRUCache<K,V> {
     if (upperWaterMark < 1) throw new IllegalArgumentException("upperWaterMark must be > 0");
     if (lowerWaterMark >= upperWaterMark)
       throw new IllegalArgumentException("lowerWaterMark must be  < upperWaterMark");
-    map = new ConcurrentHashMap<Object, CacheEntry<K,V>>(initialSize);
+    map = new ConcurrentHashMap<>(initialSize);
     newThreadForCleanup = runNewThreadForCleanup;
     this.upperWaterMark = upperWaterMark;
     this.lowerWaterMark = lowerWaterMark;
@@ -106,7 +106,7 @@ public class ConcurrentLRUCache<K,V> {
 
   public V put(K key, V val) {
     if (val == null) return null;
-    CacheEntry<K,V> e = new CacheEntry<K,V>(key, val, stats.accessCounter.incrementAndGet());
+    CacheEntry<K,V> e = new CacheEntry<>(key, val, stats.accessCounter.incrementAndGet());
     CacheEntry<K,V> oldCacheEntry = map.put(key, e);
     int currentSize;
     if (oldCacheEntry == null) {
@@ -284,7 +284,7 @@ public class ConcurrentLRUCache<K,V> {
         wantToKeep = lowerWaterMark - numKept;
         wantToRemove = sz - lowerWaterMark - numRemoved;
 
-        PQueue<K,V> queue = new PQueue<K,V>(wantToRemove);
+        PQueue<K,V> queue = new PQueue<>(wantToRemove);
 
         for (int i=eSize-1; i>=0; i--) {
           CacheEntry<K,V> ce = eset[i];
@@ -408,10 +408,10 @@ public class ConcurrentLRUCache<K,V> {
    * @return a LinkedHashMap containing 'n' or less than 'n' entries
    */
   public Map<K, V> getOldestAccessedItems(int n) {
-    Map<K, V> result = new LinkedHashMap<K, V>();
+    Map<K, V> result = new LinkedHashMap<>();
     if (n <= 0)
       return result;
-    TreeSet<CacheEntry<K,V>> tree = new TreeSet<CacheEntry<K,V>>();
+    TreeSet<CacheEntry<K,V>> tree = new TreeSet<>();
     markAndSweepLock.lock();
     try {
       for (Map.Entry<Object, CacheEntry<K,V>> entry : map.entrySet()) {
@@ -436,10 +436,10 @@ public class ConcurrentLRUCache<K,V> {
   }
 
   public Map<K,V> getLatestAccessedItems(int n) {
-    Map<K,V> result = new LinkedHashMap<K,V>();
+    Map<K,V> result = new LinkedHashMap<>();
     if (n <= 0)
       return result;
-    TreeSet<CacheEntry<K,V>> tree = new TreeSet<CacheEntry<K,V>>();
+    TreeSet<CacheEntry<K,V>> tree = new TreeSet<>();
     // we need to grab the lock since we are changing lastAccessedCopy
     markAndSweepLock.lock();
     try {
@@ -587,7 +587,7 @@ public class ConcurrentLRUCache<K,V> {
     private boolean stop = false;
 
     public CleanupThread(ConcurrentLRUCache c) {
-      cache = new WeakReference<ConcurrentLRUCache>(c);
+      cache = new WeakReference<>(c);
     }
 
     @Override
diff --git solr/core/src/java/org/apache/solr/util/DOMUtil.java solr/core/src/java/org/apache/solr/util/DOMUtil.java
index 0c78447..5c82a52 100644
--- solr/core/src/java/org/apache/solr/util/DOMUtil.java
+++ solr/core/src/java/org/apache/solr/util/DOMUtil.java
@@ -38,7 +38,7 @@ public class DOMUtil {
   }
 
   public static Map<String,String> toMapExcept(NamedNodeMap attrs, String... exclusions) {
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     outer: for (int j=0; j<attrs.getLength(); j++) {
       Node attr = attrs.item(j);
 
@@ -101,7 +101,7 @@ public class DOMUtil {
   }
 
   public static NamedList<Object> nodesToNamedList(NodeList nlst) {
-    NamedList<Object> clst = new NamedList<Object>();
+    NamedList<Object> clst = new NamedList<>();
     for (int i=0; i<nlst.getLength(); i++) {
       addToNamedList(nlst.item(i), clst, null);
     }
@@ -307,8 +307,8 @@ public class DOMUtil {
       return value;
     }
 
-    List<String> fragments = new ArrayList<String>();
-    List<String> propertyRefs = new ArrayList<String>();
+    List<String> fragments = new ArrayList<>();
+    List<String> propertyRefs = new ArrayList<>();
     parsePropertyString(value, fragments, propertyRefs);
 
     StringBuilder sb = new StringBuilder();
diff --git solr/core/src/java/org/apache/solr/util/DateMathParser.java solr/core/src/java/org/apache/solr/util/DateMathParser.java
index 53d7c55..fbeb61f 100644
--- solr/core/src/java/org/apache/solr/util/DateMathParser.java
+++ solr/core/src/java/org/apache/solr/util/DateMathParser.java
@@ -128,7 +128,7 @@ public class DateMathParser  {
     // we probably need to change "Locale loc" to default to something 
     // from a param via SolrRequestInfo as well.
     
-    Map<String,Integer> units = new HashMap<String,Integer>(13);
+    Map<String,Integer> units = new HashMap<>(13);
     units.put("YEAR",        Calendar.YEAR);
     units.put("YEARS",       Calendar.YEAR);
     units.put("MONTH",       Calendar.MONTH);
diff --git solr/core/src/java/org/apache/solr/util/MapListener.java solr/core/src/java/org/apache/solr/util/MapListener.java
index c6b4b37..c74ccad 100644
--- solr/core/src/java/org/apache/solr/util/MapListener.java
+++ solr/core/src/java/org/apache/solr/util/MapListener.java
@@ -33,7 +33,7 @@ public class MapListener<K, V> extends ForwardingMap<K, V> {
 
   public MapListener(Map<K, V> target) {
     this.target = target;
-    seenKeys = new HashSet<K>(target.size());
+    seenKeys = new HashSet<>(target.size());
   }
 
   public Set<K> getSeenKeys() {
diff --git solr/core/src/java/org/apache/solr/util/PropertiesUtil.java solr/core/src/java/org/apache/solr/util/PropertiesUtil.java
index 38da33b..20f594d 100644
--- solr/core/src/java/org/apache/solr/util/PropertiesUtil.java
+++ solr/core/src/java/org/apache/solr/util/PropertiesUtil.java
@@ -38,8 +38,8 @@ public class PropertiesUtil {
       return value;
     }
 
-    List<String> fragments = new ArrayList<String>();
-    List<String> propertyRefs = new ArrayList<String>();
+    List<String> fragments = new ArrayList<>();
+    List<String> propertyRefs = new ArrayList<>();
     parsePropertyString(value, fragments, propertyRefs);
 
     StringBuilder sb = new StringBuilder();
diff --git solr/core/src/java/org/apache/solr/util/RTimer.java solr/core/src/java/org/apache/solr/util/RTimer.java
index 2cbd044..a85f9be 100644
--- solr/core/src/java/org/apache/solr/util/RTimer.java
+++ solr/core/src/java/org/apache/solr/util/RTimer.java
@@ -47,7 +47,7 @@ public class RTimer {
   public RTimer() {
     time = 0;
     culmTime = 0;
-    children = new SimpleOrderedMap<RTimer>();
+    children = new SimpleOrderedMap<>();
     startTime = now();
     state = STARTED;
   }
@@ -116,7 +116,7 @@ public class RTimer {
   }
 
   public NamedList asNamedList() {
-    NamedList<Object> m = new SimpleOrderedMap<Object>();
+    NamedList<Object> m = new SimpleOrderedMap<>();
     m.add( "time", time );
     if( children.size() > 0 ) {
       for( Map.Entry<String, RTimer> entry : children ) {
diff --git solr/core/src/java/org/apache/solr/util/SimplePostTool.java solr/core/src/java/org/apache/solr/util/SimplePostTool.java
index edfe9c7..80e54ee 100644
--- solr/core/src/java/org/apache/solr/util/SimplePostTool.java
+++ solr/core/src/java/org/apache/solr/util/SimplePostTool.java
@@ -108,10 +108,10 @@ public class SimplePostTool {
   static HashMap<String,String> mimeMap;
   GlobFileFilter globFileFilter;
   // Backlog for crawling
-  List<LinkedHashSet<URL>> backlog = new ArrayList<LinkedHashSet<URL>>();
-  Set<URL> visited = new HashSet<URL>();
+  List<LinkedHashSet<URL>> backlog = new ArrayList<>();
+  Set<URL> visited = new HashSet<>();
   
-  static final Set<String> DATA_MODES = new HashSet<String>();
+  static final Set<String> DATA_MODES = new HashSet<>();
   static final String USAGE_STRING_SHORT =
       "Usage: java [SystemProperties] -jar post.jar [-h|-] [<file|folder|url|arg> [<file|folder|url|arg>...]]";
 
@@ -125,7 +125,7 @@ public class SimplePostTool {
     DATA_MODES.add(DATA_MODE_STDIN);
     DATA_MODES.add(DATA_MODE_WEB);
     
-    mimeMap = new HashMap<String,String>();
+    mimeMap = new HashMap<>();
     mimeMap.put("xml", "text/xml");
     mimeMap.put("csv", "text/csv");
     mimeMap.put("json", "application/json");
@@ -344,8 +344,8 @@ public class SimplePostTool {
   private void reset() {
     fileTypes = DEFAULT_FILE_TYPES;
     globFileFilter = this.getFileFilterFromFileTypes(fileTypes);
-    backlog = new ArrayList<LinkedHashSet<URL>>();
-    visited = new HashSet<URL>();
+    backlog = new ArrayList<>();
+    visited = new HashSet<>();
   }
 
 
@@ -512,7 +512,7 @@ public class SimplePostTool {
    */
   public int postWebPages(String[] args, int startIndexInArgs, OutputStream out) {
     reset();
-    LinkedHashSet<URL> s = new LinkedHashSet<URL>();
+    LinkedHashSet<URL> s = new LinkedHashSet<>();
     for (int j = startIndexInArgs; j < args.length; j++) {
       try {
         URL u = new URL(normalizeUrlEnding(args[j]));
@@ -558,7 +558,7 @@ public class SimplePostTool {
     int rawStackSize = stack.size();
     stack.removeAll(visited);
     int stackSize = stack.size();
-    LinkedHashSet<URL> subStack = new LinkedHashSet<URL>();
+    LinkedHashSet<URL> subStack = new LinkedHashSet<>();
     info("Entering crawl at level "+level+" ("+rawStackSize+" links total, "+stackSize+" new)");
     for(URL u : stack) {
       try {
@@ -1016,7 +1016,7 @@ public class SimplePostTool {
     final String DISALLOW = "Disallow:";
     
     public PageFetcher() {
-      robotsCache = new HashMap<String,List<String>>();
+      robotsCache = new HashMap<>();
     }
     
     public PageFetcherResult readPageFromUrl(URL u) {
@@ -1074,7 +1074,7 @@ public class SimplePostTool {
       String strRobot = url.getProtocol() + "://" + host + "/robots.txt";
       List<String> disallows = robotsCache.get(host);
       if(disallows == null) {
-        disallows = new ArrayList<String>();
+        disallows = new ArrayList<>();
         URL urlRobot;
         try { 
           urlRobot = new URL(strRobot);
@@ -1104,7 +1104,7 @@ public class SimplePostTool {
      * @throws IOException if problems reading the stream
      */
     protected List<String> parseRobotsTxt(InputStream is) throws IOException {
-      List<String> disallows = new ArrayList<String>();
+      List<String> disallows = new ArrayList<>();
       BufferedReader r = new BufferedReader(new InputStreamReader(is, "UTF-8"));
       String l;
       while((l = r.readLine()) != null) {
@@ -1130,7 +1130,7 @@ public class SimplePostTool {
      * @return a set of URLs parsed from the page
      */
     protected Set<URL> getLinksFromWebPage(URL u, InputStream is, String type, URL postUrl) {
-      Set<URL> l = new HashSet<URL>();
+      Set<URL> l = new HashSet<>();
       URL url = null;
       try {
         ByteArrayOutputStream os = new ByteArrayOutputStream();
diff --git solr/core/src/java/org/apache/solr/util/SolrLogLayout.java solr/core/src/java/org/apache/solr/util/SolrLogLayout.java
index 6b6d011..fe4d79c 100644
--- solr/core/src/java/org/apache/solr/util/SolrLogLayout.java
+++ solr/core/src/java/org/apache/solr/util/SolrLogLayout.java
@@ -44,7 +44,7 @@ public class SolrLogLayout extends Layout {
   
   long startTime = System.currentTimeMillis();
   long lastTime = startTime;
-  Map<Method,String> methodAlias = new HashMap<Method,String>();
+  Map<Method,String> methodAlias = new HashMap<>();
   
   public static class Method {
     public String className;
@@ -81,9 +81,9 @@ public class SolrLogLayout extends Layout {
     Map<String,Object> coreProps;
   }
   
-  Map<Integer,CoreInfo> coreInfoMap = new WeakHashMap<Integer,CoreInfo>();
+  Map<Integer,CoreInfo> coreInfoMap = new WeakHashMap<>();
   
-  public Map<String,String> classAliases = new HashMap<String,String>();
+  public Map<String,String> classAliases = new HashMap<>();
   
   public void appendThread(StringBuilder sb, LoggingEvent event) {
     Thread th = Thread.currentThread();
diff --git solr/core/src/java/org/apache/solr/util/SolrPluginUtils.java solr/core/src/java/org/apache/solr/util/SolrPluginUtils.java
index af7a963..75be77b 100644
--- solr/core/src/java/org/apache/solr/util/SolrPluginUtils.java
+++ solr/core/src/java/org/apache/solr/util/SolrPluginUtils.java
@@ -194,7 +194,7 @@ public class SolrPluginUtils {
 
       if (rb.doHighlights) {
         // copy return fields list
-        fieldFilter = new HashSet<String>(fieldFilter);
+        fieldFilter = new HashSet<>(fieldFilter);
         // add highlight fields
 
         SolrHighlighter highlighter = HighlightComponent.getHighlighter(req.getCore());
@@ -219,7 +219,7 @@ public class SolrPluginUtils {
 
 
   public static Set<String> getDebugInterests(String[] params, ResponseBuilder rb){
-    Set<String> debugInterests = new HashSet<String>();
+    Set<String> debugInterests = new HashSet<>();
     if (params != null) {
       for (int i = 0; i < params.length; i++) {
         if (params[i].equalsIgnoreCase("all") || params[i].equalsIgnoreCase("true")){
@@ -346,7 +346,7 @@ public class SolrPluginUtils {
   }
 
   public static NamedList<Object> explanationToNamedList(Explanation e) {
-    NamedList<Object> out = new SimpleOrderedMap<Object>();
+    NamedList<Object> out = new SimpleOrderedMap<>();
 
     out.add("match", e.isMatch());
     out.add("value", e.getValue());
@@ -358,7 +358,7 @@ public class SolrPluginUtils {
     if (null == details || 0 == details.length) return out;
 
     List<NamedList<Object>> kids
-      = new ArrayList<NamedList<Object>>(details.length);
+      = new ArrayList<>(details.length);
     for (Explanation d : details) {
       kids.add(explanationToNamedList(d));
     }
@@ -371,7 +371,7 @@ public class SolrPluginUtils {
     (NamedList<Explanation> explanations) {
 
     NamedList<NamedList<Object>> out
-      = new SimpleOrderedMap<NamedList<Object>>();
+      = new SimpleOrderedMap<>();
     for (Map.Entry<String,Explanation> entry : explanations) {
       out.add(entry.getKey(), explanationToNamedList(entry.getValue()));
     }
@@ -390,7 +390,7 @@ public class SolrPluginUtils {
      SolrIndexSearcher searcher,
      IndexSchema schema) throws IOException {
 
-    NamedList<Explanation> explainList = new SimpleOrderedMap<Explanation>();
+    NamedList<Explanation> explainList = new SimpleOrderedMap<>();
     DocIterator iterator = docs.iterator();
     for (int i=0; i<docs.size(); i++) {
       int id = iterator.nextDoc();
@@ -406,7 +406,7 @@ public class SolrPluginUtils {
   private static NamedList<String> explanationsToStrings
     (NamedList<Explanation> explanations) {
 
-    NamedList<String> out = new SimpleOrderedMap<String>();
+    NamedList<String> out = new SimpleOrderedMap<>();
     for (Map.Entry<String,Explanation> entry : explanations) {
       out.add(entry.getKey(), "\n"+entry.getValue().toString());
     }
@@ -470,9 +470,9 @@ public class SolrPluginUtils {
    */
   public static Map<String,Float> parseFieldBoosts(String[] fieldLists) {
     if (null == fieldLists || 0 == fieldLists.length) {
-      return new HashMap<String,Float>();
+      return new HashMap<>();
     }
-    Map<String, Float> out = new HashMap<String,Float>(7);
+    Map<String, Float> out = new HashMap<>(7);
     for (String in : fieldLists) {
       if (null == in) {
         continue;
@@ -502,9 +502,9 @@ public class SolrPluginUtils {
    */
   public static List<FieldParams> parseFieldBoostsAndSlop(String[] fieldLists,int wordGrams,int defaultSlop) {
     if (null == fieldLists || 0 == fieldLists.length) {
-        return new ArrayList<FieldParams>();
+        return new ArrayList<>();
     }
-    List<FieldParams> out = new ArrayList<FieldParams>();
+    List<FieldParams> out = new ArrayList<>();
     for (String in : fieldLists) {
       if (null == in) {
         continue;
@@ -738,7 +738,7 @@ public class SolrPluginUtils {
      * string, to Alias object containing the fields to use in our
      * DisjunctionMaxQuery and the tiebreaker to use.
      */
-    protected Map<String,Alias> aliases = new HashMap<String,Alias>(3);
+    protected Map<String,Alias> aliases = new HashMap<>(3);
     public DisjunctionMaxQueryParser(QParser qp, String defaultField) {
       super(qp,defaultField);
       // don't trust that our parent class won't ever change it's default
@@ -848,7 +848,7 @@ public class SolrPluginUtils {
   public static List<Query> parseQueryStrings(SolrQueryRequest req,
                                               String[] queries) throws SyntaxError {
     if (null == queries || 0 == queries.length) return null;
-    List<Query> out = new ArrayList<Query>(queries.length);
+    List<Query> out = new ArrayList<>(queries.length);
     for (String q : queries) {
       if (null != q && 0 != q.trim().length()) {
         out.add(QParser.getParser(q, null, req).getQuery());
diff --git solr/core/src/java/org/apache/solr/util/TimeZoneUtils.java solr/core/src/java/org/apache/solr/util/TimeZoneUtils.java
index 0aa5875..d57be19 100644
--- solr/core/src/java/org/apache/solr/util/TimeZoneUtils.java
+++ solr/core/src/java/org/apache/solr/util/TimeZoneUtils.java
@@ -43,7 +43,7 @@ public final class TimeZoneUtils {
    * @see TimeZone#getAvailableIDs
    */
   public static final Set<String> KNOWN_TIMEZONE_IDS 
-    = Collections.unmodifiableSet(new HashSet<String>
+    = Collections.unmodifiableSet(new HashSet<>
                                   (Arrays.asList(TimeZone.getAvailableIDs())));
 
   /**
diff --git solr/core/src/java/org/apache/solr/util/VersionedFile.java solr/core/src/java/org/apache/solr/util/VersionedFile.java
index 19ab3fb..fc0a13f 100644
--- solr/core/src/java/org/apache/solr/util/VersionedFile.java
+++ solr/core/src/java/org/apache/solr/util/VersionedFile.java
@@ -63,7 +63,7 @@ public class VersionedFile
           });
           Arrays.sort(names);
           f = new File(dir, names[names.length-1]);
-          oldFiles = new ArrayList<File>();
+          oldFiles = new ArrayList<>();
           for (int i=0; i<names.length-1; i++) {
             oldFiles.add(new File(dir, names[i]));
           }
@@ -88,11 +88,11 @@ public class VersionedFile
     return is;
   }
 
-  private static final Set<File> deleteList = new HashSet<File>();
+  private static final Set<File> deleteList = new HashSet<>();
   private static synchronized void delete(Collection<File> files) {
     synchronized (deleteList) {
       deleteList.addAll(files);
-      List<File> deleted = new ArrayList<File>();
+      List<File> deleted = new ArrayList<>();
       for (File df : deleteList) {
         try {
           df.delete();
diff --git solr/core/src/java/org/apache/solr/util/plugin/AbstractPluginLoader.java solr/core/src/java/org/apache/solr/util/plugin/AbstractPluginLoader.java
index 40f4f4e..a9c5c4c3 100644
--- solr/core/src/java/org/apache/solr/util/plugin/AbstractPluginLoader.java
+++ solr/core/src/java/org/apache/solr/util/plugin/AbstractPluginLoader.java
@@ -135,7 +135,7 @@ public abstract class AbstractPluginLoader<T>
    */
   public T load( SolrResourceLoader loader, NodeList nodes )
   {
-    List<PluginInitInfo> info = new ArrayList<PluginInitInfo>();
+    List<PluginInitInfo> info = new ArrayList<>();
     T defaultPlugin = null;
     
     if (nodes !=null ) {
@@ -218,7 +218,7 @@ public abstract class AbstractPluginLoader<T>
    * 
    */
   public T loadSingle(SolrResourceLoader loader, Node node) {
-    List<PluginInitInfo> info = new ArrayList<PluginInitInfo>();
+    List<PluginInitInfo> info = new ArrayList<>();
     T plugin = null;
 
     try {
diff --git solr/core/src/java/org/apache/solr/util/stats/ExponentiallyDecayingSample.java solr/core/src/java/org/apache/solr/util/stats/ExponentiallyDecayingSample.java
index 5f7728a..6a227ec 100644
--- solr/core/src/java/org/apache/solr/util/stats/ExponentiallyDecayingSample.java
+++ solr/core/src/java/org/apache/solr/util/stats/ExponentiallyDecayingSample.java
@@ -72,7 +72,7 @@ public class ExponentiallyDecayingSample implements Sample {
    *                      sample will be towards newer values
    */
   public ExponentiallyDecayingSample(int reservoirSize, double alpha, Clock clock) {
-    this.values = new ConcurrentSkipListMap<Double, Long>();
+    this.values = new ConcurrentSkipListMap<>();
     this.lock = new ReentrantReadWriteLock();
     this.alpha = alpha;
     this.reservoirSize = reservoirSize;
@@ -187,7 +187,7 @@ public class ExponentiallyDecayingSample implements Sample {
       try {
         final long oldStartTime = startTime;
         this.startTime = currentTimeInSeconds();
-        final ArrayList<Double> keys = new ArrayList<Double>(values.keySet());
+        final ArrayList<Double> keys = new ArrayList<>(values.keySet());
         for (Double key : keys) {
           final Long value = values.remove(key);
           values.put(key * exp(-alpha * (startTime - oldStartTime)), value);
diff --git solr/core/src/java/org/apache/solr/util/stats/Histogram.java solr/core/src/java/org/apache/solr/util/stats/Histogram.java
index 696884a..07d3b1f 100644
--- solr/core/src/java/org/apache/solr/util/stats/Histogram.java
+++ solr/core/src/java/org/apache/solr/util/stats/Histogram.java
@@ -74,7 +74,7 @@ public class Histogram {
   // These are for the Welford algorithm for calculating running variance
   // without floating-point doom.
   private final AtomicReference<double[]> variance =
-      new AtomicReference<double[]>(new double[]{-1, 0}); // M, S
+      new AtomicReference<>(new double[]{-1, 0}); // M, S
   private final AtomicLong count = new AtomicLong();
 
   /**
diff --git solr/core/src/java/org/apache/solr/util/stats/UniformSample.java solr/core/src/java/org/apache/solr/util/stats/UniformSample.java
index 37f5366..0293d69 100644
--- solr/core/src/java/org/apache/solr/util/stats/UniformSample.java
+++ solr/core/src/java/org/apache/solr/util/stats/UniformSample.java
@@ -100,7 +100,7 @@ public class UniformSample implements Sample {
   @Override
   public Snapshot getSnapshot() {
     final int s = size();
-    final List<Long> copy = new ArrayList<Long>(s);
+    final List<Long> copy = new ArrayList<>(s);
     for (int i = 0; i < s; i++) {
       copy.add(values.get(i));
     }
diff --git solr/core/src/test/org/apache/solr/BasicFunctionalityTest.java solr/core/src/test/org/apache/solr/BasicFunctionalityTest.java
index cfeda9b..0b254e5 100644
--- solr/core/src/test/org/apache/solr/BasicFunctionalityTest.java
+++ solr/core/src/test/org/apache/solr/BasicFunctionalityTest.java
@@ -351,7 +351,7 @@ public class BasicFunctionalityTest extends SolrTestCaseJ4 {
     final String BAD_VALUE = "NOT_A_NUMBER";
     ignoreException(BAD_VALUE);
 
-    final List<String> FIELDS = new LinkedList<String>();
+    final List<String> FIELDS = new LinkedList<>();
     for (String type : new String[] { "ti", "tf", "td", "tl" }) {
       FIELDS.add("malformed_" + type);
     }
@@ -550,7 +550,7 @@ public class BasicFunctionalityTest extends SolrTestCaseJ4 {
     nl.add("bt","true");
     nl.add("bf","false");
 
-    Map<String,String> m = new HashMap<String,String>();
+    Map<String,String> m = new HashMap<>();
     m.put("f.field1.i", "1000");
     m.put("s", "BBB");
     m.put("ss", "SSS");
diff --git solr/core/src/test/org/apache/solr/ConvertedLegacyTest.java solr/core/src/test/org/apache/solr/ConvertedLegacyTest.java
index 09c5750..508631b 100644
--- solr/core/src/test/org/apache/solr/ConvertedLegacyTest.java
+++ solr/core/src/test/org/apache/solr/ConvertedLegacyTest.java
@@ -42,7 +42,7 @@ public class ConvertedLegacyTest extends SolrTestCaseJ4 {
   public void testABunchOfConvertedStuff() {
     // these may be reused by things that need a special query
     SolrQueryRequest req = null;
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     lrf.args.put(CommonParams.VERSION,"2.2");
     lrf.args.put("defType","lucenePlusSort");
 
@@ -133,7 +133,7 @@ public class ConvertedLegacyTest extends SolrTestCaseJ4 {
             ,"*[count(//doc)=3] "
             ,"//*[@start='0']"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     req = new LocalSolrQueryRequest(h.getCore(), "val_s:[a TO z]",
                                     "standard", 2, 5 , args);
     assertQ(req
@@ -142,28 +142,28 @@ public class ConvertedLegacyTest extends SolrTestCaseJ4 {
             ,"*//doc[1]/str[.='pear'] "
             ,"//*[@start='2']"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     req = new LocalSolrQueryRequest(h.getCore(), "val_s:[a TO z]",
                                     "standard", 3, 5 , args);
     assertQ(req
             ,"//*[@numFound='3'] "
             ,"*[count(//doc)=0]"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     req = new LocalSolrQueryRequest(h.getCore(), "val_s:[a TO z]",
                                     "standard", 4, 5 , args);
     assertQ(req
             ,"//*[@numFound='3'] "
             ,"*[count(//doc)=0]"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     req = new LocalSolrQueryRequest(h.getCore(), "val_s:[a TO z]",
                                     "standard", 25, 5 , args);
     assertQ(req
             ,"//*[@numFound='3'] "
             ,"*[count(//doc)=0]"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     req = new LocalSolrQueryRequest(h.getCore(), "val_s:[a TO z]",
                                     "standard", 0, 1 , args);
     assertQ(req
@@ -171,7 +171,7 @@ public class ConvertedLegacyTest extends SolrTestCaseJ4 {
             ,"*[count(//doc)=1] "
             ,"*//doc[1]/str[.='apple']"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     req = new LocalSolrQueryRequest(h.getCore(), "val_s:[a TO z]",
                                     "standard", 0, 2 , args);
     assertQ(req
@@ -179,7 +179,7 @@ public class ConvertedLegacyTest extends SolrTestCaseJ4 {
             ,"*[count(//doc)=2] "
             ,"*//doc[2]/str[.='banana']"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     req = new LocalSolrQueryRequest(h.getCore(), "val_s:[a TO z]",
                                     "standard", 1, 1 , args);
     assertQ(req
@@ -187,35 +187,35 @@ public class ConvertedLegacyTest extends SolrTestCaseJ4 {
             ,"*[count(//doc)=1] "
             ,"*//doc[1]/str[.='banana']"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     req = new LocalSolrQueryRequest(h.getCore(), "val_s:[a TO z]",
                                     "standard", 3, 1 , args);
     assertQ(req
             ,"//*[@numFound='3'] "
             ,"*[count(//doc)=0]"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     req = new LocalSolrQueryRequest(h.getCore(), "val_s:[a TO z]",
                                     "standard", 4, 1 , args);
     assertQ(req
             ,"//*[@numFound='3'] "
             ,"*[count(//doc)=0]"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     req = new LocalSolrQueryRequest(h.getCore(), "val_s:[a TO z]",
                                     "standard", 1, 0 , args);
     assertQ(req
             ,"//*[@numFound='3'] "
             ,"*[count(//doc)=0]"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     req = new LocalSolrQueryRequest(h.getCore(), "val_s:[a TO z]",
                                     "standard", 0, 0 , args);
     assertQ(req
             ,"//*[@numFound='3'] "
             ,"*[count(//doc)=0]"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     args.put("defType","lucenePlusSort");
     req = new LocalSolrQueryRequest(h.getCore(), "val_s:[a TO z];val_s1 asc",
                                     "standard", 0, 0 , args);
@@ -223,7 +223,7 @@ public class ConvertedLegacyTest extends SolrTestCaseJ4 {
             ,"//*[@numFound='3'] "
             ,"*[count(//doc)=0]"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     args.put("defType","lucenePlusSort");
     req = new LocalSolrQueryRequest(h.getCore(), "val_s:[a TO z];val_s1 desc",
                                     "standard", 0, 0 , args);
@@ -1107,7 +1107,7 @@ public class ConvertedLegacyTest extends SolrTestCaseJ4 {
     assertU("<commit/>");
     assertQ(req("id:44")
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     args.put("fl","fname_s,arr_f  ");
     req = new LocalSolrQueryRequest(h.getCore(), "id:44",
                                     "standard", 0, 10, args);
@@ -1115,7 +1115,7 @@ public class ConvertedLegacyTest extends SolrTestCaseJ4 {
             ,"//str[.='Yonik']  "
             ,"//float[.='1.4142135']"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     args.put("fl","fname_s,score");
     req = new LocalSolrQueryRequest(h.getCore(), "id:44",
                                     "standard", 0, 10, args);
@@ -1126,7 +1126,7 @@ public class ConvertedLegacyTest extends SolrTestCaseJ4 {
 
     // test addition of score field
 
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     args.put("fl","score,* ");
     req = new LocalSolrQueryRequest(h.getCore(), "id:44",
                                     "standard", 0, 10, args);
@@ -1136,7 +1136,7 @@ public class ConvertedLegacyTest extends SolrTestCaseJ4 {
             ,"//float[@name='score'] "
             ,"*[count(//doc/*)>=13]"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     args.put("fl","*,score ");
     req = new LocalSolrQueryRequest(h.getCore(), "id:44",
                                     "standard", 0, 10, args);
@@ -1146,7 +1146,7 @@ public class ConvertedLegacyTest extends SolrTestCaseJ4 {
             ,"//float[@name='score'] "
             ,"*[count(//doc/*)>=13]"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     args.put("fl","* ");
     req = new LocalSolrQueryRequest(h.getCore(), "id:44",
                                     "standard", 0, 10, args);
@@ -1158,14 +1158,14 @@ public class ConvertedLegacyTest extends SolrTestCaseJ4 {
 
     // test maxScore
 
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     args.put("fl","score ");
     req = new LocalSolrQueryRequest(h.getCore(), "id:44",
                                     "standard", 0, 10, args);
     assertQ(req
             ,"//result[@maxScore>0]"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     args.put("fl","score ");
     args.put("defType","lucenePlusSort");
     req = new LocalSolrQueryRequest(h.getCore(), "id:44;id desc;",
@@ -1173,7 +1173,7 @@ public class ConvertedLegacyTest extends SolrTestCaseJ4 {
     assertQ(req
             ,"//result[@maxScore>0]"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     args.put("fl","score ");
     args.put("defType","lucenePlusSort");
     req = new LocalSolrQueryRequest(h.getCore(), "id:44;",
@@ -1181,7 +1181,7 @@ public class ConvertedLegacyTest extends SolrTestCaseJ4 {
     assertQ(req
             ,"//@maxScore = //doc/float[@name='score']"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     args.put("fl","score ");
     args.put("defType","lucenePlusSort");
     req = new LocalSolrQueryRequest(h.getCore(), "id:44;id desc;",
@@ -1189,7 +1189,7 @@ public class ConvertedLegacyTest extends SolrTestCaseJ4 {
     assertQ(req
             ,"//@maxScore = //doc/float[@name='score']"
             );
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     args.put("fl","*,score");
     args.put("defType","lucenePlusSort");
     req = new LocalSolrQueryRequest(h.getCore(), "id:44;id desc;",
diff --git solr/core/src/test/org/apache/solr/CursorPagingTest.java solr/core/src/test/org/apache/solr/CursorPagingTest.java
index 7afaa6f..e291d8c 100644
--- solr/core/src/test/org/apache/solr/CursorPagingTest.java
+++ solr/core/src/test/org/apache/solr/CursorPagingTest.java
@@ -645,7 +645,7 @@ public class CursorPagingTest extends SolrTestCaseJ4 {
 
     final boolean prune_dv = ! defaultCodecSupportsMissingDocValues();
 
-    ArrayList<String> names = new ArrayList<String>(37);
+    ArrayList<String> names = new ArrayList<>(37);
     for (String f : raw) {
       if (f.equals("_version_")) {
         continue;
@@ -760,7 +760,7 @@ public class CursorPagingTest extends SolrTestCaseJ4 {
     assertNotNull("facet.field param not specified", facetField);
     assertFalse("facet.field param contains multiple values", facetField.contains(","));
     assertEquals("facet.limit param not set to -1", "-1", params.get("facet.limit"));
-    final Map<String,MutableValueInt> facetCounts = new HashMap<String,MutableValueInt>();
+    final Map<String,MutableValueInt> facetCounts = new HashMap<>();
     SentinelIntSet ids = new SentinelIntSet(maxSize, -1);
     String cursorMark = CURSOR_MARK_START;
     int docsOnThisPage = Integer.MAX_VALUE;
@@ -945,7 +945,7 @@ public class CursorPagingTest extends SolrTestCaseJ4 {
    */
   public static String buildRandomSort(final Collection<String> fieldNames) {
 
-    ArrayList<String> shuffledNames = new ArrayList<String>(fieldNames);
+    ArrayList<String> shuffledNames = new ArrayList<>(fieldNames);
     Collections.replaceAll(shuffledNames, "id", "score");
     Collections.shuffle(shuffledNames, random());
 
diff --git solr/core/src/test/org/apache/solr/SolrInfoMBeanTest.java solr/core/src/test/org/apache/solr/SolrInfoMBeanTest.java
index 40f4fb7..d77b418 100644
--- solr/core/src/test/org/apache/solr/SolrInfoMBeanTest.java
+++ solr/core/src/test/org/apache/solr/SolrInfoMBeanTest.java
@@ -46,7 +46,7 @@ public class SolrInfoMBeanTest extends SolrTestCaseJ4
    * a name, description, etc...
    */
   public void testCallMBeanInfo() throws Exception {
-    List<Class> classes = new ArrayList<Class>();
+    List<Class> classes = new ArrayList<>();
     classes.addAll(getClassesForPackage(StandardRequestHandler.class.getPackage().getName()));
     classes.addAll(getClassesForPackage(SearchHandler.class.getPackage().getName()));
     classes.addAll(getClassesForPackage(SearchComponent.class.getPackage().getName()));
@@ -90,7 +90,7 @@ public class SolrInfoMBeanTest extends SolrTestCaseJ4
   }
   
   private static List<Class> getClassesForPackage(String pckgname) throws Exception {
-    ArrayList<File> directories = new ArrayList<File>();
+    ArrayList<File> directories = new ArrayList<>();
     ClassLoader cld = h.getCore().getResourceLoader().getClassLoader();
     String path = pckgname.replace('.', '/');
     Enumeration<URL> resources = cld.getResources(path);
@@ -102,7 +102,7 @@ public class SolrInfoMBeanTest extends SolrTestCaseJ4
       directories.add(f);
     }
       
-    ArrayList<Class> classes = new ArrayList<Class>();
+    ArrayList<Class> classes = new ArrayList<>();
     for (File directory : directories) {
       if (directory.exists()) {
         String[] files = directory.list();
diff --git solr/core/src/test/org/apache/solr/TestDistributedSearch.java solr/core/src/test/org/apache/solr/TestDistributedSearch.java
index 5b0a692..c583102 100644
--- solr/core/src/test/org/apache/solr/TestDistributedSearch.java
+++ solr/core/src/test/org/apache/solr/TestDistributedSearch.java
@@ -345,10 +345,10 @@ public class TestDistributedSearch extends BaseDistributedSearchTestCase {
     // test shards.tolerant=true
     for(int numDownServers = 0; numDownServers < jettys.size()-1; numDownServers++)
     {
-      List<JettySolrRunner> upJettys = new ArrayList<JettySolrRunner>(jettys);
-      List<SolrServer> upClients = new ArrayList<SolrServer>(clients);
-      List<JettySolrRunner> downJettys = new ArrayList<JettySolrRunner>();
-      List<String> upShards = new ArrayList<String>(Arrays.asList(shardsArr));
+      List<JettySolrRunner> upJettys = new ArrayList<>(jettys);
+      List<SolrServer> upClients = new ArrayList<>(clients);
+      List<JettySolrRunner> downJettys = new ArrayList<>();
+      List<String> upShards = new ArrayList<>(Arrays.asList(shardsArr));
       for(int i=0; i<numDownServers; i++)
       {
         // shut down some of the jettys
diff --git solr/core/src/test/org/apache/solr/TestDocumentBuilder.java solr/core/src/test/org/apache/solr/TestDocumentBuilder.java
index e8b5a3f..00a7906 100644
--- solr/core/src/test/org/apache/solr/TestDocumentBuilder.java
+++ solr/core/src/test/org/apache/solr/TestDocumentBuilder.java
@@ -38,7 +38,7 @@ public class TestDocumentBuilder extends LuceneTestCase {
     doc.addField("field2", "value1");
     doc.addField("field3", "value2");
     doc.addField("field4", 15);
-    List<Integer> list = new ArrayList<Integer>();
+    List<Integer> list = new ArrayList<>();
     list.add(45);
     list.add(33);
     list.add(20);
diff --git solr/core/src/test/org/apache/solr/TestGroupingSearch.java solr/core/src/test/org/apache/solr/TestGroupingSearch.java
index d1446a8..e45259e 100644
--- solr/core/src/test/org/apache/solr/TestGroupingSearch.java
+++ solr/core/src/test/org/apache/solr/TestGroupingSearch.java
@@ -675,7 +675,7 @@ public class TestGroupingSearch extends SolrTestCaseJ4 {
 
       int indexSize = random().nextInt(25 * RANDOM_MULTIPLIER);
 //indexSize=2;
-      List<FldType> types = new ArrayList<FldType>();
+      List<FldType> types = new ArrayList<>();
       types.add(new FldType("id",ONE_ONE, new SVal('A','Z',4,4)));
       types.add(new FldType("score_f",ONE_ONE, new FVal(1,100)));  // field used to score
       types.add(new FldType("foo_i",ZERO_ONE, new IRange(0,indexSize)));
@@ -774,14 +774,14 @@ public class TestGroupingSearch extends SolrTestCaseJ4 {
           for (Grp grp : groups.values()) grp.setMaxDoc(sortComparator); 
         }
 
-        List<Grp> sortedGroups = new ArrayList<Grp>(groups.values());
+        List<Grp> sortedGroups = new ArrayList<>(groups.values());
         Collections.sort(sortedGroups,  groupComparator==sortComparator ? createFirstDocComparator(sortComparator) : createMaxDocComparator(sortComparator));
 
         boolean includeNGroups = random().nextBoolean();
         Object modelResponse = buildGroupedResult(schema, sortedGroups, start, rows, group_offset, group_limit, includeNGroups);
 
         boolean truncateGroups = random().nextBoolean();
-        Map<String, Integer> facetCounts = new TreeMap<String, Integer>();
+        Map<String, Integer> facetCounts = new TreeMap<>();
         if (truncateGroups) {
           for (Grp grp : sortedGroups) {
             Doc doc = grp.docs.get(0);
@@ -808,7 +808,7 @@ public class TestGroupingSearch extends SolrTestCaseJ4 {
             }
           }
         }
-        List<Comparable> expectedFacetResponse = new ArrayList<Comparable>();
+        List<Comparable> expectedFacetResponse = new ArrayList<>();
         for (Map.Entry<String, Integer> stringIntegerEntry : facetCounts.entrySet()) {
           expectedFacetResponse.add(stringIntegerEntry.getKey());
           expectedFacetResponse.add(stringIntegerEntry.getValue());
@@ -862,7 +862,7 @@ public class TestGroupingSearch extends SolrTestCaseJ4 {
   }
 
   public static Object buildGroupedResult(IndexSchema schema, List<Grp> sortedGroups, int start, int rows, int group_offset, int group_limit, boolean includeNGroups) {
-    Map<String,Object> result = new LinkedHashMap<String,Object>();
+    Map<String,Object> result = new LinkedHashMap<>();
 
     long matches = 0;
     for (Grp grp : sortedGroups) {
@@ -877,13 +877,13 @@ public class TestGroupingSearch extends SolrTestCaseJ4 {
 
     for (int i=start; i<sortedGroups.size(); i++) {
       if (rows != -1 && groupList.size() >= rows) break;  // directly test rather than calculating, so we can catch any calc errors in the real code
-      Map<String,Object> group = new LinkedHashMap<String,Object>();
+      Map<String,Object> group = new LinkedHashMap<>();
       groupList.add(group);
 
       Grp grp = sortedGroups.get(i);
       group.put("groupValue", grp.groupValue);
 
-      Map<String,Object> resultSet = new LinkedHashMap<String,Object>();
+      Map<String,Object> resultSet = new LinkedHashMap<>();
       group.put("doclist", resultSet);
       resultSet.put("numFound", grp.docs.size());
       resultSet.put("start", group_offset);
@@ -924,7 +924,7 @@ public class TestGroupingSearch extends SolrTestCaseJ4 {
   }
 
   public static Map<Comparable, Grp> groupBy(Collection<Doc> docs, String field) {
-    Map<Comparable, Grp> groups = new HashMap<Comparable, Grp>();
+    Map<Comparable, Grp> groups = new HashMap<>();
     for (Doc doc : docs) {
       List<Comparable> vals = doc.getValues(field);
       if (vals == null) {
@@ -932,7 +932,7 @@ public class TestGroupingSearch extends SolrTestCaseJ4 {
         if (grp == null) {
           grp = new Grp();
           grp.groupValue = null;
-          grp.docs = new ArrayList<Doc>();
+          grp.docs = new ArrayList<>();
           groups.put(null, grp);
         }
         grp.docs.add(doc);
@@ -943,7 +943,7 @@ public class TestGroupingSearch extends SolrTestCaseJ4 {
           if (grp == null) {
             grp = new Grp();
             grp.groupValue = val;
-            grp.docs = new ArrayList<Doc>();
+            grp.docs = new ArrayList<>();
             groups.put(grp.groupValue, grp);
           }
           grp.docs.add(doc);
diff --git solr/core/src/test/org/apache/solr/TestJoin.java solr/core/src/test/org/apache/solr/TestJoin.java
index 03be32c..f62ffe0 100644
--- solr/core/src/test/org/apache/solr/TestJoin.java
+++ solr/core/src/test/org/apache/solr/TestJoin.java
@@ -150,7 +150,7 @@ public class TestJoin extends SolrTestCaseJ4 {
     while (--indexIter >= 0) {
       int indexSize = random().nextInt(20 * RANDOM_MULTIPLIER);
 
-      List<FldType> types = new ArrayList<FldType>();
+      List<FldType> types = new ArrayList<>();
       types.add(new FldType("id",ONE_ONE, new SVal('A','Z',4,4)));
       types.add(new FldType("score_f",ONE_ONE, new FVal(1,100)));  // field used to score
       types.add(new FldType("small_s",ZERO_ONE, new SVal('a',(char)('c'+indexSize/3),1,1)));
@@ -164,7 +164,7 @@ public class TestJoin extends SolrTestCaseJ4 {
 
       clearIndex();
       Map<Comparable, Doc> model = indexDocs(types, null, indexSize);
-      Map<String, Map<Comparable, Set<Comparable>>> pivots = new HashMap<String, Map<Comparable, Set<Comparable>>>();
+      Map<String, Map<Comparable, Set<Comparable>>> pivots = new HashMap<>();
 
       for (int qiter=0; qiter<queryIter; qiter++) {
         String fromField;
@@ -189,7 +189,7 @@ public class TestJoin extends SolrTestCaseJ4 {
 
         Collection<Doc> fromDocs = model.values();
         Set<Comparable> docs = join(fromDocs, pivot);
-        List<Doc> docList = new ArrayList<Doc>(docs.size());
+        List<Doc> docList = new ArrayList<>(docs.size());
         for (Comparable id : docs) docList.add(model.get(id));
         Collections.sort(docList, createComparator("_docid_",true,false,false,false));
         List sortedDocs = new ArrayList();
@@ -198,7 +198,7 @@ public class TestJoin extends SolrTestCaseJ4 {
           sortedDocs.add(doc.toObject(h.getCore().getLatestSchema()));
         }
 
-        Map<String,Object> resultSet = new LinkedHashMap<String,Object>();
+        Map<String,Object> resultSet = new LinkedHashMap<>();
         resultSet.put("numFound", docList.size());
         resultSet.put("start", 0);
         resultSet.put("docs", sortedDocs);
@@ -235,7 +235,7 @@ public class TestJoin extends SolrTestCaseJ4 {
 
 
   Map<Comparable, Set<Comparable>> createJoinMap(Map<Comparable, Doc> model, String fromField, String toField) {
-    Map<Comparable, Set<Comparable>> id_to_id = new HashMap<Comparable, Set<Comparable>>();
+    Map<Comparable, Set<Comparable>> id_to_id = new HashMap<>();
 
     Map<Comparable, List<Comparable>> value_to_id = invertField(model, toField);
 
@@ -248,7 +248,7 @@ public class TestJoin extends SolrTestCaseJ4 {
         if (toIds == null) continue;
         Set<Comparable> ids = id_to_id.get(fromId);
         if (ids == null) {
-          ids = new HashSet<Comparable>();
+          ids = new HashSet<>();
           id_to_id.put(fromId, ids);
         }
         for (Comparable toId : toIds)
@@ -261,7 +261,7 @@ public class TestJoin extends SolrTestCaseJ4 {
 
 
   Set<Comparable> join(Collection<Doc> input, Map<Comparable, Set<Comparable>> joinMap) {
-    Set<Comparable> ids = new HashSet<Comparable>();
+    Set<Comparable> ids = new HashSet<>();
     for (Doc doc : input) {
       Collection<Comparable> output = joinMap.get(doc.id);
       if (output == null) continue;
diff --git solr/core/src/test/org/apache/solr/TestRandomDVFaceting.java solr/core/src/test/org/apache/solr/TestRandomDVFaceting.java
index 9f12744..53334b3 100644
--- solr/core/src/test/org/apache/solr/TestRandomDVFaceting.java
+++ solr/core/src/test/org/apache/solr/TestRandomDVFaceting.java
@@ -58,7 +58,7 @@ public class TestRandomDVFaceting extends SolrTestCaseJ4 {
     model = null;
     indexSize = rand.nextBoolean() ? (rand.nextInt(10) + 1) : (rand.nextInt(100) + 10);
 
-    types = new ArrayList<FldType>();
+    types = new ArrayList<>();
     types.add(new FldType("id",ONE_ONE, new SVal('A','Z',4,4)));
     types.add(new FldType("score_f",ONE_ONE, new FVal(1,100)));
     types.add(new FldType("foo_i",ZERO_ONE, new IRange(0,indexSize)));
@@ -88,7 +88,7 @@ public class TestRandomDVFaceting extends SolrTestCaseJ4 {
     Random rand = random();
     int percent = rand.nextInt(100);
     if (model == null) return;
-    ArrayList<String> ids = new ArrayList<String>(model.size());
+    ArrayList<String> ids = new ArrayList<>(model.size());
     for (Comparable id : model.keySet()) {
       if (rand.nextInt(100) < percent) {
         ids.add(id.toString());
@@ -216,7 +216,7 @@ public class TestRandomDVFaceting extends SolrTestCaseJ4 {
       String facet_field = ftype.fname;
 
       List<String> methods = multiValued ? multiValuedMethods : singleValuedMethods;
-      List<String> responses = new ArrayList<String>(methods.size());
+      List<String> responses = new ArrayList<>(methods.size());
       for (String method : methods) {
         if (method.equals("dv")) {
           params.set("facet.field", "{!key="+facet_field+"}"+facet_field+"_dv");
diff --git solr/core/src/test/org/apache/solr/TestRandomFaceting.java solr/core/src/test/org/apache/solr/TestRandomFaceting.java
index 6ee53a9..f2d4cd3 100644
--- solr/core/src/test/org/apache/solr/TestRandomFaceting.java
+++ solr/core/src/test/org/apache/solr/TestRandomFaceting.java
@@ -53,7 +53,7 @@ public class TestRandomFaceting extends SolrTestCaseJ4 {
     model = null;
     indexSize = rand.nextBoolean() ? (rand.nextInt(10) + 1) : (rand.nextInt(100) + 10);
 
-    types = new ArrayList<FldType>();
+    types = new ArrayList<>();
     types.add(new FldType("id",ONE_ONE, new SVal('A','Z',4,4)));
     types.add(new FldType("score_f",ONE_ONE, new FVal(1,100)));
     types.add(new FldType("small_f",ONE_ONE, new FVal(-4,5)));
@@ -87,7 +87,7 @@ public class TestRandomFaceting extends SolrTestCaseJ4 {
     Random rand = random();
     int percent = rand.nextInt(100);
     if (model == null) return;
-    ArrayList<String> ids = new ArrayList<String>(model.size());
+    ArrayList<String> ids = new ArrayList<>(model.size());
     for (Comparable id : model.keySet()) {
       if (rand.nextInt(100) < percent) {
         ids.add(id.toString());
@@ -209,7 +209,7 @@ public class TestRandomFaceting extends SolrTestCaseJ4 {
       params.set("facet.field", facet_field);
 
       List<String> methods = multiValued ? multiValuedMethods : singleValuedMethods;
-      List<String> responses = new ArrayList<String>(methods.size());
+      List<String> responses = new ArrayList<>(methods.size());
       for (String method : methods) {
         // params.add("facet.field", "{!key="+method+"}" + ftype.fname);
         // TODO: allow method to be passed on local params?
diff --git solr/core/src/test/org/apache/solr/analysis/LegacyHTMLStripCharFilterTest.java solr/core/src/test/org/apache/solr/analysis/LegacyHTMLStripCharFilterTest.java
index 7579acc..10cf247 100644
--- solr/core/src/test/org/apache/solr/analysis/LegacyHTMLStripCharFilterTest.java
+++ solr/core/src/test/org/apache/solr/analysis/LegacyHTMLStripCharFilterTest.java
@@ -80,7 +80,7 @@ public class LegacyHTMLStripCharFilterTest extends BaseTokenStreamTestCase {
   public void testGamma() throws Exception {
     String test = "&Gamma;";
     String gold = "\u0393";
-    Set<String> set = new HashSet<String>();
+    Set<String> set = new HashSet<>();
     set.add("reserved");
     Reader reader = new LegacyHTMLStripCharFilter(new StringReader(test), set);
     StringBuilder builder = new StringBuilder();
@@ -97,7 +97,7 @@ public class LegacyHTMLStripCharFilterTest extends BaseTokenStreamTestCase {
   public void testEntities() throws Exception {
     String test = "&nbsp; &lt;foo&gt; &Uuml;bermensch &#61; &Gamma; bar &#x393;";
     String gold = "  <foo> \u00DCbermensch = \u0393 bar \u0393";
-    Set<String> set = new HashSet<String>();
+    Set<String> set = new HashSet<>();
     set.add("reserved");
     Reader reader = new LegacyHTMLStripCharFilter(new StringReader(test), set);
     StringBuilder builder = new StringBuilder();
@@ -114,7 +114,7 @@ public class LegacyHTMLStripCharFilterTest extends BaseTokenStreamTestCase {
   public void testMoreEntities() throws Exception {
     String test = "&nbsp; &lt;junk/&gt; &nbsp; &#33; &#64; and &#8217;";
     String gold = "  <junk/>   ! @ and ’";
-    Set<String> set = new HashSet<String>();
+    Set<String> set = new HashSet<>();
     set.add("reserved");
     Reader reader = new LegacyHTMLStripCharFilter(new StringReader(test), set);
     StringBuilder builder = new StringBuilder();
@@ -130,7 +130,7 @@ public class LegacyHTMLStripCharFilterTest extends BaseTokenStreamTestCase {
 
   public void testReserved() throws Exception {
     String test = "aaa bbb <reserved ccc=\"ddddd\"> eeee </reserved> ffff <reserved ggg=\"hhhh\"/> <other/>";
-    Set<String> set = new HashSet<String>();
+    Set<String> set = new HashSet<>();
     set.add("reserved");
     Reader reader = new LegacyHTMLStripCharFilter(new StringReader(test), set);
     StringBuilder builder = new StringBuilder();
diff --git solr/core/src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory.java solr/core/src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory.java
index bb0709a..181dd68 100644
--- solr/core/src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory.java
+++ solr/core/src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory.java
@@ -44,7 +44,7 @@ import org.junit.Test;
 import static org.apache.lucene.analysis.BaseTokenStreamTestCase.*;
 
 public class TestReversedWildcardFilterFactory extends SolrTestCaseJ4 {
-  Map<String,String> args = new HashMap<String, String>();
+  Map<String,String> args = new HashMap<>();
   IndexSchema schema;
 
   @BeforeClass
diff --git solr/core/src/test/org/apache/solr/analysis/TestWordDelimiterFilterFactory.java solr/core/src/test/org/apache/solr/analysis/TestWordDelimiterFilterFactory.java
index 6502eda..e71b8c8 100644
--- solr/core/src/test/org/apache/solr/analysis/TestWordDelimiterFilterFactory.java
+++ solr/core/src/test/org/apache/solr/analysis/TestWordDelimiterFilterFactory.java
@@ -199,7 +199,7 @@ public class TestWordDelimiterFilterFactory extends SolrTestCaseJ4 {
   public void testCustomTypes() throws Exception {
     String testText = "I borrowed $5,400.00 at 25% interest-rate";
     ResourceLoader loader = new SolrResourceLoader("solr/collection1");
-    Map<String,String> args = new HashMap<String,String>();
+    Map<String,String> args = new HashMap<>();
     args.put("generateWordParts", "1");
     args.put("generateNumberParts", "1");
     args.put("catenateWords", "1");
@@ -221,7 +221,7 @@ public class TestWordDelimiterFilterFactory extends SolrTestCaseJ4 {
 
     
     /* custom behavior */
-    args = new HashMap<String,String>();
+    args = new HashMap<>();
     // use a custom type mapping
     args.put("generateWordParts", "1");
     args.put("generateNumberParts", "1");
diff --git solr/core/src/test/org/apache/solr/analytics/AbstractAnalyticsStatsTest.java solr/core/src/test/org/apache/solr/analytics/AbstractAnalyticsStatsTest.java
index f68dbdb..7be2c33 100644
--- solr/core/src/test/org/apache/solr/analytics/AbstractAnalyticsStatsTest.java
+++ solr/core/src/test/org/apache/solr/analytics/AbstractAnalyticsStatsTest.java
@@ -51,7 +51,7 @@ import com.google.common.collect.ObjectArrays;
 public class AbstractAnalyticsStatsTest extends SolrTestCaseJ4 {
   
   protected static final String[] BASEPARMS = new String[]{ "q", "*:*", "indent", "true", "olap", "true", "rows", "0" };
-  protected static final HashMap<String,Object> defaults = new HashMap<String,Object>();
+  protected static final HashMap<String,Object> defaults = new HashMap<>();
 
   public static enum VAL_TYPE {
     INTEGER("int"),
@@ -162,7 +162,7 @@ public class AbstractAnalyticsStatsTest extends SolrTestCaseJ4 {
     } else if (stat.equals("count")) {
       result = Long.valueOf(list.size());
     } else if (stat.equals("unique")) {
-      HashSet<T> set = new HashSet<T>();
+      HashSet<T> set = new HashSet<>();
       set.addAll(list);
       result = Long.valueOf((long)set.size());
     } else if (stat.equals("max")) {
@@ -198,7 +198,7 @@ public class AbstractAnalyticsStatsTest extends SolrTestCaseJ4 {
     if (in == null) throw new FileNotFoundException("Resource not found: " + fileName);
     Scanner file = new Scanner(in, "UTF-8");
     try { 
-      ArrayList<String> strList = new ArrayList<String>();
+      ArrayList<String> strList = new ArrayList<>();
       while (file.hasNextLine()) {
         String line = file.nextLine();
         line = line.trim();
diff --git solr/core/src/test/org/apache/solr/analytics/NoFacetTest.java solr/core/src/test/org/apache/solr/analytics/NoFacetTest.java
index 9193cc5..a18e9d6 100644
--- solr/core/src/test/org/apache/solr/analytics/NoFacetTest.java
+++ solr/core/src/test/org/apache/solr/analytics/NoFacetTest.java
@@ -72,12 +72,12 @@ public class NoFacetTest extends AbstractAnalyticsStatsTest {
     defaults.put("date_dtd", "1800-12-31T23:59:59Z");
     defaults.put("string_sd", "str0");
     
-    intTestStart = new ArrayList<Integer>(); 
-    longTestStart = new ArrayList<Long>(); 
-    floatTestStart = new ArrayList<Float>(); 
-    doubleTestStart = new ArrayList<Double>(); 
-    dateTestStart = new ArrayList<String>(); 
-    stringTestStart = new ArrayList<String>(); 
+    intTestStart = new ArrayList<>();
+    longTestStart = new ArrayList<>();
+    floatTestStart = new ArrayList<>();
+    doubleTestStart = new ArrayList<>();
+    dateTestStart = new ArrayList<>();
+    stringTestStart = new ArrayList<>();
     
     for (int j = 0; j < NUM_LOOPS; ++j) {
       int i = j%INT;
@@ -86,7 +86,7 @@ public class NoFacetTest extends AbstractAnalyticsStatsTest {
       double d = j%DOUBLE;
       String dt = (1800+j%DATE) + "-12-31T23:59:59Z";
       String s = "str" + (j%STRING);
-      List<String> fields = new ArrayList<String>();
+      List<String> fields = new ArrayList<>();
       fields.add("id"); fields.add("1000"+j);
       
       if( i != 0 ){
diff --git solr/core/src/test/org/apache/solr/analytics/expression/ExpressionTest.java solr/core/src/test/org/apache/solr/analytics/expression/ExpressionTest.java
index e0d49a7..43c5f50 100644
--- solr/core/src/test/org/apache/solr/analytics/expression/ExpressionTest.java
+++ solr/core/src/test/org/apache/solr/analytics/expression/ExpressionTest.java
@@ -237,7 +237,7 @@ public class ExpressionTest extends AbstractAnalyticsStatsTest {
     if (in == null) throw new FileNotFoundException("Resource not found: " + fileName);
     Scanner file = new Scanner(in, "UTF-8");
     try { 
-      ArrayList<String> strList = new ArrayList<String>();
+      ArrayList<String> strList = new ArrayList<>();
       while (file.hasNextLine()) {
         String line = file.nextLine();
         if (line.length()<2) {
diff --git solr/core/src/test/org/apache/solr/analytics/facet/AbstractAnalyticsFacetTest.java solr/core/src/test/org/apache/solr/analytics/facet/AbstractAnalyticsFacetTest.java
index 51e7226..820a1c5 100644
--- solr/core/src/test/org/apache/solr/analytics/facet/AbstractAnalyticsFacetTest.java
+++ solr/core/src/test/org/apache/solr/analytics/facet/AbstractAnalyticsFacetTest.java
@@ -53,7 +53,7 @@ import javax.xml.xpath.XPathFactory;
 
 @SuppressCodecs({"Lucene3x","Lucene40","Lucene41","Lucene42","Appending","Asserting"})
 public class AbstractAnalyticsFacetTest extends SolrTestCaseJ4 {
-  protected static final HashMap<String,Object> defaults = new HashMap<String,Object>();
+  protected static final HashMap<String,Object> defaults = new HashMap<>();
   
   protected String latestType = "";
 
@@ -88,7 +88,7 @@ public class AbstractAnalyticsFacetTest extends SolrTestCaseJ4 {
   }
   protected ArrayList<String> getStringList(String n1, String n2, String n3, String element, String n4)
       throws XPathExpressionException {
-    ArrayList<String> ret = new ArrayList<String>();
+    ArrayList<String> ret = new ArrayList<>();
     NodeList nodes = getNodes(n1, n2, n3, element, n4);
     for (int idx = 0; idx < nodes.getLength(); ++idx) {
       ret.add(nodes.item(idx).getTextContent());
@@ -98,7 +98,7 @@ public class AbstractAnalyticsFacetTest extends SolrTestCaseJ4 {
 
   protected ArrayList<Integer> getIntegerList(String n1, String n2, String n3, String element, String n4)
       throws XPathExpressionException {
-    ArrayList<Integer> ret = new ArrayList<Integer>();
+    ArrayList<Integer> ret = new ArrayList<>();
     NodeList nodes = getNodes(n1, n2, n3, element, n4);
     for (int idx = 0; idx < nodes.getLength(); ++idx) {
       ret.add(Integer.parseInt(nodes.item(idx).getTextContent()));
@@ -107,7 +107,7 @@ public class AbstractAnalyticsFacetTest extends SolrTestCaseJ4 {
   }
   protected ArrayList<Long> getLongList(String n1, String n2, String n3, String element, String n4)
       throws XPathExpressionException {
-    ArrayList<Long> ret = new ArrayList<Long>();
+    ArrayList<Long> ret = new ArrayList<>();
     NodeList nodes = getNodes(n1, n2, n3, element, n4);
     for (int idx = 0; idx < nodes.getLength(); ++idx) {
       ret.add(Long.parseLong(nodes.item(idx).getTextContent()));
@@ -116,7 +116,7 @@ public class AbstractAnalyticsFacetTest extends SolrTestCaseJ4 {
   }
   protected ArrayList<Float> getFloatList(String n1, String n2, String n3, String element, String n4)
       throws XPathExpressionException {
-    ArrayList<Float> ret = new ArrayList<Float>();
+    ArrayList<Float> ret = new ArrayList<>();
     NodeList nodes = getNodes(n1, n2, n3, element, n4);
     for (int idx = 0; idx < nodes.getLength(); ++idx) {
       ret.add(Float.parseFloat(nodes.item(idx).getTextContent()));
@@ -126,7 +126,7 @@ public class AbstractAnalyticsFacetTest extends SolrTestCaseJ4 {
 
   protected ArrayList<Double> getDoubleList(String n1, String n2, String n3, String element, String n4)
       throws XPathExpressionException {
-    ArrayList<Double> ret = new ArrayList<Double>();
+    ArrayList<Double> ret = new ArrayList<>();
     NodeList nodes = getNodes(n1, n2, n3, element, n4);
     for (int idx = 0; idx < nodes.getLength(); ++idx) {
       ret.add(Double.parseDouble(nodes.item(idx).getTextContent()));
@@ -141,7 +141,7 @@ public class AbstractAnalyticsFacetTest extends SolrTestCaseJ4 {
   }
   
   public static String[] filter(String...args){
-    List<String> l = new ArrayList<String>();
+    List<String> l = new ArrayList<>();
     for( int i=0; i <args.length; i+=2){
       if( args[i+1].equals("0") || args[i+1].equals("0.0") || 
           args[i+1].equals("1800-12-31T23:59:59Z") || args[i+1].equals("str0") ||
@@ -237,7 +237,7 @@ public class AbstractAnalyticsFacetTest extends SolrTestCaseJ4 {
     } else if (stat.equals("unique")) {
       result = new ArrayList<Long>();
       for (List<T> list : lists) {
-        HashSet<T> set = new HashSet<T>();
+        HashSet<T> set = new HashSet<>();
         set.addAll(list);
         result.add((long)set.size());
       }
@@ -285,7 +285,7 @@ public class AbstractAnalyticsFacetTest extends SolrTestCaseJ4 {
     if (in == null) throw new FileNotFoundException("Resource not found: " + fileName);
     Scanner file = new Scanner(in, "UTF-8");
     try { 
-      ArrayList<String> strList = new ArrayList<String>();
+      ArrayList<String> strList = new ArrayList<>();
       while (file.hasNextLine()) {
         String line = file.nextLine();
         if (line.length()<2) {
diff --git solr/core/src/test/org/apache/solr/analytics/facet/FieldFacetExtrasTest.java solr/core/src/test/org/apache/solr/analytics/facet/FieldFacetExtrasTest.java
index 998be47..2e0b620 100644
--- solr/core/src/test/org/apache/solr/analytics/facet/FieldFacetExtrasTest.java
+++ solr/core/src/test/org/apache/solr/analytics/facet/FieldFacetExtrasTest.java
@@ -51,10 +51,10 @@ public class FieldFacetExtrasTest extends AbstractAnalyticsFacetTest {
     h.update("<delete><query>*:*</query></delete>");
 
     //INT
-    intLongTestStart = new ArrayList<ArrayList<Integer>>(); 
-    intFloatTestStart = new ArrayList<ArrayList<Integer>>(); 
-    intDoubleTestStart = new ArrayList<ArrayList<Integer>>(); 
-    intStringTestStart = new ArrayList<ArrayList<Integer>>(); 
+    intLongTestStart = new ArrayList<>();
+    intFloatTestStart = new ArrayList<>();
+    intDoubleTestStart = new ArrayList<>();
+    intStringTestStart = new ArrayList<>();
 
     for (int j = 0; j < NUM_LOOPS; ++j) {
       int i = j%INT;
@@ -67,7 +67,7 @@ public class FieldFacetExtrasTest extends AbstractAnalyticsFacetTest {
           "double_dd", "" + d,  "date_dtd", (1800+dt) + "-12-31T23:59:59.999Z", "string_sd", "abc" + s));
       //Long
       if (j-LONG<0) {
-        ArrayList<Integer> list1 = new ArrayList<Integer>();
+        ArrayList<Integer> list1 = new ArrayList<>();
         list1.add(i);
         intLongTestStart.add(list1);
       } else {
@@ -75,7 +75,7 @@ public class FieldFacetExtrasTest extends AbstractAnalyticsFacetTest {
       }
       //String
       if (j-FLOAT<0) {
-        ArrayList<Integer> list1 = new ArrayList<Integer>();
+        ArrayList<Integer> list1 = new ArrayList<>();
         list1.add(i);
         intFloatTestStart.add(list1);
       } else {
@@ -83,7 +83,7 @@ public class FieldFacetExtrasTest extends AbstractAnalyticsFacetTest {
       }
       //String
       if (j-DOUBLE<0) {
-        ArrayList<Integer> list1 = new ArrayList<Integer>();
+        ArrayList<Integer> list1 = new ArrayList<>();
         list1.add(i);
         intDoubleTestStart.add(list1);
       } else {
@@ -91,7 +91,7 @@ public class FieldFacetExtrasTest extends AbstractAnalyticsFacetTest {
       }
       //String
       if (j-STRING<0) {
-        ArrayList<Integer> list1 = new ArrayList<Integer>();
+        ArrayList<Integer> list1 = new ArrayList<>();
         list1.add(i);
         intStringTestStart.add(list1);
       } else {
@@ -127,7 +127,7 @@ public class FieldFacetExtrasTest extends AbstractAnalyticsFacetTest {
 
     Collection<Double> lon;
    
-    List<Double> all = new ArrayList<Double>();
+    List<Double> all = new ArrayList<>();
     lon = getDoubleList("off0", "fieldFacets", "long_ld", "double", "mean");
     assertEquals(getRawResponse(), lon.size(),2);
     assertArrayEquals(new Double[]{ 1.5,  2.0 }, lon.toArray(new Double[0]));
diff --git solr/core/src/test/org/apache/solr/analytics/facet/FieldFacetTest.java solr/core/src/test/org/apache/solr/analytics/facet/FieldFacetTest.java
index 18f8301..0c28fa5 100644
--- solr/core/src/test/org/apache/solr/analytics/facet/FieldFacetTest.java
+++ solr/core/src/test/org/apache/solr/analytics/facet/FieldFacetTest.java
@@ -101,48 +101,48 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
     defaults.put("string", "str0");
 
     //INT
-    intDateTestStart = new ArrayList<ArrayList<Integer>>(); 
-    intDateTestMissing = new ArrayList<Long>(); 
-    intStringTestStart = new ArrayList<ArrayList<Integer>>(); 
-    intStringTestMissing = new ArrayList<Long>(); 
+    intDateTestStart = new ArrayList<>();
+    intDateTestMissing = new ArrayList<>();
+    intStringTestStart = new ArrayList<>();
+    intStringTestMissing = new ArrayList<>();
     
     //LONG
-    longDateTestStart = new ArrayList<ArrayList<Long>>(); 
-    longDateTestMissing = new ArrayList<Long>(); 
-    longStringTestStart = new ArrayList<ArrayList<Long>>(); 
-    longStringTestMissing = new ArrayList<Long>(); 
+    longDateTestStart = new ArrayList<>();
+    longDateTestMissing = new ArrayList<>();
+    longStringTestStart = new ArrayList<>();
+    longStringTestMissing = new ArrayList<>();
     
     //FLOAT
-    floatDateTestStart = new ArrayList<ArrayList<Float>>(); 
-    floatDateTestMissing = new ArrayList<Long>(); 
-    floatStringTestStart = new ArrayList<ArrayList<Float>>(); 
-    floatStringTestMissing = new ArrayList<Long>(); 
+    floatDateTestStart = new ArrayList<>();
+    floatDateTestMissing = new ArrayList<>();
+    floatStringTestStart = new ArrayList<>();
+    floatStringTestMissing = new ArrayList<>();
     
     //DOUBLE
-    doubleDateTestStart = new ArrayList<ArrayList<Double>>(); 
-    doubleDateTestMissing = new ArrayList<Long>(); 
-    doubleStringTestStart = new ArrayList<ArrayList<Double>>(); 
-    doubleStringTestMissing = new ArrayList<Long>(); 
+    doubleDateTestStart = new ArrayList<>();
+    doubleDateTestMissing = new ArrayList<>();
+    doubleStringTestStart = new ArrayList<>();
+    doubleStringTestMissing = new ArrayList<>();
     
     //DATE
-    dateIntTestStart = new ArrayList<ArrayList<String>>(); 
-    dateIntTestMissing = new ArrayList<Long>(); 
-    dateLongTestStart = new ArrayList<ArrayList<String>>(); 
-    dateLongTestMissing = new ArrayList<Long>(); 
+    dateIntTestStart = new ArrayList<>();
+    dateIntTestMissing = new ArrayList<>();
+    dateLongTestStart = new ArrayList<>();
+    dateLongTestMissing = new ArrayList<>();
     
     //String
-    stringIntTestStart = new ArrayList<ArrayList<String>>(); 
-    stringIntTestMissing = new ArrayList<Long>(); 
-    stringLongTestStart = new ArrayList<ArrayList<String>>(); 
-    stringLongTestMissing = new ArrayList<Long>(); 
+    stringIntTestStart = new ArrayList<>();
+    stringIntTestMissing = new ArrayList<>();
+    stringLongTestStart = new ArrayList<>();
+    stringLongTestMissing = new ArrayList<>();
     
     //Multi-Valued
-    multiLongTestStart = new ArrayList<ArrayList<Integer>>(); 
-    multiLongTestMissing = new ArrayList<Long>(); 
-    multiStringTestStart = new ArrayList<ArrayList<Integer>>(); 
-    multiStringTestMissing = new ArrayList<Long>(); 
-    multiDateTestStart = new ArrayList<ArrayList<Integer>>(); 
-    multiDateTestMissing = new ArrayList<Long>(); 
+    multiLongTestStart = new ArrayList<>();
+    multiLongTestMissing = new ArrayList<>();
+    multiStringTestStart = new ArrayList<>();
+    multiStringTestMissing = new ArrayList<>();
+    multiDateTestStart = new ArrayList<>();
+    multiDateTestMissing = new ArrayList<>();
 
     for (int j = 0; j < NUM_LOOPS; ++j) {
       int i = j%INT;
@@ -178,7 +178,7 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
       if( dt != 0 ){
         //Dates
         if (j-DATE<0) {
-          ArrayList<Integer> list1 = new ArrayList<Integer>();
+          ArrayList<Integer> list1 = new ArrayList<>();
           if( i != 0 ){
             list1.add(i);
             intDateTestMissing.add(0l);
@@ -186,7 +186,7 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
             intDateTestMissing.add(1l);
           }
           intDateTestStart.add(list1);
-          ArrayList<Long> list2 = new ArrayList<Long>();
+          ArrayList<Long> list2 = new ArrayList<>();
           if( l != 0l ){
             list2.add(l);
             longDateTestMissing.add(0l);
@@ -194,7 +194,7 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
             longDateTestMissing.add(1l);
           }
           longDateTestStart.add(list2);
-          ArrayList<Float> list3 = new ArrayList<Float>();
+          ArrayList<Float> list3 = new ArrayList<>();
           if ( f != 0.0f ){
             list3.add(f);
             floatDateTestMissing.add(0l);
@@ -203,7 +203,7 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
             
           }
           floatDateTestStart.add(list3);
-          ArrayList<Double> list4 = new ArrayList<Double>();
+          ArrayList<Double> list4 = new ArrayList<>();
           if( d != 0.0d ){
             list4.add(d);
             doubleDateTestMissing.add(0l);
@@ -211,7 +211,7 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
             doubleDateTestMissing.add(1l);
           }
           doubleDateTestStart.add(list4);
-          ArrayList<Integer> list5 = new ArrayList<Integer>();
+          ArrayList<Integer> list5 = new ArrayList<>();
           if( i != 0 ){
             list5.add(i);
             multiDateTestMissing.add(0l);
@@ -230,7 +230,7 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
       }
       
       if (j-DATEM<0 && dtm!=dt && dtm!=0) {
-        ArrayList<Integer> list1 = new ArrayList<Integer>();
+        ArrayList<Integer> list1 = new ArrayList<>();
         if( i != 0 ){
           list1.add(i);
           multiDateTestMissing.add(0l);
@@ -245,7 +245,7 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
       if( s != 0 ){
         //Strings
         if (j-STRING<0) {
-          ArrayList<Integer> list1 = new ArrayList<Integer>();
+          ArrayList<Integer> list1 = new ArrayList<>();
           if( i != 0 ){
             list1.add(i);
             intStringTestMissing.add(0l);
@@ -253,7 +253,7 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
             intStringTestMissing.add(1l);
           }
           intStringTestStart.add(list1);
-          ArrayList<Long> list2 = new ArrayList<Long>();
+          ArrayList<Long> list2 = new ArrayList<>();
           if( l != 0l ){
             list2.add(l);
             longStringTestMissing.add(0l);
@@ -261,7 +261,7 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
             longStringTestMissing.add(1l);
           }
           longStringTestStart.add(list2);
-          ArrayList<Float> list3 = new ArrayList<Float>();
+          ArrayList<Float> list3 = new ArrayList<>();
           if( f != 0.0f ){
             list3.add(f);
             floatStringTestMissing.add(0l);
@@ -269,7 +269,7 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
             floatStringTestMissing.add(1l);
           }
           floatStringTestStart.add(list3);
-          ArrayList<Double> list4 = new ArrayList<Double>();
+          ArrayList<Double> list4 = new ArrayList<>();
           if( d != 0.0d ){
             list4.add(d);
             doubleStringTestMissing.add(0l);
@@ -277,7 +277,7 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
             doubleStringTestMissing.add(1l);
           }
           doubleStringTestStart.add(list4);
-          ArrayList<Integer> list5 = new ArrayList<Integer>();
+          ArrayList<Integer> list5 = new ArrayList<>();
           if( i != 0 ){
             list5.add(i);
             multiStringTestMissing.add(0l);
@@ -297,7 +297,7 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
       //Strings
       if( sm != 0 ){
         if (j-STRINGM<0&&sm!=s) {
-          ArrayList<Integer> list1 = new ArrayList<Integer>();
+          ArrayList<Integer> list1 = new ArrayList<>();
           if( i != 0 ){
             list1.add(i);
             multiStringTestMissing.add(0l);
@@ -313,7 +313,7 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
       //Int
       if( i != 0 ){
         if (j-INT<0) {
-          ArrayList<String> list1 = new ArrayList<String>();
+          ArrayList<String> list1 = new ArrayList<>();
           if( dt != 0 ){
             list1.add((1800+dt) + "-12-31T23:59:59Z");
             dateIntTestMissing.add(0l);
@@ -321,7 +321,7 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
             dateIntTestMissing.add(1l);
           }
           dateIntTestStart.add(list1);
-          ArrayList<String> list2 = new ArrayList<String>();
+          ArrayList<String> list2 = new ArrayList<>();
           if( s != 0 ){
             list2.add("str"+s);
             stringIntTestMissing.add(0l);
@@ -338,7 +338,7 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
       //Long
       if( l != 0 ){
         if (j-LONG<0) {
-          ArrayList<String> list1 = new ArrayList<String>();
+          ArrayList<String> list1 = new ArrayList<>();
           if( dt != 0 ){
             list1.add((1800+dt) + "-12-31T23:59:59Z");
             dateLongTestMissing.add(0l);
@@ -346,7 +346,7 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
             dateLongTestMissing.add(1l);
           }
           dateLongTestStart.add(list1);
-          ArrayList<String> list2 = new ArrayList<String>();
+          ArrayList<String> list2 = new ArrayList<>();
           if( s != 0 ){
             list2.add("str"+s);
             stringLongTestMissing.add(0l);
@@ -354,7 +354,7 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
             stringLongTestMissing.add(1l);
           }
           stringLongTestStart.add(list2);
-          ArrayList<Integer> list3 = new ArrayList<Integer>();
+          ArrayList<Integer> list3 = new ArrayList<>();
           if( i != 0 ){
             list3.add(i);
             multiLongTestMissing.add(0l);
@@ -372,7 +372,7 @@ public class FieldFacetTest extends AbstractAnalyticsFacetTest{
       //Long
       if( lm != 0 ){
         if (j-LONGM<0&&lm!=l) {
-          ArrayList<Integer> list1 = new ArrayList<Integer>();
+          ArrayList<Integer> list1 = new ArrayList<>();
           if( i != 0 ){
             list1.add(i);
             multiLongTestMissing.add(0l);
diff --git solr/core/src/test/org/apache/solr/analytics/facet/QueryFacetTest.java solr/core/src/test/org/apache/solr/analytics/facet/QueryFacetTest.java
index 583f2b1..4516c95 100644
--- solr/core/src/test/org/apache/solr/analytics/facet/QueryFacetTest.java
+++ solr/core/src/test/org/apache/solr/analytics/facet/QueryFacetTest.java
@@ -46,18 +46,18 @@ public class QueryFacetTest extends AbstractAnalyticsFacetTest {
   public void queryTest() throws Exception { 
     h.update("<delete><query>*:*</query></delete>");
     //INT
-    ArrayList<ArrayList<Integer>> int1TestStart = new ArrayList<ArrayList<Integer>>(); 
+    ArrayList<ArrayList<Integer>> int1TestStart = new ArrayList<>();
     int1TestStart.add(new ArrayList<Integer>());
-    ArrayList<ArrayList<Integer>> int2TestStart = new ArrayList<ArrayList<Integer>>(); 
+    ArrayList<ArrayList<Integer>> int2TestStart = new ArrayList<>();
     int2TestStart.add(new ArrayList<Integer>());
     
     //LONG
-    ArrayList<ArrayList<Long>> longTestStart = new ArrayList<ArrayList<Long>>(); 
+    ArrayList<ArrayList<Long>> longTestStart = new ArrayList<>();
     longTestStart.add(new ArrayList<Long>());
     longTestStart.add(new ArrayList<Long>());
     
     //FLOAT
-    ArrayList<ArrayList<Float>> floatTestStart = new ArrayList<ArrayList<Float>>(); 
+    ArrayList<ArrayList<Float>> floatTestStart = new ArrayList<>();
     floatTestStart.add(new ArrayList<Float>());
     floatTestStart.add(new ArrayList<Float>());
     floatTestStart.add(new ArrayList<Float>());
diff --git solr/core/src/test/org/apache/solr/analytics/facet/RangeFacetTest.java solr/core/src/test/org/apache/solr/analytics/facet/RangeFacetTest.java
index 6f162f0..d7477df 100644
--- solr/core/src/test/org/apache/solr/analytics/facet/RangeFacetTest.java
+++ solr/core/src/test/org/apache/solr/analytics/facet/RangeFacetTest.java
@@ -53,14 +53,14 @@ public class RangeFacetTest extends AbstractAnalyticsFacetTest {
     h.update("<delete><query>*:*</query></delete>");
     
     //INT
-    intLongTestStart = new ArrayList<ArrayList<Integer>>(); 
-    intDoubleTestStart = new ArrayList<ArrayList<Integer>>(); 
-    intDateTestStart = new ArrayList<ArrayList<Integer>>(); 
+    intLongTestStart = new ArrayList<>();
+    intDoubleTestStart = new ArrayList<>();
+    intDateTestStart = new ArrayList<>();
     
     //FLOAT
-    floatLongTestStart = new ArrayList<ArrayList<Float>>(); 
-    floatDoubleTestStart = new ArrayList<ArrayList<Float>>(); 
-    floatDateTestStart = new ArrayList<ArrayList<Float>>(); 
+    floatLongTestStart = new ArrayList<>();
+    floatDoubleTestStart = new ArrayList<>();
+    floatDateTestStart = new ArrayList<>();
     
     for (int j = 0; j < NUM_LOOPS; ++j) {
       int i = j%INT;
@@ -73,10 +73,10 @@ public class RangeFacetTest extends AbstractAnalyticsFacetTest {
           "double_dd", "" + d,  "date_dtd", (1000+dt) + "-01-01T23:59:59Z", "string_sd", "abc" + s));
       //Longs
       if (j-LONG<0) {
-        ArrayList<Integer> list1 = new ArrayList<Integer>();
+        ArrayList<Integer> list1 = new ArrayList<>();
         list1.add(i);
         intLongTestStart.add(list1);
-        ArrayList<Float> list2 = new ArrayList<Float>();
+        ArrayList<Float> list2 = new ArrayList<>();
         list2.add(f);
         floatLongTestStart.add(list2);
       } else {
@@ -85,10 +85,10 @@ public class RangeFacetTest extends AbstractAnalyticsFacetTest {
       }
       //Doubles
       if (j-DOUBLE<0) {
-        ArrayList<Integer> list1 = new ArrayList<Integer>();
+        ArrayList<Integer> list1 = new ArrayList<>();
         list1.add(i);
         intDoubleTestStart.add(list1);
-        ArrayList<Float> list2 = new ArrayList<Float>();
+        ArrayList<Float> list2 = new ArrayList<>();
         list2.add(f);
         floatDoubleTestStart.add(list2);
       } else {
@@ -97,10 +97,10 @@ public class RangeFacetTest extends AbstractAnalyticsFacetTest {
       }
       //Dates
       if (j-DATE<0) {
-        ArrayList<Integer> list1 = new ArrayList<Integer>();
+        ArrayList<Integer> list1 = new ArrayList<>();
         list1.add(i);
         intDateTestStart.add(list1);
-        ArrayList<Float> list2 = new ArrayList<Float>();
+        ArrayList<Float> list2 = new ArrayList<>();
         list2.add(f);
         floatDateTestStart.add(list2);
       } else {
@@ -235,11 +235,11 @@ public class RangeFacetTest extends AbstractAnalyticsFacetTest {
       end+=gap-off;
     }
 
-    ArrayList<ArrayList<T>> lists = new ArrayList<ArrayList<T>>();
-    ArrayList<T> between = new ArrayList<T>();
+    ArrayList<ArrayList<T>> lists = new ArrayList<>();
+    ArrayList<T> between = new ArrayList<>();
     if (incLow && incUp) {
       for (int i = start; i<end && i<listsStart.size(); i+=gap) {
-        ArrayList<T> list = new ArrayList<T>();
+        ArrayList<T> list = new ArrayList<>();
         for (int j = i; j<=i+gap && j<=end && j<listsStart.size(); j++) {
           list.addAll(listsStart.get(j));
         }
@@ -250,7 +250,7 @@ public class RangeFacetTest extends AbstractAnalyticsFacetTest {
       }
     } else if (incLow && !incUp) {
       for (int i = start; i<end && i<listsStart.size(); i+=gap) {
-        ArrayList<T> list = new ArrayList<T>();
+        ArrayList<T> list = new ArrayList<>();
         for (int j = i; j<i+gap && j<end && j<listsStart.size(); j++) {
           list.addAll(listsStart.get(j));
         }
@@ -261,7 +261,7 @@ public class RangeFacetTest extends AbstractAnalyticsFacetTest {
       }
     } else if (!incLow && incUp) {
       for (int i = start; i<end && i<listsStart.size(); i+=gap) {
-        ArrayList<T> list = new ArrayList<T>();
+        ArrayList<T> list = new ArrayList<>();
         for (int j = i+1; j<=i+gap && j<=end && j<listsStart.size(); j++) {
           list.addAll(listsStart.get(j));
         }
@@ -272,7 +272,7 @@ public class RangeFacetTest extends AbstractAnalyticsFacetTest {
       }
     } else {
       for (int i = start; i<end && i<listsStart.size(); i+=gap) {
-        ArrayList<T> list = new ArrayList<T>();
+        ArrayList<T> list = new ArrayList<>();
         for (int j = i+1; j<i+gap && j<end && j<listsStart.size(); j++) {
           list.addAll(listsStart.get(j));
         }
@@ -291,8 +291,8 @@ public class RangeFacetTest extends AbstractAnalyticsFacetTest {
       lists.get(lists.size()-1).addAll(listsStart.get(end));
       between.addAll(listsStart.get(end));
     }
-    ArrayList<T> before = new ArrayList<T>();
-    ArrayList<T> after = new ArrayList<T>();
+    ArrayList<T> before = new ArrayList<>();
+    ArrayList<T> after = new ArrayList<>();
     if (incOut || !(incLow||incEdge)) {
       for (int i = 0; i<=start; i++) {
         before.addAll(listsStart.get(i));
@@ -341,8 +341,8 @@ public class RangeFacetTest extends AbstractAnalyticsFacetTest {
       end+=last-off;
     }
     
-    ArrayList<ArrayList<T>> lists = new ArrayList<ArrayList<T>>();
-    ArrayList<T> between = new ArrayList<T>();
+    ArrayList<ArrayList<T>> lists = new ArrayList<>();
+    ArrayList<T> between = new ArrayList<>();
     int gap = 0;
     int gapCounter = 0;
     if (incLow && incUp) {
@@ -350,7 +350,7 @@ public class RangeFacetTest extends AbstractAnalyticsFacetTest {
         if (gapCounter<gaps.length) {
           gap = gaps[gapCounter++];
         }
-        ArrayList<T> list = new ArrayList<T>();
+        ArrayList<T> list = new ArrayList<>();
         for (int j = i; j<=i+gap && j<=end && j<listsStart.size(); j++) {
           list.addAll(listsStart.get(j));
         }
@@ -364,7 +364,7 @@ public class RangeFacetTest extends AbstractAnalyticsFacetTest {
         if (gapCounter<gaps.length) {
           gap = gaps[gapCounter++];
         }
-        ArrayList<T> list = new ArrayList<T>();
+        ArrayList<T> list = new ArrayList<>();
         for (int j = i; j<i+gap && j<end && j<listsStart.size(); j++) {
           list.addAll(listsStart.get(j));
         }
@@ -378,7 +378,7 @@ public class RangeFacetTest extends AbstractAnalyticsFacetTest {
         if (gapCounter<gaps.length) {
           gap = gaps[gapCounter++];
         }
-        ArrayList<T> list = new ArrayList<T>();
+        ArrayList<T> list = new ArrayList<>();
         for (int j = i+1; j<=i+gap && j<=end && j<listsStart.size(); j++) {
           list.addAll(listsStart.get(j));
         }
@@ -392,7 +392,7 @@ public class RangeFacetTest extends AbstractAnalyticsFacetTest {
         if (gapCounter<gaps.length) {
           gap = gaps[gapCounter++];
         }
-        ArrayList<T> list = new ArrayList<T>();
+        ArrayList<T> list = new ArrayList<>();
         for (int j = i+1; j<i+gap && j<end && j<listsStart.size(); j++) {
           list.addAll(listsStart.get(j));
         }
@@ -411,8 +411,8 @@ public class RangeFacetTest extends AbstractAnalyticsFacetTest {
       lists.get(lists.size()-1).addAll(listsStart.get(end));
       between.addAll(listsStart.get(end));
     }
-    ArrayList<T> before = new ArrayList<T>();
-    ArrayList<T> after = new ArrayList<T>();
+    ArrayList<T> before = new ArrayList<>();
+    ArrayList<T> after = new ArrayList<>();
     if (incOut || !(incLow||incEdge)) {
       for (int i = 0; i<=start; i++) {
         before.addAll(listsStart.get(i));
diff --git solr/core/src/test/org/apache/solr/cloud/AliasIntegrationTest.java solr/core/src/test/org/apache/solr/cloud/AliasIntegrationTest.java
index efd6dc2..a090aef9 100644
--- solr/core/src/test/org/apache/solr/cloud/AliasIntegrationTest.java
+++ solr/core/src/test/org/apache/solr/cloud/AliasIntegrationTest.java
@@ -91,7 +91,7 @@ public class AliasIntegrationTest extends AbstractFullDistribZkTestBase {
     
     createCollection("collection2", 2, 1, 10);
     
-    List<Integer> numShardsNumReplicaList = new ArrayList<Integer>(2);
+    List<Integer> numShardsNumReplicaList = new ArrayList<>(2);
     numShardsNumReplicaList.add(2);
     numShardsNumReplicaList.add(1);
     checkForCollection("collection2", numShardsNumReplicaList, null);
diff --git solr/core/src/test/org/apache/solr/cloud/AssignTest.java solr/core/src/test/org/apache/solr/cloud/AssignTest.java
index 7d05c26..6d22428 100644
--- solr/core/src/test/org/apache/solr/cloud/AssignTest.java
+++ solr/core/src/test/org/apache/solr/cloud/AssignTest.java
@@ -58,11 +58,11 @@ public class AssignTest extends SolrTestCaseJ4 {
   public void testAssignNode() throws Exception {
     String cname = "collection1";
     
-    Map<String,DocCollection> collectionStates = new HashMap<String,DocCollection>();
+    Map<String,DocCollection> collectionStates = new HashMap<>();
     
-    Map<String,Slice> slices = new HashMap<String,Slice>();
+    Map<String,Slice> slices = new HashMap<>();
     
-    Map<String,Replica> replicas = new HashMap<String,Replica>();
+    Map<String,Replica> replicas = new HashMap<>();
     
     ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, "state", 
         ZkStateReader.STATE_PROP, "ACTIVE", 
@@ -85,7 +85,7 @@ public class AssignTest extends SolrTestCaseJ4 {
 
     collectionStates.put(cname, docCollection);
     
-    Set<String> liveNodes = new HashSet<String>();
+    Set<String> liveNodes = new HashSet<>();
     ClusterState state = new ClusterState(-1,liveNodes, collectionStates,ClusterStateTest.getMockZkStateReader(collectionStates.keySet()));
     String nodeName = Assign.assignNode("collection1", state);
     
diff --git solr/core/src/test/org/apache/solr/cloud/BasicDistributedZkTest.java solr/core/src/test/org/apache/solr/cloud/BasicDistributedZkTest.java
index 7e8d32d..4497210 100644
--- solr/core/src/test/org/apache/solr/cloud/BasicDistributedZkTest.java
+++ solr/core/src/test/org/apache/solr/cloud/BasicDistributedZkTest.java
@@ -99,7 +99,7 @@ public class BasicDistributedZkTest extends AbstractFullDistribZkTestBase {
   String missingField="ignore_exception__missing_but_valid_field_t";
   String invalidField="ignore_exception__invalid_field_not_in_schema";
   
-  private Map<String,List<SolrServer>> otherCollectionClients = new HashMap<String,List<SolrServer>>();
+  private Map<String,List<SolrServer>> otherCollectionClients = new HashMap<>();
 
   private String oneInstanceCollection = "oneInstanceCollection";
   private String oneInstanceCollection2 = "oneInstanceCollection2";
@@ -131,8 +131,8 @@ public class BasicDistributedZkTest extends AbstractFullDistribZkTestBase {
     
     sliceCount = 2;
     shardCount = 4;
-    completionService = new ExecutorCompletionService<Object>(executor);
-    pending = new HashSet<Future<Object>>();
+    completionService = new ExecutorCompletionService<>(executor);
+    pending = new HashSet<>();
     
   }
   
@@ -418,12 +418,12 @@ public class BasicDistributedZkTest extends AbstractFullDistribZkTestBase {
   
   private void testShardParamVariations() throws Exception {
     SolrQuery query = new SolrQuery("*:*");
-    Map<String,Long> shardCounts = new HashMap<String,Long>();
+    Map<String,Long> shardCounts = new HashMap<>();
 
     for (String shard : shardToJetty.keySet()) {
       // every client should give the same numDocs for this shard
       // shffle the clients in a diff order for each shard
-      List<SolrServer> solrclients = new ArrayList<SolrServer>(this.clients);
+      List<SolrServer> solrclients = new ArrayList<>(this.clients);
       Collections.shuffle(solrclients, random());
       for (SolrServer client : solrclients) {
         query.set("shards", shard);
@@ -437,11 +437,11 @@ public class BasicDistributedZkTest extends AbstractFullDistribZkTestBase {
                      shardCounts.get(shard).longValue(), numDocs);
         
         List<CloudJettyRunner> replicaJetties 
-          = new ArrayList<CloudJettyRunner>(shardToJetty.get(shard));
+          = new ArrayList<>(shardToJetty.get(shard));
         Collections.shuffle(replicaJetties, random());
 
         // each replica should also give the same numDocs
-        ArrayList<String> replicaAlts = new ArrayList<String>(replicaJetties.size() * 2);
+        ArrayList<String> replicaAlts = new ArrayList<>(replicaJetties.size() * 2);
         for (CloudJettyRunner replicaJetty : shardToJetty.get(shard)) {
           String replica = replicaJetty.url;
           query.set("shards", replica);
@@ -474,7 +474,7 @@ public class BasicDistributedZkTest extends AbstractFullDistribZkTestBase {
     // sums of multiple shards should add up regardless of how we 
     // query those shards or which client we use
     long randomShardCountsExpected = 0;
-    ArrayList<String> randomShards = new ArrayList<String>(shardCounts.size());
+    ArrayList<String> randomShards = new ArrayList<>(shardCounts.size());
     for (Map.Entry<String,Long> shardData : shardCounts.entrySet()) {
       if (random().nextBoolean() || randomShards.size() < 2) {
         String shard = shardData.getKey();
@@ -484,7 +484,7 @@ public class BasicDistributedZkTest extends AbstractFullDistribZkTestBase {
           randomShards.add(shard);
         } else {
           // use some set explicit replicas
-          ArrayList<String> replicas = new ArrayList<String>(7);
+          ArrayList<String> replicas = new ArrayList<>(7);
           for (CloudJettyRunner replicaJetty : shardToJetty.get(shard)) {
             if (0 == random().nextInt(3) || 0 == replicas.size()) {
               replicas.add(replicaJetty.url);
@@ -604,7 +604,7 @@ public class BasicDistributedZkTest extends AbstractFullDistribZkTestBase {
     if (createNodeSetStr != null) params.set(OverseerCollectionProcessor.CREATE_NODE_SET, createNodeSetStr);
 
     int clientIndex = clients.size() > 1 ? random().nextInt(2) : 0;
-    List<Integer> list = new ArrayList<Integer>();
+    List<Integer> list = new ArrayList<>();
     list.add(numShards);
     list.add(numReplicas);
     if (collectionInfos != null) {
@@ -709,7 +709,7 @@ public class BasicDistributedZkTest extends AbstractFullDistribZkTestBase {
     sd =  sdoc("id", 1000, "foo_i",5);
     clients.get(0).add(sd);
 
-    List<Integer> expected = new ArrayList<Integer>();
+    List<Integer> expected = new ArrayList<>();
     int val = 0;
     for (SolrServer client : clients) {
       val += 10;
@@ -764,7 +764,7 @@ public class BasicDistributedZkTest extends AbstractFullDistribZkTestBase {
   private void testANewCollectionInOneInstanceWithManualShardAssignement() throws Exception {
     log.info("### STARTING testANewCollectionInOneInstanceWithManualShardAssignement");
     System.clearProperty("numShards");
-    List<SolrServer> collectionClients = new ArrayList<SolrServer>();
+    List<SolrServer> collectionClients = new ArrayList<>();
     SolrServer client = clients.get(0);
     final String baseUrl = ((HttpSolrServer) client).getBaseURL().substring(
         0,
@@ -896,7 +896,7 @@ public class BasicDistributedZkTest extends AbstractFullDistribZkTestBase {
 
   private void testANewCollectionInOneInstance() throws Exception {
     log.info("### STARTING testANewCollectionInOneInstance");
-    List<SolrServer> collectionClients = new ArrayList<SolrServer>();
+    List<SolrServer> collectionClients = new ArrayList<>();
     SolrServer client = clients.get(0);
     final String baseUrl = ((HttpSolrServer) client).getBaseURL().substring(
         0,
@@ -1085,7 +1085,7 @@ public class BasicDistributedZkTest extends AbstractFullDistribZkTestBase {
   }
   
   private void createNewCollection(final String collection) throws InterruptedException {
-    final List<SolrServer> collectionClients = new ArrayList<SolrServer>();
+    final List<SolrServer> collectionClients = new ArrayList<>();
     otherCollectionClients.put(collection, collectionClients);
     int unique = 0;
     for (final SolrServer client : clients) {
diff --git solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest.java solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest.java
index ba0f081..a738c2a 100644
--- solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest.java
+++ solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest.java
@@ -127,7 +127,7 @@ public class ChaosMonkeyNothingIsSafeTest extends AbstractFullDistribZkTestBase
       // as it's not supported for recovery
        del("*:*");
       
-      List<StopableThread> threads = new ArrayList<StopableThread>();
+      List<StopableThread> threads = new ArrayList<>();
       int threadCount = 1;
       int i = 0;
       for (i = 0; i < threadCount; i++) {
@@ -247,7 +247,7 @@ public class ChaosMonkeyNothingIsSafeTest extends AbstractFullDistribZkTestBase
       } finally {
         client.shutdown();
       }
-      List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);
+      List<Integer> numShardsNumReplicas = new ArrayList<>(2);
       numShardsNumReplicas.add(1);
       numShardsNumReplicas.add(1);
       checkForCollection("testcollection",numShardsNumReplicas, null);
diff --git solr/core/src/test/org/apache/solr/cloud/ChaosMonkeySafeLeaderTest.java solr/core/src/test/org/apache/solr/cloud/ChaosMonkeySafeLeaderTest.java
index 19e40bf..19ae3ed 100644
--- solr/core/src/test/org/apache/solr/cloud/ChaosMonkeySafeLeaderTest.java
+++ solr/core/src/test/org/apache/solr/cloud/ChaosMonkeySafeLeaderTest.java
@@ -105,7 +105,7 @@ public class ChaosMonkeySafeLeaderTest extends AbstractFullDistribZkTestBase {
 
     del("*:*");
     
-    List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();
+    List<StopableIndexingThread> threads = new ArrayList<>();
     int threadCount = 2;
     for (int i = 0; i < threadCount; i++) {
       StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);
@@ -169,7 +169,7 @@ public class ChaosMonkeySafeLeaderTest extends AbstractFullDistribZkTestBase {
     } finally {
       client.shutdown();
     }
-    List<Integer> numShardsNumReplicas = new ArrayList<Integer>(2);
+    List<Integer> numShardsNumReplicas = new ArrayList<>(2);
     numShardsNumReplicas.add(1);
     numShardsNumReplicas.add(1);
     checkForCollection("testcollection",numShardsNumReplicas, null);
diff --git solr/core/src/test/org/apache/solr/cloud/ClusterStateTest.java solr/core/src/test/org/apache/solr/cloud/ClusterStateTest.java
index a438c31..811a455 100644
--- solr/core/src/test/org/apache/solr/cloud/ClusterStateTest.java
+++ solr/core/src/test/org/apache/solr/cloud/ClusterStateTest.java
@@ -41,14 +41,14 @@ import static org.easymock.EasyMock.expectLastCall;
 public class ClusterStateTest extends SolrTestCaseJ4 {
   @Test
   public void testStoreAndRead() throws Exception {
-    Map<String,DocCollection> collectionStates = new HashMap<String,DocCollection>();
-    Set<String> liveNodes = new HashSet<String>();
+    Map<String,DocCollection> collectionStates = new HashMap<>();
+    Set<String> liveNodes = new HashSet<>();
     liveNodes.add("node1");
     liveNodes.add("node2");
     
-    Map<String,Slice> slices = new HashMap<String,Slice>();
-    Map<String,Replica> sliceToProps = new HashMap<String,Replica>();
-    Map<String,Object> props = new HashMap<String,Object>();
+    Map<String,Slice> slices = new HashMap<>();
+    Map<String,Replica> sliceToProps = new HashMap<>();
+    Map<String,Object> props = new HashMap<>();
 
     props.put("prop1", "value");
     props.put("prop2", "value2");
diff --git solr/core/src/test/org/apache/solr/cloud/ClusterStateUpdateTest.java solr/core/src/test/org/apache/solr/cloud/ClusterStateUpdateTest.java
index 670ed26..35d49d7 100644
--- solr/core/src/test/org/apache/solr/cloud/ClusterStateUpdateTest.java
+++ solr/core/src/test/org/apache/solr/cloud/ClusterStateUpdateTest.java
@@ -152,7 +152,7 @@ public class ClusterStateUpdateTest extends SolrTestCaseJ4  {
     System.setProperty("solrcloud.update.delay", "1");
     
    
-    Map<String,Object> props2 = new HashMap<String,Object>();
+    Map<String,Object> props2 = new HashMap<>();
     props2.put("configName", "conf1");
     ZkNodeProps zkProps2 = new ZkNodeProps(props2);
     
diff --git solr/core/src/test/org/apache/solr/cloud/CollectionsAPIDistributedZkTest.java solr/core/src/test/org/apache/solr/cloud/CollectionsAPIDistributedZkTest.java
index 5a4c45a..c9ed9ff 100644
--- solr/core/src/test/org/apache/solr/cloud/CollectionsAPIDistributedZkTest.java
+++ solr/core/src/test/org/apache/solr/cloud/CollectionsAPIDistributedZkTest.java
@@ -173,8 +173,8 @@ public class CollectionsAPIDistributedZkTest extends AbstractFullDistribZkTestBa
     
     sliceCount = 2;
     shardCount = 4;
-    completionService = new ExecutorCompletionService<Object>(executor);
-    pending = new HashSet<Future<Object>>();
+    completionService = new ExecutorCompletionService<>(executor);
+    pending = new HashSet<>();
     checkCreatedVsState = false;
     
   }
@@ -223,7 +223,7 @@ public class CollectionsAPIDistributedZkTest extends AbstractFullDistribZkTestBa
     
     String collectionName = "out_of_sync_collection";
     
-    List<Integer> numShardsNumReplicaList = new ArrayList<Integer>();
+    List<Integer> numShardsNumReplicaList = new ArrayList<>();
     numShardsNumReplicaList.add(2);
     numShardsNumReplicaList.add(1);
     
@@ -634,12 +634,12 @@ public class CollectionsAPIDistributedZkTest extends AbstractFullDistribZkTestBa
     request.setPath("/admin/collections");
     createNewSolrServer("", baseUrl).request(request);
     
-    List<Integer> numShardsNumReplicaList = new ArrayList<Integer>();
+    List<Integer> numShardsNumReplicaList = new ArrayList<>();
     numShardsNumReplicaList.add(2);
     numShardsNumReplicaList.add(2);
     checkForCollection("nodes_used_collection", numShardsNumReplicaList , null);
 
-    List<String> createNodeList = new ArrayList<String>();
+    List<String> createNodeList = new ArrayList<>();
 
     Set<String> liveNodes = cloudClient.getZkStateReader().getClusterState()
         .getLiveNodes();
@@ -677,7 +677,7 @@ public class CollectionsAPIDistributedZkTest extends AbstractFullDistribZkTestBa
     // env make this pretty fragile
     
     // create new collections rapid fire
-    Map<String,List<Integer>> collectionInfos = new HashMap<String,List<Integer>>();
+    Map<String,List<Integer>> collectionInfos = new HashMap<>();
     int cnt = random().nextInt(TEST_NIGHTLY ? 6 : 3) + 1;
     
     for (int i = 0; i < cnt; i++) {
@@ -787,7 +787,7 @@ public class CollectionsAPIDistributedZkTest extends AbstractFullDistribZkTestBa
     
     checkInstanceDirs(jettys.get(0)); 
     
-    List<String> collectionNameList = new ArrayList<String>();
+    List<String> collectionNameList = new ArrayList<>();
     collectionNameList.addAll(collectionInfos.keySet());
     String collectionName = collectionNameList.get(random().nextInt(collectionNameList.size()));
     
@@ -817,7 +817,7 @@ public class CollectionsAPIDistributedZkTest extends AbstractFullDistribZkTestBa
     // lets try a collection reload
     
     // get core open times
-    Map<String,Long> urlToTimeBefore = new HashMap<String,Long>();
+    Map<String,Long> urlToTimeBefore = new HashMap<>();
     collectStartTimes(collectionName, urlToTimeBefore);
     assertTrue(urlToTimeBefore.size() > 0);
     ModifiableSolrParams params = new ModifiableSolrParams();
@@ -883,7 +883,7 @@ public class CollectionsAPIDistributedZkTest extends AbstractFullDistribZkTestBa
     request.setPath("/admin/collections");
     createNewSolrServer("", baseUrl).request(request);
     
-    List<Integer> list = new ArrayList<Integer> (2);
+    List<Integer> list = new ArrayList<>(2);
     list.add(1);
     list.add(2);
     checkForCollection(collectionName, list, null);
@@ -904,7 +904,7 @@ public class CollectionsAPIDistributedZkTest extends AbstractFullDistribZkTestBa
     int numShards = (numLiveNodes/2) + 1;
     int replicationFactor = 2;
     int maxShardsPerNode = 1;
-    collectionInfos = new HashMap<String,List<Integer>>();
+    collectionInfos = new HashMap<>();
     CloudSolrServer client = createCloudClient("awholynewcollection_" + cnt);
     try {
       exp = false;
@@ -922,7 +922,7 @@ public class CollectionsAPIDistributedZkTest extends AbstractFullDistribZkTestBa
     
     // Test createNodeSet
     numLiveNodes = getCommonCloudSolrServer().getZkStateReader().getClusterState().getLiveNodes().size();
-    List<String> createNodeList = new ArrayList<String>();
+    List<String> createNodeList = new ArrayList<>();
     int numOfCreateNodes = numLiveNodes/2;
     assertFalse("createNodeSet test is pointless with only " + numLiveNodes + " nodes running", numOfCreateNodes == 0);
     int i = 0;
@@ -937,7 +937,7 @@ public class CollectionsAPIDistributedZkTest extends AbstractFullDistribZkTestBa
     maxShardsPerNode = 2;
     numShards = createNodeList.size() * maxShardsPerNode;
     replicationFactor = 1;
-    collectionInfos = new HashMap<String,List<Integer>>();
+    collectionInfos = new HashMap<>();
     client = createCloudClient("awholynewcollection_" + (cnt+1));
     try {
       createCollection(collectionInfos, "awholynewcollection_" + (cnt+1), numShards, replicationFactor, maxShardsPerNode, client, StrUtils.join(createNodeList, ','), "conf1");
@@ -965,7 +965,7 @@ public class CollectionsAPIDistributedZkTest extends AbstractFullDistribZkTestBa
       
       public void run() {
         // create new collections rapid fire
-        Map<String,List<Integer>> collectionInfos = new HashMap<String,List<Integer>>();
+        Map<String,List<Integer>> collectionInfos = new HashMap<>();
         int cnt = random().nextInt(TEST_NIGHTLY ? 13 : 3) + 1;
         
         for (int i = 0; i < cnt; i++) {
@@ -1012,7 +1012,7 @@ public class CollectionsAPIDistributedZkTest extends AbstractFullDistribZkTestBa
         }
       }
     }
-    List<Thread> threads = new ArrayList<Thread>();
+    List<Thread> threads = new ArrayList<>();
     int numThreads = TEST_NIGHTLY ? 6 : 2;
     for (int i = 0; i < numThreads; i++) {
       CollectionThread thread = new CollectionThread("collection" + i);
@@ -1054,7 +1054,7 @@ public class CollectionsAPIDistributedZkTest extends AbstractFullDistribZkTestBa
 
     boolean allTimesAreCorrect = false;
     while (System.currentTimeMillis() < timeoutAt) {
-      Map<String,Long> urlToTimeAfter = new HashMap<String,Long>();
+      Map<String,Long> urlToTimeAfter = new HashMap<>();
       collectStartTimes(collectionName, urlToTimeAfter);
       
       boolean retry = false;
@@ -1182,13 +1182,13 @@ public class CollectionsAPIDistributedZkTest extends AbstractFullDistribZkTestBa
   }
 
   private void checkNoTwoShardsUseTheSameIndexDir() throws Exception {
-    Map<String, Set<String>> indexDirToShardNamesMap = new HashMap<String, Set<String>>();
+    Map<String, Set<String>> indexDirToShardNamesMap = new HashMap<>();
     
-    List<MBeanServer> servers = new LinkedList<MBeanServer>();
+    List<MBeanServer> servers = new LinkedList<>();
     servers.add(ManagementFactory.getPlatformMBeanServer());
     servers.addAll(MBeanServerFactory.findMBeanServer(null));
     for (final MBeanServer server : servers) {
-      Set<ObjectName> mbeans = new HashSet<ObjectName>();
+      Set<ObjectName> mbeans = new HashSet<>();
       mbeans.addAll(server.queryNames(null, null));
       for (final ObjectName mbean : mbeans) {
         Object value;
@@ -1234,7 +1234,7 @@ public class CollectionsAPIDistributedZkTest extends AbstractFullDistribZkTestBa
     try {
       createCollection(collectionName, client,2,2);
       String newReplicaName = Assign.assignNode(collectionName , client.getZkStateReader().getClusterState() );
-      ArrayList<String> nodeList = new ArrayList<String>(client.getZkStateReader().getClusterState().getLiveNodes());
+      ArrayList<String> nodeList = new ArrayList<>(client.getZkStateReader().getClusterState().getLiveNodes());
       Collections.shuffle(nodeList);
       Map m = makeMap(
           "action", CollectionAction.ADDREPLICA.toString(),
@@ -1311,7 +1311,7 @@ public class CollectionsAPIDistributedZkTest extends AbstractFullDistribZkTestBa
         REPLICATION_FACTOR, replicationFactor,
         MAX_SHARDS_PER_NODE, maxShardsPerNode,
         NUM_SLICES, numShards);
-    Map<String,List<Integer>> collectionInfos = new HashMap<String,List<Integer>>();
+    Map<String,List<Integer>> collectionInfos = new HashMap<>();
     createCollection(collectionInfos, COLL_NAME, props, client,"conf1");
     waitForRecoveriesToFinish(COLL_NAME, false);
   }
diff --git solr/core/src/test/org/apache/solr/cloud/CustomCollectionTest.java solr/core/src/test/org/apache/solr/cloud/CustomCollectionTest.java
index 1a2ea93..de318aa 100644
--- solr/core/src/test/org/apache/solr/cloud/CustomCollectionTest.java
+++ solr/core/src/test/org/apache/solr/cloud/CustomCollectionTest.java
@@ -99,8 +99,8 @@ public class CustomCollectionTest extends AbstractFullDistribZkTestBase {
 
     sliceCount = 2;
     shardCount = 4;
-    completionService = new ExecutorCompletionService<Object>(executor);
-    pending = new HashSet<Future<Object>>();
+    completionService = new ExecutorCompletionService<>(executor);
+    pending = new HashSet<>();
     checkCreatedVsState = false;
 
   }
@@ -142,7 +142,7 @@ public class CustomCollectionTest extends AbstractFullDistribZkTestBase {
     // env make this pretty fragile
 
     // create new collections rapid fire
-    Map<String,List<Integer>> collectionInfos = new HashMap<String,List<Integer>>();
+    Map<String,List<Integer>> collectionInfos = new HashMap<>();
     int replicationFactor = TestUtil.nextInt(random(), 0, 3) + 2;
 
     int cnt = random().nextInt(6) + 1;
@@ -214,7 +214,7 @@ public class CustomCollectionTest extends AbstractFullDistribZkTestBase {
     assertNull("A shard of a Collection configured with implicit router must have null range",
         coll.getSlice("a").getRange());
 
-    List<String> collectionNameList = new ArrayList<String>();
+    List<String> collectionNameList = new ArrayList<>();
     collectionNameList.addAll(collectionInfos.keySet());
     log.info("Collections created : "+collectionNameList );
 
@@ -360,7 +360,7 @@ public class CustomCollectionTest extends AbstractFullDistribZkTestBase {
     int maxShardsPerNode = (((numShards * replicationFactor) / getCommonCloudSolrServer()
         .getZkStateReader().getClusterState().getLiveNodes().size())) + 1;
 
-    HashMap<String, List<Integer>> collectionInfos = new HashMap<String, List<Integer>>();
+    HashMap<String, List<Integer>> collectionInfos = new HashMap<>();
     CloudSolrServer client = null;
     String shard_fld = "shard_s";
     try {
@@ -419,7 +419,7 @@ public class CustomCollectionTest extends AbstractFullDistribZkTestBase {
 
   private void testCreateShardRepFactor() throws Exception  {
     String collectionName = "testCreateShardRepFactor";
-    HashMap<String, List<Integer>> collectionInfos = new HashMap<String, List<Integer>>();
+    HashMap<String, List<Integer>> collectionInfos = new HashMap<>();
     CloudSolrServer client = null;
     try {
       client = createCloudClient(null);
diff --git solr/core/src/test/org/apache/solr/cloud/DeleteReplicaTest.java solr/core/src/test/org/apache/solr/cloud/DeleteReplicaTest.java
index bcebb34..e9fbca5 100644
--- solr/core/src/test/org/apache/solr/cloud/DeleteReplicaTest.java
+++ solr/core/src/test/org/apache/solr/cloud/DeleteReplicaTest.java
@@ -148,7 +148,7 @@ public class DeleteReplicaTest extends AbstractFullDistribZkTestBase {
         REPLICATION_FACTOR, replicationFactor,
         MAX_SHARDS_PER_NODE, maxShardsPerNode,
         NUM_SLICES, numShards);
-    Map<String,List<Integer>> collectionInfos = new HashMap<String,List<Integer>>();
+    Map<String,List<Integer>> collectionInfos = new HashMap<>();
     createCollection(collectionInfos, COLL_NAME, props, client);
   }
 }
diff --git solr/core/src/test/org/apache/solr/cloud/DeleteShardTest.java solr/core/src/test/org/apache/solr/cloud/DeleteShardTest.java
index 97f86ba..a55bd6c 100644
--- solr/core/src/test/org/apache/solr/cloud/DeleteShardTest.java
+++ solr/core/src/test/org/apache/solr/cloud/DeleteShardTest.java
@@ -145,7 +145,7 @@ public class DeleteShardTest extends AbstractFullDistribZkTestBase {
   protected void setSliceAsInactive(String slice) throws SolrServerException, IOException,
       KeeperException, InterruptedException {
     DistributedQueue inQueue = Overseer.getInQueue(cloudClient.getZkStateReader().getZkClient());
-    Map<String, Object> propMap = new HashMap<String, Object>();
+    Map<String, Object> propMap = new HashMap<>();
     propMap.put(Overseer.QUEUE_OPERATION, "updateshardstate");
     propMap.put(slice, Slice.INACTIVE);
     propMap.put(ZkStateReader.COLLECTION_PROP, "collection1");
diff --git solr/core/src/test/org/apache/solr/cloud/DistribCursorPagingTest.java solr/core/src/test/org/apache/solr/cloud/DistribCursorPagingTest.java
index 835bbd2..8b8e3b5 100644
--- solr/core/src/test/org/apache/solr/cloud/DistribCursorPagingTest.java
+++ solr/core/src/test/org/apache/solr/cloud/DistribCursorPagingTest.java
@@ -523,7 +523,7 @@ public class DistribCursorPagingTest extends AbstractFullDistribZkTestBase {
     // start with a smallish number of documents, and test that we can do a full walk using a 
     // sort on *every* field in the schema...
 
-    List<SolrInputDocument> initialDocs = new ArrayList<SolrInputDocument>();
+    List<SolrInputDocument> initialDocs = new ArrayList<>();
     for (int i = 1; i <= numInitialDocs; i++) {
       SolrInputDocument doc = CursorPagingTest.buildRandomDocument(i);
       initialDocs.add(doc);
@@ -606,7 +606,7 @@ public class DistribCursorPagingTest extends AbstractFullDistribZkTestBase {
     req.setShowSchema(true); 
     NamedList<Object> rsp = controlClient.request(req);
     NamedList<Object> fields = (NamedList) ((NamedList)rsp.get("schema")).get("fields");
-    ArrayList<String> names = new ArrayList<String>(fields.size());
+    ArrayList<String> names = new ArrayList<>(fields.size());
     for (Map.Entry<String,Object> item : fields) {
       names.add(item.getKey());
     }
diff --git solr/core/src/test/org/apache/solr/cloud/FullSolrCloudDistribCmdsTest.java solr/core/src/test/org/apache/solr/cloud/FullSolrCloudDistribCmdsTest.java
index 0e3f91e..1963432 100644
--- solr/core/src/test/org/apache/solr/cloud/FullSolrCloudDistribCmdsTest.java
+++ solr/core/src/test/org/apache/solr/cloud/FullSolrCloudDistribCmdsTest.java
@@ -387,7 +387,7 @@ public class FullSolrCloudDistribCmdsTest extends AbstractFullDistribZkTestBase
         }
       }
     };
-    List<Thread> threads = new ArrayList<Thread>();
+    List<Thread> threads = new ArrayList<>();
 
     int nthreads = random().nextInt(TEST_NIGHTLY ? 4 : 2) + 1;
     for (int i = 0; i < nthreads; i++) {
diff --git solr/core/src/test/org/apache/solr/cloud/LeaderElectionIntegrationTest.java solr/core/src/test/org/apache/solr/cloud/LeaderElectionIntegrationTest.java
index acaa0c7..54e8326 100644
--- solr/core/src/test/org/apache/solr/cloud/LeaderElectionIntegrationTest.java
+++ solr/core/src/test/org/apache/solr/cloud/LeaderElectionIntegrationTest.java
@@ -57,9 +57,9 @@ public class LeaderElectionIntegrationTest extends SolrTestCaseJ4 {
   
   protected String zkDir;
   
-  private Map<Integer,CoreContainer> containerMap = new HashMap<Integer,CoreContainer>();
+  private Map<Integer,CoreContainer> containerMap = new HashMap<>();
   
-  private Map<String,Set<Integer>> shardPorts = new HashMap<String,Set<Integer>>();
+  private Map<String,Set<Integer>> shardPorts = new HashMap<>();
   
   private SolrZkClient zkClient;
 
@@ -142,7 +142,7 @@ public class LeaderElectionIntegrationTest extends SolrTestCaseJ4 {
     System.setProperty("solr.solr.home", TEST_HOME());
     Set<Integer> ports = shardPorts.get(shard);
     if (ports == null) {
-      ports = new HashSet<Integer>();
+      ports = new HashSet<>();
       shardPorts.put(shard, ports);
     }
     ports.add(port);
diff --git solr/core/src/test/org/apache/solr/cloud/LeaderElectionTest.java solr/core/src/test/org/apache/solr/cloud/LeaderElectionTest.java
index 2177201..4530264 100644
--- solr/core/src/test/org/apache/solr/cloud/LeaderElectionTest.java
+++ solr/core/src/test/org/apache/solr/cloud/LeaderElectionTest.java
@@ -228,7 +228,7 @@ public class LeaderElectionTest extends SolrTestCaseJ4 {
   @Test
   public void testElection() throws Exception {
     
-    List<ClientThread> threads = new ArrayList<ClientThread>();
+    List<ClientThread> threads = new ArrayList<>();
     
     for (int i = 0; i < 15; i++) {
       ClientThread thread = new ClientThread(i);
diff --git solr/core/src/test/org/apache/solr/cloud/MigrateRouteKeyTest.java solr/core/src/test/org/apache/solr/cloud/MigrateRouteKeyTest.java
index 71d48d2..f0eab04 100644
--- solr/core/src/test/org/apache/solr/cloud/MigrateRouteKeyTest.java
+++ solr/core/src/test/org/apache/solr/cloud/MigrateRouteKeyTest.java
@@ -141,7 +141,7 @@ public class MigrateRouteKeyTest extends BasicDistributedZkTest {
   }
 
   private void createCollection(String targetCollection) throws Exception {
-    HashMap<String, List<Integer>> collectionInfos = new HashMap<String, List<Integer>>();
+    HashMap<String, List<Integer>> collectionInfos = new HashMap<>();
     CloudSolrServer client = null;
     try {
       client = createCloudClient(null);
diff --git solr/core/src/test/org/apache/solr/cloud/OverseerCollectionProcessorTest.java solr/core/src/test/org/apache/solr/cloud/OverseerCollectionProcessorTest.java
index 9b8e012..7b24d2a 100644
--- solr/core/src/test/org/apache/solr/cloud/OverseerCollectionProcessorTest.java
+++ solr/core/src/test/org/apache/solr/cloud/OverseerCollectionProcessorTest.java
@@ -81,7 +81,7 @@ public class OverseerCollectionProcessorTest extends SolrTestCaseJ4 {
   private OverseerCollectionProcessorToBeTested underTest;
   
   private Thread thread;
-  private Queue<QueueEvent> queue = new BlockingArrayQueue<QueueEvent>();
+  private Queue<QueueEvent> queue = new BlockingArrayQueue<>();
 
   private class OverseerCollectionProcessorToBeTested extends
       OverseerCollectionProcessor {
@@ -202,7 +202,7 @@ public class OverseerCollectionProcessorTest extends SolrTestCaseJ4 {
         return collectionsSet;
       }
     }).anyTimes();
-    final Set<String> liveNodes = new HashSet<String>();
+    final Set<String> liveNodes = new HashSet<>();
     for (int i = 0; i < liveNodesCount; i++) {
       final String address = "localhost:" + (8963 + i) + "_solr";
       liveNodes.add(address);
@@ -316,14 +316,14 @@ public class OverseerCollectionProcessorTest extends SolrTestCaseJ4 {
   }
   
   private class SubmitCapture {
-    public Capture<ShardRequest> shardRequestCapture = new Capture<ShardRequest>();
-    public Capture<String> nodeUrlsWithoutProtocolPartCapture = new Capture<String>();
-    public Capture<ModifiableSolrParams> params = new Capture<ModifiableSolrParams>();
+    public Capture<ShardRequest> shardRequestCapture = new Capture<>();
+    public Capture<String> nodeUrlsWithoutProtocolPartCapture = new Capture<>();
+    public Capture<ModifiableSolrParams> params = new Capture<>();
   }
   
   protected List<SubmitCapture> mockShardHandlerForCreateJob(
       Integer numberOfSlices, Integer numberOfReplica) {
-    List<SubmitCapture> submitCaptures = new ArrayList<SubmitCapture>();
+    List<SubmitCapture> submitCaptures = new ArrayList<>();
     for (int i = 0; i < (numberOfSlices * numberOfReplica); i++) {
       SubmitCapture submitCapture = new SubmitCapture();
       shardHandlerMock.submit(capture(submitCapture.shardRequestCapture),
@@ -370,9 +370,9 @@ public class OverseerCollectionProcessorTest extends SolrTestCaseJ4 {
   
   protected void verifySubmitCaptures(List<SubmitCapture> submitCaptures,
       Integer numberOfSlices, Integer numberOfReplica, Collection<String> createNodes) {
-    List<String> coreNames = new ArrayList<String>();
-    Map<String,Map<String,Integer>> sliceToNodeUrlsWithoutProtocolPartToNumberOfShardsRunningMapMap = new HashMap<String,Map<String,Integer>>();
-    List<String> nodeUrlWithoutProtocolPartForLiveNodes = new ArrayList<String>(
+    List<String> coreNames = new ArrayList<>();
+    Map<String,Map<String,Integer>> sliceToNodeUrlsWithoutProtocolPartToNumberOfShardsRunningMapMap = new HashMap<>();
+    List<String> nodeUrlWithoutProtocolPartForLiveNodes = new ArrayList<>(
         createNodes.size());
     for (String nodeName : createNodes) {
       String nodeUrlWithoutProtocolPart = nodeName.replaceAll("_", "/");
@@ -510,7 +510,7 @@ public class OverseerCollectionProcessorTest extends SolrTestCaseJ4 {
     assertTrue("Wrong usage of testTemplage. createNodeListOption has to be " + CreateNodeListOptions.SEND + " when numberOfNodes and numberOfNodesToCreateOn are unequal", ((createNodeListOption == CreateNodeListOptions.SEND) || (numberOfNodes.intValue() == numberOfNodesToCreateOn.intValue())));
     
     Set<String> liveNodes = commonMocks(numberOfNodes);
-    List<String> createNodeList = new ArrayList<String>();
+    List<String> createNodeList = new ArrayList<>();
     int i = 0;
     for (String node : liveNodes) {
       if (i++ < numberOfNodesToCreateOn) {
diff --git solr/core/src/test/org/apache/solr/cloud/OverseerRolesTest.java solr/core/src/test/org/apache/solr/cloud/OverseerRolesTest.java
index 2f0eb4d..67ac49b 100644
--- solr/core/src/test/org/apache/solr/cloud/OverseerRolesTest.java
+++ solr/core/src/test/org/apache/solr/cloud/OverseerRolesTest.java
@@ -216,7 +216,7 @@ public class OverseerRolesTest  extends AbstractFullDistribZkTestBase{
         REPLICATION_FACTOR, replicationFactor,
         MAX_SHARDS_PER_NODE, maxShardsPerNode,
         NUM_SLICES, numShards);
-    Map<String,List<Integer>> collectionInfos = new HashMap<String,List<Integer>>();
+    Map<String,List<Integer>> collectionInfos = new HashMap<>();
     createCollection(collectionInfos, COLL_NAME, props, client);
   }
 
diff --git solr/core/src/test/org/apache/solr/cloud/OverseerTest.java solr/core/src/test/org/apache/solr/cloud/OverseerTest.java
index a67a8be..921315d 100644
--- solr/core/src/test/org/apache/solr/cloud/OverseerTest.java
+++ solr/core/src/test/org/apache/solr/cloud/OverseerTest.java
@@ -61,8 +61,8 @@ public class OverseerTest extends SolrTestCaseJ4 {
   static final int TIMEOUT = 10000;
   private static final boolean DEBUG = false;
   
-  private List<Overseer> overseers = new ArrayList<Overseer>();
-  private List<ZkStateReader> readers = new ArrayList<ZkStateReader>();
+  private List<Overseer> overseers = new ArrayList<>();
+  private List<ZkStateReader> readers = new ArrayList<>();
   
   private String collection = "collection1";
   
@@ -435,7 +435,7 @@ public class OverseerTest extends SolrTestCaseJ4 {
       assertEquals("Unable to verify all cores have been returned an id", 
                    coreCount, assignedCount);
       
-      final HashMap<String, AtomicInteger> counters = new HashMap<String,AtomicInteger>();
+      final HashMap<String, AtomicInteger> counters = new HashMap<>();
       for (int i = 1; i < sliceCount+1; i++) {
         counters.put("shard" + i, new AtomicInteger());
       }
diff --git solr/core/src/test/org/apache/solr/cloud/RemoteQueryErrorTest.java solr/core/src/test/org/apache/solr/cloud/RemoteQueryErrorTest.java
index fc7bb63..46bd598 100644
--- solr/core/src/test/org/apache/solr/cloud/RemoteQueryErrorTest.java
+++ solr/core/src/test/org/apache/solr/cloud/RemoteQueryErrorTest.java
@@ -49,7 +49,7 @@ public class RemoteQueryErrorTest extends AbstractFullDistribZkTestBase {
     
     createCollection("collection2", 2, 1, 10);
     
-    List<Integer> numShardsNumReplicaList = new ArrayList<Integer>(2);
+    List<Integer> numShardsNumReplicaList = new ArrayList<>(2);
     numShardsNumReplicaList.add(2);
     numShardsNumReplicaList.add(1);
     checkForCollection("collection2", numShardsNumReplicaList, null);
diff --git solr/core/src/test/org/apache/solr/cloud/ShardRoutingTest.java solr/core/src/test/org/apache/solr/cloud/ShardRoutingTest.java
index f432ccd..b942cc2 100644
--- solr/core/src/test/org/apache/solr/cloud/ShardRoutingTest.java
+++ solr/core/src/test/org/apache/solr/cloud/ShardRoutingTest.java
@@ -341,10 +341,10 @@ public class ShardRoutingTest extends AbstractFullDistribZkTestBase {
 
   // TODO: refactor some of this stuff up into a base class for use by other tests
   void doQuery(String expectedDocs, String... queryParams) throws Exception {
-    Set<String> expectedIds = new HashSet<String>( StrUtils.splitSmart(expectedDocs, ",", true) );
+    Set<String> expectedIds = new HashSet<>( StrUtils.splitSmart(expectedDocs, ",", true) );
 
     QueryResponse rsp = cloudClient.query(params(queryParams));
-    Set<String> obtainedIds = new HashSet<String>();
+    Set<String> obtainedIds = new HashSet<>();
     for (SolrDocument doc : rsp.getResults()) {
       obtainedIds.add((String) doc.get("id"));
     }
@@ -355,10 +355,10 @@ public class ShardRoutingTest extends AbstractFullDistribZkTestBase {
   void doRTG(String ids) throws Exception {
     cloudClient.query(params("qt","/get", "ids",ids));
 
-    Set<String> expectedIds = new HashSet<String>( StrUtils.splitSmart(ids, ",", true) );
+    Set<String> expectedIds = new HashSet<>( StrUtils.splitSmart(ids, ",", true) );
 
     QueryResponse rsp = cloudClient.query(params("qt","/get", "ids",ids));
-    Set<String> obtainedIds = new HashSet<String>();
+    Set<String> obtainedIds = new HashSet<>();
     for (SolrDocument doc : rsp.getResults()) {
       obtainedIds.add((String) doc.get("id"));
     }
diff --git solr/core/src/test/org/apache/solr/cloud/ShardSplitTest.java solr/core/src/test/org/apache/solr/cloud/ShardSplitTest.java
index f3d0e27..be4a675 100644
--- solr/core/src/test/org/apache/solr/cloud/ShardSplitTest.java
+++ solr/core/src/test/org/apache/solr/cloud/ShardSplitTest.java
@@ -115,7 +115,7 @@ public class ShardSplitTest extends BasicDistributedZkTest {
     Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);
     DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();
 
-    List<DocRouter.Range> subRanges = new ArrayList<DocRouter.Range>();
+    List<DocRouter.Range> subRanges = new ArrayList<>();
     List<DocRouter.Range> ranges = router.partitionRange(4, shard1Range);
 
     // test with only one range
@@ -158,7 +158,7 @@ public class ShardSplitTest extends BasicDistributedZkTest {
     final DocRouter router = clusterState.getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION).getRouter();
     Slice shard1 = clusterState.getSlice(AbstractDistribZkTestBase.DEFAULT_COLLECTION, SHARD1);
     DocRouter.Range shard1Range = shard1.getRange() != null ? shard1.getRange() : router.fullRange();
-    List<DocRouter.Range> subRanges = new ArrayList<DocRouter.Range>();
+    List<DocRouter.Range> subRanges = new ArrayList<>();
     if (usually())  {
       List<DocRouter.Range> ranges = router.partitionRange(4, shard1Range);
       // 75% of range goes to shard1_0 and the rest to shard1_1
@@ -185,7 +185,7 @@ public class ShardSplitTest extends BasicDistributedZkTest {
         int max = atLeast(random, 401);
         int sleep = atLeast(random, 25);
         log.info("SHARDSPLITTEST: Going to add " + max + " number of docs at 1 doc per " + sleep + "ms");
-        Set<String> deleted = new HashSet<String>();
+        Set<String> deleted = new HashSet<>();
         for (int id = 101; id < max; id++) {
           try {
             indexAndUpdateCount(router, ranges, docCounts, String.valueOf(id), id);
@@ -246,7 +246,7 @@ public class ShardSplitTest extends BasicDistributedZkTest {
     int maxShardsPerNode = (((numShards * replicationFactor) / getCommonCloudSolrServer()
         .getZkStateReader().getClusterState().getLiveNodes().size())) + 1;
 
-    HashMap<String, List<Integer>> collectionInfos = new HashMap<String, List<Integer>>();
+    HashMap<String, List<Integer>> collectionInfos = new HashMap<>();
     CloudSolrServer client = null;
     String shard_fld = "shard_s";
     try {
@@ -324,7 +324,7 @@ public class ShardSplitTest extends BasicDistributedZkTest {
     int maxShardsPerNode = (((numShards * replicationFactor) / getCommonCloudSolrServer()
         .getZkStateReader().getClusterState().getLiveNodes().size())) + 1;
 
-    HashMap<String, List<Integer>> collectionInfos = new HashMap<String, List<Integer>>();
+    HashMap<String, List<Integer>> collectionInfos = new HashMap<>();
     CloudSolrServer client = null;
     try {
       client = createCloudClient(null);
@@ -566,9 +566,9 @@ public class ShardSplitTest extends BasicDistributedZkTest {
 
     log.info("Actual docCount for shard1_0 = {}", shard10Count);
     log.info("Actual docCount for shard1_1 = {}", shard11Count);
-    Map<String, String> idVsVersion = new HashMap<String, String>();
-    Map<String, SolrDocument> shard10Docs = new HashMap<String, SolrDocument>();
-    Map<String, SolrDocument> shard11Docs = new HashMap<String, SolrDocument>();
+    Map<String, String> idVsVersion = new HashMap<>();
+    Map<String, SolrDocument> shard10Docs = new HashMap<>();
+    Map<String, SolrDocument> shard11Docs = new HashMap<>();
     for (int i = 0; i < response.getResults().size(); i++) {
       SolrDocument document = response.getResults().get(i);
       idVsVersion.put(document.getFieldValue("id").toString(), document.getFieldValue("_version_").toString());
diff --git solr/core/src/test/org/apache/solr/cloud/SliceStateTest.java solr/core/src/test/org/apache/solr/cloud/SliceStateTest.java
index cdf001a..3b7f769 100644
--- solr/core/src/test/org/apache/solr/cloud/SliceStateTest.java
+++ solr/core/src/test/org/apache/solr/cloud/SliceStateTest.java
@@ -34,13 +34,13 @@ import java.util.Set;
 public class SliceStateTest extends SolrTestCaseJ4 {
   @Test
   public void testDefaultSliceState() throws Exception {
-    Map<String, DocCollection> collectionStates = new HashMap<String, DocCollection>();
-    Set<String> liveNodes = new HashSet<String>();
+    Map<String, DocCollection> collectionStates = new HashMap<>();
+    Set<String> liveNodes = new HashSet<>();
     liveNodes.add("node1");
 
-    Map<String, Slice> slices = new HashMap<String, Slice>();
-    Map<String, Replica> sliceToProps = new HashMap<String, Replica>();
-    Map<String, Object> props = new HashMap<String, Object>();
+    Map<String, Slice> slices = new HashMap<>();
+    Map<String, Replica> sliceToProps = new HashMap<>();
+    Map<String, Object> props = new HashMap<>();
 
     Replica replica = new Replica("node1", props);
     sliceToProps.put("node1", replica);
diff --git solr/core/src/test/org/apache/solr/cloud/SliceStateUpdateTest.java solr/core/src/test/org/apache/solr/cloud/SliceStateUpdateTest.java
index 8dde806..50f5ba1 100644
--- solr/core/src/test/org/apache/solr/cloud/SliceStateUpdateTest.java
+++ solr/core/src/test/org/apache/solr/cloud/SliceStateUpdateTest.java
@@ -86,7 +86,7 @@ public class SliceStateUpdateTest extends SolrTestCaseJ4 {
         .getZkAddress(), "solrconfig.xml", "schema.xml");
 
     log.info("####SETUP_START " + getTestName());
-    Map<String, Object> props2 = new HashMap<String, Object>();
+    Map<String, Object> props2 = new HashMap<>();
     props2.put("configName", "conf1");
 
     ZkNodeProps zkProps2 = new ZkNodeProps(props2);
@@ -147,7 +147,7 @@ public class SliceStateUpdateTest extends SolrTestCaseJ4 {
 //        new LinkedHashMap<String, DocCollection>(clusterState.getCollectionStates());
 
     Map<String, Slice> slicesMap = clusterState.getSlicesMap("collection1");
-    Map<String, Object> props = new HashMap<String, Object>(1);
+    Map<String, Object> props = new HashMap<>(1);
     Slice slice = slicesMap.get("shard1");
     Map<String, Object> prop = slice.getProperties();
     prop.put("state", "inactive");
diff --git solr/core/src/test/org/apache/solr/cloud/SyncSliceTest.java solr/core/src/test/org/apache/solr/cloud/SyncSliceTest.java
index b1f04d8..adfceea 100644
--- solr/core/src/test/org/apache/solr/cloud/SyncSliceTest.java
+++ solr/core/src/test/org/apache/solr/cloud/SyncSliceTest.java
@@ -96,7 +96,7 @@ public class SyncSliceTest extends AbstractFullDistribZkTestBase {
     waitForThingsToLevelOut(30);
 
     del("*:*");
-    List<CloudJettyRunner> skipServers = new ArrayList<CloudJettyRunner>();
+    List<CloudJettyRunner> skipServers = new ArrayList<>();
     int docId = 0;
     indexDoc(skipServers, id, docId++, i1, 50, tlong, 50, t1,
         "to come to the aid of their country.");
@@ -156,7 +156,7 @@ public class SyncSliceTest extends AbstractFullDistribZkTestBase {
         "to come to the aid of their country.");
     
     
-    Set<CloudJettyRunner> jetties = new HashSet<CloudJettyRunner>();
+    Set<CloudJettyRunner> jetties = new HashSet<>();
     jetties.addAll(shardToJetty.get("shard1"));
     jetties.remove(leaderJetty);
     assertEquals(shardCount - 1, jetties.size());
@@ -217,7 +217,7 @@ public class SyncSliceTest extends AbstractFullDistribZkTestBase {
         "Test Setup Failure: shard1 should have just been set up to be inconsistent - but it's still consistent. Leader:"
             + leaderJetty.url + " Dead Guy:" + deadJetty.url + "skip list:" + skipServers, shardFailMessage);
     
-    jetties = new HashSet<CloudJettyRunner>();
+    jetties = new HashSet<>();
     jetties.addAll(shardToJetty.get("shard1"));
     jetties.remove(leaderJetty);
     assertEquals(shardCount - 1, jetties.size());
@@ -292,8 +292,8 @@ public class SyncSliceTest extends AbstractFullDistribZkTestBase {
   }
   
   private List<CloudJettyRunner> getRandomOtherJetty(CloudJettyRunner leader, CloudJettyRunner down) {
-    List<CloudJettyRunner> skipServers = new ArrayList<CloudJettyRunner>();
-    List<CloudJettyRunner> candidates = new ArrayList<CloudJettyRunner>();
+    List<CloudJettyRunner> skipServers = new ArrayList<>();
+    List<CloudJettyRunner> candidates = new ArrayList<>();
     candidates.addAll(shardToJetty.get("shard1"));
 
     if (leader != null) {
diff --git solr/core/src/test/org/apache/solr/cloud/TestDistribDocBasedVersion.java solr/core/src/test/org/apache/solr/cloud/TestDistribDocBasedVersion.java
index 86043b0..8e3b0b0 100755
--- solr/core/src/test/org/apache/solr/cloud/TestDistribDocBasedVersion.java
+++ solr/core/src/test/org/apache/solr/cloud/TestDistribDocBasedVersion.java
@@ -290,7 +290,7 @@ public class TestDistribDocBasedVersion extends AbstractFullDistribZkTestBase {
   void doQuery(String expectedDocs, String... queryParams) throws Exception {
 
     List<String> strs = StrUtils.splitSmart(expectedDocs, ",", true);
-    Map<String, Object> expectedIds = new HashMap<String,Object>();
+    Map<String, Object> expectedIds = new HashMap<>();
     for (int i=0; i<strs.size(); i+=2) {
       String id = strs.get(i);
       String vS = strs.get(i+1);
@@ -299,7 +299,7 @@ public class TestDistribDocBasedVersion extends AbstractFullDistribZkTestBase {
     }
 
     QueryResponse rsp = cloudClient.query(params(queryParams));
-    Map<String, Object> obtainedIds = new HashMap<String,Object>();
+    Map<String, Object> obtainedIds = new HashMap<>();
     for (SolrDocument doc : rsp.getResults()) {
       obtainedIds.put((String) doc.get("id"), doc.get(vfield));
     }
@@ -309,7 +309,7 @@ public class TestDistribDocBasedVersion extends AbstractFullDistribZkTestBase {
 
 
   void doRTG(String ids, String versions) throws Exception {
-    Map<String, Object> expectedIds = new HashMap<String,Object>();
+    Map<String, Object> expectedIds = new HashMap<>();
     List<String> strs = StrUtils.splitSmart(ids, ",", true);
     List<String> verS = StrUtils.splitSmart(versions, ",", true);
     for (int i=0; i<strs.size(); i++) {
@@ -319,7 +319,7 @@ public class TestDistribDocBasedVersion extends AbstractFullDistribZkTestBase {
     ss.query(params("qt","/get", "ids",ids));
 
     QueryResponse rsp = cloudClient.query(params("qt","/get", "ids",ids));
-    Map<String, Object> obtainedIds = new HashMap<String,Object>();
+    Map<String, Object> obtainedIds = new HashMap<>();
     for (SolrDocument doc : rsp.getResults()) {
       obtainedIds.put((String) doc.get("id"), doc.get(vfield));
     }
@@ -330,10 +330,10 @@ public class TestDistribDocBasedVersion extends AbstractFullDistribZkTestBase {
   void doRTG(String ids) throws Exception {
     ss.query(params("qt","/get", "ids",ids));
 
-    Set<String> expectedIds = new HashSet<String>( StrUtils.splitSmart(ids, ",", true) );
+    Set<String> expectedIds = new HashSet<>( StrUtils.splitSmart(ids, ",", true) );
 
     QueryResponse rsp = cloudClient.query(params("qt","/get", "ids",ids));
-    Set<String> obtainedIds = new HashSet<String>();
+    Set<String> obtainedIds = new HashSet<>();
     for (SolrDocument doc : rsp.getResults()) {
       obtainedIds.add((String) doc.get("id"));
     }
diff --git solr/core/src/test/org/apache/solr/cloud/TestHashPartitioner.java solr/core/src/test/org/apache/solr/cloud/TestHashPartitioner.java
index 37f9ca7..8ddca1d 100644
--- solr/core/src/test/org/apache/solr/cloud/TestHashPartitioner.java
+++ solr/core/src/test/org/apache/solr/cloud/TestHashPartitioner.java
@@ -146,8 +146,8 @@ public class TestHashPartitioner extends SolrTestCaseJ4 {
 
     List<String> expectedShardStr = StrUtils.splitSmart(expectedShards, ",", true);
 
-    HashSet<String> expectedSet = new HashSet<String>(expectedShardStr);
-    HashSet<String> obtainedSet = new HashSet<String>();
+    HashSet<String> expectedSet = new HashSet<>(expectedShardStr);
+    HashSet<String> obtainedSet = new HashSet<>();
     for (Slice slice : slices) {
       obtainedSet.add(slice.getName());
     }
@@ -222,7 +222,7 @@ public class TestHashPartitioner extends SolrTestCaseJ4 {
   DocCollection createCollection(int nSlices, DocRouter router) {
     List<Range> ranges = router.partitionRange(nSlices, router.fullRange());
 
-    Map<String,Slice> slices = new HashMap<String,Slice>();
+    Map<String,Slice> slices = new HashMap<>();
     for (int i=0; i<ranges.size(); i++) {
       Range range = ranges.get(i);
       Slice slice = new Slice("shard"+(i+1), null, map("range",range));
diff --git solr/core/src/test/org/apache/solr/cloud/TriLevelCompositeIdRoutingTest.java solr/core/src/test/org/apache/solr/cloud/TriLevelCompositeIdRoutingTest.java
index 26aa92c..70d0177 100644
--- solr/core/src/test/org/apache/solr/cloud/TriLevelCompositeIdRoutingTest.java
+++ solr/core/src/test/org/apache/solr/cloud/TriLevelCompositeIdRoutingTest.java
@@ -90,7 +90,7 @@ public class TriLevelCompositeIdRoutingTest extends ShardRoutingTest {
 
     commit();
 
-    HashMap<String, Integer> idMap = new HashMap<String, Integer>();
+    HashMap<String, Integer> idMap = new HashMap<>();
 
     for (int i = 1; i <= sliceCount; i++) {
 
@@ -122,7 +122,7 @@ public class TriLevelCompositeIdRoutingTest extends ShardRoutingTest {
 
     commit();
 
-    HashMap<String, Integer> idMap = new HashMap<String, Integer>();
+    HashMap<String, Integer> idMap = new HashMap<>();
 
     for (int i = 1; i <= sliceCount; i++) {
 
@@ -142,7 +142,7 @@ public class TriLevelCompositeIdRoutingTest extends ShardRoutingTest {
 
   Set<String> doQueryGetUniqueIdKeys(String... queryParams) throws Exception {
     QueryResponse rsp = cloudClient.query(params(queryParams));
-    Set<String> obtainedIdKeys = new HashSet<String>();
+    Set<String> obtainedIdKeys = new HashSet<>();
     for (SolrDocument doc : rsp.getResults()) {
       obtainedIdKeys.add(getKey((String) doc.get("id")));
     }
diff --git solr/core/src/test/org/apache/solr/cloud/ZkControllerTest.java solr/core/src/test/org/apache/solr/cloud/ZkControllerTest.java
index b143e99..64ce124 100644
--- solr/core/src/test/org/apache/solr/cloud/ZkControllerTest.java
+++ solr/core/src/test/org/apache/solr/cloud/ZkControllerTest.java
@@ -175,7 +175,7 @@ public class ZkControllerTest extends SolrTestCaseJ4 {
 
       zkClient.makePath(ZkController.CONFIGS_ZKNODE + "/" + actualConfigName, true);
       
-      Map<String,Object> props = new HashMap<String,Object>();
+      Map<String,Object> props = new HashMap<>();
       props.put("configName", actualConfigName);
       ZkNodeProps zkProps = new ZkNodeProps(props);
       zkClient.makePath(ZkStateReader.COLLECTIONS_ZKNODE + "/"
diff --git solr/core/src/test/org/apache/solr/cloud/ZkNodePropsTest.java solr/core/src/test/org/apache/solr/cloud/ZkNodePropsTest.java
index f47091a..5f68019 100644
--- solr/core/src/test/org/apache/solr/cloud/ZkNodePropsTest.java
+++ solr/core/src/test/org/apache/solr/cloud/ZkNodePropsTest.java
@@ -30,7 +30,7 @@ public class ZkNodePropsTest extends SolrTestCaseJ4 {
   @Test
   public void testBasic() throws IOException {
     
-    Map<String,Object> props = new HashMap<String,Object>();
+    Map<String,Object> props = new HashMap<>();
     props.put("prop1", "value1");
     props.put("prop2", "value2");
     props.put("prop3", "value3");
diff --git solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil.java solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil.java
index 6dae9b5..1384cb2 100644
--- solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil.java
+++ solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil.java
@@ -36,7 +36,7 @@ public class HdfsTestUtil {
   
   private static Locale savedLocale;
   
-  private static Map<MiniDFSCluster,Timer> timers = new ConcurrentHashMap<MiniDFSCluster,Timer>();
+  private static Map<MiniDFSCluster,Timer> timers = new ConcurrentHashMap<>();
 
   public static MiniDFSCluster setupClass(String dataDir) throws Exception {
     LuceneTestCase.assumeFalse("HDFS tests were disabled by -Dtests.disableHdfs",
diff --git solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java
index 5a73782..a43ef3c 100644
--- solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java
+++ solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java
@@ -98,8 +98,8 @@ public class HdfsWriteToMultipleCollectionsTest extends BasicDistributedZkTest {
     for (int i = 0; i < cnt; i++) {
       waitForRecoveriesToFinish(ACOLLECTION + i, false);
     }
-    List<CloudSolrServer> cloudServers = new ArrayList<CloudSolrServer>();
-    List<StopableIndexingThread> threads = new ArrayList<StopableIndexingThread>();
+    List<CloudSolrServer> cloudServers = new ArrayList<>();
+    List<StopableIndexingThread> threads = new ArrayList<>();
     for (int i = 0; i < cnt; i++) {
       CloudSolrServer server = new CloudSolrServer(zkServer.getZkAddress());
       server.setDefaultCollection(ACOLLECTION + i);
diff --git solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest.java solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest.java
index 9004b91..232536d 100644
--- solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest.java
+++ solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest.java
@@ -155,7 +155,7 @@ public class StressHdfsTest extends BasicDistributedZkTest {
     }
     
     // collect the data dirs
-    List<String> dataDirs = new ArrayList<String>();
+    List<String> dataDirs = new ArrayList<>();
     
     int i = 0;
     for (SolrServer client : clients) {
diff --git solr/core/src/test/org/apache/solr/core/CachingDirectoryFactoryTest.java solr/core/src/test/org/apache/solr/core/CachingDirectoryFactoryTest.java
index e8129ae..aa503b9 100644
--- solr/core/src/test/org/apache/solr/core/CachingDirectoryFactoryTest.java
+++ solr/core/src/test/org/apache/solr/core/CachingDirectoryFactoryTest.java
@@ -33,7 +33,7 @@ import org.junit.Test;
  */
 
 public class CachingDirectoryFactoryTest extends SolrTestCaseJ4 {
-  private Map<String,Tracker> dirs = new HashMap<String,Tracker>();
+  private Map<String,Tracker> dirs = new HashMap<>();
   private volatile boolean stop = false;
   
   private class Tracker {
@@ -46,7 +46,7 @@ public class CachingDirectoryFactoryTest extends SolrTestCaseJ4 {
   public void stressTest() throws Exception {
     final CachingDirectoryFactory df = new RAMDirectoryFactory();
     
-    List<Thread> threads = new ArrayList<Thread>();
+    List<Thread> threads = new ArrayList<>();
     int threadCount = 11;
     for (int i = 0; i < threadCount; i++) {
       Thread getDirThread = new GetDirThread(df);
@@ -126,7 +126,7 @@ public class CachingDirectoryFactoryTest extends SolrTestCaseJ4 {
         
         synchronized (dirs) {
           int sz = dirs.size();
-          List<Tracker> dirsList = new ArrayList<Tracker>();
+          List<Tracker> dirsList = new ArrayList<>();
           dirsList.addAll(dirs.values());
           if (sz > 0) {
             Tracker tracker = dirsList.get(Math.min(dirsList.size() - 1,
diff --git solr/core/src/test/org/apache/solr/core/CountUsageValueSourceParser.java solr/core/src/test/org/apache/solr/core/CountUsageValueSourceParser.java
index 60f454e..fa8eb85 100644
--- solr/core/src/test/org/apache/solr/core/CountUsageValueSourceParser.java
+++ solr/core/src/test/org/apache/solr/core/CountUsageValueSourceParser.java
@@ -40,7 +40,7 @@ import java.util.concurrent.atomic.AtomicInteger;
 public class CountUsageValueSourceParser extends ValueSourceParser {
 
   private static final ConcurrentMap<String,AtomicInteger> counters 
-    = new ConcurrentHashMap<String,AtomicInteger>();
+    = new ConcurrentHashMap<>();
 
   public static void clearCounters() {
     counters.clear();
diff --git solr/core/src/test/org/apache/solr/core/OpenCloseCoreStressTest.java solr/core/src/test/org/apache/solr/core/OpenCloseCoreStressTest.java
index bf921ef..73171e1 100644
--- solr/core/src/test/org/apache/solr/core/OpenCloseCoreStressTest.java
+++ solr/core/src/test/org/apache/solr/core/OpenCloseCoreStressTest.java
@@ -69,8 +69,8 @@ public class OpenCloseCoreStressTest extends SolrTestCaseJ4 {
 
   File solrHomeDirectory;
 
-  List<HttpSolrServer> indexingServers = new ArrayList<HttpSolrServer>(indexingThreads);
-  List<HttpSolrServer> queryServers = new ArrayList<HttpSolrServer>(queryThreads);
+  List<HttpSolrServer> indexingServers = new ArrayList<>(indexingThreads);
+  List<HttpSolrServer> queryServers = new ArrayList<>(queryThreads);
 
   static String savedFactory;
   
@@ -81,8 +81,8 @@ public class OpenCloseCoreStressTest extends SolrTestCaseJ4 {
   
   @Before
   public void setupServer() throws Exception {
-    coreCounts = new TreeMap<String, Long>();
-    coreNames = new ArrayList<String>();
+    coreCounts = new TreeMap<>();
+    coreNames = new ArrayList<>();
     cumulativeDocs = 0;
 
     solrHomeDirectory = new File(TEMP_DIR, "OpenCloseCoreStressTest_");
@@ -328,7 +328,7 @@ class Indexer {
   static volatile int lastCount;
   static volatile long nextTime;
 
-  ArrayList<OneIndexer> _threads = new ArrayList<OneIndexer>();
+  ArrayList<OneIndexer> _threads = new ArrayList<>();
 
   public Indexer(OpenCloseCoreStressTest OCCST, String url, List<HttpSolrServer> servers, int numThreads, int secondsToRun, Random random) {
     stopTime = System.currentTimeMillis() + (secondsToRun * 1000);
@@ -436,7 +436,7 @@ class OneIndexer extends Thread {
 class Queries {
   static AtomicBoolean _keepon = new AtomicBoolean(true);
 
-  List<Thread> _threads = new ArrayList<Thread>();
+  List<Thread> _threads = new ArrayList<>();
   static AtomicInteger _errors = new AtomicInteger(0);
   String baseUrl;
 
diff --git solr/core/src/test/org/apache/solr/core/QueryResultKeyTest.java solr/core/src/test/org/apache/solr/core/QueryResultKeyTest.java
index c97d500..d6aa2c6 100644
--- solr/core/src/test/org/apache/solr/core/QueryResultKeyTest.java
+++ solr/core/src/test/org/apache/solr/core/QueryResultKeyTest.java
@@ -178,7 +178,7 @@ public class QueryResultKeyTest extends SolrTestCaseJ4 {
    * specified ints
    */
   private List<Query> buildFiltersFromNumbers(int[] values) {
-    ArrayList<Query> filters = new ArrayList<Query>(values.length);
+    ArrayList<Query> filters = new ArrayList<>(values.length);
     for (int val : values) {
       filters.add(new FlatHashTermQuery(String.valueOf(val)));
     }
diff --git solr/core/src/test/org/apache/solr/core/SolrCoreTest.java solr/core/src/test/org/apache/solr/core/SolrCoreTest.java
index ead3bc8..1ae9277 100644
--- solr/core/src/test/org/apache/solr/core/SolrCoreTest.java
+++ solr/core/src/test/org/apache/solr/core/SolrCoreTest.java
@@ -172,7 +172,7 @@ public class SolrCoreTest extends SolrTestCaseJ4 {
     final int LOOP = 100;
     final int MT = 16;
     ExecutorService service = Executors.newFixedThreadPool(MT, new DefaultSolrThreadFactory("refCountMT"));
-    List<Callable<Integer>> callees = new ArrayList<Callable<Integer>>(MT);
+    List<Callable<Integer>> callees = new ArrayList<>(MT);
     final CoreContainer cores = h.getCoreContainer();
     for (int i = 0; i < MT; ++i) {
       Callable<Integer> call = new Callable<Integer>() {
diff --git solr/core/src/test/org/apache/solr/core/TestCoreContainer.java solr/core/src/test/org/apache/solr/core/TestCoreContainer.java
index 25d1e6a..ba94f7e 100644
--- solr/core/src/test/org/apache/solr/core/TestCoreContainer.java
+++ solr/core/src/test/org/apache/solr/core/TestCoreContainer.java
@@ -130,7 +130,7 @@ public class TestCoreContainer extends SolrTestCaseJ4 {
         }
       }
 
-      List<Thread> threads = new ArrayList<Thread>();
+      List<Thread> threads = new ArrayList<>();
       int numThreads = 4;
       for (int i = 0; i < numThreads; i++) {
         threads.add(new TestThread());
diff --git solr/core/src/test/org/apache/solr/core/TestJmxIntegration.java solr/core/src/test/org/apache/solr/core/TestJmxIntegration.java
index 42cbfed..a837d56 100644
--- solr/core/src/test/org/apache/solr/core/TestJmxIntegration.java
+++ solr/core/src/test/org/apache/solr/core/TestJmxIntegration.java
@@ -178,7 +178,7 @@ public class TestJmxIntegration extends AbstractSolrTestCase {
 
   private ObjectName getObjectName(String key, SolrInfoMBean infoBean)
           throws MalformedObjectNameException {
-    Hashtable<String, String> map = new Hashtable<String, String>();
+    Hashtable<String, String> map = new Hashtable<>();
     map.put("type", key);
     map.put("id", infoBean.getName());
     String coreName = h.getCore().getName();
diff --git solr/core/src/test/org/apache/solr/core/TestJmxMonitoredMap.java solr/core/src/test/org/apache/solr/core/TestJmxMonitoredMap.java
index f60402f..11c5a04 100644
--- solr/core/src/test/org/apache/solr/core/TestJmxMonitoredMap.java
+++ solr/core/src/test/org/apache/solr/core/TestJmxMonitoredMap.java
@@ -83,7 +83,7 @@ public class TestJmxMonitoredMap extends LuceneTestCase {
       AbstractSolrTestCase.log.info("Using port: " + port);
       String url = "service:jmx:rmi:///jndi/rmi://127.0.0.1:"+port+"/solrjmx";
       JmxConfiguration config = new JmxConfiguration(true, null, url, null);
-      monitoredMap = new JmxMonitoredMap<String, SolrInfoMBean>("", "", config);
+      monitoredMap = new JmxMonitoredMap<>("", "", config);
       JMXServiceURL u = new JMXServiceURL(url);
       connector = JMXConnectorFactory.connect(u);
       mbeanServer = connector.getMBeanServerConnection();
diff --git solr/core/src/test/org/apache/solr/core/TestLazyCores.java solr/core/src/test/org/apache/solr/core/TestLazyCores.java
index 53f414e..aeaf57a 100644
--- solr/core/src/test/org/apache/solr/core/TestLazyCores.java
+++ solr/core/src/test/org/apache/solr/core/TestLazyCores.java
@@ -251,7 +251,7 @@ public class TestLazyCores extends SolrTestCaseJ4 {
 
   @Test
   public void testRace() throws Exception {
-    final List<SolrCore> theCores = new ArrayList<SolrCore>();
+    final List<SolrCore> theCores = new ArrayList<>();
     final CoreContainer cc = init();
     try {
 
@@ -713,7 +713,7 @@ public class TestLazyCores extends SolrTestCaseJ4 {
     }
     NamedList.NamedListEntry[] entries = new NamedList.NamedListEntry[q.length / 2];
     for (int i = 0; i < q.length; i += 2) {
-      entries[i / 2] = new NamedList.NamedListEntry<String>(q[i], q[i + 1]);
+      entries[i / 2] = new NamedList.NamedListEntry<>(q[i], q[i + 1]);
     }
     return new LocalSolrQueryRequest(core, new NamedList<Object>(entries));
   }
diff --git solr/core/src/test/org/apache/solr/core/TestSolrXMLSerializer.java solr/core/src/test/org/apache/solr/core/TestSolrXMLSerializer.java
index 0443f2d..775fcb0 100644
--- solr/core/src/test/org/apache/solr/core/TestSolrXMLSerializer.java
+++ solr/core/src/test/org/apache/solr/core/TestSolrXMLSerializer.java
@@ -116,12 +116,12 @@ public class TestSolrXMLSerializer extends LuceneTestCase {
       String adminPathVal, String shareSchemaKey, String shareSchemaVal,
       String instanceDirKey, String instanceDirVal) {
     // <solr attrib="value">
-    Map<String,String> rootSolrAttribs = new HashMap<String,String>();
+    Map<String,String> rootSolrAttribs = new HashMap<>();
     rootSolrAttribs.put(sharedLibKey, sharedLibVal);
     rootSolrAttribs.put(peristentKey, persistentVal);
     
     // <solr attrib="value"> <cores attrib="value">
-    Map<String,String> coresAttribs = new HashMap<String,String>();
+    Map<String,String> coresAttribs = new HashMap<>();
     coresAttribs.put(adminPathKey, adminPathVal);
     coresAttribs.put(shareSchemaKey, shareSchemaVal);
     coresAttribs.put(defaultCoreNameKey, defaultCoreNameVal);
@@ -129,9 +129,9 @@ public class TestSolrXMLSerializer extends LuceneTestCase {
     SolrXMLDef solrXMLDef = new SolrXMLDef();
     
     // <solr attrib="value"> <cores attrib="value"> <core attrib="value">
-    List<SolrCoreXMLDef> solrCoreXMLDefs = new ArrayList<SolrCoreXMLDef>();
+    List<SolrCoreXMLDef> solrCoreXMLDefs = new ArrayList<>();
     SolrCoreXMLDef coreDef = new SolrCoreXMLDef();
-    Map<String,String> coreAttribs = new HashMap<String,String>();
+    Map<String,String> coreAttribs = new HashMap<>();
     coreAttribs.put(instanceDirKey, instanceDirVal);
     coreDef.coreAttribs = coreAttribs ;
     coreDef.coreProperties = new Properties();
@@ -142,9 +142,9 @@ public class TestSolrXMLSerializer extends LuceneTestCase {
     solrXMLDef.containerProperties = containerProperties ;
     solrXMLDef.solrAttribs = rootSolrAttribs;
     solrXMLDef.coresAttribs = coresAttribs;
-    solrXMLDef.loggingAttribs = new HashMap<String, String>();
-    solrXMLDef.loggingAttribs = new HashMap<String, String>();
-    solrXMLDef.watcherAttribs = new HashMap<String, String>();
+    solrXMLDef.loggingAttribs = new HashMap<>();
+    solrXMLDef.loggingAttribs = new HashMap<>();
+    solrXMLDef.watcherAttribs = new HashMap<>();
     return solrXMLDef;
   }
   
diff --git solr/core/src/test/org/apache/solr/core/TestSolrXmlPersistence.java solr/core/src/test/org/apache/solr/core/TestSolrXmlPersistence.java
index 22a8c8a..2977e7d 100644
--- solr/core/src/test/org/apache/solr/core/TestSolrXmlPersistence.java
+++ solr/core/src/test/org/apache/solr/core/TestSolrXmlPersistence.java
@@ -497,7 +497,7 @@ public class TestSolrXmlPersistence extends SolrTestCaseJ4 {
 
 
   private String[] getAllNodes(InputStream is) throws ParserConfigurationException, IOException, SAXException {
-    List<String> expressions = new ArrayList<String>(); // XPATH and value for all elements in the indicated XML
+    List<String> expressions = new ArrayList<>(); // XPATH and value for all elements in the indicated XML
     DocumentBuilderFactory docBuilderFactory = DocumentBuilderFactory
         .newInstance();
     DocumentBuilder docBuilder = docBuilderFactory.newDocumentBuilder();
diff --git solr/core/src/test/org/apache/solr/handler/MoreLikeThisHandlerTest.java solr/core/src/test/org/apache/solr/handler/MoreLikeThisHandlerTest.java
index 6c8d25a..28c0cd0 100644
--- solr/core/src/test/org/apache/solr/handler/MoreLikeThisHandlerTest.java
+++ solr/core/src/test/org/apache/solr/handler/MoreLikeThisHandlerTest.java
@@ -59,7 +59,7 @@ public class MoreLikeThisHandlerTest extends SolrTestCaseJ4 {
 
     // requires 'q' or single content stream
     try {
-      ArrayList<ContentStream> streams = new ArrayList<ContentStream>( 2 );
+      ArrayList<ContentStream> streams = new ArrayList<>( 2 );
       streams.add( new ContentStreamBase.StringStream( "hello" ) );
       streams.add( new ContentStreamBase.StringStream( "there" ) );
       req.setContentStreams( streams );
diff --git solr/core/src/test/org/apache/solr/handler/StandardRequestHandlerTest.java solr/core/src/test/org/apache/solr/handler/StandardRequestHandlerTest.java
index 797e73d..1a617ae 100644
--- solr/core/src/test/org/apache/solr/handler/StandardRequestHandlerTest.java
+++ solr/core/src/test/org/apache/solr/handler/StandardRequestHandlerTest.java
@@ -52,7 +52,7 @@ public class StandardRequestHandlerTest extends AbstractSolrTestCase {
     assertU(adoc("id", "12", "title", "test", "val_s1", "ccc"));
     assertU(commit());
     
-    Map<String,String> args = new HashMap<String, String>();
+    Map<String,String> args = new HashMap<>();
     args.put( CommonParams.Q, "title:test" );
     args.put( "indent", "true" );
     SolrQueryRequest req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
diff --git solr/core/src/test/org/apache/solr/handler/TestCSVLoader.java solr/core/src/test/org/apache/solr/handler/TestCSVLoader.java
index a51187c..4000d47 100644
--- solr/core/src/test/org/apache/solr/handler/TestCSVLoader.java
+++ solr/core/src/test/org/apache/solr/handler/TestCSVLoader.java
@@ -93,7 +93,7 @@ public class TestCSVLoader extends SolrTestCaseJ4 {
 
     // TODO: stop using locally defined streams once stream.file and
     // stream.body work everywhere
-    List<ContentStream> cs = new ArrayList<ContentStream>(1);
+    List<ContentStream> cs = new ArrayList<>(1);
     ContentStreamBase f = new ContentStreamBase.FileStream(new File(filename));
     f.setContentType("text/csv");
     cs.add(f);
diff --git solr/core/src/test/org/apache/solr/handler/XmlUpdateRequestHandlerTest.java solr/core/src/test/org/apache/solr/handler/XmlUpdateRequestHandlerTest.java
index b2ad087..fd198c2 100644
--- solr/core/src/test/org/apache/solr/handler/XmlUpdateRequestHandlerTest.java
+++ solr/core/src/test/org/apache/solr/handler/XmlUpdateRequestHandlerTest.java
@@ -191,7 +191,7 @@ public class XmlUpdateRequestHandlerTest extends SolrTestCaseJ4 {
 
     private class MockUpdateRequestProcessor extends UpdateRequestProcessor {
 
-      private Queue<DeleteUpdateCommand> deleteCommands = new LinkedList<DeleteUpdateCommand>();
+      private Queue<DeleteUpdateCommand> deleteCommands = new LinkedList<>();
 
       public MockUpdateRequestProcessor(UpdateRequestProcessor next) {
         super(next);
diff --git solr/core/src/test/org/apache/solr/handler/XsltUpdateRequestHandlerTest.java solr/core/src/test/org/apache/solr/handler/XsltUpdateRequestHandlerTest.java
index 1273aba..a7fb774 100644
--- solr/core/src/test/org/apache/solr/handler/XsltUpdateRequestHandlerTest.java
+++ solr/core/src/test/org/apache/solr/handler/XsltUpdateRequestHandlerTest.java
@@ -68,12 +68,12 @@ public class XsltUpdateRequestHandlerTest extends SolrTestCaseJ4 {
       " </document>" +
       "</random>";
 
-    Map<String,String> args = new HashMap<String, String>();
+    Map<String,String> args = new HashMap<>();
     args.put(CommonParams.TR, "xsl-update-handler-test.xsl");
       
     SolrCore core = h.getCore();
     LocalSolrQueryRequest req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
-    ArrayList<ContentStream> streams = new ArrayList<ContentStream>();
+    ArrayList<ContentStream> streams = new ArrayList<>();
     streams.add(new ContentStreamBase.StringStream(xml));
     req.setContentStreams(streams);
     SolrQueryResponse rsp = new SolrQueryResponse();
diff --git solr/core/src/test/org/apache/solr/handler/admin/MBeansHandlerTest.java solr/core/src/test/org/apache/solr/handler/admin/MBeansHandlerTest.java
index ce0a8d5..66ba14c 100644
--- solr/core/src/test/org/apache/solr/handler/admin/MBeansHandlerTest.java
+++ solr/core/src/test/org/apache/solr/handler/admin/MBeansHandlerTest.java
@@ -43,7 +43,7 @@ public class MBeansHandlerTest extends SolrTestCaseJ4 {
         "stats","true",
         CommonParams.WT,"xml"
      ));
-    List<ContentStream> streams = new ArrayList<ContentStream>();
+    List<ContentStream> streams = new ArrayList<>();
     streams.add(new ContentStreamBase.StringStream(xml));
     
     LocalSolrQueryRequest req = lrf.makeRequest(
diff --git solr/core/src/test/org/apache/solr/handler/admin/SystemInfoHandlerTest.java solr/core/src/test/org/apache/solr/handler/admin/SystemInfoHandlerTest.java
index fccc1a0..527b391 100644
--- solr/core/src/test/org/apache/solr/handler/admin/SystemInfoHandlerTest.java
+++ solr/core/src/test/org/apache/solr/handler/admin/SystemInfoHandlerTest.java
@@ -31,13 +31,13 @@ public class SystemInfoHandlerTest extends LuceneTestCase {
     OperatingSystemMXBean os = ManagementFactory.getOperatingSystemMXBean();
 
     // make one directly
-    SimpleOrderedMap<Object> info = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> info = new SimpleOrderedMap<>();
     info.add( "name", os.getName() );
     info.add( "version", os.getVersion() );
     info.add( "arch", os.getArch() );
 
     // make another using addGetterIfAvaliable 
-    SimpleOrderedMap<Object> info2 = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> info2 = new SimpleOrderedMap<>();
     SystemInfoHandler.addGetterIfAvaliable( os, "name", info2 );
     SystemInfoHandler.addGetterIfAvaliable( os, "version", info2 );
     SystemInfoHandler.addGetterIfAvaliable( os, "arch", info2 );
diff --git solr/core/src/test/org/apache/solr/handler/component/DebugComponentTest.java solr/core/src/test/org/apache/solr/handler/component/DebugComponentTest.java
index 521cd05..3d1b3f2 100644
--- solr/core/src/test/org/apache/solr/handler/component/DebugComponentTest.java
+++ solr/core/src/test/org/apache/solr/handler/component/DebugComponentTest.java
@@ -161,7 +161,7 @@ public class DebugComponentTest extends SolrTestCaseJ4 {
   @Test
   public void testModifyRequestTrack() {
     DebugComponent component = new DebugComponent();
-    List<SearchComponent> components = new ArrayList<SearchComponent>(1);
+    List<SearchComponent> components = new ArrayList<>(1);
     components.add(component);
     for(int i = 0; i < 10; i++) {
       SolrQueryRequest req = req("q", "test query", "distrib", "true", CommonParams.REQUEST_ID, "123456-my_rid");
@@ -196,7 +196,7 @@ public class DebugComponentTest extends SolrTestCaseJ4 {
   @Test
   public void testPrepare() throws IOException {
     DebugComponent component = new DebugComponent();
-    List<SearchComponent> components = new ArrayList<SearchComponent>(1);
+    List<SearchComponent> components = new ArrayList<>(1);
     components.add(component);
     SolrQueryRequest req;
     ResponseBuilder rb;
diff --git solr/core/src/test/org/apache/solr/handler/component/DistributedSpellCheckComponentTest.java solr/core/src/test/org/apache/solr/handler/component/DistributedSpellCheckComponentTest.java
index c4b356e..38f71cc 100644
--- solr/core/src/test/org/apache/solr/handler/component/DistributedSpellCheckComponentTest.java
+++ solr/core/src/test/org/apache/solr/handler/component/DistributedSpellCheckComponentTest.java
@@ -187,7 +187,7 @@ public class DistributedSpellCheckComponentTest extends BaseDistributedSearchTes
         collate, "true", maxCollationTries, "0", maxCollations, "1", collateExtended, "true"));
   }
   private Object[] buildRequest(String q, boolean useSpellcheckQ, String handlerName, boolean useGrouping, String... addlParams) {
-    List<Object> params = new ArrayList<Object>();
+    List<Object> params = new ArrayList<>();
     
     params.add("q");
     params.add(useSpellcheckQ ? "*:*" : q);
diff --git solr/core/src/test/org/apache/solr/handler/component/DistributedSuggestComponentTest.java solr/core/src/test/org/apache/solr/handler/component/DistributedSuggestComponentTest.java
index ddbdacf..86f8e8d 100644
--- solr/core/src/test/org/apache/solr/handler/component/DistributedSuggestComponentTest.java
+++ solr/core/src/test/org/apache/solr/handler/component/DistributedSuggestComponentTest.java
@@ -126,7 +126,7 @@ public class DistributedSuggestComponentTest extends BaseDistributedSearchTestCa
     
   }
   private Object[] buildRequest(String q, boolean useSuggestQ, String handlerName, String... addlParams) {
-    List<Object> params = new ArrayList<Object>();
+    List<Object> params = new ArrayList<>();
 
     if(useSuggestQ) {
       params.add("suggest.q");
diff --git solr/core/src/test/org/apache/solr/handler/component/DummyCustomParamSpellChecker.java solr/core/src/test/org/apache/solr/handler/component/DummyCustomParamSpellChecker.java
index bc78dfc..d107e9a 100644
--- solr/core/src/test/org/apache/solr/handler/component/DummyCustomParamSpellChecker.java
+++ solr/core/src/test/org/apache/solr/handler/component/DummyCustomParamSpellChecker.java
@@ -54,7 +54,7 @@ public class DummyCustomParamSpellChecker extends SolrSpellChecker {
 
     // sort the keys to make ordering predictable
     Iterator<String> iterator = options.customParams.getParameterNamesIterator();
-    List<String> lst = new ArrayList<String>();
+    List<String> lst = new ArrayList<>();
     while (iterator.hasNext()) {
       lst.add(iterator.next());
     }
diff --git solr/core/src/test/org/apache/solr/handler/component/QueryElevationComponentTest.java solr/core/src/test/org/apache/solr/handler/component/QueryElevationComponentTest.java
index 4b19022..e1646e6 100644
--- solr/core/src/test/org/apache/solr/handler/component/QueryElevationComponentTest.java
+++ solr/core/src/test/org/apache/solr/handler/component/QueryElevationComponentTest.java
@@ -348,7 +348,7 @@ public class QueryElevationComponentTest extends SolrTestCaseJ4 {
       init("schema12.xml");
       SolrCore core = h.getCore();
 
-      NamedList<String> args = new NamedList<String>();
+      NamedList<String> args = new NamedList<>();
       args.add(QueryElevationComponent.FIELD_TYPE, "string");
       args.add(QueryElevationComponent.CONFIG_FILE, "elevate.xml");
 
@@ -371,7 +371,7 @@ public class QueryElevationComponentTest extends SolrTestCaseJ4 {
       assertEquals(null, map.get("zzzz"));
 
       // Now test the same thing with a lowercase filter: 'lowerfilt'
-      args = new NamedList<String>();
+      args = new NamedList<>();
       args.add(QueryElevationComponent.FIELD_TYPE, "lowerfilt");
       args.add(QueryElevationComponent.CONFIG_FILE, "elevate.xml");
 
@@ -535,7 +535,7 @@ public class QueryElevationComponentTest extends SolrTestCaseJ4 {
 
       String query = "title:ipod";
 
-      Map<String, String> args = new HashMap<String, String>();
+      Map<String, String> args = new HashMap<>();
       args.put(CommonParams.Q, query);
       args.put(CommonParams.QT, "/elevate");
       args.put(CommonParams.FL, "id,score");
@@ -699,7 +699,7 @@ public class QueryElevationComponentTest extends SolrTestCaseJ4 {
       writeFile(f, "aaa", "A");
 
       QueryElevationComponent comp = (QueryElevationComponent) h.getCore().getSearchComponent("elevate");
-      NamedList<String> args = new NamedList<String>();
+      NamedList<String> args = new NamedList<>();
       args.add(QueryElevationComponent.CONFIG_FILE, testfile);
       comp.init(args);
       comp.inform(h.getCore());
diff --git solr/core/src/test/org/apache/solr/handler/component/SearchHandlerTest.java solr/core/src/test/org/apache/solr/handler/component/SearchHandlerTest.java
index 014c381..5106bdf 100644
--- solr/core/src/test/org/apache/solr/handler/component/SearchHandlerTest.java
+++ solr/core/src/test/org/apache/solr/handler/component/SearchHandlerTest.java
@@ -43,7 +43,7 @@ public class SearchHandlerTest extends SolrTestCaseJ4
     
     // Build an explicit list
     //-----------------------------------------------
-    List<String> names0 = new ArrayList<String>();
+    List<String> names0 = new ArrayList<>();
     names0.add( MoreLikeThisComponent.COMPONENT_NAME );
     
     NamedList args = new NamedList();
@@ -58,7 +58,7 @@ public class SearchHandlerTest extends SolrTestCaseJ4
 
     // Build an explicit list that includes the debug comp.
     //-----------------------------------------------
-    names0 = new ArrayList<String>();
+    names0 = new ArrayList<>();
     names0.add( FacetComponent.COMPONENT_NAME );
     names0.add( DebugComponent.COMPONENT_NAME );
     names0.add( MoreLikeThisComponent.COMPONENT_NAME );
@@ -80,10 +80,10 @@ public class SearchHandlerTest extends SolrTestCaseJ4
 
     // First/Last list
     //-----------------------------------------------
-    names0 = new ArrayList<String>();
+    names0 = new ArrayList<>();
     names0.add( MoreLikeThisComponent.COMPONENT_NAME );
     
-    List<String> names1 = new ArrayList<String>();
+    List<String> names1 = new ArrayList<>();
     names1.add( FacetComponent.COMPONENT_NAME );
     
     args = new NamedList();
diff --git solr/core/src/test/org/apache/solr/handler/component/StatsComponentTest.java solr/core/src/test/org/apache/solr/handler/component/StatsComponentTest.java
index 338ee57..1b0c1bd 100644
--- solr/core/src/test/org/apache/solr/handler/component/StatsComponentTest.java
+++ solr/core/src/test/org/apache/solr/handler/component/StatsComponentTest.java
@@ -166,7 +166,7 @@ public class StatsComponentTest extends AbstractSolrTestCase {
     assertU(adoc("id", "4"));
     assertU(commit());
 
-    Map<String, String> args = new HashMap<String, String>();
+    Map<String, String> args = new HashMap<>();
     args.put(CommonParams.Q, "*:*");
     args.put(StatsParams.STATS, "true");
     args.put(StatsParams.STATS_FIELD, "active_s");
@@ -197,7 +197,7 @@ public class StatsComponentTest extends AbstractSolrTestCase {
     assertU(adoc("id", "3"));
     assertU(commit());
 
-    Map<String, String> args = new HashMap<String, String>();
+    Map<String, String> args = new HashMap<>();
     args.put(CommonParams.Q, "*:*");
     args.put(StatsParams.STATS, "true");
     args.put(StatsParams.STATS_FIELD, "active_dt");
@@ -322,7 +322,7 @@ public class StatsComponentTest extends AbstractSolrTestCase {
     assertU(adoc("id", "4"));
     assertU(commit());
 
-    Map<String, String> args = new HashMap<String, String>();
+    Map<String, String> args = new HashMap<>();
     args.put(CommonParams.Q, "*:*");
     args.put(StatsParams.STATS, "true");
     args.put(StatsParams.STATS_FIELD, "active_i");
@@ -342,7 +342,7 @@ public class StatsComponentTest extends AbstractSolrTestCase {
     assertU(adoc("id", "4"));
     assertU(commit());
 
-    Map<String, String> args = new HashMap<String, String>();
+    Map<String, String> args = new HashMap<>();
     args.put(CommonParams.Q, "*:*");
     args.put(StatsParams.STATS, "true");
     args.put(StatsParams.STATS_FIELD, "active_s");
@@ -363,7 +363,7 @@ public class StatsComponentTest extends AbstractSolrTestCase {
     assertU(adoc("id", "3"));
     assertU(commit());
 
-    Map<String, String> args = new HashMap<String, String>();
+    Map<String, String> args = new HashMap<>();
     args.put(CommonParams.Q, "*:*");
     args.put(StatsParams.STATS, "true");
     args.put(StatsParams.STATS_FIELD, "active_dt");
diff --git solr/core/src/test/org/apache/solr/handler/component/TermVectorComponentTest.java solr/core/src/test/org/apache/solr/handler/component/TermVectorComponentTest.java
index 155a140..c9a6c63 100644
--- solr/core/src/test/org/apache/solr/handler/component/TermVectorComponentTest.java
+++ solr/core/src/test/org/apache/solr/handler/component/TermVectorComponentTest.java
@@ -210,7 +210,7 @@ public class TermVectorComponentTest extends SolrTestCaseJ4 {
      );
     
     // test each combination at random
-    final List<String> list = new ArrayList<String>();
+    final List<String> list = new ArrayList<>();
     list.addAll(Arrays.asList("json.nl","map", "qt",tv, "q", "id:0", TermVectorComponent.COMPONENT_NAME, "true"));
     String[][] options = new String[][] { { TermVectorParams.TF, "'tf':1" },
         { TermVectorParams.OFFSETS, "'offsets':{'start':20, 'end':27}" },
diff --git solr/core/src/test/org/apache/solr/highlight/FastVectorHighlighterTest.java solr/core/src/test/org/apache/solr/highlight/FastVectorHighlighterTest.java
index 7b4fe38..27dc391 100644
--- solr/core/src/test/org/apache/solr/highlight/FastVectorHighlighterTest.java
+++ solr/core/src/test/org/apache/solr/highlight/FastVectorHighlighterTest.java
@@ -67,7 +67,7 @@ public class FastVectorHighlighterTest extends SolrTestCaseJ4 {
 
   @Test
   public void test() {
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("hl", "true");
     args.put("hl.fl", "tv_text");
     args.put("hl.snippets", "2");
diff --git solr/core/src/test/org/apache/solr/highlight/HighlighterConfigTest.java solr/core/src/test/org/apache/solr/highlight/HighlighterConfigTest.java
index 0460b59..2004091 100644
--- solr/core/src/test/org/apache/solr/highlight/HighlighterConfigTest.java
+++ solr/core/src/test/org/apache/solr/highlight/HighlighterConfigTest.java
@@ -52,7 +52,7 @@ public class HighlighterConfigTest extends AbstractSolrTestCase {
       assertTrue( highlighter instanceof DummyHighlighter );
 
       // check to see that doHighlight is called from the DummyHighlighter
-      HashMap<String,String> args = new HashMap<String,String>();
+      HashMap<String,String> args = new HashMap<>();
       args.put("hl", "true");
       args.put("df", "t_text");
       args.put("hl.fl", "");
diff --git solr/core/src/test/org/apache/solr/highlight/HighlighterTest.java solr/core/src/test/org/apache/solr/highlight/HighlighterTest.java
index 22078ed..7498974 100644
--- solr/core/src/test/org/apache/solr/highlight/HighlighterTest.java
+++ solr/core/src/test/org/apache/solr/highlight/HighlighterTest.java
@@ -83,7 +83,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
 
   @Test
   public void testMergeContiguous() throws Exception {
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put(HighlightParams.HIGHLIGHT, "true");
     args.put("df", "t_text");
     args.put(HighlightParams.FIELDS, "");
@@ -132,7 +132,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
   public void testTermVecHighlight() {
 
     // do summarization using term vectors
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("hl", "true");
     args.put("hl.fl", "tv_text");
     args.put("hl.snippets", "2");
@@ -154,7 +154,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
   @Test
   public void testTermVectorWithoutOffsetsHighlight() {
 
-    HashMap<String,String> args = new HashMap<String, String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("hl", "true");
     args.put("hl.fl", "tv_no_off_text");
 
@@ -197,7 +197,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
   public void testTermVecMultiValuedHighlight() throws Exception {
 
     // do summarization using term vectors on multivalued field
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("hl", "true");
     args.put("hl.fl", "tv_mv_text");
     args.put("hl.snippets", "2");
@@ -224,7 +224,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
   public void testTermVecMultiValuedHighlight2() throws Exception {
 
     // do summarization using term vectors on multivalued field
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("hl", "true");
     args.put("hl.fl", "tv_mv_text");
     args.put("hl.snippets", "2");
@@ -249,7 +249,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
   public void testDisMaxHighlight() {
 
     // same test run through dismax handler
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("hl", "true");
     args.put("hl.fl", "tv_text");
     args.put("qf", "tv_text");
@@ -277,7 +277,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
   public void testMultiValueAnalysisHighlight() {
 
     // do summarization using re-analysis of the field
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("hl", "true");
     args.put("hl.fl", "textgap");
     args.put("df", "textgap");
@@ -299,7 +299,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
   
   @Test
   public void testMultiValueBestFragmentHighlight() {
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("hl", "true");
     args.put("hl.fl", "textgap");
     args.put("df", "textgap");
@@ -338,7 +338,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
   public void testDefaultFieldHighlight() {
 
     // do summarization using re-analysis of the field
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("hl", "true");
     args.put("df", "t_text");
     args.put("hl.fl", "");
@@ -361,7 +361,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
   public void testHighlightDisabled() {
 
     // ensure highlighting can be explicitly disabled
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("hl", "false");
     args.put("hl.fl", "t_text");
     TestHarness.LocalRequestFactory sumLRF = h.getRequestFactory(
@@ -379,7 +379,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
   public void testTwoFieldHighlight() {
 
     // do summarization using re-analysis of the field
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("hl", "true");
     args.put("hl.fl", "t_text tv_text");
     TestHarness.LocalRequestFactory sumLRF = h.getRequestFactory(
@@ -405,7 +405,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
      assertU(commit());
      assertU(optimize());
      
-     HashMap<String,String> args = new HashMap<String,String>();
+     HashMap<String,String> args = new HashMap<>();
      args.put("hl", "true");
      args.put("hl.fl", "t_text1 t_text2");
      
@@ -449,7 +449,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
   public void testCustomSimpleFormatterHighlight() {
 
     // do summarization using a custom formatter
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("hl", "true");
     args.put("hl.fl", "t_text");
     args.put("hl.simple.pre","<B>");
@@ -482,7 +482,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
   @Test
   public void testLongFragment() {
 
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("hl", "true");
     args.put("hl.fl", "tv_text");
     TestHarness.LocalRequestFactory sumLRF = h.getRequestFactory(
@@ -503,7 +503,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
 
   @Test
   public void testMaxChars() {
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("fl", "id score");
     args.put("hl", "true");
     args.put("hl.snippets", "10");
@@ -538,7 +538,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
   
   @Test
   public void testRegexFragmenter() {
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("fl", "id score");
     args.put("hl", "true");
     args.put("hl.snippets", "10");
@@ -585,7 +585,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
      assertU(optimize());
 
      // default length
-     HashMap<String,String> args = new HashMap<String,String>();
+     HashMap<String,String> args = new HashMap<>();
      args.put("hl", "true");
      args.put("hl.fl", "tv_text");
      TestHarness.LocalRequestFactory sumLRF = h.getRequestFactory(
@@ -628,7 +628,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
      assertU(optimize());
 
     // do summarization
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("hl", "true");
     args.put("hl.fragsize","0");
     args.put("hl.fl", "t_text");
@@ -674,7 +674,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
   
   @Test
   public void testPhraseHighlighter() {
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("hl", "true");
     args.put("hl.fl", "t_text");
     args.put("hl.fragsize", "40");
@@ -731,7 +731,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
   
   @Test
   public void testGetHighlightFields() {
-    HashMap<String, String> args = new HashMap<String, String>();
+    HashMap<String, String> args = new HashMap<>();
     args.put("fl", "id score");
     args.put("hl", "true");
     args.put("hl.fl", "t*");
@@ -776,7 +776,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
   public void testDefaultFieldPrefixWildcardHighlight() {
 
     // do summarization using re-analysis of the field
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("hl", "true");
     args.put("df", "t_text");
     args.put("hl.fl", "");
@@ -800,7 +800,7 @@ public class HighlighterTest extends SolrTestCaseJ4 {
   public void testDefaultFieldNonPrefixWildcardHighlight() {
 
     // do summarization using re-analysis of the field
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put("hl", "true");
     args.put("df", "t_text");
     args.put("hl.fl", "");
diff --git solr/core/src/test/org/apache/solr/request/SimpleFacetsTest.java solr/core/src/test/org/apache/solr/request/SimpleFacetsTest.java
index b516200..a49adbc 100644
--- solr/core/src/test/org/apache/solr/request/SimpleFacetsTest.java
+++ solr/core/src/test/org/apache/solr/request/SimpleFacetsTest.java
@@ -48,7 +48,7 @@ public class SimpleFacetsTest extends SolrTestCaseJ4 {
       assertU(commit());
   }
 
-  static ArrayList<String[]> pendingDocs = new ArrayList<String[]>();
+  static ArrayList<String[]> pendingDocs = new ArrayList<>();
 
   // committing randomly gives different looking segments each time
   static void add_doc(String... fieldsAndValues) {
diff --git solr/core/src/test/org/apache/solr/request/TestFaceting.java solr/core/src/test/org/apache/solr/request/TestFaceting.java
index a6c1dae..05fabc2 100644
--- solr/core/src/test/org/apache/solr/request/TestFaceting.java
+++ solr/core/src/test/org/apache/solr/request/TestFaceting.java
@@ -272,7 +272,7 @@ public class TestFaceting extends SolrTestCaseJ4 {
   public void testTrieFields() {
     // make sure that terms are correctly filtered even for trie fields that index several
     // terms for a single value
-    List<String> fields = new ArrayList<String>();
+    List<String> fields = new ArrayList<>();
     fields.add("id");
     fields.add("7");
     final String[] suffixes = new String[] {"ti", "tis", "tf", "tfs", "tl", "tls", "td", "tds"};
diff --git solr/core/src/test/org/apache/solr/request/TestWriterPerf.java solr/core/src/test/org/apache/solr/request/TestWriterPerf.java
index ab404df..d9a760f 100644
--- solr/core/src/test/org/apache/solr/request/TestWriterPerf.java
+++ solr/core/src/test/org/apache/solr/request/TestWriterPerf.java
@@ -67,7 +67,7 @@ public class TestWriterPerf extends AbstractSolrTestCase {
 
 
   void index(Object... olst) {
-    ArrayList<String> lst = new ArrayList<String>();
+    ArrayList<String> lst = new ArrayList<>();
     for (Object o : olst) lst.add(o.toString());
     assertU(adoc(lst.toArray(new String[lst.size()])));
   }
diff --git solr/core/src/test/org/apache/solr/response/TestPHPSerializedResponseWriter.java solr/core/src/test/org/apache/solr/response/TestPHPSerializedResponseWriter.java
index d31190e..722fac3 100644
--- solr/core/src/test/org/apache/solr/response/TestPHPSerializedResponseWriter.java
+++ solr/core/src/test/org/apache/solr/response/TestPHPSerializedResponseWriter.java
@@ -81,7 +81,7 @@ public class TestPHPSerializedResponseWriter extends SolrTestCaseJ4 {
 
     // we use LinkedHashMap because we are doing a string comparison 
     // later and we need predictible ordering
-    LinkedHashMap<String,String> nl = new LinkedHashMap<String,String>();
+    LinkedHashMap<String,String> nl = new LinkedHashMap<>();
     nl.put("data4.1", "hashmap");
     nl.put("data4.2", "hello");
     d.addField("data4",nl);
diff --git solr/core/src/test/org/apache/solr/rest/SolrRestletTestBase.java solr/core/src/test/org/apache/solr/rest/SolrRestletTestBase.java
index 5ab0681..ce01b05 100644
--- solr/core/src/test/org/apache/solr/rest/SolrRestletTestBase.java
+++ solr/core/src/test/org/apache/solr/rest/SolrRestletTestBase.java
@@ -27,7 +27,7 @@ import java.util.TreeMap;
 abstract public class SolrRestletTestBase extends RestTestBase {
   @BeforeClass
   public static void init() throws Exception {
-    final SortedMap<ServletHolder,String> extraServlets = new TreeMap<ServletHolder,String>();
+    final SortedMap<ServletHolder,String> extraServlets = new TreeMap<>();
     final ServletHolder solrRestApi = new ServletHolder("SolrRestApi", ServerServlet.class);
     solrRestApi.setInitParameter("org.restlet.application", "org.apache.solr.rest.SolrRestApi");
     extraServlets.put(solrRestApi, "/schema/*");  // '/schema/*' matches '/schema', '/schema/', and '/schema/whatever...'
diff --git solr/core/src/test/org/apache/solr/rest/schema/TestClassNameShortening.java solr/core/src/test/org/apache/solr/rest/schema/TestClassNameShortening.java
index ef5cd46..d73c5ef 100644
--- solr/core/src/test/org/apache/solr/rest/schema/TestClassNameShortening.java
+++ solr/core/src/test/org/apache/solr/rest/schema/TestClassNameShortening.java
@@ -30,7 +30,7 @@ public class TestClassNameShortening extends RestTestBase {
 
   @BeforeClass
   public static void init() throws Exception {
-    final SortedMap<ServletHolder,String> extraServlets = new TreeMap<ServletHolder,String>();
+    final SortedMap<ServletHolder,String> extraServlets = new TreeMap<>();
     final ServletHolder solrRestApi = new ServletHolder("SolrRestApi", ServerServlet.class);
     solrRestApi.setInitParameter("org.restlet.application", "org.apache.solr.rest.SolrRestApi");
     extraServlets.put(solrRestApi, "/schema/*");  // '/schema/*' matches '/schema', '/schema/', and '/schema/whatever...'
diff --git solr/core/src/test/org/apache/solr/rest/schema/TestManagedSchemaFieldResource.java solr/core/src/test/org/apache/solr/rest/schema/TestManagedSchemaFieldResource.java
index 7c679b3..71e1864 100644
--- solr/core/src/test/org/apache/solr/rest/schema/TestManagedSchemaFieldResource.java
+++ solr/core/src/test/org/apache/solr/rest/schema/TestManagedSchemaFieldResource.java
@@ -46,7 +46,7 @@ public class TestManagedSchemaFieldResource extends RestTestBase {
     tmpConfDir = new File(tmpSolrHome, confDir);
     FileUtils.copyDirectory(new File(TEST_HOME()), tmpSolrHome.getAbsoluteFile());
     
-    final SortedMap<ServletHolder,String> extraServlets = new TreeMap<ServletHolder,String>();
+    final SortedMap<ServletHolder,String> extraServlets = new TreeMap<>();
     final ServletHolder solrRestApi = new ServletHolder("SolrRestApi", ServerServlet.class);
     solrRestApi.setInitParameter("org.restlet.application", "org.apache.solr.rest.SolrRestApi");
     extraServlets.put(solrRestApi, "/schema/*");  // '/schema/*' matches '/schema', '/schema/', and '/schema/whatever...'
diff --git solr/core/src/test/org/apache/solr/rest/schema/TestSerializedLuceneMatchVersion.java solr/core/src/test/org/apache/solr/rest/schema/TestSerializedLuceneMatchVersion.java
index cfe66fc..c9d4816 100644
--- solr/core/src/test/org/apache/solr/rest/schema/TestSerializedLuceneMatchVersion.java
+++ solr/core/src/test/org/apache/solr/rest/schema/TestSerializedLuceneMatchVersion.java
@@ -30,7 +30,7 @@ public class TestSerializedLuceneMatchVersion extends RestTestBase {
 
   @BeforeClass
   public static void init() throws Exception {
-    final SortedMap<ServletHolder,String> extraServlets = new TreeMap<ServletHolder,String>();
+    final SortedMap<ServletHolder,String> extraServlets = new TreeMap<>();
     final ServletHolder solrRestApi = new ServletHolder("SolrRestApi", ServerServlet.class);
     solrRestApi.setInitParameter("org.restlet.application", "org.apache.solr.rest.SolrRestApi");
     extraServlets.put(solrRestApi, "/schema/*");  // '/schema/*' matches '/schema', '/schema/', and '/schema/whatever...'
diff --git solr/core/src/test/org/apache/solr/schema/ChangedSchemaMergeTest.java solr/core/src/test/org/apache/solr/schema/ChangedSchemaMergeTest.java
index 56247c1..6a09ef8 100644
--- solr/core/src/test/org/apache/solr/schema/ChangedSchemaMergeTest.java
+++ solr/core/src/test/org/apache/solr/schema/ChangedSchemaMergeTest.java
@@ -45,7 +45,7 @@ public class ChangedSchemaMergeTest extends SolrTestCaseJ4 {
 
   private void addDoc(SolrCore core, String... fieldValues) throws IOException {
     UpdateHandler updater = core.getUpdateHandler();
-    AddUpdateCommand cmd = new AddUpdateCommand(new LocalSolrQueryRequest(core, new NamedList<Object>()));
+    AddUpdateCommand cmd = new AddUpdateCommand(new LocalSolrQueryRequest(core, new NamedList<>()));
     cmd.solrDoc = sdoc((Object[]) fieldValues);
     updater.addDoc(cmd);
   }
@@ -83,7 +83,7 @@ public class ChangedSchemaMergeTest extends SolrTestCaseJ4 {
       addDoc(changed, "id", "2", "which", "15", "text", "some stuff with which");
       addDoc(changed, "id", "3", "which", "15", "text", "some stuff with which");
       addDoc(changed, "id", "4", "which", "15", "text", "some stuff with which");
-      SolrQueryRequest req = new LocalSolrQueryRequest(changed, new NamedList<Object>());
+      SolrQueryRequest req = new LocalSolrQueryRequest(changed, new NamedList<>());
       changed.getUpdateHandler().commit(new CommitUpdateCommand(req, false));
 
       // write the new schema out and make it current
diff --git solr/core/src/test/org/apache/solr/schema/CopyFieldTest.java solr/core/src/test/org/apache/solr/schema/CopyFieldTest.java
index 31b0037..3b09d82 100644
--- solr/core/src/test/org/apache/solr/schema/CopyFieldTest.java
+++ solr/core/src/test/org/apache/solr/schema/CopyFieldTest.java
@@ -128,7 +128,7 @@ public class CopyFieldTest extends SolrTestCaseJ4 {
       assertU(adoc("id", "10", "title", "test copy field", "text_en", "this is a simple test of the copy field functionality"));
       assertU(commit());
       
-      Map<String,String> args = new HashMap<String, String>();
+      Map<String,String> args = new HashMap<>();
       args.put( CommonParams.Q, "text_en:simple" );
       args.put( "indent", "true" );
       SolrQueryRequest req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
@@ -138,7 +138,7 @@ public class CopyFieldTest extends SolrTestCaseJ4 {
               ,"//result/doc[1]/int[@name='id'][.='10']"
               );
       
-      args = new HashMap<String, String>();
+      args = new HashMap<>();
       args.put( CommonParams.Q, "highlight:simple" );
       args.put( "indent", "true" );
       req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
@@ -148,14 +148,14 @@ public class CopyFieldTest extends SolrTestCaseJ4 {
               ,"//result/doc[1]/arr[@name='highlight']/str[.='this is a simple test of ']"
               );
 
-      args = new HashMap<String, String>();
+      args = new HashMap<>();
       args.put( CommonParams.Q, "text_en:functionality" );
       args.put( "indent", "true" );
       req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
       assertQ("Make sure they got in", req
               ,"//*[@numFound='1']");
       
-      args = new HashMap<String, String>();
+      args = new HashMap<>();
       args.put( CommonParams.Q, "highlight:functionality" );
       args.put( "indent", "true" );
       req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
@@ -189,7 +189,7 @@ public class CopyFieldTest extends SolrTestCaseJ4 {
     assertU(adoc("id", "A5", "sku1", "10-1839ACX-93", "sku2", "AAM46"));
     assertU(commit());
 
-    Map<String,String> args = new HashMap<String, String>();
+    Map<String,String> args = new HashMap<>();
     args.put( CommonParams.Q, "text:AAM46" );
     args.put( "indent", "true" );
     SolrQueryRequest req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
@@ -198,7 +198,7 @@ public class CopyFieldTest extends SolrTestCaseJ4 {
         ,"//result/doc[1]/str[@name='id'][.='A5']"
     );
 
-    args = new HashMap<String, String>();
+    args = new HashMap<>();
     args.put( CommonParams.Q, "1_s:10-1839ACX-93" );
     args.put( "indent", "true" );
     req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
@@ -208,14 +208,14 @@ public class CopyFieldTest extends SolrTestCaseJ4 {
         ,"//result/doc[1]/arr[@name='sku1']/str[.='10-1839ACX-93']"
     );
 
-    args = new HashMap<String, String>();
+    args = new HashMap<>();
     args.put( CommonParams.Q, "1_dest_sub_s:10-1839ACX-93" );
     args.put( "indent", "true" );
     req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
     assertQ("sku1 copied to *_dest_sub_s (*_s subset pattern)", req
         ,"//*[@numFound='1']");
 
-    args = new HashMap<String, String>();
+    args = new HashMap<>();
     args.put( CommonParams.Q, "dest_sub_no_ast_s:AAM46" );
     args.put( "indent", "true" );
     req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
@@ -237,7 +237,7 @@ public class CopyFieldTest extends SolrTestCaseJ4 {
     assertU(adoc("id", "A5", "sku1", "10-1839ACX-93", "testing123_s", "AAM46"));
     assertU(commit());
 
-    Map<String,String> args = new HashMap<String, String>();
+    Map<String,String> args = new HashMap<>();
     args.put( CommonParams.Q, "text:AAM46" );
     args.put( "indent", "true" );
     SolrQueryRequest req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
diff --git solr/core/src/test/org/apache/solr/schema/IndexSchemaTest.java solr/core/src/test/org/apache/solr/schema/IndexSchemaTest.java
index 5777e16..1e99720 100644
--- solr/core/src/test/org/apache/solr/schema/IndexSchemaTest.java
+++ solr/core/src/test/org/apache/solr/schema/IndexSchemaTest.java
@@ -48,7 +48,7 @@ public class IndexSchemaTest extends SolrTestCaseJ4 {
     assertU(adoc("id", "10", "title", "test", "aaa_dynamic", "aaa"));
     assertU(commit());
 
-    Map<String,String> args = new HashMap<String, String>();
+    Map<String,String> args = new HashMap<>();
     args.put( CommonParams.Q, "title:test" );
     args.put( "indent", "true" );
     SolrQueryRequest req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
@@ -58,7 +58,7 @@ public class IndexSchemaTest extends SolrTestCaseJ4 {
             ,"//result/doc[1]/int[@name='id'][.='10']"
             );
 
-    args = new HashMap<String, String>();
+    args = new HashMap<>();
     args.put( CommonParams.Q, "aaa_dynamic:aaa" );
     args.put( "indent", "true" );
     req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
@@ -67,7 +67,7 @@ public class IndexSchemaTest extends SolrTestCaseJ4 {
             ,"//result/doc[1]/int[@name='id'][.='10']"
             );
 
-    args = new HashMap<String, String>();
+    args = new HashMap<>();
     args.put( CommonParams.Q, "dynamic_aaa:aaa" );
     args.put( "indent", "true" );
     req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
diff --git solr/core/src/test/org/apache/solr/schema/MockExchangeRateProvider.java solr/core/src/test/org/apache/solr/schema/MockExchangeRateProvider.java
index a8656aa..e342e01 100644
--- solr/core/src/test/org/apache/solr/schema/MockExchangeRateProvider.java
+++ solr/core/src/test/org/apache/solr/schema/MockExchangeRateProvider.java
@@ -29,7 +29,7 @@ import org.apache.solr.common.SolrException.ErrorCode;
  * Simple mock provider with fixed rates and some assertions
  */
 public class MockExchangeRateProvider implements ExchangeRateProvider {
-  private static Map<String,Double> map = new HashMap<String,Double>();
+  private static Map<String,Double> map = new HashMap<>();
   static {
     map.put("USD,EUR", 0.8);
     map.put("EUR,USD", 1.2);
@@ -59,7 +59,7 @@ public class MockExchangeRateProvider implements ExchangeRateProvider {
     Set<String> currenciesPairs = map.keySet();
     Set<String> returnSet;
     
-    returnSet = new HashSet<String>();
+    returnSet = new HashSet<>();
     for (String c : currenciesPairs) {
       String[] pairs = c.split(",");
       returnSet.add(pairs[0]);
diff --git solr/core/src/test/org/apache/solr/schema/ModifyConfFileTest.java solr/core/src/test/org/apache/solr/schema/ModifyConfFileTest.java
index 23bc5d5..e4c4e27 100644
--- solr/core/src/test/org/apache/solr/schema/ModifyConfFileTest.java
+++ solr/core/src/test/org/apache/solr/schema/ModifyConfFileTest.java
@@ -82,7 +82,7 @@ public class ModifyConfFileTest extends SolrTestCaseJ4 {
       core.execute(handler, new LocalSolrQueryRequest(core, params), rsp);
       assertEquals(rsp.getException().getMessage(), "No file name specified for write operation.");
 
-      ArrayList<ContentStream> streams = new ArrayList<ContentStream>( 2 );
+      ArrayList<ContentStream> streams = new ArrayList<>( 2 );
       streams.add(new ContentStreamBase.StringStream("Testing rewrite of schema.xml file." ) );
 
       params = params("op", "write", "file", "bogus.txt");
diff --git solr/core/src/test/org/apache/solr/schema/OpenExchangeRatesOrgProviderTest.java solr/core/src/test/org/apache/solr/schema/OpenExchangeRatesOrgProviderTest.java
index 5aa603b..b8baff5 100644
--- solr/core/src/test/org/apache/solr/schema/OpenExchangeRatesOrgProviderTest.java
+++ solr/core/src/test/org/apache/solr/schema/OpenExchangeRatesOrgProviderTest.java
@@ -34,7 +34,7 @@ public class OpenExchangeRatesOrgProviderTest extends SolrTestCaseJ4 {
 
   OpenExchangeRatesOrgProvider oerp;
   ResourceLoader loader;
-  private final Map<String,String> mockParams = new HashMap<String,String>();
+  private final Map<String,String> mockParams = new HashMap<>();
 
 
   @Override
@@ -59,7 +59,7 @@ public class OpenExchangeRatesOrgProviderTest extends SolrTestCaseJ4 {
                  "open-exchange-rates.json", oerp.ratesFileLocation);
     assertEquals("Wrong default interval", (1440*60), oerp.refreshIntervalSeconds);
 
-    Map<String,String> params = new HashMap<String,String>();
+    Map<String,String> params = new HashMap<>();
     params.put(OpenExchangeRatesOrgProvider.PARAM_RATES_FILE_LOCATION, 
                "http://foo.bar/baz");
     params.put(OpenExchangeRatesOrgProvider.PARAM_REFRESH_INTERVAL, "100");
diff --git solr/core/src/test/org/apache/solr/schema/PreAnalyzedFieldTest.java solr/core/src/test/org/apache/solr/schema/PreAnalyzedFieldTest.java
index 119ee2f..f7135f4 100644
--- solr/core/src/test/org/apache/solr/schema/PreAnalyzedFieldTest.java
+++ solr/core/src/test/org/apache/solr/schema/PreAnalyzedFieldTest.java
@@ -85,7 +85,7 @@ public class PreAnalyzedFieldTest extends SolrTestCaseJ4 {
   public void testValidSimple() {
     PreAnalyzedField paf = new PreAnalyzedField();
     // use Simple format
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put(PreAnalyzedField.PARSER_IMPL, SimplePreAnalyzedParser.class.getName());
     paf.init(h.getCore().getLatestSchema(), args);
     PreAnalyzedParser parser = new SimplePreAnalyzedParser();
@@ -128,7 +128,7 @@ public class PreAnalyzedFieldTest extends SolrTestCaseJ4 {
   public void testParsers() {
     PreAnalyzedField paf = new PreAnalyzedField();
     // use Simple format
-    HashMap<String,String> args = new HashMap<String,String>();
+    HashMap<String,String> args = new HashMap<>();
     args.put(PreAnalyzedField.PARSER_IMPL, SimplePreAnalyzedParser.class.getName());
     paf.init(h.getCore().getLatestSchema(), args);
     try {
diff --git solr/core/src/test/org/apache/solr/schema/PrimitiveFieldTypeTest.java solr/core/src/test/org/apache/solr/schema/PrimitiveFieldTypeTest.java
index 4649103..6581e4f 100644
--- solr/core/src/test/org/apache/solr/schema/PrimitiveFieldTypeTest.java
+++ solr/core/src/test/org/apache/solr/schema/PrimitiveFieldTypeTest.java
@@ -45,7 +45,7 @@ public class PrimitiveFieldTypeTest extends SolrTestCaseJ4 {
     System.setProperty("solr.test.sys.prop2", "proptwo");
     System.setProperty("solr.allow.unsafe.resourceloading", "true");
 
-    initMap = new HashMap<String,String>();
+    initMap = new HashMap<>();
     config = new SolrConfig(new SolrResourceLoader("solr/collection1"), testConfHome + "solrconfig.xml", null);
   }
   
diff --git solr/core/src/test/org/apache/solr/schema/SortableBinaryField.java solr/core/src/test/org/apache/solr/schema/SortableBinaryField.java
index 042ea10..ea423ab 100644
--- solr/core/src/test/org/apache/solr/schema/SortableBinaryField.java
+++ solr/core/src/test/org/apache/solr/schema/SortableBinaryField.java
@@ -45,7 +45,7 @@ public class SortableBinaryField extends BinaryField {
   @Override
   public List<StorableField> createFields(SchemaField field, Object value, float boost) {
     if (field.hasDocValues()) {
-      List<StorableField> fields = new ArrayList<StorableField>();
+      List<StorableField> fields = new ArrayList<>();
       StorableField storedField = createField(field, value, boost);
       fields.add(storedField);
       ByteBuffer byteBuffer = toObject(storedField);
diff --git solr/core/src/test/org/apache/solr/schema/TestCloudManagedSchemaAddField.java solr/core/src/test/org/apache/solr/schema/TestCloudManagedSchemaAddField.java
index b525e4e..b60b3a0 100644
--- solr/core/src/test/org/apache/solr/schema/TestCloudManagedSchemaAddField.java
+++ solr/core/src/test/org/apache/solr/schema/TestCloudManagedSchemaAddField.java
@@ -57,14 +57,14 @@ public class TestCloudManagedSchemaAddField extends AbstractFullDistribZkTestBas
   
   @Override
   public SortedMap<ServletHolder,String> getExtraServlets() {
-    final SortedMap<ServletHolder,String> extraServlets = new TreeMap<ServletHolder,String>();
+    final SortedMap<ServletHolder,String> extraServlets = new TreeMap<>();
     final ServletHolder solrRestApi = new ServletHolder("SolrRestApi", ServerServlet.class);
     solrRestApi.setInitParameter("org.restlet.application", "org.apache.solr.rest.SolrRestApi");
     extraServlets.put(solrRestApi, "/schema/*");  // '/schema/*' matches '/schema', '/schema/', and '/schema/whatever...'
     return extraServlets;
   }
   
-  private List<RestTestHarness> restTestHarnesses = new ArrayList<RestTestHarness>();
+  private List<RestTestHarness> restTestHarnesses = new ArrayList<>();
   
   private void setupHarnesses() {
     for (int i = 0 ; i < clients.size() ; ++i) {
diff --git solr/core/src/test/org/apache/solr/schema/TestManagedSchema.java solr/core/src/test/org/apache/solr/schema/TestManagedSchema.java
index a127b67..2c95ef6 100644
--- solr/core/src/test/org/apache/solr/schema/TestManagedSchema.java
+++ solr/core/src/test/org/apache/solr/schema/TestManagedSchema.java
@@ -176,7 +176,7 @@ public class TestManagedSchema extends AbstractBadConfigTestBase {
     String managedSchemaContents = FileUtils.readFileToString(managedSchemaFile, "UTF-8");
     assertFalse(managedSchemaContents.contains("\"new_field\""));
     
-    Map<String,Object> options = new HashMap<String,Object>();
+    Map<String,Object> options = new HashMap<>();
     options.put("stored", "false");
     IndexSchema oldSchema = h.getCore().getLatestSchema();
     String fieldName = "new_field";
@@ -225,7 +225,7 @@ public class TestManagedSchema extends AbstractBadConfigTestBase {
     assertU(commit());
     assertQ(req("new_field:thing1"), "//*[@numFound='0']");
 
-    Map<String,Object> options = new HashMap<String,Object>();
+    Map<String,Object> options = new HashMap<>();
     options.put("stored", "false");
     IndexSchema oldSchema = h.getCore().getLatestSchema();
     String fieldName = "new_field";
@@ -252,7 +252,7 @@ public class TestManagedSchema extends AbstractBadConfigTestBase {
     String errString = "Field 'str' already exists.";
     ignoreException(Pattern.quote(errString));
     try {
-      Map<String,Object> options = new HashMap<String,Object>();
+      Map<String,Object> options = new HashMap<>();
       IndexSchema oldSchema = h.getCore().getLatestSchema();
       String fieldName = "str";
       String fieldType = "string";
@@ -280,7 +280,7 @@ public class TestManagedSchema extends AbstractBadConfigTestBase {
     System.setProperty("managed.schema.mutable", "true");
     initCore("solrconfig-managed-schema.xml", "schema-one-field-no-dynamic-field.xml", tmpSolrHome.getPath());
 
-    Map<String,Object> options = new HashMap<String,Object>();
+    Map<String,Object> options = new HashMap<>();
     options.put("stored", "false");
     IndexSchema oldSchema = h.getCore().getLatestSchema();
     String fieldName = "new_field";
@@ -320,7 +320,7 @@ public class TestManagedSchema extends AbstractBadConfigTestBase {
     String errString = "Can't add dynamic field '*_s'.";
     ignoreException(Pattern.quote(errString));
     try {
-      Map<String,Object> options = new HashMap<String,Object>();
+      Map<String,Object> options = new HashMap<>();
       IndexSchema oldSchema = h.getCore().getLatestSchema();
       String fieldName = "*_s";
       String fieldType = "string";
@@ -356,7 +356,7 @@ public class TestManagedSchema extends AbstractBadConfigTestBase {
     assertNull("Field '" + fieldName + "' is present in the schema", 
                h.getCore().getLatestSchema().getFieldOrNull(fieldName));
 
-    Map<String,Object> options = new HashMap<String,Object>();
+    Map<String,Object> options = new HashMap<>();
     IndexSchema oldSchema = h.getCore().getLatestSchema();
     String fieldType = "string_disk";
     SchemaField newField = oldSchema.newField(fieldName, fieldType, options);
@@ -384,7 +384,7 @@ public class TestManagedSchema extends AbstractBadConfigTestBase {
     assertNull("Field '" + fieldName + "' is present in the schema",
         h.getCore().getLatestSchema().getFieldOrNull(fieldName));
 
-    Map<String,Object> options = new HashMap<String,Object>();
+    Map<String,Object> options = new HashMap<>();
     IndexSchema oldSchema = h.getCore().getLatestSchema();
     String fieldType = "text";
     SchemaField newField = oldSchema.newField(fieldName, fieldType, options);
@@ -409,7 +409,7 @@ public class TestManagedSchema extends AbstractBadConfigTestBase {
     String managedSchemaContents = FileUtils.readFileToString(managedSchemaFile, "UTF-8");
     assertFalse(managedSchemaContents.contains("\"new_field\""));
 
-    Map<String,Object> options = new HashMap<String,Object>();
+    Map<String,Object> options = new HashMap<>();
     options.put("stored", "false");
     IndexSchema oldSchema = h.getCore().getLatestSchema();
     assertEquals("str", oldSchema.getUniqueKeyField().getName());
@@ -445,7 +445,7 @@ public class TestManagedSchema extends AbstractBadConfigTestBase {
     assertNull("Field '" + fieldName + "' is present in the schema",
         h.getCore().getLatestSchema().getFieldOrNull(fieldName));
 
-    Map<String,Object> options = new HashMap<String,Object>();
+    Map<String,Object> options = new HashMap<>();
     IndexSchema oldSchema = h.getCore().getLatestSchema();
     String fieldType = "text";
     SchemaField newField = oldSchema.newField(fieldName, fieldType, options);
diff --git solr/core/src/test/org/apache/solr/search/CursorMarkTest.java solr/core/src/test/org/apache/solr/search/CursorMarkTest.java
index 5a82298..950d936 100644
--- solr/core/src/test/org/apache/solr/search/CursorMarkTest.java
+++ solr/core/src/test/org/apache/solr/search/CursorMarkTest.java
@@ -248,7 +248,7 @@ public class CursorMarkTest extends SolrTestCaseJ4 {
    * a list of the fields in the schema - excluding _version_
    */
   private Collection<String> getAllFieldNames() {
-    ArrayList<String> names = new ArrayList<String>(37);
+    ArrayList<String> names = new ArrayList<>(37);
     for (String f : h.getCore().getLatestSchema().getFields().keySet()) {
       if (! f.equals("_version_")) {
         names.add(f);
diff --git solr/core/src/test/org/apache/solr/search/QueryEqualityTest.java solr/core/src/test/org/apache/solr/search/QueryEqualityTest.java
index ad6db3c..a8d4231 100644
--- solr/core/src/test/org/apache/solr/search/QueryEqualityTest.java
+++ solr/core/src/test/org/apache/solr/search/QueryEqualityTest.java
@@ -71,9 +71,9 @@ public class QueryEqualityTest extends SolrTestCaseJ4 {
   /** @see #testParserCoverage */
   private static boolean doAssertParserCoverage = false;
   /** @see #testParserCoverage */
-  private static final Set<String> qParsersTested = new HashSet<String>();
+  private static final Set<String> qParsersTested = new HashSet<>();
   /** @see #testParserCoverage */
-  private static final Set<String> valParsersTested = new HashSet<String>();
+  private static final Set<String> valParsersTested = new HashSet<>();
 
 
   public void testDateMathParsingEquality() throws Exception {
diff --git solr/core/src/test/org/apache/solr/search/TestExtendedDismaxParser.java solr/core/src/test/org/apache/solr/search/TestExtendedDismaxParser.java
index 37f2247..e4436e3 100644
--- solr/core/src/test/org/apache/solr/search/TestExtendedDismaxParser.java
+++ solr/core/src/test/org/apache/solr/search/TestExtendedDismaxParser.java
@@ -1146,7 +1146,7 @@ public class TestExtendedDismaxParser extends SolrTestCaseJ4 {
 
       public FuzzyQueryParser(QParser parser, String defaultField) {
         super(parser, defaultField);
-        frequentlyMisspelledWords = new HashSet<String>();
+        frequentlyMisspelledWords = new HashSet<>();
         frequentlyMisspelledWords.add("absence");
       }
       
diff --git solr/core/src/test/org/apache/solr/search/TestFastLRUCache.java solr/core/src/test/org/apache/solr/search/TestFastLRUCache.java
index fe33698..40ebb98 100644
--- solr/core/src/test/org/apache/solr/search/TestFastLRUCache.java
+++ solr/core/src/test/org/apache/solr/search/TestFastLRUCache.java
@@ -38,8 +38,8 @@ import java.util.concurrent.atomic.AtomicInteger;
 public class TestFastLRUCache extends LuceneTestCase {
   
   public void testPercentageAutowarm() throws IOException {
-    FastLRUCache<Object, Object> fastCache = new FastLRUCache<Object, Object>();
-    Map<String, String> params = new HashMap<String, String>();
+    FastLRUCache<Object, Object> fastCache = new FastLRUCache<>();
+    Map<String, String> params = new HashMap<>();
     params.put("size", "100");
     params.put("initialSize", "10");
     params.put("autowarmCount", "100%");
@@ -56,7 +56,7 @@ public class TestFastLRUCache extends LuceneTestCase {
     assertEquals(1L, nl.get("hits"));
     assertEquals(101L, nl.get("inserts"));
     assertEquals(null, fastCache.get(1));  // first item put in should be the first out
-    FastLRUCache<Object, Object> fastCacheNew = new FastLRUCache<Object, Object>();
+    FastLRUCache<Object, Object> fastCacheNew = new FastLRUCache<>();
     fastCacheNew.init(params, o, cr);
     fastCacheNew.warm(null, fastCache);
     fastCacheNew.setState(SolrCache.State.LIVE);
@@ -84,8 +84,8 @@ public class TestFastLRUCache extends LuceneTestCase {
   }
   
   private void doTestPercentageAutowarm(int limit, int percentage, int[] hits, int[]misses) {
-    FastLRUCache<Object, Object> fastCache = new FastLRUCache<Object, Object>();
-    Map<String, String> params = new HashMap<String, String>();
+    FastLRUCache<Object, Object> fastCache = new FastLRUCache<>();
+    Map<String, String> params = new HashMap<>();
     params.put("size", String.valueOf(limit));
     params.put("initialSize", "10");
     params.put("autowarmCount", percentage + "%");
@@ -96,7 +96,7 @@ public class TestFastLRUCache extends LuceneTestCase {
       fastCache.put(i, "" + i);//adds numbers from 1 to 100
     }
 
-    FastLRUCache<Object, Object> fastCacheNew = new FastLRUCache<Object, Object>();
+    FastLRUCache<Object, Object> fastCacheNew = new FastLRUCache<>();
     fastCacheNew.init(params, o, cr);
     fastCacheNew.warm(null, fastCache);
     fastCacheNew.setState(SolrCache.State.LIVE);
@@ -116,8 +116,8 @@ public class TestFastLRUCache extends LuceneTestCase {
   }
   
   public void testNoAutowarm() throws IOException {
-    FastLRUCache<Object, Object> fastCache = new FastLRUCache<Object, Object>();
-    Map<String, String> params = new HashMap<String, String>();
+    FastLRUCache<Object, Object> fastCache = new FastLRUCache<>();
+    Map<String, String> params = new HashMap<>();
     params.put("size", "100");
     params.put("initialSize", "10");
     CacheRegenerator cr = new NoOpRegenerator();
@@ -133,7 +133,7 @@ public class TestFastLRUCache extends LuceneTestCase {
     assertEquals(1L, nl.get("hits"));
     assertEquals(101L, nl.get("inserts"));
     assertEquals(null, fastCache.get(1));  // first item put in should be the first out
-    FastLRUCache<Object, Object> fastCacheNew = new FastLRUCache<Object, Object>();
+    FastLRUCache<Object, Object> fastCacheNew = new FastLRUCache<>();
     fastCacheNew.init(params, o, cr);
     fastCacheNew.warm(null, fastCache);
     fastCacheNew.setState(SolrCache.State.LIVE);
@@ -145,8 +145,8 @@ public class TestFastLRUCache extends LuceneTestCase {
   }
   
   public void testFullAutowarm() throws IOException {
-    FastLRUCache<Object, Object> cache = new FastLRUCache<Object, Object>();
-    Map<Object, Object> params = new HashMap<Object, Object>();
+    FastLRUCache<Object, Object> cache = new FastLRUCache<>();
+    Map<Object, Object> params = new HashMap<>();
     params.put("size", "100");
     params.put("initialSize", "10");
     params.put("autowarmCount", "-1");
@@ -162,7 +162,7 @@ public class TestFastLRUCache extends LuceneTestCase {
     assertEquals(null, cache.get(1));  // first item put in should be the first out
 
 
-    FastLRUCache<Object, Object> cacheNew = new FastLRUCache<Object, Object>();
+    FastLRUCache<Object, Object> cacheNew = new FastLRUCache<>();
     cacheNew.init(params, o, cr);
     cacheNew.warm(null, cache);
     cacheNew.setState(SolrCache.State.LIVE);
@@ -217,7 +217,7 @@ public class TestFastLRUCache extends LuceneTestCase {
   }
 
   public void testOldestItems() {
-    ConcurrentLRUCache<Integer, String> cache = new ConcurrentLRUCache<Integer, String>(100, 90);
+    ConcurrentLRUCache<Integer, String> cache = new ConcurrentLRUCache<>(100, 90);
     for (int i = 0; i < 50; i++) {
       cache.put(i + 1, "" + (i + 1));
     }
@@ -246,7 +246,7 @@ public class TestFastLRUCache extends LuceneTestCase {
     int sz = random().nextInt(100)+5;
     int lowWaterMark = random().nextInt(sz-3)+1;
     int keyrange = random().nextInt(sz*3)+1;
-    ConcurrentLRUCache<Integer, String> cache = new ConcurrentLRUCache<Integer, String>(sz, lowWaterMark);
+    ConcurrentLRUCache<Integer, String> cache = new ConcurrentLRUCache<>(sz, lowWaterMark);
     for (int i=0; i<10000; i++) {
       cache.put(random().nextInt(keyrange), "");
       cache.get(random().nextInt(keyrange));
diff --git solr/core/src/test/org/apache/solr/search/TestFiltering.java solr/core/src/test/org/apache/solr/search/TestFiltering.java
index 53665d0..f81d36e 100644
--- solr/core/src/test/org/apache/solr/search/TestFiltering.java
+++ solr/core/src/test/org/apache/solr/search/TestFiltering.java
@@ -271,7 +271,7 @@ public class TestFiltering extends SolrTestCaseJ4 {
       int nonZeros=0;
       for (int qiter=0; qiter<queryIter; qiter++) {
         model.clear();
-        List<String> params = new ArrayList<String>();
+        List<String> params = new ArrayList<>();
         params.add("q"); params.add(makeRandomQuery(model, true, false));
 
         int nFilters = random().nextInt(5);
diff --git solr/core/src/test/org/apache/solr/search/TestLFUCache.java solr/core/src/test/org/apache/solr/search/TestLFUCache.java
index 40c7500..3835af3 100644
--- solr/core/src/test/org/apache/solr/search/TestLFUCache.java
+++ solr/core/src/test/org/apache/solr/search/TestLFUCache.java
@@ -186,7 +186,7 @@ public class TestLFUCache extends SolrTestCaseJ4 {
 
   @Test
   public void testItemOrdering() {
-    ConcurrentLFUCache<Integer, String> cache = new ConcurrentLFUCache<Integer, String>(100, 90);
+    ConcurrentLFUCache<Integer, String> cache = new ConcurrentLFUCache<>(100, 90);
     try {
       for (int i = 0; i < 50; i++) {
         cache.put(i + 1, "" + (i + 1));
@@ -252,7 +252,7 @@ public class TestLFUCache extends SolrTestCaseJ4 {
 
   @Test
   public void testTimeDecay() {
-    ConcurrentLFUCache<Integer, String> cacheDecay = new ConcurrentLFUCache<Integer, String>(10, 9);
+    ConcurrentLFUCache<Integer, String> cacheDecay = new ConcurrentLFUCache<>(10, 9);
     try {
       for (int i = 1; i < 21; i++) {
         cacheDecay.put(i, Integer.toString(i));
@@ -326,7 +326,7 @@ public class TestLFUCache extends SolrTestCaseJ4 {
   @Test
   public void testTimeNoDecay() {
 
-    ConcurrentLFUCache<Integer, String> cacheNoDecay = new ConcurrentLFUCache<Integer, String>(10, 9,
+    ConcurrentLFUCache<Integer, String> cacheNoDecay = new ConcurrentLFUCache<>(10, 9,
         (int) Math.floor((9 + 10) / 2), (int) Math.ceil(0.75 * 10), false, false, null, false);
     try {
       for (int i = 1; i < 21; i++) {
diff --git solr/core/src/test/org/apache/solr/search/TestLRUCache.java solr/core/src/test/org/apache/solr/search/TestLRUCache.java
index 42c916f..90763ba 100644
--- solr/core/src/test/org/apache/solr/search/TestLRUCache.java
+++ solr/core/src/test/org/apache/solr/search/TestLRUCache.java
@@ -31,8 +31,8 @@ import org.apache.solr.common.util.NamedList;
 public class TestLRUCache extends LuceneTestCase {
 
   public void testFullAutowarm() throws IOException {
-    LRUCache<Object, Object> lruCache = new LRUCache<Object, Object>();
-    Map<String, String> params = new HashMap<String, String>();
+    LRUCache<Object, Object> lruCache = new LRUCache<>();
+    Map<String, String> params = new HashMap<>();
     params.put("size", "100");
     params.put("initialSize", "10");
     params.put("autowarmCount", "100%");
@@ -45,7 +45,7 @@ public class TestLRUCache extends LuceneTestCase {
     assertEquals("25", lruCache.get(25));
     assertEquals(null, lruCache.get(110));
     assertEquals(null, lruCache.get(1));  // first item put in should be the first out
-    LRUCache<Object, Object> lruCacheNew = new LRUCache<Object, Object>();
+    LRUCache<Object, Object> lruCacheNew = new LRUCache<>();
     lruCacheNew.init(params, o, cr);
     lruCacheNew.warm(null, lruCache);
     lruCacheNew.setState(SolrCache.State.LIVE);
@@ -64,8 +64,8 @@ public class TestLRUCache extends LuceneTestCase {
   }
   
   private void doTestPercentageAutowarm(int limit, int percentage, int[] hits, int[]misses) {
-    LRUCache<Object, Object> lruCache = new LRUCache<Object, Object>();
-    Map<String, String> params = new HashMap<String, String>();
+    LRUCache<Object, Object> lruCache = new LRUCache<>();
+    Map<String, String> params = new HashMap<>();
     params.put("size", String.valueOf(limit));
     params.put("initialSize", "10");
     params.put("autowarmCount", percentage + "%");
@@ -76,7 +76,7 @@ public class TestLRUCache extends LuceneTestCase {
       lruCache.put(i, "" + i);//adds numbers from 1 to 100
     }
 
-    LRUCache<Object, Object> lruCacheNew = new LRUCache<Object, Object>();
+    LRUCache<Object, Object> lruCacheNew = new LRUCache<>();
     lruCacheNew.init(params, o, cr);
     lruCacheNew.warm(null, lruCache);
     lruCacheNew.setState(SolrCache.State.LIVE);
@@ -94,8 +94,8 @@ public class TestLRUCache extends LuceneTestCase {
   
   @SuppressWarnings("unchecked")
   public void testNoAutowarm() throws IOException {
-    LRUCache<Object, Object> lruCache = new LRUCache<Object, Object>();
-    Map<String, String> params = new HashMap<String, String>();
+    LRUCache<Object, Object> lruCache = new LRUCache<>();
+    Map<String, String> params = new HashMap<>();
     params.put("size", "100");
     params.put("initialSize", "10");
     CacheRegenerator cr = new NoOpRegenerator();
@@ -111,7 +111,7 @@ public class TestLRUCache extends LuceneTestCase {
     assertEquals(1L, nl.get("hits"));
     assertEquals(101L, nl.get("inserts"));
     assertEquals(null, lruCache.get(1));  // first item put in should be the first out
-    LRUCache<Object, Object> lruCacheNew = new LRUCache<Object, Object>();
+    LRUCache<Object, Object> lruCacheNew = new LRUCache<>();
     lruCacheNew.init(params, o, cr);
     lruCacheNew.warm(null, lruCache);
     lruCacheNew.setState(SolrCache.State.LIVE);
diff --git solr/core/src/test/org/apache/solr/search/TestMaxScoreQueryParser.java solr/core/src/test/org/apache/solr/search/TestMaxScoreQueryParser.java
index 2023e5b..3929b35 100644
--- solr/core/src/test/org/apache/solr/search/TestMaxScoreQueryParser.java
+++ solr/core/src/test/org/apache/solr/search/TestMaxScoreQueryParser.java
@@ -135,7 +135,7 @@ public class TestMaxScoreQueryParser extends AbstractSolrTestCase {
   private Query parse(String q, String... params) {
     try {
       ModifiableSolrParams p = new ModifiableSolrParams();
-      ArrayList<String> al = new ArrayList<String>(Arrays.asList(params));
+      ArrayList<String> al = new ArrayList<>(Arrays.asList(params));
       while(al.size() >= 2) {
         p.add(al.remove(0), al.remove(0));
       }
diff --git solr/core/src/test/org/apache/solr/search/TestPseudoReturnFields.java solr/core/src/test/org/apache/solr/search/TestPseudoReturnFields.java
index f1da7c2..f0a68a6 100644
--- solr/core/src/test/org/apache/solr/search/TestPseudoReturnFields.java
+++ solr/core/src/test/org/apache/solr/search/TestPseudoReturnFields.java
@@ -471,7 +471,7 @@ public class TestPseudoReturnFields extends SolrTestCaseJ4 {
               ,"//result/doc[count(*)=6]"
               );
 
-      final List<String> params = new ArrayList<String>((fl.size()*2) + 4);
+      final List<String> params = new ArrayList<>((fl.size()*2) + 4);
       final StringBuilder info = new StringBuilder();
       params.addAll(Arrays.asList("q","*:*", "rows", "1"));
       for (String item : fl) {
diff --git solr/core/src/test/org/apache/solr/search/TestRTGBase.java solr/core/src/test/org/apache/solr/search/TestRTGBase.java
index d107a8b..aa4c3be 100644
--- solr/core/src/test/org/apache/solr/search/TestRTGBase.java
+++ solr/core/src/test/org/apache/solr/search/TestRTGBase.java
@@ -55,8 +55,8 @@ public class TestRTGBase extends SolrTestCaseJ4 {
     }
   }
 
-  protected final ConcurrentHashMap<Integer,DocInfo> model = new ConcurrentHashMap<Integer,DocInfo>();
-  protected Map<Integer,DocInfo> committedModel = new HashMap<Integer,DocInfo>();
+  protected final ConcurrentHashMap<Integer,DocInfo> model = new ConcurrentHashMap<>();
+  protected Map<Integer,DocInfo> committedModel = new HashMap<>();
   protected long snapshotCount;
   protected long committedModelClock;
   protected volatile int lastId;
diff --git solr/core/src/test/org/apache/solr/search/TestRangeQuery.java solr/core/src/test/org/apache/solr/search/TestRangeQuery.java
index 22e36ff..ad2dcad 100644
--- solr/core/src/test/org/apache/solr/search/TestRangeQuery.java
+++ solr/core/src/test/org/apache/solr/search/TestRangeQuery.java
@@ -88,7 +88,7 @@ public class TestRangeQuery extends SolrTestCaseJ4 {
     String[] dates = {"0299-12-31T23:59:59.999Z","2000-01-01T00:00:00.000Z","2000-01-01T00:00:00.001Z",  "0299-12-31T23:59:59.998Z","2000-01-01T00:00:00.002Z" };
 
     // fields that normal range queries should work on
-    Map<String,String[]> norm_fields = new HashMap<String,String[]>();
+    Map<String,String[]> norm_fields = new HashMap<>();
     norm_fields.put("foo_i", ints);
     norm_fields.put("foo_l", longs);
     norm_fields.put("foo_d", doubles);
@@ -103,7 +103,7 @@ public class TestRangeQuery extends SolrTestCaseJ4 {
 
 
     // fields that frange queries should work on
-    Map<String,String[]> frange_fields = new HashMap<String,String[]>();
+    Map<String,String[]> frange_fields = new HashMap<>();
     frange_fields.put("foo_i", ints);
     frange_fields.put("foo_l", longs);
     frange_fields.put("foo_d", doubles);
@@ -120,12 +120,12 @@ public class TestRangeQuery extends SolrTestCaseJ4 {
     frange_fields.put("foo_s", strings);
     frange_fields.put("foo_dt", dates);
 
-    Map<String,String[]> all_fields = new HashMap<String,String[]>();
+    Map<String,String[]> all_fields = new HashMap<>();
     all_fields.putAll(norm_fields);
     all_fields.putAll(frange_fields);
 
     for (int j=0; j<ints.length-2; j++) {
-      List<String> fields = new ArrayList<String>();
+      List<String> fields = new ArrayList<>();
       fields.add("id");
       fields.add(""+j);
       for (Map.Entry<String,String[]> entry : all_fields.entrySet()) {
@@ -239,7 +239,7 @@ public class TestRangeQuery extends SolrTestCaseJ4 {
       // lower=2; upper=2; inclusive=true;      
       // inclusive=true; lowerMissing=true; upperMissing=true;    
 
-      List<String> qs = new ArrayList<String>();
+      List<String> qs = new ArrayList<>();
       for (String field : norm_fields) {
         String q = field + ':' + (inclusive?'[':'{')
                 + (lowerMissing?"*":lower)
diff --git solr/core/src/test/org/apache/solr/search/TestRealTimeGet.java solr/core/src/test/org/apache/solr/search/TestRealTimeGet.java
index 43942b0..1581f64 100644
--- solr/core/src/test/org/apache/solr/search/TestRealTimeGet.java
+++ solr/core/src/test/org/apache/solr/search/TestRealTimeGet.java
@@ -438,7 +438,7 @@ public class TestRealTimeGet extends TestRTGBase {
 
     final AtomicInteger numCommitting = new AtomicInteger();
 
-    List<Thread> threads = new ArrayList<Thread>();
+    List<Thread> threads = new ArrayList<>();
 
     for (int i=0; i<nWriteThreads; i++) {
       Thread thread = new Thread("WRITER"+i) {
@@ -456,7 +456,7 @@ public class TestRealTimeGet extends TestRTGBase {
                 long version;
 
                 synchronized(TestRealTimeGet.this) {
-                  newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot
+                  newCommittedModel = new HashMap<>(model);  // take a snapshot
                   version = snapshotCount++;
                   verbose("took snapshot version=",version);
                 }
diff --git solr/core/src/test/org/apache/solr/search/TestRecovery.java solr/core/src/test/org/apache/solr/search/TestRecovery.java
index 4b5ed59..1b8bc63 100644
--- solr/core/src/test/org/apache/solr/search/TestRecovery.java
+++ solr/core/src/test/org/apache/solr/search/TestRecovery.java
@@ -124,7 +124,7 @@ public class TestRecovery extends SolrTestCaseJ4 {
       clearIndex();
       assertU(commit());
 
-      Deque<Long> versions = new ArrayDeque<Long>();
+      Deque<Long> versions = new ArrayDeque<>();
       versions.addFirst(addAndGetVersion(sdoc("id", "A1"), null));
       versions.addFirst(addAndGetVersion(sdoc("id", "A11"), null));
       versions.addFirst(addAndGetVersion(sdoc("id", "A12"), null));
@@ -771,7 +771,7 @@ public class TestRecovery extends SolrTestCaseJ4 {
       int start = 0;
       int maxReq = 50;
 
-      LinkedList<Long> versions = new LinkedList<Long>();
+      LinkedList<Long> versions = new LinkedList<>();
       addDocs(10, start, versions); start+=10;
       assertJQ(req("qt","/get", "getVersions",""+maxReq), "/versions==" + versions.subList(0,Math.min(maxReq,start)));
       assertU(commit());
diff --git solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java
index 2a86d45..9de9d3b 100644
--- solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java
+++ solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java
@@ -154,7 +154,7 @@ public class TestRecoveryHdfs extends SolrTestCaseJ4 {
       clearIndex();
       assertU(commit());
 
-      Deque<Long> versions = new ArrayDeque<Long>();
+      Deque<Long> versions = new ArrayDeque<>();
       versions.addFirst(addAndGetVersion(sdoc("id", "A1"), null));
       versions.addFirst(addAndGetVersion(sdoc("id", "A11"), null));
       versions.addFirst(addAndGetVersion(sdoc("id", "A12"), null));
@@ -768,7 +768,7 @@ public class TestRecoveryHdfs extends SolrTestCaseJ4 {
       int start = 0;
       int maxReq = 50;
 
-      LinkedList<Long> versions = new LinkedList<Long>();
+      LinkedList<Long> versions = new LinkedList<>();
       addDocs(10, start, versions); start+=10;
       assertJQ(req("qt","/get", "getVersions",""+maxReq), "/versions==" + versions.subList(0,Math.min(maxReq,start)));
       assertU(commit());
diff --git solr/core/src/test/org/apache/solr/search/TestSearchPerf.java solr/core/src/test/org/apache/solr/search/TestSearchPerf.java
index 71d7a7c..5bff259 100644
--- solr/core/src/test/org/apache/solr/search/TestSearchPerf.java
+++ solr/core/src/test/org/apache/solr/search/TestSearchPerf.java
@@ -77,7 +77,7 @@ public class TestSearchPerf extends AbstractSolrTestCase {
 
   // Skip encoding for updating the index
   void createIndex2(int nDocs, String... fields) throws IOException {
-    Set<String> fieldSet = new HashSet<String>(Arrays.asList(fields));
+    Set<String> fieldSet = new HashSet<>(Arrays.asList(fields));
 
     SolrQueryRequest req = lrf.makeRequest();
     SolrQueryResponse rsp = new SolrQueryResponse();
@@ -233,7 +233,7 @@ public class TestSearchPerf extends AbstractSolrTestCase {
 
     QParser parser = QParser.getParser("foomany_s:[" + l + " TO " + u + "]", null, req);
     Query rangeQ = parser.getQuery();
-    List<Query> filters = new ArrayList<Query>();
+    List<Query> filters = new ArrayList<>();
     filters.add(rangeQ);
     req.close();
 
diff --git solr/core/src/test/org/apache/solr/search/TestSort.java solr/core/src/test/org/apache/solr/search/TestSort.java
index 520ee73..b319521 100644
--- solr/core/src/test/org/apache/solr/search/TestSort.java
+++ solr/core/src/test/org/apache/solr/search/TestSort.java
@@ -237,7 +237,7 @@ public class TestSort extends SolrTestCaseJ4 {
         final boolean sortMissingLast = !luceneSort && r.nextBoolean();
         final boolean sortMissingFirst = !luceneSort && !sortMissingLast;
         final boolean reverse = r.nextBoolean();
-        List<SortField> sfields = new ArrayList<SortField>();
+        List<SortField> sfields = new ArrayList<>();
 
         final boolean secondary = r.nextBoolean();
         final boolean luceneSort2 = r.nextBoolean();
@@ -263,7 +263,7 @@ public class TestSort extends SolrTestCaseJ4 {
         boolean scoreInOrder = r.nextBoolean();
         final TopFieldCollector topCollector = TopFieldCollector.create(sort, top, true, trackScores, trackMaxScores, scoreInOrder);
 
-        final List<MyDoc> collectedDocs = new ArrayList<MyDoc>();
+        final List<MyDoc> collectedDocs = new ArrayList<>();
         // delegate and collect docs ourselves
         Collector myCollector = new Collector() {
           int docBase;
diff --git solr/core/src/test/org/apache/solr/search/TestStandardQParsers.java solr/core/src/test/org/apache/solr/search/TestStandardQParsers.java
index 3a74e67..5fbef6a 100644
--- solr/core/src/test/org/apache/solr/search/TestStandardQParsers.java
+++ solr/core/src/test/org/apache/solr/search/TestStandardQParsers.java
@@ -50,9 +50,9 @@ public class TestStandardQParsers extends LuceneTestCase {
   public void testRegisteredName() throws Exception {
     Map<String, Class<QParserPlugin>> standardPlugins = getStandardQParsers();
 
-    List<String> notStatic = new ArrayList<String>(standardPlugins.size());
-    List<String> notFinal = new ArrayList<String>(standardPlugins.size());
-    List<String> mismatch = new ArrayList<String>(standardPlugins.size());
+    List<String> notStatic = new ArrayList<>(standardPlugins.size());
+    List<String> notFinal = new ArrayList<>(standardPlugins.size());
+    List<String> mismatch = new ArrayList<>(standardPlugins.size());
  
     for (Map.Entry<String,Class<QParserPlugin>> pair : standardPlugins.entrySet()) {
       String regName = pair.getKey();
@@ -93,7 +93,7 @@ public class TestStandardQParsers extends LuceneTestCase {
     Object[] standardPluginsValue = QParserPlugin.standardPlugins;
 
     Map<String, Class<QParserPlugin>> standardPlugins 
-      = new HashMap<String, Class<QParserPlugin>>(standardPluginsValue.length / 2);
+      = new HashMap<>(standardPluginsValue.length / 2);
 
     for (int i = 0; i < standardPluginsValue.length; i += 2) {
       @SuppressWarnings("unchecked")
diff --git solr/core/src/test/org/apache/solr/search/TestStressLucene.java solr/core/src/test/org/apache/solr/search/TestStressLucene.java
index f27e363..c90d33d 100644
--- solr/core/src/test/org/apache/solr/search/TestStressLucene.java
+++ solr/core/src/test/org/apache/solr/search/TestStressLucene.java
@@ -79,7 +79,7 @@ public class TestStressLucene extends TestRTGBase {
 
     final AtomicInteger numCommitting = new AtomicInteger();
 
-    List<Thread> threads = new ArrayList<Thread>();
+    List<Thread> threads = new ArrayList<>();
 
 
     final FieldType idFt = new FieldType();
@@ -143,7 +143,7 @@ public class TestStressLucene extends TestRTGBase {
                   if (reopenLock != null) reopenLock.lock();
 
                   synchronized(globalLock) {
-                    newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot
+                    newCommittedModel = new HashMap<>(model);  // take a snapshot
                     version = snapshotCount++;
                     oldReader = reader;
                     oldReader.incRef();  // increment the reference since we will use this for reopening
diff --git solr/core/src/test/org/apache/solr/search/TestStressRecovery.java solr/core/src/test/org/apache/solr/search/TestStressRecovery.java
index b403c8d..412908d 100644
--- solr/core/src/test/org/apache/solr/search/TestStressRecovery.java
+++ solr/core/src/test/org/apache/solr/search/TestStressRecovery.java
@@ -82,7 +82,7 @@ public class TestStressRecovery extends TestRTGBase {
 
     final AtomicInteger numCommitting = new AtomicInteger();
 
-    List<Thread> threads = new ArrayList<Thread>();
+    List<Thread> threads = new ArrayList<>();
 
 
     final AtomicLong testVersion = new AtomicLong(0);
@@ -119,7 +119,7 @@ public class TestStressRecovery extends TestRTGBase {
                   long version;
 
                   synchronized(globalLock) {
-                    newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot
+                    newCommittedModel = new HashMap<>(model);  // take a snapshot
                     version = snapshotCount++;
                   }
 
@@ -348,7 +348,7 @@ public class TestStressRecovery extends TestRTGBase {
       // before we start buffering updates, we want to point
       // visibleModel away from the live model.
 
-      visibleModel = new ConcurrentHashMap<Integer, DocInfo>(model);
+      visibleModel = new ConcurrentHashMap<>(model);
 
       synchronized (stateChangeLock) {
         uLog.bufferUpdates();
diff --git solr/core/src/test/org/apache/solr/search/TestStressReorder.java solr/core/src/test/org/apache/solr/search/TestStressReorder.java
index f4e62f0..5d3ce31 100644
--- solr/core/src/test/org/apache/solr/search/TestStressReorder.java
+++ solr/core/src/test/org/apache/solr/search/TestStressReorder.java
@@ -99,7 +99,7 @@ public class TestStressReorder extends TestRTGBase {
 
     final AtomicInteger numCommitting = new AtomicInteger();
 
-    List<Thread> threads = new ArrayList<Thread>();
+    List<Thread> threads = new ArrayList<>();
 
 
     final AtomicLong testVersion = new AtomicLong(0);
@@ -120,7 +120,7 @@ public class TestStressReorder extends TestRTGBase {
                 long version;
 
                 synchronized(TestStressReorder.this) {
-                  newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot
+                  newCommittedModel = new HashMap<>(model);  // take a snapshot
                   version = snapshotCount++;
                 }
 
diff --git solr/core/src/test/org/apache/solr/search/TestStressUserVersions.java solr/core/src/test/org/apache/solr/search/TestStressUserVersions.java
index 2778051..807221a 100755
--- solr/core/src/test/org/apache/solr/search/TestStressUserVersions.java
+++ solr/core/src/test/org/apache/solr/search/TestStressUserVersions.java
@@ -102,7 +102,7 @@ public class TestStressUserVersions extends TestRTGBase {
 
     final AtomicInteger numCommitting = new AtomicInteger();
 
-    List<Thread> threads = new ArrayList<Thread>();
+    List<Thread> threads = new ArrayList<>();
 
 
     final AtomicLong testVersion = new AtomicLong(0);
@@ -123,7 +123,7 @@ public class TestStressUserVersions extends TestRTGBase {
                   long version;
 
                   synchronized(TestStressUserVersions.this) {
-                    newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot
+                    newCommittedModel = new HashMap<>(model);  // take a snapshot
                     version = snapshotCount++;
                   }
 
diff --git solr/core/src/test/org/apache/solr/search/TestStressVersions.java solr/core/src/test/org/apache/solr/search/TestStressVersions.java
index 5d3f6f6..d91a2cc 100644
--- solr/core/src/test/org/apache/solr/search/TestStressVersions.java
+++ solr/core/src/test/org/apache/solr/search/TestStressVersions.java
@@ -69,7 +69,7 @@ public class TestStressVersions extends TestRTGBase {
 
     final AtomicInteger numCommitting = new AtomicInteger();
 
-    List<Thread> threads = new ArrayList<Thread>();
+    List<Thread> threads = new ArrayList<>();
 
     for (int i=0; i<nWriteThreads; i++) {
       Thread thread = new Thread("WRITER"+i) {
@@ -87,7 +87,7 @@ public class TestStressVersions extends TestRTGBase {
                   long version;
 
                   synchronized(globalLock) {
-                    newCommittedModel = new HashMap<Integer,DocInfo>(model);  // take a snapshot
+                    newCommittedModel = new HashMap<>(model);  // take a snapshot
                     version = snapshotCount++;
                   }
 
diff --git solr/core/src/test/org/apache/solr/search/function/TestFunctionQuery.java solr/core/src/test/org/apache/solr/search/function/TestFunctionQuery.java
index 1ed6dbc..7f1bba7 100644
--- solr/core/src/test/org/apache/solr/search/function/TestFunctionQuery.java
+++ solr/core/src/test/org/apache/solr/search/function/TestFunctionQuery.java
@@ -103,7 +103,7 @@ public class TestFunctionQuery extends SolrTestCaseJ4 {
   void singleTest(String field, String funcTemplate, List<String> args, float... results) {
     String parseableQuery = func(field, funcTemplate);
 
-    List<String> nargs = new ArrayList<String>(Arrays.asList("q", parseableQuery
+    List<String> nargs = new ArrayList<>(Arrays.asList("q", parseableQuery
             ,"fl", "*,score"
             ,"indent","on"
             ,"rows","100"));
@@ -114,7 +114,7 @@ public class TestFunctionQuery extends SolrTestCaseJ4 {
       }
     }
 
-    List<String> tests = new ArrayList<String>();
+    List<String> tests = new ArrayList<>();
 
     // Construct xpaths like the following:
     // "//doc[./float[@name='foo_pf']='10.0' and ./float[@name='score']='10.0']"
diff --git solr/core/src/test/org/apache/solr/search/join/BJQParserTest.java solr/core/src/test/org/apache/solr/search/join/BJQParserTest.java
index 8e1e282..3d80862 100644
--- solr/core/src/test/org/apache/solr/search/join/BJQParserTest.java
+++ solr/core/src/test/org/apache/solr/search/join/BJQParserTest.java
@@ -81,7 +81,7 @@ public class BJQParserTest extends SolrTestCaseJ4 {
 
   private static int id=0;
   private static List<List<String[]>> createBlocks() {
-    List<List<String[]>> blocks = new ArrayList<List<String[]>>();
+    List<List<String[]>> blocks = new ArrayList<>();
     for (String parent : abcdef) {
       List<String[]> block = createChildrenBlock(parent);
       block.add(new String[] {"parent_s", parent});
@@ -92,7 +92,7 @@ public class BJQParserTest extends SolrTestCaseJ4 {
   }
 
   private static List<String[]> createChildrenBlock(String parent) {
-    List<String[]> block = new ArrayList<String[]>();
+    List<String[]> block = new ArrayList<>();
     for (String child : klm) {
       block
           .add(new String[] {"child_s", child, "parentchild_s", parent + child});
@@ -103,7 +103,7 @@ public class BJQParserTest extends SolrTestCaseJ4 {
   }
   
   private static void addGrandChildren(List<String[]> block) {
-    List<String> grandChildren = new ArrayList<String>(xyz);
+    List<String> grandChildren = new ArrayList<>(xyz);
     // add grandchildren after children
     for (ListIterator<String[]> iter = block.listIterator(); iter.hasNext();) {
       String[] child = iter.next();
diff --git solr/core/src/test/org/apache/solr/servlet/CacheHeaderTestBase.java solr/core/src/test/org/apache/solr/servlet/CacheHeaderTestBase.java
index d8c5468..b34538d 100644
--- solr/core/src/test/org/apache/solr/servlet/CacheHeaderTestBase.java
+++ solr/core/src/test/org/apache/solr/servlet/CacheHeaderTestBase.java
@@ -39,7 +39,7 @@ public abstract class CacheHeaderTestBase extends SolrJettyTestBase {
     HttpSolrServer httpserver = (HttpSolrServer)getSolrServer();
     HttpRequestBase m = null;
     
-    ArrayList<BasicNameValuePair> qparams = new ArrayList<BasicNameValuePair>();
+    ArrayList<BasicNameValuePair> qparams = new ArrayList<>();
     if(params.length==0) {
       qparams.add(new BasicNameValuePair("q", "solr"));
       qparams.add(new BasicNameValuePair("qt", "standard"));
@@ -66,7 +66,7 @@ public abstract class CacheHeaderTestBase extends SolrJettyTestBase {
     HttpSolrServer httpserver = (HttpSolrServer)getSolrServer();
     HttpRequestBase m = null;
     
-    ArrayList<BasicNameValuePair> qparams = new ArrayList<BasicNameValuePair>();
+    ArrayList<BasicNameValuePair> qparams = new ArrayList<>();
     for(int i=0;i<params.length/2;i++) {
       qparams.add(new BasicNameValuePair(params[i*2], params[i*2+1]));
     }
diff --git solr/core/src/test/org/apache/solr/servlet/SolrRequestParserTest.java solr/core/src/test/org/apache/solr/servlet/SolrRequestParserTest.java
index e569b51..2ee9f76 100644
--- solr/core/src/test/org/apache/solr/servlet/SolrRequestParserTest.java
+++ solr/core/src/test/org/apache/solr/servlet/SolrRequestParserTest.java
@@ -76,23 +76,23 @@ public class SolrRequestParserTest extends SolrTestCaseJ4 {
     
     SolrCore core = h.getCore();
     
-    Map<String,String[]> args = new HashMap<String, String[]>();
+    Map<String,String[]> args = new HashMap<>();
     args.put( CommonParams.STREAM_BODY, new String[] {body1} );
     
     // Make sure it got a single stream in and out ok
-    List<ContentStream> streams = new ArrayList<ContentStream>();
+    List<ContentStream> streams = new ArrayList<>();
     SolrQueryRequest req = parser.buildRequestFrom( core, new MultiMapSolrParams( args ), streams );
     assertEquals( 1, streams.size() );
     assertEquals( body1, IOUtils.toString( streams.get(0).getReader() ) );
     req.close();
 
     // Now add three and make sure they come out ok
-    streams = new ArrayList<ContentStream>();
+    streams = new ArrayList<>();
     args.put( CommonParams.STREAM_BODY, new String[] {body1,body2,body3} );
     req = parser.buildRequestFrom( core, new MultiMapSolrParams( args ), streams );
     assertEquals( 3, streams.size() );
-    ArrayList<String> input  = new ArrayList<String>();
-    ArrayList<String> output = new ArrayList<String>();
+    ArrayList<String> input  = new ArrayList<>();
+    ArrayList<String> output = new ArrayList<>();
     input.add( body1 );
     input.add( body2 );
     input.add( body3 );
@@ -107,7 +107,7 @@ public class SolrRequestParserTest extends SolrTestCaseJ4 {
 
     // set the contentType and make sure tat gets set
     String ctype = "text/xxx";
-    streams = new ArrayList<ContentStream>();
+    streams = new ArrayList<>();
     args.put( CommonParams.STREAM_CONTENTTYPE, new String[] {ctype} );
     req = parser.buildRequestFrom( core, new MultiMapSolrParams( args ), streams );
     for( ContentStream s : streams ) {
@@ -138,11 +138,11 @@ public class SolrRequestParserTest extends SolrTestCaseJ4 {
 
     SolrCore core = h.getCore();
     
-    Map<String,String[]> args = new HashMap<String, String[]>();
+    Map<String,String[]> args = new HashMap<>();
     args.put( CommonParams.STREAM_URL, new String[] {url} );
     
     // Make sure it got a single stream in and out ok
-    List<ContentStream> streams = new ArrayList<ContentStream>();
+    List<ContentStream> streams = new ArrayList<>();
     SolrQueryRequest req = parser.buildRequestFrom( core, new MultiMapSolrParams( args ), streams );
     assertEquals( 1, streams.size() );
     try {
@@ -358,11 +358,11 @@ public class SolrRequestParserTest extends SolrTestCaseJ4 {
     expect(request.getMethod()).andReturn("GET").anyTimes();
     expect(request.getContentType()).andReturn( "application/x-www-form-urlencoded" ).anyTimes();
     expect(request.getQueryString()).andReturn("q=title:solr").anyTimes();
-    Map<String, String> headers = new HashMap<String,String>();
+    Map<String, String> headers = new HashMap<>();
     headers.put("X-Forwarded-For", "10.0.0.1");
-    expect(request.getHeaderNames()).andReturn(new Vector<String>(headers.keySet()).elements()).anyTimes();
+    expect(request.getHeaderNames()).andReturn(new Vector<>(headers.keySet()).elements()).anyTimes();
     for(Map.Entry<String,String> entry:headers.entrySet()) {
-      Vector<String> v = new Vector<String>();
+      Vector<String> v = new Vector<>();
       v.add(entry.getValue());
       expect(request.getHeaders(entry.getKey())).andReturn(v.elements()).anyTimes();
     }
diff --git solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java
index 69d70ab..c8ca39e 100644
--- solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java
+++ solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java
@@ -40,7 +40,7 @@ class SimpleQueryConverter extends SpellingQueryConverter {
 
   @Override
   public Collection<Token> convert(String origQuery) {
-    Collection<Token> result = new HashSet<Token>();
+    Collection<Token> result = new HashSet<>();
     WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(LuceneTestCase.TEST_VERSION_CURRENT);
     
     try (TokenStream ts = analyzer.tokenStream("", origQuery)) {
diff --git solr/core/src/test/org/apache/solr/spelling/SpellCheckCollatorTest.java solr/core/src/test/org/apache/solr/spelling/SpellCheckCollatorTest.java
index 95bda28..5e71e14 100644
--- solr/core/src/test/org/apache/solr/spelling/SpellCheckCollatorTest.java
+++ solr/core/src/test/org/apache/solr/spelling/SpellCheckCollatorTest.java
@@ -348,7 +348,7 @@ public class SpellCheckCollatorTest extends SolrTestCaseJ4 {
     spellCheck = (NamedList) values.get("spellcheck");
     suggestions = (NamedList) spellCheck.get("suggestions");
     List<NamedList> expandedCollationList = suggestions.getAll("collation");
-    Set<String> usedcollations = new HashSet<String>();
+    Set<String> usedcollations = new HashSet<>();
     assertTrue(expandedCollationList.size() == 2);
     for (NamedList expandedCollation : expandedCollationList) {
       String multipleCollation = (String) expandedCollation.get("collationQuery");
diff --git solr/core/src/test/org/apache/solr/spelling/SpellPossibilityIteratorTest.java solr/core/src/test/org/apache/solr/spelling/SpellPossibilityIteratorTest.java
index cca7b76..9c62151 100644
--- solr/core/src/test/org/apache/solr/spelling/SpellPossibilityIteratorTest.java
+++ solr/core/src/test/org/apache/solr/spelling/SpellPossibilityIteratorTest.java
@@ -43,7 +43,7 @@ public class SpellPossibilityIteratorTest extends SolrTestCaseJ4 {
   public void setUp() throws Exception {
     super.setUp();
 
-    AYE = new LinkedHashMap<String, Integer>();
+    AYE = new LinkedHashMap<>();
     AYE.put("I", 0);
     AYE.put("II", 0);
     AYE.put("III", 0);
@@ -53,7 +53,7 @@ public class SpellPossibilityIteratorTest extends SolrTestCaseJ4 {
     AYE.put("VII", 0);
     AYE.put("VIII", 0);
 
-    BEE = new LinkedHashMap<String, Integer>();
+    BEE = new LinkedHashMap<>();
     BEE.put("alpha", 0);
     BEE.put("beta", 0);
     BEE.put("gamma", 0);
@@ -64,7 +64,7 @@ public class SpellPossibilityIteratorTest extends SolrTestCaseJ4 {
     BEE.put("theta", 0);
     BEE.put("iota", 0);
 
-    AYE_BEE = new LinkedHashMap<String, Integer>();
+    AYE_BEE = new LinkedHashMap<>();
     AYE_BEE.put("one-alpha", 0);
     AYE_BEE.put("two-beta", 0);
     AYE_BEE.put("three-gamma", 0);
@@ -76,7 +76,7 @@ public class SpellPossibilityIteratorTest extends SolrTestCaseJ4 {
     AYE_BEE.put("nine-iota", 0);
 
 
-    CEE = new LinkedHashMap<String, Integer>();
+    CEE = new LinkedHashMap<>();
     CEE.put("one", 0);
     CEE.put("two", 0);
     CEE.put("three", 0);
@@ -91,7 +91,7 @@ public class SpellPossibilityIteratorTest extends SolrTestCaseJ4 {
 
   @Test
   public void testScalability() throws Exception {
-    Map<Token, LinkedHashMap<String, Integer>> lotsaSuggestions = new LinkedHashMap<Token, LinkedHashMap<String, Integer>>();
+    Map<Token, LinkedHashMap<String, Integer>> lotsaSuggestions = new LinkedHashMap<>();
     lotsaSuggestions.put(TOKEN_AYE , AYE);
     lotsaSuggestions.put(TOKEN_BEE , BEE);
     lotsaSuggestions.put(TOKEN_CEE , CEE);
@@ -135,7 +135,7 @@ public class SpellPossibilityIteratorTest extends SolrTestCaseJ4 {
 
   @Test
   public void testSpellPossibilityIterator() throws Exception {
-    Map<Token, LinkedHashMap<String, Integer>> suggestions = new LinkedHashMap<Token, LinkedHashMap<String, Integer>>();
+    Map<Token, LinkedHashMap<String, Integer>> suggestions = new LinkedHashMap<>();
     suggestions.put(TOKEN_AYE , AYE);
     suggestions.put(TOKEN_BEE , BEE);
     suggestions.put(TOKEN_CEE , CEE);
@@ -185,7 +185,7 @@ public class SpellPossibilityIteratorTest extends SolrTestCaseJ4 {
 
   @Test
   public void testOverlappingTokens() throws Exception {
-    Map<Token, LinkedHashMap<String, Integer>> overlappingSuggestions = new LinkedHashMap<Token, LinkedHashMap<String, Integer>>();
+    Map<Token, LinkedHashMap<String, Integer>> overlappingSuggestions = new LinkedHashMap<>();
     overlappingSuggestions.put(TOKEN_AYE, AYE);
     overlappingSuggestions.put(TOKEN_BEE, BEE);
     overlappingSuggestions.put(TOKEN_AYE_BEE, AYE_BEE);
@@ -194,7 +194,7 @@ public class SpellPossibilityIteratorTest extends SolrTestCaseJ4 {
     PossibilityIterator iter = new PossibilityIterator(overlappingSuggestions, Integer.MAX_VALUE, Integer.MAX_VALUE, true);
     int aCount = 0;
     int abCount = 0;
-    Set<PossibilityIterator.RankedSpellPossibility> dupChecker = new HashSet<PossibilityIterator.RankedSpellPossibility>();
+    Set<PossibilityIterator.RankedSpellPossibility> dupChecker = new HashSet<>();
     while (iter.hasNext()) {
       PossibilityIterator.RankedSpellPossibility rsp = iter.next();
       Token a = null;
diff --git solr/core/src/test/org/apache/solr/spelling/SpellingQueryConverterTest.java solr/core/src/test/org/apache/solr/spelling/SpellingQueryConverterTest.java
index b76cad4..bccdbbc 100644
--- solr/core/src/test/org/apache/solr/spelling/SpellingQueryConverterTest.java
+++ solr/core/src/test/org/apache/solr/spelling/SpellingQueryConverterTest.java
@@ -136,49 +136,49 @@ public class SpellingQueryConverterTest extends LuceneTestCase {
     converter.setAnalyzer(new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
 
     {
-      List<Token> tokens = new ArrayList<Token>(converter.convert("aaa bbb ccc"));
+      List<Token> tokens = new ArrayList<>(converter.convert("aaa bbb ccc"));
       assertTrue("Should have 3 tokens",          tokens != null && tokens.size()==3);
       assertTrue("token 1 should be optional",    !hasRequiredFlag(tokens.get(0)) && !hasProhibitedFlag(tokens.get(0)));
       assertTrue("token 2 should be optional",    !hasRequiredFlag(tokens.get(1)) && !hasProhibitedFlag(tokens.get(1)));
       assertTrue("token 3 should be optional",    !hasRequiredFlag(tokens.get(2)) && !hasProhibitedFlag(tokens.get(2)));
     }
     {
-      List<Token> tokens = new ArrayList<Token>(converter.convert("+aaa bbb -ccc"));
+      List<Token> tokens = new ArrayList<>(converter.convert("+aaa bbb -ccc"));
       assertTrue("Should have 3 tokens",          tokens != null && tokens.size()==3);
       assertTrue("token 1 should be required",     hasRequiredFlag(tokens.get(0)) && !hasProhibitedFlag(tokens.get(0)));
       assertTrue("token 2 should be optional",    !hasRequiredFlag(tokens.get(1)) && !hasProhibitedFlag(tokens.get(1)));
       assertTrue("token 3 should be prohibited",  !hasRequiredFlag(tokens.get(2)) &&  hasProhibitedFlag(tokens.get(2)));
     }
     {
-      List<Token> tokens = new ArrayList<Token>(converter.convert("aaa AND bbb ccc"));
+      List<Token> tokens = new ArrayList<>(converter.convert("aaa AND bbb ccc"));
       assertTrue("Should have 3 tokens",           tokens != null && tokens.size()==3);
       assertTrue("token 1 doesn't precede n.b.o.",  !hasNBOFlag(tokens.get(0)) && hasInBooleanFlag(tokens.get(0)));
       assertTrue("token 2 doesn't precede n.b.o.",  !hasNBOFlag(tokens.get(1)) && hasInBooleanFlag(tokens.get(0)));
       assertTrue("token 3 doesn't precede n.b.o.",  !hasNBOFlag(tokens.get(2)) && hasInBooleanFlag(tokens.get(0)));
     }
     {
-      List<Token> tokens = new ArrayList<Token>(converter.convert("aaa OR bbb OR ccc"));
+      List<Token> tokens = new ArrayList<>(converter.convert("aaa OR bbb OR ccc"));
       assertTrue("Should have 3 tokens",           tokens != null && tokens.size()==3);
       assertTrue("token 1 doesn't precede n.b.o.",  !hasNBOFlag(tokens.get(0)) && hasInBooleanFlag(tokens.get(0)));
       assertTrue("token 2 doesn't precede n.b.o.",  !hasNBOFlag(tokens.get(1)) && hasInBooleanFlag(tokens.get(0)));
       assertTrue("token 3 doesn't precede n.b.o.",  !hasNBOFlag(tokens.get(2)) && hasInBooleanFlag(tokens.get(0)));
     }
     {
-      List<Token> tokens = new ArrayList<Token>(converter.convert("aaa AND bbb NOT ccc"));
+      List<Token> tokens = new ArrayList<>(converter.convert("aaa AND bbb NOT ccc"));
       assertTrue("Should have 3 tokens",            tokens != null && tokens.size()==3);
       assertTrue("token 1 doesn't precede n.b.o.",  !hasNBOFlag(tokens.get(0)) && hasInBooleanFlag(tokens.get(0)));
       assertTrue("token 2 precedes n.b.o.",          hasNBOFlag(tokens.get(1)) && hasInBooleanFlag(tokens.get(0)));
       assertTrue("token 3 doesn't precede n.b.o.",  !hasNBOFlag(tokens.get(2)) && hasInBooleanFlag(tokens.get(0)));
     }
     {
-      List<Token> tokens = new ArrayList<Token>(converter.convert("aaa NOT bbb AND ccc"));
+      List<Token> tokens = new ArrayList<>(converter.convert("aaa NOT bbb AND ccc"));
       assertTrue("Should have 3 tokens",           tokens != null && tokens.size()==3);
       assertTrue("token 1 precedes n.b.o.",          hasNBOFlag(tokens.get(0)) && hasInBooleanFlag(tokens.get(0)));
       assertTrue("token 2 precedes n.b.o.",          hasNBOFlag(tokens.get(1)) && hasInBooleanFlag(tokens.get(0)));
       assertTrue("token 3 doesn't precedes n.b.o.", !hasNBOFlag(tokens.get(2)) && hasInBooleanFlag(tokens.get(0)));
     }
     {
-      List<Token> tokens = new ArrayList<Token>(converter.convert("aaa AND NOT bbb AND ccc"));
+      List<Token> tokens = new ArrayList<>(converter.convert("aaa AND NOT bbb AND ccc"));
       assertTrue("Should have 3 tokens",           tokens != null && tokens.size()==3);
       assertTrue("token 1 precedes n.b.o.",          hasNBOFlag(tokens.get(0)) && hasInBooleanFlag(tokens.get(0)));
       assertTrue("token 2 precedes n.b.o.",          hasNBOFlag(tokens.get(1)) && hasInBooleanFlag(tokens.get(0)));
diff --git solr/core/src/test/org/apache/solr/spelling/WordBreakSolrSpellCheckerTest.java solr/core/src/test/org/apache/solr/spelling/WordBreakSolrSpellCheckerTest.java
index 8d3e724..dfa4a6c 100644
--- solr/core/src/test/org/apache/solr/spelling/WordBreakSolrSpellCheckerTest.java
+++ solr/core/src/test/org/apache/solr/spelling/WordBreakSolrSpellCheckerTest.java
@@ -58,7 +58,7 @@ public class WordBreakSolrSpellCheckerTest extends SolrTestCaseJ4 {
   public void testStandAlone() throws Exception {
     SolrCore core = h.getCore();
     WordBreakSolrSpellChecker checker = new WordBreakSolrSpellChecker();
-    NamedList<String> params = new NamedList<String>();
+    NamedList<String> params = new NamedList<>();
     params.add("field", "lowerfilt");
     params.add(WordBreakSolrSpellChecker.PARAM_BREAK_WORDS, "true");
     params.add(WordBreakSolrSpellChecker.PARAM_COMBINE_WORDS, "true");
diff --git solr/core/src/test/org/apache/solr/store/hdfs/HdfsDirectoryTest.java solr/core/src/test/org/apache/solr/store/hdfs/HdfsDirectoryTest.java
index 2349112..34b2dc0 100644
--- solr/core/src/test/org/apache/solr/store/hdfs/HdfsDirectoryTest.java
+++ solr/core/src/test/org/apache/solr/store/hdfs/HdfsDirectoryTest.java
@@ -150,7 +150,7 @@ public class HdfsDirectoryTest extends SolrTestCaseJ4 {
   public void testRandomAccessWrites() throws IOException {
     int i = 0;
     try {
-      Set<String> names = new HashSet<String>();
+      Set<String> names = new HashSet<>();
       for (; i< 10; i++) {
         Directory fsDir = new RAMDirectory();
         String name = getName();
diff --git solr/core/src/test/org/apache/solr/update/AddBlockUpdateTest.java solr/core/src/test/org/apache/solr/update/AddBlockUpdateTest.java
index f040445..e9aae01 100644
--- solr/core/src/test/org/apache/solr/update/AddBlockUpdateTest.java
+++ solr/core/src/test/org/apache/solr/update/AddBlockUpdateTest.java
@@ -188,7 +188,7 @@ public class AddBlockUpdateTest extends SolrTestCaseJ4 {
   
   @Test
   public void testBasics() throws Exception {
-    List<Document> blocks = new ArrayList<Document>(Arrays.asList(
+    List<Document> blocks = new ArrayList<>(Arrays.asList(
         block("abcD"),
         block("efgH"),
         merge(block("ijkL"), block("mnoP")),
@@ -268,7 +268,7 @@ public class AddBlockUpdateTest extends SolrTestCaseJ4 {
   public void testSolrJXML() throws IOException {
     UpdateRequest req = new UpdateRequest();
     
-    List<SolrInputDocument> docs = new ArrayList<SolrInputDocument>();
+    List<SolrInputDocument> docs = new ArrayList<>();
     
     SolrInputDocument document1 = new SolrInputDocument() {
       {
@@ -276,7 +276,7 @@ public class AddBlockUpdateTest extends SolrTestCaseJ4 {
         addField("id", id);
         addField("parent_s", "X");
         
-        ArrayList<SolrInputDocument> ch1 = new ArrayList<SolrInputDocument>(
+        ArrayList<SolrInputDocument> ch1 = new ArrayList<>(
             Arrays.asList(new SolrInputDocument() {
               {
                 addField("id", id());
@@ -336,7 +336,7 @@ public class AddBlockUpdateTest extends SolrTestCaseJ4 {
   public void testXML() throws IOException, XMLStreamException {
     UpdateRequest req = new UpdateRequest();
     
- List<SolrInputDocument> docs = new ArrayList<SolrInputDocument>();
+ List<SolrInputDocument> docs = new ArrayList<>();
     
  
     String xml_doc1 =
@@ -571,7 +571,7 @@ public class AddBlockUpdateTest extends SolrTestCaseJ4 {
   }
   
   private Collection<? extends Callable<Void>> callables(List<Document> blocks) {
-    final List<Callable<Void>> rez = new ArrayList<Callable<Void>>();
+    final List<Callable<Void>> rez = new ArrayList<>();
     for (Document block : blocks) {
       final String msg = block.asXML();
       if (msg.length() > 0) {
diff --git solr/core/src/test/org/apache/solr/update/AutoCommitTest.java solr/core/src/test/org/apache/solr/update/AutoCommitTest.java
index fbf35ec..db2b672 100644
--- solr/core/src/test/org/apache/solr/update/AutoCommitTest.java
+++ solr/core/src/test/org/apache/solr/update/AutoCommitTest.java
@@ -142,7 +142,7 @@ public class AutoCommitTest extends AbstractSolrTestCase {
    */
   public static Collection<ContentStream> toContentStreams( final String str, final String contentType )
   {
-    ArrayList<ContentStream> streams = new ArrayList<ContentStream>();
+    ArrayList<ContentStream> streams = new ArrayList<>();
     ContentStreamBase stream = new ContentStreamBase.StringStream( str );
     stream.setContentType( contentType );
     streams.add( stream );
diff --git solr/core/src/test/org/apache/solr/update/DirectUpdateHandlerTest.java solr/core/src/test/org/apache/solr/update/DirectUpdateHandlerTest.java
index 77267cb..5fb1e14 100644
--- solr/core/src/test/org/apache/solr/update/DirectUpdateHandlerTest.java
+++ solr/core/src/test/org/apache/solr/update/DirectUpdateHandlerTest.java
@@ -171,7 +171,7 @@ public class DirectUpdateHandlerTest extends SolrTestCaseJ4 {
     ureq.close();
     
     // search - "B" should not be found.
-    Map<String,String> args = new HashMap<String, String>();
+    Map<String,String> args = new HashMap<>();
     args.put( CommonParams.Q, "id:A OR id:B" );
     args.put( "indent", "true" );
     SolrQueryRequest req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
@@ -217,7 +217,7 @@ public class DirectUpdateHandlerTest extends SolrTestCaseJ4 {
     ureq.close();
 
     // search - "A","B" should be found.
-    Map<String,String> args = new HashMap<String, String>();
+    Map<String,String> args = new HashMap<>();
     args.put( CommonParams.Q, "id:A OR id:B" );
     args.put( "indent", "true" );
     SolrQueryRequest req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
diff --git solr/core/src/test/org/apache/solr/update/SoftAutoCommitTest.java solr/core/src/test/org/apache/solr/update/SoftAutoCommitTest.java
index 8ca9f3e..d1b00ce 100644
--- solr/core/src/test/org/apache/solr/update/SoftAutoCommitTest.java
+++ solr/core/src/test/org/apache/solr/update/SoftAutoCommitTest.java
@@ -346,9 +346,9 @@ public class SoftAutoCommitTest extends AbstractSolrTestCase {
 class MockEventListener implements SolrEventListener {
 
   // use capacity bound Queues just so we're sure we don't OOM 
-  public final BlockingQueue<Long> soft = new LinkedBlockingQueue<Long>(1000);
-  public final BlockingQueue<Long> hard = new LinkedBlockingQueue<Long>(1000);
-  public final BlockingQueue<Long> searcher = new LinkedBlockingQueue<Long>(1000);
+  public final BlockingQueue<Long> soft = new LinkedBlockingQueue<>(1000);
+  public final BlockingQueue<Long> hard = new LinkedBlockingQueue<>(1000);
+  public final BlockingQueue<Long> searcher = new LinkedBlockingQueue<>(1000);
 
   // if non enpty, then at least one offer failed (queues full)
   private StringBuffer fail = new StringBuffer();
diff --git solr/core/src/test/org/apache/solr/update/SolrCmdDistributorTest.java solr/core/src/test/org/apache/solr/update/SolrCmdDistributorTest.java
index 05e1f8d..07ccd71 100644
--- solr/core/src/test/org/apache/solr/update/SolrCmdDistributorTest.java
+++ solr/core/src/test/org/apache/solr/update/SolrCmdDistributorTest.java
@@ -133,7 +133,7 @@ public class SolrCmdDistributorTest extends BaseDistributedSearchTestCase {
     
     ModifiableSolrParams params = new ModifiableSolrParams();
 
-    List<Node> nodes = new ArrayList<Node>();
+    List<Node> nodes = new ArrayList<>();
 
     ZkNodeProps nodeProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP,
         ((HttpSolrServer) controlClient).getBaseURL(),
@@ -325,7 +325,7 @@ public class SolrCmdDistributorTest extends BaseDistributedSearchTestCase {
     final MockStreamingSolrServers ss = new MockStreamingSolrServers(updateShardHandler);
     SolrCmdDistributor cmdDistrib = new SolrCmdDistributor(ss, 5, 0);
     ss.setExp(Exp.CONNECT_EXCEPTION);
-    ArrayList<Node> nodes = new ArrayList<Node>();
+    ArrayList<Node> nodes = new ArrayList<>();
     final HttpSolrServer solrclient1 = (HttpSolrServer) clients.get(0);
     
     final AtomicInteger retries = new AtomicInteger();
@@ -359,7 +359,7 @@ public class SolrCmdDistributorTest extends BaseDistributedSearchTestCase {
     final MockStreamingSolrServers ss = new MockStreamingSolrServers(updateShardHandler);
     SolrCmdDistributor cmdDistrib = new SolrCmdDistributor(ss, 5, 0);
     ss.setExp(Exp.CONNECT_EXCEPTION);
-    ArrayList<Node> nodes = new ArrayList<Node>();
+    ArrayList<Node> nodes = new ArrayList<>();
 
     ZkNodeProps nodeProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP, solrclient.getBaseURL(),
         ZkStateReader.CORE_NAME_PROP, "");
@@ -405,7 +405,7 @@ public class SolrCmdDistributorTest extends BaseDistributedSearchTestCase {
     final MockStreamingSolrServers ss = new MockStreamingSolrServers(updateShardHandler);
     SolrCmdDistributor cmdDistrib = new SolrCmdDistributor(ss, 5, 0);
     ss.setExp(Exp.SOCKET_EXCEPTION);
-    ArrayList<Node> nodes = new ArrayList<Node>();
+    ArrayList<Node> nodes = new ArrayList<>();
 
     ZkNodeProps nodeProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP, solrclient.getBaseURL(),
         ZkStateReader.CORE_NAME_PROP, "");
@@ -453,7 +453,7 @@ public class SolrCmdDistributorTest extends BaseDistributedSearchTestCase {
     long numFoundBefore = solrclient.query(new SolrQuery("*:*")).getResults()
         .getNumFound();
     
-    ArrayList<Node> nodes = new ArrayList<Node>();
+    ArrayList<Node> nodes = new ArrayList<>();
 
     ZkNodeProps nodeProps = new ZkNodeProps(ZkStateReader.BASE_URL_PROP, "[ff01::114]:33332" + context, ZkStateReader.CORE_NAME_PROP, "");
     RetryNode retryNode = new RetryNode(new ZkCoreNodeProps(nodeProps), null, "collection1", "shard1") {
diff --git solr/core/src/test/org/apache/solr/update/TestDocBasedVersionConstraints.java solr/core/src/test/org/apache/solr/update/TestDocBasedVersionConstraints.java
index 21ceaba..89e898c 100755
--- solr/core/src/test/org/apache/solr/update/TestDocBasedVersionConstraints.java
+++ solr/core/src/test/org/apache/solr/update/TestDocBasedVersionConstraints.java
@@ -367,7 +367,7 @@ public class TestDocBasedVersionConstraints extends SolrTestCaseJ4 {
         final int winner = TestUtil.nextInt(random(), 0, numAdds - 1);
         final int winnerVersion = atLeast(100);
         final boolean winnerIsDeleted = (0 == TestUtil.nextInt(random(), 0, 4));
-        List<Callable<Object>> tasks = new ArrayList<Callable<Object>>(numAdds);
+        List<Callable<Object>> tasks = new ArrayList<>(numAdds);
         for (int variant = 0; variant < numAdds; variant++) {
           final boolean iShouldWin = (variant==winner);
           final long version = (iShouldWin ? winnerVersion 
diff --git solr/core/src/test/org/apache/solr/update/processor/FieldMutatingUpdateProcessorTest.java solr/core/src/test/org/apache/solr/update/processor/FieldMutatingUpdateProcessorTest.java
index 9e3fe69..48ac66c 100644
--- solr/core/src/test/org/apache/solr/update/processor/FieldMutatingUpdateProcessorTest.java
+++ solr/core/src/test/org/apache/solr/update/processor/FieldMutatingUpdateProcessorTest.java
@@ -415,7 +415,7 @@ public class FieldMutatingUpdateProcessorTest extends UpdateProcessorTestBase {
     // test something that's definitely a SortedSet
 
     special = new SolrInputField("foo_s");
-    special.setValue(new TreeSet<String>
+    special.setValue(new TreeSet<>
                      (Arrays.asList("ggg", "first", "last", "hhh")), 1.2F);
     
     d = processAdd("last-value", 
@@ -443,7 +443,7 @@ public class FieldMutatingUpdateProcessorTest extends UpdateProcessorTestBase {
     // (ie: get default behavior of Collection using iterator)
 
     special = new SolrInputField("foo_s");
-    special.setValue(new LinkedHashSet<String>
+    special.setValue(new LinkedHashSet<>
                      (Arrays.asList("first", "ggg", "hhh", "last")), 1.2F);
     
     d = processAdd("last-value", 
diff --git solr/core/src/test/org/apache/solr/update/processor/ParsingFieldUpdateProcessorsTest.java solr/core/src/test/org/apache/solr/update/processor/ParsingFieldUpdateProcessorsTest.java
index 4be6c21..691e8ff 100644
--- solr/core/src/test/org/apache/solr/update/processor/ParsingFieldUpdateProcessorsTest.java
+++ solr/core/src/test/org/apache/solr/update/processor/ParsingFieldUpdateProcessorsTest.java
@@ -240,7 +240,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
     IndexSchema schema = h.getCore().getLatestSchema();
     assertNull(schema.getFieldOrNull("not_in_schema"));
     DateTimeFormatter dateTimeFormatter = ISODateTimeFormat.dateOptionalTimeParser().withZoneUTC();
-    Map<Object,Object> mixed = new HashMap<Object,Object>();
+    Map<Object,Object> mixed = new HashMap<>();
     String[] dateStrings = { "2020-05-13T18:47", "1989-12-14", "1682-07-22T18:33:00.000Z" };
     for (String dateString : dateStrings) {
       mixed.put(dateTimeFormatter.parseDateTime(dateString).toDate(), dateString);
@@ -339,7 +339,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
   public void testFailedParseMixedInt() throws Exception {
     IndexSchema schema = h.getCore().getLatestSchema();
     assertNull(schema.getFieldOrNull("not_in_schema"));
-    Map<Object,Object> mixed = new HashMap<Object,Object>();
+    Map<Object,Object> mixed = new HashMap<>();
     Float floatVal = 294423.0f;
     mixed.put(85, "85");
     mixed.put(floatVal, floatVal); // Float-typed field value
@@ -422,7 +422,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
   public void testFailedParseMixedLong() throws Exception {
     IndexSchema schema = h.getCore().getLatestSchema();
     assertNull(schema.getFieldOrNull("not_in_schema"));
-    Map<Object,Object> mixed = new HashMap<Object,Object>();
+    Map<Object,Object> mixed = new HashMap<>();
     Float floatVal = 294423.0f;
     mixed.put(85L, "85");
     mixed.put(floatVal, floatVal); // Float-typed field value
@@ -506,7 +506,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
   public void testMixedFloats() throws Exception {
     IndexSchema schema = h.getCore().getLatestSchema();
     assertNotNull(schema.getFieldOrNull("float_tf")); // should match dynamic field "*_tf"
-    Map<Float,Object> mixedFloats = new HashMap<Float,Object>();
+    Map<Float,Object> mixedFloats = new HashMap<>();
     mixedFloats.put(85.0f, "85");
     mixedFloats.put(2894518.0f, "2,894,518");
     mixedFloats.put(2.94423E-9f, 2.94423E-9f); // Float-typed field value
@@ -524,7 +524,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
   public void testFailedParseMixedFloat() throws Exception {
     IndexSchema schema = h.getCore().getLatestSchema();
     assertNull(schema.getFieldOrNull("not_in_schema"));
-    Map<Object,Object> mixed = new HashMap<Object,Object>();
+    Map<Object,Object> mixed = new HashMap<>();
     Long longVal = 294423L;
     mixed.put(85L, "85");
     mixed.put(longVal, longVal); // Float-typed field value
@@ -608,7 +608,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
   public void testFailedParseMixedDouble() throws Exception {
     IndexSchema schema = h.getCore().getLatestSchema();
     assertNull(schema.getFieldOrNull("not_in_schema"));
-    Map<Object,Object> mixed = new HashMap<Object,Object>();
+    Map<Object,Object> mixed = new HashMap<>();
     Long longVal = 294423L;
     mixed.put(85, "85.0");
     mixed.put(longVal, longVal); // Float-typed field value
@@ -710,7 +710,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
   public void testFailedParseMixedBoolean() throws Exception {
     IndexSchema schema = h.getCore().getLatestSchema();
     assertNull(schema.getFieldOrNull("not_in_schema"));
-    Map<Object,Object> mixed = new HashMap<Object,Object>();
+    Map<Object,Object> mixed = new HashMap<>();
     Long longVal = 294423L;
     mixed.put(true, "true");
     mixed.put(longVal, longVal); // Float-typed field value
@@ -739,7 +739,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
     SolrInputDocument d = null;
     String chain = "cascading-parsers-no-run-processor";
     
-    Map<Boolean,String> booleans = new HashMap<Boolean,String>();
+    Map<Boolean,String> booleans = new HashMap<>();
     booleans.put(true, "truE");
     booleans.put(false, "False");
     d = processAdd(chain, doc(f("id", "341"), f(fieldName, booleans.values())));
@@ -750,7 +750,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
     }
     assertTrue(booleans.isEmpty());
 
-    Map<Integer,String> ints = new HashMap<Integer,String>();
+    Map<Integer,String> ints = new HashMap<>();
     ints.put(2, "2");
     ints.put(50928, "50928");
     ints.put(86942008, "86,942,008");
@@ -762,7 +762,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
     }
     assertTrue(ints.isEmpty());
 
-    Map<Long,String> longs = new HashMap<Long,String>();
+    Map<Long,String> longs = new HashMap<>();
     longs.put(2L, "2");
     longs.put(50928L, "50928");
     longs.put(86942008987654L, "86,942,008,987,654");
@@ -789,7 +789,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
     }
     */
 
-    Map<Double,String> doubles = new HashMap<Double,String>();
+    Map<Double,String> doubles = new HashMap<>();
     doubles.put(2.0, "2.");
     doubles.put(509.28, "509.28");
     doubles.put(86942.008, "86,942.008");
@@ -801,7 +801,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
     }
 
     DateTimeFormatter dateTimeFormatter = ISODateTimeFormat.dateOptionalTimeParser().withZoneUTC();
-    Map<Date,String> dates = new HashMap<Date,String>();
+    Map<Date,String> dates = new HashMap<>();
     String[] dateStrings = { "2020-05-13T18:47", "1989-12-14", "1682-07-22T18:33:00.000Z" };
     for (String dateString : dateStrings) {
       dates.put(dateTimeFormatter.parseDateTime(dateString).toDate(), dateString);
@@ -814,7 +814,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
     }
     assertTrue(dates.isEmpty());
     
-    Map<Double,String> mixedLongsAndDoubles = new LinkedHashMap<Double,String>(); // preserve order
+    Map<Double,String> mixedLongsAndDoubles = new LinkedHashMap<>(); // preserve order
     mixedLongsAndDoubles.put(85.0, "85");
     mixedLongsAndDoubles.put(2.94423E-9, "2.94423E-9");
     mixedLongsAndDoubles.put(2894518.0, "2,894,518");
@@ -827,7 +827,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
     }
     assertTrue(mixedLongsAndDoubles.isEmpty());
     
-    Set<String> mixed = new HashSet<String>();
+    Set<String> mixed = new HashSet<>();
     mixed.add("true");
     mixed.add("1682-07-22T18:33:00.000Z");
     mixed.add("2,894,518");
@@ -839,7 +839,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
       assertTrue(o instanceof String);
     }
 
-    Map<Double,Object> mixedDoubles = new LinkedHashMap<Double,Object>(); // preserve order
+    Map<Double,Object> mixedDoubles = new LinkedHashMap<>(); // preserve order
     mixedDoubles.put(85.0, "85");
     mixedDoubles.put(2.94423E-9, 2.94423E-9); // Double-typed field value
     mixedDoubles.put(2894518.0, "2,894,518");
@@ -852,7 +852,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
     }
     assertTrue(mixedDoubles.isEmpty());
 
-    Map<Integer,Object> mixedInts = new LinkedHashMap<Integer,Object>(); // preserve order
+    Map<Integer,Object> mixedInts = new LinkedHashMap<>(); // preserve order
     mixedInts.put(85, "85");
     mixedInts.put(294423, 294423); // Integer-typed field value
     mixedInts.put(-2894518, "-2,894,518");
@@ -865,7 +865,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
     }
     assertTrue(mixedInts.isEmpty());
 
-    Map<Long,Object> mixedLongs = new LinkedHashMap<Long,Object>(); // preserve order
+    Map<Long,Object> mixedLongs = new LinkedHashMap<>(); // preserve order
     mixedLongs.put(85L, "85");
     mixedLongs.put(42944233L, 42944233L); // Long-typed field value
     mixedLongs.put(2894518L, "2,894,518");
@@ -878,7 +878,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
     }
     assertTrue(mixedLongs.isEmpty());
 
-    Map<Boolean,Object> mixedBooleans = new LinkedHashMap<Boolean,Object>(); // preserve order
+    Map<Boolean,Object> mixedBooleans = new LinkedHashMap<>(); // preserve order
     mixedBooleans.put(true, "true");
     mixedBooleans.put(false, false); // Boolean-typed field value
     mixedBooleans.put(false, "false");
@@ -892,7 +892,7 @@ public class ParsingFieldUpdateProcessorsTest extends UpdateProcessorTestBase {
     assertTrue(mixedBooleans.isEmpty());
 
     dateTimeFormatter = ISODateTimeFormat.dateOptionalTimeParser().withZoneUTC();
-    Map<Date,Object> mixedDates = new HashMap<Date,Object>();
+    Map<Date,Object> mixedDates = new HashMap<>();
     dateStrings = new String[] { "2020-05-13T18:47", "1989-12-14", "1682-07-22T18:33:00.000Z" };
     for (String dateString : dateStrings) {
       mixedDates.put(dateTimeFormatter.parseDateTime(dateString).toDate(), dateString);
diff --git solr/core/src/test/org/apache/solr/update/processor/SignatureUpdateProcessorFactoryTest.java solr/core/src/test/org/apache/solr/update/processor/SignatureUpdateProcessorFactoryTest.java
index ce0d72a..dde06e4 100644
--- solr/core/src/test/org/apache/solr/update/processor/SignatureUpdateProcessorFactoryTest.java
+++ solr/core/src/test/org/apache/solr/update/processor/SignatureUpdateProcessorFactoryTest.java
@@ -253,7 +253,7 @@ public class SignatureUpdateProcessorFactoryTest extends SolrTestCaseJ4 {
   public void testFailNonIndexedSigWithOverwriteDupes() throws Exception {
     SolrCore core = h.getCore();
     SignatureUpdateProcessorFactory f = new SignatureUpdateProcessorFactory();
-    NamedList<String> initArgs = new NamedList<String>();
+    NamedList<String> initArgs = new NamedList<>();
     initArgs.add("overwriteDupes", "true");
     initArgs.add("signatureField", "signatureField_sS");
     f.init(initArgs);
@@ -278,7 +278,7 @@ public class SignatureUpdateProcessorFactoryTest extends SolrTestCaseJ4 {
         .getFactories()[0]);
     factory.setEnabled(true);
     
-    Map<String,String[]> params = new HashMap<String,String[]>();
+    Map<String,String[]> params = new HashMap<>();
     MultiMapSolrParams mmparams = new MultiMapSolrParams(params);
     params.put(UpdateParams.UPDATE_CHAIN, new String[] {chain});
     
@@ -307,7 +307,7 @@ public class SignatureUpdateProcessorFactoryTest extends SolrTestCaseJ4 {
       SolrInputDocument docA = new SolrInputDocument();
       SolrInputDocument docB = new SolrInputDocument();
 
-      UnusualList<Integer> ints = new UnusualList<Integer>(3);
+      UnusualList<Integer> ints = new UnusualList<>(3);
       for (int val : new int[] {42, 66, 34}) {
         docA.addField("ints_is", new Integer(val));
         ints.add(val);
@@ -333,7 +333,7 @@ public class SignatureUpdateProcessorFactoryTest extends SolrTestCaseJ4 {
     }
         
 
-    ArrayList<ContentStream> streams = new ArrayList<ContentStream>(2);
+    ArrayList<ContentStream> streams = new ArrayList<>(2);
     streams.add(new BinaryRequestWriter().getContentStream(ureq));
     LocalSolrQueryRequest req = new LocalSolrQueryRequest(h.getCore(), mmparams);
     try {
@@ -368,7 +368,7 @@ public class SignatureUpdateProcessorFactoryTest extends SolrTestCaseJ4 {
   }
 
   static void addDoc(String doc, String chain) throws Exception {
-    Map<String, String[]> params = new HashMap<String, String[]>();
+    Map<String, String[]> params = new HashMap<>();
     MultiMapSolrParams mmparams = new MultiMapSolrParams(params);
     params.put(UpdateParams.UPDATE_CHAIN, new String[] { chain });
     SolrQueryRequestBase req = new SolrQueryRequestBase(h.getCore(),
@@ -377,7 +377,7 @@ public class SignatureUpdateProcessorFactoryTest extends SolrTestCaseJ4 {
 
     UpdateRequestHandler handler = new UpdateRequestHandler();
     handler.init(null);
-    ArrayList<ContentStream> streams = new ArrayList<ContentStream>(2);
+    ArrayList<ContentStream> streams = new ArrayList<>(2);
     streams.add(new ContentStreamBase.StringStream(doc));
     req.setContentStreams(streams);
     handler.handleRequestBody(req, new SolrQueryResponse());
diff --git solr/core/src/test/org/apache/solr/update/processor/StatelessScriptUpdateProcessorFactoryTest.java solr/core/src/test/org/apache/solr/update/processor/StatelessScriptUpdateProcessorFactoryTest.java
index 9dad69c..7ea4b6b 100644
--- solr/core/src/test/org/apache/solr/update/processor/StatelessScriptUpdateProcessorFactoryTest.java
+++ solr/core/src/test/org/apache/solr/update/processor/StatelessScriptUpdateProcessorFactoryTest.java
@@ -77,7 +77,7 @@ public class StatelessScriptUpdateProcessorFactoryTest extends UpdateProcessorTe
     SolrCore core = h.getCore();
     UpdateRequestProcessorChain chained = core.getUpdateProcessingChain("single-script");
     final StatelessScriptUpdateProcessorFactory factory = ((StatelessScriptUpdateProcessorFactory) chained.getFactories()[0]);
-    final List<String> functionMessages = new ArrayList<String>();
+    final List<String> functionMessages = new ArrayList<>();
     factory.setScriptEngineCustomizer(new ScriptEngineCustomizer() {
       @Override
       public void customize(ScriptEngine engine) {
@@ -122,7 +122,7 @@ public class StatelessScriptUpdateProcessorFactoryTest extends UpdateProcessorTe
       UpdateRequestProcessorChain chained = core.getUpdateProcessingChain(chain);
       final StatelessScriptUpdateProcessorFactory factory = 
         ((StatelessScriptUpdateProcessorFactory) chained.getFactories()[0]);
-      final List<String> functionMessages = new ArrayList<String>();
+      final List<String> functionMessages = new ArrayList<>();
       ScriptEngineCustomizer customizer = new ScriptEngineCustomizer() {
           @Override
           public void customize(ScriptEngine engine) {
diff --git solr/core/src/test/org/apache/solr/update/processor/UniqFieldsUpdateProcessorFactoryTest.java solr/core/src/test/org/apache/solr/update/processor/UniqFieldsUpdateProcessorFactoryTest.java
index 78ac58a..b2e5ff0 100644
--- solr/core/src/test/org/apache/solr/update/processor/UniqFieldsUpdateProcessorFactoryTest.java
+++ solr/core/src/test/org/apache/solr/update/processor/UniqFieldsUpdateProcessorFactoryTest.java
@@ -106,7 +106,7 @@ public class UniqFieldsUpdateProcessorFactoryTest extends SolrTestCaseJ4 {
   }
 
   private void addDoc(String doc) throws Exception {
-    Map<String, String[]> params = new HashMap<String, String[]>();
+    Map<String, String[]> params = new HashMap<>();
     MultiMapSolrParams mmparams = new MultiMapSolrParams(params);
     params.put(UpdateParams.UPDATE_CHAIN, new String[] { "uniq-fields" });
     SolrQueryRequestBase req = new SolrQueryRequestBase(h.getCore(),
@@ -115,7 +115,7 @@ public class UniqFieldsUpdateProcessorFactoryTest extends SolrTestCaseJ4 {
 
     UpdateRequestHandler handler = new UpdateRequestHandler();
     handler.init(null);
-    ArrayList<ContentStream> streams = new ArrayList<ContentStream>(2);
+    ArrayList<ContentStream> streams = new ArrayList<>(2);
     streams.add(new ContentStreamBase.StringStream(doc));
     req.setContentStreams(streams);
     handler.handleRequestBody(req, new SolrQueryResponse());
diff --git solr/core/src/test/org/apache/solr/util/CircularListTest.java solr/core/src/test/org/apache/solr/util/CircularListTest.java
index f4c4b9d..5267428 100644
--- solr/core/src/test/org/apache/solr/util/CircularListTest.java
+++ solr/core/src/test/org/apache/solr/util/CircularListTest.java
@@ -30,7 +30,7 @@ public class CircularListTest  extends LuceneTestCase {
 
   @Test
   public void testCircularList() throws IOException {
-    CircularList<Integer> list = new CircularList<Integer>(10);
+    CircularList<Integer> list = new CircularList<>(10);
     for(int i=0;i<10; i++) {
       list.add(new Integer(i));
     }
diff --git solr/core/src/test/org/apache/solr/util/DOMUtilTest.java solr/core/src/test/org/apache/solr/util/DOMUtilTest.java
index 8b4fc9c..b80f622 100644
--- solr/core/src/test/org/apache/solr/util/DOMUtilTest.java
+++ solr/core/src/test/org/apache/solr/util/DOMUtilTest.java
@@ -23,7 +23,7 @@ import org.apache.solr.common.util.SimpleOrderedMap;
 public class DOMUtilTest extends DOMUtilTestBase {
   
   public void testAddToNamedListPrimitiveTypes() throws Exception {
-    NamedList<Object> namedList = new SimpleOrderedMap<Object>();
+    NamedList<Object> namedList = new SimpleOrderedMap<>();
     DOMUtil.addToNamedList( getNode( "<str name=\"String\">STRING</str>", "/str" ), namedList, null );
     assertTypeAndValue( namedList, "String", "STRING" );
     DOMUtil.addToNamedList( getNode( "<int name=\"Integer\">100</int>", "/int" ), namedList, null );
diff --git solr/core/src/test/org/apache/solr/util/DateMathParserTest.java solr/core/src/test/org/apache/solr/util/DateMathParserTest.java
index 715055b..a17ed66 100644
--- solr/core/src/test/org/apache/solr/util/DateMathParserTest.java
+++ solr/core/src/test/org/apache/solr/util/DateMathParserTest.java
@@ -314,7 +314,7 @@ public class DateMathParserTest extends LuceneTestCase {
     DateMathParser p = new DateMathParser(UTC, Locale.ROOT);
     p.setNow(parser.parse("2001-07-04T12:08:56.235"));
     
-    Map<String,Integer> badCommands = new HashMap<String,Integer>();
+    Map<String,Integer> badCommands = new HashMap<>();
     badCommands.put("/", 1);
     badCommands.put("+", 1);
     badCommands.put("-", 1);
diff --git solr/core/src/test/org/apache/solr/util/SimplePostToolTest.java solr/core/src/test/org/apache/solr/util/SimplePostToolTest.java
index 08dbcc1..8655bd9 100644
--- solr/core/src/test/org/apache/solr/util/SimplePostToolTest.java
+++ solr/core/src/test/org/apache/solr/util/SimplePostToolTest.java
@@ -177,8 +177,8 @@ public class SimplePostToolTest extends SolrTestCaseJ4 {
   }
 
   static class MockPageFetcher extends PageFetcher {
-    HashMap<String,String> htmlMap = new HashMap<String,String>();
-    HashMap<String,Set<URL>> linkMap = new HashMap<String,Set<URL>>();
+    HashMap<String,String> htmlMap = new HashMap<>();
+    HashMap<String,Set<URL>> linkMap = new HashMap<>();
     
     public MockPageFetcher() throws IOException {
       (new SimplePostTool()).super();
@@ -190,18 +190,18 @@ public class SimplePostToolTest extends SolrTestCaseJ4 {
       htmlMap.put("http://[ff01::114]/page2", "<html><body><a href=\"http://[ff01::114]/\"><a href=\"http://[ff01::114]/disallowed\"/></body></html>");
       htmlMap.put("http://[ff01::114]/disallowed", "<html><body><a href=\"http://[ff01::114]/\"></body></html>");
 
-      Set<URL> s = new HashSet<URL>();
+      Set<URL> s = new HashSet<>();
       s.add(new URL("http://[ff01::114]/page1"));
       s.add(new URL("http://[ff01::114]/page2"));
       linkMap.put("http://[ff01::114]", s);
       linkMap.put("http://[ff01::114]/index.html", s);
-      s = new HashSet<URL>();
+      s = new HashSet<>();
       s.add(new URL("http://[ff01::114]/page1/foo"));
       linkMap.put("http://[ff01::114]/page1", s);
-      s = new HashSet<URL>();
+      s = new HashSet<>();
       s.add(new URL("http://[ff01::114]/page1/foo/bar"));
       linkMap.put("http://[ff01::114]/page1/foo", s);
-      s = new HashSet<URL>();
+      s = new HashSet<>();
       s.add(new URL("http://[ff01::114]/disallowed"));
       linkMap.put("http://[ff01::114]/page2", s);
       
@@ -237,7 +237,7 @@ public class SimplePostToolTest extends SolrTestCaseJ4 {
     public Set<URL> getLinksFromWebPage(URL u, InputStream is, String type, URL postUrl) {
       Set<URL> s = linkMap.get(SimplePostTool.normalizeUrlEnding(u.toString()));
       if(s == null)
-        s = new HashSet<URL>();
+        s = new HashSet<>();
       return s;
     }
   }
diff --git solr/core/src/test/org/apache/solr/util/SolrPluginUtilsTest.java solr/core/src/test/org/apache/solr/util/SolrPluginUtilsTest.java
index 7894d34..a4d9092 100644
--- solr/core/src/test/org/apache/solr/util/SolrPluginUtilsTest.java
+++ solr/core/src/test/org/apache/solr/util/SolrPluginUtilsTest.java
@@ -76,7 +76,7 @@ public class SolrPluginUtilsTest extends SolrTestCaseJ4 {
       
       DocList docs = qr.getDocList();
       assertEquals("wrong docs size", 3, docs.size());
-      Set<String> fields = new HashSet<String>();
+      Set<String> fields = new HashSet<>();
       fields.add("val_dynamic");
       fields.add("dynamic_val");
       fields.add("range_facet_l"); // copied from id
@@ -149,7 +149,7 @@ public class SolrPluginUtilsTest extends SolrTestCaseJ4 {
   @Test
   public void testParseFieldBoosts() throws Exception {
 
-    Map<String,Float> e1 = new HashMap<String,Float>();
+    Map<String,Float> e1 = new HashMap<>();
     e1.put("fieldOne",2.3f);
     e1.put("fieldTwo",null);
     e1.put("fieldThree",-0.4f);
@@ -165,7 +165,7 @@ public class SolrPluginUtilsTest extends SolrTestCaseJ4 {
                                "  fieldTwo fieldThree^-0.4   ",
                                " "}));
 
-    Map<String,Float> e2 = new HashMap<String,Float>();
+    Map<String,Float> e2 = new HashMap<>();
     assertEquals("empty e2", e2, SolrPluginUtils.parseFieldBoosts
                  (""));
     assertEquals("spacey e2", e2, SolrPluginUtils.parseFieldBoosts
diff --git solr/core/src/test/org/apache/solr/util/TestFastWriter.java solr/core/src/test/org/apache/solr/util/TestFastWriter.java
index 47dd8f7..b503cbc 100644
--- solr/core/src/test/org/apache/solr/util/TestFastWriter.java
+++ solr/core/src/test/org/apache/solr/util/TestFastWriter.java
@@ -28,7 +28,7 @@ import java.util.Random;
 
 
 class MemWriter extends FastWriter {
-  public List<char[]> buffers = new LinkedList<char[]>();
+  public List<char[]> buffers = new LinkedList<>();
 
   Random r;
   public MemWriter(char[] tempBuffer, Random r) {
diff --git solr/core/src/test/org/apache/solr/util/TestNumberUtils.java solr/core/src/test/org/apache/solr/util/TestNumberUtils.java
index 61e2bfa..8e5f3f3 100644
--- solr/core/src/test/org/apache/solr/util/TestNumberUtils.java
+++ solr/core/src/test/org/apache/solr/util/TestNumberUtils.java
@@ -126,7 +126,7 @@ public class TestNumberUtils extends LuceneTestCase {
     int iter=1000;
 
     // INTEGERS
-    List<Converter> converters = new ArrayList<Converter>();
+    List<Converter> converters = new ArrayList<>();
     converters.add( new Int2Int() );
     converters.add( new SortInt() );
     converters.add( new Base10kS() );
diff --git solr/core/src/test/org/apache/solr/util/TestUtils.java solr/core/src/test/org/apache/solr/util/TestUtils.java
index 4c06f0b..d057fc9 100644
--- solr/core/src/test/org/apache/solr/util/TestUtils.java
+++ solr/core/src/test/org/apache/solr/util/TestUtils.java
@@ -74,20 +74,20 @@ public class TestUtils extends LuceneTestCase {
 
   public void testNamedLists()
   {
-    SimpleOrderedMap<Integer> map = new SimpleOrderedMap<Integer>();
+    SimpleOrderedMap<Integer> map = new SimpleOrderedMap<>();
     map.add( "test", 10 );
     SimpleOrderedMap<Integer> clone = map.clone();
     assertEquals( map.toString(), clone.toString() );
     assertEquals( new Integer(10), clone.get( "test" ) );
   
-    Map<String,Integer> realMap = new HashMap<String, Integer>();
+    Map<String,Integer> realMap = new HashMap<>();
     realMap.put( "one", 1 );
     realMap.put( "two", 2 );
     realMap.put( "three", 3 );
-    map = new SimpleOrderedMap<Integer>();
+    map = new SimpleOrderedMap<>();
     map.addAll( realMap );
     assertEquals( 3, map.size() );
-    map = new SimpleOrderedMap<Integer>();
+    map = new SimpleOrderedMap<>();
     map.add( "one", 1 );
     map.add( "two", 2 );
     map.add( "three", 3 );
@@ -101,7 +101,7 @@ public class TestUtils extends LuceneTestCase {
     assertEquals( 4, map.indexOf( null, 1 ) );
     assertEquals( null, map.get( null, 1 ) );
 
-    map = new SimpleOrderedMap<Integer>();
+    map = new SimpleOrderedMap<>();
     map.add( "one", 1 );
     map.add( "two", 2 );
     Iterator<Map.Entry<String, Integer>> iter = map.iterator();
diff --git solr/core/src/test/org/apache/solr/util/TimeZoneUtilsTest.java solr/core/src/test/org/apache/solr/util/TimeZoneUtilsTest.java
index e4f695f..727accb 100644
--- solr/core/src/test/org/apache/solr/util/TimeZoneUtilsTest.java
+++ solr/core/src/test/org/apache/solr/util/TimeZoneUtilsTest.java
@@ -46,7 +46,7 @@ public class TimeZoneUtilsTest extends LuceneTestCase {
 
   public void testValidIds() throws Exception {
 
-    final Set<String> idsTested = new HashSet<String>();
+    final Set<String> idsTested = new HashSet<>();
 
     // brain dead: anything the JVM supports, should work
     for (String validId : TimeZone.getAvailableIDs()) {
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/SolrQuery.java solr/solrj/src/java/org/apache/solr/client/solrj/SolrQuery.java
index e53dac9..9e09245 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/SolrQuery.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/SolrQuery.java
@@ -620,7 +620,7 @@ public class SolrQuery extends ModifiableSolrParams
    * @since 4.2
    */
   public SolrQuery setSorts(List<SortClause> value) {
-    sortClauses = new ArrayList<SortClause>(value);
+    sortClauses = new ArrayList<>(value);
     serializeSorts();
     return this;
   }
@@ -674,7 +674,7 @@ public class SolrQuery extends ModifiableSolrParams
    * @since 4.2
    */
   public SolrQuery addSort(SortClause sortClause) {
-    if (sortClauses == null) sortClauses = new ArrayList<SortClause>();
+    if (sortClauses == null) sortClauses = new ArrayList<>();
     sortClauses.add(sortClause);
     serializeSorts();
     return this;
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/SolrServer.java solr/solrj/src/java/org/apache/solr/client/solrj/SolrServer.java
index dcb0c83..9da6e71 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/SolrServer.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/SolrServer.java
@@ -86,7 +86,7 @@ public abstract class SolrServer implements Serializable
    */
   public UpdateResponse addBeans(Collection<?> beans, int commitWithinMs) throws SolrServerException, IOException {
     DocumentObjectBinder binder = this.getBinder();
-    ArrayList<SolrInputDocument> docs =  new ArrayList<SolrInputDocument>(beans.size());
+    ArrayList<SolrInputDocument> docs =  new ArrayList<>(beans.size());
     for (Object bean : beans) {
       docs.add(binder.toSolrInputDocument(bean));
     }
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/beans/DocumentObjectBinder.java solr/solrj/src/java/org/apache/solr/client/solrj/beans/DocumentObjectBinder.java
index b772ea0..877c1c3 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/beans/DocumentObjectBinder.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/beans/DocumentObjectBinder.java
@@ -34,14 +34,14 @@ import java.nio.ByteBuffer;
  */
 public class DocumentObjectBinder {
   
-  private final Map<Class, List<DocField>> infocache = new ConcurrentHashMap<Class, List<DocField>>();
+  private final Map<Class, List<DocField>> infocache = new ConcurrentHashMap<>();
 
   public DocumentObjectBinder() {
   }
 
   public <T> List<T> getBeans(Class<T> clazz, SolrDocumentList solrDocList) {
     List<DocField> fields = getDocFields(clazz);
-    List<T> result = new ArrayList<T>(solrDocList.size());
+    List<T> result = new ArrayList<>(solrDocList.size());
 
     for (SolrDocument sdoc : solrDocList) {
       result.add(getBean(clazz, fields, sdoc));
@@ -103,9 +103,9 @@ public class DocumentObjectBinder {
   }
 
   private List<DocField> collectInfo(Class clazz) {
-    List<DocField> fields = new ArrayList<DocField>();
+    List<DocField> fields = new ArrayList<>();
     Class superClazz = clazz;
-    List<AccessibleObject> members = new ArrayList<AccessibleObject>();
+    List<AccessibleObject> members = new ArrayList<>();
 
     while (superClazz != null && superClazz != Object.class) {
       members.addAll(Arrays.asList(superClazz.getDeclaredFields()));
@@ -275,7 +275,7 @@ public class DocumentObjectBinder {
       Map<String, Object> allValuesMap = null;
       List allValuesList = null;
       if (isContainedInMap) {
-        allValuesMap = new HashMap<String, Object>();
+        allValuesMap = new HashMap<>();
       } else {
         allValuesList = new ArrayList();
       }
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/impl/BinaryRequestWriter.java solr/solrj/src/java/org/apache/solr/client/solrj/impl/BinaryRequestWriter.java
index 5d2f81e..67274c2 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/impl/BinaryRequestWriter.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/impl/BinaryRequestWriter.java
@@ -46,7 +46,7 @@ public class BinaryRequestWriter extends RequestWriter {
               && (updateRequest.getDocIterator() == null) ) {
         return null;
       }
-      List<ContentStream> l = new ArrayList<ContentStream>();
+      List<ContentStream> l = new ArrayList<>();
       l.add(new LazyContentStream(updateRequest));
       return l;
     } else {
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/impl/CloudSolrServer.java solr/solrj/src/java/org/apache/solr/client/solrj/impl/CloudSolrServer.java
index 5a16220..62c3e37 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/impl/CloudSolrServer.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/impl/CloudSolrServer.java
@@ -97,7 +97,7 @@ public class CloudSolrServer extends SolrServer {
   private String idField = "id";
   private final Set<String> NON_ROUTABLE_PARAMS;
   {
-    NON_ROUTABLE_PARAMS = new HashSet<String>();
+    NON_ROUTABLE_PARAMS = new HashSet<>();
     NON_ROUTABLE_PARAMS.add(UpdateParams.EXPUNGE_DELETES);
     NON_ROUTABLE_PARAMS.add(UpdateParams.MAX_OPTIMIZE_SEGMENTS);
     NON_ROUTABLE_PARAMS.add(UpdateParams.COMMIT);
@@ -322,7 +322,7 @@ public class CloudSolrServer extends SolrServer {
     long start = System.nanoTime();
 
     if (parallelUpdates) {
-      final Map<String, Future<NamedList<?>>> responseFutures = new HashMap<String, Future<NamedList<?>>>(routes.size());
+      final Map<String, Future<NamedList<?>>> responseFutures = new HashMap<>(routes.size());
       for (final Map.Entry<String, LBHttpSolrServer.Req> entry : routes.entrySet()) {
         final String url = entry.getKey();
         final LBHttpSolrServer.Req lbRequest = entry.getValue();
@@ -373,7 +373,7 @@ public class CloudSolrServer extends SolrServer {
     
     Set<String> paramNames = nonRoutableParams.getParameterNames();
     
-    Set<String> intersection = new HashSet<String>(paramNames);
+    Set<String> intersection = new HashSet<>(paramNames);
     intersection.retainAll(NON_ROUTABLE_PARAMS);
     
     if (nonRoutableRequest != null || intersection.size() > 0) {
@@ -381,7 +381,7 @@ public class CloudSolrServer extends SolrServer {
         nonRoutableRequest = new UpdateRequest();
       }
       nonRoutableRequest.setParams(nonRoutableParams);
-      List<String> urlList = new ArrayList<String>();
+      List<String> urlList = new ArrayList<>();
       urlList.addAll(routes.keySet());
       Collections.shuffle(urlList, rand);
       LBHttpSolrServer.Req req = new LBHttpSolrServer.Req(nonRoutableRequest, urlList);
@@ -402,13 +402,13 @@ public class CloudSolrServer extends SolrServer {
   }
 
   private Map<String,List<String>> buildUrlMap(DocCollection col) {
-    Map<String, List<String>> urlMap = new HashMap<String, List<String>>();
+    Map<String, List<String>> urlMap = new HashMap<>();
     Collection<Slice> slices = col.getActiveSlices();
     Iterator<Slice> sliceIterator = slices.iterator();
     while (sliceIterator.hasNext()) {
       Slice slice = sliceIterator.next();
       String name = slice.getName();
-      List<String> urls = new ArrayList<String>();
+      List<String> urls = new ArrayList<>();
       Replica leader = slice.getLeader();
       if (leader == null) {
         // take unoptimized general path - we cannot find a leader yet
@@ -514,14 +514,14 @@ public class CloudSolrServer extends SolrServer {
         }
       }
       sendToLeaders = true;
-      replicas = new ArrayList<String>();
+      replicas = new ArrayList<>();
     }
     
     SolrParams reqParams = request.getParams();
     if (reqParams == null) {
       reqParams = new ModifiableSolrParams();
     }
-    List<String> theUrlList = new ArrayList<String>();
+    List<String> theUrlList = new ArrayList<>();
     if (request.getPath().equals("/admin/collections")
         || request.getPath().equals("/admin/cores")) {
       Set<String> liveNodes = clusterState.getLiveNodes();
@@ -549,7 +549,7 @@ public class CloudSolrServer extends SolrServer {
       // Retrieve slices from the cloud state and, for each collection
       // specified,
       // add it to the Map of slices.
-      Map<String,Slice> slices = new HashMap<String,Slice>();
+      Map<String,Slice> slices = new HashMap<>();
       for (String collectionName : collectionsList) {
         Collection<Slice> colSlices = clusterState
             .getActiveSlices(collectionName);
@@ -567,8 +567,8 @@ public class CloudSolrServer extends SolrServer {
       
       // build a map of unique nodes
       // TODO: allow filtering by group, role, etc
-      Map<String,ZkNodeProps> nodes = new HashMap<String,ZkNodeProps>();
-      List<String> urlList2 = new ArrayList<String>();
+      Map<String,ZkNodeProps> nodes = new HashMap<>();
+      List<String> urlList2 = new ArrayList<>();
       for (Slice slice : slices.values()) {
         for (ZkNodeProps nodeProps : slice.getReplicasMap().values()) {
           ZkCoreNodeProps coreNodeProps = new ZkCoreNodeProps(nodeProps);
@@ -609,15 +609,15 @@ public class CloudSolrServer extends SolrServer {
       }
       
       if (sendToLeaders) {
-        theUrlList = new ArrayList<String>(leaderUrlList.size());
+        theUrlList = new ArrayList<>(leaderUrlList.size());
         theUrlList.addAll(leaderUrlList);
       } else {
-        theUrlList = new ArrayList<String>(urlList.size());
+        theUrlList = new ArrayList<>(urlList.size());
         theUrlList.addAll(urlList);
       }
       Collections.shuffle(theUrlList, rand);
       if (sendToLeaders) {
-        ArrayList<String> theReplicas = new ArrayList<String>(
+        ArrayList<String> theReplicas = new ArrayList<>(
             replicasList.size());
         theReplicas.addAll(replicasList);
         Collections.shuffle(theReplicas, rand);
@@ -640,7 +640,7 @@ public class CloudSolrServer extends SolrServer {
       String collection) {
     // Extract each comma separated collection name and store in a List.
     List<String> rawCollectionsList = StrUtils.splitSmart(collection, ",", true);
-    Set<String> collectionsList = new HashSet<String>();
+    Set<String> collectionsList = new HashSet<>();
     // validate collections
     for (String collectionName : rawCollectionsList) {
       if (!clusterState.getCollections().contains(collectionName)) {
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/impl/ConcurrentUpdateSolrServer.java solr/solrj/src/java/org/apache/solr/client/solrj/impl/ConcurrentUpdateSolrServer.java
index 6e1a65b..dadc235 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/impl/ConcurrentUpdateSolrServer.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/impl/ConcurrentUpdateSolrServer.java
@@ -121,9 +121,9 @@ public class ConcurrentUpdateSolrServer extends SolrServer {
       HttpClient client, int queueSize, int threadCount, ExecutorService es, boolean streamDeletes) {
     this.server = new HttpSolrServer(solrServerUrl, client);
     this.server.setFollowRedirects(false);
-    queue = new LinkedBlockingQueue<UpdateRequest>(queueSize);
+    queue = new LinkedBlockingQueue<>(queueSize);
     this.threadCount = threadCount;
-    runners = new LinkedList<Runner>();
+    runners = new LinkedList<>();
     scheduler = es;
     this.streamDeletes = streamDeletes;
   }
@@ -364,7 +364,7 @@ public class ConcurrentUpdateSolrServer extends SolrServer {
     }
 
     // RETURN A DUMMY result
-    NamedList<Object> dummy = new NamedList<Object>();
+    NamedList<Object> dummy = new NamedList<>();
     dummy.add("NOTE", "the request is processed in a background stream");
     return dummy;
   }
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/impl/HttpSolrServer.java solr/solrj/src/java/org/apache/solr/client/solrj/impl/HttpSolrServer.java
index 7ba878a..e026d9a 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/impl/HttpSolrServer.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/impl/HttpSolrServer.java
@@ -269,7 +269,7 @@ public class HttpSolrServer extends SolrServer {
               }
             }
             
-            LinkedList<NameValuePair> postParams = new LinkedList<NameValuePair>();
+            LinkedList<NameValuePair> postParams = new LinkedList<>();
             if (streams == null || isMultipart) {
               HttpPost post = new HttpPost(url + ClientUtils.toQueryString( queryParams, false ));
               post.setHeader("Content-Charset", "UTF-8");
@@ -278,7 +278,7 @@ public class HttpSolrServer extends SolrServer {
                     "application/x-www-form-urlencoded; charset=UTF-8");
               }
 
-              List<FormBodyPart> parts = new LinkedList<FormBodyPart>();
+              List<FormBodyPart> parts = new LinkedList<>();
               Iterator<String> iter = wparams.getParameterNamesIterator();
               while (iter.hasNext()) {
                 String p = iter.next();
@@ -436,7 +436,7 @@ public class HttpSolrServer extends SolrServer {
       if (processor == null) {
         
         // no processor specified, return raw stream
-        NamedList<Object> rsp = new NamedList<Object>();
+        NamedList<Object> rsp = new NamedList<>();
         rsp.add("stream", respBody);
         // Only case where stream should not be closed
         shouldClose = false;
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/impl/LBHttpSolrServer.java solr/solrj/src/java/org/apache/solr/client/solrj/impl/LBHttpSolrServer.java
index 3687c97..a1ebefe 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/impl/LBHttpSolrServer.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/impl/LBHttpSolrServer.java
@@ -76,7 +76,7 @@ import java.util.*;
  * @since solr 1.4
  */
 public class LBHttpSolrServer extends SolrServer {
-  private static Set<Integer> RETRY_CODES = new HashSet<Integer>(4);
+  private static Set<Integer> RETRY_CODES = new HashSet<>(4);
 
   static {
     RETRY_CODES.add(404);
@@ -87,10 +87,10 @@ public class LBHttpSolrServer extends SolrServer {
 
   // keys to the maps are currently of the form "http://localhost:8983/solr"
   // which should be equivalent to CommonsHttpSolrServer.getBaseURL()
-  private final Map<String, ServerWrapper> aliveServers = new LinkedHashMap<String, ServerWrapper>();
+  private final Map<String, ServerWrapper> aliveServers = new LinkedHashMap<>();
   // access to aliveServers should be synchronized on itself
   
-  protected final Map<String, ServerWrapper> zombieServers = new ConcurrentHashMap<String, ServerWrapper>();
+  protected final Map<String, ServerWrapper> zombieServers = new ConcurrentHashMap<>();
 
   // changes to aliveServers are reflected in this array, no need to synchronize
   private volatile ServerWrapper[] aliveServerList = new ServerWrapper[0];
@@ -283,7 +283,7 @@ public class LBHttpSolrServer extends SolrServer {
     Rsp rsp = new Rsp();
     Exception ex = null;
     boolean isUpdate = req.request instanceof IsUpdateRequest;
-    List<ServerWrapper> skipped = new ArrayList<ServerWrapper>(req.getNumDeadServersToTry());
+    List<ServerWrapper> skipped = new ArrayList<>(req.getNumDeadServersToTry());
 
     for (String serverStr : req.getServers()) {
       serverStr = normalize(serverStr);
@@ -505,7 +505,7 @@ public class LBHttpSolrServer extends SolrServer {
         if (e.getRootCause() instanceof IOException) {
           ex = e;
           moveAliveToDead(wrapper);
-          if (justFailed == null) justFailed = new HashMap<String,ServerWrapper>();
+          if (justFailed == null) justFailed = new HashMap<>();
           justFailed.put(wrapper.getKey(), wrapper);
         } else {
           throw e;
@@ -619,7 +619,7 @@ public class LBHttpSolrServer extends SolrServer {
           aliveCheckExecutor = Executors.newSingleThreadScheduledExecutor(
               new SolrjNamedThreadFactory("aliveCheckExecutor"));
           aliveCheckExecutor.scheduleAtFixedRate(
-                  getAliveCheckRunner(new WeakReference<LBHttpSolrServer>(this)),
+                  getAliveCheckRunner(new WeakReference<>(this)),
                   this.interval, this.interval, TimeUnit.MILLISECONDS);
         }
       }
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/impl/XMLResponseParser.java solr/solrj/src/java/org/apache/solr/client/solrj/impl/XMLResponseParser.java
index 491caaf..ad5a548 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/impl/XMLResponseParser.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/impl/XMLResponseParser.java
@@ -135,7 +135,7 @@ public class XMLResponseParser extends ResponseParser
               response = readNamedList( parser );
             }
             else if( name.equals( "solr" ) ) {
-              return new SimpleOrderedMap<Object>();
+              return new SimpleOrderedMap<>();
             }
             else {
               throw new Exception( "really needs to be response or result.  " +
@@ -212,7 +212,7 @@ public class XMLResponseParser extends ResponseParser
     }
 
     StringBuilder builder = new StringBuilder();
-    NamedList<Object> nl = new SimpleOrderedMap<Object>();
+    NamedList<Object> nl = new SimpleOrderedMap<>();
     KnownType type = null;
     String name = null;
     
@@ -284,7 +284,7 @@ public class XMLResponseParser extends ResponseParser
     StringBuilder builder = new StringBuilder();
     KnownType type = null;
 
-    List<Object> vals = new ArrayList<Object>();
+    List<Object> vals = new ArrayList<>();
 
     int depth = 0;
     while( true ) 
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/request/ContentStreamUpdateRequest.java solr/solrj/src/java/org/apache/solr/client/solrj/request/ContentStreamUpdateRequest.java
index c9d464d..6b02486 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/request/ContentStreamUpdateRequest.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/request/ContentStreamUpdateRequest.java
@@ -45,7 +45,7 @@ public class ContentStreamUpdateRequest extends AbstractUpdateRequest {
    */
   public ContentStreamUpdateRequest(String url) {
     super(METHOD.POST, url);
-    contentStreams = new ArrayList<ContentStream>();
+    contentStreams = new ArrayList<>();
   }
 
   @Override
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/request/DocumentAnalysisRequest.java solr/solrj/src/java/org/apache/solr/client/solrj/request/DocumentAnalysisRequest.java
index 1e41ca4..fa84111 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/request/DocumentAnalysisRequest.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/request/DocumentAnalysisRequest.java
@@ -42,7 +42,7 @@ import java.util.concurrent.TimeUnit;
  */
 public class DocumentAnalysisRequest extends SolrRequest {
 
-  private List<SolrInputDocument> documents = new ArrayList<SolrInputDocument>();
+  private List<SolrInputDocument> documents = new ArrayList<>();
   private String query;
   private boolean showMatch = false;
 
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/request/FieldAnalysisRequest.java solr/solrj/src/java/org/apache/solr/client/solrj/request/FieldAnalysisRequest.java
index e5821b7..47f340d 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/request/FieldAnalysisRequest.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/request/FieldAnalysisRequest.java
@@ -212,7 +212,7 @@ public class FieldAnalysisRequest extends SolrRequest {
    */
   public FieldAnalysisRequest addFieldName(String fieldName) {
     if (fieldNames == null) {
-      fieldNames = new LinkedList<String>();
+      fieldNames = new LinkedList<>();
     }
     fieldNames.add(fieldName);
     return this;
@@ -249,7 +249,7 @@ public class FieldAnalysisRequest extends SolrRequest {
    */
   public FieldAnalysisRequest addFieldType(String fieldTypeName) {
     if (fieldTypes == null) {
-      fieldTypes = new LinkedList<String>();
+      fieldTypes = new LinkedList<>();
     }
     fieldTypes.add(fieldTypeName);
     return this;
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/request/LukeRequest.java solr/solrj/src/java/org/apache/solr/client/solrj/request/LukeRequest.java
index 052c189..8cd5305 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/request/LukeRequest.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/request/LukeRequest.java
@@ -59,7 +59,7 @@ public class LukeRequest extends SolrRequest
   public void addField( String f )
   {
     if( fields == null ) {
-      fields = new ArrayList<String>();
+      fields = new ArrayList<>();
     }
     fields.add( f );
   }
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/request/RequestWriter.java solr/solrj/src/java/org/apache/solr/client/solrj/request/RequestWriter.java
index 8e901ea..2990653 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/request/RequestWriter.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/request/RequestWriter.java
@@ -44,7 +44,7 @@ public class RequestWriter {
     if (req instanceof UpdateRequest) {
       UpdateRequest updateRequest = (UpdateRequest) req;
       if (isEmpty(updateRequest)) return null;
-      List<ContentStream> l = new ArrayList<ContentStream>();
+      List<ContentStream> l = new ArrayList<>();
       l.add(new LazyContentStream(updateRequest));
       return l;
     }
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/request/UpdateRequest.java solr/solrj/src/java/org/apache/solr/client/solrj/request/UpdateRequest.java
index 154d7e0..962d247 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/request/UpdateRequest.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/request/UpdateRequest.java
@@ -86,7 +86,7 @@ public class UpdateRequest extends AbstractUpdateRequest {
   
   public UpdateRequest add(final SolrInputDocument doc) {
     if (documents == null) {
-      documents = new LinkedHashMap<SolrInputDocument,Map<String,Object>>();
+      documents = new LinkedHashMap<>();
     }
     documents.put(doc, null);
     return this;
@@ -103,9 +103,9 @@ public class UpdateRequest extends AbstractUpdateRequest {
   public UpdateRequest add(final SolrInputDocument doc, Integer commitWithin,
       Boolean overwrite) {
     if (documents == null) {
-      documents = new LinkedHashMap<SolrInputDocument,Map<String,Object>>();
+      documents = new LinkedHashMap<>();
     }
-    Map<String,Object> params = new HashMap<String,Object>(2);
+    Map<String,Object> params = new HashMap<>(2);
     if (commitWithin != null) params.put(COMMIT_WITHIN, commitWithin);
     if (overwrite != null) params.put(OVERWRITE, overwrite);
     
@@ -116,7 +116,7 @@ public class UpdateRequest extends AbstractUpdateRequest {
   
   public UpdateRequest add(final Collection<SolrInputDocument> docs) {
     if (documents == null) {
-      documents = new LinkedHashMap<SolrInputDocument,Map<String,Object>>();
+      documents = new LinkedHashMap<>();
     }
     for (SolrInputDocument doc : docs) {
       documents.put(doc, null);
@@ -126,7 +126,7 @@ public class UpdateRequest extends AbstractUpdateRequest {
   
   public UpdateRequest deleteById(String id) {
     if (deleteById == null) {
-      deleteById = new LinkedHashMap<String,Map<String,Object>>();
+      deleteById = new LinkedHashMap<>();
     }
     deleteById.put(id, null);
     return this;
@@ -134,7 +134,7 @@ public class UpdateRequest extends AbstractUpdateRequest {
   
   public UpdateRequest deleteById(List<String> ids) {
     if (deleteById == null) {
-      deleteById = new LinkedHashMap<String,Map<String,Object>>();
+      deleteById = new LinkedHashMap<>();
     }
     
     for (String id : ids) {
@@ -146,9 +146,9 @@ public class UpdateRequest extends AbstractUpdateRequest {
   
   public UpdateRequest deleteById(String id, Long version) {
     if (deleteById == null) {
-      deleteById = new LinkedHashMap<String,Map<String,Object>>();
+      deleteById = new LinkedHashMap<>();
     }
-    Map<String,Object> params = new HashMap<String,Object>(1);
+    Map<String,Object> params = new HashMap<>(1);
     params.put(VER, version);
     deleteById.put(id, params);
     return this;
@@ -156,7 +156,7 @@ public class UpdateRequest extends AbstractUpdateRequest {
   
   public UpdateRequest deleteByQuery(String q) {
     if (deleteQuery == null) {
-      deleteQuery = new ArrayList<String>();
+      deleteQuery = new ArrayList<>();
     }
     deleteQuery.add(q);
     return this;
@@ -179,7 +179,7 @@ public class UpdateRequest extends AbstractUpdateRequest {
       return null;
     }
     
-    Map<String,LBHttpSolrServer.Req> routes = new HashMap<String,LBHttpSolrServer.Req>();
+    Map<String,LBHttpSolrServer.Req> routes = new HashMap<>();
     if (documents != null) {
       Set<Entry<SolrInputDocument,Map<String,Object>>> entries = documents.entrySet();
       for (Entry<SolrInputDocument,Map<String,Object>> entry : entries) {
@@ -278,7 +278,7 @@ public class UpdateRequest extends AbstractUpdateRequest {
   }
   
   private List<Map<SolrInputDocument,Map<String,Object>>> getDocLists(Map<SolrInputDocument,Map<String,Object>> documents) {
-    List<Map<SolrInputDocument,Map<String,Object>>> docLists = new ArrayList<Map<SolrInputDocument,Map<String,Object>>>();
+    List<Map<SolrInputDocument,Map<String,Object>>> docLists = new ArrayList<>();
     Map<SolrInputDocument,Map<String,Object>> docList = null;
     if (this.documents != null) {
       
@@ -297,7 +297,7 @@ public class UpdateRequest extends AbstractUpdateRequest {
         }
         if (overwrite != lastOverwrite || commitWithin != lastCommitWithin
             || docLists.size() == 0) {
-          docList = new LinkedHashMap<SolrInputDocument,Map<String,Object>>();
+          docList = new LinkedHashMap<>();
           docLists.add(docList);
         }
         docList.put(entry.getKey(), entry.getValue());
@@ -307,7 +307,7 @@ public class UpdateRequest extends AbstractUpdateRequest {
     }
     
     if (docIterator != null) {
-      docList = new LinkedHashMap<SolrInputDocument,Map<String,Object>>();
+      docList = new LinkedHashMap<>();
       docLists.add(docList);
       while (docIterator.hasNext()) {
         SolrInputDocument doc = docIterator.next();
@@ -404,7 +404,7 @@ public class UpdateRequest extends AbstractUpdateRequest {
   
   public List<SolrInputDocument> getDocuments() {
     if (documents == null) return null;
-    List<SolrInputDocument> docs = new ArrayList<SolrInputDocument>(documents.size());
+    List<SolrInputDocument> docs = new ArrayList<>(documents.size());
     docs.addAll(documents.keySet());
     return docs;
   }
@@ -419,7 +419,7 @@ public class UpdateRequest extends AbstractUpdateRequest {
   
   public List<String> getDeleteById() {
     if (deleteById == null) return null;
-    List<String> deletes = new ArrayList<String>(deleteById.keySet());
+    List<String> deletes = new ArrayList<>(deleteById.keySet());
     return deletes;
   }
   
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/response/AnalysisResponseBase.java solr/solrj/src/java/org/apache/solr/client/solrj/response/AnalysisResponseBase.java
index 42732a2..1e399f3 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/response/AnalysisResponseBase.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/response/AnalysisResponseBase.java
@@ -63,7 +63,7 @@ public class AnalysisResponseBase extends SolrResponseBase {
    * @return The built analysis phases list.
    */
   protected List<AnalysisPhase> buildPhases(NamedList<List<NamedList<Object>>> phaseNL) {
-    List<AnalysisPhase> phases = new ArrayList<AnalysisPhase>(phaseNL.size());
+    List<AnalysisPhase> phases = new ArrayList<>(phaseNL.size());
     for (Map.Entry<String, List<NamedList<Object>>> phaseEntry : phaseNL) {
       AnalysisPhase phase = new AnalysisPhase(phaseEntry.getKey());
       List<NamedList<Object>> tokens = phaseEntry.getValue();
@@ -116,7 +116,7 @@ public class AnalysisResponseBase extends SolrResponseBase {
   public static class AnalysisPhase {
 
     private final String className;
-    private List<TokenInfo> tokens = new ArrayList<TokenInfo>();
+    private List<TokenInfo> tokens = new ArrayList<>();
 
     AnalysisPhase(String className) {
       this.className = className;
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/response/CollectionAdminResponse.java solr/solrj/src/java/org/apache/solr/client/solrj/response/CollectionAdminResponse.java
index 3a6b643..8acf2e2 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/response/CollectionAdminResponse.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/response/CollectionAdminResponse.java
@@ -46,7 +46,7 @@ public class CollectionAdminResponse extends SolrResponseBase
   @SuppressWarnings("unchecked")
   public Map<String, NamedList<Integer>> getCollectionCoresStatus()
   {
-    Map<String, NamedList<Integer>> res = new HashMap<String, NamedList<Integer>>();
+    Map<String, NamedList<Integer>> res = new HashMap<>();
     NamedList<NamedList<Object>> cols = getCollectionStatus();
     if( cols != null ) {
       for (Map.Entry<String, NamedList<Object>> e : cols) {
@@ -64,7 +64,7 @@ public class CollectionAdminResponse extends SolrResponseBase
   @SuppressWarnings("unchecked")
   public Map<String, NamedList<Integer>> getCollectionNodesStatus()
   {
-    Map<String, NamedList<Integer>> res = new HashMap<String, NamedList<Integer>>();
+    Map<String, NamedList<Integer>> res = new HashMap<>();
     NamedList<NamedList<Object>> cols = getCollectionStatus();
     if( cols != null ) {
       for (Map.Entry<String,NamedList<Object>> e : cols) {
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/response/DocumentAnalysisResponse.java solr/solrj/src/java/org/apache/solr/client/solrj/response/DocumentAnalysisResponse.java
index 3ec590a..2f11d78 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/response/DocumentAnalysisResponse.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/response/DocumentAnalysisResponse.java
@@ -33,7 +33,7 @@ import java.util.Map;
  */
 public class DocumentAnalysisResponse extends AnalysisResponseBase implements Iterable<Map.Entry<String, DocumentAnalysisResponse.DocumentAnalysis>> {
 
-  private final Map<String, DocumentAnalysis> documentAnalysisByKey = new HashMap<String, DocumentAnalysis>();
+  private final Map<String, DocumentAnalysis> documentAnalysisByKey = new HashMap<>();
 
   /**
    * {@inheritDoc}
@@ -116,7 +116,7 @@ public class DocumentAnalysisResponse extends AnalysisResponseBase implements It
   public static class DocumentAnalysis implements Iterable<Map.Entry<String, FieldAnalysis>> {
 
     private final String documentKey;
-    private Map<String, FieldAnalysis> fieldAnalysisByFieldName = new HashMap<String, FieldAnalysis>();
+    private Map<String, FieldAnalysis> fieldAnalysisByFieldName = new HashMap<>();
 
     private DocumentAnalysis(String documentKey) {
       this.documentKey = documentKey;
@@ -168,7 +168,7 @@ public class DocumentAnalysisResponse extends AnalysisResponseBase implements It
 
     private final String fieldName;
     private List<AnalysisPhase> queryPhases;
-    private Map<String, List<AnalysisPhase>> indexPhasesByFieldValue = new HashMap<String, List<AnalysisPhase>>();
+    private Map<String, List<AnalysisPhase>> indexPhasesByFieldValue = new HashMap<>();
 
     private FieldAnalysis(String fieldName) {
       this.fieldName = fieldName;
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/response/FacetField.java solr/solrj/src/java/org/apache/solr/client/solrj/response/FacetField.java
index 6592061..086c999 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/response/FacetField.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/response/FacetField.java
@@ -125,7 +125,7 @@ import org.apache.solr.client.solrj.util.ClientUtils;
    public void add( String name, long cnt )
    {
      if( _values == null ) {
-       _values = new ArrayList<Count>( 30 );
+       _values = new ArrayList<>( 30 );
      }
      _values.add( new Count( this, name, cnt ) );
    }
@@ -136,7 +136,7 @@ import org.apache.solr.client.solrj.util.ClientUtils;
    public void insert( String name, long cnt )
    {
      if( _values == null ) {
-       _values = new ArrayList<Count>( 30 );
+       _values = new ArrayList<>( 30 );
      }
      _values.add( 0, new Count( this, name, cnt ) );
    }
@@ -158,7 +158,7 @@ import org.apache.solr.client.solrj.util.ClientUtils;
    {
      FacetField ff = new FacetField( _name );
      if( _values != null ) {
-       ff._values = new ArrayList<Count>( _values.size() );
+       ff._values = new ArrayList<>( _values.size() );
        for( Count c : _values ) {
          if( c._count < max ) { // !equal to
            ff._values.add( c );
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/response/FieldAnalysisResponse.java solr/solrj/src/java/org/apache/solr/client/solrj/response/FieldAnalysisResponse.java
index d6d2092..9c542d7 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/response/FieldAnalysisResponse.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/response/FieldAnalysisResponse.java
@@ -32,8 +32,8 @@ import java.util.Map;
  */
 public class FieldAnalysisResponse extends AnalysisResponseBase {
 
-  private Map<String, Analysis> analysisByFieldTypeName = new HashMap<String, Analysis>();
-  private Map<String, Analysis> analysisByFieldName = new HashMap<String, Analysis>();
+  private Map<String, Analysis> analysisByFieldTypeName = new HashMap<>();
+  private Map<String, Analysis> analysisByFieldName = new HashMap<>();
 
   /**
    * {@inheritDoc}
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/response/FieldStatsInfo.java solr/solrj/src/java/org/apache/solr/client/solrj/response/FieldStatsInfo.java
index b56938b..3c17848 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/response/FieldStatsInfo.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/response/FieldStatsInfo.java
@@ -85,9 +85,9 @@ public class FieldStatsInfo implements Serializable {
       else if( "facets".equals( entry.getKey() ) ) {
         @SuppressWarnings("unchecked")
         NamedList<Object> fields = (NamedList<Object>)entry.getValue();
-        facets = new HashMap<String, List<FieldStatsInfo>>();
+        facets = new HashMap<>();
         for( Map.Entry<String, Object> ev : fields ) {
-          List<FieldStatsInfo> vals = new ArrayList<FieldStatsInfo>();
+          List<FieldStatsInfo> vals = new ArrayList<>();
           facets.put( ev.getKey(), vals );
           @SuppressWarnings("unchecked")
           NamedList<NamedList<Object>> vnl = (NamedList<NamedList<Object>>) ev.getValue();
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/response/GroupCommand.java solr/solrj/src/java/org/apache/solr/client/solrj/response/GroupCommand.java
index 80c7726..c2c8127 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/response/GroupCommand.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/response/GroupCommand.java
@@ -45,7 +45,7 @@ import java.util.List;
 public class GroupCommand implements Serializable {
 
   private final String _name;
-  private final List<Group> _values = new ArrayList<Group>();
+  private final List<Group> _values = new ArrayList<>();
   private final int _matches;
   private final Integer _ngroups;
 
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/response/GroupResponse.java solr/solrj/src/java/org/apache/solr/client/solrj/response/GroupResponse.java
index 9ec5462..210bb5e 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/response/GroupResponse.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/response/GroupResponse.java
@@ -34,7 +34,7 @@ import java.util.List;
  */
 public class GroupResponse implements Serializable {
 
-  private final List<GroupCommand> _values = new ArrayList<GroupCommand>();
+  private final List<GroupCommand> _values = new ArrayList<>();
 
   /**
    * Adds a grouping command to the response.
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/response/LukeResponse.java solr/solrj/src/java/org/apache/solr/client/solrj/response/LukeResponse.java
index 2109980..5d2f328 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/response/LukeResponse.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/response/LukeResponse.java
@@ -91,7 +91,7 @@ public class LukeResponse extends SolrResponseBase {
         String key = entry.getKey();
         if ("fields".equals(key) && entry.getValue() != null) {
           List<String> theFields = (List<String>) entry.getValue();
-          fields = new ArrayList<String>(theFields);
+          fields = new ArrayList<>(theFields);
         } else if ("tokenized".equals(key) == true) {
           tokenized = Boolean.parseBoolean(entry.getValue().toString());
         } else if ("analyzer".equals(key) == true) {
@@ -202,7 +202,7 @@ public class LukeResponse extends SolrResponseBase {
       flds = (NamedList<Object>) schema.get("fields");
     }
     if (flds != null) {
-      fieldInfo = new HashMap<String, FieldInfo>();
+      fieldInfo = new HashMap<>();
       for (Map.Entry<String, Object> field : flds) {
         FieldInfo f = new FieldInfo(field.getKey());
         f.read((NamedList<Object>) field.getValue());
@@ -213,7 +213,7 @@ public class LukeResponse extends SolrResponseBase {
     if( schema != null ) {
       NamedList<Object> fldTypes = (NamedList<Object>) schema.get("types");
       if (fldTypes != null) {
-        fieldTypeInfo = new HashMap<String, FieldTypeInfo>();
+        fieldTypeInfo = new HashMap<>();
         for (Map.Entry<String, Object> fieldType : fldTypes) {
           FieldTypeInfo ft = new FieldTypeInfo(fieldType.getKey());
           ft.read((NamedList<Object>) fieldType.getValue());
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/response/QueryResponse.java solr/solrj/src/java/org/apache/solr/client/solrj/response/QueryResponse.java
index 0901147..6deedeb 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/response/QueryResponse.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/response/QueryResponse.java
@@ -158,7 +158,7 @@ public class QueryResponse extends SolrResponseBase
   
   private void extractStatsInfo(NamedList<Object> info) {
     if( info != null ) {
-      _fieldStatsInfo = new HashMap<String, FieldStatsInfo>();
+      _fieldStatsInfo = new HashMap<>();
       NamedList<NamedList<Object>> ff = (NamedList<NamedList<Object>>) info.get( "stats_fields" );
       if( ff != null ) {
         for( Map.Entry<String,NamedList<Object>> entry : ff ) {
@@ -174,13 +174,13 @@ public class QueryResponse extends SolrResponseBase
 
   private void extractDebugInfo( NamedList<Object> debug )
   {
-    _debugMap = new LinkedHashMap<String, Object>(); // keep the order
+    _debugMap = new LinkedHashMap<>(); // keep the order
     for( Map.Entry<String, Object> info : debug ) {
       _debugMap.put( info.getKey(), info.getValue() );
     }
 
     // Parse out interesting bits from the debug info
-    _explainMap = new HashMap<String, String>();
+    _explainMap = new HashMap<>();
     NamedList<String> explain = (NamedList<String>)_debugMap.get( "explain" );
     if( explain != null ) {
       for( Map.Entry<String, String> info : explain ) {
@@ -246,9 +246,9 @@ public class QueryResponse extends SolrResponseBase
 
   private void extractHighlightingInfo( NamedList<Object> info )
   {
-    _highlighting = new HashMap<String,Map<String,List<String>>>();
+    _highlighting = new HashMap<>();
     for( Map.Entry<String, Object> doc : info ) {
-      Map<String,List<String>> fieldMap = new HashMap<String, List<String>>();
+      Map<String,List<String>> fieldMap = new HashMap<>();
       _highlighting.put( doc.getKey(), fieldMap );
       
       NamedList<List<String>> fnl = (NamedList<List<String>>)doc.getValue();
@@ -261,7 +261,7 @@ public class QueryResponse extends SolrResponseBase
   private void extractFacetInfo( NamedList<Object> info )
   {
     // Parse the queries
-    _facetQuery = new LinkedHashMap<String, Integer>();
+    _facetQuery = new LinkedHashMap<>();
     NamedList<Integer> fq = (NamedList<Integer>) info.get( "facet_queries" );
     if (fq != null) {
       for( Map.Entry<String, Integer> entry : fq ) {
@@ -273,8 +273,8 @@ public class QueryResponse extends SolrResponseBase
     // TODO?? The list could be <int> or <long>?  If always <long> then we can switch to <Long>
     NamedList<NamedList<Number>> ff = (NamedList<NamedList<Number>>) info.get( "facet_fields" );
     if( ff != null ) {
-      _facetFields = new ArrayList<FacetField>( ff.size() );
-      _limitingFacets = new ArrayList<FacetField>( ff.size() );
+      _facetFields = new ArrayList<>( ff.size() );
+      _limitingFacets = new ArrayList<>( ff.size() );
       
       long minsize = _results == null ? Long.MAX_VALUE :_results.getNumFound();
       for( Map.Entry<String,NamedList<Number>> facet : ff ) {
@@ -295,7 +295,7 @@ public class QueryResponse extends SolrResponseBase
     NamedList<NamedList<Object>> df = (NamedList<NamedList<Object>>) info.get("facet_dates");
     if (df != null) {
       // System.out.println(df);
-      _facetDates = new ArrayList<FacetField>( df.size() );
+      _facetDates = new ArrayList<>( df.size() );
       for (Map.Entry<String, NamedList<Object>> facet : df) {
         // System.out.println("Key: " + facet.getKey() + " Value: " + facet.getValue());
         NamedList<Object> values = facet.getValue();
@@ -318,7 +318,7 @@ public class QueryResponse extends SolrResponseBase
     //Parse range facets
     NamedList<NamedList<Object>> rf = (NamedList<NamedList<Object>>) info.get("facet_ranges");
     if (rf != null) {
-      _facetRanges = new ArrayList<RangeFacet>( rf.size() );
+      _facetRanges = new ArrayList<>( rf.size() );
       for (Map.Entry<String, NamedList<Object>> facet : rf) {
         NamedList<Object> values = facet.getValue();
         Object rawGap = values.get("gap");
@@ -358,7 +358,7 @@ public class QueryResponse extends SolrResponseBase
     //Parse pivot facets
     NamedList pf = (NamedList) info.get("facet_pivot");
     if (pf != null) {
-      _facetPivot = new NamedList<List<PivotField>>();
+      _facetPivot = new NamedList<>();
       for( int i=0; i<pf.size(); i++ ) {
         _facetPivot.add( pf.getName(i), readPivots( (List<NamedList>)pf.getVal(i) ) );
       }
@@ -367,7 +367,7 @@ public class QueryResponse extends SolrResponseBase
   
   protected List<PivotField> readPivots( List<NamedList> list )
   {
-    ArrayList<PivotField> values = new ArrayList<PivotField>( list.size() );
+    ArrayList<PivotField> values = new ArrayList<>( list.size() );
     for( NamedList nl : list ) {
       // NOTE, this is cheating, but we know the order they are written in, so no need to check
       String f = (String)nl.getVal( 0 );
@@ -386,7 +386,7 @@ public class QueryResponse extends SolrResponseBase
    * Remove the field facet info
    */
   public void removeFacets() {
-    _facetFields = new ArrayList<FacetField>();
+    _facetFields = new ArrayList<>();
   }
   
   //------------------------------------------------------
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/response/RangeFacet.java solr/solrj/src/java/org/apache/solr/client/solrj/response/RangeFacet.java
index 520746e..22708ce 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/response/RangeFacet.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/response/RangeFacet.java
@@ -26,7 +26,7 @@ import java.util.List;
 public abstract class RangeFacet<B, G> {
 
   private final String name;
-  private final List<Count> counts = new ArrayList<Count>();
+  private final List<Count> counts = new ArrayList<>();
 
   private final B start;
   private final B end;
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/response/SpellCheckResponse.java solr/solrj/src/java/org/apache/solr/client/solrj/response/SpellCheckResponse.java
index a38fdb6..69e53a6 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/response/SpellCheckResponse.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/response/SpellCheckResponse.java
@@ -32,8 +32,8 @@ import java.util.Map;
 public class SpellCheckResponse {
   private boolean correctlySpelled;
   private List<Collation> collations;
-  private List<Suggestion> suggestions = new ArrayList<Suggestion>();
-  Map<String, Suggestion> suggestionMap = new LinkedHashMap<String, Suggestion>();
+  private List<Suggestion> suggestions = new ArrayList<>();
+  Map<String, Suggestion> suggestionMap = new LinkedHashMap<>();
 
   public SpellCheckResponse(NamedList<NamedList<Object>> spellInfo) {
     NamedList<Object> sugg = spellInfo.get("suggestions");
@@ -49,7 +49,7 @@ public class SpellCheckResponse {
         //continue;
       } else if ("collation".equals(n)) {
         List<Object> collationInfo = sugg.getAll(n);
-        collations = new ArrayList<Collation>(collationInfo.size());
+        collations = new ArrayList<>(collationInfo.size());
         for (Object o : collationInfo) {
           if (o instanceof String) {
             collations.add(new Collation()
@@ -138,7 +138,7 @@ public class SpellCheckResponse {
     private int startOffset;
     private int endOffset;
     private int originalFrequency;
-    private List<String> alternatives = new ArrayList<String>();
+    private List<String> alternatives = new ArrayList<>();
     private List<Integer> alternativeFrequencies;
 
     public Suggestion(String token, NamedList<Object> suggestion) {
@@ -161,7 +161,7 @@ public class SpellCheckResponse {
             // extended results detected
             @SuppressWarnings("unchecked")
             List<NamedList> extended = (List<NamedList>)list;
-            alternativeFrequencies = new ArrayList<Integer>();
+            alternativeFrequencies = new ArrayList<>();
             for (NamedList nl : extended) {
               alternatives.add((String)nl.get("word"));
               alternativeFrequencies.add((Integer)nl.get("freq"));
@@ -221,7 +221,7 @@ public class SpellCheckResponse {
 
   public class Collation {
     private String collationQueryString;
-    private List<Correction> misspellingsAndCorrections = new ArrayList<Correction>();
+    private List<Correction> misspellingsAndCorrections = new ArrayList<>();
     private long numberOfHits;
 
     public long getNumberOfHits() {
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/response/TermsResponse.java solr/solrj/src/java/org/apache/solr/client/solrj/response/TermsResponse.java
index 92f6e55..5d8c0b7 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/response/TermsResponse.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/response/TermsResponse.java
@@ -27,12 +27,12 @@ import java.util.Map;
  * Encapsulates responses from TermsComponent
  */
 public class TermsResponse {
-  private Map<String, List<Term>> termMap = new HashMap<String, List<Term>>();
+  private Map<String, List<Term>> termMap = new HashMap<>();
   
   public TermsResponse(NamedList<NamedList<Number>> termsInfo) {
     for (int i = 0; i < termsInfo.size(); i++) {
       String fieldName = termsInfo.getName(i);
-      List<Term> itemList = new ArrayList<Term>();
+      List<Term> itemList = new ArrayList<>();
       NamedList<Number> items = termsInfo.getVal(i);
       
       for (int j = 0; j < items.size(); j++) {
diff --git solr/solrj/src/java/org/apache/solr/client/solrj/util/ClientUtils.java solr/solrj/src/java/org/apache/solr/client/solrj/util/ClientUtils.java
index cb76605..a4bcca0 100644
--- solr/solrj/src/java/org/apache/solr/client/solrj/util/ClientUtils.java
+++ solr/solrj/src/java/org/apache/solr/client/solrj/util/ClientUtils.java
@@ -61,7 +61,7 @@ public class ClientUtils
     if( str == null )
       return null;
 
-    ArrayList<ContentStream> streams = new ArrayList<ContentStream>( 1 );
+    ArrayList<ContentStream> streams = new ArrayList<>( 1 );
     ContentStreamBase ccc = new ContentStreamBase.StringStream( str );
     ccc.setContentType( contentType );
     streams.add( ccc );
diff --git solr/solrj/src/java/org/apache/solr/common/SolrDocument.java solr/solrj/src/java/org/apache/solr/common/SolrDocument.java
index eae6835..32c60df 100644
--- solr/solrj/src/java/org/apache/solr/common/SolrDocument.java
+++ solr/solrj/src/java/org/apache/solr/common/SolrDocument.java
@@ -46,7 +46,7 @@ public class SolrDocument implements Map<String,Object>, Iterable<Map.Entry<Stri
   
   public SolrDocument()
   {
-    _fields = new LinkedHashMap<String,Object>();
+    _fields = new LinkedHashMap<>();
   }
 
   /**
@@ -96,7 +96,7 @@ public class SolrDocument implements Map<String,Object>, Iterable<Map.Entry<Stri
       // nothing
     }
     else if( value instanceof Iterable ) {
-      ArrayList<Object> lst = new ArrayList<Object>();
+      ArrayList<Object> lst = new ArrayList<>();
       for( Object o : (Iterable)value ) {
         lst.add( o );
       }
@@ -122,7 +122,7 @@ public class SolrDocument implements Map<String,Object>, Iterable<Map.Entry<Stri
     Object existing = _fields.get(name);
     if (existing == null) {
       if( value instanceof Collection ) {
-        Collection<Object> c = new ArrayList<Object>( 3 );
+        Collection<Object> c = new ArrayList<>( 3 );
         for ( Object o : (Collection<Object>)value ) {
           c.add(o);
         }
@@ -138,7 +138,7 @@ public class SolrDocument implements Map<String,Object>, Iterable<Map.Entry<Stri
       vals = (Collection<Object>)existing;
     }
     else {
-      vals = new ArrayList<Object>( 3 );
+      vals = new ArrayList<>( 3 );
       vals.add( existing );
     }
     
@@ -193,7 +193,7 @@ public class SolrDocument implements Map<String,Object>, Iterable<Map.Entry<Stri
       return (Collection<Object>)v;
     }
     if( v != null ) {
-      ArrayList<Object> arr = new ArrayList<Object>(1);
+      ArrayList<Object> arr = new ArrayList<>(1);
       arr.add( v );
       return arr;
     }
diff --git solr/solrj/src/java/org/apache/solr/common/SolrInputDocument.java solr/solrj/src/java/org/apache/solr/common/SolrInputDocument.java
index f731910..61d65d3 100644
--- solr/solrj/src/java/org/apache/solr/common/SolrInputDocument.java
+++ solr/solrj/src/java/org/apache/solr/common/SolrInputDocument.java
@@ -42,7 +42,7 @@ public class SolrInputDocument implements Map<String,SolrInputField>, Iterable<S
   private List<SolrInputDocument> _childDocuments;
   
   public SolrInputDocument() {
-    _fields = new LinkedHashMap<String,SolrInputField>();
+    _fields = new LinkedHashMap<>();
   }
   
   public SolrInputDocument(Map<String,SolrInputField> fields) {
@@ -208,7 +208,7 @@ public class SolrInputDocument implements Map<String,SolrInputField>, Iterable<S
     clone._documentBoost = _documentBoost;
 
     if (_childDocuments != null) {
-      clone._childDocuments = new ArrayList<SolrInputDocument>(_childDocuments.size());
+      clone._childDocuments = new ArrayList<>(_childDocuments.size());
       for (SolrInputDocument child : _childDocuments) {
         clone._childDocuments.add(child.deepCopy());
       }
@@ -278,7 +278,7 @@ public class SolrInputDocument implements Map<String,SolrInputField>, Iterable<S
   
   public void addChildDocument(SolrInputDocument child) {
    if (_childDocuments == null) {
-     _childDocuments = new ArrayList<SolrInputDocument>();
+     _childDocuments = new ArrayList<>();
    }
     _childDocuments.add(child);
   }
diff --git solr/solrj/src/java/org/apache/solr/common/SolrInputField.java solr/solrj/src/java/org/apache/solr/common/SolrInputField.java
index 4c1a729..02b6856 100644
--- solr/solrj/src/java/org/apache/solr/common/SolrInputField.java
+++ solr/solrj/src/java/org/apache/solr/common/SolrInputField.java
@@ -50,7 +50,7 @@ public class SolrInputField implements Iterable<Object>, Serializable
 
     if( v instanceof Object[] ) {
       Object[] arr = (Object[])v;
-      Collection<Object> c = new ArrayList<Object>( arr.length );
+      Collection<Object> c = new ArrayList<>( arr.length );
       for( Object o : arr ) {
         c.add( o );
       }
@@ -69,7 +69,7 @@ public class SolrInputField implements Iterable<Object>, Serializable
   public void addValue(Object v, float b) {
     if( value == null ) {
       if ( v instanceof Collection ) {
-        Collection<Object> c = new ArrayList<Object>( 3 );
+        Collection<Object> c = new ArrayList<>( 3 );
         for ( Object o : (Collection<Object>)v ) {
           c.add( o );
         }
@@ -92,7 +92,7 @@ public class SolrInputField implements Iterable<Object>, Serializable
       vals = (Collection<Object>)value;
     }
     else {
-      vals = new ArrayList<Object>( 3 );
+      vals = new ArrayList<>( 3 );
       vals.add( value );
       value = vals;
     }
@@ -146,7 +146,7 @@ public class SolrInputField implements Iterable<Object>, Serializable
       return (Collection<Object>)value;
     }
     if( value != null ) {
-      Collection<Object> vals = new ArrayList<Object>(1);
+      Collection<Object> vals = new ArrayList<>(1);
       vals.add( value );
       return vals;
     }
@@ -221,7 +221,7 @@ public class SolrInputField implements Iterable<Object>, Serializable
     // We can't clone here, so we rely on simple primitives
     if (value instanceof Collection) {
       Collection<Object> values = (Collection<Object>) value;
-      Collection<Object> cloneValues = new ArrayList<Object>(values.size());
+      Collection<Object> cloneValues = new ArrayList<>(values.size());
       cloneValues.addAll(values);
       clone.value = cloneValues;
     } else {
diff --git solr/solrj/src/java/org/apache/solr/common/cloud/Aliases.java solr/solrj/src/java/org/apache/solr/common/cloud/Aliases.java
index 28e76bb..1d18323 100644
--- solr/solrj/src/java/org/apache/solr/common/cloud/Aliases.java
+++ solr/solrj/src/java/org/apache/solr/common/cloud/Aliases.java
@@ -30,7 +30,7 @@ public class Aliases {
   }
 
   public Aliases() {
-    this.aliasMap = new HashMap<String,Map<String,String>>();
+    this.aliasMap = new HashMap<>();
   }
   
   public Map<String,String> getCollectionAliasMap() {
diff --git solr/solrj/src/java/org/apache/solr/common/cloud/ClusterState.java solr/solrj/src/java/org/apache/solr/common/cloud/ClusterState.java
index 0c8549d..237cfda 100644
--- solr/solrj/src/java/org/apache/solr/common/cloud/ClusterState.java
+++ solr/solrj/src/java/org/apache/solr/common/cloud/ClusterState.java
@@ -74,9 +74,9 @@ public class ClusterState implements JSONWriter.Writable {
   public ClusterState(Integer zkClusterStateVersion, Set<String> liveNodes,
       Map<String, DocCollection> collectionStates, ZkStateReader stateReader) {
     this.zkClusterStateVersion = zkClusterStateVersion;
-    this.liveNodes = new HashSet<String>(liveNodes.size());
+    this.liveNodes = new HashSet<>(liveNodes.size());
     this.liveNodes.addAll(liveNodes);
-    this.collectionStates = new LinkedHashMap<String, DocCollection>(collectionStates.size());
+    this.collectionStates = new LinkedHashMap<>(collectionStates.size());
     this.collectionStates.putAll(collectionStates);
     this.stateReader = stateReader;
 
@@ -258,7 +258,7 @@ public class ClusterState implements JSONWriter.Writable {
       return new ClusterState(version, liveNodes, Collections.<String, DocCollection>emptyMap(),stateReader);
     }
     Map<String, Object> stateMap = (Map<String, Object>) ZkStateReader.fromJSON(bytes);
-    Map<String,DocCollection> collections = new LinkedHashMap<String,DocCollection>(stateMap.size());
+    Map<String,DocCollection> collections = new LinkedHashMap<>(stateMap.size());
     for (Entry<String, Object> entry : stateMap.entrySet()) {
       String collectionName = entry.getKey();
       DocCollection coll = collectionFromObjects(collectionName, (Map<String,Object>)entry.getValue());
@@ -289,7 +289,7 @@ public class ClusterState implements JSONWriter.Writable {
       props = Collections.emptyMap();
     } else {
       slices = makeSlices(sliceObjs);
-      props = new HashMap<String, Object>(objs);
+      props = new HashMap<>(objs);
       objs.remove(DocCollection.SHARDS);
     }
 
@@ -310,7 +310,7 @@ public class ClusterState implements JSONWriter.Writable {
 
   private static Map<String,Slice> makeSlices(Map<String,Object> genericSlices) {
     if (genericSlices == null) return Collections.emptyMap();
-    Map<String,Slice> result = new LinkedHashMap<String, Slice>(genericSlices.size());
+    Map<String,Slice> result = new LinkedHashMap<>(genericSlices.size());
     for (Map.Entry<String,Object> entry : genericSlices.entrySet()) {
       String name = entry.getKey();
       Object val = entry.getValue();
diff --git solr/solrj/src/java/org/apache/solr/common/cloud/CompositeIdRouter.java solr/solrj/src/java/org/apache/solr/common/cloud/CompositeIdRouter.java
index e649ad1..d01644f 100644
--- solr/solrj/src/java/org/apache/solr/common/cloud/CompositeIdRouter.java
+++ solr/solrj/src/java/org/apache/solr/common/cloud/CompositeIdRouter.java
@@ -90,7 +90,7 @@ public class CompositeIdRouter extends HashBasedRouter {
 
     Range completeRange = new KeyParser(id).getRange();
 
-    List<Slice> targetSlices = new ArrayList<Slice>(1);
+    List<Slice> targetSlices = new ArrayList<>(1);
     for (Slice slice : collection.getActiveSlices()) {
       Range range = slice.getRange();
       if (range != null && range.overlaps(completeRange)) {
@@ -102,7 +102,7 @@ public class CompositeIdRouter extends HashBasedRouter {
   }
 
   public List<Range> partitionRangeByKey(String key, Range range) {
-    List<Range> result = new ArrayList<Range>(3);
+    List<Range> result = new ArrayList<>(3);
     Range keyRange = keyHashRange(key);
     if (!keyRange.overlaps(range)) {
       throw new IllegalArgumentException("Key range does not overlap given range");
@@ -133,7 +133,7 @@ public class CompositeIdRouter extends HashBasedRouter {
     long rangeSize = (long) max - (long) min;
     long rangeStep = Math.max(1, rangeSize / partitions);
 
-    List<Range> ranges = new ArrayList<Range>(partitions);
+    List<Range> ranges = new ArrayList<>(partitions);
 
     long start = min;
     long end = start;
diff --git solr/solrj/src/java/org/apache/solr/common/cloud/DocCollection.java solr/solrj/src/java/org/apache/solr/common/cloud/DocCollection.java
index 17ac732..5755d71 100644
--- solr/solrj/src/java/org/apache/solr/common/cloud/DocCollection.java
+++ solr/solrj/src/java/org/apache/solr/common/cloud/DocCollection.java
@@ -45,11 +45,11 @@ public class DocCollection extends ZkNodeProps {
    * @param props  The properties of the slice.  This is used directly and a copy is not made.
    */
   public DocCollection(String name, Map<String, Slice> slices, Map<String, Object> props, DocRouter router) {
-    super( props==null ? props = new HashMap<String,Object>() : props);
+    super( props==null ? props = new HashMap<>() : props);
     this.name = name;
 
     this.slices = slices;
-    this.activeSlices = new HashMap<String, Slice>();
+    this.activeSlices = new HashMap<>();
 
     Iterator<Map.Entry<String, Slice>> iter = slices.entrySet().iterator();
 
@@ -115,7 +115,7 @@ public class DocCollection extends ZkNodeProps {
 
   @Override
   public void write(JSONWriter jsonWriter) {
-    LinkedHashMap<String, Object> all = new LinkedHashMap<String, Object>(slices.size() + 1);
+    LinkedHashMap<String, Object> all = new LinkedHashMap<>(slices.size() + 1);
     all.putAll(propMap);
     all.put(SHARDS, slices);
     jsonWriter.write(all);
diff --git solr/solrj/src/java/org/apache/solr/common/cloud/DocRouter.java solr/solrj/src/java/org/apache/solr/common/cloud/DocRouter.java
index c19eebb..088d272 100644
--- solr/solrj/src/java/org/apache/solr/common/cloud/DocRouter.java
+++ solr/solrj/src/java/org/apache/solr/common/cloud/DocRouter.java
@@ -63,7 +63,7 @@ public abstract class DocRouter {
   }
 
   public static Map<String,Object> getRouterSpec(ZkNodeProps props){
-    Map<String,Object> map =  new LinkedHashMap<String, Object>();
+    Map<String,Object> map =  new LinkedHashMap<>();
     for (String s : props.keySet()) {
       if(s.startsWith("router.")){
         map.put(s.substring(7), props.get(s));
@@ -81,7 +81,7 @@ public abstract class DocRouter {
   // currently just an implementation detail...
   private final static Map<String, DocRouter> routerMap;
   static {
-    routerMap = new HashMap<String, DocRouter>();
+    routerMap = new HashMap<>();
     PlainIdRouter plain = new PlainIdRouter();
     // instead of doing back compat this way, we could always convert the clusterstate on first read to "plain" if it doesn't have any properties.
     routerMap.put(null, plain);     // back compat with 4.0
@@ -174,7 +174,7 @@ public abstract class DocRouter {
     long rangeSize = (long)max - (long)min;
     long rangeStep = Math.max(1, rangeSize / partitions);
 
-    List<Range> ranges = new ArrayList<Range>(partitions);
+    List<Range> ranges = new ArrayList<>(partitions);
 
     long start = min;
     long end = start;
@@ -216,7 +216,7 @@ public abstract class DocRouter {
     }
 
     List<String> shardKeyList = StrUtils.splitSmart(shardKeys, ",", true);
-    HashSet<Slice> allSlices = new HashSet<Slice>();
+    HashSet<Slice> allSlices = new HashSet<>();
     for (String shardKey : shardKeyList) {
       allSlices.addAll( getSearchSlicesSingle(shardKey, params, collection) );
     }
diff --git solr/solrj/src/java/org/apache/solr/common/cloud/RoutingRule.java solr/solrj/src/java/org/apache/solr/common/cloud/RoutingRule.java
index 5f7658f..69f810f 100644
--- solr/solrj/src/java/org/apache/solr/common/cloud/RoutingRule.java
+++ solr/solrj/src/java/org/apache/solr/common/cloud/RoutingRule.java
@@ -37,7 +37,7 @@ public class RoutingRule extends ZkNodeProps {
     this.routeRangesStr = (String) propMap.get("routeRanges");
     String[] rangesArr = this.routeRangesStr.split(",");
     if (rangesArr != null && rangesArr.length > 0)  {
-      this.routeRanges = new ArrayList<DocRouter.Range>();
+      this.routeRanges = new ArrayList<>();
       for (String r : rangesArr) {
         routeRanges.add(DocRouter.DEFAULT.fromString(r));
       }
diff --git solr/solrj/src/java/org/apache/solr/common/cloud/Slice.java solr/solrj/src/java/org/apache/solr/common/cloud/Slice.java
index 0200ff6..70abf6e 100644
--- solr/solrj/src/java/org/apache/solr/common/cloud/Slice.java
+++ solr/solrj/src/java/org/apache/solr/common/cloud/Slice.java
@@ -54,7 +54,7 @@ public class Slice extends ZkNodeProps {
    * @param props  The properties of the slice - a shallow copy will always be made.
    */
   public Slice(String name, Map<String,Replica> replicas, Map<String,Object> props) {
-    super( props==null ? new LinkedHashMap<String,Object>(2) : new LinkedHashMap<String,Object>(props));
+    super( props==null ? new LinkedHashMap<String,Object>(2) : new LinkedHashMap<>(props));
     this.name = name;
 
     Object rangeObj = propMap.get(RANGE);
@@ -92,7 +92,7 @@ public class Slice extends ZkNodeProps {
 
     Map<String, Object> rules = (Map<String, Object>) propMap.get("routingRules");
     if (rules != null) {
-      this.routingRules = new HashMap<String, RoutingRule>();
+      this.routingRules = new HashMap<>();
       for (Map.Entry<String, Object> entry : rules.entrySet()) {
         Object o = entry.getValue();
         if (o instanceof Map) {
@@ -112,8 +112,8 @@ public class Slice extends ZkNodeProps {
 
 
   private Map<String,Replica> makeReplicas(Map<String,Object> genericReplicas) {
-    if (genericReplicas == null) return new HashMap<String,Replica>(1);
-    Map<String,Replica> result = new LinkedHashMap<String, Replica>(genericReplicas.size());
+    if (genericReplicas == null) return new HashMap<>(1);
+    Map<String,Replica> result = new LinkedHashMap<>(genericReplicas.size());
     for (Map.Entry<String,Object> entry : genericReplicas.entrySet()) {
       String name = entry.getKey();
       Object val = entry.getValue();
@@ -157,7 +157,7 @@ public class Slice extends ZkNodeProps {
   }
 
   public Map<String,Replica> getReplicasCopy() {
-    return new LinkedHashMap<String,Replica>(replicas);
+    return new LinkedHashMap<>(replicas);
   }
 
   public Replica getLeader() {
diff --git solr/solrj/src/java/org/apache/solr/common/cloud/SolrZooKeeper.java solr/solrj/src/java/org/apache/solr/common/cloud/SolrZooKeeper.java
index df352cd..dd89e1c 100644
--- solr/solrj/src/java/org/apache/solr/common/cloud/SolrZooKeeper.java
+++ solr/solrj/src/java/org/apache/solr/common/cloud/SolrZooKeeper.java
@@ -31,7 +31,7 @@ import org.apache.zookeeper.ZooKeeper;
 
 // we use this class to expose nasty stuff for tests
 public class SolrZooKeeper extends ZooKeeper {
-  final Set<Thread> spawnedThreads = new CopyOnWriteArraySet<Thread>();
+  final Set<Thread> spawnedThreads = new CopyOnWriteArraySet<>();
   
   // for test debug
   //static Map<SolrZooKeeper,Exception> clients = new ConcurrentHashMap<SolrZooKeeper,Exception>();
diff --git solr/solrj/src/java/org/apache/solr/common/cloud/ZkClientConnectionStrategy.java solr/solrj/src/java/org/apache/solr/common/cloud/ZkClientConnectionStrategy.java
index c1465b8..5dec65d 100644
--- solr/solrj/src/java/org/apache/solr/common/cloud/ZkClientConnectionStrategy.java
+++ solr/solrj/src/java/org/apache/solr/common/cloud/ZkClientConnectionStrategy.java
@@ -33,8 +33,8 @@ import org.slf4j.LoggerFactory;
 public abstract class ZkClientConnectionStrategy {
   private static Logger log = LoggerFactory.getLogger(ZkClientConnectionStrategy.class);
   
-  private List<DisconnectedListener> disconnectedListeners = new ArrayList<DisconnectedListener>();
-  private List<ConnectedListener> connectedListeners = new ArrayList<ConnectedListener>();
+  private List<DisconnectedListener> disconnectedListeners = new ArrayList<>();
+  private List<ConnectedListener> connectedListeners = new ArrayList<>();
   
   public abstract void connect(String zkServerAddress, int zkClientTimeout, Watcher watcher, ZkUpdate updater) throws IOException, InterruptedException, TimeoutException;
   public abstract void reconnect(String serverAddress, int zkClientTimeout, Watcher watcher, ZkUpdate updater) throws IOException, InterruptedException, TimeoutException;
diff --git solr/solrj/src/java/org/apache/solr/common/cloud/ZkNodeProps.java solr/solrj/src/java/org/apache/solr/common/cloud/ZkNodeProps.java
index 45f735c..5ddfa24 100644
--- solr/solrj/src/java/org/apache/solr/common/cloud/ZkNodeProps.java
+++ solr/solrj/src/java/org/apache/solr/common/cloud/ZkNodeProps.java
@@ -58,7 +58,7 @@ public class ZkNodeProps implements JSONWriter.Writable {
     if ((keyVals.length & 0x01) != 0) {
       throw new IllegalArgumentException("arguments should be key,value");
     }
-    Map<String,Object> propMap = new LinkedHashMap<String,Object>(keyVals.length>>1);
+    Map<String,Object> propMap = new LinkedHashMap<>(keyVals.length>>1);
     for (int i = 0; i < keyVals.length; i+=2) {
       propMap.put(keyVals[i].toString(), keyVals[i+1]);
     }
@@ -82,7 +82,7 @@ public class ZkNodeProps implements JSONWriter.Writable {
 
   /** Returns a shallow writable copy of the properties */
   public Map<String,Object> shallowCopy() {
-    return new LinkedHashMap<String, Object>(propMap);
+    return new LinkedHashMap<>(propMap);
   }
 
   /**
diff --git solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.java solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.java
index 12dc700..09c06c1 100644
--- solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.java
+++ solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.java
@@ -313,7 +313,7 @@ public class ZkStateReader {
                   List<String> liveNodes = zkClient.getChildren(
                       LIVE_NODES_ZKNODE, this, true);
                   log.info("Updating live nodes... ({})", liveNodes.size());
-                  Set<String> liveNodesSet = new HashSet<String>();
+                  Set<String> liveNodesSet = new HashSet<>();
                   liveNodesSet.addAll(liveNodes);
                   ClusterState clusterState = new ClusterState(
                       ZkStateReader.this.clusterState.getZkClusterStateVersion(),
@@ -340,7 +340,7 @@ public class ZkStateReader {
             
           }, true);
     
-      Set<String> liveNodeSet = new HashSet<String>();
+      Set<String> liveNodeSet = new HashSet<>();
       liveNodeSet.addAll(liveNodes);
       ClusterState clusterState = ClusterState.load(zkClient, liveNodeSet, ZkStateReader.this);
       this.clusterState = clusterState;
@@ -403,7 +403,7 @@ public class ZkStateReader {
       synchronized (getUpdateLock()) {
         List<String> liveNodes = zkClient.getChildren(LIVE_NODES_ZKNODE, null,
             true);
-        Set<String> liveNodesSet = new HashSet<String>();
+        Set<String> liveNodesSet = new HashSet<>();
         liveNodesSet.addAll(liveNodes);
         
         if (!onlyLiveNodes) {
@@ -439,7 +439,7 @@ public class ZkStateReader {
             try {
               List<String> liveNodes = zkClient.getChildren(LIVE_NODES_ZKNODE,
                   null, true);
-              Set<String> liveNodesSet = new HashSet<String>();
+              Set<String> liveNodesSet = new HashSet<>();
               liveNodesSet.addAll(liveNodes);
               
               if (!onlyLiveNodes) {
@@ -575,7 +575,7 @@ public class ZkStateReader {
     }
     
     Map<String,Replica> shardMap = replicas.getReplicasMap();
-    List<ZkCoreNodeProps> nodes = new ArrayList<ZkCoreNodeProps>(shardMap.size());
+    List<ZkCoreNodeProps> nodes = new ArrayList<>(shardMap.size());
     for (Entry<String,Replica> entry : shardMap.entrySet()) {
       ZkCoreNodeProps nodeProps = new ZkCoreNodeProps(entry.getValue());
       
diff --git solr/solrj/src/java/org/apache/solr/common/params/DefaultSolrParams.java solr/solrj/src/java/org/apache/solr/common/params/DefaultSolrParams.java
index c2dd5d3..bea2051 100644
--- solr/solrj/src/java/org/apache/solr/common/params/DefaultSolrParams.java
+++ solr/solrj/src/java/org/apache/solr/common/params/DefaultSolrParams.java
@@ -52,7 +52,7 @@ public class DefaultSolrParams extends SolrParams {
 
   @Override
   public Iterator<String> getParameterNamesIterator() {
-    final IteratorChain<String> c = new IteratorChain<String>();
+    final IteratorChain<String> c = new IteratorChain<>();
     c.addIterator(defaults.getParameterNamesIterator());
     c.addIterator(params.getParameterNamesIterator());
     return c;
diff --git solr/solrj/src/java/org/apache/solr/common/params/ModifiableSolrParams.java solr/solrj/src/java/org/apache/solr/common/params/ModifiableSolrParams.java
index 05a6c43..b84f4aa 100644
--- solr/solrj/src/java/org/apache/solr/common/params/ModifiableSolrParams.java
+++ solr/solrj/src/java/org/apache/solr/common/params/ModifiableSolrParams.java
@@ -39,7 +39,7 @@ public class ModifiableSolrParams extends SolrParams
   public ModifiableSolrParams()
   {
     // LinkedHashMap so params show up in CGI in the same order as they are entered
-    vals = new LinkedHashMap<String, String[]>();
+    vals = new LinkedHashMap<>();
   }
 
   /** Constructs a new ModifiableSolrParams directly using the provided Map&lt;String,String[]&gt; */
@@ -51,7 +51,7 @@ public class ModifiableSolrParams extends SolrParams
   /** Constructs a new ModifiableSolrParams, copying values from an existing SolrParams */
   public ModifiableSolrParams(SolrParams params)
   {
-    vals = new LinkedHashMap<String, String[]>();
+    vals = new LinkedHashMap<>();
     if( params != null ) {
       this.add( params );
     }
diff --git solr/solrj/src/java/org/apache/solr/common/params/SolrParams.java solr/solrj/src/java/org/apache/solr/common/params/SolrParams.java
index 02501d2..9a26396 100644
--- solr/solrj/src/java/org/apache/solr/common/params/SolrParams.java
+++ solr/solrj/src/java/org/apache/solr/common/params/SolrParams.java
@@ -270,7 +270,7 @@ public abstract class SolrParams implements Serializable {
 
   /** Create a Map&lt;String,String&gt; from a NamedList given no keys are repeated */
   public static Map<String,String> toMap(NamedList params) {
-    HashMap<String,String> map = new HashMap<String,String>();
+    HashMap<String,String> map = new HashMap<>();
     for (int i=0; i<params.size(); i++) {
       map.put(params.getName(i), params.getVal(i).toString());
     }
@@ -279,7 +279,7 @@ public abstract class SolrParams implements Serializable {
 
   /** Create a Map&lt;String,String[]&gt; from a NamedList */
   public static Map<String,String[]> toMultiMap(NamedList params) {
-    HashMap<String,String[]> map = new HashMap<String,String[]>();
+    HashMap<String,String[]> map = new HashMap<>();
     for (int i=0; i<params.size(); i++) {
       String name = params.getName(i);
       String val = params.getVal(i).toString();
@@ -291,7 +291,7 @@ public abstract class SolrParams implements Serializable {
   /** Create SolrParams from NamedList. */
   public static SolrParams toSolrParams(NamedList params) {
     // if no keys are repeated use the faster MapSolrParams
-    HashMap<String,String> map = new HashMap<String,String>();
+    HashMap<String,String> map = new HashMap<>();
     for (int i=0; i<params.size(); i++) {
       String prev = map.put(params.getName(i), params.getVal(i).toString());
       if (prev!=null) return new MultiMapSolrParams(toMultiMap(params));
@@ -301,7 +301,7 @@ public abstract class SolrParams implements Serializable {
   
   /** Create filtered SolrParams. */
   public SolrParams toFilteredSolrParams(List<String> names) {
-    NamedList<String> nl = new NamedList<String>();
+    NamedList<String> nl = new NamedList<>();
     for (Iterator<String> it = getParameterNamesIterator(); it.hasNext();) {
       final String name = it.next();
       if (names.contains(name)) {
@@ -316,7 +316,7 @@ public abstract class SolrParams implements Serializable {
   
   /** Convert this to a NamedList */
   public NamedList<Object> toNamedList() {
-    final SimpleOrderedMap<Object> result = new SimpleOrderedMap<Object>();
+    final SimpleOrderedMap<Object> result = new SimpleOrderedMap<>();
     
     for(Iterator<String> it=getParameterNamesIterator(); it.hasNext(); ) {
       final String name = it.next();
diff --git solr/solrj/src/java/org/apache/solr/common/util/DateUtil.java solr/solrj/src/java/org/apache/solr/common/util/DateUtil.java
index e1dc6b1..c77080a 100644
--- solr/solrj/src/java/org/apache/solr/common/util/DateUtil.java
+++ solr/solrj/src/java/org/apache/solr/common/util/DateUtil.java
@@ -71,7 +71,7 @@ public class DateUtil {
   /**
    * A suite of default date formats that can be parsed, and thus transformed to the Solr specific format
    */
-  public static final Collection<String> DEFAULT_DATE_FORMATS = new ArrayList<String>();
+  public static final Collection<String> DEFAULT_DATE_FORMATS = new ArrayList<>();
 
   static {
     DEFAULT_DATE_FORMATS.add("yyyy-MM-dd'T'HH:mm:ss'Z'");
diff --git solr/solrj/src/java/org/apache/solr/common/util/IteratorChain.java solr/solrj/src/java/org/apache/solr/common/util/IteratorChain.java
index a4109a5..ad384f5 100644
--- solr/solrj/src/java/org/apache/solr/common/util/IteratorChain.java
+++ solr/solrj/src/java/org/apache/solr/common/util/IteratorChain.java
@@ -27,7 +27,7 @@ import java.util.List;
 
 public class IteratorChain<E> implements Iterator<E> {
 
-  private final List<Iterator<E>> iterators = new ArrayList<Iterator<E>>();
+  private final List<Iterator<E>> iterators = new ArrayList<>();
   private Iterator<Iterator<E>> itit;
   private Iterator<E> current;
  
diff --git solr/solrj/src/java/org/apache/solr/common/util/JavaBinCodec.java solr/solrj/src/java/org/apache/solr/common/util/JavaBinCodec.java
index 3e83f24..7be4246 100644
--- solr/solrj/src/java/org/apache/solr/common/util/JavaBinCodec.java
+++ solr/solrj/src/java/org/apache/solr/common/util/JavaBinCodec.java
@@ -119,7 +119,7 @@ public class JavaBinCodec {
 
   public SimpleOrderedMap<Object> readOrderedMap(DataInputInputStream dis) throws IOException {
     int sz = readSize(dis);
-    SimpleOrderedMap<Object> nl = new SimpleOrderedMap<Object>();
+    SimpleOrderedMap<Object> nl = new SimpleOrderedMap<>();
     for (int i = 0; i < sz; i++) {
       String name = (String) readVal(dis);
       Object val = readVal(dis);
@@ -130,7 +130,7 @@ public class JavaBinCodec {
 
   public NamedList<Object> readNamedList(DataInputInputStream dis) throws IOException {
     int sz = readSize(dis);
-    NamedList<Object> nl = new NamedList<Object>();
+    NamedList<Object> nl = new NamedList<>();
     for (int i = 0; i < sz; i++) {
       String name = (String) readVal(dis);
       Object val = readVal(dis);
@@ -364,7 +364,7 @@ public class JavaBinCodec {
   public void writeSolrDocumentList(SolrDocumentList docs)
           throws IOException {
     writeTag(SOLRDOCLST);
-    List<Number> l = new ArrayList<Number>(3);
+    List<Number> l = new ArrayList<>(3);
     l.add(docs.getNumFound());
     l.add(docs.getStart());
     l.add(docs.getMaxScore());
@@ -419,7 +419,7 @@ public class JavaBinCodec {
   public Map<Object,Object> readMap(DataInputInputStream dis)
           throws IOException {
     int sz = readVInt(dis);
-    Map<Object,Object> m = new LinkedHashMap<Object,Object>();
+    Map<Object,Object> m = new LinkedHashMap<>();
     for (int i = 0; i < sz; i++) {
       Object key = readVal(dis);
       Object val = readVal(dis);
@@ -438,7 +438,7 @@ public class JavaBinCodec {
   }
 
   public List<Object> readIterator(DataInputInputStream fis) throws IOException {
-    ArrayList<Object> l = new ArrayList<Object>();
+    ArrayList<Object> l = new ArrayList<>();
     while (true) {
       Object o = readVal(fis);
       if (o == END_OBJ) break;
@@ -472,7 +472,7 @@ public class JavaBinCodec {
 
   public List<Object> readArray(DataInputInputStream dis) throws IOException {
     int sz = readSize(dis);
-    ArrayList<Object> l = new ArrayList<Object>(sz);
+    ArrayList<Object> l = new ArrayList<>(sz);
     for (int i = 0; i < sz; i++) {
       l.add(readVal(dis));
     }
@@ -757,7 +757,7 @@ public class JavaBinCodec {
     writeTag(EXTERN_STRING, idx);
     if (idx == 0) {
       writeStr(s);
-      if (stringsMap == null) stringsMap = new HashMap<String, Integer>();
+      if (stringsMap == null) stringsMap = new HashMap<>();
       stringsMap.put(s, ++stringsCount);
     }
 
@@ -769,7 +769,7 @@ public class JavaBinCodec {
       return stringsList.get(idx - 1);
     } else {// idx == 0 means it has a string value
       String s = (String) readVal(fis);
-      if (stringsList == null) stringsList = new ArrayList<String>();
+      if (stringsList == null) stringsList = new ArrayList<>();
       stringsList.add(s);
       return s;
     }
diff --git solr/solrj/src/java/org/apache/solr/common/util/NamedList.java solr/solrj/src/java/org/apache/solr/common/util/NamedList.java
index 63309c4..de6be33 100644
--- solr/solrj/src/java/org/apache/solr/common/util/NamedList.java
+++ solr/solrj/src/java/org/apache/solr/common/util/NamedList.java
@@ -60,7 +60,7 @@ public class NamedList<T> implements Cloneable, Serializable, Iterable<Map.Entry
 
   /** Creates an empty instance */
   public NamedList() {
-    nvPairs = new ArrayList<Object>();
+    nvPairs = new ArrayList<>();
   }
 
   /**
@@ -109,7 +109,7 @@ public class NamedList<T> implements Cloneable, Serializable, Iterable<Map.Entry
    */
   @Deprecated
   private List<Object> nameValueMapToList(Map.Entry<String, ? extends T>[] nameValuePairs) {
-    List<Object> result = new ArrayList<Object>();
+    List<Object> result = new ArrayList<>();
     for (Map.Entry<String, ?> ent : nameValuePairs) {
       result.add(ent.getKey());
       result.add(ent.getValue());
@@ -251,7 +251,7 @@ public class NamedList<T> implements Cloneable, Serializable, Iterable<Map.Entry
    * @return List of values
    */
   public List<T> getAll(String name) {
-    List<T> result = new ArrayList<T>();
+    List<T> result = new ArrayList<>();
     int sz = size();
     for (int i = 0; i < sz; i++) {
       String n = getName(i);
@@ -418,9 +418,9 @@ public class NamedList<T> implements Cloneable, Serializable, Iterable<Map.Entry
    */
   @Override
   public NamedList<T> clone() {
-    ArrayList<Object> newList = new ArrayList<Object>(nvPairs.size());
+    ArrayList<Object> newList = new ArrayList<>(nvPairs.size());
     newList.addAll(nvPairs);
-    return new NamedList<T>(newList);
+    return new NamedList<>(newList);
   }
 
   //----------------------------------------------------------------------------
@@ -501,7 +501,7 @@ public class NamedList<T> implements Cloneable, Serializable, Iterable<Map.Entry
    * @return List of values
    */
   public List<T> removeAll(String name) {
-    List<T> result = new ArrayList<T>();
+    List<T> result = new ArrayList<>();
     result = getAll(name);
     if (result.size() > 0 ) {
       killAll(name);
@@ -575,7 +575,7 @@ public class NamedList<T> implements Cloneable, Serializable, Iterable<Map.Entry
   public Collection<String> removeConfigArgs(final String name)
       throws SolrException {
     List<T> objects = getAll(name);
-    List<String> collection = new ArrayList<String>(size() / 2);
+    List<String> collection = new ArrayList<>(size() / 2);
     final String err = "init arg '" + name + "' must be a string "
         + "(ie: 'str'), or an array (ie: 'arr') containing strings; found: ";
     
diff --git solr/solrj/src/java/org/apache/solr/common/util/SimpleOrderedMap.java solr/solrj/src/java/org/apache/solr/common/util/SimpleOrderedMap.java
index 736ad18..4608f90 100644
--- solr/solrj/src/java/org/apache/solr/common/util/SimpleOrderedMap.java
+++ solr/solrj/src/java/org/apache/solr/common/util/SimpleOrderedMap.java
@@ -60,8 +60,8 @@ public class SimpleOrderedMap<T> extends NamedList<T> {
 
   @Override
   public SimpleOrderedMap<T> clone() {
-    ArrayList<Object> newList = new ArrayList<Object>(nvPairs.size());
+    ArrayList<Object> newList = new ArrayList<>(nvPairs.size());
     newList.addAll(nvPairs);
-    return new SimpleOrderedMap<T>(newList);
+    return new SimpleOrderedMap<>(newList);
   }
 }
diff --git solr/solrj/src/java/org/apache/solr/common/util/StrUtils.java solr/solrj/src/java/org/apache/solr/common/util/StrUtils.java
index 16f9cd5..3c4ddfe 100644
--- solr/solrj/src/java/org/apache/solr/common/util/StrUtils.java
+++ solr/solrj/src/java/org/apache/solr/common/util/StrUtils.java
@@ -38,7 +38,7 @@ public class StrUtils {
    * outside strings.
    */
   public static List<String> splitSmart(String s, char separator) {
-    ArrayList<String> lst = new ArrayList<String>(4);
+    ArrayList<String> lst = new ArrayList<>(4);
     int pos=0, start=0, end=s.length();
     char inString=0;
     char ch=0;
@@ -85,7 +85,7 @@ public class StrUtils {
    * @param decode decode backslash escaping
    */
   public static List<String> splitSmart(String s, String separator, boolean decode) {
-    ArrayList<String> lst = new ArrayList<String>(2);
+    ArrayList<String> lst = new ArrayList<>(2);
     StringBuilder sb = new StringBuilder();
     int pos=0, end=s.length();
     while (pos < end) {
@@ -135,7 +135,7 @@ public class StrUtils {
     if (fileNames == null)
       return Collections.<String>emptyList();
 
-    List<String> result = new ArrayList<String>();
+    List<String> result = new ArrayList<>();
     for (String file : fileNames.split("(?<!\\\\),")) {
       result.add(file.replaceAll("\\\\(?=,)", ""));
     }
@@ -168,7 +168,7 @@ public class StrUtils {
 
 
   public static List<String> splitWS(String s, boolean decode) {
-    ArrayList<String> lst = new ArrayList<String>(2);
+    ArrayList<String> lst = new ArrayList<>(2);
     StringBuilder sb = new StringBuilder();
     int pos=0, end=s.length();
     while (pos < end) {
@@ -207,7 +207,7 @@ public class StrUtils {
   }
 
   public static List<String> toLower(List<String> strings) {
-    ArrayList<String> ret = new ArrayList<String>(strings.size());
+    ArrayList<String> ret = new ArrayList<>(strings.size());
     for (String str : strings) {
       ret.add(str.toLowerCase(Locale.ROOT));
     }
diff --git solr/solrj/src/test/org/apache/solr/client/solrj/LargeVolumeTestBase.java solr/solrj/src/test/org/apache/solr/client/solrj/LargeVolumeTestBase.java
index e467bbc..287f00a 100644
--- solr/solrj/src/test/org/apache/solr/client/solrj/LargeVolumeTestBase.java
+++ solr/solrj/src/test/org/apache/solr/client/solrj/LargeVolumeTestBase.java
@@ -87,12 +87,12 @@ public abstract class LargeVolumeTestBase extends SolrJettyTestBase
     public void run() {
       try {
         UpdateResponse resp = null;
-        List<SolrInputDocument> docs = new ArrayList<SolrInputDocument>();
+        List<SolrInputDocument> docs = new ArrayList<>();
         for (int i = 0; i < numdocs; i++) {
           if (i > 0 && i % 200 == 0) {
             resp = tserver.add(docs);
             assertEquals(0, resp.getStatus());
-            docs = new ArrayList<SolrInputDocument>();
+            docs = new ArrayList<>();
           }
           if (i > 0 && i % 5000 == 0) {
             log.info(getName() + " - Committing " + i);
diff --git solr/solrj/src/test/org/apache/solr/client/solrj/SolrExampleTests.java solr/solrj/src/test/org/apache/solr/client/solrj/SolrExampleTests.java
index 86341d0..1c68f18 100644
--- solr/solrj/src/test/org/apache/solr/client/solrj/SolrExampleTests.java
+++ solr/solrj/src/test/org/apache/solr/client/solrj/SolrExampleTests.java
@@ -114,7 +114,7 @@ abstract public class SolrExampleTests extends SolrExampleTestsBase
     Assert.assertEquals(docID, response.getResults().get(0).getFieldValue("id") );
     
     // Now add a few docs for facet testing...
-    List<SolrInputDocument> docs = new ArrayList<SolrInputDocument>();
+    List<SolrInputDocument> docs = new ArrayList<>();
     SolrInputDocument doc2 = new SolrInputDocument();
     doc2.addField( "id", "2", 1.0f );
     doc2.addField( "inStock", true, 1.0f );
@@ -241,7 +241,7 @@ abstract public class SolrExampleTests extends SolrExampleTestsBase
     doc2.addField( "name", "h\uD866\uDF05llo", 1.0f );
     doc2.addField( "price", 20 );
     
-    Collection<SolrInputDocument> docs = new ArrayList<SolrInputDocument>();
+    Collection<SolrInputDocument> docs = new ArrayList<>();
     docs.add( doc1 );
     docs.add( doc2 );
     
@@ -358,7 +358,7 @@ abstract public class SolrExampleTests extends SolrExampleTestsBase
         // Empty the database...
         server.deleteByQuery("*:*");// delete everything!
         
-        List<SolrInputDocument> docs = new ArrayList<SolrInputDocument>();
+        List<SolrInputDocument> docs = new ArrayList<>();
         for (int i = 0; i < numDocs; i++) {
           // Now add something...
           SolrInputDocument doc = new SolrInputDocument();
@@ -739,7 +739,7 @@ abstract public class SolrExampleTests extends SolrExampleTestsBase
     server.commit();
     assertNumFound( "*:*", 0 ); // make sure it got in
     
-    ArrayList<SolrInputDocument> docs = new ArrayList<SolrInputDocument>(10);
+    ArrayList<SolrInputDocument> docs = new ArrayList<>(10);
     for( int i=1; i<=10; i++ ) {
       SolrInputDocument doc = new SolrInputDocument();
       doc.setField( "id", i+"", 1.0f );
@@ -816,7 +816,7 @@ abstract public class SolrExampleTests extends SolrExampleTestsBase
     assertNumFound( "*:*", 0 ); // make sure it got in
     
     int id = 1;
-    ArrayList<SolrInputDocument> docs = new ArrayList<SolrInputDocument>();
+    ArrayList<SolrInputDocument> docs = new ArrayList<>();
     docs.add( makeTestDoc( "id", id++, "features", "aaa",  "cat", "a", "inStock", true  ) );
     docs.add( makeTestDoc( "id", id++, "features", "aaa",  "cat", "a", "inStock", false ) );
     docs.add( makeTestDoc( "id", id++, "features", "aaa",  "cat", "a", "inStock", true ) );
@@ -1120,7 +1120,7 @@ abstract public class SolrExampleTests extends SolrExampleTestsBase
     assertEquals(1.0f, resp.getResults().get(0).getFirstValue("price_f"));
 
     //update "price" with incorrect version (optimistic locking)
-    HashMap<String, Object> oper = new HashMap<String, Object>();  //need better api for this???
+    HashMap<String, Object> oper = new HashMap<>();  //need better api for this???
     oper.put("set",100);
 
     doc = new SolrInputDocument();
@@ -1174,7 +1174,7 @@ abstract public class SolrExampleTests extends SolrExampleTestsBase
     QueryResponse response = solrServer.query(new SolrQuery("id:123"));
     assertEquals("Failed to add doc to cloud server", 1, response.getResults().getNumFound());
 
-    Map<String, List<String>> operation = new HashMap<String, List<String>>();
+    Map<String, List<String>> operation = new HashMap<>();
     operation.put("set", Arrays.asList("first", "second", "third"));
     doc.addField("multi_ss", operation);
     solrServer.add(doc);
diff --git solr/solrj/src/test/org/apache/solr/client/solrj/SolrExampleTestsBase.java solr/solrj/src/test/org/apache/solr/client/solrj/SolrExampleTestsBase.java
index 72947d4..5e56819 100644
--- solr/solrj/src/test/org/apache/solr/client/solrj/SolrExampleTestsBase.java
+++ solr/solrj/src/test/org/apache/solr/client/solrj/SolrExampleTestsBase.java
@@ -202,7 +202,7 @@ abstract public class SolrExampleTestsBase extends SolrJettyTestBase {
     assertNumFound("*:*", 3); // make sure it got in
     
     // should be able to handle multiple delete commands in a single go
-    List<String> ids = new ArrayList<String>();
+    List<String> ids = new ArrayList<>();
     for (SolrInputDocument d : doc) {
       ids.add(d.getFieldValue("id").toString());
     }
diff --git solr/solrj/src/test/org/apache/solr/client/solrj/TestLBHttpSolrServer.java solr/solrj/src/test/org/apache/solr/client/solrj/TestLBHttpSolrServer.java
index 6279df9..afe712d 100644
--- solr/solrj/src/test/org/apache/solr/client/solrj/TestLBHttpSolrServer.java
+++ solr/solrj/src/test/org/apache/solr/client/solrj/TestLBHttpSolrServer.java
@@ -99,7 +99,7 @@ public class TestLBHttpSolrServer extends SolrTestCaseJ4 {
   }
 
   private void addDocs(SolrInstance solrInstance) throws IOException, SolrServerException {
-    List<SolrInputDocument> docs = new ArrayList<SolrInputDocument>();
+    List<SolrInputDocument> docs = new ArrayList<>();
     for (int i = 0; i < 10; i++) {
       SolrInputDocument doc = new SolrInputDocument();
       doc.addField("id", i);
@@ -135,7 +135,7 @@ public class TestLBHttpSolrServer extends SolrTestCaseJ4 {
     LBHttpSolrServer lbHttpSolrServer = new LBHttpSolrServer(httpClient, s);
     lbHttpSolrServer.setAliveCheckInterval(500);
     SolrQuery solrQuery = new SolrQuery("*:*");
-    Set<String> names = new HashSet<String>();
+    Set<String> names = new HashSet<>();
     QueryResponse resp = null;
     for (String value : s) {
       resp = lbHttpSolrServer.query(solrQuery);
diff --git solr/solrj/src/test/org/apache/solr/client/solrj/beans/TestDocumentObjectBinder.java solr/solrj/src/test/org/apache/solr/client/solrj/beans/TestDocumentObjectBinder.java
index 0fa5291..5c83016 100644
--- solr/solrj/src/test/org/apache/solr/client/solrj/beans/TestDocumentObjectBinder.java
+++ solr/solrj/src/test/org/apache/solr/client/solrj/beans/TestDocumentObjectBinder.java
@@ -111,11 +111,11 @@ public class TestDocumentObjectBinder extends LuceneTestCase {
     item.features = Arrays.asList(item.categories);
     List<String> supA = Arrays.asList("supA1", "supA2", "supA3");
     List<String> supB = Arrays.asList("supB1", "supB2", "supB3");
-    item.supplier = new HashMap<String, List<String>>();
+    item.supplier = new HashMap<>();
     item.supplier.put("supplier_supA", supA);
     item.supplier.put("supplier_supB", supB);
     
-    item.supplier_simple = new HashMap<String, String>();
+    item.supplier_simple = new HashMap<>();
     item.supplier_simple.put("sup_simple_supA", "supA_val");
     item.supplier_simple.put("sup_simple_supB", "supB_val");
     
diff --git solr/solrj/src/test/org/apache/solr/client/solrj/embedded/SolrExampleStreamingTest.java solr/solrj/src/test/org/apache/solr/client/solrj/embedded/SolrExampleStreamingTest.java
index 35abdde..fa9521c 100644
--- solr/solrj/src/test/org/apache/solr/client/solrj/embedded/SolrExampleStreamingTest.java
+++ solr/solrj/src/test/org/apache/solr/client/solrj/embedded/SolrExampleStreamingTest.java
@@ -78,7 +78,7 @@ public class SolrExampleStreamingTest extends SolrExampleTests {
 
   public void testWaitOptions() throws Exception {
     // SOLR-3903
-    final List<Throwable> failures = new ArrayList<Throwable>();
+    final List<Throwable> failures = new ArrayList<>();
     ConcurrentUpdateSolrServer s = new ConcurrentUpdateSolrServer
       (jetty.getBaseUrl().toString() + "/collection1", 2, 2) {
         @Override
diff --git solr/solrj/src/test/org/apache/solr/client/solrj/embedded/TestEmbeddedSolrServer.java solr/solrj/src/test/org/apache/solr/client/solrj/embedded/TestEmbeddedSolrServer.java
index 4baddae..5d17d78 100644
--- solr/solrj/src/test/org/apache/solr/client/solrj/embedded/TestEmbeddedSolrServer.java
+++ solr/solrj/src/test/org/apache/solr/client/solrj/embedded/TestEmbeddedSolrServer.java
@@ -60,7 +60,7 @@ public class TestEmbeddedSolrServer extends AbstractEmbeddedSolrServerTestCase {
     EmbeddedSolrServer solrServer = (EmbeddedSolrServer)getSolrCore0();
     
     Assert.assertEquals(3, cores.getCores().size());
-    List<SolrCore> solrCores = new ArrayList<SolrCore>();
+    List<SolrCore> solrCores = new ArrayList<>();
     for (SolrCore solrCore : cores.getCores()) {
       Assert.assertEquals(false, solrCore.isClosed());
       solrCores.add(solrCore);
diff --git solr/solrj/src/test/org/apache/solr/client/solrj/impl/BasicHttpSolrServerTest.java solr/solrj/src/test/org/apache/solr/client/solrj/impl/BasicHttpSolrServerTest.java
index 5e80106..7b33b70 100644
--- solr/solrj/src/test/org/apache/solr/client/solrj/impl/BasicHttpSolrServerTest.java
+++ solr/solrj/src/test/org/apache/solr/client/solrj/impl/BasicHttpSolrServerTest.java
@@ -99,7 +99,7 @@ public class BasicHttpSolrServerTest extends SolrJettyTestBase {
     
     private void setHeaders(HttpServletRequest req) {
       Enumeration<String> headerNames = req.getHeaderNames();
-      headers = new HashMap<String,String>();
+      headers = new HashMap<>();
       while (headerNames.hasMoreElements()) {
         final String name = headerNames.nextElement();
         headers.put(name, req.getHeader(name));
diff --git solr/solrj/src/test/org/apache/solr/client/solrj/request/TestUpdateRequestCodec.java solr/solrj/src/test/org/apache/solr/client/solrj/request/TestUpdateRequestCodec.java
index d03ed94..5a126d3 100644
--- solr/solrj/src/test/org/apache/solr/client/solrj/request/TestUpdateRequestCodec.java
+++ solr/solrj/src/test/org/apache/solr/client/solrj/request/TestUpdateRequestCodec.java
@@ -73,7 +73,7 @@ public class TestUpdateRequestCodec extends LuceneTestCase {
     updateRequest.add(doc);
 
     doc = new SolrInputDocument();
-    Collection<String> foobar = new HashSet<String>();
+    Collection<String> foobar = new HashSet<>();
     foobar.add("baz1");
     foobar.add("baz2");
     doc.addField("foobar",foobar);
@@ -85,7 +85,7 @@ public class TestUpdateRequestCodec extends LuceneTestCase {
     JavaBinUpdateRequestCodec codec = new JavaBinUpdateRequestCodec();
     ByteArrayOutputStream baos = new ByteArrayOutputStream();
     codec.marshal(updateRequest, baos);
-    final List<SolrInputDocument> docs = new ArrayList<SolrInputDocument>();
+    final List<SolrInputDocument> docs = new ArrayList<>();
     JavaBinUpdateRequestCodec.StreamingUpdateHandler handler = new JavaBinUpdateRequestCodec.StreamingUpdateHandler() {
       @Override
       public void update(SolrInputDocument document, UpdateRequest req, Integer commitWithin, Boolean overwrite) {
@@ -114,7 +114,7 @@ public class TestUpdateRequestCodec extends LuceneTestCase {
 
   @Test
   public void testIteratable() throws IOException {
-    final List<String> values = new ArrayList<String>();
+    final List<String> values = new ArrayList<>();
     values.add("iterItem1");
     values.add("iterItem2");
 
@@ -136,7 +136,7 @@ public class TestUpdateRequestCodec extends LuceneTestCase {
     JavaBinUpdateRequestCodec codec = new JavaBinUpdateRequestCodec();
     ByteArrayOutputStream baos = new ByteArrayOutputStream();
     codec.marshal(updateRequest, baos);
-    final List<SolrInputDocument> docs = new ArrayList<SolrInputDocument>();
+    final List<SolrInputDocument> docs = new ArrayList<>();
     JavaBinUpdateRequestCodec.StreamingUpdateHandler handler = new JavaBinUpdateRequestCodec.StreamingUpdateHandler() {
       @Override
       public void update(SolrInputDocument document, UpdateRequest req, Integer commitWithin, Boolean overwrite) {
@@ -191,7 +191,7 @@ public class TestUpdateRequestCodec extends LuceneTestCase {
     updateRequest.add(doc);
 
     doc = new SolrInputDocument();
-    Collection<String> foobar = new HashSet<String>();
+    Collection<String> foobar = new HashSet<>();
     foobar.add("baz1");
     foobar.add("baz2");
     doc.addField("foobar",foobar);
diff --git solr/solrj/src/test/org/apache/solr/client/solrj/response/AnlysisResponseBaseTest.java solr/solrj/src/test/org/apache/solr/client/solrj/response/AnlysisResponseBaseTest.java
index d2a7948..62bd947 100644
--- solr/solrj/src/test/org/apache/solr/client/solrj/response/AnlysisResponseBaseTest.java
+++ solr/solrj/src/test/org/apache/solr/client/solrj/response/AnlysisResponseBaseTest.java
@@ -102,7 +102,7 @@ public class AnlysisResponseBaseTest extends LuceneTestCase {
   //================================================ Helper Methods ==================================================
 
   private List<NamedList> buildFakeTokenInfoList(int numberOfTokens) {
-    List<NamedList> list = new ArrayList<NamedList>(numberOfTokens);
+    List<NamedList> list = new ArrayList<>(numberOfTokens);
     for (int i = 0; i < numberOfTokens; i++) {
       list.add(new NamedList());
     }
diff --git solr/solrj/src/test/org/apache/solr/client/solrj/response/DocumentAnalysisResponseTest.java solr/solrj/src/test/org/apache/solr/client/solrj/response/DocumentAnalysisResponseTest.java
index 4003230..f07a140 100644
--- solr/solrj/src/test/org/apache/solr/client/solrj/response/DocumentAnalysisResponseTest.java
+++ solr/solrj/src/test/org/apache/solr/client/solrj/response/DocumentAnalysisResponseTest.java
@@ -40,7 +40,7 @@ public class DocumentAnalysisResponseTest extends LuceneTestCase {
 
     // the parsing of the analysis phases is already tested in the AnalysisResponseBaseTest. So we can just fake
     // the phases list here and use it.
-    final List<AnalysisResponseBase.AnalysisPhase> phases = new ArrayList<AnalysisResponseBase.AnalysisPhase>(1);
+    final List<AnalysisResponseBase.AnalysisPhase> phases = new ArrayList<>(1);
     AnalysisResponseBase.AnalysisPhase expectedPhase = new AnalysisResponseBase.AnalysisPhase("Tokenizer");
     phases.add(expectedPhase);
 
diff --git solr/solrj/src/test/org/apache/solr/client/solrj/response/FieldAnalysisResponseTest.java solr/solrj/src/test/org/apache/solr/client/solrj/response/FieldAnalysisResponseTest.java
index 7611ff8..59a154b 100644
--- solr/solrj/src/test/org/apache/solr/client/solrj/response/FieldAnalysisResponseTest.java
+++ solr/solrj/src/test/org/apache/solr/client/solrj/response/FieldAnalysisResponseTest.java
@@ -42,7 +42,7 @@ public class FieldAnalysisResponseTest extends LuceneTestCase {
 
     // the parsing of the analysis phases is already tested in the AnalysisResponseBaseTest. So we can just fake
     // the phases list here and use it.
-    final List<AnalysisResponseBase.AnalysisPhase> phases = new ArrayList<AnalysisResponseBase.AnalysisPhase>(1);
+    final List<AnalysisResponseBase.AnalysisPhase> phases = new ArrayList<>(1);
     AnalysisResponseBase.AnalysisPhase expectedPhase = new AnalysisResponseBase.AnalysisPhase("Tokenizer");
     phases.add(expectedPhase);
 
diff --git solr/solrj/src/test/org/apache/solr/common/SolrDocumentTest.java solr/solrj/src/test/org/apache/solr/common/SolrDocumentTest.java
index a67c603..52fc832 100644
--- solr/solrj/src/test/org/apache/solr/common/SolrDocumentTest.java
+++ solr/solrj/src/test/org/apache/solr/common/SolrDocumentTest.java
@@ -55,7 +55,7 @@ public class SolrDocumentTest extends LuceneTestCase
     assertNull( doc.getFieldValue( "xxxxx" ) );
     assertNull( doc.getFieldValues( "xxxxx" ) );
     
-    List<String> keys = new ArrayList<String>();
+    List<String> keys = new ArrayList<>();
     for( String s : doc.getFieldNames() ) {
       keys.add( s );
     }
@@ -105,7 +105,7 @@ public class SolrDocumentTest extends LuceneTestCase
   
   public void testAddCollections()
   {
-    final List<String> c0 = new ArrayList<String>();
+    final List<String> c0 = new ArrayList<>();
     c0.add( "aaa" );
     c0.add( "aaa" );
     c0.add( "aaa" );
@@ -170,7 +170,7 @@ public class SolrDocumentTest extends LuceneTestCase
 
     // set field using a collection is documented to be backed by 
     // that collection, so changes should affect it.
-    Collection<String> tmp = new ArrayList<String>(3);
+    Collection<String> tmp = new ArrayList<>(3);
     tmp.add("one");
     doc.setField( "collection_backed", tmp );
     assertEquals("collection not the same", 
diff --git solr/solrj/src/test/org/apache/solr/common/params/SolrParamTest.java solr/solrj/src/test/org/apache/solr/common/params/SolrParamTest.java
index 5aa3bd7..f3eb141 100644
--- solr/solrj/src/test/org/apache/solr/common/params/SolrParamTest.java
+++ solr/solrj/src/test/org/apache/solr/common/params/SolrParamTest.java
@@ -28,7 +28,7 @@ import org.apache.solr.common.SolrException;
 public class SolrParamTest extends LuceneTestCase 
 {  
   public void testGetParams() {
-    Map<String,String> pmap = new HashMap<String, String>();
+    Map<String,String> pmap = new HashMap<>();
     pmap.put( "str"        , "string"   );
     pmap.put( "bool"       , "true"     );
     pmap.put( "true-0"     , "true"     );
@@ -161,7 +161,7 @@ public class SolrParamTest extends LuceneTestCase
         required.getInt( "f.bad.nnnn", pint ) );
     
     // Check default SolrParams
-    Map<String,String> dmap = new HashMap<String, String>();
+    Map<String,String> dmap = new HashMap<>();
     // these are not defined in params
     dmap.put( "dstr"               , "default"   );
     dmap.put( "dint"               , "123"       );
diff --git solr/solrj/src/test/org/apache/solr/common/util/IteratorChainTest.java solr/solrj/src/test/org/apache/solr/common/util/IteratorChainTest.java
index c2ce6e5..99d7340 100644
--- solr/solrj/src/test/org/apache/solr/common/util/IteratorChainTest.java
+++ solr/solrj/src/test/org/apache/solr/common/util/IteratorChainTest.java
@@ -27,7 +27,7 @@ import org.apache.solr.common.util.IteratorChain;
 public class IteratorChainTest extends LuceneTestCase {
   
   private Iterator<String> makeIterator(String marker,int howMany) {
-    final List<String> c = new ArrayList<String>();
+    final List<String> c = new ArrayList<>();
     for(int i = 1; i <= howMany; i++) {
       c.add(marker + i);
     }
@@ -35,13 +35,13 @@ public class IteratorChainTest extends LuceneTestCase {
   }
   
   public void testNoIterator() {
-    final IteratorChain<String> c = new IteratorChain<String>();
+    final IteratorChain<String> c = new IteratorChain<>();
     assertFalse("Empty IteratorChain.hastNext() is false",c.hasNext());
     assertEquals("",getString(c));
   }
   
   public void testCallNextTooEarly() {
-    final IteratorChain<String> c = new IteratorChain<String>();
+    final IteratorChain<String> c = new IteratorChain<>();
     c.addIterator(makeIterator("a",3));
     try {
       c.next();
@@ -52,7 +52,7 @@ public class IteratorChainTest extends LuceneTestCase {
   }
   
   public void testCallAddTooLate() {
-    final IteratorChain<String> c = new IteratorChain<String>();
+    final IteratorChain<String> c = new IteratorChain<>();
     c.hasNext();
     try {
       c.addIterator(makeIterator("a",3));
@@ -63,7 +63,7 @@ public class IteratorChainTest extends LuceneTestCase {
   }
   
   public void testRemove() {
-    final IteratorChain<String> c = new IteratorChain<String>();
+    final IteratorChain<String> c = new IteratorChain<>();
     try {
       c.remove();
       fail("Calling remove should throw UnsupportedOperationException");
@@ -73,20 +73,20 @@ public class IteratorChainTest extends LuceneTestCase {
   }
   
   public void testOneIterator() {
-    final IteratorChain<String> c = new IteratorChain<String>();
+    final IteratorChain<String> c = new IteratorChain<>();
     c.addIterator(makeIterator("a",3));
     assertEquals("a1a2a3",getString(c));
   }
   
   public void testTwoIterators() {
-    final IteratorChain<String> c = new IteratorChain<String>();
+    final IteratorChain<String> c = new IteratorChain<>();
     c.addIterator(makeIterator("a",3));
     c.addIterator(makeIterator("b",2));
     assertEquals("a1a2a3b1b2",getString(c));
   }
   
   public void testEmptyIteratorsInTheMiddle() {
-    final IteratorChain<String> c = new IteratorChain<String>();
+    final IteratorChain<String> c = new IteratorChain<>();
     c.addIterator(makeIterator("a",3));
     c.addIterator(makeIterator("b",0));
     c.addIterator(makeIterator("c",1));
diff --git solr/solrj/src/test/org/apache/solr/common/util/NamedListTest.java solr/solrj/src/test/org/apache/solr/common/util/NamedListTest.java
index f7e869c..7bf3f4d 100644
--- solr/solrj/src/test/org/apache/solr/common/util/NamedListTest.java
+++ solr/solrj/src/test/org/apache/solr/common/util/NamedListTest.java
@@ -25,7 +25,7 @@ import org.apache.solr.common.SolrException;
 
 public class NamedListTest extends LuceneTestCase {
   public void testRemove() {
-    NamedList<String> nl = new NamedList<String>();
+    NamedList<String> nl = new NamedList<>();
     nl.add("key1", "value1");
     nl.add("key2", "value2");
     assertEquals(2, nl.size());
@@ -39,7 +39,7 @@ public class NamedListTest extends LuceneTestCase {
   }
   
   public void testRemoveAll() {
-    NamedList<String> nl = new NamedList<String>();
+    NamedList<String> nl = new NamedList<>();
     nl.add("key1", "value1-1");
     nl.add("key2", "value2-1");
     nl.add("key1", "value1-2");
@@ -64,7 +64,7 @@ public class NamedListTest extends LuceneTestCase {
   }
   
   public void testRemoveArgs() {
-    NamedList<Object> nl = new NamedList<Object>();
+    NamedList<Object> nl = new NamedList<>();
     nl.add("key1", "value1-1");
     nl.add("key2", "value2-1");
     nl.add("key1", "value1-2");
@@ -114,23 +114,23 @@ public class NamedListTest extends LuceneTestCase {
     // - key3c
     
     // this is a varied NL structure.
-    NamedList<String> nl2b = new NamedList<String>();
+    NamedList<String> nl2b = new NamedList<>();
     nl2b.add("key2b1", "value2b1");
     nl2b.add("key2b2", "value2b2");
-    NamedList<String> nl3a = new NamedList<String>();
+    NamedList<String> nl3a = new NamedList<>();
     nl3a.add("key3a1", "value3a1");
     nl3a.add("key3a2", "value3a2");
     nl3a.add("key3a3", "value3a3");
-    NamedList<Object> nl2 = new NamedList<Object>();
+    NamedList<Object> nl2 = new NamedList<>();
     nl2.add("key2a", "value2a");
     nl2.add("key2b", nl2b);
     nl2.add("k2int1", (int) 5);
-    NamedList<Object> nl3 = new NamedList<Object>();
+    NamedList<Object> nl3 = new NamedList<>();
     nl3.add("key3a", nl3a);
     nl3.add("key3b", "value3b");
     nl3.add("key3c", "value3c");
     nl3.add("key3c", "value3c2");
-    NamedList<Object> nl = new NamedList<Object>();
+    NamedList<Object> nl = new NamedList<>();
     nl.add("key1", "value1");
     nl.add("key2", nl2);
     nl.add("key3", nl3);
@@ -165,8 +165,8 @@ public class NamedListTest extends LuceneTestCase {
     // - key1a
     // - key1b
     // key2 (null list)
-    NamedList<NamedList<String>> enl = new NamedList<NamedList<String>>();
-    NamedList<String> enlkey1 = new NamedList<String>();
+    NamedList<NamedList<String>> enl = new NamedList<>();
+    NamedList<String> enlkey1 = new NamedList<>();
     NamedList<String> enlkey2 = null;
     enlkey1.add("key1a", "value1a");
     enlkey1.add("key1b", "value1b");
diff --git solr/solrj/src/test/org/apache/solr/common/util/TestNamedListCodec.java solr/solrj/src/test/org/apache/solr/common/util/TestNamedListCodec.java
index 312d994..ae2a036 100644
--- solr/solrj/src/test/org/apache/solr/common/util/TestNamedListCodec.java
+++ solr/solrj/src/test/org/apache/solr/common/util/TestNamedListCodec.java
@@ -128,7 +128,7 @@ public class TestNamedListCodec  extends LuceneTestCase {
 
     NamedList r = new NamedList();
 
-    Map<String, String> map = new HashMap<String, String>();
+    Map<String, String> map = new HashMap<>();
     map.put("foo", "bar");
     map.put("junk", "funk");
     map.put("ham", "burger");
diff --git solr/test-framework/src/java/org/apache/solr/BaseDistributedSearchTestCase.java solr/test-framework/src/java/org/apache/solr/BaseDistributedSearchTestCase.java
index 5b96642..11f8883 100644
--- solr/test-framework/src/java/org/apache/solr/BaseDistributedSearchTestCase.java
+++ solr/test-framework/src/java/org/apache/solr/BaseDistributedSearchTestCase.java
@@ -187,8 +187,8 @@ public abstract class BaseDistributedSearchTestCase extends SolrTestCaseJ4 {
   protected boolean fixShardCount = false;
 
   protected JettySolrRunner controlJetty;
-  protected List<SolrServer> clients = new ArrayList<SolrServer>();
-  protected List<JettySolrRunner> jettys = new ArrayList<JettySolrRunner>();
+  protected List<SolrServer> clients = new ArrayList<>();
+  protected List<JettySolrRunner> jettys = new ArrayList<>();
   
   protected String context;
   protected String[] deadServers;
@@ -211,7 +211,7 @@ public abstract class BaseDistributedSearchTestCase extends SolrTestCaseJ4 {
   public static int UNORDERED = 8;
 
   protected int flags;
-  protected Map<String, Integer> handle = new HashMap<String, Integer>();
+  protected Map<String, Integer> handle = new HashMap<>();
 
   protected String id = "id";
   public static Logger log = LoggerFactory.getLogger(BaseDistributedSearchTestCase.class);
@@ -328,7 +328,7 @@ public abstract class BaseDistributedSearchTestCase extends SolrTestCaseJ4 {
       if (sb.length() > 0) sb.append(',');
       int nDeadServers = r.nextInt(deadServers.length+1);
       if (nDeadServers > 0) {
-        List<String> replicas = new ArrayList<String>(Arrays.asList(deadServers));
+        List<String> replicas = new ArrayList<>(Arrays.asList(deadServers));
         Collections.shuffle(replicas, r);
         replicas.add(r.nextInt(nDeadServers+1), shard);
         for (int i=0; i<nDeadServers+1; i++) {
diff --git solr/test-framework/src/java/org/apache/solr/JSONTestUtil.java solr/test-framework/src/java/org/apache/solr/JSONTestUtil.java
index fe40227..b964358 100644
--- solr/test-framework/src/java/org/apache/solr/JSONTestUtil.java
+++ solr/test-framework/src/java/org/apache/solr/JSONTestUtil.java
@@ -118,7 +118,7 @@ class CollectionTester {
     this.val = val;
     this.valRoot = val;
     this.delta = delta;
-    path = new ArrayList<Object>();
+    path = new ArrayList<>();
   }
   public CollectionTester(Object val) {
     this(val, JSONTestUtil.DEFAULT_DELTA);
@@ -259,7 +259,7 @@ class CollectionTester {
     return true;
   }
 
-  private static Set<String> reserved = new HashSet<String>(Arrays.asList("_SKIP_","_MATCH_","_ORDERED_","_UNORDERED_"));
+  private static Set<String> reserved = new HashSet<>(Arrays.asList("_SKIP_","_MATCH_","_ORDERED_","_UNORDERED_"));
 
   boolean matchMap() {
     Map<String,Object> expectedMap = (Map<String,Object>)expected;
@@ -286,7 +286,7 @@ class CollectionTester {
     }
 
     Set<String> keys = match != null ? match : expectedMap.keySet();
-    Set<String> visited = new HashSet<String>();
+    Set<String> visited = new HashSet<>();
 
     Iterator<Map.Entry<String,Object>> iter = ordered ? v.entrySet().iterator() : null;
 
@@ -346,7 +346,7 @@ class CollectionTester {
           if (v.containsKey(skipStr)) skipped++;
       }
       if (numExpected != (v.size() - skipped)) {
-        HashSet<String> set = new HashSet<String>(v.keySet());
+        HashSet<String> set = new HashSet<>(v.keySet());
         set.removeAll(expectedMap.keySet());
         setErr("unexpected map keys " + set); 
         return false;
diff --git solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java
index 2840a50..bbd433b 100644
--- solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java
+++ solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java
@@ -437,7 +437,7 @@ public abstract class SolrTestCaseJ4 extends LuceneTestCase {
   /** Causes an exception matching the regex pattern to not be logged. */
   public static void ignoreException(String pattern) {
     if (SolrException.ignorePatterns == null)
-      SolrException.ignorePatterns = new HashSet<String>();
+      SolrException.ignorePatterns = new HashSet<>();
     SolrException.ignorePatterns.add(pattern);
   }
 
@@ -1374,7 +1374,7 @@ public abstract class SolrTestCaseJ4 extends LuceneTestCase {
     }
 
     public Map<String,Object> toObject(IndexSchema schema) {
-      Map<String,Object> result = new HashMap<String,Object>();
+      Map<String,Object> result = new HashMap<>();
       for (Fld fld : fields) {
         SchemaField sf = schema.getField(fld.ftype.fname);
         if (!sf.multiValued()) {
@@ -1419,7 +1419,7 @@ public abstract class SolrTestCaseJ4 extends LuceneTestCase {
     public List<Comparable> createValues() {
       int nVals = numValues.getInt();
       if (nVals <= 0) return null;
-      List<Comparable> vals = new ArrayList<Comparable>(nVals);
+      List<Comparable> vals = new ArrayList<>(nVals);
       for (int i=0; i<nVals; i++)
         vals.add(createValue());
       return vals;
@@ -1440,7 +1440,7 @@ public abstract class SolrTestCaseJ4 extends LuceneTestCase {
 
   public Map<Comparable,Doc> indexDocs(List<FldType> descriptor, Map<Comparable,Doc> model, int nDocs) throws Exception {
     if (model == null) {
-      model = new LinkedHashMap<Comparable,Doc>();
+      model = new LinkedHashMap<>();
     }
 
     // commit an average of 10 times for large sets, or 10% of the time for small sets
@@ -1499,7 +1499,7 @@ public abstract class SolrTestCaseJ4 extends LuceneTestCase {
 
   public static Doc createDoc(List<FldType> descriptor) {
     Doc doc = new Doc();
-    doc.fields = new ArrayList<Fld>();
+    doc.fields = new ArrayList<>();
     for (FldType ftype : descriptor) {
       Fld fld = ftype.createField();
       if (fld != null) {
@@ -1514,7 +1514,7 @@ public abstract class SolrTestCaseJ4 extends LuceneTestCase {
   public static Comparator<Doc> createSort(IndexSchema schema, List<FldType> fieldTypes, String[] out) {
     StringBuilder sortSpec = new StringBuilder();
     int nSorts = random().nextInt(4);
-    List<Comparator<Doc>> comparators = new ArrayList<Comparator<Doc>>();
+    List<Comparator<Doc>> comparators = new ArrayList<>();
     for (int i=0; i<nSorts; i++) {
       if (i>0) sortSpec.append(',');
 
@@ -1653,7 +1653,7 @@ public abstract class SolrTestCaseJ4 extends LuceneTestCase {
 
   /** Return a Map from field value to a list of document ids */
   public Map<Comparable, List<Comparable>> invertField(Map<Comparable, Doc> model, String field) {
-    Map<Comparable, List<Comparable>> value_to_id = new HashMap<Comparable, List<Comparable>>();
+    Map<Comparable, List<Comparable>> value_to_id = new HashMap<>();
 
     // invert field
     for (Comparable key : model.keySet()) {
@@ -1663,7 +1663,7 @@ public abstract class SolrTestCaseJ4 extends LuceneTestCase {
       for (Comparable val : vals) {
         List<Comparable> ids = value_to_id.get(val);
         if (ids == null) {
-          ids = new ArrayList<Comparable>(2);
+          ids = new ArrayList<>(2);
           value_to_id.put(val, ids);
         }
         ids.add(key);
diff --git solr/test-framework/src/java/org/apache/solr/cloud/AbstractFullDistribZkTestBase.java solr/test-framework/src/java/org/apache/solr/cloud/AbstractFullDistribZkTestBase.java
index 66a3adf..4203ff4 100644
--- solr/test-framework/src/java/org/apache/solr/cloud/AbstractFullDistribZkTestBase.java
+++ solr/test-framework/src/java/org/apache/solr/cloud/AbstractFullDistribZkTestBase.java
@@ -119,12 +119,12 @@ public abstract class AbstractFullDistribZkTestBase extends AbstractDistribZkTes
   protected CloudSolrServer controlClientCloud;  // cloud version of the control client
   protected volatile CloudSolrServer cloudClient;
   
-  protected List<CloudJettyRunner> cloudJettys = new ArrayList<CloudJettyRunner>();
-  protected Map<String,List<CloudJettyRunner>> shardToJetty = new HashMap<String,List<CloudJettyRunner>>();
+  protected List<CloudJettyRunner> cloudJettys = new ArrayList<>();
+  protected Map<String,List<CloudJettyRunner>> shardToJetty = new HashMap<>();
   private AtomicInteger jettyIntCntr = new AtomicInteger(0);
   protected ChaosMonkey chaosMonkey;
   
-  protected Map<String,CloudJettyRunner> shardToLeaderJetty = new HashMap<String,CloudJettyRunner>();
+  protected Map<String,CloudJettyRunner> shardToLeaderJetty = new HashMap<>();
   private boolean cloudInit;
   protected boolean checkCreatedVsState;
   protected boolean useJettyDataDir = true;
@@ -354,8 +354,8 @@ public abstract class AbstractFullDistribZkTestBase extends AbstractDistribZkTes
    *          be the case
    */
   protected List<JettySolrRunner> createJettys(int numJettys, boolean checkCreatedVsState) throws Exception {
-    List<JettySolrRunner> jettys = new ArrayList<JettySolrRunner>();
-    List<SolrServer> clients = new ArrayList<SolrServer>();
+    List<JettySolrRunner> jettys = new ArrayList<>();
+    List<SolrServer> clients = new ArrayList<>();
     StringBuilder sb = new StringBuilder();
     for (int i = 1; i <= numJettys; i++) {
       if (sb.length() > 0) sb.append(',');
@@ -531,7 +531,7 @@ public abstract class AbstractFullDistribZkTestBase extends AbstractDistribZkTes
     ClusterState clusterState = zkStateReader.getClusterState();
     DocCollection coll = clusterState.getCollection(DEFAULT_COLLECTION);
 
-    List<CloudSolrServerClient> theClients = new ArrayList<CloudSolrServerClient>();
+    List<CloudSolrServerClient> theClients = new ArrayList<>();
     for (SolrServer client : clients) {
       // find info for this client in zk 
       nextClient:
@@ -570,7 +570,7 @@ public abstract class AbstractFullDistribZkTestBase extends AbstractDistribZkTes
           if (replica.getStr(ZkStateReader.BASE_URL_PROP).contains(":" + port)) {
             List<CloudJettyRunner> list = shardToJetty.get(slice.getName());
             if (list == null) {
-              list = new ArrayList<CloudJettyRunner>();
+              list = new ArrayList<>();
               shardToJetty.put(slice.getName(), list);
             }
             boolean isLeader = slice.getLeader() == replica;
@@ -915,7 +915,7 @@ public abstract class AbstractFullDistribZkTestBase extends AbstractDistribZkTes
   public QueryResponse queryAndCompareReplicas(SolrParams params, String shard) 
     throws Exception {
 
-    ArrayList<SolrServer> shardClients = new ArrayList<SolrServer>(7);
+    ArrayList<SolrServer> shardClients = new ArrayList<>(7);
 
     updateMappingsFromZk(jettys, clients);
     ZkStateReader zkStateReader = cloudClient.getZkStateReader();
@@ -947,7 +947,7 @@ public abstract class AbstractFullDistribZkTestBase extends AbstractDistribZkTes
   public void queryAndCompareShards(SolrParams params) throws Exception {
 
     updateMappingsFromZk(jettys, clients);
-    List<String> shards = new ArrayList<String>(shardToJetty.keySet());
+    List<String> shards = new ArrayList<>(shardToJetty.keySet());
     for (String shard : shards) {
       queryAndCompareReplicas(params, shard);
     }
@@ -1121,19 +1121,19 @@ public abstract class AbstractFullDistribZkTestBase extends AbstractDistribZkTes
     //  System.err.println("######"+bName+ ": " + toStr(b,10));
     //System.err.println("###### sizes=" + a.size() + "," + b.size());
     boolean legal = true;
-    Set<SolrDocument> setA = new HashSet<SolrDocument>();
+    Set<SolrDocument> setA = new HashSet<>();
     for (SolrDocument sdoc : a) {
       setA.add(sdoc);
     }
 
-    Set<SolrDocument> setB = new HashSet<SolrDocument>();
+    Set<SolrDocument> setB = new HashSet<>();
     for (SolrDocument sdoc : b) {
       setB.add(sdoc);
     }
 
-    Set<SolrDocument> onlyInA = new HashSet<SolrDocument>(setA);
+    Set<SolrDocument> onlyInA = new HashSet<>(setA);
     onlyInA.removeAll(setB);
-    Set<SolrDocument> onlyInB = new HashSet<SolrDocument>(setB);
+    Set<SolrDocument> onlyInB = new HashSet<>(setB);
     onlyInB.removeAll(setA);
 
     if (onlyInA.size() > 0) {
@@ -1168,19 +1168,19 @@ public abstract class AbstractFullDistribZkTestBase extends AbstractDistribZkTes
     System.err.println("######"+bName+ ": " + toStr(b,10));
     System.err.println("###### sizes=" + a.size() + "," + b.size());
     
-    Set<Map> setA = new HashSet<Map>();
+    Set<Map> setA = new HashSet<>();
     for (SolrDocument sdoc : a) {
       setA.add(new HashMap(sdoc));
     }
 
-    Set<Map> setB = new HashSet<Map>();
+    Set<Map> setB = new HashSet<>();
     for (SolrDocument sdoc : b) {
       setB.add(new HashMap(sdoc));
     }
 
-    Set<Map> onlyInA = new HashSet<Map>(setA);
+    Set<Map> onlyInA = new HashSet<>(setA);
     onlyInA.removeAll(setB);
-    Set<Map> onlyInB = new HashSet<Map>(setB);
+    Set<Map> onlyInB = new HashSet<>(setB);
     onlyInB.removeAll(setA);
 
     if (onlyInA.size() > 0) {
@@ -1590,7 +1590,7 @@ public abstract class AbstractFullDistribZkTestBase extends AbstractDistribZkTes
     }
     
     int clientIndex = random().nextInt(2);
-    List<Integer> list = new ArrayList<Integer>();
+    List<Integer> list = new ArrayList<>();
     list.add(numShards);
     list.add(numReplicas);
     if (collectionInfos != null) {
diff --git solr/test-framework/src/java/org/apache/solr/cloud/AbstractZkTestCase.java solr/test-framework/src/java/org/apache/solr/cloud/AbstractZkTestCase.java
index ec3625a..d8b7d62 100644
--- solr/test-framework/src/java/org/apache/solr/cloud/AbstractZkTestCase.java
+++ solr/test-framework/src/java/org/apache/solr/cloud/AbstractZkTestCase.java
@@ -92,7 +92,7 @@ public abstract class AbstractZkTestCase extends SolrTestCaseJ4 {
 
     zkClient = new SolrZkClient(zkAddress, AbstractZkTestCase.TIMEOUT);
 
-    Map<String,Object> props = new HashMap<String,Object>();
+    Map<String,Object> props = new HashMap<>();
     props.put("configName", "conf1");
     final ZkNodeProps zkProps = new ZkNodeProps(props);
     
diff --git solr/test-framework/src/java/org/apache/solr/cloud/ChaosMonkey.java solr/test-framework/src/java/org/apache/solr/cloud/ChaosMonkey.java
index b9a3b12..487d327 100644
--- solr/test-framework/src/java/org/apache/solr/cloud/ChaosMonkey.java
+++ solr/test-framework/src/java/org/apache/solr/cloud/ChaosMonkey.java
@@ -80,7 +80,7 @@ public class ChaosMonkey {
   private Map<String,CloudJettyRunner> shardToLeaderJetty;
   private volatile long startTime;
   
-  private List<CloudJettyRunner> deadPool = new ArrayList<CloudJettyRunner>();
+  private List<CloudJettyRunner> deadPool = new ArrayList<>();
 
   private Thread monkeyThread;
   
@@ -305,7 +305,7 @@ public class ChaosMonkey {
   private String getRandomSlice() {
     Map<String,Slice> slices = zkStateReader.getClusterState().getSlicesMap(collection);
     
-    List<String> sliceKeyList = new ArrayList<String>(slices.size());
+    List<String> sliceKeyList = new ArrayList<>(slices.size());
     sliceKeyList.addAll(slices.keySet());
     String sliceName = sliceKeyList.get(LuceneTestCase.random().nextInt(sliceKeyList.size()));
     return sliceName;
diff --git solr/test-framework/src/java/org/apache/solr/cloud/StopableIndexingThread.java solr/test-framework/src/java/org/apache/solr/cloud/StopableIndexingThread.java
index 8446f08..1b0e97f 100644
--- solr/test-framework/src/java/org/apache/solr/cloud/StopableIndexingThread.java
+++ solr/test-framework/src/java/org/apache/solr/cloud/StopableIndexingThread.java
@@ -33,9 +33,9 @@ public class StopableIndexingThread extends AbstractFullDistribZkTestBase.Stopab
   private static String i1 = "a_si";
   private volatile boolean stop = false;
   protected final String id;
-  protected final List<String> deletes = new ArrayList<String>();
-  protected Set<String> addFails = new HashSet<String>();
-  protected Set<String> deleteFails = new HashSet<String>();
+  protected final List<String> deletes = new ArrayList<>();
+  protected Set<String> addFails = new HashSet<>();
+  protected Set<String> deleteFails = new HashSet<>();
   protected boolean doDeletes;
   private int numCycles;
   private SolrServer controlClient;
diff --git solr/test-framework/src/java/org/apache/solr/cloud/ZkTestServer.java solr/test-framework/src/java/org/apache/solr/cloud/ZkTestServer.java
index 7769456..b890c23 100644
--- solr/test-framework/src/java/org/apache/solr/cloud/ZkTestServer.java
+++ solr/test-framework/src/java/org/apache/solr/cloud/ZkTestServer.java
@@ -328,7 +328,7 @@ public class ZkTestServer {
   }
   
   public static List<HostPort> parseHostPortList(String hplist) {
-    ArrayList<HostPort> alist = new ArrayList<HostPort>();
+    ArrayList<HostPort> alist = new ArrayList<>();
     for (String hp : hplist.split(",")) {
       int idx = hp.lastIndexOf(':');
       String host = hp.substring(0, idx);
diff --git solr/test-framework/src/java/org/apache/solr/update/processor/BufferingRequestProcessor.java solr/test-framework/src/java/org/apache/solr/update/processor/BufferingRequestProcessor.java
index a57f25f..658d29b 100644
--- solr/test-framework/src/java/org/apache/solr/update/processor/BufferingRequestProcessor.java
+++ solr/test-framework/src/java/org/apache/solr/update/processor/BufferingRequestProcessor.java
@@ -28,10 +28,10 @@ import org.apache.solr.update.RollbackUpdateCommand;
 
 public class BufferingRequestProcessor extends UpdateRequestProcessor
 {
-  public List<AddUpdateCommand> addCommands = new ArrayList<AddUpdateCommand>();
-  public List<DeleteUpdateCommand> deleteCommands = new ArrayList<DeleteUpdateCommand>();
-  public List<CommitUpdateCommand> commitCommands = new ArrayList<CommitUpdateCommand>();
-  public List<RollbackUpdateCommand> rollbackCommands = new ArrayList<RollbackUpdateCommand>();
+  public List<AddUpdateCommand> addCommands = new ArrayList<>();
+  public List<DeleteUpdateCommand> deleteCommands = new ArrayList<>();
+  public List<CommitUpdateCommand> commitCommands = new ArrayList<>();
+  public List<RollbackUpdateCommand> rollbackCommands = new ArrayList<>();
   
   public BufferingRequestProcessor(UpdateRequestProcessor next) {
     super(next);
diff --git solr/test-framework/src/java/org/apache/solr/util/AbstractSolrTestCase.java solr/test-framework/src/java/org/apache/solr/util/AbstractSolrTestCase.java
index b739623..b2bb7d3 100644
--- solr/test-framework/src/java/org/apache/solr/util/AbstractSolrTestCase.java
+++ solr/test-framework/src/java/org/apache/solr/util/AbstractSolrTestCase.java
@@ -67,7 +67,7 @@ public abstract class AbstractSolrTestCase extends SolrTestCaseJ4 {
     /** Causes an exception matching the regex pattern to not be logged. */
   public static void ignoreException(String pattern) {
     if (SolrException.ignorePatterns == null)
-      SolrException.ignorePatterns = new HashSet<String>();
+      SolrException.ignorePatterns = new HashSet<>();
     SolrException.ignorePatterns.add(pattern);
   }
 
diff --git solr/test-framework/src/java/org/apache/solr/util/BaseTestHarness.java solr/test-framework/src/java/org/apache/solr/util/BaseTestHarness.java
index 083c81d..b51fc65 100644
--- solr/test-framework/src/java/org/apache/solr/util/BaseTestHarness.java
+++ solr/test-framework/src/java/org/apache/solr/util/BaseTestHarness.java
@@ -33,8 +33,8 @@ import java.io.StringWriter;
 import java.io.UnsupportedEncodingException;
 
 abstract public class BaseTestHarness {
-  private static final ThreadLocal<DocumentBuilder> builderTL = new ThreadLocal<DocumentBuilder>();
-  private static final ThreadLocal<XPath> xpathTL = new ThreadLocal<XPath>();
+  private static final ThreadLocal<DocumentBuilder> builderTL = new ThreadLocal<>();
+  private static final ThreadLocal<XPath> xpathTL = new ThreadLocal<>();
 
   public static DocumentBuilder getXmlDocumentBuilder() {
     try {
diff --git solr/test-framework/src/java/org/apache/solr/util/TestHarness.java solr/test-framework/src/java/org/apache/solr/util/TestHarness.java
index 793a7b3..187b578 100644
--- solr/test-framework/src/java/org/apache/solr/util/TestHarness.java
+++ solr/test-framework/src/java/org/apache/solr/util/TestHarness.java
@@ -388,7 +388,7 @@ public class TestHarness extends BaseTestHarness {
     public String qtype = null;
     public int start = 0;
     public int limit = 1000;
-    public Map<String,String> args = new HashMap<String,String>();
+    public Map<String,String> args = new HashMap<>();
     public LocalRequestFactory() {
     }
     /**
@@ -423,7 +423,7 @@ public class TestHarness extends BaseTestHarness {
       }
       Map.Entry<String, String> [] entries = new NamedListEntry[q.length / 2];
       for (int i = 0; i < q.length; i += 2) {
-        entries[i/2] = new NamedListEntry<String>(q[i], q[i+1]);
+        entries[i/2] = new NamedListEntry<>(q[i], q[i+1]);
       }
       return new LocalSolrQueryRequest(TestHarness.this.getCore(), new NamedList(entries));
     }
