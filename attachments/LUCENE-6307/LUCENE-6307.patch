Index: lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.java	(working copy)
@@ -144,8 +144,8 @@
         final long sumDocFreq = in.readVLong();
         final int docCount = in.readVInt();
         final int longsSize = in.readVInt();
-        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs
-          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + state.segmentInfo.getDocCount(), in);
+        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs
+          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + state.segmentInfo.maxDoc(), in);
         }
         if (sumDocFreq < docCount) {  // #postings must be >= #docs with field
           throw new CorruptIndexException("invalid sumDocFreq: " + sumDocFreq + " docCount: " + docCount, in);
Index: lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsWriter.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsWriter.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsWriter.java	(working copy)
@@ -105,7 +105,7 @@
       throws IOException {
     final String termsFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_EXTENSION);
     this.termsIndexWriter = termsIndexWriter;
-    maxDoc = state.segmentInfo.getDocCount();
+    maxDoc = state.segmentInfo.maxDoc();
     out = state.directory.createOutput(termsFileName, state.context);
     boolean success = false;
     try {
Index: lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader.java	(working copy)
@@ -134,8 +134,8 @@
 
         BytesRef minTerm = readBytesRef(in);
         BytesRef maxTerm = readBytesRef(in);
-        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs
-          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + state.segmentInfo.getDocCount(), in);
+        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs
+          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + state.segmentInfo.maxDoc(), in);
         }
         if (sumDocFreq < docCount) {  // #postings must be >= #docs with field
           throw new CorruptIndexException("invalid sumDocFreq: " + sumDocFreq + " docCount: " + docCount, in);
Index: lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsWriter.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsWriter.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsWriter.java	(working copy)
@@ -182,7 +182,7 @@
   {
     BlockTreeTermsWriter.validateSettings(minItemsInBlock, maxItemsInBlock);
 
-    maxDoc = state.segmentInfo.getDocCount();
+    maxDoc = state.segmentInfo.maxDoc();
 
     final String termsFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_EXTENSION);
     out = state.directory.createOutput(termsFileName, state.context);
Index: lucene/codecs/src/java/org/apache/lucene/codecs/bloom/DefaultBloomFilterFactory.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/bloom/DefaultBloomFilterFactory.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/bloom/DefaultBloomFilterFactory.java	(working copy)
@@ -28,7 +28,7 @@
   @Override
   public FuzzySet getSetForField(SegmentWriteState state,FieldInfo info) {
     //Assume all of the docs have a unique term (e.g. a primary key) and we hope to maintain a set with 10% of bits set
-    return FuzzySet.createSetBasedOnQuality(state.segmentInfo.getDocCount(), 0.10f);
+    return FuzzySet.createSetBasedOnQuality(state.segmentInfo.maxDoc(), 0.10f);
   }
   
   @Override
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesConsumer.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesConsumer.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesConsumer.java	(working copy)
@@ -47,7 +47,7 @@
   final int maxDoc;
 
   DirectDocValuesConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    maxDoc = state.segmentInfo.getDocCount();
+    maxDoc = state.segmentInfo.maxDoc();
     boolean success = false;
     try {
       String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesProducer.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesProducer.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesProducer.java	(working copy)
@@ -115,7 +115,7 @@
   }
     
   DirectDocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    maxDoc = state.segmentInfo.getDocCount();
+    maxDoc = state.segmentInfo.maxDoc();
     merging = false;
     String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
     // read in the entries from the metadata file.
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsReader.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsReader.java	(working copy)
@@ -139,8 +139,8 @@
   }
   private void checkFieldSummary(SegmentInfo info, IndexInput indexIn, IndexInput blockIn, TermsReader field, TermsReader previous) throws IOException {
     // #docs with field must be <= #docs
-    if (field.docCount < 0 || field.docCount > info.getDocCount()) {
-      throw new CorruptIndexException("invalid docCount: " + field.docCount + " maxDoc: " + info.getDocCount() + " (blockIn=" + blockIn + ")", indexIn);
+    if (field.docCount < 0 || field.docCount > info.maxDoc()) {
+      throw new CorruptIndexException("invalid docCount: " + field.docCount + " maxDoc: " + info.maxDoc() + " (blockIn=" + blockIn + ")", indexIn);
     }
     // #postings must be >= #docs with field
     if (field.sumDocFreq < field.docCount) {
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsWriter.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsWriter.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsWriter.java	(working copy)
@@ -166,7 +166,7 @@
 
     this.postingsWriter = postingsWriter;
     this.fieldInfos = state.fieldInfos;
-    this.maxDoc = state.segmentInfo.getDocCount();
+    this.maxDoc = state.segmentInfo.maxDoc();
 
     boolean success = false;
     try {
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsReader.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsReader.java	(working copy)
@@ -119,8 +119,8 @@
   }
   private void checkFieldSummary(SegmentInfo info, IndexInput in, TermsReader field, TermsReader previous) throws IOException {
     // #docs with field must be <= #docs
-    if (field.docCount < 0 || field.docCount > info.getDocCount()) {
-      throw new CorruptIndexException("invalid docCount: " + field.docCount + " maxDoc: " + info.getDocCount(), in);
+    if (field.docCount < 0 || field.docCount > info.maxDoc()) {
+      throw new CorruptIndexException("invalid docCount: " + field.docCount + " maxDoc: " + info.maxDoc(), in);
     }
     // #postings must be >= #docs with field
     if (field.sumDocFreq < field.docCount) {
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsWriter.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsWriter.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsWriter.java	(working copy)
@@ -137,7 +137,7 @@
     this.postingsWriter = postingsWriter;
     this.fieldInfos = state.fieldInfos;
     this.out = state.directory.createOutput(termsFileName, state.context);
-    this.maxDoc = state.segmentInfo.getDocCount();
+    this.maxDoc = state.segmentInfo.maxDoc();
 
     boolean success = false;
     try {
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer.java	(working copy)
@@ -69,7 +69,7 @@
   
   MemoryDocValuesConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension, float acceptableOverheadRatio) throws IOException {
     this.acceptableOverheadRatio = acceptableOverheadRatio;
-    maxDoc = state.segmentInfo.getDocCount();
+    maxDoc = state.segmentInfo.maxDoc();
     boolean success = false;
     try {
       String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesProducer.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesProducer.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesProducer.java	(working copy)
@@ -139,7 +139,7 @@
   }
     
   MemoryDocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    maxDoc = state.segmentInfo.getDocCount();
+    maxDoc = state.segmentInfo.maxDoc();
     merging = false;
     String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
     // read in the entries from the metadata file.
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java	(working copy)
@@ -313,7 +313,7 @@
         TermsWriter termsWriter = new TermsWriter(out, fieldInfo,
                                                   doPackFST, acceptableOverheadRatio);
 
-        FixedBitSet docsSeen = new FixedBitSet(state.segmentInfo.getDocCount());
+        FixedBitSet docsSeen = new FixedBitSet(state.segmentInfo.maxDoc());
         long sumTotalTermFreq = 0;
         long sumDocFreq = 0;
         PostingsEnum postingsEnum = null;
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesReader.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesReader.java	(working copy)
@@ -44,7 +44,6 @@
 import org.apache.lucene.store.BufferedChecksumIndexInput;
 import org.apache.lucene.store.ChecksumIndexInput;
 import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Accountable;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefBuilder;
@@ -86,7 +85,7 @@
   public SimpleTextDocValuesReader(SegmentReadState state, String ext) throws IOException {
     // System.out.println("dir=" + state.directory + " seg=" + state.segmentInfo.name + " file=" + IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, ext));
     data = state.directory.openInput(IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, ext), state.context);
-    maxDoc = state.segmentInfo.getDocCount();
+    maxDoc = state.segmentInfo.maxDoc();
     while(true) {
       readLine();
       //System.out.println("READ field=" + scratch.utf8ToString());
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesWriter.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesWriter.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesWriter.java	(working copy)
@@ -56,9 +56,9 @@
   private final Set<String> fieldsSeen = new HashSet<>(); // for asserting
   
   public SimpleTextDocValuesWriter(SegmentWriteState state, String ext) throws IOException {
-    // System.out.println("WRITE: " + IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, ext) + " " + state.segmentInfo.getDocCount() + " docs");
+    // System.out.println("WRITE: " + IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, ext) + " " + state.segmentInfo.maxDoc() + " docs");
     data = state.directory.createOutput(IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, ext), state.context);
-    numDocs = state.segmentInfo.getDocCount();
+    numDocs = state.segmentInfo.maxDoc();
   }
 
   // for asserting
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java	(working copy)
@@ -79,7 +79,7 @@
   private final int maxDoc;
 
   public SimpleTextFieldsReader(SegmentReadState state) throws IOException {
-    this.maxDoc = state.segmentInfo.getDocCount();
+    this.maxDoc = state.segmentInfo.maxDoc();
     fieldInfos = state.fieldInfos;
     in = state.directory.openInput(SimpleTextPostingsFormat.getPostingsFileName(state.segmentInfo.name, state.segmentSuffix), state.context);
     boolean success = false;
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoFormat.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoFormat.java	(working copy)
@@ -164,7 +164,7 @@
       SimpleTextUtil.writeNewline(output);
     
       SimpleTextUtil.write(output, SI_DOCCOUNT);
-      SimpleTextUtil.write(output, Integer.toString(si.getDocCount()), scratch);
+      SimpleTextUtil.write(output, Integer.toString(si.maxDoc()), scratch);
       SimpleTextUtil.writeNewline(output);
     
       SimpleTextUtil.write(output, SI_USECOMPOUND);
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsReader.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsReader.java	(working copy)
@@ -75,7 +75,7 @@
         } catch (Throwable t) {} // ensure we throw our original exception
       }
     }
-    readIndex(si.getDocCount());
+    readIndex(si.maxDoc());
   }
   
   // used by clone
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java	(revision 1662913)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java	(working copy)
@@ -79,7 +79,7 @@
         } catch (Throwable t) {} // ensure we throw our original exception
       }
     }
-    readIndex(si.getDocCount());
+    readIndex(si.maxDoc());
   }
 
   // used by clone
Index: lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader.java	(working copy)
@@ -180,8 +180,8 @@
         }
         BytesRef minTerm = readBytesRef(termsIn);
         BytesRef maxTerm = readBytesRef(termsIn);
-        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs
-          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + state.segmentInfo.getDocCount(), termsIn);
+        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs
+          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + state.segmentInfo.maxDoc(), termsIn);
         }
         if (sumDocFreq < docCount) {  // #postings must be >= #docs with field
           throw new CorruptIndexException("invalid sumDocFreq: " + sumDocFreq + " docCount: " + docCount, termsIn);
Index: lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java	(working copy)
@@ -257,7 +257,7 @@
   {
     validateSettings(minItemsInBlock, maxItemsInBlock);
 
-    this.maxDoc = state.segmentInfo.getDocCount();
+    this.maxDoc = state.segmentInfo.maxDoc();
     this.fieldInfos = state.fieldInfos;
     this.minItemsInBlock = minItemsInBlock;
     this.maxItemsInBlock = maxItemsInBlock;
Index: lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexReader.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexReader.java	(working copy)
@@ -54,7 +54,7 @@
   // It is the responsibility of the caller to close fieldsIndexIn after this constructor
   // has been called
   CompressingStoredFieldsIndexReader(IndexInput fieldsIndexIn, SegmentInfo si) throws IOException {
-    maxDoc = si.getDocCount();
+    maxDoc = si.maxDoc();
     int[] docBases = new int[16];
     long[] startPointers = new long[16];
     int[] avgChunkDocs = new int[16];
Index: lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java	(working copy)
@@ -119,7 +119,7 @@
     final String segment = si.name;
     boolean success = false;
     fieldInfos = fn;
-    numDocs = si.getDocCount();
+    numDocs = si.maxDoc();
     
     int version = -1;
     long maxPointer = -1;
Index: lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsReader.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsReader.java	(working copy)
@@ -110,7 +110,7 @@
     final String segment = si.name;
     boolean success = false;
     fieldInfos = fn;
-    numDocs = si.getDocCount();
+    numDocs = si.maxDoc();
     int version = -1;
     CompressingStoredFieldsIndexReader indexReader = null;
     
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesConsumer.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesConsumer.java	(working copy)
@@ -105,7 +105,7 @@
       String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
       meta = state.directory.createOutput(metaName, state.context);
       CodecUtil.writeIndexHeader(meta, metaCodec, Lucene50DocValuesFormat.VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
-      maxDoc = state.segmentInfo.getDocCount();
+      maxDoc = state.segmentInfo.maxDoc();
       success = true;
     } finally {
       if (!success) {
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesProducer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesProducer.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesProducer.java	(working copy)
@@ -120,7 +120,7 @@
   /** expert: instantiates a new reader */
   Lucene50DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
     String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-    this.maxDoc = state.segmentInfo.getDocCount();
+    this.maxDoc = state.segmentInfo.maxDoc();
     merging = false;
     
     int version = -1;
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50LiveDocsFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50LiveDocsFormat.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50LiveDocsFormat.java	(working copy)
@@ -79,7 +79,7 @@
   public Bits readLiveDocs(Directory dir, SegmentCommitInfo info, IOContext context) throws IOException {
     long gen = info.getDelGen();
     String name = IndexFileNames.fileNameFromGeneration(info.info.name, EXTENSION, gen);
-    final int length = info.info.getDocCount();
+    final int length = info.info.maxDoc();
     try (ChecksumIndexInput input = dir.openChecksumInput(name, context)) {
       Throwable priorE = null;
       try {
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsProducer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsProducer.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsProducer.java	(working copy)
@@ -87,7 +87,7 @@
     
   Lucene50NormsProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
     merging = false;
-    maxDoc = state.segmentInfo.getDocCount();
+    maxDoc = state.segmentInfo.maxDoc();
     String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
     ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
     int version = -1;
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50PostingsWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50PostingsWriter.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50PostingsWriter.java	(working copy)
@@ -160,7 +160,7 @@
     // TODO: should we try skipping every 2/4 blocks...?
     skipWriter = new Lucene50SkipWriter(MAX_SKIP_LEVELS,
                                         BLOCK_SIZE, 
-                                        state.segmentInfo.getDocCount(),
+                                        state.segmentInfo.maxDoc(),
                                         docOut,
                                         posOut,
                                         payOut);
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50SegmentInfoFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50SegmentInfoFormat.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50SegmentInfoFormat.java	(working copy)
@@ -129,7 +129,7 @@
       output.writeInt(version.minor);
       output.writeInt(version.bugfix);
       assert version.prerelease == 0;
-      output.writeInt(si.getDocCount());
+      output.writeInt(si.maxDoc());
 
       output.writeByte((byte) (si.getUseCompoundFile() ? SegmentInfo.YES : SegmentInfo.NO));
       output.writeStringStringMap(si.getDiagnostics());
Index: lucene/core/src/java/org/apache/lucene/index/BinaryDocValuesWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/BinaryDocValuesWriter.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/BinaryDocValuesWriter.java	(working copy)
@@ -111,7 +111,7 @@
 
   @Override
   public void flush(SegmentWriteState state, DocValuesConsumer dvConsumer) throws IOException {
-    final int maxDoc = state.segmentInfo.getDocCount();
+    final int maxDoc = state.segmentInfo.maxDoc();
     bytes.freeze(false);
     final PackedLongValues lengths = this.lengths.build();
     dvConsumer.addBinaryField(fieldInfo,
Index: lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream.java	(working copy)
@@ -445,8 +445,8 @@
         totDelCount += segState.rld.getPendingDeleteCount() - segState.startDelCount;
         segState.reader.getSegmentInfo().setBufferedDeletesGen(gen);
         int fullDelCount = segState.rld.info.getDelCount() + segState.rld.getPendingDeleteCount();
-        assert fullDelCount <= segState.rld.info.info.getDocCount();
-        if (fullDelCount == segState.rld.info.info.getDocCount()) {
+        assert fullDelCount <= segState.rld.info.info.maxDoc();
+        if (fullDelCount == segState.rld.info.info.maxDoc()) {
           if (allDeleted == null) {
             allDeleted = new ArrayList<>();
           }
Index: lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/CheckIndex.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/CheckIndex.java	(working copy)
@@ -156,7 +156,7 @@
       public Codec codec;
 
       /** Document count (does not take deletions into account). */
-      public int docCount;
+      public int maxDoc;
 
       /** True if segment is compound file format. */
       public boolean compound;
@@ -586,16 +586,16 @@
       }
       Status.SegmentInfoStatus segInfoStat = new Status.SegmentInfoStatus();
       result.segmentInfos.add(segInfoStat);
-      msg(infoStream, "  " + (1+i) + " of " + numSegments + ": name=" + info.info.name + " docCount=" + info.info.getDocCount());
+      msg(infoStream, "  " + (1+i) + " of " + numSegments + ": name=" + info.info.name + " maxDoc=" + info.info.maxDoc());
       segInfoStat.name = info.info.name;
-      segInfoStat.docCount = info.info.getDocCount();
+      segInfoStat.maxDoc = info.info.maxDoc();
       
       final Version version = info.info.getVersion();
-      if (info.info.getDocCount() <= 0) {
-        throw new RuntimeException("illegal number of documents: maxDoc=" + info.info.getDocCount());
+      if (info.info.maxDoc() <= 0) {
+        throw new RuntimeException("illegal number of documents: maxDoc=" + info.info.maxDoc());
       }
 
-      int toLoseDocCount = info.info.getDocCount();
+      int toLoseDocCount = info.info.maxDoc();
 
       SegmentReader reader = null;
 
@@ -620,8 +620,7 @@
         if (!info.hasDeletions()) {
           msg(infoStream, "    no deletions");
           segInfoStat.hasDeletions = false;
-        }
-        else{
+        } else {
           msg(infoStream, "    has deletions [delGen=" + info.getDelGen() + "]");
           segInfoStat.hasDeletions = true;
           segInfoStat.deletionsGen = info.getDelGen();
@@ -641,8 +640,8 @@
         reader.checkIntegrity();
         msg(infoStream, String.format(Locale.ROOT, "OK [took %.3f sec]", nsToSec(System.nanoTime()-startIntegrityNS)));
 
-        if (reader.maxDoc() != info.info.getDocCount()) {
-          throw new RuntimeException("SegmentReader.maxDoc() " + reader.maxDoc() + " != SegmentInfos.docCount " + info.info.getDocCount());
+        if (reader.maxDoc() != info.info.maxDoc()) {
+          throw new RuntimeException("SegmentReader.maxDoc() " + reader.maxDoc() + " != SegmentInfo.maxDoc " + info.info.maxDoc());
         }
         
         final int numDocs = reader.numDocs();
@@ -649,18 +648,18 @@
         toLoseDocCount = numDocs;
         
         if (reader.hasDeletions()) {
-          if (reader.numDocs() != info.info.getDocCount() - info.getDelCount()) {
-            throw new RuntimeException("delete count mismatch: info=" + (info.info.getDocCount() - info.getDelCount()) + " vs reader=" + reader.numDocs());
+          if (reader.numDocs() != info.info.maxDoc() - info.getDelCount()) {
+            throw new RuntimeException("delete count mismatch: info=" + (info.info.maxDoc() - info.getDelCount()) + " vs reader=" + reader.numDocs());
           }
-          if ((info.info.getDocCount() - reader.numDocs()) > reader.maxDoc()) {
-            throw new RuntimeException("too many deleted docs: maxDoc()=" + reader.maxDoc() + " vs del count=" + (info.info.getDocCount() - reader.numDocs()));
+          if ((info.info.maxDoc() - reader.numDocs()) > reader.maxDoc()) {
+            throw new RuntimeException("too many deleted docs: maxDoc()=" + reader.maxDoc() + " vs del count=" + (info.info.maxDoc() - reader.numDocs()));
           }
-          if (info.info.getDocCount() - reader.numDocs() != info.getDelCount()) {
-            throw new RuntimeException("delete count mismatch: info=" + info.getDelCount() + " vs reader=" + (info.info.getDocCount() - reader.numDocs()));
+          if (info.info.maxDoc() - reader.numDocs() != info.getDelCount()) {
+            throw new RuntimeException("delete count mismatch: info=" + info.getDelCount() + " vs reader=" + (info.info.maxDoc() - reader.numDocs()));
           }
         } else {
           if (info.getDelCount() != 0) {
-            throw new RuntimeException("delete count mismatch: info=" + info.getDelCount() + " vs reader=" + (info.info.getDocCount() - reader.numDocs()));
+            throw new RuntimeException("delete count mismatch: info=" + info.getDelCount() + " vs reader=" + (info.info.maxDoc() - reader.numDocs()));
           }
         }
         
Index: lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java	(working copy)
@@ -88,14 +88,14 @@
     // NOTE: caller (DocumentsWriterPerThread) handles
     // aborting on any exception from this method
 
-    int numDocs = state.segmentInfo.getDocCount();
+    int maxDoc = state.segmentInfo.maxDoc();
     writeNorms(state);
     writeDocValues(state);
     
     // it's possible all docs hit non-aborting exceptions...
     initStoredFieldsWriter();
-    fillStoredFields(numDocs);
-    storedFieldsWriter.finish(state.fieldInfos, numDocs);
+    fillStoredFields(maxDoc);
+    storedFieldsWriter.finish(state.fieldInfos, maxDoc);
     storedFieldsWriter.close();
 
     Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();
@@ -120,7 +120,7 @@
 
   /** Writes all buffered doc values (called from {@link #flush}). */
   private void writeDocValues(SegmentWriteState state) throws IOException {
-    int docCount = state.segmentInfo.getDocCount();
+    int maxDoc = state.segmentInfo.maxDoc();
     DocValuesConsumer dvConsumer = null;
     boolean success = false;
     try {
@@ -138,7 +138,7 @@
               dvConsumer = fmt.fieldsConsumer(state);
             }
 
-            perField.docValuesWriter.finish(docCount);
+            perField.docValuesWriter.finish(maxDoc);
             perField.docValuesWriter.flush(state, dvConsumer);
             perField.docValuesWriter = null;
           } else if (perField.fieldInfo.getDocValuesType() != DocValuesType.NONE) {
@@ -200,7 +200,7 @@
           // changed for this field since the first time we added it.
           if (fi.omitsNorms() == false && fi.getIndexOptions() != IndexOptions.NONE) {
             assert perField.norms != null: "field=" + fi.name;
-            perField.norms.finish(state.segmentInfo.getDocCount());
+            perField.norms.finish(state.segmentInfo.maxDoc());
             perField.norms.flush(state, normsConsumer);
           }
         }
Index: lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java	(working copy)
@@ -387,7 +387,7 @@
   FlushedSegment flush() throws IOException, AbortingException {
     assert numDocsInRAM > 0;
     assert deleteSlice.isEmpty() : "all deletes must be applied in prepareFlush";
-    segmentInfo.setDocCount(numDocsInRAM);
+    segmentInfo.setMaxDoc(numDocsInRAM);
     final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segmentInfo, fieldInfos.finish(),
         pendingUpdates, new IOContext(new FlushInfo(numDocsInRAM, bytesUsed())));
     final double startMBUsed = bytesUsed() / 1024. / 1024.;
@@ -447,7 +447,7 @@
         infoStream.message("DWPT", "flushed: segment=" + segmentInfo.name + 
                 " ramUsed=" + nf.format(startMBUsed) + " MB" +
                 " newFlushedSize=" + nf.format(newSegmentSize) + " MB" +
-                " docs/MB=" + nf.format(flushState.segmentInfo.getDocCount() / newSegmentSize));
+                " docs/MB=" + nf.format(flushState.segmentInfo.maxDoc() / newSegmentSize));
       }
 
       assert segmentInfo != null;
@@ -479,7 +479,7 @@
 
     IndexWriter.setDiagnostics(newSegment.info, IndexWriter.SOURCE_FLUSH);
     
-    IOContext context = new IOContext(new FlushInfo(newSegment.info.getDocCount(), newSegment.sizeInBytes()));
+    IOContext context = new IOContext(new FlushInfo(newSegment.info.maxDoc(), newSegment.sizeInBytes()));
 
     boolean success = false;
     try {
Index: lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter.java	(working copy)
@@ -63,7 +63,7 @@
             int doc = postingsEnum.nextDoc();
             if (doc < delDocLimit) {
               if (state.liveDocs == null) {
-                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());
+                state.liveDocs = state.segmentInfo.getCodec().liveDocsFormat().newLiveDocs(state.segmentInfo.maxDoc());
               }
               if (state.liveDocs.get(doc)) {
                 state.delCountOnFlush++;
Index: lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/IndexWriter.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -839,7 +839,7 @@
       }
 
       rollbackSegments = segmentInfos.createBackupSegmentInfos();
-      pendingNumDocs.set(segmentInfos.totalDocCount());
+      pendingNumDocs.set(segmentInfos.totalMaxDoc());
 
       // start with previous field numbers, but new FieldInfos
       globalFieldNumberMap = getFieldNumberMap();
@@ -1047,7 +1047,7 @@
    *  @see #numDocs */
   public synchronized int maxDoc() {
     ensureOpen();
-    return docWriter.getNumDocs() + segmentInfos.totalDocCount();
+    return docWriter.getNumDocs() + segmentInfos.totalMaxDoc();
   }
 
   /** Returns total number of docs in this index, including
@@ -1060,7 +1060,7 @@
     ensureOpen();
     int count = docWriter.getNumDocs();
     for (final SegmentCommitInfo info : segmentInfos) {
-      count += info.info.getDocCount() - numDeletedDocs(info);
+      count += info.info.maxDoc() - numDeletedDocs(info);
     }
     return count;
   }
@@ -1255,7 +1255,7 @@
           rld.initWritableLiveDocs();
           if (rld.delete(docID)) {
             final int fullDelCount = rld.info.getDelCount() + rld.getPendingDeleteCount();
-            if (fullDelCount == rld.info.info.getDocCount()) {
+            if (fullDelCount == rld.info.info.maxDoc()) {
               // If a merge has already registered for this
               // segment, we leave it in the readerPool; the
               // merge will skip merging it and will then drop
@@ -1489,9 +1489,9 @@
   }
 
   // for test purpose
-  final synchronized int getDocCount(int i) {
+  final synchronized int maxDoc(int i) {
     if (i >= 0 && i < segmentInfos.size()) {
-      return segmentInfos.info(i).info.getDocCount();
+      return segmentInfos.info(i).info.maxDoc();
     } else {
       return -1;
     }
@@ -2065,7 +2065,7 @@
             // Abort any running merges
             abortMerges();
             // Remove all segments
-            pendingNumDocs.addAndGet(-segmentInfos.totalDocCount());
+            pendingNumDocs.addAndGet(-segmentInfos.totalMaxDoc());
             segmentInfos.clear();
             // Ask deleter to locate unreferenced files & remove them:
             deleter.checkpoint(segmentInfos, false);
@@ -2344,7 +2344,7 @@
       List<SegmentCommitInfo> infos = new ArrayList<>();
 
       // long so we can detect int overflow:
-      long totalDocCount = 0;
+      long totalMaxDoc = 0;
       List<SegmentInfos> commits = new ArrayList<>(dirs.length);
       for (Directory dir : dirs) {
         if (infoStream.isEnabled("IW")) {
@@ -2351,12 +2351,12 @@
           infoStream.message("IW", "addIndexes: process directory " + dir);
         }
         SegmentInfos sis = SegmentInfos.readLatestCommit(dir); // read infos from dir
-        totalDocCount += sis.totalDocCount();
+        totalMaxDoc += sis.totalMaxDoc();
         commits.add(sis);
       }
 
       // Best-effort up front check:
-      testReserveDocs(totalDocCount);
+      testReserveDocs(totalMaxDoc);
         
       boolean success = false;
       try {
@@ -2370,7 +2370,7 @@
               infoStream.message("IW", "addIndexes: process segment origName=" + info.info.name + " newName=" + newSegName + " info=" + info);
             }
 
-            IOContext context = new IOContext(new FlushInfo(info.info.getDocCount(), info.sizeInBytes()));
+            IOContext context = new IOContext(new FlushInfo(info.info.maxDoc(), info.sizeInBytes()));
 
             FieldInfos fis = readFieldInfos(info);
             for(FieldInfo fi : fis) {
@@ -2394,7 +2394,7 @@
           ensureOpen();
 
           // Now reserve the docs, just before we update SIS:
-          reserveDocs(totalDocCount);
+          reserveDocs(totalMaxDoc);
 
           success = true;
         } finally {
@@ -2586,7 +2586,7 @@
     
     //System.out.println("copy seg=" + info.info.name + " version=" + info.info.getVersion());
     // Same SI as before but we change directory and name
-    SegmentInfo newInfo = new SegmentInfo(directory, info.info.getVersion(), segName, info.info.getDocCount(),
+    SegmentInfo newInfo = new SegmentInfo(directory, info.info.getVersion(), segName, info.info.maxDoc(),
                                           info.info.getUseCompoundFile(), info.info.getCodec(), 
                                           info.info.getDiagnostics(), info.info.getId(), info.info.getAttributes());
     SegmentCommitInfo newInfoPerCommit = new SegmentCommitInfo(newInfo, info.getDelCount(), info.getDelGen(), 
@@ -3013,7 +3013,7 @@
     flushDeletesCount.incrementAndGet();
     final BufferedUpdatesStream.ApplyDeletesResult result;
     if (infoStream.isEnabled("IW")) {
-      infoStream.message("IW", "now apply all deletes for all segments maxDoc=" + (docWriter.getNumDocs() + segmentInfos.totalDocCount()));
+      infoStream.message("IW", "now apply all deletes for all segments maxDoc=" + (docWriter.getNumDocs() + segmentInfos.totalMaxDoc()));
     }
     result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, segmentInfos.asList());
     if (result.anyDeletes) {
@@ -3030,7 +3030,7 @@
         // it once it's done:
         if (!mergingSegments.contains(info)) {
           segmentInfos.remove(info);
-          pendingNumDocs.addAndGet(-info.info.getDocCount());
+          pendingNumDocs.addAndGet(-info.info.maxDoc());
           readerPool.drop(info);
         }
       }
@@ -3083,7 +3083,7 @@
       if (mergedDeletesAndUpdates == null) {
         mergedDeletesAndUpdates = readerPool.get(merge.info, true);
         docMap = merge.getDocMap(mergeState);
-        assert docMap.isConsistent(merge.info.info.getDocCount());
+        assert docMap.isConsistent(merge.info.info.maxDoc());
       }
       if (initWritableLiveDocs && !initializedWritableLiveDocs) {
         mergedDeletesAndUpdates.initWritableLiveDocs();
@@ -3147,7 +3147,7 @@
     for (int i = 0; i < sourceSegments.size(); i++) {
       SegmentCommitInfo info = sourceSegments.get(i);
       minGen = Math.min(info.getBufferedDeletesGen(), minGen);
-      final int docCount = info.info.getDocCount();
+      final int maxDoc = info.info.maxDoc();
       final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();
       final ReadersAndUpdates rld = readerPool.get(info, false);
       // We hold a ref so it should still be in the pool:
@@ -3172,7 +3172,7 @@
           mergingFields[idx] = field;
           dvFieldUpdates[idx] = mergedDVUpdates.getUpdates(field, updates.type);
           if (dvFieldUpdates[idx] == null) {
-            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.getDocCount());
+            dvFieldUpdates[idx] = mergedDVUpdates.newUpdates(field, updates.type, mergeState.segmentInfo.maxDoc());
           }
           updatesIters[idx] = updates.iterator();
           updatesIters[idx].nextDoc(); // advance to first update doc
@@ -3186,8 +3186,8 @@
         // If we had deletions on starting the merge we must
         // still have deletions now:
         assert currentLiveDocs != null;
-        assert prevLiveDocs.length() == docCount;
-        assert currentLiveDocs.length() == docCount;
+        assert prevLiveDocs.length() == maxDoc;
+        assert currentLiveDocs.length() == maxDoc;
 
         // There were deletes on this segment when the merge
         // started.  The merge has collapsed away those
@@ -3205,7 +3205,7 @@
           // This means this segment received new deletes
           // since we started the merge, so we
           // must merge them:
-          for (int j = 0; j < docCount; j++) {
+          for (int j = 0; j < maxDoc; j++) {
             if (!prevLiveDocs.get(j)) {
               assert !currentLiveDocs.get(j);
             } else {
@@ -3225,7 +3225,7 @@
           }
         } else if (mergingFields != null) {
           // need to check each non-deleted document if it has any updates
-          for (int j = 0; j < docCount; j++) {
+          for (int j = 0; j < maxDoc; j++) {
             if (prevLiveDocs.get(j)) {
               // document isn't deleted, check if any of the fields have an update to it
               maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);
@@ -3237,13 +3237,13 @@
             }
           }
         } else {
-          docUpto += info.info.getDocCount() - info.getDelCount() - rld.getPendingDeleteCount();
+          docUpto += info.info.maxDoc() - info.getDelCount() - rld.getPendingDeleteCount();
         }
       } else if (currentLiveDocs != null) {
-        assert currentLiveDocs.length() == docCount;
+        assert currentLiveDocs.length() == maxDoc;
         // This segment had no deletes before but now it
         // does:
-        for (int j = 0; j < docCount; j++) {
+        for (int j = 0; j < maxDoc; j++) {
           if (!currentLiveDocs.get(j)) {
             if (holder.mergedDeletesAndUpdates == null || !holder.initializedWritableLiveDocs) {
               holder.init(readerPool, merge, mergeState, true);
@@ -3259,7 +3259,7 @@
         }
       } else if (mergingFields != null) {
         // no deletions before or after, but there were updates
-        for (int j = 0; j < docCount; j++) {
+        for (int j = 0; j < maxDoc; j++) {
           maybeApplyMergedDVUpdates(merge, mergeState, docUpto, holder, mergingFields, dvFieldUpdates, updatesIters, j);
           // advance docUpto for every non-deleted document
           docUpto++;
@@ -3266,11 +3266,11 @@
         }
       } else {
         // No deletes or updates before or after
-        docUpto += info.info.getDocCount();
+        docUpto += info.info.maxDoc();
       }
     }
 
-    assert docUpto == merge.info.info.getDocCount();
+    assert docUpto == merge.info.info.maxDoc();
 
     if (mergedDVUpdates.any()) {
 //      System.out.println("[" + Thread.currentThread().getName() + "] IW.commitMergedDeletes: mergedDeletes.info=" + mergedDeletes.info + ", mergedFieldUpdates=" + mergedFieldUpdates);
@@ -3348,7 +3348,7 @@
       return false;
     }
 
-    final ReadersAndUpdates mergedUpdates = merge.info.info.getDocCount() == 0 ? null : commitMergedDeletesAndUpdates(merge, mergeState);
+    final ReadersAndUpdates mergedUpdates = merge.info.info.maxDoc() == 0 ? null : commitMergedDeletesAndUpdates(merge, mergeState);
 //    System.out.println("[" + Thread.currentThread().getName() + "] IW.commitMerge: mergedDeletes=" + mergedDeletes);
 
     // If the doc store we are using has been closed and
@@ -3359,9 +3359,9 @@
     assert !segmentInfos.contains(merge.info);
 
     final boolean allDeleted = merge.segments.size() == 0 ||
-      merge.info.info.getDocCount() == 0 ||
+      merge.info.info.maxDoc() == 0 ||
       (mergedUpdates != null &&
-       mergedUpdates.getPendingDeleteCount() == merge.info.info.getDocCount());
+       mergedUpdates.getPendingDeleteCount() == merge.info.info.maxDoc());
 
     if (infoStream.isEnabled("IW")) {
       if (allDeleted) {
@@ -3375,7 +3375,7 @@
     // the new segment:
     assert merge.segments.size() > 0 || dropSegment;
 
-    assert merge.info.info.getDocCount() != 0 || keepFullyDeletedSegments || dropSegment;
+    assert merge.info.info.maxDoc() != 0 || keepFullyDeletedSegments || dropSegment;
 
     if (mergedUpdates != null) {
       boolean success = false;
@@ -3404,7 +3404,7 @@
 
     // Now deduct the deleted docs that we just reclaimed from this
     // merge:
-    int delDocCount = merge.totalDocCount - merge.info.info.getDocCount();
+    int delDocCount = merge.totalMaxDoc - merge.info.info.maxDoc();
     assert delDocCount >= 0;
     pendingNumDocs.addAndGet(-delDocCount);
 
@@ -3538,7 +3538,7 @@
     }
     if (merge.info != null && merge.rateLimiter.getAbort() == false) {
       if (infoStream.isEnabled("IW")) {
-        infoStream.message("IW", "merge time " + (System.currentTimeMillis()-t0) + " msec for " + merge.info.info.getDocCount() + " docs");
+        infoStream.message("IW", "merge time " + (System.currentTimeMillis()-t0) + " msec for " + merge.info.info.maxDoc() + " docs");
       }
     }
   }
@@ -3624,10 +3624,10 @@
     assert merge.estimatedMergeBytes == 0;
     assert merge.totalMergeBytes == 0;
     for(SegmentCommitInfo info : merge.segments) {
-      if (info.info.getDocCount() > 0) {
+      if (info.info.maxDoc() > 0) {
         final int delCount = numDeletedDocs(info);
-        assert delCount <= info.info.getDocCount();
-        final double delRatio = ((double) delCount)/info.info.getDocCount();
+        assert delCount <= info.info.maxDoc();
+        final double delRatio = ((double) delCount)/info.info.maxDoc();
         merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);
         merge.totalMergeBytes += info.sizeInBytes();
       }
@@ -3698,7 +3698,7 @@
       }
       for(SegmentCommitInfo info : result.allDeleted) {
         segmentInfos.remove(info);
-        pendingNumDocs.addAndGet(-info.info.getDocCount());
+        pendingNumDocs.addAndGet(-info.info.maxDoc());
         if (merge.segments.contains(info)) {
           mergingSegments.remove(info);
           merge.segments.remove(info);
@@ -3887,7 +3887,7 @@
           synchronized (this) {
             // We must also sync on IW here, because another thread could be writing
             // new DV updates / remove old gen field infos files causing FNFE:
-            newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);
+            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);
           }
 
           boolean released = false;
@@ -3904,7 +3904,7 @@
         }
 
         merge.readers.add(reader);
-        assert delCount <= info.info.getDocCount(): "delCount=" + delCount + " info.docCount=" + info.info.getDocCount() + " rld.pendingDeleteCount=" + rld.getPendingDeleteCount() + " info.getDelCount()=" + info.getDelCount();
+        assert delCount <= info.info.maxDoc(): "delCount=" + delCount + " info.maxDoc=" + info.info.maxDoc() + " rld.pendingDeleteCount=" + rld.getPendingDeleteCount() + " info.getDelCount()=" + info.getDelCount();
         segUpto++;
       }
 
@@ -3946,7 +3946,7 @@
           double segmentMB = (merge.info.sizeInBytes()/1024./1024.);
           double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;
           double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;
-          infoStream.message("IW", "merge codec=" + codec + " docCount=" + merge.info.info.getDocCount() + "; merged segment has " +
+          infoStream.message("IW", "merge codec=" + codec + " maxDoc=" + merge.info.info.maxDoc() + "; merged segment has " +
                              (mergeState.mergeFieldInfos.hasVectors() ? "vectors" : "no vectors") + "; " +
                              (mergeState.mergeFieldInfos.hasNorms() ? "norms" : "no norms") + "; " + 
                              (mergeState.mergeFieldInfos.hasDocValues() ? "docValues" : "no docValues") + "; " + 
@@ -3966,12 +3966,12 @@
 
       if (merger.shouldMerge() == false) {
         // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we "merged":
-        assert merge.info.info.getDocCount() == 0;
+        assert merge.info.info.maxDoc() == 0;
         commitMerge(merge, mergeState);
         return 0;
       }
 
-      assert merge.info.info.getDocCount() > 0;
+      assert merge.info.info.maxDoc() > 0;
 
       // Very important to do this before opening the reader
       // because codec must know if prox was written for
@@ -4103,7 +4103,7 @@
       }
     }
 
-    return merge.info.info.getDocCount();
+    return merge.info.info.maxDoc();
   }
 
   synchronized void addMergeException(MergePolicy.OneMerge merge) {
Index: lucene/core/src/java/org/apache/lucene/index/LogMergePolicy.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/LogMergePolicy.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/LogMergePolicy.java	(working copy)
@@ -152,10 +152,10 @@
   protected long sizeDocs(SegmentCommitInfo info, IndexWriter writer) throws IOException {
     if (calibrateSizeByDeletes) {
       int delCount = writer.numDeletedDocs(info);
-      assert delCount <= info.info.getDocCount();
-      return (info.info.getDocCount() - (long)delCount);
+      assert delCount <= info.info.maxDoc();
+      return (info.info.maxDoc() - (long)delCount);
     } else {
-      return info.info.getDocCount();
+      return info.info.maxDoc();
     }
   }
 
Index: lucene/core/src/java/org/apache/lucene/index/MergePolicy.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/MergePolicy.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/MergePolicy.java	(working copy)
@@ -114,7 +114,7 @@
     volatile long mergeStartNS = -1;
 
     /** Total number of documents in segments to be merged, not accounting for deletions. */
-    public final int totalDocCount;
+    public final int totalMaxDoc;
     Throwable error;
 
     /** Sole constructor.
@@ -128,9 +128,9 @@
       this.segments = new ArrayList<>(segments);
       int count = 0;
       for(SegmentCommitInfo info : segments) {
-        count += info.info.getDocCount();
+        count += info.info.maxDoc();
       }
-      totalDocCount = count;
+      totalMaxDoc = count;
 
       rateLimiter = new MergeRateLimiter(this);
     }
@@ -232,7 +232,7 @@
     public int totalNumDocs() throws IOException {
       int total = 0;
       for (SegmentCommitInfo info : segments) {
-        total += info.info.getDocCount();
+        total += info.info.maxDoc();
       }
       return total;
     }
@@ -239,7 +239,7 @@
 
     /** Return {@link MergeInfo} describing this merge. */
     public MergeInfo getMergeInfo() {
-      return new MergeInfo(totalDocCount, estimatedMergeBytes, isExternal, maxNumSegments);
+      return new MergeInfo(totalMaxDoc, estimatedMergeBytes, isExternal, maxNumSegments);
     }    
   }
 
@@ -440,9 +440,9 @@
   protected long size(SegmentCommitInfo info, IndexWriter writer) throws IOException {
     long byteSize = info.sizeInBytes();
     int delCount = writer.numDeletedDocs(info);
-    double delRatio = info.info.getDocCount() <= 0 ? 0.0f : (float) delCount / (float) info.info.getDocCount();
+    double delRatio = info.info.maxDoc() <= 0 ? 0.0f : (float) delCount / (float) info.info.maxDoc();
     assert delRatio <= 1.0;
-    return (info.info.getDocCount() <= 0 ? byteSize : (long) (byteSize * (1.0 - delRatio)));
+    return (info.info.maxDoc() <= 0 ? byteSize : (long) (byteSize * (1.0 - delRatio)));
   }
   
   /** Returns true if this single info is already fully merged (has no
Index: lucene/core/src/java/org/apache/lucene/index/MergeState.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/MergeState.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/MergeState.java	(working copy)
@@ -139,7 +139,7 @@
       docBase += docMap.numDocs();
     }
 
-    segmentInfo.setDocCount(docBase);
+    segmentInfo.setMaxDoc(docBase);
   }
 
   /**
Index: lucene/core/src/java/org/apache/lucene/index/NormValuesWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/NormValuesWriter.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/NormValuesWriter.java	(working copy)
@@ -66,7 +66,7 @@
 
   public void flush(SegmentWriteState state, NormsConsumer normsConsumer) throws IOException {
 
-    final int maxDoc = state.segmentInfo.getDocCount();
+    final int maxDoc = state.segmentInfo.maxDoc();
     final PackedLongValues values = pending.build();
 
     normsConsumer.addNormsField(fieldInfo,
Index: lucene/core/src/java/org/apache/lucene/index/NumericDocValuesWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/NumericDocValuesWriter.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/NumericDocValuesWriter.java	(working copy)
@@ -84,7 +84,7 @@
   @Override
   public void flush(SegmentWriteState state, DocValuesConsumer dvConsumer) throws IOException {
 
-    final int maxDoc = state.segmentInfo.getDocCount();
+    final int maxDoc = state.segmentInfo.maxDoc();
     final PackedLongValues values = pending.build();
 
     dvConsumer.addNumericField(fieldInfo,
Index: lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates.java	(working copy)
@@ -31,8 +31,6 @@
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.document.BinaryDocValuesField;
-import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.FlushInfo;
 import org.apache.lucene.store.IOContext;
@@ -114,16 +112,16 @@
     int count;
     if (liveDocs != null) {
       count = 0;
-      for(int docID=0;docID<info.info.getDocCount();docID++) {
+      for(int docID=0;docID<info.info.maxDoc();docID++) {
         if (liveDocs.get(docID)) {
           count++;
         }
       }
     } else {
-      count = info.info.getDocCount();
+      count = info.info.maxDoc();
     }
 
-    assert info.info.getDocCount() - info.getDelCount() - pendingDeleteCount == count: "info.docCount=" + info.info.getDocCount() + " info.getDelCount()=" + info.getDelCount() + " pendingDeleteCount=" + pendingDeleteCount + " count=" + count;
+    assert info.info.maxDoc() - info.getDelCount() - pendingDeleteCount == count: "info.maxDoc=" + info.info.maxDoc() + " info.getDelCount()=" + info.getDelCount() + " pendingDeleteCount=" + pendingDeleteCount + " count=" + count;
     return true;
   }
 
@@ -150,13 +148,13 @@
   public synchronized boolean delete(int docID) {
     assert liveDocs != null;
     assert Thread.holdsLock(writer);
-    assert docID >= 0 && docID < liveDocs.length() : "out of bounds: docid=" + docID + " liveDocsLength=" + liveDocs.length() + " seg=" + info.info.name + " docCount=" + info.info.getDocCount();
+    assert docID >= 0 && docID < liveDocs.length() : "out of bounds: docid=" + docID + " liveDocsLength=" + liveDocs.length() + " seg=" + info.info.name + " maxDoc=" + info.info.maxDoc();
     assert !liveDocsShared;
     final boolean didDelete = liveDocs.get(docID);
     if (didDelete) {
       ((MutableBits) liveDocs).clear(docID);
       pendingDeleteCount++;
-      //System.out.println("  new del seg=" + info + " docID=" + docID + " pendingDelCount=" + pendingDeleteCount + " totDelCount=" + (info.docCount-liveDocs.count()));
+      //System.out.println("  new del seg=" + info + " docID=" + docID + " pendingDelCount=" + pendingDeleteCount + " totDelCount=" + (info.info.maxDoc()-liveDocs.count()));
     }
     return didDelete;
   }
@@ -189,7 +187,7 @@
     // force new liveDocs in initWritableLiveDocs even if it's null
     liveDocsShared = true;
     if (liveDocs != null) {
-      return new SegmentReader(reader.getSegmentInfo(), reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);
+      return new SegmentReader(reader.getSegmentInfo(), reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);
     } else {
       // liveDocs == null and reader != null. That can only be if there are no deletes
       assert reader.getLiveDocs() == null;
@@ -200,7 +198,7 @@
 
   public synchronized void initWritableLiveDocs() throws IOException {
     assert Thread.holdsLock(writer);
-    assert info.info.getDocCount() > 0;
+    assert info.info.maxDoc() > 0;
     //System.out.println("initWritableLivedocs seg=" + info + " liveDocs=" + liveDocs + " shared=" + shared);
     if (liveDocsShared) {
       // Copy on write: this means we've cloned a
@@ -210,7 +208,7 @@
       LiveDocsFormat liveDocsFormat = info.info.getCodec().liveDocsFormat();
       if (liveDocs == null) {
         //System.out.println("create BV seg=" + info);
-        liveDocs = liveDocsFormat.newLiveDocs(info.info.getDocCount());
+        liveDocs = liveDocsFormat.newLiveDocs(info.info.maxDoc());
       } else {
         liveDocs = liveDocsFormat.newLiveDocs(liveDocs);
       }
@@ -256,7 +254,7 @@
     }
     
     // We have new deletes
-    assert liveDocs.length() == info.info.getDocCount();
+    assert liveDocs.length() == info.info.maxDoc();
     
     // Do this so we can delete any created files on
     // exception; this saves all codecs from having to do
@@ -303,8 +301,8 @@
 
       final long nextDocValuesGen = info.getNextDocValuesGen();
       final String segmentSuffix = Long.toString(nextDocValuesGen, Character.MAX_RADIX);
-      final long estUpdatesSize = fieldUpdates.ramBytesPerDoc() * info.info.getDocCount();
-      final IOContext updatesContext = new IOContext(new FlushInfo(info.info.getDocCount(), estUpdatesSize));
+      final long estUpdatesSize = fieldUpdates.ramBytesPerDoc() * info.info.maxDoc();
+      final IOContext updatesContext = new IOContext(new FlushInfo(info.info.maxDoc(), estUpdatesSize));
       final FieldInfo fieldInfo = infos.fieldInfo(field);
       assert fieldInfo != null;
       fieldInfo.setDocValuesGen(nextDocValuesGen);
@@ -376,8 +374,8 @@
 
       final long nextDocValuesGen = info.getNextDocValuesGen();
       final String segmentSuffix = Long.toString(nextDocValuesGen, Character.MAX_RADIX);
-      final long estUpdatesSize = fieldUpdates.ramBytesPerDoc() * info.info.getDocCount();
-      final IOContext updatesContext = new IOContext(new FlushInfo(info.info.getDocCount(), estUpdatesSize));
+      final long estUpdatesSize = fieldUpdates.ramBytesPerDoc() * info.info.maxDoc();
+      final IOContext updatesContext = new IOContext(new FlushInfo(info.info.maxDoc(), estUpdatesSize));
       final FieldInfo fieldInfo = infos.fieldInfo(field);
       assert fieldInfo != null;
       fieldInfo.setDocValuesGen(nextDocValuesGen);
@@ -448,7 +446,7 @@
     // HEADER + FOOTER: 40
     // 90 bytes per-field (over estimating long name and attributes map)
     final long estInfosSize = 40 + 90 * fieldInfos.size();
-    final IOContext infosContext = new IOContext(new FlushInfo(info.info.getDocCount(), estInfosSize));
+    final IOContext infosContext = new IOContext(new FlushInfo(info.info.maxDoc(), estInfosSize));
     // separately also track which files were created for this gen
     final TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(dir);
     infosFormat.write(trackingDir, info.info, segmentSuffix, fieldInfos, infosContext);
@@ -579,7 +577,7 @@
 
     // if there is a reader open, reopen it to reflect the updates
     if (reader != null) {
-      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);
+      SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - info.getDelCount() - pendingDeleteCount);
       boolean reopened = false;
       try {
         reader.decRef();
Index: lucene/core/src/java/org/apache/lucene/index/SegmentCommitInfo.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentCommitInfo.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentCommitInfo.java	(working copy)
@@ -26,8 +26,6 @@
 import java.util.Map.Entry;
 import java.util.Set;
 
-import org.apache.lucene.store.Directory;
-
 /** Embeds a [read-only] SegmentInfo and adds per-commit
  *  fields.
  *
@@ -312,8 +310,8 @@
   }
 
   void setDelCount(int delCount) {
-    if (delCount < 0 || delCount > info.getDocCount()) {
-      throw new IllegalArgumentException("invalid delCount=" + delCount + " (docCount=" + info.getDocCount() + ")");
+    if (delCount < 0 || delCount > info.maxDoc()) {
+      throw new IllegalArgumentException("invalid delCount=" + delCount + " (maxDoc=" + info.maxDoc() + ")");
     }
     this.delCount = delCount;
   }
Index: lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java	(working copy)
@@ -54,7 +54,7 @@
   /** Unique segment name in the directory. */
   public final String name;
 
-  private int docCount;         // number of docs in seg
+  private int maxDoc;         // number of docs in seg
 
   /** Where this segment resides. */
   public final Directory dir;
@@ -92,7 +92,7 @@
    * <p>Note: this is public only to allow access from
    * the codecs package.</p>
    */
-  public SegmentInfo(Directory dir, Version version, String name, int docCount,
+  public SegmentInfo(Directory dir, Version version, String name, int maxDoc,
                      boolean isCompoundFile, Codec codec, Map<String,String> diagnostics,
                      byte[] id, Map<String,String> attributes) {
     assert !(dir instanceof TrackingDirectoryWrapper);
@@ -99,7 +99,7 @@
     this.dir = dir;
     this.version = version;
     this.name = name;
-    this.docCount = docCount;
+    this.maxDoc = maxDoc;
     this.isCompoundFile = isCompoundFile;
     this.codec = codec;
     this.diagnostics = diagnostics;
@@ -144,19 +144,19 @@
 
   /** Returns number of documents in this segment (deletions
    *  are not taken into account). */
-  public int getDocCount() {
-    if (this.docCount == -1) {
-      throw new IllegalStateException("docCount isn't set yet");
+  public int maxDoc() {
+    if (this.maxDoc == -1) {
+      throw new IllegalStateException("maxDoc isn't set yet");
     }
-    return docCount;
+    return maxDoc;
   }
 
   // NOTE: leave package private
-  void setDocCount(int docCount) {
-    if (this.docCount != -1) {
-      throw new IllegalStateException("docCount was already set: this.docCount=" + this.docCount + " vs docCount=" + docCount);
+  void setMaxDoc(int maxDoc) {
+    if (this.maxDoc != -1) {
+      throw new IllegalStateException("maxDoc was already set: this.maxDoc=" + this.maxDoc + " vs maxDoc=" + maxDoc);
     }
-    this.docCount = docCount;
+    this.maxDoc = maxDoc;
   }
 
   /** Return all files referenced by this SegmentInfo. */
@@ -188,7 +188,7 @@
     char cfs = getUseCompoundFile() ? 'c' : 'C';
     s.append(cfs);
 
-    s.append(docCount);
+    s.append(maxDoc);
 
     if (delCount != 0) {
       s.append('/').append(delCount);
Index: lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java	(working copy)
@@ -298,11 +298,11 @@
         Codec codec = Codec.forName(input.readString());
         SegmentInfo info = codec.segmentInfoFormat().read(directory, segName, segmentID, IOContext.READ);
         info.setCodec(codec);
-        totalDocs += info.getDocCount();
+        totalDocs += info.maxDoc();
         long delGen = input.readLong();
         int delCount = input.readInt();
-        if (delCount < 0 || delCount > info.getDocCount()) {
-          throw new CorruptIndexException("invalid deletion count: " + delCount + " vs docCount=" + info.getDocCount(), input);
+        if (delCount < 0 || delCount > info.maxDoc()) {
+          throw new CorruptIndexException("invalid deletion count: " + delCount + " vs maxDoc=" + info.maxDoc(), input);
         }
         long fieldInfosGen = input.readLong();
         long dvGen = input.readLong();
@@ -386,8 +386,8 @@
         segnOutput.writeString(si.getCodec().getName());
         segnOutput.writeLong(siPerCommit.getDelGen());
         int delCount = siPerCommit.getDelCount();
-        if (delCount < 0 || delCount > si.getDocCount()) {
-          throw new IllegalStateException("cannot write segment: invalid docCount segment=" + si.name + " docCount=" + si.getDocCount() + " delCount=" + delCount);
+        if (delCount < 0 || delCount > si.maxDoc()) {
+          throw new IllegalStateException("cannot write segment: invalid maxDoc segment=" + si.name + " maxDoc=" + si.maxDoc() + " delCount=" + delCount);
         }
         segnOutput.writeInt(delCount);
         segnOutput.writeLong(siPerCommit.getFieldInfosGen());
@@ -726,12 +726,12 @@
     lastGeneration = other.lastGeneration;
   }
 
-  /** Returns sum of all segment's docCounts.  Note that
+  /** Returns sum of all segment's maxDocs.  Note that
    *  this does not include deletions */
-  public int totalDocCount() {
+  public int totalMaxDoc() {
     long count = 0;
     for(SegmentCommitInfo info : this) {
-      count += info.info.getDocCount();
+      count += info.info.maxDoc();
     }
     // we should never hit this, checks should happen elsewhere...
     assert count <= IndexWriter.getActualMaxDocs();
Index: lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java	(working copy)
@@ -62,7 +62,7 @@
   
   /** True if any merging should happen */
   boolean shouldMerge() {
-    return mergeState.segmentInfo.getDocCount() > 0;
+    return mergeState.segmentInfo.maxDoc() > 0;
   }
 
   /**
@@ -85,7 +85,7 @@
       long t1 = System.nanoTime();
       mergeState.infoStream.message("SM", ((t1-t0)/1000000) + " msec to merge stored fields [" + numMerged + " docs]");
     }
-    assert numMerged == mergeState.segmentInfo.getDocCount(): "numMerged=" + numMerged + " vs mergeState.segmentInfo.getDocCount()=" + mergeState.segmentInfo.getDocCount();
+    assert numMerged == mergeState.segmentInfo.maxDoc(): "numMerged=" + numMerged + " vs mergeState.segmentInfo.maxDoc()=" + mergeState.segmentInfo.maxDoc();
 
     final SegmentWriteState segmentWriteState = new SegmentWriteState(mergeState.infoStream, directory, mergeState.segmentInfo,
                                                                       mergeState.mergeFieldInfos, null, context);
@@ -129,7 +129,7 @@
         long t1 = System.nanoTime();
         mergeState.infoStream.message("SM", ((t1-t0)/1000000) + " msec to merge vectors [" + numMerged + " docs]");
       }
-      assert numMerged == mergeState.segmentInfo.getDocCount();
+      assert numMerged == mergeState.segmentInfo.maxDoc();
     }
     
     // write the merged infos
Index: lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentReader.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentReader.java	(working copy)
@@ -43,9 +43,9 @@
   private final SegmentCommitInfo si;
   private final Bits liveDocs;
 
-  // Normally set to si.docCount - si.delDocCount, unless we
+  // Normally set to si.maxDoc - si.delDocCount, unless we
   // were created as an NRT reader from IW, in which case IW
-  // tells us the docCount:
+  // tells us the number of live docs:
   private final int numDocs;
 
   final SegmentCoreReaders core;
@@ -75,7 +75,7 @@
         assert si.getDelCount() == 0;
         liveDocs = null;
       }
-      numDocs = si.info.getDocCount() - si.getDelCount();
+      numDocs = si.info.maxDoc() - si.getDelCount();
       
       fieldInfos = initFieldInfos();
       docValuesProducer = initDocValuesProducer();
@@ -99,7 +99,7 @@
   SegmentReader(SegmentCommitInfo si, SegmentReader sr) throws IOException {
     this(si, sr,
          si.info.getCodec().liveDocsFormat().readLiveDocs(si.info.dir, si, IOContext.READONCE),
-         si.info.getDocCount() - si.getDelCount());
+         si.info.maxDoc() - si.getDelCount());
   }
 
   /** Create new SegmentReader sharing core from a previous
@@ -195,7 +195,7 @@
   @Override
   public int maxDoc() {
     // Don't call ensureOpen() here (it could affect performance)
-    return si.info.getDocCount();
+    return si.info.maxDoc();
   }
 
   @Override
@@ -232,7 +232,7 @@
   public String toString() {
     // SegmentInfo.toString takes dir and number of
     // *pending* deletions; so we reverse compute that here:
-    return si.toString(si.info.getDocCount() - numDocs - si.getDelCount());
+    return si.toString(si.info.maxDoc() - numDocs - si.getDelCount());
   }
   
   /**
Index: lucene/core/src/java/org/apache/lucene/index/SortedDocValuesWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SortedDocValuesWriter.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/SortedDocValuesWriter.java	(working copy)
@@ -108,7 +108,7 @@
 
   @Override
   public void flush(SegmentWriteState state, DocValuesConsumer dvConsumer) throws IOException {
-    final int maxDoc = state.segmentInfo.getDocCount();
+    final int maxDoc = state.segmentInfo.maxDoc();
 
     assert pending.size() == maxDoc;
     final int valueCount = hash.size();
Index: lucene/core/src/java/org/apache/lucene/index/SortedNumericDocValuesWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SortedNumericDocValuesWriter.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/SortedNumericDocValuesWriter.java	(working copy)
@@ -103,7 +103,7 @@
 
   @Override
   public void flush(SegmentWriteState state, DocValuesConsumer dvConsumer) throws IOException {
-    final int maxDoc = state.segmentInfo.getDocCount();
+    final int maxDoc = state.segmentInfo.maxDoc();
     assert pendingCounts.size() == maxDoc;
     final PackedLongValues values = pending.build();
     final PackedLongValues valueCounts = pendingCounts.build();
Index: lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesWriter.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesWriter.java	(working copy)
@@ -147,7 +147,7 @@
 
   @Override
   public void flush(SegmentWriteState state, DocValuesConsumer dvConsumer) throws IOException {
-    final int maxDoc = state.segmentInfo.getDocCount();
+    final int maxDoc = state.segmentInfo.maxDoc();
     final int maxCountPerDoc = maxCount;
     assert pendingCounts.size() == maxDoc;
     final int valueCount = hash.size();
Index: lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java	(working copy)
@@ -184,7 +184,7 @@
 
             // Make a best effort to detect when the app illegally "rm -rf" their
             // index while a reader was open, and then called openIfChanged:
-            boolean illegalDocCountChange = commitInfo.info.getDocCount() != oldReader.getSegmentInfo().info.getDocCount();
+            boolean illegalDocCountChange = commitInfo.info.maxDoc() != oldReader.getSegmentInfo().info.maxDoc();
             
             boolean hasNeitherDeletionsNorUpdates = commitInfo.hasDeletions()== false && commitInfo.hasFieldUpdates() == false;
 
Index: lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumer.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumer.java	(working copy)
@@ -56,7 +56,7 @@
   @Override
   void flush(Map<String, TermsHashPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {
     if (writer != null) {
-      int numDocs = state.segmentInfo.getDocCount();
+      int numDocs = state.segmentInfo.maxDoc();
       assert numDocs > 0;
       // At least one doc in this run had term vectors enabled
       try {
Index: lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java	(working copy)
@@ -585,7 +585,7 @@
     final List<SegmentCommitInfo> eligible = new ArrayList<>();
     final Collection<SegmentCommitInfo> merging = writer.getMergingSegments();
     for(SegmentCommitInfo info : infos) {
-      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.getDocCount();
+      double pctDeletes = 100.*((double) writer.numDeletedDocs(info))/info.info.maxDoc();
       if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {
         eligible.add(info);
       }
Index: lucene/core/src/java/org/apache/lucene/store/MergeInfo.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/store/MergeInfo.java	(revision 1662913)
+++ lucene/core/src/java/org/apache/lucene/store/MergeInfo.java	(working copy)
@@ -23,7 +23,7 @@
 
 public class MergeInfo {
   
-  public final int totalDocCount;
+  public final int totalMaxDoc;
   
   public final long estimatedMergeBytes;
   
@@ -40,8 +40,8 @@
    * 
    */
 
-  public MergeInfo(int totalDocCount, long estimatedMergeBytes, boolean isExternal, int mergeMaxNumSegments) {
-    this.totalDocCount = totalDocCount;
+  public MergeInfo(int totalMaxDoc, long estimatedMergeBytes, boolean isExternal, int mergeMaxNumSegments) {
+    this.totalMaxDoc = totalMaxDoc;
     this.estimatedMergeBytes = estimatedMergeBytes;
     this.isExternal = isExternal;
     this.mergeMaxNumSegments = mergeMaxNumSegments;
@@ -56,7 +56,7 @@
         + (int) (estimatedMergeBytes ^ (estimatedMergeBytes >>> 32));
     result = prime * result + (isExternal ? 1231 : 1237);
     result = prime * result + mergeMaxNumSegments;
-    result = prime * result + totalDocCount;
+    result = prime * result + totalMaxDoc;
     return result;
   }
 
@@ -75,7 +75,7 @@
       return false;
     if (mergeMaxNumSegments != other.mergeMaxNumSegments)
       return false;
-    if (totalDocCount != other.totalDocCount)
+    if (totalMaxDoc != other.totalMaxDoc)
       return false;
     return true;
   }
@@ -82,7 +82,7 @@
 
   @Override
   public String toString() {
-    return "MergeInfo [totalDocCount=" + totalDocCount
+    return "MergeInfo [totalMaxDoc=" + totalMaxDoc
         + ", estimatedMergeBytes=" + estimatedMergeBytes + ", isExternal="
         + isExternal + ", mergeMaxNumSegments=" + mergeMaxNumSegments + "]";
   }
Index: lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java	(revision 1662913)
+++ lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java	(working copy)
@@ -339,7 +339,7 @@
 
     writer.addIndexes(aux);
     assertEquals(1040, writer.maxDoc());
-    assertEquals(1000, writer.getDocCount(0));
+    assertEquals(1000, writer.maxDoc(0));
     writer.close();
 
     // make sure the index is correct
@@ -368,7 +368,7 @@
 
     writer.addIndexes(aux);
     assertEquals(1032, writer.maxDoc());
-    assertEquals(1000, writer.getDocCount(0));
+    assertEquals(1000, writer.maxDoc(0));
     writer.close();
 
     // make sure the index is correct
@@ -396,7 +396,7 @@
 
     writer.addIndexes(aux, new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(aux)));
     assertEquals(1060, writer.maxDoc());
-    assertEquals(1000, writer.getDocCount(0));
+    assertEquals(1000, writer.maxDoc(0));
     writer.close();
 
     // make sure the index is correct
@@ -438,7 +438,7 @@
     }
     writer.addIndexes(aux, new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(aux)));
     assertEquals(1020, writer.maxDoc());
-    assertEquals(1000, writer.getDocCount(0));
+    assertEquals(1000, writer.maxDoc(0));
     writer.close();
     dir.close();
     aux.close();
@@ -498,7 +498,7 @@
 
     writer.addIndexes(aux, aux2);
     assertEquals(1040, writer.maxDoc());
-    assertEquals(1000, writer.getDocCount(0));
+    assertEquals(1000, writer.maxDoc(0));
     writer.close();
     dir.close();
     aux.close();
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java	(revision 1662913)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java	(working copy)
@@ -248,7 +248,7 @@
 
     int segmentCount = writer.getSegmentCount();
     for (int i = segmentCount - 1; i >= 0; i--) {
-      int docCount = writer.getDocCount(i);
+      int docCount = writer.maxDoc(i);
       assertTrue("docCount=" + docCount + " lowerBound=" + lowerBound + " upperBound=" + upperBound + " i=" + i + " segmentCount=" + segmentCount + " index=" + writer.segString() + " config=" + writer.getConfig(), docCount > lowerBound);
 
       if (docCount <= upperBound) {
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java	(revision 1662913)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java	(working copy)
@@ -320,7 +320,7 @@
           break;
         }
         for(int i=0;i<merge.segments.size();i++) {
-          assert merge.segments.get(i).info.getDocCount() < 20;
+          assert merge.segments.get(i).info.maxDoc() < 20;
         }
         writer.merge(merge);
       }
Index: lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java	(revision 1662913)
+++ lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java	(working copy)
@@ -89,7 +89,7 @@
                                              new FieldInfos.FieldNumbers(),
                                              newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));
     MergeState mergeState = merger.merge();
-    int docsMerged = mergeState.segmentInfo.getDocCount();
+    int docsMerged = mergeState.segmentInfo.maxDoc();
     assertTrue(docsMerged == 2);
     //Should be able to open a new SegmentReader against the new directory
     SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(
Index: lucene/core/src/test/org/apache/lucene/util/makeEuroparlLineFile.py
===================================================================
--- lucene/core/src/test/org/apache/lucene/util/makeEuroparlLineFile.py	(revision 1662913)
+++ lucene/core/src/test/org/apache/lucene/util/makeEuroparlLineFile.py	(working copy)
@@ -35,16 +35,16 @@
 reTagOnly = re.compile('^<.*?>$')
 reNumberOnly = re.compile(r'^\d+\.?$')
 
-docCount = 0
+maxDoc = 0
 didEnglish = False
 
 def write(date, title, pending, fOut):
-  global docCount
+  global maxDoc
   body = ' '.join(pending).replace('\t', ' ').strip()
   if len(body) > 0:
     line = '%s\t%s\t%s\n' % (title, date, body)
     fOut.write(line)
-    docCount += 1
+    maxDoc += 1
     del pending[:]
     if VERBOSE:
       print len(body)
@@ -109,10 +109,10 @@
 
 for fileName in glob.glob('%s/??-??.tgz' % dirIn):
   if fileName.endswith('.tgz'):
-    print 'process %s; %d docs so far...' % (fileName, docCount)
+    print 'process %s; %d docs so far...' % (fileName, maxDoc)
     processTar(fileName, fOut)
 
-print 'TOTAL: %s' % docCount
+print 'TOTAL: %s' % maxDoc
 
 #run something like this:
 """
Index: lucene/misc/src/java/org/apache/lucene/index/IndexSplitter.java
===================================================================
--- lucene/misc/src/java/org/apache/lucene/index/IndexSplitter.java	(revision 1662913)
+++ lucene/misc/src/java/org/apache/lucene/index/IndexSplitter.java	(working copy)
@@ -29,7 +29,6 @@
 import java.util.Locale;
 
 import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.util.StringHelper;
 
 /**
  * Command-line tool that enables listing segments in an
@@ -137,7 +136,7 @@
       SegmentCommitInfo infoPerCommit = getInfo(n);
       SegmentInfo info = infoPerCommit.info;
       // Same info just changing the dir:
-      SegmentInfo newInfo = new SegmentInfo(destFSDir, info.getVersion(), info.name, info.getDocCount(), 
+      SegmentInfo newInfo = new SegmentInfo(destFSDir, info.getVersion(), info.name, info.maxDoc(),
                                             info.getUseCompoundFile(), info.getCodec(), info.getDiagnostics(), info.getId(), new HashMap<>());
       destInfos.add(new SegmentCommitInfo(newInfo, infoPerCommit.getDelCount(),
           infoPerCommit.getDelGen(), infoPerCommit.getFieldInfosGen(),
Index: lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsReader.java
===================================================================
--- lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsReader.java	(revision 1662913)
+++ lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsReader.java	(working copy)
@@ -132,8 +132,8 @@
 
         BytesRef minTerm = readBytesRef(in);
         BytesRef maxTerm = readBytesRef(in);
-        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs
-          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + state.segmentInfo.getDocCount(), in);
+        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs
+          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + state.segmentInfo.maxDoc(), in);
         }
         if (sumDocFreq < docCount) {  // #postings must be >= #docs with field
           throw new CorruptIndexException("invalid sumDocFreq: " + sumDocFreq + " docCount: " + docCount, in);
Index: lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsWriter.java
===================================================================
--- lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsWriter.java	(revision 1662913)
+++ lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsWriter.java	(working copy)
@@ -178,7 +178,7 @@
   {
     BlockTreeTermsWriter.validateSettings(minItemsInBlock, maxItemsInBlock);
 
-    maxDoc = state.segmentInfo.getDocCount();
+    maxDoc = state.segmentInfo.maxDoc();
 
     final String termsFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_EXTENSION);
     out = state.directory.createOutput(termsFileName, state.context);
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingDocValuesFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingDocValuesFormat.java	(revision 1662913)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingDocValuesFormat.java	(working copy)
@@ -58,7 +58,7 @@
   public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
     DocValuesConsumer consumer = in.fieldsConsumer(state);
     assert consumer != null;
-    return new AssertingDocValuesConsumer(consumer, state.segmentInfo.getDocCount());
+    return new AssertingDocValuesConsumer(consumer, state.segmentInfo.maxDoc());
   }
 
   @Override
@@ -66,7 +66,7 @@
     assert state.fieldInfos.hasDocValues();
     DocValuesProducer producer = in.fieldsProducer(state);
     assert producer != null;
-    return new AssertingDocValuesProducer(producer, state.segmentInfo.getDocCount());
+    return new AssertingDocValuesProducer(producer, state.segmentInfo.maxDoc());
   }
   
   static class AssertingDocValuesConsumer extends DocValuesConsumer {
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingLiveDocsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingLiveDocsFormat.java	(revision 1662913)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingLiveDocsFormat.java	(working copy)
@@ -63,7 +63,7 @@
   public Bits readLiveDocs(Directory dir, SegmentCommitInfo info, IOContext context) throws IOException {
     Bits raw = in.readLiveDocs(dir, info, context);
     assert raw != null;
-    check(raw, info.info.getDocCount(), info.getDelCount());
+    check(raw, info.info.maxDoc(), info.getDelCount());
     return new AssertingBits(raw);
   }
 
@@ -71,7 +71,7 @@
   public void writeLiveDocs(MutableBits bits, Directory dir, SegmentCommitInfo info, int newDelCount, IOContext context) throws IOException {
     assert bits instanceof AssertingMutableBits;
     MutableBits raw = (MutableBits) ((AssertingMutableBits)bits).in;
-    check(raw, info.info.getDocCount(), info.getDelCount() + newDelCount);
+    check(raw, info.info.maxDoc(), info.getDelCount() + newDelCount);
     in.writeLiveDocs(raw, dir, info, newDelCount, context);
   }
   
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingNormsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingNormsFormat.java	(revision 1662913)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingNormsFormat.java	(working copy)
@@ -41,7 +41,7 @@
   public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
     NormsConsumer consumer = in.normsConsumer(state);
     assert consumer != null;
-    return new AssertingNormsConsumer(consumer, state.segmentInfo.getDocCount());
+    return new AssertingNormsConsumer(consumer, state.segmentInfo.maxDoc());
   }
 
   @Override
@@ -49,7 +49,7 @@
     assert state.fieldInfos.hasNorms();
     NormsProducer producer = in.normsProducer(state);
     assert producer != null;
-    return new AssertingNormsProducer(producer, state.segmentInfo.getDocCount());
+    return new AssertingNormsProducer(producer, state.segmentInfo.maxDoc());
   }
   
   static class AssertingNormsConsumer extends NormsConsumer {
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingStoredFieldsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingStoredFieldsFormat.java	(revision 1662913)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingStoredFieldsFormat.java	(working copy)
@@ -41,7 +41,7 @@
 
   @Override
   public StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
-    return new AssertingStoredFieldsReader(in.fieldsReader(directory, si, fn, context), si.getDocCount());
+    return new AssertingStoredFieldsReader(in.fieldsReader(directory, si, fn, context), si.maxDoc());
   }
 
   @Override
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java	(revision 1662913)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java	(working copy)
@@ -84,7 +84,7 @@
   @Override
   public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
     int minSkipInterval;
-    if (state.segmentInfo.getDocCount() > 1000000) {
+    if (state.segmentInfo.maxDoc() > 1000000) {
       // Test2BPostings can OOME otherwise:
       minSkipInterval = 3;
     } else {
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java	(revision 1662913)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java	(working copy)
@@ -248,7 +248,7 @@
         postings.fieldToTerms.put(field, ramField);
         termsConsumer.reset(ramField);
 
-        FixedBitSet docsSeen = new FixedBitSet(state.segmentInfo.getDocCount());
+        FixedBitSet docsSeen = new FixedBitSet(state.segmentInfo.maxDoc());
         long sumTotalTermFreq = 0;
         long sumDocFreq = 0;
         PostingsEnum postingsEnum = null;
Index: lucene/test-framework/src/java/org/apache/lucene/index/BaseSegmentInfoFormatTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/index/BaseSegmentInfoFormatTestCase.java	(revision 1662913)
+++ lucene/test-framework/src/java/org/apache/lucene/index/BaseSegmentInfoFormatTestCase.java	(working copy)
@@ -194,7 +194,7 @@
     // we don't assert this, because SI format has nothing to do with it... set by SIS
     // assertSame(expected.getCodec(), actual.getCodec());
     assertEquals(expected.getDiagnostics(), actual.getDiagnostics());
-    assertEquals(expected.getDocCount(), actual.getDocCount());
+    assertEquals(expected.maxDoc(), actual.maxDoc());
     assertIDEquals(expected.getId(), actual.getId());
     assertEquals(expected.getUseCompoundFile(), actual.getUseCompoundFile());
     assertEquals(expected.getVersion(), actual.getVersion());
