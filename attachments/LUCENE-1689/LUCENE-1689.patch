Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/ar/ArabicLetterTokenizer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/ar/ArabicLetterTokenizer.java	(revision 802475)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/ar/ArabicLetterTokenizer.java	(working copy)
@@ -18,6 +18,7 @@
 
 import java.io.Reader;
 
+import org.apache.lucene.analysis.CharTokenizer;
 import org.apache.lucene.analysis.LetterTokenizer;
 
 /**
@@ -30,14 +31,25 @@
 
   public ArabicLetterTokenizer(Reader in) {
     super(in);
+    /* remove in Lucene 3.2 */
+    usesCharMethods = isTokenCharCharImpl != ArabicLetterTokenizer.class
+        || normalizeCharImpl != CharTokenizer.class;
   }
 
   /** 
    * Allows for Letter category or NonspacingMark category
-   * @see org.apache.lucene.analysis.LetterTokenizer#isTokenChar(char)
+   * @deprecated Use {@link #isTokenChar(int)} instead.
    */
   protected boolean isTokenChar(char c) {
     return super.isTokenChar(c) || Character.getType(c) == Character.NON_SPACING_MARK;
   }
+  
+  /** 
+   * Allows for Letter category or NonspacingMark category
+   * @see org.apache.lucene.analysis.LetterTokenizer#isTokenChar(int)
+   */
+  protected boolean isTokenChar(int ch) {
+    return super.isTokenChar(ch) || Character.getType(ch) == Character.NON_SPACING_MARK;
+  }
 
 }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/cjk/CJKTokenizer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/cjk/CJKTokenizer.java	(revision 802475)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/cjk/CJKTokenizer.java	(working copy)
@@ -134,14 +134,14 @@
 
           while (true) { // loop until we've found a full token
             /** current character */
-            char c;
+            int c;
+            int c_len;
 
             /** unicode block of current character for detail */
             Character.UnicodeBlock ub;
 
-            offset++;
-
             if (bufferIndex >= dataLen) {
+              /* TODO: this could split a surrogate in half */ 
                 dataLen = input.read(ioBuffer);
                 bufferIndex = 0;
             }
@@ -159,7 +159,10 @@
                 }
             } else {
                 //get current character
-                c = ioBuffer[bufferIndex++];
+                c = Character.codePointAt(ioBuffer, bufferIndex);
+                c_len = Character.charCount(c);
+                bufferIndex += c_len;
+                offset += c_len;
 
                 //get the UnicodeBlock of the current character
                 ub = Character.UnicodeBlock.of(c);
@@ -186,13 +189,13 @@
                         // "javaC1C2C3C4linux" <br>
                         //      ^--: the current character begin to token the ASCII
                         // letter
-                        start = offset - 1;
+                        start = offset - c_len;
                     } else if (tokenType == DOUBLE_TOKEN_TYPE) {
                         // "javaC1C2C3C4linux" <br>
                         //              ^--: the previous non-ASCII
                         // : the current character
-                        offset--;
-                        bufferIndex--;
+                        offset -= c_len;
+                        bufferIndex -= c_len;
 
                         if (preIsTokened == true) {
                             // there is only one non-ASCII has been stored
@@ -205,7 +208,7 @@
                     }
 
                     // store the LowerCase(c) in the buffer
-                    buffer[length++] = Character.toLowerCase(c);
+                    length += Character.toChars(Character.toLowerCase(c), buffer, length);
                     tokenType = SINGLE_TOKEN_TYPE;
 
                     // break the procedure if buffer overflowed!
@@ -224,23 +227,23 @@
                 // non-ASCII letter, e.g."C1C2C3C4"
                 if (Character.isLetter(c)) {
                     if (length == 0) {
-                        start = offset - 1;
-                        buffer[length++] = c;
+                        start = offset - c_len;
+                        length += Character.toChars(Character.toLowerCase(c), buffer, length);
                         tokenType = DOUBLE_TOKEN_TYPE;
                     } else {
                       if (tokenType == SINGLE_TOKEN_TYPE) {
-                            offset--;
-                            bufferIndex--;
+                            offset -= c_len;
+                            bufferIndex -= c_len;
 
                             //return the previous ASCII characters
                             break;
                         } else {
-                            buffer[length++] = c;
+                            length += Character.toChars(Character.toLowerCase(c), buffer, length);
                             tokenType = DOUBLE_TOKEN_TYPE;
 
-                            if (length == 2) {
-                                offset--;
-                                bufferIndex--;
+                            if (length >= 2) {
+                                offset -= c_len;
+                                bufferIndex -= c_len;
                                 preIsTokened = true;
 
                                 break;
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java	(revision 802475)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java	(working copy)
@@ -75,7 +75,7 @@
 
           // why not key off token type here assuming ChineseTokenizer comes first?
             if (stopTable.get(text) == null) {
-                switch (Character.getType(text.charAt(0))) {
+                switch (Character.getType(text.codePointAt(0))) {
 
                 case Character.LOWERCASE_LETTER:
                 case Character.UPPERCASE_LETTER:
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/ChineseTokenizer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/ChineseTokenizer.java	(revision 802475)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/ChineseTokenizer.java	(working copy)
@@ -75,10 +75,10 @@
     private TermAttribute termAtt;
     private OffsetAttribute offsetAtt;
     
-    private final void push(char c) {
+    private final void push(int ch) {
 
-        if (length == 0) start = offset-1;            // start of token
-        buffer[length++] = Character.toLowerCase(c);  // buffer it
+        if (length == 0) start = offset-Character.charCount(ch);            // start of token
+        length += Character.toChars(Character.toLowerCase(ch), buffer, length);  // buffer it
 
     }
 
@@ -103,19 +103,23 @@
 
         while (true) {
 
-            final char c;
-            offset++;
+            final int c;
+            final int c_len;
 
             if (bufferIndex >= dataLen) {
+              /* TODO: this could split a surrogate in half */
                 dataLen = input.read(ioBuffer);
                 bufferIndex = 0;
             }
 
             if (dataLen == -1) return flush();
-            else
-                c = ioBuffer[bufferIndex++];
+            else {
+                c = Character.codePointAt(ioBuffer, bufferIndex);
+                c_len = Character.charCount(c);
+                bufferIndex += c_len;
+                offset += c_len;
+            }
 
-
             switch(Character.getType(c)) {
 
             case Character.DECIMAL_DIGIT_NUMBER:
@@ -127,8 +131,8 @@
 
             case Character.OTHER_LETTER:
                 if (length>0) {
-                    bufferIndex--;
-                    offset--;
+                    bufferIndex -= c_len;
+                    offset -= c_len;
                     return flush();
                 }
                 push(c);
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/el/GreekCharsets.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/el/GreekCharsets.java	(revision 802475)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/el/GreekCharsets.java	(working copy)
@@ -251,6 +251,9 @@
 		0xdb
     };
 
+    /**
+     * @deprecated Use {@link #toLowerCase(int, char[])} instead.
+     **/
     public static char toLowerCase(char letter, char[] charset)
     {
         if (charset == UnicodeGreek) {
@@ -477,4 +480,231 @@
 
         return Character.toLowerCase(letter);
     }
+    
+    public static int toLowerCase(int letter, char[] charset)
+    {
+        if (charset == UnicodeGreek) {
+            // First deal with lower case, not accented letters
+            if (letter >= '\u03B1' && letter <= '\u03C9')
+            {
+                // Special case 'small final sigma', where we return 'small sigma'
+                if (letter == '\u03C2') {
+                    return '\u03C3';
+                } else {
+                    return letter;
+                }
+            }
+            // Then deal with lower case, accented letters
+            // alpha with acute
+            if (letter == '\u03AC') {
+                return '\u03B1';
+            }
+            // epsilon with acute
+            if (letter == '\u03AD') {
+                return '\u03B5';
+            }
+            // eta with acute
+            if (letter == '\u03AE') {
+                return '\u03B7';
+            }
+            // iota with acute, iota with diaeresis, iota with acute and diaeresis
+            if (letter == '\u03AF' || letter == '\u03CA' || letter == '\u0390') {
+                return '\u03B9';
+            }
+            // upsilon with acute, upsilon with diaeresis, upsilon with acute and diaeresis
+            if (letter == '\u03CD' || letter == '\u03CB' || letter == '\u03B0') {
+                return '\u03C5';
+            }
+            // omicron with acute
+            if (letter == '\u03CC') {
+                return '\u03BF';
+            }
+            // omega with acute
+            if (letter == '\u03CE') {
+                return '\u03C9';
+            }
+            // After that, deal with upper case, not accented letters
+            if (letter >= '\u0391' && letter <= '\u03A9')
+            {
+                return (char) (letter + 32);
+            }
+            // Finally deal with upper case, accented letters
+            // alpha with acute
+            if (letter == '\u0386') {
+                return '\u03B1';
+            }
+            // epsilon with acute
+            if (letter == '\u0388') {
+                return '\u03B5';
+            }
+            // eta with acute
+            if (letter == '\u0389') {
+                return '\u03B7';
+            }
+            // iota with acute, iota with diaeresis
+            if (letter == '\u038A' || letter == '\u03AA') {
+                return '\u03B9';
+            }
+            // upsilon with acute, upsilon with diaeresis
+            if (letter == '\u038E' || letter == '\u03AB') {
+                return '\u03C5';
+            }
+            // omicron with acute
+            if (letter == '\u038C') {
+                return '\u03BF';
+            }
+            // omega with acute
+            if (letter == '\u038F') {
+                return '\u03C9';
+            }
+        } else if (charset == ISO) {
+            // First deal with lower case, not accented letters
+            if (letter >= 0xe1 && letter <= 0xf9)
+            {
+                // Special case 'small final sigma', where we return 'small sigma'
+                if (letter == 0xf2) {
+                    return 0xf3;
+                } else {
+                    return letter;
+                }
+            }
+            // Then deal with lower case, accented letters
+            // alpha with acute
+            if (letter == 0xdc) {
+                return 0xe1;
+            }
+            // epsilon with acute
+            if (letter == 0xdd) {
+                return 0xe5;
+            }
+            // eta with acute
+            if (letter == 0xde) {
+                return 0xe7;
+            }
+            // iota with acute, iota with diaeresis, iota with acute and diaeresis
+            if (letter == 0xdf || letter == 0xfa || letter == 0xc0) {
+                return '\u03B9';
+            }
+            // upsilon with acute, upsilon with diaeresis, upsilon with acute and diaeresis
+            if (letter == 0xfd || letter == 0xfb || letter == 0xe0) {
+                return 0xf5;
+            }
+            // omicron with acute
+            if (letter == 0xfc) {
+                return 0xef;
+            }
+            // omega with acute
+            if (letter == 0xfe) {
+                return 0xf9;
+            }
+            // After that, deal with upper case, not accented letters
+            if (letter >= 0xc1 && letter <= 0xd9) {
+                return (char) (letter + 32);
+            }
+            // Finally deal with upper case, accented letters
+            // alpha with acute
+            if (letter == 0xb6) {
+                return 0xe1;
+            }
+            // epsilon with acute
+            if (letter == 0xb8) {
+                return 0xe5;
+            }
+            // eta with acute
+            if (letter == 0xb9) {
+                return 0xe7;
+            }
+            // iota with acute, iota with diaeresis
+            if (letter == 0xba || letter == 0xda) {
+                return 0xe9;
+            }
+            // upsilon with acute, upsilon with diaeresis
+            if (letter == 0xbe || letter == 0xdb) {
+                return 0xf5;
+            }
+            // omicron with acute
+            if (letter == 0xbc) {
+                return 0xef;
+            }
+            // omega with acute
+            if (letter == 0xbf) {
+                return 0xf9;
+            }
+        } else if (charset == CP1253) {
+            // First deal with lower case, not accented letters
+            if (letter >= 0xe1 && letter <= 0xf9)
+            {
+                // Special case 'small final sigma', where we return 'small sigma'
+                if (letter == 0xf2) {
+                    return 0xf3;
+                } else {
+                    return letter;
+                }
+            }
+            // Then deal with lower case, accented letters
+            // alpha with acute
+            if (letter == 0xdc) {
+                return 0xe1;
+            }
+            // epsilon with acute
+            if (letter == 0xdd) {
+                return 0xe5;
+            }
+            // eta with acute
+            if (letter == 0xde) {
+                return 0xe7;
+            }
+            // iota with acute, iota with diaeresis, iota with acute and diaeresis
+            if (letter == 0xdf || letter == 0xfa || letter == 0xc0) {
+                return '\u03B9';
+            }
+            // upsilon with acute, upsilon with diaeresis, upsilon with acute and diaeresis
+            if (letter == 0xfd || letter == 0xfb || letter == 0xe0) {
+                return 0xf5;
+            }
+            // omicron with acute
+            if (letter == 0xfc) {
+                return 0xef;
+            }
+            // omega with acute
+            if (letter == 0xfe) {
+                return 0xf9;
+            }
+            // After that, deal with upper case, not accented letters
+            if (letter >= 0xc1 && letter <= 0xd9) {
+                return (char) (letter + 32);
+            }
+            // Finally deal with upper case, accented letters
+            // alpha with acute
+            if (letter == 0xa2) {
+                return 0xe1;
+            }
+            // epsilon with acute
+            if (letter == 0xb8) {
+                return 0xe5;
+            }
+            // eta with acute
+            if (letter == 0xb9) {
+                return 0xe7;
+            }
+            // iota with acute, iota with diaeresis
+            if (letter == 0xba || letter == 0xda) {
+                return 0xe9;
+            }
+            // upsilon with acute, upsilon with diaeresis
+            if (letter == 0xbe || letter == 0xdb) {
+                return 0xf5;
+            }
+            // omicron with acute
+            if (letter == 0xbc) {
+                return 0xef;
+            }
+            // omega with acute
+            if (letter == 0xbf) {
+                return 0xf9;
+            }
+        }
+
+        return Character.toLowerCase(letter);
+    }
 }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/el/GreekLowerCaseFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/el/GreekLowerCaseFilter.java	(revision 802475)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/el/GreekLowerCaseFilter.java	(working copy)
@@ -43,9 +43,10 @@
       if (input.incrementToken()) {
         char[] chArray = termAtt.termBuffer();
         int chLen = termAtt.termLength();
-        for (int i = 0; i < chLen; i++)
+        for (int i = 0; i < chLen;)
         {
-          chArray[i] = GreekCharsets.toLowerCase(chArray[i], charset);
+          i += Character.toChars(GreekCharsets.toLowerCase(Character
+              .codePointAt(chArray, i), charset), chArray, i);
         }
         return true;
       } else {
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/reverse/ReverseStringFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/reverse/ReverseStringFilter.java	(revision 802475)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/reverse/ReverseStringFilter.java	(working copy)
@@ -60,13 +60,61 @@
     reverse( buffer, 0, len );
   }
   
-  public static void reverse( char[] buffer, int start, int len ){
-    if( len <= 1 ) return;
-    int num = len>>1;
-    for( int i = start; i < ( start + num ); i++ ){
-      char c = buffer[i];
-      buffer[i] = buffer[start * 2 + len - i - 1];
-      buffer[start * 2 + len - i - 1] = c;
+  public static void reverse(char[] buffer, int start, int len) {
+    /* modified version of Apache Harmony AbstractStringBuilder reverse0() */
+    if (len < 2)
+      return;
+    int end = (start + len) - 1;
+    char frontHigh = buffer[start];
+    char endLow = buffer[end];
+    boolean allowFrontSur = true, allowEndSur = true;
+    for (int i = start, mid = start + (len / 2); i < mid; i++, --end) {
+      char frontLow = buffer[i + 1];
+      char endHigh = buffer[end - 1];
+      boolean surAtFront = allowFrontSur && frontLow >= 0xdc00
+          && frontLow <= 0xdfff && frontHigh >= 0xd800 && frontHigh <= 0xdbff;
+      if (surAtFront && (len < 3)) {
+        return;
+      }
+      boolean surAtEnd = allowEndSur && endHigh >= 0xd800 && endHigh <= 0xdbff
+          && endLow >= 0xdc00 && endLow <= 0xdfff;
+      allowFrontSur = allowEndSur = true;
+      if (surAtFront == surAtEnd) {
+        if (surAtFront) {
+          // both surrogates
+          buffer[end] = frontLow;
+          buffer[end - 1] = frontHigh;
+          buffer[i] = endHigh;
+          buffer[i + 1] = endLow;
+          frontHigh = buffer[i + 2];
+          endLow = buffer[end - 2];
+          i++;
+          end--;
+        } else {
+          // neither surrogates
+          buffer[end] = frontHigh;
+          buffer[i] = endLow;
+          frontHigh = frontLow;
+          endLow = endHigh;
+        }
+      } else {
+        if (surAtFront) {
+          // surrogate only at the front
+          buffer[end] = frontLow;
+          buffer[i] = endLow;
+          endLow = endHigh;
+          allowFrontSur = false;
+        } else {
+          // surrogate only at the end
+          buffer[end] = frontHigh;
+          buffer[i] = endHigh;
+          frontHigh = frontLow;
+          allowEndSur = false;
+        }
+      }
     }
+    if ((len & 1) == 1 && (!allowFrontSur || !allowEndSur)) {
+      buffer[end] = allowFrontSur ? endLow : frontHigh;
+    }
   }
 }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianCharsets.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianCharsets.java	(revision 802475)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianCharsets.java	(working copy)
@@ -268,6 +268,9 @@
         '9'
     };
 
+    /**
+     * @deprecated Use {@link #toLowerCase(int, char[])} instead.
+     **/
     public static char toLowerCase(char letter, char[] charset)
     {
         if (charset == UnicodeRussian)
@@ -310,4 +313,47 @@
 
         return Character.toLowerCase(letter);
     }
+    
+    public static int toLowerCase(int letter, char[] charset)
+    {
+        if (charset == UnicodeRussian)
+        {
+            if (letter >= '\u0430' && letter <= '\u044F')
+            {
+                return letter;
+            }
+            if (letter >= '\u0410' && letter <= '\u042F')
+            {
+                return (char) (letter + 32);
+            }
+        }
+
+        if (charset == KOI8)
+        {
+            if (letter >= 0xe0 && letter <= 0xff)
+            {
+                return (char) (letter - 32);
+            }
+            if (letter >= 0xc0 && letter <= 0xdf)
+            {
+                return letter;
+            }
+
+        }
+
+        if (charset == CP1251)
+        {
+            if (letter >= 0xC0 && letter <= 0xDF)
+            {
+                return (char) (letter + 32);
+            }
+            if (letter >= 0xE0 && letter <= 0xFF)
+            {
+                return letter;
+            }
+
+        }
+
+        return Character.toLowerCase(letter);
+    }
 }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianLetterTokenizer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianLetterTokenizer.java	(revision 802475)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianLetterTokenizer.java	(working copy)
@@ -39,11 +39,15 @@
     {
         super(in);
         this.charset = charset;
+        /* remove in Lucene 3.2 */
+        usesCharMethods = isTokenCharCharImpl != RussianLetterTokenizer.class
+        || normalizeCharImpl != CharTokenizer.class;
     }
 
     /**
      * Collects only characters which satisfy
      * {@link Character#isLetter(char)}.
+     * @deprecated Use {@link #isTokenChar(int)} instead.
      */
     protected boolean isTokenChar(char c)
     {
@@ -56,4 +60,20 @@
         }
         return false;
     }
+    
+    /**
+     * Collects only characters which satisfy
+     * {@link Character#isLetter(int)}.
+     */
+    protected boolean isTokenChar(int ch)
+    {
+        if (Character.isLetter(ch))
+            return true;
+        for (int i = 0; i < charset.length; i++)
+        {
+            if (ch == charset[i])
+                return true;
+        }
+        return false;
+    }
 }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianLowerCaseFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianLowerCaseFilter.java	(revision 802475)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianLowerCaseFilter.java	(working copy)
@@ -48,9 +48,10 @@
       if (input.incrementToken()) {
         char[] chArray = termAtt.termBuffer();
         int chLen = termAtt.termLength();
-        for (int i = 0; i < chLen; i++)
+        for (int i = 0; i < chLen;)
         {
-          chArray[i] = RussianCharsets.toLowerCase(chArray[i], charset);
+          i += Character.toChars(RussianCharsets.toLowerCase(Character
+            .codePointAt(chArray, i), charset), chArray, i);
         }
         return true;
       } else {
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java	(revision 802475)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java	(working copy)
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 
+import java.io.Reader;
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.Analyzer;
@@ -66,6 +67,47 @@
         "english", "text" });
   }
   
+  /**
+   * Supplementary characters testcase
+   */
+  public void testSupplementaryCharacters() throws Exception {
+    assertAnalyzesTo(new ArabicAnalyzer(), "ð–ð–ð–ð–ð– ð–ð–ð–ð–ð–", new String [] { "ð¾ð¾ð¾ð¾ð¾", "ð¾ð¾ð¾ð¾ð¾" });
+  }
+  
+  /**
+   * Test Backwards compat of an analyzer that extends the char-based method of
+   * ArabicLetterTokenizer, supplementary characters will be discarded as before.
+   * @deprecated Remove this test in Lucene 3.2
+   */
+  static class ArabicTokenizerWithNumerics extends ArabicLetterTokenizer {
+
+    public ArabicTokenizerWithNumerics(Reader in) {
+      super(in);
+    }
+    
+    protected boolean isTokenChar(char c) {
+      return super.isTokenChar(c) || Character.isDigit(c);
+    }
+  }
+  
+  /**
+   * Test the analyzer behaves correctly with arabic text and numerics,
+   * but discarding supplementary characters.
+   * @deprecated Remove this test in Lucene 3.2
+   */
+  public void testCharTokenizerBackCompat() throws Exception {
+    Analyzer a = new Analyzer() {
+
+      public TokenStream tokenStream(String fieldName, Reader reader) {
+        return new ArabicStemFilter(new ArabicNormalizationFilter(
+            new ArabicTokenizerWithNumerics(reader)));
+      }
+      
+    };
+    assertAnalyzesTo(a, "ð–ð–ð–ð–ð– ð–ð–ð–ð–ð– ÙƒÙŽØ¨ÙŠÙ€Ù€Ù€Ù€Ù€Ø±Ø© 1234", new String[] {
+        "ÙƒØ¨ÙŠØ±", "1234" });
+  }
+  
   private void assertAnalyzesTo(Analyzer a, String input, String[] output)
       throws Exception {
     TokenStream ts = a.tokenStream("dummy", new StringReader(input));
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java	(revision 802475)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java	(working copy)
@@ -151,4 +151,25 @@
     };
     checkCJKToken(str, out_tokens);
   }
+  
+  public void testSupplementaryChar() throws IOException {
+    String str = "ç€›æ„¯ä‡¹éŸè‰±ð©¬…æ¶“ã·‡æŸŠç·—æˆ é—‚";
+    
+    TestToken[] out_tokens = {
+     newToken("ç€›æ„¯", 0, 2, CJKTokenizer.DOUBLE_TOKEN_TYPE),
+     newToken("æ„¯ä‡¹", 1, 3, CJKTokenizer.DOUBLE_TOKEN_TYPE),
+     newToken("ä‡¹éŸ", 2, 4, CJKTokenizer.DOUBLE_TOKEN_TYPE),
+     newToken("éŸè‰±", 3, 5, CJKTokenizer.DOUBLE_TOKEN_TYPE),
+     // ð©¬… is a supplementary character
+     newToken("è‰±ð©¬…", 4, 7, CJKTokenizer.DOUBLE_TOKEN_TYPE),
+     // ð©¬… is a supplementary character
+     newToken("ð©¬…æ¶“", 5, 8, CJKTokenizer.DOUBLE_TOKEN_TYPE),
+     newToken("æ¶“ã·‡", 7, 9, CJKTokenizer.DOUBLE_TOKEN_TYPE),
+     newToken("ã·‡æŸŠ", 8, 10, CJKTokenizer.DOUBLE_TOKEN_TYPE),
+     newToken("æŸŠç·—", 9, 11, CJKTokenizer.DOUBLE_TOKEN_TYPE),
+     newToken("ç·—æˆ ", 10, 12, CJKTokenizer.DOUBLE_TOKEN_TYPE),
+     newToken("æˆ é—‚", 11, 13, CJKTokenizer.DOUBLE_TOKEN_TYPE),
+    };
+    checkCJKToken(str, out_tokens);
+  }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java	(revision 802475)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java	(working copy)
@@ -22,7 +22,10 @@
 
 import junit.framework.TestCase;
 
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 
 public class TestChineseTokenizer extends TestCase
@@ -42,4 +45,28 @@
           correctEndOffset++;
         }
     }
+    
+    public void testSupplementaryCharacters() throws Exception {
+      Analyzer a = new ChineseAnalyzer();
+      assertAnalyzesTo(a, "ç€›æ„¯ä‡¹éŸè‰±ð©¬…",
+        new String[] { "ç€›", "æ„¯", "ä‡¹", "éŸ", "è‰±", "ð©¬…" });
+      assertAnalyzesTo(a, "ç€›æ„¯ä‡¹éŸè‰±ð©¬…æ¶“ã·‡æŸŠç·—æˆ é—‚",
+        new String[] { "ç€›", "æ„¯", "ä‡¹", "éŸ", "è‰±", "ð©¬…", 
+          "æ¶“", "ã·‡", "æŸŠ", "ç·—", "æˆ ", "é—‚" });
+    }
+
+    private void assertAnalyzesTo(Analyzer a, String input, String[] output)
+      throws Exception {
+      TokenStream ts = a.tokenStream("dummy", new StringReader(input));
+      TermAttribute termAtt = (TermAttribute) ts
+        .getAttribute(TermAttribute.class);
+
+      for (int i = 0; i < output.length; i++) {
+        assertTrue(ts.incrementToken());
+        assertEquals(output[i], termAtt.term());
+      }
+
+      assertFalse(ts.incrementToken());
+      ts.close();
+    }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java	(revision 802475)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java	(working copy)
@@ -16,11 +16,13 @@
  * limitations under the License.
  */
 
+import java.io.Reader;
 import java.io.StringReader;
 
 import junit.framework.TestCase;
 
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.LetterTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
@@ -70,5 +72,21 @@
 		assertAnalyzesTo(a, "\u03a0\u03a1\u039f\u03ab\u03a0\u039f\u0398\u0395\u03a3\u0395\u0399\u03a3  \u0386\u03c8\u03bf\u03b3\u03bf\u03c2, \u03bf \u03bc\u03b5\u03c3\u03c4\u03cc\u03c2 \u03ba\u03b1\u03b9 \u03bf\u03b9 \u03ac\u03bb\u03bb\u03bf\u03b9",
 				new String[] { "\u03c0\u03c1\u03bf\u03c5\u03c0\u03bf\u03b8\u03b5\u03c3\u03b5\u03b9\u03c3", "\u03b1\u03c8\u03bf\u03b3\u03bf\u03c3", "\u03bc\u03b5\u03c3\u03c4\u03bf\u03c3", "\u03b1\u03bb\u03bb\u03bf\u03b9" });
 	}
+	
+    /**
+     * Supplementary characters testcase
+     * TODO: when StandardTokenizer is fixed switch this to just use GreekAnalyzer
+     * For now, test the GreekLowerCaseFilter
+     */
+    public void testSupplementaryCharacters() throws Exception {
+      Analyzer a = new Analyzer() {
 
+        public TokenStream tokenStream(String fieldName, Reader reader) {
+          return new GreekLowerCaseFilter(new LetterTokenizer(reader), GreekCharsets.UnicodeGreek);
+        }
+        
+      };
+      assertAnalyzesTo(a, "ð–ð–ð–ð–ð– ð–ð–ð–ð–ð–", new String [] { "ð¾ð¾ð¾ð¾ð¾", "ð¾ð¾ð¾ð¾ð¾" });
+    }
+
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java	(revision 802475)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java	(working copy)
@@ -54,4 +54,40 @@
     ReverseStringFilter.reverse( buffer, 2, 3 );
     assertEquals( "ABEDCF", new String( buffer ) );
   }
+  
+  public void testReverseSupplementary() throws Exception {
+    // supplementary at end
+    assertEquals("ð©¬…è‰±éŸä‡¹æ„¯ç€›", ReverseStringFilter.reverse("ç€›æ„¯ä‡¹éŸè‰±ð©¬…"));
+    // supplementary at end - 1
+    assertEquals("að©¬…è‰±éŸä‡¹æ„¯ç€›", ReverseStringFilter.reverse("ç€›æ„¯ä‡¹éŸè‰±ð©¬…a"));
+    // supplementary at start
+    assertEquals("fedcbað©¬…", ReverseStringFilter.reverse("ð©¬…abcdef"));
+    // supplementary at start + 1
+    assertEquals("fedcbað©¬…z", ReverseStringFilter.reverse("zð©¬…abcdef"));
+    // supplementary medial
+    assertEquals("gfeð©¬…dcba", ReverseStringFilter.reverse("abcdð©¬…efg"));
+  }
+  
+  public void testReverseSupplementaryChar() throws Exception {
+    // supplementary at end
+    char[] buffer = "abcç€›æ„¯ä‡¹éŸè‰±ð©¬…".toCharArray();
+    ReverseStringFilter.reverse(buffer, 3, 7);    
+    assertEquals("abcð©¬…è‰±éŸä‡¹æ„¯ç€›", new String(buffer));
+    // supplementary at end - 1
+    buffer = "abcç€›æ„¯ä‡¹éŸè‰±ð©¬…d".toCharArray();
+    ReverseStringFilter.reverse(buffer, 3, 8);    
+    assertEquals("abcdð©¬…è‰±éŸä‡¹æ„¯ç€›", new String(buffer));
+    // supplementary at start
+    buffer = "abcð©¬…ç€›æ„¯ä‡¹éŸè‰±".toCharArray();
+    ReverseStringFilter.reverse(buffer, 3, 7);
+    assertEquals("abcè‰±éŸä‡¹æ„¯ç€›ð©¬…", new String(buffer));
+    // supplementary at start + 1
+    buffer = "abcdð©¬…ç€›æ„¯ä‡¹éŸè‰±".toCharArray();
+    ReverseStringFilter.reverse(buffer, 3, 8);
+    assertEquals("abcè‰±éŸä‡¹æ„¯ç€›ð©¬…d", new String(buffer));
+    // supplementary medial
+    buffer = "abcç€›æ„¯ð©¬…def".toCharArray();
+    ReverseStringFilter.reverse(buffer, 3, 7);
+    assertEquals("abcfedð©¬…æ„¯ç€›", new String(buffer));
+  }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java	(revision 802475)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java	(working copy)
@@ -26,6 +26,7 @@
 
 import junit.framework.TestCase;
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
@@ -187,5 +188,63 @@
             fail("unexpected IOException");
         }
     }
+    
+    /**
+     * Supplementary characters testcase
+     */
+    public void testSupplementaryCharacters() throws Exception {
+      assertAnalyzesTo(new RussianAnalyzer(), "ð–ð–ð–ð–ð– ð–ð–ð–ð–ð–", new String [] { "ð¾ð¾ð¾ð¾ð¾", "ð¾ð¾ð¾ð¾ð¾" });
+    }
+    
+    /**
+     * Test Backwards compat of an analyzer that extends the char-based method of
+     * RussianLetterTokenizer, supplementary characters will be discarded as before.
+     * @deprecated Remove this test in Lucene 3.2
+     */
+    
+    static class RussianTokenizerWithNumerics extends RussianLetterTokenizer {
 
+      public RussianTokenizerWithNumerics(Reader in) {
+        super(in, RussianCharsets.UnicodeRussian);
+      }
+      
+      protected boolean isTokenChar(char c) {
+        return super.isTokenChar(c) || Character.isDigit(c);
+      }
+    }
+    
+    /**
+     * Test the analyzer behaves correctly with arabic text and numerics,
+     * but discarding supplementary characters.
+     * @deprecated Remove this test in Lucene 3.2
+     */
+    public void testCharTokenizerBackCompat() throws Exception {
+      Analyzer a = new Analyzer() {
+
+        public TokenStream tokenStream(String fieldName, Reader reader) {
+          return new RussianLowerCaseFilter(new RussianTokenizerWithNumerics(
+            reader), RussianCharsets.UnicodeRussian);
+        }
+        
+      };
+      
+      assertAnalyzesTo(a, "ð–ð–ð–ð–ð– ð–ð–ð–ð–ð– ÐœÐ¾ÑÐºÐ²a 1234", new String[] {
+          "Ð¼Ð¾ÑÐºÐ²a", "1234" });
+    }
+    
+    private void assertAnalyzesTo(Analyzer a, String input, String[] output)
+      throws Exception {
+      TokenStream ts = a.tokenStream("dummy", new StringReader(input));
+      TermAttribute termAtt = (TermAttribute) ts
+        .getAttribute(TermAttribute.class);
+
+      for (int i = 0; i < output.length; i++) {
+        assertTrue(ts.incrementToken());
+        assertEquals(output[i], termAtt.term());
+      }
+
+      assertFalse(ts.incrementToken());
+      ts.close();
+    }
+
 }
Index: src/java/org/apache/lucene/analysis/CharTokenizer.java
===================================================================
--- src/java/org/apache/lucene/analysis/CharTokenizer.java	(revision 802475)
+++ src/java/org/apache/lucene/analysis/CharTokenizer.java	(working copy)
@@ -42,17 +42,42 @@
   /** Returns true iff a character should be included in a token.  This
    * tokenizer generates as tokens adjacent sequences of characters which
    * satisfy this predicate.  Characters for which this is false are used to
-   * define token boundaries and are not included in tokens. */
-  protected abstract boolean isTokenChar(char c);
+   * define token boundaries and are not included in tokens. 
+   * @deprecated Use {@link #isTokenChar(int)} instead. */
+  protected boolean isTokenChar(char c) {
+    return false;
+  }
+  
+  /** Returns true iff a character should be included in a token.  This
+   * tokenizer generates as tokens adjacent sequences of characters which
+   * satisfy this predicate.  Characters for which this is false are used to
+   * define token boundaries and are not included in tokens. 
+   * <b>Note that this method will be defined abstract in Lucene 3.2.</b>
+   * */
+  protected boolean isTokenChar(int ch) {
+    return false;
+  }
 
   /** Called on each token character to normalize it before it is added to the
    * token.  The default implementation does nothing.  Subclasses may use this
-   * to, e.g., lowercase tokens. */
+   * to, e.g., lowercase tokens. 
+   * @deprecated Use {@link #normalize(int)} instead. */
   protected char normalize(char c) {
     return c;
   }
+  
+  /** Returns true iff a character should be included in a token.  This
+   * tokenizer generates as tokens adjacent sequences of characters which
+   * satisfy this predicate.  Characters for which this is false are used to
+   * define token boundaries and are not included in tokens. */
+  protected int normalize(int ch) {
+    return ch;
+  }
 
-  public final boolean incrementToken() throws IOException {
+  /** 
+   * @deprecated For backwards compatibility, remove in Lucene 3.2
+   */
+  private final boolean incrementTokenChar() throws IOException {
     clearAttributes();
     int length = 0;
     int start = bufferIndex;
@@ -94,7 +119,86 @@
     offsetAtt.setOffset(input.correctOffset(start), input.correctOffset(start+length));
     return true;
   }
+    
+  /** @deprecated Remove this in Lucene 3.2 */
+  private static final Class[] METHOD_CHAR_PARAM = new Class[]{char.class};
+
+  /** @deprecated Remove this in Lucene 3.2 */
+  protected final Class 
+    isTokenCharCharImpl = getDeclaringClass("isTokenChar", METHOD_CHAR_PARAM),
+    normalizeCharImpl = getDeclaringClass("normalize", METHOD_CHAR_PARAM);
   
+  /**
+   * backwards compatibility switch to turn on the old behavior.
+   * @deprecated For backwards compatibility, remove in Lucene 3.2
+   */
+  protected boolean usesCharMethods = isTokenCharCharImpl != CharTokenizer.class;
+  
+  /** @deprecated Remove this in Lucene 3.2! */
+  private Class getDeclaringClass(String name, Class[] params) {
+    Class clazz = this.getClass();
+    while (clazz != Object.class)
+      try {
+        clazz.getDeclaredMethod(name, params);
+        return clazz;
+      } catch (NoSuchMethodException e) {
+        clazz = clazz.getSuperclass();
+      }
+    return null; /* impossible */
+  }
+  
+  public final boolean incrementToken() throws IOException {
+    if (usesCharMethods)
+      return incrementTokenChar();
+    clearAttributes();
+    int length = 0;
+    int start = bufferIndex;
+    char[] buffer = termAtt.termBuffer();
+    while (true) {
+
+      if (bufferIndex >= dataLen) {
+        offset += dataLen;
+        /*
+         * TODO: The last character read could be a lead surrogate.
+         * this case needs to be checked, and additional characters read() so that a surrogate pair is not split in half.
+         * this is a bit tricky (not just reading an additional character) because in the case of invalid unicode text,
+         * the last character could be an unpaired lead surrogate, followed by a surrogate pair (lead + trail)
+         */
+        dataLen = input.read(ioBuffer);
+        if (dataLen == -1) {
+          dataLen = 0;                            // so next offset += dataLen won't decrement offset
+          if (length > 0)
+            break;
+          else
+            return false;
+        }
+        bufferIndex = 0;
+      }
+
+      final int ch = Character.codePointAt(ioBuffer, bufferIndex);
+      bufferIndex += Character.charCount(ch);
+
+      if (isTokenChar(ch)) {               // if it's a token char
+
+        if (length == 0)                 // start of token
+          start = offset + bufferIndex - 1;
+        else if (length >= buffer.length - 1)
+          buffer = termAtt.resizeTermBuffer(2+length);
+
+        length += Character.toChars(normalize(ch), buffer, length); // buffer it, normalized
+
+        if (length >= MAX_WORD_LEN)      // buffer overflow!
+          break;
+
+      } else if (length > 0)             // at non-Letter w/ chars
+        break;                           // return 'em
+    }
+
+    termAtt.setTermLength(length);
+    offsetAtt.setOffset(input.correctOffset(start), input.correctOffset(start+length));
+    return true;
+  }
+  
   public final void end() {
     // set final offset
     int finalOffset = input.correctOffset(offset);
Index: src/java/org/apache/lucene/analysis/LengthFilter.java
===================================================================
--- src/java/org/apache/lucene/analysis/LengthFilter.java	(revision 802475)
+++ src/java/org/apache/lucene/analysis/LengthFilter.java	(working copy)
@@ -23,8 +23,9 @@
 
 /**
  * Removes words that are too long and too short from the stream.
- *
- *
+ * <p>
+ * Note: Length is calculated as UTF-16 code units.
+ * </p>
  * @version $Id$
  */
 public final class LengthFilter extends TokenFilter {
Index: src/java/org/apache/lucene/analysis/LetterTokenizer.java
===================================================================
--- src/java/org/apache/lucene/analysis/LetterTokenizer.java	(revision 802475)
+++ src/java/org/apache/lucene/analysis/LetterTokenizer.java	(working copy)
@@ -30,11 +30,21 @@
   /** Construct a new LetterTokenizer. */
   public LetterTokenizer(Reader in) {
     super(in);
+    /* remove in Lucene 3.2 */
+    usesCharMethods = isTokenCharCharImpl != LetterTokenizer.class
+        || normalizeCharImpl != CharTokenizer.class;
   }
 
   /** Collects only characters which satisfy
-   * {@link Character#isLetter(char)}.*/
+   * {@link Character#isLetter(char)}.
+   * @deprecated Use {@link #isTokenChar(int)} instead. */
   protected boolean isTokenChar(char c) {
     return Character.isLetter(c);
   }
+  
+  /** Collects only characters which satisfy
+   * {@link Character#isLetter(int)}. */
+  protected boolean isTokenChar(int ch) {
+    return Character.isLetter(ch);
+  }
 }
Index: src/java/org/apache/lucene/analysis/LowerCaseFilter.java
===================================================================
--- src/java/org/apache/lucene/analysis/LowerCaseFilter.java	(revision 802475)
+++ src/java/org/apache/lucene/analysis/LowerCaseFilter.java	(working copy)
@@ -39,9 +39,17 @@
 
       final char[] buffer = termAtt.termBuffer();
       final int length = termAtt.termLength();
-      for(int i=0;i<length;i++)
-        buffer[i] = Character.toLowerCase(buffer[i]);
-
+      for (int i = 0; i < length; i++) {
+        final char ch = buffer[i];
+        if (Character.isHighSurrogate(ch)) {
+          final int codepoint = Character.toLowerCase(Character.codePointAt(
+              buffer, i, length));
+          /* skip over the trail surrogate, if there is one */
+          i += Character.toChars(codepoint, buffer, i) - 1;
+        } else {
+          buffer[i] = Character.toLowerCase(buffer[i]);
+        }
+      }
       return true;
     } else
       return false;
Index: src/java/org/apache/lucene/analysis/LowerCaseTokenizer.java
===================================================================
--- src/java/org/apache/lucene/analysis/LowerCaseTokenizer.java	(revision 802475)
+++ src/java/org/apache/lucene/analysis/LowerCaseTokenizer.java	(working copy)
@@ -35,9 +35,9 @@
     super(in);
   }
 
-  /** Collects only characters which satisfy
-   * {@link Character#isLetter(char)}.*/
-  protected char normalize(char c) {
-    return Character.toLowerCase(c);
+  /** Normalizes all characters with
+   * {@link Character#toLowerCase(int)}.*/
+  protected int normalize(int ch) {
+    return Character.toLowerCase(ch);
   }
 }
Index: src/java/org/apache/lucene/analysis/WhitespaceTokenizer.java
===================================================================
--- src/java/org/apache/lucene/analysis/WhitespaceTokenizer.java	(revision 802475)
+++ src/java/org/apache/lucene/analysis/WhitespaceTokenizer.java	(working copy)
@@ -26,11 +26,21 @@
   /** Construct a new WhitespaceTokenizer. */
   public WhitespaceTokenizer(Reader in) {
     super(in);
+    /* remove in Lucene 3.2 */
+    usesCharMethods = isTokenCharCharImpl != WhitespaceTokenizer.class
+        || normalizeCharImpl != CharTokenizer.class;
   }
 
   /** Collects only characters which do not satisfy
-   * {@link Character#isWhitespace(char)}.*/
+   * {@link Character#isWhitespace(char)}.
+   * @deprecated Use {@link #isTokenChar(int)} instead. */
   protected boolean isTokenChar(char c) {
     return !Character.isWhitespace(c);
   }
+  
+  /** Collects only characters which do not satisfy
+   * {@link Character#isWhitespace(int)}. */
+  protected boolean isTokenChar(int ch) {
+    return !Character.isWhitespace(ch);
+  }
 }
Index: src/test/org/apache/lucene/analysis/TestAnalyzers.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestAnalyzers.java	(revision 802475)
+++ src/test/org/apache/lucene/analysis/TestAnalyzers.java	(working copy)
@@ -65,6 +65,8 @@
                      new String[] { "b" });
     assertAnalyzesTo(a, "\"QUOTED\" word", 
                      new String[] { "quoted", "word" });
+    /* supplementary characters test case */
+    assertAnalyzesTo(a, "ð–ð–ð–ð–ð– ð–ð–ð–ð–ð–", new String [] { "ð¾ð¾ð¾ð¾ð¾", "ð¾ð¾ð¾ð¾ð¾" });
   }
 
   public void testNull() throws Exception {
@@ -85,6 +87,8 @@
                      new String[] { "2B" });
     assertAnalyzesTo(a, "\"QUOTED\" word", 
                      new String[] { "\"QUOTED\"", "word" });
+    /* supplementary characters test case */
+    assertAnalyzesTo(a, "ð–ð–ð–ð–ð– ð–ð–ð–ð–ð–", new String [] { "ð–ð–ð–ð–ð–", "ð–ð–ð–ð–ð–" });
   }
 
   public void testStop() throws Exception {
@@ -93,6 +97,8 @@
                      new String[] { "foo", "bar", "foo", "bar" });
     assertAnalyzesTo(a, "foo a bar such FOO THESE BAR", 
                      new String[] { "foo", "bar", "foo", "bar" });
+    /* supplementary characters test case */
+    assertAnalyzesTo(a, "ð–ð–ð–ð–ð– ð–ð–ð–ð–ð–", new String [] { "ð¾ð¾ð¾ð¾ð¾", "ð¾ð¾ð¾ð¾ð¾" });
   }
 
   void verifyPayload(TokenStream ts) throws IOException {
@@ -147,6 +153,20 @@
     assertTrue(ts.incrementToken());
     assertFalse(ts.incrementToken());
   }
+  
+  /* 
+   * LUCENE-1689: test that supplementary characters are lowercased properly
+   */
+  public void testLowercaseSupplementary() throws Exception {
+    Analyzer a = new Analyzer() {
+      public TokenStream tokenStream(String fieldName, Reader reader) {
+        return new LowerCaseFilter(new WhitespaceTokenizer(reader));
+      }
+    };
+
+    assertAnalyzesTo(a, "ð–ð–ð–ð–ð–", new String [] { "ð¾ð¾ð¾ð¾ð¾" });
+  }
+
 }
 
 class PayloadSetter extends TokenFilter {
Index: src/test/org/apache/lucene/analysis/TestCharTokenizerBWComp.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestCharTokenizerBWComp.java	(revision 0)
+++ src/test/org/apache/lucene/analysis/TestCharTokenizerBWComp.java	(revision 0)
@@ -0,0 +1,110 @@
+package org.apache.lucene.analysis;
+
+import java.io.IOException;
+import java.io.Reader;
+import java.io.StringReader;
+
+import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.util.LuceneTestCase;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Test backwards compatibility of CharTokenizer (with the codepoint-based api) with old tokenizers.
+ * @deprecated Remove this class in Lucene 3.2
+ */
+public class TestCharTokenizerBWComp extends LuceneTestCase {
+
+  /**
+   * Class that allows for alphas (just like the old LetterTokenizer)
+   * Extends the old api, so it won't support supplementary characters... just as before.
+   */
+  static class SimpleLetterTokenizer extends CharTokenizer {
+    public SimpleLetterTokenizer(Reader input) {
+      super(input);
+    }
+    
+    protected boolean isTokenChar(char c) {
+      return Character.isLetter(c);
+    }
+  }
+  
+  /**
+   * Something that extends old LetterTokenizer API
+   */
+  public void testExtendsCharTokenizer() throws Exception {
+    String doc = "ð–ð–ð–ð–ð– ð–ð–ð–ð–ð– abcd"; 
+    /* Supp codepoints discarded as before */
+    check(new SimpleLetterTokenizer(new StringReader(doc)), "abcd");
+  }
+  
+  /**
+   * Class that allows for alphanumerics, extending LetterTokenizer
+   * Extends the old api, so it won't support supplementary characters... just as before.
+   */
+  static class LetterDigitTokenizer extends LetterTokenizer {
+
+    public LetterDigitTokenizer(Reader input) {
+      super(input);
+    }
+    
+    protected boolean isTokenChar(char c) {
+      return super.isTokenChar(c) || Character.isDigit(c);
+    }  
+  }
+  
+  /**
+   * Something that extends old LetterTokenizer API
+   */
+  public void testExtendsLetterTokenizer() throws Exception {
+    String doc = "ð–ð–ð–ð–ð– ð–ð–ð–ð–ð– 1234"; 
+    /* Supp codepoints discarded as before */
+    check(new LetterDigitTokenizer(new StringReader(doc)), "1234");
+  }
+  
+  /**
+   * Class that extends Lettertokenizer but calls normalize to lowercase
+   * Extends the old api, so it won't support supplementary characters... just as before.
+   *
+   */
+  static class LowercasingLetterTokenizer extends LetterTokenizer {
+    public LowercasingLetterTokenizer(Reader input) {
+      super(input);
+    }
+    
+    protected char normalize(char c) {
+      return Character.toLowerCase(c);
+    }
+  }
+  
+  /**
+   * Something that extends old LetterTokenizer API
+   */
+  public void testExtendsLetterTokenizerNormalize() throws Exception {
+    String doc = "ð–ð–ð–ð–ð– ð–ð–ð–ð–ð– ABCD"; 
+    /* Supp codepoints discarded as before */
+    check(new LowercasingLetterTokenizer(new StringReader(doc)), "abcd");
+  }
+
+  private void check(final TokenStream stream, final String expected) throws IOException {
+    TermAttribute termAtt = (TermAttribute) stream.getAttribute(TermAttribute.class);
+    assertTrue(stream.incrementToken());
+    assertEquals(expected, termAtt.term());
+    stream.close();
+  }
+}

Property changes on: src\test\org\apache\lucene\analysis\TestCharTokenizerBWComp.java
___________________________________________________________________
Added: svn:eol-style
   + native

