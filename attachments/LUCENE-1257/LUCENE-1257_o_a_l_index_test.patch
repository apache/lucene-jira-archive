Index: src/test/org/apache/lucene/index/TestDoc.java
===================================================================
--- src/test/org/apache/lucene/index/TestDoc.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestDoc.java	(working copy)
@@ -22,7 +22,7 @@
 import java.io.IOException;
 import java.io.PrintWriter;
 import java.io.StringWriter;
-import java.util.Iterator;
+
 import java.util.LinkedList;
 import java.util.List;
 
@@ -48,7 +48,7 @@
 
     private File workDir;
     private File indexDir;
-    private LinkedList files;
+    private LinkedList<File> files;
 
 
     /** Set the test case. This test case needs
@@ -65,7 +65,7 @@
         Directory directory = FSDirectory.open(indexDir);
         directory.close();
 
-        files = new LinkedList();
+        files = new LinkedList<File>();
         files.add(createOutput("test.txt",
             "This is the first test file"
         ));
@@ -187,9 +187,9 @@
       merger.closeReaders();
       
       if (useCompoundFile) {
-        List filesToDelete = merger.createCompoundFile(merged + ".cfs");
-        for (Iterator iter = filesToDelete.iterator(); iter.hasNext();)
-          si1.dir.deleteFile((String) iter.next());
+        List<String> filesToDelete = merger.createCompoundFile(merged + ".cfs");
+        for (final String file : filesToDelete)
+          si1.dir.deleteFile(file);
       }
 
       return new SegmentInfo(merged, si1.docCount + si2.docCount, si1.dir, useCompoundFile, true);
Index: src/test/org/apache/lucene/index/TestIndexReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexReader.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestIndexReader.java	(working copy)
@@ -25,9 +25,11 @@
 import java.util.Collection;
 import java.util.HashSet;
 import java.util.Iterator;
+import java.util.List;
 import java.util.Map;
 import java.util.HashMap;
 import java.util.Set;
+import java.util.SortedSet;
 
 import junit.framework.TestSuite;
 import junit.textui.TestRunner;
@@ -72,7 +74,7 @@
     public void testCommitUserData() throws Exception {
       RAMDirectory d = new MockRAMDirectory();
 
-      Map commitUserData = new HashMap();
+      Map<String,String> commitUserData = new HashMap<String,String>();
       commitUserData.put("foo", "fighters");
       
       // set up writer
@@ -260,12 +262,11 @@
     IndexReader reader = IndexReader.open(d, false);
     FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());
     reader.getTermFreqVector(0, mapper);
-    Map map = mapper.getFieldToTerms();
+    Map<String,SortedSet<TermVectorEntry>> map = mapper.getFieldToTerms();
     assertTrue("map is null and it shouldn't be", map != null);
     assertTrue("map Size: " + map.size() + " is not: " + 4, map.size() == 4);
-    Set set = (Set) map.get("termvector");
-    for (Iterator iterator = set.iterator(); iterator.hasNext();) {
-      TermVectorEntry entry = (TermVectorEntry) iterator.next();
+    SortedSet<TermVectorEntry> set = map.get("termvector");
+    for (final TermVectorEntry entry  : set) {
       assertTrue("entry is null and it shouldn't be", entry != null);
       System.out.println("Entry: " + entry);
     }
@@ -380,9 +381,9 @@
         for (int i = 0; i < bin.length; i++) {
           assertEquals(bin[i], data1[i + b1.getBinaryOffset()]);
         }
-        Set lazyFields = new HashSet();
+        Set<String> lazyFields = new HashSet<String>();
         lazyFields.add("bin1");
-        FieldSelector sel = new SetBasedFieldSelector(new HashSet(), lazyFields);
+        FieldSelector sel = new SetBasedFieldSelector(new HashSet<String>(), lazyFields);
         doc = reader.document(reader.maxDoc() - 1, sel);
         Fieldable[] fieldables = doc.getFieldables("bin1");
         assertNotNull(fieldables);
@@ -1340,11 +1341,11 @@
       assertEquals("Only one index is optimized.", index1.isOptimized(), index2.isOptimized());
       
       // check field names
-      Collection fields1 = index1.getFieldNames(FieldOption.ALL);
-      Collection fields2 = index1.getFieldNames(FieldOption.ALL);
+      Collection<String> fields1 = index1.getFieldNames(FieldOption.ALL);
+      Collection<String> fields2 = index1.getFieldNames(FieldOption.ALL);
       assertEquals("IndexReaders have different numbers of fields.", fields1.size(), fields2.size());
-      Iterator it1 = fields1.iterator();
-      Iterator it2 = fields1.iterator();
+      Iterator<String> it1 = fields1.iterator();
+      Iterator<String> it2 = fields1.iterator();
       while (it1.hasNext()) {
         assertEquals("Different field names.", (String) it1.next(), (String) it2.next());
       }
@@ -1378,14 +1379,14 @@
         if (!index1.isDeleted(i)) {
           Document doc1 = index1.document(i);
           Document doc2 = index2.document(i);
-          fields1 = doc1.getFields();
-          fields2 = doc2.getFields();
+          List<Fieldable> fieldable1 = doc1.getFields();
+          List<Fieldable> fieldable2 = doc2.getFields();
           assertEquals("Different numbers of fields for doc " + i + ".", fields1.size(), fields2.size());
-          it1 = fields1.iterator();
-          it2 = fields2.iterator();
-          while (it1.hasNext()) {
-            Field curField1 = (Field) it1.next();
-            Field curField2 = (Field) it2.next();
+          Iterator<Fieldable> itField1 = fieldable1.iterator();
+          Iterator<Fieldable> itField2 = fieldable2.iterator();
+          while (itField1.hasNext()) {
+            Field curField1 = (Field) itField1.next();
+            Field curField2 = (Field) itField2.next();
             assertEquals("Different fields names for doc " + i + ".", curField1.name(), curField2.name());
             assertEquals("Different field values for doc " + i + ".", curField1.stringValue(), curField2.stringValue());
           }          
@@ -1587,15 +1588,11 @@
     writer.addDocument(createDocument("a"));
     writer.close();
     
-    Collection commits = IndexReader.listCommits(dir);
-    Iterator it = commits.iterator();
-    while(it.hasNext()) {
-      IndexCommit commit = (IndexCommit) it.next();
-      Collection files = commit.getFileNames();
-      HashSet seen = new HashSet();
-      Iterator it2 = files.iterator();
-      while(it2.hasNext()) {
-        String fileName = (String) it2.next();
+    Collection<IndexCommit> commits = IndexReader.listCommits(dir);
+    for (final IndexCommit commit : commits) {
+      Collection<String> files = commit.getFileNames();
+      HashSet<String> seen = new HashSet<String>();
+      for (final String fileName : files) {
         assertTrue("file " + fileName + " was duplicated", !seen.contains(fileName));
         seen.add(fileName);
       }
Index: src/test/org/apache/lucene/index/TestStressIndexing2.java
===================================================================
--- src/test/org/apache/lucene/index/TestStressIndexing2.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestStressIndexing2.java	(working copy)
@@ -72,7 +72,7 @@
     // dir1 = FSDirectory.open("foofoofoo");
     Directory dir2 = new MockRAMDirectory();
     // mergeFactor=2; maxBufferedDocs=2; Map docs = indexRandom(1, 3, 2, dir1);
-    Map docs = indexRandom(10, 100, 100, dir1);
+    Map<String,Document> docs = indexRandom(10, 100, 100, dir1);
     indexSerial(docs, dir2);
 
     // verifying verify
@@ -96,7 +96,7 @@
       int range=r.nextInt(20)+1;
       Directory dir1 = new MockRAMDirectory();
       Directory dir2 = new MockRAMDirectory();
-      Map docs = indexRandom(nThreads, iter, range, dir1);
+      Map<String,Document> docs = indexRandom(nThreads, iter, range, dir1);
       indexSerial(docs, dir2);
       verifyEquals(dir1, dir2, "id");
     }
@@ -105,9 +105,9 @@
 
   static Term idTerm = new Term("id","");
   IndexingThread[] threads;
-  static Comparator fieldNameComparator = new Comparator() {
-        public int compare(Object o1, Object o2) {
-          return ((Fieldable)o1).name().compareTo(((Fieldable)o2).name());
+  static Comparator<Fieldable> fieldNameComparator = new Comparator<Fieldable>() {
+        public int compare(Fieldable o1, Fieldable o2) {
+          return o1.name().compareTo(o2.name());
         }
   };
 
@@ -116,12 +116,12 @@
   // everything.
   
   public static class DocsAndWriter {
-    Map docs;
+    Map<String,Document> docs;
     IndexWriter writer;
   }
   
   public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {
-    Map docs = new HashMap();
+    Map<String,Document> docs = new HashMap<String,Document>();
     IndexWriter w = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
     w.setUseCompoundFile(false);
 
@@ -171,8 +171,8 @@
     return dw;
   }
   
-  public Map indexRandom(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {
-    Map docs = new HashMap();
+  public Map<String,Document> indexRandom(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {
+    Map<String,Document> docs = new HashMap<String,Document>();
     for(int iter=0;iter<3;iter++) {
       IndexWriter w = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
       w.setUseCompoundFile(false);
@@ -216,14 +216,12 @@
   }
 
   
-  public static void indexSerial(Map docs, Directory dir) throws IOException {
+  public static void indexSerial(Map<String,Document> docs, Directory dir) throws IOException {
     IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
 
     // index all docs in a single thread
-    Iterator iter = docs.values().iterator();
-    while (iter.hasNext()) {
-      Document d = (Document)iter.next();
-      ArrayList fields = new ArrayList();
+    for(final Document d : docs.values()) {
+      ArrayList<Fieldable> fields = new ArrayList<Fieldable>();
       fields.addAll(d.getFields());
       // put fields in same order each time
       Collections.sort(fields, fieldNameComparator);
@@ -390,8 +388,8 @@
   }
 
   public static void verifyEquals(Document d1, Document d2) {
-    List ff1 = d1.getFields();
-    List ff2 = d2.getFields();
+    List<Fieldable> ff1 = d1.getFields();
+    List<Fieldable> ff2 = d2.getFields();
 
     Collections.sort(ff1, fieldNameComparator);
     Collections.sort(ff2, fieldNameComparator);
@@ -479,7 +477,7 @@
     int base;
     int range;
     int iterations;
-    Map docs = new HashMap();  // Map<String,Document>
+    Map<String,Document> docs = new HashMap<String,Document>(); 
     Random r;
 
     public int nextInt(int lim) {
@@ -560,7 +558,7 @@
     public void indexDoc() throws IOException {
       Document d = new Document();
 
-      ArrayList fields = new ArrayList();      
+      ArrayList<Field> fields = new ArrayList<Field>();      
       String idString = getIdString();
       Field idField =  new Field(idTerm.field(), idString, Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS);
       fields.add(idField);
Index: src/test/org/apache/lucene/index/TestWordlistLoader.java
===================================================================
--- src/test/org/apache/lucene/index/TestWordlistLoader.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestWordlistLoader.java	(working copy)
@@ -30,22 +30,22 @@
 
   public void testWordlistLoading() throws IOException {
     String s = "ONE\n  two \nthree";
-    HashSet wordSet1 = WordlistLoader.getWordSet(new StringReader(s));
+    HashSet<String> wordSet1 = WordlistLoader.getWordSet(new StringReader(s));
     checkSet(wordSet1);
-    HashSet wordSet2 = WordlistLoader.getWordSet(new BufferedReader(new StringReader(s)));
+    HashSet<String> wordSet2 = WordlistLoader.getWordSet(new BufferedReader(new StringReader(s)));
     checkSet(wordSet2);
   }
 
   public void testComments() throws Exception {
     String s = "ONE\n  two \nthree\n#comment";
-    HashSet wordSet1 = WordlistLoader.getWordSet(new StringReader(s), "#");
+    HashSet<String> wordSet1 = WordlistLoader.getWordSet(new StringReader(s), "#");
     checkSet(wordSet1);
     assertFalse(wordSet1.contains("#comment"));
     assertFalse(wordSet1.contains("comment"));
   }
 
 
-  private void checkSet(HashSet wordset) {
+  private void checkSet(HashSet<String> wordset) {
     assertEquals(3, wordset.size());
     assertTrue(wordset.contains("ONE"));		// case is not modified
     assertTrue(wordset.contains("two"));		// surrounding whitespace is removed
Index: src/test/org/apache/lucene/index/TestTermVectorsReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestTermVectorsReader.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestTermVectorsReader.java	(working copy)
@@ -20,12 +20,10 @@
 import java.io.IOException;
 import java.io.Reader;
 import java.util.Arrays;
-import java.util.Iterator;
 import java.util.Map;
 import java.util.SortedSet;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
@@ -255,13 +253,12 @@
     assertTrue(reader != null);
     SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());
     reader.get(0, mapper);
-    SortedSet set = mapper.getTermVectorEntrySet();
+    SortedSet<TermVectorEntry> set = mapper.getTermVectorEntrySet();
     assertTrue("set is null and it shouldn't be", set != null);
     //three fields, 4 terms, all terms are the same
     assertTrue("set Size: " + set.size() + " is not: " + 4, set.size() == 4);
     //Check offsets and positions
-    for (Iterator iterator = set.iterator(); iterator.hasNext();) {
-      TermVectorEntry tve = (TermVectorEntry) iterator.next();
+    for (final TermVectorEntry tve :  set) {
       assertTrue("tve is null and it shouldn't be", tve != null);
       assertTrue("tve.getOffsets() is null and it shouldn't be", tve.getOffsets() != null);
       assertTrue("tve.getPositions() is null and it shouldn't be", tve.getPositions() != null);
@@ -275,8 +272,7 @@
     //three fields, 4 terms, all terms are the same
     assertTrue("set Size: " + set.size() + " is not: " + 4, set.size() == 4);
     //Should have offsets and positions b/c we are munging all the fields together
-    for (Iterator iterator = set.iterator(); iterator.hasNext();) {
-      TermVectorEntry tve = (TermVectorEntry) iterator.next();
+    for (final TermVectorEntry tve  : set) {
       assertTrue("tve is null and it shouldn't be", tve != null);
       assertTrue("tve.getOffsets() is null and it shouldn't be", tve.getOffsets() != null);
       assertTrue("tve.getPositions() is null and it shouldn't be", tve.getPositions() != null);
@@ -286,14 +282,12 @@
 
     FieldSortedTermVectorMapper fsMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());
     reader.get(0, fsMapper);
-    Map map = fsMapper.getFieldToTerms();
+    Map<String,SortedSet<TermVectorEntry>> map = fsMapper.getFieldToTerms();
     assertTrue("map Size: " + map.size() + " is not: " + testFields.length, map.size() == testFields.length);
-    for (Iterator iterator = map.entrySet().iterator(); iterator.hasNext();) {
-      Map.Entry entry = (Map.Entry) iterator.next();
-      SortedSet sortedSet = (SortedSet) entry.getValue();
+    for (final Map.Entry<String,SortedSet<TermVectorEntry>> entry : map.entrySet()) {
+      SortedSet<TermVectorEntry> sortedSet = entry.getValue();
       assertTrue("sortedSet Size: " + sortedSet.size() + " is not: " + 4, sortedSet.size() == 4);
-      for (Iterator inner = sortedSet.iterator(); inner.hasNext();) {
-        TermVectorEntry tve = (TermVectorEntry) inner.next();
+      for (final TermVectorEntry tve : sortedSet) {
         assertTrue("tve is null and it shouldn't be", tve != null);
         //Check offsets and positions.
         assertTrue("tve is null and it shouldn't be", tve != null);
@@ -317,12 +311,10 @@
     reader.get(0, fsMapper);
     map = fsMapper.getFieldToTerms();
     assertTrue("map Size: " + map.size() + " is not: " + testFields.length, map.size() == testFields.length);
-    for (Iterator iterator = map.entrySet().iterator(); iterator.hasNext();) {
-      Map.Entry entry = (Map.Entry) iterator.next();
-      SortedSet sortedSet = (SortedSet) entry.getValue();
+    for (final Map.Entry<String,SortedSet<TermVectorEntry>> entry : map.entrySet()) {
+      final SortedSet<TermVectorEntry> sortedSet = entry.getValue();
       assertTrue("sortedSet Size: " + sortedSet.size() + " is not: " + 4, sortedSet.size() == 4);
-      for (Iterator inner = sortedSet.iterator(); inner.hasNext();) {
-        TermVectorEntry tve = (TermVectorEntry) inner.next();
+      for (final TermVectorEntry tve : sortedSet) {
         assertTrue("tve is null and it shouldn't be", tve != null);
         //Check offsets and positions.
         assertTrue("tve is null and it shouldn't be", tve != null);
Index: src/test/org/apache/lucene/index/TestIndexWriter.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexWriter.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestIndexWriter.java	(working copy)
@@ -2121,7 +2121,7 @@
         writer.setMergeFactor(2);
 
         final IndexWriter finalWriter = writer;
-        final ArrayList failure = new ArrayList();
+        final ArrayList<Throwable> failure = new ArrayList<Throwable>();
         Thread t1 = new Thread() {
             public void run() {
               boolean done = false;
@@ -3457,13 +3457,13 @@
       final TermAttribute termAtt = addAttribute(TermAttribute.class);
       final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
       
-      final Iterator tokens = Arrays.asList(new String[]{"a","b","c"}).iterator();
+      final Iterator<String> tokens = Arrays.asList(new String[]{"a","b","c"}).iterator();
       boolean first = true;
       
       public boolean incrementToken() {
         if (!tokens.hasNext()) return false;
         clearAttributes();
-        termAtt.setTermBuffer((String) tokens.next());
+        termAtt.setTermBuffer(tokens.next());
         posIncrAtt.setPositionIncrement(first ? 0 : 1);
         first = false;
         return true;
@@ -3624,7 +3624,7 @@
     Directory dir, dir2;
     final static int NUM_INIT_DOCS = 17;
     IndexWriter writer2;
-    final List failures = new ArrayList();
+    final List<Throwable> failures = new ArrayList<Throwable>();
     volatile boolean didClose;
     final IndexReader[] readers;
     final int NUM_COPY;
Index: src/test/org/apache/lucene/index/TestCheckIndex.java
===================================================================
--- src/test/org/apache/lucene/index/TestCheckIndex.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestCheckIndex.java	(working copy)
@@ -84,7 +84,7 @@
     assertEquals(18, seg.termVectorStatus.totVectors);
 
     assertTrue(seg.diagnostics.size() > 0);
-    final List onlySegments = new ArrayList();
+    final List<String> onlySegments = new ArrayList<String>();
     onlySegments.add("_0");
     
     assertTrue(checker.checkIndex(onlySegments).clean == true);
Index: src/test/org/apache/lucene/index/TestIndexFileDeleter.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexFileDeleter.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestIndexFileDeleter.java	(working copy)
@@ -28,8 +28,8 @@
 import org.apache.lucene.document.Field;
 import java.io.*;
 import java.util.*;
-import java.util.zip.*;
 
+
 /*
   Verify we can read the pre-2.1 file format, do searches
   against it, and add documents to it.
@@ -155,33 +155,33 @@
     Arrays.sort(files);
     Arrays.sort(files2);
     
-    Set dif = difFiles(files, files2);
+    Set<String> dif = difFiles(files, files2);
     
     if (!Arrays.equals(files, files2)) {
       fail("IndexFileDeleter failed to delete unreferenced extra files: should have deleted " + (filesPre.length-files.length) + " files but only deleted " + (filesPre.length - files2.length) + "; expected files:\n    " + asString(files) + "\n  actual files:\n    " + asString(files2)+"\ndif: "+dif);
     }
   }
 
-  private static Set difFiles(String[] files1, String[] files2) {
-    Set set1 = new HashSet();
-    Set set2 = new HashSet();
-    Set extra = new HashSet();
+  private static Set<String> difFiles(String[] files1, String[] files2) {
+    Set<String> set1 = new HashSet<String>();
+    Set<String> set2 = new HashSet<String>();
+    Set<String> extra = new HashSet<String>();
     for (int x=0; x < files1.length; x++) {
       set1.add(files1[x]);
     }
     for (int x=0; x < files2.length; x++) {
       set2.add(files2[x]);
     }
-    Iterator i1 = set1.iterator();
+    Iterator<String> i1 = set1.iterator();
     while (i1.hasNext()) {
-      Object o = i1.next();
+      String o = i1.next();
       if (!set2.contains(o)) {
         extra.add(o);
       }
     }
-    Iterator i2 = set2.iterator();
+    Iterator<String> i2 = set2.iterator();
     while (i2.hasNext()) {
-      Object o = i2.next();
+      String o = i2.next();
       if (!set1.contains(o)) {
         extra.add(o);
       }
Index: src/test/org/apache/lucene/index/TestIndexReaderCloneNorms.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexReaderCloneNorms.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestIndexReaderCloneNorms.java	(working copy)
@@ -54,9 +54,9 @@
 
   private int numDocNorms;
 
-  private ArrayList norms;
+  private ArrayList<Float> norms;
 
-  private ArrayList modifiedNorms;
+  private ArrayList<Float> modifiedNorms;
 
   private float lastNorm = 0;
 
@@ -89,19 +89,19 @@
     Directory dir1 = FSDirectory.open(indexDir1);
     IndexWriter.unlock(dir1);
 
-    norms = new ArrayList();
-    modifiedNorms = new ArrayList();
+    norms = new ArrayList<Float>();
+    modifiedNorms = new ArrayList<Float>();
 
     createIndex(dir1);
     doTestNorms(dir1);
 
     // test with a single index: index2
-    ArrayList norms1 = norms;
-    ArrayList modifiedNorms1 = modifiedNorms;
+    ArrayList<Float> norms1 = norms;
+    ArrayList<Float> modifiedNorms1 = modifiedNorms;
     int numDocNorms1 = numDocNorms;
 
-    norms = new ArrayList();
-    modifiedNorms = new ArrayList();
+    norms = new ArrayList<Float>();
+    modifiedNorms = new ArrayList<Float>();
     numDocNorms = 0;
 
     File indexDir2 = new File(tempDir, "lucenetestindex2");
@@ -279,7 +279,7 @@
       String field = "f" + i;
       byte b[] = ir.norms(field);
       assertEquals("number of norms mismatches", numDocNorms, b.length);
-      ArrayList storedNorms = (i == 1 ? modifiedNorms : norms);
+      ArrayList<Float> storedNorms = (i == 1 ? modifiedNorms : norms);
       for (int j = 0; j < b.length; j++) {
         float norm = Similarity.decodeNorm(b[j]);
         float norm1 = ((Float) storedNorms.get(j)).floatValue();
Index: src/test/org/apache/lucene/index/TestByteSlices.java
===================================================================
--- src/test/org/apache/lucene/index/TestByteSlices.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestByteSlices.java	(working copy)
@@ -21,7 +21,7 @@
 public class TestByteSlices extends LuceneTestCase {
 
   private static class ByteBlockAllocator extends ByteBlockPool.Allocator {
-    ArrayList freeByteBlocks = new ArrayList();
+    ArrayList<byte[]> freeByteBlocks = new ArrayList<byte[]>();
     
     /* Allocate another byte[] from the shared pool */
     synchronized byte[] getByteBlock(boolean trackAllocations) {
Index: src/test/org/apache/lucene/index/TestAtomicUpdate.java
===================================================================
--- src/test/org/apache/lucene/index/TestAtomicUpdate.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestAtomicUpdate.java	(working copy)
@@ -20,8 +20,6 @@
 import org.apache.lucene.store.*;
 import org.apache.lucene.document.*;
 import org.apache.lucene.analysis.*;
-import org.apache.lucene.search.*;
-import org.apache.lucene.queryParser.*;
 
 import java.util.Random;
 import java.io.File;
Index: src/test/org/apache/lucene/index/TestPositionBasedTermVectorMapper.java
===================================================================
--- src/test/org/apache/lucene/index/TestPositionBasedTermVectorMapper.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestPositionBasedTermVectorMapper.java	(working copy)
@@ -15,11 +15,12 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.PositionBasedTermVectorMapper.TVPositionInfo;
 import org.apache.lucene.util.LuceneTestCase;
 
 import java.io.IOException;
 import java.util.BitSet;
-import java.util.Iterator;
+
 import java.util.Map;
 
 public class TestPositionBasedTermVectorMapper extends LuceneTestCase {
@@ -68,17 +69,16 @@
       mapper.map(token, 1, null, thePositions[i]);
 
     }
-    Map map = mapper.getFieldToTerms();
+    Map<String,Map<Integer,TVPositionInfo>> map = mapper.getFieldToTerms();
     assertTrue("map is null and it shouldn't be", map != null);
     assertTrue("map Size: " + map.size() + " is not: " + 1, map.size() == 1);
-    Map positions = (Map) map.get("test");
+    Map<Integer,TVPositionInfo> positions = map.get("test");
     assertTrue("thePositions is null and it shouldn't be", positions != null);
     
     assertTrue("thePositions Size: " + positions.size() + " is not: " + numPositions, positions.size() == numPositions);
     BitSet bits = new BitSet(numPositions);
-    for (Iterator iterator = positions.entrySet().iterator(); iterator.hasNext();) {
-      Map.Entry entry = (Map.Entry) iterator.next();
-      PositionBasedTermVectorMapper.TVPositionInfo info = (PositionBasedTermVectorMapper.TVPositionInfo) entry.getValue();
+    for (Map.Entry<Integer,TVPositionInfo> entry : positions.entrySet()) {
+      PositionBasedTermVectorMapper.TVPositionInfo info = entry.getValue();
       assertTrue("info is null and it shouldn't be", info != null);
       int pos = ((Integer) entry.getKey()).intValue();
       bits.set(pos);
Index: src/test/org/apache/lucene/index/TestIndexWriterReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexWriterReader.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestIndexWriterReader.java	(working copy)
@@ -305,10 +305,10 @@
     final static int NUM_THREADS = 5;
     final Thread[] threads = new Thread[NUM_THREADS];
     IndexWriter mainWriter;
-    List deletedTerms = new ArrayList();
-    LinkedList toDeleteTerms = new LinkedList();
+    List<Term> deletedTerms = new ArrayList<Term>();
+    LinkedList<Term> toDeleteTerms = new LinkedList<Term>();
     Random random;
-    final List failures = new ArrayList();
+    final List<Throwable> failures = new ArrayList<Throwable>();
     
     public DeleteThreads(IndexWriter mainWriter) throws IOException {
       this.mainWriter = mainWriter;
@@ -371,7 +371,7 @@
     int numDirs;
     final Thread[] threads = new Thread[NUM_THREADS];
     IndexWriter mainWriter;
-    final List failures = new ArrayList();
+    final List<Throwable> failures = new ArrayList<Throwable>();
     IndexReader[] readers;
     boolean didClose = false;
     HeavyAtomicInt count = new HeavyAtomicInt(0);
@@ -719,7 +719,7 @@
     final float SECONDS = 3;
 
     final long endTime = (long) (System.currentTimeMillis() + 1000.*SECONDS);
-    final List excs = Collections.synchronizedList(new ArrayList());
+    final List<Throwable> excs = Collections.synchronizedList(new ArrayList<Throwable>());
 
     final Thread[] threads = new Thread[NUM_THREAD];
     for(int i=0;i<NUM_THREAD;i++) {
@@ -782,7 +782,7 @@
     final float SECONDS = 3;
 
     final long endTime = (long) (System.currentTimeMillis() + 1000.*SECONDS);
-    final List excs = Collections.synchronizedList(new ArrayList());
+    final List<Throwable> excs = Collections.synchronizedList(new ArrayList<Throwable>());
 
     final Thread[] threads = new Thread[NUM_THREAD];
     for(int i=0;i<NUM_THREAD;i++) {
Index: src/test/org/apache/lucene/index/TestFieldsReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestFieldsReader.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestFieldsReader.java	(working copy)
@@ -99,10 +99,10 @@
     FieldsReader reader = new FieldsReader(dir, TEST_SEGMENT_NAME, fieldInfos);
     assertTrue(reader != null);
     assertTrue(reader.size() == 1);
-    Set loadFieldNames = new HashSet();
+    Set<String> loadFieldNames = new HashSet<String>();
     loadFieldNames.add(DocHelper.TEXT_FIELD_1_KEY);
     loadFieldNames.add(DocHelper.TEXT_FIELD_UTF1_KEY);
-    Set lazyFieldNames = new HashSet();
+    Set<String> lazyFieldNames = new HashSet<String>();
     //new String[]{DocHelper.LARGE_LAZY_FIELD_KEY, DocHelper.LAZY_FIELD_KEY, DocHelper.LAZY_FIELD_BINARY_KEY};
     lazyFieldNames.add(DocHelper.LARGE_LAZY_FIELD_KEY);
     lazyFieldNames.add(DocHelper.LAZY_FIELD_KEY);
@@ -149,10 +149,10 @@
     FieldsReader reader = new FieldsReader(dir, TEST_SEGMENT_NAME, fieldInfos);
     assertTrue(reader != null);
     assertTrue(reader.size() == 1);
-    Set loadFieldNames = new HashSet();
+    Set<String> loadFieldNames = new HashSet<String>();
     loadFieldNames.add(DocHelper.TEXT_FIELD_1_KEY);
     loadFieldNames.add(DocHelper.TEXT_FIELD_UTF1_KEY);
-    Set lazyFieldNames = new HashSet();
+    Set<String> lazyFieldNames = new HashSet<String>();
     lazyFieldNames.add(DocHelper.LARGE_LAZY_FIELD_KEY);
     lazyFieldNames.add(DocHelper.LAZY_FIELD_KEY);
     lazyFieldNames.add(DocHelper.LAZY_FIELD_BINARY_KEY);
@@ -182,9 +182,7 @@
     Document doc = reader.doc(0, fieldSelector);
     assertTrue("doc is null and it shouldn't be", doc != null);
     int count = 0;
-    List l = doc.getFields();
-    for (Iterator iter = l.iterator(); iter.hasNext();) {
-      Field field = (Field) iter.next();
+    for (final Fieldable field : doc.getFields()) {
       assertTrue("field is null and it shouldn't be", field != null);
       String sv = field.stringValue();
       assertTrue("sv is null and it shouldn't be", sv != null);
@@ -219,7 +217,7 @@
     long lazyTime = 0;
     long regularTime = 0;
     int length = 50;
-    Set lazyFieldNames = new HashSet();
+    Set<String> lazyFieldNames = new HashSet<String>();
     lazyFieldNames.add(DocHelper.LARGE_LAZY_FIELD_KEY);
     SetBasedFieldSelector fieldSelector = new SetBasedFieldSelector(Collections.EMPTY_SET, lazyFieldNames);
 
Index: src/test/org/apache/lucene/index/TestTransactionRollback.java
===================================================================
--- src/test/org/apache/lucene/index/TestTransactionRollback.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestTransactionRollback.java	(working copy)
@@ -21,7 +21,6 @@
 import java.io.IOException;
 import java.util.BitSet;
 import java.util.Collection;
-import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.HashMap;
@@ -54,10 +53,9 @@
     // System.out.println("Attempting to rollback to "+id);
     String ids="-"+id;
     IndexCommit last=null;
-    Collection commits = IndexReader.listCommits(dir);
-    for (Iterator iterator = commits.iterator(); iterator.hasNext();) {
-      IndexCommit commit = (IndexCommit) iterator.next();
-      Map ud=commit.getUserData();
+    Collection<IndexCommit> commits = IndexReader.listCommits(dir);
+    for (final IndexCommit commit : commits) {
+      Map<String,String> ud=commit.getUserData();
       if (ud.size() > 0)
         if (((String) ud.get("index")).endsWith(ids))
           last=commit;
@@ -68,7 +66,7 @@
 		
     IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(),
                                     new RollbackDeletionPolicy(id), MaxFieldLength.UNLIMITED, last);
-    Map data = new HashMap();
+    Map<String,String> data = new HashMap<String,String>();
     data.put("index", "Rolled back to 1-"+id);
     w.commit(data);
     w.close();
@@ -134,7 +132,7 @@
       w.addDocument(doc);
 			
       if (currentRecordId%10 == 0) {
-        Map data = new HashMap();
+        Map<String,String> data = new HashMap<String,String>();
         data.put("index", "records 1-"+currentRecordId);
         w.commit(data);
       }
@@ -151,18 +149,17 @@
       this.rollbackPoint = rollbackPoint;
     }
 
-    public void onCommit(List commits) throws IOException {
+    public void onCommit(List<IndexCommit> commits) throws IOException {
     }
 
-    public void onInit(List commits) throws IOException {
-      for (Iterator iterator = commits.iterator(); iterator.hasNext();) {
-        IndexCommit commit = (IndexCommit) iterator.next();
-        Map userData=commit.getUserData();
+    public void onInit(List<IndexCommit> commits) throws IOException {
+      for (final IndexCommit commit : commits) {
+        Map<String,String> userData=commit.getUserData();
         if (userData.size() > 0) {
           // Label for a commit point is "Records 1-30"
           // This code reads the last id ("30" in this example) and deletes it
           // if it is after the desired rollback point
-          String x = (String) userData.get("index");
+          String x =  userData.get("index");
           String lastVal = x.substring(x.lastIndexOf("-")+1);
           int last = Integer.parseInt(lastVal);
           if (last>rollbackPoint) {
@@ -185,10 +182,10 @@
 
   class DeleteLastCommitPolicy implements IndexDeletionPolicy {
 
-    public void onCommit(List commits) throws IOException {}
+    public void onCommit(List<IndexCommit> commits) throws IOException {}
 
-    public void onInit(List commits) throws IOException {
-      ((IndexCommit) commits.get(commits.size()-1)).delete();
+    public void onInit(List<IndexCommit> commits) throws IOException {
+      commits.get(commits.size()-1).delete();
     }
   }
 
@@ -207,7 +204,7 @@
 	
   // Keeps all commit points (used to build index)
   class KeepAllDeletionPolicy implements IndexDeletionPolicy {
-    public void onCommit(List commits) throws IOException {}
-    public void onInit(List commits) throws IOException {}
+    public void onCommit(List<IndexCommit> commits) throws IOException {}
+    public void onInit(List<IndexCommit> commits) throws IOException {}
   }
 }
Index: src/test/org/apache/lucene/index/TestIndexReaderReopen.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexReaderReopen.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestIndexReaderReopen.java	(working copy)
@@ -22,7 +22,6 @@
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashSet;
-import java.util.Iterator;
 import java.util.List;
 import java.util.Random;
 import java.util.Map;
@@ -716,13 +715,13 @@
       }      
     };
     
-    final List readers = Collections.synchronizedList(new ArrayList());
+    final List<ReaderCouple> readers = Collections.synchronizedList(new ArrayList<ReaderCouple>());
     IndexReader firstReader = IndexReader.open(dir, false);
     IndexReader reader = firstReader;
     final Random rnd = newRandom();
     
     ReaderThread[] threads = new ReaderThread[n];
-    final Set readersToClose = Collections.synchronizedSet(new HashSet());
+    final Set<IndexReader> readersToClose = Collections.synchronizedSet(new HashSet<IndexReader>());
     
     for (int i = 0; i < n; i++) {
       if (i % 10 == 0) {
@@ -827,17 +826,15 @@
       
     }
     
-    Iterator it = readersToClose.iterator();
-    while (it.hasNext()) {
-      ((IndexReader) it.next()).close();
+    for (final IndexReader ir : readersToClose) {
+      ir.close();
     }
     
     firstReader.close();
     reader.close();
     
-    it = readersToClose.iterator();
-    while (it.hasNext()) {
-      assertReaderClosed((IndexReader) it.next(), true, true);
+    for (final IndexReader ir : readersToClose) {
+      assertReaderClosed(ir, true, true);
     }
 
     assertReaderClosed(reader, true, true);
@@ -1165,9 +1162,9 @@
   }
 
   private static class KeepAllCommits implements IndexDeletionPolicy {
-    public void onInit(List commits) {
+    public void onInit(List<IndexCommit> commits) {
     }
-    public void onCommit(List commits) {
+    public void onCommit(List<IndexCommit> commits) {
     }
   }
 
@@ -1178,13 +1175,13 @@
       Document doc = new Document();
       doc.add(new Field("id", ""+i, Field.Store.NO, Field.Index.NOT_ANALYZED));
       writer.addDocument(doc);
-      Map data = new HashMap();
+      Map<String,String> data = new HashMap<String,String>();
       data.put("index", i+"");
       writer.commit(data);
     }
     for(int i=0;i<4;i++) {
       writer.deleteDocuments(new Term("id", ""+i));
-      Map data = new HashMap();
+      Map<String,String> data = new HashMap<String,String>();
       data.put("index", (4+i)+"");
       writer.commit(data);
     }
@@ -1194,9 +1191,7 @@
     assertEquals(0, r.numDocs());
     assertEquals(4, r.maxDoc());
 
-    Iterator it = IndexReader.listCommits(dir).iterator();
-    while(it.hasNext()) {
-      IndexCommit commit = (IndexCommit) it.next();
+    for (final IndexCommit commit : IndexReader.listCommits(dir)) {
       IndexReader r2 = r.reopen(commit);
       assertTrue(r2 != r);
 
@@ -1208,13 +1203,13 @@
         // expected
       }
 
-      final Map s = commit.getUserData();
+      final Map<String,String> s = commit.getUserData();
       final int v;
       if (s.size() == 0) {
         // First commit created by IW
         v = -1;
       } else {
-        v = Integer.parseInt((String) s.get("index"));
+        v = Integer.parseInt(s.get("index"));
       }
       if (v < 4) {
         assertEquals(1+v, r2.numDocs());
Index: src/test/org/apache/lucene/index/TestSegmentMerger.java
===================================================================
--- src/test/org/apache/lucene/index/TestSegmentMerger.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestSegmentMerger.java	(working copy)
@@ -84,7 +84,7 @@
     assertTrue(termDocs != null);
     assertTrue(termDocs.next() == true);
     
-    Collection stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);
+    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);
     assertTrue(stored != null);
     //System.out.println("stored size: " + stored.size());
     assertTrue("We do not have 3 fields that were indexed with term vector",stored.size() == 3);
Index: src/test/org/apache/lucene/index/TestStressIndexing.java
===================================================================
--- src/test/org/apache/lucene/index/TestStressIndexing.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestStressIndexing.java	(working copy)
@@ -21,8 +21,8 @@
 import org.apache.lucene.document.*;
 import org.apache.lucene.analysis.*;
 import org.apache.lucene.search.*;
-import org.apache.lucene.queryParser.*;
 
+
 import java.util.Random;
 import java.io.File;
 
Index: src/test/org/apache/lucene/index/TestPayloads.java
===================================================================
--- src/test/org/apache/lucene/index/TestPayloads.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestPayloads.java	(working copy)
@@ -382,7 +382,7 @@
      * This Analyzer uses an WhitespaceTokenizer and PayloadFilter.
      */
     private static class PayloadAnalyzer extends Analyzer {
-        Map fieldToData = new HashMap();
+        Map<String,PayloadData> fieldToData = new HashMap<String,PayloadData>();
         
         void setPayloadData(String field, byte[] data, int offset, int length) {
             fieldToData.put(field, new PayloadData(0, data, offset, length));
@@ -545,10 +545,10 @@
     }
     
     private static class ByteArrayPool {
-        private List pool;
+        private List<byte[]> pool;
         
         ByteArrayPool(int capacity, int size) {
-            pool = new ArrayList();
+            pool = new ArrayList<byte[]>();
             for (int i = 0; i < capacity; i++) {
                 pool.add(new byte[size]);
             }
Index: src/test/org/apache/lucene/index/TestLazyBug.java
===================================================================
--- src/test/org/apache/lucene/index/TestLazyBug.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestLazyBug.java	(working copy)
@@ -46,7 +46,7 @@
     "this string is a bigger string, mary had a little lamb, little lamb, little lamb!"
   };
 
-  private static Set dataset = new HashSet(Arrays.asList(data));
+  private static Set<String> dataset = new HashSet<String>(Arrays.asList(data));
   
   private static String MAGIC_FIELD = "f"+(NUM_FIELDS/3);
   
@@ -93,11 +93,9 @@
       Document d = reader.document(docs[i], SELECTOR);
       d.get(MAGIC_FIELD);
       
-      List fields = d.getFields();
-      for (Iterator fi = fields.iterator(); fi.hasNext(); ) {
-        Fieldable f=null;
+      List<Fieldable> fields = d.getFields();
+      for (final Fieldable f : fields ) {
         try {
-          f = (Fieldable) fi.next();
           String fname = f.name();
           String fval = f.stringValue();
           assertNotNull(docs[i]+" FIELD: "+fname, fval);
Index: src/test/org/apache/lucene/index/TestParallelReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestParallelReader.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestParallelReader.java	(working copy)
@@ -70,7 +70,7 @@
     ParallelReader pr = new ParallelReader();
     pr.add(IndexReader.open(dir1, false));
     pr.add(IndexReader.open(dir2, false));
-    Collection fieldNames = pr.getFieldNames(IndexReader.FieldOption.ALL);
+    Collection<String> fieldNames = pr.getFieldNames(IndexReader.FieldOption.ALL);
     assertEquals(4, fieldNames.size());
     assertTrue(fieldNames.contains("f1"));
     assertTrue(fieldNames.contains("f2"));
Index: src/test/org/apache/lucene/index/TestDeletionPolicy.java
===================================================================
--- src/test/org/apache/lucene/index/TestDeletionPolicy.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestDeletionPolicy.java	(working copy)
@@ -19,7 +19,7 @@
 
 import java.io.IOException;
 import java.util.HashSet;
-import java.util.Iterator;
+
 import java.util.List;
 import java.util.Set;
 import java.util.Collection;
@@ -43,14 +43,14 @@
 
 public class TestDeletionPolicy extends LuceneTestCase
 {
-  private void verifyCommitOrder(List commits) throws IOException {
-    final IndexCommit firstCommit = ((IndexCommit) commits.get(0));
+  private void verifyCommitOrder(List<IndexCommit> commits) throws IOException {
+    final IndexCommit firstCommit = commits.get(0);
     long last = SegmentInfos.generationFromSegmentsFileName(firstCommit.getSegmentsFileName());
     assertEquals(last, firstCommit.getGeneration());
     long lastVersion = firstCommit.getVersion();
     long lastTimestamp = firstCommit.getTimestamp();
     for(int i=1;i<commits.size();i++) {
-      final IndexCommit commit = ((IndexCommit) commits.get(i));
+      final IndexCommit commit = commits.get(i);
       long now = SegmentInfos.generationFromSegmentsFileName(commit.getSegmentsFileName());
       long nowVersion = commit.getVersion();
       long nowTimestamp = commit.getTimestamp();
@@ -68,12 +68,12 @@
     int numOnInit;
     int numOnCommit;
     Directory dir;
-    public void onInit(List commits) throws IOException {
+    public void onInit(List<IndexCommit> commits) throws IOException {
       verifyCommitOrder(commits);
       numOnInit++;
     }
-    public void onCommit(List commits) throws IOException {
-      IndexCommit lastCommit = (IndexCommit) commits.get(commits.size()-1);
+    public void onCommit(List<IndexCommit> commits) throws IOException {
+      IndexCommit lastCommit = commits.get(commits.size()-1);
       IndexReader r = IndexReader.open(dir, true);
       assertEquals("lastCommit.isOptimized()=" + lastCommit.isOptimized() + " vs IndexReader.isOptimized=" + r.isOptimized(), r.isOptimized(), lastCommit.isOptimized());
       r.close();
@@ -89,23 +89,21 @@
   class KeepNoneOnInitDeletionPolicy implements IndexDeletionPolicy {
     int numOnInit;
     int numOnCommit;
-    public void onInit(List commits) throws IOException {
+    public void onInit(List<IndexCommit> commits) throws IOException {
       verifyCommitOrder(commits);
       numOnInit++;
       // On init, delete all commit points:
-      Iterator it = commits.iterator();
-      while(it.hasNext()) {
-        final IndexCommit commit = (IndexCommit) it.next();
+      for (final IndexCommit commit : commits) {
         commit.delete();
         assertTrue(commit.isDeleted());
       }
     }
-    public void onCommit(List commits) throws IOException {
+    public void onCommit(List<IndexCommit> commits) throws IOException {
       verifyCommitOrder(commits);
       int size = commits.size();
       // Delete all but last one:
       for(int i=0;i<size-1;i++) {
-        ((IndexCommit) commits.get(i)).delete();
+        commits.get(i).delete();
       }
       numOnCommit++;
     }
@@ -116,30 +114,30 @@
     int numOnCommit;
     int numToKeep;
     int numDelete;
-    Set seen = new HashSet();
+    Set<String> seen = new HashSet<String>();
 
     public KeepLastNDeletionPolicy(int numToKeep) {
       this.numToKeep = numToKeep;
     }
 
-    public void onInit(List commits) throws IOException {
+    public void onInit(List<IndexCommit> commits) throws IOException {
       verifyCommitOrder(commits);
       numOnInit++;
       // do no deletions on init
       doDeletes(commits, false);
     }
 
-    public void onCommit(List commits) throws IOException {
+    public void onCommit(List<IndexCommit> commits) throws IOException {
       verifyCommitOrder(commits);
       doDeletes(commits, true);
     }
     
-    private void doDeletes(List commits, boolean isCommit) {
+    private void doDeletes(List<IndexCommit> commits, boolean isCommit) {
 
       // Assert that we really are only called for each new
       // commit:
       if (isCommit) {
-        String fileName = ((IndexCommit) commits.get(commits.size()-1)).getSegmentsFileName();
+        String fileName = commits.get(commits.size()-1).getSegmentsFileName();
         if (seen.contains(fileName)) {
           throw new RuntimeException("onCommit was called twice on the same commit point: " + fileName);
         }
@@ -148,7 +146,7 @@
       }
       int size = commits.size();
       for(int i=0;i<size-numToKeep;i++) {
-        ((IndexCommit) commits.get(i)).delete();
+        commits.get(i).delete();
         numDelete++;
       }
     }
@@ -169,23 +167,21 @@
       this.expirationTimeSeconds = seconds;
     }
 
-    public void onInit(List commits) throws IOException {
+    public void onInit(List<IndexCommit> commits) throws IOException {
       verifyCommitOrder(commits);
       onCommit(commits);
     }
 
-    public void onCommit(List commits) throws IOException {
+    public void onCommit(List<IndexCommit> commits) throws IOException {
       verifyCommitOrder(commits);
 
-      IndexCommit lastCommit = (IndexCommit) commits.get(commits.size()-1);
+      IndexCommit lastCommit = commits.get(commits.size()-1);
 
       // Any commit older than expireTime should be deleted:
       double expireTime = dir.fileModified(lastCommit.getSegmentsFileName())/1000.0 - expirationTimeSeconds;
 
-      Iterator it = commits.iterator();
 
-      while(it.hasNext()) {
-        IndexCommit commit = (IndexCommit) it.next();
+      for (final IndexCommit commit : commits) {
         double modTime = dir.fileModified(commit.getSegmentsFileName())/1000.0;
         if (commit != lastCommit && modTime < expireTime) {
           commit.delete();
@@ -297,14 +293,12 @@
       assertEquals(2, policy.numOnCommit);
 
       // Test listCommits
-      Collection commits = IndexReader.listCommits(dir);
+      Collection<IndexCommit> commits = IndexReader.listCommits(dir);
       // 1 from opening writer + 2 from closing writer
       assertEquals(3, commits.size());
 
-      Iterator it = commits.iterator();
       // Make sure we can open a reader on each commit:
-      while(it.hasNext()) {
-        IndexCommit commit = (IndexCommit) it.next();
+      for (final IndexCommit commit : commits) {
         IndexReader r = IndexReader.open(commit, null, false);
         r.close();
       }
@@ -356,12 +350,10 @@
     }
     writer.close();
 
-    Collection commits = IndexReader.listCommits(dir);
+    Collection<IndexCommit> commits = IndexReader.listCommits(dir);
     assertEquals(6, commits.size());
     IndexCommit lastCommit = null;
-    Iterator it = commits.iterator();
-    while(it.hasNext()) {
-      IndexCommit commit = (IndexCommit) it.next();
+    for (final IndexCommit commit : commits) {
       if (lastCommit == null || commit.getGeneration() > lastCommit.getGeneration())
         lastCommit = commit;
     }
Index: src/test/org/apache/lucene/index/TestSegmentReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestSegmentReader.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestSegmentReader.java	(working copy)
@@ -19,7 +19,6 @@
 
 import java.io.IOException;
 import java.util.Collection;
-import java.util.Iterator;
 import java.util.List;
 
 import org.apache.lucene.util.LuceneTestCase;
@@ -61,9 +60,8 @@
     //There are 2 unstored fields on the document that are not preserved across writing
     assertTrue(DocHelper.numFields(result) == DocHelper.numFields(testDoc) - DocHelper.unstored.size());
     
-    List fields = result.getFields();
-    for (Iterator iter = fields.iterator(); iter.hasNext();) {
-      Fieldable field = (Fieldable) iter.next();
+    List<Fieldable> fields = result.getFields();
+    for (final Fieldable field : fields) {
       assertTrue(field != null);
       assertTrue(DocHelper.nameValues.containsKey(field.name()));
     }
@@ -83,19 +81,17 @@
   }    
   
   public void testGetFieldNameVariations() {
-    Collection result = reader.getFieldNames(IndexReader.FieldOption.ALL);
+    Collection<String> result = reader.getFieldNames(IndexReader.FieldOption.ALL);
     assertTrue(result != null);
     assertTrue(result.size() == DocHelper.all.size());
-    for (Iterator iter = result.iterator(); iter.hasNext();) {
-      String s = (String) iter.next();
+    for (final String s : result) {
       //System.out.println("Name: " + s);
       assertTrue(DocHelper.nameValues.containsKey(s) == true || s.equals(""));
     }                                                                               
     result = reader.getFieldNames(IndexReader.FieldOption.INDEXED);
     assertTrue(result != null);
     assertTrue(result.size() == DocHelper.indexed.size());
-    for (Iterator iter = result.iterator(); iter.hasNext();) {
-      String s = (String) iter.next();
+    for (final String s : result ) {
       assertTrue(DocHelper.indexed.containsKey(s) == true || s.equals(""));
     }
     
Index: src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
===================================================================
--- src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	(working copy)
@@ -32,6 +32,7 @@
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.TermQuery;
@@ -71,7 +72,7 @@
      first */
   public void unzip(String zipName, String destDirName) throws IOException {
 
-    Enumeration entries;
+    Enumeration<? extends ZipEntry> entries;
     ZipFile zipFile;
     zipFile = new ZipFile(zipName + ".zip");
 
@@ -196,7 +197,7 @@
     for(int i=0;i<35;i++) {
       if (!reader.isDeleted(i)) {
         Document d = reader.document(i);
-        List fields = d.getFields();
+        List<Fieldable> fields = d.getFields();
         if (!oldName.startsWith("19.") &&
             !oldName.startsWith("20.") &&
             !oldName.startsWith("21.") &&
Index: src/test/org/apache/lucene/index/TestNorms.java
===================================================================
--- src/test/org/apache/lucene/index/TestNorms.java	(revision 826485)
+++ src/test/org/apache/lucene/index/TestNorms.java	(working copy)
@@ -51,8 +51,8 @@
   private Similarity similarityOne;
   private Analyzer anlzr;
   private int numDocNorms;
-  private ArrayList norms; 
-  private ArrayList modifiedNorms; 
+  private ArrayList<Float> norms; 
+  private ArrayList<Float> modifiedNorms; 
   private float lastNorm = 0;
   private float normDelta = (float) 0.001;
 
@@ -83,19 +83,19 @@
     File indexDir1 = new File(tempDir, "lucenetestindex1");
     Directory dir1 = FSDirectory.open(indexDir1);
 
-    norms = new ArrayList();
-    modifiedNorms = new ArrayList();
+    norms = new ArrayList<Float>();
+    modifiedNorms = new ArrayList<Float>();
 
     createIndex(dir1);
     doTestNorms(dir1);
 
     // test with a single index: index2
-    ArrayList norms1 = norms;
-    ArrayList modifiedNorms1 = modifiedNorms;
+    ArrayList<Float> norms1 = norms;
+    ArrayList<Float> modifiedNorms1 = modifiedNorms;
     int numDocNorms1 = numDocNorms;
 
-    norms = new ArrayList();
-    modifiedNorms = new ArrayList();
+    norms = new ArrayList<Float>();
+    modifiedNorms = new ArrayList<Float>();
     numDocNorms = 0;
     
     File indexDir2 = new File(tempDir, "lucenetestindex2");
@@ -185,7 +185,7 @@
       String field = "f"+i;
       byte b[] = ir.norms(field);
       assertEquals("number of norms mismatches",numDocNorms,b.length);
-      ArrayList storedNorms = (i==1 ? modifiedNorms : norms);
+      ArrayList<Float> storedNorms = (i==1 ? modifiedNorms : norms);
       for (int j = 0; j < b.length; j++) {
         float norm = Similarity.decodeNorm(b[j]);
         float norm1 = ((Float)storedNorms.get(j)).floatValue();
Index: src/java/org/apache/lucene/index/DocumentsWriter.java
===================================================================
--- src/java/org/apache/lucene/index/DocumentsWriter.java	(revision 826485)
+++ src/java/org/apache/lucene/index/DocumentsWriter.java	(working copy)
@@ -395,17 +395,17 @@
       writer.message("DW: " + message);
   }
 
-  final List<String> openFiles = new ArrayList<String>();
-  final List<String> closedFiles = new ArrayList<String>();
+  final ArrayList<String> openFiles = new ArrayList<String>();
+  final ArrayList<String> closedFiles = new ArrayList<String>();
 
   /* Returns Collection of files in use by this instance,
    * including any flushed segments. */
   synchronized List<String> openFiles() {
-    return ( List<String>) ((ArrayList<String>) openFiles).clone();
+    return (List<String>) openFiles.clone();
   }
 
   synchronized List<String> closedFiles() {
-    return (List<String>) ((ArrayList<String>) closedFiles).clone();
+    return (List<String>) closedFiles.clone();
   }
 
   synchronized void addOpenFile(String name) {
