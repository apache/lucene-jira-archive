Index: src/java/org/apache/lucene/index/BufferedDeletes.java
===================================================================
--- src/java/org/apache/lucene/index/BufferedDeletes.java	(revision 826184)
+++ src/java/org/apache/lucene/index/BufferedDeletes.java	(working copy)
@@ -20,9 +20,10 @@
 import java.util.HashMap;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.Iterator;
 import java.util.Map.Entry;
 
+import org.apache.lucene.search.Query;
+
 /** Holds buffered deletes, by docID, term or query.  We
  *  hold two instances of this class: one for the deletes
  *  prior to the last flush, the other for deletes after
@@ -32,9 +33,9 @@
  *  previously flushed segments. */
 class BufferedDeletes {
   int numTerms;
-  HashMap terms = new HashMap();
-  HashMap queries = new HashMap();
-  List docIDs = new ArrayList();
+  HashMap<Term, Num> terms = new HashMap<Term, Num>();
+  HashMap<Query, Integer> queries = new HashMap<Query, Integer>();
+  List<Integer> docIDs = new ArrayList<Integer>();
   long bytesUsed;
 
   // Number of documents a delete term applies to.
@@ -103,42 +104,38 @@
                           MergePolicy.OneMerge merge,
                           int mergeDocCount) {
 
-    final HashMap newDeleteTerms;
+    final HashMap<Term, Num> newDeleteTerms;
 
     // Remap delete-by-term
     if (terms.size() > 0) {
-      newDeleteTerms = new HashMap();
-      Iterator iter = terms.entrySet().iterator();
-      while(iter.hasNext()) {
-        Entry entry = (Entry) iter.next();
-        Num num = (Num) entry.getValue();
+      newDeleteTerms = new HashMap<Term, Num>();
+      for(Entry<Term, Num> entry : terms.entrySet()) {
+        Num num = entry.getValue();
         newDeleteTerms.put(entry.getKey(),
                            new Num(mapper.remap(num.getNum())));
       }
-    } else
+    } else 
       newDeleteTerms = null;
+    
 
     // Remap delete-by-docID
-    final List newDeleteDocIDs;
+    final List<Integer> newDeleteDocIDs;
 
     if (docIDs.size() > 0) {
-      newDeleteDocIDs = new ArrayList(docIDs.size());
-      Iterator iter = docIDs.iterator();
-      while(iter.hasNext()) {
-        Integer num = (Integer) iter.next();
+      newDeleteDocIDs = new ArrayList<Integer>(docIDs.size());
+      for (Integer num : docIDs) {
         newDeleteDocIDs.add(Integer.valueOf(mapper.remap(num.intValue())));
       }
-    } else
+    } else 
       newDeleteDocIDs = null;
+    
 
     // Remap delete-by-query
-    final HashMap newDeleteQueries;
+    final HashMap<Query, Integer> newDeleteQueries;
     
     if (queries.size() > 0) {
-      newDeleteQueries = new HashMap(queries.size());
-      Iterator iter = queries.entrySet().iterator();
-      while(iter.hasNext()) {
-        Entry entry = (Entry) iter.next();
+      newDeleteQueries = new HashMap<Query, Integer>(queries.size());
+      for(Entry<Query, Integer> entry: queries.entrySet()) {
         Integer num = (Integer) entry.getValue();
         newDeleteQueries.put(entry.getKey(),
                              Integer.valueOf(mapper.remap(num.intValue())));
Index: src/java/org/apache/lucene/index/DocumentsWriter.java
===================================================================
--- src/java/org/apache/lucene/index/DocumentsWriter.java	(revision 826184)
+++ src/java/org/apache/lucene/index/DocumentsWriter.java	(working copy)
@@ -968,15 +968,13 @@
     boolean any = false;
 
     // Delete by term
-    Iterator iter = deletesFlushed.terms.entrySet().iterator();
     TermDocs docs = reader.termDocs();
     try {
-      while (iter.hasNext()) {
-        Entry entry = (Entry) iter.next();
-        Term term = (Term) entry.getKey();
+      for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {
+        Term term = entry.getKey();
 
         docs.seek(term);
-        int limit = ((BufferedDeletes.Num) entry.getValue()).getNum();
+        int limit = entry.getValue().getNum();
         while (docs.next()) {
           int docID = docs.doc();
           if (docIDStart+docID >= limit)
@@ -990,9 +988,8 @@
     }
 
     // Delete by docID
-    iter = deletesFlushed.docIDs.iterator();
-    while(iter.hasNext()) {
-      int docID = ((Integer) iter.next()).intValue();
+    for (Integer docIdInt : deletesFlushed.docIDs) {
+      int docID = docIdInt.intValue();
       if (docID >= docIDStart && docID < docEnd) {
         reader.deleteDocument(docID-docIDStart);
         any = true;
@@ -1001,11 +998,9 @@
 
     // Delete by query
     IndexSearcher searcher = new IndexSearcher(reader);
-    iter = deletesFlushed.queries.entrySet().iterator();
-    while(iter.hasNext()) {
-      Entry entry = (Entry) iter.next();
-      Query query = (Query) entry.getKey();
-      int limit = ((Integer) entry.getValue()).intValue();
+    for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {
+      Query query = entry.getKey();
+      int limit = entry.getValue().intValue();
       Weight weight = query.weight(searcher);
       Scorer scorer = weight.scorer(reader, true, false);
       if (scorer != null) {
@@ -1027,7 +1022,7 @@
   // delete term will be applied to those documents as well
   // as the disk segments.
   synchronized private void addDeleteTerm(Term term, int docCount) {
-    BufferedDeletes.Num num = (BufferedDeletes.Num) deletesInRAM.terms.get(term);
+    BufferedDeletes.Num num = deletesInRAM.terms.get(term);
     final int docIDUpto = flushedDocCount + docCount;
     if (num == null)
       deletesInRAM.terms.put(term, new BufferedDeletes.Num(docIDUpto));
