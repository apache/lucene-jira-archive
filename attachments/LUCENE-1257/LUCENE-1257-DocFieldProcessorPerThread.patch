Index: src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java
===================================================================
--- src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java	(revision 826278)
+++ src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java	(working copy)
@@ -76,8 +76,8 @@
     consumer.abort();
   }
 
-  public Collection fields() {
-    Collection fields = new HashSet();
+  public Collection<DocFieldConsumerPerField> fields() {
+    Collection<DocFieldConsumerPerField> fields = new HashSet<DocFieldConsumerPerField>();
     for(int i=0;i<fieldHash.length;i++) {
       DocFieldProcessorPerField field = fieldHash[i];
       while(field != null) {
@@ -163,7 +163,7 @@
     
     final int thisFieldGen = fieldGen++;
 
-    final List docFields = doc.getFields();
+    final List<Fieldable> docFields = doc.getFields();
     final int numDocFields = docFields.size();
 
     // Absorb any new fields first seen in this document.
Index: src/java/org/apache/lucene/index/DocFieldProcessor.java
===================================================================
--- src/java/org/apache/lucene/index/DocFieldProcessor.java	(revision 826278)
+++ src/java/org/apache/lucene/index/DocFieldProcessor.java	(working copy)
@@ -21,8 +21,8 @@
 import java.util.Collection;
 import java.util.Map;
 import java.util.HashMap;
-import java.util.Iterator;
 
+
 /**
  * This is a DocConsumer that gathers all fields under the
  * same name, and calls per-field consumers to process field
@@ -50,12 +50,10 @@
     fieldsWriter.closeDocStore(state);
   }
 
-  public void flush(Collection threads, SegmentWriteState state) throws IOException {
+  public void flush(Collection<DocFieldProcessorPerThread> threads, SegmentWriteState state) throws IOException {
 
-    Map childThreadsAndFields = new HashMap();
-    Iterator it = threads.iterator();
-    while(it.hasNext()) {
-      DocFieldProcessorPerThread perThread = (DocFieldProcessorPerThread) it.next();
+    Map<DocFieldConsumerPerThread, Collection<DocFieldConsumerPerField>> childThreadsAndFields = new HashMap<DocFieldConsumerPerThread, Collection<DocFieldConsumerPerField>>();
+    for ( DocFieldProcessorPerThread perThread : threads) {
       childThreadsAndFields.put(perThread.consumer, perThread.fields());
       perThread.trimFields(state);
     }
Index: src/java/org/apache/lucene/index/SegmentWriteState.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentWriteState.java	(revision 826278)
+++ src/java/org/apache/lucene/index/SegmentWriteState.java	(working copy)
@@ -30,7 +30,7 @@
   int numDocs;
   int termIndexInterval;
   int numDocsInStore;
-  Collection flushedFiles;
+  Collection<String> flushedFiles;
 
   public SegmentWriteState(DocumentsWriter docWriter, Directory directory, String segmentName, String docStoreSegmentName, int numDocs,
                            int numDocsInStore, int termIndexInterval) {
@@ -41,7 +41,7 @@
     this.numDocs = numDocs;
     this.numDocsInStore = numDocsInStore;
     this.termIndexInterval = termIndexInterval;
-    flushedFiles = new HashSet();
+    flushedFiles = new HashSet<String>();
   }
 
   public String segmentFileName(String ext) {
Index: src/java/org/apache/lucene/index/DocConsumer.java
===================================================================
--- src/java/org/apache/lucene/index/DocConsumer.java	(revision 826278)
+++ src/java/org/apache/lucene/index/DocConsumer.java	(working copy)
@@ -22,7 +22,7 @@
 
 abstract class DocConsumer {
   abstract DocConsumerPerThread addThread(DocumentsWriterThreadState perThread) throws IOException;
-  abstract void flush(final Collection threads, final SegmentWriteState state) throws IOException;
+  abstract void flush(final Collection<DocFieldProcessorPerThread> threads, final SegmentWriteState state) throws IOException;
   abstract void closeDocStore(final SegmentWriteState state) throws IOException;
   abstract void abort();
   abstract boolean freeRAM();
