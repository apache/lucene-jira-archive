Index: solr/core/src/test/org/apache/solr/search/SpatialFilterTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/search/SpatialFilterTest.java	(revision 1386643)
+++ solr/core/src/test/org/apache/solr/search/SpatialFilterTest.java	(working copy)
@@ -119,19 +119,19 @@
     checkHits(fieldName, false, "43.517030,-96.789603", 110, 1, 17);
     
     
-	// Tests SOLR-2829
-	String fieldNameHome = "home_ll";
-	String fieldNameWork = "work_ll";
+    // Tests SOLR-2829
+    String fieldNameHome = "home_ll";
+    String fieldNameWork = "work_ll";
 
-	clearIndex();
-	assertU(adoc("id", "1", fieldNameHome, "52.67,7.30", fieldNameWork,"48.60,11.61"));
-	assertU(commit());
+    clearIndex();
+    assertU(adoc("id", "1", fieldNameHome, "52.67,7.30", fieldNameWork,"48.60,11.61"));
+    assertU(commit());
 
-	checkHits(fieldNameHome, "52.67,7.30", 1, 1);
-	checkHits(fieldNameWork, "48.60,11.61", 1, 1);
-	checkHits(fieldNameWork, "52.67,7.30", 1, 0);
-	checkHits(fieldNameHome, "48.60,11.61", 1, 0); 
-	  
+    checkHits(fieldNameHome, "52.67,7.30", 1, 1);
+    checkHits(fieldNameWork, "48.60,11.61", 1, 1);
+    checkHits(fieldNameWork, "52.67,7.30", 1, 0);
+    checkHits(fieldNameHome, "48.60,11.61", 1, 0);
+
   }
 
   private void checkHits(String fieldName, String pt, double distance, int count, int ... docIds) {
@@ -206,4 +206,4 @@
             + NumericRangeQuery.class,
             query instanceof NumericRangeQuery);
     req.close();
-  }*/
\ No newline at end of file
+  }*/
Index: solr/core/src/test/org/apache/solr/search/function/NvlValueSourceParser.java
===================================================================
--- solr/core/src/test/org/apache/solr/search/function/NvlValueSourceParser.java	(revision 1386643)
+++ solr/core/src/test/org/apache/solr/search/function/NvlValueSourceParser.java	(working copy)
@@ -47,33 +47,33 @@
 
     @Override
     public ValueSource parse(FunctionQParser fp) throws ParseException {
-	ValueSource source = fp.parseValueSource();
-	final float nvl = fp.parseFloat();
+      ValueSource source = fp.parseValueSource();
+      final float nvl = fp.parseFloat();
 
-	return new SimpleFloatFunction(source) {
-	    @Override
+      return new SimpleFloatFunction(source) {
+        @Override
       protected String name() {
-		return "nvl";
-	    }
+          return "nvl";
+        }
 
-	    @Override
-      protected float func(int doc, FunctionValues vals) {
-		float v = vals.floatVal(doc);
-		if (v == nvlFloatValue) {
-		    return nvl;
-		} else {
-		    return v;
-		}
-	    }
-	};
+        @Override
+        protected float func(int doc, FunctionValues vals) {
+          float v = vals.floatVal(doc);
+          if (v == nvlFloatValue) {
+            return nvl;
+          } else {
+            return v;
+          }
+        }
+      };
     }
 
-    @Override
-    public void init(NamedList args) {
-	/* initialize the value to consider as null */
-	Float nvlFloatValueArg = (Float) args.get("nvlFloatValue");
-	if (nvlFloatValueArg != null) {
-	    this.nvlFloatValue = nvlFloatValueArg;
-	}
+  @Override
+  public void init(NamedList args) {
+    /* initialize the value to consider as null */
+    Float nvlFloatValueArg = (Float) args.get("nvlFloatValue");
+    if (nvlFloatValueArg != null) {
+      this.nvlFloatValue = nvlFloatValueArg;
     }
+  }
 }
\ No newline at end of file
Index: solr/core/src/test/org/apache/solr/spelling/DirectSolrSpellCheckerTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/spelling/DirectSolrSpellCheckerTest.java	(revision 1386643)
+++ solr/core/src/test/org/apache/solr/spelling/DirectSolrSpellCheckerTest.java	(working copy)
@@ -86,12 +86,12 @@
   
   @Test
   public void testOnlyMorePopularWithExtendedResults() throws Exception {
-  	assertQ(req("q", "teststop:fox", "qt", "spellCheckCompRH", SpellCheckComponent.COMPONENT_NAME, "true", SpellingParams.SPELLCHECK_DICT, "direct", SpellingParams.SPELLCHECK_EXTENDED_RESULTS, "true", SpellingParams.SPELLCHECK_ONLY_MORE_POPULAR, "true"),
+    assertQ(req("q", "teststop:fox", "qt", "spellCheckCompRH", SpellCheckComponent.COMPONENT_NAME, "true", SpellingParams.SPELLCHECK_DICT, "direct", SpellingParams.SPELLCHECK_EXTENDED_RESULTS, "true", SpellingParams.SPELLCHECK_ONLY_MORE_POPULAR, "true"),
         "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='fox']/int[@name='origFreq']=1",
         "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='fox']/arr[@name='suggestion']/lst/str[@name='word']='foo'",
         "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='fox']/arr[@name='suggestion']/lst/int[@name='freq']=2",
         "//lst[@name='spellcheck']/lst[@name='suggestions']/bool[@name='correctlySpelled']='true'"
-  	);
+    );
   }  
   
 }
Index: solr/core/src/test/org/apache/solr/spelling/SpellPossibilityIteratorTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/spelling/SpellPossibilityIteratorTest.java	(revision 1386643)
+++ solr/core/src/test/org/apache/solr/spelling/SpellPossibilityIteratorTest.java	(working copy)
@@ -28,71 +28,71 @@
 import org.junit.Test;
 
 public class SpellPossibilityIteratorTest extends SolrTestCaseJ4 {
-	private static final Token TOKEN_AYE = new Token("AYE", 0, 3);
-	private static final Token TOKEN_BEE = new Token("BEE", 4, 7);
-	private static final Token TOKEN_AYE_BEE = new Token("AYE BEE", 0, 7);
-	private static final Token TOKEN_CEE = new Token("CEE", 8, 11);
-	
-	private LinkedHashMap<String, Integer> AYE;
-	private LinkedHashMap<String, Integer> BEE;
-	private LinkedHashMap<String, Integer> AYE_BEE;
-	private LinkedHashMap<String, Integer> CEE;
-	
-	@Override
+  private static final Token TOKEN_AYE = new Token("AYE", 0, 3);
+  private static final Token TOKEN_BEE = new Token("BEE", 4, 7);
+  private static final Token TOKEN_AYE_BEE = new Token("AYE BEE", 0, 7);
+  private static final Token TOKEN_CEE = new Token("CEE", 8, 11);
+
+  private LinkedHashMap<String, Integer> AYE;
+  private LinkedHashMap<String, Integer> BEE;
+  private LinkedHashMap<String, Integer> AYE_BEE;
+  private LinkedHashMap<String, Integer> CEE;
+
+  @Override
   @Before
-	public void setUp() throws Exception {
-	  super.setUp();
+  public void setUp() throws Exception {
+    super.setUp();
 
-		AYE = new LinkedHashMap<String, Integer>();
-		AYE.put("I", 0);
-		AYE.put("II", 0);
-		AYE.put("III", 0);
-		AYE.put("IV", 0);
-		AYE.put("V", 0);
-		AYE.put("VI", 0);
-		AYE.put("VII", 0);
-		AYE.put("VIII", 0);
-		
-		BEE = new LinkedHashMap<String, Integer>();
-		BEE.put("alpha", 0);
-		BEE.put("beta", 0);
-		BEE.put("gamma", 0);
-		BEE.put("delta", 0);
-		BEE.put("epsilon", 0);
-		BEE.put("zeta", 0);
-		BEE.put("eta", 0);
-		BEE.put("theta", 0);
-		BEE.put("iota", 0);
-		
-		AYE_BEE = new LinkedHashMap<String, Integer>();
-		AYE_BEE.put("one-alpha", 0);
-		AYE_BEE.put("two-beta", 0);
-		AYE_BEE.put("three-gamma", 0);
-		AYE_BEE.put("four-delta", 0);
-		AYE_BEE.put("five-epsilon", 0);
-		AYE_BEE.put("six-zeta", 0);
-		AYE_BEE.put("seven-eta", 0);
-		AYE_BEE.put("eight-theta", 0);
-		AYE_BEE.put("nine-iota", 0);
-		
+    AYE = new LinkedHashMap<String, Integer>();
+    AYE.put("I", 0);
+    AYE.put("II", 0);
+    AYE.put("III", 0);
+    AYE.put("IV", 0);
+    AYE.put("V", 0);
+    AYE.put("VI", 0);
+    AYE.put("VII", 0);
+    AYE.put("VIII", 0);
 
-		CEE = new LinkedHashMap<String, Integer>();
-		CEE.put("one", 0);
-		CEE.put("two", 0);
-		CEE.put("three", 0);
-		CEE.put("four", 0);
-		CEE.put("five", 0);
-		CEE.put("six", 0);
-		CEE.put("seven", 0);
-		CEE.put("eight", 0);
-		CEE.put("nine", 0);
-		CEE.put("ten", 0);
-	}
-	
-	@Test
-	public void testScalability() throws Exception {
-	  Map<Token, LinkedHashMap<String, Integer>> lotsaSuggestions = new LinkedHashMap<Token, LinkedHashMap<String, Integer>>();
-	  lotsaSuggestions.put(TOKEN_AYE , AYE);
+    BEE = new LinkedHashMap<String, Integer>();
+    BEE.put("alpha", 0);
+    BEE.put("beta", 0);
+    BEE.put("gamma", 0);
+    BEE.put("delta", 0);
+    BEE.put("epsilon", 0);
+    BEE.put("zeta", 0);
+    BEE.put("eta", 0);
+    BEE.put("theta", 0);
+    BEE.put("iota", 0);
+
+    AYE_BEE = new LinkedHashMap<String, Integer>();
+    AYE_BEE.put("one-alpha", 0);
+    AYE_BEE.put("two-beta", 0);
+    AYE_BEE.put("three-gamma", 0);
+    AYE_BEE.put("four-delta", 0);
+    AYE_BEE.put("five-epsilon", 0);
+    AYE_BEE.put("six-zeta", 0);
+    AYE_BEE.put("seven-eta", 0);
+    AYE_BEE.put("eight-theta", 0);
+    AYE_BEE.put("nine-iota", 0);
+
+
+    CEE = new LinkedHashMap<String, Integer>();
+    CEE.put("one", 0);
+    CEE.put("two", 0);
+    CEE.put("three", 0);
+    CEE.put("four", 0);
+    CEE.put("five", 0);
+    CEE.put("six", 0);
+    CEE.put("seven", 0);
+    CEE.put("eight", 0);
+    CEE.put("nine", 0);
+    CEE.put("ten", 0);
+  }
+
+  @Test
+  public void testScalability() throws Exception {
+    Map<Token, LinkedHashMap<String, Integer>> lotsaSuggestions = new LinkedHashMap<Token, LinkedHashMap<String, Integer>>();
+    lotsaSuggestions.put(TOKEN_AYE , AYE);
     lotsaSuggestions.put(TOKEN_BEE , BEE);
     lotsaSuggestions.put(TOKEN_CEE , CEE);
     
@@ -112,15 +112,15 @@
     lotsaSuggestions.put(new Token("BEE4", 4, 7),  BEE);
     lotsaSuggestions.put(new Token("CEE4", 8, 11), CEE);
     
-		PossibilityIterator iter = new PossibilityIterator(lotsaSuggestions, 1000, 10000, false);
-		int count = 0;
-		while (iter.hasNext()) {			
-			PossibilityIterator.RankedSpellPossibility rsp = iter.next();
-			count++;
-		}
-		assertTrue(count==1000);
-		
-		lotsaSuggestions.put(new Token("AYE_BEE1", 0, 7), AYE_BEE);
+    PossibilityIterator iter = new PossibilityIterator(lotsaSuggestions, 1000, 10000, false);
+    int count = 0;
+    while (iter.hasNext()) {
+      PossibilityIterator.RankedSpellPossibility rsp = iter.next();
+      count++;
+    }
+    assertTrue(count==1000);
+
+    lotsaSuggestions.put(new Token("AYE_BEE1", 0, 7), AYE_BEE);
     lotsaSuggestions.put(new Token("AYE_BEE2", 0, 7), AYE_BEE);
     lotsaSuggestions.put(new Token("AYE_BEE3", 0, 7), AYE_BEE);
     lotsaSuggestions.put(new Token("AYE_BEE4", 0, 7), AYE_BEE);
@@ -131,62 +131,62 @@
       count++;
     }
     assertTrue(count<100);
-	}
-	
-	@Test
-	public void testSpellPossibilityIterator() throws Exception {
-	  Map<Token, LinkedHashMap<String, Integer>> suggestions = new LinkedHashMap<Token, LinkedHashMap<String, Integer>>();
-	  suggestions.put(TOKEN_AYE , AYE);
+  }
+
+  @Test
+  public void testSpellPossibilityIterator() throws Exception {
+    Map<Token, LinkedHashMap<String, Integer>> suggestions = new LinkedHashMap<Token, LinkedHashMap<String, Integer>>();
+    suggestions.put(TOKEN_AYE , AYE);
     suggestions.put(TOKEN_BEE , BEE);
     suggestions.put(TOKEN_CEE , CEE);
     
-		PossibilityIterator iter = new PossibilityIterator(suggestions, 1000, 10000, false);
-		int count = 0;
-		while (iter.hasNext()) {
-			
-		  PossibilityIterator.RankedSpellPossibility rsp = iter.next();
-			if(count==0) {
-				assertTrue("I".equals(rsp.corrections.get(0).getCorrection()));
-				assertTrue("alpha".equals(rsp.corrections.get(1).getCorrection()));
-				assertTrue("one".equals(rsp.corrections.get(2).getCorrection()));
-			}
-			count++;
-		}
-		assertTrue(("Three maps (8*9*10) should return 720 iterations but instead returned " + count), count == 720);
+    PossibilityIterator iter = new PossibilityIterator(suggestions, 1000, 10000, false);
+    int count = 0;
+    while (iter.hasNext()) {
 
-		suggestions.remove(TOKEN_CEE);
-		iter = new PossibilityIterator(suggestions, 100, 10000, false);
-		count = 0;
-		while (iter.hasNext()) {
-			iter.next();
-			count++;
-		}
-		assertTrue(("Two maps (8*9) should return 72 iterations but instead returned " + count), count == 72);
+      PossibilityIterator.RankedSpellPossibility rsp = iter.next();
+      if(count==0) {
+        assertTrue("I".equals(rsp.corrections.get(0).getCorrection()));
+        assertTrue("alpha".equals(rsp.corrections.get(1).getCorrection()));
+        assertTrue("one".equals(rsp.corrections.get(2).getCorrection()));
+      }
+      count++;
+    }
+    assertTrue(("Three maps (8*9*10) should return 720 iterations but instead returned " + count), count == 720);
 
-		suggestions.remove(TOKEN_BEE);
-		iter = new PossibilityIterator(suggestions, 5, 10000, false);
-		count = 0;
-		while (iter.hasNext()) {
-			iter.next();
-			count++;
-		}
-		assertTrue(("We requested 5 suggestions but got " + count), count == 5);
+    suggestions.remove(TOKEN_CEE);
+    iter = new PossibilityIterator(suggestions, 100, 10000, false);
+    count = 0;
+    while (iter.hasNext()) {
+      iter.next();
+      count++;
+    }
+    assertTrue(("Two maps (8*9) should return 72 iterations but instead returned " + count), count == 72);
 
-		suggestions.remove(TOKEN_AYE);
-		iter = new PossibilityIterator(suggestions, Integer.MAX_VALUE, 10000, false);
-		count = 0;
-		while (iter.hasNext()) {
-			iter.next();
-			count++;
-		}
-		assertTrue(("No maps should return 0 iterations but instead returned " + count), count == 0);
+    suggestions.remove(TOKEN_BEE);
+    iter = new PossibilityIterator(suggestions, 5, 10000, false);
+    count = 0;
+    while (iter.hasNext()) {
+      iter.next();
+      count++;
+    }
+    assertTrue(("We requested 5 suggestions but got " + count), count == 5);
 
-	}
-	
-	@Test
+    suggestions.remove(TOKEN_AYE);
+    iter = new PossibilityIterator(suggestions, Integer.MAX_VALUE, 10000, false);
+    count = 0;
+    while (iter.hasNext()) {
+      iter.next();
+      count++;
+    }
+    assertTrue(("No maps should return 0 iterations but instead returned " + count), count == 0);
+
+  }
+
+  @Test
   public void testOverlappingTokens() throws Exception {
-	  Map<Token, LinkedHashMap<String, Integer>> overlappingSuggestions = new LinkedHashMap<Token, LinkedHashMap<String, Integer>>();
-	  overlappingSuggestions.put(TOKEN_AYE, AYE);
+    Map<Token, LinkedHashMap<String, Integer>> overlappingSuggestions = new LinkedHashMap<Token, LinkedHashMap<String, Integer>>();
+    overlappingSuggestions.put(TOKEN_AYE, AYE);
     overlappingSuggestions.put(TOKEN_BEE, BEE);
     overlappingSuggestions.put(TOKEN_AYE_BEE, AYE_BEE);
     overlappingSuggestions.put(TOKEN_CEE, CEE);
Index: solr/core/src/test/org/apache/solr/spelling/SpellCheckCollatorTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/spelling/SpellCheckCollatorTest.java	(revision 1386643)
+++ solr/core/src/test/org/apache/solr/spelling/SpellCheckCollatorTest.java	(working copy)
@@ -40,30 +40,30 @@
 
 @Slow
 public class SpellCheckCollatorTest extends SolrTestCaseJ4 {
-	@BeforeClass
-	public static void beforeClass() throws Exception {
- 		initCore("solrconfig-spellcheckcomponent.xml", "schema.xml");
-		assertNull(h.validateUpdate(adoc("id", "0", "lowerfilt", "faith hope and love")));
-		assertNull(h.validateUpdate(adoc("id", "1", "lowerfilt", "faith hope and loaves")));
-		assertNull(h.validateUpdate(adoc("id", "2", "lowerfilt", "fat hops and loaves")));
-		assertNull(h.validateUpdate(adoc("id", "3", "lowerfilt", "faith of homer")));
-		assertNull(h.validateUpdate(adoc("id", "4", "lowerfilt", "fat of homer")));
-		assertNull(h.validateUpdate(adoc("id", "5", "lowerfilt1", "peace")));
-		assertNull(h.validateUpdate(adoc("id", "6", "lowerfilt", "hyphenated word")));
- 		assertNull(h.validateUpdate(adoc("id", "7", "teststop", "Jane filled out a form at Charles De Gaulle")));
- 		assertNull(h.validateUpdate(adoc("id", "8", "teststop", "Dick flew from Heathrow")));
- 		assertNull(h.validateUpdate(adoc("id", "9", "teststop", "Jane is stuck in customs because Spot chewed up the form")));
- 		assertNull(h.validateUpdate(adoc("id", "10", "teststop", "Once in Paris Dick built a fire on the hearth")));
- 		assertNull(h.validateUpdate(adoc("id", "11", "teststop", "Dick waited for Jane as he watched the sparks flow upward")));
- 		assertNull(h.validateUpdate(adoc("id", "12", "teststop", "This June parisian rendez-vous is ruined because of a customs snafu")));
- 		assertNull(h.validateUpdate(adoc("id", "13", "teststop", "partisan political machine")));
-		assertNull(h.validateUpdate(commit()));
-	}
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+     initCore("solrconfig-spellcheckcomponent.xml", "schema.xml");
+    assertNull(h.validateUpdate(adoc("id", "0", "lowerfilt", "faith hope and love")));
+    assertNull(h.validateUpdate(adoc("id", "1", "lowerfilt", "faith hope and loaves")));
+    assertNull(h.validateUpdate(adoc("id", "2", "lowerfilt", "fat hops and loaves")));
+    assertNull(h.validateUpdate(adoc("id", "3", "lowerfilt", "faith of homer")));
+    assertNull(h.validateUpdate(adoc("id", "4", "lowerfilt", "fat of homer")));
+    assertNull(h.validateUpdate(adoc("id", "5", "lowerfilt1", "peace")));
+    assertNull(h.validateUpdate(adoc("id", "6", "lowerfilt", "hyphenated word")));
+     assertNull(h.validateUpdate(adoc("id", "7", "teststop", "Jane filled out a form at Charles De Gaulle")));
+     assertNull(h.validateUpdate(adoc("id", "8", "teststop", "Dick flew from Heathrow")));
+     assertNull(h.validateUpdate(adoc("id", "9", "teststop", "Jane is stuck in customs because Spot chewed up the form")));
+     assertNull(h.validateUpdate(adoc("id", "10", "teststop", "Once in Paris Dick built a fire on the hearth")));
+     assertNull(h.validateUpdate(adoc("id", "11", "teststop", "Dick waited for Jane as he watched the sparks flow upward")));
+     assertNull(h.validateUpdate(adoc("id", "12", "teststop", "This June parisian rendez-vous is ruined because of a customs snafu")));
+     assertNull(h.validateUpdate(adoc("id", "13", "teststop", "partisan political machine")));
+    assertNull(h.validateUpdate(commit()));
+  }
 
-	@Test
-	public void testCollationWithHypens() throws Exception
-	{
-	  SolrCore core = h.getCore();
+  @Test
+  public void testCollationWithHypens() throws Exception
+  {
+    SolrCore core = h.getCore();
     SearchComponent speller = core.getSearchComponent("spellcheck");
     assertTrue("speller is null and it shouldn't be", speller != null);
     
@@ -111,12 +111,12 @@
     }
 
   }
-	
-	public void testCollateWithOverride() throws Exception
-	{
-	  assertQ(
+
+  public void testCollateWithOverride() throws Exception
+  {
+    assertQ(
       req(
-    	  SpellCheckComponent.COMPONENT_NAME, "true",
+        SpellCheckComponent.COMPONENT_NAME, "true",
         SpellCheckComponent.SPELLCHECK_DICT, "direct",
         SpellingParams.SPELLCHECK_COUNT, "10",   
         SpellingParams.SPELLCHECK_COLLATE, "true",
@@ -130,303 +130,303 @@
       ),
       "//lst[@name='spellcheck']/lst[@name='suggestions']/str[@name='collation']='parisian political machine'"
     );
-	  assertQ(
-	      req(
-	        SpellCheckComponent.COMPONENT_NAME, "true",
-	        SpellCheckComponent.SPELLCHECK_DICT, "direct",
-	        SpellingParams.SPELLCHECK_COUNT, "10",   
-	        SpellingParams.SPELLCHECK_COLLATE, "true",
-	        SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "10",
-	        SpellingParams.SPELLCHECK_MAX_COLLATIONS, "10",
-	        "qt", "spellCheckCompRH",
-	        "defType", "edismax",
-	        "qf", "teststop",
-	        "mm", "1",
-	        SpellingParams.SPELLCHECK_COLLATE_PARAM_OVERRIDE + "mm", "100%",
-	        CommonParams.Q, "partisian politcal mashine"
-	      ),
-	     "//lst[@name='spellcheck']/lst[@name='suggestions']/str[@name='collation']='partisan political machine'"
-	   );
+    assertQ(
+        req(
+          SpellCheckComponent.COMPONENT_NAME, "true",
+          SpellCheckComponent.SPELLCHECK_DICT, "direct",
+          SpellingParams.SPELLCHECK_COUNT, "10",
+          SpellingParams.SPELLCHECK_COLLATE, "true",
+          SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "10",
+          SpellingParams.SPELLCHECK_MAX_COLLATIONS, "10",
+          "qt", "spellCheckCompRH",
+          "defType", "edismax",
+          "qf", "teststop",
+          "mm", "1",
+          SpellingParams.SPELLCHECK_COLLATE_PARAM_OVERRIDE + "mm", "100%",
+          CommonParams.Q, "partisian politcal mashine"
+        ),
+       "//lst[@name='spellcheck']/lst[@name='suggestions']/str[@name='collation']='partisan political machine'"
+     );
     
-	}
+  }
 
-	@Test
-	public void testCollateWithFilter() throws Exception
-	{
-		SolrCore core = h.getCore();
-		SearchComponent speller = core.getSearchComponent("spellcheck");
-		assertTrue("speller is null and it shouldn't be", speller != null);
-		
-		ModifiableSolrParams params = new ModifiableSolrParams();		
-		params.add(SpellCheckComponent.COMPONENT_NAME, "true");
-		params.add(SpellingParams.SPELLCHECK_BUILD, "true");
-		params.add(SpellingParams.SPELLCHECK_COUNT, "10");		
-		params.add(SpellingParams.SPELLCHECK_COLLATE, "true");
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "10");
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "10");
-		params.add(CommonParams.Q, "lowerfilt:(+fauth +home +loane)");
-		params.add(CommonParams.FQ, "NOT(id:1)");
-		
-		//Because a FilterQuery is applied which removes doc id#1 from possible hits, we would
-		//not want the collations to return us "lowerfilt:(+faith +hope +loaves)" as this only matches doc id#1.
-		SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
-		SolrQueryResponse rsp = new SolrQueryResponse();
-		rsp.add("responseHeader", new SimpleOrderedMap());
-		SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
-		handler.handleRequest(req, rsp);
-		req.close();
-		NamedList values = rsp.getValues();
-		NamedList spellCheck = (NamedList) values.get("spellcheck");
-		NamedList suggestions = (NamedList) spellCheck.get("suggestions");
-		List<String> collations = suggestions.getAll("collation");
-		assertTrue(collations.size() > 0);
-		for(String collation : collations) {
-			assertTrue(!collation.equals("lowerfilt:(+faith +hope +loaves)"));	
-		}
-	}
-	
-	@Test
-	public void testCollateWithMultipleRequestHandlers() throws Exception
-	{
-		SolrCore core = h.getCore();
-		SearchComponent speller = core.getSearchComponent("spellcheck");
-		assertTrue("speller is null and it shouldn't be", speller != null);
-		
-		ModifiableSolrParams params = new ModifiableSolrParams();		
-		params.add(SpellCheckComponent.COMPONENT_NAME, "true");
-		params.add(SpellingParams.SPELLCHECK_DICT, "multipleFields");
-		params.add(SpellingParams.SPELLCHECK_BUILD, "true");
-		params.add(SpellingParams.SPELLCHECK_COUNT, "10");		
-		params.add(SpellingParams.SPELLCHECK_COLLATE, "true");
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "1");
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "1");
-		params.add(CommonParams.Q, "peac");	
-		
-		//SpellCheckCompRH has no "qf" defined.  It will not find "peace" from "peac" despite it being in the dictionary
-		//because requrying against this Request Handler results in 0 hits.	
-		SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
-		SolrQueryResponse rsp = new SolrQueryResponse();
-		rsp.add("responseHeader", new SimpleOrderedMap());
-		SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
-		handler.handleRequest(req, rsp);
-		req.close();
-		NamedList values = rsp.getValues();
-		NamedList spellCheck = (NamedList) values.get("spellcheck");
-		NamedList suggestions = (NamedList) spellCheck.get("suggestions");
-		String singleCollation = (String) suggestions.get("collation");
-		assertNull(singleCollation);
-		
-		//SpellCheckCompRH1 has "lowerfilt1" defined in the "qf" param.  It will find "peace" from "peac" because
-		//requrying field "lowerfilt1" returns the hit.
-		params.remove(SpellingParams.SPELLCHECK_BUILD);
-		handler = core.getRequestHandler("spellCheckCompRH1");
-		rsp = new SolrQueryResponse();
-		rsp.add("responseHeader", new SimpleOrderedMap());
-		req = new LocalSolrQueryRequest(core, params);
-		handler.handleRequest(req, rsp);
-		req.close();
-		values = rsp.getValues();
-		spellCheck = (NamedList) values.get("spellcheck");
-		suggestions = (NamedList) spellCheck.get("suggestions");
-		singleCollation = (String) suggestions.get("collation");
-		assertEquals(singleCollation, "peace");		
-	}
+  @Test
+  public void testCollateWithFilter() throws Exception
+  {
+    SolrCore core = h.getCore();
+    SearchComponent speller = core.getSearchComponent("spellcheck");
+    assertTrue("speller is null and it shouldn't be", speller != null);
 
-	@Test
-	public void testExtendedCollate() throws Exception {
-		SolrCore core = h.getCore();
-		SearchComponent speller = core.getSearchComponent("spellcheck");
-		assertTrue("speller is null and it shouldn't be", speller != null);
+    ModifiableSolrParams params = new ModifiableSolrParams();
+    params.add(SpellCheckComponent.COMPONENT_NAME, "true");
+    params.add(SpellingParams.SPELLCHECK_BUILD, "true");
+    params.add(SpellingParams.SPELLCHECK_COUNT, "10");
+    params.add(SpellingParams.SPELLCHECK_COLLATE, "true");
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "10");
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "10");
+    params.add(CommonParams.Q, "lowerfilt:(+fauth +home +loane)");
+    params.add(CommonParams.FQ, "NOT(id:1)");
 
-		ModifiableSolrParams params = new ModifiableSolrParams();
-		params.add(CommonParams.QT, "spellCheckCompRH");
-		params.add(CommonParams.Q, "lowerfilt:(+fauth +home +loane)");
-		params.add(SpellingParams.SPELLCHECK_EXTENDED_RESULTS, "true");
-		params.add(SpellCheckComponent.COMPONENT_NAME, "true");
-		params.add(SpellingParams.SPELLCHECK_BUILD, "true");
-		params.add(SpellingParams.SPELLCHECK_COUNT, "10");
-		params.add(SpellingParams.SPELLCHECK_COLLATE, "true");
+    //Because a FilterQuery is applied which removes doc id#1 from possible hits, we would
+    //not want the collations to return us "lowerfilt:(+faith +hope +loaves)" as this only matches doc id#1.
+    SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
+    SolrQueryResponse rsp = new SolrQueryResponse();
+    rsp.add("responseHeader", new SimpleOrderedMap());
+    SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
+    handler.handleRequest(req, rsp);
+    req.close();
+    NamedList values = rsp.getValues();
+    NamedList spellCheck = (NamedList) values.get("spellcheck");
+    NamedList suggestions = (NamedList) spellCheck.get("suggestions");
+    List<String> collations = suggestions.getAll("collation");
+    assertTrue(collations.size() > 0);
+    for(String collation : collations) {
+      assertTrue(!collation.equals("lowerfilt:(+faith +hope +loaves)"));
+    }
+  }
 
-		// Testing backwards-compatible behavior.
-		// Returns 1 collation as a single string.
-		// All words are "correct" per the dictionary, but this collation would
-		// return no results if tried.
-		SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
-		SolrQueryResponse rsp = new SolrQueryResponse();
-		rsp.add("responseHeader", new SimpleOrderedMap());
-		SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
-		handler.handleRequest(req, rsp);
-		req.close();
-		NamedList values = rsp.getValues();
-		NamedList spellCheck = (NamedList) values.get("spellcheck");
-		NamedList suggestions = (NamedList) spellCheck.get("suggestions");
-		String singleCollation = (String) suggestions.get("collation");
-		assertEquals("lowerfilt:(+faith +homer +loaves)", singleCollation);
+  @Test
+  public void testCollateWithMultipleRequestHandlers() throws Exception
+  {
+    SolrCore core = h.getCore();
+    SearchComponent speller = core.getSearchComponent("spellcheck");
+    assertTrue("speller is null and it shouldn't be", speller != null);
 
-		// Testing backwards-compatible response format but will only return a
-		// collation that would return results.
-		params.remove(SpellingParams.SPELLCHECK_BUILD);
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "5");
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "1");
-		handler = core.getRequestHandler("spellCheckCompRH");
-		rsp = new SolrQueryResponse();
-		rsp.add("responseHeader", new SimpleOrderedMap());
-		req = new LocalSolrQueryRequest(core, params);
-		handler.handleRequest(req, rsp);
+    ModifiableSolrParams params = new ModifiableSolrParams();
+    params.add(SpellCheckComponent.COMPONENT_NAME, "true");
+    params.add(SpellingParams.SPELLCHECK_DICT, "multipleFields");
+    params.add(SpellingParams.SPELLCHECK_BUILD, "true");
+    params.add(SpellingParams.SPELLCHECK_COUNT, "10");
+    params.add(SpellingParams.SPELLCHECK_COLLATE, "true");
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "1");
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "1");
+    params.add(CommonParams.Q, "peac");
+
+    //SpellCheckCompRH has no "qf" defined.  It will not find "peace" from "peac" despite it being in the dictionary
+    //because requrying against this Request Handler results in 0 hits.
+    SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
+    SolrQueryResponse rsp = new SolrQueryResponse();
+    rsp.add("responseHeader", new SimpleOrderedMap());
+    SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
+    handler.handleRequest(req, rsp);
     req.close();
-		values = rsp.getValues();
-		spellCheck = (NamedList) values.get("spellcheck");
-		suggestions = (NamedList) spellCheck.get("suggestions");
-		singleCollation = (String) suggestions.get("collation");
-		assertEquals("lowerfilt:(+faith +hope +loaves)", singleCollation);
+    NamedList values = rsp.getValues();
+    NamedList spellCheck = (NamedList) values.get("spellcheck");
+    NamedList suggestions = (NamedList) spellCheck.get("suggestions");
+    String singleCollation = (String) suggestions.get("collation");
+    assertNull(singleCollation);
 
-		// Testing returning multiple collations if more than one valid
-		// combination exists.
-		params.remove(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES);
-		params.remove(SpellingParams.SPELLCHECK_MAX_COLLATIONS);
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "10");
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "2");
-		handler = core.getRequestHandler("spellCheckCompRH");
-		rsp = new SolrQueryResponse();
-		rsp.add("responseHeader", new SimpleOrderedMap());
-		req = new LocalSolrQueryRequest(core, params);
-		handler.handleRequest(req, rsp);
-		req.close();
-		values = rsp.getValues();
-		spellCheck = (NamedList) values.get("spellcheck");
-		suggestions = (NamedList) spellCheck.get("suggestions");
-		List<String> collations = suggestions.getAll("collation");
-		assertTrue(collations.size() == 2);
-		for (String multipleCollation : collations) {
-			assertTrue(multipleCollation.equals("lowerfilt:(+faith +hope +love)")
-					|| multipleCollation.equals("lowerfilt:(+faith +hope +loaves)"));
-		}
+    //SpellCheckCompRH1 has "lowerfilt1" defined in the "qf" param.  It will find "peace" from "peac" because
+    //requrying field "lowerfilt1" returns the hit.
+    params.remove(SpellingParams.SPELLCHECK_BUILD);
+    handler = core.getRequestHandler("spellCheckCompRH1");
+    rsp = new SolrQueryResponse();
+    rsp.add("responseHeader", new SimpleOrderedMap());
+    req = new LocalSolrQueryRequest(core, params);
+    handler.handleRequest(req, rsp);
+    req.close();
+    values = rsp.getValues();
+    spellCheck = (NamedList) values.get("spellcheck");
+    suggestions = (NamedList) spellCheck.get("suggestions");
+    singleCollation = (String) suggestions.get("collation");
+    assertEquals(singleCollation, "peace");
+  }
 
-		// Testing return multiple collations with expanded collation response
-		// format.
-		params.add(SpellingParams.SPELLCHECK_COLLATE_EXTENDED_RESULTS, "true");
-		handler = core.getRequestHandler("spellCheckCompRH");
-		rsp = new SolrQueryResponse();
-		rsp.add("responseHeader", new SimpleOrderedMap());
-		req = new LocalSolrQueryRequest(core, params);
-		handler.handleRequest(req, rsp);
-		req.close();
-		values = rsp.getValues();
-		spellCheck = (NamedList) values.get("spellcheck");
-		suggestions = (NamedList) spellCheck.get("suggestions");
-		List<NamedList> expandedCollationList = suggestions.getAll("collation");
-		Set<String> usedcollations = new HashSet<String>();
-		assertTrue(expandedCollationList.size() == 2);
-		for (NamedList expandedCollation : expandedCollationList) {
-			String multipleCollation = (String) expandedCollation.get("collationQuery");
-			assertTrue(multipleCollation.equals("lowerfilt:(+faith +hope +love)")
-					|| multipleCollation.equals("lowerfilt:(+faith +hope +loaves)"));
-			assertTrue(!usedcollations.contains(multipleCollation));
-			usedcollations.add(multipleCollation);
+  @Test
+  public void testExtendedCollate() throws Exception {
+    SolrCore core = h.getCore();
+    SearchComponent speller = core.getSearchComponent("spellcheck");
+    assertTrue("speller is null and it shouldn't be", speller != null);
 
-			int hits = (Integer) expandedCollation.get("hits");
-			assertTrue(hits == 1);
+    ModifiableSolrParams params = new ModifiableSolrParams();
+    params.add(CommonParams.QT, "spellCheckCompRH");
+    params.add(CommonParams.Q, "lowerfilt:(+fauth +home +loane)");
+    params.add(SpellingParams.SPELLCHECK_EXTENDED_RESULTS, "true");
+    params.add(SpellCheckComponent.COMPONENT_NAME, "true");
+    params.add(SpellingParams.SPELLCHECK_BUILD, "true");
+    params.add(SpellingParams.SPELLCHECK_COUNT, "10");
+    params.add(SpellingParams.SPELLCHECK_COLLATE, "true");
 
-			NamedList misspellingsAndCorrections = (NamedList) expandedCollation.get("misspellingsAndCorrections");
-			assertTrue(misspellingsAndCorrections.size() == 3);
+    // Testing backwards-compatible behavior.
+    // Returns 1 collation as a single string.
+    // All words are "correct" per the dictionary, but this collation would
+    // return no results if tried.
+    SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
+    SolrQueryResponse rsp = new SolrQueryResponse();
+    rsp.add("responseHeader", new SimpleOrderedMap());
+    SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
+    handler.handleRequest(req, rsp);
+    req.close();
+    NamedList values = rsp.getValues();
+    NamedList spellCheck = (NamedList) values.get("spellcheck");
+    NamedList suggestions = (NamedList) spellCheck.get("suggestions");
+    String singleCollation = (String) suggestions.get("collation");
+    assertEquals("lowerfilt:(+faith +homer +loaves)", singleCollation);
 
-			String correctionForFauth = (String) misspellingsAndCorrections.get("fauth");
-			String correctionForHome = (String) misspellingsAndCorrections.get("home");
-			String correctionForLoane = (String) misspellingsAndCorrections.get("loane");
-			assertTrue(correctionForFauth.equals("faith"));
-			assertTrue(correctionForHome.equals("hope"));
-			assertTrue(correctionForLoane.equals("love") || correctionForLoane.equals("loaves"));
-		}
-	}
-	
-	@Test
-	public void testCollateWithGrouping() throws Exception
-	{
-		SolrCore core = h.getCore();
-		SearchComponent speller = core.getSearchComponent("spellcheck");
-		assertTrue("speller is null and it shouldn't be", speller != null);
-		
-		ModifiableSolrParams params = new ModifiableSolrParams();		
-		params.add(SpellCheckComponent.COMPONENT_NAME, "true");
-		params.add(SpellingParams.SPELLCHECK_BUILD, "true");
-		params.add(SpellingParams.SPELLCHECK_COUNT, "10");		
-		params.add(SpellingParams.SPELLCHECK_COLLATE, "true");
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "5");
-		params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "1");
-		params.add(CommonParams.Q, "lowerfilt:(+fauth)");
-		params.add(GroupParams.GROUP, "true");
-		params.add(GroupParams.GROUP_FIELD, "id");
-		
-		//Because a FilterQuery is applied which removes doc id#1 from possible hits, we would
-		//not want the collations to return us "lowerfilt:(+faith +hope +loaves)" as this only matches doc id#1.
-		SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
-		SolrQueryResponse rsp = new SolrQueryResponse();
-		rsp.add("responseHeader", new SimpleOrderedMap());
-		SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
-		handler.handleRequest(req, rsp);
-		req.close();
-		NamedList values = rsp.getValues();
-		NamedList spellCheck = (NamedList) values.get("spellcheck");
-		NamedList suggestions = (NamedList) spellCheck.get("suggestions");
-		List<String> collations = suggestions.getAll("collation");
-		assertTrue(collations.size() == 1);
-	}
-	
-	@Test
-	public void testContextSensitiveCollate() throws Exception {
-		//                     DirectSolrSpellChecker   IndexBasedSpellChecker
-		String[] dictionary = {"direct",                "default_teststop" };
-		for(int i=0 ; i<1 ; i++) {		
-			assertQ(
-				req(
-					"q", "teststop:(flew AND form AND heathrow)", 
-					"qt", "spellCheckCompRH",
-					"indent", "true",
-					SpellCheckComponent.COMPONENT_NAME, "true",
-					SpellCheckComponent.SPELLCHECK_DICT, dictionary[i],
-					SpellCheckComponent.SPELLCHECK_EXTENDED_RESULTS, "true", 
-					SpellCheckComponent.SPELLCHECK_COUNT, "10",
-					SpellCheckComponent.SPELLCHECK_ALTERNATIVE_TERM_COUNT, "5",
-					SpellCheckComponent.SPELLCHECK_MAX_RESULTS_FOR_SUGGEST, "0",
-					SpellCheckComponent.SPELLCHECK_COLLATE, "true",
-					SpellCheckComponent.SPELLCHECK_MAX_COLLATION_TRIES, "10",
-					SpellCheckComponent.SPELLCHECK_MAX_COLLATIONS, "1",
-					SpellCheckComponent.SPELLCHECK_COLLATE_EXTENDED_RESULTS, "true"
-				),
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='flew']/arr[@name='suggestion']/lst/str[@name='word']='flow'",
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='form']/arr[@name='suggestion']/lst/str[@name='word']='from'",
+    // Testing backwards-compatible response format but will only return a
+    // collation that would return results.
+    params.remove(SpellingParams.SPELLCHECK_BUILD);
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "5");
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "1");
+    handler = core.getRequestHandler("spellCheckCompRH");
+    rsp = new SolrQueryResponse();
+    rsp.add("responseHeader", new SimpleOrderedMap());
+    req = new LocalSolrQueryRequest(core, params);
+    handler.handleRequest(req, rsp);
+    req.close();
+    values = rsp.getValues();
+    spellCheck = (NamedList) values.get("spellcheck");
+    suggestions = (NamedList) spellCheck.get("suggestions");
+    singleCollation = (String) suggestions.get("collation");
+    assertEquals("lowerfilt:(+faith +hope +loaves)", singleCollation);
+
+    // Testing returning multiple collations if more than one valid
+    // combination exists.
+    params.remove(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES);
+    params.remove(SpellingParams.SPELLCHECK_MAX_COLLATIONS);
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "10");
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "2");
+    handler = core.getRequestHandler("spellCheckCompRH");
+    rsp = new SolrQueryResponse();
+    rsp.add("responseHeader", new SimpleOrderedMap());
+    req = new LocalSolrQueryRequest(core, params);
+    handler.handleRequest(req, rsp);
+    req.close();
+    values = rsp.getValues();
+    spellCheck = (NamedList) values.get("spellcheck");
+    suggestions = (NamedList) spellCheck.get("suggestions");
+    List<String> collations = suggestions.getAll("collation");
+    assertTrue(collations.size() == 2);
+    for (String multipleCollation : collations) {
+      assertTrue(multipleCollation.equals("lowerfilt:(+faith +hope +love)")
+          || multipleCollation.equals("lowerfilt:(+faith +hope +loaves)"));
+    }
+
+    // Testing return multiple collations with expanded collation response
+    // format.
+    params.add(SpellingParams.SPELLCHECK_COLLATE_EXTENDED_RESULTS, "true");
+    handler = core.getRequestHandler("spellCheckCompRH");
+    rsp = new SolrQueryResponse();
+    rsp.add("responseHeader", new SimpleOrderedMap());
+    req = new LocalSolrQueryRequest(core, params);
+    handler.handleRequest(req, rsp);
+    req.close();
+    values = rsp.getValues();
+    spellCheck = (NamedList) values.get("spellcheck");
+    suggestions = (NamedList) spellCheck.get("suggestions");
+    List<NamedList> expandedCollationList = suggestions.getAll("collation");
+    Set<String> usedcollations = new HashSet<String>();
+    assertTrue(expandedCollationList.size() == 2);
+    for (NamedList expandedCollation : expandedCollationList) {
+      String multipleCollation = (String) expandedCollation.get("collationQuery");
+      assertTrue(multipleCollation.equals("lowerfilt:(+faith +hope +love)")
+          || multipleCollation.equals("lowerfilt:(+faith +hope +loaves)"));
+      assertTrue(!usedcollations.contains(multipleCollation));
+      usedcollations.add(multipleCollation);
+
+      int hits = (Integer) expandedCollation.get("hits");
+      assertTrue(hits == 1);
+
+      NamedList misspellingsAndCorrections = (NamedList) expandedCollation.get("misspellingsAndCorrections");
+      assertTrue(misspellingsAndCorrections.size() == 3);
+
+      String correctionForFauth = (String) misspellingsAndCorrections.get("fauth");
+      String correctionForHome = (String) misspellingsAndCorrections.get("home");
+      String correctionForLoane = (String) misspellingsAndCorrections.get("loane");
+      assertTrue(correctionForFauth.equals("faith"));
+      assertTrue(correctionForHome.equals("hope"));
+      assertTrue(correctionForLoane.equals("love") || correctionForLoane.equals("loaves"));
+    }
+  }
+
+  @Test
+  public void testCollateWithGrouping() throws Exception
+  {
+    SolrCore core = h.getCore();
+    SearchComponent speller = core.getSearchComponent("spellcheck");
+    assertTrue("speller is null and it shouldn't be", speller != null);
+
+    ModifiableSolrParams params = new ModifiableSolrParams();
+    params.add(SpellCheckComponent.COMPONENT_NAME, "true");
+    params.add(SpellingParams.SPELLCHECK_BUILD, "true");
+    params.add(SpellingParams.SPELLCHECK_COUNT, "10");
+    params.add(SpellingParams.SPELLCHECK_COLLATE, "true");
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATION_TRIES, "5");
+    params.add(SpellingParams.SPELLCHECK_MAX_COLLATIONS, "1");
+    params.add(CommonParams.Q, "lowerfilt:(+fauth)");
+    params.add(GroupParams.GROUP, "true");
+    params.add(GroupParams.GROUP_FIELD, "id");
+
+    //Because a FilterQuery is applied which removes doc id#1 from possible hits, we would
+    //not want the collations to return us "lowerfilt:(+faith +hope +loaves)" as this only matches doc id#1.
+    SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
+    SolrQueryResponse rsp = new SolrQueryResponse();
+    rsp.add("responseHeader", new SimpleOrderedMap());
+    SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
+    handler.handleRequest(req, rsp);
+    req.close();
+    NamedList values = rsp.getValues();
+    NamedList spellCheck = (NamedList) values.get("spellcheck");
+    NamedList suggestions = (NamedList) spellCheck.get("suggestions");
+    List<String> collations = suggestions.getAll("collation");
+    assertTrue(collations.size() == 1);
+  }
+
+  @Test
+  public void testContextSensitiveCollate() throws Exception {
+    //                     DirectSolrSpellChecker   IndexBasedSpellChecker
+    String[] dictionary = {"direct",                "default_teststop" };
+    for(int i=0 ; i<1 ; i++) {
+      assertQ(
+        req(
+          "q", "teststop:(flew AND form AND heathrow)",
+          "qt", "spellCheckCompRH",
+          "indent", "true",
+          SpellCheckComponent.COMPONENT_NAME, "true",
+          SpellCheckComponent.SPELLCHECK_DICT, dictionary[i],
+          SpellCheckComponent.SPELLCHECK_EXTENDED_RESULTS, "true",
+          SpellCheckComponent.SPELLCHECK_COUNT, "10",
+          SpellCheckComponent.SPELLCHECK_ALTERNATIVE_TERM_COUNT, "5",
+          SpellCheckComponent.SPELLCHECK_MAX_RESULTS_FOR_SUGGEST, "0",
+          SpellCheckComponent.SPELLCHECK_COLLATE, "true",
+          SpellCheckComponent.SPELLCHECK_MAX_COLLATION_TRIES, "10",
+          SpellCheckComponent.SPELLCHECK_MAX_COLLATIONS, "1",
+          SpellCheckComponent.SPELLCHECK_COLLATE_EXTENDED_RESULTS, "true"
+        ),
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='flew']/arr[@name='suggestion']/lst/str[@name='word']='flow'",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='form']/arr[@name='suggestion']/lst/str[@name='word']='from'",
 /* DirectSolrSpellChecker won't suggest if the edit distance > 2, so we can't test for this one...
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='heathrow']/arr[@name='suggestion']/lst/str[@name='word']='hearth'",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='heathrow']/arr[@name='suggestion']/lst/str[@name='word']='hearth'",
 */
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/bool[@name='correctlySpelled']='false'",
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/str[@name='collationQuery']='teststop:(flew AND from AND heathrow)'",
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/int[@name='hits']=1",
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/lst[@name='misspellingsAndCorrections']/str[@name='form']='from'"		
-			);
-			
-			assertQ(
-				req(
-					"q", "teststop:(june AND customs)", 
-					"qt", "spellCheckCompRH",
-					"indent", "true",
-					SpellCheckComponent.COMPONENT_NAME, "true",
-					SpellCheckComponent.SPELLCHECK_DICT, dictionary[i],
-					SpellCheckComponent.SPELLCHECK_EXTENDED_RESULTS, "true", 
-					SpellCheckComponent.SPELLCHECK_COUNT, "10",
-					SpellCheckComponent.SPELLCHECK_ALTERNATIVE_TERM_COUNT, "5",
-					SpellCheckComponent.SPELLCHECK_MAX_RESULTS_FOR_SUGGEST, "1",
-					SpellCheckComponent.SPELLCHECK_COLLATE, "true",
-					SpellCheckComponent.SPELLCHECK_MAX_COLLATION_TRIES, "10",
-					SpellCheckComponent.SPELLCHECK_MAX_COLLATIONS, "1",
-					SpellCheckComponent.SPELLCHECK_COLLATE_EXTENDED_RESULTS, "true"
-				),
-				"//result[@numFound=1]",
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='june']/arr[@name='suggestion']/lst/str[@name='word']='jane'",
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/bool[@name='correctlySpelled']='false'",
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/str[@name='collationQuery']='teststop:(jane AND customs)'",
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/int[@name='hits']=1",
-				"//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/lst[@name='misspellingsAndCorrections']/str[@name='june']='jane'"
-			);
-		}				
-	}
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/bool[@name='correctlySpelled']='false'",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/str[@name='collationQuery']='teststop:(flew AND from AND heathrow)'",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/int[@name='hits']=1",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/lst[@name='misspellingsAndCorrections']/str[@name='form']='from'"
+      );
+
+      assertQ(
+        req(
+          "q", "teststop:(june AND customs)",
+          "qt", "spellCheckCompRH",
+          "indent", "true",
+          SpellCheckComponent.COMPONENT_NAME, "true",
+          SpellCheckComponent.SPELLCHECK_DICT, dictionary[i],
+          SpellCheckComponent.SPELLCHECK_EXTENDED_RESULTS, "true",
+          SpellCheckComponent.SPELLCHECK_COUNT, "10",
+          SpellCheckComponent.SPELLCHECK_ALTERNATIVE_TERM_COUNT, "5",
+          SpellCheckComponent.SPELLCHECK_MAX_RESULTS_FOR_SUGGEST, "1",
+          SpellCheckComponent.SPELLCHECK_COLLATE, "true",
+          SpellCheckComponent.SPELLCHECK_MAX_COLLATION_TRIES, "10",
+          SpellCheckComponent.SPELLCHECK_MAX_COLLATIONS, "1",
+          SpellCheckComponent.SPELLCHECK_COLLATE_EXTENDED_RESULTS, "true"
+        ),
+        "//result[@numFound=1]",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='june']/arr[@name='suggestion']/lst/str[@name='word']='jane'",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/bool[@name='correctlySpelled']='false'",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/str[@name='collationQuery']='teststop:(jane AND customs)'",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/int[@name='hits']=1",
+        "//lst[@name='spellcheck']/lst[@name='suggestions']/lst[@name='collation']/lst[@name='misspellingsAndCorrections']/str[@name='june']='jane'"
+      );
+    }
+  }
 }
Index: solr/core/src/test/org/apache/solr/handler/XsltUpdateRequestHandlerTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/handler/XsltUpdateRequestHandlerTest.java	(revision 1386643)
+++ solr/core/src/test/org/apache/solr/handler/XsltUpdateRequestHandlerTest.java	(working copy)
@@ -64,29 +64,29 @@
       " </document>" +
       "</random>";
 
-  	Map<String,String> args = new HashMap<String, String>();
-  	args.put(CommonParams.TR, "xsl-update-handler-test.xsl");
+    Map<String,String> args = new HashMap<String, String>();
+    args.put(CommonParams.TR, "xsl-update-handler-test.xsl");
       
-  	SolrCore core = h.getCore();
-  	LocalSolrQueryRequest req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
-  	ArrayList<ContentStream> streams = new ArrayList<ContentStream>();
-  	streams.add(new ContentStreamBase.StringStream(xml));
-  	req.setContentStreams(streams);
-  	SolrQueryResponse rsp = new SolrQueryResponse();
-  	UpdateRequestHandler handler = new UpdateRequestHandler();
-  	handler.init(new NamedList<String>());
-  	handler.handleRequestBody(req, rsp);
-  	StringWriter sw = new StringWriter(32000);
-  	QueryResponseWriter responseWriter = core.getQueryResponseWriter(req);
-  	responseWriter.write(sw,req,rsp);
-  	req.close();
-  	String response = sw.toString();
-  	assertU(response);
+    SolrCore core = h.getCore();
+    LocalSolrQueryRequest req = new LocalSolrQueryRequest( core, new MapSolrParams( args) );
+    ArrayList<ContentStream> streams = new ArrayList<ContentStream>();
+    streams.add(new ContentStreamBase.StringStream(xml));
+    req.setContentStreams(streams);
+    SolrQueryResponse rsp = new SolrQueryResponse();
+    UpdateRequestHandler handler = new UpdateRequestHandler();
+    handler.init(new NamedList<String>());
+    handler.handleRequestBody(req, rsp);
+    StringWriter sw = new StringWriter(32000);
+    QueryResponseWriter responseWriter = core.getQueryResponseWriter(req);
+    responseWriter.write(sw,req,rsp);
+    req.close();
+    String response = sw.toString();
+    assertU(response);
     assertU(commit());
 
     assertQ("test document was correctly committed", req("q","*:*")
             , "//result[@numFound='1']"
             , "//int[@name='id'][.='12345']"
-    		);  
+        );
   }
 }
Index: solr/core/src/test/org/apache/solr/handler/XmlUpdateRequestHandlerTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/handler/XmlUpdateRequestHandlerTest.java	(revision 1386643)
+++ solr/core/src/test/org/apache/solr/handler/XmlUpdateRequestHandlerTest.java	(working copy)
@@ -112,65 +112,65 @@
   
   @Test
   public void testReadDelete() throws Exception {
-	    String xml =
-	      "<update>" +
-	      " <delete>" +
-	      "   <query>id:150</query>" +
-	      "   <id>150</id>" +
-	      "   <id>200</id>" +
-	      "   <query>id:200</query>" +
-	      " </delete>" +
-	      " <delete commitWithin=\"500\">" +
-	      "   <query>id:150</query>" +
-	      " </delete>" +
-	      " <delete>" +
-	      "   <id>150</id>" +
-	      " </delete>" +
-	      "</update>";
-	    
-	    MockUpdateRequestProcessor p = new MockUpdateRequestProcessor(null);
-	    p.expectDelete(null, "id:150", -1);
-	    p.expectDelete("150", null, -1);
-	    p.expectDelete("200", null, -1);
-	    p.expectDelete(null, "id:200", -1);
-	    p.expectDelete(null, "id:150", 500);
-	    p.expectDelete("150", null, -1);
+      String xml =
+        "<update>" +
+        " <delete>" +
+        "   <query>id:150</query>" +
+        "   <id>150</id>" +
+        "   <id>200</id>" +
+        "   <query>id:200</query>" +
+        " </delete>" +
+        " <delete commitWithin=\"500\">" +
+        "   <query>id:150</query>" +
+        " </delete>" +
+        " <delete>" +
+        "   <id>150</id>" +
+        " </delete>" +
+        "</update>";
 
-	    XMLLoader loader = new XMLLoader().init(null);
-	    loader.load(req(), new SolrQueryResponse(), new ContentStreamBase.StringStream(xml), p);
-	    
-	    p.assertNoCommandsPending();
-	  }
-	  
-	  private class MockUpdateRequestProcessor extends UpdateRequestProcessor {
-	    
-	    private Queue<DeleteUpdateCommand> deleteCommands = new LinkedList<DeleteUpdateCommand>();
-	    
-	    public MockUpdateRequestProcessor(UpdateRequestProcessor next) {
-	      super(next);
-	    }
-	    
-	    public void expectDelete(String id, String query, int commitWithin) {
-	      DeleteUpdateCommand cmd = new DeleteUpdateCommand(null);
-	      cmd.id = id;
-	      cmd.query = query;
-	      cmd.commitWithin = commitWithin;
-	      deleteCommands.add(cmd);
-	    }
-	    
-	    public void assertNoCommandsPending() {
-	      assertTrue(deleteCommands.isEmpty());
-	    }
-	    
-	    @Override
-	    public void processDelete(DeleteUpdateCommand cmd) throws IOException {
-	      DeleteUpdateCommand expected = deleteCommands.poll();
-	      assertNotNull("Unexpected delete command: [" + cmd + "]", expected);
-	      assertTrue("Expected [" + expected + "] but found [" + cmd + "]",
-	          ObjectUtils.equals(expected.id, cmd.id) &&
-	          ObjectUtils.equals(expected.query, cmd.query) &&
-	          expected.commitWithin==cmd.commitWithin);
-	    }
-	  }
+      MockUpdateRequestProcessor p = new MockUpdateRequestProcessor(null);
+      p.expectDelete(null, "id:150", -1);
+      p.expectDelete("150", null, -1);
+      p.expectDelete("200", null, -1);
+      p.expectDelete(null, "id:200", -1);
+      p.expectDelete(null, "id:150", 500);
+      p.expectDelete("150", null, -1);
 
+      XMLLoader loader = new XMLLoader().init(null);
+      loader.load(req(), new SolrQueryResponse(), new ContentStreamBase.StringStream(xml), p);
+
+      p.assertNoCommandsPending();
+    }
+
+    private class MockUpdateRequestProcessor extends UpdateRequestProcessor {
+
+      private Queue<DeleteUpdateCommand> deleteCommands = new LinkedList<DeleteUpdateCommand>();
+
+      public MockUpdateRequestProcessor(UpdateRequestProcessor next) {
+        super(next);
+      }
+
+      public void expectDelete(String id, String query, int commitWithin) {
+        DeleteUpdateCommand cmd = new DeleteUpdateCommand(null);
+        cmd.id = id;
+        cmd.query = query;
+        cmd.commitWithin = commitWithin;
+        deleteCommands.add(cmd);
+      }
+
+      public void assertNoCommandsPending() {
+        assertTrue(deleteCommands.isEmpty());
+      }
+
+      @Override
+      public void processDelete(DeleteUpdateCommand cmd) throws IOException {
+        DeleteUpdateCommand expected = deleteCommands.poll();
+        assertNotNull("Unexpected delete command: [" + cmd + "]", expected);
+        assertTrue("Expected [" + expected + "] but found [" + cmd + "]",
+            ObjectUtils.equals(expected.id, cmd.id) &&
+            ObjectUtils.equals(expected.query, cmd.query) &&
+            expected.commitWithin==cmd.commitWithin);
+      }
+    }
+
 }
Index: solr/core/src/test/org/apache/solr/handler/component/SpellCheckComponentTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/handler/component/SpellCheckComponentTest.java	(revision 1386643)
+++ solr/core/src/test/org/apache/solr/handler/component/SpellCheckComponentTest.java	(working copy)
@@ -206,55 +206,55 @@
     
     @Test
     public void testThresholdTokenFrequency() throws Exception {
-    	
-  	  	//"document" is in 2 documents but "another" is only in 1.  
-  	  	//So with a threshold of 29%, "another" is absent from the dictionary 
-  	  	//while "document" is present.
-    	
-  	  	assertJQ(req("qt",rh, SpellCheckComponent.COMPONENT_NAME, "true", "q","documenq", SpellingParams.SPELLCHECK_DICT, "threshold", SpellingParams.SPELLCHECK_COUNT,"5", SpellingParams.SPELLCHECK_EXTENDED_RESULTS,"true")
-  	        ,"/spellcheck/suggestions/[1]/suggestion==[{'word':'document','freq':2}]"
-  	    );
-  	  	
-  	  	assertJQ(req("qt",rh, SpellCheckComponent.COMPONENT_NAME, "true", "q","documenq", SpellingParams.SPELLCHECK_DICT, "threshold_direct", SpellingParams.SPELLCHECK_COUNT,"5", SpellingParams.SPELLCHECK_EXTENDED_RESULTS,"true")
-  	        ,"/spellcheck/suggestions/[1]/suggestion==[{'word':'document','freq':2}]"
-  	    );
-  	  	
-  	  	//TODO:  how do we make this into a 1-liner using "assertQ()" ???
-  	  	SolrCore core = h.getCore();
-  	  	SearchComponent speller = core.getSearchComponent("spellcheck");
-  	  	assertTrue("speller is null and it shouldn't be", speller != null);
-  	  	
-  	  	ModifiableSolrParams params = new ModifiableSolrParams();		
-  			params.add(SpellCheckComponent.COMPONENT_NAME, "true");
-  			params.add(SpellingParams.SPELLCHECK_COUNT, "10");	
-  			params.add(SpellingParams.SPELLCHECK_DICT, "threshold");
-  			params.add(SpellingParams.SPELLCHECK_EXTENDED_RESULTS,"true");
-  			params.add(CommonParams.Q, "anotheq");
-  			
-  			SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
-  			SolrQueryResponse rsp = new SolrQueryResponse();
-  			rsp.add("responseHeader", new SimpleOrderedMap());
-  			SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
-  			handler.handleRequest(req, rsp);
-  			req.close();
-  			NamedList values = rsp.getValues();
-  			NamedList spellCheck = (NamedList) values.get("spellcheck");
-  			NamedList suggestions = (NamedList) spellCheck.get("suggestions");
-  			assertTrue(suggestions.get("suggestion")==null);
-  			assertTrue((Boolean) suggestions.get("correctlySpelled")==false);
-  			
-  			params.remove(SpellingParams.SPELLCHECK_DICT);
-  			params.add(SpellingParams.SPELLCHECK_DICT, "threshold_direct");
-  			rsp = new SolrQueryResponse();
-  			rsp.add("responseHeader", new SimpleOrderedMap());
-  			req = new LocalSolrQueryRequest(core, params);
-  			handler.handleRequest(req, rsp);
-  			req.close();
-  			values = rsp.getValues();
-  			spellCheck = (NamedList) values.get("spellcheck");
-  			suggestions = (NamedList) spellCheck.get("suggestions");
-  			assertTrue(suggestions.get("suggestion")==null);
-  			
-  			assertTrue((Boolean) suggestions.get("correctlySpelled")==false);
+
+        //"document" is in 2 documents but "another" is only in 1.
+        //So with a threshold of 29%, "another" is absent from the dictionary
+        //while "document" is present.
+
+        assertJQ(req("qt",rh, SpellCheckComponent.COMPONENT_NAME, "true", "q","documenq", SpellingParams.SPELLCHECK_DICT, "threshold", SpellingParams.SPELLCHECK_COUNT,"5", SpellingParams.SPELLCHECK_EXTENDED_RESULTS,"true")
+            ,"/spellcheck/suggestions/[1]/suggestion==[{'word':'document','freq':2}]"
+        );
+
+        assertJQ(req("qt",rh, SpellCheckComponent.COMPONENT_NAME, "true", "q","documenq", SpellingParams.SPELLCHECK_DICT, "threshold_direct", SpellingParams.SPELLCHECK_COUNT,"5", SpellingParams.SPELLCHECK_EXTENDED_RESULTS,"true")
+            ,"/spellcheck/suggestions/[1]/suggestion==[{'word':'document','freq':2}]"
+        );
+
+        //TODO:  how do we make this into a 1-liner using "assertQ()" ???
+        SolrCore core = h.getCore();
+        SearchComponent speller = core.getSearchComponent("spellcheck");
+        assertTrue("speller is null and it shouldn't be", speller != null);
+
+        ModifiableSolrParams params = new ModifiableSolrParams();
+        params.add(SpellCheckComponent.COMPONENT_NAME, "true");
+        params.add(SpellingParams.SPELLCHECK_COUNT, "10");
+        params.add(SpellingParams.SPELLCHECK_DICT, "threshold");
+        params.add(SpellingParams.SPELLCHECK_EXTENDED_RESULTS,"true");
+        params.add(CommonParams.Q, "anotheq");
+
+        SolrRequestHandler handler = core.getRequestHandler("spellCheckCompRH");
+        SolrQueryResponse rsp = new SolrQueryResponse();
+        rsp.add("responseHeader", new SimpleOrderedMap());
+        SolrQueryRequest req = new LocalSolrQueryRequest(core, params);
+        handler.handleRequest(req, rsp);
+        req.close();
+        NamedList values = rsp.getValues();
+        NamedList spellCheck = (NamedList) values.get("spellcheck");
+        NamedList suggestions = (NamedList) spellCheck.get("suggestions");
+        assertTrue(suggestions.get("suggestion")==null);
+        assertTrue((Boolean) suggestions.get("correctlySpelled")==false);
+
+        params.remove(SpellingParams.SPELLCHECK_DICT);
+        params.add(SpellingParams.SPELLCHECK_DICT, "threshold_direct");
+        rsp = new SolrQueryResponse();
+        rsp.add("responseHeader", new SimpleOrderedMap());
+        req = new LocalSolrQueryRequest(core, params);
+        handler.handleRequest(req, rsp);
+        req.close();
+        values = rsp.getValues();
+        spellCheck = (NamedList) values.get("spellcheck");
+        suggestions = (NamedList) spellCheck.get("suggestions");
+        assertTrue(suggestions.get("suggestion")==null);
+
+        assertTrue((Boolean) suggestions.get("correctlySpelled")==false);
     }
 }
Index: solr/core/src/test/org/apache/solr/handler/component/StatsComponentTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/handler/component/StatsComponentTest.java	(revision 1386643)
+++ solr/core/src/test/org/apache/solr/handler/component/StatsComponentTest.java	(working copy)
@@ -261,34 +261,34 @@
   }
   
   public void doTestFacetStatisticsMissingResult(String f) throws Exception {
-	    assertU(adoc("id", "1", f, "10", "active_s", "true"));
-	    assertU(adoc("id", "2", f, "20", "active_s", "true"));
-	    assertU(adoc("id", "3", "active_s", "false"));
-	    assertU(adoc("id", "4", f, "40", "active_s", "false"));
-	    assertU(commit());
+      assertU(adoc("id", "1", f, "10", "active_s", "true"));
+      assertU(adoc("id", "2", f, "20", "active_s", "true"));
+      assertU(adoc("id", "3", "active_s", "false"));
+      assertU(adoc("id", "4", f, "40", "active_s", "false"));
+      assertU(commit());
 
-	    assertQ("test value for active_s=true", req("q","*:*", "stats","true", "stats.field",f, "stats.facet","active_s")
-	            , "//lst[@name='true']/double[@name='min'][.='10.0']"
-	            , "//lst[@name='true']/double[@name='max'][.='20.0']"
-	            , "//lst[@name='true']/double[@name='sum'][.='30.0']"
-	            , "//lst[@name='true']/long[@name='count'][.='2']"
-	            , "//lst[@name='true']/long[@name='missing'][.='0']"
-	            , "//lst[@name='true']/double[@name='sumOfSquares'][.='500.0']"
-	            , "//lst[@name='true']/double[@name='mean'][.='15.0']"
-	            , "//lst[@name='true']/double[@name='stddev'][.='7.0710678118654755']"
-	    );
+      assertQ("test value for active_s=true", req("q","*:*", "stats","true", "stats.field",f, "stats.facet","active_s")
+              , "//lst[@name='true']/double[@name='min'][.='10.0']"
+              , "//lst[@name='true']/double[@name='max'][.='20.0']"
+              , "//lst[@name='true']/double[@name='sum'][.='30.0']"
+              , "//lst[@name='true']/long[@name='count'][.='2']"
+              , "//lst[@name='true']/long[@name='missing'][.='0']"
+              , "//lst[@name='true']/double[@name='sumOfSquares'][.='500.0']"
+              , "//lst[@name='true']/double[@name='mean'][.='15.0']"
+              , "//lst[@name='true']/double[@name='stddev'][.='7.0710678118654755']"
+      );
 
-	    assertQ("test value for active_s=false", req("q","*:*", "stats","true", "stats.field",f, "stats.facet","active_s")
-	            , "//lst[@name='false']/double[@name='min'][.='40.0']"
-	            , "//lst[@name='false']/double[@name='max'][.='40.0']"
-	            , "//lst[@name='false']/double[@name='sum'][.='40.0']"
-	            , "//lst[@name='false']/long[@name='count'][.='1']"
-	            , "//lst[@name='false']/long[@name='missing'][.='1']"
-	            , "//lst[@name='false']/double[@name='sumOfSquares'][.='1600.0']"
-	            , "//lst[@name='false']/double[@name='mean'][.='40.0']"
-	            , "//lst[@name='false']/double[@name='stddev'][.='0.0']"
-	    );
-	  }
+      assertQ("test value for active_s=false", req("q","*:*", "stats","true", "stats.field",f, "stats.facet","active_s")
+              , "//lst[@name='false']/double[@name='min'][.='40.0']"
+              , "//lst[@name='false']/double[@name='max'][.='40.0']"
+              , "//lst[@name='false']/double[@name='sum'][.='40.0']"
+              , "//lst[@name='false']/long[@name='count'][.='1']"
+              , "//lst[@name='false']/long[@name='missing'][.='1']"
+              , "//lst[@name='false']/double[@name='sumOfSquares'][.='1600.0']"
+              , "//lst[@name='false']/double[@name='mean'][.='40.0']"
+              , "//lst[@name='false']/double[@name='stddev'][.='0.0']"
+      );
+    }
 
   public void testFieldStatisticsResultsNumericFieldAlwaysMissing() throws Exception {
     SolrCore core = h.getCore();
Index: solr/core/src/test/org/apache/solr/handler/component/DistributedSpellCheckComponentTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/handler/component/DistributedSpellCheckComponentTest.java	(revision 1386643)
+++ solr/core/src/test/org/apache/solr/handler/component/DistributedSpellCheckComponentTest.java	(working copy)
@@ -42,12 +42,12 @@
   private String requestHandlerName;
   private String reqHandlerWithWordbreak;
   
-	public DistributedSpellCheckComponentTest()
-	{
-		//fixShardCount=true;
-		//shardCount=2;
-		//stress=0;
-	}
+  public DistributedSpellCheckComponentTest()
+  {
+    //fixShardCount=true;
+    //shardCount=2;
+    //stress=0;
+  }
 
   @BeforeClass
   public static void beforeClass() throws Exception {
@@ -100,7 +100,7 @@
   
   @Override
   public void doTest() throws Exception {
-  	del("*:*");
+    del("*:*");
     index(id, "1", "lowerfilt", "toyota");
     index(id, "2", "lowerfilt", "chevrolet");
     index(id, "3", "lowerfilt", "suzuki");
Index: solr/core/src/test/org/apache/solr/TestDistributedGrouping.java
===================================================================
--- solr/core/src/test/org/apache/solr/TestDistributedGrouping.java	(revision 1386643)
+++ solr/core/src/test/org/apache/solr/TestDistributedGrouping.java	(working copy)
@@ -140,7 +140,7 @@
 
     commit();
 
-	  // test grouping
+    // test grouping
     // The second sort = id asc . The sorting behaviour is different in dist mode. See TopDocs#merge
     // The shard the result came from matters in the order if both document sortvalues are equal
     query("q", "*:*", "rows", 100, "fl", "id," + i1, "group", "true", "group.field", i1, "group.limit", 10, "sort", i1 + " asc, id asc");
Index: solr/core/src/test/org/apache/solr/update/UpdateParamsTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/update/UpdateParamsTest.java	(revision 1386643)
+++ solr/core/src/test/org/apache/solr/update/UpdateParamsTest.java	(working copy)
@@ -54,8 +54,8 @@
     
     // First check that the old param behaves as it should
     try {
-    	handler.handleRequestBody(req, rsp);
-    	assertTrue("Old param update.processor should not have any effect anymore", true);
+      handler.handleRequestBody(req, rsp);
+      assertTrue("Old param update.processor should not have any effect anymore", true);
     } catch (Exception e) {
       assertFalse("Got wrong exception while testing update.chain", e.getMessage().equals("unknown UpdateRequestProcessorChain: nonexistant"));
     }
@@ -65,10 +65,10 @@
     params.getMap().put(UpdateParams.UPDATE_CHAIN, "nonexistant");    
     req.setParams(params);
     try {
-    	handler.handleRequestBody(req, rsp);
-    	assertFalse("Faulty update.chain parameter not causing an error - i.e. it is not detected", true);
+      handler.handleRequestBody(req, rsp);
+      assertFalse("Faulty update.chain parameter not causing an error - i.e. it is not detected", true);
     } catch (Exception e) {
-    	assertEquals("Got wrong exception while testing update.chain", e.getMessage(), "unknown UpdateRequestProcessorChain: nonexistant");
+      assertEquals("Got wrong exception while testing update.chain", e.getMessage(), "unknown UpdateRequestProcessorChain: nonexistant");
     }
     
   }
Index: solr/core/src/test/org/apache/solr/highlight/HighlighterConfigTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/highlight/HighlighterConfigTest.java	(revision 1386643)
+++ solr/core/src/test/org/apache/solr/highlight/HighlighterConfigTest.java	(working copy)
@@ -23,47 +23,47 @@
 import java.util.HashMap;
 
 public class HighlighterConfigTest extends AbstractSolrTestCase {
-	  @Override public String getSchemaFile() { return "schema.xml"; }
-	  // the default case (i.e. <highlight> without a class attribute) is tested every time sorlconfig.xml is used
-	  @Override public String getSolrConfigFile() { return "solrconfig-highlight.xml"; }
+    @Override public String getSchemaFile() { return "schema.xml"; }
+    // the default case (i.e. <highlight> without a class attribute) is tested every time sorlconfig.xml is used
+    @Override public String getSolrConfigFile() { return "solrconfig-highlight.xml"; }
 
-	  @Override 
-	  public void setUp() throws Exception {
-	    // if you override setUp or tearDown, you better call
-	    // the super classes version
-	    super.setUp();
-	  }
-	  
-	  @Override 
-	  public void tearDown() throws Exception {
-	    // if you override setUp or tearDown, you better call
-	    // the super classes version
-	    super.tearDown();
-	  }
-	  
-	  public void testConfig()
-	  {
+    @Override
+    public void setUp() throws Exception {
+      // if you override setUp or tearDown, you better call
+      // the super classes version
+      super.setUp();
+    }
+
+    @Override
+    public void tearDown() throws Exception {
+      // if you override setUp or tearDown, you better call
+      // the super classes version
+      super.tearDown();
+    }
+
+    public void testConfig()
+    {
             SolrHighlighter highlighter = HighlightComponent.getHighlighter(h.getCore());
-	    log.info( "highlighter" );
+      log.info( "highlighter" );
 
-	    assertTrue( highlighter instanceof DummyHighlighter );
-	    
-	    // check to see that doHighlight is called from the DummyHighlighter
-	    HashMap<String,String> args = new HashMap<String,String>();
-	    args.put("hl", "true");
-	    args.put("df", "t_text");
-	    args.put("hl.fl", "");
-	    TestHarness.LocalRequestFactory sumLRF = h.getRequestFactory(
-	      "standard", 0, 200, args);
-	    
-	    assertU(adoc("t_text", "a long day's night", "id", "1"));
-	    assertU(commit());
-	    assertU(optimize());
-	    assertQ("Basic summarization",
-	            sumLRF.makeRequest("long"),
-	            "//lst[@name='highlighting']/str[@name='dummy']"
-	            );
-	  }
+      assertTrue( highlighter instanceof DummyHighlighter );
+
+      // check to see that doHighlight is called from the DummyHighlighter
+      HashMap<String,String> args = new HashMap<String,String>();
+      args.put("hl", "true");
+      args.put("df", "t_text");
+      args.put("hl.fl", "");
+      TestHarness.LocalRequestFactory sumLRF = h.getRequestFactory(
+        "standard", 0, 200, args);
+
+      assertU(adoc("t_text", "a long day's night", "id", "1"));
+      assertU(commit());
+      assertU(optimize());
+      assertQ("Basic summarization",
+              sumLRF.makeRequest("long"),
+              "//lst[@name='highlighting']/str[@name='dummy']"
+              );
+    }
 }
 
 
Index: solr/core/src/test/org/apache/solr/highlight/DummyHighlighter.java
===================================================================
--- solr/core/src/test/org/apache/solr/highlight/DummyHighlighter.java	(revision 1386643)
+++ solr/core/src/test/org/apache/solr/highlight/DummyHighlighter.java	(working copy)
@@ -27,17 +27,17 @@
 
 public class DummyHighlighter extends SolrHighlighter {
 
-	@Override
-	public NamedList<Object> doHighlighting(DocList docs, Query query,
-			SolrQueryRequest req, String[] defaultFields) throws IOException {
-		NamedList fragments = new SimpleOrderedMap();
-		fragments.add("dummy", "thing1");
-		return fragments;
-	}
+  @Override
+  public NamedList<Object> doHighlighting(DocList docs, Query query,
+      SolrQueryRequest req, String[] defaultFields) throws IOException {
+    NamedList fragments = new SimpleOrderedMap();
+    fragments.add("dummy", "thing1");
+    return fragments;
+  }
 
-	@Override
-	public void initalize(SolrConfig config) {
-		// do nothing
-	}
+  @Override
+  public void initalize(SolrConfig config) {
+    // do nothing
+  }
 
 }
Index: solr/core/src/java/org/apache/solr/update/UpdateLog.java
===================================================================
--- solr/core/src/java/org/apache/solr/update/UpdateLog.java	(revision 1386643)
+++ solr/core/src/java/org/apache/solr/update/UpdateLog.java	(working copy)
@@ -63,17 +63,17 @@
 
 
   public enum SyncLevel { NONE, FLUSH, FSYNC;
-	  public static SyncLevel getSyncLevel(String level){
-	    if (level == null) {
-	      return SyncLevel.FLUSH;
-	    }
-		  try{
-			  return SyncLevel.valueOf(level.toUpperCase(Locale.ROOT));
-		  } catch(Exception ex){
-		    log.warn("There was an error reading the SyncLevel - default to " + SyncLevel.FLUSH, ex);
-			  return SyncLevel.FLUSH;
-		  }
-	  }
+    public static SyncLevel getSyncLevel(String level){
+      if (level == null) {
+        return SyncLevel.FLUSH;
+      }
+      try{
+        return SyncLevel.valueOf(level.toUpperCase(Locale.ROOT));
+      } catch(Exception ex){
+        log.warn("There was an error reading the SyncLevel - default to " + SyncLevel.FLUSH, ex);
+        return SyncLevel.FLUSH;
+      }
+    }
   }
   public enum State { REPLAYING, BUFFERING, APPLYING_BUFFERED, ACTIVE }
 
Index: solr/core/src/java/org/apache/solr/core/Config.java
===================================================================
--- solr/core/src/java/org/apache/solr/core/Config.java	(revision 1386643)
+++ solr/core/src/java/org/apache/solr/core/Config.java	(working copy)
@@ -141,8 +141,8 @@
       SolrException.log(log, "Exception during parsing file: " + name, e);
       throw e;
     } catch( SolrException e ){
-    	SolrException.log(log,"Error in "+name,e);
-    	throw e;
+      SolrException.log(log,"Error in "+name,e);
+      throw e;
     }
   }
   
Index: solr/core/src/java/org/apache/solr/core/CoreContainer.java
===================================================================
--- solr/core/src/java/org/apache/solr/core/CoreContainer.java	(revision 1386643)
+++ solr/core/src/java/org/apache/solr/core/CoreContainer.java	(working copy)
@@ -529,7 +529,7 @@
           }
           opt = DOMUtil.getAttr(node, CORE_ROLES, null);
           if(opt != null){
-        	  p.getCloudDescriptor().setRoles(opt);
+            p.getCloudDescriptor().setRoles(opt);
           }
         }
         opt = DOMUtil.getAttr(node, CORE_PROPERTIES, null);
Index: solr/core/src/java/org/apache/solr/response/RawResponseWriter.java
===================================================================
--- solr/core/src/java/org/apache/solr/response/RawResponseWriter.java	(revision 1386643)
+++ solr/core/src/java/org/apache/solr/response/RawResponseWriter.java	(working copy)
@@ -96,7 +96,7 @@
   }
 
 public void write(OutputStream out, SolrQueryRequest request,
-		SolrQueryResponse response) throws IOException {
+    SolrQueryResponse response) throws IOException {
     Object obj = response.getValues().get( CONTENT );
     if( obj != null && (obj instanceof ContentStream ) ) {
       // copy the contents to the writer...
@@ -110,8 +110,8 @@
     }
     else {
       //getBaseWriter( request ).write( writer, request, response );
-    	throw new IOException("did not find a CONTENT object");
+      throw new IOException("did not find a CONTENT object");
     }
-	
+
+  }
 }
-}
Index: solr/core/src/java/org/apache/solr/response/PHPSerializedResponseWriter.java
===================================================================
--- solr/core/src/java/org/apache/solr/response/PHPSerializedResponseWriter.java	(revision 1386643)
+++ solr/core/src/java/org/apache/solr/response/PHPSerializedResponseWriter.java	(working copy)
@@ -173,10 +173,10 @@
   
   @Override
   public void writeMapOpener(int size) throws IOException, IllegalArgumentException {
-  	// negative size value indicates that something has gone wrong
-  	if (size < 0) {
-  		throw new IllegalArgumentException("Map size must not be negative");
-  	}
+    // negative size value indicates that something has gone wrong
+    if (size < 0) {
+      throw new IllegalArgumentException("Map size must not be negative");
+    }
     writer.write("a:"+size+":{");
   }
   
@@ -192,10 +192,10 @@
 
   @Override
   public void writeArrayOpener(int size) throws IOException, IllegalArgumentException {
-  	// negative size value indicates that something has gone wrong
-  	if (size < 0) {
-  		throw new IllegalArgumentException("Array size must not be negative");
-  	}
+    // negative size value indicates that something has gone wrong
+    if (size < 0) {
+      throw new IllegalArgumentException("Array size must not be negative");
+    }
     writer.write("a:"+size+":{");
   }
 
Index: solr/core/src/java/org/apache/solr/response/SolrQueryResponse.java
===================================================================
--- solr/core/src/java/org/apache/solr/response/SolrQueryResponse.java	(revision 1386643)
+++ solr/core/src/java/org/apache/solr/response/SolrQueryResponse.java	(working copy)
@@ -194,8 +194,8 @@
   /** Repsonse header to be logged */ 
   public NamedList<Object> getResponseHeader() {
     @SuppressWarnings("unchecked")
-	  SimpleOrderedMap<Object> header = (SimpleOrderedMap<Object>) values.get("responseHeader");
-	  return header;
+    SimpleOrderedMap<Object> header = (SimpleOrderedMap<Object>) values.get("responseHeader");
+    return header;
   }
   
   /** Add a value to be logged.
@@ -204,7 +204,7 @@
    * @param val value of the thing to log
    */
   public void addToLog(String name, Object val) {
-	  toLog.add(name, val);
+    toLog.add(name, val);
   }
   
   /** Get loggable items.
@@ -212,7 +212,7 @@
    * @return things to log
    */
   public NamedList<Object> getToLog() {
-	  return toLog;
+    return toLog;
   }
   
   /**
Index: solr/core/src/java/org/apache/solr/schema/SchemaField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/SchemaField.java	(revision 1386643)
+++ solr/core/src/java/org/apache/solr/schema/SchemaField.java	(working copy)
@@ -184,7 +184,7 @@
 
     String defaultValue = null;
     if( props.containsKey( "default" ) ) {
-    	defaultValue = props.get( "default" );
+      defaultValue = props.get( "default" );
     }
     return new SchemaField(name, ft, calcProps(name, ft, props), defaultValue );
   }
Index: solr/core/src/java/org/apache/solr/search/QParser.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/QParser.java	(revision 1386643)
+++ solr/core/src/java/org/apache/solr/search/QParser.java	(working copy)
@@ -224,18 +224,18 @@
     String pageScoreS = null;
     String pageDocS = null;
 
-	  pageScoreS = params.get(CommonParams.PAGESCORE);
-	  pageDocS = params.get(CommonParams.PAGEDOC);
-		  
-	  if (pageScoreS == null || pageDocS == null)
-		  return null;
-	  
-	  int pageDoc = pageDocS != null ? Integer.parseInt(pageDocS) : -1;
-	  float pageScore = pageScoreS != null ? new Float(pageScoreS) : -1;
-	  if(pageDoc != -1 && pageScore != -1){
+    pageScoreS = params.get(CommonParams.PAGESCORE);
+    pageDocS = params.get(CommonParams.PAGEDOC);
+
+    if (pageScoreS == null || pageDocS == null)
+      return null;
+
+    int pageDoc = pageDocS != null ? Integer.parseInt(pageDocS) : -1;
+    float pageScore = pageScoreS != null ? new Float(pageScoreS) : -1;
+    if(pageDoc != -1 && pageScore != -1){
       return new ScoreDoc(pageDoc, pageScore);
     }
-	  else {
+    else {
       return null;
     }
 
Index: solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java	(revision 1386643)
+++ solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java	(working copy)
@@ -1369,7 +1369,7 @@
       TopDocsCollector topCollector;
       if (cmd.getSort() == null) {
         if(cmd.getScoreDoc() != null) {
-        	topCollector = TopScoreDocCollector.create(len, cmd.getScoreDoc(), true); //create the Collector with InOrderPagingCollector
+          topCollector = TopScoreDocCollector.create(len, cmd.getScoreDoc(), true); //create the Collector with InOrderPagingCollector
         } else {
           topCollector = TopScoreDocCollector.create(len, true);
         }
@@ -2029,11 +2029,11 @@
     
     public ScoreDoc getScoreDoc()
     {
-    	return scoreDoc;
+      return scoreDoc;
     }
     public void setScoreDoc(ScoreDoc scoreDoc)
     {
-    	this.scoreDoc = scoreDoc;
+      this.scoreDoc = scoreDoc;
     }
     //Issue 1726 end
 
Index: solr/core/src/java/org/apache/solr/cloud/CloudDescriptor.java
===================================================================
--- solr/core/src/java/org/apache/solr/cloud/CloudDescriptor.java	(revision 1386643)
+++ solr/core/src/java/org/apache/solr/cloud/CloudDescriptor.java	(working copy)
@@ -54,11 +54,11 @@
   }
 
   public String getRoles(){
-	  return roles;
+    return roles;
   }
   
   public void setRoles(String roles){
-	  this.roles = roles;
+    this.roles = roles;
   }
   
   /** Optional parameters that can change how a core is created. */
Index: solr/core/src/java/org/apache/solr/spelling/DirectSolrSpellChecker.java
===================================================================
--- solr/core/src/java/org/apache/solr/spelling/DirectSolrSpellChecker.java	(revision 1386643)
+++ solr/core/src/java/org/apache/solr/spelling/DirectSolrSpellChecker.java	(working copy)
@@ -214,7 +214,7 @@
         result.add(token, empty);
       } else {        
         for (SuggestWord suggestion : suggestions) {
-          result.add(token, suggestion.string, suggestion.freq);      	
+          result.add(token, suggestion.string, suggestion.freq);
         }
       }
     }
Index: solr/core/src/java/org/apache/solr/spelling/ConjunctionSolrSpellChecker.java
===================================================================
--- solr/core/src/java/org/apache/solr/spelling/ConjunctionSolrSpellChecker.java	(revision 1386643)
+++ solr/core/src/java/org/apache/solr/spelling/ConjunctionSolrSpellChecker.java	(working copy)
@@ -139,7 +139,7 @@
     Map<Token, Integer> combinedTokenFrequency = new HashMap<Token, Integer>();
     Map<Token, List<LinkedHashMap<String, Integer>>> allSuggestions = new LinkedHashMap<Token, List<LinkedHashMap<String, Integer>>>();
     for(SpellingResult result : results) {
-    	if(result.getTokenFrequency()!=null) {
+      if(result.getTokenFrequency()!=null) {
         combinedTokenFrequency.putAll(result.getTokenFrequency());
       }
       for(Map.Entry<Token, LinkedHashMap<String, Integer>> entry : result.getSuggestions().entrySet()) {
Index: solr/core/src/java/org/apache/solr/spelling/AbstractLuceneSpellChecker.java
===================================================================
--- solr/core/src/java/org/apache/solr/spelling/AbstractLuceneSpellChecker.java	(revision 1386643)
+++ solr/core/src/java/org/apache/solr/spelling/AbstractLuceneSpellChecker.java	(working copy)
@@ -142,7 +142,7 @@
   
   @Override
   public SpellingResult getSuggestions(SpellingOptions options) throws IOException {
-  	SpellingResult result = new SpellingResult(options.tokens);
+    SpellingResult result = new SpellingResult(options.tokens);
     IndexReader reader = determineReader(options.reader);
     Term term = field != null ? new Term(field, "") : null;
     float theAccuracy = (options.accuracy == Float.MIN_VALUE) ? spellChecker.getAccuracy() : options.accuracy;
@@ -187,13 +187,13 @@
         int countLimit = Math.min(options.count, suggestions.length);
         if(countLimit>0)
         {
-	        for (int i = 0; i < countLimit; i++) {
-	          term = new Term(field, suggestions[i]);
-	          result.add(token, suggestions[i], reader.docFreq(term));
-	        }
+          for (int i = 0; i < countLimit; i++) {
+            term = new Term(field, suggestions[i]);
+            result.add(token, suggestions[i], reader.docFreq(term));
+          }
         } else {
-        	List<String> suggList = Collections.emptyList();
-        	result.add(token, suggList);
+          List<String> suggList = Collections.emptyList();
+          result.add(token, suggList);
         }
       } else {
         if (suggestions.length > 0) {
@@ -203,8 +203,8 @@
           }
           result.add(token, suggList);
         } else {
-        	List<String> suggList = Collections.emptyList();
-        	result.add(token, suggList);
+          List<String> suggList = Collections.emptyList();
+          result.add(token, suggList);
         }
       }
     }
Index: solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java	(revision 1386643)
+++ solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java	(working copy)
@@ -191,42 +191,42 @@
   }
   
   @SuppressWarnings("unchecked")
-	protected void addCollationsToResponse(SolrParams params, SpellingResult spellingResult, ResponseBuilder rb, String q,
-	    NamedList response, boolean suggestionsMayOverlap) {
-		int maxCollations = params.getInt(SPELLCHECK_MAX_COLLATIONS, 1);
-		int maxCollationTries = params.getInt(SPELLCHECK_MAX_COLLATION_TRIES, 0);
-		int maxCollationEvaluations = params.getInt(SPELLCHECK_MAX_COLLATION_EVALUATIONS, 10000);
-		boolean collationExtendedResults = params.getBool(SPELLCHECK_COLLATE_EXTENDED_RESULTS, false);
-		boolean shard = params.getBool(ShardParams.IS_SHARD, false);
+  protected void addCollationsToResponse(SolrParams params, SpellingResult spellingResult, ResponseBuilder rb, String q,
+      NamedList response, boolean suggestionsMayOverlap) {
+    int maxCollations = params.getInt(SPELLCHECK_MAX_COLLATIONS, 1);
+    int maxCollationTries = params.getInt(SPELLCHECK_MAX_COLLATION_TRIES, 0);
+    int maxCollationEvaluations = params.getInt(SPELLCHECK_MAX_COLLATION_EVALUATIONS, 10000);
+    boolean collationExtendedResults = params.getBool(SPELLCHECK_COLLATE_EXTENDED_RESULTS, false);
+    boolean shard = params.getBool(ShardParams.IS_SHARD, false);
 
-		SpellCheckCollator collator = new SpellCheckCollator();
-		List<SpellCheckCollation> collations = collator.collate(spellingResult, q, rb, maxCollations, maxCollationTries, maxCollationEvaluations, suggestionsMayOverlap);
+    SpellCheckCollator collator = new SpellCheckCollator();
+    List<SpellCheckCollation> collations = collator.collate(spellingResult, q, rb, maxCollations, maxCollationTries, maxCollationEvaluations, suggestionsMayOverlap);
     //by sorting here we guarantee a non-distributed request returns all 
-		//results in the same order as a distributed request would, 
-		//even in cases when the internal rank is the same.
-		Collections.sort(collations);
-		
-		for (SpellCheckCollation collation : collations) {
-			if (collationExtendedResults) {
-				NamedList extendedResult = new NamedList();
-				extendedResult.add("collationQuery", collation.getCollationQuery());
-				extendedResult.add("hits", collation.getHits());
-				extendedResult.add("misspellingsAndCorrections", collation.getMisspellingsAndCorrections());
-				if(maxCollationTries>0 && shard)
-				{
-					extendedResult.add("collationInternalRank", collation.getInternalRank());
-				}
-				response.add("collation", extendedResult);
-			} else {
-				response.add("collation", collation.getCollationQuery());
-				if(maxCollationTries>0 && shard)
-				{
-					response.add("collationInternalRank", collation.getInternalRank());
-				}
-			}
-		}
-	}
+    //results in the same order as a distributed request would,
+    //even in cases when the internal rank is the same.
+    Collections.sort(collations);
 
+    for (SpellCheckCollation collation : collations) {
+      if (collationExtendedResults) {
+        NamedList extendedResult = new NamedList();
+        extendedResult.add("collationQuery", collation.getCollationQuery());
+        extendedResult.add("hits", collation.getHits());
+        extendedResult.add("misspellingsAndCorrections", collation.getMisspellingsAndCorrections());
+        if(maxCollationTries>0 && shard)
+        {
+          extendedResult.add("collationInternalRank", collation.getInternalRank());
+        }
+        response.add("collation", extendedResult);
+      } else {
+        response.add("collation", collation.getCollationQuery());
+        if(maxCollationTries>0 && shard)
+        {
+          response.add("collationInternalRank", collation.getInternalRank());
+        }
+      }
+    }
+  }
+
   /**
    * For every param that is of the form "spellcheck.[dictionary name].XXXX=YYYY, add
    * XXXX=YYYY as a param to the custom param list
@@ -297,8 +297,8 @@
           NamedList nl = (NamedList) srsp.getSolrResponse().getResponse().get("spellcheck");
           LOG.info(srsp.getShard() + " " + nl);
           if (nl != null) {
-          	mergeData.totalNumberShardResponses++;
-          	collectShardSuggestions(nl, mergeData);          
+            mergeData.totalNumberShardResponses++;
+            collectShardSuggestions(nl, mergeData);
             collectShardCollations(mergeData, nl, maxCollationTries);
           }
         }
@@ -317,22 +317,22 @@
       SpellCheckCollation[] sortedCollations = mergeData.collations.values()
           .toArray(new SpellCheckCollation[mergeData.collations.size()]);
       Arrays.sort(sortedCollations);
-			int i = 0;
-			while (i < maxCollations && i < sortedCollations.length) {
-				SpellCheckCollation collation = sortedCollations[i];
-				i++;
-				if (collationExtendedResults) {
-					NamedList extendedResult = new NamedList();
-					extendedResult.add("collationQuery", collation.getCollationQuery());
-					extendedResult.add("hits", collation.getHits());
-					extendedResult.add("misspellingsAndCorrections", collation
-							.getMisspellingsAndCorrections());
-					suggestions.add("collation", extendedResult);
-				} else {
-					suggestions.add("collation", collation.getCollationQuery());
-				}
-			}
-		}
+      int i = 0;
+      while (i < maxCollations && i < sortedCollations.length) {
+        SpellCheckCollation collation = sortedCollations[i];
+        i++;
+        if (collationExtendedResults) {
+          NamedList extendedResult = new NamedList();
+          extendedResult.add("collationQuery", collation.getCollationQuery());
+          extendedResult.add("hits", collation.getHits());
+          extendedResult.add("misspellingsAndCorrections", collation
+              .getMisspellingsAndCorrections());
+          suggestions.add("collation", extendedResult);
+        } else {
+          suggestions.add("collation", collation.getCollationQuery());
+        }
+      }
+    }
     
     response.add("suggestions", suggestions);
     rb.rsp.add("spellcheck", response);
Index: solr/core/src/java/org/apache/solr/handler/component/ResponseBuilder.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/component/ResponseBuilder.java	(revision 1386643)
+++ solr/core/src/java/org/apache/solr/handler/component/ResponseBuilder.java	(working copy)
@@ -409,11 +409,11 @@
 
   public ScoreDoc getScoreDoc()
   {
-	  return scoreDoc;
+    return scoreDoc;
   }
   
   public void setScoreDoc(ScoreDoc scoreDoc)
   {
-	  this.scoreDoc = scoreDoc;
+    this.scoreDoc = scoreDoc;
   }
 }
Index: solr/core/src/java/org/apache/solr/handler/loader/CSVLoaderBase.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/loader/CSVLoaderBase.java	(revision 1386643)
+++ solr/core/src/java/org/apache/solr/handler/loader/CSVLoaderBase.java	(working copy)
@@ -388,12 +388,12 @@
 
     // add any literals
     for (SchemaField sf : literals.keySet()) {
-    	String fn = sf.getName();
-    	String val = literals.get(sf);
-    	doc.addField(fn, val);
+      String fn = sf.getName();
+      String val = literals.get(sf);
+      doc.addField(fn, val);
     }
    
     template.solrDoc = doc;
     processor.processAdd(template);
   }
-}
\ No newline at end of file
+}
Index: solr/core/src/java/org/apache/solr/util/LongPriorityQueue.java
===================================================================
--- solr/core/src/java/org/apache/solr/util/LongPriorityQueue.java	(revision 1386643)
+++ solr/core/src/java/org/apache/solr/util/LongPriorityQueue.java	(working copy)
@@ -152,10 +152,10 @@
     time.  Only valid if size() > 0.
    */
   public long pop() {
-    long result = heap[1];	          // save first value
-    heap[1] = heap[size];	          // move last to first
+    long result = heap[1];            // save first value
+    heap[1] = heap[size];            // move last to first
     size--;
-    downHeap();				  // adjust heap
+    downHeap();          // adjust heap
     return result;
   }
   
@@ -187,11 +187,11 @@
    */
   public long[] sort(int n) {
     while (--n >= 0) {
-      long result = heap[1];	          // save first value
-      heap[1] = heap[size];	          // move last to first
+      long result = heap[1];            // save first value
+      heap[1] = heap[size];            // move last to first
       heap[size] = result;                  // place it last
       size--;
-      downHeap();				  // adjust heap
+      downHeap();          // adjust heap
     }
     return heap;
   }
@@ -203,26 +203,26 @@
 
   private void upHeap() {
     int i = size;
-    long node = heap[i];			  // save bottom node
+    long node = heap[i];        // save bottom node
     int j = i >>> 1;
     while (j > 0 && node < heap[j]) {
-      heap[i] = heap[j];			  // shift parents down
+      heap[i] = heap[j];        // shift parents down
       i = j;
       j = j >>> 1;
     }
-    heap[i] = node;				  // install saved node
+    heap[i] = node;          // install saved node
   }
 
   private void downHeap() {
     int i = 1;
-    long node = heap[i];			  // save top node
-    int j = i << 1;				  // find smaller child
+    long node = heap[i];        // save top node
+    int j = i << 1;          // find smaller child
     int k = j + 1;
     if (k <= size && heap[k] < heap[j]) {
       j = k;
     }
     while (j <= size && heap[j] < node) {
-      heap[i] = heap[j];			  // shift up child
+      heap[i] = heap[j];        // shift up child
       i = j;
       j = i << 1;
       k = j + 1;
@@ -230,6 +230,6 @@
         j = k;
       }
     }
-    heap[i] = node;				  // install saved node
+    heap[i] = node;          // install saved node
   }
 }
Index: solr/solrj/src/test/org/apache/solr/client/solrj/response/TestSpellCheckResponse.java
===================================================================
--- solr/solrj/src/test/org/apache/solr/client/solrj/response/TestSpellCheckResponse.java	(revision 1386643)
+++ solr/solrj/src/test/org/apache/solr/client/solrj/response/TestSpellCheckResponse.java	(working copy)
@@ -108,7 +108,7 @@
   
   @Test
   public void testSpellCheckCollationResponse() throws Exception {
-  	getSolrServer();
+    getSolrServer();
     server.deleteByQuery("*:*");
     server.commit(true, true);
     SolrInputDocument doc = new SolrInputDocument();
@@ -156,27 +156,27 @@
     assertEquals(2, collations.size());
     for(Collation collation : collations)
     {
-    	assertTrue("name:(+faith +hope +love)".equals(collation.getCollationQueryString()) || "name:(+faith +hope +loaves)".equals(collation.getCollationQueryString()));
+      assertTrue("name:(+faith +hope +love)".equals(collation.getCollationQueryString()) || "name:(+faith +hope +loaves)".equals(collation.getCollationQueryString()));
       assertTrue(collation.getNumberOfHits()==1);
-    	
-    	List<Correction> misspellingsAndCorrections = collation.getMisspellingsAndCorrections();
-    	assertTrue(misspellingsAndCorrections.size()==3);
-    	for(Correction correction : misspellingsAndCorrections)
-    	{    	
-    		if("fauth".equals(correction.getOriginal()))
-    		{
-    			assertTrue("faith".equals(correction.getCorrection()));
-    		} else if("home".equals(correction.getOriginal()))
-    		{
-    			assertTrue("hope".equals(correction.getCorrection()));
-    		} else if("loane".equals(correction.getOriginal()))
-    		{
-    			assertTrue("love".equals(correction.getCorrection()) || "loaves".equals(correction.getCorrection()));
-    		} else
-    		{
-    			fail("Original Word Should have been either fauth, home or loane.");
-    		}	    	
-    	}
+
+      List<Correction> misspellingsAndCorrections = collation.getMisspellingsAndCorrections();
+      assertTrue(misspellingsAndCorrections.size()==3);
+      for(Correction correction : misspellingsAndCorrections)
+      {
+        if("fauth".equals(correction.getOriginal()))
+        {
+          assertTrue("faith".equals(correction.getCorrection()));
+        } else if("home".equals(correction.getOriginal()))
+        {
+          assertTrue("hope".equals(correction.getCorrection()));
+        } else if("loane".equals(correction.getOriginal()))
+        {
+          assertTrue("love".equals(correction.getCorrection()) || "loaves".equals(correction.getCorrection()));
+        } else
+        {
+          fail("Original Word Should have been either fauth, home or loane.");
+        }
+      }
     }
     
     query.set(SpellingParams.SPELLCHECK_COLLATE_EXTENDED_RESULTS, false);
Index: solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.java
===================================================================
--- solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.java	(revision 1386643)
+++ solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.java	(working copy)
@@ -141,7 +141,7 @@
 
           public void command() {
             try {
-            	ZkStateReader.this.createClusterStateWatchersAndUpdate();
+              ZkStateReader.this.createClusterStateWatchersAndUpdate();
             } catch (KeeperException e) {
               log.error("", e);
               throw new ZooKeeperException(SolrException.ErrorCode.SERVER_ERROR,
@@ -378,12 +378,12 @@
   }
   
   abstract class RunnableWatcher implements Runnable {
-		Watcher watcher;
-		public RunnableWatcher(Watcher watcher){
-			this.watcher = watcher;
-		}
+    Watcher watcher;
+    public RunnableWatcher(Watcher watcher){
+      this.watcher = watcher;
+    }
 
-	}
+  }
   
   public String getLeaderUrl(String collection, String shard, int timeout)
       throws InterruptedException, KeeperException {
Index: solr/solrj/src/java/org/apache/solr/common/util/NamedList.java
===================================================================
--- solr/solrj/src/java/org/apache/solr/common/util/NamedList.java	(revision 1386643)
+++ solr/solrj/src/java/org/apache/solr/common/util/NamedList.java	(working copy)
@@ -367,12 +367,12 @@
           @Override
           public String toString()
           {
-        	  return getKey()+"="+getValue();
+            return getKey()+"="+getValue();
           }
 
-    		  public T setValue(T value) {
+          public T setValue(T value) {
             return list.setVal(index, value);
-    		  }
+          }
         };
         return nv;
       }
Index: solr/solrj/src/java/org/apache/solr/client/solrj/beans/DocumentObjectBinder.java
===================================================================
--- solr/solrj/src/java/org/apache/solr/client/solrj/beans/DocumentObjectBinder.java	(revision 1386643)
+++ solr/solrj/src/java/org/apache/solr/client/solrj/beans/DocumentObjectBinder.java	(working copy)
@@ -88,7 +88,7 @@
       } else {
         doc.setField(field.name, field.get(obj), 1.0f);
       }
-	}
+    }
     return doc;
   }
   
Index: solr/solrj/src/java/org/apache/solr/client/solrj/request/LukeRequest.java
===================================================================
--- solr/solrj/src/java/org/apache/solr/client/solrj/request/LukeRequest.java	(revision 1386643)
+++ solr/solrj/src/java/org/apache/solr/client/solrj/request/LukeRequest.java	(working copy)
@@ -108,7 +108,7 @@
       params.add( "numTerms", numTerms+"" );
     }
     if (showSchema) {
-    	params.add("show", "schema");
+      params.add("show", "schema");
     }
     return params;
   }
Index: solr/solrj/src/java/org/apache/solr/client/solrj/response/SpellCheckResponse.java
===================================================================
--- solr/solrj/src/java/org/apache/solr/client/solrj/response/SpellCheckResponse.java	(revision 1386643)
+++ solr/solrj/src/java/org/apache/solr/client/solrj/response/SpellCheckResponse.java	(working copy)
@@ -45,41 +45,41 @@
       String n = sugg.getName(i);
       if ("correctlySpelled".equals(n)) {
         correctlySpelled = (Boolean) sugg.getVal(i);
-			} else if ("collationInternalRank".equals(n)){
-				//continue;
-			} else if ("collation".equals(n)) {
-				List<Object> collationInfo = sugg.getAll(n);
-				collations = new ArrayList<Collation>(collationInfo.size());
-				for (Object o : collationInfo) {
-					if (o instanceof String) {
-						collations.add(new Collation()
-								.setCollationQueryString((String) o));
-					} else if (o instanceof NamedList) {
+      } else if ("collationInternalRank".equals(n)){
+        //continue;
+      } else if ("collation".equals(n)) {
+        List<Object> collationInfo = sugg.getAll(n);
+        collations = new ArrayList<Collation>(collationInfo.size());
+        for (Object o : collationInfo) {
+          if (o instanceof String) {
+            collations.add(new Collation()
+                .setCollationQueryString((String) o));
+          } else if (o instanceof NamedList) {
             @SuppressWarnings("unchecked")
-						NamedList<Object> expandedCollation = (NamedList<Object>) o;
-						String collationQuery 
+            NamedList<Object> expandedCollation = (NamedList<Object>) o;
+            String collationQuery
               = (String) expandedCollation.get("collationQuery");
-						int hits = (Integer) expandedCollation.get("hits");
+            int hits = (Integer) expandedCollation.get("hits");
             @SuppressWarnings("unchecked")
-						NamedList<String> misspellingsAndCorrections 
+            NamedList<String> misspellingsAndCorrections
               = (NamedList<String>) expandedCollation.get("misspellingsAndCorrections");
 
-						Collation collation = new Collation();
-						collation.setCollationQueryString(collationQuery);
-						collation.setNumberOfHits(hits);
+            Collation collation = new Collation();
+            collation.setCollationQueryString(collationQuery);
+            collation.setNumberOfHits(hits);
 
-						for (int ii = 0; ii < misspellingsAndCorrections.size(); ii++) {
-							String misspelling = misspellingsAndCorrections.getName(ii);
-							String correction = misspellingsAndCorrections.getVal(ii);
-							collation.addMisspellingsAndCorrection(new Correction(
-									misspelling, correction));
-						}
-						collations.add(collation);
-					} else {
-						throw new AssertionError(
-								"Should get Lists of Strings or List of NamedLists here.");
-					}
-				} 	
+            for (int ii = 0; ii < misspellingsAndCorrections.size(); ii++) {
+              String misspelling = misspellingsAndCorrections.getName(ii);
+              String correction = misspellingsAndCorrections.getVal(ii);
+              collation.addMisspellingsAndCorrection(new Correction(
+                  misspelling, correction));
+            }
+            collations.add(collation);
+          } else {
+            throw new AssertionError(
+                "Should get Lists of Strings or List of NamedLists here.");
+          }
+        }
       } else {
         @SuppressWarnings("unchecked")
         Suggestion s = new Suggestion(n, (NamedList<Object>) sugg.getVal(i));
@@ -129,7 +129,7 @@
    * @return all collations
    */
   public List<Collation> getCollatedResults() {
-  	return collations;
+    return collations;
   }
 
   public static class Suggestion {
@@ -219,62 +219,62 @@
 
   }
 
-	public class Collation {
-		private String collationQueryString;
-		private List<Correction> misspellingsAndCorrections = new ArrayList<Correction>();
-		private long numberOfHits;
+  public class Collation {
+    private String collationQueryString;
+    private List<Correction> misspellingsAndCorrections = new ArrayList<Correction>();
+    private long numberOfHits;
 
-		public long getNumberOfHits() {
-			return numberOfHits;
-		}
+    public long getNumberOfHits() {
+      return numberOfHits;
+    }
 
-		public void setNumberOfHits(long numberOfHits) {
-			this.numberOfHits = numberOfHits;
-		}
+    public void setNumberOfHits(long numberOfHits) {
+      this.numberOfHits = numberOfHits;
+    }
 
-		public String getCollationQueryString() {
-			return collationQueryString;
-		}
+    public String getCollationQueryString() {
+      return collationQueryString;
+    }
 
-		public Collation setCollationQueryString(String collationQueryString) {
-			this.collationQueryString = collationQueryString;
-			return this;
-		}
+    public Collation setCollationQueryString(String collationQueryString) {
+      this.collationQueryString = collationQueryString;
+      return this;
+    }
 
-		public List<Correction> getMisspellingsAndCorrections() {
-			return misspellingsAndCorrections;
-		}
+    public List<Correction> getMisspellingsAndCorrections() {
+      return misspellingsAndCorrections;
+    }
 
-		public Collation addMisspellingsAndCorrection(Correction correction) {
-			this.misspellingsAndCorrections.add(correction);
-			return this;
-		}
+    public Collation addMisspellingsAndCorrection(Correction correction) {
+      this.misspellingsAndCorrections.add(correction);
+      return this;
+    }
 
-	}
+  }
 
-	public class Correction {
-		private String original;
-		private String correction;
+  public class Correction {
+    private String original;
+    private String correction;
 
-		public Correction(String original, String correction) {
-			this.original = original;
-			this.correction = correction;
-		}
+    public Correction(String original, String correction) {
+      this.original = original;
+      this.correction = correction;
+    }
 
-		public String getOriginal() {
-			return original;
-		}
+    public String getOriginal() {
+      return original;
+    }
 
-		public void setOriginal(String original) {
-			this.original = original;
-		}
+    public void setOriginal(String original) {
+      this.original = original;
+    }
 
-		public String getCorrection() {
-			return correction;
-		}
+    public String getCorrection() {
+      return correction;
+    }
 
-		public void setCorrection(String correction) {
-			this.correction = correction;
-		}
-	}
+    public void setCorrection(String correction) {
+      this.correction = correction;
+    }
+  }
 }
Index: solr/example/solr/collection1/conf/velocity/product-doc.vm
===================================================================
--- solr/example/solr/collection1/conf/velocity/product-doc.vm	(revision 1386643)
+++ solr/example/solr/collection1/conf/velocity/product-doc.vm	(working copy)
@@ -6,6 +6,7 @@
 <div>Price: #field('price_c')</div>
 <div>Features: #field('features')</div>
 <div>In Stock: #field('inStock')</div>
+<div>jEOE: #field('cat')</div>
 <div class="mlt">
   #set($mlt = $mltResults.get($docId))
   #set($mltOn = $params.getBool('mlt'))
@@ -24,4 +25,4 @@
     <div>No Similar Items Found</div>
   #end
 </div>
-#parse('debug.vm')
\ No newline at end of file
+#parse('debug.vm')
Index: solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractDIHCacheTestCase.java
===================================================================
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractDIHCacheTestCase.java	(revision 1386643)
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/AbstractDIHCacheTestCase.java	(working copy)
@@ -38,197 +38,197 @@
 import org.junit.Assert;
 import org.junit.Before;
 
-public class AbstractDIHCacheTestCase {	
-	protected static final Date Feb21_2011 = new Date(1298268000000l);
-	protected final String[] fieldTypes = { "INTEGER", "BIGDECIMAL", "STRING", "STRING",   "FLOAT",   "DATE",   "CLOB" };
-	protected final String[] fieldNames = { "a_id",    "PI",         "letter", "examples", "a_float", "a_date", "DESCRIPTION" };
-	protected List<ControlData> data = new ArrayList<ControlData>();
-	protected Clob APPLE = null;
-	
-	@Before
-	public void setup() {
-		try {
-			APPLE = new SerialClob(new String("Apples grow on trees and they are good to eat.").toCharArray());
-		} catch (SQLException sqe) {
-			Assert.fail("Could not Set up Test");
-		}
+public class AbstractDIHCacheTestCase {
+  protected static final Date Feb21_2011 = new Date(1298268000000l);
+  protected final String[] fieldTypes = { "INTEGER", "BIGDECIMAL", "STRING", "STRING",   "FLOAT",   "DATE",   "CLOB" };
+  protected final String[] fieldNames = { "a_id",    "PI",         "letter", "examples", "a_float", "a_date", "DESCRIPTION" };
+  protected List<ControlData> data = new ArrayList<ControlData>();
+  protected Clob APPLE = null;
 
-		// The first row needs to have all non-null fields,
-		// otherwise we would have to always send the fieldTypes & fieldNames as CacheProperties when building.
-		data = new ArrayList<ControlData>();
-		data.add(new ControlData(new Object[] { new Integer(1), new BigDecimal(Math.PI), "A", "Apple", new Float(1.11), Feb21_2011, APPLE }));
-		data.add(new ControlData(new Object[] { new Integer(2), new BigDecimal(Math.PI), "B", "Ball", new Float(2.22), Feb21_2011, null }));
-		data.add(new ControlData(new Object[] { new Integer(4), new BigDecimal(Math.PI), "D", "Dog", new Float(4.44), Feb21_2011, null }));
-		data.add(new ControlData(new Object[] { new Integer(3), new BigDecimal(Math.PI), "C", "Cookie", new Float(3.33), Feb21_2011, null }));
-		data.add(new ControlData(new Object[] { new Integer(4), new BigDecimal(Math.PI), "D", "Daisy", new Float(4.44), Feb21_2011, null }));
-		data.add(new ControlData(new Object[] { new Integer(4), new BigDecimal(Math.PI), "D", "Drawing", new Float(4.44), Feb21_2011, null }));
-		data.add(new ControlData(new Object[] { new Integer(5), new BigDecimal(Math.PI), "E",
-				Arrays.asList("Eggplant", "Ear", "Elephant", "Engine"), new Float(5.55), Feb21_2011, null }));
-	}
+  @Before
+  public void setup() {
+    try {
+      APPLE = new SerialClob(new String("Apples grow on trees and they are good to eat.").toCharArray());
+    } catch (SQLException sqe) {
+      Assert.fail("Could not Set up Test");
+    }
 
-	@After
-	public void teardown() {
-		APPLE = null;
-		data = null;
-	}
-	
-	//A limitation of this test class is that the primary key needs to be the first one in the list.
-	//DIHCaches, however, can handle any field being the primary key.
-	class ControlData implements Comparable<ControlData>, Iterable<Object> {
-		Object[] data;
+    // The first row needs to have all non-null fields,
+    // otherwise we would have to always send the fieldTypes & fieldNames as CacheProperties when building.
+    data = new ArrayList<ControlData>();
+    data.add(new ControlData(new Object[] { new Integer(1), new BigDecimal(Math.PI), "A", "Apple", new Float(1.11), Feb21_2011, APPLE }));
+    data.add(new ControlData(new Object[] { new Integer(2), new BigDecimal(Math.PI), "B", "Ball", new Float(2.22), Feb21_2011, null }));
+    data.add(new ControlData(new Object[] { new Integer(4), new BigDecimal(Math.PI), "D", "Dog", new Float(4.44), Feb21_2011, null }));
+    data.add(new ControlData(new Object[] { new Integer(3), new BigDecimal(Math.PI), "C", "Cookie", new Float(3.33), Feb21_2011, null }));
+    data.add(new ControlData(new Object[] { new Integer(4), new BigDecimal(Math.PI), "D", "Daisy", new Float(4.44), Feb21_2011, null }));
+    data.add(new ControlData(new Object[] { new Integer(4), new BigDecimal(Math.PI), "D", "Drawing", new Float(4.44), Feb21_2011, null }));
+    data.add(new ControlData(new Object[] { new Integer(5), new BigDecimal(Math.PI), "E",
+        Arrays.asList("Eggplant", "Ear", "Elephant", "Engine"), new Float(5.55), Feb21_2011, null }));
+  }
 
-		ControlData(Object[] data) {
-			this.data = data;
-		}
+  @After
+  public void teardown() {
+    APPLE = null;
+    data = null;
+  }
 
-		@SuppressWarnings("unchecked")
-		public int compareTo(ControlData cd) {
-			Comparable c1 = (Comparable) data[0];
-			Comparable c2 = (Comparable) cd.data[0];
-			return c1.compareTo(c2);
-		}
+  //A limitation of this test class is that the primary key needs to be the first one in the list.
+  //DIHCaches, however, can handle any field being the primary key.
+  class ControlData implements Comparable<ControlData>, Iterable<Object> {
+    Object[] data;
 
-		public Iterator<Object> iterator() {
-			return Arrays.asList(data).iterator();
-		}
-	}
-	
-	protected void loadData(DIHCache cache, List<ControlData> theData, String[] theFieldNames, boolean keepOrdered) {
-		for (ControlData cd : theData) {
-			cache.add(controlDataToMap(cd, theFieldNames, keepOrdered));
-		}
-	}
+    ControlData(Object[] data) {
+      this.data = data;
+    }
 
-	protected List<ControlData> extractDataInKeyOrder(DIHCache cache, String[] theFieldNames) {
-		List<Object[]> data = new ArrayList<Object[]>();
-		Iterator<Map<String, Object>> cacheIter = cache.iterator();
-		while (cacheIter.hasNext()) {
-			data.add(mapToObjectArray(cacheIter.next(), theFieldNames));
-		}
-		return listToControlData(data);
-	}
+    @SuppressWarnings("unchecked")
+    public int compareTo(ControlData cd) {
+      Comparable c1 = (Comparable) data[0];
+      Comparable c2 = (Comparable) cd.data[0];
+      return c1.compareTo(c2);
+    }
 
-	//This method assumes that the Primary Keys are integers and that the first id=1.  
-	//It will look for id's sequentially until one is skipped, then will stop.
-	protected List<ControlData> extractDataByKeyLookup(DIHCache cache, String[] theFieldNames) {
-		int recId = 1;
-		List<Object[]> data = new ArrayList<Object[]>();
-		while (true) {
-			Iterator<Map<String, Object>> listORecs = cache.iterator(recId);
-			if (listORecs == null) {
-				break;
-			}
+    public Iterator<Object> iterator() {
+      return Arrays.asList(data).iterator();
+    }
+  }
 
-			while(listORecs.hasNext()) {
-				data.add(mapToObjectArray(listORecs.next(), theFieldNames));
-			}
-			recId++;
-		}
-		return listToControlData(data);
-	}
+  protected void loadData(DIHCache cache, List<ControlData> theData, String[] theFieldNames, boolean keepOrdered) {
+    for (ControlData cd : theData) {
+      cache.add(controlDataToMap(cd, theFieldNames, keepOrdered));
+    }
+  }
 
-	protected List<ControlData> listToControlData(List<Object[]> data) {
-		List<ControlData> returnData = new ArrayList<ControlData>(data.size());
-		for (int i = 0; i < data.size(); i++) {
-			returnData.add(new ControlData(data.get(i)));
-		}
-		return returnData;
-	}
+  protected List<ControlData> extractDataInKeyOrder(DIHCache cache, String[] theFieldNames) {
+    List<Object[]> data = new ArrayList<Object[]>();
+    Iterator<Map<String, Object>> cacheIter = cache.iterator();
+    while (cacheIter.hasNext()) {
+      data.add(mapToObjectArray(cacheIter.next(), theFieldNames));
+    }
+    return listToControlData(data);
+  }
 
-	protected Object[] mapToObjectArray(Map<String, Object> rec, String[] theFieldNames) {
-		Object[] oos = new Object[theFieldNames.length];
-		for (int i = 0; i < theFieldNames.length; i++) {
-			oos[i] = rec.get(theFieldNames[i]);
-		}
-		return oos;
-	}
+  //This method assumes that the Primary Keys are integers and that the first id=1.
+  //It will look for id's sequentially until one is skipped, then will stop.
+  protected List<ControlData> extractDataByKeyLookup(DIHCache cache, String[] theFieldNames) {
+    int recId = 1;
+    List<Object[]> data = new ArrayList<Object[]>();
+    while (true) {
+      Iterator<Map<String, Object>> listORecs = cache.iterator(recId);
+      if (listORecs == null) {
+        break;
+      }
 
-	protected void compareData(List<ControlData> theControl, List<ControlData> test) {
-		// The test data should come back primarily in Key order and secondarily in insertion order.
-		List<ControlData> control = new ArrayList<ControlData>(theControl);
-		Collections.sort(control);
+      while(listORecs.hasNext()) {
+        data.add(mapToObjectArray(listORecs.next(), theFieldNames));
+      }
+      recId++;
+    }
+    return listToControlData(data);
+  }
 
-		StringBuilder errors = new StringBuilder();
-		if (test.size() != control.size()) {
-			errors.append("-Returned data has " + test.size() + " records.  expected: " + control.size() + "\n");
-		}
-		for (int i = 0; i < control.size() && i < test.size(); i++) {
-			Object[] controlRec = control.get(i).data;
-			Object[] testRec = test.get(i).data;
-			if (testRec.length != controlRec.length) {
-				errors.append("-Record indexAt=" + i + " has " + testRec.length + " data elements.  extpected: " + controlRec.length + "\n");
-			}
-			for (int j = 0; j < controlRec.length && j < testRec.length; j++) {
-				Object controlObj = controlRec[j];
-				Object testObj = testRec[j];
-				if (controlObj == null && testObj != null) {
-					errors.append("-Record indexAt=" + i + ", Data Element indexAt=" + j + " is not NULL as expected.\n");
-				} else if (controlObj != null && testObj == null) {
-					errors.append("-Record indexAt=" + i + ", Data Element indexAt=" + j + " is NULL.  Expected: " + controlObj + " (class="
-							+ controlObj.getClass().getName() + ")\n");
-				} else if (controlObj != null && testObj != null && controlObj instanceof Clob) {
-					String controlString = clobToString((Clob) controlObj);
-					String testString = clobToString((Clob) testObj);
-					if (!controlString.equals(testString)) {
-						errors.append("-Record indexAt=" + i + ", Data Element indexAt=" + j + " has: " + testString + " (class=Clob) ... Expected: " + controlString
-								+ " (class=Clob)\n");
-					}
-				} else if (controlObj != null && !controlObj.equals(testObj)) {
-					errors.append("-Record indexAt=" + i + ", Data Element indexAt=" + j + " has: " + testObj + " (class=" + testObj.getClass().getName()
-							+ ") ... Expected: " + controlObj + " (class=" + controlObj.getClass().getName() + ")\n");
-				}
-			}
-		}
-		if (errors.length() > 0) {
-			Assert.fail(errors.toString());
-		}
-	}
+  protected List<ControlData> listToControlData(List<Object[]> data) {
+    List<ControlData> returnData = new ArrayList<ControlData>(data.size());
+    for (int i = 0; i < data.size(); i++) {
+      returnData.add(new ControlData(data.get(i)));
+    }
+    return returnData;
+  }
 
-	protected Map<String, Object> controlDataToMap(ControlData cd, String[] theFieldNames, boolean keepOrdered) {
-		Map<String, Object> rec = null;
-		if (keepOrdered) {
-			rec = new LinkedHashMap<String, Object>();
-		} else {
-			rec = new HashMap<String, Object>();
-		}
-		for (int i = 0; i < cd.data.length; i++) {
-			String fieldName = theFieldNames[i];
-			Object data = cd.data[i];
-			rec.put(fieldName, data);
-		}
-		return rec;
-	}
+  protected Object[] mapToObjectArray(Map<String, Object> rec, String[] theFieldNames) {
+    Object[] oos = new Object[theFieldNames.length];
+    for (int i = 0; i < theFieldNames.length; i++) {
+      oos[i] = rec.get(theFieldNames[i]);
+    }
+    return oos;
+  }
 
-	protected String stringArrayToCommaDelimitedList(String[] strs) {
-		StringBuilder sb = new StringBuilder();
-		for (String a : strs) {
-			if (sb.length() > 0) {
-				sb.append(",");
-			}
-			sb.append(a);
-		}
-		return sb.toString();
-	}
+  protected void compareData(List<ControlData> theControl, List<ControlData> test) {
+    // The test data should come back primarily in Key order and secondarily in insertion order.
+    List<ControlData> control = new ArrayList<ControlData>(theControl);
+    Collections.sort(control);
 
-	protected String clobToString(Clob cl) {
-		StringBuilder sb = new StringBuilder();
-		try {
-			Reader in = cl.getCharacterStream();
-			char[] cbuf = new char[1024];
-			int numGot = -1;
-			while ((numGot = in.read(cbuf)) != -1) {
-				sb.append(String.valueOf(cbuf, 0, numGot));
-			}
-		} catch (Exception e) {
-			Assert.fail(e.toString());
-		}
-		return sb.toString();
-	}
-	
-	public static Context getContext(final Map<String, String> entityAttrs) {
-		VariableResolverImpl resolver = new VariableResolverImpl();
+    StringBuilder errors = new StringBuilder();
+    if (test.size() != control.size()) {
+      errors.append("-Returned data has " + test.size() + " records.  expected: " + control.size() + "\n");
+    }
+    for (int i = 0; i < control.size() && i < test.size(); i++) {
+      Object[] controlRec = control.get(i).data;
+      Object[] testRec = test.get(i).data;
+      if (testRec.length != controlRec.length) {
+        errors.append("-Record indexAt=" + i + " has " + testRec.length + " data elements.  extpected: " + controlRec.length + "\n");
+      }
+      for (int j = 0; j < controlRec.length && j < testRec.length; j++) {
+        Object controlObj = controlRec[j];
+        Object testObj = testRec[j];
+        if (controlObj == null && testObj != null) {
+          errors.append("-Record indexAt=" + i + ", Data Element indexAt=" + j + " is not NULL as expected.\n");
+        } else if (controlObj != null && testObj == null) {
+          errors.append("-Record indexAt=" + i + ", Data Element indexAt=" + j + " is NULL.  Expected: " + controlObj + " (class="
+              + controlObj.getClass().getName() + ")\n");
+        } else if (controlObj != null && testObj != null && controlObj instanceof Clob) {
+          String controlString = clobToString((Clob) controlObj);
+          String testString = clobToString((Clob) testObj);
+          if (!controlString.equals(testString)) {
+            errors.append("-Record indexAt=" + i + ", Data Element indexAt=" + j + " has: " + testString + " (class=Clob) ... Expected: " + controlString
+                + " (class=Clob)\n");
+          }
+        } else if (controlObj != null && !controlObj.equals(testObj)) {
+          errors.append("-Record indexAt=" + i + ", Data Element indexAt=" + j + " has: " + testObj + " (class=" + testObj.getClass().getName()
+              + ") ... Expected: " + controlObj + " (class=" + controlObj.getClass().getName() + ")\n");
+        }
+      }
+    }
+    if (errors.length() > 0) {
+      Assert.fail(errors.toString());
+    }
+  }
+
+  protected Map<String, Object> controlDataToMap(ControlData cd, String[] theFieldNames, boolean keepOrdered) {
+    Map<String, Object> rec = null;
+    if (keepOrdered) {
+      rec = new LinkedHashMap<String, Object>();
+    } else {
+      rec = new HashMap<String, Object>();
+    }
+    for (int i = 0; i < cd.data.length; i++) {
+      String fieldName = theFieldNames[i];
+      Object data = cd.data[i];
+      rec.put(fieldName, data);
+    }
+    return rec;
+  }
+
+  protected String stringArrayToCommaDelimitedList(String[] strs) {
+    StringBuilder sb = new StringBuilder();
+    for (String a : strs) {
+      if (sb.length() > 0) {
+        sb.append(",");
+      }
+      sb.append(a);
+    }
+    return sb.toString();
+  }
+
+  protected String clobToString(Clob cl) {
+    StringBuilder sb = new StringBuilder();
+    try {
+      Reader in = cl.getCharacterStream();
+      char[] cbuf = new char[1024];
+      int numGot = -1;
+      while ((numGot = in.read(cbuf)) != -1) {
+        sb.append(String.valueOf(cbuf, 0, numGot));
+      }
+    } catch (Exception e) {
+      Assert.fail(e.toString());
+    }
+    return sb.toString();
+  }
+
+  public static Context getContext(final Map<String, String> entityAttrs) {
+    VariableResolverImpl resolver = new VariableResolverImpl();
     final Context delegate = new ContextImpl(null, resolver, null, null, new HashMap<String, Object>(), null, null);
     return new TestContext(entityAttrs, delegate, null, true);
   }
-	
+
 }
Index: solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSortedMapBackedCache.java
===================================================================
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSortedMapBackedCache.java	(revision 1386643)
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestSortedMapBackedCache.java	(working copy)
@@ -30,56 +30,56 @@
 import org.slf4j.LoggerFactory;
 
 public class TestSortedMapBackedCache extends AbstractDIHCacheTestCase {
-	
-	public static Logger log = LoggerFactory.getLogger(TestSortedMapBackedCache.class);
-	
-	@Test
-	public void testCacheWithKeyLookup() {
-		DIHCache cache = null;
-		try {
-			cache = new SortedMapBackedCache();
-			cache.open(getContext(new HashMap<String,String>()));
-			loadData(cache, data, fieldNames, true);
-			List<ControlData> testData = extractDataByKeyLookup(cache, fieldNames);
-			compareData(data, testData);
-		} catch (Exception e) {
-			log.warn("Exception thrown: " + e.toString());
-			Assert.fail();
-		} finally {
-			try {
-				cache.destroy();
-			} catch (Exception ex) {
-			}
-		}
-	}
+  
+  public static Logger log = LoggerFactory.getLogger(TestSortedMapBackedCache.class);
+  
+  @Test
+  public void testCacheWithKeyLookup() {
+    DIHCache cache = null;
+    try {
+      cache = new SortedMapBackedCache();
+      cache.open(getContext(new HashMap<String,String>()));
+      loadData(cache, data, fieldNames, true);
+      List<ControlData> testData = extractDataByKeyLookup(cache, fieldNames);
+      compareData(data, testData);
+    } catch (Exception e) {
+      log.warn("Exception thrown: " + e.toString());
+      Assert.fail();
+    } finally {
+      try {
+        cache.destroy();
+      } catch (Exception ex) {
+      }
+    }
+  }
 
-	@Test
-	public void testCacheWithOrderedLookup() {
-		DIHCache cache = null;
-		try {
-			cache = new SortedMapBackedCache();
-			cache.open(getContext(new HashMap<String,String>()));
-			loadData(cache, data, fieldNames, true);
-			List<ControlData> testData = extractDataInKeyOrder(cache, fieldNames);
-			compareData(data, testData);
-		} catch (Exception e) {
-			log.warn("Exception thrown: " + e.toString());
-			Assert.fail();
-		} finally {
-			try {
-				cache.destroy();
-			} catch (Exception ex) {
-			}
-		}
-	}
-	
-	@Test
-	public void testNullKeys() throws Exception {
-	  //A null key should just be ignored, but not throw an exception
-	  DIHCache cache = null;
-	  try {
-	    cache = new SortedMapBackedCache();
-	    Map<String, String> cacheProps = new HashMap<String, String>();
+  @Test
+  public void testCacheWithOrderedLookup() {
+    DIHCache cache = null;
+    try {
+      cache = new SortedMapBackedCache();
+      cache.open(getContext(new HashMap<String,String>()));
+      loadData(cache, data, fieldNames, true);
+      List<ControlData> testData = extractDataInKeyOrder(cache, fieldNames);
+      compareData(data, testData);
+    } catch (Exception e) {
+      log.warn("Exception thrown: " + e.toString());
+      Assert.fail();
+    } finally {
+      try {
+        cache.destroy();
+      } catch (Exception ex) {
+      }
+    }
+  }
+  
+  @Test
+  public void testNullKeys() throws Exception {
+    //A null key should just be ignored, but not throw an exception
+    DIHCache cache = null;
+    try {
+      cache = new SortedMapBackedCache();
+      Map<String, String> cacheProps = new HashMap<String, String>();
       cacheProps.put(DIHCacheSupport.CACHE_PRIMARY_KEY, "a_id");
       cache.open(getContext(cacheProps));
       
@@ -94,98 +94,98 @@
       }
       Assert.assertNull(cache.iterator(null));
       cache.delete(null);      
-	  } catch (Exception e) {
-	    throw e;
+    } catch (Exception e) {
+      throw e;
     } finally {
       try {
         cache.destroy();
       } catch (Exception ex) {
       }
-    }	  
-	}
+    }    
+  }
 
-	@Test
-	public void testCacheReopensWithUpdate() {
-		DIHCache cache = null;
-		try {			
-			Map<String, String> cacheProps = new HashMap<String, String>();
-			cacheProps.put(DIHCacheSupport.CACHE_PRIMARY_KEY, "a_id");
-			
-			cache = new SortedMapBackedCache();
-			cache.open(getContext(cacheProps));
-			// We can let the data hit the cache with the fields out of order because
-			// we've identified the pk up-front.
-			loadData(cache, data, fieldNames, false);
+  @Test
+  public void testCacheReopensWithUpdate() {
+    DIHCache cache = null;
+    try {      
+      Map<String, String> cacheProps = new HashMap<String, String>();
+      cacheProps.put(DIHCacheSupport.CACHE_PRIMARY_KEY, "a_id");
+      
+      cache = new SortedMapBackedCache();
+      cache.open(getContext(cacheProps));
+      // We can let the data hit the cache with the fields out of order because
+      // we've identified the pk up-front.
+      loadData(cache, data, fieldNames, false);
 
-			// Close the cache.
-			cache.close();
+      // Close the cache.
+      cache.close();
 
-			List<ControlData> newControlData = new ArrayList<ControlData>();
-			Object[] newIdEqualsThree = null;
-			int j = 0;
-			for (int i = 0; i < data.size(); i++) {
-				// We'll be deleting a_id=1 so remove it from the control data.
-				if (data.get(i).data[0].equals(new Integer(1))) {
-					continue;
-				}
+      List<ControlData> newControlData = new ArrayList<ControlData>();
+      Object[] newIdEqualsThree = null;
+      int j = 0;
+      for (int i = 0; i < data.size(); i++) {
+        // We'll be deleting a_id=1 so remove it from the control data.
+        if (data.get(i).data[0].equals(new Integer(1))) {
+          continue;
+        }
 
-				// We'll be changing "Cookie" to "Carrot" in a_id=3 so change it in the control data.
-				if (data.get(i).data[0].equals(new Integer(3))) {
-					newIdEqualsThree = new Object[data.get(i).data.length];
-					System.arraycopy(data.get(i).data, 0, newIdEqualsThree, 0, newIdEqualsThree.length);
-					newIdEqualsThree[3] = "Carrot";
-					newControlData.add(new ControlData(newIdEqualsThree));
-				}
-				// Everything else can just be copied over.
-				else {
-					newControlData.add(data.get(i));
-				}
+        // We'll be changing "Cookie" to "Carrot" in a_id=3 so change it in the control data.
+        if (data.get(i).data[0].equals(new Integer(3))) {
+          newIdEqualsThree = new Object[data.get(i).data.length];
+          System.arraycopy(data.get(i).data, 0, newIdEqualsThree, 0, newIdEqualsThree.length);
+          newIdEqualsThree[3] = "Carrot";
+          newControlData.add(new ControlData(newIdEqualsThree));
+        }
+        // Everything else can just be copied over.
+        else {
+          newControlData.add(data.get(i));
+        }
 
-				j++;
-			}
+        j++;
+      }
 
-			// These new rows of data will get added to the cache, so add them to the control data too.
-			Object[] newDataRow1 = new Object[] { new Integer(99), new BigDecimal(Math.PI), "Z", "Zebra", new Float(99.99), Feb21_2011, null };
-			Object[] newDataRow2 = new Object[] { new Integer(2), new BigDecimal(Math.PI), "B", "Ballerina", new Float(2.22), Feb21_2011, null };
+      // These new rows of data will get added to the cache, so add them to the control data too.
+      Object[] newDataRow1 = new Object[] { new Integer(99), new BigDecimal(Math.PI), "Z", "Zebra", new Float(99.99), Feb21_2011, null };
+      Object[] newDataRow2 = new Object[] { new Integer(2), new BigDecimal(Math.PI), "B", "Ballerina", new Float(2.22), Feb21_2011, null };
 
-			newControlData.add(new ControlData(newDataRow1));
-			newControlData.add(new ControlData(newDataRow2));
+      newControlData.add(new ControlData(newDataRow1));
+      newControlData.add(new ControlData(newDataRow2));
 
-			// Re-open the cache
-			cache.open(getContext(new HashMap<String,String>()));
+      // Re-open the cache
+      cache.open(getContext(new HashMap<String,String>()));
 
-			// Delete a_id=1 from the cache.
-			cache.delete(new Integer(1));
+      // Delete a_id=1 from the cache.
+      cache.delete(new Integer(1));
 
-			// Because the cache allows duplicates, the only way to update is to
-			// delete first then add.
-			cache.delete(new Integer(3));
-			cache.add(controlDataToMap(new ControlData(newIdEqualsThree), fieldNames, false));
+      // Because the cache allows duplicates, the only way to update is to
+      // delete first then add.
+      cache.delete(new Integer(3));
+      cache.add(controlDataToMap(new ControlData(newIdEqualsThree), fieldNames, false));
 
-			// Add this row with a new Primary key.
-			cache.add(controlDataToMap(new ControlData(newDataRow1), fieldNames, false));
+      // Add this row with a new Primary key.
+      cache.add(controlDataToMap(new ControlData(newDataRow1), fieldNames, false));
 
-			// Add this row, creating two records in the cache with a_id=2.
-			cache.add(controlDataToMap(new ControlData(newDataRow2), fieldNames, false));
+      // Add this row, creating two records in the cache with a_id=2.
+      cache.add(controlDataToMap(new ControlData(newDataRow2), fieldNames, false));
 
-			// Read the cache back and compare to the newControlData
-			List<ControlData> testData = extractDataInKeyOrder(cache, fieldNames);
-			compareData(newControlData, testData);
+      // Read the cache back and compare to the newControlData
+      List<ControlData> testData = extractDataInKeyOrder(cache, fieldNames);
+      compareData(newControlData, testData);
 
-			// Now try reading the cache read-only.
-			cache.close();
-			cache.open(getContext(new HashMap<String,String>()));
-			testData = extractDataInKeyOrder(cache, fieldNames);
-			compareData(newControlData, testData);
+      // Now try reading the cache read-only.
+      cache.close();
+      cache.open(getContext(new HashMap<String,String>()));
+      testData = extractDataInKeyOrder(cache, fieldNames);
+      compareData(newControlData, testData);
 
-		} catch (Exception e) {
-			log.warn("Exception thrown: " + e.toString());
-			Assert.fail();
-		} finally {
-			try {
-				cache.destroy();
-			} catch (Exception ex) {
-			}
-		}
-	}
+    } catch (Exception e) {
+      log.warn("Exception thrown: " + e.toString());
+      Assert.fail();
+    } finally {
+      try {
+        cache.destroy();
+      } catch (Exception ex) {
+      }
+    }
+  }
 }
Index: solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/EntityProcessorBase.java
===================================================================
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/EntityProcessorBase.java	(revision 1386643)
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/EntityProcessorBase.java	(working copy)
@@ -133,11 +133,11 @@
 
   @Override
   public void destroy() {
-  	query = null;
-  	if(cacheSupport!=null){
-  	  cacheSupport.destroyAll();
-  	}
-  	cacheSupport = null;
+    query = null;
+    if(cacheSupport!=null){
+      cacheSupport.destroyAll();
+    }
+    cacheSupport = null;
   }
 
   
Index: solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SolrWriter.java
===================================================================
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SolrWriter.java	(revision 1386643)
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SolrWriter.java	(working copy)
@@ -54,12 +54,12 @@
   
   @Override
   public void close() {
-  	try {
-  		processor.finish();
-  	} catch (IOException e) {
-  		throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
-  				"Unable to call finish() on UpdateRequestProcessor", e);
-  	}
+    try {
+      processor.finish();
+    } catch (IOException e) {
+      throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
+          "Unable to call finish() on UpdateRequestProcessor", e);
+    }
   }
   @Override
   public boolean upload(SolrInputDocument d) {
@@ -87,8 +87,8 @@
       log.error("Exception while deleteing: " + id, e);
     }
   }
-  	
-	@Override
+
+  @Override
   public void deleteByQuery(String query) {
     try {
       log.info("Deleting documents from Solr with query: " + query);
@@ -100,7 +100,7 @@
     }
   }
 
-	@Override
+  @Override
   public void commit(boolean optimize) {
     try {
       CommitUpdateCommand commit = new CommitUpdateCommand(req,optimize);
@@ -110,7 +110,7 @@
     }
   }
 
-	@Override
+  @Override
   public void rollback() {
     try {
       RollbackUpdateCommand rollback = new RollbackUpdateCommand(req);
@@ -120,7 +120,7 @@
     }
   }
 
-	@Override
+  @Override
   public void doDeleteAll() {
     try {
       DeleteUpdateCommand deleteCommand = new DeleteUpdateCommand(req);
@@ -158,8 +158,8 @@
       return null;
     }
   }
-	@Override
-	public void init(Context context) {
-		/* NO-OP */		
-	}	
+  @Override
+  public void init(Context context) {
+    /* NO-OP */
+  }
 }
Index: solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHLogLevels.java
===================================================================
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHLogLevels.java	(revision 1386643)
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHLogLevels.java	(working copy)
@@ -18,5 +18,5 @@
  */
 
 public enum DIHLogLevels {
-	START_ENTITY, END_ENTITY, TRANSFORMED_ROW, ENTITY_META, PRE_TRANSFORMER_ROW, START_DOC, END_DOC, ENTITY_OUT, ROW_END, TRANSFORMER_EXCEPTION, ENTITY_EXCEPTION, DISABLE_LOGGING, ENABLE_LOGGING, NONE
+  START_ENTITY, END_ENTITY, TRANSFORMED_ROW, ENTITY_META, PRE_TRANSFORMER_ROW, START_DOC, END_DOC, ENTITY_OUT, ROW_END, TRANSFORMER_EXCEPTION, ENTITY_EXCEPTION, DISABLE_LOGGING, ENABLE_LOGGING, NONE
 }
Index: solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImporter.java
===================================================================
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImporter.java	(revision 1386643)
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImporter.java	(working copy)
@@ -393,7 +393,7 @@
   }
 
   private void checkWritablePersistFile(SolrWriter writer) {
-//  	File persistFile = propWriter.getPersistFile();
+//    File persistFile = propWriter.getPersistFile();
 //    boolean isWritable = persistFile.exists() ? persistFile.canWrite() : persistFile.getParentFile().canWrite();
     if (isDeltaImportSupported && !propWriter.isWritable()) {
       throw new DataImportHandlerException(SEVERE,
Index: solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHWriter.java
===================================================================
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHWriter.java	(revision 1386643)
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DIHWriter.java	(working copy)
@@ -25,81 +25,81 @@
  *
  */
 public interface DIHWriter {
-	
-	/**
-	 * <p>
-	 *  If this writer supports transactions or commit points, then commit any changes, 
-	 *  optionally optimizing the data for read/write performance
-	 * </p>
-	 * @param optimize
-	 */
-	public void commit(boolean optimize);
-	
-	/**
-	 * <p>
-	 *  Release resources used by this writer.  After calling close, reads & updates will throw exceptions.
-	 * </p>
-	 */
-	public void close();
 
-	/**
-	 * <p>
-	 *  If this writer supports transactions or commit points, then roll back any uncommitted changes.
-	 * </p>
-	 */
-	public void rollback();
+  /**
+   * <p>
+   *  If this writer supports transactions or commit points, then commit any changes,
+   *  optionally optimizing the data for read/write performance
+   * </p>
+   * @param optimize
+   */
+  public void commit(boolean optimize);
 
-	/**
-	 * <p>
-	 *  Delete from the writer's underlying data store based the passed-in writer-specific query. (Optional Operation)
-	 * </p>
-	 * @param q
-	 */
-	public void deleteByQuery(String q);
+  /**
+   * <p>
+   *  Release resources used by this writer.  After calling close, reads & updates will throw exceptions.
+   * </p>
+   */
+  public void close();
 
-	/**
-	 * <p>
-	 *  Delete everything from the writer's underlying data store
-	 * </p>
-	 */
-	public void doDeleteAll();
+  /**
+   * <p>
+   *  If this writer supports transactions or commit points, then roll back any uncommitted changes.
+   * </p>
+   */
+  public void rollback();
 
-	/**
-	 * <p>
-	 *  Delete from the writer's underlying data store based on the passed-in Primary Key
-	 * </p>
-	 * @param key
-	 */
-	public void deleteDoc(Object key);
-	
+  /**
+   * <p>
+   *  Delete from the writer's underlying data store based the passed-in writer-specific query. (Optional Operation)
+   * </p>
+   * @param q
+   */
+  public void deleteByQuery(String q);
 
+  /**
+   * <p>
+   *  Delete everything from the writer's underlying data store
+   * </p>
+   */
+  public void doDeleteAll();
 
-	/**
-	 * <p>
-	 *  Add a document to this writer's underlying data store.
-	 * </p>
-	 * @param doc
-	 * @return true on success, false on failure
-	 */
-	public boolean upload(SolrInputDocument doc);
+  /**
+   * <p>
+   *  Delete from the writer's underlying data store based on the passed-in Primary Key
+   * </p>
+   * @param key
+   */
+  public void deleteDoc(Object key);
 
 
-	
-	/**
-	 * <p>
-	 *  Provide context information for this writer.  init() should be called before using the writer.
-	 * </p>
-	 * @param context
-	 */
-	public void init(Context context) ;
 
-	
-	/**
-	 * <p>
-	 *  Specify the keys to be modified by a delta update (required by writers that can store duplicate keys)
-	 * </p>
-	 * @param deltaKeys
-	 */
-	public void setDeltaKeys(Set<Map<String, Object>> deltaKeys) ;
+  /**
+   * <p>
+   *  Add a document to this writer's underlying data store.
+   * </p>
+   * @param doc
+   * @return true on success, false on failure
+   */
+  public boolean upload(SolrInputDocument doc);
 
+
+
+  /**
+   * <p>
+   *  Provide context information for this writer.  init() should be called before using the writer.
+   * </p>
+   * @param context
+   */
+  public void init(Context context) ;
+
+
+  /**
+   * <p>
+   *  Specify the keys to be modified by a delta update (required by writers that can store duplicate keys)
+   * </p>
+   * @param deltaKeys
+   */
+  public void setDeltaKeys(Set<Map<String, Object>> deltaKeys) ;
+
 }
Index: solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java
===================================================================
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java	(revision 1386643)
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java	(working copy)
@@ -178,7 +178,7 @@
             rsp.add("mode", "debug");
             rsp.add("documents", requestParams.getDebugInfo().debugDocuments);
             if (requestParams.getDebugInfo().debugVerboseOutput != null) {
-            	rsp.add("verbose-output", requestParams.getDebugInfo().debugVerboseOutput);
+              rsp.add("verbose-output", requestParams.getDebugInfo().debugVerboseOutput);
             }
           } else {
             message = DataImporter.MSG.DEBUG_NOT_ENABLED;
Index: solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SimplePropertiesWriter.java
===================================================================
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SimplePropertiesWriter.java	(revision 1386643)
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/SimplePropertiesWriter.java	(working copy)
@@ -30,15 +30,15 @@
 import org.slf4j.LoggerFactory;
 
 public class SimplePropertiesWriter implements DIHPropertiesWriter {
-	private static final Logger log = LoggerFactory.getLogger(SimplePropertiesWriter.class);
+  private static final Logger log = LoggerFactory.getLogger(SimplePropertiesWriter.class);
 
-	static final String IMPORTER_PROPERTIES = "dataimport.properties";
+  static final String IMPORTER_PROPERTIES = "dataimport.properties";
 
-	static final String LAST_INDEX_KEY = "last_index_time";
+  static final String LAST_INDEX_KEY = "last_index_time";
 
-	private String persistFilename = IMPORTER_PROPERTIES;
+  private String persistFilename = IMPORTER_PROPERTIES;
 
-	private String configDir = null;
+  private String configDir = null;
 
 
 
@@ -48,15 +48,15 @@
       String persistFileName = dataImporter.getHandlerName();
 
       this.configDir = configDir;
-	  if(persistFileName != null){
+    if(persistFileName != null){
         persistFilename = persistFileName + ".properties";
       }
     }
 
 
 
-	
-	private File getPersistFile() {
+
+  private File getPersistFile() {
     String filePath = configDir;
     if (configDir != null && !configDir.endsWith(File.separator))
       filePath += File.separator;
@@ -71,53 +71,53 @@
     }
 
     @Override
-	public void persist(Properties p) {
-		OutputStream propOutput = null;
+  public void persist(Properties p) {
+    OutputStream propOutput = null;
 
-		Properties props = readIndexerProperties();
+    Properties props = readIndexerProperties();
 
-		try {
-			props.putAll(p);
-			String filePath = configDir;
-			if (configDir != null && !configDir.endsWith(File.separator))
-				filePath += File.separator;
-			filePath += persistFilename;
-			propOutput = new FileOutputStream(filePath);
-			props.store(propOutput, null);
-			log.info("Wrote last indexed time to " + persistFilename);
-		} catch (Exception e) {
-			throw new DataImportHandlerException(DataImportHandlerException.SEVERE, "Unable to persist Index Start Time", e);
-		} finally {
-			try {
-				if (propOutput != null)
-					propOutput.close();
-			} catch (IOException e) {
-				propOutput = null;
-			}
-		}
-	}
+    try {
+      props.putAll(p);
+      String filePath = configDir;
+      if (configDir != null && !configDir.endsWith(File.separator))
+        filePath += File.separator;
+      filePath += persistFilename;
+      propOutput = new FileOutputStream(filePath);
+      props.store(propOutput, null);
+      log.info("Wrote last indexed time to " + persistFilename);
+    } catch (Exception e) {
+      throw new DataImportHandlerException(DataImportHandlerException.SEVERE, "Unable to persist Index Start Time", e);
+    } finally {
+      try {
+        if (propOutput != null)
+          propOutput.close();
+      } catch (IOException e) {
+        propOutput = null;
+      }
+    }
+  }
 
-	@Override
-	public Properties readIndexerProperties() {
-		Properties props = new Properties();
-		InputStream propInput = null;
+  @Override
+  public Properties readIndexerProperties() {
+    Properties props = new Properties();
+    InputStream propInput = null;
 
-		try {
-			propInput = new FileInputStream(configDir + persistFilename);
-			props.load(propInput);
-			log.info("Read " + persistFilename);
-		} catch (Exception e) {
-			log.warn("Unable to read: " + persistFilename);
-		} finally {
-			try {
-				if (propInput != null)
-					propInput.close();
-			} catch (IOException e) {
-				propInput = null;
-			}
-		}
+    try {
+      propInput = new FileInputStream(configDir + persistFilename);
+      props.load(propInput);
+      log.info("Read " + persistFilename);
+    } catch (Exception e) {
+      log.warn("Unable to read: " + persistFilename);
+    } finally {
+      try {
+        if (propInput != null)
+          propInput.close();
+      } catch (IOException e) {
+        propInput = null;
+      }
+    }
 
-		return props;
-	}
+    return props;
+  }
 
 }
Index: solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DocBuilder.java
===================================================================
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DocBuilder.java	(revision 1386643)
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DocBuilder.java	(working copy)
@@ -89,17 +89,17 @@
     
     String writerClassStr = null;
     if(reqParams!=null && reqParams.getRawParams() != null) {
-    	writerClassStr = (String) reqParams.getRawParams().get(PARAM_WRITER_IMPL);
+      writerClassStr = (String) reqParams.getRawParams().get(PARAM_WRITER_IMPL);
     }
     if(writerClassStr != null && !writerClassStr.equals(DEFAULT_WRITER_NAME) && !writerClassStr.equals(DocBuilder.class.getPackage().getName() + "." + DEFAULT_WRITER_NAME)) {
-    	try {
-    		Class<DIHWriter> writerClass = loadClass(writerClassStr, dataImporter.getCore());
-    		this.writer = writerClass.newInstance();
-    	} catch (Exception e) {
-    		throw new DataImportHandlerException(DataImportHandlerException.SEVERE, "Unable to load Writer implementation:" + writerClassStr, e);
-    	}
-   	} else {
-    	writer = solrWriter;
+      try {
+        Class<DIHWriter> writerClass = loadClass(writerClassStr, dataImporter.getCore());
+        this.writer = writerClass.newInstance();
+      } catch (Exception e) {
+        throw new DataImportHandlerException(DataImportHandlerException.SEVERE, "Unable to load Writer implementation:" + writerClassStr, e);
+      }
+     } else {
+      writer = solrWriter;
     }
     ContextImpl ctx = new ContextImpl(null, null, null, null, reqParams.getRawParams(), null, this);
     writer.init(ctx);
@@ -178,111 +178,111 @@
   @SuppressWarnings("unchecked")
   public void execute() {
     List<EntityProcessorWrapper> epwList = null;
-  	try {
-	    dataImporter.store(DataImporter.STATUS_MSGS, statusMessages);
-	    config = dataImporter.getConfig();
-	    final AtomicLong startTime = new AtomicLong(System.currentTimeMillis());
-	    statusMessages.put(TIME_ELAPSED, new Object() {
-	      @Override
-	      public String toString() {
-	        return getTimeElapsedSince(startTime.get());
-	      }
-	    });
-	
-	    statusMessages.put(DataImporter.MSG.TOTAL_QUERIES_EXECUTED,
-	            importStatistics.queryCount);
-	    statusMessages.put(DataImporter.MSG.TOTAL_ROWS_EXECUTED,
-	            importStatistics.rowsCount);
-	    statusMessages.put(DataImporter.MSG.TOTAL_DOC_PROCESSED,
-	            importStatistics.docCount);
-	    statusMessages.put(DataImporter.MSG.TOTAL_DOCS_SKIPPED,
-	            importStatistics.skipDocCount);
-	
-	    List<String> entities = reqParams.getEntitiesToRun();
-	
-	    // Trigger onImportStart
-	    if (config.getOnImportStart() != null) {
-	      invokeEventListener(config.getOnImportStart());
-	    }
-	    AtomicBoolean fullCleanDone = new AtomicBoolean(false);
-	    //we must not do a delete of *:* multiple times if there are multiple root entities to be run
-	    Properties lastIndexTimeProps = new Properties();
-	    lastIndexTimeProps.setProperty(LAST_INDEX_KEY,
-	            DataImporter.DATE_TIME_FORMAT.get().format(dataImporter.getIndexStartTime()));
-	    
-	    epwList = new ArrayList<EntityProcessorWrapper>(config.getEntities().size());
-	    for (Entity e : config.getEntities()) {
-	      epwList.add(getEntityProcessorWrapper(e));
-	    }	    
-	    for (EntityProcessorWrapper epw : epwList) {
-	      if (entities != null && !entities.contains(epw.getEntity().getName()))
-	        continue;
-	      lastIndexTimeProps.setProperty(epw.getEntity().getName() + "." + LAST_INDEX_KEY,
-	              DataImporter.DATE_TIME_FORMAT.get().format(new Date()));
-	      currentEntityProcessorWrapper = epw;
-	      String delQuery = epw.getEntity().getAllAttributes().get("preImportDeleteQuery");
-	      if (dataImporter.getStatus() == DataImporter.Status.RUNNING_DELTA_DUMP) {
-	        cleanByQuery(delQuery, fullCleanDone);
-	        doDelta();
-	        delQuery = epw.getEntity().getAllAttributes().get("postImportDeleteQuery");
-	        if (delQuery != null) {
-	          fullCleanDone.set(false);
-	          cleanByQuery(delQuery, fullCleanDone);
-	        }
-	      } else {
-	        cleanByQuery(delQuery, fullCleanDone);
-	        doFullDump();
-	        delQuery = epw.getEntity().getAllAttributes().get("postImportDeleteQuery");
-	        if (delQuery != null) {
-	          fullCleanDone.set(false);
-	          cleanByQuery(delQuery, fullCleanDone);
-	        }
-	      }
-	      statusMessages.remove(DataImporter.MSG.TOTAL_DOC_PROCESSED);
-	    }
-	
-	    if (stop.get()) {
-	      // Dont commit if aborted using command=abort
-	      statusMessages.put("Aborted", DataImporter.DATE_TIME_FORMAT.get().format(new Date()));
-	      rollback();
-	    } else {
-	      // Do not commit unnecessarily if this is a delta-import and no documents were created or deleted
-	      if (!reqParams.isClean()) {
-	        if (importStatistics.docCount.get() > 0 || importStatistics.deletedDocCount.get() > 0) {
-	          finish(lastIndexTimeProps);
-	        }
-	      } else {
-	        // Finished operation normally, commit now
-	        finish(lastIndexTimeProps);
-	      } 
-	      
-	      if (config.getOnImportEnd() != null) {
-	        invokeEventListener(config.getOnImportEnd());
-	      }
-	    }
-	
-	    statusMessages.remove(TIME_ELAPSED);
-	    statusMessages.put(DataImporter.MSG.TOTAL_DOC_PROCESSED, ""+ importStatistics.docCount.get());
-	    if(importStatistics.failedDocCount.get() > 0)
-	      statusMessages.put(DataImporter.MSG.TOTAL_FAILED_DOCS, ""+ importStatistics.failedDocCount.get());
-	
-	    statusMessages.put("Time taken", getTimeElapsedSince(startTime.get()));
-	    LOG.info("Time taken = " + getTimeElapsedSince(startTime.get()));
-	  } catch(Exception e)
-		{
-			throw new RuntimeException(e);
-		} finally
-		{
-			if (writer != null) {
-	      writer.close();
-	    }
-			if (epwList != null) {
-			  closeEntityProcessorWrappers(epwList);
-			}
-			if(reqParams.isDebug()) {
-				reqParams.getDebugInfo().debugVerboseOutput = getDebugLogger().output;	
-			}
-		}
+    try {
+      dataImporter.store(DataImporter.STATUS_MSGS, statusMessages);
+      config = dataImporter.getConfig();
+      final AtomicLong startTime = new AtomicLong(System.currentTimeMillis());
+      statusMessages.put(TIME_ELAPSED, new Object() {
+        @Override
+        public String toString() {
+          return getTimeElapsedSince(startTime.get());
+        }
+      });
+
+      statusMessages.put(DataImporter.MSG.TOTAL_QUERIES_EXECUTED,
+              importStatistics.queryCount);
+      statusMessages.put(DataImporter.MSG.TOTAL_ROWS_EXECUTED,
+              importStatistics.rowsCount);
+      statusMessages.put(DataImporter.MSG.TOTAL_DOC_PROCESSED,
+              importStatistics.docCount);
+      statusMessages.put(DataImporter.MSG.TOTAL_DOCS_SKIPPED,
+              importStatistics.skipDocCount);
+
+      List<String> entities = reqParams.getEntitiesToRun();
+
+      // Trigger onImportStart
+      if (config.getOnImportStart() != null) {
+        invokeEventListener(config.getOnImportStart());
+      }
+      AtomicBoolean fullCleanDone = new AtomicBoolean(false);
+      //we must not do a delete of *:* multiple times if there are multiple root entities to be run
+      Properties lastIndexTimeProps = new Properties();
+      lastIndexTimeProps.setProperty(LAST_INDEX_KEY,
+              DataImporter.DATE_TIME_FORMAT.get().format(dataImporter.getIndexStartTime()));
+
+      epwList = new ArrayList<EntityProcessorWrapper>(config.getEntities().size());
+      for (Entity e : config.getEntities()) {
+        epwList.add(getEntityProcessorWrapper(e));
+      }
+      for (EntityProcessorWrapper epw : epwList) {
+        if (entities != null && !entities.contains(epw.getEntity().getName()))
+          continue;
+        lastIndexTimeProps.setProperty(epw.getEntity().getName() + "." + LAST_INDEX_KEY,
+                DataImporter.DATE_TIME_FORMAT.get().format(new Date()));
+        currentEntityProcessorWrapper = epw;
+        String delQuery = epw.getEntity().getAllAttributes().get("preImportDeleteQuery");
+        if (dataImporter.getStatus() == DataImporter.Status.RUNNING_DELTA_DUMP) {
+          cleanByQuery(delQuery, fullCleanDone);
+          doDelta();
+          delQuery = epw.getEntity().getAllAttributes().get("postImportDeleteQuery");
+          if (delQuery != null) {
+            fullCleanDone.set(false);
+            cleanByQuery(delQuery, fullCleanDone);
+          }
+        } else {
+          cleanByQuery(delQuery, fullCleanDone);
+          doFullDump();
+          delQuery = epw.getEntity().getAllAttributes().get("postImportDeleteQuery");
+          if (delQuery != null) {
+            fullCleanDone.set(false);
+            cleanByQuery(delQuery, fullCleanDone);
+          }
+        }
+        statusMessages.remove(DataImporter.MSG.TOTAL_DOC_PROCESSED);
+      }
+
+      if (stop.get()) {
+        // Dont commit if aborted using command=abort
+        statusMessages.put("Aborted", DataImporter.DATE_TIME_FORMAT.get().format(new Date()));
+        rollback();
+      } else {
+        // Do not commit unnecessarily if this is a delta-import and no documents were created or deleted
+        if (!reqParams.isClean()) {
+          if (importStatistics.docCount.get() > 0 || importStatistics.deletedDocCount.get() > 0) {
+            finish(lastIndexTimeProps);
+          }
+        } else {
+          // Finished operation normally, commit now
+          finish(lastIndexTimeProps);
+        }
+
+        if (config.getOnImportEnd() != null) {
+          invokeEventListener(config.getOnImportEnd());
+        }
+      }
+
+      statusMessages.remove(TIME_ELAPSED);
+      statusMessages.put(DataImporter.MSG.TOTAL_DOC_PROCESSED, ""+ importStatistics.docCount.get());
+      if(importStatistics.failedDocCount.get() > 0)
+        statusMessages.put(DataImporter.MSG.TOTAL_FAILED_DOCS, ""+ importStatistics.failedDocCount.get());
+
+      statusMessages.put("Time taken", getTimeElapsedSince(startTime.get()));
+      LOG.info("Time taken = " + getTimeElapsedSince(startTime.get()));
+    } catch(Exception e)
+    {
+      throw new RuntimeException(e);
+    } finally
+    {
+      if (writer != null) {
+        writer.close();
+      }
+      if (epwList != null) {
+        closeEntityProcessorWrappers(epwList);
+      }
+      if(reqParams.isDebug()) {
+        reqParams.getDebugInfo().debugVerboseOutput = getDebugLogger().output;
+      }
+    }
   }
   private void closeEntityProcessorWrappers(List<EntityProcessorWrapper> epwList) {
     for(EntityProcessorWrapper epw : epwList) {
@@ -506,7 +506,7 @@
             if (!doc.isEmpty()) {
               boolean result = writer.upload(doc);
               if(reqParams.isDebug()) {
-              	reqParams.getDebugInfo().debugDocuments.add(doc);
+                reqParams.getDebugInfo().debugDocuments.add(doc);
               }
               doc = null;
               if (result){
Index: solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DebugLogger.java
===================================================================
--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DebugLogger.java	(revision 1386643)
+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DebugLogger.java	(working copy)
@@ -143,7 +143,7 @@
 
   private void popAllTransformers() {
     while (true) {
-    	DIHLogLevels type = debugStack.peek().type;
+      DIHLogLevels type = debugStack.peek().type;
       if (type == DIHLogLevels.START_DOC || type == DIHLogLevels.START_ENTITY)
         break;
       debugStack.pop();
Index: solr/contrib/uima/src/test/org/apache/solr/uima/ts/SentimentAnnotation_Type.java
===================================================================
--- solr/contrib/uima/src/test/org/apache/solr/uima/ts/SentimentAnnotation_Type.java	(revision 1386643)
+++ solr/contrib/uima/src/test/org/apache/solr/uima/ts/SentimentAnnotation_Type.java	(working copy)
@@ -1,4 +1,3 @@
-
 /* First created by JCasGen Fri Mar 04 13:08:40 CET 2011 */
 package org.apache.solr.uima.ts;
 
@@ -23,17 +22,17 @@
   private final FSGenerator fsGenerator = 
     new FSGenerator() {
       public FeatureStructure createFS(int addr, CASImpl cas) {
-  			 if (SentimentAnnotation_Type.this.useExistingInstance) {
-  			   // Return eq fs instance if already created
-  		     FeatureStructure fs = SentimentAnnotation_Type.this.jcas.getJfsFromCaddr(addr);
-  		     if (null == fs) {
-  		       fs = new SentimentAnnotation(addr, SentimentAnnotation_Type.this);
-  			   SentimentAnnotation_Type.this.jcas.putJfsFromCaddr(addr, fs);
-  			   return fs;
-  		     }
-  		     return fs;
+        if (SentimentAnnotation_Type.this.useExistingInstance) {
+          // Return eq fs instance if already created
+          FeatureStructure fs = SentimentAnnotation_Type.this.jcas.getJfsFromCaddr(addr);
+          if (null == fs) {
+            fs = new SentimentAnnotation(addr, SentimentAnnotation_Type.this);
+            SentimentAnnotation_Type.this.jcas.putJfsFromCaddr(addr, fs);
+            return fs;
+          }
+          return fs;
         } else return new SentimentAnnotation(addr, SentimentAnnotation_Type.this);
-  	  }
+      }
     };
   /** @generated */
   public final static int typeIndexID = SentimentAnnotation.typeIndexID;
@@ -62,7 +61,7 @@
 
 
   /** initialize variables to correspond with Cas Type and Features
-	* @generated */
+  * @generated */
   public SentimentAnnotation_Type(JCas jcas, Type casType) {
     super(jcas, casType);
     casImpl.getFSClassRegistry().addGeneratorForType((TypeImpl)this.casType, getFSGenerator());
@@ -76,4 +75,4 @@
 
 
 
-    
\ No newline at end of file
+    
Index: solr/contrib/uima/src/test/org/apache/solr/uima/ts/EntityAnnotation_Type.java
===================================================================
--- solr/contrib/uima/src/test/org/apache/solr/uima/ts/EntityAnnotation_Type.java	(revision 1386643)
+++ solr/contrib/uima/src/test/org/apache/solr/uima/ts/EntityAnnotation_Type.java	(working copy)
@@ -1,4 +1,3 @@
-
 /* First created by JCasGen Sat May 07 22:33:38 JST 2011 */
 package org.apache.solr.uima.ts;
 
@@ -23,17 +22,17 @@
   private final FSGenerator fsGenerator = 
     new FSGenerator() {
       public FeatureStructure createFS(int addr, CASImpl cas) {
-  			 if (EntityAnnotation_Type.this.useExistingInstance) {
-  			   // Return eq fs instance if already created
-  		     FeatureStructure fs = EntityAnnotation_Type.this.jcas.getJfsFromCaddr(addr);
-  		     if (null == fs) {
-  		       fs = new EntityAnnotation(addr, EntityAnnotation_Type.this);
-  			   EntityAnnotation_Type.this.jcas.putJfsFromCaddr(addr, fs);
-  			   return fs;
-  		     }
-  		     return fs;
+         if (EntityAnnotation_Type.this.useExistingInstance) {
+           // Return eq fs instance if already created
+           FeatureStructure fs = EntityAnnotation_Type.this.jcas.getJfsFromCaddr(addr);
+           if (null == fs) {
+             fs = new EntityAnnotation(addr, EntityAnnotation_Type.this);
+             EntityAnnotation_Type.this.jcas.putJfsFromCaddr(addr, fs);
+             return fs;
+           }
+           return fs;
         } else return new EntityAnnotation(addr, EntityAnnotation_Type.this);
-  	  }
+      }
     };
   /** @generated */
   public final static int typeIndexID = EntityAnnotation.typeIndexID;
@@ -80,7 +79,7 @@
 
 
   /** initialize variables to correspond with Cas Type and Features
-	* @generated */
+  * @generated */
   public EntityAnnotation_Type(JCas jcas, Type casType) {
     super(jcas, casType);
     casImpl.getFSClassRegistry().addGeneratorForType((TypeImpl)this.casType, getFSGenerator());
@@ -98,4 +97,4 @@
 
 
 
-    
\ No newline at end of file
+    
Index: solr/contrib/velocity/src/java/org/apache/solr/response/PageTool.java
===================================================================
--- solr/contrib/velocity/src/java/org/apache/solr/response/PageTool.java	(revision 1386643)
+++ solr/contrib/velocity/src/java/org/apache/solr/response/PageTool.java	(working copy)
@@ -58,7 +58,7 @@
         results_found = doc_list.getNumFound();
         start = doc_list.getStart();
       } else {
-	  throw new SolrException(SolrException.ErrorCode.UNKNOWN, "Unknown response type "+docs+". Expected one of DocSlice, ResultContext or SolrDocumentList");
+        throw new SolrException(SolrException.ErrorCode.UNKNOWN, "Unknown response type "+docs+". Expected one of DocSlice, ResultContext or SolrDocumentList");
       }
     }
 
Index: lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestQueryTemplateManager.java
===================================================================
--- lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestQueryTemplateManager.java	(revision 1386643)
+++ lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestQueryTemplateManager.java	(working copy)
@@ -89,7 +89,7 @@
       Properties queryFormProperties = getPropsFromString(queryForm);
 
       //Get the required query XSL template for this test
-//			Templates template=getTemplate(queryFormProperties.getProperty("template"));
+//      Templates template=getTemplate(queryFormProperties.getProperty("template"));
 
       //Transform the queryFormProperties into a Lucene XML query
       Document doc = qtm.getQueryAsDOM(queryFormProperties, queryFormProperties.getProperty("template"));
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/DOMUtils.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/DOMUtils.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/DOMUtils.java	(working copy)
@@ -212,7 +212,7 @@
     }
 
     return doc;
-	}	
+  }
 }
 
 
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/CoreParser.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/CoreParser.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/CoreParser.java	(working copy)
@@ -153,5 +153,5 @@
 
   public Query getQuery(Element e) throws ParserException {
     return queryFactory.getQuery(e);
-	}
+  }
 }
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/CorePlusExtensionsParser.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/CorePlusExtensionsParser.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/CorePlusExtensionsParser.java	(working copy)
@@ -58,6 +58,6 @@
     queryFactory.addBuilder("LikeThisQuery", new LikeThisQueryBuilder(analyzer, fields));
     queryFactory.addBuilder("BoostingQuery", new BoostingQueryBuilder(queryFactory));
     queryFactory.addBuilder("FuzzyLikeThisQuery", new FuzzyLikeThisQueryBuilder(analyzer));
-		
-	}
+
+  }
 }
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/CachedFilterBuilder.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/CachedFilterBuilder.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/CachedFilterBuilder.java	(working copy)
@@ -32,10 +32,10 @@
  * Filters are cached in an LRU Cache keyed on the contained query or filter object. Using this will
  * speed up overall performance for repeated uses of the same expensive query/filter. The sorts of
  * queries/filters likely to benefit from caching need not necessarily be complex - e.g. simple
- * TermQuerys with a large DF (document frequency) can be expensive	on large indexes.
- * A good example of this might be a term query on a field with only 2 possible	values -
+ * TermQuerys with a large DF (document frequency) can be expensive  on large indexes.
+ * A good example of this might be a term query on a field with only 2 possible  values -
  * "true" or "false". In a large index, querying or filtering on this field requires reading
- * millions	of document ids from disk which can more usefully be cached as a filter bitset.
+ * millions  of document ids from disk which can more usefully be cached as a filter bitset.
  * <p/>
  * For Queries/Filters to be cached and reused the object must implement hashcode and
  * equals methods correctly so that duplicate queries/filters can be detected in the cache.
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/UserInputQueryBuilder.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/UserInputQueryBuilder.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/UserInputQueryBuilder.java	(working copy)
@@ -88,6 +88,6 @@
    */
   protected QueryParser createQueryParser(String fieldName, Analyzer analyzer) {
     return new QueryParser(Version.LUCENE_CURRENT, fieldName, analyzer);
-	}
+  }
 
 }
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java	(working copy)
@@ -158,7 +158,7 @@
     @Override
     public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
       return null;
-		}
+    }
 
-	}
+  }
 }
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanQueryBuilder.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanQueryBuilder.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/SpanQueryBuilder.java	(working copy)
@@ -24,6 +24,6 @@
  * Interface for retrieving a {@link SpanQuery}.
  */
 public interface SpanQueryBuilder extends QueryBuilder {
-	
-	public SpanQuery getSpanQuery(Element e) throws ParserException;
+
+  public SpanQuery getSpanQuery(Element e) throws ParserException;
 }
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/ParserException.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/ParserException.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/ParserException.java	(working copy)
@@ -25,32 +25,32 @@
  */
 public class ParserException extends Exception {
 
-	/**
-	 * 
-	 */
-	public ParserException() {
-		super();
-	}
+  /**
+   *
+   */
+  public ParserException() {
+    super();
+  }
 
-	/**
-	 * @param message
-	 */
-	public ParserException(String message) {
-		super(message);
-	}
+  /**
+   * @param message
+   */
+  public ParserException(String message) {
+    super(message);
+  }
 
-	/**
-	 * @param message
-	 * @param cause
-	 */
-	public ParserException(String message, Throwable cause) {
-		super(message, cause);
-	}
+  /**
+   * @param message
+   * @param cause
+   */
+  public ParserException(String message, Throwable cause) {
+    super(message, cause);
+  }
 
-	/**
-	 * @param cause
-	 */
-	public ParserException(Throwable cause) {
-		super(cause);
-	}
+  /**
+   * @param cause
+   */
+  public ParserException(Throwable cause) {
+    super(cause);
+  }
 }
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/FilterBuilder.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/FilterBuilder.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/FilterBuilder.java	(working copy)
@@ -27,5 +27,5 @@
  */
 public interface FilterBuilder {
 
-	 public Filter getFilter(Element e) throws ParserException;
+   public Filter getFilter(Element e) throws ParserException;
 }
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryBuilder.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryBuilder.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryBuilder.java	(working copy)
@@ -23,6 +23,6 @@
  * expected to be thread-safe so that they can be used to simultaneously parse multiple XML documents.
  */
 public interface QueryBuilder {
-	
-	public Query getQuery(Element e) throws ParserException;
+
+  public Query getQuery(Element e) throws ParserException;
 }
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryTemplateManager.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryTemplateManager.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/QueryTemplateManager.java	(working copy)
@@ -199,5 +199,5 @@
     org.w3c.dom.Document xslDoc = builder.parse(xslIs);
     DOMSource ds = new DOMSource(xslDoc);
     return tFactory.newTemplates(ds);
-	}
+  }
 }
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.java	(working copy)
@@ -166,10 +166,10 @@
 
 // This makes sure that there is no garbage after the query string
   final public Query TopLevelQuery(String field) throws ParseException {
-        Query q;
+  Query q;
     q = Query(field);
     jj_consume_token(0);
-                {if (true) return q;}
+    {if (true) return q;}
     throw new Error("Missing return statement in function");
   }
 
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.jj
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.jj	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParser.jj	(working copy)
@@ -211,13 +211,13 @@
 // This makes sure that there is no garbage after the query string
 Query TopLevelQuery(String field) : 
 {
-	Query q;
+  Query q;
 }
 {
-	q=Query(field) <EOF>
-	{
-		return q;
-	}
+  q=Query(field) <EOF>
+  {
+    return q;
+  }
 }
 
 Query Query(String field) :
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/FastCharStream.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/FastCharStream.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/FastCharStream.java	(working copy)
@@ -29,13 +29,13 @@
 public final class FastCharStream implements CharStream {
   char[] buffer = null;
 
-  int bufferLength = 0;				  // end of valid chars
-  int bufferPosition = 0;			  // next char to read
+  int bufferLength = 0;          // end of valid chars
+  int bufferPosition = 0;        // next char to read
 
-  int tokenStart = 0;				  // offset in buffer
-  int bufferStart = 0;				  // position in file of buffer
+  int tokenStart = 0;          // offset in buffer
+  int bufferStart = 0;          // position in file of buffer
 
-  Reader input;					  // source of chars
+  Reader input;            // source of chars
 
   /** Constructs from a Reader. */
   public FastCharStream(Reader r) {
@@ -51,24 +51,24 @@
   private final void refill() throws IOException {
     int newPosition = bufferLength - tokenStart;
 
-    if (tokenStart == 0) {			  // token won't fit in buffer
-      if (buffer == null) {			  // first time: alloc buffer
-	buffer = new char[2048];
+    if (tokenStart == 0) {        // token won't fit in buffer
+      if (buffer == null) {        // first time: alloc buffer
+  buffer = new char[2048];
       } else if (bufferLength == buffer.length) { // grow buffer
-	char[] newBuffer = new char[buffer.length*2];
-	System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
-	buffer = newBuffer;
+  char[] newBuffer = new char[buffer.length*2];
+  System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
+  buffer = newBuffer;
       }
-    } else {					  // shift token to front
+    } else {            // shift token to front
       System.arraycopy(buffer, tokenStart, buffer, 0, newPosition);
     }
 
-    bufferLength = newPosition;			  // update state
+    bufferLength = newPosition;        // update state
     bufferPosition = newPosition;
     bufferStart += tokenStart;
     tokenStart = 0;
 
-    int charsRead =				  // fill space in buffer
+    int charsRead =          // fill space in buffer
       input.read(buffer, newPosition, buffer.length-newPosition);
     if (charsRead == -1)
       throw new IOException("read past eof");
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/BasicQueryFactory.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/BasicQueryFactory.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/BasicQueryFactory.java	(working copy)
@@ -47,9 +47,9 @@
   
   public String toString() {
     return getClass().getName()
-	  + "(maxBasicQueries: " + maxBasicQueries
-	  + ", queriesMade: " + queriesMade
-	  + ")";
+    + "(maxBasicQueries: " + maxBasicQueries
+    + ", queriesMade: " + queriesMade
+    + ")";
   }
 
   private boolean atMax() {
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/RewriteQuery.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/RewriteQuery.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/RewriteQuery.java	(working copy)
@@ -45,19 +45,19 @@
   @Override
   public String toString(String field) {
     return getClass().getName()
-	  + (field == null ? "" : "(unused: " + field + ")")
-	  + "(" + fieldName
-	  + ", " + srndQuery.toString()
-	  + ", " + qf.toString()
-	  + ")";
+    + (field == null ? "" : "(unused: " + field + ")")
+    + "(" + fieldName
+    + ", " + srndQuery.toString()
+    + ", " + qf.toString()
+    + ")";
   }
 
   @Override
   public int hashCode() {
     return getClass().hashCode()
-	  ^ fieldName.hashCode()
-	  ^ qf.hashCode()
-	  ^ srndQuery.hashCode();
+    ^ fieldName.hashCode()
+    ^ qf.hashCode()
+    ^ srndQuery.hashCode();
   }
 
   @Override
@@ -68,8 +68,8 @@
       return false;
     RewriteQuery other = (RewriteQuery)obj;
     return fieldName.equals(other.fieldName)
-	&& qf.equals(other.qf)
-	&& srndQuery.equals(other.srndQuery);
+  && qf.equals(other.qf)
+  && srndQuery.equals(other.srndQuery);
   }
 
   /** @throws UnsupportedOperationException */
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SimpleTermRewriteQuery.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SimpleTermRewriteQuery.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SimpleTermRewriteQuery.java	(working copy)
@@ -37,16 +37,16 @@
   public Query rewrite(IndexReader reader) throws IOException {
     final List<Query> luceneSubQueries = new ArrayList<Query>();
     srndQuery.visitMatchingTerms(reader, fieldName,
-	  new SimpleTerm.MatchingTermVisitor() {
-	    public void visitMatchingTerm(Term term) throws IOException {
-	      luceneSubQueries.add(qf.newTermQuery(term));
-	    }
-	  });
+    new SimpleTerm.MatchingTermVisitor() {
+      public void visitMatchingTerm(Term term) throws IOException {
+        luceneSubQueries.add(qf.newTermQuery(term));
+      }
+    });
     return  (luceneSubQueries.size() == 0) ? SrndQuery.theEmptyLcnQuery
-	  : (luceneSubQueries.size() == 1) ? luceneSubQueries.get(0)
-	  : SrndBooleanQuery.makeBooleanQuery(
-	    /* luceneSubQueries all have default weight */
-	    luceneSubQueries, BooleanClause.Occur.SHOULD); /* OR the subquery terms */
+    : (luceneSubQueries.size() == 1) ? luceneSubQueries.get(0)
+    : SrndBooleanQuery.makeBooleanQuery(
+      /* luceneSubQueries all have default weight */
+      luceneSubQueries, BooleanClause.Occur.SHOULD); /* OR the subquery terms */
   }
 }
 
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/FastCharStream.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/FastCharStream.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/FastCharStream.java	(working copy)
@@ -26,13 +26,13 @@
 public final class FastCharStream implements CharStream {
   char[] buffer = null;
 
-  int bufferLength = 0;				  // end of valid chars
-  int bufferPosition = 0;			  // next char to read
+  int bufferLength = 0;          // end of valid chars
+  int bufferPosition = 0;        // next char to read
 
-  int tokenStart = 0;				  // offset in buffer
-  int bufferStart = 0;				  // position in file of buffer
+  int tokenStart = 0;          // offset in buffer
+  int bufferStart = 0;          // position in file of buffer
 
-  Reader input;					  // source of chars
+  Reader input;            // source of chars
 
   /** Constructs from a Reader. */
   public FastCharStream(Reader r) {
@@ -48,24 +48,24 @@
   private final void refill() throws IOException {
     int newPosition = bufferLength - tokenStart;
 
-    if (tokenStart == 0) {			  // token won't fit in buffer
-      if (buffer == null) {			  // first time: alloc buffer
-	buffer = new char[2048];
+    if (tokenStart == 0) {        // token won't fit in buffer
+      if (buffer == null) {        // first time: alloc buffer
+        buffer = new char[2048];
       } else if (bufferLength == buffer.length) { // grow buffer
-	char[] newBuffer = new char[buffer.length*2];
-	System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
-	buffer = newBuffer;
+        char[] newBuffer = new char[buffer.length * 2];
+        System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
+        buffer = newBuffer;
       }
-    } else {					  // shift token to front
+    } else {            // shift token to front
       System.arraycopy(buffer, tokenStart, buffer, 0, newPosition);
     }
 
-    bufferLength = newPosition;			  // update state
+    bufferLength = newPosition;        // update state
     bufferPosition = newPosition;
     bufferStart += tokenStart;
     tokenStart = 0;
 
-    int charsRead =				  // fill space in buffer
+    int charsRead =          // fill space in buffer
       input.read(buffer, newPosition, buffer.length-newPosition);
     if (charsRead == -1)
       throw new IOException("read past eof");
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/StandardSyntaxParser.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/StandardSyntaxParser.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/StandardSyntaxParser.java	(working copy)
@@ -45,14 +45,14 @@
  */
 public class StandardSyntaxParser implements SyntaxParser, StandardSyntaxParserConstants {
 
-        private static final int CONJ_NONE =0;
-        private static final int CONJ_AND =2;
-        private static final int CONJ_OR =2;
+  private static final int CONJ_NONE =0;
+  private static final int CONJ_AND =2;
+  private static final int CONJ_OR =2;
 
 
    // syntax parser constructor
    public StandardSyntaxParser() {
-        this(new FastCharStream(new StringReader("")));
+     this(new FastCharStream(new StringReader("")));
   }
      /** Parses a query string, returning a {@link org.apache.lucene.queryparser.flexible.core.nodes.QueryNode}.
      *  @param query  the query string to be parsed.
@@ -143,10 +143,10 @@
 
 // This makes sure that there is no garbage after the query string
   final public QueryNode TopLevelQuery(CharSequence field) throws ParseException {
-        QueryNode q;
+  QueryNode q;
     q = Query(field);
     jj_consume_token(0);
-                {if (true) return q;}
+    {if (true) return q;}
     throw new Error("Missing return statement in function");
   }
 
@@ -184,23 +184,23 @@
         break label_1;
       }
       c = DisjQuery(field);
-             if (clauses == null) {
-                 clauses = new Vector<QueryNode>();
-                 clauses.addElement(first);
-             }
-         clauses.addElement(c);
+       if (clauses == null) {
+           clauses = new Vector<QueryNode>();
+           clauses.addElement(first);
+       }
+       clauses.addElement(c);
     }
         if (clauses != null) {
-                {if (true) return new BooleanQueryNode(clauses);}
-        } else {
-                {if (true) return first;}
-        }
+        {if (true) return new BooleanQueryNode(clauses);}
+      } else {
+          {if (true) return first;}
+      }
     throw new Error("Missing return statement in function");
   }
 
   final public QueryNode DisjQuery(CharSequence field) throws ParseException {
-        QueryNode first, c;
-        Vector<QueryNode> clauses = null;
+  QueryNode first, c;
+  Vector<QueryNode> clauses = null;
     first = ConjQuery(field);
     label_2:
     while (true) {
@@ -221,7 +221,7 @@
      clauses.addElement(c);
     }
     if (clauses != null) {
-            {if (true) return new OrQueryNode(clauses);}
+      {if (true) return new OrQueryNode(clauses);}
     } else {
         {if (true) return first;}
     }
@@ -229,8 +229,8 @@
   }
 
   final public QueryNode ConjQuery(CharSequence field) throws ParseException {
-        QueryNode first, c;
-        Vector<QueryNode> clauses = null;
+  QueryNode first, c;
+  Vector<QueryNode> clauses = null;
     first = ModClause(field);
     label_3:
     while (true) {
@@ -251,7 +251,7 @@
      clauses.addElement(c);
     }
     if (clauses != null) {
-            {if (true) return new AndQueryNode(clauses);}
+      {if (true) return new AndQueryNode(clauses);}
     } else {
         {if (true) return first;}
     }
@@ -272,27 +272,27 @@
 //     if (mods == ModifierQueryNode.Modifier.MOD_NONE) firstQuery=q;
 //     
 //     // do not create modifier nodes with MOD_NONE
-//    	if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
-//    		q = new ModifierQueryNode(q, mods);
-//    	}
-//    	clauses.add(q);
+//      if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
+//        q = new ModifierQueryNode(q, mods);
+//      }
+//      clauses.add(q);
 //   }
 //   (
 //     conj=Conjunction() mods=Modifiers() q=Clause(field)
 //     { 
-// 	    // do not create modifier nodes with MOD_NONE
-// 	   	if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
-// 	   		q = new ModifierQueryNode(q, mods);
-// 	   	}
-// 	   	clauses.add(q);
-// 	   	//TODO: figure out what to do with AND and ORs
+//       // do not create modifier nodes with MOD_NONE
+//        if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
+//          q = new ModifierQueryNode(q, mods);
+//        }
+//        clauses.add(q);
+//        //TODO: figure out what to do with AND and ORs
 //   }
 //   )*
 //     {
 //      if (clauses.size() == 1 && firstQuery != null)
 //         return firstQuery;
 //       else {
-//   		return new BooleanQueryNode(clauses);
+//       return new BooleanQueryNode(clauses);
 //       }
 //     }
 // }
@@ -301,10 +301,10 @@
   ModifierQueryNode.Modifier mods;
     mods = Modifiers();
     q = Clause(field);
-                if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
-                        q = new ModifierQueryNode(q, mods);
-                }
-                {if (true) return q;}
+        if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
+          q = new ModifierQueryNode(q, mods);
+        }
+        {if (true) return q;}
     throw new Error("Missing return statement in function");
   }
 
@@ -378,18 +378,18 @@
         }
         switch (operator.kind) {
             case OP_LESSTHAN:
-                lowerInclusive = true;
-                upperInclusive = false;
+              lowerInclusive = true;
+              upperInclusive = false;
 
-                qLower = new FieldQueryNode(field,
+              qLower = new FieldQueryNode(field,
                                          "*", term.beginColumn, term.endColumn);
-                        qUpper = new FieldQueryNode(field,
+            qUpper = new FieldQueryNode(field,
                                  EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
 
                 break;
             case OP_LESSTHANEQ:
-                lowerInclusive = true;
-                upperInclusive = true;
+              lowerInclusive = true;
+              upperInclusive = true;
 
                 qLower = new FieldQueryNode(field,
                                          "*", term.beginColumn, term.endColumn);
@@ -397,8 +397,8 @@
                                          EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
                 break;
             case OP_MORETHAN:
-                lowerInclusive = false;
-                upperInclusive = true;
+              lowerInclusive = false;
+              upperInclusive = true;
 
                 qLower = new FieldQueryNode(field,
                                          EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
@@ -406,8 +406,8 @@
                                          "*", term.beginColumn, term.endColumn);
                 break;
             case OP_MORETHANEQ:
-                lowerInclusive = true;
-                upperInclusive = true;
+              lowerInclusive = true;
+              upperInclusive = true;
 
                 qLower = new FieldQueryNode(field,
                                          EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
@@ -488,19 +488,19 @@
       }
     }
       if (boost != null) {
-                  float f = (float)1.0;
-                  try {
-                    f = Float.valueOf(boost.image).floatValue();
-                    // avoid boosting null queries, such as those caused by stop words
-                if (q != null) {
-                        q = new BoostQueryNode(q, f);
-                }
-                  } catch (Exception ignored) {
-                        /* Should this be handled somehow? (defaults to "no boost", if
+      float f = (float)1.0;
+      try {
+        f = Float.valueOf(boost.image).floatValue();
+        // avoid boosting null queries, such as those caused by stop words
+          if (q != null) {
+            q = new BoostQueryNode(q, f);
+          }
+      } catch (Exception ignored) {
+        /* Should this be handled somehow? (defaults to "no boost", if
              * boost number is invalid)
              */
-                  }
       }
+      }
       if (group) { q = new GroupQueryNode(q);}
       {if (true) return q;}
     throw new Error("Missing return statement in function");
@@ -522,7 +522,7 @@
       switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
       case TERM:
         term = jj_consume_token(TERM);
-                         q = new FieldQueryNode(field, EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
+                    q = new FieldQueryNode(field, EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
         break;
       case REGEXPTERM:
         term = jj_consume_token(REGEXPTERM);
@@ -564,16 +564,16 @@
         ;
       }
        if (fuzzy) {
-          float fms = defaultMinSimilarity;
-          try {
+           float fms = defaultMinSimilarity;
+           try {
             fms = Float.valueOf(fuzzySlop.image.substring(1)).floatValue();
-          } catch (Exception ignored) { }
-         if(fms < 0.0f){
-           {if (true) throw new ParseException(new MessageImpl(QueryParserMessages.INVALID_SYNTAX_FUZZY_LIMITS));}
-         } else if (fms >= 1.0f && fms != (int) fms) {
-           {if (true) throw new ParseException(new MessageImpl(QueryParserMessages.INVALID_SYNTAX_FUZZY_EDITS));}
-         }
-         q = new FuzzyQueryNode(field, EscapeQuerySyntaxImpl.discardEscapeChar(term.image), fms, term.beginColumn, term.endColumn);
+           } catch (Exception ignored) { }
+          if(fms < 0.0f){
+            {if (true) throw new ParseException(new MessageImpl(QueryParserMessages.INVALID_SYNTAX_FUZZY_LIMITS));}
+          } else if (fms >= 1.0f && fms != (int) fms) {
+            {if (true) throw new ParseException(new MessageImpl(QueryParserMessages.INVALID_SYNTAX_FUZZY_EDITS));}
+          }
+          q = new FuzzyQueryNode(field, EscapeQuerySyntaxImpl.discardEscapeChar(term.image), fms, term.beginColumn, term.endColumn);
        } else if (regexp) {
          String re = term.image.substring(1, term.image.length()-1);
          q = new RegexpQueryNode(field, re, 0, re.length());
@@ -656,9 +656,9 @@
           }
 
           qLower = new FieldQueryNode(field,
-                                               EscapeQuerySyntaxImpl.discardEscapeChar(goop1.image), goop1.beginColumn, goop1.endColumn);
-                  qUpper = new FieldQueryNode(field,
-                                               EscapeQuerySyntaxImpl.discardEscapeChar(goop2.image), goop2.beginColumn, goop2.endColumn);
+                                   EscapeQuerySyntaxImpl.discardEscapeChar(goop1.image), goop1.beginColumn, goop1.endColumn);
+      qUpper = new FieldQueryNode(field,
+                                   EscapeQuerySyntaxImpl.discardEscapeChar(goop2.image), goop2.beginColumn, goop2.endColumn);
           q = new TermRangeQueryNode(qLower, qUpper, startInc ? true : false, endInc ? true : false);
       break;
     case QUOTED:
@@ -690,8 +690,8 @@
            }
            catch (Exception ignored) {
             /* Should this be handled somehow? (defaults to "no PhraseSlop", if
-	         * slop number is invalid)
-	         */
+           * slop number is invalid)
+           */
            }
          }
       break;
@@ -700,20 +700,20 @@
       jj_consume_token(-1);
       throw new ParseException();
     }
-          if (boost != null) {
-                  float f = (float)1.0;
-                  try {
-                    f = Float.valueOf(boost.image).floatValue();
-                    // avoid boosting null queries, such as those caused by stop words
-                if (q != null) {
-                        q = new BoostQueryNode(q, f);
-                }
-                  } catch (Exception ignored) {
-                        /* Should this be handled somehow? (defaults to "no boost", if
-	         * boost number is invalid)
-	         */
-                  }
+    if (boost != null) {
+      float f = (float)1.0;
+      try {
+        f = Float.valueOf(boost.image).floatValue();
+        // avoid boosting null queries, such as those caused by stop words
+          if (q != null) {
+            q = new BoostQueryNode(q, f);
           }
+      } catch (Exception ignored) {
+        /* Should this be handled somehow? (defaults to "no boost", if
+           * boost number is invalid)
+           */
+      }
+    }
       {if (true) return q;}
     throw new Error("Missing return statement in function");
   }
@@ -748,11 +748,6 @@
     return false;
   }
 
-  private boolean jj_3R_10() {
-    if (jj_scan_token(TERM)) return true;
-    return false;
-  }
-
   private boolean jj_3R_11() {
     if (jj_scan_token(REGEXPTERM)) return true;
     return false;
@@ -779,6 +774,11 @@
     return false;
   }
 
+  private boolean jj_3R_10() {
+    if (jj_scan_token(TERM)) return true;
+    return false;
+  }
+
   private boolean jj_3R_7() {
     Token xsp;
     xsp = jj_scanpos;
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/StandardSyntaxParser.jj
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/StandardSyntaxParser.jj	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/StandardSyntaxParser.jj	(working copy)
@@ -57,14 +57,14 @@
  */
 public class StandardSyntaxParser implements SyntaxParser {
 
-	private static final int CONJ_NONE =0;
-	private static final int CONJ_AND =2;
-	private static final int CONJ_OR =2;
+  private static final int CONJ_NONE =0;
+  private static final int CONJ_AND =2;
+  private static final int CONJ_OR =2;
 
  
    // syntax parser constructor
    public StandardSyntaxParser() {
-   	this(new FastCharStream(new StringReader("")));
+     this(new FastCharStream(new StringReader("")));
   }
      /** Parses a query string, returning a {@link org.apache.lucene.queryparser.flexible.core.nodes.QueryNode}.
      *  @param query  the query string to be parsed.
@@ -178,13 +178,13 @@
 // This makes sure that there is no garbage after the query string
 QueryNode TopLevelQuery(CharSequence field) : 
 {
-	QueryNode q;
+  QueryNode q;
 }
 {
-	q=Query(field) <EOF>
-	{
-		return q;
-	}
+  q=Query(field) <EOF>
+  {
+    return q;
+  }
 }
 
 // These changes were made to introduce operator precedence:
@@ -209,25 +209,25 @@
   (
     c=DisjQuery(field)
     { 
-	     if (clauses == null) {
-	         clauses = new Vector<QueryNode>();
-	         clauses.addElement(first); 
-	     } 
-    	 clauses.addElement(c);
+       if (clauses == null) {
+           clauses = new Vector<QueryNode>();
+           clauses.addElement(first);
+       }
+       clauses.addElement(c);
     }
     )*
     {
         if (clauses != null) { 
-	    	return new BooleanQueryNode(clauses);
-    	} else {
-        	return first;
-    	}
+        return new BooleanQueryNode(clauses);
+      } else {
+          return first;
+      }
     }
 }
 
 QueryNode DisjQuery(CharSequence field) : {
-	QueryNode first, c;
-	Vector<QueryNode> clauses = null;
+  QueryNode first, c;
+  Vector<QueryNode> clauses = null;
 }
 {
   first = ConjQuery(field)
@@ -243,7 +243,7 @@
   )*
   {
     if (clauses != null) { 
-	    return new OrQueryNode(clauses);
+      return new OrQueryNode(clauses);
     } else {
         return first;
     }
@@ -251,8 +251,8 @@
 }
 
 QueryNode ConjQuery(CharSequence field) : {
-	QueryNode first, c;
-	Vector<QueryNode> clauses = null;
+  QueryNode first, c;
+  Vector<QueryNode> clauses = null;
 }
 {
   first = ModClause(field)
@@ -268,7 +268,7 @@
   )*
   {
     if (clauses != null) {     
-	    return new AndQueryNode(clauses);
+      return new AndQueryNode(clauses);
     } else {
         return first;
     }
@@ -289,27 +289,27 @@
 //     if (mods == ModifierQueryNode.Modifier.MOD_NONE) firstQuery=q;
 //     
 //     // do not create modifier nodes with MOD_NONE
-//    	if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
-//    		q = new ModifierQueryNode(q, mods);
-//    	}
-//    	clauses.add(q);
+//      if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
+//        q = new ModifierQueryNode(q, mods);
+//      }
+//      clauses.add(q);
 //   }
 //   (
 //     conj=Conjunction() mods=Modifiers() q=Clause(field)
 //     { 
-// 	    // do not create modifier nodes with MOD_NONE
-// 	   	if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
-// 	   		q = new ModifierQueryNode(q, mods);
-// 	   	}
-// 	   	clauses.add(q);
-// 	   	//TODO: figure out what to do with AND and ORs
+//       // do not create modifier nodes with MOD_NONE
+//        if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
+//          q = new ModifierQueryNode(q, mods);
+//        }
+//        clauses.add(q);
+//        //TODO: figure out what to do with AND and ORs
 //   }
 //   )*
 //     {
 //      if (clauses.size() == 1 && firstQuery != null)
 //         return firstQuery;
 //       else {
-//   		return new BooleanQueryNode(clauses);
+//       return new BooleanQueryNode(clauses);
 //       }
 //     }
 // }
@@ -320,10 +320,10 @@
 }
 {
    mods=Modifiers() q= Clause(field) {
- 	   	if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
- 	   		q = new ModifierQueryNode(q, mods);
- 	   	}
- 	   	return q;
+        if (mods != ModifierQueryNode.Modifier.MOD_NONE) {
+          q = new ModifierQueryNode(q, mods);
+        }
+        return q;
    }
 }
 
@@ -347,18 +347,18 @@
         }
         switch (operator.kind) {
             case OP_LESSTHAN:
-            	lowerInclusive = true;
-            	upperInclusive = false;
-            	
-            	qLower = new FieldQueryNode(field,
+              lowerInclusive = true;
+              upperInclusive = false;
+
+              qLower = new FieldQueryNode(field,
                                          "*", term.beginColumn, term.endColumn);
-        		qUpper = new FieldQueryNode(field, 
+            qUpper = new FieldQueryNode(field,
                                  EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
                 
                 break;
             case OP_LESSTHANEQ:
-            	lowerInclusive = true;
-            	upperInclusive = true;
+              lowerInclusive = true;
+              upperInclusive = true;
             
                 qLower = new FieldQueryNode(field, 
                                          "*", term.beginColumn, term.endColumn);
@@ -366,8 +366,8 @@
                                          EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
                 break;
             case OP_MORETHAN:
-            	lowerInclusive = false;
-            	upperInclusive = true;
+              lowerInclusive = false;
+              upperInclusive = true;
             
                 qLower = new FieldQueryNode(field, 
                                          EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
@@ -375,9 +375,9 @@
                                          "*", term.beginColumn, term.endColumn);
                 break;
             case OP_MORETHANEQ:
-            	lowerInclusive = true;
-            	upperInclusive = true;
-            	
+              lowerInclusive = true;
+              upperInclusive = true;
+
                 qLower = new FieldQueryNode(field, 
                                          EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn);
                 qUpper = new FieldQueryNode(field, 
@@ -401,19 +401,19 @@
 )
     {
       if (boost != null) {
-		  float f = (float)1.0;
-		  try {
-		    f = Float.valueOf(boost.image).floatValue();
-		    // avoid boosting null queries, such as those caused by stop words
-	      	if (q != null) {
-	        	q = new BoostQueryNode(q, f);
-	      	}
-		  } catch (Exception ignored) {
-		  	/* Should this be handled somehow? (defaults to "no boost", if
+      float f = (float)1.0;
+      try {
+        f = Float.valueOf(boost.image).floatValue();
+        // avoid boosting null queries, such as those caused by stop words
+          if (q != null) {
+            q = new BoostQueryNode(q, f);
+          }
+      } catch (Exception ignored) {
+        /* Should this be handled somehow? (defaults to "no boost", if
              * boost number is invalid)
-             */		  
-		  }
+             */
       }
+      }
       if (group) { q = new GroupQueryNode(q);}
       return q;
     }
@@ -433,7 +433,7 @@
 {
   (
      (
- 	   term=<TERM> { q = new FieldQueryNode(field, EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn); }
+      term=<TERM> { q = new FieldQueryNode(field, EscapeQuerySyntaxImpl.discardEscapeChar(term.image), term.beginColumn, term.endColumn); }
        | term=<REGEXPTERM> { regexp=true; }
        | term=<NUMBER>
      )
@@ -441,16 +441,16 @@
      [ <CARAT> boost=<NUMBER> [ fuzzySlop=<FUZZY_SLOP> { fuzzy=true; } ] ]
      {
        if (fuzzy) {
-       	  float fms = defaultMinSimilarity;
-       	  try {
+           float fms = defaultMinSimilarity;
+           try {
             fms = Float.valueOf(fuzzySlop.image.substring(1)).floatValue();
-       	  } catch (Exception ignored) { }
-       	 if(fms < 0.0f){
-       	   throw new ParseException(new MessageImpl(QueryParserMessages.INVALID_SYNTAX_FUZZY_LIMITS));
-       	 } else if (fms >= 1.0f && fms != (int) fms) {
-       	   throw new ParseException(new MessageImpl(QueryParserMessages.INVALID_SYNTAX_FUZZY_EDITS));
-       	 }
-       	 q = new FuzzyQueryNode(field, EscapeQuerySyntaxImpl.discardEscapeChar(term.image), fms, term.beginColumn, term.endColumn);
+           } catch (Exception ignored) { }
+          if(fms < 0.0f){
+            throw new ParseException(new MessageImpl(QueryParserMessages.INVALID_SYNTAX_FUZZY_LIMITS));
+          } else if (fms >= 1.0f && fms != (int) fms) {
+            throw new ParseException(new MessageImpl(QueryParserMessages.INVALID_SYNTAX_FUZZY_EDITS));
+          }
+          q = new FuzzyQueryNode(field, EscapeQuerySyntaxImpl.discardEscapeChar(term.image), fms, term.beginColumn, term.endColumn);
        } else if (regexp) {
          String re = term.image.substring(1, term.image.length()-1);
          q = new RegexpQueryNode(field, re, 0, re.length());
@@ -471,9 +471,9 @@
           }
           
           qLower = new FieldQueryNode(field, 
-		                               EscapeQuerySyntaxImpl.discardEscapeChar(goop1.image), goop1.beginColumn, goop1.endColumn);
-		  qUpper = new FieldQueryNode(field,  
-		                               EscapeQuerySyntaxImpl.discardEscapeChar(goop2.image), goop2.beginColumn, goop2.endColumn);
+                                   EscapeQuerySyntaxImpl.discardEscapeChar(goop1.image), goop1.beginColumn, goop1.endColumn);
+      qUpper = new FieldQueryNode(field,
+                                   EscapeQuerySyntaxImpl.discardEscapeChar(goop2.image), goop2.beginColumn, goop2.endColumn);
           q = new TermRangeQueryNode(qLower, qUpper, startInc ? true : false, endInc ? true : false);
         }
      | term=<QUOTED> {q = new QuotedFieldQueryNode(field, EscapeQuerySyntaxImpl.discardEscapeChar(term.image.substring(1, term.image.length()-1)), term.beginColumn + 1, term.endColumn - 1);}
@@ -489,28 +489,28 @@
            }
            catch (Exception ignored) {
             /* Should this be handled somehow? (defaults to "no PhraseSlop", if
-	         * slop number is invalid)
-	         */		
+           * slop number is invalid)
+           */
            }
          }
               
        }
   )
   {
-	  if (boost != null) {
-		  float f = (float)1.0;
-		  try {
-		    f = Float.valueOf(boost.image).floatValue();
-		    // avoid boosting null queries, such as those caused by stop words
-	      	if (q != null) {
-	        	q = new BoostQueryNode(q, f);
-	      	}
-		  } catch (Exception ignored) {
-		  	/* Should this be handled somehow? (defaults to "no boost", if
-	         * boost number is invalid)
-	         */		  
-		  }
-	  }
+    if (boost != null) {
+      float f = (float)1.0;
+      try {
+        f = Float.valueOf(boost.image).floatValue();
+        // avoid boosting null queries, such as those caused by stop words
+          if (q != null) {
+            q = new BoostQueryNode(q, f);
+          }
+      } catch (Exception ignored) {
+        /* Should this be handled somehow? (defaults to "no boost", if
+           * boost number is invalid)
+           */
+      }
+    }
       return q;
   }
 }
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/FastCharStream.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/FastCharStream.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/FastCharStream.java	(working copy)
@@ -29,13 +29,13 @@
 public final class FastCharStream implements CharStream {
   char[] buffer = null;
 
-  int bufferLength = 0;				  // end of valid chars
-  int bufferPosition = 0;			  // next char to read
+  int bufferLength = 0;          // end of valid chars
+  int bufferPosition = 0;        // next char to read
 
-  int tokenStart = 0;				  // offset in buffer
-  int bufferStart = 0;				  // position in file of buffer
+  int tokenStart = 0;          // offset in buffer
+  int bufferStart = 0;          // position in file of buffer
 
-  Reader input;					  // source of chars
+  Reader input;            // source of chars
 
   /** Constructs from a Reader. */
   public FastCharStream(Reader r) {
@@ -51,24 +51,24 @@
   private final void refill() throws IOException {
     int newPosition = bufferLength - tokenStart;
 
-    if (tokenStart == 0) {			  // token won't fit in buffer
-      if (buffer == null) {			  // first time: alloc buffer
-	buffer = new char[2048];
+    if (tokenStart == 0) {        // token won't fit in buffer
+      if (buffer == null) {        // first time: alloc buffer
+        buffer = new char[2048];
       } else if (bufferLength == buffer.length) { // grow buffer
-	char[] newBuffer = new char[buffer.length*2];
-	System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
-	buffer = newBuffer;
+        char[] newBuffer = new char[buffer.length * 2];
+        System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
+        buffer = newBuffer;
       }
-    } else {					  // shift token to front
+    } else {            // shift token to front
       System.arraycopy(buffer, tokenStart, buffer, 0, newPosition);
     }
 
-    bufferLength = newPosition;			  // update state
+    bufferLength = newPosition;        // update state
     bufferPosition = newPosition;
     bufferStart += tokenStart;
     tokenStart = 0;
 
-    int charsRead =				  // fill space in buffer
+    int charsRead =          // fill space in buffer
       input.read(buffer, newPosition, buffer.length-newPosition);
     if (charsRead == -1)
       throw new IOException("read past eof");
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/FieldQueryNode.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/FieldQueryNode.java	(revision 1386643)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/nodes/FieldQueryNode.java	(working copy)
@@ -179,12 +179,12 @@
 
   }
 
-	public CharSequence getValue() {
-		return getText();
-	}
+  public CharSequence getValue() {
+    return getText();
+  }
 
-	public void setValue(CharSequence value) {
-		setText(value);
-	}
+  public void setValue(CharSequence value) {
+    setText(value);
+  }
 
 }
Index: lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java
===================================================================
--- lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java	(revision 1386643)
+++ lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java	(working copy)
@@ -356,7 +356,7 @@
     // only possible writer, and it is "synchronized" to avoid this case).
     DirectoryReader r2 = DirectoryReader.openIfChanged(indexReader);
     if (r2 == null) {
-    	return false; // no changes, nothing to do
+      return false; // no changes, nothing to do
     } 
     
     // validate that a refresh is valid at this point, i.e. that the taxonomy 
@@ -364,13 +364,13 @@
     String t1 = indexReader.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_CREATE_TIME);
     String t2 = r2.getIndexCommit().getUserData().get(DirectoryTaxonomyWriter.INDEX_CREATE_TIME);
     if (t1==null) {
-    	if (t2!=null) {
-    		r2.close();
-    		throw new InconsistentTaxonomyException("Taxonomy was recreated at: "+t2);
-    	}
+      if (t2!=null) {
+        r2.close();
+        throw new InconsistentTaxonomyException("Taxonomy was recreated at: "+t2);
+      }
     } else if (!t1.equals(t2)) {
-    	r2.close();
-    	throw new InconsistentTaxonomyException("Taxonomy was recreated at: "+t2+"  !=  "+t1);
+      r2.close();
+      throw new InconsistentTaxonomyException("Taxonomy was recreated at: "+t2+"  !=  "+t1);
     }
     
       IndexReader oldreader = indexReader;
Index: lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadNearQuery.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadNearQuery.java	(revision 1386643)
+++ lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadNearQuery.java	(working copy)
@@ -140,7 +140,7 @@
 
     query = newPhraseQuery("field", "twenty two", true, new AveragePayloadFunction());
     QueryUtils.check(query);
-		
+
     // all 10 hits should have score = 3 because adjacent terms have payloads of 2,4
     // and all the similarity factors are set to 1
     hits = searcher.search(query, null, 100);
@@ -162,8 +162,8 @@
       assertEquals("should be 100 hits", 100, hits.totalHits);
       for (int j = 0; j < hits.scoreDocs.length; j++) {
         ScoreDoc doc = hits.scoreDocs[j];
-        //				System.out.println("Doc: " + doc.toString());
-        //				System.out.println("Explain: " + searcher.explain(query, doc.doc));
+        //        System.out.println("Doc: " + doc.toString());
+        //        System.out.println("Explain: " + searcher.explain(query, doc.doc));
         assertTrue(doc.score + " does not equal: " + 3, doc.score == 3);
       }
     }
@@ -192,71 +192,71 @@
   }
   
   public void testAverageFunction() throws IOException {
-	  PayloadNearQuery query;
-	  TopDocs hits;
+    PayloadNearQuery query;
+    TopDocs hits;
 
-	  query = newPhraseQuery("field", "twenty two", true, new AveragePayloadFunction());
-	  QueryUtils.check(query);
-	  // all 10 hits should have score = 3 because adjacent terms have payloads of 2,4
-	  // and all the similarity factors are set to 1
-	  hits = searcher.search(query, null, 100);
-	  assertTrue("hits is null and it shouldn't be", hits != null);
-	  assertTrue("should be 10 hits", hits.totalHits == 10);
-	  for (int j = 0; j < hits.scoreDocs.length; j++) {
-		  ScoreDoc doc = hits.scoreDocs[j];
-		  assertTrue(doc.score + " does not equal: " + 3, doc.score == 3);
-		  Explanation explain = searcher.explain(query, hits.scoreDocs[j].doc);
-		  String exp = explain.toString();
-		  assertTrue(exp, exp.indexOf("AveragePayloadFunction") > -1);
-		  assertTrue(hits.scoreDocs[j].score + " explain value does not equal: " + 3, explain.getValue() == 3f);
-	  }
+    query = newPhraseQuery("field", "twenty two", true, new AveragePayloadFunction());
+    QueryUtils.check(query);
+    // all 10 hits should have score = 3 because adjacent terms have payloads of 2,4
+    // and all the similarity factors are set to 1
+    hits = searcher.search(query, null, 100);
+    assertTrue("hits is null and it shouldn't be", hits != null);
+    assertTrue("should be 10 hits", hits.totalHits == 10);
+    for (int j = 0; j < hits.scoreDocs.length; j++) {
+      ScoreDoc doc = hits.scoreDocs[j];
+      assertTrue(doc.score + " does not equal: " + 3, doc.score == 3);
+      Explanation explain = searcher.explain(query, hits.scoreDocs[j].doc);
+      String exp = explain.toString();
+      assertTrue(exp, exp.indexOf("AveragePayloadFunction") > -1);
+      assertTrue(hits.scoreDocs[j].score + " explain value does not equal: " + 3, explain.getValue() == 3f);
+    }
   }
   public void testMaxFunction() throws IOException {
-	  PayloadNearQuery query;
-	  TopDocs hits;
+    PayloadNearQuery query;
+    TopDocs hits;
 
-	  query = newPhraseQuery("field", "twenty two", true, new MaxPayloadFunction());
-	  QueryUtils.check(query);
-	  // all 10 hits should have score = 4 (max payload value)
-	  hits = searcher.search(query, null, 100);
-	  assertTrue("hits is null and it shouldn't be", hits != null);
-	  assertTrue("should be 10 hits", hits.totalHits == 10);
-	  for (int j = 0; j < hits.scoreDocs.length; j++) {
-		  ScoreDoc doc = hits.scoreDocs[j];
-		  assertTrue(doc.score + " does not equal: " + 4, doc.score == 4);
-		  Explanation explain = searcher.explain(query, hits.scoreDocs[j].doc);
-		  String exp = explain.toString();
-		  assertTrue(exp, exp.indexOf("MaxPayloadFunction") > -1);
-		  assertTrue(hits.scoreDocs[j].score + " explain value does not equal: " + 4, explain.getValue() == 4f);
-	  }
+    query = newPhraseQuery("field", "twenty two", true, new MaxPayloadFunction());
+    QueryUtils.check(query);
+    // all 10 hits should have score = 4 (max payload value)
+    hits = searcher.search(query, null, 100);
+    assertTrue("hits is null and it shouldn't be", hits != null);
+    assertTrue("should be 10 hits", hits.totalHits == 10);
+    for (int j = 0; j < hits.scoreDocs.length; j++) {
+      ScoreDoc doc = hits.scoreDocs[j];
+      assertTrue(doc.score + " does not equal: " + 4, doc.score == 4);
+      Explanation explain = searcher.explain(query, hits.scoreDocs[j].doc);
+      String exp = explain.toString();
+      assertTrue(exp, exp.indexOf("MaxPayloadFunction") > -1);
+      assertTrue(hits.scoreDocs[j].score + " explain value does not equal: " + 4, explain.getValue() == 4f);
+    }
   }  
   public void testMinFunction() throws IOException {
-	  PayloadNearQuery query;
-	  TopDocs hits;
+    PayloadNearQuery query;
+    TopDocs hits;
 
-	  query = newPhraseQuery("field", "twenty two", true, new MinPayloadFunction());
-	  QueryUtils.check(query);
-	  // all 10 hits should have score = 2 (min payload value)
-	  hits = searcher.search(query, null, 100);
-	  assertTrue("hits is null and it shouldn't be", hits != null);
-	  assertTrue("should be 10 hits", hits.totalHits == 10);
-	  for (int j = 0; j < hits.scoreDocs.length; j++) {
-		  ScoreDoc doc = hits.scoreDocs[j];
-		  assertTrue(doc.score + " does not equal: " + 2, doc.score == 2);
-		  Explanation explain = searcher.explain(query, hits.scoreDocs[j].doc);
-		  String exp = explain.toString();
-		  assertTrue(exp, exp.indexOf("MinPayloadFunction") > -1);
-		  assertTrue(hits.scoreDocs[j].score + " explain value does not equal: " + 2, explain.getValue() == 2f);
-	  }
+    query = newPhraseQuery("field", "twenty two", true, new MinPayloadFunction());
+    QueryUtils.check(query);
+    // all 10 hits should have score = 2 (min payload value)
+    hits = searcher.search(query, null, 100);
+    assertTrue("hits is null and it shouldn't be", hits != null);
+    assertTrue("should be 10 hits", hits.totalHits == 10);
+    for (int j = 0; j < hits.scoreDocs.length; j++) {
+      ScoreDoc doc = hits.scoreDocs[j];
+      assertTrue(doc.score + " does not equal: " + 2, doc.score == 2);
+      Explanation explain = searcher.explain(query, hits.scoreDocs[j].doc);
+      String exp = explain.toString();
+      assertTrue(exp, exp.indexOf("MinPayloadFunction") > -1);
+      assertTrue(hits.scoreDocs[j].score + " explain value does not equal: " + 2, explain.getValue() == 2f);
+    }
   }  
   private SpanQuery[] getClauses() {
-	    SpanNearQuery q1, q2;
-	    q1 = spanNearQuery("field2", "twenty two");
-	    q2 = spanNearQuery("field2", "twenty three");
-	    SpanQuery[] clauses = new SpanQuery[2];
-	    clauses[0] = q1;
-	    clauses[1] = q2;
-	    return clauses;
+      SpanNearQuery q1, q2;
+      q1 = spanNearQuery("field2", "twenty two");
+      q2 = spanNearQuery("field2", "twenty three");
+      SpanQuery[] clauses = new SpanQuery[2];
+      clauses[0] = q1;
+      clauses[1] = q2;
+      return clauses;
   }
   private SpanNearQuery spanNearQuery(String fieldName, String words) {
     String[] wordList = words.split("[\\s]+");
@@ -274,8 +274,8 @@
     hits = searcher.search(query, null, 100);
     assertTrue("hits is null and it shouldn't be", hits != null);
     ScoreDoc doc = hits.scoreDocs[0];
-    //		System.out.println("Doc: " + doc.toString());
-    //		System.out.println("Explain: " + searcher.explain(query, doc.doc));
+    //    System.out.println("Doc: " + doc.toString());
+    //    System.out.println("Explain: " + searcher.explain(query, doc.doc));
     assertTrue("there should only be one hit", hits.totalHits == 1);
     // should have score = 3 because adjacent terms have payloads of 2,4
     assertTrue(doc.score + " does not equal: " + 3, doc.score == 3); 
@@ -299,8 +299,8 @@
     assertTrue("should only be one hit", hits.scoreDocs.length == 1);
     // the score should be 3 - the average of all the underlying payloads
     ScoreDoc doc = hits.scoreDocs[0];
-    //		System.out.println("Doc: " + doc.toString());
-    //		System.out.println("Explain: " + searcher.explain(query, doc.doc));
+    //    System.out.println("Doc: " + doc.toString());
+    //    System.out.println("Explain: " + searcher.explain(query, doc.doc));
     assertTrue(doc.score + " does not equal: " + 3, doc.score == 3);  
   }
 
Index: lucene/core/src/test/org/apache/lucene/search/TestDocIdSet.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestDocIdSet.java	(revision 1386643)
+++ lucene/core/src/test/org/apache/lucene/search/TestDocIdSet.java	(working copy)
@@ -62,15 +62,15 @@
           };
         } 
       };
-	  
-		
+
+
     DocIdSet filteredSet = new FilteredDocIdSet(innerSet){
         @Override
         protected boolean match(int docid) {
           return docid%2 == 0;  //validate only even docids
-        }	
+        }
       };
-	  
+
     DocIdSetIterator iter = filteredSet.iterator();
     ArrayList<Integer> list = new ArrayList<Integer>();
     int doc = iter.advance(3);
@@ -80,7 +80,7 @@
         list.add(Integer.valueOf(doc));
       }
     }
-	  
+
     int[] docs = new int[list.size()];
     int c=0;
     Iterator<Integer> intIter = list.iterator();
@@ -151,7 +151,7 @@
           @Override
           protected boolean match(int docid) {
             return true;
-          }	
+          }
         };
       }
     };
Index: lucene/core/src/test/org/apache/lucene/search/spans/TestBasics.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/spans/TestBasics.java	(revision 1386643)
+++ lucene/core/src/test/org/apache/lucene/search/spans/TestBasics.java	(working copy)
@@ -582,21 +582,21 @@
   
   @Test
   public void testSpansSkipTo() throws Exception {
-	  SpanTermQuery t1 = new SpanTermQuery(new Term("field", "seventy"));
-	  SpanTermQuery t2 = new SpanTermQuery(new Term("field", "seventy"));
-	  Spans s1 = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), t1);
-	  Spans s2 = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), t2);
-	  
-	  assertTrue(s1.next());
-	  assertTrue(s2.next());
-	  
-	  boolean hasMore = true;
-	  
-	  do {
-		  hasMore = skipToAccoringToJavaDocs(s1, s1.doc());
-		  assertEquals(hasMore, s2.skipTo(s2.doc()));
-		  assertEquals(s1.doc(), s2.doc());
-	  } while (hasMore);
+    SpanTermQuery t1 = new SpanTermQuery(new Term("field", "seventy"));
+    SpanTermQuery t2 = new SpanTermQuery(new Term("field", "seventy"));
+    Spans s1 = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), t1);
+    Spans s2 = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), t2);
+
+    assertTrue(s1.next());
+    assertTrue(s2.next());
+
+    boolean hasMore = true;
+
+    do {
+      hasMore = skipToAccoringToJavaDocs(s1, s1.doc());
+      assertEquals(hasMore, s2.skipTo(s2.doc()));
+      assertEquals(s1.doc(), s2.doc());
+    } while (hasMore);
   }
 
   /** Skips to the first match beyond the current, whose document number is
Index: lucene/core/src/test/org/apache/lucene/search/TestCachingCollector.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestCachingCollector.java	(revision 1386643)
+++ lucene/core/src/test/org/apache/lucene/search/TestCachingCollector.java	(working copy)
@@ -145,7 +145,7 @@
     try {
       cc.replay(new NoOpCollector(false)); // this call should fail
       fail("should have failed if an in-order Collector was given to replay(), " +
-      		"while CachingCollector was initialized with out-of-order collection");
+           "while CachingCollector was initialized with out-of-order collection");
     } catch (IllegalArgumentException e) {
       // ok
     }
Index: lucene/core/src/test/org/apache/lucene/search/TestFuzzyQuery.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestFuzzyQuery.java	(revision 1386643)
+++ lucene/core/src/test/org/apache/lucene/search/TestFuzzyQuery.java	(working copy)
@@ -101,7 +101,7 @@
     }
 
     // not similar enough:
-    query = new FuzzyQuery(new Term("field", "xxxxx"), FuzzyQuery.defaultMaxEdits, 0);  	
+    query = new FuzzyQuery(new Term("field", "xxxxx"), FuzzyQuery.defaultMaxEdits, 0);
     hits = searcher.search(query, null, 1000).scoreDocs;
     assertEquals(0, hits.length);
     query = new FuzzyQuery(new Term("field", "aaccc"), FuzzyQuery.defaultMaxEdits, 0);   // edit distance to "aaaaa" = 3
Index: lucene/core/src/test/org/apache/lucene/index/TestTransactions.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestTransactions.java	(revision 1386643)
+++ lucene/core/src/test/org/apache/lucene/index/TestTransactions.java	(working copy)
@@ -129,7 +129,7 @@
           }
           try {
             writer2.prepareCommit();
-          } catch (Throwable t) { 	
+          } catch (Throwable t) {
             writer1.rollback();
             writer2.rollback();
             return;
Index: lucene/core/src/test/org/apache/lucene/index/TestTransactionRollback.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestTransactionRollback.java	(revision 1386643)
+++ lucene/core/src/test/org/apache/lucene/index/TestTransactionRollback.java	(working copy)
@@ -41,13 +41,13 @@
  */
 
 public class TestTransactionRollback extends LuceneTestCase {
-	
+
   private static final String FIELD_RECORD_ID = "record_id";
   private Directory dir;
-	
+
   //Rolls back index to a chosen ID
   private void rollBackLast(int id) throws Exception {
-		
+
     // System.out.println("Attempting to rollback to "+id);
     String ids="-"+id;
     IndexCommit last=null;
@@ -62,7 +62,7 @@
 
     if (last==null)
       throw new RuntimeException("Couldn't find commit point "+id);
-		
+
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random())).setIndexDeletionPolicy(
         new RollbackDeletionPolicy(id)).setIndexCommit(last));
@@ -72,22 +72,22 @@
     w.close();
   }
 
-  public void testRepeatedRollBacks() throws Exception {		
+  public void testRepeatedRollBacks() throws Exception {
 
     int expectedLastRecordId=100;
     while (expectedLastRecordId>10) {
-      expectedLastRecordId -=10;			
+      expectedLastRecordId -=10;
       rollBackLast(expectedLastRecordId);
       
       BitSet expecteds = new BitSet(100);
       expecteds.set(1,(expectedLastRecordId+1),true);
-      checkExpecteds(expecteds);			
+      checkExpecteds(expecteds);
     }
   }
-	
+
   private void checkExpecteds(BitSet expecteds) throws Exception {
     IndexReader r = DirectoryReader.open(dir);
-		
+
     //Perhaps not the most efficient approach but meets our
     //needs here.
     final Bits liveDocs = MultiFields.getLiveDocs(r);
@@ -114,7 +114,7 @@
       Collection files = comm.getFileNames();
       for (Iterator iterator2 = files.iterator(); iterator2.hasNext();) {
         String filename = (String) iterator2.next();
-        System.out.print(filename+", ");				
+        System.out.print(filename+", ");
       }
       System.out.println();
     }
@@ -133,7 +133,7 @@
       Document doc=new Document();
       doc.add(newTextField(FIELD_RECORD_ID, ""+currentRecordId, Field.Store.YES));
       w.addDocument(doc);
-			
+
       if (currentRecordId%10 == 0) {
         Map<String,String> data = new HashMap<String,String>();
         data.put("index", "records 1-"+currentRecordId);
@@ -177,16 +177,16 @@
                              " UserData="+commit.getUserData() +")  ("+(commits.size()-1)+" commit points left) files=");
             Collection files = commit.getFileNames();
             for (Iterator iterator2 = files.iterator(); iterator2.hasNext();) {
-              System.out.print(" "+iterator2.next());				
+              System.out.print(" "+iterator2.next());
             }
             System.out.println();
             */
-						
-            commit.delete();									
+
+            commit.delete();
           }
         }
       }
-    }		
+    }
   }
 
   class DeleteLastCommitPolicy implements IndexDeletionPolicy {
@@ -198,7 +198,7 @@
     }
   }
 
-  public void testRollbackDeletionPolicy() throws Exception {		
+  public void testRollbackDeletionPolicy() throws Exception {
     for(int i=0;i<2;i++) {
       // Unless you specify a prior commit point, rollback
       // should not work:
@@ -209,7 +209,7 @@
       r.close();
     }
   }
-	
+
   // Keeps all commit points (used to build index)
   class KeepAllDeletionPolicy implements IndexDeletionPolicy {
     public void onCommit(List<? extends IndexCommit> commits) throws IOException {}
Index: lucene/core/src/test/org/apache/lucene/index/TestSameTokenSamePosition.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestSameTokenSamePosition.java	(revision 1386643)
+++ lucene/core/src/test/org/apache/lucene/index/TestSameTokenSamePosition.java	(working copy)
@@ -78,7 +78,7 @@
       offsetAtt.setOffset(starts[nextTokenIndex], ends[nextTokenIndex]);
       posIncAtt.setPositionIncrement(incs[nextTokenIndex]);
       nextTokenIndex++;
-      return true;			
+      return true;
     } else {
       return false;
     }
Index: lucene/core/src/test/org/apache/lucene/index/TestLongPostings.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestLongPostings.java	(revision 1386643)
+++ lucene/core/src/test/org/apache/lucene/index/TestLongPostings.java	(working copy)
@@ -111,27 +111,27 @@
     }
 
     final IndexReader r;
-	  final IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))
-	    .setOpenMode(IndexWriterConfig.OpenMode.CREATE)
-	    .setMergePolicy(newLogMergePolicy());
-	  iwc.setRAMBufferSizeMB(16.0 + 16.0 * random().nextDouble());
-	  iwc.setMaxBufferedDocs(-1);
-	  final RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);
-	
-	  for(int idx=0;idx<NUM_DOCS;idx++) {
-	    final Document doc = new Document();
-	    String s = isS1.get(idx) ? s1 : s2;
-	    final Field f = newTextField("field", s, Field.Store.NO);
-	    final int count = _TestUtil.nextInt(random(), 1, 4);
-	    for(int ct=0;ct<count;ct++) {
-	      doc.add(f);
-	    }
-	    riw.addDocument(doc);
-	  }
-	
-	  r = riw.getReader();
-	  riw.close();
+    final IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))
+      .setOpenMode(IndexWriterConfig.OpenMode.CREATE)
+      .setMergePolicy(newLogMergePolicy());
+    iwc.setRAMBufferSizeMB(16.0 + 16.0 * random().nextDouble());
+    iwc.setMaxBufferedDocs(-1);
+    final RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);
 
+    for(int idx=0;idx<NUM_DOCS;idx++) {
+      final Document doc = new Document();
+      String s = isS1.get(idx) ? s1 : s2;
+      final Field f = newTextField("field", s, Field.Store.NO);
+      final int count = _TestUtil.nextInt(random(), 1, 4);
+      for(int ct=0;ct<count;ct++) {
+        doc.add(f);
+      }
+      riw.addDocument(doc);
+    }
+
+    r = riw.getReader();
+    riw.close();
+
     /*
     if (VERBOSE) {
       System.out.println("TEST: terms");
Index: lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java	(revision 1386643)
+++ lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java	(working copy)
@@ -152,7 +152,7 @@
 
     rd1.close();
     rd2.close();
-		
+
     iwOut.forceMerge(1);
     iwOut.close();
     
Index: lucene/core/src/test/org/apache/lucene/store/TestWindowsMMap.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/store/TestWindowsMMap.java	(revision 1386643)
+++ lucene/core/src/test/org/apache/lucene/store/TestWindowsMMap.java	(working copy)
@@ -84,7 +84,7 @@
     for(int dx = 0; dx < num; dx ++) {
       String f = randomField();
       Document doc = new Document();
-      doc.add(newTextField("data", f, Field.Store.YES));	
+      doc.add(newTextField("data", f, Field.Store.YES));  
       writer.addDocument(doc);
     }
     
Index: lucene/core/src/test/org/apache/lucene/util/TestFixedBitSet.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/util/TestFixedBitSet.java	(revision 1386643)
+++ lucene/core/src/test/org/apache/lucene/util/TestFixedBitSet.java	(working copy)
@@ -49,7 +49,7 @@
       // aa = a.prevSetBit(aa-1);
       aa--;
       while ((aa >= 0) && (! a.get(aa))) {
-      	aa--;
+        aa--;
       }
       if (b.length() == 0) {
         bb = -1;
Index: lucene/core/src/test/org/apache/lucene/util/TestBitUtil.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/util/TestBitUtil.java	(revision 1386643)
+++ lucene/core/src/test/org/apache/lucene/util/TestBitUtil.java	(working copy)
@@ -71,12 +71,12 @@
     long sumRes = 0;
     while (iters-- >= 0) {
       for (int i = 1; i <= 63; i++) {
-      	long a = testArg(i);
-	sumRes += BitUtil.nlz(a);
-	sumRes += BitUtil.nlz(a+1);
-	sumRes += BitUtil.nlz(a-1);
-	sumRes += BitUtil.nlz(a+10);
-	sumRes += BitUtil.nlz(a-10);
+        long a = testArg(i);
+        sumRes += BitUtil.nlz(a);
+        sumRes += BitUtil.nlz(a + 1);
+        sumRes += BitUtil.nlz(a - 1);
+        sumRes += BitUtil.nlz(a + 10);
+        sumRes += BitUtil.nlz(a - 10);
       }
     }
     return sumRes;
@@ -86,12 +86,12 @@
     long sumRes = 0;
     while (iters-- >= 0) {
       for (int i = 1; i <= 63; i++) {
-      	long a = testArg(i);
-	sumRes += Long.numberOfLeadingZeros(a);
-	sumRes += Long.numberOfLeadingZeros(a+1);
-	sumRes += Long.numberOfLeadingZeros(a-1);
-	sumRes += Long.numberOfLeadingZeros(a+10);
-	sumRes += Long.numberOfLeadingZeros(a-10);
+        long a = testArg(i);
+        sumRes += Long.numberOfLeadingZeros(a);
+        sumRes += Long.numberOfLeadingZeros(a + 1);
+        sumRes += Long.numberOfLeadingZeros(a - 1);
+        sumRes += Long.numberOfLeadingZeros(a + 10);
+        sumRes += Long.numberOfLeadingZeros(a - 10);
       }
     }
     return sumRes;
Index: lucene/core/src/test/org/apache/lucene/util/TestOpenBitSet.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/util/TestOpenBitSet.java	(revision 1386643)
+++ lucene/core/src/test/org/apache/lucene/util/TestOpenBitSet.java	(working copy)
@@ -71,7 +71,7 @@
       // aa = a.prevSetBit(aa-1);
       aa--;
       while ((aa >= 0) && (! a.get(aa))) {
-      	aa--;
+        aa--;
       }
       bb = b.prevSetBit(bb-1);
       assertEquals(aa,bb);
@@ -85,7 +85,7 @@
       // aa = a.prevSetBit(aa-1);
       aa--;
       while ((aa >= 0) && (! a.get(aa))) {
-      	aa--;
+        aa--;
       }
       bb = (int) b.prevSetBit((long) (bb-1));
       assertEquals(aa,bb);
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/BitVector.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/BitVector.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/BitVector.java	(working copy)
@@ -163,7 +163,7 @@
       int c = 0;
       int end = bits.length;
       for (int i = 0; i < end; i++) {
-        c += BYTE_COUNTS[bits[i] & 0xFF];	  // sum bits per byte
+        c += BYTE_COUNTS[bits[i] & 0xFF];  // sum bits per byte
       }
       count = c;
     }
@@ -176,12 +176,12 @@
     int c = 0;
     int end = bits.length;
     for (int i = 0; i < end; i++) {
-      c += BYTE_COUNTS[bits[i] & 0xFF];	  // sum bits per byte
+      c += BYTE_COUNTS[bits[i] & 0xFF];  // sum bits per byte
     }
     return c;
   }
 
-  private static final byte[] BYTE_COUNTS = {	  // table of bits/byte
+  private static final byte[] BYTE_COUNTS = {  // table of bits/byte
     0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,
     1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5,
     1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5,
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene3x/TermBuffer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene3x/TermBuffer.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene3x/TermBuffer.java	(working copy)
@@ -46,7 +46,7 @@
   int newSuffixStart;                             // only valid right after .read is called
 
   public int compareTo(TermBuffer other) {
-    if (field == other.field) 	  // fields are interned
+    if (field == other.field)     // fields are interned
                                   // (only by PreFlex codec)
       return utf8AsUTF16Comparator.compare(bytes, other.bytes);
     else
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermEnum.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermEnum.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermEnum.java	(working copy)
@@ -141,15 +141,15 @@
     termBuffer.read(input, fieldInfos);
     newSuffixStart = termBuffer.newSuffixStart;
 
-    termInfo.docFreq = input.readVInt();	  // read doc freq
-    termInfo.freqPointer += input.readVLong();	  // read freq pointer
-    termInfo.proxPointer += input.readVLong();	  // read prox pointer
+    termInfo.docFreq = input.readVInt();    // read doc freq
+    termInfo.freqPointer += input.readVLong();    // read freq pointer
+    termInfo.proxPointer += input.readVLong();    // read prox pointer
     
     if (termInfo.docFreq >= skipInterval) 
       termInfo.skipOffset = input.readVInt();
 
     if (isIndex)
-      indexPointer += input.readVLong();	  // read index pointer
+      indexPointer += input.readVLong();    // read index pointer
 
     //System.out.println("  ste ret term=" + term());
     return true;
Index: lucene/core/src/java/org/apache/lucene/search/FieldCache.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/FieldCache.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/search/FieldCache.java	(working copy)
@@ -508,7 +508,7 @@
       // this special case is the reason that Arrays.binarySearch() isn't useful.
       if (key == null)
         return 0;
-	  
+  
       int low = 1;
       int high = numOrd()-1;
 
Index: lucene/core/src/java/org/apache/lucene/search/payloads/PayloadFunction.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/payloads/PayloadFunction.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/search/payloads/PayloadFunction.java	(working copy)
@@ -56,10 +56,10 @@
   public abstract float docScore(int docId, String field, int numPayloadsSeen, float payloadScore);
   
   public Explanation explain(int docId, String field, int numPayloadsSeen, float payloadScore){
-	  Explanation result = new Explanation();
-	  result.setDescription(getClass().getSimpleName() + ".docScore()");
-	  result.setValue(docScore(docId, field, numPayloadsSeen, payloadScore));
-	  return result;
+    Explanation result = new Explanation();
+    result.setDescription(getClass().getSimpleName() + ".docScore()");
+    result.setValue(docScore(docId, field, numPayloadsSeen, payloadScore));
+    return result;
   };
   
   @Override
Index: lucene/core/src/java/org/apache/lucene/search/payloads/MinPayloadFunction.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/payloads/MinPayloadFunction.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/search/payloads/MinPayloadFunction.java	(working copy)
@@ -24,12 +24,12 @@
 public class MinPayloadFunction extends PayloadFunction {
 
   @Override
-	public float currentScore(int docId, String field, int start, int end, int numPayloadsSeen, float currentScore, float currentPayloadScore) {
+  public float currentScore(int docId, String field, int start, int end, int numPayloadsSeen, float currentScore, float currentPayloadScore) {
     if (numPayloadsSeen == 0) {
       return currentPayloadScore;
     } else {
-		return Math.min(currentPayloadScore, currentScore);
-	}
+      return Math.min(currentPayloadScore, currentScore);
+    }
   }
 
   @Override
Index: lucene/core/src/java/org/apache/lucene/search/payloads/PayloadNearQuery.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/payloads/PayloadNearQuery.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/search/payloads/PayloadNearQuery.java	(working copy)
@@ -257,7 +257,7 @@
             getPayloads(spansArr);            
             more = spans.next();
           } while (more && (doc == spans.doc()));
-          return true;    	
+          return true;
     }
 
     @Override
Index: lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java	(working copy)
@@ -117,7 +117,7 @@
   public int end() { return matchEnd; }
   
   public Spans[] getSubSpans() {
-	  return subSpans;
+    return subSpans;
   }  
 
   // TODO: Remove warning after API has been finalized
Index: lucene/core/src/java/org/apache/lucene/search/spans/NearSpansUnordered.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/spans/NearSpansUnordered.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/search/spans/NearSpansUnordered.java	(working copy)
@@ -151,7 +151,7 @@
     }
   }
   public Spans[] getSubSpans() {
-	  return subSpans;
+    return subSpans;
   }
   @Override
   public boolean next() throws IOException {
@@ -286,7 +286,7 @@
   }
 
   private void addToList(SpansCell cell) {
-    if (last != null) {			  // add next to end of list
+    if (last != null) {  // add next to end of list
       last.next = cell;
     } else
       first = cell;
@@ -295,7 +295,7 @@
   }
 
   private void firstToLast() {
-    last.next = first;			  // move first to end of list
+    last.next = first;  // move first to end of list
     last = first;
     first = first.next;
     last.next = null;
Index: lucene/core/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java	(working copy)
@@ -57,7 +57,7 @@
 
   @Override
   public void extractTerms(Set<Term> terms) {
-	    match.extractTerms(terms);
+    match.extractTerms(terms);
   }
 
   /** 
@@ -186,4 +186,4 @@
       }
 
   }
-}
\ No newline at end of file
+}
Index: lucene/core/src/java/org/apache/lucene/search/spans/Spans.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/spans/Spans.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/search/spans/Spans.java	(working copy)
@@ -34,7 +34,7 @@
    *   boolean skipTo(int target) {
    *     do {
    *       if (!next())
-   * 	     return false;
+   *         return false;
    *     } while (target > doc());
    *     return true;
    *   }
Index: lucene/core/src/java/org/apache/lucene/search/spans/SpanNearQuery.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanNearQuery.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanNearQuery.java	(working copy)
@@ -92,9 +92,9 @@
   
   @Override
   public void extractTerms(Set<Term> terms) {
-	    for (final SpanQuery clause : clauses) {
-	      clause.extractTerms(terms);
-	    }
+    for (final SpanQuery clause : clauses) {
+      clause.extractTerms(terms);
+    }
   }  
   
 
Index: lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java	(working copy)
@@ -110,7 +110,7 @@
    * Do not modify the List or its contents.
    */
   public List<Term[]> getTermArrays() {
-	  return Collections.unmodifiableList(termArrays);
+    return Collections.unmodifiableList(termArrays);
   }
 
   /**
Index: lucene/core/src/java/org/apache/lucene/search/FilteredDocIdSetIterator.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/FilteredDocIdSetIterator.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/search/FilteredDocIdSetIterator.java	(working copy)
@@ -28,7 +28,7 @@
 public abstract class FilteredDocIdSetIterator extends DocIdSetIterator {
   protected DocIdSetIterator _innerIter;
   private int doc;
-	
+
   /**
    * Constructor.
    * @param innerIter Underlying DocIdSetIterator.
@@ -40,7 +40,7 @@
     _innerIter = innerIter;
     doc = -1;
   }
-	
+
   /**
    * Validation method to determine whether a docid should be in the result set.
    * @param doc docid to be tested
@@ -48,7 +48,7 @@
    * @see #FilteredDocIdSetIterator(DocIdSetIterator)
    */
   protected abstract boolean match(int doc);
-	
+
   @Override
   public int docID() {
     return doc;
Index: lucene/core/src/java/org/apache/lucene/search/PhrasePositions.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/PhrasePositions.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/search/PhrasePositions.java	(working copy)
@@ -24,13 +24,13 @@
  * Position of a term in a document that takes into account the term offset within the phrase. 
  */
 final class PhrasePositions {
-  int doc;					  // current doc
-  int position;					  // position in doc
-  int count;					  // remaining pos in this doc
-  int offset;					  // position in phrase
+  int doc;              // current doc
+  int position;         // position in doc
+  int count;            // remaining pos in this doc
+  int offset;           // position in phrase
   final int ord;                                  // unique across all PhrasePositions instances
-  final DocsAndPositionsEnum postings;  	  // stream of docs & positions
-  PhrasePositions next;	                          // used to make lists
+  final DocsAndPositionsEnum postings;            // stream of docs & positions
+  PhrasePositions next;                           // used to make lists
   int rptGroup = -1; // >=0 indicates that this is a repeating PP
   int rptInd; // index in the rptGroup
   final Term[] terms; // for repetitions initialization 
@@ -42,7 +42,7 @@
     this.terms = terms;
   }
 
-  final boolean next() throws IOException {	  // increments to next doc
+  final boolean next() throws IOException {  // increments to next doc
     doc = postings.nextDoc();
     if (doc == DocIdSetIterator.NO_MORE_DOCS) {
       return false;
@@ -59,7 +59,7 @@
   }
 
   final void firstPosition() throws IOException {
-    count = postings.freq();				  // read first pos
+    count = postings.freq();  // read first pos
     nextPosition();
   }
 
@@ -70,7 +70,7 @@
    * have exactly the same <code>position</code>.
    */
   final boolean nextPosition() throws IOException {
-    if (count-- > 0) {				  // read subsequent pos's
+    if (count-- > 0) {  // read subsequent pos's
       position = postings.nextPosition() - offset;
       return true;
     } else
Index: lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java	(working copy)
@@ -404,7 +404,7 @@
   public Query rewrite(IndexReader reader) throws IOException {
     if (minNrShouldMatch == 0 && clauses.size() == 1) {                    // optimize 1-clause queries
       BooleanClause c = clauses.get(0);
-      if (!c.isProhibited()) {			  // just return clause
+      if (!c.isProhibited()) {  // just return clause
 
         Query query = c.getQuery().rewrite(reader);    // rewrite first
 
@@ -475,7 +475,7 @@
 
       Query subQuery = c.getQuery();
       if (subQuery != null) {
-        if (subQuery instanceof BooleanQuery) {	  // wrap sub-bools in parens
+        if (subQuery instanceof BooleanQuery) {  // wrap sub-bools in parens
           buffer.append("(");
           buffer.append(subQuery.toString(field));
           buffer.append(")");
Index: lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java	(working copy)
@@ -281,7 +281,7 @@
         ArrayUtil.mergeSort(postingsFreqs);
       }
 
-      if (slop == 0) {				  // optimize exact case
+      if (slop == 0) {  // optimize exact case
         ExactPhraseScorer s = new ExactPhraseScorer(this, postingsFreqs, similarity.exactSimScorer(stats, context));
         if (s.noDocs) {
           return null;
Index: lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.java	(working copy)
@@ -52,7 +52,7 @@
  * @since   lucene 1.4
  */
 class FieldCacheImpl implements FieldCache {
-	
+
   private Map<Class<?>,Cache> caches;
   FieldCacheImpl() {
     init();
@@ -173,7 +173,7 @@
         ((AtomicReader)key).addReaderClosedListener(purgeReader); 
       } else {
         // last chance
-        reader.addReaderClosedListener(purgeReader); 				
+        reader.addReaderClosedListener(purgeReader);
       }
     }
   }
Index: lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java	(working copy)
@@ -43,9 +43,9 @@
   public static final int NO = -1;          // e.g. no norms; no deletes;
   public static final int YES = 1;          // e.g. have norms; have deletes;
 
-  public final String name;				  // unique name in dir
-  private int docCount;				  // number of docs in seg
-  public final Directory dir;				  // where segment resides
+  public final String name;     // unique name in dir
+  private int docCount;         // number of docs in seg
+  public final Directory dir;   // where segment resides
 
   private boolean isCompoundFile;
 
Index: lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/CheckIndex.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/index/CheckIndex.java	(working copy)
@@ -1689,7 +1689,7 @@
                          "              times, to check more than one segment, eg '-segment _2 -segment _a'.\n" +
                          "              You can't use this with the -fix option\n" +
                          "  -dir-impl X: use a specific " + FSDirectory.class.getSimpleName() + " implementation. " +
-                         		"If no package is specified the " + FSDirectory.class.getPackage().getName() + " package will be used.\n" +
+                         "If no package is specified the " + FSDirectory.class.getPackage().getName() + " package will be used.\n" +
                          "\n" +
                          "**WARNING**: -fix should only be used on an emergency basis as it will cause\n" +
                          "documents (perhaps many) to be permanently removed from the index.  Always make\n" +
Index: lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushControl.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushControl.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushControl.java	(working copy)
@@ -396,11 +396,11 @@
     return flushingWriters.size();
   }
   
-  public boolean doApplyAllDeletes() {	
+  public boolean doApplyAllDeletes() {
     return flushDeletes.getAndSet(false);
   }
 
-  public void setApplyAllDeletes() {	
+  public void setApplyAllDeletes() {
     flushDeletes.set(true);
   }
   
Index: lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/IndexWriter.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -2894,7 +2894,7 @@
       final boolean anySegmentFlushed;
       
       synchronized (fullFlushLock) {
-    	boolean flushSuccess = false;
+      boolean flushSuccess = false;
         try {
           anySegmentFlushed = docWriter.flushAllThreads();
           flushSuccess = true;
Index: lucene/core/src/java/org/apache/lucene/index/IndexFileDeleter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/IndexFileDeleter.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/index/IndexFileDeleter.java	(working copy)
@@ -571,7 +571,7 @@
         infoStream.message("IFD", "delete \"" + fileName + "\"");
       }
       directory.deleteFile(fileName);
-    } catch (IOException e) {			  // if delete fails
+    } catch (IOException e) {  // if delete fails
       if (directory.fileExists(fileName)) {
 
         // Some operating systems (e.g. Windows) don't
Index: lucene/core/src/java/org/apache/lucene/store/Lock.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/store/Lock.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/store/Lock.java	(working copy)
@@ -135,7 +135,7 @@
          return doBody();
       } finally {
         if (locked)
-	      lock.release();
+          lock.release();
       }
     }
   }
Index: lucene/core/src/java/org/apache/lucene/store/BufferedIndexInput.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/store/BufferedIndexInput.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/store/BufferedIndexInput.java	(working copy)
@@ -41,9 +41,9 @@
   
   protected byte[] buffer;
   
-  private long bufferStart = 0;			  // position in file of buffer
-  private int bufferLength = 0;			  // end of valid bytes
-  private int bufferPosition = 0;		  // next byte to read
+  private long bufferStart = 0;       // position in file of buffer
+  private int bufferLength = 0;       // end of valid bytes
+  private int bufferPosition = 0;     // next byte to read
 
   @Override
   public final byte readByte() throws IOException {
@@ -259,7 +259,7 @@
   private void refill() throws IOException {
     long start = bufferStart + bufferPosition;
     long end = start + bufferSize;
-    if (end > length())				  // don't read past EOF
+    if (end > length())  // don't read past EOF
       end = length();
     int newLength = (int)(end - start);
     if (newLength <= 0)
@@ -294,7 +294,7 @@
     else {
       bufferStart = pos;
       bufferPosition = 0;
-      bufferLength = 0;				  // trigger refill() on read()
+      bufferLength = 0;  // trigger refill() on read()
       seekInternal(pos);
     }
   }
Index: lucene/core/src/java/org/apache/lucene/util/Constants.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/Constants.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/util/Constants.java	(working copy)
@@ -26,7 +26,7 @@
  **/
 
 public final class Constants {
-  private Constants() {}			  // can't construct
+  private Constants() {}  // can't construct
 
   /** JVM vendor info. */
   public static final String JVM_VENDOR = System.getProperty("java.vm.vendor");
Index: lucene/core/src/java/org/apache/lucene/util/PriorityQueue.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/PriorityQueue.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/util/PriorityQueue.java	(working copy)
@@ -177,11 +177,11 @@
     time. */
   public final T pop() {
     if (size > 0) {
-      T result = heap[1];			  // save first value
-      heap[1] = heap[size];			  // move last to first
-      heap[size] = null;			  // permit GC of objects
+      T result = heap[1];       // save first value
+      heap[1] = heap[size];     // move last to first
+      heap[size] = null;        // permit GC of objects
       size--;
-      downHeap();				  // adjust heap
+      downHeap();               // adjust heap
       return result;
     } else
       return null;
@@ -226,26 +226,26 @@
 
   private final void upHeap() {
     int i = size;
-    T node = heap[i];			  // save bottom node
+    T node = heap[i];          // save bottom node
     int j = i >>> 1;
     while (j > 0 && lessThan(node, heap[j])) {
-      heap[i] = heap[j];			  // shift parents down
+      heap[i] = heap[j];       // shift parents down
       i = j;
       j = j >>> 1;
     }
-    heap[i] = node;				  // install saved node
+    heap[i] = node;            // install saved node
   }
 
   private final void downHeap() {
     int i = 1;
-    T node = heap[i];			  // save top node
-    int j = i << 1;				  // find smaller child
+    T node = heap[i];          // save top node
+    int j = i << 1;            // find smaller child
     int k = j + 1;
     if (k <= size && lessThan(heap[k], heap[j])) {
       j = k;
     }
     while (j <= size && lessThan(heap[j], node)) {
-      heap[i] = heap[j];			  // shift up child
+      heap[i] = heap[j];       // shift up child
       i = j;
       j = i << 1;
       k = j + 1;
@@ -253,7 +253,7 @@
         j = k;
       }
     }
-    heap[i] = node;				  // install saved node
+    heap[i] = node;            // install saved node
   }
   
   /** This method returns the internal heap array as Object[].
Index: lucene/core/src/java/org/apache/lucene/document/Field.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/document/Field.java	(revision 1386643)
+++ lucene/core/src/java/org/apache/lucene/document/Field.java	(working copy)
@@ -691,7 +691,7 @@
       @Override
       public boolean isAnalyzed() { return true;  }
       @Override
-      public boolean omitNorms()  { return false; }   	
+      public boolean omitNorms()  { return false; }
     },
 
     /** Index the field's value without using an Analyzer, so it can be searched.
@@ -704,7 +704,7 @@
       @Override
       public boolean isAnalyzed() { return false; }
       @Override
-      public boolean omitNorms()  { return false; }   	
+      public boolean omitNorms()  { return false; }
     },
 
     /** Expert: Index the field's value without an Analyzer,
@@ -727,7 +727,7 @@
       @Override
       public boolean isAnalyzed() { return false; }
       @Override
-      public boolean omitNorms()  { return true;  }   	
+      public boolean omitNorms()  { return true;  }
     },
 
     /** Expert: Index the tokens produced by running the
@@ -741,7 +741,7 @@
       @Override
       public boolean isAnalyzed() { return true;  }
       @Override
-      public boolean omitNorms()  { return true;  }   	
+      public boolean omitNorms()  { return true;  }
     };
 
     /** Get the best representation of the index given the flags. */
@@ -774,7 +774,7 @@
 
     public abstract boolean isIndexed();
     public abstract boolean isAnalyzed();
-    public abstract boolean omitNorms();  	
+    public abstract boolean omitNorms();
   }
 
   /** Specifies whether and how a field should have term vectors.
Index: lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/utils/StreamUtilsTest.java
===================================================================
--- lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/utils/StreamUtilsTest.java	(revision 1386643)
+++ lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/utils/StreamUtilsTest.java	(working copy)
@@ -57,38 +57,38 @@
 
   @Test
   public void testGetInputStreamBzip2() throws Exception {
-  	assertReadText(rawBzip2File("bz2"));
-  	assertReadText(rawBzip2File("bzip"));
-  	assertReadText(rawBzip2File("BZ2"));
-  	assertReadText(rawBzip2File("BZIP"));
+    assertReadText(rawBzip2File("bz2"));
+    assertReadText(rawBzip2File("bzip"));
+    assertReadText(rawBzip2File("BZ2"));
+    assertReadText(rawBzip2File("BZIP"));
   }
 
   @Test
   public void testGetOutputStreamBzip2() throws Exception {
-  	assertReadText(autoOutFile("bz2"));
-  	assertReadText(autoOutFile("bzip"));
-  	assertReadText(autoOutFile("BZ2"));
-  	assertReadText(autoOutFile("BZIP"));
+    assertReadText(autoOutFile("bz2"));
+    assertReadText(autoOutFile("bzip"));
+    assertReadText(autoOutFile("BZ2"));
+    assertReadText(autoOutFile("BZIP"));
   }
   
   @Test
   public void testGetOutputStreamGzip() throws Exception {
-  	assertReadText(autoOutFile("gz"));
-  	assertReadText(autoOutFile("gzip"));
-  	assertReadText(autoOutFile("GZ"));
-  	assertReadText(autoOutFile("GZIP"));
+    assertReadText(autoOutFile("gz"));
+    assertReadText(autoOutFile("gzip"));
+    assertReadText(autoOutFile("GZ"));
+    assertReadText(autoOutFile("GZIP"));
   }
 
   @Test
   public void testGetOutputStreamPlain() throws Exception {
-  	assertReadText(autoOutFile("txt"));
-  	assertReadText(autoOutFile("text"));
-  	assertReadText(autoOutFile("TXT"));
-  	assertReadText(autoOutFile("TEXT"));
+    assertReadText(autoOutFile("txt"));
+    assertReadText(autoOutFile("text"));
+    assertReadText(autoOutFile("TXT"));
+    assertReadText(autoOutFile("TEXT"));
   }
   
   private File rawTextFile(String ext) throws Exception {
-    File f = new File(testDir,"testfile." +	ext);
+    File f = new File(testDir,"testfile." +  ext);
     BufferedWriter w = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(f), IOUtils.CHARSET_UTF_8));
     w.write(TEXT);
     w.newLine();
@@ -97,32 +97,32 @@
   }
   
   private File rawGzipFile(String ext) throws Exception {
-    File f = new File(testDir,"testfile." +	ext);
+    File f = new File(testDir,"testfile." +  ext);
     OutputStream os = new CompressorStreamFactory().createCompressorOutputStream(CompressorStreamFactory.GZIP, new FileOutputStream(f));
     writeText(os);
     return f;
   }
 
   private File rawBzip2File(String ext) throws Exception {
-  	File f = new File(testDir,"testfile." +	ext);
-  	OutputStream os = new CompressorStreamFactory().createCompressorOutputStream(CompressorStreamFactory.BZIP2, new FileOutputStream(f));
-  	writeText(os);
-  	return f;
+    File f = new File(testDir,"testfile." +  ext);
+    OutputStream os = new CompressorStreamFactory().createCompressorOutputStream(CompressorStreamFactory.BZIP2, new FileOutputStream(f));
+    writeText(os);
+    return f;
   }
 
   private File autoOutFile(String ext) throws Exception {
-  	File f = new File(testDir,"testfile." +	ext);
-  	OutputStream os = StreamUtils.outputStream(f);
-  	writeText(os);
-  	return f;
+    File f = new File(testDir,"testfile." +  ext);
+    OutputStream os = StreamUtils.outputStream(f);
+    writeText(os);
+    return f;
   }
 
-	private void writeText(OutputStream os) throws IOException {
-		BufferedWriter w = new BufferedWriter(new OutputStreamWriter(os, IOUtils.CHARSET_UTF_8));
-  	w.write(TEXT);
-  	w.newLine();
-  	w.close();
-	}
+  private void writeText(OutputStream os) throws IOException {
+    BufferedWriter w = new BufferedWriter(new OutputStreamWriter(os, IOUtils.CHARSET_UTF_8));
+    w.write(TEXT);
+    w.newLine();
+    w.close();
+  }
 
   private void assertReadText(File f) throws Exception {
     InputStream ir = StreamUtils.inputStream(f);
Index: lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTaskTest.java
===================================================================
--- lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTaskTest.java	(revision 1386643)
+++ lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/WriteLineDocTaskTest.java	(working copy)
@@ -157,16 +157,16 @@
                           String expDate, String expBody) throws Exception {
     InputStream in = new FileInputStream(file);
     switch(fileType) {
-    	case BZIP2:
-    		in = csFactory.createCompressorInputStream(CompressorStreamFactory.BZIP2, in);
-    		break;
-    	case GZIP:
-    		in = csFactory.createCompressorInputStream(CompressorStreamFactory.GZIP, in);
-                break;
-    	case PLAIN:
-    		break; // nothing to do
-    	default:
-    		assertFalse("Unknown file type!",true); //fail, should not happen
+      case BZIP2:
+        in = csFactory.createCompressorInputStream(CompressorStreamFactory.BZIP2, in);
+        break;
+      case GZIP:
+        in = csFactory.createCompressorInputStream(CompressorStreamFactory.GZIP, in);
+        break;
+      case PLAIN:
+        break; // nothing to do
+      default:
+        assertFalse("Unknown file type!",true); //fail, should not happen
     }
     BufferedReader br = new BufferedReader(new InputStreamReader(in, "utf-8"));
     try {
Index: lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/StreamUtils.java
===================================================================
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/StreamUtils.java	(revision 1386643)
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/StreamUtils.java	(working copy)
@@ -37,43 +37,43 @@
  */
 public class StreamUtils {
 
-	/** Buffer size used across the benchmark package */
-	public static final int BUFFER_SIZE = 1 << 16; // 64K
-	
-	/** File format type */
-	public enum Type {
-		/** BZIP2 is automatically used for <b>.bz2</b> and <b>.bzip2</b> extensions. */
-		BZIP2(CompressorStreamFactory.BZIP2),
-		/** GZIP is automatically used for <b>.gz</b> and <b>.gzip</b> extensions. */
-		GZIP(CompressorStreamFactory.GZIP),
-		/** Plain text is used for anything which is not GZIP or BZIP. */
-		PLAIN(null);
-		private final String csfType;
-		Type(String csfType) {
-			this.csfType = csfType;
-		}
-		private InputStream inputStream(InputStream in) throws IOException {
-			try {
-				return csfType==null ? in : new CompressorStreamFactory().createCompressorInputStream(csfType, in);
-			} catch (CompressorException e) {
-    		IOException ioe = new IOException(e.getMessage());
-    		ioe.initCause(e);
-    		throw ioe;			}  
-		}
-		private OutputStream outputStream(OutputStream os) throws IOException {
-			try {
-				return csfType==null ? os : new CompressorStreamFactory().createCompressorOutputStream(csfType, os);
-			} catch (CompressorException e) {
-				IOException ioe = new IOException(e.getMessage());
-				ioe.initCause(e);
-				throw ioe;  
-			}  
-		}
-	}
-	
+  /** Buffer size used across the benchmark package */
+  public static final int BUFFER_SIZE = 1 << 16; // 64K
+
+  /** File format type */
+  public enum Type {
+    /** BZIP2 is automatically used for <b>.bz2</b> and <b>.bzip2</b> extensions. */
+    BZIP2(CompressorStreamFactory.BZIP2),
+    /** GZIP is automatically used for <b>.gz</b> and <b>.gzip</b> extensions. */
+    GZIP(CompressorStreamFactory.GZIP),
+    /** Plain text is used for anything which is not GZIP or BZIP. */
+    PLAIN(null);
+    private final String csfType;
+    Type(String csfType) {
+      this.csfType = csfType;
+    }
+    private InputStream inputStream(InputStream in) throws IOException {
+      try {
+        return csfType==null ? in : new CompressorStreamFactory().createCompressorInputStream(csfType, in);
+      } catch (CompressorException e) {
+        IOException ioe = new IOException(e.getMessage());
+        ioe.initCause(e);
+        throw ioe;      }
+    }
+    private OutputStream outputStream(OutputStream os) throws IOException {
+      try {
+        return csfType==null ? os : new CompressorStreamFactory().createCompressorOutputStream(csfType, os);
+      } catch (CompressorException e) {
+        IOException ioe = new IOException(e.getMessage());
+        ioe.initCause(e);
+        throw ioe;
+      }
+    }
+  }
+
   private static final Map<String,Type> extensionToType = new HashMap<String,Type>();
   static {
-  	// these in are lower case, we will lower case at the test as well
+    // these in are lower case, we will lower case at the test as well
     extensionToType.put(".bz2", Type.BZIP2);
     extensionToType.put(".bzip", Type.BZIP2);
     extensionToType.put(".gz", Type.GZIP);
@@ -95,14 +95,14 @@
 
   /** Return the type of the file, or null if unknown */
   private static Type fileType(File file) {
-  	Type type = null;
+    Type type = null;
     String fileName = file.getName();
     int idx = fileName.lastIndexOf('.');
     if (idx != -1) {
       type = extensionToType.get(fileName.substring(idx).toLowerCase(Locale.ROOT));
     }
     return type==null ? Type.PLAIN : type;
-	}
+  }
   
   /**
    * Returns an {@link OutputStream} over the requested file, identifying
Index: lucene/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java	(revision 1386643)
+++ lucene/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java	(working copy)
@@ -294,7 +294,7 @@
    */
   public SuggestWord[] suggestSimilar(Term term, int numSug, IndexReader ir, 
       SuggestMode suggestMode) throws IOException {
-  	return suggestSimilar(term, numSug, ir, suggestMode, this.accuracy);
+    return suggestSimilar(term, numSug, ir, suggestMode, this.accuracy);
   }
   
   /**
Index: lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java	(revision 1386643)
+++ lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java	(working copy)
@@ -283,7 +283,7 @@
    */
   public String[] suggestSimilar(String word, int numSug, IndexReader ir,
       String field, SuggestMode suggestMode) throws IOException {
-  	return suggestSimilar(word, numSug, ir, field, suggestMode, this.accuracy);
+    return suggestSimilar(word, numSug, ir, field, suggestMode, this.accuracy);
   }
   
   /**
Index: lucene/suggest/src/java/org/apache/lucene/search/suggest/tst/TernaryTreeNode.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/tst/TernaryTreeNode.java	(revision 1386643)
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/tst/TernaryTreeNode.java	(working copy)
@@ -23,20 +23,20 @@
 
 public class TernaryTreeNode {
   /** the character stored by a node. */
-	char splitchar;
-	/** a reference object to the node containing character smaller than this node's character. */
-	TernaryTreeNode loKid;
-	/** 
-	 *  a reference object to the node containing character next to this node's character as 
-	 *  occurring in the inserted token.
-	 */
-	TernaryTreeNode eqKid;
-	/** a reference object to the node containing character higher than this node's character. */
-	TernaryTreeNode hiKid;
-	/** 
-	 * used by leaf nodes to store the complete tokens to be added to suggest list while 
-	 * auto-completing the prefix.
-	 */
-	String token;
-	Object val;
+  char splitchar;
+  /** a reference object to the node containing character smaller than this node's character. */
+  TernaryTreeNode loKid;
+  /** 
+   *  a reference object to the node containing character next to this node's character as 
+   *  occurring in the inserted token.
+   */
+  TernaryTreeNode eqKid;
+  /** a reference object to the node containing character higher than this node's character. */
+  TernaryTreeNode hiKid;
+  /** 
+   * used by leaf nodes to store the complete tokens to be added to suggest list while 
+   * auto-completing the prefix.
+   */
+  String token;
+  Object val;
 }
Index: lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/dict/UserDictionaryTest.java
===================================================================
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/dict/UserDictionaryTest.java	(revision 1386643)
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/dict/UserDictionaryTest.java	(working copy)
@@ -75,6 +75,6 @@
   @Test
   public void testRead() throws IOException {
     UserDictionary dictionary = TestJapaneseTokenizer.readDict();
-    assertNotNull(dictionary);		
+    assertNotNull(dictionary);
   }
 }
Index: lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java
===================================================================
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java	(revision 1386643)
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java	(working copy)
@@ -326,12 +326,12 @@
   
   public void testSegmentation() throws Exception {
     // Skip tests for Michelle Kwan -- UniDic segments Kwan as  
-    //		String input = "";
-    //		String[] surfaceForms = {
-    //				"", "", "", "", "", "", "", "", "",
-    //				"", "", "", "", "", "",
-    //				"", ""
-    //		};
+    //   String input = "";
+    //   String[] surfaceForms = {
+        //        "", "", "", "", "", "", "", "", "",
+        //        "", "", "", "", "", "",
+        //        "", ""
+    //   };
     String input = "";
     String[] surfaceForms = {
         "", "", "", "", "", "",
Index: lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder.java
===================================================================
--- lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder.java	(revision 1386643)
+++ lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder.java	(working copy)
@@ -174,26 +174,26 @@
   /*
    * IPADIC features
    * 
-   * 0	- surface
-   * 1	- left cost
-   * 2	- right cost
-   * 3	- word cost
-   * 4-9	- pos
-   * 10	- base form
-   * 11	- reading
-   * 12	- pronounciation
+   * 0   - surface
+   * 1   - left cost
+   * 2   - right cost
+   * 3   - word cost
+   * 4-9 - pos
+   * 10  - base form
+   * 11  - reading
+   * 12  - pronounciation
    *
    * UniDic features
    * 
-   * 0	- surface
-   * 1	- left cost
-   * 2	- right cost
-   * 3	- word cost
-   * 4-9	- pos
-   * 10	- base form reading
-   * 11	- base form
-   * 12	- surface form
-   * 13	- surface reading
+   * 0   - surface
+   * 1   - left cost
+   * 2   - right cost
+   * 3   - word cost
+   * 4-9 - pos
+   * 10  - base form reading
+   * 11  - base form
+   * 12  - surface form
+   * 13  - surface reading
    */
   
   public String[] formatEntry(String[] features) {
@@ -221,7 +221,7 @@
       } else {
         features2[11] = features[13];
         features2[12] = features[13];
-      }			
+      }
       return features2;
     }
   }
Index: lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/UnknownDictionaryBuilder.java
===================================================================
--- lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/UnknownDictionaryBuilder.java	(revision 1386643)
+++ lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/UnknownDictionaryBuilder.java	(working copy)
@@ -107,22 +107,22 @@
         continue;
       }
       
-      if(line.startsWith("0x")) {	// Category mapping
-        String[] values = line.split(" ", 2);	// Split only first space
+      if(line.startsWith("0x")) {  // Category mapping
+        String[] values = line.split(" ", 2);  // Split only first space
         
         if(!values[0].contains("..")) {
           int cp = Integer.decode(values[0]).intValue();
-          dictionary.putCharacterCategory(cp, values[1]);					
+          dictionary.putCharacterCategory(cp, values[1]);
         } else {
           String[] codePoints = values[0].split("\\.\\.");
           int cpFrom = Integer.decode(codePoints[0]).intValue();
           int cpTo = Integer.decode(codePoints[1]).intValue();
           
           for(int i = cpFrom; i <= cpTo; i++){
-            dictionary.putCharacterCategory(i, values[1]);					
+            dictionary.putCharacterCategory(i, values[1]);
           }
         }
-      } else {	// Invoke definition
+      } else {  // Invoke definition
         String[] values = line.split(" "); // Consecutive space is merged above
         String characterClassName = values[0];
         int invoke = Integer.parseInt(values[1]);
Index: lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java
===================================================================
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java	(revision 1386643)
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java	(working copy)
@@ -246,7 +246,7 @@
       return null;
     }
     
-    return allFeatures.split(INTERNAL_SEPARATOR);		
+    return allFeatures.split(INTERNAL_SEPARATOR);
   }
   
   
@@ -261,7 +261,7 @@
         sb.append(CSVUtil.quoteEscape(feature)).append(",");
       }
     } else if (fields.length == 1) { // One feature doesn't need to escape value
-      sb.append(allFeatures[fields[0]]).append(",");			
+      sb.append(allFeatures[fields[0]]).append(",");
     } else {
       for (int field : fields){
         sb.append(CSVUtil.quoteEscape(allFeatures[field])).append(",");
Index: lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/Dictionary.java
===================================================================
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/Dictionary.java	(revision 1386643)
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/Dictionary.java	(working copy)
@@ -28,21 +28,21 @@
   /**
    * Get left id of specified word
    * @param wordId
-   * @return	left id
+   * @return left id
    */
   public int getLeftId(int wordId);
   
   /**
    * Get right id of specified word
    * @param wordId
-   * @return	left id
+   * @return left id
    */
   public int getRightId(int wordId);
   
   /**
    * Get word cost of specified word
    * @param wordId
-   * @return	left id
+   * @return left id
    */
   public int getWordCost(int wordId);
   
Index: lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UnknownDictionary.java
===================================================================
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UnknownDictionary.java	(revision 1386643)
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UnknownDictionary.java	(working copy)
@@ -40,7 +40,7 @@
     int length = 1;
     for (int i = 1; i < len; i++) {
       if (characterIdOfFirstCharacter == characterDefinition.getCharacterClass(text[offset+i])){
-        length++;    			
+        length++;
       } else {
         break;
       }
Index: lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/BinaryDictionary.java
===================================================================
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/BinaryDictionary.java	(revision 1386643)
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/BinaryDictionary.java	(working copy)
@@ -150,7 +150,7 @@
     ref.length = targetMapOffsets[sourceId + 1] - ref.offset;
   }
   
-  @Override	
+  @Override
   public int getLeftId(int wordId) {
     return buffer.getShort(wordId) >>> 3;
   }
@@ -162,7 +162,7 @@
   
   @Override
   public int getWordCost(int wordId) {
-    return buffer.getShort(wordId + 2);	// Skip id
+    return buffer.getShort(wordId + 2);  // Skip id
   }
 
   @Override
Index: lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java
===================================================================
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java	(revision 1386643)
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java	(working copy)
@@ -292,12 +292,12 @@
         if (!characterDefinition.isKanji((char) buffer.get(pos2))) {
           allKanji = false;
           break;
-        }				
+        }
       }
-      if (allKanji) {	// Process only Kanji keywords
+      if (allKanji) {  // Process only Kanji keywords
         return (length - SEARCH_MODE_KANJI_LENGTH) * SEARCH_MODE_KANJI_PENALTY;
       } else if (length > SEARCH_MODE_OTHER_LENGTH) {
-        return (length - SEARCH_MODE_OTHER_LENGTH) * SEARCH_MODE_OTHER_PENALTY;								
+        return (length - SEARCH_MODE_OTHER_LENGTH) * SEARCH_MODE_OTHER_PENALTY;
       }
     }
     return 0;
@@ -807,7 +807,7 @@
             }
             if (characterId == characterDefinition.getCharacterClass((char) ch) &&
                 isPunctuation((char) ch) == isPunct) {
-              unknownWordLength++;    			
+              unknownWordLength++;
             } else {
               break;
             }
Index: lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/util/CSVUtil.java
===================================================================
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/util/CSVUtil.java	(revision 1386643)
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/util/CSVUtil.java	(working copy)
@@ -42,7 +42,7 @@
    */
   public static String[] parse(String line) {
     boolean insideQuote = false;
-    ArrayList<String> result = new ArrayList<String>();		
+    ArrayList<String> result = new ArrayList<String>();
     int quoteCount = 0;
     StringBuilder sb = new StringBuilder();
     for(int i = 0; i < line.length(); i++) {
Index: lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java
===================================================================
--- lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java	(revision 1386643)
+++ lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java	(working copy)
@@ -115,7 +115,7 @@
       }
       int b0 = (buffer[0] & 0x0FF) - 161; // Code starts from A1, therefore subtract 0xA1=161
       int b1 = (buffer[1] & 0x0FF) - 161; // There is no Chinese char for the first and last symbol. 
-      											// Therefore, each code page only has 16*6-2=94 characters.
+                                          // Therefore, each code page only has 16*6-2=94 characters.
       return (short) (b0 * 94 + b1);
     } catch (UnsupportedEncodingException e) {
       throw new RuntimeException(e);
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/core/WordBreakTestUnicode_6_1_0.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/WordBreakTestUnicode_6_1_0.java	(revision 1386643)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/WordBreakTestUnicode_6_1_0.java	(working copy)
@@ -44,3915 +44,3915 @@
 public class WordBreakTestUnicode_6_1_0 extends BaseTokenStreamTestCase {
 
   public void test(Analyzer analyzer) throws Exception {
-    //  0001  0001 	#   [0.2] <START OF HEADING> (Other)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0001  0001   #   [0.2] <START OF HEADING> (Other)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0001",
                      new String[] {  });
 
-    //  0001  0308  0001 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0001  0308  0001   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0001",
                      new String[] {  });
 
-    //  0001  000D 	#   [0.2] <START OF HEADING> (Other)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0001  000D   #   [0.2] <START OF HEADING> (Other)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\r",
                      new String[] {  });
 
-    //  0001  0308  000D 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0001  0308  000D   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\r",
                      new String[] {  });
 
-    //  0001  000A 	#   [0.2] <START OF HEADING> (Other)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0001  000A   #   [0.2] <START OF HEADING> (Other)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\n",
                      new String[] {  });
 
-    //  0001  0308  000A 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0001  0308  000A   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\n",
                      new String[] {  });
 
-    //  0001  000B 	#   [0.2] <START OF HEADING> (Other)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0001  000B   #   [0.2] <START OF HEADING> (Other)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u000B",
                      new String[] {  });
 
-    //  0001  0308  000B 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0001  0308  000B   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u000B",
                      new String[] {  });
 
-    //  0001  3031 	#   [0.2] <START OF HEADING> (Other)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0001  3031   #   [0.2] <START OF HEADING> (Other)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u3031",
                      new String[] { "\u3031" });
 
-    //  0001  0308  3031 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0001  0308  3031   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u3031",
                      new String[] { "\u3031" });
 
-    //  0001  0041 	#   [0.2] <START OF HEADING> (Other)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0001  0041   #   [0.2] <START OF HEADING> (Other)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0041",
                      new String[] { "\u0041" });
 
-    //  0001  0308  0041 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0001  0308  0041   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0041",
                      new String[] { "\u0041" });
 
-    //  0001  003A 	#   [0.2] <START OF HEADING> (Other)  [999.0] COLON (MidLetter)  [0.3]
+    //  0001  003A   #   [0.2] <START OF HEADING> (Other)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u003A",
                      new String[] {  });
 
-    //  0001  0308  003A 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  0001  0308  003A   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u003A",
                      new String[] {  });
 
-    //  0001  002C 	#   [0.2] <START OF HEADING> (Other)  [999.0] COMMA (MidNum)  [0.3]
+    //  0001  002C   #   [0.2] <START OF HEADING> (Other)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u002C",
                      new String[] {  });
 
-    //  0001  0308  002C 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  0001  0308  002C   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u002C",
                      new String[] {  });
 
-    //  0001  0027 	#   [0.2] <START OF HEADING> (Other)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0001  0027   #   [0.2] <START OF HEADING> (Other)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0027",
                      new String[] {  });
 
-    //  0001  0308  0027 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0001  0308  0027   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0027",
                      new String[] {  });
 
-    //  0001  0030 	#   [0.2] <START OF HEADING> (Other)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0001  0030   #   [0.2] <START OF HEADING> (Other)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0030",
                      new String[] { "\u0030" });
 
-    //  0001  0308  0030 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0001  0308  0030   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0030",
                      new String[] { "\u0030" });
 
-    //  0001  005F 	#   [0.2] <START OF HEADING> (Other)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0001  005F   #   [0.2] <START OF HEADING> (Other)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u005F",
                      new String[] {  });
 
-    //  0001  0308  005F 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0001  0308  005F   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u005F",
                      new String[] {  });
 
-    //  0001  00AD 	#   [0.2] <START OF HEADING> (Other)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0001  00AD   #   [0.2] <START OF HEADING> (Other)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u00AD",
                      new String[] {  });
 
-    //  0001  0308  00AD 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0001  0308  00AD   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u00AD",
                      new String[] {  });
 
-    //  0001  0300 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0001  0300   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0300",
                      new String[] {  });
 
-    //  0001  0308  0300 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0001  0308  0300   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0300",
                      new String[] {  });
 
-    //  0001  0061  2060 	#   [0.2] <START OF HEADING> (Other)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0001  0061  2060   #   [0.2] <START OF HEADING> (Other)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    //  0001  0308  0061  2060 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0001  0308  0061  2060   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    //  0001  0061  003A 	#   [0.2] <START OF HEADING> (Other)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0001  0061  003A   #   [0.2] <START OF HEADING> (Other)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0061\u003A",
                      new String[] { "\u0061" });
 
-    //  0001  0308  0061  003A 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0001  0308  0061  003A   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0061\u003A",
                      new String[] { "\u0061" });
 
-    //  0001  0061  0027 	#   [0.2] <START OF HEADING> (Other)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0001  0061  0027   #   [0.2] <START OF HEADING> (Other)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0061\u0027",
                      new String[] { "\u0061" });
 
-    //  0001  0308  0061  0027 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0001  0308  0061  0027   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0061\u0027",
                      new String[] { "\u0061" });
 
-    //  0001  0061  0027  2060 	#   [0.2] <START OF HEADING> (Other)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0001  0061  0027  2060   #   [0.2] <START OF HEADING> (Other)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    //  0001  0308  0061  0027  2060 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0001  0308  0061  0027  2060   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    //  0001  0061  002C 	#   [0.2] <START OF HEADING> (Other)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0001  0061  002C   #   [0.2] <START OF HEADING> (Other)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0061\u002C",
                      new String[] { "\u0061" });
 
-    //  0001  0308  0061  002C 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0001  0308  0061  002C   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0061\u002C",
                      new String[] { "\u0061" });
 
-    //  0001  0031  003A 	#   [0.2] <START OF HEADING> (Other)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0001  0031  003A   #   [0.2] <START OF HEADING> (Other)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0031\u003A",
                      new String[] { "\u0031" });
 
-    //  0001  0308  0031  003A 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0001  0308  0031  003A   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0031\u003A",
                      new String[] { "\u0031" });
 
-    //  0001  0031  0027 	#   [0.2] <START OF HEADING> (Other)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0001  0031  0027   #   [0.2] <START OF HEADING> (Other)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0031\u0027",
                      new String[] { "\u0031" });
 
-    //  0001  0308  0031  0027 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0001  0308  0031  0027   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0031\u0027",
                      new String[] { "\u0031" });
 
-    //  0001  0031  002C 	#   [0.2] <START OF HEADING> (Other)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0001  0031  002C   #   [0.2] <START OF HEADING> (Other)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0031\u002C",
                      new String[] { "\u0031" });
 
-    //  0001  0308  0031  002C 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0001  0308  0031  002C   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0031\u002C",
                      new String[] { "\u0031" });
 
-    //  0001  0031  002E  2060 	#   [0.2] <START OF HEADING> (Other)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0001  0031  002E  2060   #   [0.2] <START OF HEADING> (Other)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    //  0001  0308  0031  002E  2060 	#   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0001  0308  0031  002E  2060   #   [0.2] <START OF HEADING> (Other)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0001\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    //  000D  0001 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] <START OF HEADING> (Other)  [0.3]
+    //  000D  0001   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0001",
                      new String[] {  });
 
-    //  000D  0308  0001 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  000D  0308  0001   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0001",
                      new String[] {  });
 
-    //  000D  000D 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  000D  000D   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\r\r",
                      new String[] {  });
 
-    //  000D  0308  000D 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  000D  0308  000D   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\r",
                      new String[] {  });
 
-    //  000D  000A 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.0] <LINE FEED (LF)> (LF)  [0.3]
+    //  000D  000A   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.0] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\r\n",
                      new String[] {  });
 
-    //  000D  0308  000A 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  000D  0308  000A   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\n",
                      new String[] {  });
 
-    //  000D  000B 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] <LINE TABULATION> (Newline)  [0.3]
+    //  000D  000B   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u000B",
                      new String[] {  });
 
-    //  000D  0308  000B 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  000D  0308  000B   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u000B",
                      new String[] {  });
 
-    //  000D  3031 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  000D  3031   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u3031",
                      new String[] { "\u3031" });
 
-    //  000D  0308  3031 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  000D  0308  3031   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u3031",
                      new String[] { "\u3031" });
 
-    //  000D  0041 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  000D  0041   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0041",
                      new String[] { "\u0041" });
 
-    //  000D  0308  0041 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  000D  0308  0041   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0041",
                      new String[] { "\u0041" });
 
-    //  000D  003A 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COLON (MidLetter)  [0.3]
+    //  000D  003A   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u003A",
                      new String[] {  });
 
-    //  000D  0308  003A 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  000D  0308  003A   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u003A",
                      new String[] {  });
 
-    //  000D  002C 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMMA (MidNum)  [0.3]
+    //  000D  002C   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u002C",
                      new String[] {  });
 
-    //  000D  0308  002C 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  000D  0308  002C   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u002C",
                      new String[] {  });
 
-    //  000D  0027 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] APOSTROPHE (MidNumLet)  [0.3]
+    //  000D  0027   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0027",
                      new String[] {  });
 
-    //  000D  0308  0027 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  000D  0308  0027   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0027",
                      new String[] {  });
 
-    //  000D  0030 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] DIGIT ZERO (Numeric)  [0.3]
+    //  000D  0030   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0030",
                      new String[] { "\u0030" });
 
-    //  000D  0308  0030 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  000D  0308  0030   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0030",
                      new String[] { "\u0030" });
 
-    //  000D  005F 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] LOW LINE (ExtendNumLet)  [0.3]
+    //  000D  005F   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u005F",
                      new String[] {  });
 
-    //  000D  0308  005F 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  000D  0308  005F   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u005F",
                      new String[] {  });
 
-    //  000D  00AD 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] SOFT HYPHEN (Format_FE)  [0.3]
+    //  000D  00AD   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u00AD",
                      new String[] {  });
 
-    //  000D  0308  00AD 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  000D  0308  00AD   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u00AD",
                      new String[] {  });
 
-    //  000D  0300 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  000D  0300   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0300",
                      new String[] {  });
 
-    //  000D  0308  0300 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  000D  0308  0300   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0300",
                      new String[] {  });
 
-    //  000D  0061  2060 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  000D  0061  2060   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    //  000D  0308  0061  2060 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  000D  0308  0061  2060   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    //  000D  0061  003A 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  000D  0061  003A   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0061\u003A",
                      new String[] { "\u0061" });
 
-    //  000D  0308  0061  003A 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  000D  0308  0061  003A   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0061\u003A",
                      new String[] { "\u0061" });
 
-    //  000D  0061  0027 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  000D  0061  0027   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0061\u0027",
                      new String[] { "\u0061" });
 
-    //  000D  0308  0061  0027 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  000D  0308  0061  0027   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0061\u0027",
                      new String[] { "\u0061" });
 
-    //  000D  0061  0027  2060 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  000D  0061  0027  2060   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    //  000D  0308  0061  0027  2060 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  000D  0308  0061  0027  2060   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    //  000D  0061  002C 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  000D  0061  002C   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0061\u002C",
                      new String[] { "\u0061" });
 
-    //  000D  0308  0061  002C 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  000D  0308  0061  002C   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0061\u002C",
                      new String[] { "\u0061" });
 
-    //  000D  0031  003A 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  000D  0031  003A   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0031\u003A",
                      new String[] { "\u0031" });
 
-    //  000D  0308  0031  003A 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  000D  0308  0031  003A   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0031\u003A",
                      new String[] { "\u0031" });
 
-    //  000D  0031  0027 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  000D  0031  0027   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0031\u0027",
                      new String[] { "\u0031" });
 
-    //  000D  0308  0031  0027 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  000D  0308  0031  0027   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0031\u0027",
                      new String[] { "\u0031" });
 
-    //  000D  0031  002C 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  000D  0031  002C   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0031\u002C",
                      new String[] { "\u0031" });
 
-    //  000D  0308  0031  002C 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  000D  0308  0031  002C   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0031\u002C",
                      new String[] { "\u0031" });
 
-    //  000D  0031  002E  2060 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  000D  0031  002E  2060   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    //  000D  0308  0031  002E  2060 	#   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  000D  0308  0031  002E  2060   #   [0.2] <CARRIAGE RETURN (CR)> (CR)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\r\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    //  000A  0001 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] <START OF HEADING> (Other)  [0.3]
+    //  000A  0001   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0001",
                      new String[] {  });
 
-    //  000A  0308  0001 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  000A  0308  0001   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0001",
                      new String[] {  });
 
-    //  000A  000D 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  000A  000D   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\n\r",
                      new String[] {  });
 
-    //  000A  0308  000D 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  000A  0308  000D   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\r",
                      new String[] {  });
 
-    //  000A  000A 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] <LINE FEED (LF)> (LF)  [0.3]
+    //  000A  000A   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\n\n",
                      new String[] {  });
 
-    //  000A  0308  000A 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  000A  0308  000A   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\n",
                      new String[] {  });
 
-    //  000A  000B 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] <LINE TABULATION> (Newline)  [0.3]
+    //  000A  000B   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u000B",
                      new String[] {  });
 
-    //  000A  0308  000B 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  000A  0308  000B   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u000B",
                      new String[] {  });
 
-    //  000A  3031 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  000A  3031   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u3031",
                      new String[] { "\u3031" });
 
-    //  000A  0308  3031 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  000A  0308  3031   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u3031",
                      new String[] { "\u3031" });
 
-    //  000A  0041 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  000A  0041   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0041",
                      new String[] { "\u0041" });
 
-    //  000A  0308  0041 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  000A  0308  0041   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0041",
                      new String[] { "\u0041" });
 
-    //  000A  003A 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COLON (MidLetter)  [0.3]
+    //  000A  003A   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u003A",
                      new String[] {  });
 
-    //  000A  0308  003A 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  000A  0308  003A   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u003A",
                      new String[] {  });
 
-    //  000A  002C 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMMA (MidNum)  [0.3]
+    //  000A  002C   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u002C",
                      new String[] {  });
 
-    //  000A  0308  002C 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  000A  0308  002C   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u002C",
                      new String[] {  });
 
-    //  000A  0027 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] APOSTROPHE (MidNumLet)  [0.3]
+    //  000A  0027   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0027",
                      new String[] {  });
 
-    //  000A  0308  0027 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  000A  0308  0027   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0027",
                      new String[] {  });
 
-    //  000A  0030 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] DIGIT ZERO (Numeric)  [0.3]
+    //  000A  0030   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0030",
                      new String[] { "\u0030" });
 
-    //  000A  0308  0030 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  000A  0308  0030   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0030",
                      new String[] { "\u0030" });
 
-    //  000A  005F 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] LOW LINE (ExtendNumLet)  [0.3]
+    //  000A  005F   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u005F",
                      new String[] {  });
 
-    //  000A  0308  005F 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  000A  0308  005F   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u005F",
                      new String[] {  });
 
-    //  000A  00AD 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] SOFT HYPHEN (Format_FE)  [0.3]
+    //  000A  00AD   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u00AD",
                      new String[] {  });
 
-    //  000A  0308  00AD 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  000A  0308  00AD   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u00AD",
                      new String[] {  });
 
-    //  000A  0300 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  000A  0300   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0300",
                      new String[] {  });
 
-    //  000A  0308  0300 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  000A  0308  0300   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0300",
                      new String[] {  });
 
-    //  000A  0061  2060 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  000A  0061  2060   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    //  000A  0308  0061  2060 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  000A  0308  0061  2060   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    //  000A  0061  003A 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  000A  0061  003A   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0061\u003A",
                      new String[] { "\u0061" });
 
-    //  000A  0308  0061  003A 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  000A  0308  0061  003A   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0061\u003A",
                      new String[] { "\u0061" });
 
-    //  000A  0061  0027 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  000A  0061  0027   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0061\u0027",
                      new String[] { "\u0061" });
 
-    //  000A  0308  0061  0027 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  000A  0308  0061  0027   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0061\u0027",
                      new String[] { "\u0061" });
 
-    //  000A  0061  0027  2060 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  000A  0061  0027  2060   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    //  000A  0308  0061  0027  2060 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  000A  0308  0061  0027  2060   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    //  000A  0061  002C 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  000A  0061  002C   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0061\u002C",
                      new String[] { "\u0061" });
 
-    //  000A  0308  0061  002C 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  000A  0308  0061  002C   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0061\u002C",
                      new String[] { "\u0061" });
 
-    //  000A  0031  003A 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  000A  0031  003A   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0031\u003A",
                      new String[] { "\u0031" });
 
-    //  000A  0308  0031  003A 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  000A  0308  0031  003A   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0031\u003A",
                      new String[] { "\u0031" });
 
-    //  000A  0031  0027 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  000A  0031  0027   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0031\u0027",
                      new String[] { "\u0031" });
 
-    //  000A  0308  0031  0027 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  000A  0308  0031  0027   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0031\u0027",
                      new String[] { "\u0031" });
 
-    //  000A  0031  002C 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  000A  0031  002C   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0031\u002C",
                      new String[] { "\u0031" });
 
-    //  000A  0308  0031  002C 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  000A  0308  0031  002C   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0031\u002C",
                      new String[] { "\u0031" });
 
-    //  000A  0031  002E  2060 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  000A  0031  002E  2060   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    //  000A  0308  0031  002E  2060 	#   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  000A  0308  0031  002E  2060   #   [0.2] <LINE FEED (LF)> (LF)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\n\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    //  000B  0001 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] <START OF HEADING> (Other)  [0.3]
+    //  000B  0001   #   [0.2] <LINE TABULATION> (Newline)  [3.1] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0001",
                      new String[] {  });
 
-    //  000B  0308  0001 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  000B  0308  0001   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0001",
                      new String[] {  });
 
-    //  000B  000D 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  000B  000D   #   [0.2] <LINE TABULATION> (Newline)  [3.1] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\r",
                      new String[] {  });
 
-    //  000B  0308  000D 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  000B  0308  000D   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\r",
                      new String[] {  });
 
-    //  000B  000A 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] <LINE FEED (LF)> (LF)  [0.3]
+    //  000B  000A   #   [0.2] <LINE TABULATION> (Newline)  [3.1] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\n",
                      new String[] {  });
 
-    //  000B  0308  000A 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  000B  0308  000A   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\n",
                      new String[] {  });
 
-    //  000B  000B 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] <LINE TABULATION> (Newline)  [0.3]
+    //  000B  000B   #   [0.2] <LINE TABULATION> (Newline)  [3.1] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u000B",
                      new String[] {  });
 
-    //  000B  0308  000B 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  000B  0308  000B   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u000B",
                      new String[] {  });
 
-    //  000B  3031 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  000B  3031   #   [0.2] <LINE TABULATION> (Newline)  [3.1] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u3031",
                      new String[] { "\u3031" });
 
-    //  000B  0308  3031 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  000B  0308  3031   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u3031",
                      new String[] { "\u3031" });
 
-    //  000B  0041 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  000B  0041   #   [0.2] <LINE TABULATION> (Newline)  [3.1] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0041",
                      new String[] { "\u0041" });
 
-    //  000B  0308  0041 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  000B  0308  0041   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0041",
                      new String[] { "\u0041" });
 
-    //  000B  003A 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COLON (MidLetter)  [0.3]
+    //  000B  003A   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u003A",
                      new String[] {  });
 
-    //  000B  0308  003A 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  000B  0308  003A   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u003A",
                      new String[] {  });
 
-    //  000B  002C 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMMA (MidNum)  [0.3]
+    //  000B  002C   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u002C",
                      new String[] {  });
 
-    //  000B  0308  002C 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  000B  0308  002C   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u002C",
                      new String[] {  });
 
-    //  000B  0027 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] APOSTROPHE (MidNumLet)  [0.3]
+    //  000B  0027   #   [0.2] <LINE TABULATION> (Newline)  [3.1] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0027",
                      new String[] {  });
 
-    //  000B  0308  0027 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  000B  0308  0027   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0027",
                      new String[] {  });
 
-    //  000B  0030 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] DIGIT ZERO (Numeric)  [0.3]
+    //  000B  0030   #   [0.2] <LINE TABULATION> (Newline)  [3.1] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0030",
                      new String[] { "\u0030" });
 
-    //  000B  0308  0030 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  000B  0308  0030   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0030",
                      new String[] { "\u0030" });
 
-    //  000B  005F 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] LOW LINE (ExtendNumLet)  [0.3]
+    //  000B  005F   #   [0.2] <LINE TABULATION> (Newline)  [3.1] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u005F",
                      new String[] {  });
 
-    //  000B  0308  005F 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  000B  0308  005F   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u005F",
                      new String[] {  });
 
-    //  000B  00AD 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] SOFT HYPHEN (Format_FE)  [0.3]
+    //  000B  00AD   #   [0.2] <LINE TABULATION> (Newline)  [3.1] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u00AD",
                      new String[] {  });
 
-    //  000B  0308  00AD 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  000B  0308  00AD   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u00AD",
                      new String[] {  });
 
-    //  000B  0300 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  000B  0300   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0300",
                      new String[] {  });
 
-    //  000B  0308  0300 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  000B  0308  0300   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0300",
                      new String[] {  });
 
-    //  000B  0061  2060 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  000B  0061  2060   #   [0.2] <LINE TABULATION> (Newline)  [3.1] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    //  000B  0308  0061  2060 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  000B  0308  0061  2060   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    //  000B  0061  003A 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  000B  0061  003A   #   [0.2] <LINE TABULATION> (Newline)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0061\u003A",
                      new String[] { "\u0061" });
 
-    //  000B  0308  0061  003A 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  000B  0308  0061  003A   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0061\u003A",
                      new String[] { "\u0061" });
 
-    //  000B  0061  0027 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  000B  0061  0027   #   [0.2] <LINE TABULATION> (Newline)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0061\u0027",
                      new String[] { "\u0061" });
 
-    //  000B  0308  0061  0027 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  000B  0308  0061  0027   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0061\u0027",
                      new String[] { "\u0061" });
 
-    //  000B  0061  0027  2060 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  000B  0061  0027  2060   #   [0.2] <LINE TABULATION> (Newline)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    //  000B  0308  0061  0027  2060 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  000B  0308  0061  0027  2060   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    //  000B  0061  002C 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  000B  0061  002C   #   [0.2] <LINE TABULATION> (Newline)  [3.1] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0061\u002C",
                      new String[] { "\u0061" });
 
-    //  000B  0308  0061  002C 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  000B  0308  0061  002C   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0061\u002C",
                      new String[] { "\u0061" });
 
-    //  000B  0031  003A 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  000B  0031  003A   #   [0.2] <LINE TABULATION> (Newline)  [3.1] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0031\u003A",
                      new String[] { "\u0031" });
 
-    //  000B  0308  0031  003A 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  000B  0308  0031  003A   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0031\u003A",
                      new String[] { "\u0031" });
 
-    //  000B  0031  0027 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  000B  0031  0027   #   [0.2] <LINE TABULATION> (Newline)  [3.1] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0031\u0027",
                      new String[] { "\u0031" });
 
-    //  000B  0308  0031  0027 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  000B  0308  0031  0027   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0031\u0027",
                      new String[] { "\u0031" });
 
-    //  000B  0031  002C 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  000B  0031  002C   #   [0.2] <LINE TABULATION> (Newline)  [3.1] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0031\u002C",
                      new String[] { "\u0031" });
 
-    //  000B  0308  0031  002C 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  000B  0308  0031  002C   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0031\u002C",
                      new String[] { "\u0031" });
 
-    //  000B  0031  002E  2060 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  000B  0031  002E  2060   #   [0.2] <LINE TABULATION> (Newline)  [3.1] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    //  000B  0308  0031  002E  2060 	#   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  000B  0308  0031  002E  2060   #   [0.2] <LINE TABULATION> (Newline)  [3.1] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u000B\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    //  3031  0001 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  3031  0001   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0001",
                      new String[] { "\u3031" });
 
-    //  3031  0308  0001 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  3031  0308  0001   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0001",
                      new String[] { "\u3031\u0308" });
 
-    //  3031  000D 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  3031  000D   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\r",
                      new String[] { "\u3031" });
 
-    //  3031  0308  000D 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  3031  0308  000D   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\r",
                      new String[] { "\u3031\u0308" });
 
-    //  3031  000A 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  3031  000A   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\n",
                      new String[] { "\u3031" });
 
-    //  3031  0308  000A 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  3031  0308  000A   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\n",
                      new String[] { "\u3031\u0308" });
 
-    //  3031  000B 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  3031  000B   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u000B",
                      new String[] { "\u3031" });
 
-    //  3031  0308  000B 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  3031  0308  000B   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u000B",
                      new String[] { "\u3031\u0308" });
 
-    //  3031  3031 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [13.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  3031  3031   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [13.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u3031",
                      new String[] { "\u3031\u3031" });
 
-    //  3031  0308  3031 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  3031  0308  3031   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u3031",
                      new String[] { "\u3031\u0308\u3031" });
 
-    //  3031  0041 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  3031  0041   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0041",
                      new String[] { "\u3031", "\u0041" });
 
-    //  3031  0308  0041 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  3031  0308  0041   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0041",
                      new String[] { "\u3031\u0308", "\u0041" });
 
-    //  3031  003A 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] COLON (MidLetter)  [0.3]
+    //  3031  003A   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u003A",
                      new String[] { "\u3031" });
 
-    //  3031  0308  003A 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  3031  0308  003A   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u003A",
                      new String[] { "\u3031\u0308" });
 
-    //  3031  002C 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] COMMA (MidNum)  [0.3]
+    //  3031  002C   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u002C",
                      new String[] { "\u3031" });
 
-    //  3031  0308  002C 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  3031  0308  002C   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u002C",
                      new String[] { "\u3031\u0308" });
 
-    //  3031  0027 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  3031  0027   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0027",
                      new String[] { "\u3031" });
 
-    //  3031  0308  0027 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  3031  0308  0027   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0027",
                      new String[] { "\u3031\u0308" });
 
-    //  3031  0030 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  3031  0030   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0030",
                      new String[] { "\u3031", "\u0030" });
 
-    //  3031  0308  0030 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  3031  0308  0030   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0030",
                      new String[] { "\u3031\u0308", "\u0030" });
 
-    //  3031  005F 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
+    //  3031  005F   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u005F",
                      new String[] { "\u3031\u005F" });
 
-    //  3031  0308  005F 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
+    //  3031  0308  005F   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u005F",
                      new String[] { "\u3031\u0308\u005F" });
 
-    //  3031  00AD 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  3031  00AD   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u00AD",
                      new String[] { "\u3031\u00AD" });
 
-    //  3031  0308  00AD 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  3031  0308  00AD   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u00AD",
                      new String[] { "\u3031\u0308\u00AD" });
 
-    //  3031  0300 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  3031  0300   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0300",
                      new String[] { "\u3031\u0300" });
 
-    //  3031  0308  0300 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  3031  0308  0300   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0300",
                      new String[] { "\u3031\u0308\u0300" });
 
-    //  3031  0061  2060 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  3031  0061  2060   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0061\u2060",
                      new String[] { "\u3031", "\u0061\u2060" });
 
-    //  3031  0308  0061  2060 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  3031  0308  0061  2060   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0061\u2060",
                      new String[] { "\u3031\u0308", "\u0061\u2060" });
 
-    //  3031  0061  003A 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  3031  0061  003A   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0061\u003A",
                      new String[] { "\u3031", "\u0061" });
 
-    //  3031  0308  0061  003A 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  3031  0308  0061  003A   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0061\u003A",
                      new String[] { "\u3031\u0308", "\u0061" });
 
-    //  3031  0061  0027 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  3031  0061  0027   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0061\u0027",
                      new String[] { "\u3031", "\u0061" });
 
-    //  3031  0308  0061  0027 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  3031  0308  0061  0027   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0061\u0027",
                      new String[] { "\u3031\u0308", "\u0061" });
 
-    //  3031  0061  0027  2060 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  3031  0061  0027  2060   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0061\u0027\u2060",
                      new String[] { "\u3031", "\u0061" });
 
-    //  3031  0308  0061  0027  2060 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  3031  0308  0061  0027  2060   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0061\u0027\u2060",
                      new String[] { "\u3031\u0308", "\u0061" });
 
-    //  3031  0061  002C 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  3031  0061  002C   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0061\u002C",
                      new String[] { "\u3031", "\u0061" });
 
-    //  3031  0308  0061  002C 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  3031  0308  0061  002C   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0061\u002C",
                      new String[] { "\u3031\u0308", "\u0061" });
 
-    //  3031  0031  003A 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  3031  0031  003A   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0031\u003A",
                      new String[] { "\u3031", "\u0031" });
 
-    //  3031  0308  0031  003A 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  3031  0308  0031  003A   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0031\u003A",
                      new String[] { "\u3031\u0308", "\u0031" });
 
-    //  3031  0031  0027 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  3031  0031  0027   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0031\u0027",
                      new String[] { "\u3031", "\u0031" });
 
-    //  3031  0308  0031  0027 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  3031  0308  0031  0027   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0031\u0027",
                      new String[] { "\u3031\u0308", "\u0031" });
 
-    //  3031  0031  002C 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  3031  0031  002C   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0031\u002C",
                      new String[] { "\u3031", "\u0031" });
 
-    //  3031  0308  0031  002C 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  3031  0308  0031  002C   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0031\u002C",
                      new String[] { "\u3031\u0308", "\u0031" });
 
-    //  3031  0031  002E  2060 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  3031  0031  002E  2060   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0031\u002E\u2060",
                      new String[] { "\u3031", "\u0031" });
 
-    //  3031  0308  0031  002E  2060 	#   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  3031  0308  0031  002E  2060   #   [0.2] VERTICAL KANA REPEAT MARK (Katakana)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u3031\u0308\u0031\u002E\u2060",
                      new String[] { "\u3031\u0308", "\u0031" });
 
-    //  0041  0001 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0041  0001   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0001",
                      new String[] { "\u0041" });
 
-    //  0041  0308  0001 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0041  0308  0001   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0001",
                      new String[] { "\u0041\u0308" });
 
-    //  0041  000D 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0041  000D   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\r",
                      new String[] { "\u0041" });
 
-    //  0041  0308  000D 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0041  0308  000D   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\r",
                      new String[] { "\u0041\u0308" });
 
-    //  0041  000A 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0041  000A   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\n",
                      new String[] { "\u0041" });
 
-    //  0041  0308  000A 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0041  0308  000A   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\n",
                      new String[] { "\u0041\u0308" });
 
-    //  0041  000B 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0041  000B   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u000B",
                      new String[] { "\u0041" });
 
-    //  0041  0308  000B 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0041  0308  000B   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u000B",
                      new String[] { "\u0041\u0308" });
 
-    //  0041  3031 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0041  3031   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u3031",
                      new String[] { "\u0041", "\u3031" });
 
-    //  0041  0308  3031 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0041  0308  3031   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u3031",
                      new String[] { "\u0041\u0308", "\u3031" });
 
-    //  0041  0041 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [5.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0041  0041   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [5.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0041",
                      new String[] { "\u0041\u0041" });
 
-    //  0041  0308  0041 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0041  0308  0041   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0041",
                      new String[] { "\u0041\u0308\u0041" });
 
-    //  0041  003A 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0041  003A   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u003A",
                      new String[] { "\u0041" });
 
-    //  0041  0308  003A 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  0041  0308  003A   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u003A",
                      new String[] { "\u0041\u0308" });
 
-    //  0041  002C 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0041  002C   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u002C",
                      new String[] { "\u0041" });
 
-    //  0041  0308  002C 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  0041  0308  002C   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u002C",
                      new String[] { "\u0041\u0308" });
 
-    //  0041  0027 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0041  0027   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0027",
                      new String[] { "\u0041" });
 
-    //  0041  0308  0027 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0041  0308  0027   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0027",
                      new String[] { "\u0041\u0308" });
 
-    //  0041  0030 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [9.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0041  0030   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [9.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0030",
                      new String[] { "\u0041\u0030" });
 
-    //  0041  0308  0030 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0041  0308  0030   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0030",
                      new String[] { "\u0041\u0308\u0030" });
 
-    //  0041  005F 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
+    //  0041  005F   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u005F",
                      new String[] { "\u0041\u005F" });
 
-    //  0041  0308  005F 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
+    //  0041  0308  005F   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u005F",
                      new String[] { "\u0041\u0308\u005F" });
 
-    //  0041  00AD 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0041  00AD   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u00AD",
                      new String[] { "\u0041\u00AD" });
 
-    //  0041  0308  00AD 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0041  0308  00AD   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u00AD",
                      new String[] { "\u0041\u0308\u00AD" });
 
-    //  0041  0300 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0041  0300   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0300",
                      new String[] { "\u0041\u0300" });
 
-    //  0041  0308  0300 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0041  0308  0300   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0300",
                      new String[] { "\u0041\u0308\u0300" });
 
-    //  0041  0061  2060 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [5.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0041  0061  2060   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [5.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0061\u2060",
                      new String[] { "\u0041\u0061\u2060" });
 
-    //  0041  0308  0061  2060 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0041  0308  0061  2060   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0061\u2060",
                      new String[] { "\u0041\u0308\u0061\u2060" });
 
-    //  0041  0061  003A 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0041  0061  003A   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0061\u003A",
                      new String[] { "\u0041\u0061" });
 
-    //  0041  0308  0061  003A 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0041  0308  0061  003A   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0061\u003A",
                      new String[] { "\u0041\u0308\u0061" });
 
-    //  0041  0061  0027 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0041  0061  0027   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0061\u0027",
                      new String[] { "\u0041\u0061" });
 
-    //  0041  0308  0061  0027 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0041  0308  0061  0027   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0061\u0027",
                      new String[] { "\u0041\u0308\u0061" });
 
-    //  0041  0061  0027  2060 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0041  0061  0027  2060   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0061\u0027\u2060",
                      new String[] { "\u0041\u0061" });
 
-    //  0041  0308  0061  0027  2060 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0041  0308  0061  0027  2060   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0061\u0027\u2060",
                      new String[] { "\u0041\u0308\u0061" });
 
-    //  0041  0061  002C 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0041  0061  002C   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0061\u002C",
                      new String[] { "\u0041\u0061" });
 
-    //  0041  0308  0061  002C 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0041  0308  0061  002C   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0061\u002C",
                      new String[] { "\u0041\u0308\u0061" });
 
-    //  0041  0031  003A 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [9.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0041  0031  003A   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [9.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0031\u003A",
                      new String[] { "\u0041\u0031" });
 
-    //  0041  0308  0031  003A 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0041  0308  0031  003A   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0031\u003A",
                      new String[] { "\u0041\u0308\u0031" });
 
-    //  0041  0031  0027 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [9.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0041  0031  0027   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [9.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0031\u0027",
                      new String[] { "\u0041\u0031" });
 
-    //  0041  0308  0031  0027 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0041  0308  0031  0027   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0031\u0027",
                      new String[] { "\u0041\u0308\u0031" });
 
-    //  0041  0031  002C 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [9.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0041  0031  002C   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [9.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0031\u002C",
                      new String[] { "\u0041\u0031" });
 
-    //  0041  0308  0031  002C 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0041  0308  0031  002C   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0031\u002C",
                      new String[] { "\u0041\u0308\u0031" });
 
-    //  0041  0031  002E  2060 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [9.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0041  0031  002E  2060   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [9.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0031\u002E\u2060",
                      new String[] { "\u0041\u0031" });
 
-    //  0041  0308  0031  002E  2060 	#   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0041  0308  0031  002E  2060   #   [0.2] LATIN CAPITAL LETTER A (ALetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0041\u0308\u0031\u002E\u2060",
                      new String[] { "\u0041\u0308\u0031" });
 
-    //  003A  0001 	#   [0.2] COLON (MidLetter)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  003A  0001   #   [0.2] COLON (MidLetter)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0001",
                      new String[] {  });
 
-    //  003A  0308  0001 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  003A  0308  0001   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0001",
                      new String[] {  });
 
-    //  003A  000D 	#   [0.2] COLON (MidLetter)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  003A  000D   #   [0.2] COLON (MidLetter)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\r",
                      new String[] {  });
 
-    //  003A  0308  000D 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  003A  0308  000D   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\r",
                      new String[] {  });
 
-    //  003A  000A 	#   [0.2] COLON (MidLetter)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  003A  000A   #   [0.2] COLON (MidLetter)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\n",
                      new String[] {  });
 
-    //  003A  0308  000A 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  003A  0308  000A   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\n",
                      new String[] {  });
 
-    //  003A  000B 	#   [0.2] COLON (MidLetter)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  003A  000B   #   [0.2] COLON (MidLetter)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u000B",
                      new String[] {  });
 
-    //  003A  0308  000B 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  003A  0308  000B   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u000B",
                      new String[] {  });
 
-    //  003A  3031 	#   [0.2] COLON (MidLetter)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  003A  3031   #   [0.2] COLON (MidLetter)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u3031",
                      new String[] { "\u3031" });
 
-    //  003A  0308  3031 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  003A  0308  3031   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u3031",
                      new String[] { "\u3031" });
 
-    //  003A  0041 	#   [0.2] COLON (MidLetter)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  003A  0041   #   [0.2] COLON (MidLetter)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0041",
                      new String[] { "\u0041" });
 
-    //  003A  0308  0041 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  003A  0308  0041   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0041",
                      new String[] { "\u0041" });
 
-    //  003A  003A 	#   [0.2] COLON (MidLetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  003A  003A   #   [0.2] COLON (MidLetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u003A",
                      new String[] {  });
 
-    //  003A  0308  003A 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  003A  0308  003A   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u003A",
                      new String[] {  });
 
-    //  003A  002C 	#   [0.2] COLON (MidLetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  003A  002C   #   [0.2] COLON (MidLetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u002C",
                      new String[] {  });
 
-    //  003A  0308  002C 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  003A  0308  002C   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u002C",
                      new String[] {  });
 
-    //  003A  0027 	#   [0.2] COLON (MidLetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  003A  0027   #   [0.2] COLON (MidLetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0027",
                      new String[] {  });
 
-    //  003A  0308  0027 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  003A  0308  0027   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0027",
                      new String[] {  });
 
-    //  003A  0030 	#   [0.2] COLON (MidLetter)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  003A  0030   #   [0.2] COLON (MidLetter)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0030",
                      new String[] { "\u0030" });
 
-    //  003A  0308  0030 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  003A  0308  0030   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0030",
                      new String[] { "\u0030" });
 
-    //  003A  005F 	#   [0.2] COLON (MidLetter)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  003A  005F   #   [0.2] COLON (MidLetter)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u005F",
                      new String[] {  });
 
-    //  003A  0308  005F 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  003A  0308  005F   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u005F",
                      new String[] {  });
 
-    //  003A  00AD 	#   [0.2] COLON (MidLetter)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  003A  00AD   #   [0.2] COLON (MidLetter)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u00AD",
                      new String[] {  });
 
-    //  003A  0308  00AD 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  003A  0308  00AD   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u00AD",
                      new String[] {  });
 
-    //  003A  0300 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  003A  0300   #   [0.2] COLON (MidLetter)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0300",
                      new String[] {  });
 
-    //  003A  0308  0300 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  003A  0308  0300   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0300",
                      new String[] {  });
 
-    //  003A  0061  2060 	#   [0.2] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  003A  0061  2060   #   [0.2] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    //  003A  0308  0061  2060 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  003A  0308  0061  2060   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    //  003A  0061  003A 	#   [0.2] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  003A  0061  003A   #   [0.2] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0061\u003A",
                      new String[] { "\u0061" });
 
-    //  003A  0308  0061  003A 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  003A  0308  0061  003A   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0061\u003A",
                      new String[] { "\u0061" });
 
-    //  003A  0061  0027 	#   [0.2] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  003A  0061  0027   #   [0.2] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0061\u0027",
                      new String[] { "\u0061" });
 
-    //  003A  0308  0061  0027 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  003A  0308  0061  0027   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0061\u0027",
                      new String[] { "\u0061" });
 
-    //  003A  0061  0027  2060 	#   [0.2] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  003A  0061  0027  2060   #   [0.2] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    //  003A  0308  0061  0027  2060 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  003A  0308  0061  0027  2060   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    //  003A  0061  002C 	#   [0.2] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  003A  0061  002C   #   [0.2] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0061\u002C",
                      new String[] { "\u0061" });
 
-    //  003A  0308  0061  002C 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  003A  0308  0061  002C   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0061\u002C",
                      new String[] { "\u0061" });
 
-    //  003A  0031  003A 	#   [0.2] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  003A  0031  003A   #   [0.2] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0031\u003A",
                      new String[] { "\u0031" });
 
-    //  003A  0308  0031  003A 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  003A  0308  0031  003A   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0031\u003A",
                      new String[] { "\u0031" });
 
-    //  003A  0031  0027 	#   [0.2] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  003A  0031  0027   #   [0.2] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0031\u0027",
                      new String[] { "\u0031" });
 
-    //  003A  0308  0031  0027 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  003A  0308  0031  0027   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0031\u0027",
                      new String[] { "\u0031" });
 
-    //  003A  0031  002C 	#   [0.2] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  003A  0031  002C   #   [0.2] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0031\u002C",
                      new String[] { "\u0031" });
 
-    //  003A  0308  0031  002C 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  003A  0308  0031  002C   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0031\u002C",
                      new String[] { "\u0031" });
 
-    //  003A  0031  002E  2060 	#   [0.2] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  003A  0031  002E  2060   #   [0.2] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    //  003A  0308  0031  002E  2060 	#   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  003A  0308  0031  002E  2060   #   [0.2] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u003A\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    //  002C  0001 	#   [0.2] COMMA (MidNum)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  002C  0001   #   [0.2] COMMA (MidNum)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0001",
                      new String[] {  });
 
-    //  002C  0308  0001 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  002C  0308  0001   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0001",
                      new String[] {  });
 
-    //  002C  000D 	#   [0.2] COMMA (MidNum)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  002C  000D   #   [0.2] COMMA (MidNum)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\r",
                      new String[] {  });
 
-    //  002C  0308  000D 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  002C  0308  000D   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\r",
                      new String[] {  });
 
-    //  002C  000A 	#   [0.2] COMMA (MidNum)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  002C  000A   #   [0.2] COMMA (MidNum)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\n",
                      new String[] {  });
 
-    //  002C  0308  000A 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  002C  0308  000A   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\n",
                      new String[] {  });
 
-    //  002C  000B 	#   [0.2] COMMA (MidNum)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  002C  000B   #   [0.2] COMMA (MidNum)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u000B",
                      new String[] {  });
 
-    //  002C  0308  000B 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  002C  0308  000B   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u000B",
                      new String[] {  });
 
-    //  002C  3031 	#   [0.2] COMMA (MidNum)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  002C  3031   #   [0.2] COMMA (MidNum)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u3031",
                      new String[] { "\u3031" });
 
-    //  002C  0308  3031 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  002C  0308  3031   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u3031",
                      new String[] { "\u3031" });
 
-    //  002C  0041 	#   [0.2] COMMA (MidNum)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  002C  0041   #   [0.2] COMMA (MidNum)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0041",
                      new String[] { "\u0041" });
 
-    //  002C  0308  0041 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  002C  0308  0041   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0041",
                      new String[] { "\u0041" });
 
-    //  002C  003A 	#   [0.2] COMMA (MidNum)  [999.0] COLON (MidLetter)  [0.3]
+    //  002C  003A   #   [0.2] COMMA (MidNum)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u003A",
                      new String[] {  });
 
-    //  002C  0308  003A 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  002C  0308  003A   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u003A",
                      new String[] {  });
 
-    //  002C  002C 	#   [0.2] COMMA (MidNum)  [999.0] COMMA (MidNum)  [0.3]
+    //  002C  002C   #   [0.2] COMMA (MidNum)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u002C",
                      new String[] {  });
 
-    //  002C  0308  002C 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  002C  0308  002C   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u002C",
                      new String[] {  });
 
-    //  002C  0027 	#   [0.2] COMMA (MidNum)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  002C  0027   #   [0.2] COMMA (MidNum)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0027",
                      new String[] {  });
 
-    //  002C  0308  0027 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  002C  0308  0027   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0027",
                      new String[] {  });
 
-    //  002C  0030 	#   [0.2] COMMA (MidNum)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  002C  0030   #   [0.2] COMMA (MidNum)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0030",
                      new String[] { "\u0030" });
 
-    //  002C  0308  0030 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  002C  0308  0030   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0030",
                      new String[] { "\u0030" });
 
-    //  002C  005F 	#   [0.2] COMMA (MidNum)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  002C  005F   #   [0.2] COMMA (MidNum)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u005F",
                      new String[] {  });
 
-    //  002C  0308  005F 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  002C  0308  005F   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u005F",
                      new String[] {  });
 
-    //  002C  00AD 	#   [0.2] COMMA (MidNum)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  002C  00AD   #   [0.2] COMMA (MidNum)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u00AD",
                      new String[] {  });
 
-    //  002C  0308  00AD 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  002C  0308  00AD   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u00AD",
                      new String[] {  });
 
-    //  002C  0300 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  002C  0300   #   [0.2] COMMA (MidNum)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0300",
                      new String[] {  });
 
-    //  002C  0308  0300 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  002C  0308  0300   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0300",
                      new String[] {  });
 
-    //  002C  0061  2060 	#   [0.2] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  002C  0061  2060   #   [0.2] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    //  002C  0308  0061  2060 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  002C  0308  0061  2060   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    //  002C  0061  003A 	#   [0.2] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  002C  0061  003A   #   [0.2] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0061\u003A",
                      new String[] { "\u0061" });
 
-    //  002C  0308  0061  003A 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  002C  0308  0061  003A   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0061\u003A",
                      new String[] { "\u0061" });
 
-    //  002C  0061  0027 	#   [0.2] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  002C  0061  0027   #   [0.2] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0061\u0027",
                      new String[] { "\u0061" });
 
-    //  002C  0308  0061  0027 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  002C  0308  0061  0027   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0061\u0027",
                      new String[] { "\u0061" });
 
-    //  002C  0061  0027  2060 	#   [0.2] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  002C  0061  0027  2060   #   [0.2] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    //  002C  0308  0061  0027  2060 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  002C  0308  0061  0027  2060   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    //  002C  0061  002C 	#   [0.2] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  002C  0061  002C   #   [0.2] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0061\u002C",
                      new String[] { "\u0061" });
 
-    //  002C  0308  0061  002C 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  002C  0308  0061  002C   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0061\u002C",
                      new String[] { "\u0061" });
 
-    //  002C  0031  003A 	#   [0.2] COMMA (MidNum)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  002C  0031  003A   #   [0.2] COMMA (MidNum)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0031\u003A",
                      new String[] { "\u0031" });
 
-    //  002C  0308  0031  003A 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  002C  0308  0031  003A   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0031\u003A",
                      new String[] { "\u0031" });
 
-    //  002C  0031  0027 	#   [0.2] COMMA (MidNum)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  002C  0031  0027   #   [0.2] COMMA (MidNum)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0031\u0027",
                      new String[] { "\u0031" });
 
-    //  002C  0308  0031  0027 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  002C  0308  0031  0027   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0031\u0027",
                      new String[] { "\u0031" });
 
-    //  002C  0031  002C 	#   [0.2] COMMA (MidNum)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  002C  0031  002C   #   [0.2] COMMA (MidNum)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0031\u002C",
                      new String[] { "\u0031" });
 
-    //  002C  0308  0031  002C 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  002C  0308  0031  002C   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0031\u002C",
                      new String[] { "\u0031" });
 
-    //  002C  0031  002E  2060 	#   [0.2] COMMA (MidNum)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  002C  0031  002E  2060   #   [0.2] COMMA (MidNum)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    //  002C  0308  0031  002E  2060 	#   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  002C  0308  0031  002E  2060   #   [0.2] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u002C\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    //  0027  0001 	#   [0.2] APOSTROPHE (MidNumLet)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0027  0001   #   [0.2] APOSTROPHE (MidNumLet)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0001",
                      new String[] {  });
 
-    //  0027  0308  0001 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0027  0308  0001   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0001",
                      new String[] {  });
 
-    //  0027  000D 	#   [0.2] APOSTROPHE (MidNumLet)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0027  000D   #   [0.2] APOSTROPHE (MidNumLet)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\r",
                      new String[] {  });
 
-    //  0027  0308  000D 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0027  0308  000D   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\r",
                      new String[] {  });
 
-    //  0027  000A 	#   [0.2] APOSTROPHE (MidNumLet)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0027  000A   #   [0.2] APOSTROPHE (MidNumLet)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\n",
                      new String[] {  });
 
-    //  0027  0308  000A 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0027  0308  000A   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\n",
                      new String[] {  });
 
-    //  0027  000B 	#   [0.2] APOSTROPHE (MidNumLet)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0027  000B   #   [0.2] APOSTROPHE (MidNumLet)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u000B",
                      new String[] {  });
 
-    //  0027  0308  000B 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0027  0308  000B   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u000B",
                      new String[] {  });
 
-    //  0027  3031 	#   [0.2] APOSTROPHE (MidNumLet)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0027  3031   #   [0.2] APOSTROPHE (MidNumLet)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u3031",
                      new String[] { "\u3031" });
 
-    //  0027  0308  3031 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0027  0308  3031   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u3031",
                      new String[] { "\u3031" });
 
-    //  0027  0041 	#   [0.2] APOSTROPHE (MidNumLet)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0027  0041   #   [0.2] APOSTROPHE (MidNumLet)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0041",
                      new String[] { "\u0041" });
 
-    //  0027  0308  0041 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0027  0308  0041   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0041",
                      new String[] { "\u0041" });
 
-    //  0027  003A 	#   [0.2] APOSTROPHE (MidNumLet)  [999.0] COLON (MidLetter)  [0.3]
+    //  0027  003A   #   [0.2] APOSTROPHE (MidNumLet)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u003A",
                      new String[] {  });
 
-    //  0027  0308  003A 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  0027  0308  003A   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u003A",
                      new String[] {  });
 
-    //  0027  002C 	#   [0.2] APOSTROPHE (MidNumLet)  [999.0] COMMA (MidNum)  [0.3]
+    //  0027  002C   #   [0.2] APOSTROPHE (MidNumLet)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u002C",
                      new String[] {  });
 
-    //  0027  0308  002C 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  0027  0308  002C   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u002C",
                      new String[] {  });
 
-    //  0027  0027 	#   [0.2] APOSTROPHE (MidNumLet)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0027  0027   #   [0.2] APOSTROPHE (MidNumLet)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0027",
                      new String[] {  });
 
-    //  0027  0308  0027 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0027  0308  0027   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0027",
                      new String[] {  });
 
-    //  0027  0030 	#   [0.2] APOSTROPHE (MidNumLet)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0027  0030   #   [0.2] APOSTROPHE (MidNumLet)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0030",
                      new String[] { "\u0030" });
 
-    //  0027  0308  0030 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0027  0308  0030   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0030",
                      new String[] { "\u0030" });
 
-    //  0027  005F 	#   [0.2] APOSTROPHE (MidNumLet)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0027  005F   #   [0.2] APOSTROPHE (MidNumLet)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u005F",
                      new String[] {  });
 
-    //  0027  0308  005F 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0027  0308  005F   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u005F",
                      new String[] {  });
 
-    //  0027  00AD 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0027  00AD   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u00AD",
                      new String[] {  });
 
-    //  0027  0308  00AD 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0027  0308  00AD   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u00AD",
                      new String[] {  });
 
-    //  0027  0300 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0027  0300   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0300",
                      new String[] {  });
 
-    //  0027  0308  0300 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0027  0308  0300   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0300",
                      new String[] {  });
 
-    //  0027  0061  2060 	#   [0.2] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0027  0061  2060   #   [0.2] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    //  0027  0308  0061  2060 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0027  0308  0061  2060   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    //  0027  0061  003A 	#   [0.2] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0027  0061  003A   #   [0.2] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0061\u003A",
                      new String[] { "\u0061" });
 
-    //  0027  0308  0061  003A 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0027  0308  0061  003A   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0061\u003A",
                      new String[] { "\u0061" });
 
-    //  0027  0061  0027 	#   [0.2] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0027  0061  0027   #   [0.2] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0061\u0027",
                      new String[] { "\u0061" });
 
-    //  0027  0308  0061  0027 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0027  0308  0061  0027   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0061\u0027",
                      new String[] { "\u0061" });
 
-    //  0027  0061  0027  2060 	#   [0.2] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0027  0061  0027  2060   #   [0.2] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    //  0027  0308  0061  0027  2060 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0027  0308  0061  0027  2060   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    //  0027  0061  002C 	#   [0.2] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0027  0061  002C   #   [0.2] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0061\u002C",
                      new String[] { "\u0061" });
 
-    //  0027  0308  0061  002C 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0027  0308  0061  002C   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0061\u002C",
                      new String[] { "\u0061" });
 
-    //  0027  0031  003A 	#   [0.2] APOSTROPHE (MidNumLet)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0027  0031  003A   #   [0.2] APOSTROPHE (MidNumLet)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0031\u003A",
                      new String[] { "\u0031" });
 
-    //  0027  0308  0031  003A 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0027  0308  0031  003A   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0031\u003A",
                      new String[] { "\u0031" });
 
-    //  0027  0031  0027 	#   [0.2] APOSTROPHE (MidNumLet)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0027  0031  0027   #   [0.2] APOSTROPHE (MidNumLet)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0031\u0027",
                      new String[] { "\u0031" });
 
-    //  0027  0308  0031  0027 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0027  0308  0031  0027   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0031\u0027",
                      new String[] { "\u0031" });
 
-    //  0027  0031  002C 	#   [0.2] APOSTROPHE (MidNumLet)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0027  0031  002C   #   [0.2] APOSTROPHE (MidNumLet)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0031\u002C",
                      new String[] { "\u0031" });
 
-    //  0027  0308  0031  002C 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0027  0308  0031  002C   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0031\u002C",
                      new String[] { "\u0031" });
 
-    //  0027  0031  002E  2060 	#   [0.2] APOSTROPHE (MidNumLet)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0027  0031  002E  2060   #   [0.2] APOSTROPHE (MidNumLet)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    //  0027  0308  0031  002E  2060 	#   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0027  0308  0031  002E  2060   #   [0.2] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0027\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    //  0030  0001 	#   [0.2] DIGIT ZERO (Numeric)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0030  0001   #   [0.2] DIGIT ZERO (Numeric)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0001",
                      new String[] { "\u0030" });
 
-    //  0030  0308  0001 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0030  0308  0001   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0001",
                      new String[] { "\u0030\u0308" });
 
-    //  0030  000D 	#   [0.2] DIGIT ZERO (Numeric)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0030  000D   #   [0.2] DIGIT ZERO (Numeric)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\r",
                      new String[] { "\u0030" });
 
-    //  0030  0308  000D 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0030  0308  000D   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\r",
                      new String[] { "\u0030\u0308" });
 
-    //  0030  000A 	#   [0.2] DIGIT ZERO (Numeric)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0030  000A   #   [0.2] DIGIT ZERO (Numeric)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\n",
                      new String[] { "\u0030" });
 
-    //  0030  0308  000A 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0030  0308  000A   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\n",
                      new String[] { "\u0030\u0308" });
 
-    //  0030  000B 	#   [0.2] DIGIT ZERO (Numeric)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0030  000B   #   [0.2] DIGIT ZERO (Numeric)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u000B",
                      new String[] { "\u0030" });
 
-    //  0030  0308  000B 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0030  0308  000B   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u000B",
                      new String[] { "\u0030\u0308" });
 
-    //  0030  3031 	#   [0.2] DIGIT ZERO (Numeric)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0030  3031   #   [0.2] DIGIT ZERO (Numeric)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u3031",
                      new String[] { "\u0030", "\u3031" });
 
-    //  0030  0308  3031 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0030  0308  3031   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u3031",
                      new String[] { "\u0030\u0308", "\u3031" });
 
-    //  0030  0041 	#   [0.2] DIGIT ZERO (Numeric)  [10.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0030  0041   #   [0.2] DIGIT ZERO (Numeric)  [10.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0041",
                      new String[] { "\u0030\u0041" });
 
-    //  0030  0308  0041 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [10.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0030  0308  0041   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [10.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0041",
                      new String[] { "\u0030\u0308\u0041" });
 
-    //  0030  003A 	#   [0.2] DIGIT ZERO (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0030  003A   #   [0.2] DIGIT ZERO (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u003A",
                      new String[] { "\u0030" });
 
-    //  0030  0308  003A 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  0030  0308  003A   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u003A",
                      new String[] { "\u0030\u0308" });
 
-    //  0030  002C 	#   [0.2] DIGIT ZERO (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0030  002C   #   [0.2] DIGIT ZERO (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u002C",
                      new String[] { "\u0030" });
 
-    //  0030  0308  002C 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  0030  0308  002C   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u002C",
                      new String[] { "\u0030\u0308" });
 
-    //  0030  0027 	#   [0.2] DIGIT ZERO (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0030  0027   #   [0.2] DIGIT ZERO (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0027",
                      new String[] { "\u0030" });
 
-    //  0030  0308  0027 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0030  0308  0027   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0027",
                      new String[] { "\u0030\u0308" });
 
-    //  0030  0030 	#   [0.2] DIGIT ZERO (Numeric)  [8.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0030  0030   #   [0.2] DIGIT ZERO (Numeric)  [8.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0030",
                      new String[] { "\u0030\u0030" });
 
-    //  0030  0308  0030 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [8.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0030  0308  0030   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [8.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0030",
                      new String[] { "\u0030\u0308\u0030" });
 
-    //  0030  005F 	#   [0.2] DIGIT ZERO (Numeric)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
+    //  0030  005F   #   [0.2] DIGIT ZERO (Numeric)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u005F",
                      new String[] { "\u0030\u005F" });
 
-    //  0030  0308  005F 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
+    //  0030  0308  005F   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u005F",
                      new String[] { "\u0030\u0308\u005F" });
 
-    //  0030  00AD 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0030  00AD   #   [0.2] DIGIT ZERO (Numeric)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u00AD",
                      new String[] { "\u0030\u00AD" });
 
-    //  0030  0308  00AD 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0030  0308  00AD   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u00AD",
                      new String[] { "\u0030\u0308\u00AD" });
 
-    //  0030  0300 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0030  0300   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0300",
                      new String[] { "\u0030\u0300" });
 
-    //  0030  0308  0300 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0030  0308  0300   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0300",
                      new String[] { "\u0030\u0308\u0300" });
 
-    //  0030  0061  2060 	#   [0.2] DIGIT ZERO (Numeric)  [10.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0030  0061  2060   #   [0.2] DIGIT ZERO (Numeric)  [10.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0061\u2060",
                      new String[] { "\u0030\u0061\u2060" });
 
-    //  0030  0308  0061  2060 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [10.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0030  0308  0061  2060   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [10.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0061\u2060",
                      new String[] { "\u0030\u0308\u0061\u2060" });
 
-    //  0030  0061  003A 	#   [0.2] DIGIT ZERO (Numeric)  [10.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0030  0061  003A   #   [0.2] DIGIT ZERO (Numeric)  [10.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0061\u003A",
                      new String[] { "\u0030\u0061" });
 
-    //  0030  0308  0061  003A 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [10.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0030  0308  0061  003A   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [10.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0061\u003A",
                      new String[] { "\u0030\u0308\u0061" });
 
-    //  0030  0061  0027 	#   [0.2] DIGIT ZERO (Numeric)  [10.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0030  0061  0027   #   [0.2] DIGIT ZERO (Numeric)  [10.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0061\u0027",
                      new String[] { "\u0030\u0061" });
 
-    //  0030  0308  0061  0027 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [10.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0030  0308  0061  0027   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [10.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0061\u0027",
                      new String[] { "\u0030\u0308\u0061" });
 
-    //  0030  0061  0027  2060 	#   [0.2] DIGIT ZERO (Numeric)  [10.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0030  0061  0027  2060   #   [0.2] DIGIT ZERO (Numeric)  [10.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0061\u0027\u2060",
                      new String[] { "\u0030\u0061" });
 
-    //  0030  0308  0061  0027  2060 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [10.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0030  0308  0061  0027  2060   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [10.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0061\u0027\u2060",
                      new String[] { "\u0030\u0308\u0061" });
 
-    //  0030  0061  002C 	#   [0.2] DIGIT ZERO (Numeric)  [10.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0030  0061  002C   #   [0.2] DIGIT ZERO (Numeric)  [10.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0061\u002C",
                      new String[] { "\u0030\u0061" });
 
-    //  0030  0308  0061  002C 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [10.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0030  0308  0061  002C   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [10.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0061\u002C",
                      new String[] { "\u0030\u0308\u0061" });
 
-    //  0030  0031  003A 	#   [0.2] DIGIT ZERO (Numeric)  [8.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0030  0031  003A   #   [0.2] DIGIT ZERO (Numeric)  [8.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0031\u003A",
                      new String[] { "\u0030\u0031" });
 
-    //  0030  0308  0031  003A 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [8.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0030  0308  0031  003A   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [8.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0031\u003A",
                      new String[] { "\u0030\u0308\u0031" });
 
-    //  0030  0031  0027 	#   [0.2] DIGIT ZERO (Numeric)  [8.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0030  0031  0027   #   [0.2] DIGIT ZERO (Numeric)  [8.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0031\u0027",
                      new String[] { "\u0030\u0031" });
 
-    //  0030  0308  0031  0027 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [8.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0030  0308  0031  0027   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [8.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0031\u0027",
                      new String[] { "\u0030\u0308\u0031" });
 
-    //  0030  0031  002C 	#   [0.2] DIGIT ZERO (Numeric)  [8.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0030  0031  002C   #   [0.2] DIGIT ZERO (Numeric)  [8.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0031\u002C",
                      new String[] { "\u0030\u0031" });
 
-    //  0030  0308  0031  002C 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [8.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0030  0308  0031  002C   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [8.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0031\u002C",
                      new String[] { "\u0030\u0308\u0031" });
 
-    //  0030  0031  002E  2060 	#   [0.2] DIGIT ZERO (Numeric)  [8.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0030  0031  002E  2060   #   [0.2] DIGIT ZERO (Numeric)  [8.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0031\u002E\u2060",
                      new String[] { "\u0030\u0031" });
 
-    //  0030  0308  0031  002E  2060 	#   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [8.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0030  0308  0031  002E  2060   #   [0.2] DIGIT ZERO (Numeric)  [4.0] COMBINING DIAERESIS (Extend_FE)  [8.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0030\u0308\u0031\u002E\u2060",
                      new String[] { "\u0030\u0308\u0031" });
 
-    //  005F  0001 	#   [0.2] LOW LINE (ExtendNumLet)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  005F  0001   #   [0.2] LOW LINE (ExtendNumLet)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0001",
                      new String[] {  });
 
-    //  005F  0308  0001 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  005F  0308  0001   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0001",
                      new String[] {  });
 
-    //  005F  000D 	#   [0.2] LOW LINE (ExtendNumLet)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  005F  000D   #   [0.2] LOW LINE (ExtendNumLet)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\r",
                      new String[] {  });
 
-    //  005F  0308  000D 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  005F  0308  000D   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\r",
                      new String[] {  });
 
-    //  005F  000A 	#   [0.2] LOW LINE (ExtendNumLet)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  005F  000A   #   [0.2] LOW LINE (ExtendNumLet)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\n",
                      new String[] {  });
 
-    //  005F  0308  000A 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  005F  0308  000A   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\n",
                      new String[] {  });
 
-    //  005F  000B 	#   [0.2] LOW LINE (ExtendNumLet)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  005F  000B   #   [0.2] LOW LINE (ExtendNumLet)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u000B",
                      new String[] {  });
 
-    //  005F  0308  000B 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  005F  0308  000B   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u000B",
                      new String[] {  });
 
-    //  005F  3031 	#   [0.2] LOW LINE (ExtendNumLet)  [13.2] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  005F  3031   #   [0.2] LOW LINE (ExtendNumLet)  [13.2] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u3031",
                      new String[] { "\u005F\u3031" });
 
-    //  005F  0308  3031 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  005F  0308  3031   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u3031",
                      new String[] { "\u005F\u0308\u3031" });
 
-    //  005F  0041 	#   [0.2] LOW LINE (ExtendNumLet)  [13.2] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  005F  0041   #   [0.2] LOW LINE (ExtendNumLet)  [13.2] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0041",
                      new String[] { "\u005F\u0041" });
 
-    //  005F  0308  0041 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  005F  0308  0041   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0041",
                      new String[] { "\u005F\u0308\u0041" });
 
-    //  005F  003A 	#   [0.2] LOW LINE (ExtendNumLet)  [999.0] COLON (MidLetter)  [0.3]
+    //  005F  003A   #   [0.2] LOW LINE (ExtendNumLet)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u003A",
                      new String[] {  });
 
-    //  005F  0308  003A 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  005F  0308  003A   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u003A",
                      new String[] {  });
 
-    //  005F  002C 	#   [0.2] LOW LINE (ExtendNumLet)  [999.0] COMMA (MidNum)  [0.3]
+    //  005F  002C   #   [0.2] LOW LINE (ExtendNumLet)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u002C",
                      new String[] {  });
 
-    //  005F  0308  002C 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  005F  0308  002C   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u002C",
                      new String[] {  });
 
-    //  005F  0027 	#   [0.2] LOW LINE (ExtendNumLet)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  005F  0027   #   [0.2] LOW LINE (ExtendNumLet)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0027",
                      new String[] {  });
 
-    //  005F  0308  0027 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  005F  0308  0027   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0027",
                      new String[] {  });
 
-    //  005F  0030 	#   [0.2] LOW LINE (ExtendNumLet)  [13.2] DIGIT ZERO (Numeric)  [0.3]
+    //  005F  0030   #   [0.2] LOW LINE (ExtendNumLet)  [13.2] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0030",
                      new String[] { "\u005F\u0030" });
 
-    //  005F  0308  0030 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] DIGIT ZERO (Numeric)  [0.3]
+    //  005F  0308  0030   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0030",
                      new String[] { "\u005F\u0308\u0030" });
 
-    //  005F  005F 	#   [0.2] LOW LINE (ExtendNumLet)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
+    //  005F  005F   #   [0.2] LOW LINE (ExtendNumLet)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u005F",
                      new String[] {  });
 
-    //  005F  0308  005F 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
+    //  005F  0308  005F   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u005F",
                      new String[] {  });
 
-    //  005F  00AD 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  005F  00AD   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u00AD",
                      new String[] {  });
 
-    //  005F  0308  00AD 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  005F  0308  00AD   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u00AD",
                      new String[] {  });
 
-    //  005F  0300 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  005F  0300   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0300",
                      new String[] {  });
 
-    //  005F  0308  0300 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  005F  0308  0300   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0300",
                      new String[] {  });
 
-    //  005F  0061  2060 	#   [0.2] LOW LINE (ExtendNumLet)  [13.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  005F  0061  2060   #   [0.2] LOW LINE (ExtendNumLet)  [13.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0061\u2060",
                      new String[] { "\u005F\u0061\u2060" });
 
-    //  005F  0308  0061  2060 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  005F  0308  0061  2060   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0061\u2060",
                      new String[] { "\u005F\u0308\u0061\u2060" });
 
-    //  005F  0061  003A 	#   [0.2] LOW LINE (ExtendNumLet)  [13.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  005F  0061  003A   #   [0.2] LOW LINE (ExtendNumLet)  [13.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0061\u003A",
                      new String[] { "\u005F\u0061" });
 
-    //  005F  0308  0061  003A 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  005F  0308  0061  003A   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0061\u003A",
                      new String[] { "\u005F\u0308\u0061" });
 
-    //  005F  0061  0027 	#   [0.2] LOW LINE (ExtendNumLet)  [13.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  005F  0061  0027   #   [0.2] LOW LINE (ExtendNumLet)  [13.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0061\u0027",
                      new String[] { "\u005F\u0061" });
 
-    //  005F  0308  0061  0027 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  005F  0308  0061  0027   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0061\u0027",
                      new String[] { "\u005F\u0308\u0061" });
 
-    //  005F  0061  0027  2060 	#   [0.2] LOW LINE (ExtendNumLet)  [13.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  005F  0061  0027  2060   #   [0.2] LOW LINE (ExtendNumLet)  [13.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0061\u0027\u2060",
                      new String[] { "\u005F\u0061" });
 
-    //  005F  0308  0061  0027  2060 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  005F  0308  0061  0027  2060   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0061\u0027\u2060",
                      new String[] { "\u005F\u0308\u0061" });
 
-    //  005F  0061  002C 	#   [0.2] LOW LINE (ExtendNumLet)  [13.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  005F  0061  002C   #   [0.2] LOW LINE (ExtendNumLet)  [13.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0061\u002C",
                      new String[] { "\u005F\u0061" });
 
-    //  005F  0308  0061  002C 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  005F  0308  0061  002C   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0061\u002C",
                      new String[] { "\u005F\u0308\u0061" });
 
-    //  005F  0031  003A 	#   [0.2] LOW LINE (ExtendNumLet)  [13.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  005F  0031  003A   #   [0.2] LOW LINE (ExtendNumLet)  [13.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0031\u003A",
                      new String[] { "\u005F\u0031" });
 
-    //  005F  0308  0031  003A 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  005F  0308  0031  003A   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0031\u003A",
                      new String[] { "\u005F\u0308\u0031" });
 
-    //  005F  0031  0027 	#   [0.2] LOW LINE (ExtendNumLet)  [13.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  005F  0031  0027   #   [0.2] LOW LINE (ExtendNumLet)  [13.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0031\u0027",
                      new String[] { "\u005F\u0031" });
 
-    //  005F  0308  0031  0027 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  005F  0308  0031  0027   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0031\u0027",
                      new String[] { "\u005F\u0308\u0031" });
 
-    //  005F  0031  002C 	#   [0.2] LOW LINE (ExtendNumLet)  [13.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  005F  0031  002C   #   [0.2] LOW LINE (ExtendNumLet)  [13.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0031\u002C",
                      new String[] { "\u005F\u0031" });
 
-    //  005F  0308  0031  002C 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  005F  0308  0031  002C   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0031\u002C",
                      new String[] { "\u005F\u0308\u0031" });
 
-    //  005F  0031  002E  2060 	#   [0.2] LOW LINE (ExtendNumLet)  [13.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  005F  0031  002E  2060   #   [0.2] LOW LINE (ExtendNumLet)  [13.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0031\u002E\u2060",
                      new String[] { "\u005F\u0031" });
 
-    //  005F  0308  0031  002E  2060 	#   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  005F  0308  0031  002E  2060   #   [0.2] LOW LINE (ExtendNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u005F\u0308\u0031\u002E\u2060",
                      new String[] { "\u005F\u0308\u0031" });
 
-    //  00AD  0001 	#   [0.2] SOFT HYPHEN (Format_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  00AD  0001   #   [0.2] SOFT HYPHEN (Format_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0001",
                      new String[] {  });
 
-    //  00AD  0308  0001 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  00AD  0308  0001   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0001",
                      new String[] {  });
 
-    //  00AD  000D 	#   [0.2] SOFT HYPHEN (Format_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  00AD  000D   #   [0.2] SOFT HYPHEN (Format_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\r",
                      new String[] {  });
 
-    //  00AD  0308  000D 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  00AD  0308  000D   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\r",
                      new String[] {  });
 
-    //  00AD  000A 	#   [0.2] SOFT HYPHEN (Format_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  00AD  000A   #   [0.2] SOFT HYPHEN (Format_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\n",
                      new String[] {  });
 
-    //  00AD  0308  000A 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  00AD  0308  000A   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\n",
                      new String[] {  });
 
-    //  00AD  000B 	#   [0.2] SOFT HYPHEN (Format_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  00AD  000B   #   [0.2] SOFT HYPHEN (Format_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u000B",
                      new String[] {  });
 
-    //  00AD  0308  000B 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  00AD  0308  000B   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u000B",
                      new String[] {  });
 
-    //  00AD  3031 	#   [0.2] SOFT HYPHEN (Format_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  00AD  3031   #   [0.2] SOFT HYPHEN (Format_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u3031",
                      new String[] { "\u3031" });
 
-    //  00AD  0308  3031 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  00AD  0308  3031   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u3031",
                      new String[] { "\u3031" });
 
-    //  00AD  0041 	#   [0.2] SOFT HYPHEN (Format_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  00AD  0041   #   [0.2] SOFT HYPHEN (Format_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0041",
                      new String[] { "\u0041" });
 
-    //  00AD  0308  0041 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  00AD  0308  0041   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0041",
                      new String[] { "\u0041" });
 
-    //  00AD  003A 	#   [0.2] SOFT HYPHEN (Format_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  00AD  003A   #   [0.2] SOFT HYPHEN (Format_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u003A",
                      new String[] {  });
 
-    //  00AD  0308  003A 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  00AD  0308  003A   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u003A",
                      new String[] {  });
 
-    //  00AD  002C 	#   [0.2] SOFT HYPHEN (Format_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  00AD  002C   #   [0.2] SOFT HYPHEN (Format_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u002C",
                      new String[] {  });
 
-    //  00AD  0308  002C 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  00AD  0308  002C   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u002C",
                      new String[] {  });
 
-    //  00AD  0027 	#   [0.2] SOFT HYPHEN (Format_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  00AD  0027   #   [0.2] SOFT HYPHEN (Format_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0027",
                      new String[] {  });
 
-    //  00AD  0308  0027 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  00AD  0308  0027   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0027",
                      new String[] {  });
 
-    //  00AD  0030 	#   [0.2] SOFT HYPHEN (Format_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  00AD  0030   #   [0.2] SOFT HYPHEN (Format_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0030",
                      new String[] { "\u0030" });
 
-    //  00AD  0308  0030 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  00AD  0308  0030   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0030",
                      new String[] { "\u0030" });
 
-    //  00AD  005F 	#   [0.2] SOFT HYPHEN (Format_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  00AD  005F   #   [0.2] SOFT HYPHEN (Format_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u005F",
                      new String[] {  });
 
-    //  00AD  0308  005F 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  00AD  0308  005F   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u005F",
                      new String[] {  });
 
-    //  00AD  00AD 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  00AD  00AD   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u00AD",
                      new String[] {  });
 
-    //  00AD  0308  00AD 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  00AD  0308  00AD   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u00AD",
                      new String[] {  });
 
-    //  00AD  0300 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  00AD  0300   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0300",
                      new String[] {  });
 
-    //  00AD  0308  0300 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  00AD  0308  0300   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0300",
                      new String[] {  });
 
-    //  00AD  0061  2060 	#   [0.2] SOFT HYPHEN (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  00AD  0061  2060   #   [0.2] SOFT HYPHEN (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    //  00AD  0308  0061  2060 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  00AD  0308  0061  2060   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    //  00AD  0061  003A 	#   [0.2] SOFT HYPHEN (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  00AD  0061  003A   #   [0.2] SOFT HYPHEN (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0061\u003A",
                      new String[] { "\u0061" });
 
-    //  00AD  0308  0061  003A 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  00AD  0308  0061  003A   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0061\u003A",
                      new String[] { "\u0061" });
 
-    //  00AD  0061  0027 	#   [0.2] SOFT HYPHEN (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  00AD  0061  0027   #   [0.2] SOFT HYPHEN (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0061\u0027",
                      new String[] { "\u0061" });
 
-    //  00AD  0308  0061  0027 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  00AD  0308  0061  0027   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0061\u0027",
                      new String[] { "\u0061" });
 
-    //  00AD  0061  0027  2060 	#   [0.2] SOFT HYPHEN (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  00AD  0061  0027  2060   #   [0.2] SOFT HYPHEN (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    //  00AD  0308  0061  0027  2060 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  00AD  0308  0061  0027  2060   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    //  00AD  0061  002C 	#   [0.2] SOFT HYPHEN (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  00AD  0061  002C   #   [0.2] SOFT HYPHEN (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0061\u002C",
                      new String[] { "\u0061" });
 
-    //  00AD  0308  0061  002C 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  00AD  0308  0061  002C   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0061\u002C",
                      new String[] { "\u0061" });
 
-    //  00AD  0031  003A 	#   [0.2] SOFT HYPHEN (Format_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  00AD  0031  003A   #   [0.2] SOFT HYPHEN (Format_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0031\u003A",
                      new String[] { "\u0031" });
 
-    //  00AD  0308  0031  003A 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  00AD  0308  0031  003A   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0031\u003A",
                      new String[] { "\u0031" });
 
-    //  00AD  0031  0027 	#   [0.2] SOFT HYPHEN (Format_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  00AD  0031  0027   #   [0.2] SOFT HYPHEN (Format_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0031\u0027",
                      new String[] { "\u0031" });
 
-    //  00AD  0308  0031  0027 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  00AD  0308  0031  0027   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0031\u0027",
                      new String[] { "\u0031" });
 
-    //  00AD  0031  002C 	#   [0.2] SOFT HYPHEN (Format_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  00AD  0031  002C   #   [0.2] SOFT HYPHEN (Format_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0031\u002C",
                      new String[] { "\u0031" });
 
-    //  00AD  0308  0031  002C 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  00AD  0308  0031  002C   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0031\u002C",
                      new String[] { "\u0031" });
 
-    //  00AD  0031  002E  2060 	#   [0.2] SOFT HYPHEN (Format_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  00AD  0031  002E  2060   #   [0.2] SOFT HYPHEN (Format_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    //  00AD  0308  0031  002E  2060 	#   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  00AD  0308  0031  002E  2060   #   [0.2] SOFT HYPHEN (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u00AD\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    //  0300  0001 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0300  0001   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0001",
                      new String[] {  });
 
-    //  0300  0308  0001 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0300  0308  0001   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0001",
                      new String[] {  });
 
-    //  0300  000D 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0300  000D   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\r",
                      new String[] {  });
 
-    //  0300  0308  000D 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0300  0308  000D   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\r",
                      new String[] {  });
 
-    //  0300  000A 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0300  000A   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\n",
                      new String[] {  });
 
-    //  0300  0308  000A 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0300  0308  000A   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\n",
                      new String[] {  });
 
-    //  0300  000B 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0300  000B   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u000B",
                      new String[] {  });
 
-    //  0300  0308  000B 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0300  0308  000B   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u000B",
                      new String[] {  });
 
-    //  0300  3031 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0300  3031   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u3031",
                      new String[] { "\u3031" });
 
-    //  0300  0308  3031 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0300  0308  3031   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u3031",
                      new String[] { "\u3031" });
 
-    //  0300  0041 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0300  0041   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0041",
                      new String[] { "\u0041" });
 
-    //  0300  0308  0041 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0300  0308  0041   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0041",
                      new String[] { "\u0041" });
 
-    //  0300  003A 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  0300  003A   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u003A",
                      new String[] {  });
 
-    //  0300  0308  003A 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  0300  0308  003A   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u003A",
                      new String[] {  });
 
-    //  0300  002C 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  0300  002C   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u002C",
                      new String[] {  });
 
-    //  0300  0308  002C 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  0300  0308  002C   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u002C",
                      new String[] {  });
 
-    //  0300  0027 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0300  0027   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0027",
                      new String[] {  });
 
-    //  0300  0308  0027 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0300  0308  0027   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0027",
                      new String[] {  });
 
-    //  0300  0030 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0300  0030   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0030",
                      new String[] { "\u0030" });
 
-    //  0300  0308  0030 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0300  0308  0030   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0030",
                      new String[] { "\u0030" });
 
-    //  0300  005F 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0300  005F   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u005F",
                      new String[] {  });
 
-    //  0300  0308  005F 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0300  0308  005F   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u005F",
                      new String[] {  });
 
-    //  0300  00AD 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0300  00AD   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u00AD",
                      new String[] {  });
 
-    //  0300  0308  00AD 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0300  0308  00AD   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u00AD",
                      new String[] {  });
 
-    //  0300  0300 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0300  0300   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0300",
                      new String[] {  });
 
-    //  0300  0308  0300 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0300  0308  0300   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0300",
                      new String[] {  });
 
-    //  0300  0061  2060 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0300  0061  2060   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    //  0300  0308  0061  2060 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0300  0308  0061  2060   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060" });
 
-    //  0300  0061  003A 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0300  0061  003A   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0061\u003A",
                      new String[] { "\u0061" });
 
-    //  0300  0308  0061  003A 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0300  0308  0061  003A   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0061\u003A",
                      new String[] { "\u0061" });
 
-    //  0300  0061  0027 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0300  0061  0027   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0061\u0027",
                      new String[] { "\u0061" });
 
-    //  0300  0308  0061  0027 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0300  0308  0061  0027   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0061\u0027",
                      new String[] { "\u0061" });
 
-    //  0300  0061  0027  2060 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0300  0061  0027  2060   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    //  0300  0308  0061  0027  2060 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0300  0308  0061  0027  2060   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061" });
 
-    //  0300  0061  002C 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0300  0061  002C   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0061\u002C",
                      new String[] { "\u0061" });
 
-    //  0300  0308  0061  002C 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0300  0308  0061  002C   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0061\u002C",
                      new String[] { "\u0061" });
 
-    //  0300  0031  003A 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0300  0031  003A   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0031\u003A",
                      new String[] { "\u0031" });
 
-    //  0300  0308  0031  003A 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0300  0308  0031  003A   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0031\u003A",
                      new String[] { "\u0031" });
 
-    //  0300  0031  0027 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0300  0031  0027   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0031\u0027",
                      new String[] { "\u0031" });
 
-    //  0300  0308  0031  0027 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0300  0308  0031  0027   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0031\u0027",
                      new String[] { "\u0031" });
 
-    //  0300  0031  002C 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0300  0031  002C   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0031\u002C",
                      new String[] { "\u0031" });
 
-    //  0300  0308  0031  002C 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0300  0308  0031  002C   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0031\u002C",
                      new String[] { "\u0031" });
 
-    //  0300  0031  002E  2060 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0300  0031  002E  2060   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    //  0300  0308  0031  002E  2060 	#   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0300  0308  0031  002E  2060   #   [0.2] COMBINING GRAVE ACCENT (Extend_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0300\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031" });
 
-    //  0061  2060  0001 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0061  2060  0001   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0001",
                      new String[] { "\u0061\u2060" });
 
-    //  0061  2060  0308  0001 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0061  2060  0308  0001   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0001",
                      new String[] { "\u0061\u2060\u0308" });
 
-    //  0061  2060  000D 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0061  2060  000D   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\r",
                      new String[] { "\u0061\u2060" });
 
-    //  0061  2060  0308  000D 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0061  2060  0308  000D   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\r",
                      new String[] { "\u0061\u2060\u0308" });
 
-    //  0061  2060  000A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0061  2060  000A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\n",
                      new String[] { "\u0061\u2060" });
 
-    //  0061  2060  0308  000A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0061  2060  0308  000A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\n",
                      new String[] { "\u0061\u2060\u0308" });
 
-    //  0061  2060  000B 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0061  2060  000B   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u000B",
                      new String[] { "\u0061\u2060" });
 
-    //  0061  2060  0308  000B 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0061  2060  0308  000B   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u000B",
                      new String[] { "\u0061\u2060\u0308" });
 
-    //  0061  2060  3031 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0061  2060  3031   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u3031",
                      new String[] { "\u0061\u2060", "\u3031" });
 
-    //  0061  2060  0308  3031 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0061  2060  0308  3031   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u3031",
                      new String[] { "\u0061\u2060\u0308", "\u3031" });
 
-    //  0061  2060  0041 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0061  2060  0041   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0041",
                      new String[] { "\u0061\u2060\u0041" });
 
-    //  0061  2060  0308  0041 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0061  2060  0308  0041   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0041",
                      new String[] { "\u0061\u2060\u0308\u0041" });
 
-    //  0061  2060  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  2060  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u003A",
                      new String[] { "\u0061\u2060" });
 
-    //  0061  2060  0308  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  2060  0308  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u003A",
                      new String[] { "\u0061\u2060\u0308" });
 
-    //  0061  2060  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  2060  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u002C",
                      new String[] { "\u0061\u2060" });
 
-    //  0061  2060  0308  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  2060  0308  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u002C",
                      new String[] { "\u0061\u2060\u0308" });
 
-    //  0061  2060  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  2060  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0027",
                      new String[] { "\u0061\u2060" });
 
-    //  0061  2060  0308  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  2060  0308  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0027",
                      new String[] { "\u0061\u2060\u0308" });
 
-    //  0061  2060  0030 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [9.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0061  2060  0030   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [9.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0030",
                      new String[] { "\u0061\u2060\u0030" });
 
-    //  0061  2060  0308  0030 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0061  2060  0308  0030   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0030",
                      new String[] { "\u0061\u2060\u0308\u0030" });
 
-    //  0061  2060  005F 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
+    //  0061  2060  005F   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u005F",
                      new String[] { "\u0061\u2060\u005F" });
 
-    //  0061  2060  0308  005F 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
+    //  0061  2060  0308  005F   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [13.1] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u005F",
                      new String[] { "\u0061\u2060\u0308\u005F" });
 
-    //  0061  2060  00AD 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0061  2060  00AD   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u00AD",
                      new String[] { "\u0061\u2060\u00AD" });
 
-    //  0061  2060  0308  00AD 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0061  2060  0308  00AD   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u00AD",
                      new String[] { "\u0061\u2060\u0308\u00AD" });
 
-    //  0061  2060  0300 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0061  2060  0300   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0300",
                      new String[] { "\u0061\u2060\u0300" });
 
-    //  0061  2060  0308  0300 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0061  2060  0308  0300   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0300",
                      new String[] { "\u0061\u2060\u0308\u0300" });
 
-    //  0061  2060  0061  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  2060  0061  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0061\u2060",
                      new String[] { "\u0061\u2060\u0061\u2060" });
 
-    //  0061  2060  0308  0061  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  2060  0308  0061  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0061\u2060",
                      new String[] { "\u0061\u2060\u0308\u0061\u2060" });
 
-    //  0061  2060  0061  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  2060  0061  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0061\u003A",
                      new String[] { "\u0061\u2060\u0061" });
 
-    //  0061  2060  0308  0061  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  2060  0308  0061  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0061\u003A",
                      new String[] { "\u0061\u2060\u0308\u0061" });
 
-    //  0061  2060  0061  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  2060  0061  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0061\u0027",
                      new String[] { "\u0061\u2060\u0061" });
 
-    //  0061  2060  0308  0061  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  2060  0308  0061  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0061\u0027",
                      new String[] { "\u0061\u2060\u0308\u0061" });
 
-    //  0061  2060  0061  0027  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  2060  0061  0027  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0061\u0027\u2060",
                      new String[] { "\u0061\u2060\u0061" });
 
-    //  0061  2060  0308  0061  0027  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  2060  0308  0061  0027  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061\u2060\u0308\u0061" });
 
-    //  0061  2060  0061  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  2060  0061  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0061\u002C",
                      new String[] { "\u0061\u2060\u0061" });
 
-    //  0061  2060  0308  0061  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  2060  0308  0061  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0061\u002C",
                      new String[] { "\u0061\u2060\u0308\u0061" });
 
-    //  0061  2060  0031  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  2060  0031  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0031\u003A",
                      new String[] { "\u0061\u2060\u0031" });
 
-    //  0061  2060  0308  0031  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  2060  0308  0031  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0031\u003A",
                      new String[] { "\u0061\u2060\u0308\u0031" });
 
-    //  0061  2060  0031  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  2060  0031  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0031\u0027",
                      new String[] { "\u0061\u2060\u0031" });
 
-    //  0061  2060  0308  0031  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  2060  0308  0031  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0031\u0027",
                      new String[] { "\u0061\u2060\u0308\u0031" });
 
-    //  0061  2060  0031  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  2060  0031  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0031\u002C",
                      new String[] { "\u0061\u2060\u0031" });
 
-    //  0061  2060  0308  0031  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  2060  0308  0031  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0031\u002C",
                      new String[] { "\u0061\u2060\u0308\u0031" });
 
-    //  0061  2060  0031  002E  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  2060  0031  002E  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0031\u002E\u2060",
                      new String[] { "\u0061\u2060\u0031" });
 
-    //  0061  2060  0308  0031  002E  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  2060  0308  0031  002E  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [9.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u2060\u0308\u0031\u002E\u2060",
                      new String[] { "\u0061\u2060\u0308\u0031" });
 
-    //  0061  003A  0001 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0061  003A  0001   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0001",
                      new String[] { "\u0061" });
 
-    //  0061  003A  0308  0001 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0061  003A  0308  0001   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0001",
                      new String[] { "\u0061" });
 
-    //  0061  003A  000D 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0061  003A  000D   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\r",
                      new String[] { "\u0061" });
 
-    //  0061  003A  0308  000D 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0061  003A  0308  000D   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\r",
                      new String[] { "\u0061" });
 
-    //  0061  003A  000A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0061  003A  000A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\n",
                      new String[] { "\u0061" });
 
-    //  0061  003A  0308  000A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0061  003A  0308  000A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\n",
                      new String[] { "\u0061" });
 
-    //  0061  003A  000B 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0061  003A  000B   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u000B",
                      new String[] { "\u0061" });
 
-    //  0061  003A  0308  000B 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0061  003A  0308  000B   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u000B",
                      new String[] { "\u0061" });
 
-    //  0061  003A  3031 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0061  003A  3031   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u3031",
                      new String[] { "\u0061", "\u3031" });
 
-    //  0061  003A  0308  3031 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0061  003A  0308  3031   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u3031",
                      new String[] { "\u0061", "\u3031" });
 
-    //  0061  003A  0041 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [7.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0061  003A  0041   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [7.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0041",
                      new String[] { "\u0061\u003A\u0041" });
 
-    //  0061  003A  0308  0041 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0061  003A  0308  0041   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0041",
                      new String[] { "\u0061\u003A\u0308\u0041" });
 
-    //  0061  003A  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  003A  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u003A",
                      new String[] { "\u0061" });
 
-    //  0061  003A  0308  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  003A  0308  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u003A",
                      new String[] { "\u0061" });
 
-    //  0061  003A  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  003A  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u002C",
                      new String[] { "\u0061" });
 
-    //  0061  003A  0308  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  003A  0308  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u002C",
                      new String[] { "\u0061" });
 
-    //  0061  003A  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  003A  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0027",
                      new String[] { "\u0061" });
 
-    //  0061  003A  0308  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  003A  0308  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0027",
                      new String[] { "\u0061" });
 
-    //  0061  003A  0030 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0061  003A  0030   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0030",
                      new String[] { "\u0061", "\u0030" });
 
-    //  0061  003A  0308  0030 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0061  003A  0308  0030   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0030",
                      new String[] { "\u0061", "\u0030" });
 
-    //  0061  003A  005F 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0061  003A  005F   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u005F",
                      new String[] { "\u0061" });
 
-    //  0061  003A  0308  005F 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0061  003A  0308  005F   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u005F",
                      new String[] { "\u0061" });
 
-    //  0061  003A  00AD 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0061  003A  00AD   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u00AD",
                      new String[] { "\u0061" });
 
-    //  0061  003A  0308  00AD 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0061  003A  0308  00AD   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u00AD",
                      new String[] { "\u0061" });
 
-    //  0061  003A  0300 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0061  003A  0300   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0300",
                      new String[] { "\u0061" });
 
-    //  0061  003A  0308  0300 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0061  003A  0308  0300   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0300",
                      new String[] { "\u0061" });
 
-    //  0061  003A  0061  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [7.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  003A  0061  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [7.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0061\u2060",
                      new String[] { "\u0061\u003A\u0061\u2060" });
 
-    //  0061  003A  0308  0061  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  003A  0308  0061  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0061\u2060",
                      new String[] { "\u0061\u003A\u0308\u0061\u2060" });
 
-    //  0061  003A  0061  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  003A  0061  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0061\u003A",
                      new String[] { "\u0061\u003A\u0061" });
 
-    //  0061  003A  0308  0061  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  003A  0308  0061  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0061\u003A",
                      new String[] { "\u0061\u003A\u0308\u0061" });
 
-    //  0061  003A  0061  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  003A  0061  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0061\u0027",
                      new String[] { "\u0061\u003A\u0061" });
 
-    //  0061  003A  0308  0061  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  003A  0308  0061  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0061\u0027",
                      new String[] { "\u0061\u003A\u0308\u0061" });
 
-    //  0061  003A  0061  0027  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  003A  0061  0027  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0061\u0027\u2060",
                      new String[] { "\u0061\u003A\u0061" });
 
-    //  0061  003A  0308  0061  0027  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  003A  0308  0061  0027  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061\u003A\u0308\u0061" });
 
-    //  0061  003A  0061  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  003A  0061  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0061\u002C",
                      new String[] { "\u0061\u003A\u0061" });
 
-    //  0061  003A  0308  0061  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  003A  0308  0061  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0061\u002C",
                      new String[] { "\u0061\u003A\u0308\u0061" });
 
-    //  0061  003A  0031  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  003A  0031  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0031\u003A",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  003A  0308  0031  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  003A  0308  0031  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0031\u003A",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  003A  0031  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  003A  0031  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0031\u0027",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  003A  0308  0031  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  003A  0308  0031  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0031\u0027",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  003A  0031  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  003A  0031  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0031\u002C",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  003A  0308  0031  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  003A  0308  0031  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0031\u002C",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  003A  0031  002E  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  003A  0031  002E  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0031\u002E\u2060",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  003A  0308  0031  002E  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  003A  0308  0031  002E  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u003A\u0308\u0031\u002E\u2060",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  0027  0001 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0061  0027  0001   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0001",
                      new String[] { "\u0061" });
 
-    //  0061  0027  0308  0001 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0061  0027  0308  0001   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0001",
                      new String[] { "\u0061" });
 
-    //  0061  0027  000D 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0061  0027  000D   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\r",
                      new String[] { "\u0061" });
 
-    //  0061  0027  0308  000D 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0061  0027  0308  000D   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\r",
                      new String[] { "\u0061" });
 
-    //  0061  0027  000A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0061  0027  000A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\n",
                      new String[] { "\u0061" });
 
-    //  0061  0027  0308  000A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0061  0027  0308  000A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\n",
                      new String[] { "\u0061" });
 
-    //  0061  0027  000B 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0061  0027  000B   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u000B",
                      new String[] { "\u0061" });
 
-    //  0061  0027  0308  000B 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0061  0027  0308  000B   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u000B",
                      new String[] { "\u0061" });
 
-    //  0061  0027  3031 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0061  0027  3031   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u3031",
                      new String[] { "\u0061", "\u3031" });
 
-    //  0061  0027  0308  3031 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0061  0027  0308  3031   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u3031",
                      new String[] { "\u0061", "\u3031" });
 
-    //  0061  0027  0041 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [7.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0061  0027  0041   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [7.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0041",
                      new String[] { "\u0061\u0027\u0041" });
 
-    //  0061  0027  0308  0041 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0061  0027  0308  0041   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0041",
                      new String[] { "\u0061\u0027\u0308\u0041" });
 
-    //  0061  0027  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  0027  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u003A",
                      new String[] { "\u0061" });
 
-    //  0061  0027  0308  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  0027  0308  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u003A",
                      new String[] { "\u0061" });
 
-    //  0061  0027  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  0027  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u002C",
                      new String[] { "\u0061" });
 
-    //  0061  0027  0308  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  0027  0308  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u002C",
                      new String[] { "\u0061" });
 
-    //  0061  0027  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  0027  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0027",
                      new String[] { "\u0061" });
 
-    //  0061  0027  0308  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  0027  0308  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0027",
                      new String[] { "\u0061" });
 
-    //  0061  0027  0030 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0061  0027  0030   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0030",
                      new String[] { "\u0061", "\u0030" });
 
-    //  0061  0027  0308  0030 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0061  0027  0308  0030   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0030",
                      new String[] { "\u0061", "\u0030" });
 
-    //  0061  0027  005F 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0061  0027  005F   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u005F",
                      new String[] { "\u0061" });
 
-    //  0061  0027  0308  005F 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0061  0027  0308  005F   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u005F",
                      new String[] { "\u0061" });
 
-    //  0061  0027  00AD 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0061  0027  00AD   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u00AD",
                      new String[] { "\u0061" });
 
-    //  0061  0027  0308  00AD 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0061  0027  0308  00AD   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u00AD",
                      new String[] { "\u0061" });
 
-    //  0061  0027  0300 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0061  0027  0300   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0300",
                      new String[] { "\u0061" });
 
-    //  0061  0027  0308  0300 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0061  0027  0308  0300   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0300",
                      new String[] { "\u0061" });
 
-    //  0061  0027  0061  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [7.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  0027  0061  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [7.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0061\u2060",
                      new String[] { "\u0061\u0027\u0061\u2060" });
 
-    //  0061  0027  0308  0061  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  0027  0308  0061  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0061\u2060",
                      new String[] { "\u0061\u0027\u0308\u0061\u2060" });
 
-    //  0061  0027  0061  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  0027  0061  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0061\u003A",
                      new String[] { "\u0061\u0027\u0061" });
 
-    //  0061  0027  0308  0061  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  0027  0308  0061  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0061\u003A",
                      new String[] { "\u0061\u0027\u0308\u0061" });
 
-    //  0061  0027  0061  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  0027  0061  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0061\u0027",
                      new String[] { "\u0061\u0027\u0061" });
 
-    //  0061  0027  0308  0061  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  0027  0308  0061  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0061\u0027",
                      new String[] { "\u0061\u0027\u0308\u0061" });
 
-    //  0061  0027  0061  0027  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  0027  0061  0027  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0061\u0027\u2060",
                      new String[] { "\u0061\u0027\u0061" });
 
-    //  0061  0027  0308  0061  0027  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  0027  0308  0061  0027  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061\u0027\u0308\u0061" });
 
-    //  0061  0027  0061  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  0027  0061  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0061\u002C",
                      new String[] { "\u0061\u0027\u0061" });
 
-    //  0061  0027  0308  0061  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  0027  0308  0061  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0061\u002C",
                      new String[] { "\u0061\u0027\u0308\u0061" });
 
-    //  0061  0027  0031  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  0027  0031  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0031\u003A",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  0027  0308  0031  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  0027  0308  0031  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0031\u003A",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  0027  0031  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  0027  0031  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0031\u0027",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  0027  0308  0031  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  0027  0308  0031  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0031\u0027",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  0027  0031  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  0027  0031  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0031\u002C",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  0027  0308  0031  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  0027  0308  0031  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0031\u002C",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  0027  0031  002E  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  0027  0031  002E  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0031\u002E\u2060",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  0027  0308  0031  002E  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  0027  0308  0031  002E  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u0308\u0031\u002E\u2060",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  0027  2060  0001 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0061  0027  2060  0001   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0001",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  0308  0001 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0061  0027  2060  0308  0001   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0001",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  000D 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0061  0027  2060  000D   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\r",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  0308  000D 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0061  0027  2060  0308  000D   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\r",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  000A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0061  0027  2060  000A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\n",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  0308  000A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0061  0027  2060  0308  000A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\n",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  000B 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0061  0027  2060  000B   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u000B",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  0308  000B 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0061  0027  2060  0308  000B   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u000B",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  3031 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0061  0027  2060  3031   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u3031",
                      new String[] { "\u0061", "\u3031" });
 
-    //  0061  0027  2060  0308  3031 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0061  0027  2060  0308  3031   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u3031",
                      new String[] { "\u0061", "\u3031" });
 
-    //  0061  0027  2060  0041 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [7.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0061  0027  2060  0041   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [7.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0041",
                      new String[] { "\u0061\u0027\u2060\u0041" });
 
-    //  0061  0027  2060  0308  0041 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0061  0027  2060  0308  0041   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0041",
                      new String[] { "\u0061\u0027\u2060\u0308\u0041" });
 
-    //  0061  0027  2060  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  0027  2060  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u003A",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  0308  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  0027  2060  0308  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u003A",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  0027  2060  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u002C",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  0308  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  0027  2060  0308  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u002C",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  0027  2060  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0027",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  0308  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  0027  2060  0308  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0027",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  0030 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0061  0027  2060  0030   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0030",
                      new String[] { "\u0061", "\u0030" });
 
-    //  0061  0027  2060  0308  0030 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0061  0027  2060  0308  0030   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0030",
                      new String[] { "\u0061", "\u0030" });
 
-    //  0061  0027  2060  005F 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0061  0027  2060  005F   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u005F",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  0308  005F 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0061  0027  2060  0308  005F   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u005F",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  00AD 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0061  0027  2060  00AD   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u00AD",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  0308  00AD 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0061  0027  2060  0308  00AD   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u00AD",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  0300 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0061  0027  2060  0300   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0300",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  0308  0300 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0061  0027  2060  0308  0300   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0300",
                      new String[] { "\u0061" });
 
-    //  0061  0027  2060  0061  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  0027  2060  0061  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0061\u2060",
                      new String[] { "\u0061\u0027\u2060\u0061\u2060" });
 
-    //  0061  0027  2060  0308  0061  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  0027  2060  0308  0061  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0061\u2060",
                      new String[] { "\u0061\u0027\u2060\u0308\u0061\u2060" });
 
-    //  0061  0027  2060  0061  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  0027  2060  0061  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0061\u003A",
                      new String[] { "\u0061\u0027\u2060\u0061" });
 
-    //  0061  0027  2060  0308  0061  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  0027  2060  0308  0061  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0061\u003A",
                      new String[] { "\u0061\u0027\u2060\u0308\u0061" });
 
-    //  0061  0027  2060  0061  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  0027  2060  0061  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0061\u0027",
                      new String[] { "\u0061\u0027\u2060\u0061" });
 
-    //  0061  0027  2060  0308  0061  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  0027  2060  0308  0061  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0061\u0027",
                      new String[] { "\u0061\u0027\u2060\u0308\u0061" });
 
-    //  0061  0027  2060  0061  0027  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  0027  2060  0061  0027  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0061\u0027\u2060",
                      new String[] { "\u0061\u0027\u2060\u0061" });
 
-    //  0061  0027  2060  0308  0061  0027  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  0027  2060  0308  0061  0027  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061\u0027\u2060\u0308\u0061" });
 
-    //  0061  0027  2060  0061  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  0027  2060  0061  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0061\u002C",
                      new String[] { "\u0061\u0027\u2060\u0061" });
 
-    //  0061  0027  2060  0308  0061  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  0027  2060  0308  0061  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [7.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0061\u002C",
                      new String[] { "\u0061\u0027\u2060\u0308\u0061" });
 
-    //  0061  0027  2060  0031  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  0027  2060  0031  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0031\u003A",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  0027  2060  0308  0031  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  0027  2060  0308  0031  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0031\u003A",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  0027  2060  0031  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  0027  2060  0031  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0031\u0027",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  0027  2060  0308  0031  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  0027  2060  0308  0031  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0031\u0027",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  0027  2060  0031  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  0027  2060  0031  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0031\u002C",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  0027  2060  0308  0031  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  0027  2060  0308  0031  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0031\u002C",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  0027  2060  0031  002E  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  0027  2060  0031  002E  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0031\u002E\u2060",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  0027  2060  0308  0031  002E  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  0027  2060  0308  0031  002E  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0027\u2060\u0308\u0031\u002E\u2060",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  002C  0001 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0061  002C  0001   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0001",
                      new String[] { "\u0061" });
 
-    //  0061  002C  0308  0001 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0061  002C  0308  0001   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0001",
                      new String[] { "\u0061" });
 
-    //  0061  002C  000D 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0061  002C  000D   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\r",
                      new String[] { "\u0061" });
 
-    //  0061  002C  0308  000D 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0061  002C  0308  000D   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\r",
                      new String[] { "\u0061" });
 
-    //  0061  002C  000A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0061  002C  000A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\n",
                      new String[] { "\u0061" });
 
-    //  0061  002C  0308  000A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0061  002C  0308  000A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\n",
                      new String[] { "\u0061" });
 
-    //  0061  002C  000B 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0061  002C  000B   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u000B",
                      new String[] { "\u0061" });
 
-    //  0061  002C  0308  000B 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0061  002C  0308  000B   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u000B",
                      new String[] { "\u0061" });
 
-    //  0061  002C  3031 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0061  002C  3031   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u3031",
                      new String[] { "\u0061", "\u3031" });
 
-    //  0061  002C  0308  3031 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0061  002C  0308  3031   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u3031",
                      new String[] { "\u0061", "\u3031" });
 
-    //  0061  002C  0041 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0061  002C  0041   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0041",
                      new String[] { "\u0061", "\u0041" });
 
-    //  0061  002C  0308  0041 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0061  002C  0308  0041   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0041",
                      new String[] { "\u0061", "\u0041" });
 
-    //  0061  002C  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  002C  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u003A",
                      new String[] { "\u0061" });
 
-    //  0061  002C  0308  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  002C  0308  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u003A",
                      new String[] { "\u0061" });
 
-    //  0061  002C  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  002C  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u002C",
                      new String[] { "\u0061" });
 
-    //  0061  002C  0308  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  002C  0308  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u002C",
                      new String[] { "\u0061" });
 
-    //  0061  002C  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  002C  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0027",
                      new String[] { "\u0061" });
 
-    //  0061  002C  0308  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  002C  0308  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0027",
                      new String[] { "\u0061" });
 
-    //  0061  002C  0030 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0061  002C  0030   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0030",
                      new String[] { "\u0061", "\u0030" });
 
-    //  0061  002C  0308  0030 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0061  002C  0308  0030   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0030",
                      new String[] { "\u0061", "\u0030" });
 
-    //  0061  002C  005F 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0061  002C  005F   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u005F",
                      new String[] { "\u0061" });
 
-    //  0061  002C  0308  005F 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0061  002C  0308  005F   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u005F",
                      new String[] { "\u0061" });
 
-    //  0061  002C  00AD 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0061  002C  00AD   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u00AD",
                      new String[] { "\u0061" });
 
-    //  0061  002C  0308  00AD 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0061  002C  0308  00AD   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u00AD",
                      new String[] { "\u0061" });
 
-    //  0061  002C  0300 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0061  002C  0300   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0300",
                      new String[] { "\u0061" });
 
-    //  0061  002C  0308  0300 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0061  002C  0308  0300   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0300",
                      new String[] { "\u0061" });
 
-    //  0061  002C  0061  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  002C  0061  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0061\u2060",
                      new String[] { "\u0061", "\u0061\u2060" });
 
-    //  0061  002C  0308  0061  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  002C  0308  0061  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0061\u2060",
                      new String[] { "\u0061", "\u0061\u2060" });
 
-    //  0061  002C  0061  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  002C  0061  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0061\u003A",
                      new String[] { "\u0061", "\u0061" });
 
-    //  0061  002C  0308  0061  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  002C  0308  0061  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0061\u003A",
                      new String[] { "\u0061", "\u0061" });
 
-    //  0061  002C  0061  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  002C  0061  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0061\u0027",
                      new String[] { "\u0061", "\u0061" });
 
-    //  0061  002C  0308  0061  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  002C  0308  0061  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0061\u0027",
                      new String[] { "\u0061", "\u0061" });
 
-    //  0061  002C  0061  0027  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  002C  0061  0027  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0061\u0027\u2060",
                      new String[] { "\u0061", "\u0061" });
 
-    //  0061  002C  0308  0061  0027  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  002C  0308  0061  0027  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0061\u0027\u2060",
                      new String[] { "\u0061", "\u0061" });
 
-    //  0061  002C  0061  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  002C  0061  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0061\u002C",
                      new String[] { "\u0061", "\u0061" });
 
-    //  0061  002C  0308  0061  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  002C  0308  0061  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0061\u002C",
                      new String[] { "\u0061", "\u0061" });
 
-    //  0061  002C  0031  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  002C  0031  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0031\u003A",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  002C  0308  0031  003A 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0061  002C  0308  0031  003A   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0031\u003A",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  002C  0031  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  002C  0031  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0031\u0027",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  002C  0308  0031  0027 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0061  002C  0308  0031  0027   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0031\u0027",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  002C  0031  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  002C  0031  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0031\u002C",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  002C  0308  0031  002C 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0061  002C  0308  0031  002C   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0031\u002C",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  002C  0031  002E  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  002C  0031  002E  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0031\u002E\u2060",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0061  002C  0308  0031  002E  2060 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0061  002C  0308  0031  002E  2060   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u002C\u0308\u0031\u002E\u2060",
                      new String[] { "\u0061", "\u0031" });
 
-    //  0031  003A  0001 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0031  003A  0001   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0001",
                      new String[] { "\u0031" });
 
-    //  0031  003A  0308  0001 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0031  003A  0308  0001   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0001",
                      new String[] { "\u0031" });
 
-    //  0031  003A  000D 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0031  003A  000D   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\r",
                      new String[] { "\u0031" });
 
-    //  0031  003A  0308  000D 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0031  003A  0308  000D   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\r",
                      new String[] { "\u0031" });
 
-    //  0031  003A  000A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0031  003A  000A   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\n",
                      new String[] { "\u0031" });
 
-    //  0031  003A  0308  000A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0031  003A  0308  000A   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\n",
                      new String[] { "\u0031" });
 
-    //  0031  003A  000B 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0031  003A  000B   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u000B",
                      new String[] { "\u0031" });
 
-    //  0031  003A  0308  000B 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0031  003A  0308  000B   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u000B",
                      new String[] { "\u0031" });
 
-    //  0031  003A  3031 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0031  003A  3031   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u3031",
                      new String[] { "\u0031", "\u3031" });
 
-    //  0031  003A  0308  3031 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0031  003A  0308  3031   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u3031",
                      new String[] { "\u0031", "\u3031" });
 
-    //  0031  003A  0041 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0031  003A  0041   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0041",
                      new String[] { "\u0031", "\u0041" });
 
-    //  0031  003A  0308  0041 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0031  003A  0308  0041   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0041",
                      new String[] { "\u0031", "\u0041" });
 
-    //  0031  003A  003A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  003A  003A   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u003A",
                      new String[] { "\u0031" });
 
-    //  0031  003A  0308  003A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  003A  0308  003A   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u003A",
                      new String[] { "\u0031" });
 
-    //  0031  003A  002C 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  003A  002C   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u002C",
                      new String[] { "\u0031" });
 
-    //  0031  003A  0308  002C 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  003A  0308  002C   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u002C",
                      new String[] { "\u0031" });
 
-    //  0031  003A  0027 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  003A  0027   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0027",
                      new String[] { "\u0031" });
 
-    //  0031  003A  0308  0027 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  003A  0308  0027   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0027",
                      new String[] { "\u0031" });
 
-    //  0031  003A  0030 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0031  003A  0030   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0030",
                      new String[] { "\u0031", "\u0030" });
 
-    //  0031  003A  0308  0030 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0031  003A  0308  0030   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0030",
                      new String[] { "\u0031", "\u0030" });
 
-    //  0031  003A  005F 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0031  003A  005F   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u005F",
                      new String[] { "\u0031" });
 
-    //  0031  003A  0308  005F 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0031  003A  0308  005F   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u005F",
                      new String[] { "\u0031" });
 
-    //  0031  003A  00AD 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0031  003A  00AD   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u00AD",
                      new String[] { "\u0031" });
 
-    //  0031  003A  0308  00AD 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0031  003A  0308  00AD   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u00AD",
                      new String[] { "\u0031" });
 
-    //  0031  003A  0300 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0031  003A  0300   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0300",
                      new String[] { "\u0031" });
 
-    //  0031  003A  0308  0300 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0031  003A  0308  0300   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0300",
                      new String[] { "\u0031" });
 
-    //  0031  003A  0061  2060 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  003A  0061  2060   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0061\u2060",
                      new String[] { "\u0031", "\u0061\u2060" });
 
-    //  0031  003A  0308  0061  2060 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  003A  0308  0061  2060   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0061\u2060",
                      new String[] { "\u0031", "\u0061\u2060" });
 
-    //  0031  003A  0061  003A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  003A  0061  003A   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0061\u003A",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  003A  0308  0061  003A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  003A  0308  0061  003A   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0061\u003A",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  003A  0061  0027 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  003A  0061  0027   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0061\u0027",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  003A  0308  0061  0027 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  003A  0308  0061  0027   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0061\u0027",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  003A  0061  0027  2060 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  003A  0061  0027  2060   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0061\u0027\u2060",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  003A  0308  0061  0027  2060 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  003A  0308  0061  0027  2060   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0061\u0027\u2060",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  003A  0061  002C 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  003A  0061  002C   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0061\u002C",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  003A  0308  0061  002C 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  003A  0308  0061  002C   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0061\u002C",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  003A  0031  003A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  003A  0031  003A   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0031\u003A",
                      new String[] { "\u0031", "\u0031" });
 
-    //  0031  003A  0308  0031  003A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  003A  0308  0031  003A   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0031\u003A",
                      new String[] { "\u0031", "\u0031" });
 
-    //  0031  003A  0031  0027 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  003A  0031  0027   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0031\u0027",
                      new String[] { "\u0031", "\u0031" });
 
-    //  0031  003A  0308  0031  0027 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  003A  0308  0031  0027   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0031\u0027",
                      new String[] { "\u0031", "\u0031" });
 
-    //  0031  003A  0031  002C 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  003A  0031  002C   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0031\u002C",
                      new String[] { "\u0031", "\u0031" });
 
-    //  0031  003A  0308  0031  002C 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  003A  0308  0031  002C   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0031\u002C",
                      new String[] { "\u0031", "\u0031" });
 
-    //  0031  003A  0031  002E  2060 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  003A  0031  002E  2060   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0031\u002E\u2060",
                      new String[] { "\u0031", "\u0031" });
 
-    //  0031  003A  0308  0031  002E  2060 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  003A  0308  0031  002E  2060   #   [0.2] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u003A\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031", "\u0031" });
 
-    //  0031  0027  0001 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0031  0027  0001   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0001",
                      new String[] { "\u0031" });
 
-    //  0031  0027  0308  0001 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0031  0027  0308  0001   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0001",
                      new String[] { "\u0031" });
 
-    //  0031  0027  000D 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0031  0027  000D   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\r",
                      new String[] { "\u0031" });
 
-    //  0031  0027  0308  000D 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0031  0027  0308  000D   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\r",
                      new String[] { "\u0031" });
 
-    //  0031  0027  000A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0031  0027  000A   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\n",
                      new String[] { "\u0031" });
 
-    //  0031  0027  0308  000A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0031  0027  0308  000A   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\n",
                      new String[] { "\u0031" });
 
-    //  0031  0027  000B 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0031  0027  000B   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u000B",
                      new String[] { "\u0031" });
 
-    //  0031  0027  0308  000B 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0031  0027  0308  000B   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u000B",
                      new String[] { "\u0031" });
 
-    //  0031  0027  3031 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0031  0027  3031   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u3031",
                      new String[] { "\u0031", "\u3031" });
 
-    //  0031  0027  0308  3031 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0031  0027  0308  3031   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u3031",
                      new String[] { "\u0031", "\u3031" });
 
-    //  0031  0027  0041 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0031  0027  0041   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0041",
                      new String[] { "\u0031", "\u0041" });
 
-    //  0031  0027  0308  0041 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0031  0027  0308  0041   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0041",
                      new String[] { "\u0031", "\u0041" });
 
-    //  0031  0027  003A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  0027  003A   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u003A",
                      new String[] { "\u0031" });
 
-    //  0031  0027  0308  003A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  0027  0308  003A   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u003A",
                      new String[] { "\u0031" });
 
-    //  0031  0027  002C 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  0027  002C   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u002C",
                      new String[] { "\u0031" });
 
-    //  0031  0027  0308  002C 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  0027  0308  002C   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u002C",
                      new String[] { "\u0031" });
 
-    //  0031  0027  0027 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  0027  0027   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0027",
                      new String[] { "\u0031" });
 
-    //  0031  0027  0308  0027 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  0027  0308  0027   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0027",
                      new String[] { "\u0031" });
 
-    //  0031  0027  0030 	#   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [11.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0031  0027  0030   #   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [11.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0030",
                      new String[] { "\u0031\u0027\u0030" });
 
-    //  0031  0027  0308  0030 	#   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0031  0027  0308  0030   #   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0030",
                      new String[] { "\u0031\u0027\u0308\u0030" });
 
-    //  0031  0027  005F 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0031  0027  005F   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u005F",
                      new String[] { "\u0031" });
 
-    //  0031  0027  0308  005F 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0031  0027  0308  005F   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u005F",
                      new String[] { "\u0031" });
 
-    //  0031  0027  00AD 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0031  0027  00AD   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u00AD",
                      new String[] { "\u0031" });
 
-    //  0031  0027  0308  00AD 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0031  0027  0308  00AD   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u00AD",
                      new String[] { "\u0031" });
 
-    //  0031  0027  0300 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0031  0027  0300   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0300",
                      new String[] { "\u0031" });
 
-    //  0031  0027  0308  0300 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0031  0027  0308  0300   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0300",
                      new String[] { "\u0031" });
 
-    //  0031  0027  0061  2060 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  0027  0061  2060   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0061\u2060",
                      new String[] { "\u0031", "\u0061\u2060" });
 
-    //  0031  0027  0308  0061  2060 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  0027  0308  0061  2060   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0061\u2060",
                      new String[] { "\u0031", "\u0061\u2060" });
 
-    //  0031  0027  0061  003A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  0027  0061  003A   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0061\u003A",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  0027  0308  0061  003A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  0027  0308  0061  003A   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0061\u003A",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  0027  0061  0027 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  0027  0061  0027   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0061\u0027",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  0027  0308  0061  0027 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  0027  0308  0061  0027   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0061\u0027",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  0027  0061  0027  2060 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  0027  0061  0027  2060   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0061\u0027\u2060",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  0027  0308  0061  0027  2060 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  0027  0308  0061  0027  2060   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0061\u0027\u2060",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  0027  0061  002C 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  0027  0061  002C   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0061\u002C",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  0027  0308  0061  002C 	#   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  0027  0308  0061  002C   #   [0.2] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0061\u002C",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  0027  0031  003A 	#   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [11.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  0027  0031  003A   #   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [11.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0031\u003A",
                      new String[] { "\u0031\u0027\u0031" });
 
-    //  0031  0027  0308  0031  003A 	#   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  0027  0308  0031  003A   #   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0031\u003A",
                      new String[] { "\u0031\u0027\u0308\u0031" });
 
-    //  0031  0027  0031  0027 	#   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [11.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  0027  0031  0027   #   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [11.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0031\u0027",
                      new String[] { "\u0031\u0027\u0031" });
 
-    //  0031  0027  0308  0031  0027 	#   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  0027  0308  0031  0027   #   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0031\u0027",
                      new String[] { "\u0031\u0027\u0308\u0031" });
 
-    //  0031  0027  0031  002C 	#   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [11.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  0027  0031  002C   #   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [11.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0031\u002C",
                      new String[] { "\u0031\u0027\u0031" });
 
-    //  0031  0027  0308  0031  002C 	#   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  0027  0308  0031  002C   #   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0031\u002C",
                      new String[] { "\u0031\u0027\u0308\u0031" });
 
-    //  0031  0027  0031  002E  2060 	#   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [11.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  0027  0031  002E  2060   #   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [11.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0031\u002E\u2060",
                      new String[] { "\u0031\u0027\u0031" });
 
-    //  0031  0027  0308  0031  002E  2060 	#   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  0027  0308  0031  002E  2060   #   [0.2] DIGIT ONE (Numeric)  [12.0] APOSTROPHE (MidNumLet)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u0027\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031\u0027\u0308\u0031" });
 
-    //  0031  002C  0001 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0031  002C  0001   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0001",
                      new String[] { "\u0031" });
 
-    //  0031  002C  0308  0001 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0031  002C  0308  0001   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0001",
                      new String[] { "\u0031" });
 
-    //  0031  002C  000D 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0031  002C  000D   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\r",
                      new String[] { "\u0031" });
 
-    //  0031  002C  0308  000D 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0031  002C  0308  000D   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\r",
                      new String[] { "\u0031" });
 
-    //  0031  002C  000A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0031  002C  000A   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\n",
                      new String[] { "\u0031" });
 
-    //  0031  002C  0308  000A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0031  002C  0308  000A   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\n",
                      new String[] { "\u0031" });
 
-    //  0031  002C  000B 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0031  002C  000B   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u000B",
                      new String[] { "\u0031" });
 
-    //  0031  002C  0308  000B 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0031  002C  0308  000B   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u000B",
                      new String[] { "\u0031" });
 
-    //  0031  002C  3031 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0031  002C  3031   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u3031",
                      new String[] { "\u0031", "\u3031" });
 
-    //  0031  002C  0308  3031 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0031  002C  0308  3031   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u3031",
                      new String[] { "\u0031", "\u3031" });
 
-    //  0031  002C  0041 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0031  002C  0041   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0041",
                      new String[] { "\u0031", "\u0041" });
 
-    //  0031  002C  0308  0041 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0031  002C  0308  0041   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0041",
                      new String[] { "\u0031", "\u0041" });
 
-    //  0031  002C  003A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  002C  003A   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u003A",
                      new String[] { "\u0031" });
 
-    //  0031  002C  0308  003A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  002C  0308  003A   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u003A",
                      new String[] { "\u0031" });
 
-    //  0031  002C  002C 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  002C  002C   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u002C",
                      new String[] { "\u0031" });
 
-    //  0031  002C  0308  002C 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  002C  0308  002C   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u002C",
                      new String[] { "\u0031" });
 
-    //  0031  002C  0027 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  002C  0027   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0027",
                      new String[] { "\u0031" });
 
-    //  0031  002C  0308  0027 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  002C  0308  0027   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0027",
                      new String[] { "\u0031" });
 
-    //  0031  002C  0030 	#   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [11.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0031  002C  0030   #   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [11.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0030",
                      new String[] { "\u0031\u002C\u0030" });
 
-    //  0031  002C  0308  0030 	#   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0031  002C  0308  0030   #   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0030",
                      new String[] { "\u0031\u002C\u0308\u0030" });
 
-    //  0031  002C  005F 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0031  002C  005F   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u005F",
                      new String[] { "\u0031" });
 
-    //  0031  002C  0308  005F 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0031  002C  0308  005F   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u005F",
                      new String[] { "\u0031" });
 
-    //  0031  002C  00AD 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0031  002C  00AD   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u00AD",
                      new String[] { "\u0031" });
 
-    //  0031  002C  0308  00AD 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0031  002C  0308  00AD   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u00AD",
                      new String[] { "\u0031" });
 
-    //  0031  002C  0300 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0031  002C  0300   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0300",
                      new String[] { "\u0031" });
 
-    //  0031  002C  0308  0300 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0031  002C  0308  0300   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0300",
                      new String[] { "\u0031" });
 
-    //  0031  002C  0061  2060 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  002C  0061  2060   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0061\u2060",
                      new String[] { "\u0031", "\u0061\u2060" });
 
-    //  0031  002C  0308  0061  2060 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  002C  0308  0061  2060   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0061\u2060",
                      new String[] { "\u0031", "\u0061\u2060" });
 
-    //  0031  002C  0061  003A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  002C  0061  003A   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0061\u003A",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  002C  0308  0061  003A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  002C  0308  0061  003A   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0061\u003A",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  002C  0061  0027 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  002C  0061  0027   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0061\u0027",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  002C  0308  0061  0027 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  002C  0308  0061  0027   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0061\u0027",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  002C  0061  0027  2060 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  002C  0061  0027  2060   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0061\u0027\u2060",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  002C  0308  0061  0027  2060 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  002C  0308  0061  0027  2060   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0061\u0027\u2060",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  002C  0061  002C 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  002C  0061  002C   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0061\u002C",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  002C  0308  0061  002C 	#   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  002C  0308  0061  002C   #   [0.2] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0061\u002C",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  002C  0031  003A 	#   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [11.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  002C  0031  003A   #   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [11.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0031\u003A",
                      new String[] { "\u0031\u002C\u0031" });
 
-    //  0031  002C  0308  0031  003A 	#   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  002C  0308  0031  003A   #   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0031\u003A",
                      new String[] { "\u0031\u002C\u0308\u0031" });
 
-    //  0031  002C  0031  0027 	#   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [11.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  002C  0031  0027   #   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [11.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0031\u0027",
                      new String[] { "\u0031\u002C\u0031" });
 
-    //  0031  002C  0308  0031  0027 	#   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  002C  0308  0031  0027   #   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0031\u0027",
                      new String[] { "\u0031\u002C\u0308\u0031" });
 
-    //  0031  002C  0031  002C 	#   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [11.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  002C  0031  002C   #   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [11.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0031\u002C",
                      new String[] { "\u0031\u002C\u0031" });
 
-    //  0031  002C  0308  0031  002C 	#   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  002C  0308  0031  002C   #   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0031\u002C",
                      new String[] { "\u0031\u002C\u0308\u0031" });
 
-    //  0031  002C  0031  002E  2060 	#   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [11.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  002C  0031  002E  2060   #   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [11.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0031\u002E\u2060",
                      new String[] { "\u0031\u002C\u0031" });
 
-    //  0031  002C  0308  0031  002E  2060 	#   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  002C  0308  0031  002E  2060   #   [0.2] DIGIT ONE (Numeric)  [12.0] COMMA (MidNum)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002C\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031\u002C\u0308\u0031" });
 
-    //  0031  002E  2060  0001 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0031  002E  2060  0001   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0001",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  0308  0001 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
+    //  0031  002E  2060  0308  0001   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] <START OF HEADING> (Other)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0001",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  000D 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0031  002E  2060  000D   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\r",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  0308  000D 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
+    //  0031  002E  2060  0308  000D   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <CARRIAGE RETURN (CR)> (CR)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\r",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  000A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0031  002E  2060  000A   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\n",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  0308  000A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
+    //  0031  002E  2060  0308  000A   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE FEED (LF)> (LF)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\n",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  000B 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0031  002E  2060  000B   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u000B",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  0308  000B 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
+    //  0031  002E  2060  0308  000B   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [3.2] <LINE TABULATION> (Newline)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u000B",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  3031 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0031  002E  2060  3031   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u3031",
                      new String[] { "\u0031", "\u3031" });
 
-    //  0031  002E  2060  0308  3031 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
+    //  0031  002E  2060  0308  3031   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] VERTICAL KANA REPEAT MARK (Katakana)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u3031",
                      new String[] { "\u0031", "\u3031" });
 
-    //  0031  002E  2060  0041 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0031  002E  2060  0041   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0041",
                      new String[] { "\u0031", "\u0041" });
 
-    //  0031  002E  2060  0308  0041 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
+    //  0031  002E  2060  0308  0041   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN CAPITAL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0041",
                      new String[] { "\u0031", "\u0041" });
 
-    //  0031  002E  2060  003A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  002E  2060  003A   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u003A",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  0308  003A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  002E  2060  0308  003A   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u003A",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  002C 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  002E  2060  002C   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u002C",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  0308  002C 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  002E  2060  0308  002C   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u002C",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  0027 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  002E  2060  0027   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0027",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  0308  0027 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  002E  2060  0308  0027   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0027",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  0030 	#   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [11.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0031  002E  2060  0030   #   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [11.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0030",
                      new String[] { "\u0031\u002E\u2060\u0030" });
 
-    //  0031  002E  2060  0308  0030 	#   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ZERO (Numeric)  [0.3]
+    //  0031  002E  2060  0308  0030   #   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ZERO (Numeric)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0030",
                      new String[] { "\u0031\u002E\u2060\u0308\u0030" });
 
-    //  0031  002E  2060  005F 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0031  002E  2060  005F   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u005F",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  0308  005F 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
+    //  0031  002E  2060  0308  005F   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LOW LINE (ExtendNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u005F",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  00AD 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0031  002E  2060  00AD   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u00AD",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  0308  00AD 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
+    //  0031  002E  2060  0308  00AD   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] SOFT HYPHEN (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u00AD",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  0300 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0031  002E  2060  0300   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0300",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  0308  0300 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
+    //  0031  002E  2060  0308  0300   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [4.0] COMBINING GRAVE ACCENT (Extend_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0300",
                      new String[] { "\u0031" });
 
-    //  0031  002E  2060  0061  2060 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  002E  2060  0061  2060   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0061\u2060",
                      new String[] { "\u0031", "\u0061\u2060" });
 
-    //  0031  002E  2060  0308  0061  2060 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  002E  2060  0308  0061  2060   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0061\u2060",
                      new String[] { "\u0031", "\u0061\u2060" });
 
-    //  0031  002E  2060  0061  003A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  002E  2060  0061  003A   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0061\u003A",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  002E  2060  0308  0061  003A 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  002E  2060  0308  0061  003A   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0061\u003A",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  002E  2060  0061  0027 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  002E  2060  0061  0027   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0061\u0027",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  002E  2060  0308  0061  0027 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  002E  2060  0308  0061  0027   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0061\u0027",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  002E  2060  0061  0027  2060 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  002E  2060  0061  0027  2060   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0061\u0027\u2060",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  002E  2060  0308  0061  0027  2060 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  002E  2060  0308  0061  0027  2060   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0061\u0027\u2060",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  002E  2060  0061  002C 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  002E  2060  0061  002C   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0061\u002C",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  002E  2060  0308  0061  002C 	#   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  002E  2060  0308  0061  002C   #   [0.2] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0061\u002C",
                      new String[] { "\u0031", "\u0061" });
 
-    //  0031  002E  2060  0031  003A 	#   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  002E  2060  0031  003A   #   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0031\u003A",
                      new String[] { "\u0031\u002E\u2060\u0031" });
 
-    //  0031  002E  2060  0308  0031  003A 	#   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
+    //  0031  002E  2060  0308  0031  003A   #   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] COLON (MidLetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0031\u003A",
                      new String[] { "\u0031\u002E\u2060\u0308\u0031" });
 
-    //  0031  002E  2060  0031  0027 	#   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  002E  2060  0031  0027   #   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0031\u0027",
                      new String[] { "\u0031\u002E\u2060\u0031" });
 
-    //  0031  002E  2060  0308  0031  0027 	#   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
+    //  0031  002E  2060  0308  0031  0027   #   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] APOSTROPHE (MidNumLet)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0031\u0027",
                      new String[] { "\u0031\u002E\u2060\u0308\u0031" });
 
-    //  0031  002E  2060  0031  002C 	#   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  002E  2060  0031  002C   #   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0031\u002C",
                      new String[] { "\u0031\u002E\u2060\u0031" });
 
-    //  0031  002E  2060  0308  0031  002C 	#   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
+    //  0031  002E  2060  0308  0031  002C   #   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] COMMA (MidNum)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0031\u002C",
                      new String[] { "\u0031\u002E\u2060\u0308\u0031" });
 
-    //  0031  002E  2060  0031  002E  2060 	#   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  002E  2060  0031  002E  2060   #   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0031\u002E\u2060",
                      new String[] { "\u0031\u002E\u2060\u0031" });
 
-    //  0031  002E  2060  0308  0031  002E  2060 	#   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  0031  002E  2060  0308  0031  002E  2060   #   [0.2] DIGIT ONE (Numeric)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [4.0] COMBINING DIAERESIS (Extend_FE)  [11.0] DIGIT ONE (Numeric)  [999.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u0031\u002E\u2060\u0308\u0031\u002E\u2060",
                      new String[] { "\u0031\u002E\u2060\u0308\u0031" });
 
-    //  0063  0061  006E  0027  0074 	#   [0.2] LATIN SMALL LETTER C (ALetter)  [5.0] LATIN SMALL LETTER A (ALetter)  [5.0] LATIN SMALL LETTER N (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [7.0] LATIN SMALL LETTER T (ALetter)  [0.3]
+    //  0063  0061  006E  0027  0074   #   [0.2] LATIN SMALL LETTER C (ALetter)  [5.0] LATIN SMALL LETTER A (ALetter)  [5.0] LATIN SMALL LETTER N (ALetter)  [6.0] APOSTROPHE (MidNumLet)  [7.0] LATIN SMALL LETTER T (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0063\u0061\u006E\u0027\u0074",
                      new String[] { "\u0063\u0061\u006E\u0027\u0074" });
 
-    //  0063  0061  006E  2019  0074 	#   [0.2] LATIN SMALL LETTER C (ALetter)  [5.0] LATIN SMALL LETTER A (ALetter)  [5.0] LATIN SMALL LETTER N (ALetter)  [6.0] RIGHT SINGLE QUOTATION MARK (MidNumLet)  [7.0] LATIN SMALL LETTER T (ALetter)  [0.3]
+    //  0063  0061  006E  2019  0074   #   [0.2] LATIN SMALL LETTER C (ALetter)  [5.0] LATIN SMALL LETTER A (ALetter)  [5.0] LATIN SMALL LETTER N (ALetter)  [6.0] RIGHT SINGLE QUOTATION MARK (MidNumLet)  [7.0] LATIN SMALL LETTER T (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0063\u0061\u006E\u2019\u0074",
                      new String[] { "\u0063\u0061\u006E\u2019\u0074" });
 
-    //  0061  0062  00AD  0062  0079 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [5.0] LATIN SMALL LETTER B (ALetter)  [4.0] SOFT HYPHEN (Format_FE)  [5.0] LATIN SMALL LETTER B (ALetter)  [5.0] LATIN SMALL LETTER Y (ALetter)  [0.3]
+    //  0061  0062  00AD  0062  0079   #   [0.2] LATIN SMALL LETTER A (ALetter)  [5.0] LATIN SMALL LETTER B (ALetter)  [4.0] SOFT HYPHEN (Format_FE)  [5.0] LATIN SMALL LETTER B (ALetter)  [5.0] LATIN SMALL LETTER Y (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0062\u00AD\u0062\u0079",
                      new String[] { "\u0061\u0062\u00AD\u0062\u0079" });
 
-    //  0061  0024  002D  0033  0034  002C  0035  0036  0037  002E  0031  0034  0025  0062 	#   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] DOLLAR SIGN (Other)  [999.0] HYPHEN-MINUS (Other)  [999.0] DIGIT THREE (Numeric)  [8.0] DIGIT FOUR (Numeric)  [12.0] COMMA (MidNum)  [11.0] DIGIT FIVE (Numeric)  [8.0] DIGIT SIX (Numeric)  [8.0] DIGIT SEVEN (Numeric)  [12.0] FULL STOP (MidNumLet)  [11.0] DIGIT ONE (Numeric)  [8.0] DIGIT FOUR (Numeric)  [999.0] PERCENT SIGN (Other)  [999.0] LATIN SMALL LETTER B (ALetter)  [0.3]
+    //  0061  0024  002D  0033  0034  002C  0035  0036  0037  002E  0031  0034  0025  0062   #   [0.2] LATIN SMALL LETTER A (ALetter)  [999.0] DOLLAR SIGN (Other)  [999.0] HYPHEN-MINUS (Other)  [999.0] DIGIT THREE (Numeric)  [8.0] DIGIT FOUR (Numeric)  [12.0] COMMA (MidNum)  [11.0] DIGIT FIVE (Numeric)  [8.0] DIGIT SIX (Numeric)  [8.0] DIGIT SEVEN (Numeric)  [12.0] FULL STOP (MidNumLet)  [11.0] DIGIT ONE (Numeric)  [8.0] DIGIT FOUR (Numeric)  [999.0] PERCENT SIGN (Other)  [999.0] LATIN SMALL LETTER B (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0061\u0024\u002D\u0033\u0034\u002C\u0035\u0036\u0037\u002E\u0031\u0034\u0025\u0062",
                      new String[] { "\u0061", "\u0033\u0034\u002C\u0035\u0036\u0037\u002E\u0031\u0034", "\u0062" });
 
-    //  0033  0061 	#   [0.2] DIGIT THREE (Numeric)  [10.0] LATIN SMALL LETTER A (ALetter)  [0.3]
+    //  0033  0061   #   [0.2] DIGIT THREE (Numeric)  [10.0] LATIN SMALL LETTER A (ALetter)  [0.3]
     assertAnalyzesTo(analyzer, "\u0033\u0061",
                      new String[] { "\u0033\u0061" });
 
-    //  2060  0063  2060  0061  2060  006E  2060  0027  2060  0074  2060  2060 	#   [0.2] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER C (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER N (ALetter)  [4.0] WORD JOINER (Format_FE)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [7.0] LATIN SMALL LETTER T (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  2060  0063  2060  0061  2060  006E  2060  0027  2060  0074  2060  2060   #   [0.2] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER C (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER N (ALetter)  [4.0] WORD JOINER (Format_FE)  [6.0] APOSTROPHE (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [7.0] LATIN SMALL LETTER T (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u2060\u0063\u2060\u0061\u2060\u006E\u2060\u0027\u2060\u0074\u2060\u2060",
                      new String[] { "\u0063\u2060\u0061\u2060\u006E\u2060\u0027\u2060\u0074\u2060\u2060" });
 
-    //  2060  0063  2060  0061  2060  006E  2060  2019  2060  0074  2060  2060 	#   [0.2] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER C (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER N (ALetter)  [4.0] WORD JOINER (Format_FE)  [6.0] RIGHT SINGLE QUOTATION MARK (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [7.0] LATIN SMALL LETTER T (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  2060  0063  2060  0061  2060  006E  2060  2019  2060  0074  2060  2060   #   [0.2] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER C (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER N (ALetter)  [4.0] WORD JOINER (Format_FE)  [6.0] RIGHT SINGLE QUOTATION MARK (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [7.0] LATIN SMALL LETTER T (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u2060\u0063\u2060\u0061\u2060\u006E\u2060\u2019\u2060\u0074\u2060\u2060",
                      new String[] { "\u0063\u2060\u0061\u2060\u006E\u2060\u2019\u2060\u0074\u2060\u2060" });
 
-    //  2060  0061  2060  0062  2060  00AD  2060  0062  2060  0079  2060  2060 	#   [0.2] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER B (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] SOFT HYPHEN (Format_FE)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER B (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER Y (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  2060  0061  2060  0062  2060  00AD  2060  0062  2060  0079  2060  2060   #   [0.2] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER B (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] SOFT HYPHEN (Format_FE)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER B (ALetter)  [4.0] WORD JOINER (Format_FE)  [5.0] LATIN SMALL LETTER Y (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u2060\u0061\u2060\u0062\u2060\u00AD\u2060\u0062\u2060\u0079\u2060\u2060",
                      new String[] { "\u0061\u2060\u0062\u2060\u00AD\u2060\u0062\u2060\u0079\u2060\u2060" });
 
-    //  2060  0061  2060  0024  2060  002D  2060  0033  2060  0034  2060  002C  2060  0035  2060  0036  2060  0037  2060  002E  2060  0031  2060  0034  2060  0025  2060  0062  2060  2060 	#   [0.2] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [999.0] DOLLAR SIGN (Other)  [4.0] WORD JOINER (Format_FE)  [999.0] HYPHEN-MINUS (Other)  [4.0] WORD JOINER (Format_FE)  [999.0] DIGIT THREE (Numeric)  [4.0] WORD JOINER (Format_FE)  [8.0] DIGIT FOUR (Numeric)  [4.0] WORD JOINER (Format_FE)  [12.0] COMMA (MidNum)  [4.0] WORD JOINER (Format_FE)  [11.0] DIGIT FIVE (Numeric)  [4.0] WORD JOINER (Format_FE)  [8.0] DIGIT SIX (Numeric)  [4.0] WORD JOINER (Format_FE)  [8.0] DIGIT SEVEN (Numeric)  [4.0] WORD JOINER (Format_FE)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [11.0] DIGIT ONE (Numeric)  [4.0] WORD JOINER (Format_FE)  [8.0] DIGIT FOUR (Numeric)  [4.0] WORD JOINER (Format_FE)  [999.0] PERCENT SIGN (Other)  [4.0] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER B (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  2060  0061  2060  0024  2060  002D  2060  0033  2060  0034  2060  002C  2060  0035  2060  0036  2060  0037  2060  002E  2060  0031  2060  0034  2060  0025  2060  0062  2060  2060   #   [0.2] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [999.0] DOLLAR SIGN (Other)  [4.0] WORD JOINER (Format_FE)  [999.0] HYPHEN-MINUS (Other)  [4.0] WORD JOINER (Format_FE)  [999.0] DIGIT THREE (Numeric)  [4.0] WORD JOINER (Format_FE)  [8.0] DIGIT FOUR (Numeric)  [4.0] WORD JOINER (Format_FE)  [12.0] COMMA (MidNum)  [4.0] WORD JOINER (Format_FE)  [11.0] DIGIT FIVE (Numeric)  [4.0] WORD JOINER (Format_FE)  [8.0] DIGIT SIX (Numeric)  [4.0] WORD JOINER (Format_FE)  [8.0] DIGIT SEVEN (Numeric)  [4.0] WORD JOINER (Format_FE)  [12.0] FULL STOP (MidNumLet)  [4.0] WORD JOINER (Format_FE)  [11.0] DIGIT ONE (Numeric)  [4.0] WORD JOINER (Format_FE)  [8.0] DIGIT FOUR (Numeric)  [4.0] WORD JOINER (Format_FE)  [999.0] PERCENT SIGN (Other)  [4.0] WORD JOINER (Format_FE)  [999.0] LATIN SMALL LETTER B (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u2060\u0061\u2060\u0024\u2060\u002D\u2060\u0033\u2060\u0034\u2060\u002C\u2060\u0035\u2060\u0036\u2060\u0037\u2060\u002E\u2060\u0031\u2060\u0034\u2060\u0025\u2060\u0062\u2060\u2060",
                      new String[] { "\u0061\u2060", "\u0033\u2060\u0034\u2060\u002C\u2060\u0035\u2060\u0036\u2060\u0037\u2060\u002E\u2060\u0031\u2060\u0034\u2060", "\u0062\u2060\u2060" });
 
-    //  2060  0033  2060  0061  2060  2060 	#   [0.2] WORD JOINER (Format_FE)  [999.0] DIGIT THREE (Numeric)  [4.0] WORD JOINER (Format_FE)  [10.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] WORD JOINER (Format_FE)  [0.3]
+    //  2060  0033  2060  0061  2060  2060   #   [0.2] WORD JOINER (Format_FE)  [999.0] DIGIT THREE (Numeric)  [4.0] WORD JOINER (Format_FE)  [10.0] LATIN SMALL LETTER A (ALetter)  [4.0] WORD JOINER (Format_FE)  [4.0] WORD JOINER (Format_FE)  [0.3]
     assertAnalyzesTo(analyzer, "\u2060\u0033\u2060\u0061\u2060\u2060",
                      new String[] { "\u0033\u2060\u0061\u2060\u2060" });
 
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java	(revision 1386643)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java	(working copy)
@@ -32,37 +32,37 @@
  * HyphenatedWordsFilter test
  */
 public class TestHyphenatedWordsFilter extends BaseTokenStreamTestCase {
-	public void testHyphenatedWords() throws Exception {
-		String input = "ecologi-\r\ncal devel-\r\n\r\nop compre-\u0009hensive-hands-on and ecologi-\ncal";
-		// first test
-		TokenStream ts = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);
-		ts = new HyphenatedWordsFilter(ts);
-		assertTokenStreamContents(ts, 
-		    new String[] { "ecological", "develop", "comprehensive-hands-on", "and", "ecological" });
-	}
-	
-	/**
-	 * Test that HyphenatedWordsFilter behaves correctly with a final hyphen
-	 */
-	public void testHyphenAtEnd() throws Exception {
-	    String input = "ecologi-\r\ncal devel-\r\n\r\nop compre-\u0009hensive-hands-on and ecology-";
-	    // first test
-	    TokenStream ts = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);
-	    ts = new HyphenatedWordsFilter(ts);
-	    assertTokenStreamContents(ts, 
-	        new String[] { "ecological", "develop", "comprehensive-hands-on", "and", "ecology-" });
-	}
-	
-	public void testOffsets() throws Exception {
-	  String input = "abc- def geh 1234- 5678-";
+  public void testHyphenatedWords() throws Exception {
+    String input = "ecologi-\r\ncal devel-\r\n\r\nop compre-\u0009hensive-hands-on and ecologi-\ncal";
+    // first test
     TokenStream ts = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);
     ts = new HyphenatedWordsFilter(ts);
+    assertTokenStreamContents(ts,
+        new String[] { "ecological", "develop", "comprehensive-hands-on", "and", "ecological" });
+  }
+
+  /**
+   * Test that HyphenatedWordsFilter behaves correctly with a final hyphen
+   */
+  public void testHyphenAtEnd() throws Exception {
+      String input = "ecologi-\r\ncal devel-\r\n\r\nop compre-\u0009hensive-hands-on and ecology-";
+      // first test
+      TokenStream ts = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);
+      ts = new HyphenatedWordsFilter(ts);
+      assertTokenStreamContents(ts,
+          new String[] { "ecological", "develop", "comprehensive-hands-on", "and", "ecology-" });
+  }
+
+  public void testOffsets() throws Exception {
+    String input = "abc- def geh 1234- 5678-";
+    TokenStream ts = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);
+    ts = new HyphenatedWordsFilter(ts);
     assertTokenStreamContents(ts, 
         new String[] { "abcdef", "geh", "12345678-" },
         new int[] { 0, 9, 13 },
         new int[] { 8, 12, 24 });
-	}
-	
+  }
+
   /** blast some random strings through the analyzer */
   public void testRandomString() throws Exception {
     Analyzer a = new Analyzer() {
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java	(revision 1386643)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java	(working copy)
@@ -38,87 +38,87 @@
 public class TestBrazilianStemmer extends BaseTokenStreamTestCase {
   
   public void testWithSnowballExamples() throws Exception {
-	 check("boa", "boa");
-	 check("boainain", "boainain");
-	 check("boas", "boas");
-	 check("bas", "boas"); // removes diacritic: different from snowball portugese
-	 check("boassu", "boassu");
-	 check("boataria", "boat");
-	 check("boate", "boat");
-	 check("boates", "boat");
-	 check("boatos", "boat");
-	 check("bob", "bob");
-	 check("boba", "bob");
-	 check("bobagem", "bobag");
-	 check("bobagens", "bobagens");
-	 check("bobalhes", "bobalho"); // removes diacritic: different from snowball portugese
-	 check("bobear", "bob");
-	 check("bobeira", "bobeir");
-	 check("bobinho", "bobinh");
-	 check("bobinhos", "bobinh");
-	 check("bobo", "bob");
-	 check("bobs", "bobs");
-	 check("boca", "boc");
-	 check("bocadas", "boc");
-	 check("bocadinho", "bocadinh");
-	 check("bocado", "boc");
-	 check("bocaiva", "bocaiuv"); // removes diacritic: different from snowball portuguese
-	 check("boal", "bocal"); // removes diacritic: different from snowball portuguese
-	 check("bocarra", "bocarr");
-	 check("bocas", "boc");
-	 check("bode", "bod");
-	 check("bodoque", "bodoqu");
-	 check("body", "body");
-	 check("boeing", "boeing");
-	 check("boem", "boem");
-	 check("boemia", "boem");
-	 check("bomio", "boemi"); // removes diacritic: different from snowball portuguese
-	 check("bogot", "bogot");
-	 check("boi", "boi");
-	 check("bia", "boi"); // removes diacritic: different from snowball portuguese
-	 check("boiando", "boi");
-	 check("quiabo", "quiab");
-	 check("quicaram", "quic");
-	 check("quickly", "quickly");
-	 check("quieto", "quiet");
-	 check("quietos", "quiet");
-	 check("quilate", "quilat");
-	 check("quilates", "quilat");
-	 check("quilinhos", "quilinh");
-	 check("quilo", "quil");
-	 check("quilombo", "quilomb");
-	 check("quilomtricas", "quilometr"); // removes diacritic: different from snowball portuguese
-	 check("quilomtricos", "quilometr"); // removes diacritic: different from snowball portuguese
-	 check("quilmetro", "quilometr"); // removes diacritic: different from snowball portoguese
-	 check("quilmetros", "quilometr"); // removes diacritic: different from snowball portoguese
-	 check("quilos", "quil");
-	 check("quimica", "quimic");
-	 check("quilos", "quil");
-	 check("quimica", "quimic");
-	 check("quimicas", "quimic");
-	 check("quimico", "quimic");
-	 check("quimicos", "quimic");
-	 check("quimioterapia", "quimioterap");
-	 check("quimioterpicos", "quimioterap"); // removes diacritic: different from snowball portoguese
-	 check("quimono", "quimon");
-	 check("quincas", "quinc");
-	 check("quinho", "quinha"); // removes diacritic: different from snowball portoguese
-	 check("quinhentos", "quinhent");
-	 check("quinn", "quinn");
-	 check("quino", "quin");
-	 check("quinta", "quint");
-	 check("quintal", "quintal");
-	 check("quintana", "quintan");
-	 check("quintanilha", "quintanilh");
-	 check("quinto", "quinta"); // removes diacritic: different from snowball portoguese
-	 check("quintessncia", "quintessente"); // versus snowball portuguese 'quintessent'
-	 check("quintino", "quintin");
-	 check("quinto", "quint");
-	 check("quintos", "quint");
-	 check("quintuplicou", "quintuplic");
-	 check("quinze", "quinz");
-	 check("quinzena", "quinzen");
-	 check("quiosque", "quiosqu");
+   check("boa", "boa");
+   check("boainain", "boainain");
+   check("boas", "boas");
+   check("bas", "boas"); // removes diacritic: different from snowball portugese
+   check("boassu", "boassu");
+   check("boataria", "boat");
+   check("boate", "boat");
+   check("boates", "boat");
+   check("boatos", "boat");
+   check("bob", "bob");
+   check("boba", "bob");
+   check("bobagem", "bobag");
+   check("bobagens", "bobagens");
+   check("bobalhes", "bobalho"); // removes diacritic: different from snowball portugese
+   check("bobear", "bob");
+   check("bobeira", "bobeir");
+   check("bobinho", "bobinh");
+   check("bobinhos", "bobinh");
+   check("bobo", "bob");
+   check("bobs", "bobs");
+   check("boca", "boc");
+   check("bocadas", "boc");
+   check("bocadinho", "bocadinh");
+   check("bocado", "boc");
+   check("bocaiva", "bocaiuv"); // removes diacritic: different from snowball portuguese
+   check("boal", "bocal"); // removes diacritic: different from snowball portuguese
+   check("bocarra", "bocarr");
+   check("bocas", "boc");
+   check("bode", "bod");
+   check("bodoque", "bodoqu");
+   check("body", "body");
+   check("boeing", "boeing");
+   check("boem", "boem");
+   check("boemia", "boem");
+   check("bomio", "boemi"); // removes diacritic: different from snowball portuguese
+   check("bogot", "bogot");
+   check("boi", "boi");
+   check("bia", "boi"); // removes diacritic: different from snowball portuguese
+   check("boiando", "boi");
+   check("quiabo", "quiab");
+   check("quicaram", "quic");
+   check("quickly", "quickly");
+   check("quieto", "quiet");
+   check("quietos", "quiet");
+   check("quilate", "quilat");
+   check("quilates", "quilat");
+   check("quilinhos", "quilinh");
+   check("quilo", "quil");
+   check("quilombo", "quilomb");
+   check("quilomtricas", "quilometr"); // removes diacritic: different from snowball portuguese
+   check("quilomtricos", "quilometr"); // removes diacritic: different from snowball portuguese
+   check("quilmetro", "quilometr"); // removes diacritic: different from snowball portoguese
+   check("quilmetros", "quilometr"); // removes diacritic: different from snowball portoguese
+   check("quilos", "quil");
+   check("quimica", "quimic");
+   check("quilos", "quil");
+   check("quimica", "quimic");
+   check("quimicas", "quimic");
+   check("quimico", "quimic");
+   check("quimicos", "quimic");
+   check("quimioterapia", "quimioterap");
+   check("quimioterpicos", "quimioterap"); // removes diacritic: different from snowball portoguese
+   check("quimono", "quimon");
+   check("quincas", "quinc");
+   check("quinho", "quinha"); // removes diacritic: different from snowball portoguese
+   check("quinhentos", "quinhent");
+   check("quinn", "quinn");
+   check("quino", "quin");
+   check("quinta", "quint");
+   check("quintal", "quintal");
+   check("quintana", "quintan");
+   check("quintanilha", "quintanilh");
+   check("quinto", "quinta"); // removes diacritic: different from snowball portoguese
+   check("quintessncia", "quintessente"); // versus snowball portuguese 'quintessent'
+   check("quintino", "quintin");
+   check("quinto", "quint");
+   check("quintos", "quint");
+   check("quintuplicou", "quintuplic");
+   check("quinze", "quinz");
+   check("quinzena", "quinzen");
+   check("quiosque", "quiosqu");
   }
   
   public void testNormalization() throws Exception {
@@ -175,4 +175,4 @@
     };
     checkOneTermReuse(a, "", "");
   }
-}
\ No newline at end of file
+}
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java	(revision 1386643)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java	(working copy)
@@ -31,181 +31,181 @@
 
 public class TestFrenchAnalyzer extends BaseTokenStreamTestCase {
 
-	public void testAnalyzer() throws Exception {
-		FrenchAnalyzer fa = new FrenchAnalyzer(TEST_VERSION_CURRENT);
-	
-		assertAnalyzesTo(fa, "", new String[] {
-		});
+  public void testAnalyzer() throws Exception {
+    FrenchAnalyzer fa = new FrenchAnalyzer(TEST_VERSION_CURRENT);
 
-		assertAnalyzesTo(
-			fa,
-			"chien chat cheval",
-			new String[] { "chien", "chat", "cheval" });
+    assertAnalyzesTo(fa, "", new String[] {
+    });
 
-		assertAnalyzesTo(
-			fa,
-			"chien CHAT CHEVAL",
-			new String[] { "chien", "chat", "cheval" });
+    assertAnalyzesTo(
+      fa,
+      "chien chat cheval",
+      new String[] { "chien", "chat", "cheval" });
 
-		assertAnalyzesTo(
-			fa,
-			"  chien  ,? + = -  CHAT /: > CHEVAL",
-			new String[] { "chien", "chat", "cheval" });
+    assertAnalyzesTo(
+      fa,
+      "chien CHAT CHEVAL",
+      new String[] { "chien", "chat", "cheval" });
 
-		assertAnalyzesTo(fa, "chien++", new String[] { "chien" });
+    assertAnalyzesTo(
+      fa,
+      "  chien  ,? + = -  CHAT /: > CHEVAL",
+      new String[] { "chien", "chat", "cheval" });
 
-		assertAnalyzesTo(
-			fa,
-			"mot \"entreguillemet\"",
-			new String[] { "mot", "entreguilemet" });
+    assertAnalyzesTo(fa, "chien++", new String[] { "chien" });
 
-		// let's do some french specific tests now	
+    assertAnalyzesTo(
+      fa,
+      "mot \"entreguillemet\"",
+      new String[] { "mot", "entreguilemet" });
 
-		/* 1. couldn't resist
-		 I would expect this to stay one term as in French the minus 
-		sign is often used for composing words */
-		assertAnalyzesTo(
-			fa,
-			"Jean-Franois",
-			new String[] { "jean", "francoi" });
+    // let's do some french specific tests now
 
-		// 2. stopwords
-		assertAnalyzesTo(
-			fa,
-			"le la chien les aux chat du des  cheval",
-			new String[] { "chien", "chat", "cheval" });
+    /* 1. couldn't resist
+     I would expect this to stay one term as in French the minus
+    sign is often used for composing words */
+    assertAnalyzesTo(
+      fa,
+      "Jean-Franois",
+      new String[] { "jean", "francoi" });
 
-		// some nouns and adjectives
-		assertAnalyzesTo(
-			fa,
-			"lances chismes habitable chiste lments captifs",
-			new String[] {
-				"lanc",
-				"chism",
-				"habitabl",
-				"chist",
-				"element",
-				"captif" });
+    // 2. stopwords
+    assertAnalyzesTo(
+      fa,
+      "le la chien les aux chat du des  cheval",
+      new String[] { "chien", "chat", "cheval" });
 
-		// some verbs
-		assertAnalyzesTo(
-			fa,
-			"finissions souffrirent rugissante",
-			new String[] { "finision", "soufrirent", "rugisant" });
+    // some nouns and adjectives
+    assertAnalyzesTo(
+      fa,
+      "lances chismes habitable chiste lments captifs",
+      new String[] {
+        "lanc",
+        "chism",
+        "habitabl",
+        "chist",
+        "element",
+        "captif" });
 
-		// some everything else
-		// aujourd'hui stays one term which is OK
-		assertAnalyzesTo(
-			fa,
-			"C3PO aujourd'hui oeuf  anticonstitutionnellement Java++ ",
-			new String[] {
-				"c3po",
-				"aujourd'hui",
-				"oeuf",
-				"aua",
-				"anticonstitutionel",
-				"java" });
+    // some verbs
+    assertAnalyzesTo(
+      fa,
+      "finissions souffrirent rugissante",
+      new String[] { "finision", "soufrirent", "rugisant" });
 
-		// some more everything else
-		// here 1940-1945 stays as one term, 1940:1945 not ?
-		assertAnalyzesTo(
-			fa,
-			"33Bis 1940-1945 1940:1945 (---i+++)*",
-			new String[] { "33bi", "1940", "1945", "1940", "1945", "i" });
+    // some everything else
+    // aujourd'hui stays one term which is OK
+    assertAnalyzesTo(
+      fa,
+      "C3PO aujourd'hui oeuf  anticonstitutionnellement Java++ ",
+      new String[] {
+        "c3po",
+        "aujourd'hui",
+        "oeuf",
+        "aua",
+        "anticonstitutionel",
+        "java" });
 
-	}
-	
-	/**
-	 * @deprecated (3.1) remove this test for Lucene 5.0
-	 */
-	@Deprecated
-	public void testAnalyzer30() throws Exception {
-	    FrenchAnalyzer fa = new FrenchAnalyzer(Version.LUCENE_30);
-	  
-	    assertAnalyzesTo(fa, "", new String[] {
-	    });
+    // some more everything else
+    // here 1940-1945 stays as one term, 1940:1945 not ?
+    assertAnalyzesTo(
+      fa,
+      "33Bis 1940-1945 1940:1945 (---i+++)*",
+      new String[] { "33bi", "1940", "1945", "1940", "1945", "i" });
 
-	    assertAnalyzesTo(
-	      fa,
-	      "chien chat cheval",
-	      new String[] { "chien", "chat", "cheval" });
+  }
 
-	    assertAnalyzesTo(
-	      fa,
-	      "chien CHAT CHEVAL",
-	      new String[] { "chien", "chat", "cheval" });
+  /**
+   * @deprecated (3.1) remove this test for Lucene 5.0
+   */
+  @Deprecated
+  public void testAnalyzer30() throws Exception {
+      FrenchAnalyzer fa = new FrenchAnalyzer(Version.LUCENE_30);
 
-	    assertAnalyzesTo(
-	      fa,
-	      "  chien  ,? + = -  CHAT /: > CHEVAL",
-	      new String[] { "chien", "chat", "cheval" });
+      assertAnalyzesTo(fa, "", new String[] {
+      });
 
-	    assertAnalyzesTo(fa, "chien++", new String[] { "chien" });
+      assertAnalyzesTo(
+        fa,
+        "chien chat cheval",
+        new String[] { "chien", "chat", "cheval" });
 
-	    assertAnalyzesTo(
-	      fa,
-	      "mot \"entreguillemet\"",
-	      new String[] { "mot", "entreguillemet" });
+      assertAnalyzesTo(
+        fa,
+        "chien CHAT CHEVAL",
+        new String[] { "chien", "chat", "cheval" });
 
-	    // let's do some french specific tests now  
+      assertAnalyzesTo(
+        fa,
+        "  chien  ,? + = -  CHAT /: > CHEVAL",
+        new String[] { "chien", "chat", "cheval" });
 
-	    /* 1. couldn't resist
-	     I would expect this to stay one term as in French the minus 
-	    sign is often used for composing words */
-	    assertAnalyzesTo(
-	      fa,
-	      "Jean-Franois",
-	      new String[] { "jean", "franois" });
+      assertAnalyzesTo(fa, "chien++", new String[] { "chien" });
 
-	    // 2. stopwords
-	    assertAnalyzesTo(
-	      fa,
-	      "le la chien les aux chat du des  cheval",
-	      new String[] { "chien", "chat", "cheval" });
+      assertAnalyzesTo(
+        fa,
+        "mot \"entreguillemet\"",
+        new String[] { "mot", "entreguillemet" });
 
-	    // some nouns and adjectives
-	    assertAnalyzesTo(
-	      fa,
-	      "lances chismes habitable chiste lments captifs",
-	      new String[] {
-	        "lanc",
-	        "chism",
-	        "habit",
-	        "chist",
-	        "lment",
-	        "captif" });
+      // let's do some french specific tests now
 
-	    // some verbs
-	    assertAnalyzesTo(
-	      fa,
-	      "finissions souffrirent rugissante",
-	      new String[] { "fin", "souffr", "rug" });
+      /* 1. couldn't resist
+       I would expect this to stay one term as in French the minus
+      sign is often used for composing words */
+      assertAnalyzesTo(
+        fa,
+        "Jean-Franois",
+        new String[] { "jean", "franois" });
 
-	    // some everything else
-	    // aujourd'hui stays one term which is OK
-	    assertAnalyzesTo(
-	      fa,
-	      "C3PO aujourd'hui oeuf  anticonstitutionnellement Java++ ",
-	      new String[] {
-	        "c3po",
-	        "aujourd'hui",
-	        "oeuf",
-	        "",
-	        "anticonstitutionnel",
-	        "jav" });
+      // 2. stopwords
+      assertAnalyzesTo(
+        fa,
+        "le la chien les aux chat du des  cheval",
+        new String[] { "chien", "chat", "cheval" });
 
-	    // some more everything else
-	    // here 1940-1945 stays as one term, 1940:1945 not ?
-	    assertAnalyzesTo(
-	      fa,
-	      "33Bis 1940-1945 1940:1945 (---i+++)*",
-	      new String[] { "33bis", "1940-1945", "1940", "1945", "i" });
+      // some nouns and adjectives
+      assertAnalyzesTo(
+        fa,
+        "lances chismes habitable chiste lments captifs",
+        new String[] {
+          "lanc",
+          "chism",
+          "habit",
+          "chist",
+          "lment",
+          "captif" });
 
-	  }
-	
-	public void testReusableTokenStream() throws Exception {
-	  FrenchAnalyzer fa = new FrenchAnalyzer(TEST_VERSION_CURRENT);
-	  // stopwords
+      // some verbs
+      assertAnalyzesTo(
+        fa,
+        "finissions souffrirent rugissante",
+        new String[] { "fin", "souffr", "rug" });
+
+      // some everything else
+      // aujourd'hui stays one term which is OK
+      assertAnalyzesTo(
+        fa,
+        "C3PO aujourd'hui oeuf  anticonstitutionnellement Java++ ",
+        new String[] {
+          "c3po",
+          "aujourd'hui",
+          "oeuf",
+          "",
+          "anticonstitutionnel",
+          "jav" });
+
+      // some more everything else
+      // here 1940-1945 stays as one term, 1940:1945 not ?
+      assertAnalyzesTo(
+        fa,
+        "33Bis 1940-1945 1940:1945 (---i+++)*",
+        new String[] { "33bis", "1940-1945", "1940", "1945", "i" });
+
+    }
+
+  public void testReusableTokenStream() throws Exception {
+    FrenchAnalyzer fa = new FrenchAnalyzer(TEST_VERSION_CURRENT);
+    // stopwords
       assertAnalyzesToReuse(
           fa,
           "le la chien les aux chat du des  cheval",
@@ -222,7 +222,7 @@
               "chist",
               "element",
               "captif" });
-	}
+  }
 
   public void testExclusionTableViaCtor() throws Exception {
     CharArraySet set = new CharArraySet(TEST_VERSION_CURRENT, 1, true);
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java	(revision 1386643)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java	(working copy)
@@ -34,83 +34,83 @@
 public class TestDutchStemmer extends BaseTokenStreamTestCase {
   
   public void testWithSnowballExamples() throws Exception {
-	 check("lichaamsziek", "lichaamsziek");
-	 check("lichamelijk", "licham");
-	 check("lichamelijke", "licham");
-	 check("lichamelijkheden", "licham");
-	 check("lichamen", "licham");
-	 check("lichere", "licher");
-	 check("licht", "licht");
-	 check("lichtbeeld", "lichtbeeld");
-	 check("lichtbruin", "lichtbruin");
-	 check("lichtdoorlatende", "lichtdoorlat");
-	 check("lichte", "licht");
-	 check("lichten", "licht");
-	 check("lichtende", "lichtend");
-	 check("lichtenvoorde", "lichtenvoord");
-	 check("lichter", "lichter");
-	 check("lichtere", "lichter");
-	 check("lichters", "lichter");
-	 check("lichtgevoeligheid", "lichtgevoel");
-	 check("lichtgewicht", "lichtgewicht");
-	 check("lichtgrijs", "lichtgrijs");
-	 check("lichthoeveelheid", "lichthoevel");
-	 check("lichtintensiteit", "lichtintensiteit");
-	 check("lichtje", "lichtj");
-	 check("lichtjes", "lichtjes");
-	 check("lichtkranten", "lichtkrant");
-	 check("lichtkring", "lichtkring");
-	 check("lichtkringen", "lichtkring");
-	 check("lichtregelsystemen", "lichtregelsystem");
-	 check("lichtste", "lichtst");
-	 check("lichtstromende", "lichtstrom");
-	 check("lichtte", "licht");
-	 check("lichtten", "licht");
-	 check("lichttoetreding", "lichttoetred");
-	 check("lichtverontreinigde", "lichtverontreinigd");
-	 check("lichtzinnige", "lichtzinn");
-	 check("lid", "lid");
-	 check("lidia", "lidia");
-	 check("lidmaatschap", "lidmaatschap");
-	 check("lidstaten", "lidstat");
-	 check("lidvereniging", "lidveren");
-	 check("opgingen", "opging");
-	 check("opglanzing", "opglanz");
-	 check("opglanzingen", "opglanz");
-	 check("opglimlachten", "opglimlacht");
-	 check("opglimpen", "opglimp");
-	 check("opglimpende", "opglimp");
-	 check("opglimping", "opglimp");
-	 check("opglimpingen", "opglimp");
-	 check("opgraven", "opgrav");
-	 check("opgrijnzen", "opgrijnz");
-	 check("opgrijzende", "opgrijz");
-	 check("opgroeien", "opgroei");
-	 check("opgroeiende", "opgroei");
-	 check("opgroeiplaats", "opgroeiplat");
-	 check("ophaal", "ophal");
-	 check("ophaaldienst", "ophaaldienst");
-	 check("ophaalkosten", "ophaalkost");
-	 check("ophaalsystemen", "ophaalsystem");
-	 check("ophaalt", "ophaalt");
-	 check("ophaaltruck", "ophaaltruck");
-	 check("ophalen", "ophal");
-	 check("ophalend", "ophal");
-	 check("ophalers", "ophaler");
-	 check("ophef", "ophef");
-	 check("opheldering", "ophelder");
-	 check("ophemelde", "ophemeld");
-	 check("ophemelen", "ophemel");
-	 check("opheusden", "opheusd");
-	 check("ophief", "ophief");
-	 check("ophield", "ophield");
-	 check("ophieven", "ophiev");
-	 check("ophoepelt", "ophoepelt");
-	 check("ophoog", "ophog");
-	 check("ophoogzand", "ophoogzand");
-	 check("ophopen", "ophop");
-	 check("ophoping", "ophop");
-	 check("ophouden", "ophoud");
+   check("lichaamsziek", "lichaamsziek");
+   check("lichamelijk", "licham");
+   check("lichamelijke", "licham");
+   check("lichamelijkheden", "licham");
+   check("lichamen", "licham");
+   check("lichere", "licher");
+   check("licht", "licht");
+   check("lichtbeeld", "lichtbeeld");
+   check("lichtbruin", "lichtbruin");
+   check("lichtdoorlatende", "lichtdoorlat");
+   check("lichte", "licht");
+   check("lichten", "licht");
+   check("lichtende", "lichtend");
+   check("lichtenvoorde", "lichtenvoord");
+   check("lichter", "lichter");
+   check("lichtere", "lichter");
+   check("lichters", "lichter");
+   check("lichtgevoeligheid", "lichtgevoel");
+   check("lichtgewicht", "lichtgewicht");
+   check("lichtgrijs", "lichtgrijs");
+   check("lichthoeveelheid", "lichthoevel");
+   check("lichtintensiteit", "lichtintensiteit");
+   check("lichtje", "lichtj");
+   check("lichtjes", "lichtjes");
+   check("lichtkranten", "lichtkrant");
+   check("lichtkring", "lichtkring");
+   check("lichtkringen", "lichtkring");
+   check("lichtregelsystemen", "lichtregelsystem");
+   check("lichtste", "lichtst");
+   check("lichtstromende", "lichtstrom");
+   check("lichtte", "licht");
+   check("lichtten", "licht");
+   check("lichttoetreding", "lichttoetred");
+   check("lichtverontreinigde", "lichtverontreinigd");
+   check("lichtzinnige", "lichtzinn");
+   check("lid", "lid");
+   check("lidia", "lidia");
+   check("lidmaatschap", "lidmaatschap");
+   check("lidstaten", "lidstat");
+   check("lidvereniging", "lidveren");
+   check("opgingen", "opging");
+   check("opglanzing", "opglanz");
+   check("opglanzingen", "opglanz");
+   check("opglimlachten", "opglimlacht");
+   check("opglimpen", "opglimp");
+   check("opglimpende", "opglimp");
+   check("opglimping", "opglimp");
+   check("opglimpingen", "opglimp");
+   check("opgraven", "opgrav");
+   check("opgrijnzen", "opgrijnz");
+   check("opgrijzende", "opgrijz");
+   check("opgroeien", "opgroei");
+   check("opgroeiende", "opgroei");
+   check("opgroeiplaats", "opgroeiplat");
+   check("ophaal", "ophal");
+   check("ophaaldienst", "ophaaldienst");
+   check("ophaalkosten", "ophaalkost");
+   check("ophaalsystemen", "ophaalsystem");
+   check("ophaalt", "ophaalt");
+   check("ophaaltruck", "ophaaltruck");
+   check("ophalen", "ophal");
+   check("ophalend", "ophal");
+   check("ophalers", "ophaler");
+   check("ophef", "ophef");
+   check("opheldering", "ophelder");
+   check("ophemelde", "ophemeld");
+   check("ophemelen", "ophemel");
+   check("opheusden", "opheusd");
+   check("ophief", "ophief");
+   check("ophield", "ophield");
+   check("ophieven", "ophiev");
+   check("ophoepelt", "ophoepelt");
+   check("ophoog", "ophog");
+   check("ophoogzand", "ophoogzand");
+   check("ophopen", "ophop");
+   check("ophoping", "ophop");
+   check("ophouden", "ophoud");
   }
   
   /**
@@ -204,4 +204,4 @@
     checkRandomData(random(), new DutchAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
   }
   
-}
\ No newline at end of file
+}
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java	(revision 1386643)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java	(working copy)
@@ -38,38 +38,38 @@
  */
 
 public class TestThaiAnalyzer extends BaseTokenStreamTestCase {
-	
+
   @Override
   public void setUp() throws Exception {
     super.setUp();
     assumeTrue("JRE does not support Thai dictionary-based BreakIterator", ThaiWordFilter.DBBI_AVAILABLE);
   }
-	/* 
-	 * testcase for offsets
-	 */
-	public void testOffsets() throws Exception {
-		assertAnalyzesTo(new ThaiAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET), "", 
-		    new String[] { "", "", "", "", "", "", "", "" },
-				new int[] { 0, 3, 6, 9, 13, 17, 20, 23 },
-				new int[] { 3, 6, 9, 13, 17, 20, 23, 25 });
-	}
-	
-	public void testStopWords() throws Exception {
-	  assertAnalyzesTo(new ThaiAnalyzer(TEST_VERSION_CURRENT), "", 
-	      new String[] { "", "", "" },
-	      new int[] { 13, 20, 23 },
-	      new int[] { 17, 23, 25 },
-	      new int[] { 5, 2, 1 });
-	}
-	
-	public void testBackwardsStopWords() throws Exception {
-	   assertAnalyzesTo(new ThaiAnalyzer(Version.LUCENE_35), "", 
-	        new String[] { "", "", "", "", "", "", "", "" },
-	        new int[] { 0, 3, 6, 9, 13, 17, 20, 23 },
-	        new int[] { 3, 6, 9, 13, 17, 20, 23, 25 });
-	}
-	
-	public void testTokenType() throws Exception {
+  /*
+   * testcase for offsets
+   */
+  public void testOffsets() throws Exception {
+    assertAnalyzesTo(new ThaiAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET), "",
+        new String[] { "", "", "", "", "", "", "", "" },
+        new int[] { 0, 3, 6, 9, 13, 17, 20, 23 },
+        new int[] { 3, 6, 9, 13, 17, 20, 23, 25 });
+  }
+
+  public void testStopWords() throws Exception {
+    assertAnalyzesTo(new ThaiAnalyzer(TEST_VERSION_CURRENT), "",
+        new String[] { "", "", "" },
+        new int[] { 13, 20, 23 },
+        new int[] { 17, 23, 25 },
+        new int[] { 5, 2, 1 });
+  }
+
+  public void testBackwardsStopWords() throws Exception {
+     assertAnalyzesTo(new ThaiAnalyzer(Version.LUCENE_35), "",
+          new String[] { "", "", "", "", "", "", "", "" },
+          new int[] { 0, 3, 6, 9, 13, 17, 20, 23 },
+          new int[] { 3, 6, 9, 13, 17, 20, 23, 25 });
+  }
+
+  public void testTokenType() throws Exception {
       assertAnalyzesTo(new ThaiAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET), " ", 
                        new String[] { "", "", "", "", "", "", "", "", "" },
                        new String[] { "<SOUTHEAST_ASIAN>", "<SOUTHEAST_ASIAN>", 
@@ -77,69 +77,69 @@
                                       "<SOUTHEAST_ASIAN>", "<SOUTHEAST_ASIAN>",
                                       "<SOUTHEAST_ASIAN>", "<SOUTHEAST_ASIAN>",
                                       "<NUM>" });
-	}
+  }
 
-	/**
-	 * Thai numeric tokens were typed as <ALPHANUM> instead of <NUM>.
-	 * @deprecated (3.1) testing backwards behavior
- 	 */
-	@Deprecated
-	public void testBuggyTokenType30() throws Exception {
-		assertAnalyzesTo(new ThaiAnalyzer(Version.LUCENE_30), " ", 
+  /**
+   * Thai numeric tokens were typed as <ALPHANUM> instead of <NUM>.
+   * @deprecated (3.1) testing backwards behavior
+    */
+  @Deprecated
+  public void testBuggyTokenType30() throws Exception {
+    assertAnalyzesTo(new ThaiAnalyzer(Version.LUCENE_30), " ",
                          new String[] { "", "", "", "", "", "", "", "", "" },
                          new String[] { "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>", 
                                         "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>", 
                                         "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>" });
-	}
-	
-	/** @deprecated (3.1) testing backwards behavior */
-	@Deprecated
+  }
+
+  /** @deprecated (3.1) testing backwards behavior */
+  @Deprecated
     public void testAnalyzer30() throws Exception {
         ThaiAnalyzer analyzer = new ThaiAnalyzer(Version.LUCENE_30);
-	
-		assertAnalyzesTo(analyzer, "", new String[] {});
 
-		assertAnalyzesTo(
-			analyzer,
-			"",
-			new String[] { "", "", "", "", "", "", "", ""});
+    assertAnalyzesTo(analyzer, "", new String[] {});
 
-		assertAnalyzesTo(
-			analyzer,
-			" XY&Z -  xyz@demo.com",
-			new String[] { "", "", "xy&z", "", "", "xyz@demo.com" });
+    assertAnalyzesTo(
+      analyzer,
+      "",
+      new String[] { "", "", "", "", "", "", "", ""});
 
+    assertAnalyzesTo(
+      analyzer,
+      " XY&Z -  xyz@demo.com",
+      new String[] { "", "", "xy&z", "", "", "xyz@demo.com" });
+
     // English stop words
-		assertAnalyzesTo(
-			analyzer,
-			" The quick brown fox jumped over the lazy dogs",
-			new String[] { "", "", "quick", "brown", "fox", "jumped", "over", "lazy", "dogs" });
-	}
-	
-	/*
-	 * Test that position increments are adjusted correctly for stopwords.
-	 */
-	// note this test uses stopfilter's stopset
-	public void testPositionIncrements() throws Exception {
-	  final ThaiAnalyzer analyzer = new ThaiAnalyzer(TEST_VERSION_CURRENT, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
+    assertAnalyzesTo(
+      analyzer,
+      " The quick brown fox jumped over the lazy dogs",
+      new String[] { "", "", "quick", "brown", "fox", "jumped", "over", "lazy", "dogs" });
+  }
+
+  /*
+   * Test that position increments are adjusted correctly for stopwords.
+   */
+  // note this test uses stopfilter's stopset
+  public void testPositionIncrements() throws Exception {
+    final ThaiAnalyzer analyzer = new ThaiAnalyzer(TEST_VERSION_CURRENT, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
     assertAnalyzesTo(analyzer, " the ", 
         new String[] { "", "", "", "", "", "", "", "" },
         new int[] { 0, 3, 6, 9, 18, 22, 25, 28 },
         new int[] { 3, 6, 9, 13, 22, 25, 28, 30 },
         new int[] { 1, 1, 1, 1, 2, 1, 1, 1 });
-	 
-	  // case that a stopword is adjacent to thai text, with no whitespace
+
+    // case that a stopword is adjacent to thai text, with no whitespace
     assertAnalyzesTo(analyzer, "the ", 
         new String[] { "", "", "", "", "", "", "", "" },
         new int[] { 0, 3, 6, 9, 17, 21, 24, 27 },
         new int[] { 3, 6, 9, 13, 21, 24, 27, 29 },
         new int[] { 1, 1, 1, 1, 2, 1, 1, 1 });
-	}
-	
-	public void testReusableTokenStream() throws Exception {
-	  ThaiAnalyzer analyzer = new ThaiAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET);
-	  assertAnalyzesToReuse(analyzer, "", new String[] {});
+  }
 
+  public void testReusableTokenStream() throws Exception {
+    ThaiAnalyzer analyzer = new ThaiAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET);
+    assertAnalyzesToReuse(analyzer, "", new String[] {});
+
       assertAnalyzesToReuse(
           analyzer,
           "",
@@ -149,25 +149,25 @@
           analyzer,
           " XY&Z -  xyz@demo.com",
           new String[] { "", "", "xy", "z", "", "", "xyz", "demo.com" });
-	}
-	
-	/** @deprecated (3.1) for version back compat */
-	@Deprecated
-	public void testReusableTokenStream30() throws Exception {
-	    ThaiAnalyzer analyzer = new ThaiAnalyzer(Version.LUCENE_30);
-	    assertAnalyzesToReuse(analyzer, "", new String[] {});
+  }
 
-	    assertAnalyzesToReuse(
+  /** @deprecated (3.1) for version back compat */
+  @Deprecated
+  public void testReusableTokenStream30() throws Exception {
+      ThaiAnalyzer analyzer = new ThaiAnalyzer(Version.LUCENE_30);
+      assertAnalyzesToReuse(analyzer, "", new String[] {});
+
+      assertAnalyzesToReuse(
             analyzer,
             "",
             new String[] { "", "", "", "", "", "", "", ""});
 
-	    assertAnalyzesToReuse(
+      assertAnalyzesToReuse(
             analyzer,
             " XY&Z -  xyz@demo.com",
             new String[] { "", "", "xy&z", "", "", "xyz@demo.com" });
   }
-	
+
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
     checkRandomData(random(), new ThaiAnalyzer(TEST_VERSION_CURRENT), 1000*RANDOM_MULTIPLIER);
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer.java	(revision 1386643)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer.java	(working copy)
@@ -37,7 +37,7 @@
 
 public class TestPatternTokenizer extends BaseTokenStreamTestCase 
 {
-	public void testSplitting() throws Exception 
+  public void testSplitting() throws Exception 
   {
     String qpattern = "\\'([^\\']+)\\'"; // get stuff between "'"
     String[][] tests = {
@@ -71,8 +71,8 @@
         }
       }*/
     } 
-	}
-	
+  }
+
   public void testOffsetCorrection() throws Exception {
     final String INPUT = "G&uuml;nther G&uuml;nther is here";
 
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java	(revision 1386643)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java	(working copy)
@@ -48,29 +48,29 @@
         new String[] { "", "", "", "" });
   }
   
-	/**
-	 * Test the analysis of various greek strings.
-	 *
-	 * @throws Exception in case an error occurs
-	 * @deprecated (3.1) Remove this test when support for 3.0 is no longer needed
-	 */
+  /**
+   * Test the analysis of various greek strings.
+   *
+   * @throws Exception in case an error occurs
+   * @deprecated (3.1) Remove this test when support for 3.0 is no longer needed
+   */
   @Deprecated
-	public void testAnalyzerBWCompat() throws Exception {
-		Analyzer a = new GreekAnalyzer(Version.LUCENE_30);
-		// Verify the correct analysis of capitals and small accented letters
-		assertAnalyzesTo(a, "         ",
-				new String[] { "", "", "", "", "", "",
-				"", "" });
-		// Verify the correct analysis of small letters with diaeresis and the elimination
-		// of punctuation marks
-		assertAnalyzesTo(a, " ()     [] - ",
-				new String[] { "", "", "" });
-		// Verify the correct analysis of capital accented letters and capital letters with diaeresis,
-		// as well as the elimination of stop words
-		assertAnalyzesTo(a, "  ,     ",
-				new String[] { "", "", "", "" });
-	}
-	
+  public void testAnalyzerBWCompat() throws Exception {
+    Analyzer a = new GreekAnalyzer(Version.LUCENE_30);
+    // Verify the correct analysis of capitals and small accented letters
+    assertAnalyzesTo(a, "         ",
+        new String[] { "", "", "", "", "", "",
+        "", "" });
+    // Verify the correct analysis of small letters with diaeresis and the elimination
+    // of punctuation marks
+    assertAnalyzesTo(a, " ()     [] - ",
+        new String[] { "", "", "" });
+    // Verify the correct analysis of capital accented letters and capital letters with diaeresis,
+    // as well as the elimination of stop words
+    assertAnalyzesTo(a, "  ,     ",
+        new String[] { "", "", "", "" });
+  }
+
   public void testReusableTokenStream() throws Exception {
     Analyzer a = new GreekAnalyzer(TEST_VERSION_CURRENT);
     // Verify the correct analysis of capitals and small accented letters, and
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymMap.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymMap.java	(revision 1386643)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymMap.java	(working copy)
@@ -49,7 +49,7 @@
   }
   
   public void testReadMappingRules() throws Exception {
-	SlowSynonymMap synMap;
+    SlowSynonymMap synMap;
 
     // (a)->[b]
     List<String> rules = new ArrayList<String>();
@@ -244,15 +244,15 @@
   }
   
   public void testBigramTokenizer() throws Exception {
-	SlowSynonymMap synMap;
-	
-	// prepare bi-gram tokenizer factory
-	TokenizerFactory tf = new NGramTokenizerFactory();
-	Map<String, String> args = new HashMap<String, String>();
-	args.put("minGramSize","2");
-	args.put("maxGramSize","2");
-	tf.init( args );
+    SlowSynonymMap synMap;
 
+    // prepare bi-gram tokenizer factory
+    TokenizerFactory tf = new NGramTokenizerFactory();
+    Map<String, String> args = new HashMap<String, String>();
+    args.put("minGramSize","2");
+    args.put("maxGramSize","2");
+    tf.init( args );
+
     // (ab)->(bc)->(cd)->[ef][fg][gh]
     List<String> rules = new ArrayList<String>();
     rules.add( "abcd=>efgh" );
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestWordlistLoader.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestWordlistLoader.java	(revision 1386643)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestWordlistLoader.java	(working copy)
@@ -46,8 +46,8 @@
 
   private void checkSet(CharArraySet wordset) {
     assertEquals(3, wordset.size());
-    assertTrue(wordset.contains("ONE"));		// case is not modified
-    assertTrue(wordset.contains("two"));		// surrounding whitespace is removed
+    assertTrue(wordset.contains("ONE"));  // case is not modified
+    assertTrue(wordset.contains("two"));  // surrounding whitespace is removed
     assertTrue(wordset.contains("three"));
     assertFalse(wordset.contains("four"));
   }
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex	(working copy)
@@ -90,18 +90,18 @@
                            [bB][lL][uU][rR]                     |
                            [cC][hH][aA][nN][gG][eE]             |
                            [cC][lL][iI][cC][kK]                 |
-	                         [dD][bB][lL][cC][lL][iI][cC][kK]     |
+                           [dD][bB][lL][cC][lL][iI][cC][kK]     |
                            [eE][rR][rR][oO][rR]                 |
                            [fF][oO][cC][uU][sS]                 |
-	                         [kK][eE][yY][dD][oO][wW][nN]         |
-	                         [kK][eE][yY][pP][rR][eE][sS][sS]     |
-	                         [kK][eE][yY][uU][pP]                 |
+                           [kK][eE][yY][dD][oO][wW][nN]         |
+                           [kK][eE][yY][pP][rR][eE][sS][sS]     |
+                           [kK][eE][yY][uU][pP]                 |
                            [lL][oO][aA][dD]                     |
-	                         [mM][oO][uU][sS][eE][dD][oO][wW][nN] |
-	                         [mM][oO][uU][sS][eE][mM][oO][vV][eE] |
+                           [mM][oO][uU][sS][eE][dD][oO][wW][nN] |
+                           [mM][oO][uU][sS][eE][mM][oO][vV][eE] |
                            [mM][oO][uU][sS][eE][oO][uU][tT]     |
                            [mM][oO][uU][sS][eE][oO][vV][eE][rR] |
-	                         [mM][oO][uU][sS][eE][uU][pP]         |
+                           [mM][oO][uU][sS][eE][uU][pP]         |
                            [rR][eE][sS][eE][tT]                 |
                            [sS][eE][lL][eE][cC][tT]             |
                            [sS][uU][bB][mM][iI][tT]             |
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.java	(working copy)
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 8/6/12 1:23 PM */
+/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/17/12 9:15 AM */
 
 package org.apache.lucene.analysis.charfilter;
 
@@ -40,8 +40,8 @@
 /**
  * This class is a scanner generated by 
  * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 8/6/12 1:23 PM from the specification file
- * <tt>/home/rmuir/workspace/branch_4x/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex</tt>
+ * on 9/17/12 9:15 AM from the specification file
+ * <tt>/Users/Erick/apache/4x_4326/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex</tt>
  */
 public final class HTMLStripCharFilter extends BaseCharFilter {
 
@@ -52,29 +52,29 @@
   private static final int ZZ_BUFFERSIZE = 16384;
 
   /** lexical states */
-  private static final int DOUBLE_QUOTED_STRING = 28;
+  private static final int YYINITIAL = 0;
+  private static final int AMPERSAND = 2;
+  private static final int NUMERIC_CHARACTER = 4;
   private static final int CHARACTER_REFERENCE_TAIL = 6;
-  private static final int START_TAG_TAIL_EXCLUDE = 38;
+  private static final int LEFT_ANGLE_BRACKET = 8;
+  private static final int BANG = 10;
+  private static final int COMMENT = 12;
   private static final int SCRIPT = 14;
+  private static final int SCRIPT_COMMENT = 16;
+  private static final int LEFT_ANGLE_BRACKET_SLASH = 18;
+  private static final int LEFT_ANGLE_BRACKET_SPACE = 20;
   private static final int CDATA = 22;
-  private static final int LEFT_ANGLE_BRACKET = 8;
+  private static final int SERVER_SIDE_INCLUDE = 24;
+  private static final int SINGLE_QUOTED_STRING = 26;
+  private static final int DOUBLE_QUOTED_STRING = 28;
+  private static final int END_TAG_TAIL_INCLUDE = 30;
   private static final int END_TAG_TAIL_EXCLUDE = 32;
-  private static final int SERVER_SIDE_INCLUDE = 24;
   private static final int END_TAG_TAIL_SUBSTITUTE = 34;
-  private static final int SINGLE_QUOTED_STRING = 26;
-  private static final int YYINITIAL = 0;
-  private static final int STYLE = 42;
   private static final int START_TAG_TAIL_INCLUDE = 36;
-  private static final int AMPERSAND = 2;
-  private static final int BANG = 10;
-  private static final int LEFT_ANGLE_BRACKET_SLASH = 18;
+  private static final int START_TAG_TAIL_EXCLUDE = 38;
   private static final int START_TAG_TAIL_SUBSTITUTE = 40;
-  private static final int COMMENT = 12;
-  private static final int SCRIPT_COMMENT = 16;
-  private static final int LEFT_ANGLE_BRACKET_SPACE = 20;
+  private static final int STYLE = 42;
   private static final int STYLE_COMMENT = 44;
-  private static final int NUMERIC_CHARACTER = 4;
-  private static final int END_TAG_TAIL_INCLUDE = 30;
 
   /**
    * ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
@@ -30967,7 +30967,7 @@
       }     
     }
 
-	// numRead < 0
+    // numRead < 0
     return true;
   }
 
@@ -31247,57 +31247,79 @@
       zzMarkedPos = zzMarkedPosL;
 
       switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
-        case 15: 
-          { 
+        case 1: 
+          { return zzBuffer[zzStartRead];
           }
         case 54: break;
-        case 39: 
-          { yybegin(STYLE);
+        case 2: 
+          { inputStart = yychar;
+  inputSegment.clear();
+  inputSegment.append('<');
+  yybegin(LEFT_ANGLE_BRACKET);
           }
         case 55: break;
-        case 27: 
-          { // add (previously matched input length) + (this match length) - (substitution length)
-    cumulativeDiff += inputSegment.length() + yylength() - 1;
-    // position the correction at (already output length) + (substitution length)
-    addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
-    inputSegment.clear();
-    yybegin(YYINITIAL);
-    return BLOCK_LEVEL_START_TAG_REPLACEMENT;
+        case 3: 
+          { inputStart = yychar;
+  inputSegment.clear();
+  inputSegment.append('&');
+  yybegin(AMPERSAND);
           }
         case 56: break;
-        case 30: 
-          { int length = yylength();
-    inputSegment.write(zzBuffer, zzStartRead, length);
-    entitySegment.clear();
-    char ch = entityValues.get(zzBuffer, zzStartRead, length).charValue();
-    entitySegment.append(ch);
-    outputSegment = entitySegment;
-    yybegin(CHARACTER_REFERENCE_TAIL);
+        case 4: 
+          { yypushback(1);
+    outputSegment = inputSegment;
+    outputSegment.restart();
+    yybegin(YYINITIAL);
+    return outputSegment.nextChar();
           }
         case 57: break;
-        case 48: 
-          { inputSegment.clear();
-    yybegin(YYINITIAL);
-    // add (previously matched input length) -- current match and substitution handled below
-    cumulativeDiff += yychar - inputStart;
-    // position the offset correction at (already output length) -- substitution handled below
-    int offsetCorrectionPos = outputCharCount;
-    int returnValue;
-    if (escapeSTYLE) {
-      inputSegment.write(zzBuffer, zzStartRead, yylength());
+        case 5: 
+          { inputSegment.append('#'); yybegin(NUMERIC_CHARACTER);
+          }
+        case 58: break;
+        case 6: 
+          { int matchLength = yylength();
+    inputSegment.write(zzBuffer, zzStartRead, matchLength);
+    if (matchLength <= 7) { // 0x10FFFF = 1114111: max 7 decimal chars
+      String decimalCharRef = yytext();
+      int codePoint = 0;
+      try {
+        codePoint = Integer.parseInt(decimalCharRef);
+      } catch(Exception e) {
+        assert false: "Exception parsing code point '" + decimalCharRef + "'";
+      }
+      if (codePoint <= 0x10FFFF) {
+        outputSegment = entitySegment;
+        outputSegment.clear();
+        if (codePoint >= Character.MIN_SURROGATE
+            && codePoint <= Character.MAX_SURROGATE) {
+          outputSegment.unsafeWrite(REPLACEMENT_CHARACTER);
+        } else {
+          outputSegment.setLength
+              (Character.toChars(codePoint, outputSegment.getArray(), 0));
+        }
+        yybegin(CHARACTER_REFERENCE_TAIL);
+      } else {
+        outputSegment = inputSegment;
+        yybegin(YYINITIAL);
+        return outputSegment.nextChar();
+      }
+    } else {
       outputSegment = inputSegment;
-      returnValue = outputSegment.nextChar();
-    } else {
-      // add (this match length) - (substitution length)
-      cumulativeDiff += yylength() - 1;
-      // add (substitution length)
-      ++offsetCorrectionPos;
-      returnValue = STYLE_REPLACEMENT;
+      yybegin(YYINITIAL);
+      return outputSegment.nextChar();
     }
-    addOffCorrectMap(offsetCorrectionPos, cumulativeDiff);
-    return returnValue;
           }
-        case 58: break;
+        case 59: break;
+        case 7: 
+          { // add (previously matched input length) + (this match length) - (substitution length)
+    cumulativeDiff += inputSegment.length() + yylength() - outputSegment.length();
+    // position the correction at (already output length) + (substitution length)
+    addOffCorrectMap(outputCharCount + outputSegment.length(), cumulativeDiff);
+    yybegin(YYINITIAL);
+    return outputSegment.nextChar();
+          }
+        case 60: break;
         case 8: 
           { inputSegment.write(zzBuffer, zzStartRead, yylength());
     if (null != escapedTags
@@ -31307,162 +31329,112 @@
       yybegin(START_TAG_TAIL_SUBSTITUTE);
     }
           }
-        case 59: break;
-        case 2: 
-          { inputStart = yychar;
-  inputSegment.clear();
-  inputSegment.append('<');
-  yybegin(LEFT_ANGLE_BRACKET);
-          }
-        case 60: break;
-        case 44: 
-          { restoreState = STYLE_COMMENT; yybegin(SERVER_SIDE_INCLUDE);
-          }
         case 61: break;
-        case 21: 
-          { previousRestoreState = restoreState;
-    restoreState = SERVER_SIDE_INCLUDE;
-    yybegin(SINGLE_QUOTED_STRING);
+        case 9: 
+          { inputSegment.write(zzBuffer, zzStartRead, yylength());
+    if (null != escapedTags
+        && escapedTags.contains(zzBuffer, zzStartRead, yylength())) {
+      yybegin(START_TAG_TAIL_INCLUDE);
+    } else {
+      yybegin(START_TAG_TAIL_EXCLUDE);
+    }
           }
         case 62: break;
+        case 10: 
+          { inputSegment.append('!'); yybegin(BANG);
+          }
+        case 63: break;
         case 11: 
           { inputSegment.write(zzBuffer, zzStartRead, yylength());
     yybegin(LEFT_ANGLE_BRACKET_SPACE);
           }
-        case 63: break;
-        case 35: 
-          { yybegin(SCRIPT);
-          }
         case 64: break;
-        case 42: 
-          { restoreState = COMMENT; yybegin(SERVER_SIDE_INCLUDE);
+        case 12: 
+          { inputSegment.append('/'); yybegin(LEFT_ANGLE_BRACKET_SLASH);
           }
         case 65: break;
-        case 10: 
-          { inputSegment.append('!'); yybegin(BANG);
+        case 13: 
+          { inputSegment.append(zzBuffer[zzStartRead]);
           }
         case 66: break;
-        case 51: 
-          { // Handle paired UTF-16 surrogates.
-    String surrogatePair = yytext();
-    char highSurrogate = '\u0000';
-    char lowSurrogate = '\u0000';
-    try {
-      highSurrogate = (char)Integer.parseInt(surrogatePair.substring(2, 6), 16);
-    } catch(Exception e) { // should never happen
-      assert false: "Exception parsing high surrogate '"
-                  + surrogatePair.substring(2, 6) + "'";
-    }
-    try { // Low surrogates are in decimal range [56320, 57343]
-      lowSurrogate = (char)Integer.parseInt(surrogatePair.substring(9, 14));
-    } catch(Exception e) { // should never happen
-      assert false: "Exception parsing low surrogate '"
-                  + surrogatePair.substring(9, 14) + "'";
-    }
-    if (Character.isLowSurrogate(lowSurrogate)) {
-      outputSegment = entitySegment;
-      outputSegment.clear();
-      outputSegment.unsafeWrite(lowSurrogate);
-      // add (previously matched input length) + (this match length) - (substitution length)
-      cumulativeDiff += inputSegment.length() + yylength() - 2;
-      // position the correction at (already output length) + (substitution length)
-      addOffCorrectMap(outputCharCount + 2, cumulativeDiff);
-      inputSegment.clear();
-      yybegin(YYINITIAL);
-      return highSurrogate;
-    }
-    yypushback(surrogatePair.length() - 1); // Consume only '#'
-    inputSegment.append('#');
-    yybegin(NUMERIC_CHARACTER);
+        case 14: 
+          { // add (previously matched input length) + (this match length) [ - (substitution length) = 0 ]
+    cumulativeDiff += inputSegment.length() + yylength();
+    // position the correction at (already output length) [ + (substitution length) = 0 ]
+    addOffCorrectMap(outputCharCount, cumulativeDiff);
+    inputSegment.clear();
+    yybegin(YYINITIAL);
           }
         case 67: break;
-        case 4: 
-          { yypushback(1);
-    outputSegment = inputSegment;
-    outputSegment.restart();
-    yybegin(YYINITIAL);
-    return outputSegment.nextChar();
+        case 15: 
+          { 
           }
         case 68: break;
-        case 43: 
-          { restoreState = SCRIPT_COMMENT; yybegin(SERVER_SIDE_INCLUDE);
+        case 16: 
+          { restoreState = SCRIPT_COMMENT; yybegin(SINGLE_QUOTED_STRING);
           }
         case 69: break;
-        case 52: 
-          { // Handle paired UTF-16 surrogates.
-    String surrogatePair = yytext();
-    char highSurrogate = '\u0000';
-    try { // High surrogates are in decimal range [55296, 56319]
-      highSurrogate = (char)Integer.parseInt(surrogatePair.substring(1, 6));
-    } catch(Exception e) { // should never happen
-      assert false: "Exception parsing high surrogate '"
-                  + surrogatePair.substring(1, 6) + "'";
-    }
-    if (Character.isHighSurrogate(highSurrogate)) {
-      outputSegment = entitySegment;
-      outputSegment.clear();
-      try {
-        outputSegment.unsafeWrite
-            ((char)Integer.parseInt(surrogatePair.substring(10, 14), 16));
-      } catch(Exception e) { // should never happen
-        assert false: "Exception parsing low surrogate '"
-                    + surrogatePair.substring(10, 14) + "'";
-      }
-      // add (previously matched input length) + (this match length) - (substitution length)
-      cumulativeDiff += inputSegment.length() + yylength() - 2;
-      // position the correction at (already output length) + (substitution length)
-      addOffCorrectMap(outputCharCount + 2, cumulativeDiff);
-      inputSegment.clear();
-      yybegin(YYINITIAL);
-      return highSurrogate;
-    }
-    yypushback(surrogatePair.length() - 1); // Consume only '#'
-    inputSegment.append('#');
-    yybegin(NUMERIC_CHARACTER);
+        case 17: 
+          { restoreState = SCRIPT_COMMENT; yybegin(DOUBLE_QUOTED_STRING);
           }
         case 70: break;
-        case 28: 
-          { restoreState = STYLE_COMMENT; yybegin(SINGLE_QUOTED_STRING);
+        case 18: 
+          { inputSegment.write(zzBuffer, zzStartRead, yylength());
+    if (null != escapedTags
+        && escapedTags.contains(zzBuffer, zzStartRead, yylength())) {
+      yybegin(END_TAG_TAIL_INCLUDE);
+    } else {
+      yybegin(END_TAG_TAIL_SUBSTITUTE);
+    }
           }
         case 71: break;
-        case 50: 
-          { // Handle paired UTF-16 surrogates.
-    outputSegment = entitySegment;
-    outputSegment.clear();
-    String surrogatePair = yytext();
-    char highSurrogate = '\u0000';
-    try {
-      highSurrogate = (char)Integer.parseInt(surrogatePair.substring(2, 6), 16);
-    } catch(Exception e) { // should never happen
-      assert false: "Exception parsing high surrogate '"
-                  + surrogatePair.substring(2, 6) + "'";
+        case 19: 
+          { inputSegment.write(zzBuffer, zzStartRead, yylength());
+    if (null != escapedTags
+        && escapedTags.contains(zzBuffer, zzStartRead, yylength())) {
+      yybegin(END_TAG_TAIL_INCLUDE);
+    } else {
+      yybegin(END_TAG_TAIL_EXCLUDE);
     }
-    try {
-      outputSegment.unsafeWrite
-          ((char)Integer.parseInt(surrogatePair.substring(10, 14), 16));
-    } catch(Exception e) { // should never happen
-      assert false: "Exception parsing low surrogate '"
-                  + surrogatePair.substring(10, 14) + "'";
-    }
-    // add (previously matched input length) + (this match length) - (substitution length)
-    cumulativeDiff += inputSegment.length() + yylength() - 2;
-    // position the correction at (already output length) + (substitution length)
-    addOffCorrectMap(outputCharCount + 2, cumulativeDiff);
-    inputSegment.clear();
-    yybegin(YYINITIAL);
-    return highSurrogate;
           }
         case 72: break;
-        case 16: 
-          { restoreState = SCRIPT_COMMENT; yybegin(SINGLE_QUOTED_STRING);
+        case 20: 
+          { inputSegment.write(zzBuffer, zzStartRead, yylength());
           }
         case 73: break;
+        case 21: 
+          { previousRestoreState = restoreState;
+    restoreState = SERVER_SIDE_INCLUDE;
+    yybegin(SINGLE_QUOTED_STRING);
+          }
+        case 74: break;
         case 22: 
           { previousRestoreState = restoreState;
     restoreState = SERVER_SIDE_INCLUDE;
     yybegin(DOUBLE_QUOTED_STRING);
           }
-        case 74: break;
+        case 75: break;
+        case 23: 
+          { yybegin(restoreState); restoreState = previousRestoreState;
+          }
+        case 76: break;
+        case 24: 
+          { inputSegment.write(zzBuffer, zzStartRead, yylength());
+     outputSegment = inputSegment;
+     yybegin(YYINITIAL);
+     return outputSegment.nextChar();
+          }
+        case 77: break;
+        case 25: 
+          { // add (previously matched input length) + (this match length) - (substitution length)
+    cumulativeDiff += inputSegment.length() + yylength() - 1;
+    // position the correction at (already output length) + (substitution length)
+    addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
+    inputSegment.clear();
+    yybegin(YYINITIAL);
+    return BLOCK_LEVEL_END_TAG_REPLACEMENT;
+          }
+        case 78: break;
         case 26: 
           { // add (previously matched input length) + (this match length) [ - (substitution length) = 0 ]
     cumulativeDiff += inputSegment.length() + yylength();
@@ -31472,87 +31444,46 @@
     outputSegment = inputSegment;
     yybegin(YYINITIAL);
           }
-        case 75: break;
-        case 20: 
-          { inputSegment.write(zzBuffer, zzStartRead, yylength());
-          }
-        case 76: break;
-        case 47: 
-          { // add (previously matched input length) + (this match length) [ - (substitution length) = 0 ]
-    cumulativeDiff += inputSegment.length() + yylength();
-    // position the correction at (already output length) [ + (substitution length) = 0 ]
-    addOffCorrectMap(outputCharCount, cumulativeDiff);
+        case 79: break;
+        case 27: 
+          { // add (previously matched input length) + (this match length) - (substitution length)
+    cumulativeDiff += inputSegment.length() + yylength() - 1;
+    // position the correction at (already output length) + (substitution length)
+    addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
     inputSegment.clear();
-    yybegin(CDATA);
+    yybegin(YYINITIAL);
+    return BLOCK_LEVEL_START_TAG_REPLACEMENT;
           }
-        case 77: break;
-        case 33: 
-          { yybegin(YYINITIAL);
-    if (escapeBR) {
-      inputSegment.write(zzBuffer, zzStartRead, yylength());
-      outputSegment = inputSegment;
-      return outputSegment.nextChar();
-    } else {
-      // add (previously matched input length) + (this match length) - (substitution length)
-      cumulativeDiff += inputSegment.length() + yylength() - 1;
-      // position the correction at (already output length) + (substitution length)
-      addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
-      inputSegment.reset();
-      return BR_START_TAG_REPLACEMENT;
-    }
-          }
-        case 78: break;
-        case 23: 
-          { yybegin(restoreState); restoreState = previousRestoreState;
-          }
-        case 79: break;
-        case 32: 
-          { yybegin(COMMENT);
-          }
         case 80: break;
-        case 24: 
-          { inputSegment.write(zzBuffer, zzStartRead, yylength());
-     outputSegment = inputSegment;
-     yybegin(YYINITIAL);
-     return outputSegment.nextChar();
+        case 28: 
+          { restoreState = STYLE_COMMENT; yybegin(SINGLE_QUOTED_STRING);
           }
         case 81: break;
-        case 3: 
-          { inputStart = yychar;
-  inputSegment.clear();
-  inputSegment.append('&');
-  yybegin(AMPERSAND);
+        case 29: 
+          { restoreState = STYLE_COMMENT; yybegin(DOUBLE_QUOTED_STRING);
           }
         case 82: break;
-        case 46: 
-          { yybegin(SCRIPT);
-    if (escapeSCRIPT) {
-      inputSegment.write(zzBuffer, zzStartRead, yylength());
-      outputSegment = inputSegment;
-      inputStart += 1 + yylength();
-      return outputSegment.nextChar();
-    }
+        case 30: 
+          { int length = yylength();
+    inputSegment.write(zzBuffer, zzStartRead, length);
+    entitySegment.clear();
+    char ch = entityValues.get(zzBuffer, zzStartRead, length).charValue();
+    entitySegment.append(ch);
+    outputSegment = entitySegment;
+    yybegin(CHARACTER_REFERENCE_TAIL);
           }
         case 83: break;
-        case 14: 
-          { // add (previously matched input length) + (this match length) [ - (substitution length) = 0 ]
-    cumulativeDiff += inputSegment.length() + yylength();
-    // position the correction at (already output length) [ + (substitution length) = 0 ]
-    addOffCorrectMap(outputCharCount, cumulativeDiff);
-    inputSegment.clear();
-    yybegin(YYINITIAL);
-          }
-        case 84: break;
-        case 6: 
+        case 31: 
           { int matchLength = yylength();
     inputSegment.write(zzBuffer, zzStartRead, matchLength);
-    if (matchLength <= 7) { // 0x10FFFF = 1114111: max 7 decimal chars
-      String decimalCharRef = yytext();
+    if (matchLength <= 6) { // 10FFFF: max 6 hex chars
+      String hexCharRef
+          = new String(zzBuffer, zzStartRead + 1, matchLength - 1);
       int codePoint = 0;
       try {
-        codePoint = Integer.parseInt(decimalCharRef);
+        codePoint = Integer.parseInt(hexCharRef, 16);
       } catch(Exception e) {
-        assert false: "Exception parsing code point '" + decimalCharRef + "'";
+        assert false: "Exception parsing hex code point '" + hexCharRef + "'";
       }
       if (codePoint <= 0x10FFFF) {
         outputSegment = entitySegment;
@@ -31576,7 +31507,27 @@
       return outputSegment.nextChar();
     }
           }
+        case 84: break;
+        case 32: 
+          { yybegin(COMMENT);
+          }
         case 85: break;
+        case 33: 
+          { yybegin(YYINITIAL);
+    if (escapeBR) {
+      inputSegment.write(zzBuffer, zzStartRead, yylength());
+      outputSegment = inputSegment;
+      return outputSegment.nextChar();
+    } else {
+      // add (previously matched input length) + (this match length) - (substitution length)
+      cumulativeDiff += inputSegment.length() + yylength() - 1;
+      // position the correction at (already output length) + (substitution length)
+      addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
+      inputSegment.reset();
+      return BR_START_TAG_REPLACEMENT;
+    }
+          }
+        case 86: break;
         case 34: 
           { // add (previously matched input length) + (this match length) [ - (substitution length) = 0]
     cumulativeDiff += yychar - inputStart + yylength();
@@ -31585,29 +31536,27 @@
     inputSegment.clear();
     yybegin(YYINITIAL);
           }
-        case 86: break;
-        case 5: 
-          { inputSegment.append('#'); yybegin(NUMERIC_CHARACTER);
-          }
         case 87: break;
-        case 13: 
-          { inputSegment.append(zzBuffer[zzStartRead]);
+        case 35: 
+          { yybegin(SCRIPT);
           }
         case 88: break;
-        case 18: 
-          { inputSegment.write(zzBuffer, zzStartRead, yylength());
-    if (null != escapedTags
-        && escapedTags.contains(zzBuffer, zzStartRead, yylength())) {
-      yybegin(END_TAG_TAIL_INCLUDE);
+        case 36: 
+          { yybegin(YYINITIAL);
+    if (escapeBR) {
+      inputSegment.write(zzBuffer, zzStartRead, yylength());
+      outputSegment = inputSegment;
+      return outputSegment.nextChar();
     } else {
-      yybegin(END_TAG_TAIL_SUBSTITUTE);
+      // add (previously matched input length) + (this match length) - (substitution length)
+      cumulativeDiff += inputSegment.length() + yylength() - 1;
+      // position the correction at (already output length) + (substitution length)
+      addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
+      inputSegment.reset();
+      return BR_END_TAG_REPLACEMENT;
     }
           }
         case 89: break;
-        case 40: 
-          { yybegin(SCRIPT_COMMENT);
-          }
-        case 90: break;
         case 37: 
           { // add (this match length) [ - (substitution length) = 0 ]
     cumulativeDiff += yylength();
@@ -31615,21 +31564,87 @@
     addOffCorrectMap(outputCharCount, cumulativeDiff);
     yybegin(YYINITIAL);
           }
+        case 90: break;
+        case 38: 
+          { yybegin(restoreState);
+          }
         case 91: break;
-        case 12: 
-          { inputSegment.append('/'); yybegin(LEFT_ANGLE_BRACKET_SLASH);
+        case 39: 
+          { yybegin(STYLE);
           }
         case 92: break;
-        case 9: 
-          { inputSegment.write(zzBuffer, zzStartRead, yylength());
-    if (null != escapedTags
-        && escapedTags.contains(zzBuffer, zzStartRead, yylength())) {
-      yybegin(START_TAG_TAIL_INCLUDE);
+        case 40: 
+          { yybegin(SCRIPT_COMMENT);
+          }
+        case 93: break;
+        case 41: 
+          { yybegin(STYLE_COMMENT);
+          }
+        case 94: break;
+        case 42: 
+          { restoreState = COMMENT; yybegin(SERVER_SIDE_INCLUDE);
+          }
+        case 95: break;
+        case 43: 
+          { restoreState = SCRIPT_COMMENT; yybegin(SERVER_SIDE_INCLUDE);
+          }
+        case 96: break;
+        case 44: 
+          { restoreState = STYLE_COMMENT; yybegin(SERVER_SIDE_INCLUDE);
+          }
+        case 97: break;
+        case 45: 
+          { yybegin(STYLE);
+    if (escapeSTYLE) {
+      inputSegment.write(zzBuffer, zzStartRead, yylength());
+      outputSegment = inputSegment;
+      inputStart += 1 + yylength();
+      return outputSegment.nextChar();
+    }
+          }
+        case 98: break;
+        case 46: 
+          { yybegin(SCRIPT);
+    if (escapeSCRIPT) {
+      inputSegment.write(zzBuffer, zzStartRead, yylength());
+      outputSegment = inputSegment;
+      inputStart += 1 + yylength();
+      return outputSegment.nextChar();
+    }
+          }
+        case 99: break;
+        case 47: 
+          { // add (previously matched input length) + (this match length) [ - (substitution length) = 0 ]
+    cumulativeDiff += inputSegment.length() + yylength();
+    // position the correction at (already output length) [ + (substitution length) = 0 ]
+    addOffCorrectMap(outputCharCount, cumulativeDiff);
+    inputSegment.clear();
+    yybegin(CDATA);
+          }
+        case 100: break;
+        case 48: 
+          { inputSegment.clear();
+    yybegin(YYINITIAL);
+    // add (previously matched input length) -- current match and substitution handled below
+    cumulativeDiff += yychar - inputStart;
+    // position the offset correction at (already output length) -- substitution handled below
+    int offsetCorrectionPos = outputCharCount;
+    int returnValue;
+    if (escapeSTYLE) {
+      inputSegment.write(zzBuffer, zzStartRead, yylength());
+      outputSegment = inputSegment;
+      returnValue = outputSegment.nextChar();
     } else {
-      yybegin(START_TAG_TAIL_EXCLUDE);
+      // add (this match length) - (substitution length)
+      cumulativeDiff += yylength() - 1;
+      // add (substitution length)
+      ++offsetCorrectionPos;
+      returnValue = STYLE_REPLACEMENT;
     }
+    addOffCorrectMap(offsetCorrectionPos, cumulativeDiff);
+    return returnValue;
           }
-        case 93: break;
+        case 101: break;
         case 49: 
           { inputSegment.clear();
     yybegin(YYINITIAL);
@@ -31652,89 +31667,102 @@
     addOffCorrectMap(offsetCorrectionPos, cumulativeDiff);
     return returnValue;
           }
-        case 94: break;
-        case 29: 
-          { restoreState = STYLE_COMMENT; yybegin(DOUBLE_QUOTED_STRING);
-          }
-        case 95: break;
-        case 17: 
-          { restoreState = SCRIPT_COMMENT; yybegin(DOUBLE_QUOTED_STRING);
-          }
-        case 96: break;
-        case 45: 
-          { yybegin(STYLE);
-    if (escapeSTYLE) {
-      inputSegment.write(zzBuffer, zzStartRead, yylength());
-      outputSegment = inputSegment;
-      inputStart += 1 + yylength();
-      return outputSegment.nextChar();
+        case 102: break;
+        case 50: 
+          { // Handle paired UTF-16 surrogates.
+    outputSegment = entitySegment;
+    outputSegment.clear();
+    String surrogatePair = yytext();
+    char highSurrogate = '\u0000';
+    try {
+      highSurrogate = (char)Integer.parseInt(surrogatePair.substring(2, 6), 16);
+    } catch(Exception e) { // should never happen
+      assert false: "Exception parsing high surrogate '"
+                  + surrogatePair.substring(2, 6) + "'";
     }
-          }
-        case 97: break;
-        case 7: 
-          { // add (previously matched input length) + (this match length) - (substitution length)
-    cumulativeDiff += inputSegment.length() + yylength() - outputSegment.length();
+    try {
+      outputSegment.unsafeWrite
+          ((char)Integer.parseInt(surrogatePair.substring(10, 14), 16));
+    } catch(Exception e) { // should never happen
+      assert false: "Exception parsing low surrogate '"
+                  + surrogatePair.substring(10, 14) + "'";
+    }
+    // add (previously matched input length) + (this match length) - (substitution length)
+    cumulativeDiff += inputSegment.length() + yylength() - 2;
     // position the correction at (already output length) + (substitution length)
-    addOffCorrectMap(outputCharCount + outputSegment.length(), cumulativeDiff);
+    addOffCorrectMap(outputCharCount + 2, cumulativeDiff);
+    inputSegment.clear();
     yybegin(YYINITIAL);
-    return outputSegment.nextChar();
+    return highSurrogate;
           }
-        case 98: break;
-        case 19: 
-          { inputSegment.write(zzBuffer, zzStartRead, yylength());
-    if (null != escapedTags
-        && escapedTags.contains(zzBuffer, zzStartRead, yylength())) {
-      yybegin(END_TAG_TAIL_INCLUDE);
-    } else {
-      yybegin(END_TAG_TAIL_EXCLUDE);
+        case 103: break;
+        case 51: 
+          { // Handle paired UTF-16 surrogates.
+    String surrogatePair = yytext();
+    char highSurrogate = '\u0000';
+    char lowSurrogate = '\u0000';
+    try {
+      highSurrogate = (char)Integer.parseInt(surrogatePair.substring(2, 6), 16);
+    } catch(Exception e) { // should never happen
+      assert false: "Exception parsing high surrogate '"
+                  + surrogatePair.substring(2, 6) + "'";
     }
+    try { // Low surrogates are in decimal range [56320, 57343]
+      lowSurrogate = (char)Integer.parseInt(surrogatePair.substring(9, 14));
+    } catch(Exception e) { // should never happen
+      assert false: "Exception parsing low surrogate '"
+                  + surrogatePair.substring(9, 14) + "'";
+    }
+    if (Character.isLowSurrogate(lowSurrogate)) {
+      outputSegment = entitySegment;
+      outputSegment.clear();
+      outputSegment.unsafeWrite(lowSurrogate);
+      // add (previously matched input length) + (this match length) - (substitution length)
+      cumulativeDiff += inputSegment.length() + yylength() - 2;
+      // position the correction at (already output length) + (substitution length)
+      addOffCorrectMap(outputCharCount + 2, cumulativeDiff);
+      inputSegment.clear();
+      yybegin(YYINITIAL);
+      return highSurrogate;
+    }
+    yypushback(surrogatePair.length() - 1); // Consume only '#'
+    inputSegment.append('#');
+    yybegin(NUMERIC_CHARACTER);
           }
-        case 99: break;
-        case 25: 
-          { // add (previously matched input length) + (this match length) - (substitution length)
-    cumulativeDiff += inputSegment.length() + yylength() - 1;
-    // position the correction at (already output length) + (substitution length)
-    addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
-    inputSegment.clear();
-    yybegin(YYINITIAL);
-    return BLOCK_LEVEL_END_TAG_REPLACEMENT;
-          }
-        case 100: break;
-        case 31: 
-          { int matchLength = yylength();
-    inputSegment.write(zzBuffer, zzStartRead, matchLength);
-    if (matchLength <= 6) { // 10FFFF: max 6 hex chars
-      String hexCharRef
-          = new String(zzBuffer, zzStartRead + 1, matchLength - 1);
-      int codePoint = 0;
+        case 104: break;
+        case 52: 
+          { // Handle paired UTF-16 surrogates.
+    String surrogatePair = yytext();
+    char highSurrogate = '\u0000';
+    try { // High surrogates are in decimal range [55296, 56319]
+      highSurrogate = (char)Integer.parseInt(surrogatePair.substring(1, 6));
+    } catch(Exception e) { // should never happen
+      assert false: "Exception parsing high surrogate '"
+                  + surrogatePair.substring(1, 6) + "'";
+    }
+    if (Character.isHighSurrogate(highSurrogate)) {
+      outputSegment = entitySegment;
+      outputSegment.clear();
       try {
-        codePoint = Integer.parseInt(hexCharRef, 16);
-      } catch(Exception e) {
-        assert false: "Exception parsing hex code point '" + hexCharRef + "'";
+        outputSegment.unsafeWrite
+            ((char)Integer.parseInt(surrogatePair.substring(10, 14), 16));
+      } catch(Exception e) { // should never happen
+        assert false: "Exception parsing low surrogate '"
+                    + surrogatePair.substring(10, 14) + "'";
       }
-      if (codePoint <= 0x10FFFF) {
-        outputSegment = entitySegment;
-        outputSegment.clear();
-        if (codePoint >= Character.MIN_SURROGATE
-            && codePoint <= Character.MAX_SURROGATE) {
-          outputSegment.unsafeWrite(REPLACEMENT_CHARACTER);
-        } else {
-          outputSegment.setLength
-              (Character.toChars(codePoint, outputSegment.getArray(), 0));
-        }
-        yybegin(CHARACTER_REFERENCE_TAIL);
-      } else {
-        outputSegment = inputSegment;
-        yybegin(YYINITIAL);
-        return outputSegment.nextChar();
-      }
-    } else {
-      outputSegment = inputSegment;
+      // add (previously matched input length) + (this match length) - (substitution length)
+      cumulativeDiff += inputSegment.length() + yylength() - 2;
+      // position the correction at (already output length) + (substitution length)
+      addOffCorrectMap(outputCharCount + 2, cumulativeDiff);
+      inputSegment.clear();
       yybegin(YYINITIAL);
-      return outputSegment.nextChar();
+      return highSurrogate;
     }
+    yypushback(surrogatePair.length() - 1); // Consume only '#'
+    inputSegment.append('#');
+    yybegin(NUMERIC_CHARACTER);
           }
-        case 101: break;
+        case 105: break;
         case 53: 
           { // Handle paired UTF-16 surrogates.
     String surrogatePair = yytext();
@@ -31770,34 +31798,6 @@
     inputSegment.append('#');
     yybegin(NUMERIC_CHARACTER);
           }
-        case 102: break;
-        case 36: 
-          { yybegin(YYINITIAL);
-    if (escapeBR) {
-      inputSegment.write(zzBuffer, zzStartRead, yylength());
-      outputSegment = inputSegment;
-      return outputSegment.nextChar();
-    } else {
-      // add (previously matched input length) + (this match length) - (substitution length)
-      cumulativeDiff += inputSegment.length() + yylength() - 1;
-      // position the correction at (already output length) + (substitution length)
-      addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
-      inputSegment.reset();
-      return BR_END_TAG_REPLACEMENT;
-    }
-          }
-        case 103: break;
-        case 38: 
-          { yybegin(restoreState);
-          }
-        case 104: break;
-        case 41: 
-          { yybegin(STYLE_COMMENT);
-          }
-        case 105: break;
-        case 1: 
-          { return zzBuffer[zzStartRead];
-          }
         case 106: break;
         default: 
           if (zzInput == YYEOF && zzStartRead == zzCurrentPos) {
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilterFactory.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilterFactory.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilterFactory.java	(working copy)
@@ -30,7 +30,7 @@
  *   &lt;analyzer&gt;
  *     &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
  *     &lt;filter class="solr.DictionaryCompoundWordTokenFilterFactory" dictionary="dictionary.txt"
- *     	     minWordSize="5" minSubwordSize="2" maxSubwordSize="15" onlyLongestMatch="true"/&gt;
+ *         minWordSize="5" minSubwordSize="2" maxSubwordSize="15" onlyLongestMatch="true"/&gt;
  *   &lt;/analyzer&gt;
  * &lt;/fieldType&gt;</pre>
  *
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java	(working copy)
@@ -50,7 +50,7 @@
  *   &lt;analyzer&gt;
  *     &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
  *     &lt;filter class="solr.HyphenationCompoundWordTokenFilterFactory" hyphenator="hyphenator.xml" encoding="UTF-8"
- *     	     dictionary="dictionary.txt" minWordSize="5" minSubwordSize="2" maxSubwordSize="15" onlyLongestMatch="false"/&gt;
+ *         dictionary="dictionary.txt" minWordSize="5" minSubwordSize="2" maxSubwordSize="15" onlyLongestMatch="false"/&gt;
  *   &lt;/analyzer&gt;
  * &lt;/fieldType&gt;</pre>
  *
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.jflex
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.jflex	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.jflex	(working copy)
@@ -115,8 +115,8 @@
 
 %%
 
-// UAX#29 WB1. 	sot 	 	
-//        WB2. 		 	eot
+// UAX#29 WB1.   sot   
+//        WB2.        eot
 //
 <<EOF>> { return StandardTokenizerInterface.YYEOF; }
 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.jflex
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.jflex	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.jflex	(working copy)
@@ -200,8 +200,8 @@
 
 %%
 
-// UAX#29 WB1. 	sot 	 	
-//        WB2. 		 	eot
+// UAX#29 WB1.   sot   
+//        WB2.        eot
 //
 <<EOF>> { return StandardTokenizerInterface.YYEOF; }
 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.java	(working copy)
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 8/6/12 1:23 PM */
+/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/17/12 9:14 AM */
 
 package org.apache.lucene.analysis.standard;
 
@@ -936,7 +936,7 @@
       }     
     }
 
-	// numRead < 0
+    // numRead < 0
     return true;
   }
 
@@ -1157,36 +1157,36 @@
       zzMarkedPos = zzMarkedPosL;
 
       switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
+        case 1: 
+          { /* Break so we don't hit fall-through warning: */ break; /* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
+          }
+        case 9: break;
         case 2: 
           { return WORD_TYPE;
           }
-        case 9: break;
-        case 5: 
-          { return SOUTH_EAST_ASIAN_TYPE;
+        case 10: break;
+        case 3: 
+          { return NUMERIC_TYPE;
           }
-        case 10: break;
+        case 11: break;
         case 4: 
           { return KATAKANA_TYPE;
           }
-        case 11: break;
-        case 6: 
-          { return IDEOGRAPHIC_TYPE;
-          }
         case 12: break;
-        case 8: 
-          { return HANGUL_TYPE;
+        case 5: 
+          { return SOUTH_EAST_ASIAN_TYPE;
           }
         case 13: break;
-        case 3: 
-          { return NUMERIC_TYPE;
+        case 6: 
+          { return IDEOGRAPHIC_TYPE;
           }
         case 14: break;
         case 7: 
           { return HIRAGANA_TYPE;
           }
         case 15: break;
-        case 1: 
-          { /* Break so we don't hit fall-through warning: */ break; /* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
+        case 8: 
+          { return HANGUL_TYPE;
           }
         case 16: break;
         default: 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/StandardTokenizerImpl31.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/StandardTokenizerImpl31.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/StandardTokenizerImpl31.java	(working copy)
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 8/6/12 1:23 PM */
+/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/17/12 9:14 AM */
 
 package org.apache.lucene.analysis.standard.std31;
 
@@ -34,8 +34,8 @@
 /**
  * This class is a scanner generated by 
  * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 8/6/12 1:23 PM from the specification file
- * <tt>/home/rmuir/workspace/branch_4x/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/StandardTokenizerImpl31.jflex</tt>
+ * on 9/17/12 9:14 AM from the specification file
+ * <tt>/Users/Erick/apache/4x_4326/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/StandardTokenizerImpl31.jflex</tt>
  */
 public final class StandardTokenizerImpl31 implements StandardTokenizerInterface {
 
@@ -810,7 +810,7 @@
       }     
     }
 
-	// numRead < 0
+    // numRead < 0
     return true;
   }
 
@@ -1031,36 +1031,36 @@
       zzMarkedPos = zzMarkedPosL;
 
       switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
+        case 1: 
+          { /* Break so we don't hit fall-through warning: */ break; /* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
+          }
+        case 9: break;
         case 2: 
           { return WORD_TYPE;
           }
-        case 9: break;
-        case 5: 
-          { return SOUTH_EAST_ASIAN_TYPE;
+        case 10: break;
+        case 3: 
+          { return NUMERIC_TYPE;
           }
-        case 10: break;
+        case 11: break;
         case 4: 
           { return KATAKANA_TYPE;
           }
-        case 11: break;
-        case 6: 
-          { return IDEOGRAPHIC_TYPE;
-          }
         case 12: break;
-        case 8: 
-          { return HANGUL_TYPE;
+        case 5: 
+          { return SOUTH_EAST_ASIAN_TYPE;
           }
         case 13: break;
-        case 3: 
-          { return NUMERIC_TYPE;
+        case 6: 
+          { return IDEOGRAPHIC_TYPE;
           }
         case 14: break;
         case 7: 
           { return HIRAGANA_TYPE;
           }
         case 15: break;
-        case 1: 
-          { /* Break so we don't hit fall-through warning: */ break; /* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
+        case 8: 
+          { return HANGUL_TYPE;
           }
         case 16: break;
         default: 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/UAX29URLEmailTokenizerImpl31.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/UAX29URLEmailTokenizerImpl31.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/UAX29URLEmailTokenizerImpl31.java	(working copy)
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 8/6/12 1:23 PM */
+/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/17/12 9:14 AM */
 
 package org.apache.lucene.analysis.standard.std31;
 
@@ -34,8 +34,8 @@
 /**
  * This class is a scanner generated by 
  * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 8/6/12 1:23 PM from the specification file
- * <tt>/home/rmuir/workspace/branch_4x/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/UAX29URLEmailTokenizerImpl31.jflex</tt>
+ * on 9/17/12 9:14 AM from the specification file
+ * <tt>/Users/Erick/apache/4x_4326/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/UAX29URLEmailTokenizerImpl31.jflex</tt>
  */
 public final class UAX29URLEmailTokenizerImpl31 implements StandardTokenizerInterface {
 
@@ -3385,7 +3385,7 @@
       }     
     }
 
-	// numRead < 0
+    // numRead < 0
     return true;
   }
 
@@ -3606,44 +3606,44 @@
       zzMarkedPos = zzMarkedPosL;
 
       switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
+        case 1: 
+          { /* Break so we don't hit fall-through warning: */ break;/* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
+          }
+        case 11: break;
         case 2: 
           { return WORD_TYPE;
           }
-        case 11: break;
-        case 5: 
-          { return SOUTH_EAST_ASIAN_TYPE;
-          }
         case 12: break;
-        case 1: 
-          { /* Break so we don't hit fall-through warning: */ break;/* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
+        case 3: 
+          { return NUMERIC_TYPE;
           }
         case 13: break;
-        case 10: 
-          { return URL_TYPE;
+        case 4: 
+          { return KATAKANA_TYPE;
           }
         case 14: break;
-        case 9: 
-          { return EMAIL_TYPE;
+        case 5: 
+          { return SOUTH_EAST_ASIAN_TYPE;
           }
         case 15: break;
-        case 4: 
-          { return KATAKANA_TYPE;
-          }
-        case 16: break;
         case 6: 
           { return IDEOGRAPHIC_TYPE;
           }
+        case 16: break;
+        case 7: 
+          { return HIRAGANA_TYPE;
+          }
         case 17: break;
         case 8: 
           { return HANGUL_TYPE;
           }
         case 18: break;
-        case 3: 
-          { return NUMERIC_TYPE;
+        case 9: 
+          { return EMAIL_TYPE;
           }
         case 19: break;
-        case 7: 
-          { return HIRAGANA_TYPE;
+        case 10: 
+          { return URL_TYPE;
           }
         case 20: break;
         default: 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/StandardTokenizerImpl31.jflex
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/StandardTokenizerImpl31.jflex	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/StandardTokenizerImpl31.jflex	(working copy)
@@ -107,8 +107,8 @@
 
 %%
 
-// UAX#29 WB1. 	sot 	 	
-//        WB2. 		 	eot
+// UAX#29 WB1.   sot   
+//        WB2.        eot
 //
 <<EOF>> { return StandardTokenizerInterface.YYEOF; }
 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/UAX29URLEmailTokenizerImpl31.jflex
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/UAX29URLEmailTokenizerImpl31.jflex	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/UAX29URLEmailTokenizerImpl31.jflex	(working copy)
@@ -189,8 +189,8 @@
 
 %%
 
-// UAX#29 WB1. 	sot 	 	
-//        WB2. 		 	eot
+// UAX#29 WB1.   sot   
+//        WB2.        eot
 //
 <<EOF>> { return StandardTokenizerInterface.YYEOF; }
 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.java	(working copy)
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 8/6/12 1:23 PM */
+/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/17/12 9:14 AM */
 
 package org.apache.lucene.analysis.standard;
 
@@ -4126,7 +4126,7 @@
       }     
     }
 
-	// numRead < 0
+    // numRead < 0
     return true;
   }
 
@@ -4347,50 +4347,50 @@
       zzMarkedPos = zzMarkedPosL;
 
       switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
-        case 11: 
-          // lookahead expression with fixed base length
-          zzMarkedPos = zzStartRead + 6;
-          { return WORD_TYPE;
+        case 1: 
+          { /* Break so we don't hit fall-through warning: */ break;/* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
           }
         case 12: break;
         case 2: 
           { return WORD_TYPE;
           }
         case 13: break;
-        case 5: 
-          { return SOUTH_EAST_ASIAN_TYPE;
+        case 3: 
+          { return NUMERIC_TYPE;
           }
         case 14: break;
-        case 1: 
-          { /* Break so we don't hit fall-through warning: */ break;/* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
+        case 4: 
+          { return KATAKANA_TYPE;
           }
         case 15: break;
-        case 10: 
-          { return URL_TYPE;
+        case 5: 
+          { return SOUTH_EAST_ASIAN_TYPE;
           }
         case 16: break;
-        case 9: 
-          { return EMAIL_TYPE;
+        case 6: 
+          { return IDEOGRAPHIC_TYPE;
           }
         case 17: break;
-        case 4: 
-          { return KATAKANA_TYPE;
+        case 7: 
+          { return HIRAGANA_TYPE;
           }
         case 18: break;
-        case 6: 
-          { return IDEOGRAPHIC_TYPE;
-          }
-        case 19: break;
         case 8: 
           { return HANGUL_TYPE;
           }
+        case 19: break;
+        case 9: 
+          { return EMAIL_TYPE;
+          }
         case 20: break;
-        case 3: 
-          { return NUMERIC_TYPE;
+        case 10: 
+          { return URL_TYPE;
           }
         case 21: break;
-        case 7: 
-          { return HIRAGANA_TYPE;
+        case 11: 
+          // lookahead expression with fixed base length
+          zzMarkedPos = zzStartRead + 6;
+          { return WORD_TYPE;
           }
         case 22: break;
         default: 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/StandardTokenizerImpl34.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/StandardTokenizerImpl34.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/StandardTokenizerImpl34.java	(working copy)
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 8/6/12 1:23 PM */
+/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/17/12 9:14 AM */
 
 package org.apache.lucene.analysis.standard.std34;
 
@@ -32,8 +32,8 @@
 /**
  * This class is a scanner generated by 
  * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 8/6/12 1:23 PM from the specification file
- * <tt>/home/rmuir/workspace/branch_4x/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/StandardTokenizerImpl34.jflex</tt>
+ * on 9/17/12 9:14 AM from the specification file
+ * <tt>/Users/Erick/apache/4x_4326/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/StandardTokenizerImpl34.jflex</tt>
  */
 public final class StandardTokenizerImpl34 implements StandardTokenizerInterface {
 
@@ -827,7 +827,7 @@
       }     
     }
 
-	// numRead < 0
+    // numRead < 0
     return true;
   }
 
@@ -1048,36 +1048,36 @@
       zzMarkedPos = zzMarkedPosL;
 
       switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
+        case 1: 
+          { /* Break so we don't hit fall-through warning: */ break; /* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
+          }
+        case 9: break;
         case 2: 
           { return WORD_TYPE;
           }
-        case 9: break;
-        case 5: 
-          { return SOUTH_EAST_ASIAN_TYPE;
+        case 10: break;
+        case 3: 
+          { return NUMERIC_TYPE;
           }
-        case 10: break;
+        case 11: break;
         case 4: 
           { return KATAKANA_TYPE;
           }
-        case 11: break;
-        case 6: 
-          { return IDEOGRAPHIC_TYPE;
-          }
         case 12: break;
-        case 8: 
-          { return HANGUL_TYPE;
+        case 5: 
+          { return SOUTH_EAST_ASIAN_TYPE;
           }
         case 13: break;
-        case 3: 
-          { return NUMERIC_TYPE;
+        case 6: 
+          { return IDEOGRAPHIC_TYPE;
           }
         case 14: break;
         case 7: 
           { return HIRAGANA_TYPE;
           }
         case 15: break;
-        case 1: 
-          { /* Break so we don't hit fall-through warning: */ break; /* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
+        case 8: 
+          { return HANGUL_TYPE;
           }
         case 16: break;
         default: 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/UAX29URLEmailTokenizerImpl34.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/UAX29URLEmailTokenizerImpl34.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/UAX29URLEmailTokenizerImpl34.java	(working copy)
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 8/6/12 1:23 PM */
+/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/17/12 9:14 AM */
 
 package org.apache.lucene.analysis.standard.std34;
 
@@ -35,8 +35,8 @@
 /**
  * This class is a scanner generated by 
  * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 8/6/12 1:23 PM from the specification file
- * <tt>/home/rmuir/workspace/branch_4x/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/UAX29URLEmailTokenizerImpl34.jflex</tt>
+ * on 9/17/12 9:14 AM from the specification file
+ * <tt>/Users/Erick/apache/4x_4326/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/UAX29URLEmailTokenizerImpl34.jflex</tt>
  */
 public final class UAX29URLEmailTokenizerImpl34 implements StandardTokenizerInterface {
 
@@ -3473,7 +3473,7 @@
       }     
     }
 
-	// numRead < 0
+    // numRead < 0
     return true;
   }
 
@@ -3694,44 +3694,44 @@
       zzMarkedPos = zzMarkedPosL;
 
       switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
+        case 1: 
+          { /* Break so we don't hit fall-through warning: */ break;/* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
+          }
+        case 11: break;
         case 2: 
           { return WORD_TYPE;
           }
-        case 11: break;
-        case 5: 
-          { return SOUTH_EAST_ASIAN_TYPE;
-          }
         case 12: break;
-        case 1: 
-          { /* Break so we don't hit fall-through warning: */ break;/* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
+        case 3: 
+          { return NUMERIC_TYPE;
           }
         case 13: break;
-        case 10: 
-          { return URL_TYPE;
+        case 4: 
+          { return KATAKANA_TYPE;
           }
         case 14: break;
-        case 9: 
-          { return EMAIL_TYPE;
+        case 5: 
+          { return SOUTH_EAST_ASIAN_TYPE;
           }
         case 15: break;
-        case 4: 
-          { return KATAKANA_TYPE;
-          }
-        case 16: break;
         case 6: 
           { return IDEOGRAPHIC_TYPE;
           }
+        case 16: break;
+        case 7: 
+          { return HIRAGANA_TYPE;
+          }
         case 17: break;
         case 8: 
           { return HANGUL_TYPE;
           }
         case 18: break;
-        case 3: 
-          { return NUMERIC_TYPE;
+        case 9: 
+          { return EMAIL_TYPE;
           }
         case 19: break;
-        case 7: 
-          { return HIRAGANA_TYPE;
+        case 10: 
+          { return URL_TYPE;
           }
         case 20: break;
         default: 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/StandardTokenizerImpl34.jflex
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/StandardTokenizerImpl34.jflex	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/StandardTokenizerImpl34.jflex	(working copy)
@@ -107,8 +107,8 @@
 
 %%
 
-// UAX#29 WB1. 	sot 	 	
-//        WB2. 		 	eot
+// UAX#29 WB1.   sot   
+//        WB2.        eot
 //
 <<EOF>> { return StandardTokenizerInterface.YYEOF; }
 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/UAX29URLEmailTokenizerImpl34.jflex
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/UAX29URLEmailTokenizerImpl34.jflex	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/UAX29URLEmailTokenizerImpl34.jflex	(working copy)
@@ -192,8 +192,8 @@
 
 %%
 
-// UAX#29 WB1. 	sot 	 	
-//        WB2. 		 	eot
+// UAX#29 WB1.   sot   
+//        WB2.        eot
 //
 <<EOF>> { return StandardTokenizerInterface.YYEOF; }
 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std36/UAX29URLEmailTokenizerImpl36.jflex
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std36/UAX29URLEmailTokenizerImpl36.jflex	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std36/UAX29URLEmailTokenizerImpl36.jflex	(working copy)
@@ -189,8 +189,8 @@
 
 %%
 
-// UAX#29 WB1. 	sot 	 	
-//        WB2. 		 	eot
+// UAX#29 WB1.   sot   
+//        WB2.        eot
 //
 <<EOF>> { return StandardTokenizerInterface.YYEOF; }
 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std36/UAX29URLEmailTokenizerImpl36.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std36/UAX29URLEmailTokenizerImpl36.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std36/UAX29URLEmailTokenizerImpl36.java	(working copy)
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 8/6/12 1:23 PM */
+/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/17/12 9:14 AM */
 
 package org.apache.lucene.analysis.standard.std36;
 
@@ -32,8 +32,8 @@
 /**
  * This class is a scanner generated by 
  * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 8/6/12 1:23 PM from the specification file
- * <tt>/home/rmuir/workspace/branch_4x/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std36/UAX29URLEmailTokenizerImpl36.jflex</tt>
+ * on 9/17/12 9:14 AM from the specification file
+ * <tt>/Users/Erick/apache/4x_4326/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std36/UAX29URLEmailTokenizerImpl36.jflex</tt>
  */
 public final class UAX29URLEmailTokenizerImpl36 implements StandardTokenizerInterface {
 
@@ -3909,7 +3909,7 @@
       }     
     }
 
-	// numRead < 0
+    // numRead < 0
     return true;
   }
 
@@ -4130,50 +4130,50 @@
       zzMarkedPos = zzMarkedPosL;
 
       switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
-        case 11: 
-          // lookahead expression with fixed base length
-          zzMarkedPos = zzStartRead + 6;
-          { return WORD_TYPE;
+        case 1: 
+          { /* Break so we don't hit fall-through warning: */ break;/* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
           }
         case 12: break;
         case 2: 
           { return WORD_TYPE;
           }
         case 13: break;
-        case 5: 
-          { return SOUTH_EAST_ASIAN_TYPE;
+        case 3: 
+          { return NUMERIC_TYPE;
           }
         case 14: break;
-        case 1: 
-          { /* Break so we don't hit fall-through warning: */ break;/* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */
+        case 4: 
+          { return KATAKANA_TYPE;
           }
         case 15: break;
-        case 10: 
-          { return URL_TYPE;
+        case 5: 
+          { return SOUTH_EAST_ASIAN_TYPE;
           }
         case 16: break;
-        case 9: 
-          { return EMAIL_TYPE;
+        case 6: 
+          { return IDEOGRAPHIC_TYPE;
           }
         case 17: break;
-        case 4: 
-          { return KATAKANA_TYPE;
+        case 7: 
+          { return HIRAGANA_TYPE;
           }
         case 18: break;
-        case 6: 
-          { return IDEOGRAPHIC_TYPE;
-          }
-        case 19: break;
         case 8: 
           { return HANGUL_TYPE;
           }
+        case 19: break;
+        case 9: 
+          { return EMAIL_TYPE;
+          }
         case 20: break;
-        case 3: 
-          { return NUMERIC_TYPE;
+        case 10: 
+          { return URL_TYPE;
           }
         case 21: break;
-        case 7: 
-          { return HIRAGANA_TYPE;
+        case 11: 
+          // lookahead expression with fixed base length
+          zzMarkedPos = zzStartRead + 6;
+          { return WORD_TYPE;
           }
         case 22: break;
         default: 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex	(working copy)
@@ -79,7 +79,7 @@
 // use a post-filter to remove dots
 ACRONYM    =  {LETTER} "." ({LETTER} ".")+
 
-ACRONYM_DEP	= {ALPHANUM} "." ({ALPHANUM} ".")+
+ACRONYM_DEP  = {ALPHANUM} "." ({ALPHANUM} ".")+
 
 // company names like AT&T and Excite@Home.
 COMPANY    =  {ALPHA} ("&"|"@") {ALPHA}
@@ -100,7 +100,7 @@
            | {HAS_DIGIT} {P} {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+)
 
 // punctuation
-P	         = ("_"|"-"|"/"|"."|",")
+P           = ("_"|"-"|"/"|"."|",")
 
 // at least one digit
 HAS_DIGIT  = ({LETTER}|[:digit:])* [:digit:] ({LETTER}|[:digit:])*
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/SUPPLEMENTARY.jflex-macro
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/SUPPLEMENTARY.jflex-macro	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/SUPPLEMENTARY.jflex-macro	(working copy)
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-// Generated using ICU4J 49.1.0.0 on Monday, August 6, 2012 5:23:08 PM UTC
+// Generated using ICU4J 49.1.0.0 on Monday, September 17, 2012 1:14:38 PM UTC
 // by org.apache.lucene.analysis.icu.GenerateJFlexSupplementaryMacros
 
 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.java	(working copy)
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 8/6/12 1:23 PM */
+/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/17/12 9:14 AM */
 
 package org.apache.lucene.analysis.standard;
 
@@ -33,8 +33,8 @@
 /**
  * This class is a scanner generated by 
  * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 8/6/12 1:23 PM from the specification file
- * <tt>/home/rmuir/workspace/branch_4x/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex</tt>
+ * on 9/17/12 9:14 AM from the specification file
+ * <tt>/Users/Erick/apache/4x_4326/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex</tt>
  */
 class ClassicTokenizerImpl implements StandardTokenizerInterface {
 
@@ -453,7 +453,7 @@
       }     
     }
 
-	// numRead < 0
+    // numRead < 0
     return true;
   }
 
@@ -674,44 +674,44 @@
       zzMarkedPos = zzMarkedPosL;
 
       switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
-        case 10: 
-          { return EMAIL;
+        case 1: 
+          { /* Break so we don't hit fall-through warning: */ break;/* ignore */
           }
         case 11: break;
         case 2: 
           { return ALPHANUM;
           }
         case 12: break;
+        case 3: 
+          { return CJ;
+          }
+        case 13: break;
         case 4: 
           { return HOST;
           }
-        case 13: break;
-        case 8: 
-          { return ACRONYM_DEP;
-          }
         case 14: break;
         case 5: 
           { return NUM;
           }
         case 15: break;
-        case 1: 
-          { /* Break so we don't hit fall-through warning: */ break;/* ignore */
+        case 6: 
+          { return APOSTROPHE;
           }
         case 16: break;
-        case 9: 
-          { return ACRONYM;
-          }
-        case 17: break;
         case 7: 
           { return COMPANY;
           }
+        case 17: break;
+        case 8: 
+          { return ACRONYM_DEP;
+          }
         case 18: break;
-        case 6: 
-          { return APOSTROPHE;
+        case 9: 
+          { return ACRONYM;
           }
         case 19: break;
-        case 3: 
-          { return CJ;
+        case 10: 
+          { return EMAIL;
           }
         case 20: break;
         default: 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilterFactory.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilterFactory.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilterFactory.java	(working copy)
@@ -49,8 +49,8 @@
  *   &lt;analyzer&gt;
  *     &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
  *     &lt;filter class="solr.CapitalizationFilterFactory" onlyFirstWord="true"
- *     	     keep="java solr lucene" keepIgnoreCase="false"
- *     	     okPrefix="McK McD McA"/&gt;   
+ *           keep="java solr lucene" keepIgnoreCase="false"
+ *           okPrefix="McK McD McA"/&gt;   
  *   &lt;/analyzer&gt;
  * &lt;/fieldType&gt;</pre>
  *
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/HyphenatedWordsFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/HyphenatedWordsFilter.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/HyphenatedWordsFilter.java	(working copy)
@@ -31,8 +31,8 @@
  * Example field definition in schema.xml:
  * <pre class="prettyprint">
  * &lt;fieldtype name="text" class="solr.TextField" positionIncrementGap="100"&gt;
- * 	&lt;analyzer type="index"&gt;
- * 		&lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
+ *  &lt;analyzer type="index"&gt;
+ *    &lt;tokenizer class="solr.WhitespaceTokenizerFactory"/&gt;
  *      &lt;filter class="solr.SynonymFilterFactory" synonyms="index_synonyms.txt" ignoreCase="true" expand="false"/&gt;
  *      &lt;filter class="solr.StopFilterFactory" ignoreCase="true"/&gt;
  *      &lt;filter class="solr.HyphenatedWordsFilterFactory"/&gt;
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PatternAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PatternAnalyzer.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PatternAnalyzer.java	(working copy)
@@ -366,7 +366,7 @@
     public final void end() {
       // set final offset
       final int finalOffset = correctOffset(str.length());
-    	this.offsetAtt.setOffset(finalOffset, finalOffset);
+      this.offsetAtt.setOffset(finalOffset, finalOffset);
     }
 
     @Override
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/HyphenatedWordsFilterFactory.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/HyphenatedWordsFilterFactory.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/HyphenatedWordsFilterFactory.java	(working copy)
@@ -33,7 +33,7 @@
  *
  */
 public class HyphenatedWordsFilterFactory extends TokenFilterFactory {
-	public HyphenatedWordsFilter create(TokenStream input) {
-		return new HyphenatedWordsFilter(input);
-	}
+  public HyphenatedWordsFilter create(TokenStream input) {
+    return new HyphenatedWordsFilter(input);
+  }
 }
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java	(working copy)
@@ -50,7 +50,7 @@
   /** File containing default Brazilian Portuguese stopwords. */
   public final static String DEFAULT_STOPWORD_FILE = "stopwords.txt";
   
-	/**
+  /**
    * Returns an unmodifiable instance of the default stop-words set.
    * @return an unmodifiable instance of the default stop-words set.
    */
@@ -74,19 +74,19 @@
   }
 
 
-	/**
-	 * Contains words that should be indexed but not stemmed.
-	 */
-	private CharArraySet excltable = CharArraySet.EMPTY_SET;
-	
-	/**
-	 * Builds an analyzer with the default stop words ({@link #getDefaultStopSet()}).
-	 */
-	public BrazilianAnalyzer(Version matchVersion) {
+  /**
+   * Contains words that should be indexed but not stemmed.
+   */
+  private CharArraySet excltable = CharArraySet.EMPTY_SET;
+
+  /**
+   * Builds an analyzer with the default stop words ({@link #getDefaultStopSet()}).
+   */
+  public BrazilianAnalyzer(Version matchVersion) {
     this(matchVersion, DefaultSetHolder.DEFAULT_STOP_SET);
-	}
-	
-	/**
+  }
+
+  /**
    * Builds an analyzer with the given stop words
    * 
    * @param matchVersion
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianStemmer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianStemmer.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianStemmer.java	(working copy)
@@ -25,37 +25,37 @@
 public class BrazilianStemmer {
   private static final Locale locale = new Locale("pt", "BR");
 
-	/**
-	 * Changed term
-	 */
-	private   String TERM ;
-	private   String CT ;
-	private   String R1 ;
-	private   String R2 ;
-	private   String RV ;
+  /**
+   * Changed term
+   */
+  private   String TERM ;
+  private   String CT ;
+  private   String R1 ;
+  private   String R2 ;
+  private   String RV ;
 
 
-	public BrazilianStemmer() {
-	}
+  public BrazilianStemmer() {
+  }
 
-	/**
-	 * Stems the given term to an unique <tt>discriminator</tt>.
-	 *
-	 * @param term  The term that should be stemmed.
-	 * @return      Discriminator for <tt>term</tt>
-	 */
-	protected String stem( String term ) {
+  /**
+   * Stems the given term to an unique <tt>discriminator</tt>.
+   *
+   * @param term  The term that should be stemmed.
+   * @return      Discriminator for <tt>term</tt>
+   */
+  protected String stem( String term ) {
     boolean altered = false ; // altered the term
 
     // creates CT
     createCT(term) ;
 
-		if ( !isIndexable( CT ) ) {
-			return null;
-		}
-		if ( !isStemmable( CT ) ) {
-			return CT ;
-		}
+    if ( !isIndexable( CT ) ) {
+      return null;
+    }
+    if ( !isStemmable( CT ) ) {
+      return CT ;
+    }
 
     R1 = getR1(CT) ;
     R2 = getR1(R1) ;
@@ -76,38 +76,38 @@
     step5() ;
 
     return CT ;
-	}
+  }
 
-	/**
-	 * Checks a term if it can be processed correctly.
-	 *
-	 * @return  true if, and only if, the given term consists in letters.
-	 */
-	private boolean isStemmable( String term ) {
-		for ( int c = 0; c < term.length(); c++ ) {
-			// Discard terms that contain non-letter characters.
-			if ( !Character.isLetter(term.charAt(c))) {
-				return false;
-			}
-		}
-		return true;
-	}
+  /**
+   * Checks a term if it can be processed correctly.
+   *
+   * @return  true if, and only if, the given term consists in letters.
+   */
+  private boolean isStemmable( String term ) {
+    for ( int c = 0; c < term.length(); c++ ) {
+      // Discard terms that contain non-letter characters.
+      if ( !Character.isLetter(term.charAt(c))) {
+        return false;
+      }
+    }
+    return true;
+  }
 
-	/**
-	 * Checks a term if it can be processed indexed.
-	 *
-	 * @return  true if it can be indexed
-	 */
-	private boolean isIndexable( String term ) {
-		return (term.length() < 30) && (term.length() > 2) ;
-	}
+  /**
+   * Checks a term if it can be processed indexed.
+   *
+   * @return  true if it can be indexed
+   */
+  private boolean isIndexable( String term ) {
+    return (term.length() < 30) && (term.length() > 2) ;
+  }
 
-	/**
-	 * See if string is 'a','e','i','o','u'
+  /**
+   * See if string is 'a','e','i','o','u'
    *
    * @return true if is vowel
-	 */
-	private boolean isVowel( char value ) {
+   */
+  private boolean isVowel( char value ) {
     return (value == 'a') ||
            (value == 'e') ||
            (value == 'i') ||
@@ -115,16 +115,16 @@
            (value == 'u') ;
   }
 
-	/**
-	 * Gets R1
+  /**
+   * Gets R1
    *
    * R1 - is the region after the first non-vowel following a vowel,
    *      or is the null region at the end of the word if there is
    *      no such non-vowel.
    *
    * @return null or a string representing R1
-	 */
-	private String getR1( String value ) {
+   */
+  private String getR1( String value ) {
     int     i;
     int     j;
 
@@ -159,8 +159,8 @@
     return value.substring(j+1) ;
   }
 
-	/**
-	 * Gets RV
+  /**
+   * Gets RV
    *
    * RV - IF the second letter is a consonant, RV is the region after
    *      the next following vowel,
@@ -175,8 +175,8 @@
    *      found.
    *
    * @return null or a string representing RV
-	 */
-	private String getRV( String value ) {
+   */
+  private String getRV( String value ) {
     int     i;
     int     j;
 
@@ -229,15 +229,15 @@
     return null ;
   }
 
-	/**
+  /**
    * 1) Turn to lowercase
    * 2) Remove accents
    * 3)  -> a ;  -> o
    * 4)  -> c
    *
    * @return null or a string transformed
-	 */
-	private String changeTerm( String value ) {
+   */
+  private String changeTerm( String value ) {
     int     j;
     String  r = "" ;
 
@@ -282,12 +282,12 @@
     return r ;
   }
 
-	/**
+  /**
    * Check if a string ends with a suffix
    *
    * @return true if the string ends with the specified suffix
-	 */
-	private boolean suffix( String value, String suffix ) {
+   */
+  private boolean suffix( String value, String suffix ) {
 
     // be-safe !!!
     if ((value == null) || (suffix == null)) {
@@ -301,12 +301,12 @@
     return value.substring(value.length()-suffix.length()).equals(suffix);
   }
 
-	/**
+  /**
    * Replace a string suffix by another
    *
    * @return the replaced String
-	 */
-	private String replaceSuffix( String value, String toReplace, String changeTo ) {
+   */
+  private String replaceSuffix( String value, String toReplace, String changeTo ) {
     String vvalue ;
 
     // be-safe !!!
@@ -325,12 +325,12 @@
     }
   }
 
-	/**
+  /**
    * Remove a string suffix
    *
    * @return the String without the suffix
-	 */
-	private String removeSuffix( String value, String toRemove ) {
+   */
+  private String removeSuffix( String value, String toRemove ) {
     // be-safe !!!
     if ((value == null) ||
         (toRemove == null) ||
@@ -341,12 +341,12 @@
     return value.substring(0,value.length()-toRemove.length()) ;
   }
 
-	/**
+  /**
    * See if a suffix is preceded by a String
    *
    * @return true if the suffix is preceded
-	 */
-	private boolean suffixPreceded( String value, String suffix, String preceded ) {
+   */
+  private boolean suffixPreceded( String value, String suffix, String preceded ) {
     // be-safe !!!
     if ((value == null) ||
         (suffix == null) ||
@@ -358,10 +358,10 @@
     return suffix(removeSuffix(value,suffix),preceded) ;
   }
 
-	/**
-	 * Creates CT (changed term) , substituting * '' and '' for 'a~' and 'o~'.
-	 */
-	private void createCT( String term ) {
+  /**
+   * Creates CT (changed term) , substituting * '' and '' for 'a~' and 'o~'.
+   */
+  private void createCT( String term ) {
     CT = changeTerm(term) ;
 
     if (CT.length() < 2) return ;
@@ -396,14 +396,14 @@
   }
 
 
-	/**
-	 * Standard suffix removal.
+  /**
+   * Standard suffix removal.
    * Search for the longest among the following suffixes, and perform
    * the following actions:
    *
    * @return false if no ending was removed
-	 */
-	private boolean step1() {
+   */
+  private boolean step1() {
     if (CT == null) return false ;
 
     // suffix length = 7
@@ -559,15 +559,15 @@
   }
 
 
-	/**
-	 * Verb suffixes.
+  /**
+   * Verb suffixes.
    *
    * Search for the longest among the following suffixes in RV,
    * and if found, delete.
    *
    * @return false if no ending was removed
-	*/
-	private boolean step2() {
+  */
+  private boolean step2() {
     if (RV == null) return false ;
 
     // suffix lenght = 7
@@ -941,11 +941,11 @@
     return false ;
   }
 
-	/**
-	 * Delete suffix 'i' if in RV and preceded by 'c'
+  /**
+   * Delete suffix 'i' if in RV and preceded by 'c'
    *
-	*/
-	private void step3() {
+  */
+  private void step3() {
     if (RV == null) return ;
 
     if (suffix(RV,"i") && suffixPreceded(RV,"i","c")) {
@@ -954,14 +954,14 @@
 
   }
 
-	/**
-	 * Residual suffix
+  /**
+   * Residual suffix
    *
    * If the word ends with one of the suffixes (os a i o   )
    * in RV, delete it
    *
-	*/
-	private void step4() {
+  */
+  private void step4() {
     if (RV == null) return  ;
 
     if (suffix(RV,"os")) {
@@ -979,15 +979,15 @@
 
   }
 
-	/**
-	 * If the word ends with one of ( e  ) in RV,delete it,
+  /**
+   * If the word ends with one of ( e  ) in RV,delete it,
    * and if preceded by 'gu' (or 'ci') with the 'u' (or 'i') in RV,
    * delete the 'u' (or 'i')
    *
    * Or if the word ends  remove the cedilha
    *
-	*/
-	private void step5() {
+  */
+  private void step5() {
     if (RV == null) return  ;
 
     if (suffix(RV,"e")) {
@@ -1007,18 +1007,18 @@
     }
   }
 
-	/**
-	 * For log and debug purpose
-	 *
-	 * @return  TERM, CT, RV, R1 and R2
-	 */
-	public String log() {
+  /**
+   * For log and debug purpose
+   *
+   * @return  TERM, CT, RV, R1 and R2
+   */
+  public String log() {
     return " (TERM = " + TERM + ")" +
            " (CT = " + CT +")" +
            " (RV = " + RV +")" +
            " (R1 = " + R1 +")" +
            " (R2 = " + R2 +")" ;
-	}
+  }
 
 }
 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java	(working copy)
@@ -211,6 +211,6 @@
       }
     }
     return allStopWords.toArray(new Term[allStopWords.size()]);
-	}
+  }
 
 }
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchStemmer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchStemmer.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchStemmer.java	(working copy)
@@ -45,42 +45,42 @@
    */
    private StringBuilder tb = new StringBuilder();
 
-	/**
-	 * Region R0 is equal to the whole buffer
-	 */
-	private String R0;
+  /**
+   * Region R0 is equal to the whole buffer
+   */
+  private String R0;
 
-	/**
-	 * Region RV
-	 * "If the word begins with two vowels, RV is the region after the third letter,
-	 * otherwise the region after the first vowel not at the beginning of the word,
-	 * or the end of the word if these positions cannot be found."
-	 */
+  /**
+   * Region RV
+   * "If the word begins with two vowels, RV is the region after the third letter,
+   * otherwise the region after the first vowel not at the beginning of the word,
+   * or the end of the word if these positions cannot be found."
+   */
     private String RV;
 
-	/**
-	 * Region R1
-	 * "R1 is the region after the first non-vowel following a vowel
-	 * or is the null region at the end of the word if there is no such non-vowel"
-	 */
+  /**
+   * Region R1
+   * "R1 is the region after the first non-vowel following a vowel
+   * or is the null region at the end of the word if there is no such non-vowel"
+   */
     private String R1;
 
-	/**
-	 * Region R2
-	 * "R2 is the region after the first non-vowel in R1 following a vowel
-	 * or is the null region at the end of the word if there is no such non-vowel"
-	 */
+  /**
+   * Region R2
+   * "R2 is the region after the first non-vowel in R1 following a vowel
+   * or is the null region at the end of the word if there is no such non-vowel"
+   */
     private String R2;
 
 
-	/**
-	 * Set to true if we need to perform step 2
-	 */
+  /**
+   * Set to true if we need to perform step 2
+   */
     private boolean suite;
 
-	/**
-	 * Set to true if the buffer was modified
-	 */
+  /**
+   * Set to true if the buffer was modified
+   */
     private boolean modified;
 
 
@@ -91,626 +91,626 @@
      * @return java.lang.String  Discriminator for <tt>term</tt>
      */
     protected String stem( String term ) {
-		if ( !isStemmable( term ) ) {
-			return term;
-		}
+    if ( !isStemmable( term ) ) {
+      return term;
+    }
 
-		// Use lowercase for medium stemming.
-		term = term.toLowerCase(locale);
+    // Use lowercase for medium stemming.
+    term = term.toLowerCase(locale);
 
-		// Reset the StringBuilder.
-		sb.delete( 0, sb.length() );
-		sb.insert( 0, term );
+    // Reset the StringBuilder.
+    sb.delete( 0, sb.length() );
+    sb.insert( 0, term );
 
-		// reset the booleans
-		modified = false;
-		suite = false;
+    // reset the booleans
+    modified = false;
+    suite = false;
 
-		sb = treatVowels( sb );
+    sb = treatVowels( sb );
 
-		setStrings();
+    setStrings();
 
-		step1();
+    step1();
 
-		if (!modified || suite)
-		{
-			if (RV != null)
-			{
-				suite = step2a();
-				if (!suite)
-					step2b();
-			}
-		}
+    if (!modified || suite)
+    {
+      if (RV != null)
+      {
+        suite = step2a();
+        if (!suite)
+          step2b();
+      }
+    }
 
-		if (modified || suite)
-			step3();
-		else
-			step4();
+    if (modified || suite)
+      step3();
+    else
+      step4();
 
-		step5();
+    step5();
 
-		step6();
+    step6();
 
-		return sb.toString();
+    return sb.toString();
     }
 
-	/**
-	 * Sets the search region Strings<br>
-	 * it needs to be done each time the buffer was modified
-	 */
-	private void setStrings() {
-		// set the strings
-		R0 = sb.toString();
-		RV = retrieveRV( sb );
-		R1 = retrieveR( sb );
-		if ( R1 != null )
-		{
-			tb.delete( 0, tb.length() );
-			tb.insert( 0, R1 );
-			R2 = retrieveR( tb );
-		}
-		else
-			R2 = null;
-	}
+  /**
+   * Sets the search region Strings<br>
+   * it needs to be done each time the buffer was modified
+   */
+  private void setStrings() {
+    // set the strings
+    R0 = sb.toString();
+    RV = retrieveRV( sb );
+    R1 = retrieveR( sb );
+    if ( R1 != null )
+    {
+      tb.delete( 0, tb.length() );
+      tb.insert( 0, R1 );
+      R2 = retrieveR( tb );
+    }
+    else
+      R2 = null;
+  }
 
-	/**
-	 * First step of the Porter Algorithm<br>
-	 * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
-	 */
-	private void step1( ) {
-		String[] suffix = { "ances", "iqUes", "ismes", "ables", "istes", "ance", "iqUe", "isme", "able", "iste" };
-		deleteFrom( R2, suffix );
+  /**
+   * First step of the Porter Algorithm<br>
+   * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
+   */
+  private void step1( ) {
+    String[] suffix = { "ances", "iqUes", "ismes", "ables", "istes", "ance", "iqUe", "isme", "able", "iste" };
+    deleteFrom( R2, suffix );
 
-		replaceFrom( R2, new String[] { "logies", "logie" }, "log" );
-		replaceFrom( R2, new String[] { "usions", "utions", "usion", "ution" }, "u" );
-		replaceFrom( R2, new String[] { "ences", "ence" }, "ent" );
+    replaceFrom( R2, new String[] { "logies", "logie" }, "log" );
+    replaceFrom( R2, new String[] { "usions", "utions", "usion", "ution" }, "u" );
+    replaceFrom( R2, new String[] { "ences", "ence" }, "ent" );
 
-		String[] search = { "atrices", "ateurs", "ations", "atrice", "ateur", "ation"};
-		deleteButSuffixFromElseReplace( R2, search, "ic",  true, R0, "iqU" );
+    String[] search = { "atrices", "ateurs", "ations", "atrice", "ateur", "ation"};
+    deleteButSuffixFromElseReplace( R2, search, "ic",  true, R0, "iqU" );
 
-		deleteButSuffixFromElseReplace( R2, new String[] { "ements", "ement" }, "eus", false, R0, "eux" );
-		deleteButSuffixFrom( R2, new String[] { "ements", "ement" }, "ativ", false );
-		deleteButSuffixFrom( R2, new String[] { "ements", "ement" }, "iv", false );
-		deleteButSuffixFrom( R2, new String[] { "ements", "ement" }, "abl", false );
-		deleteButSuffixFrom( R2, new String[] { "ements", "ement" }, "iqU", false );
+    deleteButSuffixFromElseReplace( R2, new String[] { "ements", "ement" }, "eus", false, R0, "eux" );
+    deleteButSuffixFrom( R2, new String[] { "ements", "ement" }, "ativ", false );
+    deleteButSuffixFrom( R2, new String[] { "ements", "ement" }, "iv", false );
+    deleteButSuffixFrom( R2, new String[] { "ements", "ement" }, "abl", false );
+    deleteButSuffixFrom( R2, new String[] { "ements", "ement" }, "iqU", false );
 
-		deleteFromIfTestVowelBeforeIn( R1, new String[] { "issements", "issement" }, false, R0 );
-		deleteFrom( RV, new String[] { "ements", "ement" } );
+    deleteFromIfTestVowelBeforeIn( R1, new String[] { "issements", "issement" }, false, R0 );
+    deleteFrom( RV, new String[] { "ements", "ement" } );
 
-		deleteButSuffixFromElseReplace( R2, new String[] { "its", "it" }, "abil", false, R0, "abl" );
-		deleteButSuffixFromElseReplace( R2, new String[] { "its", "it" }, "ic", false, R0, "iqU" );
-		deleteButSuffixFrom( R2, new String[] { "its", "it" }, "iv", true );
+    deleteButSuffixFromElseReplace( R2, new String[] { "its", "it" }, "abil", false, R0, "abl" );
+    deleteButSuffixFromElseReplace( R2, new String[] { "its", "it" }, "ic", false, R0, "iqU" );
+    deleteButSuffixFrom( R2, new String[] { "its", "it" }, "iv", true );
 
-		String[] autre = { "ifs", "ives", "if", "ive" };
-		deleteButSuffixFromElseReplace( R2, autre, "icat", false, R0, "iqU" );
-		deleteButSuffixFromElseReplace( R2, autre, "at", true, R2, "iqU" );
+    String[] autre = { "ifs", "ives", "if", "ive" };
+    deleteButSuffixFromElseReplace( R2, autre, "icat", false, R0, "iqU" );
+    deleteButSuffixFromElseReplace( R2, autre, "at", true, R2, "iqU" );
 
-		replaceFrom( R0, new String[] { "eaux" }, "eau" );
+    replaceFrom( R0, new String[] { "eaux" }, "eau" );
 
-		replaceFrom( R1, new String[] { "aux" }, "al" );
+    replaceFrom( R1, new String[] { "aux" }, "al" );
 
-		deleteButSuffixFromElseReplace( R2, new String[] { "euses", "euse" }, "", true, R1, "eux" );
+    deleteButSuffixFromElseReplace( R2, new String[] { "euses", "euse" }, "", true, R1, "eux" );
 
-		deleteFrom( R2, new String[] { "eux" } );
+    deleteFrom( R2, new String[] { "eux" } );
 
-		// if one of the next steps is performed, we will need to perform step2a
-		boolean temp = false;
-		temp = replaceFrom( RV, new String[] { "amment" }, "ant" );
-		if (temp == true)
-			suite = true;
-		temp = replaceFrom( RV, new String[] { "emment" }, "ent" );
-		if (temp == true)
-			suite = true;
-		temp = deleteFromIfTestVowelBeforeIn( RV, new String[] { "ments", "ment" }, true, RV );
-		if (temp == true)
-			suite = true;
+    // if one of the next steps is performed, we will need to perform step2a
+    boolean temp = false;
+    temp = replaceFrom( RV, new String[] { "amment" }, "ant" );
+    if (temp == true)
+      suite = true;
+    temp = replaceFrom( RV, new String[] { "emment" }, "ent" );
+    if (temp == true)
+      suite = true;
+    temp = deleteFromIfTestVowelBeforeIn( RV, new String[] { "ments", "ment" }, true, RV );
+    if (temp == true)
+      suite = true;
 
-	}
+  }
 
-	/**
-	 * Second step (A) of the Porter Algorithm<br>
-	 * Will be performed if nothing changed from the first step
-	 * or changed were done in the amment, emment, ments or ment suffixes<br>
-	 * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
-	 *
-	 * @return boolean - true if something changed in the StringBuilder
-	 */
-	private boolean step2a() {
-		String[] search = { "mes", "tes", "iraIent", "irait", "irais", "irai", "iras", "ira",
-							"irent", "iriez", "irez", "irions", "irons", "iront",
-							"issaIent", "issais", "issantes", "issante", "issants", "issant",
-							"issait", "issais", "issions", "issons", "issiez", "issez", "issent",
-							"isses", "isse", "ir", "is", "t", "it", "ies", "ie", "i" };
-		return deleteFromIfTestVowelBeforeIn( RV, search, false, RV );
-	}
+  /**
+   * Second step (A) of the Porter Algorithm<br>
+   * Will be performed if nothing changed from the first step
+   * or changed were done in the amment, emment, ments or ment suffixes<br>
+   * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
+   *
+   * @return boolean - true if something changed in the StringBuilder
+   */
+  private boolean step2a() {
+    String[] search = { "mes", "tes", "iraIent", "irait", "irais", "irai", "iras", "ira",
+              "irent", "iriez", "irez", "irions", "irons", "iront",
+              "issaIent", "issais", "issantes", "issante", "issants", "issant",
+              "issait", "issais", "issions", "issons", "issiez", "issez", "issent",
+              "isses", "isse", "ir", "is", "t", "it", "ies", "ie", "i" };
+    return deleteFromIfTestVowelBeforeIn( RV, search, false, RV );
+  }
 
-	/**
-	 * Second step (B) of the Porter Algorithm<br>
-	 * Will be performed if step 2 A was performed unsuccessfully<br>
-	 * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
-	 */
-	private void step2b() {
-		String[] suffix = { "eraIent", "erais", "erait", "erai", "eras", "erions", "eriez",
-							"erons", "eront","erez", "rent", "era", "es", "iez",
-							"e", "s", "er", "ez", "" };
-		deleteFrom( RV, suffix );
+  /**
+   * Second step (B) of the Porter Algorithm<br>
+   * Will be performed if step 2 A was performed unsuccessfully<br>
+   * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
+   */
+  private void step2b() {
+    String[] suffix = { "eraIent", "erais", "erait", "erai", "eras", "erions", "eriez",
+              "erons", "eront","erez", "rent", "era", "es", "iez",
+              "e", "s", "er", "ez", "" };
+    deleteFrom( RV, suffix );
 
-		String[] search = { "assions", "assiez", "assent", "asses", "asse", "aIent",
-							"antes", "aIent", "Aient", "ante", "mes", "tes", "ants", "ant",
-							"ait", "at", "ais", "Ait", "At", "Ais", "t", "as", "ai", "Ai", "a" };
-		deleteButSuffixFrom( RV, search, "e", true );
+    String[] search = { "assions", "assiez", "assent", "asses", "asse", "aIent",
+              "antes", "aIent", "Aient", "ante", "mes", "tes", "ants", "ant",
+              "ait", "at", "ais", "Ait", "At", "Ais", "t", "as", "ai", "Ai", "a" };
+    deleteButSuffixFrom( RV, search, "e", true );
 
-		deleteFrom( R2, new String[] { "ions" } );
-	}
+    deleteFrom( R2, new String[] { "ions" } );
+  }
 
-	/**
-	 * Third step of the Porter Algorithm<br>
-	 * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
-	 */
-	private void step3() {
-		if (sb.length()>0)
-		{
-			char ch = sb.charAt( sb.length()-1 );
-			if (ch == 'Y')
-			{
-				sb.setCharAt( sb.length()-1, 'i' );
-				setStrings();
-			}
-			else if (ch == '')
-			{
-				sb.setCharAt( sb.length()-1, 'c' );
-				setStrings();
-			}
-		}
-	}
+  /**
+   * Third step of the Porter Algorithm<br>
+   * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
+   */
+  private void step3() {
+    if (sb.length()>0)
+    {
+      char ch = sb.charAt( sb.length()-1 );
+      if (ch == 'Y')
+      {
+        sb.setCharAt( sb.length()-1, 'i' );
+        setStrings();
+      }
+      else if (ch == '')
+      {
+        sb.setCharAt( sb.length()-1, 'c' );
+        setStrings();
+      }
+    }
+  }
 
-	/**
-	 * Fourth step of the Porter Algorithm<br>
-	 * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
-	 */
-	private void step4() {
-		if (sb.length() > 1)
-		{
-			char ch = sb.charAt( sb.length()-1 );
-			if (ch == 's')
-			{
-				char b = sb.charAt( sb.length()-2 );
-				if (b != 'a' && b != 'i' && b != 'o' && b != 'u' && b != '' && b != 's')
-				{
-					sb.delete( sb.length() - 1, sb.length());
-					setStrings();
-				}
-			}
-		}
-		boolean found = deleteFromIfPrecededIn( R2, new String[] { "ion" }, RV, "s" );
-		if (!found)
-		found = deleteFromIfPrecededIn( R2, new String[] { "ion" }, RV, "t" );
+  /**
+   * Fourth step of the Porter Algorithm<br>
+   * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
+   */
+  private void step4() {
+    if (sb.length() > 1)
+    {
+      char ch = sb.charAt( sb.length()-1 );
+      if (ch == 's')
+      {
+        char b = sb.charAt( sb.length()-2 );
+        if (b != 'a' && b != 'i' && b != 'o' && b != 'u' && b != '' && b != 's')
+        {
+          sb.delete( sb.length() - 1, sb.length());
+          setStrings();
+        }
+      }
+    }
+    boolean found = deleteFromIfPrecededIn( R2, new String[] { "ion" }, RV, "s" );
+    if (!found)
+    found = deleteFromIfPrecededIn( R2, new String[] { "ion" }, RV, "t" );
 
-		replaceFrom( RV, new String[] { "Ire", "ire", "Ier", "ier" }, "i" );
-		deleteFrom( RV, new String[] { "e" } );
-		deleteFromIfPrecededIn( RV, new String[] { "" }, R0, "gu" );
-	}
+    replaceFrom( RV, new String[] { "Ire", "ire", "Ier", "ier" }, "i" );
+    deleteFrom( RV, new String[] { "e" } );
+    deleteFromIfPrecededIn( RV, new String[] { "" }, R0, "gu" );
+  }
 
-	/**
-	 * Fifth step of the Porter Algorithm<br>
-	 * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
-	 */
-	private void step5() {
-		if (R0 != null)
-		{
-			if (R0.endsWith("enn") || R0.endsWith("onn") || R0.endsWith("ett") || R0.endsWith("ell") || R0.endsWith("eill"))
-			{
-				sb.delete( sb.length() - 1, sb.length() );
-				setStrings();
-			}
-		}
-	}
+  /**
+   * Fifth step of the Porter Algorithm<br>
+   * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
+   */
+  private void step5() {
+    if (R0 != null)
+    {
+      if (R0.endsWith("enn") || R0.endsWith("onn") || R0.endsWith("ett") || R0.endsWith("ell") || R0.endsWith("eill"))
+      {
+        sb.delete( sb.length() - 1, sb.length() );
+        setStrings();
+      }
+    }
+  }
 
-	/**
-	 * Sixth (and last!) step of the Porter Algorithm<br>
-	 * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
-	 */
-	private void step6() {
-		if (R0!=null && R0.length()>0)
-		{
-			boolean seenVowel = false;
-			boolean seenConson = false;
-			int pos = -1;
-			for (int i = R0.length()-1; i > -1; i--)
-			{
-				char ch = R0.charAt(i);
-				if (isVowel(ch))
-				{
-					if (!seenVowel)
-					{
-						if (ch == '' || ch == '')
-						{
-							pos = i;
-							break;
-						}
-					}
-					seenVowel = true;
-				}
-				else
-				{
-					if (seenVowel)
-						break;
-					else
-						seenConson = true;
-				}
-			}
-			if (pos > -1 && seenConson && !seenVowel)
-				sb.setCharAt(pos, 'e');
-		}
-	}
+  /**
+   * Sixth (and last!) step of the Porter Algorithm<br>
+   * refer to http://snowball.sourceforge.net/french/stemmer.html for an explanation
+   */
+  private void step6() {
+    if (R0!=null && R0.length()>0)
+    {
+      boolean seenVowel = false;
+      boolean seenConson = false;
+      int pos = -1;
+      for (int i = R0.length()-1; i > -1; i--)
+      {
+        char ch = R0.charAt(i);
+        if (isVowel(ch))
+        {
+          if (!seenVowel)
+          {
+            if (ch == '' || ch == '')
+            {
+              pos = i;
+              break;
+            }
+          }
+          seenVowel = true;
+        }
+        else
+        {
+          if (seenVowel)
+            break;
+          else
+            seenConson = true;
+        }
+      }
+      if (pos > -1 && seenConson && !seenVowel)
+        sb.setCharAt(pos, 'e');
+    }
+  }
 
-	/**
-	 * Delete a suffix searched in zone "source" if zone "from" contains prefix + search string
-	 *
-	 * @param source java.lang.String - the primary source zone for search
-	 * @param search java.lang.String[] - the strings to search for suppression
-	 * @param from java.lang.String - the secondary source zone for search
-	 * @param prefix java.lang.String - the prefix to add to the search string to test
-	 * @return boolean - true if modified
-	 */
-	private boolean deleteFromIfPrecededIn( String source, String[] search, String from, String prefix ) {
-		boolean found = false;
-		if (source!=null )
-		{
-			for (int i = 0; i < search.length; i++) {
-				if ( source.endsWith( search[i] ))
-				{
-					if (from!=null && from.endsWith( prefix + search[i] ))
-					{
-						sb.delete( sb.length() - search[i].length(), sb.length());
-						found = true;
-						setStrings();
-						break;
-					}
-				}
-			}
-		}
-		return found;
-	}
+  /**
+   * Delete a suffix searched in zone "source" if zone "from" contains prefix + search string
+   *
+   * @param source java.lang.String - the primary source zone for search
+   * @param search java.lang.String[] - the strings to search for suppression
+   * @param from java.lang.String - the secondary source zone for search
+   * @param prefix java.lang.String - the prefix to add to the search string to test
+   * @return boolean - true if modified
+   */
+  private boolean deleteFromIfPrecededIn( String source, String[] search, String from, String prefix ) {
+    boolean found = false;
+    if (source!=null )
+    {
+      for (int i = 0; i < search.length; i++) {
+        if ( source.endsWith( search[i] ))
+        {
+          if (from!=null && from.endsWith( prefix + search[i] ))
+          {
+            sb.delete( sb.length() - search[i].length(), sb.length());
+            found = true;
+            setStrings();
+            break;
+          }
+        }
+      }
+    }
+    return found;
+  }
 
-	/**
-	 * Delete a suffix searched in zone "source" if the preceding letter is (or isn't) a vowel
-	 *
-	 * @param source java.lang.String - the primary source zone for search
-	 * @param search java.lang.String[] - the strings to search for suppression
-	 * @param vowel boolean - true if we need a vowel before the search string
-	 * @param from java.lang.String - the secondary source zone for search (where vowel could be)
-	 * @return boolean - true if modified
-	 */
-	private boolean deleteFromIfTestVowelBeforeIn( String source, String[] search, boolean vowel, String from ) {
-		boolean found = false;
-		if (source!=null && from!=null)
-		{
-			for (int i = 0; i < search.length; i++) {
-				if ( source.endsWith( search[i] ))
-				{
-					if ((search[i].length() + 1) <= from.length())
-					{
-						boolean test = isVowel(sb.charAt(sb.length()-(search[i].length()+1)));
-						if (test == vowel)
-						{
-							sb.delete( sb.length() - search[i].length(), sb.length());
-							modified = true;
-							found = true;
-							setStrings();
-							break;
-						}
-					}
-				}
-			}
-		}
-		return found;
-	}
+  /**
+   * Delete a suffix searched in zone "source" if the preceding letter is (or isn't) a vowel
+   *
+   * @param source java.lang.String - the primary source zone for search
+   * @param search java.lang.String[] - the strings to search for suppression
+   * @param vowel boolean - true if we need a vowel before the search string
+   * @param from java.lang.String - the secondary source zone for search (where vowel could be)
+   * @return boolean - true if modified
+   */
+  private boolean deleteFromIfTestVowelBeforeIn( String source, String[] search, boolean vowel, String from ) {
+    boolean found = false;
+    if (source!=null && from!=null)
+    {
+      for (int i = 0; i < search.length; i++) {
+        if ( source.endsWith( search[i] ))
+        {
+          if ((search[i].length() + 1) <= from.length())
+          {
+            boolean test = isVowel(sb.charAt(sb.length()-(search[i].length()+1)));
+            if (test == vowel)
+            {
+              sb.delete( sb.length() - search[i].length(), sb.length());
+              modified = true;
+              found = true;
+              setStrings();
+              break;
+            }
+          }
+        }
+      }
+    }
+    return found;
+  }
 
-	/**
-	 * Delete a suffix searched in zone "source" if preceded by the prefix
-	 *
-	 * @param source java.lang.String - the primary source zone for search
-	 * @param search java.lang.String[] - the strings to search for suppression
-	 * @param prefix java.lang.String - the prefix to add to the search string to test
-	 * @param without boolean - true if it will be deleted even without prefix found
-	 */
-	private void deleteButSuffixFrom( String source, String[] search, String prefix, boolean without ) {
-		if (source!=null)
-		{
-			for (int i = 0; i < search.length; i++) {
-				if ( source.endsWith( prefix + search[i] ))
-				{
-					sb.delete( sb.length() - (prefix.length() + search[i].length()), sb.length() );
-					modified = true;
-					setStrings();
-					break;
-				}
-				else if ( without && source.endsWith( search[i] ))
-				{
-					sb.delete( sb.length() - search[i].length(), sb.length() );
-					modified = true;
-					setStrings();
-					break;
-				}
-			}
-		}
-	}
+  /**
+   * Delete a suffix searched in zone "source" if preceded by the prefix
+   *
+   * @param source java.lang.String - the primary source zone for search
+   * @param search java.lang.String[] - the strings to search for suppression
+   * @param prefix java.lang.String - the prefix to add to the search string to test
+   * @param without boolean - true if it will be deleted even without prefix found
+   */
+  private void deleteButSuffixFrom( String source, String[] search, String prefix, boolean without ) {
+    if (source!=null)
+    {
+      for (int i = 0; i < search.length; i++) {
+        if ( source.endsWith( prefix + search[i] ))
+        {
+          sb.delete( sb.length() - (prefix.length() + search[i].length()), sb.length() );
+          modified = true;
+          setStrings();
+          break;
+        }
+        else if ( without && source.endsWith( search[i] ))
+        {
+          sb.delete( sb.length() - search[i].length(), sb.length() );
+          modified = true;
+          setStrings();
+          break;
+        }
+      }
+    }
+  }
 
-	/**
-	 * Delete a suffix searched in zone "source" if preceded by prefix<br>
-	 * or replace it with the replace string if preceded by the prefix in the zone "from"<br>
-	 * or delete the suffix if specified
-	 *
-	 * @param source java.lang.String - the primary source zone for search
-	 * @param search java.lang.String[] - the strings to search for suppression
-	 * @param prefix java.lang.String - the prefix to add to the search string to test
-	 * @param without boolean - true if it will be deleted even without prefix found
-	 */
-	private void deleteButSuffixFromElseReplace( String source, String[] search, String prefix, boolean without, String from, String replace ) {
-		if (source!=null)
-		{
-			for (int i = 0; i < search.length; i++) {
-				if ( source.endsWith( prefix + search[i] ))
-				{
-					sb.delete( sb.length() - (prefix.length() + search[i].length()), sb.length() );
-					modified = true;
-					setStrings();
-					break;
-				}
-				else if ( from!=null && from.endsWith( prefix + search[i] ))
-				{
-					sb.replace( sb.length() - (prefix.length() + search[i].length()), sb.length(), replace );
-					modified = true;
-					setStrings();
-					break;
-				}
-				else if ( without && source.endsWith( search[i] ))
-				{
-					sb.delete( sb.length() - search[i].length(), sb.length() );
-					modified = true;
-					setStrings();
-					break;
-				}
-			}
-		}
-	}
+  /**
+   * Delete a suffix searched in zone "source" if preceded by prefix<br>
+   * or replace it with the replace string if preceded by the prefix in the zone "from"<br>
+   * or delete the suffix if specified
+   *
+   * @param source java.lang.String - the primary source zone for search
+   * @param search java.lang.String[] - the strings to search for suppression
+   * @param prefix java.lang.String - the prefix to add to the search string to test
+   * @param without boolean - true if it will be deleted even without prefix found
+   */
+  private void deleteButSuffixFromElseReplace( String source, String[] search, String prefix, boolean without, String from, String replace ) {
+    if (source!=null)
+    {
+      for (int i = 0; i < search.length; i++) {
+        if ( source.endsWith( prefix + search[i] ))
+        {
+          sb.delete( sb.length() - (prefix.length() + search[i].length()), sb.length() );
+          modified = true;
+          setStrings();
+          break;
+        }
+        else if ( from!=null && from.endsWith( prefix + search[i] ))
+        {
+          sb.replace( sb.length() - (prefix.length() + search[i].length()), sb.length(), replace );
+          modified = true;
+          setStrings();
+          break;
+        }
+        else if ( without && source.endsWith( search[i] ))
+        {
+          sb.delete( sb.length() - search[i].length(), sb.length() );
+          modified = true;
+          setStrings();
+          break;
+        }
+      }
+    }
+  }
 
-	/**
-	 * Replace a search string with another within the source zone
-	 *
-	 * @param source java.lang.String - the source zone for search
-	 * @param search java.lang.String[] - the strings to search for replacement
-	 * @param replace java.lang.String - the replacement string
-	 */
-	private boolean replaceFrom( String source, String[] search, String replace ) {
-		boolean found = false;
-		if (source!=null)
-		{
-			for (int i = 0; i < search.length; i++) {
-				if ( source.endsWith( search[i] ))
-				{
-					sb.replace( sb.length() - search[i].length(), sb.length(), replace );
-					modified = true;
-					found = true;
-					setStrings();
-					break;
-				}
-			}
-		}
-		return found;
-	}
+  /**
+   * Replace a search string with another within the source zone
+   *
+   * @param source java.lang.String - the source zone for search
+   * @param search java.lang.String[] - the strings to search for replacement
+   * @param replace java.lang.String - the replacement string
+   */
+  private boolean replaceFrom( String source, String[] search, String replace ) {
+    boolean found = false;
+    if (source!=null)
+    {
+      for (int i = 0; i < search.length; i++) {
+        if ( source.endsWith( search[i] ))
+        {
+          sb.replace( sb.length() - search[i].length(), sb.length(), replace );
+          modified = true;
+          found = true;
+          setStrings();
+          break;
+        }
+      }
+    }
+    return found;
+  }
 
-	/**
-	 * Delete a search string within the source zone
-	 *
-	 * @param source the source zone for search
-	 * @param suffix the strings to search for suppression
-	 */
-	private void deleteFrom(String source, String[] suffix ) {
-		if (source!=null)
-		{
-			for (int i = 0; i < suffix.length; i++) {
-				if (source.endsWith( suffix[i] ))
-				{
-					sb.delete( sb.length() - suffix[i].length(), sb.length());
-					modified = true;
-					setStrings();
-					break;
-				}
-			}
-		}
-	}
+  /**
+   * Delete a search string within the source zone
+   *
+   * @param source the source zone for search
+   * @param suffix the strings to search for suppression
+   */
+  private void deleteFrom(String source, String[] suffix ) {
+    if (source!=null)
+    {
+      for (int i = 0; i < suffix.length; i++) {
+        if (source.endsWith( suffix[i] ))
+        {
+          sb.delete( sb.length() - suffix[i].length(), sb.length());
+          modified = true;
+          setStrings();
+          break;
+        }
+      }
+    }
+  }
 
-	/**
-	 * Test if a char is a french vowel, including accentuated ones
-	 *
-	 * @param ch the char to test
-	 * @return boolean - true if the char is a vowel
-	 */
-	private boolean isVowel(char ch) {
-		switch (ch)
-		{
-			case 'a':
-			case 'e':
-			case 'i':
-			case 'o':
-			case 'u':
-			case 'y':
-			case '':
-			case '':
-			case '':
-			case '':
-			case '':
-			case '':
-			case '':
-			case '':
-			case '':
-			case '':
-			case '':
-			case '':
-				return true;
-			default:
-				return false;
-		}
-	}
+  /**
+   * Test if a char is a french vowel, including accentuated ones
+   *
+   * @param ch the char to test
+   * @return boolean - true if the char is a vowel
+   */
+  private boolean isVowel(char ch) {
+    switch (ch)
+    {
+      case 'a':
+      case 'e':
+      case 'i':
+      case 'o':
+      case 'u':
+      case 'y':
+      case '':
+      case '':
+      case '':
+      case '':
+      case '':
+      case '':
+      case '':
+      case '':
+      case '':
+      case '':
+      case '':
+      case '':
+        return true;
+      default:
+        return false;
+    }
+  }
 
-	/**
-	 * Retrieve the "R zone" (1 or 2 depending on the buffer) and return the corresponding string<br>
-	 * "R is the region after the first non-vowel following a vowel
-	 * or is the null region at the end of the word if there is no such non-vowel"<br>
-	 * @param buffer java.lang.StringBuilder - the in buffer
-	 * @return java.lang.String - the resulting string
-	 */
-	private String retrieveR( StringBuilder buffer ) {
-		int len = buffer.length();
-		int pos = -1;
-		for (int c = 0; c < len; c++) {
-			if (isVowel( buffer.charAt( c )))
-			{
-				pos = c;
-				break;
-			}
-		}
-		if (pos > -1)
-		{
-			int consonne = -1;
-			for (int c = pos; c < len; c++) {
-				if (!isVowel(buffer.charAt( c )))
-				{
-					consonne = c;
-					break;
-				}
-			}
-			if (consonne > -1 && (consonne+1) < len)
-				return buffer.substring( consonne+1, len );
-			else
-				return null;
-		}
-		else
-			return null;
-	}
+  /**
+   * Retrieve the "R zone" (1 or 2 depending on the buffer) and return the corresponding string<br>
+   * "R is the region after the first non-vowel following a vowel
+   * or is the null region at the end of the word if there is no such non-vowel"<br>
+   * @param buffer java.lang.StringBuilder - the in buffer
+   * @return java.lang.String - the resulting string
+   */
+  private String retrieveR( StringBuilder buffer ) {
+    int len = buffer.length();
+    int pos = -1;
+    for (int c = 0; c < len; c++) {
+      if (isVowel( buffer.charAt( c )))
+      {
+        pos = c;
+        break;
+      }
+    }
+    if (pos > -1)
+    {
+      int consonne = -1;
+      for (int c = pos; c < len; c++) {
+        if (!isVowel(buffer.charAt( c )))
+        {
+          consonne = c;
+          break;
+        }
+      }
+      if (consonne > -1 && (consonne+1) < len)
+        return buffer.substring( consonne+1, len );
+      else
+        return null;
+    }
+    else
+      return null;
+  }
 
-	/**
-	 * Retrieve the "RV zone" from a buffer an return the corresponding string<br>
-	 * "If the word begins with two vowels, RV is the region after the third letter,
-	 * otherwise the region after the first vowel not at the beginning of the word,
-	 * or the end of the word if these positions cannot be found."<br>
-	 * @param buffer java.lang.StringBuilder - the in buffer
-	 * @return java.lang.String - the resulting string
-	 */
-	private String retrieveRV( StringBuilder buffer ) {
-		int len = buffer.length();
-		if ( buffer.length() > 3)
-		{
-			if ( isVowel(buffer.charAt( 0 )) && isVowel(buffer.charAt( 1 ))) {
-				return buffer.substring(3,len);
-			}
-			else
-			{
-				int pos = 0;
-				for (int c = 1; c < len; c++) {
-					if (isVowel( buffer.charAt( c )))
-					{
-						pos = c;
-						break;
-					}
-				}
-				if ( pos+1 < len )
-					return buffer.substring( pos+1, len );
-				else
-					return null;
-			}
-		}
-		else
-			return null;
-	}
+  /**
+   * Retrieve the "RV zone" from a buffer an return the corresponding string<br>
+   * "If the word begins with two vowels, RV is the region after the third letter,
+   * otherwise the region after the first vowel not at the beginning of the word,
+   * or the end of the word if these positions cannot be found."<br>
+   * @param buffer java.lang.StringBuilder - the in buffer
+   * @return java.lang.String - the resulting string
+   */
+  private String retrieveRV( StringBuilder buffer ) {
+    int len = buffer.length();
+    if ( buffer.length() > 3)
+    {
+      if ( isVowel(buffer.charAt( 0 )) && isVowel(buffer.charAt( 1 ))) {
+        return buffer.substring(3,len);
+      }
+      else
+      {
+        int pos = 0;
+        for (int c = 1; c < len; c++) {
+          if (isVowel( buffer.charAt( c )))
+          {
+            pos = c;
+            break;
+          }
+        }
+        if ( pos+1 < len )
+          return buffer.substring( pos+1, len );
+        else
+          return null;
+      }
+    }
+    else
+      return null;
+  }
 
 
 
     /**
-	 * Turns u and i preceded AND followed by a vowel to UpperCase<br>
-	 * Turns y preceded OR followed by a vowel to UpperCase<br>
-	 * Turns u preceded by q to UpperCase<br>
+   * Turns u and i preceded AND followed by a vowel to UpperCase<br>
+   * Turns y preceded OR followed by a vowel to UpperCase<br>
+   * Turns u preceded by q to UpperCase<br>
      *
      * @param buffer java.util.StringBuilder - the buffer to treat
      * @return java.util.StringBuilder - the treated buffer
      */
     private StringBuilder treatVowels( StringBuilder buffer ) {
-		for ( int c = 0; c < buffer.length(); c++ ) {
-			char ch = buffer.charAt( c );
+    for ( int c = 0; c < buffer.length(); c++ ) {
+      char ch = buffer.charAt( c );
 
-			if (c == 0) // first char
-			{
-				if (buffer.length()>1)
-				{
-					if (ch == 'y' && isVowel(buffer.charAt( c + 1 )))
-						buffer.setCharAt( c, 'Y' );
-				}
-			}
-			else if (c == buffer.length()-1) // last char
-			{
-				if (ch == 'u' && buffer.charAt( c - 1 ) == 'q')
-					buffer.setCharAt( c, 'U' );
-				if (ch == 'y' && isVowel(buffer.charAt( c - 1 )))
-					buffer.setCharAt( c, 'Y' );
-			}
-			else // other cases
-			{
-				if (ch == 'u')
-				{
-					if (buffer.charAt( c - 1) == 'q')
-						buffer.setCharAt( c, 'U' );
-					else if (isVowel(buffer.charAt( c - 1 )) && isVowel(buffer.charAt( c + 1 )))
-						buffer.setCharAt( c, 'U' );
-				}
-				if (ch == 'i')
-				{
-					if (isVowel(buffer.charAt( c - 1 )) && isVowel(buffer.charAt( c + 1 )))
-						buffer.setCharAt( c, 'I' );
-				}
-				if (ch == 'y')
-				{
-					if (isVowel(buffer.charAt( c - 1 )) || isVowel(buffer.charAt( c + 1 )))
-						buffer.setCharAt( c, 'Y' );
-				}
-			}
-		}
-
-		return buffer;
+      if (c == 0) // first char
+      {
+        if (buffer.length()>1)
+        {
+          if (ch == 'y' && isVowel(buffer.charAt( c + 1 )))
+            buffer.setCharAt( c, 'Y' );
+        }
+      }
+      else if (c == buffer.length()-1) // last char
+      {
+        if (ch == 'u' && buffer.charAt( c - 1 ) == 'q')
+          buffer.setCharAt( c, 'U' );
+        if (ch == 'y' && isVowel(buffer.charAt( c - 1 )))
+          buffer.setCharAt( c, 'Y' );
+      }
+      else // other cases
+      {
+        if (ch == 'u')
+        {
+          if (buffer.charAt( c - 1) == 'q')
+            buffer.setCharAt( c, 'U' );
+          else if (isVowel(buffer.charAt( c - 1 )) && isVowel(buffer.charAt( c + 1 )))
+            buffer.setCharAt( c, 'U' );
+        }
+        if (ch == 'i')
+        {
+          if (isVowel(buffer.charAt( c - 1 )) && isVowel(buffer.charAt( c + 1 )))
+            buffer.setCharAt( c, 'I' );
+        }
+        if (ch == 'y')
+        {
+          if (isVowel(buffer.charAt( c - 1 )) || isVowel(buffer.charAt( c + 1 )))
+            buffer.setCharAt( c, 'Y' );
+        }
+      }
     }
 
+    return buffer;
+   }
+
     /**
      * Checks a term if it can be processed correctly.
      *
      * @return boolean - true if, and only if, the given term consists in letters.
      */
     private boolean isStemmable( String term ) {
-		boolean upper = false;
-		int first = -1;
-		for ( int c = 0; c < term.length(); c++ ) {
-			// Discard terms that contain non-letter characters.
-			if ( !Character.isLetter( term.charAt( c ) ) ) {
-				return false;
-			}
-			// Discard terms that contain multiple uppercase letters.
-			if ( Character.isUpperCase( term.charAt( c ) ) ) {
-				if ( upper ) {
-					return false;
-				}
-			// First encountered uppercase letter, set flag and save
-			// position.
-				else {
-					first = c;
-					upper = true;
-				}
-			}
-		}
-		// Discard the term if it contains a single uppercase letter that
-		// is not starting the term.
-		if ( first > 0 ) {
-			return false;
-		}
-		return true;
+    boolean upper = false;
+    int first = -1;
+    for ( int c = 0; c < term.length(); c++ ) {
+      // Discard terms that contain non-letter characters.
+      if ( !Character.isLetter( term.charAt( c ) ) ) {
+        return false;
+      }
+      // Discard terms that contain multiple uppercase letters.
+      if ( Character.isUpperCase( term.charAt( c ) ) ) {
+        if ( upper ) {
+          return false;
+        }
+      // First encountered uppercase letter, set flag and save
+      // position.
+        else {
+          first = c;
+          upper = true;
+        }
+      }
     }
+    // Discard the term if it contains a single uppercase letter that
+    // is not starting the term.
+    if ( first > 0 ) {
+      return false;
+    }
+    return true;
+  }
 }
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchStemFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchStemFilter.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchStemFilter.java	(working copy)
@@ -45,46 +45,46 @@
 @Deprecated
 public final class FrenchStemFilter extends TokenFilter {
 
-	/**
-	 * The actual token in the input stream.
-	 */
-	private FrenchStemmer stemmer = new FrenchStemmer();
-	
-	private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+  /**
+   * The actual token in the input stream.
+   */
+  private FrenchStemmer stemmer = new FrenchStemmer();
+  
+  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
   private final KeywordAttribute keywordAttr = addAttribute(KeywordAttribute.class);
 
-	public FrenchStemFilter( TokenStream in ) {
+  public FrenchStemFilter( TokenStream in ) {
     super(in);
-	}
+  }
 
-	/**
-	 * @return  Returns true for the next token in the stream, or false at EOS
-	 */
-	@Override
-	public boolean incrementToken() throws IOException {
-	  if (input.incrementToken()) {
-	    String term = termAtt.toString();
+  /**
+   * @return  Returns true for the next token in the stream, or false at EOS
+   */
+  @Override
+  public boolean incrementToken() throws IOException {
+    if (input.incrementToken()) {
+      String term = termAtt.toString();
 
-	    // Check the exclusion table
-	    if (!keywordAttr.isKeyword()) {
-	      String s = stemmer.stem( term );
-	      // If not stemmed, don't waste the time  adjusting the token.
-	      if ((s != null) && !s.equals( term ) )
-	        termAtt.setEmpty().append(s);
-	    }
-	    return true;
-	  } else {
-	    return false;
-	  }
-	}
-	/**
-	 * Set a alternative/custom {@link FrenchStemmer} for this filter.
-	 */
-	public void setStemmer( FrenchStemmer stemmer ) {
-		if ( stemmer != null ) {
-			this.stemmer = stemmer;
-		}
-	}
+      // Check the exclusion table
+      if (!keywordAttr.isKeyword()) {
+        String s = stemmer.stem( term );
+        // If not stemmed, don't waste the time  adjusting the token.
+        if ((s != null) && !s.equals( term ) )
+          termAtt.setEmpty().append(s);
+      }
+      return true;
+    } else {
+      return false;
+    }
+  }
+  /**
+   * Set a alternative/custom {@link FrenchStemmer} for this filter.
+   */
+  public void setStemmer( FrenchStemmer stemmer ) {
+    if ( stemmer != null ) {
+      this.stemmer = stemmer;
+    }
+  }
 }
 
 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java	(working copy)
@@ -395,7 +395,7 @@
       exhausted = true;
     }
     return newTarget;
-	}
+  }
 
   /**
    * <p>Fills {@link #inputWindow} with input stream tokens, if available, 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/path/PathHierarchyTokenizerFactory.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/path/PathHierarchyTokenizerFactory.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/path/PathHierarchyTokenizerFactory.java	(working copy)
@@ -43,10 +43,10 @@
  * <pre class="prettyprint" >
  * &lt;fieldType name="descendent_path" class="solr.TextField"&gt;
  *   &lt;analyzer type="index"&gt;
- * 	   &lt;tokenizer class="solr.PathHierarchyTokenizerFactory" delimiter="/" /&gt;
+ *     &lt;tokenizer class="solr.PathHierarchyTokenizerFactory" delimiter="/" /&gt;
  *   &lt;/analyzer&gt;
  *   &lt;analyzer type="query"&gt;
- * 	   &lt;tokenizer class="solr.KeywordTokenizerFactory" /&gt;
+ *     &lt;tokenizer class="solr.KeywordTokenizerFactory" /&gt;
  *   &lt;/analyzer&gt;
  * &lt;/fieldType&gt;
  * </pre>
@@ -61,10 +61,10 @@
  * <pre class="prettyprint" >
  * &lt;fieldType name="descendent_path" class="solr.TextField"&gt;
  *   &lt;analyzer type="index"&gt;
- * 	   &lt;tokenizer class="solr.KeywordTokenizerFactory" /&gt;
+ *     &lt;tokenizer class="solr.KeywordTokenizerFactory" /&gt;
  *   &lt;/analyzer&gt;
  *   &lt;analyzer type="query"&gt;
- * 	   &lt;tokenizer class="solr.PathHierarchyTokenizerFactory" delimiter="/" /&gt;
+ *     &lt;tokenizer class="solr.PathHierarchyTokenizerFactory" delimiter="/" /&gt;
  *   &lt;/analyzer&gt;
  * &lt;/fieldType&gt;
  * </pre>
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.java	(working copy)
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 8/6/12 1:23 PM */
+/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/17/12 9:15 AM */
 
 package org.apache.lucene.analysis.wikipedia;
 
@@ -25,8 +25,8 @@
 /**
  * This class is a scanner generated by 
  * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 8/6/12 1:23 PM from the specification file
- * <tt>/home/rmuir/workspace/branch_4x/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex</tt>
+ * on 9/17/12 9:15 AM from the specification file
+ * <tt>/Users/Erick/apache/4x_4326/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex</tt>
  */
 class WikipediaTokenizerImpl {
 
@@ -37,16 +37,16 @@
   private static final int ZZ_BUFFERSIZE = 4096;
 
   /** lexical states */
+  public static final int YYINITIAL = 0;
+  public static final int CATEGORY_STATE = 2;
+  public static final int INTERNAL_LINK_STATE = 4;
+  public static final int EXTERNAL_LINK_STATE = 6;
+  public static final int TWO_SINGLE_QUOTES_STATE = 8;
   public static final int THREE_SINGLE_QUOTES_STATE = 10;
-  public static final int EXTERNAL_LINK_STATE = 6;
+  public static final int FIVE_SINGLE_QUOTES_STATE = 12;
   public static final int DOUBLE_EQUALS_STATE = 14;
-  public static final int INTERNAL_LINK_STATE = 4;
   public static final int DOUBLE_BRACE_STATE = 16;
-  public static final int CATEGORY_STATE = 2;
-  public static final int YYINITIAL = 0;
   public static final int STRING = 18;
-  public static final int FIVE_SINGLE_QUOTES_STATE = 12;
-  public static final int TWO_SINGLE_QUOTES_STATE = 8;
 
   /**
    * ZZ_LEXSTATE[l] is the state in the DFA for the lexical state l
@@ -589,7 +589,7 @@
       }     
     }
 
-	// numRead < 0
+    // numRead < 0
     return true;
   }
 
@@ -810,188 +810,188 @@
       zzMarkedPos = zzMarkedPosL;
 
       switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
-        case 44: 
-          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = CATEGORY; yybegin(CATEGORY_STATE);/* Break so we don't hit fall-through warning: */ break;
+        case 1: 
+          { numWikiTokensSeen = 0;  positionInc = 1; /* Break so we don't hit fall-through warning: */ break;
           }
         case 47: break;
-        case 37: 
-          { currentTokType = BOLD_ITALICS;  yybegin(FIVE_SINGLE_QUOTES_STATE); /* Break so we don't hit fall-through warning: */ break;
+        case 2: 
+          { positionInc = 1; return ALPHANUM;
           }
         case 48: break;
-        case 16: 
-          { currentTokType = HEADING; yybegin(DOUBLE_EQUALS_STATE); numWikiTokensSeen++; return currentTokType;
+        case 3: 
+          { positionInc = 1; return CJ;
           }
         case 49: break;
-        case 20: 
-          { numBalanced = 0; numWikiTokensSeen = 0; currentTokType = EXTERNAL_LINK;yybegin(EXTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
+        case 4: 
+          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = EXTERNAL_LINK_URL; yybegin(EXTERNAL_LINK_STATE);/* Break so we don't hit fall-through warning: */ break;
           }
         case 50: break;
-        case 40: 
-          { positionInc = 1; return ACRONYM;
-          }
-        case 51: break;
         case 5: 
           { positionInc = 1; /* Break so we don't hit fall-through warning: */ break;
           }
+        case 51: break;
+        case 6: 
+          { yybegin(CATEGORY_STATE); numWikiTokensSeen++; return currentTokType;
+          }
         case 52: break;
-        case 36: 
-          { positionInc = 1; return COMPANY;
+        case 7: 
+          { yybegin(INTERNAL_LINK_STATE); numWikiTokensSeen++; return currentTokType;
           }
         case 53: break;
-        case 10: 
-          { numLinkToks = 0; positionInc = 0; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;
+        case 8: 
+          { /* Break so we don't hit fall-through warning: */ break;/* ignore */
           }
         case 54: break;
-        case 15: 
-          { currentTokType = SUB_HEADING; numWikiTokensSeen = 0; yybegin(STRING); /* Break so we don't hit fall-through warning: */ break;
+        case 9: 
+          { if (numLinkToks == 0){positionInc = 0;} else{positionInc = 1;} numWikiTokensSeen++; currentTokType = EXTERNAL_LINK; yybegin(EXTERNAL_LINK_STATE); numLinkToks++; return currentTokType;
           }
         case 55: break;
-        case 22: 
-          { numWikiTokensSeen = 0; positionInc = 1; if (numBalanced == 0){numBalanced++;yybegin(TWO_SINGLE_QUOTES_STATE);} else{numBalanced = 0;}/* Break so we don't hit fall-through warning: */ break;
+        case 10: 
+          { numLinkToks = 0; positionInc = 0; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;
           }
         case 56: break;
-        case 35: 
-          { positionInc = 1; return NUM;
+        case 11: 
+          { currentTokType = BOLD;  yybegin(THREE_SINGLE_QUOTES_STATE); /* Break so we don't hit fall-through warning: */ break;
           }
         case 57: break;
-        case 33: 
-          { positionInc = 1; return APOSTROPHE;
+        case 12: 
+          { currentTokType = ITALICS; numWikiTokensSeen++;  yybegin(STRING); return currentTokType;/*italics*/
           }
         case 58: break;
-        case 21: 
-          { yybegin(STRING); return currentTokType;/*pipe*/
+        case 13: 
+          { currentTokType = EXTERNAL_LINK; numWikiTokensSeen = 0; yybegin(EXTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
           }
         case 59: break;
-        case 18: 
-          { /* Break so we don't hit fall-through warning: */ break;/* ignore STRING */
+        case 14: 
+          { yybegin(STRING); numWikiTokensSeen++; return currentTokType;
           }
         case 60: break;
-        case 2: 
-          { positionInc = 1; return ALPHANUM;
+        case 15: 
+          { currentTokType = SUB_HEADING; numWikiTokensSeen = 0; yybegin(STRING); /* Break so we don't hit fall-through warning: */ break;
           }
         case 61: break;
-        case 1: 
-          { numWikiTokensSeen = 0;  positionInc = 1; /* Break so we don't hit fall-through warning: */ break;
+        case 16: 
+          { currentTokType = HEADING; yybegin(DOUBLE_EQUALS_STATE); numWikiTokensSeen++; return currentTokType;
           }
         case 62: break;
         case 17: 
           { yybegin(DOUBLE_BRACE_STATE); numWikiTokensSeen = 0; return currentTokType;
           }
         case 63: break;
-        case 39: 
-          { numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end sub header*/
+        case 18: 
+          { /* Break so we don't hit fall-through warning: */ break;/* ignore STRING */
           }
         case 64: break;
-        case 29: 
-          { currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0;  yybegin(INTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
+        case 19: 
+          { yybegin(STRING); numWikiTokensSeen++; return currentTokType;/* STRING ALPHANUM*/
           }
         case 65: break;
-        case 46: 
-          { numBalanced = 0; numWikiTokensSeen = 0; currentTokType = CATEGORY;yybegin(CATEGORY_STATE); /* Break so we don't hit fall-through warning: */ break;
+        case 20: 
+          { numBalanced = 0; numWikiTokensSeen = 0; currentTokType = EXTERNAL_LINK;yybegin(EXTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
           }
         case 66: break;
-        case 27: 
-          { numLinkToks = 0; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;
+        case 21: 
+          { yybegin(STRING); return currentTokType;/*pipe*/
           }
         case 67: break;
-        case 4: 
-          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = EXTERNAL_LINK_URL; yybegin(EXTERNAL_LINK_STATE);/* Break so we don't hit fall-through warning: */ break;
+        case 22: 
+          { numWikiTokensSeen = 0; positionInc = 1; if (numBalanced == 0){numBalanced++;yybegin(TWO_SINGLE_QUOTES_STATE);} else{numBalanced = 0;}/* Break so we don't hit fall-through warning: */ break;
           }
         case 68: break;
-        case 38: 
-          { numBalanced = 0;currentTokType = ALPHANUM;yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end bold*/
+        case 23: 
+          { numWikiTokensSeen = 0; positionInc = 1; yybegin(DOUBLE_EQUALS_STATE);/* Break so we don't hit fall-through warning: */ break;
           }
         case 69: break;
-        case 13: 
-          { currentTokType = EXTERNAL_LINK; numWikiTokensSeen = 0; yybegin(EXTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
+        case 24: 
+          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = INTERNAL_LINK; yybegin(INTERNAL_LINK_STATE);/* Break so we don't hit fall-through warning: */ break;
           }
         case 70: break;
-        case 3: 
-          { positionInc = 1; return CJ;
+        case 25: 
+          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = CITATION; yybegin(DOUBLE_BRACE_STATE);/* Break so we don't hit fall-through warning: */ break;
           }
         case 71: break;
-        case 45: 
-          { currentTokType = CATEGORY; numWikiTokensSeen = 0; yybegin(CATEGORY_STATE); /* Break so we don't hit fall-through warning: */ break;
+        case 26: 
+          { yybegin(YYINITIAL);/* Break so we don't hit fall-through warning: */ break;
           }
         case 72: break;
-        case 6: 
-          { yybegin(CATEGORY_STATE); numWikiTokensSeen++; return currentTokType;
+        case 27: 
+          { numLinkToks = 0; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;
           }
         case 73: break;
-        case 11: 
-          { currentTokType = BOLD;  yybegin(THREE_SINGLE_QUOTES_STATE); /* Break so we don't hit fall-through warning: */ break;
+        case 28: 
+          { currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0; yybegin(INTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
           }
         case 74: break;
-        case 25: 
-          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = CITATION; yybegin(DOUBLE_BRACE_STATE);/* Break so we don't hit fall-through warning: */ break;
+        case 29: 
+          { currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0;  yybegin(INTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
           }
         case 75: break;
-        case 8: 
-          { /* Break so we don't hit fall-through warning: */ break;/* ignore */
+        case 30: 
+          { yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;
           }
         case 76: break;
-        case 19: 
-          { yybegin(STRING); numWikiTokensSeen++; return currentTokType;/* STRING ALPHANUM*/
+        case 31: 
+          { numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end italics*/
           }
         case 77: break;
-        case 43: 
-          { positionInc = 1; numWikiTokensSeen++; yybegin(EXTERNAL_LINK_STATE); return currentTokType;
+        case 32: 
+          { numBalanced = 0; numWikiTokensSeen = 0; currentTokType = INTERNAL_LINK;yybegin(INTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
           }
         case 78: break;
-        case 42: 
-          { numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end bold italics*/
+        case 33: 
+          { positionInc = 1; return APOSTROPHE;
           }
         case 79: break;
-        case 30: 
-          { yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;
+        case 34: 
+          { positionInc = 1; return HOST;
           }
         case 80: break;
-        case 14: 
-          { yybegin(STRING); numWikiTokensSeen++; return currentTokType;
+        case 35: 
+          { positionInc = 1; return NUM;
           }
         case 81: break;
-        case 9: 
-          { if (numLinkToks == 0){positionInc = 0;} else{positionInc = 1;} numWikiTokensSeen++; currentTokType = EXTERNAL_LINK; yybegin(EXTERNAL_LINK_STATE); numLinkToks++; return currentTokType;
+        case 36: 
+          { positionInc = 1; return COMPANY;
           }
         case 82: break;
-        case 7: 
-          { yybegin(INTERNAL_LINK_STATE); numWikiTokensSeen++; return currentTokType;
+        case 37: 
+          { currentTokType = BOLD_ITALICS;  yybegin(FIVE_SINGLE_QUOTES_STATE); /* Break so we don't hit fall-through warning: */ break;
           }
         case 83: break;
-        case 41: 
-          { positionInc = 1; return EMAIL;
+        case 38: 
+          { numBalanced = 0;currentTokType = ALPHANUM;yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end bold*/
           }
         case 84: break;
-        case 28: 
-          { currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0; yybegin(INTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
+        case 39: 
+          { numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end sub header*/
           }
         case 85: break;
-        case 23: 
-          { numWikiTokensSeen = 0; positionInc = 1; yybegin(DOUBLE_EQUALS_STATE);/* Break so we don't hit fall-through warning: */ break;
+        case 40: 
+          { positionInc = 1; return ACRONYM;
           }
         case 86: break;
-        case 34: 
-          { positionInc = 1; return HOST;
+        case 41: 
+          { positionInc = 1; return EMAIL;
           }
         case 87: break;
-        case 32: 
-          { numBalanced = 0; numWikiTokensSeen = 0; currentTokType = INTERNAL_LINK;yybegin(INTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;
+        case 42: 
+          { numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end bold italics*/
           }
         case 88: break;
-        case 12: 
-          { currentTokType = ITALICS; numWikiTokensSeen++;  yybegin(STRING); return currentTokType;/*italics*/
+        case 43: 
+          { positionInc = 1; numWikiTokensSeen++; yybegin(EXTERNAL_LINK_STATE); return currentTokType;
           }
         case 89: break;
-        case 24: 
-          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = INTERNAL_LINK; yybegin(INTERNAL_LINK_STATE);/* Break so we don't hit fall-through warning: */ break;
+        case 44: 
+          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = CATEGORY; yybegin(CATEGORY_STATE);/* Break so we don't hit fall-through warning: */ break;
           }
         case 90: break;
-        case 31: 
-          { numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end italics*/
+        case 45: 
+          { currentTokType = CATEGORY; numWikiTokensSeen = 0; yybegin(CATEGORY_STATE); /* Break so we don't hit fall-through warning: */ break;
           }
         case 91: break;
-        case 26: 
-          { yybegin(YYINITIAL);/* Break so we don't hit fall-through warning: */ break;
+        case 46: 
+          { numBalanced = 0; numWikiTokensSeen = 0; currentTokType = CATEGORY;yybegin(CATEGORY_STATE); /* Break so we don't hit fall-through warning: */ break;
           }
         case 92: break;
         default: 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex	(working copy)
@@ -136,7 +136,7 @@
 TAGS = "<"\/?{ALPHANUM}({WHITESPACE}*{ALPHANUM}=\"{ALPHANUM}\")*">"
 
 // punctuation
-P	         = ("_"|"-"|"/"|"."|",")
+P           = ("_"|"-"|"/"|"."|",")
 
 // at least one digit
 HAS_DIGIT  =
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java	(working copy)
@@ -61,25 +61,25 @@
    * 
    * @return a set of default Czech-stopwords
    */
-	public static final CharArraySet getDefaultStopSet(){
-	  return DefaultSetHolder.DEFAULT_SET;
-	}
-	
-	private static class DefaultSetHolder {
-	  private static final CharArraySet DEFAULT_SET;
-	  
-	  static {
-	    try {
-	      DEFAULT_SET = WordlistLoader.getWordSet(IOUtils.getDecodingReader(CzechAnalyzer.class, 
-	          DEFAULT_STOPWORD_FILE, IOUtils.CHARSET_UTF_8), "#", Version.LUCENE_CURRENT);
-	    } catch (IOException ex) {
-	      // default set should always be present as it is part of the
-	      // distribution (JAR)
-	      throw new RuntimeException("Unable to load default stopword set");
-	    }
-	  }
-	}
+  public static final CharArraySet getDefaultStopSet(){
+    return DefaultSetHolder.DEFAULT_SET;
+  }
 
+  private static class DefaultSetHolder {
+    private static final CharArraySet DEFAULT_SET;
+  
+    static {
+      try {
+        DEFAULT_SET = WordlistLoader.getWordSet(IOUtils.getDecodingReader(CzechAnalyzer.class, 
+            DEFAULT_STOPWORD_FILE, IOUtils.CHARSET_UTF_8), "#", Version.LUCENE_CURRENT);
+      } catch (IOException ex) {
+        // default set should always be present as it is part of the
+        // distribution (JAR)
+        throw new RuntimeException("Unable to load default stopword set");
+      }
+    }
+  }
+
  
   private final CharArraySet stemExclusionTable;
 
@@ -89,9 +89,9 @@
    * @param matchVersion Lucene version to match See
    *          {@link <a href="#version">above</a>}
    */
-	public CzechAnalyzer(Version matchVersion) {
+  public CzechAnalyzer(Version matchVersion) {
     this(matchVersion, DefaultSetHolder.DEFAULT_SET);
-	}
+  }
 
   /**
    * Builds an analyzer with the given stop words.
Index: lucene/analysis/common/src/java/org/tartarus/snowball/Among.java
===================================================================
--- lucene/analysis/common/src/java/org/tartarus/snowball/Among.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/tartarus/snowball/Among.java	(working copy)
@@ -43,25 +43,26 @@
  * reflection calls (Lovins, etc) use EMPTY_ARGS/EMPTY_PARAMS
  */
 public class Among {
-    private static final Class<?>[] EMPTY_PARAMS = new Class[0];
-    public Among (String s, int substring_i, int result,
-		  String methodname, SnowballProgram methodobject) {
-        this.s_size = s.length();
-        this.s = s.toCharArray();
-        this.substring_i = substring_i;
-	this.result = result;
-	this.methodobject = methodobject;
-	if (methodname.length() == 0) {
-	    this.method = null;
-	} else {
-	    try {
-		this.method = methodobject.getClass().
-		getDeclaredMethod(methodname, EMPTY_PARAMS);
-	    } catch (NoSuchMethodException e) {
-		throw new RuntimeException(e);
-	    }
-	}
+  private static final Class<?>[] EMPTY_PARAMS = new Class[0];
+
+  public Among(String s, int substring_i, int result,
+               String methodname, SnowballProgram methodobject) {
+    this.s_size = s.length();
+    this.s = s.toCharArray();
+    this.substring_i = substring_i;
+    this.result = result;
+    this.methodobject = methodobject;
+    if (methodname.length() == 0) {
+      this.method = null;
+    } else {
+      try {
+        this.method = methodobject.getClass().
+            getDeclaredMethod(methodname, EMPTY_PARAMS);
+      } catch (NoSuchMethodException e) {
+        throw new RuntimeException(e);
+      }
     }
+  }
 
     public final int s_size; /* search string */
     public final char[] s; /* search string */
Index: lucene/analysis/common/src/java/org/tartarus/snowball/SnowballProgram.java
===================================================================
--- lucene/analysis/common/src/java/org/tartarus/snowball/SnowballProgram.java	(revision 1386643)
+++ lucene/analysis/common/src/java/org/tartarus/snowball/SnowballProgram.java	(working copy)
@@ -51,8 +51,8 @@
 
     protected SnowballProgram()
     {
-	current = new char[8];
-	setCurrent("");
+      current = new char[8];
+      setCurrent("");
     }
 
     public abstract boolean stem();
@@ -62,12 +62,12 @@
      */
     public void setCurrent(String value)
     {
-	current = value.toCharArray();
-	cursor = 0;
-	limit = value.length();
-	limit_backward = 0;
-	bra = cursor;
-	ket = limit;
+      current = value.toCharArray();
+      cursor = 0;
+      limit = value.length();
+      limit_backward = 0;
+      bra = cursor;
+      ket = limit;
     }
 
     /**
@@ -130,354 +130,350 @@
 
     protected void copy_from(SnowballProgram other)
     {
-	current          = other.current;
-	cursor           = other.cursor;
-	limit            = other.limit;
-	limit_backward   = other.limit_backward;
-	bra              = other.bra;
-	ket              = other.ket;
+      current          = other.current;
+      cursor           = other.cursor;
+      limit            = other.limit;
+      limit_backward   = other.limit_backward;
+      bra              = other.bra;
+      ket              = other.ket;
     }
 
     protected boolean in_grouping(char [] s, int min, int max)
     {
-	if (cursor >= limit) return false;
-	char ch = current[cursor];
-	if (ch > max || ch < min) return false;
-	ch -= min;
-	if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) return false;
-	cursor++;
-	return true;
+      if (cursor >= limit) return false;
+      char ch = current[cursor];
+      if (ch > max || ch < min) return false;
+      ch -= min;
+      if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) return false;
+      cursor++;
+      return true;
     }
 
     protected boolean in_grouping_b(char [] s, int min, int max)
     {
-	if (cursor <= limit_backward) return false;
-	char ch = current[cursor - 1];
-	if (ch > max || ch < min) return false;
-	ch -= min;
-	if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) return false;
-	cursor--;
-	return true;
+      if (cursor <= limit_backward) return false;
+      char ch = current[cursor - 1];
+      if (ch > max || ch < min) return false;
+      ch -= min;
+      if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) return false;
+      cursor--;
+      return true;
     }
 
     protected boolean out_grouping(char [] s, int min, int max)
     {
-	if (cursor >= limit) return false;
-	char ch = current[cursor];
-	if (ch > max || ch < min) {
-	    cursor++;
-	    return true;
-	}
-	ch -= min;
-	if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) {
-	    cursor ++;
-	    return true;
-	}
-	return false;
+      if (cursor >= limit) return false;
+      char ch = current[cursor];
+      if (ch > max || ch < min) {
+          cursor++;
+          return true;
+      }
+      ch -= min;
+      if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) {
+          cursor ++;
+          return true;
+      }
+      return false;
     }
 
     protected boolean out_grouping_b(char [] s, int min, int max)
     {
-	if (cursor <= limit_backward) return false;
-	char ch = current[cursor - 1];
-	if (ch > max || ch < min) {
-	    cursor--;
-	    return true;
-	}
-	ch -= min;
-	if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) {
-	    cursor--;
-	    return true;
-	}
-	return false;
+      if (cursor <= limit_backward) return false;
+      char ch = current[cursor - 1];
+      if (ch > max || ch < min) {
+          cursor--;
+          return true;
+      }
+      ch -= min;
+      if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) {
+          cursor--;
+          return true;
+      }
+      return false;
     }
 
     protected boolean in_range(int min, int max)
     {
-	if (cursor >= limit) return false;
-	char ch = current[cursor];
-	if (ch > max || ch < min) return false;
-	cursor++;
-	return true;
+      if (cursor >= limit) return false;
+      char ch = current[cursor];
+      if (ch > max || ch < min) return false;
+      cursor++;
+      return true;
     }
 
     protected boolean in_range_b(int min, int max)
     {
-	if (cursor <= limit_backward) return false;
-	char ch = current[cursor - 1];
-	if (ch > max || ch < min) return false;
-	cursor--;
-	return true;
+      if (cursor <= limit_backward) return false;
+      char ch = current[cursor - 1];
+      if (ch > max || ch < min) return false;
+      cursor--;
+      return true;
     }
 
     protected boolean out_range(int min, int max)
     {
-	if (cursor >= limit) return false;
-	char ch = current[cursor];
-	if (!(ch > max || ch < min)) return false;
-	cursor++;
-	return true;
+      if (cursor >= limit) return false;
+      char ch = current[cursor];
+      if (!(ch > max || ch < min)) return false;
+      cursor++;
+      return true;
     }
 
     protected boolean out_range_b(int min, int max)
     {
-	if (cursor <= limit_backward) return false;
-	char ch = current[cursor - 1];
-	if(!(ch > max || ch < min)) return false;
-	cursor--;
-	return true;
+      if (cursor <= limit_backward) return false;
+      char ch = current[cursor - 1];
+      if(!(ch > max || ch < min)) return false;
+      cursor--;
+      return true;
     }
 
     protected boolean eq_s(int s_size, CharSequence s)
     {
-	if (limit - cursor < s_size) return false;
-	int i;
-	for (i = 0; i != s_size; i++) {
-	    if (current[cursor + i] != s.charAt(i)) return false;
-	}
-	cursor += s_size;
-	return true;
+      if (limit - cursor < s_size) return false;
+      int i;
+      for (i = 0; i != s_size; i++) {
+          if (current[cursor + i] != s.charAt(i)) return false;
+      }
+      cursor += s_size;
+      return true;
     }
 
     protected boolean eq_s_b(int s_size, CharSequence s)
     {
-	if (cursor - limit_backward < s_size) return false;
-	int i;
-	for (i = 0; i != s_size; i++) {
-	    if (current[cursor - s_size + i] != s.charAt(i)) return false;
-	}
-	cursor -= s_size;
-	return true;
+      if (cursor - limit_backward < s_size) return false;
+      int i;
+      for (i = 0; i != s_size; i++) {
+          if (current[cursor - s_size + i] != s.charAt(i)) return false;
+      }
+      cursor -= s_size;
+      return true;
     }
 
     protected boolean eq_v(CharSequence s)
     {
-	return eq_s(s.length(), s);
+      return eq_s(s.length(), s);
     }
 
     protected boolean eq_v_b(CharSequence s)
-    {   return eq_s_b(s.length(), s);
+    {
+      return eq_s_b(s.length(), s);
     }
 
     protected int find_among(Among v[], int v_size)
     {
-	int i = 0;
-	int j = v_size;
+      int i = 0;
+      int j = v_size;
 
-	int c = cursor;
-	int l = limit;
+      int c = cursor;
+      int l = limit;
 
-	int common_i = 0;
-	int common_j = 0;
+      int common_i = 0;
+      int common_j = 0;
 
-	boolean first_key_inspected = false;
+      boolean first_key_inspected = false;
 
-	while(true) {
-	    int k = i + ((j - i) >> 1);
-	    int diff = 0;
-	    int common = common_i < common_j ? common_i : common_j; // smaller
-	    Among w = v[k];
-	    int i2;
-	    for (i2 = common; i2 < w.s_size; i2++) {
-		if (c + common == l) {
-		    diff = -1;
-		    break;
-		}
-		diff = current[c + common] - w.s[i2];
-		if (diff != 0) break;
-		common++;
-	    }
-	    if (diff < 0) {
-		j = k;
-		common_j = common;
-	    } else {
-		i = k;
-		common_i = common;
-	    }
-	    if (j - i <= 1) {
-		if (i > 0) break; // v->s has been inspected
-		if (j == i) break; // only one item in v
+      while (true) {
+        int k = i + ((j - i) >> 1);
+        int diff = 0;
+        int common = common_i < common_j ? common_i : common_j; // smaller
+        Among w = v[k];
+        int i2;
+        for (i2 = common; i2 < w.s_size; i2++) {
+          if (c + common == l) {
+            diff = -1;
+            break;
+          }
+          diff = current[c + common] - w.s[i2];
+          if (diff != 0) break;
+          common++;
+        }
+        if (diff < 0) {
+          j = k;
+          common_j = common;
+        } else {
+          i = k;
+          common_i = common;
+        }
+        if (j - i <= 1) {
+          if (i > 0) break; // v->s has been inspected
+          if (j == i) break; // only one item in v
 
-		// - but now we need to go round once more to get
-		// v->s inspected. This looks messy, but is actually
-		// the optimal approach.
+          // - but now we need to go round once more to get
+          // v->s inspected. This looks messy, but is actually
+          // the optimal approach.
 
-		if (first_key_inspected) break;
-		first_key_inspected = true;
-	    }
-	}
-	while(true) {
-	    Among w = v[i];
-	    if (common_i >= w.s_size) {
-		cursor = c + w.s_size;
-		if (w.method == null) return w.result;
-		boolean res;
-		try {
-		    Object resobj = w.method.invoke(w.methodobject, EMPTY_ARGS);
-		    res = resobj.toString().equals("true");
-		} catch (InvocationTargetException e) {
-		    res = false;
-		    // FIXME - debug message
-		} catch (IllegalAccessException e) {
-		    res = false;
-		    // FIXME - debug message
-		}
-		cursor = c + w.s_size;
-		if (res) return w.result;
-	    }
-	    i = w.substring_i;
-	    if (i < 0) return 0;
-	}
+          if (first_key_inspected) break;
+          first_key_inspected = true;
+        }
+      }
+      while (true) {
+        Among w = v[i];
+        if (common_i >= w.s_size) {
+          cursor = c + w.s_size;
+          if (w.method == null) return w.result;
+          boolean res;
+          try {
+            Object resobj = w.method.invoke(w.methodobject, EMPTY_ARGS);
+            res = resobj.toString().equals("true");
+          } catch (InvocationTargetException e) {
+            res = false;
+            // FIXME - debug message
+          } catch (IllegalAccessException e) {
+            res = false;
+            // FIXME - debug message
+          }
+          cursor = c + w.s_size;
+          if (res) return w.result;
+        }
+        i = w.substring_i;
+        if (i < 0) return 0;
+      }
     }
 
-    // find_among_b is for backwards processing. Same comments apply
+  // find_among_b is for backwards processing. Same comments apply
     protected int find_among_b(Among v[], int v_size)
     {
-	int i = 0;
-	int j = v_size;
+  int i = 0;
+  int j = v_size;
 
-	int c = cursor;
-	int lb = limit_backward;
+  int c = cursor;
+  int lb = limit_backward;
 
-	int common_i = 0;
-	int common_j = 0;
+  int common_i = 0;
+  int common_j = 0;
 
-	boolean first_key_inspected = false;
+  boolean first_key_inspected = false;
 
-	while(true) {
-	    int k = i + ((j - i) >> 1);
-	    int diff = 0;
-	    int common = common_i < common_j ? common_i : common_j;
-	    Among w = v[k];
-	    int i2;
-	    for (i2 = w.s_size - 1 - common; i2 >= 0; i2--) {
-		if (c - common == lb) {
-		    diff = -1;
-		    break;
-		}
-		diff = current[c - 1 - common] - w.s[i2];
-		if (diff != 0) break;
-		common++;
-	    }
-	    if (diff < 0) {
-		j = k;
-		common_j = common;
-	    } else {
-		i = k;
-		common_i = common;
-	    }
-	    if (j - i <= 1) {
-		if (i > 0) break;
-		if (j == i) break;
-		if (first_key_inspected) break;
-		first_key_inspected = true;
-	    }
-	}
-	while(true) {
-	    Among w = v[i];
-	    if (common_i >= w.s_size) {
-		cursor = c - w.s_size;
-		if (w.method == null) return w.result;
+      while (true) {
+        int k = i + ((j - i) >> 1);
+        int diff = 0;
+        int common = common_i < common_j ? common_i : common_j;
+        Among w = v[k];
+        int i2;
+        for (i2 = w.s_size - 1 - common; i2 >= 0; i2--) {
+          if (c - common == lb) {
+            diff = -1;
+            break;
+          }
+          diff = current[c - 1 - common] - w.s[i2];
+          if (diff != 0) break;
+          common++;
+        }
+        if (diff < 0) {
+          j = k;
+          common_j = common;
+        } else {
+          i = k;
+          common_i = common;
+        }
+        if (j - i <= 1) {
+          if (i > 0) break;
+          if (j == i) break;
+          if (first_key_inspected) break;
+          first_key_inspected = true;
+        }
+      }
+      while (true) {
+        Among w = v[i];
+        if (common_i >= w.s_size) {
+          cursor = c - w.s_size;
+          if (w.method == null) return w.result;
 
-		boolean res;
-		try {
-		    Object resobj = w.method.invoke(w.methodobject, EMPTY_ARGS);
-		    res = resobj.toString().equals("true");
-		} catch (InvocationTargetException e) {
-		    res = false;
-		    // FIXME - debug message
-		} catch (IllegalAccessException e) {
-		    res = false;
-		    // FIXME - debug message
-		}
-		cursor = c - w.s_size;
-		if (res) return w.result;
-	    }
-	    i = w.substring_i;
-	    if (i < 0) return 0;
-	}
+          boolean res;
+          try {
+            Object resobj = w.method.invoke(w.methodobject, EMPTY_ARGS);
+            res = resobj.toString().equals("true");
+          } catch (InvocationTargetException e) {
+            res = false;
+            // FIXME - debug message
+          } catch (IllegalAccessException e) {
+            res = false;
+            // FIXME - debug message
+          }
+          cursor = c - w.s_size;
+          if (res) return w.result;
+        }
+        i = w.substring_i;
+        if (i < 0) return 0;
+      }
     }
 
-    /* to replace chars between c_bra and c_ket in current by the
+  /* to replace chars between c_bra and c_ket in current by the
      * chars in s.
      */
-    protected int replace_s(int c_bra, int c_ket, CharSequence s)
-    {
-	final int adjustment = s.length() - (c_ket - c_bra);
-	final int newLength = limit + adjustment;
-	//resize if necessary
-	if (newLength > current.length) {
-	  char newBuffer[] = new char[ArrayUtil.oversize(newLength, RamUsageEstimator.NUM_BYTES_CHAR)];
-	  System.arraycopy(current, 0, newBuffer, 0, limit);
-	  current = newBuffer;
-	}
-	// if the substring being replaced is longer or shorter than the
-	// replacement, need to shift things around
-	if (adjustment != 0 && c_ket < limit) {
-	  System.arraycopy(current, c_ket, current, c_bra + s.length(), 
-	      limit - c_ket);
-	}
-	// insert the replacement text
-	// Note, faster is s.getChars(0, s.length(), current, c_bra);
-	// but would have to duplicate this method for both String and StringBuilder
-	for (int i = 0; i < s.length(); i++)
-	  current[c_bra + i] = s.charAt(i);
-	
-	limit += adjustment;
-	if (cursor >= c_ket) cursor += adjustment;
-	else if (cursor > c_bra) cursor = c_bra;
-	return adjustment;
+  protected int replace_s(int c_bra, int c_ket, CharSequence s) {
+    final int adjustment = s.length() - (c_ket - c_bra);
+    final int newLength = limit + adjustment;
+    //resize if necessary
+    if (newLength > current.length) {
+      char newBuffer[] = new char[ArrayUtil.oversize(newLength, RamUsageEstimator.NUM_BYTES_CHAR)];
+      System.arraycopy(current, 0, newBuffer, 0, limit);
+      current = newBuffer;
     }
-
-    protected void slice_check()
-    {
-	if (bra < 0 ||
-	    bra > ket ||
-	    ket > limit)
-	{
-	    throw new IllegalArgumentException("faulty slice operation: bra=" + bra + ",ket=" + ket + ",limit=" + limit);
-	// FIXME: report error somehow.
-	/*
-	    fprintf(stderr, "faulty slice operation:\n");
-	    debug(z, -1, 0);
-	    exit(1);
-	    */
-	}
+    // if the substring being replaced is longer or shorter than the
+    // replacement, need to shift things around
+    if (adjustment != 0 && c_ket < limit) {
+      System.arraycopy(current, c_ket, current, c_bra + s.length(),
+          limit - c_ket);
     }
+    // insert the replacement text
+    // Note, faster is s.getChars(0, s.length(), current, c_bra);
+    // but would have to duplicate this method for both String and StringBuilder
+    for (int i = 0; i < s.length(); i++)
+      current[c_bra + i] = s.charAt(i);
 
-    protected void slice_from(CharSequence s)
-    {
-	slice_check();
-	replace_s(bra, ket, s);
+    limit += adjustment;
+    if (cursor >= c_ket) cursor += adjustment;
+    else if (cursor > c_bra) cursor = c_bra;
+    return adjustment;
+  }
+
+  protected void slice_check() {
+    if (bra < 0 ||
+        bra > ket ||
+        ket > limit) {
+      throw new IllegalArgumentException("faulty slice operation: bra=" + bra + ",ket=" + ket + ",limit=" + limit);
+      // FIXME: report error somehow.
+      /*
+      fprintf(stderr, "faulty slice operation:\n");
+      debug(z, -1, 0);
+      exit(1);
+      */
     }
- 
-    protected void slice_del()
-    {
-	slice_from((CharSequence)"");
-    }
+  }
 
-    protected void insert(int c_bra, int c_ket, CharSequence s)
+  protected void slice_from(CharSequence s) {
+    slice_check();
+    replace_s(bra, ket, s);
+  }
+
+  protected void slice_del() {
+    slice_from((CharSequence) "");
+  }
+
+  protected void insert(int c_bra, int c_ket, CharSequence s)
     {
-	int adjustment = replace_s(c_bra, c_ket, s);
-	if (c_bra <= bra) bra += adjustment;
-	if (c_bra <= ket) ket += adjustment;
+      int adjustment = replace_s(c_bra, c_ket, s);
+      if (c_bra <= bra) bra += adjustment;
+      if (c_bra <= ket) ket += adjustment;
     }
 
     /* Copy the slice into the supplied StringBuffer */
     protected StringBuilder slice_to(StringBuilder s)
     {
-	slice_check();
-	int len = ket - bra;
-	s.setLength(0);
-	s.append(current, bra, len);
-	return s;
+      slice_check();
+      int len = ket - bra;
+      s.setLength(0);
+      s.append(current, bra, len);
+      return s;
     }
 
     protected StringBuilder assign_to(StringBuilder s)
     {
-	s.setLength(0);
-	s.append(current, 0, limit);
-	return s;
+      s.setLength(0);
+      s.append(current, 0, limit);
+      return s;
     }
 
 /*
Index: lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestPhoneticFilterFactory.java
===================================================================
--- lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestPhoneticFilterFactory.java	(revision 1386643)
+++ lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestPhoneticFilterFactory.java	(working copy)
@@ -175,23 +175,23 @@
   }
   
   public void testSpeed() throws Exception {
-	  checkSpeedEncoding("Metaphone", "easgasg", "ESKS");
-	  checkSpeedEncoding("DoubleMetaphone", "easgasg", "ASKS");
-	  checkSpeedEncoding("Soundex", "easgasg", "E220");
-	  checkSpeedEncoding("RefinedSoundex", "easgasg", "E034034");
-	  checkSpeedEncoding("Caverphone", "Carlene", "KLN1111111");
-	  checkSpeedEncoding("ColognePhonetic", "Schmitt", "862");
+    checkSpeedEncoding("Metaphone", "easgasg", "ESKS");
+    checkSpeedEncoding("DoubleMetaphone", "easgasg", "ASKS");
+    checkSpeedEncoding("Soundex", "easgasg", "E220");
+    checkSpeedEncoding("RefinedSoundex", "easgasg", "E034034");
+    checkSpeedEncoding("Caverphone", "Carlene", "KLN1111111");
+    checkSpeedEncoding("ColognePhonetic", "Schmitt", "862");
   }
   
   private void checkSpeedEncoding(String encoder, String toBeEncoded, String estimated) throws Exception {
-	  long start = System.currentTimeMillis();
-	  for ( int i=0; i<REPEATS; i++) {
-		    assertAlgorithm(encoder, "false", toBeEncoded,
-		            new String[] { estimated });
-	  }
-	  long duration = System.currentTimeMillis()-start;
-	  if (VERBOSE)
-	    System.out.println(encoder + " encodings per msec: "+(REPEATS/duration));
+    long start = System.currentTimeMillis();
+    for ( int i=0; i<REPEATS; i++) {
+        assertAlgorithm(encoder, "false", toBeEncoded,
+                new String[] { estimated });
+    }
+    long duration = System.currentTimeMillis()-start;
+    if (VERBOSE)
+      System.out.println(encoder + " encodings per msec: "+(REPEATS/duration));
   }
   
 }
Index: lucene/highlighter/src/java/org/apache/lucene/search/highlight/InvalidTokenOffsetsException.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/InvalidTokenOffsetsException.java	(revision 1386643)
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/InvalidTokenOffsetsException.java	(working copy)
@@ -23,9 +23,9 @@
 public class InvalidTokenOffsetsException extends Exception
 {
 
-	public InvalidTokenOffsetsException(String message)
-	{
-		super(message);
-	}
+  public InvalidTokenOffsetsException(String message)
+  {
+    super(message);
+  }
 
 }
Index: lucene/highlighter/src/java/org/apache/lucene/search/highlight/SimpleHTMLFormatter.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/SimpleHTMLFormatter.java	(revision 1386643)
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/SimpleHTMLFormatter.java	(working copy)
@@ -26,34 +26,34 @@
   private static final String DEFAULT_PRE_TAG = "<B>";
   private static final String DEFAULT_POST_TAG = "</B>";
   
-	private String preTag;
-	private String postTag;
-	
-	public SimpleHTMLFormatter(String preTag, String postTag) {
-		this.preTag = preTag;
-		this.postTag = postTag;
-	}
+  private String preTag;
+  private String postTag;
 
-	/** Default constructor uses HTML: &lt;B&gt; tags to markup terms. */
-	public SimpleHTMLFormatter() {
-	  this(DEFAULT_PRE_TAG, DEFAULT_POST_TAG);
-	}
+  public SimpleHTMLFormatter(String preTag, String postTag) {
+    this.preTag = preTag;
+    this.postTag = postTag;
+  }
 
-	/* (non-Javadoc)
-	 * @see org.apache.lucene.search.highlight.Formatter#highlightTerm(java.lang.String, org.apache.lucene.search.highlight.TokenGroup)
-	 */
-	public String highlightTerm(String originalText, TokenGroup tokenGroup) {
-	  if (tokenGroup.getTotalScore() <= 0) {
-	    return originalText;
-	  }
-	  
-	  // Allocate StringBuilder with the right number of characters from the
+  /** Default constructor uses HTML: &lt;B&gt; tags to markup terms. */
+  public SimpleHTMLFormatter() {
+    this(DEFAULT_PRE_TAG, DEFAULT_POST_TAG);
+  }
+
+  /* (non-Javadoc)
+   * @see org.apache.lucene.search.highlight.Formatter#highlightTerm(java.lang.String, org.apache.lucene.search.highlight.TokenGroup)
+   */
+  public String highlightTerm(String originalText, TokenGroup tokenGroup) {
+    if (tokenGroup.getTotalScore() <= 0) {
+      return originalText;
+    }
+
+    // Allocate StringBuilder with the right number of characters from the
     // beginning, to avoid char[] allocations in the middle of appends.
-	  StringBuilder returnBuffer = new StringBuilder(preTag.length() + originalText.length() + postTag.length());
-	  returnBuffer.append(preTag);
-	  returnBuffer.append(originalText);
-	  returnBuffer.append(postTag);
-	  return returnBuffer.toString();
-	}
-	
+    StringBuilder returnBuffer = new StringBuilder(preTag.length() + originalText.length() + postTag.length());
+    returnBuffer.append(preTag);
+    returnBuffer.append(originalText);
+    returnBuffer.append(postTag);
+    return returnBuffer.toString();
+  }
+
 }
Index: lucene/highlighter/src/java/org/apache/lucene/search/highlight/Formatter.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/Formatter.java	(revision 1386643)
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/Formatter.java	(working copy)
@@ -24,10 +24,10 @@
  */
 public interface Formatter
 {
-	/**
-	 * @param originalText The section of text being considered for markup
-	 * @param tokenGroup contains one or several overlapping Tokens along with
-	 * their scores and positions.
-	 */
-	String highlightTerm(String originalText, TokenGroup tokenGroup);
+  /**
+   * @param originalText The section of text being considered for markup
+   * @param tokenGroup contains one or several overlapping Tokens along with
+   * their scores and positions.
+   */
+  String highlightTerm(String originalText, TokenGroup tokenGroup);
 }
Index: lucene/highlighter/src/java/org/apache/lucene/search/highlight/SpanGradientFormatter.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/SpanGradientFormatter.java	(revision 1386643)
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/SpanGradientFormatter.java	(working copy)
@@ -22,57 +22,49 @@
  * doesn't work in Mozilla, thus this class.
  *
  * @see GradientFormatter
- *
  */
 
 public class SpanGradientFormatter
-	extends GradientFormatter
-{
-	public SpanGradientFormatter(float maxScore, String minForegroundColor,
-            String maxForegroundColor, String minBackgroundColor,
-            String maxBackgroundColor)
-    {
-		super( maxScore, minForegroundColor,
-			   maxForegroundColor, minBackgroundColor,
-			   maxBackgroundColor);
-	}
-	
+    extends GradientFormatter {
+  public SpanGradientFormatter(float maxScore, String minForegroundColor,
+                               String maxForegroundColor, String minBackgroundColor,
+                               String maxBackgroundColor) {
+    super(maxScore, minForegroundColor,
+        maxForegroundColor, minBackgroundColor,
+        maxBackgroundColor);
+  }
 
-	
-	@Override
-	public String highlightTerm(String originalText, TokenGroup tokenGroup)
-    {
-        if (tokenGroup.getTotalScore() == 0)
-            return originalText;
-        float score = tokenGroup.getTotalScore();
-        if (score == 0)
-        {
-            return originalText;
-        }
 
-		// try to size sb correctly
-        StringBuilder sb = new StringBuilder( originalText.length() + EXTRA);
-		
-		sb.append("<span style=\""); 
-		if (highlightForeground) 
-		{
-			sb.append("color: "); 
-			sb.append(getForegroundColorString(score)); 
-			sb.append("; "); 
-		}
-		if (highlightBackground)
-		{
-			sb.append("background: ");
-			sb.append(getBackgroundColorString(score));
-			sb.append("; ");
-		}
-		sb.append("\">");
-		sb.append(originalText);
-		sb.append("</span>");
-        return sb.toString();
+  @Override
+  public String highlightTerm(String originalText, TokenGroup tokenGroup) {
+    if (tokenGroup.getTotalScore() == 0)
+      return originalText;
+    float score = tokenGroup.getTotalScore();
+    if (score == 0) {
+      return originalText;
     }
 
-	// guess how much extra text we'll add to the text we're highlighting to try to avoid a  StringBuilder resize
-	private static final String TEMPLATE = "<span style=\"background: #EEEEEE; color: #000000;\">...</span>";
-	private static final int EXTRA = TEMPLATE.length();	
+    // try to size sb correctly
+    StringBuilder sb = new StringBuilder(originalText.length() + EXTRA);
+
+    sb.append("<span style=\"");
+    if (highlightForeground) {
+      sb.append("color: ");
+      sb.append(getForegroundColorString(score));
+      sb.append("; ");
+    }
+    if (highlightBackground) {
+      sb.append("background: ");
+      sb.append(getBackgroundColorString(score));
+      sb.append("; ");
+    }
+    sb.append("\">");
+    sb.append(originalText);
+    sb.append("</span>");
+    return sb.toString();
+  }
+
+  // guess how much extra text we'll add to the text we're highlighting to try to avoid a  StringBuilder resize
+  private static final String TEMPLATE = "<span style=\"background: #EEEEEE; color: #000000;\">...</span>";
+  private static final int EXTRA = TEMPLATE.length();
 }
Index: lucene/highlighter/src/java/org/apache/lucene/search/highlight/TextFragment.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/TextFragment.java	(revision 1386643)
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/TextFragment.java	(working copy)
@@ -25,57 +25,57 @@
  */
 public class TextFragment
 {
-	CharSequence markedUpText;
-	int fragNum;
-	int textStartPos;
-	int textEndPos;
-	float score;
+  CharSequence markedUpText;
+  int fragNum;
+  int textStartPos;
+  int textEndPos;
+  float score;
 
-	public TextFragment(CharSequence markedUpText,int textStartPos, int fragNum)
-	{
-		this.markedUpText=markedUpText;
-		this.textStartPos = textStartPos;
-		this.fragNum = fragNum;
-	}
+  public TextFragment(CharSequence markedUpText,int textStartPos, int fragNum)
+  {
+    this.markedUpText=markedUpText;
+    this.textStartPos = textStartPos;
+    this.fragNum = fragNum;
+  }
 
-	void setScore(float score)
-	{
-		this.score=score;
-	}
-	public float getScore()
-	{
-		return score;
-	}
-	/**
-	 * @param frag2 Fragment to be merged into this one
-	 */
+  void setScore(float score)
+  {
+    this.score=score;
+  }
+  public float getScore()
+  {
+    return score;
+  }
+  /**
+   * @param frag2 Fragment to be merged into this one
+   */
   public void merge(TextFragment frag2)
   {
     textEndPos = frag2.textEndPos;
     score=Math.max(score,frag2.score);
   }
   /**
-	 * @param fragment 
-	 * @return true if this fragment follows the one passed
-	 */
-	public boolean follows(TextFragment fragment)
-	{
-		return textStartPos == fragment.textEndPos;
-	}
+   * @param fragment
+   * @return true if this fragment follows the one passed
+   */
+  public boolean follows(TextFragment fragment)
+  {
+    return textStartPos == fragment.textEndPos;
+  }
 
-	/**
-	 * @return the fragment sequence number
-	 */
-	public int getFragNum()
-	{
-		return fragNum;
-	}
+  /**
+   * @return the fragment sequence number
+   */
+  public int getFragNum()
+  {
+    return fragNum;
+  }
 
-	/* Returns the marked-up text for this text fragment 
-	 */
-	@Override
-	public String toString() {
-		return markedUpText.subSequence(textStartPos, textEndPos).toString();
-	}
+  /* Returns the marked-up text for this text fragment
+   */
+  @Override
+  public String toString() {
+    return markedUpText.subSequence(textStartPos, textEndPos).toString();
+  }
 
 }
Index: lucene/highlighter/src/java/org/apache/lucene/search/highlight/SimpleHTMLEncoder.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/SimpleHTMLEncoder.java	(revision 1386643)
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/SimpleHTMLEncoder.java	(working copy)
@@ -21,61 +21,61 @@
  */
 public class SimpleHTMLEncoder implements Encoder
 {
-	public SimpleHTMLEncoder()
-	{
-	}
+  public SimpleHTMLEncoder()
+  {
+  }
 
-	public String encodeText(String originalText)
-	{
-		return htmlEncode(originalText);
-	}
-	
-	/**
-	 * Encode string into HTML
-	 */
-	public final static String htmlEncode(String plainText) 
-	{
-		if (plainText == null || plainText.length() == 0)
-		{
-			return "";
-		}
+  public String encodeText(String originalText)
+  {
+    return htmlEncode(originalText);
+  }
 
-		StringBuilder result = new StringBuilder(plainText.length());
+  /**
+   * Encode string into HTML
+   */
+  public final static String htmlEncode(String plainText)
+  {
+    if (plainText == null || plainText.length() == 0)
+    {
+      return "";
+    }
 
-		for (int index=0; index<plainText.length(); index++) 
-		{
-			char ch = plainText.charAt(index);
+    StringBuilder result = new StringBuilder(plainText.length());
 
-			switch (ch) 
-			{
-			case '"':
-				result.append("&quot;");
-				break;
+    for (int index=0; index<plainText.length(); index++)
+    {
+      char ch = plainText.charAt(index);
 
-			case '&':
-				result.append("&amp;");
-				break;
+      switch (ch)
+      {
+      case '"':
+        result.append("&quot;");
+        break;
 
-			case '<':
-				result.append("&lt;");
-				break;
+      case '&':
+        result.append("&amp;");
+        break;
 
-			case '>':
-				result.append("&gt;");
-				break;
+      case '<':
+        result.append("&lt;");
+        break;
 
-			default:
-				   if (ch < 128) 
-				   {
-			           result.append(ch);
-			       } 
-				   else 
-			       {
-			           result.append("&#").append((int)ch).append(";");
-			       }
-			}
-		}
+      case '>':
+        result.append("&gt;");
+        break;
 
-		return result.toString();
-	}
+      default:
+           if (ch < 128)
+           {
+                 result.append(ch);
+             }
+           else
+             {
+                 result.append("&#").append((int)ch).append(";");
+             }
+      }
+    }
+
+    return result.toString();
+  }
 }
\ No newline at end of file
Index: lucene/highlighter/src/java/org/apache/lucene/search/highlight/Encoder.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/Encoder.java	(revision 1386643)
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/Encoder.java	(working copy)
@@ -22,8 +22,8 @@
  */
 public interface Encoder
 {
-	/**
-	 * @param originalText The section of text being output
-	 */
-	String encodeText(String originalText);
+  /**
+   * @param originalText The section of text being output
+   */
+  String encodeText(String originalText);
 }
\ No newline at end of file
Index: lucene/highlighter/src/java/org/apache/lucene/search/highlight/GradientFormatter.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/GradientFormatter.java	(revision 1386643)
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/GradientFormatter.java	(working copy)
@@ -42,7 +42,7 @@
      * 
      * @param maxScore
      *            The score (and above) displayed as maxColor (See QueryScorer.getMaxWeight 
-     * 			  which can be used to calibrate scoring scale)
+     *         which can be used to calibrate scoring scale)
      * @param minForegroundColor
      *            The hex color used for representing IDF scores of zero eg
      *            #FFFFFF (white) or null if no foreground color required
Index: lucene/highlighter/src/java/org/apache/lucene/search/highlight/DefaultEncoder.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/DefaultEncoder.java	(revision 1386643)
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/DefaultEncoder.java	(working copy)
@@ -21,12 +21,12 @@
  */
 public class DefaultEncoder implements Encoder
 {
-	public DefaultEncoder()
-	{
-	}
+  public DefaultEncoder()
+  {
+  }
 
-	public String encodeText(String originalText)
-	{
-		return originalText;
-	}
+  public String encodeText(String originalText)
+  {
+    return originalText;
+  }
 }
\ No newline at end of file
Index: lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedTerm.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedTerm.java	(revision 1386643)
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedTerm.java	(working copy)
@@ -20,45 +20,45 @@
  */
 public class WeightedTerm
 {
-	float weight; // multiplier
-	String term; //stemmed form
-	public WeightedTerm (float weight,String term)
-	{
-		this.weight=weight;
-		this.term=term;
-	}
-	
-	
-	/**
-	 * @return the term value (stemmed)
-	 */
-	public String getTerm()
-	{
-		return term;
-	}
+  float weight; // multiplier
+  String term; //stemmed form
+  public WeightedTerm (float weight,String term)
+  {
+    this.weight=weight;
+    this.term=term;
+  }
 
-	/**
-	 * @return the weight associated with this term
-	 */
-	public float getWeight()
-	{
-		return weight;
-	}
 
-	/**
-	 * @param term the term value (stemmed)
-	 */
-	public void setTerm(String term)
-	{
-		this.term = term;
-	}
+  /**
+   * @return the term value (stemmed)
+   */
+  public String getTerm()
+  {
+    return term;
+  }
 
-	/**
-	 * @param weight the weight associated with this term
-	 */
-	public void setWeight(float weight)
-	{
-		this.weight = weight;
-	}
+  /**
+   * @return the weight associated with this term
+   */
+  public float getWeight()
+  {
+    return weight;
+  }
 
+  /**
+   * @param term the term value (stemmed)
+   */
+  public void setTerm(String term)
+  {
+    this.term = term;
+  }
+
+  /**
+   * @param weight the weight associated with this term
+   */
+  public void setWeight(float weight)
+  {
+    this.weight = weight;
+  }
+
 }
Index: lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryTermExtractor.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryTermExtractor.java	(revision 1386643)
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/QueryTermExtractor.java	(working copy)
@@ -37,126 +37,118 @@
 public final class QueryTermExtractor
 {
 
-	/**
-	 * Extracts all terms texts of a given Query into an array of WeightedTerms
-	 *
-	 * @param query      Query to extract term texts from
-	 * @return an array of the terms used in a query, plus their weights.
-	 */
-	public static final WeightedTerm[] getTerms(Query query) 
-	{
-		return getTerms(query,false);
-	}
+  /**
+   * Extracts all terms texts of a given Query into an array of WeightedTerms
+   *
+   * @param query      Query to extract term texts from
+   * @return an array of the terms used in a query, plus their weights.
+   */
+  public static final WeightedTerm[] getTerms(Query query)
+  {
+    return getTerms(query,false);
+  }
 
-	/**
-	 * Extracts all terms texts of a given Query into an array of WeightedTerms
-	 *
-	 * @param query      Query to extract term texts from
-	 * @param reader used to compute IDF which can be used to a) score selected fragments better 
-	 * b) use graded highlights eg changing intensity of font color
-	 * @param fieldName the field on which Inverse Document Frequency (IDF) calculations are based
-	 * @return an array of the terms used in a query, plus their weights.
-	 */
-	public static final WeightedTerm[] getIdfWeightedTerms(Query query, IndexReader reader, String fieldName) 
-	{
-	    WeightedTerm[] terms=getTerms(query,false, fieldName);
-	    int totalNumDocs=reader.maxDoc();
-	    for (int i = 0; i < terms.length; i++)
+  /**
+   * Extracts all terms texts of a given Query into an array of WeightedTerms
+   *
+   * @param query      Query to extract term texts from
+   * @param reader used to compute IDF which can be used to a) score selected fragments better
+   * b) use graded highlights eg changing intensity of font color
+   * @param fieldName the field on which Inverse Document Frequency (IDF) calculations are based
+   * @return an array of the terms used in a query, plus their weights.
+   */
+  public static final WeightedTerm[] getIdfWeightedTerms(Query query, IndexReader reader, String fieldName)
+  {
+      WeightedTerm[] terms=getTerms(query,false, fieldName);
+      int totalNumDocs=reader.maxDoc();
+      for (int i = 0; i < terms.length; i++)
         {
-	        try
+          try
             {
                 int docFreq=reader.docFreq(new Term(fieldName,terms[i].term));
                 //IDF algorithm taken from DefaultSimilarity class
                 float idf=(float)(Math.log(totalNumDocs/(double)(docFreq+1)) + 1.0);
                 terms[i].weight*=idf;
             } 
-	        catch (IOException e)
+          catch (IOException e)
             {
-	            //ignore 
+              //ignore
             }
         }
-		return terms;
-	}
+    return terms;
+  }
 
-	/**
-	 * Extracts all terms texts of a given Query into an array of WeightedTerms
-	 *
-	 * @param query      Query to extract term texts from
-	 * @param prohibited <code>true</code> to extract "prohibited" terms, too
-	 * @param fieldName  The fieldName used to filter query terms
+  /**
+   * Extracts all terms texts of a given Query into an array of WeightedTerms
+   *
+   * @param query      Query to extract term texts from
+   * @param prohibited <code>true</code> to extract "prohibited" terms, too
+   * @param fieldName  The fieldName used to filter query terms
    * @return an array of the terms used in a query, plus their weights.
    */
-	public static final WeightedTerm[] getTerms(Query query, boolean prohibited, String fieldName) 
-	{
-		HashSet<WeightedTerm> terms=new HashSet<WeightedTerm>();
-		getTerms(query,terms,prohibited,fieldName);
-		return terms.toArray(new WeightedTerm[0]);
-	}
-	
-	/**
-	 * Extracts all terms texts of a given Query into an array of WeightedTerms
-	 *
-	 * @param query      Query to extract term texts from
-	 * @param prohibited <code>true</code> to extract "prohibited" terms, too
+  public static final WeightedTerm[] getTerms(Query query, boolean prohibited, String fieldName)
+  {
+    HashSet<WeightedTerm> terms=new HashSet<WeightedTerm>();
+    getTerms(query,terms,prohibited,fieldName);
+    return terms.toArray(new WeightedTerm[0]);
+  }
+
+  /**
+   * Extracts all terms texts of a given Query into an array of WeightedTerms
+   *
+   * @param query      Query to extract term texts from
+   * @param prohibited <code>true</code> to extract "prohibited" terms, too
    * @return an array of the terms used in a query, plus their weights.
    */
-	public static final WeightedTerm[] getTerms(Query query, boolean prohibited) 
-	{
-	    return getTerms(query,prohibited,null);
-	}	
+  public static final WeightedTerm[] getTerms(Query query, boolean prohibited)
+  {
+      return getTerms(query,prohibited,null);
+  }
 
-	private static final void getTerms(Query query, HashSet<WeightedTerm> terms,boolean prohibited, String fieldName) 
-	{
-       	try
-       	{
-    		if (query instanceof BooleanQuery)
-    			getTermsFromBooleanQuery((BooleanQuery) query, terms, prohibited, fieldName);
-    		else
-    			if(query instanceof FilteredQuery)
-    				getTermsFromFilteredQuery((FilteredQuery)query, terms,prohibited, fieldName);
-    			else
-    		{
-	       		HashSet<Term> nonWeightedTerms=new HashSet<Term>();
-	       		query.extractTerms(nonWeightedTerms);
-	       		for (Iterator<Term> iter = nonWeightedTerms.iterator(); iter.hasNext();)
-				{
-					Term term = iter.next();
-                                        if((fieldName==null)||(term.field().equals(fieldName)))
-					{
-						terms.add(new WeightedTerm(query.getBoost(),term.text()));
-					}
-				}
-    		}
-	      }
-	      catch(UnsupportedOperationException ignore)
-	      {
-	    	  //this is non-fatal for our purposes
-       	  }		        			        	
-	}
+  private static final void getTerms(Query query, HashSet<WeightedTerm> terms, boolean prohibited, String fieldName) {
+    try {
+      if (query instanceof BooleanQuery)
+        getTermsFromBooleanQuery((BooleanQuery) query, terms, prohibited, fieldName);
+      else if (query instanceof FilteredQuery)
+        getTermsFromFilteredQuery((FilteredQuery) query, terms, prohibited, fieldName);
+      else {
+        HashSet<Term> nonWeightedTerms = new HashSet<Term>();
+        query.extractTerms(nonWeightedTerms);
+        for (Iterator<Term> iter = nonWeightedTerms.iterator(); iter.hasNext(); ) {
+          Term term = iter.next();
+          if ((fieldName == null) || (term.field().equals(fieldName))) {
+            terms.add(new WeightedTerm(query.getBoost(), term.text()));
+          }
+        }
+      }
+    } catch (UnsupportedOperationException ignore) {
+      //this is non-fatal for our purposes
+    }
+  }
 
-	/**
-	 * extractTerms is currently the only query-independent means of introspecting queries but it only reveals
-	 * a list of terms for that query - not the boosts each individual term in that query may or may not have.
-	 * "Container" queries such as BooleanQuery should be unwrapped to get at the boost info held
-	 * in each child element. 
-	 * Some discussion around this topic here:
-	 * http://www.gossamer-threads.com/lists/lucene/java-dev/34208?search_string=introspection;#34208
-	 * Unfortunately there seemed to be limited interest in requiring all Query objects to implement
-	 * something common which would allow access to child queries so what follows here are query-specific
-	 * implementations for accessing embedded query elements. 
-	 */
-	private static final void getTermsFromBooleanQuery(BooleanQuery query, HashSet<WeightedTerm> terms, boolean prohibited, String fieldName)
-	{
-		BooleanClause[] queryClauses = query.getClauses();
-		for (int i = 0; i < queryClauses.length; i++)
-		{
-			if (prohibited || queryClauses[i].getOccur()!=BooleanClause.Occur.MUST_NOT)
-				getTerms(queryClauses[i].getQuery(), terms, prohibited, fieldName);
-		}
-	}	
-	private static void getTermsFromFilteredQuery(FilteredQuery query, HashSet<WeightedTerm> terms, boolean prohibited, String fieldName)
-	{
-		getTerms(query.getQuery(),terms,prohibited,fieldName);		
-	}
-	
+  /**
+   * extractTerms is currently the only query-independent means of introspecting queries but it only reveals
+   * a list of terms for that query - not the boosts each individual term in that query may or may not have.
+   * "Container" queries such as BooleanQuery should be unwrapped to get at the boost info held
+   * in each child element.
+   * Some discussion around this topic here:
+   * http://www.gossamer-threads.com/lists/lucene/java-dev/34208?search_string=introspection;#34208
+   * Unfortunately there seemed to be limited interest in requiring all Query objects to implement
+   * something common which would allow access to child queries so what follows here are query-specific
+   * implementations for accessing embedded query elements.
+   */
+  private static final void getTermsFromBooleanQuery(BooleanQuery query, HashSet<WeightedTerm> terms, boolean prohibited, String fieldName)
+  {
+    BooleanClause[] queryClauses = query.getClauses();
+    for (int i = 0; i < queryClauses.length; i++)
+    {
+      if (prohibited || queryClauses[i].getOccur()!=BooleanClause.Occur.MUST_NOT)
+        getTerms(queryClauses[i].getQuery(), terms, prohibited, fieldName);
+    }
+  }
+  private static void getTermsFromFilteredQuery(FilteredQuery query, HashSet<WeightedTerm> terms, boolean prohibited, String fieldName)
+  {
+    getTerms(query.getQuery(),terms,prohibited,fieldName);
+  }
+
 }
Index: lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter.java	(revision 1386643)
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter.java	(working copy)
@@ -38,446 +38,446 @@
   public static final int DEFAULT_MAX_CHARS_TO_ANALYZE = 50*1024;
 
   private int maxDocCharsToAnalyze = DEFAULT_MAX_CHARS_TO_ANALYZE;
-	private Formatter formatter;
-	private Encoder encoder;
-	private Fragmenter textFragmenter=new SimpleFragmenter();
-	private Scorer fragmentScorer=null;
+  private Formatter formatter;
+  private Encoder encoder;
+  private Fragmenter textFragmenter=new SimpleFragmenter();
+  private Scorer fragmentScorer=null;
 
-	public Highlighter(Scorer fragmentScorer)
-	{
-		this(new SimpleHTMLFormatter(),fragmentScorer);
-	}
+  public Highlighter(Scorer fragmentScorer)
+  {
+    this(new SimpleHTMLFormatter(),fragmentScorer);
+  }
 
 
- 	public Highlighter(Formatter formatter, Scorer fragmentScorer)
- 	{
-		this(formatter,new DefaultEncoder(),fragmentScorer);
-	}
+   public Highlighter(Formatter formatter, Scorer fragmentScorer)
+   {
+    this(formatter,new DefaultEncoder(),fragmentScorer);
+  }
 
 
-	public Highlighter(Formatter formatter, Encoder encoder, Scorer fragmentScorer)
-	{
- 		this.formatter = formatter;
-		this.encoder = encoder;
- 		this.fragmentScorer = fragmentScorer;
- 	}
+  public Highlighter(Formatter formatter, Encoder encoder, Scorer fragmentScorer)
+  {
+     this.formatter = formatter;
+    this.encoder = encoder;
+     this.fragmentScorer = fragmentScorer;
+   }
 
-	/**
-	 * Highlights chosen terms in a text, extracting the most relevant section.
-	 * This is a convenience method that calls
-	 * {@link #getBestFragment(TokenStream, String)}
-	 *
-	 * @param analyzer   the analyzer that will be used to split <code>text</code>
-	 * into chunks
-	 * @param text text to highlight terms in
-	 * @param fieldName Name of field used to influence analyzer's tokenization policy
-	 *
-	 * @return highlighted text fragment or null if no terms found
-	 * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
-	 */
-	public final String getBestFragment(Analyzer analyzer, String fieldName,String text)
-		throws IOException, InvalidTokenOffsetsException
-	{
-		TokenStream tokenStream = analyzer.tokenStream(fieldName, new StringReader(text));
-		return getBestFragment(tokenStream, text);
-	}
+  /**
+   * Highlights chosen terms in a text, extracting the most relevant section.
+   * This is a convenience method that calls
+   * {@link #getBestFragment(TokenStream, String)}
+   *
+   * @param analyzer   the analyzer that will be used to split <code>text</code>
+   * into chunks
+   * @param text text to highlight terms in
+   * @param fieldName Name of field used to influence analyzer's tokenization policy
+   *
+   * @return highlighted text fragment or null if no terms found
+   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
+   */
+  public final String getBestFragment(Analyzer analyzer, String fieldName,String text)
+    throws IOException, InvalidTokenOffsetsException
+  {
+    TokenStream tokenStream = analyzer.tokenStream(fieldName, new StringReader(text));
+    return getBestFragment(tokenStream, text);
+  }
 
-	/**
-	 * Highlights chosen terms in a text, extracting the most relevant section.
-	 * The document text is analysed in chunks to record hit statistics
-	 * across the document. After accumulating stats, the fragment with the highest score
-	 * is returned
-	 *
-	 * @param tokenStream   a stream of tokens identified in the text parameter, including offset information.
-	 * This is typically produced by an analyzer re-parsing a document's
-	 * text. Some work may be done on retrieving TokenStreams more efficiently
-	 * by adding support for storing original text position data in the Lucene
-	 * index but this support is not currently available (as of Lucene 1.4 rc2).
-	 * @param text text to highlight terms in
-	 *
-	 * @return highlighted text fragment or null if no terms found
-	 * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
-	 */
-	public final String getBestFragment(TokenStream tokenStream, String text)
-		throws IOException, InvalidTokenOffsetsException
-	{
-		String[] results = getBestFragments(tokenStream,text, 1);
-		if (results.length > 0)
-		{
-			return results[0];
-		}
-		return null;
-	}
+  /**
+   * Highlights chosen terms in a text, extracting the most relevant section.
+   * The document text is analysed in chunks to record hit statistics
+   * across the document. After accumulating stats, the fragment with the highest score
+   * is returned
+   *
+   * @param tokenStream   a stream of tokens identified in the text parameter, including offset information.
+   * This is typically produced by an analyzer re-parsing a document's
+   * text. Some work may be done on retrieving TokenStreams more efficiently
+   * by adding support for storing original text position data in the Lucene
+   * index but this support is not currently available (as of Lucene 1.4 rc2).
+   * @param text text to highlight terms in
+   *
+   * @return highlighted text fragment or null if no terms found
+   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
+   */
+  public final String getBestFragment(TokenStream tokenStream, String text)
+    throws IOException, InvalidTokenOffsetsException
+  {
+    String[] results = getBestFragments(tokenStream,text, 1);
+    if (results.length > 0)
+    {
+      return results[0];
+    }
+    return null;
+  }
 
-	/**
-	 * Highlights chosen terms in a text, extracting the most relevant sections.
-	 * This is a convenience method that calls
-	 * {@link #getBestFragments(TokenStream, String, int)}
-	 *
-	 * @param analyzer   the analyzer that will be used to split <code>text</code>
-	 * into chunks
-	 * @param fieldName     the name of the field being highlighted (used by analyzer)
-	 * @param text        	text to highlight terms in
-	 * @param maxNumFragments  the maximum number of fragments.
-	 *
-	 * @return highlighted text fragments (between 0 and maxNumFragments number of fragments)
-	 * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
-	 */
-	public final String[] getBestFragments(
-		Analyzer analyzer,
-		String fieldName,
-		String text,
-		int maxNumFragments)
-		throws IOException, InvalidTokenOffsetsException
-	{
-		TokenStream tokenStream = analyzer.tokenStream(fieldName, new StringReader(text));
-		return getBestFragments(tokenStream, text, maxNumFragments);
-	}
+  /**
+   * Highlights chosen terms in a text, extracting the most relevant sections.
+   * This is a convenience method that calls
+   * {@link #getBestFragments(TokenStream, String, int)}
+   *
+   * @param analyzer   the analyzer that will be used to split <code>text</code>
+   * into chunks
+   * @param fieldName     the name of the field being highlighted (used by analyzer)
+   * @param text          text to highlight terms in
+   * @param maxNumFragments  the maximum number of fragments.
+   *
+   * @return highlighted text fragments (between 0 and maxNumFragments number of fragments)
+   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
+   */
+  public final String[] getBestFragments(
+    Analyzer analyzer,
+    String fieldName,
+    String text,
+    int maxNumFragments)
+    throws IOException, InvalidTokenOffsetsException
+  {
+    TokenStream tokenStream = analyzer.tokenStream(fieldName, new StringReader(text));
+    return getBestFragments(tokenStream, text, maxNumFragments);
+  }
 
-	/**
-	 * Highlights chosen terms in a text, extracting the most relevant sections.
-	 * The document text is analysed in chunks to record hit statistics
-	 * across the document. After accumulating stats, the fragments with the highest scores
-	 * are returned as an array of strings in order of score (contiguous fragments are merged into
-	 * one in their original order to improve readability)
-	 *
-	 * @param text        	text to highlight terms in
-	 * @param maxNumFragments  the maximum number of fragments.
-	 *
-	 * @return highlighted text fragments (between 0 and maxNumFragments number of fragments)
-	 * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
-	 */
-	public final String[] getBestFragments(
-		TokenStream tokenStream,
-		String text,
-		int maxNumFragments)
-		throws IOException, InvalidTokenOffsetsException
-	{
-		maxNumFragments = Math.max(1, maxNumFragments); //sanity check
+  /**
+   * Highlights chosen terms in a text, extracting the most relevant sections.
+   * The document text is analysed in chunks to record hit statistics
+   * across the document. After accumulating stats, the fragments with the highest scores
+   * are returned as an array of strings in order of score (contiguous fragments are merged into
+   * one in their original order to improve readability)
+   *
+   * @param text          text to highlight terms in
+   * @param maxNumFragments  the maximum number of fragments.
+   *
+   * @return highlighted text fragments (between 0 and maxNumFragments number of fragments)
+   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
+   */
+  public final String[] getBestFragments(
+    TokenStream tokenStream,
+    String text,
+    int maxNumFragments)
+    throws IOException, InvalidTokenOffsetsException
+  {
+    maxNumFragments = Math.max(1, maxNumFragments); //sanity check
 
-		TextFragment[] frag =getBestTextFragments(tokenStream,text, true,maxNumFragments);
+    TextFragment[] frag =getBestTextFragments(tokenStream,text, true,maxNumFragments);
 
-		//Get text
-		ArrayList<String> fragTexts = new ArrayList<String>();
-		for (int i = 0; i < frag.length; i++)
-		{
-			if ((frag[i] != null) && (frag[i].getScore() > 0))
-			{
-				fragTexts.add(frag[i].toString());
-			}
-		}
-		return fragTexts.toArray(new String[0]);
-	}
+    //Get text
+    ArrayList<String> fragTexts = new ArrayList<String>();
+    for (int i = 0; i < frag.length; i++)
+    {
+      if ((frag[i] != null) && (frag[i].getScore() > 0))
+      {
+        fragTexts.add(frag[i].toString());
+      }
+    }
+    return fragTexts.toArray(new String[0]);
+  }
 
 
-	/**
-	 * Low level api to get the most relevant (formatted) sections of the document.
-	 * This method has been made public to allow visibility of score information held in TextFragment objects.
-	 * Thanks to Jason Calabrese for help in redefining the interface.
-	 * @param tokenStream
-	 * @param text
-	 * @param maxNumFragments
-	 * @param mergeContiguousFragments
-	 * @throws IOException
-	 * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
-	 */
-	public final TextFragment[] getBestTextFragments(
-		TokenStream tokenStream,
-		String text,
-		boolean mergeContiguousFragments,
-		int maxNumFragments)
-		throws IOException, InvalidTokenOffsetsException
-	{
-		ArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();
-		StringBuilder newText=new StringBuilder();
-		
-	    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);
-	    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);
-	    tokenStream.addAttribute(PositionIncrementAttribute.class);
-	    tokenStream.reset();
-	    
-		TextFragment currentFrag =	new TextFragment(newText,newText.length(), docFrags.size());
-		
+  /**
+   * Low level api to get the most relevant (formatted) sections of the document.
+   * This method has been made public to allow visibility of score information held in TextFragment objects.
+   * Thanks to Jason Calabrese for help in redefining the interface.
+   * @param tokenStream
+   * @param text
+   * @param maxNumFragments
+   * @param mergeContiguousFragments
+   * @throws IOException
+   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
+   */
+  public final TextFragment[] getBestTextFragments(
+    TokenStream tokenStream,
+    String text,
+    boolean mergeContiguousFragments,
+    int maxNumFragments)
+    throws IOException, InvalidTokenOffsetsException
+  {
+    ArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();
+    StringBuilder newText=new StringBuilder();
+
+      CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);
+      OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);
+      tokenStream.addAttribute(PositionIncrementAttribute.class);
+      tokenStream.reset();
+
+    TextFragment currentFrag =  new TextFragment(newText,newText.length(), docFrags.size());
+
     if (fragmentScorer instanceof QueryScorer) {
       ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);
     }
     
-		TokenStream newStream = fragmentScorer.init(tokenStream);
-		if(newStream != null) {
-		  tokenStream = newStream;
-		}
-		fragmentScorer.startFragment(currentFrag);
-		docFrags.add(currentFrag);
+    TokenStream newStream = fragmentScorer.init(tokenStream);
+    if(newStream != null) {
+      tokenStream = newStream;
+    }
+    fragmentScorer.startFragment(currentFrag);
+    docFrags.add(currentFrag);
 
-		FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);
+    FragmentQueue fragQueue = new FragmentQueue(maxNumFragments);
 
-		try
-		{
+    try
+    {
 
-			String tokenText;
-			int startOffset;
-			int endOffset;
-			int lastEndOffset = 0;
-			textFragmenter.start(text, tokenStream);
+      String tokenText;
+      int startOffset;
+      int endOffset;
+      int lastEndOffset = 0;
+      textFragmenter.start(text, tokenStream);
 
-			TokenGroup tokenGroup=new TokenGroup(tokenStream);
+      TokenGroup tokenGroup=new TokenGroup(tokenStream);
 
-			for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);
-			      next = tokenStream.incrementToken())
-			{
-				if(	(offsetAtt.endOffset()>text.length())
-					||
-					(offsetAtt.startOffset()>text.length())
-					)						
-				{
-					throw new InvalidTokenOffsetsException("Token "+ termAtt.toString()
-							+" exceeds length of provided text sized "+text.length());
-				}
-				if((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))
-				{
-					//the current token is distinct from previous tokens -
-					// markup the cached token group info
-					startOffset = tokenGroup.matchStartOffset;
-					endOffset = tokenGroup.matchEndOffset;
-					tokenText = text.substring(startOffset, endOffset);
-					String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);
-					//store any whitespace etc from between this and last group
-					if (startOffset > lastEndOffset)
-						newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));
-					newText.append(markedUpText);
-					lastEndOffset=Math.max(endOffset, lastEndOffset);
-					tokenGroup.clear();
+      for (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);
+            next = tokenStream.incrementToken())
+      {
+        if(  (offsetAtt.endOffset()>text.length())
+          ||
+          (offsetAtt.startOffset()>text.length())
+          )
+        {
+          throw new InvalidTokenOffsetsException("Token "+ termAtt.toString()
+              +" exceeds length of provided text sized "+text.length());
+        }
+        if((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))
+        {
+          //the current token is distinct from previous tokens -
+          // markup the cached token group info
+          startOffset = tokenGroup.matchStartOffset;
+          endOffset = tokenGroup.matchEndOffset;
+          tokenText = text.substring(startOffset, endOffset);
+          String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);
+          //store any whitespace etc from between this and last group
+          if (startOffset > lastEndOffset)
+            newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));
+          newText.append(markedUpText);
+          lastEndOffset=Math.max(endOffset, lastEndOffset);
+          tokenGroup.clear();
 
-					//check if current token marks the start of a new fragment
-					if(textFragmenter.isNewFragment())
-					{
-						currentFrag.setScore(fragmentScorer.getFragmentScore());
-						//record stats for a new fragment
-						currentFrag.textEndPos = newText.length();
-						currentFrag =new TextFragment(newText, newText.length(), docFrags.size());
-						fragmentScorer.startFragment(currentFrag);
-						docFrags.add(currentFrag);
-					}
-				}
+          //check if current token marks the start of a new fragment
+          if(textFragmenter.isNewFragment())
+          {
+            currentFrag.setScore(fragmentScorer.getFragmentScore());
+            //record stats for a new fragment
+            currentFrag.textEndPos = newText.length();
+            currentFrag =new TextFragment(newText, newText.length(), docFrags.size());
+            fragmentScorer.startFragment(currentFrag);
+            docFrags.add(currentFrag);
+          }
+        }
 
-				tokenGroup.addToken(fragmentScorer.getTokenScore());
+        tokenGroup.addToken(fragmentScorer.getTokenScore());
 
-//				if(lastEndOffset>maxDocBytesToAnalyze)
-//				{
-//					break;
-//				}
-			}
-			currentFrag.setScore(fragmentScorer.getFragmentScore());
+//        if(lastEndOffset>maxDocBytesToAnalyze)
+//        {
+//          break;
+//        }
+      }
+      currentFrag.setScore(fragmentScorer.getFragmentScore());
 
-			if(tokenGroup.numTokens>0)
-			{
-				//flush the accumulated text (same code as in above loop)
-				startOffset = tokenGroup.matchStartOffset;
-				endOffset = tokenGroup.matchEndOffset;
-				tokenText = text.substring(startOffset, endOffset);
-				String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);
-				//store any whitespace etc from between this and last group
-				if (startOffset > lastEndOffset)
-					newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));
-				newText.append(markedUpText);
-				lastEndOffset=Math.max(lastEndOffset,endOffset);
-			}
+      if(tokenGroup.numTokens>0)
+      {
+        //flush the accumulated text (same code as in above loop)
+        startOffset = tokenGroup.matchStartOffset;
+        endOffset = tokenGroup.matchEndOffset;
+        tokenText = text.substring(startOffset, endOffset);
+        String markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);
+        //store any whitespace etc from between this and last group
+        if (startOffset > lastEndOffset)
+          newText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));
+        newText.append(markedUpText);
+        lastEndOffset=Math.max(lastEndOffset,endOffset);
+      }
 
-			//Test what remains of the original text beyond the point where we stopped analyzing 
-			if (
-//					if there is text beyond the last token considered..
-					(lastEndOffset < text.length()) 
-					&&
-//					and that text is not too large...
-					(text.length()<= maxDocCharsToAnalyze)
-				)				
-			{
-				//append it to the last fragment
-				newText.append(encoder.encodeText(text.substring(lastEndOffset)));
-			}
+      //Test what remains of the original text beyond the point where we stopped analyzing
+      if (
+//          if there is text beyond the last token considered..
+          (lastEndOffset < text.length())
+          &&
+//          and that text is not too large...
+          (text.length()<= maxDocCharsToAnalyze)
+        )
+      {
+        //append it to the last fragment
+        newText.append(encoder.encodeText(text.substring(lastEndOffset)));
+      }
 
-			currentFrag.textEndPos = newText.length();
+      currentFrag.textEndPos = newText.length();
 
-			//sort the most relevant sections of the text
-			for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)
-			{
-				currentFrag = i.next();
+      //sort the most relevant sections of the text
+      for (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)
+      {
+        currentFrag = i.next();
 
-				//If you are running with a version of Lucene before 11th Sept 03
-				// you do not have PriorityQueue.insert() - so uncomment the code below
-				/*
-									if (currentFrag.getScore() >= minScore)
-									{
-										fragQueue.put(currentFrag);
-										if (fragQueue.size() > maxNumFragments)
-										{ // if hit queue overfull
-											fragQueue.pop(); // remove lowest in hit queue
-											minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore
-										}
+        //If you are running with a version of Lucene before 11th Sept 03
+        // you do not have PriorityQueue.insert() - so uncomment the code below
+        /*
+                  if (currentFrag.getScore() >= minScore)
+                  {
+                    fragQueue.put(currentFrag);
+                    if (fragQueue.size() > maxNumFragments)
+                    { // if hit queue overfull
+                      fragQueue.pop(); // remove lowest in hit queue
+                      minScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore
+                    }
 
 
-									}
-				*/
-				//The above code caused a problem as a result of Christoph Goller's 11th Sept 03
-				//fix to PriorityQueue. The correct method to use here is the new "insert" method
-				// USE ABOVE CODE IF THIS DOES NOT COMPILE!
-				fragQueue.insertWithOverflow(currentFrag);
-			}
+                  }
+        */
+        //The above code caused a problem as a result of Christoph Goller's 11th Sept 03
+        //fix to PriorityQueue. The correct method to use here is the new "insert" method
+        // USE ABOVE CODE IF THIS DOES NOT COMPILE!
+        fragQueue.insertWithOverflow(currentFrag);
+      }
 
-			//return the most relevant fragments
-			TextFragment frag[] = new TextFragment[fragQueue.size()];
-			for (int i = frag.length - 1; i >= 0; i--)
-			{
-				frag[i] = fragQueue.pop();
-			}
+      //return the most relevant fragments
+      TextFragment frag[] = new TextFragment[fragQueue.size()];
+      for (int i = frag.length - 1; i >= 0; i--)
+      {
+        frag[i] = fragQueue.pop();
+      }
 
-			//merge any contiguous fragments to improve readability
-			if(mergeContiguousFragments)
-			{
-				mergeContiguousFragments(frag);
-				ArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();
-				for (int i = 0; i < frag.length; i++)
-				{
-					if ((frag[i] != null) && (frag[i].getScore() > 0))
-					{
-						fragTexts.add(frag[i]);
-					}
-				}
-				frag= fragTexts.toArray(new TextFragment[0]);
-			}
+      //merge any contiguous fragments to improve readability
+      if(mergeContiguousFragments)
+      {
+        mergeContiguousFragments(frag);
+        ArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();
+        for (int i = 0; i < frag.length; i++)
+        {
+          if ((frag[i] != null) && (frag[i].getScore() > 0))
+          {
+            fragTexts.add(frag[i]);
+          }
+        }
+        frag= fragTexts.toArray(new TextFragment[0]);
+      }
 
-			return frag;
+      return frag;
 
-		}
-		finally
-		{
-			if (tokenStream != null)
-			{
-				try
-				{
-				  tokenStream.end();
-					tokenStream.close();
-				}
-				catch (Exception e)
-				{
-				}
-			}
-		}
-	}
+    }
+    finally
+    {
+      if (tokenStream != null)
+      {
+        try
+        {
+          tokenStream.end();
+          tokenStream.close();
+        }
+        catch (Exception e)
+        {
+        }
+      }
+    }
+  }
 
 
-	/** Improves readability of a score-sorted list of TextFragments by merging any fragments
-	 * that were contiguous in the original text into one larger fragment with the correct order.
-	 * This will leave a "null" in the array entry for the lesser scored fragment. 
-	 * 
-	 * @param frag An array of document fragments in descending score
-	 */
-	private void mergeContiguousFragments(TextFragment[] frag)
-	{
-		boolean mergingStillBeingDone;
-		if (frag.length > 1)
-			do
-			{
-				mergingStillBeingDone = false; //initialise loop control flag
-				//for each fragment, scan other frags looking for contiguous blocks
-				for (int i = 0; i < frag.length; i++)
-				{
-					if (frag[i] == null)
-					{
-						continue;
-					}
-					//merge any contiguous blocks 
-					for (int x = 0; x < frag.length; x++)
-					{
-						if (frag[x] == null)
-						{
-							continue;
-						}
-						if (frag[i] == null)
-						{
-							break;
-						}
-						TextFragment frag1 = null;
-						TextFragment frag2 = null;
-						int frag1Num = 0;
-						int frag2Num = 0;
-						int bestScoringFragNum;
-						int worstScoringFragNum;
-						//if blocks are contiguous....
-						if (frag[i].follows(frag[x]))
-						{
-							frag1 = frag[x];
-							frag1Num = x;
-							frag2 = frag[i];
-							frag2Num = i;
-						}
-						else
-							if (frag[x].follows(frag[i]))
-							{
-								frag1 = frag[i];
-								frag1Num = i;
-								frag2 = frag[x];
-								frag2Num = x;
-							}
-						//merging required..
-						if (frag1 != null)
-						{
-							if (frag1.getScore() > frag2.getScore())
-							{
-								bestScoringFragNum = frag1Num;
-								worstScoringFragNum = frag2Num;
-							}
-							else
-							{
-								bestScoringFragNum = frag2Num;
-								worstScoringFragNum = frag1Num;
-							}
-							frag1.merge(frag2);
-							frag[worstScoringFragNum] = null;
-							mergingStillBeingDone = true;
-							frag[bestScoringFragNum] = frag1;
-						}
-					}
-				}
-			}
-			while (mergingStillBeingDone);
-	}
-	
-	
-	/**
-	 * Highlights terms in the  text , extracting the most relevant sections
-	 * and concatenating the chosen fragments with a separator (typically "...").
-	 * The document text is analysed in chunks to record hit statistics
-	 * across the document. After accumulating stats, the fragments with the highest scores
-	 * are returned in order as "separator" delimited strings.
-	 *
-	 * @param text        text to highlight terms in
-	 * @param maxNumFragments  the maximum number of fragments.
-	 * @param separator  the separator used to intersperse the document fragments (typically "...")
-	 *
-	 * @return highlighted text
-	 * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
-	 */
-	public final String getBestFragments(
-		TokenStream tokenStream,	
-		String text,
-		int maxNumFragments,
-		String separator)
-		throws IOException, InvalidTokenOffsetsException
-	{
-		String sections[] =	getBestFragments(tokenStream,text, maxNumFragments);
-		StringBuilder result = new StringBuilder();
-		for (int i = 0; i < sections.length; i++)
-		{
-			if (i > 0)
-			{
-				result.append(separator);
-			}
-			result.append(sections[i]);
-		}
-		return result.toString();
-	}
+  /** Improves readability of a score-sorted list of TextFragments by merging any fragments
+   * that were contiguous in the original text into one larger fragment with the correct order.
+   * This will leave a "null" in the array entry for the lesser scored fragment.
+   *
+   * @param frag An array of document fragments in descending score
+   */
+  private void mergeContiguousFragments(TextFragment[] frag)
+  {
+    boolean mergingStillBeingDone;
+    if (frag.length > 1)
+      do
+      {
+        mergingStillBeingDone = false; //initialise loop control flag
+        //for each fragment, scan other frags looking for contiguous blocks
+        for (int i = 0; i < frag.length; i++)
+        {
+          if (frag[i] == null)
+          {
+            continue;
+          }
+          //merge any contiguous blocks
+          for (int x = 0; x < frag.length; x++)
+          {
+            if (frag[x] == null)
+            {
+              continue;
+            }
+            if (frag[i] == null)
+            {
+              break;
+            }
+            TextFragment frag1 = null;
+            TextFragment frag2 = null;
+            int frag1Num = 0;
+            int frag2Num = 0;
+            int bestScoringFragNum;
+            int worstScoringFragNum;
+            //if blocks are contiguous....
+            if (frag[i].follows(frag[x]))
+            {
+              frag1 = frag[x];
+              frag1Num = x;
+              frag2 = frag[i];
+              frag2Num = i;
+            }
+            else
+              if (frag[x].follows(frag[i]))
+              {
+                frag1 = frag[i];
+                frag1Num = i;
+                frag2 = frag[x];
+                frag2Num = x;
+              }
+            //merging required..
+            if (frag1 != null)
+            {
+              if (frag1.getScore() > frag2.getScore())
+              {
+                bestScoringFragNum = frag1Num;
+                worstScoringFragNum = frag2Num;
+              }
+              else
+              {
+                bestScoringFragNum = frag2Num;
+                worstScoringFragNum = frag1Num;
+              }
+              frag1.merge(frag2);
+              frag[worstScoringFragNum] = null;
+              mergingStillBeingDone = true;
+              frag[bestScoringFragNum] = frag1;
+            }
+          }
+        }
+      }
+      while (mergingStillBeingDone);
+  }
 
+
+  /**
+   * Highlights terms in the  text , extracting the most relevant sections
+   * and concatenating the chosen fragments with a separator (typically "...").
+   * The document text is analysed in chunks to record hit statistics
+   * across the document. After accumulating stats, the fragments with the highest scores
+   * are returned in order as "separator" delimited strings.
+   *
+   * @param text        text to highlight terms in
+   * @param maxNumFragments  the maximum number of fragments.
+   * @param separator  the separator used to intersperse the document fragments (typically "...")
+   *
+   * @return highlighted text
+   * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length
+   */
+  public final String getBestFragments(
+    TokenStream tokenStream,
+    String text,
+    int maxNumFragments,
+    String separator)
+    throws IOException, InvalidTokenOffsetsException
+  {
+    String sections[] =  getBestFragments(tokenStream,text, maxNumFragments);
+    StringBuilder result = new StringBuilder();
+    for (int i = 0; i < sections.length; i++)
+    {
+      if (i > 0)
+      {
+        result.append(separator);
+      }
+      result.append(sections[i]);
+    }
+    return result.toString();
+  }
+
   public int getMaxDocCharsToAnalyze() {
     return maxDocCharsToAnalyze;
   }
@@ -487,35 +487,35 @@
   }
 
   
-	public Fragmenter getTextFragmenter()
-	{
-		return textFragmenter;
-	}
+  public Fragmenter getTextFragmenter()
+  {
+    return textFragmenter;
+  }
 
-	/**
-	 * @param fragmenter
-	 */
-	public void setTextFragmenter(Fragmenter fragmenter)
-	{
-		textFragmenter = fragmenter;
-	}
+  /**
+   * @param fragmenter
+   */
+  public void setTextFragmenter(Fragmenter fragmenter)
+  {
+    textFragmenter = fragmenter;
+  }
 
-	/**
-	 * @return Object used to score each text fragment 
-	 */
-	public Scorer getFragmentScorer()
-	{
-		return fragmentScorer;
-	}
+  /**
+   * @return Object used to score each text fragment
+   */
+  public Scorer getFragmentScorer()
+  {
+    return fragmentScorer;
+  }
 
 
-	/**
-	 * @param scorer
-	 */
-	public void setFragmentScorer(Scorer scorer)
-	{
-		fragmentScorer = scorer;
-	}
+  /**
+   * @param scorer
+   */
+  public void setFragmentScorer(Scorer scorer)
+  {
+    fragmentScorer = scorer;
+  }
 
     public Encoder getEncoder()
     {
@@ -528,17 +528,17 @@
 }
 class FragmentQueue extends PriorityQueue<TextFragment>
 {
-	public FragmentQueue(int size)
-	{
-		super(size);
-	}
+  public FragmentQueue(int size)
+  {
+    super(size);
+  }
 
-	@Override
-	public final boolean lessThan(TextFragment fragA, TextFragment fragB)
-	{
-		if (fragA.getScore() == fragB.getScore())
-			return fragA.fragNum > fragB.fragNum;
-		else
-			return fragA.getScore() < fragB.getScore();
-	}
+  @Override
+  public final boolean lessThan(TextFragment fragA, TextFragment fragB)
+  {
+    if (fragA.getScore() == fragB.getScore())
+      return fragA.fragNum > fragB.fragNum;
+    else
+      return fragA.getScore() < fragB.getScore();
+  }
 }
Index: lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSlowFuzzyQuery.java
===================================================================
--- lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSlowFuzzyQuery.java	(revision 1386643)
+++ lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSlowFuzzyQuery.java	(working copy)
@@ -105,7 +105,7 @@
     }
 
     // not similar enough:
-    query = new SlowFuzzyQuery(new Term("field", "xxxxx"), SlowFuzzyQuery.defaultMinSimilarity, 0);  	
+    query = new SlowFuzzyQuery(new Term("field", "xxxxx"), SlowFuzzyQuery.defaultMinSimilarity, 0);
     hits = searcher.search(query, null, 1000).scoreDocs;
     assertEquals(0, hits.length);
     query = new SlowFuzzyQuery(new Term("field", "aaccc"), SlowFuzzyQuery.defaultMinSimilarity, 0);   // edit distance to "aaaaa" = 3
Index: lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery.java
===================================================================
--- lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery.java	(revision 1386643)
+++ lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/FuzzyLikeThisQuery.java	(working copy)
@@ -121,17 +121,17 @@
 
     class FieldVals
     {
-    	String queryString;
-    	String fieldName;
-    	float minSimilarity;
-    	int prefixLength;
-		public FieldVals(String name, float similarity, int length, String queryString)
-		{
-			fieldName = name;
-			minSimilarity = similarity;
-			prefixLength = length;
-			this.queryString = queryString;
-		}
+      String queryString;
+      String fieldName;
+      float minSimilarity;
+      int prefixLength;
+    public FieldVals(String name, float similarity, int length, String queryString)
+    {
+      fieldName = name;
+      minSimilarity = similarity;
+      prefixLength = length;
+      this.queryString = queryString;
+    }
 
     @Override
     public int hashCode() {
@@ -174,7 +174,7 @@
     }
     
 
-    	
+
     }
     
     /**
@@ -186,77 +186,72 @@
      */
     public void addTerms(String queryString, String fieldName,float minSimilarity, int prefixLength) 
     {
-    	fieldVals.add(new FieldVals(fieldName,minSimilarity,prefixLength,queryString));
+      fieldVals.add(new FieldVals(fieldName,minSimilarity,prefixLength,queryString));
     }
-    
-    
-    private void addTerms(IndexReader reader,FieldVals f) throws IOException
-    {
-        if(f.queryString==null) return;
-        TokenStream ts=analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));
-        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
-        
-        int corpusNumDocs=reader.numDocs();
-        HashSet<String> processedTerms=new HashSet<String>();
-        ts.reset();
-        while (ts.incrementToken()) 
-        {
-                String term = termAtt.toString();
-        	if(!processedTerms.contains(term))
-        	{
-                  processedTerms.add(term);
-                  ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term
-                  float minScore=0;
-                  Term startTerm=new Term(f.fieldName, term);
-                  AttributeSource atts = new AttributeSource();
-                  MaxNonCompetitiveBoostAttribute maxBoostAtt =
-                    atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);
-                  SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength);
-                  //store the df so all variants use same idf
-                  int df = reader.docFreq(startTerm);
-                  int numVariants=0;
-                  int totalVariantDocFreqs=0;
-                  BytesRef possibleMatch;
-                  BoostAttribute boostAtt =
-                    fe.attributes().addAttribute(BoostAttribute.class);
-                  while ((possibleMatch = fe.next()) != null) {
-                      numVariants++;
-                      totalVariantDocFreqs+=fe.docFreq();
-                      float score=boostAtt.getBoost();
-                      if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore){
-                        ScoreTerm st=new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)),score,startTerm);                    
-                        variantsQ.insertWithOverflow(st);
-                        minScore = variantsQ.top().score; // maintain minScore
-                      }
-                      maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);
-                    }
 
-                  if(numVariants>0)
-                    {
-                      int avgDf=totalVariantDocFreqs/numVariants;
-                      if(df==0)//no direct match we can use as df for all variants 
-	                {
-	                    df=avgDf; //use avg df of all variants
-	                }
-	                
-                    // take the top variants (scored by edit distance) and reset the score
-                    // to include an IDF factor then add to the global queue for ranking 
-                    // overall top query terms
-                    int size = variantsQ.size();
-                    for(int i = 0; i < size; i++)
-	                {
-	                  ScoreTerm st = variantsQ.pop();
-	                  st.score=(st.score*st.score)*sim.idf(df,corpusNumDocs);
-	                  q.insertWithOverflow(st);
-	                }                            
-                }
-        	}
+
+  private void addTerms(IndexReader reader, FieldVals f) throws IOException {
+    if (f.queryString == null) return;
+    TokenStream ts = analyzer.tokenStream(f.fieldName, new StringReader(f.queryString));
+    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
+
+    int corpusNumDocs = reader.numDocs();
+    HashSet<String> processedTerms = new HashSet<String>();
+    ts.reset();
+    while (ts.incrementToken()) {
+      String term = termAtt.toString();
+      if (!processedTerms.contains(term)) {
+        processedTerms.add(term);
+        ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term
+        float minScore = 0;
+        Term startTerm = new Term(f.fieldName, term);
+        AttributeSource atts = new AttributeSource();
+        MaxNonCompetitiveBoostAttribute maxBoostAtt =
+            atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);
+        SlowFuzzyTermsEnum fe = new SlowFuzzyTermsEnum(MultiFields.getTerms(reader, startTerm.field()), atts, startTerm, f.minSimilarity, f.prefixLength);
+        //store the df so all variants use same idf
+        int df = reader.docFreq(startTerm);
+        int numVariants = 0;
+        int totalVariantDocFreqs = 0;
+        BytesRef possibleMatch;
+        BoostAttribute boostAtt =
+            fe.attributes().addAttribute(BoostAttribute.class);
+        while ((possibleMatch = fe.next()) != null) {
+          numVariants++;
+          totalVariantDocFreqs += fe.docFreq();
+          float score = boostAtt.getBoost();
+          if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {
+            ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);
+            variantsQ.insertWithOverflow(st);
+            minScore = variantsQ.top().score; // maintain minScore
+          }
+          maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);
         }
-        ts.end();
-        ts.close();
+
+        if (numVariants > 0) {
+          int avgDf = totalVariantDocFreqs / numVariants;
+          if (df == 0)//no direct match we can use as df for all variants
+          {
+            df = avgDf; //use avg df of all variants
+          }
+
+          // take the top variants (scored by edit distance) and reset the score
+          // to include an IDF factor then add to the global queue for ranking
+          // overall top query terms
+          int size = variantsQ.size();
+          for (int i = 0; i < size; i++) {
+            ScoreTerm st = variantsQ.pop();
+            st.score = (st.score * st.score) * sim.idf(df, corpusNumDocs);
+            q.insertWithOverflow(st);
+          }
+        }
+      }
     }
-            
-    @Override
+    ts.end();
+    ts.close();
+  }
+
+  @Override
     public Query rewrite(IndexReader reader) throws IOException
     {
         if(rewrittenQuery!=null)
@@ -264,12 +259,11 @@
             return rewrittenQuery;
         }
         //load up the list of possible terms
-        for (Iterator<FieldVals> iter = fieldVals.iterator(); iter.hasNext();)
-		{
-			FieldVals f = iter.next();
-			addTerms(reader,f);			
-		}
-        //clear the list of fields
+        for (Iterator<FieldVals> iter = fieldVals.iterator(); iter.hasNext(); ) {
+          FieldVals f = iter.next();
+          addTerms(reader, f);
+        }
+      //clear the list of fields
         fieldVals.clear();
         
         BooleanQuery bq=new BooleanQuery();
@@ -368,15 +362,15 @@
     }
 
 
-	public boolean isIgnoreTF()
-	{
-		return ignoreTF;
-	}
+  public boolean isIgnoreTF()
+  {
+    return ignoreTF;
+  }
 
 
-	public void setIgnoreTF(boolean ignoreTF)
-	{
-		this.ignoreTF = ignoreTF;
-	}   
+  public void setIgnoreTF(boolean ignoreTF)
+  {
+    this.ignoreTF = ignoreTF;
+  }
     
 }
Index: lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java	(revision 1386643)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java	(working copy)
@@ -170,7 +170,7 @@
 
     @Override
     public long seek(BytesRef target) {
-      int lo = 0;				  // binary search
+      int lo = 0;          // binary search
       int hi = fieldIndex.numIndexTerms - 1;
       assert totalIndexInterval > 0 : "totalIndexInterval=" + totalIndexInterval;
 
Index: lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer.java	(revision 1386643)
+++ lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer.java	(working copy)
@@ -160,7 +160,7 @@
           return Character.toCodePoint((char) ch, (char) ch2);
         } else {
           assert false : "stream ends with unpaired high surrogate: " + Integer.toHexString(ch);
-	}
+        }
       }
       return ch;
     }
Index: lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java	(revision 1386643)
+++ lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java	(working copy)
@@ -359,7 +359,7 @@
       } else {
         Logger.getLogger(LuceneTestCase.class.getSimpleName()).warning(
             "Property '" + SYSPROP_MAXFAILURES + "'=" + maxFailures + ", 'failfast' is" +
-            		" ignored.");
+            " ignored.");
       }
     }
 
Index: lucene/test-framework/src/java/org/apache/lucene/util/RunListenerPrintReproduceInfo.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/RunListenerPrintReproduceInfo.java	(revision 1386643)
+++ lucene/test-framework/src/java/org/apache/lucene/util/RunListenerPrintReproduceInfo.java	(working copy)
@@ -129,7 +129,7 @@
   private void reportAdditionalFailureInfo(final String testName) {
     if (TEST_LINE_DOCS_FILE.endsWith(JENKINS_LARGE_LINE_DOCS_FILE)) {
       System.err.println("NOTE: download the large Jenkins line-docs file by running " +
-      		"'ant get-jenkins-line-docs' in the lucene directory.");
+        "'ant get-jenkins-line-docs' in the lucene directory.");
     }
 
     final StringBuilder b = new StringBuilder();
Index: lucene/test-framework/src/java/org/apache/lucene/util/TestRuleAssertionsRequired.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/TestRuleAssertionsRequired.java	(revision 1386643)
+++ lucene/test-framework/src/java/org/apache/lucene/util/TestRuleAssertionsRequired.java	(working copy)
@@ -33,7 +33,7 @@
         try {
           assert false;
           String msg = "Test class requires enabled assertions, enable globally (-ea)" +
-          		" or for Solr/Lucene subpackages only: " + description.getClassName();
+              " or for Solr/Lucene subpackages only: " + description.getClassName();
           System.err.println(msg);
           throw new Exception(msg);
         } catch (AssertionError e) {
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleBoolFunction.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleBoolFunction.java	(revision 1386643)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleBoolFunction.java	(working copy)
@@ -44,11 +44,11 @@
     return new BoolDocValues(this) {
       @Override
       public boolean boolVal(int doc) {
-	      return func(doc, vals);
+        return func(doc, vals);
       }
       @Override
       public String toString(int doc) {
-	      return name() + '(' + vals.toString(doc) + ')';
+        return name() + '(' + vals.toString(doc) + ')';
       }
     };
   }
@@ -74,4 +74,4 @@
   public void createWeight(Map context, IndexSearcher searcher) throws IOException {
     source.createWeight(context, searcher);
   }
-}
\ No newline at end of file
+}
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java	(revision 1386643)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java	(working copy)
@@ -115,11 +115,11 @@
     return new FloatDocValues(this) {
       @Override
       public float floatVal(int doc) {
-	return (vals.floatVal(doc) - minSource) * scale + min;
+        return (vals.floatVal(doc) - minSource) * scale + min;
       }
       @Override
       public String toString(int doc) {
-	return "scale(" + vals.toString(doc) + ",toMin=" + min + ",toMax=" + max
+        return "scale(" + vals.toString(doc) + ",toMin=" + min + ",toMax=" + max
                 + ",fromMin=" + minSource
                 + ",fromMax=" + maxSource
                 + ")";
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiBoolFunction.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiBoolFunction.java	(revision 1386643)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiBoolFunction.java	(working copy)
@@ -50,7 +50,7 @@
     return new BoolDocValues(this) {
       @Override
       public boolean boolVal(int doc) {
-	      return func(doc, vals);
+        return func(doc, vals);
       }
 
       @Override
@@ -105,4 +105,4 @@
       source.createWeight(context, searcher);
     }
   }
-}
\ No newline at end of file
+}
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java	(revision 1386643)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java	(working copy)
@@ -54,11 +54,12 @@
     return new FloatDocValues(this) {
       @Override
       public float floatVal(int doc) {
-	return func(doc, aVals, bVals);
+        return func(doc, aVals, bVals);
       }
+
       @Override
       public String toString(int doc) {
-	return name() + '(' + aVals.toString(doc) + ',' + bVals.toString(doc) + ')';
+        return name() + '(' + aVals.toString(doc) + ',' + bVals.toString(doc) + ')';
       }
     };
   }
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java	(revision 1386643)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java	(working copy)
@@ -40,11 +40,11 @@
     return new FloatDocValues(this) {
       @Override
       public float floatVal(int doc) {
-	return func(doc, vals);
+        return func(doc, vals);
       }
       @Override
       public String toString(int doc) {
-	return name() + '(' + vals.toString(doc) + ')';
+        return name() + '(' + vals.toString(doc) + ')';
       }
     };
   }
