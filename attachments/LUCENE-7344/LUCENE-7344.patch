diff --git a/lucene/core/build.xml b/lucene/core/build.xml
index 4e62e1c..c6978a2 100644
--- a/lucene/core/build.xml
+++ b/lucene/core/build.xml
@@ -32,6 +32,7 @@
   <path id="classpath"/>
   
   <path id="test.classpath">
+    <pathelement location="${common.dir}/build/sandbox/classes/java"/> <!-- nocommit -->
     <pathelement location="${common.dir}/build/codecs/classes/java"/>
     <pathelement location="${common.dir}/build/test-framework/classes/java"/>
     <path refid="junit-path"/>
diff --git a/lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream.java b/lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream.java
index 9da1e09..6060622 100644
--- a/lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream.java
+++ b/lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream.java
@@ -637,7 +637,7 @@ class BufferedUpdatesStream implements Accountable {
       int limit = update.docIDUpto;
       
       // TODO: we traverse the terms in update order (not term order) so that we
-      // apply the updates in the correct order, i.e. if two terms udpate the
+      // apply the updates in the correct order, i.e. if two terms update the
       // same document, the last one that came in wins, irrespective of the
       // terms lexical order.
       // we can apply the updates in terms order if we keep an updatesGen (and
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java b/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
index 219dd05..ea6f61a 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
@@ -17,13 +17,14 @@
 package org.apache.lucene.index;
 
 import java.io.IOException;
-import java.util.HashSet;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.Map;
 import java.util.Random;
 import java.util.Set;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.atomic.AtomicLong;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
@@ -37,6 +38,7 @@ import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.SortedSetDocValuesField;
 import org.apache.lucene.document.StringField;
+import org.apache.lucene.search.DocValuesRangeQuery;
 import org.apache.lucene.search.FieldDoc;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Sort;
@@ -49,25 +51,39 @@ import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.LuceneTestCase.*; // nocommit
 import org.apache.lucene.util.TestUtil;
 import org.junit.Test;
 
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
 
-@SuppressWarnings("resource")
+@SuppressSysoutChecks(bugUrl="nocommit") // also: comment out System.out.println's
 public class TestNumericDocValuesUpdates extends LuceneTestCase {
 
+
+  // nocommit: either these variables & every conditional line using them should be removed, OR...
+  // the "HACK" should be officially documented, the vars removed, and conditional code should alway run
+  private static final boolean FORCE_NEW_READER_HACK_PRE = true; // nocommit
+  private static final boolean FORCE_NEW_READER_HACK_POST = false; // nocommit
+    
   private Document doc(int id) {
     // make sure we don't set the doc's value to 0, to not confuse with a document that's missing values
     return doc(id, id +1);
   }
-  
+
   private Document doc(int id, long val) {
     Document doc = new Document();
     doc.add(new StringField("id", "doc-" + id, Store.NO));
+    for (int div : MOD_DIVISORS) {
+      doc.add(new StringField("mod" + div, "mod-" + (id % div), Store.NO));
+    }
     doc.add(new NumericDocValuesField("val", val));
     return doc;
   }
+  
+  /** @see #biasedMixOfRandomUpdates */
+  private static final int[] MOD_DIVISORS = new int[] { 3, 5, 7, 10, 13 };
+  
 
   public void testMultipleUpdatesSameDoc() throws Exception {
 
@@ -102,74 +118,275 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
     IOUtils.close(reader, writer, dir);
   }
 
+  public void testEvenMixOfRandomUpdates() throws Exception {
+    biasedMixOfRandomUpdates(33, 66, // even odds of add, update, updateDV
+                             0, 0); // no deletes or commits
+  }
+  
   public void testBiasedMixOfRandomUpdates() throws Exception {
-    // 3 types of operations: add, updated, updateDV.
-    // rather then randomizing equally, we'll pick (random) cutoffs so each test run is biased,
-    // in terms of some ops happen more often then others
     final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);
-    final int UPD_CUTOFF = TestUtil.nextInt(random(), ADD_CUTOFF+1, 99);
+    biasedMixOfRandomUpdates(ADD_CUTOFF,
+                             TestUtil.nextInt(random(), ADD_CUTOFF+1, 99),
+                             0, 0); // no deletes or commits
+  }
+  public void testBiasedMixOfRandomUpdatesWithCommits() throws Exception {
+    final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);
+    biasedMixOfRandomUpdates(ADD_CUTOFF,
+                             TestUtil.nextInt(random(), ADD_CUTOFF+1, 99),
+                             0, // no deletes
+                             TestUtil.nextInt(random(), 0, 1)); // 0-1% chance of commits
+  }
+  public void testBiasedMixOfRandomUpdatesWithDeletes() throws Exception {
+    final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);
+    biasedMixOfRandomUpdates(ADD_CUTOFF,
+                             TestUtil.nextInt(random(), ADD_CUTOFF+1, 99),
+                             TestUtil.nextInt(random(), 0, 4), // rarely delete
+                             0); // no commits
+  }
+  public void testBiasedMixOfRandomUpdatesWithDeletesAndCommits() throws Exception {
+    final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);
+    biasedMixOfRandomUpdates(ADD_CUTOFF, TestUtil.nextInt(random(), ADD_CUTOFF+1, 99),
+                             // rarely delete and (even rarer) commit
+                             TestUtil.nextInt(random(), 0, 4), 
+                             TestUtil.nextInt(random(), 0, 1));
+  }
+  public void testBiasedMixOfRandomUpdatesWithCommitsAndLotsOfDeletes() throws Exception {
+    // the deletes will lead to lots of updates being needed in place of updateDV,
+    // so bias our results to try updateDV as often as possible (ie: no explicit updates)
+    biasedMixOfRandomUpdates(30, 30, // 1/3 ADD, 2/3 updateDV
+                             50,     // 50% chance of a delete after every op
+                             TestUtil.nextInt(random(), 0, 1)); // 0-1% chance of commits
+  }
+  public void testBiasedMixOfRandomUpdatesWithNarrowValuesAndDeletes() throws Exception {
+    final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);
+    biasedMixOfRandomUpdates(ADD_CUTOFF, TestUtil.nextInt(random(), ADD_CUTOFF+1, 99),
+                             TestUtil.nextInt(random(), 0, 4), // rarely delete
+                             0, // never commit 
+                             -1000L, 1000L, 100); // narrow range of doc values
+  }
+  public void testBiasedMixOfRandomUpdatesWithNarrowValuesAndDeletesAndCommits() throws Exception {
+    final int ADD_CUTOFF = TestUtil.nextInt(random(), 1, 98);
+    biasedMixOfRandomUpdates(ADD_CUTOFF, TestUtil.nextInt(random(), ADD_CUTOFF+1, 99),
+                             TestUtil.nextInt(random(), 0, 4), // rarely delete 
+                             TestUtil.nextInt(random(), 0, 2), // rarer commit
+                             -1000L, 1000L, 100); // narrow range of doc values
+  }
 
-    Directory dir = newDirectory();
-    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
+  /** 
+   * Thin wrapper that hardcodes 
+   * <code>MIN_VALUE=Long.MIN_VALUE, MAX_VALUE=Long.MAX_VALUE, MAX_DEL_RANGE_SIZE=Long.MAX_VALUE/2</code>
+   */
+  public void biasedMixOfRandomUpdates(final int ADD_CUTOFF, final int UPD_CUTOFF,
+                                       final int DELETE_PERCENT, final int COMMIT_PERCENT) throws Exception {
+    biasedMixOfRandomUpdates(ADD_CUTOFF, UPD_CUTOFF,
+                             DELETE_PERCENT, COMMIT_PERCENT,
+                             Long.MIN_VALUE, Long.MAX_VALUE, Long.MAX_VALUE / 2);
+  }
+  
+  /**
+   * <p>
+   * Given <code>ADD_CUTOFF &lt; UPD_CUTOFF</code> executes a random sequence of Add, Update, or UpdateDV 
+   * operations on documents.  The operations are selected by picking an int in the range 
+   * [1,100] and determining where it falls relative to the cutoffs. 
+   * ie: <code>ADD_CUTOFF=25, UPD_CUTOFF=75</code> means a 25% chance of attempting an ADD, a 50% 
+   * chance of attempting an Update, and a (remaining) 25% change of attempting an UpdateDV.  Adds &amp; 
+   * Updates are done against randomly selected individual docs; UpdateDVs are typically done on randomly 
+   * selected individual docs, but have a small chance of instead being applied to all docs matching a 
+   * randomly selected term in one of the "mod*" fields
+   * </p>
+   * <p>
+   * This method may also attempt a randomly selected range of values to use in a 
+   * <code>deleteDocuments(DocValuesRangeQuery)</code> against the DV value field and/or a commit 
+   * after every op, according to the specified percentages.  The random ranges used for deletions will 
+   * tend to include at least one doc
+   * </p>
+   * <p>
+   * The Caller can also specify the <code>MIN_VALUE</code> and <code>MAX_VALUE</code> to be used when 
+   * randomly generating values for the DV field, as well as a <code>MAX_DEL_RANGE_SIZE</code> for the 
+   * (max) size of the <code>delete(Query)</code> to be executed.   The smaller the difference between 
+   * <code>MIN_VALUE</code> and <code>MAX_VALUE</code>, and/or the larger the value of 
+   * <code>MAX_DEL_RANGE_SIZE</code>, the greater the chance that the ranges match multiple docs -- 
+   * but obviously as the odds of deleting multiple docs increases, the odds of any single doc getting 
+   * multiple DV updates decreases 
+   * </p>
+   * 
+   * @param ADD_CUTOFF cutoff value to use for selecting an add if rand[1,100] less than this
+   * @param UPD_CUTOFF cutoff value to use for selecting update vs updateDV if rand[1,100] greater than <code>ADD_CUTOFF</code> but less/greater than this
+   * @param DELETE_PERCENT how often to attempt a random delete after any operation
+   * @param COMMIT_PERCENT how often to attempt a commit after any operation
+   * @param MIN_VALUE smallest value to use in the DV field
+   * @param MIN_VALUE largest value to use in the DV field
+   * @param MAX_DEL_RANGE_SIZE the maximum range size for use in the <code>deleteDocuments(DocValuesRangeQuery)</code>
+   * @see #MOD_DIVISORS
+   */
+  private void biasedMixOfRandomUpdates(final int ADD_CUTOFF, final int UPD_CUTOFF,
+                                        final int DELETE_PERCENT, final int COMMIT_PERCENT,
+                                        final long MIN_VALUE, final long MAX_VALUE,
+                                        final long MAX_DEL_RANGE_SIZE) throws Exception {
 
-    IndexWriter writer = new IndexWriter(dir, conf);
+    // sanity check usage
+    assert 0 <= ADD_CUTOFF : ADD_CUTOFF;
+    assert ADD_CUTOFF <= UPD_CUTOFF : ADD_CUTOFF + " < " + UPD_CUTOFF;
+    assert UPD_CUTOFF <= 100 : UPD_CUTOFF;
+    assert MIN_VALUE < MAX_VALUE :  MIN_VALUE + " < " + MAX_VALUE;
+    assert MIN_VALUE + MAX_DEL_RANGE_SIZE <= MAX_VALUE
+      : MIN_VALUE + " + " + MAX_DEL_RANGE_SIZE + " <= " + MAX_VALUE;
+
+    final Directory dir = newDirectory();
+    final IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
+
+    final IndexWriter writer = new IndexWriter(dir, conf);
+    final AtomicLong seq = new AtomicLong(writer.getMaxCompletedSequenceNumber());
     
     final int numOperations = atLeast(1000);
-    final Map<Integer,Long> expected = new HashMap<>(numOperations / 3);
-
     // start with at least one doc before any chance of updates
-    final int numSeedDocs = atLeast(1); 
+    final int numSeedDocs = atLeast(1);
+    // expected value for each doc, null if doc should not exist
+    final Long[] expected = new Long[numOperations + numSeedDocs];
+
     for (int i = 0; i < numSeedDocs; i++) {
-      final long val = random().nextLong();
-      expected.put(i, val);
-      writer.addDocument(doc(i, val));
+      final long val = TestUtil.nextLong(random(), MIN_VALUE, MAX_VALUE);
+      System.out.println("SEED id=" + i + " val=" + val);
+      expected[i] = val;
+      assertSeq(seq, writer.addDocument(doc(i, val)));
     }
+    
+    final AtomicInteger maxDoc = new AtomicInteger(numSeedDocs-1);
 
-    int numDocUpdates = 0;
-    int numValueUpdates = 0;
-
-    //System.out.println("TEST: numOperations=" + numOperations + " ADD_CUTOFF=" + ADD_CUTOFF + " UPD_CUTOFF=" + UPD_CUTOFF);
+    System.out.println("TEST: numOperations=" + numOperations + " ADD_CUTOFF=" + ADD_CUTOFF + " UPD_CUTOFF=" + UPD_CUTOFF + " DELETE_PERCENT=" + DELETE_PERCENT + " COMMIT_PERCENT=" + COMMIT_PERCENT);
 
     for (int i = 0; i < numOperations; i++) {
       final int op = TestUtil.nextInt(random(), 1, 100);
-      final long val = random().nextLong();
+      final long val = TestUtil.nextLong(random(), MIN_VALUE, MAX_VALUE);
       if (op <= ADD_CUTOFF) {
-        final int id = expected.size();
-        //System.out.println("TEST i=" + i + ": addDocument id=" + id + " val=" + val);
-        expected.put(id, val);
-        writer.addDocument(doc(id, val));
+        final int id = maxDoc.incrementAndGet();
+        System.out.println("TEST i=" + i + ": addDocument id=" + id + " val=" + val);
+        expected[id] = val;
+        assertSeq(seq, writer.addDocument(doc(id, val)));
       } else {
-        final int id = TestUtil.nextInt(random(), 0, expected.size()-1);
-        expected.put(id, val);
         if (op <= UPD_CUTOFF) {
-          numDocUpdates++;
-          //System.out.println("TEST i=" + i + ": updateDocument id=" + id + " val=" + val);
-          writer.updateDocument(new Term("id","doc-" + id), doc(id, val));
-        } else {
-          numValueUpdates++;
-          //System.out.println("TEST i=" + i + ": updateDV id=" + id + " val=" + val);
-          writer.updateNumericDocValue(new Term("id","doc-" + id), "val", val);
+          final int id = TestUtil.nextInt(random(), 0, maxDoc.get());
+          System.out.println("TEST i=" + i + ": updateDocument id=" + id + " val=" + val);
+          expected[id] = val;
+          assertSeq(seq, writer.updateDocument(new Term("id","doc-" + id), doc(id, val)));
+        } else { // attempt DV update
+          if (0 == TestUtil.nextInt(random(), 0, 17)) { // occasionally do a bulk update of DV
+            final int maxId = maxDoc.get();
+            final int divisor = MOD_DIVISORS[random().nextInt(MOD_DIVISORS.length)];
+            final int mod = random().nextInt(divisor);
+            System.out.println("TEST i=" + i + ": updateDV mod"+divisor+"=" + mod + " val=" + val);
+            for (int id = mod; id <= maxId; id += divisor) {
+              if (null != expected[id]) {
+                System.out.println("TEST i=" + i + ":    ...DV  id=" + id + " val=" + val);
+                expected[id] = val;
+              }
+            }
+            assertSeq(seq, writer.updateNumericDocValue(new Term("mod" + divisor,"mod-" + mod), "val", val));
+          } else { // single doc DV update
+            final int id = TestUtil.nextInt(random(), 0, maxDoc.get());
+            if (null == expected[id]) {
+              // don't attempt updateNumericDocValue on a previously deleted document,
+              // DV will be udpated, but doc will remain deleted                         // nocommit: correct?
+              // instead fall back to a regular update
+              System.out.println("TEST i=" + i + ": updateDV/Document id=" + id + " val=" + val);
+              assertSeq(seq, writer.updateDocument(new Term("id","doc-" + id), doc(id, val)));
+            } else {
+              System.out.println("TEST i=" + i + ": updateDV id=" + id + " val=" + val);
+              assertSeq(seq, writer.updateNumericDocValue(new Term("id","doc-" + id), "val", val));
+            }
+            expected[id] = val;
+          }
+        }
+      }
+
+      // after every add/upd, there's a chance for delete and commit
+      if (TestUtil.nextInt(random(), 1, 100) <= DELETE_PERCENT) {
+        // pick a "span" that will be half the range size
+        final long delSpan = TestUtil.nextLong(random(), 0, MAX_DEL_RANGE_SIZE / 2);
+        // pick a doc, if it has a value, create a range around that midpoint...
+        final int sacrafice = TestUtil.nextInt(random(), 0, maxDoc.get());
+        // ...otherwise just choose a random range midpoint.
+        final long delMid = (null != expected[sacrafice]) ? expected[sacrafice] : TestUtil.nextLong(random(), MIN_VALUE, MAX_VALUE);
+
+        final long lo = (delMid < (Long.MIN_VALUE + delSpan)) ? Long.MIN_VALUE : (delMid-delSpan);
+        final long hi = (delMid > (Long.MAX_VALUE - delSpan)) ? Long.MAX_VALUE : (delMid+delSpan);
+        
+        System.out.println("TEST i=" + i + ": delete range: " + lo + " TO " + hi);
+
+        // TODO: make more efficient if max numOperations is going to be increased much
+        for (int id = 0; id <= maxDoc.get(); id++) {
+          final Long v = expected[id];
+          if (null != v) {
+            if (lo <= v && v <= hi) {
+              System.out.println("TEST i=" + i + ": ...del id=" + id + " val=" + v);
+              expected[id] = null;
+            }
+          }
+        }
+        if (FORCE_NEW_READER_HACK_PRE) DirectoryReader.open(writer).close();
+        assertSeq(seq, writer.deleteDocuments(DocValuesRangeQuery.newLongRange("val", lo, hi, true, true)));
+        if (FORCE_NEW_READER_HACK_POST) DirectoryReader.open(writer).close();
+      }
+      if (TestUtil.nextInt(random(), 1, 100) <= COMMIT_PERCENT) {
+        System.out.println("TEST i=" + i + ": commit"); 
+        assertSeq(seq, writer.commit());
+      }
+
+      // every 100 ops, check our current results using an NRT reader
+      if (0 == i % 100) {
+        try (final DirectoryReader reader = DirectoryReader.open(writer)) {
+          final IndexSearcher searcher = new IndexSearcher(reader);
+          assertExpected("TEST i=" + i + ": check: ", expected, searcher); 
         }
       }
     }
 
-    writer.commit();
+    assertSeq(seq, writer.commit());
     
     final DirectoryReader reader = DirectoryReader.open(dir);
     final IndexSearcher searcher = new IndexSearcher(reader);
 
-    // TODO: make more efficient if max numOperations is going to be increased much
-    for (Map.Entry<Integer,Long> expect : expected.entrySet()) {
-      String id = "doc-" + expect.getKey();
+    assertExpected("FINISH: ", expected, searcher);
+    
+    IOUtils.close(reader, writer, dir);
+  }
+
+  // nocommit: jdocs
+  private void assertExpected(final String msgPre, final Long[] expected, final IndexSearcher searcher) throws Exception {
+    
+    // TODO: more efficient check w/o doing a query per doc?
+    for (int i = 0; i < expected.length; i++) {
+      final String id = "doc-" + i;
+      final Long val = expected[i];
+      
       TopFieldDocs td = searcher.search(new TermQuery(new Term("id", id)), 1, 
                                         new Sort(new SortField("val", SortField.Type.LONG)));
-      assertEquals(id + " missing?", 1, td.totalHits);
-      assertEquals(id + " value", expect.getValue(), ((FieldDoc)td.scoreDocs[0]).fields[0]);
+      if (null == val) {
+        assertEquals(msgPre + id + " found, should be missing", 0, td.totalHits);
+      } else {
+        assertEquals(msgPre + id + " missing?", 1, td.totalHits);
+        assertEquals(msgPre + id + " value", val, ((FieldDoc)td.scoreDocs[0]).fields[0]);
+      }
     }
-    
-    IOUtils.close(reader, writer, dir);
   }
 
+  /** 
+   * Asserts that the <code>newSeq</code> is either <code>-1</code> or is greater then the 
+   * existing <code>seq</code> value.  In the later case, the <code>seq</code> value is 
+   * updated to <code>newSeq</code>.
+   *
+   * @return <code>newSeq</code>
+   */
+  private long assertSeq(final AtomicLong seq, final long newSeq) {
+    
+    if (-1 == newSeq) return newSeq;
+    
+    final long old = seq.getAndSet(newSeq);
+    assertTrue("Got new seq=" + newSeq + " which is not greater then old seq=" + old,
+               old < newSeq);
+    return newSeq;
+  }
+  
   
   @Test
   public void testUpdatesAreFlushed() throws IOException {
@@ -334,7 +551,7 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
     // update docs 3 and 5
     writer.updateNumericDocValue(new Term("id", "doc-3"), "val", 17L);
     writer.updateNumericDocValue(new Term("id", "doc-5"), "val", 17L);
-    
+
     final DirectoryReader reader;
     if (random().nextBoolean()) { // not NRT
       writer.close();
@@ -396,6 +613,90 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
   }
 
   @Test
+  public void testDeleteUsingUpdatedValue() throws Exception {
+    // update a value, and then use the updated value to delete that document
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
+    conf.setMaxBufferedDocs(10); // control segment flushing
+    IndexWriter writer = new IndexWriter(dir, conf);
+
+    writer.addDocument(doc(0));
+    writer.addDocument(doc(1));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    writer.updateNumericDocValue(new Term("id", "doc-1"), "val", 17L);
+    
+    if (FORCE_NEW_READER_HACK_PRE) DirectoryReader.open(writer).close();
+    writer.deleteDocuments(DocValuesRangeQuery.newLongRange("val", 17L, 17L, true, true));
+    if (FORCE_NEW_READER_HACK_POST) DirectoryReader.open(writer).close();
+
+    final DirectoryReader reader;
+    if (random().nextBoolean()) { // not NRT
+      writer.close();
+      reader = DirectoryReader.open(dir);
+    } else { // NRT
+      reader = DirectoryReader.open(writer);
+      writer.close();
+    }
+
+    LeafReader r = reader.leaves().get(0).reader();
+    assertNotNull("There must've been deleted documents.", r.getLiveDocs());
+    assertFalse("Document 1 should have been deleted.", r.getLiveDocs().get(1));
+
+    IOUtils.close(reader, dir);
+  }
+  
+  @Test
+  public void testDeleteFollwedByUpdateOfDeletedValue() throws Exception {
+    // update a value, and then use the updated value to delete that document
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
+    conf.setMaxBufferedDocs(10); // control segment flushing
+    IndexWriter writer = new IndexWriter(dir, conf);
+
+    writer.addDocument(doc(0));
+    writer.addDocument(doc(1));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    writer.updateNumericDocValue(new Term("id", "doc-1"), "val",  7L);
+    writer.updateNumericDocValue(new Term("id", "doc-0"), "val", 17L);
+    
+    if (FORCE_NEW_READER_HACK_PRE) DirectoryReader.open(writer).close();
+    writer.deleteDocuments(DocValuesRangeQuery.newLongRange("val", 5L, 10L, true, true));
+    if (FORCE_NEW_READER_HACK_POST) DirectoryReader.open(writer).close();
+    
+    writer.updateNumericDocValue(new Term("id", "doc-0"), "val",  7L);
+    
+    final DirectoryReader reader;
+    if (random().nextBoolean()) { // not NRT
+      writer.close();
+      reader = DirectoryReader.open(dir);
+    } else { // NRT
+      reader = DirectoryReader.open(writer);
+      writer.close();
+    }
+
+    LeafReader r = reader.leaves().get(0).reader();
+    assertNotNull("There must've been deleted documents.", r.getLiveDocs());
+    assertFalse("Document 1 should have been deleted.", r.getLiveDocs().get(1));
+    assertTrue("Document 0 should not have been deleted.", r.getLiveDocs().get(0));
+    
+    final IndexSearcher searcher = new IndexSearcher(reader);
+    TopFieldDocs td = searcher.search(new TermQuery(new Term("id", "doc-0")), 1, 
+                                      new Sort(new SortField("val", SortField.Type.LONG)));
+    assertEquals("doc-0 missing?", 1, td.scoreDocs.length);
+    assertEquals("doc-0 value", 7L, ((FieldDoc)td.scoreDocs[0]).fields[0]);
+
+    IOUtils.close(reader, dir);
+  }
+
+  @Test
   public void testMultipleDocValuesTypes() throws Exception {
     Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
