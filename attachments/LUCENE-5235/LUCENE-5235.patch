Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordTokenizer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordTokenizer.java	(revision 1525305)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordTokenizer.java	(working copy)
@@ -88,6 +88,7 @@
 
   @Override
   public void reset() throws IOException {
+    super.reset();
     this.done = false;
   }
 }
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43NGramTokenizer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43NGramTokenizer.java	(revision 1525305)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43NGramTokenizer.java	(working copy)
@@ -140,7 +140,8 @@
   }
   
   @Override
-  public void end() {
+  public void end() throws IOException {
+    super.end();
     // set final offset
     final int finalOffset = correctOffset(charsRead);
     this.offsetAtt.setOffset(finalOffset, finalOffset);
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternTokenizer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternTokenizer.java	(revision 1525305)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternTokenizer.java	(working copy)
@@ -138,6 +138,7 @@
 
   @Override
   public void reset() throws IOException {
+    super.reset();
     fillBuffer(str, input);
     matcher.reset(str);
     index = 0;
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizer.java	(revision 1525305)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizer.java	(working copy)
@@ -114,7 +114,7 @@
   }
 
   private void init(Version matchVersion) {
-    this.scanner = new ClassicTokenizerImpl(null); // best effort NPE if you dont call reset
+    this.scanner = new ClassicTokenizerImpl(input);
   }
 
   // this tokenizer generates three attributes:
@@ -170,9 +170,16 @@
     // adjust any skipped tokens
     posIncrAtt.setPositionIncrement(posIncrAtt.getPositionIncrement()+skippedPositions);
   }
+  
+  @Override
+  public void close() throws IOException {
+    super.close();
+    scanner.yyreset(input);
+  }
 
   @Override
   public void reset() throws IOException {
+    super.reset();
     scanner.yyreset(input);
     skippedPositions = 0;
   }
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java	(revision 1525305)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java	(working copy)
@@ -128,7 +128,7 @@
   }
 
   private final void init(Version matchVersion) {
-    this.scanner = new StandardTokenizerImpl(null); // best effort NPE if you dont call reset
+    this.scanner = new StandardTokenizerImpl(input);
   }
 
   // this tokenizer generates three attributes:
@@ -180,7 +180,14 @@
   }
 
   @Override
+  public void close() throws IOException {
+    super.close();
+    scanner.yyreset(input);
+  }
+
+  @Override
   public void reset() throws IOException {
+    super.reset();
     scanner.yyreset(input);
     skippedPositions = 0;
   }
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.java	(revision 1525305)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.java	(working copy)
@@ -111,8 +111,8 @@
     this.scanner = getScannerFor(matchVersion);
   }
 
-  private static StandardTokenizerInterface getScannerFor(Version matchVersion) {
-    return new UAX29URLEmailTokenizerImpl(null); // best effort NPE if you dont call reset
+  private StandardTokenizerInterface getScannerFor(Version matchVersion) {
+    return new UAX29URLEmailTokenizerImpl(input);
   }
 
   // this tokenizer generates three attributes:
@@ -157,9 +157,16 @@
     // adjust any skipped tokens
     posIncrAtt.setPositionIncrement(posIncrAtt.getPositionIncrement()+skippedPositions);
   }
+  
+  @Override
+  public void close() throws IOException {
+    super.close();
+    scanner.yyreset(input);
+  }
 
   @Override
   public void reset() throws IOException {
+    super.reset();
     scanner.yyreset(input);
     skippedPositions = 0;
   }
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java	(revision 1525305)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java	(working copy)
@@ -62,8 +62,7 @@
     charUtils = CharacterUtils.getInstance(matchVersion);
   }
   
-  // note: bufferIndex is -1 here to best-effort AIOOBE consumers that don't call reset()
-  private int offset = 0, bufferIndex = -1, dataLen = 0, finalOffset = 0;
+  private int offset = 0, bufferIndex = 0, dataLen = 0, finalOffset = 0;
   private static final int MAX_WORD_LEN = 255;
   private static final int IO_BUFFER_SIZE = 4096;
   
@@ -150,6 +149,7 @@
 
   @Override
   public void reset() throws IOException {
+    super.reset();
     bufferIndex = 0;
     offset = 0;
     dataLen = 0;
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizer.java	(revision 1525305)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizer.java	(working copy)
@@ -143,7 +143,7 @@
    */
   public WikipediaTokenizer(Reader input, int tokenOutput, Set<String> untokenizedTypes) {
     super(input);
-    this.scanner = new WikipediaTokenizerImpl(null); // best effort NPE if you dont call reset
+    this.scanner = new WikipediaTokenizerImpl(this.input);
     init(tokenOutput, untokenizedTypes);
   }
 
@@ -156,7 +156,7 @@
    */
   public WikipediaTokenizer(AttributeFactory factory, Reader input, int tokenOutput, Set<String> untokenizedTypes) {
     super(factory, input);
-    this.scanner = new WikipediaTokenizerImpl(null); // best effort NPE if you dont call reset
+    this.scanner = new WikipediaTokenizerImpl(this.input);
     init(tokenOutput, untokenizedTypes);
   }
   
@@ -295,6 +295,12 @@
     offsetAtt.setOffset(correctOffset(start), correctOffset(start + termAtt.length()));
   }
 
+  @Override
+  public void close() throws IOException {
+    super.close();
+    scanner.yyreset(input);
+  }
+
   /*
   * (non-Javadoc)
   *
@@ -302,6 +308,7 @@
   */
   @Override
   public void reset() throws IOException {
+    super.reset();
     scanner.yyreset(input);
     tokens = null;
     scanner.reset();
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java	(working copy)
@@ -60,8 +60,8 @@
    */
   public void testReusableTokenStream() throws Exception {
     ArabicAnalyzer a = new ArabicAnalyzer(TEST_VERSION_CURRENT);
-    assertAnalyzesToReuse(a, "كبير", new String[] { "كبير" });
-    assertAnalyzesToReuse(a, "كبيرة", new String[] { "كبير" }); // feminine marker
+    assertAnalyzesTo(a, "كبير", new String[] { "كبير" });
+    assertAnalyzesTo(a, "كبيرة", new String[] { "كبير" }); // feminine marker
   }
 
   /**
@@ -86,12 +86,12 @@
     CharArraySet set = new CharArraySet(TEST_VERSION_CURRENT, asSet("ساهدهات"), false);
     ArabicAnalyzer a = new ArabicAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET, set);
     assertAnalyzesTo(a, "كبيرة the quick ساهدهات", new String[] { "كبير","the", "quick", "ساهدهات" });
-    assertAnalyzesToReuse(a, "كبيرة the quick ساهدهات", new String[] { "كبير","the", "quick", "ساهدهات" });
+    assertAnalyzesTo(a, "كبيرة the quick ساهدهات", new String[] { "كبير","the", "quick", "ساهدهات" });
 
     
     a = new ArabicAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET, CharArraySet.EMPTY_SET);
     assertAnalyzesTo(a, "كبيرة the quick ساهدهات", new String[] { "كبير","the", "quick", "ساهد" });
-    assertAnalyzesToReuse(a, "كبيرة the quick ساهدهات", new String[] { "كبير","the", "quick", "ساهد" });
+    assertAnalyzesTo(a, "كبيرة the quick ساهدهات", new String[] { "كبير","the", "quick", "ساهد" });
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java	(working copy)
@@ -102,7 +102,7 @@
         return new TokenStreamComponents(tokenizer, new ArabicNormalizationFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java	(working copy)
@@ -142,6 +142,6 @@
         return new TokenStreamComponents(tokenizer, new ArabicStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianAnalyzer.java	(working copy)
@@ -49,8 +49,8 @@
   
   public void testReusableTokenStream() throws IOException {
     Analyzer a = new BulgarianAnalyzer(TEST_VERSION_CURRENT);
-    assertAnalyzesToReuse(a, "документи", new String[] {"документ"});
-    assertAnalyzesToReuse(a, "документ", new String[] {"документ"});
+    assertAnalyzesTo(a, "документи", new String[] {"документ"});
+    assertAnalyzesTo(a, "документ", new String[] {"документ"});
   }
   
   /**
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java	(working copy)
@@ -234,6 +234,6 @@
         return new TokenStreamComponents(tokenizer, new BulgarianStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java	(working copy)
@@ -157,7 +157,7 @@
   }
   
   private void checkReuse(Analyzer a, String input, String expected) throws Exception {
-    checkOneTermReuse(a, input, expected);
+    checkOneTerm(a, input, expected);
   }
 
   /** blast some random strings through the analyzer */
@@ -173,6 +173,6 @@
         return new TokenStreamComponents(tokenizer, new BrazilianStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ca/TestCatalanAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ca/TestCatalanAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ca/TestCatalanAnalyzer.java	(working copy)
@@ -34,8 +34,8 @@
   public void testBasics() throws IOException {
     Analyzer a = new CatalanAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "llengües", "llengu");
-    checkOneTermReuse(a, "llengua", "llengu");
+    checkOneTerm(a, "llengües", "llengu");
+    checkOneTerm(a, "llengua", "llengu");
     // stopword
     assertAnalyzesTo(a, "un", new String[] { });
   }
@@ -52,8 +52,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("llengües"), false);
     Analyzer a = new CatalanAnalyzer(TEST_VERSION_CURRENT, 
         CatalanAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "llengües", "llengües");
-    checkOneTermReuse(a, "llengua", "llengu");
+    checkOneTerm(a, "llengües", "llengües");
+    checkOneTerm(a, "llengua", "llengu");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKAnalyzer.java	(working copy)
@@ -167,14 +167,14 @@
   }
   
   public void testReusableTokenStream() throws IOException {
-    assertAnalyzesToReuse(analyzer, "あいうえおabcかきくけこ",
+    assertAnalyzesTo(analyzer, "あいうえおabcかきくけこ",
         new String[] { "あい", "いう", "うえ", "えお", "abc", "かき", "きく", "くけ", "けこ" },
         new int[] { 0, 1, 2, 3, 5,  8,  9, 10, 11 },
         new int[] { 2, 3, 4, 5, 8, 10, 11, 12, 13 },
         new String[] { "<DOUBLE>", "<DOUBLE>", "<DOUBLE>", "<DOUBLE>", "<ALPHANUM>", "<DOUBLE>", "<DOUBLE>", "<DOUBLE>", "<DOUBLE>" },
         new int[] { 1, 1, 1, 1, 1,  1,  1,  1,  1});
     
-    assertAnalyzesToReuse(analyzer, "あいうえおabんcかきくけ こ",
+    assertAnalyzesTo(analyzer, "あいうえおabんcかきくけ こ",
         new String[] { "あい", "いう", "うえ", "えお", "ab", "ん", "c", "かき", "きく", "くけ", "こ" },
         new int[] { 0, 1, 2, 3, 5, 7, 8,  9, 10, 11, 14 },
         new int[] { 2, 3, 4, 5, 7, 8, 9, 11, 12, 13, 15 },
@@ -288,6 +288,6 @@
         return new TokenStreamComponents(tokenizer, new CJKBigramFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKWidthFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKWidthFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKWidthFilter.java	(working copy)
@@ -74,6 +74,6 @@
         return new TokenStreamComponents(tokenizer, new CJKWidthFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java	(working copy)
@@ -377,7 +377,7 @@
         return new TokenStreamComponents(tokenizer, new DictionaryCompoundWordTokenFilter(TEST_VERSION_CURRENT, tokenizer, dict));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
     
     InputSource is = new InputSource(getClass().getResource("da_UTF8.xml").toExternalForm());
     final HyphenationTree hyphenator = HyphenationCompoundWordTokenFilter.getHyphenationTree(is);
@@ -390,6 +390,6 @@
         return new TokenStreamComponents(tokenizer, filter);
       }
     };
-    checkOneTermReuse(b, "", "");
+    checkOneTerm(b, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java	(working copy)
@@ -39,8 +39,8 @@
   
   public void testReusableTokenStream() throws Exception {
     Analyzer analyzer = new CzechAnalyzer(TEST_VERSION_CURRENT);
-    assertAnalyzesToReuse(analyzer, "Pokud mluvime o volnem", new String[] { "mluvim", "voln" });
-    assertAnalyzesToReuse(analyzer, "Česká Republika", new String[] { "česk", "republik" });
+    assertAnalyzesTo(analyzer, "Pokud mluvime o volnem", new String[] { "mluvim", "voln" });
+    assertAnalyzesTo(analyzer, "Česká Republika", new String[] { "česk", "republik" });
   }
 
   public void testWithStemExclusionSet() throws IOException{
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java	(working copy)
@@ -294,7 +294,7 @@
         return new TokenStreamComponents(tokenizer, new CzechStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
   
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/da/TestDanishAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/da/TestDanishAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/da/TestDanishAnalyzer.java	(working copy)
@@ -34,8 +34,8 @@
   public void testBasics() throws IOException {
     Analyzer a = new DanishAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "undersøg", "undersøg");
-    checkOneTermReuse(a, "undersøgelse", "undersøg");
+    checkOneTerm(a, "undersøg", "undersøg");
+    checkOneTerm(a, "undersøgelse", "undersøg");
     // stopword
     assertAnalyzesTo(a, "på", new String[] {});
   }
@@ -45,8 +45,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("undersøgelse"), false);
     Analyzer a = new DanishAnalyzer(TEST_VERSION_CURRENT, 
         DanishAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "undersøgelse", "undersøgelse");
-    checkOneTermReuse(a, "undersøg", "undersøg");
+    checkOneTerm(a, "undersøgelse", "undersøgelse");
+    checkOneTerm(a, "undersøg", "undersøg");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java	(working copy)
@@ -29,9 +29,9 @@
 public class TestGermanAnalyzer extends BaseTokenStreamTestCase {
   public void testReusableTokenStream() throws Exception {
     Analyzer a = new GermanAnalyzer(TEST_VERSION_CURRENT);
-    checkOneTermReuse(a, "Tisch", "tisch");
-    checkOneTermReuse(a, "Tische", "tisch");
-    checkOneTermReuse(a, "Tischen", "tisch");
+    checkOneTerm(a, "Tisch", "tisch");
+    checkOneTerm(a, "Tische", "tisch");
+    checkOneTerm(a, "Tischen", "tisch");
   }
   
   public void testWithKeywordAttribute() throws IOException {
@@ -46,7 +46,7 @@
   public void testStemExclusionTable() throws Exception {
     GermanAnalyzer a = new GermanAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET, 
         new CharArraySet(TEST_VERSION_CURRENT, asSet("tischen"), false));
-    checkOneTermReuse(a, "tischen", "tischen");
+    checkOneTerm(a, "tischen", "tischen");
   }
   
   /** test some features of the new snowball filter
@@ -55,8 +55,8 @@
   public void testGermanSpecials() throws Exception {
     GermanAnalyzer a = new GermanAnalyzer(TEST_VERSION_CURRENT);
     // a/o/u + e is equivalent to the umlaut form
-    checkOneTermReuse(a, "Schaltflächen", "schaltflach");
-    checkOneTermReuse(a, "Schaltflaechen", "schaltflach");
+    checkOneTerm(a, "Schaltflächen", "schaltflach");
+    checkOneTerm(a, "Schaltflaechen", "schaltflach");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanLightStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanLightStemFilter.java	(working copy)
@@ -75,6 +75,6 @@
         return new TokenStreamComponents(tokenizer, new GermanLightStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanMinimalStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanMinimalStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanMinimalStemFilter.java	(working copy)
@@ -87,6 +87,6 @@
         return new TokenStreamComponents(tokenizer, new GermanMinimalStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanNormalizationFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanNormalizationFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanNormalizationFilter.java	(working copy)
@@ -75,6 +75,6 @@
         return new TokenStreamComponents(tokenizer, new GermanNormalizationFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java	(working copy)
@@ -88,6 +88,6 @@
         return new TokenStreamComponents(tokenizer, new GermanStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java	(working copy)
@@ -51,16 +51,16 @@
     Analyzer a = new GreekAnalyzer(TEST_VERSION_CURRENT);
     // Verify the correct analysis of capitals and small accented letters, and
     // stemming
-    assertAnalyzesToReuse(a, "Μία εξαιρετικά καλή και πλούσια σειρά χαρακτήρων της Ελληνικής γλώσσας",
+    assertAnalyzesTo(a, "Μία εξαιρετικά καλή και πλούσια σειρά χαρακτήρων της Ελληνικής γλώσσας",
         new String[] { "μια", "εξαιρετ", "καλ", "πλουσ", "σειρ", "χαρακτηρ",
         "ελληνικ", "γλωσσ" });
     // Verify the correct analysis of small letters with diaeresis and the elimination
     // of punctuation marks
-    assertAnalyzesToReuse(a, "Προϊόντα (και)     [πολλαπλές] - ΑΝΑΓΚΕΣ",
+    assertAnalyzesTo(a, "Προϊόντα (και)     [πολλαπλές] - ΑΝΑΓΚΕΣ",
         new String[] { "προιοντ", "πολλαπλ", "αναγκ" });
     // Verify the correct analysis of capital accented letters and capital letters with diaeresis,
     // as well as the elimination of stop words
-    assertAnalyzesToReuse(a, "ΠΡΟΫΠΟΘΕΣΕΙΣ  Άψογος, ο μεστός και οι άλλοι",
+    assertAnalyzesTo(a, "ΠΡΟΫΠΟΘΕΣΕΙΣ  Άψογος, ο μεστός και οι άλλοι",
         new String[] { "προυποθεσ", "αψογ", "μεστ", "αλλ" });
   }
   
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/el/TestGreekStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/el/TestGreekStemmer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/el/TestGreekStemmer.java	(working copy)
@@ -536,6 +536,6 @@
         return new TokenStreamComponents(tokenizer, new GreekStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestEnglishAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestEnglishAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestEnglishAnalyzer.java	(working copy)
@@ -34,14 +34,14 @@
   public void testBasics() throws IOException {
     Analyzer a = new EnglishAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "books", "book");
-    checkOneTermReuse(a, "book", "book");
+    checkOneTerm(a, "books", "book");
+    checkOneTerm(a, "book", "book");
     // stopword
     assertAnalyzesTo(a, "the", new String[] {});
     // possessive removal
-    checkOneTermReuse(a, "steven's", "steven");
-    checkOneTermReuse(a, "steven\u2019s", "steven");
-    checkOneTermReuse(a, "steven\uFF07s", "steven");
+    checkOneTerm(a, "steven's", "steven");
+    checkOneTerm(a, "steven\u2019s", "steven");
+    checkOneTerm(a, "steven\uFF07s", "steven");
   }
   
   /** test use of exclusion set */
@@ -49,8 +49,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("books"), false);
     Analyzer a = new EnglishAnalyzer(TEST_VERSION_CURRENT, 
         EnglishAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "books", "books");
-    checkOneTermReuse(a, "book", "book");
+    checkOneTerm(a, "books", "books");
+    checkOneTerm(a, "book", "book");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestEnglishMinimalStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestEnglishMinimalStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestEnglishMinimalStemFilter.java	(working copy)
@@ -65,6 +65,6 @@
         return new TokenStreamComponents(tokenizer, new EnglishMinimalStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestKStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestKStemmer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestKStemmer.java	(working copy)
@@ -62,7 +62,7 @@
         return new TokenStreamComponents(tokenizer, new KStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 
   /****** requires original java kstem source code to create map
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestPorterStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestPorterStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestPorterStemFilter.java	(working copy)
@@ -74,6 +74,6 @@
         return new TokenStreamComponents(tokenizer, new PorterStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/es/TestSpanishAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/es/TestSpanishAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/es/TestSpanishAnalyzer.java	(working copy)
@@ -34,8 +34,8 @@
   public void testBasics() throws IOException {
     Analyzer a = new SpanishAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "chicana", "chican");
-    checkOneTermReuse(a, "chicano", "chican");
+    checkOneTerm(a, "chicana", "chican");
+    checkOneTerm(a, "chicano", "chican");
     // stopword
     assertAnalyzesTo(a, "los", new String[] {});
   }
@@ -45,8 +45,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("chicano"), false);
     Analyzer a = new SpanishAnalyzer(TEST_VERSION_CURRENT, 
         SpanishAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "chicana", "chican");
-    checkOneTermReuse(a, "chicano", "chicano");
+    checkOneTerm(a, "chicana", "chican");
+    checkOneTerm(a, "chicano", "chicano");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/es/TestSpanishLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/es/TestSpanishLightStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/es/TestSpanishLightStemFilter.java	(working copy)
@@ -59,6 +59,6 @@
         return new TokenStreamComponents(tokenizer, new SpanishLightStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/eu/TestBasqueAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/eu/TestBasqueAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/eu/TestBasqueAnalyzer.java	(working copy)
@@ -34,8 +34,8 @@
   public void testBasics() throws IOException {
     Analyzer a = new BasqueAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "zaldi", "zaldi");
-    checkOneTermReuse(a, "zaldiak", "zaldi");
+    checkOneTerm(a, "zaldi", "zaldi");
+    checkOneTerm(a, "zaldiak", "zaldi");
     // stopword
     assertAnalyzesTo(a, "izan", new String[] { });
   }
@@ -45,8 +45,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("zaldiak"), false);
     Analyzer a = new BasqueAnalyzer(TEST_VERSION_CURRENT, 
         BasqueAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "zaldiak", "zaldiak");
-    checkOneTermReuse(a, "mendiari", "mendi");
+    checkOneTerm(a, "zaldiak", "zaldiak");
+    checkOneTerm(a, "mendiari", "mendi");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianAnalyzer.java	(working copy)
@@ -208,8 +208,8 @@
    */
   public void testReusableTokenStream() throws Exception {
     Analyzer a = new PersianAnalyzer(TEST_VERSION_CURRENT);
-    assertAnalyzesToReuse(a, "خورده مي شده بوده باشد", new String[] { "خورده" });
-    assertAnalyzesToReuse(a, "برگ‌ها", new String[] { "برگ" });
+    assertAnalyzesTo(a, "خورده مي شده بوده باشد", new String[] { "خورده" });
+    assertAnalyzesTo(a, "برگ‌ها", new String[] { "برگ" });
   }
   
   /**
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java	(working copy)
@@ -72,7 +72,7 @@
         return new TokenStreamComponents(tokenizer, new PersianNormalizationFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/fi/TestFinnishAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/fi/TestFinnishAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/fi/TestFinnishAnalyzer.java	(working copy)
@@ -34,8 +34,8 @@
   public void testBasics() throws IOException {
     Analyzer a = new FinnishAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "edeltäjiinsä", "edeltäj");
-    checkOneTermReuse(a, "edeltäjistään", "edeltäj");
+    checkOneTerm(a, "edeltäjiinsä", "edeltäj");
+    checkOneTerm(a, "edeltäjistään", "edeltäj");
     // stopword
     assertAnalyzesTo(a, "olla", new String[] {});
   }
@@ -45,8 +45,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("edeltäjistään"), false);
     Analyzer a = new FinnishAnalyzer(TEST_VERSION_CURRENT, 
         FinnishAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "edeltäjiinsä", "edeltäj");
-    checkOneTermReuse(a, "edeltäjistään", "edeltäjistään");
+    checkOneTerm(a, "edeltäjiinsä", "edeltäj");
+    checkOneTerm(a, "edeltäjistään", "edeltäjistään");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/fi/TestFinnishLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/fi/TestFinnishLightStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/fi/TestFinnishLightStemFilter.java	(working copy)
@@ -75,6 +75,6 @@
         return new TokenStreamComponents(tokenizer, new FinnishLightStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java	(working copy)
@@ -117,13 +117,13 @@
   public void testReusableTokenStream() throws Exception {
     FrenchAnalyzer fa = new FrenchAnalyzer(TEST_VERSION_CURRENT);
     // stopwords
-      assertAnalyzesToReuse(
+      assertAnalyzesTo(
           fa,
           "le la chien les aux chat du des à cheval",
           new String[] { "chien", "chat", "cheval" });
 
       // some nouns and adjectives
-      assertAnalyzesToReuse(
+      assertAnalyzesTo(
           fa,
           "lances chismes habitable chiste éléments captifs",
           new String[] {
@@ -140,7 +140,7 @@
     set.add("habitable");
     FrenchAnalyzer fa = new FrenchAnalyzer(TEST_VERSION_CURRENT,
         CharArraySet.EMPTY_SET, set);
-    assertAnalyzesToReuse(fa, "habitable chiste", new String[] { "habitable",
+    assertAnalyzesTo(fa, "habitable chiste", new String[] { "habitable",
         "chist" });
 
     fa = new FrenchAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET, set);
@@ -169,7 +169,7 @@
   /** test accent-insensitive */
   public void testAccentInsensitive() throws Exception {
     Analyzer a = new FrenchAnalyzer(TEST_VERSION_CURRENT);
-    checkOneTermReuse(a, "sécuritaires", "securitair");
-    checkOneTermReuse(a, "securitaires", "securitair");
+    checkOneTerm(a, "sécuritaires", "securitair");
+    checkOneTerm(a, "securitaires", "securitair");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchLightStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchLightStemFilter.java	(working copy)
@@ -205,6 +205,6 @@
         return new TokenStreamComponents(tokenizer, new FrenchLightStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchMinimalStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchMinimalStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchMinimalStemFilter.java	(working copy)
@@ -89,6 +89,6 @@
         return new TokenStreamComponents(tokenizer, new FrenchMinimalStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ga/TestIrishAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ga/TestIrishAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ga/TestIrishAnalyzer.java	(working copy)
@@ -34,8 +34,8 @@
   public void testBasics() throws IOException {
     Analyzer a = new IrishAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "siopadóireacht", "siopadóir");
-    checkOneTermReuse(a, "síceapatacha", "síceapaite");
+    checkOneTerm(a, "siopadóireacht", "siopadóir");
+    checkOneTerm(a, "síceapatacha", "síceapaite");
     // stopword
     assertAnalyzesTo(a, "le", new String[] { });
   }
@@ -52,8 +52,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("feirmeoireacht"), false);
     Analyzer a = new IrishAnalyzer(TEST_VERSION_CURRENT, 
         IrishAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "feirmeoireacht", "feirmeoireacht");
-    checkOneTermReuse(a, "siopadóireacht", "siopadóir");
+    checkOneTerm(a, "feirmeoireacht", "feirmeoireacht");
+    checkOneTerm(a, "siopadóireacht", "siopadóir");
   }
   
   /** test special hyphen handling */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ga/TestIrishLowerCaseFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ga/TestIrishLowerCaseFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ga/TestIrishLowerCaseFilter.java	(working copy)
@@ -52,6 +52,6 @@
         return new TokenStreamComponents(tokenizer, new IrishLowerCaseFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/gl/TestGalicianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/gl/TestGalicianAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/gl/TestGalicianAnalyzer.java	(working copy)
@@ -34,8 +34,8 @@
   public void testBasics() throws IOException {
     Analyzer a = new GalicianAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "correspondente", "correspond");
-    checkOneTermReuse(a, "corresponderá", "correspond");
+    checkOneTerm(a, "correspondente", "correspond");
+    checkOneTerm(a, "corresponderá", "correspond");
     // stopword
     assertAnalyzesTo(a, "e", new String[] {});
   }
@@ -45,8 +45,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("correspondente"), false);
     Analyzer a = new GalicianAnalyzer(TEST_VERSION_CURRENT, 
         GalicianAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "correspondente", "correspondente");
-    checkOneTermReuse(a, "corresponderá", "correspond");
+    checkOneTerm(a, "correspondente", "correspondente");
+    checkOneTerm(a, "corresponderá", "correspond");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/gl/TestGalicianMinimalStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/gl/TestGalicianMinimalStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/gl/TestGalicianMinimalStemFilter.java	(working copy)
@@ -79,6 +79,6 @@
         return new TokenStreamComponents(tokenizer, new GalicianMinimalStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/gl/TestGalicianStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/gl/TestGalicianStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/gl/TestGalicianStemFilter.java	(working copy)
@@ -58,6 +58,6 @@
         return new TokenStreamComponents(tokenizer, new GalicianStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiAnalyzer.java	(working copy)
@@ -34,15 +34,15 @@
   public void testBasics() throws Exception {
     Analyzer a = new HindiAnalyzer(TEST_VERSION_CURRENT);
     // two ways to write 'hindi' itself.
-    checkOneTermReuse(a, "हिन्दी", "हिंद");
-    checkOneTermReuse(a, "हिंदी", "हिंद");
+    checkOneTerm(a, "हिन्दी", "हिंद");
+    checkOneTerm(a, "हिंदी", "हिंद");
   }
   
   public void testExclusionSet() throws Exception {
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("हिंदी"), false);
     Analyzer a = new HindiAnalyzer(TEST_VERSION_CURRENT, 
         HindiAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "हिंदी", "हिंदी");
+    checkOneTerm(a, "हिंदी", "हिंदी");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiNormalizer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiNormalizer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiNormalizer.java	(working copy)
@@ -75,6 +75,6 @@
         return new TokenStreamComponents(tokenizer, new HindiNormalizationFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiStemmer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiStemmer.java	(working copy)
@@ -97,6 +97,6 @@
         return new TokenStreamComponents(tokenizer, new HindiStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/hu/TestHungarianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/hu/TestHungarianAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/hu/TestHungarianAnalyzer.java	(working copy)
@@ -34,8 +34,8 @@
   public void testBasics() throws IOException {
     Analyzer a = new HungarianAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "babakocsi", "babakocs");
-    checkOneTermReuse(a, "babakocsijáért", "babakocs");
+    checkOneTerm(a, "babakocsi", "babakocs");
+    checkOneTerm(a, "babakocsijáért", "babakocs");
     // stopword
     assertAnalyzesTo(a, "által", new String[] {});
   }
@@ -45,8 +45,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("babakocsi"), false);
     Analyzer a = new HungarianAnalyzer(TEST_VERSION_CURRENT, 
         HungarianAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "babakocsi", "babakocsi");
-    checkOneTermReuse(a, "babakocsijáért", "babakocs");
+    checkOneTerm(a, "babakocsi", "babakocsi");
+    checkOneTerm(a, "babakocsijáért", "babakocs");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/hu/TestHungarianLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/hu/TestHungarianLightStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/hu/TestHungarianLightStemFilter.java	(working copy)
@@ -70,6 +70,6 @@
         return new TokenStreamComponents(tokenizer, new HungarianLightStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/hunspell/HunspellStemFilterTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/hunspell/HunspellStemFilterTest.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/hunspell/HunspellStemFilterTest.java	(working copy)
@@ -89,6 +89,6 @@
         return new TokenStreamComponents(tokenizer, new HunspellStemFilter(tokenizer, DICTIONARY, _TestUtil.nextInt(random(), 1, 3)));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/hy/TestArmenianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/hy/TestArmenianAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/hy/TestArmenianAnalyzer.java	(working copy)
@@ -34,8 +34,8 @@
   public void testBasics() throws IOException {
     Analyzer a = new ArmenianAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "արծիվ", "արծ");
-    checkOneTermReuse(a, "արծիվներ", "արծ");
+    checkOneTerm(a, "արծիվ", "արծ");
+    checkOneTerm(a, "արծիվներ", "արծ");
     // stopword
     assertAnalyzesTo(a, "է", new String[] { });
   }
@@ -45,8 +45,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("արծիվներ"), false);
     Analyzer a = new ArmenianAnalyzer(TEST_VERSION_CURRENT, 
         ArmenianAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "արծիվներ", "արծիվներ");
-    checkOneTermReuse(a, "արծիվ", "արծ");
+    checkOneTerm(a, "արծիվներ", "արծիվներ");
+    checkOneTerm(a, "արծիվ", "արծ");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianAnalyzer.java	(working copy)
@@ -34,8 +34,8 @@
   public void testBasics() throws IOException {
     Analyzer a = new IndonesianAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "peledakan", "ledak");
-    checkOneTermReuse(a, "pembunuhan", "bunuh");
+    checkOneTerm(a, "peledakan", "ledak");
+    checkOneTerm(a, "pembunuhan", "bunuh");
     // stopword
     assertAnalyzesTo(a, "bahwa", new String[] {});
   }
@@ -45,8 +45,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("peledakan"), false);
     Analyzer a = new IndonesianAnalyzer(TEST_VERSION_CURRENT, 
         IndonesianAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "peledakan", "peledakan");
-    checkOneTermReuse(a, "pembunuhan", "bunuh");
+    checkOneTerm(a, "peledakan", "peledakan");
+    checkOneTerm(a, "pembunuhan", "bunuh");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianStemmer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianStemmer.java	(working copy)
@@ -41,73 +41,73 @@
   /** Some examples from the paper */
   public void testExamples() throws IOException {
     checkOneTerm(a, "bukukah", "buku");
-    checkOneTermReuse(a, "adalah", "ada");
-    checkOneTermReuse(a, "bukupun", "buku");
-    checkOneTermReuse(a, "bukuku", "buku");
-    checkOneTermReuse(a, "bukumu", "buku");
-    checkOneTermReuse(a, "bukunya", "buku");
-    checkOneTermReuse(a, "mengukur", "ukur");
-    checkOneTermReuse(a, "menyapu", "sapu");
-    checkOneTermReuse(a, "menduga", "duga");
-    checkOneTermReuse(a, "menuduh", "uduh");
-    checkOneTermReuse(a, "membaca", "baca");
-    checkOneTermReuse(a, "merusak", "rusak");
-    checkOneTermReuse(a, "pengukur", "ukur");
-    checkOneTermReuse(a, "penyapu", "sapu");
-    checkOneTermReuse(a, "penduga", "duga");
-    checkOneTermReuse(a, "pembaca", "baca");
-    checkOneTermReuse(a, "diukur", "ukur");
-    checkOneTermReuse(a, "tersapu", "sapu");
-    checkOneTermReuse(a, "kekasih", "kasih");
-    checkOneTermReuse(a, "berlari", "lari");
-    checkOneTermReuse(a, "belajar", "ajar");
-    checkOneTermReuse(a, "bekerja", "kerja");
-    checkOneTermReuse(a, "perjelas", "jelas");
-    checkOneTermReuse(a, "pelajar", "ajar");
-    checkOneTermReuse(a, "pekerja", "kerja");
-    checkOneTermReuse(a, "tarikkan", "tarik");
-    checkOneTermReuse(a, "ambilkan", "ambil");
-    checkOneTermReuse(a, "mengambilkan", "ambil");
-    checkOneTermReuse(a, "makanan", "makan");
-    checkOneTermReuse(a, "janjian", "janji");
-    checkOneTermReuse(a, "perjanjian", "janji");
-    checkOneTermReuse(a, "tandai", "tanda");
-    checkOneTermReuse(a, "dapati", "dapat");
-    checkOneTermReuse(a, "mendapati", "dapat");
-    checkOneTermReuse(a, "pantai", "panta");
+    checkOneTerm(a, "adalah", "ada");
+    checkOneTerm(a, "bukupun", "buku");
+    checkOneTerm(a, "bukuku", "buku");
+    checkOneTerm(a, "bukumu", "buku");
+    checkOneTerm(a, "bukunya", "buku");
+    checkOneTerm(a, "mengukur", "ukur");
+    checkOneTerm(a, "menyapu", "sapu");
+    checkOneTerm(a, "menduga", "duga");
+    checkOneTerm(a, "menuduh", "uduh");
+    checkOneTerm(a, "membaca", "baca");
+    checkOneTerm(a, "merusak", "rusak");
+    checkOneTerm(a, "pengukur", "ukur");
+    checkOneTerm(a, "penyapu", "sapu");
+    checkOneTerm(a, "penduga", "duga");
+    checkOneTerm(a, "pembaca", "baca");
+    checkOneTerm(a, "diukur", "ukur");
+    checkOneTerm(a, "tersapu", "sapu");
+    checkOneTerm(a, "kekasih", "kasih");
+    checkOneTerm(a, "berlari", "lari");
+    checkOneTerm(a, "belajar", "ajar");
+    checkOneTerm(a, "bekerja", "kerja");
+    checkOneTerm(a, "perjelas", "jelas");
+    checkOneTerm(a, "pelajar", "ajar");
+    checkOneTerm(a, "pekerja", "kerja");
+    checkOneTerm(a, "tarikkan", "tarik");
+    checkOneTerm(a, "ambilkan", "ambil");
+    checkOneTerm(a, "mengambilkan", "ambil");
+    checkOneTerm(a, "makanan", "makan");
+    checkOneTerm(a, "janjian", "janji");
+    checkOneTerm(a, "perjanjian", "janji");
+    checkOneTerm(a, "tandai", "tanda");
+    checkOneTerm(a, "dapati", "dapat");
+    checkOneTerm(a, "mendapati", "dapat");
+    checkOneTerm(a, "pantai", "panta");
   }
   
   /** Some detailed analysis examples (that might not be the best) */
   public void testIRExamples() throws IOException {
     checkOneTerm(a, "penyalahgunaan", "salahguna");
-    checkOneTermReuse(a, "menyalahgunakan", "salahguna");
-    checkOneTermReuse(a, "disalahgunakan", "salahguna");
+    checkOneTerm(a, "menyalahgunakan", "salahguna");
+    checkOneTerm(a, "disalahgunakan", "salahguna");
        
-    checkOneTermReuse(a, "pertanggungjawaban", "tanggungjawab");
-    checkOneTermReuse(a, "mempertanggungjawabkan", "tanggungjawab");
-    checkOneTermReuse(a, "dipertanggungjawabkan", "tanggungjawab");
+    checkOneTerm(a, "pertanggungjawaban", "tanggungjawab");
+    checkOneTerm(a, "mempertanggungjawabkan", "tanggungjawab");
+    checkOneTerm(a, "dipertanggungjawabkan", "tanggungjawab");
     
-    checkOneTermReuse(a, "pelaksanaan", "laksana");
-    checkOneTermReuse(a, "pelaksana", "laksana");
-    checkOneTermReuse(a, "melaksanakan", "laksana");
-    checkOneTermReuse(a, "dilaksanakan", "laksana");
+    checkOneTerm(a, "pelaksanaan", "laksana");
+    checkOneTerm(a, "pelaksana", "laksana");
+    checkOneTerm(a, "melaksanakan", "laksana");
+    checkOneTerm(a, "dilaksanakan", "laksana");
     
-    checkOneTermReuse(a, "melibatkan", "libat");
-    checkOneTermReuse(a, "terlibat", "libat");
+    checkOneTerm(a, "melibatkan", "libat");
+    checkOneTerm(a, "terlibat", "libat");
     
-    checkOneTermReuse(a, "penculikan", "culik");
-    checkOneTermReuse(a, "menculik", "culik");
-    checkOneTermReuse(a, "diculik", "culik");
-    checkOneTermReuse(a, "penculik", "culik");
+    checkOneTerm(a, "penculikan", "culik");
+    checkOneTerm(a, "menculik", "culik");
+    checkOneTerm(a, "diculik", "culik");
+    checkOneTerm(a, "penculik", "culik");
     
-    checkOneTermReuse(a, "perubahan", "ubah");
-    checkOneTermReuse(a, "peledakan", "ledak");
-    checkOneTermReuse(a, "penanganan", "tangan");
-    checkOneTermReuse(a, "kepolisian", "polisi");
-    checkOneTermReuse(a, "kenaikan", "naik");
-    checkOneTermReuse(a, "bersenjata", "senjata");
-    checkOneTermReuse(a, "penyelewengan", "seleweng");
-    checkOneTermReuse(a, "kecelakaan", "celaka");
+    checkOneTerm(a, "perubahan", "ubah");
+    checkOneTerm(a, "peledakan", "ledak");
+    checkOneTerm(a, "penanganan", "tangan");
+    checkOneTerm(a, "kepolisian", "polisi");
+    checkOneTerm(a, "kenaikan", "naik");
+    checkOneTerm(a, "bersenjata", "senjata");
+    checkOneTerm(a, "penyelewengan", "seleweng");
+    checkOneTerm(a, "kecelakaan", "celaka");
   }
   
   /* inflectional-only stemming */
@@ -122,15 +122,15 @@
   /** Test stemming only inflectional suffixes */
   public void testInflectionalOnly() throws IOException {
     checkOneTerm(b, "bukunya", "buku");
-    checkOneTermReuse(b, "bukukah", "buku");
-    checkOneTermReuse(b, "bukunyakah", "buku");
-    checkOneTermReuse(b, "dibukukannya", "dibukukan");
+    checkOneTerm(b, "bukukah", "buku");
+    checkOneTerm(b, "bukunyakah", "buku");
+    checkOneTerm(b, "dibukukannya", "dibukukan");
   }
   
   public void testShouldntStem() throws IOException {
     checkOneTerm(a, "bersenjata", "senjata");
-    checkOneTermReuse(a, "bukukah", "buku");
-    checkOneTermReuse(a, "gigi", "gigi");
+    checkOneTerm(a, "bukukah", "buku");
+    checkOneTerm(a, "gigi", "gigi");
   }
   
   public void testEmptyTerm() throws IOException {
@@ -141,6 +141,6 @@
         return new TokenStreamComponents(tokenizer, new IndonesianStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/in/TestIndicNormalizer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/in/TestIndicNormalizer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/in/TestIndicNormalizer.java	(working copy)
@@ -60,6 +60,6 @@
         return new TokenStreamComponents(tokenizer, new IndicNormalizationFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianAnalyzer.java	(working copy)
@@ -37,8 +37,8 @@
   public void testBasics() throws IOException {
     Analyzer a = new ItalianAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "abbandonata", "abbandonat");
-    checkOneTermReuse(a, "abbandonati", "abbandonat");
+    checkOneTerm(a, "abbandonata", "abbandonat");
+    checkOneTerm(a, "abbandonati", "abbandonat");
     // stopword
     assertAnalyzesTo(a, "dallo", new String[] {});
   }
@@ -48,8 +48,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("abbandonata"), false);
     Analyzer a = new ItalianAnalyzer(TEST_VERSION_CURRENT, 
         ItalianAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "abbandonata", "abbandonata");
-    checkOneTermReuse(a, "abbandonati", "abbandonat");
+    checkOneTerm(a, "abbandonata", "abbandonata");
+    checkOneTerm(a, "abbandonati", "abbandonat");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianLightStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianLightStemFilter.java	(working copy)
@@ -59,6 +59,6 @@
         return new TokenStreamComponents(tokenizer, new ItalianLightStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/lv/TestLatvianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/lv/TestLatvianAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/lv/TestLatvianAnalyzer.java	(working copy)
@@ -34,8 +34,8 @@
   public void testBasics() throws IOException {
     Analyzer a = new LatvianAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "tirgiem", "tirg");
-    checkOneTermReuse(a, "tirgus", "tirg");
+    checkOneTerm(a, "tirgiem", "tirg");
+    checkOneTerm(a, "tirgus", "tirg");
     // stopword
     assertAnalyzesTo(a, "un", new String[] {});
   }
@@ -45,8 +45,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("tirgiem"), false);
     Analyzer a = new LatvianAnalyzer(TEST_VERSION_CURRENT, 
         LatvianAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "tirgiem", "tirgiem");
-    checkOneTermReuse(a, "tirgus", "tirg");
+    checkOneTerm(a, "tirgiem", "tirgiem");
+    checkOneTerm(a, "tirgus", "tirg");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/lv/TestLatvianStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/lv/TestLatvianStemmer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/lv/TestLatvianStemmer.java	(working copy)
@@ -278,6 +278,6 @@
         return new TokenStreamComponents(tokenizer, new LatvianStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestASCIIFoldingFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestASCIIFoldingFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestASCIIFoldingFilter.java	(working copy)
@@ -1934,6 +1934,6 @@
         return new TokenStreamComponents(tokenizer, new ASCIIFoldingFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestCapitalizationFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestCapitalizationFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestCapitalizationFilter.java	(working copy)
@@ -143,6 +143,6 @@
         return new TokenStreamComponents(tokenizer, new CapitalizationFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java	(working copy)
@@ -85,6 +85,6 @@
         return new TokenStreamComponents(tokenizer, new HyphenatedWordsFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java	(working copy)
@@ -48,7 +48,7 @@
         return new TokenStreamComponents(tokenizer, new LengthFilter(TEST_VERSION_CURRENT, tokenizer, 0, 5));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestRemoveDuplicatesTokenFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestRemoveDuplicatesTokenFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestRemoveDuplicatesTokenFilter.java	(working copy)
@@ -175,7 +175,7 @@
         return new TokenStreamComponents(tokenizer, new RemoveDuplicatesTokenFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestTrimFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestTrimFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestTrimFilter.java	(working copy)
@@ -115,6 +115,6 @@
         return new TokenStreamComponents(tokenizer, new TrimFilter(version, tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java	(working copy)
@@ -197,7 +197,6 @@
     TokenStream tk = new LetterTokenizer(TEST_VERSION_CURRENT, new StringReader("abc d efgh ij klmno p q"));
     tk = new ShingleFilter(tk);
     tk = new EdgeNGramTokenFilter(TEST_VERSION_CURRENT, tk, 7, 10);
-    tk.reset();
     assertTokenStreamContents(tk,
         new String[] { "efgh ij", "ij klmn", "ij klmno", "klmno p" },
         new int[]    { 6,11,11,14 },
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java	(working copy)
@@ -115,24 +115,24 @@
   
   public void testSnowballCorrectness() throws Exception {
     Analyzer a = new DutchAnalyzer(TEST_VERSION_CURRENT);
-    checkOneTermReuse(a, "opheffen", "opheff");
-    checkOneTermReuse(a, "opheffende", "opheff");
-    checkOneTermReuse(a, "opheffing", "opheff");
+    checkOneTerm(a, "opheffen", "opheff");
+    checkOneTerm(a, "opheffende", "opheff");
+    checkOneTerm(a, "opheffing", "opheff");
   }
   
   public void testReusableTokenStream() throws Exception {
     Analyzer a = new DutchAnalyzer(TEST_VERSION_CURRENT); 
-    checkOneTermReuse(a, "lichaamsziek", "lichaamsziek");
-    checkOneTermReuse(a, "lichamelijk", "licham");
-    checkOneTermReuse(a, "lichamelijke", "licham");
-    checkOneTermReuse(a, "lichamelijkheden", "licham");
+    checkOneTerm(a, "lichaamsziek", "lichaamsziek");
+    checkOneTerm(a, "lichamelijk", "licham");
+    checkOneTerm(a, "lichamelijke", "licham");
+    checkOneTerm(a, "lichamelijkheden", "licham");
   }
   
   public void testExclusionTableViaCtor() throws IOException {
     CharArraySet set = new CharArraySet(TEST_VERSION_CURRENT, 1, true);
     set.add("lichamelijk");
     DutchAnalyzer a = new DutchAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET, set);
-    assertAnalyzesToReuse(a, "lichamelijk lichamelijke", new String[] { "lichamelijk", "licham" });
+    assertAnalyzesTo(a, "lichamelijk lichamelijke", new String[] { "lichamelijk", "licham" });
     
     a = new DutchAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET, set);
     assertAnalyzesTo(a, "lichamelijk lichamelijke", new String[] { "lichamelijk", "licham" });
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/no/TestNorwegianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/no/TestNorwegianAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/no/TestNorwegianAnalyzer.java	(working copy)
@@ -34,8 +34,8 @@
   public void testBasics() throws IOException {
     Analyzer a = new NorwegianAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "havnedistriktene", "havnedistrikt");
-    checkOneTermReuse(a, "havnedistrikter", "havnedistrikt");
+    checkOneTerm(a, "havnedistriktene", "havnedistrikt");
+    checkOneTerm(a, "havnedistrikter", "havnedistrikt");
     // stopword
     assertAnalyzesTo(a, "det", new String[] {});
   }
@@ -45,8 +45,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("havnedistriktene"), false);
     Analyzer a = new NorwegianAnalyzer(TEST_VERSION_CURRENT, 
         NorwegianAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "havnedistriktene", "havnedistriktene");
-    checkOneTermReuse(a, "havnedistrikter", "havnedistrikt");
+    checkOneTerm(a, "havnedistriktene", "havnedistriktene");
+    checkOneTerm(a, "havnedistrikter", "havnedistrikt");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/no/TestNorwegianLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/no/TestNorwegianLightStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/no/TestNorwegianLightStemFilter.java	(working copy)
@@ -93,6 +93,6 @@
         return new TokenStreamComponents(tokenizer, new NorwegianLightStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/no/TestNorwegianMinimalStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/no/TestNorwegianMinimalStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/no/TestNorwegianMinimalStemFilter.java	(working copy)
@@ -92,6 +92,6 @@
         return new TokenStreamComponents(tokenizer, new NorwegianMinimalStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceFilter.java	(working copy)
@@ -114,7 +114,7 @@
         return new TokenStreamComponents(tokenizer,  new PatternReplaceFilter(tokenizer, Pattern.compile("a"), "b", true));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilterTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilterTest.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilterTest.java	(working copy)
@@ -34,6 +34,7 @@
        DelimitedPayloadTokenFilter.DEFAULT_DELIMITER, new IdentityEncoder());
     CharTermAttribute termAtt = filter.getAttribute(CharTermAttribute.class);
     PayloadAttribute payAtt = filter.getAttribute(PayloadAttribute.class);
+    filter.reset();
     assertTermEquals("The", filter, termAtt, payAtt, null);
     assertTermEquals("quick", filter, termAtt, payAtt, "JJ".getBytes("UTF-8"));
     assertTermEquals("red", filter, termAtt, payAtt, "JJ".getBytes("UTF-8"));
@@ -45,6 +46,8 @@
     assertTermEquals("brown", filter, termAtt, payAtt, "JJ".getBytes("UTF-8"));
     assertTermEquals("dogs", filter, termAtt, payAtt, "NN".getBytes("UTF-8"));
     assertFalse(filter.incrementToken());
+    filter.end();
+    filter.close();
   }
 
   public void testNext() throws Exception {
@@ -53,6 +56,7 @@
     DelimitedPayloadTokenFilter filter = new DelimitedPayloadTokenFilter
       (new MockTokenizer(new StringReader(test), MockTokenizer.WHITESPACE, false), 
        DelimitedPayloadTokenFilter.DEFAULT_DELIMITER, new IdentityEncoder());
+    filter.reset();
     assertTermEquals("The", filter, null);
     assertTermEquals("quick", filter, "JJ".getBytes("UTF-8"));
     assertTermEquals("red", filter, "JJ".getBytes("UTF-8"));
@@ -64,6 +68,8 @@
     assertTermEquals("brown", filter, "JJ".getBytes("UTF-8"));
     assertTermEquals("dogs", filter, "NN".getBytes("UTF-8"));
     assertFalse(filter.incrementToken());
+    filter.end();
+    filter.close();
   }
 
 
@@ -72,6 +78,7 @@
     DelimitedPayloadTokenFilter filter = new DelimitedPayloadTokenFilter(new MockTokenizer(new StringReader(test), MockTokenizer.WHITESPACE, false), '|', new FloatEncoder());
     CharTermAttribute termAtt = filter.getAttribute(CharTermAttribute.class);
     PayloadAttribute payAtt = filter.getAttribute(PayloadAttribute.class);
+    filter.reset();
     assertTermEquals("The", filter, termAtt, payAtt, null);
     assertTermEquals("quick", filter, termAtt, payAtt, PayloadHelper.encodeFloat(1.0f));
     assertTermEquals("red", filter, termAtt, payAtt, PayloadHelper.encodeFloat(2.0f));
@@ -83,6 +90,8 @@
     assertTermEquals("brown", filter, termAtt, payAtt, PayloadHelper.encodeFloat(99.3f));
     assertTermEquals("dogs", filter, termAtt, payAtt, PayloadHelper.encodeFloat(83.7f));
     assertFalse(filter.incrementToken());
+    filter.end();
+    filter.close();
   }
 
   public void testIntEncoding() throws Exception {
@@ -90,6 +99,7 @@
     DelimitedPayloadTokenFilter filter = new DelimitedPayloadTokenFilter(new MockTokenizer(new StringReader(test), MockTokenizer.WHITESPACE, false), '|', new IntegerEncoder());
     CharTermAttribute termAtt = filter.getAttribute(CharTermAttribute.class);
     PayloadAttribute payAtt = filter.getAttribute(PayloadAttribute.class);
+    filter.reset();
     assertTermEquals("The", filter, termAtt, payAtt, null);
     assertTermEquals("quick", filter, termAtt, payAtt, PayloadHelper.encodeInt(1));
     assertTermEquals("red", filter, termAtt, payAtt, PayloadHelper.encodeInt(2));
@@ -101,12 +111,13 @@
     assertTermEquals("brown", filter, termAtt, payAtt, PayloadHelper.encodeInt(99));
     assertTermEquals("dogs", filter, termAtt, payAtt, PayloadHelper.encodeInt(83));
     assertFalse(filter.incrementToken());
+    filter.end();
+    filter.close();
   }
 
   void assertTermEquals(String expected, TokenStream stream, byte[] expectPay) throws Exception {
     CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
     PayloadAttribute payloadAtt = stream.getAttribute(PayloadAttribute.class);
-    stream.reset();
     assertTrue(stream.incrementToken());
     assertEquals(expected, termAtt.toString());
     BytesRef payload = payloadAtt.getPayload();
@@ -123,7 +134,6 @@
 
 
   void assertTermEquals(String expected, TokenStream stream, CharTermAttribute termAtt, PayloadAttribute payAtt, byte[] expectPay) throws Exception {
-    stream.reset();
     assertTrue(stream.incrementToken());
     assertEquals(expected, termAtt.toString());
     BytesRef payload = payAtt.getPayload();
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseAnalyzer.java	(working copy)
@@ -34,8 +34,8 @@
   public void testBasics() throws IOException {
     Analyzer a = new PortugueseAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "quilométricas", "quilometric");
-    checkOneTermReuse(a, "quilométricos", "quilometric");
+    checkOneTerm(a, "quilométricas", "quilometric");
+    checkOneTerm(a, "quilométricos", "quilometric");
     // stopword
     assertAnalyzesTo(a, "não", new String[] {});
   }
@@ -45,8 +45,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("quilométricas"), false);
     Analyzer a = new PortugueseAnalyzer(TEST_VERSION_CURRENT, 
         PortugueseAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "quilométricas", "quilométricas");
-    checkOneTermReuse(a, "quilométricos", "quilometric");
+    checkOneTerm(a, "quilométricas", "quilométricas");
+    checkOneTerm(a, "quilométricos", "quilometric");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseLightStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseLightStemFilter.java	(working copy)
@@ -123,6 +123,6 @@
         return new TokenStreamComponents(tokenizer, new PortugueseLightStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseMinimalStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseMinimalStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseMinimalStemFilter.java	(working copy)
@@ -97,6 +97,6 @@
         return new TokenStreamComponents(tokenizer, new PortugueseMinimalStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseStemFilter.java	(working copy)
@@ -96,6 +96,6 @@
         return new TokenStreamComponents(tokenizer, new PortugueseStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java	(working copy)
@@ -113,6 +113,6 @@
         return new TokenStreamComponents(tokenizer, new ReverseStringFilter(TEST_VERSION_CURRENT, tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ro/TestRomanianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ro/TestRomanianAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ro/TestRomanianAnalyzer.java	(working copy)
@@ -34,8 +34,8 @@
   public void testBasics() throws IOException {
     Analyzer a = new RomanianAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "absenţa", "absenţ");
-    checkOneTermReuse(a, "absenţi", "absenţ");
+    checkOneTerm(a, "absenţa", "absenţ");
+    checkOneTerm(a, "absenţi", "absenţ");
     // stopword
     assertAnalyzesTo(a, "îl", new String[] {});
   }
@@ -45,8 +45,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("absenţa"), false);
     Analyzer a = new RomanianAnalyzer(TEST_VERSION_CURRENT, 
         RomanianAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "absenţa", "absenţa");
-    checkOneTermReuse(a, "absenţi", "absenţ");
+    checkOneTerm(a, "absenţa", "absenţa");
+    checkOneTerm(a, "absenţi", "absenţ");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java	(working copy)
@@ -39,9 +39,9 @@
     
     public void testReusableTokenStream() throws Exception {
       Analyzer a = new RussianAnalyzer(TEST_VERSION_CURRENT);
-      assertAnalyzesToReuse(a, "Вместе с тем о силе электромагнитной энергии имели представление еще",
+      assertAnalyzesTo(a, "Вместе с тем о силе электромагнитной энергии имели представление еще",
           new String[] { "вмест", "сил", "электромагнитн", "энерг", "имел", "представлен" });
-      assertAnalyzesToReuse(a, "Но знание это хранилось в тайне",
+      assertAnalyzesTo(a, "Но знание это хранилось в тайне",
           new String[] { "знан", "эт", "хран", "тайн" });
     }
     
@@ -50,7 +50,7 @@
       CharArraySet set = new CharArraySet(TEST_VERSION_CURRENT, 1, true);
       set.add("представление");
       Analyzer a = new RussianAnalyzer(TEST_VERSION_CURRENT, RussianAnalyzer.getDefaultStopSet() , set);
-      assertAnalyzesToReuse(a, "Вместе с тем о силе электромагнитной энергии имели представление еще",
+      assertAnalyzesTo(a, "Вместе с тем о силе электромагнитной энергии имели представление еще",
           new String[] { "вмест", "сил", "электромагнитн", "энерг", "имел", "представление" });
      
     }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianLightStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianLightStemFilter.java	(working copy)
@@ -75,6 +75,6 @@
         return new TokenStreamComponents(tokenizer, new RussianLightStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java	(working copy)
@@ -140,12 +140,12 @@
   
   public void testReusableTokenStream() throws Exception {
     Analyzer a = new ShingleAnalyzerWrapper(new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false), 2);
-    assertAnalyzesToReuse(a, "please divide into shingles",
+    assertAnalyzesTo(a, "please divide into shingles",
         new String[] { "please", "please divide", "divide", "divide into", "into", "into shingles", "shingles" },
         new int[] { 0, 0, 7, 7, 14, 14, 19 },
         new int[] { 6, 13, 13, 18, 18, 27, 27 },
         new int[] { 1, 0, 1, 0, 1, 0, 1 });
-    assertAnalyzesToReuse(a, "divide me up again",
+    assertAnalyzesTo(a, "divide me up again",
         new String[] { "divide", "divide me", "me", "me up", "up", "up again", "again" },
         new int[] { 0, 0, 7, 7, 10, 10, 13 },
         new int[] { 6, 9, 9, 12, 12, 18, 18 },
@@ -155,7 +155,7 @@
   public void testNonDefaultMinShingleSize() throws Exception {
     ShingleAnalyzerWrapper analyzer 
       = new ShingleAnalyzerWrapper(new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false), 3, 4);
-    assertAnalyzesToReuse(analyzer, "please divide this sentence into shingles",
+    assertAnalyzesTo(analyzer, "please divide this sentence into shingles",
                           new String[] { "please",   "please divide this",   "please divide this sentence", 
                                          "divide",   "divide this sentence", "divide this sentence into", 
                                          "this",     "this sentence into",   "this sentence into shingles",
@@ -168,7 +168,7 @@
 
     analyzer = new ShingleAnalyzerWrapper(
         new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false), 3, 4, ShingleFilter.TOKEN_SEPARATOR, false, false);
-    assertAnalyzesToReuse(analyzer, "please divide this sentence into shingles",
+    assertAnalyzesTo(analyzer, "please divide this sentence into shingles",
                           new String[] { "please divide this",   "please divide this sentence", 
                                          "divide this sentence", "divide this sentence into", 
                                          "this sentence into",   "this sentence into shingles",
@@ -181,7 +181,7 @@
   public void testNonDefaultMinAndSameMaxShingleSize() throws Exception {
     ShingleAnalyzerWrapper analyzer
       = new ShingleAnalyzerWrapper(new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false), 3, 3);
-    assertAnalyzesToReuse(analyzer, "please divide this sentence into shingles",
+    assertAnalyzesTo(analyzer, "please divide this sentence into shingles",
                           new String[] { "please",   "please divide this", 
                                          "divide",   "divide this sentence", 
                                          "this",     "this sentence into",
@@ -194,7 +194,7 @@
 
     analyzer = new ShingleAnalyzerWrapper(
         new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false), 3, 3, ShingleFilter.TOKEN_SEPARATOR, false, false);
-    assertAnalyzesToReuse(analyzer, "please divide this sentence into shingles",
+    assertAnalyzesTo(analyzer, "please divide this sentence into shingles",
                           new String[] { "please divide this", 
                                          "divide this sentence", 
                                          "this sentence into",
@@ -210,7 +210,7 @@
         ShingleFilter.DEFAULT_MIN_SHINGLE_SIZE,
         ShingleFilter.DEFAULT_MAX_SHINGLE_SIZE,
         "", true, false);
-    assertAnalyzesToReuse(analyzer, "please divide into shingles",
+    assertAnalyzesTo(analyzer, "please divide into shingles",
                           new String[] { "please", "pleasedivide", 
                                          "divide", "divideinto", 
                                          "into", "intoshingles", 
@@ -224,7 +224,7 @@
         ShingleFilter.DEFAULT_MIN_SHINGLE_SIZE,
         ShingleFilter.DEFAULT_MAX_SHINGLE_SIZE,
         "", false, false);
-    assertAnalyzesToReuse(analyzer, "please divide into shingles",
+    assertAnalyzesTo(analyzer, "please divide into shingles",
                           new String[] { "pleasedivide", 
                                          "divideinto", 
                                          "intoshingles" },
@@ -239,7 +239,7 @@
         ShingleFilter.DEFAULT_MIN_SHINGLE_SIZE,
         ShingleFilter.DEFAULT_MAX_SHINGLE_SIZE,
         null, true, false);
-    assertAnalyzesToReuse(analyzer, "please divide into shingles",
+    assertAnalyzesTo(analyzer, "please divide into shingles",
                           new String[] { "please", "pleasedivide", 
                                          "divide", "divideinto", 
                                          "into", "intoshingles", 
@@ -253,7 +253,7 @@
         ShingleFilter.DEFAULT_MIN_SHINGLE_SIZE,
         ShingleFilter.DEFAULT_MAX_SHINGLE_SIZE,
         "", false, false);
-    assertAnalyzesToReuse(analyzer, "please divide into shingles",
+    assertAnalyzesTo(analyzer, "please divide into shingles",
                           new String[] { "pleasedivide", 
                                          "divideinto", 
                                          "intoshingles" },
@@ -267,7 +267,7 @@
         ShingleFilter.DEFAULT_MIN_SHINGLE_SIZE,
         ShingleFilter.DEFAULT_MAX_SHINGLE_SIZE,
         "<SEP>", true, false);
-    assertAnalyzesToReuse(analyzer, "please divide into shingles",
+    assertAnalyzesTo(analyzer, "please divide into shingles",
                           new String[] { "please", "please<SEP>divide", 
                                          "divide", "divide<SEP>into", 
                                          "into", "into<SEP>shingles", 
@@ -281,7 +281,7 @@
         ShingleFilter.DEFAULT_MIN_SHINGLE_SIZE,
         ShingleFilter.DEFAULT_MAX_SHINGLE_SIZE,
         "<SEP>", false, false);
-    assertAnalyzesToReuse(analyzer, "please divide into shingles",
+    assertAnalyzesTo(analyzer, "please divide into shingles",
                           new String[] { "please<SEP>divide", 
                                          "divide<SEP>into", 
                                          "into<SEP>shingles" },
@@ -296,7 +296,7 @@
         ShingleFilter.DEFAULT_MIN_SHINGLE_SIZE,
         ShingleFilter.DEFAULT_MAX_SHINGLE_SIZE,
         "", false, true);
-    assertAnalyzesToReuse(analyzer, "please",
+    assertAnalyzesTo(analyzer, "please",
                           new String[] { "please" },
                           new int[] { 0 },
                           new int[] { 6 },
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java	(working copy)
@@ -1134,7 +1134,7 @@
         return new TokenStreamComponents(tokenizer, new ShingleFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 
   public void testTrailingHole1() throws IOException {
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java	(working copy)
@@ -114,7 +114,7 @@
           return new TokenStreamComponents(tokenizer, new SnowballFilter(tokenizer, lang));
         }
       };
-      checkOneTermReuse(a, "", "");
+      checkOneTerm(a, "", "");
     }
   }
   
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/sv/TestSwedishAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/sv/TestSwedishAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/sv/TestSwedishAnalyzer.java	(working copy)
@@ -34,8 +34,8 @@
   public void testBasics() throws IOException {
     Analyzer a = new SwedishAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "jaktkarlarne", "jaktkarl");
-    checkOneTermReuse(a, "jaktkarlens", "jaktkarl");
+    checkOneTerm(a, "jaktkarlarne", "jaktkarl");
+    checkOneTerm(a, "jaktkarlens", "jaktkarl");
     // stopword
     assertAnalyzesTo(a, "och", new String[] {});
   }
@@ -45,8 +45,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("jaktkarlarne"), false);
     Analyzer a = new SwedishAnalyzer(TEST_VERSION_CURRENT, 
         SwedishAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "jaktkarlarne", "jaktkarlarne");
-    checkOneTermReuse(a, "jaktkarlens", "jaktkarl");
+    checkOneTerm(a, "jaktkarlarne", "jaktkarlarne");
+    checkOneTerm(a, "jaktkarlens", "jaktkarl");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/sv/TestSwedishLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/sv/TestSwedishLightStemFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/sv/TestSwedishLightStemFilter.java	(working copy)
@@ -75,6 +75,6 @@
         return new TokenStreamComponents(tokenizer, new SwedishLightStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java	(working copy)
@@ -92,14 +92,14 @@
   
   public void testReusableTokenStream() throws Exception {
     ThaiAnalyzer analyzer = new ThaiAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET);
-    assertAnalyzesToReuse(analyzer, "", new String[] {});
+    assertAnalyzesTo(analyzer, "", new String[] {});
 
-      assertAnalyzesToReuse(
+      assertAnalyzesTo(
           analyzer,
           "การที่ได้ต้องแสดงว่างานดี",
           new String[] { "การ", "ที่", "ได้", "ต้อง", "แสดง", "ว่า", "งาน", "ดี"});
 
-      assertAnalyzesToReuse(
+      assertAnalyzesTo(
           analyzer,
           "บริษัทชื่อ XY&Z - คุยกับ xyz@demo.com",
           new String[] { "บริษัท", "ชื่อ", "xy", "z", "คุย", "กับ", "xyz", "demo.com" });
@@ -136,6 +136,6 @@
         return new TokenStreamComponents(tokenizer, new ThaiWordFilter(TEST_VERSION_CURRENT, tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishAnalyzer.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishAnalyzer.java	(working copy)
@@ -34,8 +34,8 @@
   public void testBasics() throws IOException {
     Analyzer a = new TurkishAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "ağacı", "ağaç");
-    checkOneTermReuse(a, "ağaç", "ağaç");
+    checkOneTerm(a, "ağacı", "ağaç");
+    checkOneTerm(a, "ağaç", "ağaç");
     // stopword
     assertAnalyzesTo(a, "dolayı", new String[] {});
   }
@@ -45,8 +45,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("ağacı"), false);
     Analyzer a = new TurkishAnalyzer(TEST_VERSION_CURRENT, 
         TurkishAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "ağacı", "ağacı");
-    checkOneTermReuse(a, "ağaç", "ağaç");
+    checkOneTerm(a, "ağacı", "ağacı");
+    checkOneTerm(a, "ağaç", "ağaç");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishLowerCaseFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishLowerCaseFilter.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishLowerCaseFilter.java	(working copy)
@@ -83,6 +83,6 @@
         return new TokenStreamComponents(tokenizer, new TurkishLowerCaseFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestElision.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestElision.java	(revision 1525305)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestElision.java	(working copy)
@@ -69,7 +69,7 @@
         return new TokenStreamComponents(tokenizer, new ElisionFilter(tokenizer, FrenchAnalyzer.DEFAULT_ARTICLES));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 
 }
Index: lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ICUTokenizer.java
===================================================================
--- lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ICUTokenizer.java	(revision 1525305)
+++ lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ICUTokenizer.java	(working copy)
@@ -45,8 +45,7 @@
   /** true length of text in the buffer */
   private int length = 0; 
   /** length in buffer that can be evaluated safely, up to a safe end point */
-  // note: usableLength is -1 here to best-effort AIOOBE consumers that don't call reset()
-  private int usableLength = -1; 
+  private int usableLength = 0; 
   /** accumulated offset of previous buffers for this reader, for offsetAtt */
   private int offset = 0; 
 
Index: lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUFoldingFilter.java
===================================================================
--- lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUFoldingFilter.java	(revision 1525305)
+++ lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUFoldingFilter.java	(working copy)
@@ -87,6 +87,6 @@
         return new TokenStreamComponents(tokenizer, new ICUFoldingFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2Filter.java
===================================================================
--- lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2Filter.java	(revision 1525305)
+++ lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2Filter.java	(working copy)
@@ -87,6 +87,6 @@
         return new TokenStreamComponents(tokenizer, new ICUNormalizer2Filter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUTransformFilter.java
===================================================================
--- lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUTransformFilter.java	(revision 1525305)
+++ lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUTransformFilter.java	(working copy)
@@ -109,6 +109,6 @@
         return new TokenStreamComponents(tokenizer, new ICUTransformFilter(tokenizer, Transliterator.getInstance("Any-Latin")));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java
===================================================================
--- lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java	(revision 1525305)
+++ lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java	(working copy)
@@ -207,7 +207,7 @@
   }
   
   public void testReusableTokenStream() throws Exception {
-    assertAnalyzesToReuse(a, "སྣོན་མཛོད་དང་ལས་འདིས་བོད་ཡིག་མི་ཉམས་གོང་འཕེལ་དུ་གཏོང་བར་ཧ་ཅང་དགེ་མཚན་མཆིས་སོ། །",
+    assertAnalyzesTo(a, "སྣོན་མཛོད་དང་ལས་འདིས་བོད་ཡིག་མི་ཉམས་གོང་འཕེལ་དུ་གཏོང་བར་ཧ་ཅང་དགེ་མཚན་མཆིས་སོ། །",
         new String[] { "སྣོན", "མཛོད", "དང", "ལས", "འདིས", "བོད", "ཡིག", "མི", "ཉམས", "གོང", 
                       "འཕེལ", "དུ", "གཏོང", "བར", "ཧ", "ཅང", "དགེ", "མཚན", "མཆིས", "སོ" });
   }
Index: lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestWithCJKBigramFilter.java
===================================================================
--- lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestWithCJKBigramFilter.java	(revision 1525305)
+++ lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestWithCJKBigramFilter.java	(working copy)
@@ -191,14 +191,14 @@
   }
   
   public void testReusableTokenStream() throws IOException {
-    assertAnalyzesToReuse(analyzer, "あいうえおabcかきくけこ",
+    assertAnalyzesTo(analyzer, "あいうえおabcかきくけこ",
         new String[] { "あい", "いう", "うえ", "えお", "abc", "かき", "きく", "くけ", "けこ" },
         new int[] { 0, 1, 2, 3, 5,  8,  9, 10, 11 },
         new int[] { 2, 3, 4, 5, 8, 10, 11, 12, 13 },
         new String[] { "<DOUBLE>", "<DOUBLE>", "<DOUBLE>", "<DOUBLE>", "<ALPHANUM>", "<DOUBLE>", "<DOUBLE>", "<DOUBLE>", "<DOUBLE>" },
         new int[] { 1, 1, 1, 1, 1,  1,  1,  1,  1});
     
-    assertAnalyzesToReuse(analyzer, "あいうえおabんcかきくけ こ",
+    assertAnalyzesTo(analyzer, "あいうえおabんcかきくけ こ",
         new String[] { "あい", "いう", "うえ", "えお", "ab", "ん", "c", "かき", "きく", "くけ", "こ" },
         new int[] { 0, 1, 2, 3, 5, 7, 8,  9, 10, 11, 14 },
         new int[] { 2, 3, 4, 5, 7, 8, 9, 11, 12, 13, 15 },
Index: lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java
===================================================================
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java	(revision 1525305)
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java	(working copy)
@@ -243,7 +243,7 @@
         outputCompounds = false;
         break;
     }
-    buffer.reset(null); // best effort NPE consumers that don't call reset()
+    buffer.reset(this.input);
 
     resetState();
 
@@ -261,7 +261,14 @@
   }
 
   @Override
+  public void close() throws IOException {
+    super.close();
+    buffer.reset(input);
+  }
+
+  @Override
   public void reset() throws IOException {
+    super.reset();
     buffer.reset(input);
     resetState();
   }
Index: lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseBaseFormFilter.java
===================================================================
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseBaseFormFilter.java	(revision 1525305)
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseBaseFormFilter.java	(working copy)
@@ -75,6 +75,6 @@
         return new TokenStreamComponents(tokenizer, new JapaneseBaseFormFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseKatakanaStemFilter.java
===================================================================
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseKatakanaStemFilter.java	(revision 1525305)
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseKatakanaStemFilter.java	(working copy)
@@ -94,6 +94,6 @@
         return new TokenStreamComponents(tokenizer, new JapaneseKatakanaStemFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseReadingFormFilter.java
===================================================================
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseReadingFormFilter.java	(revision 1525305)
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseReadingFormFilter.java	(working copy)
@@ -103,6 +103,6 @@
         return new TokenStreamComponents(tokenizer, new JapaneseReadingFormFilter(tokenizer));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/morfologik/src/test/org/apache/lucene/analysis/morfologik/TestMorfologikAnalyzer.java
===================================================================
--- lucene/analysis/morfologik/src/test/org/apache/lucene/analysis/morfologik/TestMorfologikAnalyzer.java	(revision 1525305)
+++ lucene/analysis/morfologik/src/test/org/apache/lucene/analysis/morfologik/TestMorfologikAnalyzer.java	(working copy)
@@ -44,16 +44,16 @@
   /** Test stemming of single tokens with Morfologik library. */
   public final void testSingleTokens() throws IOException {
     Analyzer a = getTestAnalyzer();
-    assertAnalyzesToReuse(a, "a", new String[] { "a" });
-    assertAnalyzesToReuse(a, "liście", new String[] { "liście", "liść", "list", "lista" });
-    assertAnalyzesToReuse(a, "danych", new String[] { "dany", "dana", "dane", "dać" });
-    assertAnalyzesToReuse(a, "ęóąśłżźćń", new String[] { "ęóąśłżźćń" });
+    assertAnalyzesTo(a, "a", new String[] { "a" });
+    assertAnalyzesTo(a, "liście", new String[] { "liście", "liść", "list", "lista" });
+    assertAnalyzesTo(a, "danych", new String[] { "dany", "dana", "dane", "dać" });
+    assertAnalyzesTo(a, "ęóąśłżźćń", new String[] { "ęóąśłżźćń" });
   }
 
   /** Test stemming of multiple tokens and proper term metrics. */
   public final void testMultipleTokens() throws IOException {
     Analyzer a = getTestAnalyzer();
-    assertAnalyzesToReuse(
+    assertAnalyzesTo(
       a,
       "liście danych",
       new String[] { "liście", "liść", "list", "lista", "dany", "dana", "dane", "dać" },
@@ -61,7 +61,7 @@
       new int[] { 6, 6, 6, 6, 13, 13, 13, 13 },
       new int[] { 1, 0, 0, 0, 1, 0, 0, 0 });
 
-    assertAnalyzesToReuse(
+    assertAnalyzesTo(
         a,
         "T. Gl\u00FCcksberg",
         new String[] { "tom", "tona", "Gl\u00FCcksberg" },
@@ -106,16 +106,16 @@
   public final void testCase() throws IOException {
     Analyzer a = getTestAnalyzer();
 
-    assertAnalyzesToReuse(a, "AGD",      new String[] { "AGD", "artykuły gospodarstwa domowego" });
-    assertAnalyzesToReuse(a, "agd",      new String[] { "artykuły gospodarstwa domowego" });
+    assertAnalyzesTo(a, "AGD",      new String[] { "AGD", "artykuły gospodarstwa domowego" });
+    assertAnalyzesTo(a, "agd",      new String[] { "artykuły gospodarstwa domowego" });
 
-    assertAnalyzesToReuse(a, "Poznania", new String[] { "Poznań" });
-    assertAnalyzesToReuse(a, "poznania", new String[] { "poznanie", "poznać" });
+    assertAnalyzesTo(a, "Poznania", new String[] { "Poznań" });
+    assertAnalyzesTo(a, "poznania", new String[] { "poznanie", "poznać" });
 
-    assertAnalyzesToReuse(a, "Aarona",   new String[] { "Aaron" });
-    assertAnalyzesToReuse(a, "aarona",   new String[] { "aarona" });
+    assertAnalyzesTo(a, "Aarona",   new String[] { "Aaron" });
+    assertAnalyzesTo(a, "aarona",   new String[] { "aarona" });
 
-    assertAnalyzesToReuse(a, "Liście",   new String[] { "liście", "liść", "list", "lista" });
+    assertAnalyzesTo(a, "Liście",   new String[] { "liście", "liść", "list", "lista" });
   }
 
   private void assertPOSToken(TokenStream ts, String term, String... tags) throws IOException {
@@ -183,7 +183,7 @@
       }
     };
 
-    assertAnalyzesToReuse(
+    assertAnalyzesTo(
       a,
       "liście danych",
       new String[] { "liście", "dany", "dana", "dane", "dać" },
Index: lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/DoubleMetaphoneFilterTest.java
===================================================================
--- lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/DoubleMetaphoneFilterTest.java	(revision 1525305)
+++ lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/DoubleMetaphoneFilterTest.java	(working copy)
@@ -105,6 +105,6 @@
         return new TokenStreamComponents(tokenizer, new DoubleMetaphoneFilter(tokenizer, 8, random().nextBoolean()));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
 }
Index: lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestBeiderMorseFilter.java
===================================================================
--- lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestBeiderMorseFilter.java	(revision 1525305)
+++ lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestBeiderMorseFilter.java	(working copy)
@@ -106,7 +106,7 @@
         return new TokenStreamComponents(tokenizer, new BeiderMorseFilter(tokenizer, new PhoneticEngine(NameType.GENERIC, RuleType.EXACT, true)));
       }
     };
-    checkOneTermReuse(a, "", "");
+    checkOneTerm(a, "", "");
   }
   
   public void testCustomAttribute() throws IOException {
Index: lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestDoubleMetaphoneFilterFactory.java
===================================================================
--- lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestDoubleMetaphoneFilterFactory.java	(revision 1525305)
+++ lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestDoubleMetaphoneFilterFactory.java	(working copy)
@@ -51,27 +51,6 @@
     assertTokenStreamContents(filteredStream, new String[] { "ANTRNXNL" });
   }
   
-  /**
-   * Ensure that reset() removes any state (buffered tokens)
-   */
-  public void testReset() throws Exception {
-    DoubleMetaphoneFilterFactory factory = new DoubleMetaphoneFilterFactory(new HashMap<String, String>());
-    TokenStream inputStream = new MockTokenizer(new StringReader("international"), MockTokenizer.WHITESPACE, false);
-
-    TokenStream filteredStream = factory.create(inputStream);
-    CharTermAttribute termAtt = filteredStream.addAttribute(CharTermAttribute.class);
-    assertEquals(DoubleMetaphoneFilter.class, filteredStream.getClass());
-    
-    filteredStream.reset();
-    assertTrue(filteredStream.incrementToken());
-    assertEquals(13, termAtt.length());
-    assertEquals("international", termAtt.toString());
-    filteredStream.reset();
-    
-    // ensure there are no more tokens, such as ANTRNXNL
-    assertFalse(filteredStream.incrementToken());
-  }
-  
   /** Test that bogus arguments result in exception */
   public void testBogusArguments() throws Exception {
     try {
Index: lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestPhoneticFilter.java
===================================================================
--- lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestPhoneticFilter.java	(revision 1525305)
+++ lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestPhoneticFilter.java	(working copy)
@@ -113,7 +113,7 @@
           return new TokenStreamComponents(tokenizer, new PhoneticFilter(tokenizer, e, random().nextBoolean()));
         }
       };
-      checkOneTermReuse(a, "", "");
+      checkOneTerm(a, "", "");
     }
   }
 }
Index: lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java
===================================================================
--- lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java	(revision 1525305)
+++ lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java	(working copy)
@@ -108,6 +108,7 @@
 
   @Override
   public void reset() throws IOException {
+    super.reset();
     tokenStart = tokenEnd = 0;
   }
 
Index: lucene/analysis/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseAnalyzer.java
===================================================================
--- lucene/analysis/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseAnalyzer.java	(revision 1525305)
+++ lucene/analysis/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseAnalyzer.java	(working copy)
@@ -79,7 +79,7 @@
     String result[] = { "我", "购买", "了", "道具", "和", "服装", "," };
     for (Analyzer analyzer : analyzers) {
       assertAnalyzesTo(analyzer, sentence, result);
-      assertAnalyzesToReuse(analyzer, sentence, result);
+      assertAnalyzesTo(analyzer, sentence, result);
     }
   }
   
@@ -167,11 +167,11 @@
   
   public void testReusableTokenStream() throws Exception {
     Analyzer a = new SmartChineseAnalyzer(Version.LUCENE_CURRENT);
-    assertAnalyzesToReuse(a, "我购买 Tests 了道具和服装", 
+    assertAnalyzesTo(a, "我购买 Tests 了道具和服装", 
         new String[] { "我", "购买", "test", "了", "道具", "和", "服装"},
         new int[] { 0, 1, 4, 10, 11, 13, 14 },
         new int[] { 1, 3, 9, 11, 13, 14, 16 });
-    assertAnalyzesToReuse(a, "我购买了道具和服装。",
+    assertAnalyzesTo(a, "我购买了道具和服装。",
         new String[] { "我", "购买", "了", "道具", "和", "服装" },
         new int[] { 0, 1, 3, 4, 6, 7 },
         new int[] { 1, 3, 4, 6, 7, 9 });
Index: lucene/analysis/stempel/src/test/org/apache/lucene/analysis/pl/TestPolishAnalyzer.java
===================================================================
--- lucene/analysis/stempel/src/test/org/apache/lucene/analysis/pl/TestPolishAnalyzer.java	(revision 1525305)
+++ lucene/analysis/stempel/src/test/org/apache/lucene/analysis/pl/TestPolishAnalyzer.java	(working copy)
@@ -34,8 +34,8 @@
   public void testBasics() throws IOException {
     Analyzer a = new PolishAnalyzer(TEST_VERSION_CURRENT);
     // stemming
-    checkOneTermReuse(a, "studenta", "student");
-    checkOneTermReuse(a, "studenci", "student");
+    checkOneTerm(a, "studenta", "student");
+    checkOneTerm(a, "studenci", "student");
     // stopword
     assertAnalyzesTo(a, "był", new String[] {});
   }
@@ -45,8 +45,8 @@
     CharArraySet exclusionSet = new CharArraySet(TEST_VERSION_CURRENT, asSet("studenta"), false);;
     Analyzer a = new PolishAnalyzer(TEST_VERSION_CURRENT, 
         PolishAnalyzer.getDefaultStopSet(), exclusionSet);
-    checkOneTermReuse(a, "studenta", "studenta");
-    checkOneTermReuse(a, "studenci", "student");
+    checkOneTerm(a, "studenta", "studenta");
+    checkOneTerm(a, "studenci", "student");
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/BaseUIMATokenizer.java
===================================================================
--- lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/BaseUIMATokenizer.java	(revision 1525305)
+++ lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/BaseUIMATokenizer.java	(working copy)
@@ -89,6 +89,7 @@
 
   @Override
   public void reset() throws IOException {
+    super.reset();
     iterator = null;
   }
 }
Index: lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java	(revision 1525305)
+++ lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java	(working copy)
@@ -30,21 +30,28 @@
   call {@link AttributeSource#clearAttributes()} before
   setting attributes.
  */
-public abstract class Tokenizer extends TokenStream {
+public abstract class Tokenizer extends TokenStream {  
   /** The text source for this Tokenizer. */
-  protected Reader input;
+  protected Reader input = ILLEGAL_STATE_READER;
+  
+  /** Pending reader: not actually assigned to input until reset() */
+  private Reader inputPending = ILLEGAL_STATE_READER;
 
   /** Construct a token stream processing the given input. */
   protected Tokenizer(Reader input) {
-    assert input != null: "input must not be null";
-    this.input = input;
+    if (input == null) {
+      throw new NullPointerException("input must not be null");
+    }
+    this.inputPending = input;
   }
   
   /** Construct a token stream processing the given input using the given AttributeFactory. */
   protected Tokenizer(AttributeFactory factory, Reader input) {
     super(factory);
-    assert input != null: "input must not be null";
-    this.input = input;
+    if (input == null) {
+      throw new NullPointerException("input must not be null");
+    }
+    this.inputPending = input;
   }
 
   /**
@@ -56,12 +63,10 @@
    */
   @Override
   public void close() throws IOException {
-    if (input != null) {
-      input.close();
-      // LUCENE-2387: don't hold onto Reader after close, so
-      // GC can reclaim
-      input = null;
-    }
+    input.close();
+    // LUCENE-2387: don't hold onto Reader after close, so
+    // GC can reclaim
+    inputPending = input = ILLEGAL_STATE_READER;
   }
   
   /** Return the corrected offset. If {@link #input} is a {@link CharFilter} subclass
@@ -71,7 +76,6 @@
    * @see CharFilter#correctOffset
    */
   protected final int correctOffset(int currentOff) {
-    assert input != null: "this tokenizer is closed";
     return (input instanceof CharFilter) ? ((CharFilter) input).correctOffset(currentOff) : currentOff;
   }
 
@@ -79,14 +83,36 @@
    *  analyzer (in its tokenStream method) will use
    *  this to re-use a previously created tokenizer. */
   public final void setReader(Reader input) throws IOException {
-    assert input != null: "input must not be null";
-    this.input = input;
+    if (input == null) {
+      throw new NullPointerException("input must not be null");
+    }
+    this.input = ILLEGAL_STATE_READER;
+    this.inputPending = input;
     assert setReaderTestPoint();
   }
   
+  @Override
+  public void reset() throws IOException {
+    super.reset();
+    input = inputPending;
+    inputPending = ILLEGAL_STATE_READER;
+  }
+
   // only used by assert, for testing
   boolean setReaderTestPoint() {
     return true;
   }
+  
+  private static final Reader ILLEGAL_STATE_READER = new Reader() {
+    @Override
+    public int read(char[] cbuf, int off, int len) {
+      throw new IllegalStateException("TokenStream contract violation: reset()/close() call missing, " +
+          "reset() called multiple times, or subclass does not call super.reset(). " +
+          "Please see Javadocs of TokenStream class for more information about the correct consuming workflow.");
+    }
+
+    @Override
+    public void close() {} 
+  };
 }
 
Index: lucene/core/src/test/org/apache/lucene/analysis/TestGraphTokenizers.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/analysis/TestGraphTokenizers.java	(revision 1525305)
+++ lucene/core/src/test/org/apache/lucene/analysis/TestGraphTokenizers.java	(working copy)
@@ -68,7 +68,8 @@
     }
 
     @Override
-    public void reset() {
+    public void reset() throws IOException {
+      super.reset();
       tokens = null;
       upto = 0;
     }
Index: lucene/core/src/test/org/apache/lucene/analysis/TestMockAnalyzer.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/analysis/TestMockAnalyzer.java	(revision 1525305)
+++ lucene/core/src/test/org/apache/lucene/analysis/TestMockAnalyzer.java	(working copy)
@@ -36,9 +36,9 @@
     Analyzer a = new MockAnalyzer(random());
     assertAnalyzesTo(a, "A bc defg hiJklmn opqrstuv wxy z ",
         new String[] { "a", "bc", "defg", "hijklmn", "opqrstuv", "wxy", "z" });
-    assertAnalyzesToReuse(a, "aba cadaba shazam",
+    assertAnalyzesTo(a, "aba cadaba shazam",
         new String[] { "aba", "cadaba", "shazam" });
-    assertAnalyzesToReuse(a, "break on whitespace",
+    assertAnalyzesTo(a, "break on whitespace",
         new String[] { "break", "on", "whitespace" });
   }
   
@@ -47,9 +47,9 @@
     Analyzer a = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
     assertAnalyzesTo(a, "a-bc123 defg+hijklmn567opqrstuv78wxy_z ",
         new String[] { "a", "bc", "defg", "hijklmn", "opqrstuv", "wxy", "z" });
-    assertAnalyzesToReuse(a, "aba4cadaba-Shazam",
+    assertAnalyzesTo(a, "aba4cadaba-Shazam",
         new String[] { "aba", "cadaba", "shazam" });
-    assertAnalyzesToReuse(a, "break+on/Letters",
+    assertAnalyzesTo(a, "break+on/Letters",
         new String[] { "break", "on", "letters" });
   }
   
@@ -58,9 +58,9 @@
     Analyzer a = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);
     assertAnalyzesTo(a, "a-bc123 defg+hijklmn567opqrstuv78wxy_z ",
         new String[] { "a-bc123 defg+hijklmn567opqrstuv78wxy_z " });
-    assertAnalyzesToReuse(a, "aba4cadaba-Shazam",
+    assertAnalyzesTo(a, "aba4cadaba-Shazam",
         new String[] { "aba4cadaba-Shazam" });
-    assertAnalyzesToReuse(a, "break+on/Nothing",
+    assertAnalyzesTo(a, "break+on/Nothing",
         new String[] { "break+on/Nothing" });
   }
   
@@ -106,7 +106,7 @@
     stream.end();
     stream.close();
     
-    assertAnalyzesToReuse(analyzer, testString, new String[] { "t" });
+    assertAnalyzesTo(analyzer, testString, new String[] { "t" });
   }
 
   /** blast some random strings through the analyzer */
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java	(revision 1525305)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java	(working copy)
@@ -1599,14 +1599,15 @@
 
     @Override
     public void reset() throws IOException {
-       this.upto = 0;
-       final StringBuilder b = new StringBuilder();
-       final char[] buffer = new char[1024];
-       int n;
-       while ((n = input.read(buffer)) != -1) {
-         b.append(buffer, 0, n);
-       }
-       this.tokens = b.toString().split(" ");
+      super.reset();
+      this.upto = 0;
+      final StringBuilder b = new StringBuilder();
+      final char[] buffer = new char[1024];
+      int n;
+      while ((n = input.read(buffer)) != -1) {
+        b.append(buffer, 0, n);
+      }
+      this.tokens = b.toString().split(" ");
     }
   }
 
Index: lucene/core/src/test/org/apache/lucene/search/TestTermRangeQuery.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestTermRangeQuery.java	(revision 1525305)
+++ lucene/core/src/test/org/apache/lucene/search/TestTermRangeQuery.java	(working copy)
@@ -227,7 +227,8 @@
       }
 
       @Override
-      public void reset() throws IOException {;
+      public void reset() throws IOException {
+        super.reset();
         done = false;
       }
     }
Index: lucene/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest.java
===================================================================
--- lucene/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest.java	(revision 1525305)
+++ lucene/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest.java	(working copy)
@@ -49,7 +49,7 @@
     assertTokenStreamContents(filter, new String[] {"short", "toolong",
         "evenmuchlongertext"});
     
-    checkOneTermReuse(new Analyzer() {
+    checkOneTerm(new Analyzer() {
       
       @Override
       public TokenStreamComponents createComponents(String fieldName, Reader reader) {
Index: lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java
===================================================================
--- lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java	(revision 1525305)
+++ lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java	(working copy)
@@ -319,7 +319,8 @@
     }
     
     @Override
-    public void reset() {
+    public void reset() throws IOException {
+      super.reset();
       startTerm = 0;
       nextStartOffset = 0;
       snippet = null;
Index: lucene/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiPhraseQueryParsing.java
===================================================================
--- lucene/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiPhraseQueryParsing.java	(revision 1525305)
+++ lucene/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiPhraseQueryParsing.java	(working copy)
@@ -82,6 +82,7 @@
 
     @Override
     public void reset() throws IOException {
+      super.reset();
       this.upto = 0;
       this.lastPos = 0;
     }
Index: lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java	(revision 1525305)
+++ lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java	(working copy)
@@ -341,14 +341,17 @@
   }
   
   public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
+    checkResetException(a, input);
     assertTokenStreamContents(a.tokenStream("dummy", input), output, startOffsets, endOffsets, types, posIncrements, null, input.length());
   }
   
   public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[]) throws IOException {
+    checkResetException(a, input);
     assertTokenStreamContents(a.tokenStream("dummy", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length());
   }
 
   public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean offsetsAreCorrect) throws IOException {
+    checkResetException(a, input);
     assertTokenStreamContents(a.tokenStream("dummy", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), offsetsAreCorrect);
   }
   
@@ -375,31 +378,29 @@
   public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], int[] posIncrements) throws IOException {
     assertAnalyzesTo(a, input, output, startOffsets, endOffsets, null, posIncrements, null);
   }
-  
 
-  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
-    assertTokenStreamContents(a.tokenStream("dummy", input), output, startOffsets, endOffsets, types, posIncrements, null, input.length());
+  static void checkResetException(Analyzer a, String input) throws IOException {
+    TokenStream ts = a.tokenStream("bogus", input);
+    try {
+      if (ts.incrementToken()) {
+        //System.out.println(ts.reflectAsString(false));
+        fail("didn't get expected exception when reset() not called");
+      }
+    } catch (IllegalStateException expected) {
+      // ok
+    } catch (AssertionError expected) {
+      // ok: MockTokenizer
+      assertTrue(expected.getMessage(), expected.getMessage() != null && expected.getMessage().contains("wrong state"));
+    } catch (Exception unexpected) {
+      fail("got wrong exception when reset() not called: " + unexpected);
+    } finally {
+      // consume correctly
+      ts.reset();
+      while (ts.incrementToken()) {}
+      ts.end();
+      ts.close();
+    }
   }
-  
-  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output) throws IOException {
-    assertAnalyzesToReuse(a, input, output, null, null, null, null);
-  }
-  
-  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, String[] types) throws IOException {
-    assertAnalyzesToReuse(a, input, output, null, null, types, null);
-  }
-  
-  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int[] posIncrements) throws IOException {
-    assertAnalyzesToReuse(a, input, output, null, null, null, posIncrements);
-  }
-  
-  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[]) throws IOException {
-    assertAnalyzesToReuse(a, input, output, startOffsets, endOffsets, null, null);
-  }
-  
-  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], int[] posIncrements) throws IOException {
-    assertAnalyzesToReuse(a, input, output, startOffsets, endOffsets, null, posIncrements);
-  }
 
   // simple utility method for testing stemmers
   
@@ -407,10 +408,6 @@
     assertAnalyzesTo(a, input, new String[]{expected});
   }
   
-  public static void checkOneTermReuse(Analyzer a, final String input, final String expected) throws IOException {
-    assertAnalyzesToReuse(a, input, new String[]{expected});
-  }
-  
   /** utility method for blasting tokenstreams with data to make sure they don't do anything crazy */
   public static void checkRandomData(Random random, Analyzer a, int iterations) throws IOException {
     checkRandomData(random, a, iterations, 20, false, true);
@@ -476,6 +473,7 @@
   }
 
   public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {
+    checkResetException(a, "best effort");
     long seed = random.nextLong();
     boolean useCharFilter = random.nextBoolean();
     Directory dir = null;
Index: lucene/test-framework/src/java/org/apache/lucene/analysis/VocabularyAssert.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/analysis/VocabularyAssert.java	(revision 1525305)
+++ lucene/test-framework/src/java/org/apache/lucene/analysis/VocabularyAssert.java	(working copy)
@@ -41,7 +41,7 @@
     while ((inputWord = vocReader.readLine()) != null) {
       String expectedWord = outputReader.readLine();
       Assert.assertNotNull(expectedWord);
-      BaseTokenStreamTestCase.checkOneTermReuse(a, inputWord, expectedWord);
+      BaseTokenStreamTestCase.checkOneTerm(a, inputWord, expectedWord);
     }
   }
   
@@ -55,7 +55,7 @@
       if (inputLine.startsWith("#") || inputLine.trim().length() == 0)
         continue; /* comment */
       String words[] = inputLine.split("\t");
-      BaseTokenStreamTestCase.checkOneTermReuse(a, words[0], words[1]);
+      BaseTokenStreamTestCase.checkOneTerm(a, words[0], words[1]);
     }
   }
   
Index: solr/core/src/java/org/apache/solr/analysis/TrieTokenizerFactory.java
===================================================================
--- solr/core/src/java/org/apache/solr/analysis/TrieTokenizerFactory.java	(revision 1525305)
+++ solr/core/src/java/org/apache/solr/analysis/TrieTokenizerFactory.java	(working copy)
@@ -96,8 +96,9 @@
   }
 
   @Override
-  public void reset() {
-   try {
+  public void reset() throws IOException {
+    super.reset();
+    try {
       int upto = 0;
       char[] buf = termAtt.buffer();
       while (true) {
@@ -167,6 +168,7 @@
 
   @Override
   public void end() throws IOException {
+    super.end();
     if (hasValue) {
       ts.end();
     }
Index: solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java
===================================================================
--- solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java	(revision 1525305)
+++ solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java	(working copy)
@@ -691,6 +691,11 @@
       return true;
     }
   }
+
+  @Override
+  public void reset() throws IOException {
+    // this looks wrong: but its correct.
+  }
 }
 
 // for TokenOrderingFilter, so it can easily sort by startOffset
Index: solr/core/src/java/org/apache/solr/schema/BoolField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/BoolField.java	(revision 1525305)
+++ solr/core/src/java/org/apache/solr/schema/BoolField.java	(working copy)
@@ -74,6 +74,7 @@
 
         @Override
         public void reset() throws IOException {
+          super.reset();
           done = false;
         }
 
Index: solr/core/src/java/org/apache/solr/schema/PreAnalyzedField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/PreAnalyzedField.java	(revision 1525305)
+++ solr/core/src/java/org/apache/solr/schema/PreAnalyzedField.java	(working copy)
@@ -252,9 +252,11 @@
     private byte[] binaryValue = null;
     private PreAnalyzedParser parser;
     private Reader lastReader;
+    private Reader input; // hides original input since we replay saved states (and dont reuse)
     
     public PreAnalyzedTokenizer(Reader reader, PreAnalyzedParser parser) {
       super(reader);
+      this.input = reader;
       this.parser = parser;
     }
     
