From acf731f349c99dbc1ffa6a888a22bc3d569c23e6 Mon Sep 17 00:00:00 2001
From: Greg Bowyer <gbowyer@apache.org>
Date: Mon, 8 Jun 2015 23:00:17 -0700
Subject: [PATCH] Migrate HDFSDirectory from solr to lucene

---
 dev-tools/idea/.idea/modules.xml                   |   1 +
 lucene/common-build.xml                            |  12 +
 lucene/hadoop/build.xml                            |  35 +++
 lucene/hadoop/ivy.xml                              |  66 ++++
 .../apache/lucene/store/blockcache/BlockCache.java | 204 +++++++++++++
 .../lucene/store/blockcache/BlockCacheKey.java     |  84 +++++
 .../store/blockcache/BlockCacheLocation.java       |  70 +++++
 .../lucene/store/blockcache/BlockDirectory.java    | 340 +++++++++++++++++++++
 .../store/blockcache/BlockDirectoryCache.java      | 125 ++++++++
 .../apache/lucene/store/blockcache/BlockLocks.java |  99 ++++++
 .../lucene/store/blockcache/BufferStore.java       | 130 ++++++++
 .../org/apache/lucene/store/blockcache/Cache.java  |  70 +++++
 .../lucene/store/blockcache/CachedIndexOutput.java |  87 ++++++
 .../store/blockcache/CustomBufferedIndexInput.java | 284 +++++++++++++++++
 .../apache/lucene/store/blockcache/Metrics.java    |  68 +++++
 .../blockcache/ReusedBufferedIndexOutput.java      | 165 ++++++++++
 .../org/apache/lucene/store/blockcache/Store.java  |  29 ++
 .../lucene/store/blockcache/package-info.java      |  23 ++
 .../apache/lucene/store/hdfs/HdfsDirectory.java    | 243 +++++++++++++++
 .../apache/lucene/store/hdfs/HdfsFileReader.java   | 105 +++++++
 .../apache/lucene/store/hdfs/HdfsFileWriter.java   |  57 ++++
 .../apache/lucene/store/hdfs/HdfsLockFactory.java  | 124 ++++++++
 .../org/apache/lucene/store/hdfs/package-info.java |  22 ++
 lucene/hadoop/src/java/overview.html               |  26 ++
 lucene/hadoop/src/test/hadoop-tests.policy         | 139 +++++++++
 .../lucene/store/blockcache/BlockCacheTest.java    | 110 +++++++
 .../store/blockcache/BlockDirectoryTest.java       | 274 +++++++++++++++++
 .../lucene/store/blockcache/BufferStoreTest.java   |  93 ++++++
 .../lucene/store/hdfs/HdfsDirectoryTest.java       | 241 +++++++++++++++
 .../lucene/store/hdfs/HdfsLockFactoryTest.java     |  84 +++++
 .../org/apache/lucene/store/hdfs/HdfsTestUtil.java | 137 +++++++++
 lucene/module-build.xml                            |  11 +
 .../apache/lucene/util/BadHdfsThreadsFilter.java   |  48 +++
 lucene/tools/junit4/tests.policy                   |  22 +-
 solr/common-build.xml                              |  14 +-
 .../solr/hadoop/MorphlineBasicMiniMRTest.java      |   2 +-
 .../solr/hadoop/MorphlineGoLiveMiniMRTest.java     |   2 +-
 .../morphlines/solr/SolrMorphlineZkAliasTest.java  |   2 +-
 .../morphlines/solr/SolrMorphlineZkAvroTest.java   |   7 +-
 .../solr/morphlines/solr/SolrMorphlineZkTest.java  |   7 +-
 solr/core/ivy.xml                                  |   1 -
 .../apache/solr/core/CachingDirectoryFactory.java  |   8 -
 .../org/apache/solr/core/HdfsDirectoryFactory.java |  21 +-
 .../java/org/apache/solr/search/SolrCacheBase.java |   2 +-
 .../apache/solr/store/blockcache/BlockCache.java   | 205 -------------
 .../solr/store/blockcache/BlockCacheKey.java       |  84 -----
 .../solr/store/blockcache/BlockCacheLocation.java  |  70 -----
 .../solr/store/blockcache/BlockDirectory.java      | 340 ---------------------
 .../solr/store/blockcache/BlockDirectoryCache.java | 125 --------
 .../apache/solr/store/blockcache/BlockLocks.java   |  99 ------
 .../apache/solr/store/blockcache/BufferStore.java  | 134 --------
 .../org/apache/solr/store/blockcache/Cache.java    |  70 -----
 .../solr/store/blockcache/CachedIndexOutput.java   |  87 ------
 .../store/blockcache/CustomBufferedIndexInput.java | 284 -----------------
 .../org/apache/solr/store/blockcache/Metrics.java  | 148 ---------
 .../blockcache/ReusedBufferedIndexOutput.java      | 165 ----------
 .../apache/solr/store/blockcache/SolrMetrics.java  | 118 +++++++
 .../org/apache/solr/store/blockcache/Store.java    |  29 --
 .../apache/solr/store/blockcache/package-info.java |  23 --
 .../org/apache/solr/store/hdfs/HdfsDirectory.java  | 244 ---------------
 .../org/apache/solr/store/hdfs/HdfsFileReader.java | 105 -------
 .../org/apache/solr/store/hdfs/HdfsFileWriter.java |  57 ----
 .../apache/solr/store/hdfs/HdfsLockFactory.java    | 124 --------
 .../org/apache/solr/store/hdfs/package-info.java   |  22 --
 .../cloud/SharedFSAutoReplicaFailoverTest.java     |   2 +-
 .../cloud/hdfs/HdfsBasicDistributedZk2Test.java    |   2 +-
 .../cloud/hdfs/HdfsBasicDistributedZkTest.java     |   2 +-
 .../cloud/hdfs/HdfsChaosMonkeySafeLeaderTest.java  |   2 +-
 .../hdfs/HdfsCollectionsAPIDistributedZkTest.java  |   2 +-
 .../apache/solr/cloud/hdfs/HdfsNNFailoverTest.java |   2 +-
 .../solr/cloud/hdfs/HdfsRecoverLeaseTest.java      |   2 +-
 .../apache/solr/cloud/hdfs/HdfsRecoveryZkTest.java |   2 +-
 .../apache/solr/cloud/hdfs/HdfsSyncSliceTest.java  |   2 +-
 .../apache/solr/cloud/hdfs/HdfsThreadLeakTest.java |   3 +-
 .../cloud/hdfs/HdfsUnloadDistributedZkTest.java    |   2 +-
 .../hdfs/HdfsWriteToMultipleCollectionsTest.java   |  11 +-
 .../org/apache/solr/cloud/hdfs/StressHdfsTest.java |   2 +-
 .../apache/solr/core/HdfsDirectoryFactoryTest.java |   2 +-
 .../org/apache/solr/search/TestRecoveryHdfs.java   |   3 +-
 .../solr/store/blockcache/BlockCacheTest.java      | 110 -------
 .../solr/store/blockcache/BlockDirectoryTest.java  | 273 -----------------
 .../solr/store/blockcache/BufferStoreTest.java     |  93 ------
 .../apache/solr/store/hdfs/HdfsDirectoryTest.java  | 237 --------------
 .../solr/store/hdfs/HdfsLockFactoryTest.java       |  82 -----
 .../org/apache/solr/update/TestHdfsUpdateLog.java  |   2 +-
 .../org/apache/solr/util/BadHdfsThreadsFilter.java |  36 ---
 86 files changed, 3768 insertions(+), 3333 deletions(-)
 create mode 100644 lucene/hadoop/build.xml
 create mode 100644 lucene/hadoop/ivy.xml
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockCache.java
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockCacheKey.java
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockCacheLocation.java
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockDirectory.java
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockDirectoryCache.java
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockLocks.java
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BufferStore.java
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/blockcache/Cache.java
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/blockcache/CachedIndexOutput.java
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/blockcache/CustomBufferedIndexInput.java
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/blockcache/Metrics.java
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/blockcache/ReusedBufferedIndexOutput.java
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/blockcache/Store.java
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/blockcache/package-info.java
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/hdfs/HdfsDirectory.java
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/hdfs/HdfsFileReader.java
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/hdfs/HdfsFileWriter.java
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/hdfs/HdfsLockFactory.java
 create mode 100644 lucene/hadoop/src/java/org/apache/lucene/store/hdfs/package-info.java
 create mode 100644 lucene/hadoop/src/java/overview.html
 create mode 100644 lucene/hadoop/src/test/hadoop-tests.policy
 create mode 100644 lucene/hadoop/src/test/org/apache/lucene/store/blockcache/BlockCacheTest.java
 create mode 100644 lucene/hadoop/src/test/org/apache/lucene/store/blockcache/BlockDirectoryTest.java
 create mode 100644 lucene/hadoop/src/test/org/apache/lucene/store/blockcache/BufferStoreTest.java
 create mode 100644 lucene/hadoop/src/test/org/apache/lucene/store/hdfs/HdfsDirectoryTest.java
 create mode 100644 lucene/hadoop/src/test/org/apache/lucene/store/hdfs/HdfsLockFactoryTest.java
 create mode 100644 lucene/hadoop/src/test/org/apache/lucene/store/hdfs/HdfsTestUtil.java
 create mode 100644 lucene/test-framework/src/java/org/apache/lucene/util/BadHdfsThreadsFilter.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/blockcache/BlockCache.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/blockcache/BlockCacheKey.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/blockcache/BlockCacheLocation.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/blockcache/BlockDirectory.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/blockcache/BlockDirectoryCache.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/blockcache/BlockLocks.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/blockcache/BufferStore.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/blockcache/Cache.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/blockcache/CachedIndexOutput.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/blockcache/CustomBufferedIndexInput.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/blockcache/Metrics.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/blockcache/ReusedBufferedIndexOutput.java
 create mode 100644 solr/core/src/java/org/apache/solr/store/blockcache/SolrMetrics.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/blockcache/Store.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/blockcache/package-info.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/hdfs/HdfsDirectory.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/hdfs/HdfsFileReader.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/hdfs/HdfsFileWriter.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/hdfs/HdfsLockFactory.java
 delete mode 100644 solr/core/src/java/org/apache/solr/store/hdfs/package-info.java
 delete mode 100644 solr/core/src/test/org/apache/solr/store/blockcache/BlockCacheTest.java
 delete mode 100644 solr/core/src/test/org/apache/solr/store/blockcache/BlockDirectoryTest.java
 delete mode 100644 solr/core/src/test/org/apache/solr/store/blockcache/BufferStoreTest.java
 delete mode 100644 solr/core/src/test/org/apache/solr/store/hdfs/HdfsDirectoryTest.java
 delete mode 100644 solr/core/src/test/org/apache/solr/store/hdfs/HdfsLockFactoryTest.java
 delete mode 100644 solr/test-framework/src/java/org/apache/solr/util/BadHdfsThreadsFilter.java

diff --git a/dev-tools/idea/.idea/modules.xml b/dev-tools/idea/.idea/modules.xml
index 5c096a6..f701b76 100644
--- a/dev-tools/idea/.idea/modules.xml
+++ b/dev-tools/idea/.idea/modules.xml
@@ -27,6 +27,7 @@
       <module group="Lucene/Other" filepath="$PROJECT_DIR$/lucene/expressions/expressions.iml" />
       <module group="Lucene/Other" filepath="$PROJECT_DIR$/lucene/facet/facet.iml" />
       <module group="Lucene/Other" filepath="$PROJECT_DIR$/lucene/grouping/grouping.iml" />
+      <module group="Lucene/Other" filepath="$PROJECT_DIR$/lucene/hadoop/hadoop.iml" />
       <module group="Lucene/Other" filepath="$PROJECT_DIR$/lucene/highlighter/highlighter.iml" />
       <module group="Lucene/Other" filepath="$PROJECT_DIR$/lucene/join/join.iml" />
       <module group="Lucene/Other" filepath="$PROJECT_DIR$/lucene/memory/memory.iml" />
diff --git a/lucene/common-build.xml b/lucene/common-build.xml
index ecf1ca8..a1ae4fb 100644
--- a/lucene/common-build.xml
+++ b/lucene/common-build.xml
@@ -140,6 +140,18 @@
     <isset property="run.clover"/>
   </condition>
 
+  <!-- 
+    - We don't test HDFS on Java 7 because it causes permgen errors. Java 8 no longer has permgen.
+    - We don't want to run HDFS tests on Windows, because they require Cygwin.
+    If you have Cygwin or manually raised permgen, you can override this property on command line:
+  -->
+  <condition property="tests.disableHdfs" value="true">
+    <or>
+      <equals arg1="${build.java.runtime}" arg2="1.7"/>
+      <os family="windows"/>
+    </or>
+  </condition>
+
   <property name="tests.heapdump.args" value=""/>
 
   <!-- Override these in your local properties to your desire. -->
diff --git a/lucene/hadoop/build.xml b/lucene/hadoop/build.xml
new file mode 100644
index 0000000..05c36d3
--- /dev/null
+++ b/lucene/hadoop/build.xml
@@ -0,0 +1,35 @@
+<?xml version="1.0"?>
+
+<!--
+    Licensed to the Apache Software Foundation (ASF) under one or more
+    contributor license agreements.  See the NOTICE file distributed with
+    this work for additional information regarding copyright ownership.
+    The ASF licenses this file to You under the Apache License, Version 2.0
+    the "License"); you may not use this file except in compliance with
+    the License.  You may obtain a copy of the License at
+ 
+        http://www.apache.org/licenses/LICENSE-2.0
+ 
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an "AS IS" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+ -->
+
+<project name="hadoop" default="default">
+
+  <property name="tests.policy" location="src/test/hadoop-tests.policy"/>
+
+  <description>
+    Low level hadoop integration modules
+  </description>
+
+  <import file="../module-build.xml"/>
+
+  <path id="classpath">
+  	<fileset dir="lib" />
+    <path refid="base.classpath"/>
+  </path>
+
+</project>
diff --git a/lucene/hadoop/ivy.xml b/lucene/hadoop/ivy.xml
new file mode 100644
index 0000000..16e84fd
--- /dev/null
+++ b/lucene/hadoop/ivy.xml
@@ -0,0 +1,66 @@
+<!--
+   Licensed to the Apache Software Foundation (ASF) under one
+   or more contributor license agreements.  See the NOTICE file
+   distributed with this work for additional information
+   regarding copyright ownership.  The ASF licenses this file
+   to you under the Apache License, Version 2.0 (the
+   "License"); you may not use this file except in compliance
+   with the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing,
+   software distributed under the License is distributed on an
+   "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+   KIND, either express or implied.  See the License for the
+   specific language governing permissions and limitations
+   under the License.    
+-->
+<ivy-module version="2.0" xmlns:maven="http://ant.apache.org/ivy/maven">
+  <info organisation="org.apache.lucene" module="hadoop"/>
+
+  <configurations defaultconfmapping="compile->master;test->master">
+    <conf name="compile" transitive="false"/>
+    <!-- artifacts in the "test", "test.DfsMiniCluster", and "test.MiniKdc" configuration will go into solr/core/test-lib/ -->
+    <conf name="test" transitive="false"/>
+  </configurations>
+
+   <dependencies>
+     <dependency org="org.slf4j" name="slf4j-api" rev="${/org.slf4j/slf4j-api}" conf="compile"/>
+
+     <dependency org="org.apache.hadoop" name="hadoop-common" rev="${/org.apache.hadoop/hadoop-common}" conf="compile"/>
+     <!--
+       hadoop-hdfs, hadoop-annotations and hadoop-auth are runtime dependencies,
+       so even though they are not compile-time dependencies, they are included
+       here as such so that they are included in the runtime distribution.
+      -->
+     <dependency org="org.apache.hadoop" name="hadoop-hdfs" rev="${/org.apache.hadoop/hadoop-hdfs}" conf="compile"/>
+     <dependency org="com.github.ben-manes.caffeine" name="caffeine" rev="${/com.github.ben-manes.caffeine/caffeine}" conf="compile"/>
+     <dependency org="org.apache.hadoop" name="hadoop-annotations" rev="${/org.apache.hadoop/hadoop-annotations}" conf="compile"/>
+
+     <!-- The following are exclusively for testing -->
+     <dependency org="org.apache.hadoop" name="hadoop-common" rev="${/org.apache.hadoop/hadoop-common}" conf="test">
+       <artifact name="hadoop-common" type="test" ext="jar" maven:classifier="tests" />
+     </dependency>
+     <dependency org="org.apache.hadoop" name="hadoop-hdfs" rev="${/org.apache.hadoop/hadoop-hdfs}" conf="test">
+       <artifact name="hadoop-hdfs" type="test" ext="jar" maven:classifier="tests" />
+     </dependency>
+     <dependency org="org.apache.hadoop" name="hadoop-auth" rev="${/org.apache.hadoop/hadoop-auth}" conf="test"/>
+     <dependency org="javax.servlet" name="javax.servlet-api" rev="${/javax.servlet/javax.servlet-api}" conf="test"/>
+     <dependency org="org.slf4j" name="jcl-over-slf4j" rev="${/org.slf4j/jcl-over-slf4j}" conf="test"/>
+     <dependency org="com.google.guava" name="guava" rev="${/com.google.guava/guava}" conf="test"/>
+     <dependency org="commons-collections" name="commons-collections" rev="${/commons-collections/commons-collections}" conf="test"/>
+     <dependency org="commons-configuration" name="commons-configuration" rev="${/commons-configuration/commons-configuration}" conf="test"/>
+     <dependency org="commons-cli" name="commons-cli" rev="${/commons-cli/commons-cli}" conf="test"/>
+     <dependency org="commons-lang" name="commons-lang" rev="${/commons-lang/commons-lang}" conf="test"/>
+     <dependency org="commons-codec" name="commons-codec" rev="${/commons-codec/commons-codec}" conf="test"/>
+     <dependency org="commons-io" name="commons-io" rev="${/commons-io/commons-io}" conf="test"/>
+     <dependency org="com.google.protobuf" name="protobuf-java" rev="${/com.google.protobuf/protobuf-java}" conf="test"/>
+     <dependency org="org.mortbay.jetty" name="jetty" rev="${/org.mortbay.jetty/jetty}" conf="test"/>
+     <dependency org="org.mortbay.jetty" name="jetty-util" rev="${/org.mortbay.jetty/jetty-util}" conf="test"/>
+     <dependency org="com.sun.jersey" name="jersey-core" rev="${/com.sun.jersey/jersey-core}" conf="test"/>
+     <dependency org="com.sun.jersey" name="jersey-server" rev="${/com.sun.jersey/jersey-server}" conf="test"/>
+     <dependency org="org.htrace" name="htrace-core" rev="${/org.htrace/htrace-core}" conf="test"/>
+     <dependency org="log4j" name="log4j" rev="${/log4j/log4j}" conf="test"/>
+   </dependencies>
+</ivy-module>
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockCache.java b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockCache.java
new file mode 100644
index 0000000..e5b9153
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockCache.java
@@ -0,0 +1,204 @@
+package org.apache.lucene.store.blockcache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.nio.ByteBuffer;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import com.github.benmanes.caffeine.cache.Cache;
+import com.github.benmanes.caffeine.cache.Caffeine;
+import com.github.benmanes.caffeine.cache.RemovalListener;
+
+/**
+ * @lucene.experimental
+ */
+public class BlockCache {
+  
+  public static final int _128M = 134217728;
+  public static final int _32K = 32768;
+  private final Cache<BlockCacheKey,BlockCacheLocation> cache;
+  private final ByteBuffer[] banks;
+  private final BlockLocks[] locks;
+  private final AtomicInteger[] lockCounters;
+  private final int blockSize;
+  private final int numberOfBlocksPerBank;
+  private final int maxEntries;
+  private final Metrics metrics;
+  
+  public BlockCache(Metrics metrics, boolean directAllocation, long totalMemory) {
+    this(metrics, directAllocation, totalMemory, _128M);
+  }
+  
+  public BlockCache(Metrics metrics, boolean directAllocation,
+      long totalMemory, int slabSize) {
+    this(metrics, directAllocation, totalMemory, slabSize, _32K);
+  }
+  
+  public BlockCache(Metrics metrics, boolean directAllocation,
+      long totalMemory, int slabSize, int blockSize) {
+    this.metrics = metrics;
+    numberOfBlocksPerBank = slabSize / blockSize;
+    int numberOfBanks = (int) (totalMemory / slabSize);
+    
+    banks = new ByteBuffer[numberOfBanks];
+    locks = new BlockLocks[numberOfBanks];
+    lockCounters = new AtomicInteger[numberOfBanks];
+    maxEntries = (numberOfBlocksPerBank * numberOfBanks) - 1;
+    for (int i = 0; i < numberOfBanks; i++) {
+      if (directAllocation) {
+        banks[i] = ByteBuffer.allocateDirect(numberOfBlocksPerBank * blockSize);
+      } else {
+        banks[i] = ByteBuffer.allocate(numberOfBlocksPerBank * blockSize);
+      }
+      locks[i] = new BlockLocks(numberOfBlocksPerBank);
+      lockCounters[i] = new AtomicInteger();
+    }
+
+    RemovalListener<BlockCacheKey,BlockCacheLocation> listener = 
+        notification -> releaseLocation(notification.getValue());
+    cache = Caffeine.newBuilder()
+        .removalListener(listener)
+        .maximumSize(maxEntries)
+        .build();
+    this.blockSize = blockSize;
+  }
+  
+  public void release(BlockCacheKey key) {
+    cache.invalidate(key);
+  }
+  
+  private void releaseLocation(BlockCacheLocation location) {
+    if (location == null) {
+      return;
+    }
+    int bankId = location.getBankId();
+    int block = location.getBlock();
+    location.setRemoved(true);
+    locks[bankId].clear(block);
+    lockCounters[bankId].decrementAndGet();
+    metrics.blockCacheEviction.incrementAndGet();
+    metrics.blockCacheSize.decrementAndGet();
+  }
+  
+  public boolean store(BlockCacheKey blockCacheKey, int blockOffset,
+      byte[] data, int offset, int length) {
+    if (length + blockOffset > blockSize) {
+      throw new RuntimeException("Buffer size exceeded, expecting max ["
+          + blockSize + "] got length [" + length + "] with blockOffset ["
+          + blockOffset + "]");
+    }
+    BlockCacheLocation location = cache.getIfPresent(blockCacheKey);
+    boolean newLocation = false;
+    if (location == null) {
+      newLocation = true;
+      location = new BlockCacheLocation();
+      if (!findEmptyLocation(location)) {
+        return false;
+      }
+    }
+    if (location.isRemoved()) {
+      return false;
+    }
+    int bankId = location.getBankId();
+    int bankOffset = location.getBlock() * blockSize;
+    ByteBuffer bank = getBank(bankId);
+    bank.position(bankOffset + blockOffset);
+    bank.put(data, offset, length);
+    if (newLocation) {
+      cache.put(blockCacheKey.clone(), location);
+      metrics.blockCacheSize.incrementAndGet();
+    }
+    return true;
+  }
+  
+  public boolean fetch(BlockCacheKey blockCacheKey, byte[] buffer,
+      int blockOffset, int off, int length) {
+    BlockCacheLocation location = cache.getIfPresent(blockCacheKey);
+    if (location == null) {
+      return false;
+    }
+    if (location.isRemoved()) {
+      return false;
+    }
+    int bankId = location.getBankId();
+    int offset = location.getBlock() * blockSize;
+    location.touch();
+    ByteBuffer bank = getBank(bankId);
+    bank.position(offset + blockOffset);
+    bank.get(buffer, off, length);
+    return true;
+  }
+  
+  public boolean fetch(BlockCacheKey blockCacheKey, byte[] buffer) {
+    checkLength(buffer);
+    return fetch(blockCacheKey, buffer, 0, 0, blockSize);
+  }
+  
+  private boolean findEmptyLocation(BlockCacheLocation location) {
+    // This is a tight loop that will try and find a location to
+    // place the block before giving up
+    for (int j = 0; j < 10; j++) {
+      OUTER: for (int bankId = 0; bankId < banks.length; bankId++) {
+        AtomicInteger bitSetCounter = lockCounters[bankId];
+        BlockLocks bitSet = locks[bankId];
+        if (bitSetCounter.get() == numberOfBlocksPerBank) {
+          // if bitset is full
+          continue OUTER;
+        }
+        // this check needs to spin, if a lock was attempted but not obtained
+        // the rest of the bank should not be skipped
+        int bit = bitSet.nextClearBit(0);
+        INNER: while (bit != -1) {
+          if (bit >= numberOfBlocksPerBank) {
+            // bit set is full
+            continue OUTER;
+          }
+          if (!bitSet.set(bit)) {
+            // lock was not obtained
+            // this restarts at 0 because another block could have been unlocked
+            // while this was executing
+            bit = bitSet.nextClearBit(0);
+            continue INNER;
+          } else {
+            // lock obtained
+            location.setBankId(bankId);
+            location.setBlock(bit);
+            bitSetCounter.incrementAndGet();
+            return true;
+          }
+        }
+      }
+    }
+    return false;
+  }
+  
+  private void checkLength(byte[] buffer) {
+    if (buffer.length != blockSize) {
+      throw new RuntimeException("Buffer wrong size, expecting [" + blockSize
+          + "] got [" + buffer.length + "]");
+    }
+  }
+  
+  private ByteBuffer getBank(int bankId) {
+    return banks[bankId].duplicate();
+  }
+  
+  public int getSize() {
+    return cache.asMap().size();
+  }
+}
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockCacheKey.java b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockCacheKey.java
new file mode 100644
index 0000000..d48736a
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockCacheKey.java
@@ -0,0 +1,84 @@
+package org.apache.lucene.store.blockcache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+/**
+ * @lucene.experimental
+ */
+public class BlockCacheKey implements Cloneable {
+  
+  private long block;
+  private int file;
+  private String path;
+  
+  public String getPath() {
+    return path;
+  }
+
+  public void setPath(String path) {
+    this.path = path;
+  }
+
+  public long getBlock() {
+    return block;
+  }
+  
+  public int getFile() {
+    return file;
+  }
+  
+  public void setBlock(long block) {
+    this.block = block;
+  }
+  
+  public void setFile(int file) {
+    this.file = file;
+  }
+  
+  @Override
+  public int hashCode() {
+    final int prime = 31;
+    int result = 1;
+    result = prime * result + (int) (block ^ (block >>> 32));
+    result = prime * result + file;
+    result = prime * result + ((path == null) ? 0 : path.hashCode());
+    return result;
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (this == obj) return true;
+    if (obj == null) return false;
+    if (getClass() != obj.getClass()) return false;
+    BlockCacheKey other = (BlockCacheKey) obj;
+    if (block != other.block) return false;
+    if (file != other.file) return false;
+    if (path == null) {
+      if (other.path != null) return false;
+    } else if (!path.equals(other.path)) return false;
+    return true;
+  }
+
+  @Override
+  public BlockCacheKey clone() {
+    try {
+      return (BlockCacheKey) super.clone();
+    } catch (CloneNotSupportedException e) {
+      throw new RuntimeException(e);
+    }
+  }
+}
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockCacheLocation.java b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockCacheLocation.java
new file mode 100644
index 0000000..ff20e19
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockCacheLocation.java
@@ -0,0 +1,70 @@
+package org.apache.lucene.store.blockcache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.concurrent.atomic.AtomicBoolean;
+
+/**
+ * @lucene.experimental
+ */
+public class BlockCacheLocation {
+  
+  private int block;
+  private int bankId;
+  private long lastAccess = System.currentTimeMillis();
+  private long accesses;
+  private AtomicBoolean removed = new AtomicBoolean(false);
+  
+  public void setBlock(int block) {
+    this.block = block;
+  }
+  
+  public void setBankId(int bankId) {
+    this.bankId = bankId;
+  }
+  
+  public int getBlock() {
+    return block;
+  }
+  
+  public int getBankId() {
+    return bankId;
+  }
+  
+  public void touch() {
+    lastAccess = System.currentTimeMillis();
+    accesses++;
+  }
+  
+  public long getLastAccess() {
+    return lastAccess;
+  }
+  
+  public long getNumberOfAccesses() {
+    return accesses;
+  }
+  
+  public boolean isRemoved() {
+    return removed.get();
+  }
+  
+  public void setRemoved(boolean removed) {
+    this.removed.set(removed);
+  }
+  
+}
\ No newline at end of file
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockDirectory.java b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockDirectory.java
new file mode 100644
index 0000000..c435b3b
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockDirectory.java
@@ -0,0 +1,340 @@
+package org.apache.lucene.store.blockcache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.store.FilterDirectory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.hdfs.HdfsDirectory;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * @lucene.experimental
+ */
+public class BlockDirectory extends FilterDirectory {
+  public static Logger LOG = LoggerFactory.getLogger(BlockDirectory.class);
+  
+  public static final long BLOCK_SHIFT = 13; // 2^13 = 8,192 bytes per block
+  public static final long BLOCK_MOD = 0x1FFF;
+  public static final int BLOCK_SIZE = 1 << BLOCK_SHIFT;
+  
+  public static long getBlock(long pos) {
+    return pos >>> BLOCK_SHIFT;
+  }
+  
+  public static long getPosition(long pos) {
+    return pos & BLOCK_MOD;
+  }
+  
+  public static long getRealPosition(long block, long positionInBlock) {
+    return (block << BLOCK_SHIFT) + positionInBlock;
+  }
+  
+  public static Cache NO_CACHE = new Cache() {
+    
+    @Override
+    public void update(String name, long blockId, int blockOffset,
+        byte[] buffer, int offset, int length) {}
+    
+    @Override
+    public boolean fetch(String name, long blockId, int blockOffset, byte[] b,
+        int off, int lengthToReadInBlock) {
+      return false;
+    }
+    
+    @Override
+    public void delete(String name) {
+      
+    }
+    
+    @Override
+    public long size() {
+      return 0;
+    }
+    
+    @Override
+    public void renameCacheFile(String source, String dest) {}
+
+    @Override
+    public void releaseResources() {}
+  };
+  
+  private final int blockSize;
+  private final String dirName;
+  private final Cache cache;
+  private final Set<String> blockCacheFileTypes;
+  private final boolean blockCacheReadEnabled;
+  private final boolean blockCacheWriteEnabled;
+
+  public BlockDirectory(String dirName, Directory directory, Cache cache,
+      Set<String> blockCacheFileTypes, boolean blockCacheReadEnabled,
+      boolean blockCacheWriteEnabled) throws IOException {
+    super(directory);
+    this.dirName = dirName;
+    blockSize = BLOCK_SIZE;
+    this.cache = cache;
+    if (blockCacheFileTypes == null || blockCacheFileTypes.isEmpty()) {
+      this.blockCacheFileTypes = null;
+    } else {
+      this.blockCacheFileTypes = blockCacheFileTypes;
+    }
+    this.blockCacheReadEnabled = blockCacheReadEnabled;
+    if (!blockCacheReadEnabled) {
+      LOG.info("Block cache on read is disabled");
+    }
+    this.blockCacheWriteEnabled = blockCacheWriteEnabled;
+    if (!blockCacheWriteEnabled) {
+      LOG.info("Block cache on write is disabled");
+    }
+  }
+  
+  private IndexInput openInput(String name, int bufferSize, IOContext context)
+      throws IOException {
+    final IndexInput source = super.openInput(name, context);
+    if (useReadCache(name, context)) {
+      return new CachedIndexInput(source, blockSize, name,
+          getFileCacheName(name), cache, bufferSize);
+    }
+    return source;
+  }
+  
+  private boolean isCachableFile(String name) {
+    for (String ext : blockCacheFileTypes) {
+      if (name.endsWith(ext)) {
+        return true;
+      }
+    }
+    return false;
+  }
+  
+  @Override
+  public IndexInput openInput(final String name, IOContext context)
+      throws IOException {
+    return openInput(name, blockSize, context);
+  }
+  
+  static class CachedIndexInput extends CustomBufferedIndexInput {
+    private final Store store;
+    private IndexInput source;
+    private final int blockSize;
+    private final long fileLength;
+    private final String cacheName;
+    private final Cache cache;
+    
+    public CachedIndexInput(IndexInput source, int blockSize, String name,
+        String cacheName, Cache cache, int bufferSize) {
+      super(name, bufferSize);
+      this.source = source;
+      this.blockSize = blockSize;
+      fileLength = source.length();
+      this.cacheName = cacheName;
+      this.cache = cache;
+      store = BufferStore.instance(blockSize);
+    }
+    
+    @Override
+    public IndexInput clone() {
+      CachedIndexInput clone = (CachedIndexInput) super.clone();
+      clone.source = source.clone();
+      return clone;
+    }
+    
+    @Override
+    public long length() {
+      return source.length();
+    }
+    
+    @Override
+    protected void seekInternal(long pos) throws IOException {}
+    
+    @Override
+    protected void readInternal(byte[] b, int off, int len) throws IOException {
+      long position = getFilePointer();
+      while (len > 0) {
+        int length = fetchBlock(position, b, off, len);
+        position += length;
+        len -= length;
+        off += length;
+      }
+    }
+    
+    private int fetchBlock(long position, byte[] b, int off, int len)
+        throws IOException {
+      // read whole block into cache and then provide needed data
+      long blockId = getBlock(position);
+      int blockOffset = (int) getPosition(position);
+      int lengthToReadInBlock = Math.min(len, blockSize - blockOffset);
+      if (checkCache(blockId, blockOffset, b, off, lengthToReadInBlock)) {
+        return lengthToReadInBlock;
+      } else {
+        readIntoCacheAndResult(blockId, blockOffset, b, off,
+            lengthToReadInBlock);
+      }
+      return lengthToReadInBlock;
+    }
+    
+    private void readIntoCacheAndResult(long blockId, int blockOffset,
+        byte[] b, int off, int lengthToReadInBlock) throws IOException {
+      long position = getRealPosition(blockId, 0);
+      int length = (int) Math.min(blockSize, fileLength - position);
+      source.seek(position);
+      
+      byte[] buf = store.takeBuffer(blockSize);
+      source.readBytes(buf, 0, length);
+      System.arraycopy(buf, blockOffset, b, off, lengthToReadInBlock);
+      cache.update(cacheName, blockId, 0, buf, 0, blockSize);
+      store.putBuffer(buf);
+    }
+    
+    private boolean checkCache(long blockId, int blockOffset, byte[] b,
+        int off, int lengthToReadInBlock) {
+      return cache.fetch(cacheName, blockId, blockOffset, b, off,
+          lengthToReadInBlock);
+    }
+    
+    @Override
+    protected void closeInternal() throws IOException {
+      source.close();
+    }
+  }
+  
+  @Override
+  public void close() throws IOException {
+    try {
+      String[] files = listAll();
+      
+      for (String file : files) {
+        cache.delete(getFileCacheName(file));
+      }
+      
+    } catch (FileNotFoundException e) {
+      // the local file system folder may be gone
+    } finally {
+      super.close();
+      cache.releaseResources();
+    }
+  }
+  
+  String getFileCacheName(String name) throws IOException {
+    return getFileCacheLocation(name) + ":" + getFileModified(name);
+  }
+  
+  private long getFileModified(String name) throws IOException {
+    if (in instanceof FSDirectory) {
+      File directory = ((FSDirectory) in).getDirectory().toFile();
+      File file = new File(directory, name);
+      if (!file.exists()) {
+        throw new FileNotFoundException("File [" + name + "] not found");
+      }
+      return file.lastModified();
+    } else if (in instanceof HdfsDirectory) {
+      return ((HdfsDirectory) in).fileModified(name);
+    } else {
+      throw new UnsupportedOperationException();
+    }
+  }
+  
+  String getFileCacheLocation(String name) {
+    return dirName + "/" + name;
+  }
+  
+  /**
+   * Expert: mostly for tests
+   * 
+   * @lucene.experimental
+   */
+  public Cache getCache() {
+    return cache;
+  }
+  
+  /**
+   * Determine whether read caching should be used for a particular
+   * file/context.
+   */
+  boolean useReadCache(String name, IOContext context) {
+    if (!blockCacheReadEnabled) {
+      return false;
+    }
+    if (blockCacheFileTypes != null && !isCachableFile(name)) {
+      return false;
+    }
+    switch (context.context) {
+      default: {
+        return true;
+      }
+    }
+  }
+  
+  /**
+   * Determine whether write caching should be used for a particular
+   * file/context.
+   */
+  boolean useWriteCache(String name, IOContext context) {
+    if (!blockCacheWriteEnabled || name.startsWith(IndexFileNames.PENDING_SEGMENTS)) {
+      // for safety, don't bother caching pending commits.
+      // the cache does support renaming (renameCacheFile), but thats a scary optimization.
+      return false;
+    }
+    if (blockCacheFileTypes != null && !isCachableFile(name)) {
+      return false;
+    }
+    switch (context.context) {
+      case MERGE: {
+        // we currently don't cache any merge context writes
+        return false;
+      }
+      default: {
+        return true;
+      }
+    }
+  }
+  
+  @Override
+  public IndexOutput createOutput(String name, IOContext context)
+      throws IOException {
+    final IndexOutput dest = super.createOutput(name, context);
+    if (useWriteCache(name, context)) {
+      return new CachedIndexOutput(this, dest, blockSize, name, cache, blockSize);
+    }
+    return dest;
+  }
+  
+  public void deleteFile(String name) throws IOException {
+    cache.delete(getFileCacheName(name));
+    super.deleteFile(name);
+  }
+    
+  public boolean isBlockCacheReadEnabled() {
+    return blockCacheReadEnabled;
+  }
+
+  public boolean isBlockCacheWriteEnabled() {
+    return blockCacheWriteEnabled;
+  }
+  
+}
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockDirectoryCache.java b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockDirectoryCache.java
new file mode 100644
index 0000000..937a5ec
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockDirectoryCache.java
@@ -0,0 +1,125 @@
+package org.apache.lucene.store.blockcache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.atomic.AtomicInteger;
+
+/**
+ * @lucene.experimental
+ */
+public class BlockDirectoryCache implements Cache {
+  private final BlockCache blockCache;
+  private final AtomicInteger counter = new AtomicInteger();
+  private final Map<String,Integer> names = new ConcurrentHashMap<>();
+  private Set<BlockCacheKey> keysToRelease;
+  private final String path;
+  private final Metrics metrics;
+  
+  public BlockDirectoryCache(BlockCache blockCache, String path, Metrics metrics) {
+    this(blockCache, path, metrics, false);
+  }
+  
+  public BlockDirectoryCache(BlockCache blockCache, String path, Metrics metrics, boolean releaseBlocks) {
+    this.blockCache = blockCache;
+    this.path = path;
+    this.metrics = metrics;
+    if (releaseBlocks) {
+      keysToRelease = Collections.synchronizedSet(new HashSet<BlockCacheKey>());
+    }
+  }
+  
+  /**
+   * Expert: mostly for tests
+   * 
+   * @lucene.experimental
+   */
+  public BlockCache getBlockCache() {
+    return blockCache;
+  }
+  
+  @Override
+  public void delete(String name) {
+    names.remove(name);
+  }
+  
+  @Override
+  public void update(String name, long blockId, int blockOffset, byte[] buffer,
+      int offset, int length) {
+    Integer file = names.get(name);
+    if (file == null) {
+      file = counter.incrementAndGet();
+      names.put(name, file);
+    }
+    BlockCacheKey blockCacheKey = new BlockCacheKey();
+    blockCacheKey.setPath(path);
+    blockCacheKey.setBlock(blockId);
+    blockCacheKey.setFile(file);
+    if (blockCache.store(blockCacheKey, blockOffset, buffer, offset, length) && keysToRelease != null) {
+      keysToRelease.add(blockCacheKey);
+    }
+  }
+  
+  @Override
+  public boolean fetch(String name, long blockId, int blockOffset, byte[] b,
+      int off, int lengthToReadInBlock) {
+    Integer file = names.get(name);
+    if (file == null) {
+      return false;
+    }
+    BlockCacheKey blockCacheKey = new BlockCacheKey();
+    blockCacheKey.setPath(path);
+    blockCacheKey.setBlock(blockId);
+    blockCacheKey.setFile(file);
+    boolean fetch = blockCache.fetch(blockCacheKey, b, blockOffset, off,
+        lengthToReadInBlock);
+    if (fetch) {
+      metrics.blockCacheHit.incrementAndGet();
+    } else {
+      metrics.blockCacheMiss.incrementAndGet();
+    }
+    return fetch;
+  }
+  
+  @Override
+  public long size() {
+    return blockCache.getSize();
+  }
+  
+  @Override
+  public void renameCacheFile(String source, String dest) {
+    Integer file = names.remove(source);
+    // possible if the file is empty
+    if (file != null) {
+      names.put(dest, file);
+    }
+  }
+
+  @Override
+  public void releaseResources() {
+    if (keysToRelease != null) {
+      for (BlockCacheKey key : keysToRelease) {
+        blockCache.release(key);
+      }
+    }
+  }
+}
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockLocks.java b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockLocks.java
new file mode 100644
index 0000000..6aa5c29
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BlockLocks.java
@@ -0,0 +1,99 @@
+package org.apache.lucene.store.blockcache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.concurrent.atomic.AtomicLongArray;
+
+import org.apache.lucene.util.LongBitSet;
+
+/**
+ * @lucene.experimental
+ */
+public class BlockLocks {
+  
+  private AtomicLongArray bits;
+  private int wlen;
+  
+  public BlockLocks(long numBits) {
+    int length = LongBitSet.bits2words(numBits);
+    bits = new AtomicLongArray(length);
+    wlen = length;
+  }
+  
+  /**
+   * Find the next clear bit in the bit set.
+   * 
+   * @param index
+   *          index
+   * @return next next bit
+   */
+  public int nextClearBit(int index) {
+    int i = index >> 6;
+    if (i >= wlen) return -1;
+    int subIndex = index & 0x3f; // index within the word
+    long word = ~bits.get(i) >> subIndex; // skip all the bits to the right of
+                                          // index
+    if (word != 0) {
+      return (i << 6) + subIndex + Long.numberOfTrailingZeros(word);
+    }
+    while (++i < wlen) {
+      word = ~bits.get(i);
+      if (word != 0) {
+        return (i << 6) + Long.numberOfTrailingZeros(word);
+      }
+    }
+    return -1;
+  }
+  
+  /**
+   * Thread safe set operation that will set the bit if and only if the bit was
+   * not previously set.
+   * 
+   * @param index
+   *          the index position to set.
+   * @return returns true if the bit was set and false if it was already set.
+   */
+  public boolean set(int index) {
+    int wordNum = index >> 6; // div 64
+    int bit = index & 0x3f; // mod 64
+    long bitmask = 1L << bit;
+    long word, oword;
+    do {
+      word = bits.get(wordNum);
+      // if set another thread stole the lock
+      if ((word & bitmask) != 0) {
+        return false;
+      }
+      oword = word;
+      word |= bitmask;
+    } while (!bits.compareAndSet(wordNum, oword, word));
+    return true;
+  }
+  
+  public void clear(int index) {
+    int wordNum = index >> 6;
+    int bit = index & 0x03f;
+    long bitmask = 1L << bit;
+    long word, oword;
+    do {
+      word = bits.get(wordNum);
+      oword = word;
+      word &= ~bitmask;
+    } while (!bits.compareAndSet(wordNum, oword, word));
+  }
+}
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BufferStore.java b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BufferStore.java
new file mode 100644
index 0000000..c460a9c
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/BufferStore.java
@@ -0,0 +1,130 @@
+package org.apache.lucene.store.blockcache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.concurrent.ArrayBlockingQueue;
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ConcurrentMap;
+import java.util.concurrent.atomic.AtomicLong;
+
+/**
+ * @lucene.experimental
+ */
+public class BufferStore implements Store {
+
+  private static final Store EMPTY = new Store() {
+
+    @Override
+    public byte[] takeBuffer(int bufferSize) {
+      return new byte[bufferSize];
+    }
+
+    @Override
+    public void putBuffer(byte[] buffer) {
+    }
+  };
+
+  private final static ConcurrentMap<Integer, BufferStore> bufferStores = new ConcurrentHashMap<>();
+
+  private final BlockingQueue<byte[]> buffers;
+
+  private final int bufferSize;
+
+  private final AtomicLong shardBuffercacheAllocate;
+  private final AtomicLong shardBuffercacheLost;
+
+  public synchronized static void initNewBuffer(int bufferSize, long totalAmount, Metrics metrics) {
+    if (totalAmount == 0) {
+      return;
+    }
+    BufferStore bufferStore = bufferStores.get(bufferSize);
+    if (bufferStore == null) {
+      long count = totalAmount / bufferSize;
+      if (count > Integer.MAX_VALUE) {
+        count = Integer.MAX_VALUE;
+      }
+      AtomicLong shardBuffercacheLost = new AtomicLong(0);
+      AtomicLong shardBuffercacheAllocate = new AtomicLong(0);
+      if (metrics != null) {
+        shardBuffercacheLost = metrics.shardBuffercacheLost;
+        shardBuffercacheAllocate = metrics.shardBuffercacheAllocate;
+      }
+      BufferStore store = new BufferStore(bufferSize, (int) count, shardBuffercacheAllocate, shardBuffercacheLost);
+      bufferStores.put(bufferSize, store);
+    }
+  }
+
+  private BufferStore(int bufferSize, int count, AtomicLong shardBuffercacheAllocate, AtomicLong shardBuffercacheLost) {
+    this.bufferSize = bufferSize;
+    this.shardBuffercacheAllocate = shardBuffercacheAllocate;
+    this.shardBuffercacheLost = shardBuffercacheLost;
+    buffers = setupBuffers(bufferSize, count);
+  }
+
+  private static BlockingQueue<byte[]> setupBuffers(int bufferSize, int count) {
+    BlockingQueue<byte[]> queue = new ArrayBlockingQueue<>(count);
+    for (int i = 0; i < count; i++) {
+      queue.add(new byte[bufferSize]);
+    }
+    return queue;
+  }
+
+  public static Store instance(int bufferSize) {
+    BufferStore bufferStore = bufferStores.get(bufferSize);
+    if (bufferStore == null) {
+      return EMPTY;
+    }
+    return bufferStore;
+  }
+
+  @Override
+  public byte[] takeBuffer(int bufferSize) {
+    if (this.bufferSize != bufferSize) {
+      throw new RuntimeException("Buffer with length [" + bufferSize + "] does not match buffer size of ["
+          + bufferSize + "]");
+    }
+    return newBuffer(buffers.poll());
+  }
+
+  @Override
+  public void putBuffer(byte[] buffer) {
+    if (buffer == null) {
+      return;
+    }
+    if (buffer.length != bufferSize) {
+      throw new RuntimeException("Buffer with length [" + buffer.length + "] does not match buffer size of ["
+          + bufferSize + "]");
+    }
+    checkReturn(buffers.offer(buffer));
+  }
+
+  private void checkReturn(boolean accepted) {
+    if (!accepted) {
+      shardBuffercacheLost.incrementAndGet();
+    }
+  }
+
+  private byte[] newBuffer(byte[] buf) {
+    if (buf != null) {
+      return buf;
+    }
+    shardBuffercacheAllocate.incrementAndGet();
+    return new byte[bufferSize];
+  }
+}
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/Cache.java b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/Cache.java
new file mode 100644
index 0000000..1bbeada
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/Cache.java
@@ -0,0 +1,70 @@
+package org.apache.lucene.store.blockcache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * @lucene.experimental
+ */
+public interface Cache {
+  
+  /**
+   * Remove a file from the cache.
+   * 
+   * @param name
+   *          cache file name
+   */
+  void delete(String name);
+  
+  /**
+   * Update the content of the specified cache file. Creates cache entry if
+   * necessary.
+   * 
+   */
+  void update(String name, long blockId, int blockOffset, byte[] buffer,
+      int offset, int length);
+  
+  /**
+   * Fetch the specified cache file content.
+   * 
+   * @return true if cached content found, otherwise return false
+   */
+  boolean fetch(String name, long blockId, int blockOffset, byte[] b, int off,
+      int lengthToReadInBlock);
+  
+  /**
+   * Number of entries in the cache.
+   */
+  long size();
+  
+  /**
+   * Expert: Rename the specified file in the cache. Allows a file to be moved
+   * without invalidating the cache.
+   * 
+   * @param source
+   *          original name
+   * @param dest
+   *          final name
+   */
+  void renameCacheFile(String source, String dest);
+
+  /**
+   * Release any resources associated with the cache.
+   */
+  void releaseResources();
+  
+}
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/CachedIndexOutput.java b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/CachedIndexOutput.java
new file mode 100644
index 0000000..a580804
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/CachedIndexOutput.java
@@ -0,0 +1,87 @@
+package org.apache.lucene.store.blockcache;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.store.IndexOutput;
+
+/**
+ * Cache the blocks as they are written. The cache file name is the name of
+ * the file until the file is closed, at which point the cache is updated
+ * to include the last modified date (which is unknown until that point).
+ * @lucene.experimental
+ */
+public class CachedIndexOutput extends ReusedBufferedIndexOutput {
+  private final BlockDirectory directory;
+  private final IndexOutput dest;
+  private final int blockSize;
+  private final String name;
+  private final String location;
+  private final Cache cache;
+  
+  public CachedIndexOutput(BlockDirectory directory, IndexOutput dest,
+      int blockSize, String name, Cache cache, int bufferSize) {
+    super("dest=" + dest + " name=" + name, bufferSize);
+    this.directory = directory;
+    this.dest = dest;
+    this.blockSize = blockSize;
+    this.name = name;
+    this.location = directory.getFileCacheLocation(name);
+    this.cache = cache;
+  }
+
+  @Override
+  public void closeInternal() throws IOException {
+    dest.close();
+    cache.renameCacheFile(location, directory.getFileCacheName(name));
+  }
+  
+  private int writeBlock(long position, byte[] b, int offset, int length)
+      throws IOException {
+    // read whole block into cache and then provide needed data
+    long blockId = BlockDirectory.getBlock(position);
+    int blockOffset = (int) BlockDirectory.getPosition(position);
+    int lengthToWriteInBlock = Math.min(length, blockSize - blockOffset);
+    
+    // write the file and copy into the cache
+    dest.writeBytes(b, offset, lengthToWriteInBlock);
+    cache.update(location, blockId, blockOffset, b, offset,
+        lengthToWriteInBlock);
+    
+    return lengthToWriteInBlock;
+  }
+  
+  @Override
+  public void writeInternal(byte[] b, int offset, int length)
+      throws IOException {
+    long position = getBufferStart();
+    while (length > 0) {
+      int len = writeBlock(position, b, offset, length);
+      position += len;
+      length -= len;
+      offset += len;
+    }
+  }
+
+  @Override
+  public long getChecksum() throws IOException {
+    flushBufferToCache();
+    return dest.getChecksum();
+  }
+}
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/CustomBufferedIndexInput.java b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/CustomBufferedIndexInput.java
new file mode 100644
index 0000000..6c6bbd5
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/CustomBufferedIndexInput.java
@@ -0,0 +1,284 @@
+package org.apache.lucene.store.blockcache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.EOFException;
+import java.io.IOException;
+
+import org.apache.lucene.store.BufferedIndexInput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+
+/**
+ * @lucene.experimental
+ */
+public abstract class CustomBufferedIndexInput extends IndexInput {
+  
+  public static final int BUFFER_SIZE = 32768;
+  
+  private int bufferSize = BUFFER_SIZE;
+  
+  protected byte[] buffer;
+  
+  private long bufferStart = 0; // position in file of buffer
+  private int bufferLength = 0; // end of valid bytes
+  private int bufferPosition = 0; // next byte to read
+  
+  private Store store;
+  
+  @Override
+  public byte readByte() throws IOException {
+    if (bufferPosition >= bufferLength) refill();
+    return buffer[bufferPosition++];
+  }
+  
+  public CustomBufferedIndexInput(String resourceDesc) {
+    this(resourceDesc, BUFFER_SIZE);
+  }
+  
+  public CustomBufferedIndexInput(String resourceDesc, int bufferSize) {
+    super(resourceDesc);
+    checkBufferSize(bufferSize);
+    this.bufferSize = bufferSize;
+    this.store = BufferStore.instance(bufferSize);
+  }
+  
+  private void checkBufferSize(int bufferSize) {
+    if (bufferSize <= 0) throw new IllegalArgumentException(
+        "bufferSize must be greater than 0 (got " + bufferSize + ")");
+  }
+  
+  @Override
+  public void readBytes(byte[] b, int offset, int len) throws IOException {
+    readBytes(b, offset, len, true);
+  }
+  
+  @Override
+  public void readBytes(byte[] b, int offset, int len, boolean useBuffer)
+      throws IOException {
+    
+    if (len <= (bufferLength - bufferPosition)) {
+      // the buffer contains enough data to satisfy this request
+      if (len > 0) // to allow b to be null if len is 0...
+      System.arraycopy(buffer, bufferPosition, b, offset, len);
+      bufferPosition += len;
+    } else {
+      // the buffer does not have enough data. First serve all we've got.
+      int available = bufferLength - bufferPosition;
+      if (available > 0) {
+        System.arraycopy(buffer, bufferPosition, b, offset, available);
+        offset += available;
+        len -= available;
+        bufferPosition += available;
+      }
+      // and now, read the remaining 'len' bytes:
+      if (useBuffer && len < bufferSize) {
+        // If the amount left to read is small enough, and
+        // we are allowed to use our buffer, do it in the usual
+        // buffered way: fill the buffer and copy from it:
+        refill();
+        if (bufferLength < len) {
+          // Throw an exception when refill() could not read len bytes:
+          System.arraycopy(buffer, 0, b, offset, bufferLength);
+          throw new IOException("read past EOF");
+        } else {
+          System.arraycopy(buffer, 0, b, offset, len);
+          bufferPosition = len;
+        }
+      } else {
+        // The amount left to read is larger than the buffer
+        // or we've been asked to not use our buffer -
+        // there's no performance reason not to read it all
+        // at once. Note that unlike the previous code of
+        // this function, there is no need to do a seek
+        // here, because there's no need to reread what we
+        // had in the buffer.
+        long after = bufferStart + bufferPosition + len;
+        if (after > length()) throw new IOException("read past EOF");
+        readInternal(b, offset, len);
+        bufferStart = after;
+        bufferPosition = 0;
+        bufferLength = 0; // trigger refill() on read
+      }
+    }
+  }
+  
+  @Override
+  public int readInt() throws IOException {
+    if (4 <= (bufferLength - bufferPosition)) {
+      return ((buffer[bufferPosition++] & 0xFF) << 24)
+          | ((buffer[bufferPosition++] & 0xFF) << 16)
+          | ((buffer[bufferPosition++] & 0xFF) << 8)
+          | (buffer[bufferPosition++] & 0xFF);
+    } else {
+      return super.readInt();
+    }
+  }
+  
+  @Override
+  public long readLong() throws IOException {
+    if (8 <= (bufferLength - bufferPosition)) {
+      final int i1 = ((buffer[bufferPosition++] & 0xff) << 24)
+          | ((buffer[bufferPosition++] & 0xff) << 16)
+          | ((buffer[bufferPosition++] & 0xff) << 8)
+          | (buffer[bufferPosition++] & 0xff);
+      final int i2 = ((buffer[bufferPosition++] & 0xff) << 24)
+          | ((buffer[bufferPosition++] & 0xff) << 16)
+          | ((buffer[bufferPosition++] & 0xff) << 8)
+          | (buffer[bufferPosition++] & 0xff);
+      return (((long) i1) << 32) | (i2 & 0xFFFFFFFFL);
+    } else {
+      return super.readLong();
+    }
+  }
+  
+  @Override
+  public int readVInt() throws IOException {
+    if (5 <= (bufferLength - bufferPosition)) {
+      byte b = buffer[bufferPosition++];
+      int i = b & 0x7F;
+      for (int shift = 7; (b & 0x80) != 0; shift += 7) {
+        b = buffer[bufferPosition++];
+        i |= (b & 0x7F) << shift;
+      }
+      return i;
+    } else {
+      return super.readVInt();
+    }
+  }
+  
+  @Override
+  public long readVLong() throws IOException {
+    if (9 <= bufferLength - bufferPosition) {
+      byte b = buffer[bufferPosition++];
+      long i = b & 0x7F;
+      for (int shift = 7; (b & 0x80) != 0; shift += 7) {
+        b = buffer[bufferPosition++];
+        i |= (b & 0x7FL) << shift;
+      }
+      return i;
+    } else {
+      return super.readVLong();
+    }
+  }
+  
+  private void refill() throws IOException {
+    long start = bufferStart + bufferPosition;
+    long end = start + bufferSize;
+    if (end > length()) // don't read past EOF
+    end = length();
+    int newLength = (int) (end - start);
+    if (newLength <= 0) throw new EOFException("read past EOF");
+    
+    if (buffer == null) {
+      buffer = store.takeBuffer(bufferSize);
+      seekInternal(bufferStart);
+    }
+    readInternal(buffer, 0, newLength);
+    bufferLength = newLength;
+    bufferStart = start;
+    bufferPosition = 0;
+  }
+  
+  @Override
+  public final void close() throws IOException {
+    closeInternal();
+    store.putBuffer(buffer);
+    buffer = null;
+  }
+  
+  protected abstract void closeInternal() throws IOException;
+  
+  /**
+   * Expert: implements buffer refill. Reads bytes from the current position in
+   * the input.
+   * 
+   * @param b
+   *          the array to read bytes into
+   * @param offset
+   *          the offset in the array to start storing bytes
+   * @param length
+   *          the number of bytes to read
+   */
+  protected abstract void readInternal(byte[] b, int offset, int length)
+      throws IOException;
+  
+  @Override
+  public long getFilePointer() {
+    return bufferStart + bufferPosition;
+  }
+  
+  @Override
+  public void seek(long pos) throws IOException {
+    if (pos >= bufferStart && pos < (bufferStart + bufferLength)) bufferPosition = (int) (pos - bufferStart); // seek
+                                                                                                              // within
+                                                                                                              // buffer
+    else {
+      bufferStart = pos;
+      bufferPosition = 0;
+      bufferLength = 0; // trigger refill() on read()
+      seekInternal(pos);
+    }
+  }
+  
+  /**
+   * Expert: implements seek. Sets current position in this file, where the next
+   * {@link #readInternal(byte[],int,int)} will occur.
+   * 
+   * @see #readInternal(byte[],int,int)
+   */
+  protected abstract void seekInternal(long pos) throws IOException;
+  
+  @Override
+  public IndexInput clone() {
+    CustomBufferedIndexInput clone = (CustomBufferedIndexInput) super.clone();
+    
+    clone.buffer = null;
+    clone.bufferLength = 0;
+    clone.bufferPosition = 0;
+    clone.bufferStart = getFilePointer();
+    
+    return clone;
+  }
+  
+  @Override
+  public IndexInput slice(String sliceDescription, long offset, long length) throws IOException {
+    return BufferedIndexInput.wrap(sliceDescription, this, offset, length);
+  }
+
+  /**
+   * Flushes the in-memory bufer to the given output, copying at most
+   * <code>numBytes</code>.
+   * <p>
+   * <b>NOTE:</b> this method does not refill the buffer, however it does
+   * advance the buffer position.
+   * 
+   * @return the number of bytes actually flushed from the in-memory buffer.
+   */
+  protected int flushBuffer(IndexOutput out, long numBytes) throws IOException {
+    int toCopy = bufferLength - bufferPosition;
+    if (toCopy > numBytes) {
+      toCopy = (int) numBytes;
+    }
+    if (toCopy > 0) {
+      out.writeBytes(buffer, bufferPosition, toCopy);
+      bufferPosition += toCopy;
+    }
+    return toCopy;
+  }
+}
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/Metrics.java b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/Metrics.java
new file mode 100644
index 0000000..af24cc6
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/Metrics.java
@@ -0,0 +1,68 @@
+package org.apache.lucene.store.blockcache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.atomic.AtomicLong;
+
+/**
+ * Simple class that provides metrics on block cache operations.
+ * @lucene.experimental
+ */
+public class Metrics {
+  
+  public static class MethodCall {
+    public AtomicLong invokes = new AtomicLong();
+    public AtomicLong times = new AtomicLong();
+  }
+
+  public AtomicLong blockCacheHit = new AtomicLong(0);
+  public AtomicLong blockCacheMiss = new AtomicLong(0);
+  public AtomicLong blockCacheEviction = new AtomicLong(0);
+  public AtomicLong blockCacheSize = new AtomicLong(0);
+  public AtomicLong rowReads = new AtomicLong(0);
+  public AtomicLong rowWrites = new AtomicLong(0);
+  public AtomicLong recordReads = new AtomicLong(0);
+  public AtomicLong recordWrites = new AtomicLong(0);
+  public AtomicLong queriesExternal = new AtomicLong(0);
+  public AtomicLong queriesInternal = new AtomicLong(0);
+  public AtomicLong shardBuffercacheAllocate = new AtomicLong(0);
+  public AtomicLong shardBuffercacheLost = new AtomicLong(0);
+  public Map<String, MethodCall> methodCalls = new ConcurrentHashMap<>();
+  
+  public AtomicLong tableCount = new AtomicLong(0);
+  public AtomicLong rowCount = new AtomicLong(0);
+  public AtomicLong recordCount = new AtomicLong(0);
+  public AtomicLong indexCount = new AtomicLong(0);
+  public AtomicLong indexMemoryUsage = new AtomicLong(0);
+  public AtomicLong segmentCount = new AtomicLong(0);
+
+  public static void main(String[] args) throws InterruptedException {
+    Metrics metrics = new Metrics();
+    MethodCall methodCall = new MethodCall();
+    metrics.methodCalls.put("test", methodCall);
+    for (int i = 0; i < 100; i++) {
+      metrics.blockCacheHit.incrementAndGet();
+      metrics.blockCacheMiss.incrementAndGet();
+      methodCall.invokes.incrementAndGet();
+      methodCall.times.addAndGet(56000000);
+      Thread.sleep(500);
+    }
+  }
+}
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/ReusedBufferedIndexOutput.java b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/ReusedBufferedIndexOutput.java
new file mode 100644
index 0000000..3dae5dd
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/ReusedBufferedIndexOutput.java
@@ -0,0 +1,165 @@
+package org.apache.lucene.store.blockcache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.store.IndexOutput;
+
+/**
+ * @lucene.experimental
+ */
+public abstract class ReusedBufferedIndexOutput extends IndexOutput {
+  
+  public static final int BUFFER_SIZE = 1024;
+  
+  private int bufferSize = BUFFER_SIZE;
+  
+  protected byte[] buffer;
+  
+  /** position in the file of buffer */
+  private long bufferStart = 0;
+  /** end of valid bytes */
+  private int bufferLength = 0;
+  /** next byte to write */
+  private int bufferPosition = 0;
+  /** total length of the file */
+  private long fileLength = 0;
+  
+  private final Store store;
+  
+  public ReusedBufferedIndexOutput(String resourceDescription) {
+    this(resourceDescription, BUFFER_SIZE);
+  }
+  
+  public ReusedBufferedIndexOutput(String resourceDescription, int bufferSize) {
+    super(resourceDescription);
+    checkBufferSize(bufferSize);
+    this.bufferSize = bufferSize;
+    store = BufferStore.instance(bufferSize);
+    buffer = store.takeBuffer(this.bufferSize);
+  }
+  
+  protected long getBufferStart() {
+    return bufferStart;
+  }
+  
+  private void checkBufferSize(int bufferSize) {
+    if (bufferSize <= 0) throw new IllegalArgumentException(
+        "bufferSize must be greater than 0 (got " + bufferSize + ")");
+  }
+  
+  /** Write the buffered bytes to cache */
+  protected void flushBufferToCache() throws IOException {
+    writeInternal(buffer, 0, bufferLength);
+    
+    bufferStart += bufferLength;
+    bufferLength = 0;
+    bufferPosition = 0;
+  }
+  
+  protected abstract void closeInternal() throws IOException;
+  
+  @Override
+  public void close() throws IOException {
+    flushBufferToCache();
+    closeInternal();
+    store.putBuffer(buffer);
+    buffer = null;
+  }
+  
+  @Override
+  public long getFilePointer() {
+    return bufferStart + bufferPosition;
+  }
+  
+  @Override
+  public void writeByte(byte b) throws IOException {
+    if (bufferPosition >= bufferSize) {
+      flushBufferToCache();
+    }
+    if (getFilePointer() >= fileLength) {
+      fileLength++;
+    }
+    buffer[bufferPosition++] = b;
+    if (bufferPosition > bufferLength) {
+      bufferLength = bufferPosition;
+    }
+  }
+  
+  /**
+   * Expert: implements buffer flushing to cache. Writes bytes to the current
+   * position in the output.
+   * 
+   * @param b
+   *          the array of bytes to write
+   * @param offset
+   *          the offset in the array of bytes to write
+   * @param length
+   *          the number of bytes to write
+   */
+  protected abstract void writeInternal(byte[] b, int offset, int length)
+      throws IOException;
+  
+  @Override
+  public void writeBytes(byte[] b, int offset, int length) throws IOException {
+    if (getFilePointer() + length > fileLength) {
+      fileLength = getFilePointer() + length;
+    }
+    if (length <= bufferSize - bufferPosition) {
+      // the buffer contains enough space to satisfy this request
+      if (length > 0) { // to allow b to be null if len is 0...
+        System.arraycopy(b, offset, buffer, bufferPosition, length);
+      }
+      bufferPosition += length;
+      if (bufferPosition > bufferLength) {
+        bufferLength = bufferPosition;
+      }
+    } else {
+      // the buffer does not have enough space. First buffer all we've got.
+      int available = bufferSize - bufferPosition;
+      if (available > 0) {
+        System.arraycopy(b, offset, buffer, bufferPosition, available);
+        offset += available;
+        length -= available;
+        bufferPosition = bufferSize;
+        bufferLength = bufferSize;
+      }
+      
+      flushBufferToCache();
+      
+      // and now, write the remaining 'length' bytes:
+      if (length < bufferSize) {
+        // If the amount left to write is small enough do it in the usual
+        // buffered way:
+        System.arraycopy(b, offset, buffer, 0, length);
+        bufferPosition = length;
+        bufferLength = length;
+      } else {
+        // The amount left to write is larger than the buffer
+        // there's no performance reason not to write it all
+        // at once.
+        writeInternal(b, offset, length);
+        bufferStart += length;
+        bufferPosition = 0;
+        bufferLength = 0;
+      }
+      
+    }
+  }
+}
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/Store.java b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/Store.java
new file mode 100644
index 0000000..b283ce4
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/Store.java
@@ -0,0 +1,29 @@
+package org.apache.lucene.store.blockcache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * @lucene.experimental
+ */
+public interface Store {
+
+  byte[] takeBuffer(int bufferSize);
+
+  void putBuffer(byte[] buffer);
+
+}
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/package-info.java b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/package-info.java
new file mode 100644
index 0000000..d4a4028
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/blockcache/package-info.java
@@ -0,0 +1,23 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+ 
+/** 
+ * An HDFS blockcache implementation.
+ */
+package org.apache.lucene.store.blockcache;
+
+
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/hdfs/HdfsDirectory.java b/lucene/hadoop/src/java/org/apache/lucene/store/hdfs/HdfsDirectory.java
new file mode 100644
index 0000000..d0e5f76
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/hdfs/HdfsDirectory.java
@@ -0,0 +1,243 @@
+package org.apache.lucene.store.hdfs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileContext;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.ipc.RemoteException;
+import org.apache.lucene.store.BaseDirectory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.LockFactory;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.store.blockcache.CustomBufferedIndexInput;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class HdfsDirectory extends BaseDirectory {
+  public static Logger LOG = LoggerFactory.getLogger(HdfsDirectory.class);
+  
+  public static final int BUFFER_SIZE = 8192;
+  
+  private static final String LF_EXT = ".lf";
+  protected final Path hdfsDirPath;
+  protected final Configuration configuration;
+  
+  private final FileSystem fileSystem;
+  private final FileContext fileContext;
+  
+  public HdfsDirectory(Path hdfsDirPath, Configuration configuration) throws IOException {
+    this(hdfsDirPath, HdfsLockFactory.INSTANCE, configuration);
+  }
+
+  public HdfsDirectory(Path hdfsDirPath, LockFactory lockFactory, Configuration configuration)
+      throws IOException {
+    super(lockFactory);
+    this.hdfsDirPath = hdfsDirPath;
+    this.configuration = configuration;
+    fileSystem = FileSystem.get(hdfsDirPath.toUri(), configuration);
+    fileContext = FileContext.getFileContext(hdfsDirPath.toUri(), configuration);
+    
+    while (true) {
+      try {
+        if (!fileSystem.exists(hdfsDirPath)) {
+          boolean success = fileSystem.mkdirs(hdfsDirPath);
+          if (!success) {
+            throw new RuntimeException("Could not create directory: " + hdfsDirPath);
+          }
+        } else {
+          fileSystem.mkdirs(hdfsDirPath); // check for safe mode
+        }
+        
+        break;
+      } catch (RemoteException e) {
+        if (e.getClassName().equals("org.apache.hadoop.hdfs.server.namenode.SafeModeException")) {
+          LOG.warn("The NameNode is in SafeMode - will wait 5 seconds and try again.");
+          try {
+            Thread.sleep(5000);
+          } catch (InterruptedException e1) {
+            Thread.interrupted();
+          }
+          continue;
+        }
+        IOUtils.closeWhileHandlingException(fileSystem);
+        throw new RuntimeException("Problem creating directory: " + hdfsDirPath, e);
+      } catch (Exception e) {
+        IOUtils.closeWhileHandlingException(fileSystem);
+        throw new RuntimeException("Problem creating directory: " + hdfsDirPath, e);
+      }
+    }
+  }
+  
+  @Override
+  public void close() throws IOException {
+    LOG.info("Closing hdfs directory {}", hdfsDirPath);
+    fileSystem.close();
+  }
+  
+  @Override
+  public IndexOutput createOutput(String name, IOContext context) throws IOException {
+    return new HdfsFileWriter(getFileSystem(), new Path(hdfsDirPath, name));
+  }
+  
+  private String[] getNormalNames(List<String> files) {
+    int size = files.size();
+    for (int i = 0; i < size; i++) {
+      String str = files.get(i);
+      files.set(i, toNormalName(str));
+    }
+    return files.toArray(new String[] {});
+  }
+  
+  private String toNormalName(String name) {
+    if (name.endsWith(LF_EXT)) {
+      return name.substring(0, name.length() - 3);
+    }
+    return name;
+  }
+  
+  @Override
+  public IndexInput openInput(String name, IOContext context)
+      throws IOException {
+    return openInput(name, BUFFER_SIZE);
+  }
+  
+  private IndexInput openInput(String name, int bufferSize) throws IOException {
+    return new HdfsIndexInput(name, getFileSystem(), new Path(
+        hdfsDirPath, name), BUFFER_SIZE);
+  }
+  
+  @Override
+  public void deleteFile(String name) throws IOException {
+    Path path = new Path(hdfsDirPath, name);
+    LOG.debug("Deleting {}", path);
+    getFileSystem().delete(path, false);
+  }
+  
+  @Override
+  public void renameFile(String source, String dest) throws IOException {
+    Path sourcePath = new Path(hdfsDirPath, source);
+    Path destPath = new Path(hdfsDirPath, dest);
+    fileContext.rename(sourcePath, destPath);
+  }
+
+  @Override
+  public long fileLength(String name) throws IOException {
+    return HdfsFileReader.getLength(getFileSystem(),
+        new Path(hdfsDirPath, name));
+  }
+  
+  public long fileModified(String name) throws IOException {
+    FileStatus fileStatus = getFileSystem().getFileStatus(
+        new Path(hdfsDirPath, name));
+    return fileStatus.getModificationTime();
+  }
+  
+  @Override
+  public String[] listAll() throws IOException {
+    FileStatus[] listStatus = getFileSystem().listStatus(hdfsDirPath);
+    List<String> files = new ArrayList<>();
+    if (listStatus == null) {
+      return new String[] {};
+    }
+    for (FileStatus status : listStatus) {
+      files.add(status.getPath().getName());
+    }
+    return getNormalNames(files);
+  }
+
+  public Path getHdfsDirPath() {
+    return hdfsDirPath;
+  }
+  
+  public FileSystem getFileSystem() {
+    return fileSystem;
+  }
+  
+  public Configuration getConfiguration() {
+    return configuration;
+  }
+  
+  static class HdfsIndexInput extends CustomBufferedIndexInput {
+    public static Logger LOG = LoggerFactory
+        .getLogger(HdfsIndexInput.class);
+    
+    private final Path path;
+    private final FSDataInputStream inputStream;
+    private final long length;
+    private boolean clone = false;
+    
+    public HdfsIndexInput(String name, FileSystem fileSystem, Path path,
+        int bufferSize) throws IOException {
+      super(name);
+      this.path = path;
+      LOG.debug("Opening normal index input on {}", path);
+      FileStatus fileStatus = fileSystem.getFileStatus(path);
+      length = fileStatus.getLen();
+      inputStream = fileSystem.open(path, bufferSize);
+    }
+    
+    @Override
+    protected void readInternal(byte[] b, int offset, int length)
+        throws IOException {
+      inputStream.readFully(getFilePointer(), b, offset, length);
+    }
+    
+    @Override
+    protected void seekInternal(long pos) throws IOException {
+
+    }
+    
+    @Override
+    protected void closeInternal() throws IOException {
+      LOG.debug("Closing normal index input on {}", path);
+      if (!clone) {
+        inputStream.close();
+      }
+    }
+    
+    @Override
+    public long length() {
+      return length;
+    }
+    
+    @Override
+    public IndexInput clone() {
+      HdfsIndexInput clone = (HdfsIndexInput) super.clone();
+      clone.clone = true;
+      return clone;
+    }
+  }
+  
+  @Override
+  public void sync(Collection<String> names) throws IOException {
+    LOG.debug("Sync called on {}", Arrays.toString(names.toArray()));
+  }
+  
+}
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/hdfs/HdfsFileReader.java b/lucene/hadoop/src/java/org/apache/lucene/store/hdfs/HdfsFileReader.java
new file mode 100644
index 0000000..a5244da
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/hdfs/HdfsFileReader.java
@@ -0,0 +1,105 @@
+package org.apache.lucene.store.hdfs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.lucene.store.DataInput;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * @lucene.experimental
+ */
+public class HdfsFileReader extends DataInput {
+  
+  public static Logger LOG = LoggerFactory.getLogger(HdfsFileReader.class);
+  
+  private final Path path;
+  private FSDataInputStream inputStream;
+  private long length;
+  private boolean isClone;
+  
+  public HdfsFileReader(FileSystem fileSystem, Path path, int bufferSize)
+      throws IOException {
+    this.path = path;
+    LOG.debug("Opening reader on {}", path);
+    if (!fileSystem.exists(path)) {
+      throw new FileNotFoundException(path.toString());
+    }
+    inputStream = fileSystem.open(path, bufferSize);
+    FileStatus fileStatus = fileSystem.getFileStatus(path);
+    length = fileStatus.getLen();
+  }
+  
+  public HdfsFileReader(FileSystem fileSystem, Path path) throws IOException {
+    this(fileSystem, path, HdfsDirectory.BUFFER_SIZE);
+  }
+  
+  public long length() {
+    return length;
+  }
+  
+  public void seek(long pos) throws IOException {
+    inputStream.seek(pos);
+  }
+  
+  public void close() throws IOException {
+    if (!isClone) {
+      inputStream.close();
+    }
+    LOG.debug("Closing reader on {}", path);
+  }
+  
+  /**
+   * This method should never be used!
+   */
+  @Override
+  public byte readByte() throws IOException {
+    LOG.warn("Should not be used!");
+    return inputStream.readByte();
+  }
+  
+  @Override
+  public void readBytes(byte[] b, int offset, int len) throws IOException {
+    while (len > 0) {
+      int lenRead = inputStream.read(b, offset, len);
+      offset += lenRead;
+      len -= lenRead;
+    }
+  }
+  
+  public static long getLength(FileSystem fileSystem, Path path)
+      throws IOException {
+    FileStatus fileStatus = fileSystem.getFileStatus(path);
+    return fileStatus.getLen();
+  }
+  
+  @Override
+  public DataInput clone() {
+    HdfsFileReader reader = (HdfsFileReader) super.clone();
+    reader.isClone = true;
+    return reader;
+  }
+  
+}
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/hdfs/HdfsFileWriter.java b/lucene/hadoop/src/java/org/apache/lucene/store/hdfs/HdfsFileWriter.java
new file mode 100644
index 0000000..0c1b5e4
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/hdfs/HdfsFileWriter.java
@@ -0,0 +1,57 @@
+package org.apache.lucene.store.hdfs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.OutputStream;
+import java.util.EnumSet;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CreateFlag;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FsServerDefaults;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.lucene.store.OutputStreamIndexOutput;
+
+/**
+ * @lucene.experimental
+ */
+public class HdfsFileWriter extends OutputStreamIndexOutput {
+  
+  public static final String HDFS_SYNC_BLOCK = "solr.hdfs.sync.block";
+  public static final int BUFFER_SIZE = 16384;
+  
+  public HdfsFileWriter(FileSystem fileSystem, Path path) throws IOException {
+    super("fileSystem=" + fileSystem + " path=" + path, getOutputStream(fileSystem, path), BUFFER_SIZE);
+  }
+  
+  private static final OutputStream getOutputStream(FileSystem fileSystem, Path path) throws IOException {
+    Configuration conf = fileSystem.getConf();
+    FsServerDefaults fsDefaults = fileSystem.getServerDefaults(path);
+    EnumSet<CreateFlag> flags = EnumSet.of(CreateFlag.CREATE,
+        CreateFlag.OVERWRITE);
+    if (Boolean.getBoolean(HDFS_SYNC_BLOCK)) {
+      flags.add(CreateFlag.SYNC_BLOCK);
+    }
+    return fileSystem.create(path, FsPermission.getDefault()
+        .applyUMask(FsPermission.getUMask(conf)), flags, fsDefaults
+        .getFileBufferSize(), fsDefaults.getReplication(), fsDefaults
+        .getBlockSize(), null);
+  }
+}
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/hdfs/HdfsLockFactory.java b/lucene/hadoop/src/java/org/apache/lucene/store/hdfs/HdfsLockFactory.java
new file mode 100644
index 0000000..052aedf
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/hdfs/HdfsLockFactory.java
@@ -0,0 +1,124 @@
+package org.apache.lucene.store.hdfs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileAlreadyExistsException;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.ipc.RemoteException;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.Lock;
+import org.apache.lucene.store.LockFactory;
+import org.apache.lucene.store.LockObtainFailedException;
+import org.apache.lucene.store.LockReleaseFailedException;
+import org.apache.lucene.util.IOUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class HdfsLockFactory extends LockFactory {
+  public static Logger log = LoggerFactory.getLogger(HdfsLockFactory.class);
+  
+  public static final HdfsLockFactory INSTANCE = new HdfsLockFactory();
+  
+  private HdfsLockFactory() {}
+  
+  @Override
+  public Lock obtainLock(Directory dir, String lockName) throws IOException {
+    if (!(dir instanceof HdfsDirectory)) {
+      throw new UnsupportedOperationException("HdfsLockFactory can only be used with HdfsDirectory subclasses, got: " + dir);
+    }
+    final HdfsDirectory hdfsDir = (HdfsDirectory) dir;
+    final Configuration conf = hdfsDir.getConfiguration();
+    final Path lockPath = hdfsDir.getHdfsDirPath();
+    final Path lockFile = new Path(lockPath, lockName);
+    
+    FSDataOutputStream file = null;
+    final FileSystem fs = FileSystem.get(lockPath.toUri(), conf);
+    while (true) {
+      try {
+        if (!fs.exists(lockPath)) {
+          boolean success = fs.mkdirs(lockPath);
+          if (!success) {
+            throw new RuntimeException("Could not create directory: " + lockPath);
+          }
+        } else {
+          // just to check for safe mode
+          fs.mkdirs(lockPath);
+        }
+        
+        file = fs.create(lockFile, false);
+        break;
+      } catch (FileAlreadyExistsException e) {
+        throw new LockObtainFailedException("Cannot obtain lock file: " + lockFile, e);
+      } catch (RemoteException e) {
+        if (e.getClassName().equals(
+            "org.apache.hadoop.hdfs.server.namenode.SafeModeException")) {
+          log.warn("The NameNode is in SafeMode - will wait 5 seconds and try again.");
+          try {
+            Thread.sleep(5000);
+          } catch (InterruptedException e1) {
+            Thread.interrupted();
+          }
+          continue;
+        }
+        throw new LockObtainFailedException("Cannot obtain lock file: " + lockFile, e);
+      } catch (IOException e) {
+        throw new LockObtainFailedException("Cannot obtain lock file: " + lockFile, e);
+      } finally {
+        IOUtils.closeWhileHandlingException(file);
+      }
+    }
+
+    return new HdfsLock(fs, lockFile);
+  }
+  
+  private static final class HdfsLock extends Lock {
+    
+    private final FileSystem fs;
+    private final Path lockFile;
+    private volatile boolean closed;
+    
+    HdfsLock(FileSystem fs, Path lockFile) {
+      this.fs = fs;
+      this.lockFile = lockFile;
+    }
+    
+    @Override
+    public void close() throws IOException {
+      if (closed) {
+        return;
+      }
+      try {
+        if (fs.exists(lockFile) && !fs.delete(lockFile, false)) {
+          throw new LockReleaseFailedException("failed to delete: " + lockFile);
+        }
+      } finally {
+        IOUtils.closeWhileHandlingException(fs);
+      }
+    }
+
+    @Override
+    public void ensureValid() throws IOException {
+      // no idea how to implement this on HDFS
+    }
+  }
+}
diff --git a/lucene/hadoop/src/java/org/apache/lucene/store/hdfs/package-info.java b/lucene/hadoop/src/java/org/apache/lucene/store/hdfs/package-info.java
new file mode 100644
index 0000000..e32fd37
--- /dev/null
+++ b/lucene/hadoop/src/java/org/apache/lucene/store/hdfs/package-info.java
@@ -0,0 +1,22 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+ 
+/** 
+ * An HDFS Directory implementation.
+ */
+package org.apache.lucene.store.hdfs;
+
diff --git a/lucene/hadoop/src/java/overview.html b/lucene/hadoop/src/java/overview.html
new file mode 100644
index 0000000..025ac74
--- /dev/null
+++ b/lucene/hadoop/src/java/overview.html
@@ -0,0 +1,26 @@
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+  <head>
+    <title>
+      hadoop
+    </title>
+  </head>
+  <body>
+  hadoop
+  </body>
+</html>
\ No newline at end of file
diff --git a/lucene/hadoop/src/test/hadoop-tests.policy b/lucene/hadoop/src/test/hadoop-tests.policy
new file mode 100644
index 0000000..b1b1d6a
--- /dev/null
+++ b/lucene/hadoop/src/test/hadoop-tests.policy
@@ -0,0 +1,139 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// Policy file to prevent tests from writing outside the test sandbox directory
+// (must be given as a sysprop: tests.sandbox.dir)
+// This policy also disallows stuff like listening on network ports of interfaces
+// different than 127.0.0.1.
+
+// PLEASE NOTE: You may need to enable other permissions when new tests are added,
+// everything not allowed here is forbidden!
+
+grant {
+  permission java.lang.RuntimePermission "*";
+
+  // JAAS needs these and wont tell you it does !
+  // Loads the jaas library for your platform e.g. jaas_unix
+  permission java.lang.RuntimePermission "loadLibrary.*";
+  permission java.lang.RuntimePermission "accessClassInPackage.sun.security.krb5";
+    
+  // contain read access to only what we need:
+  permission java.io.FilePermission "<<ALL FILES>>", "read,execute,write,delete";
+  // 3rd party jar resources (where symlinks are not supported), test-files/ resources
+  permission java.io.FilePermission "${common.dir}${/}-", "read";
+  // 3rd party jar resources (where symlinks are supported)
+  permission java.io.FilePermission "${user.home}${/}.ivy2${/}cache${/}-", "read";
+  // system jar resources, and let TestIndexWriterOnJRECrash fork its jvm
+  permission java.io.FilePermission "${java.home}${/}-", "read,execute";
+  // should be enclosed within common.dir, but just in case:
+  permission java.io.FilePermission "${junit4.childvm.cwd}", "read";
+
+  // write only to sandbox
+  permission java.io.FilePermission "${junit4.childvm.cwd}${/}temp", "read,write,delete";
+  permission java.io.FilePermission "${junit4.childvm.cwd}${/}temp${/}-", "read,write,delete";
+  permission java.io.FilePermission "${junit4.childvm.cwd}${/}jacoco.db", "write";
+  permission java.io.FilePermission "${junit4.childvm.cwd}", "read,write,delete";
+  permission java.io.FilePermission "${junit4.tempDir}${/}*", "read,write,delete";
+  permission java.io.FilePermission "${clover.db.dir}${/}-", "read,write,delete";
+
+  // all possibilities of accepting/binding connections on localhost with ports >=1024:
+  permission java.net.SocketPermission "localhost:1024-", "accept,listen";
+  permission java.net.SocketPermission "127.0.0.1:1024-", "accept,listen";
+  permission java.net.SocketPermission "[::1]:1024-", "accept,listen";
+  
+  // Allow connecting to the internet anywhere
+  permission java.net.SocketPermission "*", "connect,resolve";
+
+  // read, writer access to all system properties:
+  permission java.util.PropertyPermission "*", "read,write";
+
+  // access to hadoop env variables for hadoop test
+  permission java.lang.RuntimePermission "getenv.HADOOP_HOME";
+  permission java.lang.RuntimePermission "getenv.HADOOP_USER_NAME";
+  permission java.lang.RuntimePermission "getenv.HADOOP_PROXY_USER";
+  permission java.lang.RuntimePermission "getenv.HADOOP_TOKEN_FILE_LOCATION";
+  permission java.lang.RuntimePermission "getenv.hadoop.metrics.init.mode";
+
+
+  // needed by hadoop-auth
+  permission javax.security.auth.AuthPermission "*";
+  permission javax.security.auth.PrivateCredentialPermission "org.apache.hadoop.security.Credentials * \"*\"", "read";
+  permission java.security.SecurityPermission "putProviderProperty.SaslPlainServer";
+  permission java.security.SecurityPermission "insertProvider.SaslPlainServer";
+  permission java.lang.RuntimePermission "getenv.HADOOP_JAAS_DEBUG";
+  permission java.lang.RuntimePermission "getenv.KRB5CCNAME";
+
+  // needed by hadoop to setup the MBean server
+  permission javax.management.MBeanPermission "*", "*";
+  permission javax.management.MBeanServerPermission "*";
+  permission javax.management.MBeanTrustPermission "*";
+
+  // needed by gson serialization of junit4 runner: TODO clean that up
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
+  permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
+
+  // needed by junit4 runner to capture sysout/syserr:
+  permission java.lang.RuntimePermission "setIO";
+
+  // needed by randomized runner to catch failures from other threads:
+  permission java.lang.RuntimePermission "setDefaultUncaughtExceptionHandler";
+
+  // needed by randomized runner getTopThreadGroup:
+  permission java.lang.RuntimePermission "modifyThreadGroup";
+
+  // needed by tests e.g. shutting down executors:
+  permission java.lang.RuntimePermission "modifyThread";
+
+  // needed for tons of test hacks etc
+  permission java.lang.RuntimePermission "getStackTrace";
+
+  // needed for mock filesystems in tests
+  permission java.lang.RuntimePermission "fileSystemProvider";
+
+  // needed for mock filesystems in tests (to capture implCloseChannel) 
+  permission java.lang.RuntimePermission "accessClassInPackage.sun.nio.ch";
+
+  // needed by junit nested compat tests (due to static fields reflection), TODO clean these up:
+  permission java.lang.RuntimePermission "accessClassInPackage.sun.util.calendar";
+  permission java.lang.RuntimePermission "accessClassInPackage.sun.util.locale";
+  permission java.lang.RuntimePermission "accessClassInPackage.sun.nio.fs";
+
+  // needed by queryparser/ NLS., TODO clean this up:
+  permission java.lang.RuntimePermission "accessClassInPackage.sun.util";
+
+  // needed for test of IOUtils.spins (maybe it can be avoided)
+  permission java.lang.RuntimePermission "getFileStoreAttributes";
+
+  // analyzers/morfologik: needed for a horrible context classloader hack for solr in morfologikfilter: nuke this
+  permission java.lang.RuntimePermission "setContextClassLoader";
+
+  // analyzers/uima: needed by UIMA message localization... (?)
+  permission java.lang.RuntimePermission "createSecurityManager";
+  permission java.lang.RuntimePermission "createClassLoader";
+
+  // expressions TestCustomFunctions (only on older java8?)
+  permission java.lang.RuntimePermission "getClassLoader";
+
+  // needed to test unmap hack on platforms that support it
+  permission java.lang.RuntimePermission "accessClassInPackage.sun.misc";
+
+  // needed by cyberneko usage by benchmarks on J9
+  permission java.lang.RuntimePermission "accessClassInPackage.org.apache.xerces.util";
+
+  // needed by jacoco to dump coverage
+  permission java.lang.RuntimePermission "shutdownHooks";
+};
diff --git a/lucene/hadoop/src/test/org/apache/lucene/store/blockcache/BlockCacheTest.java b/lucene/hadoop/src/test/org/apache/lucene/store/blockcache/BlockCacheTest.java
new file mode 100644
index 0000000..ca00c57
--- /dev/null
+++ b/lucene/hadoop/src/test/org/apache/lucene/store/blockcache/BlockCacheTest.java
@@ -0,0 +1,110 @@
+package org.apache.lucene.store.blockcache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+import java.util.Random;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+
+public class BlockCacheTest extends LuceneTestCase {
+  @Test
+  public void testBlockCache() {
+    int blocksInTest = 2000000;
+    int blockSize = 1024;
+    
+    int slabSize = blockSize * 4096;
+    long totalMemory = 2 * slabSize;
+    
+    BlockCache blockCache = new BlockCache(new Metrics(), true, totalMemory, slabSize, blockSize);
+    byte[] buffer = new byte[1024];
+    Random random = random();
+    byte[] newData = new byte[blockSize];
+    AtomicLong hitsInCache = new AtomicLong();
+    AtomicLong missesInCache = new AtomicLong();
+    long storeTime = 0;
+    long fetchTime = 0;
+    int passes = 10000;
+
+    BlockCacheKey blockCacheKey = new BlockCacheKey();
+
+    for (int j = 0; j < passes; j++) {
+      long block = random.nextInt(blocksInTest);
+      int file = 0;
+      blockCacheKey.setBlock(block);
+      blockCacheKey.setFile(file);
+      blockCacheKey.setPath("/");
+
+      if (blockCache.fetch(blockCacheKey, buffer)) {
+        hitsInCache.incrementAndGet();
+      } else {
+        missesInCache.incrementAndGet();
+      }
+
+      byte[] testData = testData(random, blockSize, newData);
+      long t1 = System.nanoTime();
+      blockCache.store(blockCacheKey, 0, testData, 0, blockSize);
+      storeTime += (System.nanoTime() - t1);
+
+      long t3 = System.nanoTime();
+      if (blockCache.fetch(blockCacheKey, buffer)) {
+        fetchTime += (System.nanoTime() - t3);
+        assertTrue(Arrays.equals(testData, buffer));
+      }
+    }
+    System.out.println("Cache Hits    = " + hitsInCache.get());
+    System.out.println("Cache Misses  = " + missesInCache.get());
+    System.out.println("Store         = " + (storeTime / (double) passes) / 1000000.0);
+    System.out.println("Fetch         = " + (fetchTime / (double) passes) / 1000000.0);
+    System.out.println("# of Elements = " + blockCache.getSize());
+  }
+
+  /**
+   * Verify checking of buffer size limits against the cached block size.
+   */
+  @Test
+  public void testLongBuffer() {
+    Random random = random();
+    int blockSize = BlockCache._32K;
+    int slabSize = blockSize * 1024;
+    long totalMemory = 2 * slabSize;
+
+    BlockCache blockCache = new BlockCache(new Metrics(), true, totalMemory, slabSize);
+    BlockCacheKey blockCacheKey = new BlockCacheKey();
+    blockCacheKey.setBlock(0);
+    blockCacheKey.setFile(0);
+    blockCacheKey.setPath("/");
+    byte[] newData = new byte[blockSize*3];
+    byte[] testData = testData(random, blockSize, newData);
+
+    assertTrue(blockCache.store(blockCacheKey, 0, testData, 0, blockSize));
+    assertTrue(blockCache.store(blockCacheKey, 0, testData, blockSize, blockSize));
+    assertTrue(blockCache.store(blockCacheKey, 0, testData, blockSize*2, blockSize));
+
+    assertTrue(blockCache.store(blockCacheKey, 1, testData, 0, blockSize - 1));
+    assertTrue(blockCache.store(blockCacheKey, 1, testData, blockSize, blockSize - 1));
+    assertTrue(blockCache.store(blockCacheKey, 1, testData, blockSize*2, blockSize - 1));
+  }
+
+  private static byte[] testData(Random random, int size, byte[] buf) {
+    random.nextBytes(buf);
+    return buf;
+  }
+}
diff --git a/lucene/hadoop/src/test/org/apache/lucene/store/blockcache/BlockDirectoryTest.java b/lucene/hadoop/src/test/org/apache/lucene/store/blockcache/BlockDirectoryTest.java
new file mode 100644
index 0000000..ff62df5
--- /dev/null
+++ b/lucene/hadoop/src/test/org/apache/lucene/store/blockcache/BlockDirectoryTest.java
@@ -0,0 +1,274 @@
+package org.apache.lucene.store.blockcache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.io.IOException;
+import java.util.Map;
+import java.util.Random;
+
+import com.github.benmanes.caffeine.cache.Caffeine;
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.MergeInfo;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.After;
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.Test;
+
+public class BlockDirectoryTest extends LuceneTestCase {
+
+  private class MapperCache implements Cache {
+    public Map<String, byte[]> map = Caffeine.newBuilder()
+        .maximumSize(8)
+        .<String, byte[]>build()
+        .asMap();
+
+    @Override
+    public void update(String name, long blockId, int blockOffset, byte[] buffer, int offset, int length) {
+      byte[] cached = map.get(name + blockId);
+      if (cached != null) {
+        int newlen = Math.max(cached.length, blockOffset + length);
+        byte[] b = new byte[newlen];
+        System.arraycopy(cached, 0, b, 0, cached.length);
+        System.arraycopy(buffer, offset, b, blockOffset, length);
+        cached = b;
+      } else {
+        cached = copy(blockOffset, buffer, offset, length);
+      }
+      map.put(name + blockId, cached);
+    }
+
+    private byte[] copy(int blockOffset, byte[] buffer, int offset, int length) {
+      byte[] b = new byte[length + blockOffset];
+      System.arraycopy(buffer, offset, b, blockOffset, length);
+      return b;
+    }
+
+    @Override
+    public boolean fetch(String name, long blockId, int blockOffset, byte[] b, int off, int lengthToReadInBlock) {
+      // return false;
+      byte[] data = map.get(name + blockId);
+      if (data == null) {
+        return false;
+      }
+      System.arraycopy(data, blockOffset, b, off, lengthToReadInBlock);
+      return true;
+    }
+
+    @Override
+    public void delete(String name) {
+
+    }
+
+    @Override
+    public long size() {
+      return map.size();
+    }
+
+    @Override
+    public void renameCacheFile(String source, String dest) {
+    }
+
+    @Override
+    public void releaseResources() {}
+  }
+
+  private static final int MAX_NUMBER_OF_WRITES = 10000;
+  private static final int MIN_FILE_SIZE = 100;
+  private static final int MAX_FILE_SIZE = 100000;
+  private static final int MIN_BUFFER_SIZE = 1;
+  private static final int MAX_BUFFER_SIZE = 12000;
+  private static final int MAX_NUMBER_OF_READS = 20000;
+  private BlockDirectory directory;
+  private File file;
+  private Random random;
+  private MapperCache mapperCache;
+
+  @Before
+  public void setUp() throws Exception {
+    super.setUp();
+    file = LuceneTestCase.createTempDir().toFile();
+    FSDirectory dir = FSDirectory.open(new File(file, "base").toPath());
+    mapperCache = new MapperCache();
+    directory = new BlockDirectory("test", dir, mapperCache, null, true, true);
+    random = LuceneTestCase.random();
+  }
+  
+  @After
+  public void tearDown() throws Exception {
+    super.tearDown();
+    directory.close();
+  }
+
+  @Test
+  public void testEOF() throws IOException {
+    Directory fsDir = FSDirectory.open(new File(file, "normal").toPath());
+    String name = "test.eof";
+    createFile(name, fsDir, directory);
+    long fsLength = fsDir.fileLength(name);
+    long hdfsLength = directory.fileLength(name);
+    Assert.assertEquals(fsLength, hdfsLength);
+    testEof(name, fsDir, fsLength);
+    testEof(name, directory, hdfsLength);
+    fsDir.close();
+  }
+
+  private void testEof(String name, Directory directory, long length) throws IOException {
+    IndexInput input = directory.openInput(name, new IOContext());
+    try {
+    input.seek(length);
+      try {
+        input.readByte();
+        Assert.fail("should throw eof");
+      } catch (IOException e) {
+      }
+    } finally {
+      input.close();
+    }
+  }
+
+  @Test
+  public void testRandomAccessWrites() throws IOException {
+    long t1 = System.nanoTime();
+
+    int i = 0;
+    try {
+      for (; i < 10; i++) {
+        Directory fsDir = FSDirectory.open(new File(file, "normal").toPath());
+        String name = getName();
+        createFile(name, fsDir, directory);
+        assertInputsEquals(name, fsDir, directory);
+      }
+    } catch (Exception e) {
+      e.printStackTrace();
+      Assert.fail("Test failed on pass [" + i + "]");
+    }
+    long t2 = System.nanoTime();
+    System.out.println("Total time is " + ((t2 - t1)/1000000) + "ms");
+  }
+
+  @Test
+  public void testRandomAccessWritesLargeCache() throws IOException {
+    mapperCache.map = Caffeine.newBuilder()
+        .maximumSize(10_000)
+        .<String, byte[]>build()
+        .asMap();
+    testRandomAccessWrites();
+  }
+
+  private void assertInputsEquals(String name, Directory fsDir, Directory hdfs) throws IOException {
+    int reads = random.nextInt(MAX_NUMBER_OF_READS);
+    IndexInput fsInput = fsDir.openInput(name, new IOContext());
+    IndexInput hdfsInput = hdfs.openInput(name, new IOContext());
+    Assert.assertEquals(fsInput.length(), hdfsInput.length());
+    int fileLength = (int) fsInput.length();
+    for (int i = 0; i < reads; i++) {
+      int rnd;
+      if (fileLength == 0) {
+        rnd = 0;
+      } else {
+        rnd = random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE, fileLength));
+      }
+
+      byte[] fsBuf = new byte[rnd + MIN_BUFFER_SIZE];
+      byte[] hdfsBuf = new byte[fsBuf.length];
+      int offset = random.nextInt(fsBuf.length);
+      int length = random.nextInt(fsBuf.length - offset);
+      
+      int pos;
+      if (fileLength == 0) {
+        pos = 0;
+      } else {
+        pos = random.nextInt(fileLength - length);
+      }
+    
+      fsInput.seek(pos);
+      fsInput.readBytes(fsBuf, offset, length);
+      hdfsInput.seek(pos);
+      hdfsInput.readBytes(hdfsBuf, offset, length);
+      for (int f = offset; f < length; f++) {
+        if (fsBuf[f] != hdfsBuf[f]) {
+          Assert.fail("read [" + i + "]");
+        }
+      }
+    }
+    fsInput.close();
+    hdfsInput.close();
+  }
+
+  private void createFile(String name, Directory fsDir, Directory hdfs) throws IOException {
+    int writes = random.nextInt(MAX_NUMBER_OF_WRITES);
+    int fileLength = random.nextInt(MAX_FILE_SIZE - MIN_FILE_SIZE) + MIN_FILE_SIZE;
+    IndexOutput fsOutput = fsDir.createOutput(name, IOContext.DEFAULT);
+    IndexOutput hdfsOutput = hdfs.createOutput(name, IOContext.DEFAULT);
+    for (int i = 0; i < writes; i++) {
+      byte[] buf = new byte[random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE, fileLength)) + MIN_BUFFER_SIZE];
+      random.nextBytes(buf);
+      int offset = random.nextInt(buf.length);
+      int length = random.nextInt(buf.length - offset);
+      fsOutput.writeBytes(buf, offset, length);
+      hdfsOutput.writeBytes(buf, offset, length);
+    }
+    fsOutput.close();
+    hdfsOutput.close();
+  }
+
+  private String getName() {
+    return Long.toString(Math.abs(random.nextLong()));
+  }
+
+  public static void rm(File file) {
+    try {
+      IOUtils.rm(file.toPath());
+    } catch (Throwable ignored) {
+      // TODO: should this class care if a file couldnt be deleted?
+      // this just emulates previous behavior, where only SecurityException would be handled.
+    }
+  }
+
+  /**
+   * Verify the configuration options for the block cache are handled
+   * appropriately.
+   */
+  @Test
+  public void ensureCacheConfigurable() throws Exception {
+    IOContext mergeContext = new IOContext(new MergeInfo(1,1,false,1));
+
+    BlockDirectory d = directory;
+    Assert.assertTrue(d.useReadCache("", IOContext.DEFAULT));
+    Assert.assertTrue(d.useWriteCache("", IOContext.DEFAULT));
+    Assert.assertFalse(d.useWriteCache("", mergeContext));
+
+    d = new BlockDirectory("test", directory, mapperCache, null, true, false);
+    Assert.assertTrue(d.useReadCache("", IOContext.DEFAULT));
+    Assert.assertFalse(d.useWriteCache("", IOContext.DEFAULT));
+    Assert.assertFalse(d.useWriteCache("", mergeContext));
+
+    d = new BlockDirectory("test", directory, mapperCache, null, false, true);
+    Assert.assertFalse(d.useReadCache("", IOContext.DEFAULT));
+    Assert.assertTrue(d.useWriteCache("", IOContext.DEFAULT));
+    Assert.assertFalse(d.useWriteCache("", mergeContext));
+  }
+}
diff --git a/lucene/hadoop/src/test/org/apache/lucene/store/blockcache/BufferStoreTest.java b/lucene/hadoop/src/test/org/apache/lucene/store/blockcache/BufferStoreTest.java
new file mode 100644
index 0000000..46fc1d6
--- /dev/null
+++ b/lucene/hadoop/src/test/org/apache/lucene/store/blockcache/BufferStoreTest.java
@@ -0,0 +1,93 @@
+package org.apache.lucene.store.blockcache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Before;
+import org.junit.Test;
+
+public class BufferStoreTest extends LuceneTestCase {
+  private final static int blockSize = 1024;
+
+  private Metrics metrics;
+
+  private Store store;
+
+  @Before
+  public void setup() {
+    metrics = new Metrics();
+    BufferStore.initNewBuffer(blockSize, blockSize, metrics);
+    store = BufferStore.instance(blockSize);
+  }
+
+  @Test
+  public void testBufferTakePut() {
+    byte[] b1 = store.takeBuffer(blockSize);
+
+    assertGaugeMetricsChanged(false, false);
+
+    byte[] b2 = store.takeBuffer(blockSize);
+    byte[] b3 = store.takeBuffer(blockSize);
+
+    assertRawMetricCounts(2, 0);
+    assertGaugeMetricsChanged(true, false);
+
+    store.putBuffer(b1);
+
+    assertGaugeMetricsChanged(true, false);
+
+    store.putBuffer(b2);
+    store.putBuffer(b3);
+
+    assertRawMetricCounts(2, 2);
+    assertGaugeMetricsChanged(true, true);
+  }
+
+  private void assertRawMetricCounts(int allocated, int lost) {
+    assertEquals("Buffer allocation count is wrong.", allocated,
+        metrics.shardBuffercacheAllocate.get());
+    assertEquals("Lost buffer count is wrong", lost,
+        metrics.shardBuffercacheLost.get());
+  }
+
+  /**
+   * Stateful method to verify whether the amount of buffers allocated and lost
+   * since the last call has changed.
+   *
+   * @param allocated
+   *          whether buffers should have been allocated since the last call
+   * @param lost
+   *          whether buffers should have been lost since the last call
+   */
+  private void assertGaugeMetricsChanged(boolean allocated, boolean lost) {
+    assertEquals("Buffer allocation metric not updating correctly.",
+        allocated,
+        isMetricPositive(metrics.shardBuffercacheAllocate));
+
+    assertEquals("Buffer lost metric not updating correctly.",
+        lost,
+        isMetricPositive(metrics.shardBuffercacheLost));
+  }
+
+  private boolean isMetricPositive(AtomicLong metric) {
+    return metric.get() > 0;
+  }
+
+}
diff --git a/lucene/hadoop/src/test/org/apache/lucene/store/hdfs/HdfsDirectoryTest.java b/lucene/hadoop/src/test/org/apache/lucene/store/hdfs/HdfsDirectoryTest.java
new file mode 100644
index 0000000..5ac1e03
--- /dev/null
+++ b/lucene/hadoop/src/test/org/apache/lucene/store/hdfs/HdfsDirectoryTest.java
@@ -0,0 +1,241 @@
+package org.apache.lucene.store.hdfs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.Random;
+import java.util.Set;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.NoLockFactory;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.lucene.util.TestRuleLimitSysouts;
+import org.junit.After;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import com.carrotsearch.randomizedtesting.annotations.ThreadLeakFilters;
+
+@ThreadLeakFilters(defaultFilters = true, filters = {
+    BadHdfsThreadsFilter.class // hdfs currently leaks thread(s)
+})
+@TestRuleLimitSysouts.Limit(bytes = 32 * 1024)
+public class HdfsDirectoryTest extends LuceneTestCase {
+  
+  private static final int MAX_NUMBER_OF_WRITES = 10000;
+  private static final int MIN_FILE_SIZE = 100;
+  private static final int MAX_FILE_SIZE = 100000;
+  private static final int MIN_BUFFER_SIZE = 1;
+  private static final int MAX_BUFFER_SIZE = 5000;
+  private static final int MAX_NUMBER_OF_READS = 10000;
+
+  private static MiniDFSCluster dfsCluster;
+  private HdfsDirectory directory;
+  private Random random;
+
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    dfsCluster = HdfsTestUtil.setupClass(LuceneTestCase.createTempDir().toFile().getAbsolutePath());
+  }
+  
+  @AfterClass
+  public static void afterClass() throws Exception {
+    HdfsTestUtil.teardownClass(dfsCluster);
+    dfsCluster = null;
+  }
+  
+  @Before
+  public void setUp() throws Exception {
+    super.setUp();
+    
+    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);
+    conf.set("dfs.permissions.enabled", "false");
+
+    String path = String.format("%s%s/hdfs", HdfsTestUtil.getURI(dfsCluster),
+        LuceneTestCase.createTempDir().toFile().getAbsolutePath());
+    directory = new HdfsDirectory(new Path(path), NoLockFactory.INSTANCE, conf);
+    random = LuceneTestCase.random();
+  }
+  
+  @After
+  public void tearDown() throws Exception {
+    super.tearDown();
+  }
+  
+  @Test
+  public void testWritingAndReadingAFile() throws IOException {
+    String[] listAll = directory.listAll();
+    for (String file : listAll) {
+      directory.deleteFile(file);
+    }
+    
+    IndexOutput output = directory.createOutput("testing.test", new IOContext());
+    output.writeInt(12345);
+    output.close();
+
+    IndexInput input = directory.openInput("testing.test", new IOContext());
+    Assert.assertEquals(12345, input.readInt());
+    input.close();
+
+    listAll = directory.listAll();
+    Assert.assertEquals(1, listAll.length);
+    Assert.assertEquals("testing.test", listAll[0]);
+
+    assertEquals(4, directory.fileLength("testing.test"));
+
+    IndexInput input1 = directory.openInput("testing.test", new IOContext());
+
+    IndexInput input2 = input1.clone();
+    Assert.assertEquals(12345, input2.readInt());
+    input2.close();
+
+    Assert.assertEquals(12345, input1.readInt());
+    input1.close();
+
+    Assert.assertFalse(LuceneTestCase.slowFileExists(directory, "testing.test.other"));
+    Assert.assertTrue(LuceneTestCase.slowFileExists(directory, "testing.test"));
+    directory.deleteFile("testing.test");
+    Assert.assertFalse(LuceneTestCase.slowFileExists(directory, "testing.test"));
+  }
+  
+  public void testRename() throws IOException {
+    String[] listAll = directory.listAll();
+    for (String file : listAll) {
+      directory.deleteFile(file);
+    }
+    
+    IndexOutput output = directory.createOutput("testing.test", new IOContext());
+    output.writeInt(12345);
+    output.close();
+    directory.renameFile("testing.test", "testing.test.renamed");
+    Assert.assertFalse(LuceneTestCase.slowFileExists(directory, "testing.test"));
+    Assert.assertTrue(LuceneTestCase.slowFileExists(directory, "testing.test.renamed"));
+    IndexInput input = directory.openInput("testing.test.renamed", new IOContext());
+    Assert.assertEquals(12345, input.readInt());
+    Assert.assertEquals(input.getFilePointer(), input.length());
+    input.close();
+    directory.deleteFile("testing.test.renamed");
+    Assert.assertFalse(LuceneTestCase.slowFileExists(directory, "testing.test.renamed"));
+  }
+  
+  @Test
+  public void testEOF() throws IOException {
+    Directory fsDir = new RAMDirectory();
+    String name = "test.eof";
+    createFile(name, fsDir, directory);
+    long fsLength = fsDir.fileLength(name);
+    long hdfsLength = directory.fileLength(name);
+    Assert.assertEquals(fsLength, hdfsLength);
+    testEof(name,fsDir,fsLength);
+    testEof(name,directory,hdfsLength);
+  }
+
+  private void testEof(String name, Directory directory, long length) throws IOException {
+    IndexInput input = directory.openInput(name, new IOContext());
+    input.seek(length);
+    try {
+      input.readByte();
+      Assert.fail("should throw eof");
+    } catch (IOException e) {
+    }
+  }
+
+  @Test
+  public void testRandomAccessWrites() throws IOException {
+    int i = 0;
+    try {
+      Set<String> names = new HashSet<>();
+      for (; i< 10; i++) {
+        Directory fsDir = new RAMDirectory();
+        String name = getName();
+        System.out.println("Working on pass [" + i  +"] contains [" + names.contains(name) + "]");
+        names.add(name);
+        createFile(name,fsDir,directory);
+        assertInputsEquals(name,fsDir,directory);
+        fsDir.close();
+      }
+    } catch (Exception e) {
+      e.printStackTrace();
+      Assert.fail("Test failed on pass [" + i + "]");
+    }
+  }
+
+  private void assertInputsEquals(String name, Directory fsDir, HdfsDirectory hdfs) throws IOException {
+    int reads = random.nextInt(MAX_NUMBER_OF_READS);
+    IndexInput fsInput = fsDir.openInput(name,new IOContext());
+    IndexInput hdfsInput = hdfs.openInput(name,new IOContext());
+    Assert.assertEquals(fsInput.length(), hdfsInput.length());
+    int fileLength = (int) fsInput.length();
+    for (int i = 0; i < reads; i++) {
+      int nextInt = Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE,fileLength);
+      byte[] fsBuf = new byte[random.nextInt(nextInt > 0 ? nextInt : 1) + MIN_BUFFER_SIZE];
+      byte[] hdfsBuf = new byte[fsBuf.length];
+      int offset = random.nextInt(fsBuf.length);
+      
+      nextInt = fsBuf.length - offset;
+      int length = random.nextInt(nextInt > 0 ? nextInt : 1);
+      nextInt = fileLength - length;
+      int pos = random.nextInt(nextInt > 0 ? nextInt : 1);
+      fsInput.seek(pos);
+      fsInput.readBytes(fsBuf, offset, length);
+      hdfsInput.seek(pos);
+      hdfsInput.readBytes(hdfsBuf, offset, length);
+      for (int f = offset; f < length; f++) {
+        if (fsBuf[f] != hdfsBuf[f]) {
+          Assert.fail();
+        }
+      }
+    }
+    fsInput.close();
+    hdfsInput.close();
+  }
+
+  private void createFile(String name, Directory fsDir, HdfsDirectory hdfs) throws IOException {
+    int writes = random.nextInt(MAX_NUMBER_OF_WRITES);
+    int fileLength = random.nextInt(MAX_FILE_SIZE - MIN_FILE_SIZE) + MIN_FILE_SIZE;
+    IndexOutput fsOutput = fsDir.createOutput(name, new IOContext());
+    IndexOutput hdfsOutput = hdfs.createOutput(name, new IOContext());
+    for (int i = 0; i < writes; i++) {
+      byte[] buf = new byte[random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE,fileLength)) + MIN_BUFFER_SIZE];
+      random.nextBytes(buf);
+      int offset = random.nextInt(buf.length);
+      int length = random.nextInt(buf.length - offset);
+      fsOutput.writeBytes(buf, offset, length);
+      hdfsOutput.writeBytes(buf, offset, length);
+    }
+    fsOutput.close();
+    hdfsOutput.close();
+  }
+
+  private String getName() {
+    return Long.toString(Math.abs(random.nextLong()));
+  }
+
+}
diff --git a/lucene/hadoop/src/test/org/apache/lucene/store/hdfs/HdfsLockFactoryTest.java b/lucene/hadoop/src/test/org/apache/lucene/store/hdfs/HdfsLockFactoryTest.java
new file mode 100644
index 0000000..152ee9c
--- /dev/null
+++ b/lucene/hadoop/src/test/org/apache/lucene/store/hdfs/HdfsLockFactoryTest.java
@@ -0,0 +1,84 @@
+package org.apache.lucene.store.hdfs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.lucene.store.Lock;
+import org.apache.lucene.store.LockObtainFailedException;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestRuleLimitSysouts;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import com.carrotsearch.randomizedtesting.annotations.ThreadLeakFilters;
+
+@ThreadLeakFilters(defaultFilters = true, filters = {
+    BadHdfsThreadsFilter.class // hdfs currently leaks thread(s)
+})
+@TestRuleLimitSysouts.Limit(bytes = 32 * 1024)
+public class HdfsLockFactoryTest extends LuceneTestCase {
+
+  private static MiniDFSCluster dfsCluster;
+
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    dfsCluster = HdfsTestUtil.setupClass(LuceneTestCase.createTempDir().toFile().getAbsolutePath());
+  }
+
+  @AfterClass
+  public static void afterClass() throws Exception {
+    HdfsTestUtil.teardownClass(dfsCluster);
+    dfsCluster = null;
+  }
+  
+  @Test
+  public void testBasic() throws IOException {
+    String uri = HdfsTestUtil.getURI(dfsCluster);
+    Path lockPath = new Path(uri, "/basedir/lock");
+    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);
+    HdfsDirectory dir = new HdfsDirectory(lockPath, conf);
+    
+    try (Lock lock = dir.obtainLock("testlock")) {
+      assert lock != null;
+      try (Lock lock2 = dir.obtainLock("testlock")) {
+        assert lock2 != null;
+        Assert.fail("Locking should fail");
+      } catch (LockObtainFailedException lofe) {
+        // pass
+      }
+    }
+    // now repeat after close()
+    try (Lock lock = dir.obtainLock("testlock")) {
+      assert lock != null;
+      try (Lock lock2 = dir.obtainLock("testlock")) {
+        assert lock2 != null;
+        Assert.fail("Locking should fail");
+      } catch (LockObtainFailedException lofe) {
+        // pass
+      }
+    }
+    dir.close();
+  }
+}
diff --git a/lucene/hadoop/src/test/org/apache/lucene/store/hdfs/HdfsTestUtil.java b/lucene/hadoop/src/test/org/apache/lucene/store/hdfs/HdfsTestUtil.java
new file mode 100644
index 0000000..1568690
--- /dev/null
+++ b/lucene/hadoop/src/test/org/apache/lucene/store/hdfs/HdfsTestUtil.java
@@ -0,0 +1,137 @@
+package org.apache.lucene.store.hdfs;
+
+import java.io.File;
+import java.net.URI;
+import java.util.Locale;
+import java.util.Map;
+import java.util.Properties;
+import java.util.Timer;
+import java.util.TimerTask;
+import java.util.concurrent.ConcurrentHashMap;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.hadoop.hdfs.server.namenode.NameNodeAdapter;
+import org.apache.hadoop.hdfs.server.namenode.ha.HATestUtil;
+import org.apache.lucene.util.LuceneTestCase;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class HdfsTestUtil {
+  private static Logger log = LoggerFactory.getLogger(HdfsTestUtil.class);
+  
+  private static final String LOGICAL_HOSTNAME = "ha-nn-uri-%d";
+
+  private static Locale savedLocale;
+  private static Map<MiniDFSCluster,Timer> timers = new ConcurrentHashMap<>();
+
+  public static MiniDFSCluster setupClass(String dir) throws Exception {
+    LuceneTestCase.assumeFalse("HDFS tests were disabled by -Dtests.disableHdfs",
+      Boolean.parseBoolean(System.getProperty("tests.disableHdfs", "false")));
+
+    savedLocale = Locale.getDefault();
+    // TODO: we HACK around HADOOP-9643
+    Locale.setDefault(Locale.ENGLISH);
+
+    Configuration conf = new Configuration();
+    conf.set("dfs.block.access.token.enable", "false");
+    conf.set("dfs.permissions.enabled", "false");
+    conf.set("hadoop.security.authorization", "false");
+    conf.set("hadoop.security.authentication", "simple");
+    conf.set("hdfs.minidfs.basedir", dir + File.separator + "hdfsBaseDir");
+    conf.set("dfs.namenode.name.dir", dir + File.separator + "nameNodeNameDir");
+    conf.setBoolean("fs.hdfs.impl.disable.cache", true);
+
+    System.setProperty("test.build.data", dir + File.separator + "hdfs" + File.separator + "build");
+    System.setProperty("test.cache.data", dir + File.separator + "hdfs" + File.separator + "cache");
+    System.setProperty("java.security.debug", "all");
+    System.setProperty("javax.security.debug", "all");
+
+    final MiniDFSCluster dfsCluster = new MiniDFSCluster.Builder(conf)
+        .format(true)
+        .numDataNodes(2)
+        .build();
+    dfsCluster.waitActive();
+
+    int rndMode = LuceneTestCase.random().nextInt(3);
+    if (true && rndMode == 1) {
+      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);
+
+      int rnd = LuceneTestCase.random().nextInt(10000);
+      Timer timer = new Timer();
+      timers.put(dfsCluster, timer);
+      timer.schedule(new TimerTask() {
+
+        @Override
+        public void run() {
+          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());
+        }
+      }, rnd);
+    }
+
+    return dfsCluster;
+  }
+
+  public static Configuration getClientConfiguration(MiniDFSCluster dfsCluster) {
+    if (dfsCluster.getNameNodeInfos().length > 1) {
+      Configuration conf = new Configuration();
+      HATestUtil.setFailoverConfigurations(dfsCluster, conf);
+      return conf;
+    } else {
+      return new Configuration();
+    }
+  }
+  
+  public static void teardownClass(MiniDFSCluster dfsCluster) throws Exception {
+    System.clearProperty("test.build.data");
+    System.clearProperty("test.cache.data");
+    if (dfsCluster != null) {
+      Timer timer = timers.remove(dfsCluster);
+      if (timer != null) {
+        timer.cancel();
+      }
+      try {
+        dfsCluster.shutdown();
+      } catch (Error e) {
+        // Added in SOLR-7134
+        // Rarely, this can fail to either a NullPointerException
+        // or a class not found exception. The later may fixable
+        // by adding test dependencies.
+        log.warn("Exception shutting down dfsCluster", e);
+      }
+    }
+    
+    // TODO: we HACK around HADOOP-9643
+    if (savedLocale != null) {
+      Locale.setDefault(savedLocale);
+    }
+  }
+  
+  public static String getURI(MiniDFSCluster dfsCluster) {
+    if (dfsCluster.getNameNodeInfos().length > 1) {
+      String logicalName = String.format(Locale.ENGLISH, LOGICAL_HOSTNAME, dfsCluster.getInstanceId()); // NOTE: hdfs uses default locale
+      return "hdfs://" + logicalName;
+    } else {
+      URI uri = dfsCluster.getURI(0);
+      return uri.toString() ;
+    }
+  }
+
+}
diff --git a/lucene/module-build.xml b/lucene/module-build.xml
index 5a8fe9b..e4bf266 100644
--- a/lucene/module-build.xml
+++ b/lucene/module-build.xml
@@ -497,6 +497,17 @@
     <property name="grouping-javadocs.uptodate" value="true"/>
   </target>
 
+  <property name="hadoop.jar" value="${common.dir}/build/hadoop/lucene-hadoop-${version}.jar"/>
+  <target name="check-hadoop-uptodate" unless="hadoop.uptodate">
+    <module-uptodate name="hadoop" jarfile="${hadoop.jar}" property="hadoop.uptodate"/>
+  </target>
+  <target name="jar-hadoop" unless="hadoop.uptodate" depends="check-hadoop-uptodate">
+    <ant dir="${common.dir}/hadoop" target="jar-core" inheritall="false">
+      <propertyset refid="uptodate.and.compiled.properties"/>
+    </ant>
+    <property name="hadoop.uptodate" value="true"/>
+  </target>
+
   <property name="highlighter.jar" value="${common.dir}/build/highlighter/lucene-highlighter-${version}.jar"/>
   <target name="check-highlighter-uptodate" unless="highlighter.uptodate">
     <module-uptodate name="highlighter" jarfile="${highlighter.jar}" property="highlighter.uptodate"/>
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/BadHdfsThreadsFilter.java b/lucene/test-framework/src/java/org/apache/lucene/util/BadHdfsThreadsFilter.java
new file mode 100644
index 0000000..8ff13e3
--- /dev/null
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/BadHdfsThreadsFilter.java
@@ -0,0 +1,48 @@
+package org.apache.lucene.util;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.HashSet;
+import java.util.Set;
+
+import com.carrotsearch.randomizedtesting.ThreadFilter;
+
+public class BadHdfsThreadsFilter implements ThreadFilter {
+
+  private static final Set<String> badThreads = new HashSet<>();
+  static {
+    badThreads.add("IPC Parameter Sending Thread "); // SOLR-5007
+    badThreads.add("org.apache.hadoop.hdfs.PeerCache"); // SOLR-7288)
+    badThreads.add("LeaseRenewer"); // SOLR-7287
+    badThreads.add("Acceptor");
+    badThreads.add("Timer");
+    badThreads.add("pool");
+    badThreads.add("qtp");
+  }
+
+  @Override
+  public boolean reject(Thread t) {
+    String name = t.getName();
+    for (String badThread : badThreads) {
+      if (name.contains(badThread)) {
+        return true;
+      }
+    }
+    return false;
+  }
+}
diff --git a/lucene/tools/junit4/tests.policy b/lucene/tools/junit4/tests.policy
index 3bd0845..08fe35b 100644
--- a/lucene/tools/junit4/tests.policy
+++ b/lucene/tools/junit4/tests.policy
@@ -78,25 +78,11 @@ grant {
   // needed by jacoco to dump coverage
   permission java.lang.RuntimePermission "shutdownHooks";
   
-  // read access to all system properties:
-  permission java.util.PropertyPermission "*", "read";
-  // write access to only these:
-  // locale randomization
-  permission java.util.PropertyPermission "user.language", "write";
-  // timezone randomization
-  permission java.util.PropertyPermission "user.timezone", "write";
+  // read, writer access to all system properties:
+  permission java.util.PropertyPermission "*", "read,write";
 
-  // CMS randomization
-  permission java.util.PropertyPermission "lucene.cms.override_core_count", "write";
-  permission java.util.PropertyPermission "lucene.cms.override_spins", "write";
-
-  // used by nested tests? (e.g. TestLeaveFilesIfTestFails). TODO: look into this
-  permission java.util.PropertyPermission "tests.runnested", "write";
-
-  // solr properties. TODO: move these out to SolrTestCase
-  permission java.util.PropertyPermission "solr.data.dir", "write";
-  permission java.util.PropertyPermission "solr.solr.home", "write";
-  permission java.util.PropertyPermission "solr.directoryFactory", "write";
+  // access to hadoop env variables for hadoop test
+  permission java.lang.RuntimePermission "getenv.HADOOP_HOME";
 
   // replicator: jetty tests require some network permissions:
   // all possibilities of accepting/binding/connecting on localhost with ports >= 1024:
diff --git a/solr/common-build.xml b/solr/common-build.xml
index df0b702..2cbca1a 100644
--- a/solr/common-build.xml
+++ b/solr/common-build.xml
@@ -93,6 +93,7 @@
     <pathelement location="${analyzers-phonetic.jar}"/>
     <pathelement location="${codecs.jar}"/>
     <pathelement location="${backward-codecs.jar}"/>
+    <pathelement location="${hadoop.jar}"/>
     <pathelement location="${highlighter.jar}"/>
     <pathelement location="${memory.jar}"/>
     <pathelement location="${misc.jar}"/>
@@ -144,18 +145,6 @@
     </sequential>
   </macrodef>
 
-  <!-- 
-    - We don't test HDFS on Java 7 because it causes permgen errors. Java 8 no longer has permgen.
-    - We don't want to run HDFS tests on Windows, because they require Cygwin.
-    If you have Cygwin or manually raised permgen, you can override this property on command line:
-  -->
-  <condition property="tests.disableHdfs" value="true">
-    <or>
-      <equals arg1="${build.java.runtime}" arg2="1.7"/>
-      <os family="windows"/>
-    </or>
-  </condition>
-
   <target name="validate" depends="compile-tools">
   </target>
 
@@ -317,6 +306,7 @@
           <link offline="true" href="${lucene.javadoc.url}grouping" packagelistloc="${lucenedocs}/grouping"/>
           <link offline="true" href="${lucene.javadoc.url}queries" packagelistloc="${lucenedocs}/queries"/>
           <link offline="true" href="${lucene.javadoc.url}queryparser" packagelistloc="${lucenedocs}/queryparser"/>
+          <link offline="true" href="${lucene.javadoc.url}hadoop" packagelistloc="${lucenedocs}/hadoop"/>
           <link offline="true" href="${lucene.javadoc.url}highlighter" packagelistloc="${lucenedocs}/highlighter"/>
           <link offline="true" href="${lucene.javadoc.url}memory" packagelistloc="${lucenedocs}/memory"/>
           <link offline="true" href="${lucene.javadoc.url}misc" packagelistloc="${lucenedocs}/misc"/>
diff --git a/solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest.java b/solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest.java
index 45be1ec..c731ec5 100644
--- a/solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest.java
+++ b/solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest.java
@@ -44,7 +44,7 @@ import org.apache.solr.SolrTestCaseJ4;
 import org.apache.solr.cloud.AbstractZkTestCase;
 import org.apache.solr.hadoop.hack.MiniMRCluster;
 import org.apache.solr.morphlines.solr.AbstractSolrMorphlineTestBase;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.junit.After;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
diff --git a/solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest.java b/solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest.java
index bf0bad3..bc703bd 100644
--- a/solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest.java
+++ b/solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest.java
@@ -72,7 +72,7 @@ import org.apache.solr.common.util.NamedList;
 import org.apache.solr.hadoop.hack.MiniMRClientCluster;
 import org.apache.solr.hadoop.hack.MiniMRClientClusterFactory;
 import org.apache.solr.morphlines.solr.AbstractSolrMorphlineTestBase;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
diff --git a/solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAliasTest.java b/solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAliasTest.java
index a654e94..4630c13 100644
--- a/solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAliasTest.java
+++ b/solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAliasTest.java
@@ -29,7 +29,7 @@ import org.apache.solr.common.SolrDocument;
 import org.apache.solr.common.params.CollectionParams.CollectionAction;
 import org.apache.solr.common.params.ModifiableSolrParams;
 import org.apache.solr.common.util.NamedList;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.junit.Test;
 import org.kitesdk.morphline.api.Record;
 import org.kitesdk.morphline.base.Fields;
diff --git a/solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest.java b/solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest.java
index 0eb9e40..131a2f2 100644
--- a/solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest.java
+++ b/solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkAvroTest.java
@@ -32,18 +32,13 @@ import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.apache.solr.client.solrj.SolrQuery;
 import org.apache.solr.client.solrj.response.QueryResponse;
 import org.apache.solr.common.SolrDocument;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.junit.Test;
 import org.kitesdk.morphline.api.Record;
 import org.kitesdk.morphline.base.Fields;
 import org.kitesdk.morphline.base.Notifications;
 
-import com.carrotsearch.randomizedtesting.annotations.ThreadLeakAction;
-import com.carrotsearch.randomizedtesting.annotations.ThreadLeakAction.Action;
 import com.carrotsearch.randomizedtesting.annotations.ThreadLeakFilters;
-import com.carrotsearch.randomizedtesting.annotations.ThreadLeakLingering;
-import com.carrotsearch.randomizedtesting.annotations.ThreadLeakZombies;
-import com.carrotsearch.randomizedtesting.annotations.ThreadLeakZombies.Consequence;
 import com.google.common.base.Joiner;
 import com.google.common.base.Preconditions;
 import com.google.common.io.Files;
diff --git a/solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkTest.java b/solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkTest.java
index 2687d50..18a22b1 100644
--- a/solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkTest.java
+++ b/solr/contrib/morphlines-core/src/test/org/apache/solr/morphlines/solr/SolrMorphlineZkTest.java
@@ -23,18 +23,13 @@ import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.apache.solr.client.solrj.SolrQuery;
 import org.apache.solr.client.solrj.response.QueryResponse;
 import org.apache.solr.common.SolrDocument;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.junit.Test;
 import org.kitesdk.morphline.api.Record;
 import org.kitesdk.morphline.base.Fields;
 import org.kitesdk.morphline.base.Notifications;
 
-import com.carrotsearch.randomizedtesting.annotations.ThreadLeakAction;
-import com.carrotsearch.randomizedtesting.annotations.ThreadLeakAction.Action;
 import com.carrotsearch.randomizedtesting.annotations.ThreadLeakFilters;
-import com.carrotsearch.randomizedtesting.annotations.ThreadLeakLingering;
-import com.carrotsearch.randomizedtesting.annotations.ThreadLeakZombies;
-import com.carrotsearch.randomizedtesting.annotations.ThreadLeakZombies.Consequence;
 
 @ThreadLeakFilters(defaultFilters = true, filters = {
     BadHdfsThreadsFilter.class // hdfs currently leaks thread(s)
diff --git a/solr/core/ivy.xml b/solr/core/ivy.xml
index e56d313..2fa5bf8 100644
--- a/solr/core/ivy.xml
+++ b/solr/core/ivy.xml
@@ -66,7 +66,6 @@
     <dependency org="commons-collections" name="commons-collections" rev="${/commons-collections/commons-collections}" conf="compile.hadoop"/>
     
     <dependency org="com.google.protobuf" name="protobuf-java" rev="${/com.google.protobuf/protobuf-java}" conf="compile.hadoop"/>
-    <dependency org="com.github.ben-manes.caffeine" name="caffeine" rev="${/com.github.ben-manes.caffeine/caffeine}" conf="compile.hadoop"/>
     <dependency org="org.htrace" name="htrace-core" rev="${/org.htrace/htrace-core}" conf="compile.hadoop"/>
     
     <!-- Hadoop DfsMiniCluster Dependencies-->
diff --git a/solr/core/src/java/org/apache/solr/core/CachingDirectoryFactory.java b/solr/core/src/java/org/apache/solr/core/CachingDirectoryFactory.java
index 6bf5c66..0da5c5f 100644
--- a/solr/core/src/java/org/apache/solr/core/CachingDirectoryFactory.java
+++ b/solr/core/src/java/org/apache/solr/core/CachingDirectoryFactory.java
@@ -25,24 +25,16 @@ import java.util.HashMap;
 import java.util.HashSet;
 import java.util.IdentityHashMap;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.Set;
 
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext.Context;
 import org.apache.lucene.store.LockFactory;
-import org.apache.lucene.store.NRTCachingDirectory;
-import org.apache.lucene.store.NativeFSLockFactory;
-import org.apache.lucene.store.NoLockFactory;
-import org.apache.lucene.store.SimpleFSLockFactory;
-import org.apache.lucene.store.SingleInstanceLockFactory;
 import org.apache.lucene.util.IOUtils;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrException.ErrorCode;
 import org.apache.solr.common.util.NamedList;
-import org.apache.solr.store.blockcache.BlockDirectory;
 import org.apache.solr.store.hdfs.HdfsDirectory;
 import org.apache.solr.store.hdfs.HdfsLockFactory;
 import org.slf4j.Logger;
diff --git a/solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory.java b/solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory.java
index 08577de..79a0efa 100644
--- a/solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory.java
+++ b/solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory.java
@@ -40,20 +40,21 @@ import org.apache.lucene.store.LockFactory;
 import org.apache.lucene.store.NRTCachingDirectory;
 import org.apache.lucene.store.NoLockFactory;
 import org.apache.lucene.store.SingleInstanceLockFactory;
+import org.apache.lucene.store.blockcache.BlockCache;
+import org.apache.lucene.store.blockcache.BlockDirectory;
+import org.apache.lucene.store.blockcache.BlockDirectoryCache;
+import org.apache.lucene.store.blockcache.BufferStore;
+import org.apache.lucene.store.blockcache.Cache;
+import org.apache.lucene.store.blockcache.Metrics;
+import org.apache.lucene.store.hdfs.HdfsDirectory;
+import org.apache.lucene.store.hdfs.HdfsLockFactory;
 import org.apache.solr.cloud.ZkController;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrException.ErrorCode;
 import org.apache.solr.common.params.SolrParams;
 import org.apache.solr.common.util.IOUtils;
 import org.apache.solr.common.util.NamedList;
-import org.apache.solr.store.blockcache.BlockCache;
-import org.apache.solr.store.blockcache.BlockDirectory;
-import org.apache.solr.store.blockcache.BlockDirectoryCache;
-import org.apache.solr.store.blockcache.BufferStore;
-import org.apache.solr.store.blockcache.Cache;
-import org.apache.solr.store.blockcache.Metrics;
-import org.apache.solr.store.hdfs.HdfsDirectory;
-import org.apache.solr.store.hdfs.HdfsLockFactory;
+import org.apache.solr.store.blockcache.SolrMetrics;
 import org.apache.solr.util.HdfsUtil;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -112,7 +113,7 @@ public class HdfsDirectoryFactory extends CachingDirectoryFactory {
   private final static class MetricsHolder {
     // [JCIP SE, Goetz, 16.6] Lazy initialization
     // Won't load until MetricsHolder is referenced
-    public static final Metrics metrics = new Metrics();
+    public static final Metrics metrics = new SolrMetrics();
   }
   
   @Override
@@ -443,7 +444,7 @@ public class HdfsDirectoryFactory extends CachingDirectoryFactory {
 
   @Override
   public Collection<SolrInfoMBean> offerMBeans() {
-    return Arrays.<SolrInfoMBean>asList(MetricsHolder.metrics);
+    return Arrays.<SolrInfoMBean>asList((SolrInfoMBean) MetricsHolder.metrics);
   }
 
   @Override
diff --git a/solr/core/src/java/org/apache/solr/search/SolrCacheBase.java b/solr/core/src/java/org/apache/solr/search/SolrCacheBase.java
index 3dbff0c..37f9aa8 100644
--- a/solr/core/src/java/org/apache/solr/search/SolrCacheBase.java
+++ b/solr/core/src/java/org/apache/solr/search/SolrCacheBase.java
@@ -92,7 +92,7 @@ public abstract class SolrCacheBase {
    * Returns a "Hit Ratio" (ie: max of 1.00, not a percentage) suitable for 
    * display purposes.
    */
-  protected static float calcHitRatio(long lookups, long hits) {
+  public static float calcHitRatio(long lookups, long hits) {
     return (lookups == 0) ? 0.0f :
         BigDecimal.valueOf((double) hits / (double) lookups)
             .setScale(2, RoundingMode.HALF_EVEN)
diff --git a/solr/core/src/java/org/apache/solr/store/blockcache/BlockCache.java b/solr/core/src/java/org/apache/solr/store/blockcache/BlockCache.java
deleted file mode 100644
index 1eeabe2..0000000
--- a/solr/core/src/java/org/apache/solr/store/blockcache/BlockCache.java
+++ /dev/null
@@ -1,205 +0,0 @@
-package org.apache.solr.store.blockcache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.nio.ByteBuffer;
-import java.util.concurrent.ConcurrentMap;
-import java.util.concurrent.atomic.AtomicInteger;
-
-import com.github.benmanes.caffeine.cache.Cache;
-import com.github.benmanes.caffeine.cache.Caffeine;
-import com.github.benmanes.caffeine.cache.RemovalListener;
-
-/**
- * @lucene.experimental
- */
-public class BlockCache {
-  
-  public static final int _128M = 134217728;
-  public static final int _32K = 32768;
-  private final Cache<BlockCacheKey,BlockCacheLocation> cache;
-  private final ByteBuffer[] banks;
-  private final BlockLocks[] locks;
-  private final AtomicInteger[] lockCounters;
-  private final int blockSize;
-  private final int numberOfBlocksPerBank;
-  private final int maxEntries;
-  private final Metrics metrics;
-  
-  public BlockCache(Metrics metrics, boolean directAllocation, long totalMemory) {
-    this(metrics, directAllocation, totalMemory, _128M);
-  }
-  
-  public BlockCache(Metrics metrics, boolean directAllocation,
-      long totalMemory, int slabSize) {
-    this(metrics, directAllocation, totalMemory, slabSize, _32K);
-  }
-  
-  public BlockCache(Metrics metrics, boolean directAllocation,
-      long totalMemory, int slabSize, int blockSize) {
-    this.metrics = metrics;
-    numberOfBlocksPerBank = slabSize / blockSize;
-    int numberOfBanks = (int) (totalMemory / slabSize);
-    
-    banks = new ByteBuffer[numberOfBanks];
-    locks = new BlockLocks[numberOfBanks];
-    lockCounters = new AtomicInteger[numberOfBanks];
-    maxEntries = (numberOfBlocksPerBank * numberOfBanks) - 1;
-    for (int i = 0; i < numberOfBanks; i++) {
-      if (directAllocation) {
-        banks[i] = ByteBuffer.allocateDirect(numberOfBlocksPerBank * blockSize);
-      } else {
-        banks[i] = ByteBuffer.allocate(numberOfBlocksPerBank * blockSize);
-      }
-      locks[i] = new BlockLocks(numberOfBlocksPerBank);
-      lockCounters[i] = new AtomicInteger();
-    }
-
-    RemovalListener<BlockCacheKey,BlockCacheLocation> listener = 
-        notification -> releaseLocation(notification.getValue());
-    cache = Caffeine.newBuilder()
-        .removalListener(listener)
-        .maximumSize(maxEntries)
-        .build();
-    this.blockSize = blockSize;
-  }
-  
-  public void release(BlockCacheKey key) {
-    cache.invalidate(key);
-  }
-  
-  private void releaseLocation(BlockCacheLocation location) {
-    if (location == null) {
-      return;
-    }
-    int bankId = location.getBankId();
-    int block = location.getBlock();
-    location.setRemoved(true);
-    locks[bankId].clear(block);
-    lockCounters[bankId].decrementAndGet();
-    metrics.blockCacheEviction.incrementAndGet();
-    metrics.blockCacheSize.decrementAndGet();
-  }
-  
-  public boolean store(BlockCacheKey blockCacheKey, int blockOffset,
-      byte[] data, int offset, int length) {
-    if (length + blockOffset > blockSize) {
-      throw new RuntimeException("Buffer size exceeded, expecting max ["
-          + blockSize + "] got length [" + length + "] with blockOffset ["
-          + blockOffset + "]");
-    }
-    BlockCacheLocation location = cache.getIfPresent(blockCacheKey);
-    boolean newLocation = false;
-    if (location == null) {
-      newLocation = true;
-      location = new BlockCacheLocation();
-      if (!findEmptyLocation(location)) {
-        return false;
-      }
-    }
-    if (location.isRemoved()) {
-      return false;
-    }
-    int bankId = location.getBankId();
-    int bankOffset = location.getBlock() * blockSize;
-    ByteBuffer bank = getBank(bankId);
-    bank.position(bankOffset + blockOffset);
-    bank.put(data, offset, length);
-    if (newLocation) {
-      cache.put(blockCacheKey.clone(), location);
-      metrics.blockCacheSize.incrementAndGet();
-    }
-    return true;
-  }
-  
-  public boolean fetch(BlockCacheKey blockCacheKey, byte[] buffer,
-      int blockOffset, int off, int length) {
-    BlockCacheLocation location = cache.getIfPresent(blockCacheKey);
-    if (location == null) {
-      return false;
-    }
-    if (location.isRemoved()) {
-      return false;
-    }
-    int bankId = location.getBankId();
-    int offset = location.getBlock() * blockSize;
-    location.touch();
-    ByteBuffer bank = getBank(bankId);
-    bank.position(offset + blockOffset);
-    bank.get(buffer, off, length);
-    return true;
-  }
-  
-  public boolean fetch(BlockCacheKey blockCacheKey, byte[] buffer) {
-    checkLength(buffer);
-    return fetch(blockCacheKey, buffer, 0, 0, blockSize);
-  }
-  
-  private boolean findEmptyLocation(BlockCacheLocation location) {
-    // This is a tight loop that will try and find a location to
-    // place the block before giving up
-    for (int j = 0; j < 10; j++) {
-      OUTER: for (int bankId = 0; bankId < banks.length; bankId++) {
-        AtomicInteger bitSetCounter = lockCounters[bankId];
-        BlockLocks bitSet = locks[bankId];
-        if (bitSetCounter.get() == numberOfBlocksPerBank) {
-          // if bitset is full
-          continue OUTER;
-        }
-        // this check needs to spin, if a lock was attempted but not obtained
-        // the rest of the bank should not be skipped
-        int bit = bitSet.nextClearBit(0);
-        INNER: while (bit != -1) {
-          if (bit >= numberOfBlocksPerBank) {
-            // bit set is full
-            continue OUTER;
-          }
-          if (!bitSet.set(bit)) {
-            // lock was not obtained
-            // this restarts at 0 because another block could have been unlocked
-            // while this was executing
-            bit = bitSet.nextClearBit(0);
-            continue INNER;
-          } else {
-            // lock obtained
-            location.setBankId(bankId);
-            location.setBlock(bit);
-            bitSetCounter.incrementAndGet();
-            return true;
-          }
-        }
-      }
-    }
-    return false;
-  }
-  
-  private void checkLength(byte[] buffer) {
-    if (buffer.length != blockSize) {
-      throw new RuntimeException("Buffer wrong size, expecting [" + blockSize
-          + "] got [" + buffer.length + "]");
-    }
-  }
-  
-  private ByteBuffer getBank(int bankId) {
-    return banks[bankId].duplicate();
-  }
-  
-  public int getSize() {
-    return cache.asMap().size();
-  }
-}
diff --git a/solr/core/src/java/org/apache/solr/store/blockcache/BlockCacheKey.java b/solr/core/src/java/org/apache/solr/store/blockcache/BlockCacheKey.java
deleted file mode 100644
index cf05c69..0000000
--- a/solr/core/src/java/org/apache/solr/store/blockcache/BlockCacheKey.java
+++ /dev/null
@@ -1,84 +0,0 @@
-package org.apache.solr.store.blockcache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-/**
- * @lucene.experimental
- */
-public class BlockCacheKey implements Cloneable {
-  
-  private long block;
-  private int file;
-  private String path;
-  
-  public String getPath() {
-    return path;
-  }
-
-  public void setPath(String path) {
-    this.path = path;
-  }
-
-  public long getBlock() {
-    return block;
-  }
-  
-  public int getFile() {
-    return file;
-  }
-  
-  public void setBlock(long block) {
-    this.block = block;
-  }
-  
-  public void setFile(int file) {
-    this.file = file;
-  }
-  
-  @Override
-  public int hashCode() {
-    final int prime = 31;
-    int result = 1;
-    result = prime * result + (int) (block ^ (block >>> 32));
-    result = prime * result + file;
-    result = prime * result + ((path == null) ? 0 : path.hashCode());
-    return result;
-  }
-
-  @Override
-  public boolean equals(Object obj) {
-    if (this == obj) return true;
-    if (obj == null) return false;
-    if (getClass() != obj.getClass()) return false;
-    BlockCacheKey other = (BlockCacheKey) obj;
-    if (block != other.block) return false;
-    if (file != other.file) return false;
-    if (path == null) {
-      if (other.path != null) return false;
-    } else if (!path.equals(other.path)) return false;
-    return true;
-  }
-
-  @Override
-  public BlockCacheKey clone() {
-    try {
-      return (BlockCacheKey) super.clone();
-    } catch (CloneNotSupportedException e) {
-      throw new RuntimeException(e);
-    }
-  }
-}
diff --git a/solr/core/src/java/org/apache/solr/store/blockcache/BlockCacheLocation.java b/solr/core/src/java/org/apache/solr/store/blockcache/BlockCacheLocation.java
deleted file mode 100644
index d2a124d..0000000
--- a/solr/core/src/java/org/apache/solr/store/blockcache/BlockCacheLocation.java
+++ /dev/null
@@ -1,70 +0,0 @@
-package org.apache.solr.store.blockcache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.concurrent.atomic.AtomicBoolean;
-
-/**
- * @lucene.experimental
- */
-public class BlockCacheLocation {
-  
-  private int block;
-  private int bankId;
-  private long lastAccess = System.currentTimeMillis();
-  private long accesses;
-  private AtomicBoolean removed = new AtomicBoolean(false);
-  
-  public void setBlock(int block) {
-    this.block = block;
-  }
-  
-  public void setBankId(int bankId) {
-    this.bankId = bankId;
-  }
-  
-  public int getBlock() {
-    return block;
-  }
-  
-  public int getBankId() {
-    return bankId;
-  }
-  
-  public void touch() {
-    lastAccess = System.currentTimeMillis();
-    accesses++;
-  }
-  
-  public long getLastAccess() {
-    return lastAccess;
-  }
-  
-  public long getNumberOfAccesses() {
-    return accesses;
-  }
-  
-  public boolean isRemoved() {
-    return removed.get();
-  }
-  
-  public void setRemoved(boolean removed) {
-    this.removed.set(removed);
-  }
-  
-}
\ No newline at end of file
diff --git a/solr/core/src/java/org/apache/solr/store/blockcache/BlockDirectory.java b/solr/core/src/java/org/apache/solr/store/blockcache/BlockDirectory.java
deleted file mode 100644
index 5389ede..0000000
--- a/solr/core/src/java/org/apache/solr/store/blockcache/BlockDirectory.java
+++ /dev/null
@@ -1,340 +0,0 @@
-package org.apache.solr.store.blockcache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.store.FilterDirectory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.solr.store.hdfs.HdfsDirectory;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * @lucene.experimental
- */
-public class BlockDirectory extends FilterDirectory {
-  public static Logger LOG = LoggerFactory.getLogger(BlockDirectory.class);
-  
-  public static final long BLOCK_SHIFT = 13; // 2^13 = 8,192 bytes per block
-  public static final long BLOCK_MOD = 0x1FFF;
-  public static final int BLOCK_SIZE = 1 << BLOCK_SHIFT;
-  
-  public static long getBlock(long pos) {
-    return pos >>> BLOCK_SHIFT;
-  }
-  
-  public static long getPosition(long pos) {
-    return pos & BLOCK_MOD;
-  }
-  
-  public static long getRealPosition(long block, long positionInBlock) {
-    return (block << BLOCK_SHIFT) + positionInBlock;
-  }
-  
-  public static Cache NO_CACHE = new Cache() {
-    
-    @Override
-    public void update(String name, long blockId, int blockOffset,
-        byte[] buffer, int offset, int length) {}
-    
-    @Override
-    public boolean fetch(String name, long blockId, int blockOffset, byte[] b,
-        int off, int lengthToReadInBlock) {
-      return false;
-    }
-    
-    @Override
-    public void delete(String name) {
-      
-    }
-    
-    @Override
-    public long size() {
-      return 0;
-    }
-    
-    @Override
-    public void renameCacheFile(String source, String dest) {}
-
-    @Override
-    public void releaseResources() {}
-  };
-  
-  private final int blockSize;
-  private final String dirName;
-  private final Cache cache;
-  private final Set<String> blockCacheFileTypes;
-  private final boolean blockCacheReadEnabled;
-  private final boolean blockCacheWriteEnabled;
-
-  public BlockDirectory(String dirName, Directory directory, Cache cache,
-      Set<String> blockCacheFileTypes, boolean blockCacheReadEnabled,
-      boolean blockCacheWriteEnabled) throws IOException {
-    super(directory);
-    this.dirName = dirName;
-    blockSize = BLOCK_SIZE;
-    this.cache = cache;
-    if (blockCacheFileTypes == null || blockCacheFileTypes.isEmpty()) {
-      this.blockCacheFileTypes = null;
-    } else {
-      this.blockCacheFileTypes = blockCacheFileTypes;
-    }
-    this.blockCacheReadEnabled = blockCacheReadEnabled;
-    if (!blockCacheReadEnabled) {
-      LOG.info("Block cache on read is disabled");
-    }
-    this.blockCacheWriteEnabled = blockCacheWriteEnabled;
-    if (!blockCacheWriteEnabled) {
-      LOG.info("Block cache on write is disabled");
-    }
-  }
-  
-  private IndexInput openInput(String name, int bufferSize, IOContext context)
-      throws IOException {
-    final IndexInput source = super.openInput(name, context);
-    if (useReadCache(name, context)) {
-      return new CachedIndexInput(source, blockSize, name,
-          getFileCacheName(name), cache, bufferSize);
-    }
-    return source;
-  }
-  
-  private boolean isCachableFile(String name) {
-    for (String ext : blockCacheFileTypes) {
-      if (name.endsWith(ext)) {
-        return true;
-      }
-    }
-    return false;
-  }
-  
-  @Override
-  public IndexInput openInput(final String name, IOContext context)
-      throws IOException {
-    return openInput(name, blockSize, context);
-  }
-  
-  static class CachedIndexInput extends CustomBufferedIndexInput {
-    private final Store store;
-    private IndexInput source;
-    private final int blockSize;
-    private final long fileLength;
-    private final String cacheName;
-    private final Cache cache;
-    
-    public CachedIndexInput(IndexInput source, int blockSize, String name,
-        String cacheName, Cache cache, int bufferSize) {
-      super(name, bufferSize);
-      this.source = source;
-      this.blockSize = blockSize;
-      fileLength = source.length();
-      this.cacheName = cacheName;
-      this.cache = cache;
-      store = BufferStore.instance(blockSize);
-    }
-    
-    @Override
-    public IndexInput clone() {
-      CachedIndexInput clone = (CachedIndexInput) super.clone();
-      clone.source = (IndexInput) source.clone();
-      return clone;
-    }
-    
-    @Override
-    public long length() {
-      return source.length();
-    }
-    
-    @Override
-    protected void seekInternal(long pos) throws IOException {}
-    
-    @Override
-    protected void readInternal(byte[] b, int off, int len) throws IOException {
-      long position = getFilePointer();
-      while (len > 0) {
-        int length = fetchBlock(position, b, off, len);
-        position += length;
-        len -= length;
-        off += length;
-      }
-    }
-    
-    private int fetchBlock(long position, byte[] b, int off, int len)
-        throws IOException {
-      // read whole block into cache and then provide needed data
-      long blockId = getBlock(position);
-      int blockOffset = (int) getPosition(position);
-      int lengthToReadInBlock = Math.min(len, blockSize - blockOffset);
-      if (checkCache(blockId, blockOffset, b, off, lengthToReadInBlock)) {
-        return lengthToReadInBlock;
-      } else {
-        readIntoCacheAndResult(blockId, blockOffset, b, off,
-            lengthToReadInBlock);
-      }
-      return lengthToReadInBlock;
-    }
-    
-    private void readIntoCacheAndResult(long blockId, int blockOffset,
-        byte[] b, int off, int lengthToReadInBlock) throws IOException {
-      long position = getRealPosition(blockId, 0);
-      int length = (int) Math.min(blockSize, fileLength - position);
-      source.seek(position);
-      
-      byte[] buf = store.takeBuffer(blockSize);
-      source.readBytes(buf, 0, length);
-      System.arraycopy(buf, blockOffset, b, off, lengthToReadInBlock);
-      cache.update(cacheName, blockId, 0, buf, 0, blockSize);
-      store.putBuffer(buf);
-    }
-    
-    private boolean checkCache(long blockId, int blockOffset, byte[] b,
-        int off, int lengthToReadInBlock) {
-      return cache.fetch(cacheName, blockId, blockOffset, b, off,
-          lengthToReadInBlock);
-    }
-    
-    @Override
-    protected void closeInternal() throws IOException {
-      source.close();
-    }
-  }
-  
-  @Override
-  public void close() throws IOException {
-    try {
-      String[] files = listAll();
-      
-      for (String file : files) {
-        cache.delete(getFileCacheName(file));
-      }
-      
-    } catch (FileNotFoundException e) {
-      // the local file system folder may be gone
-    } finally {
-      super.close();
-      cache.releaseResources();
-    }
-  }
-  
-  String getFileCacheName(String name) throws IOException {
-    return getFileCacheLocation(name) + ":" + getFileModified(name);
-  }
-  
-  private long getFileModified(String name) throws IOException {
-    if (in instanceof FSDirectory) {
-      File directory = ((FSDirectory) in).getDirectory().toFile();
-      File file = new File(directory, name);
-      if (!file.exists()) {
-        throw new FileNotFoundException("File [" + name + "] not found");
-      }
-      return file.lastModified();
-    } else if (in instanceof HdfsDirectory) {
-      return ((HdfsDirectory) in).fileModified(name);
-    } else {
-      throw new UnsupportedOperationException();
-    }
-  }
-  
-  String getFileCacheLocation(String name) {
-    return dirName + "/" + name;
-  }
-  
-  /**
-   * Expert: mostly for tests
-   * 
-   * @lucene.experimental
-   */
-  public Cache getCache() {
-    return cache;
-  }
-  
-  /**
-   * Determine whether read caching should be used for a particular
-   * file/context.
-   */
-  boolean useReadCache(String name, IOContext context) {
-    if (!blockCacheReadEnabled) {
-      return false;
-    }
-    if (blockCacheFileTypes != null && !isCachableFile(name)) {
-      return false;
-    }
-    switch (context.context) {
-      default: {
-        return true;
-      }
-    }
-  }
-  
-  /**
-   * Determine whether write caching should be used for a particular
-   * file/context.
-   */
-  boolean useWriteCache(String name, IOContext context) {
-    if (!blockCacheWriteEnabled || name.startsWith(IndexFileNames.PENDING_SEGMENTS)) {
-      // for safety, don't bother caching pending commits.
-      // the cache does support renaming (renameCacheFile), but thats a scary optimization.
-      return false;
-    }
-    if (blockCacheFileTypes != null && !isCachableFile(name)) {
-      return false;
-    }
-    switch (context.context) {
-      case MERGE: {
-        // we currently don't cache any merge context writes
-        return false;
-      }
-      default: {
-        return true;
-      }
-    }
-  }
-  
-  @Override
-  public IndexOutput createOutput(String name, IOContext context)
-      throws IOException {
-    final IndexOutput dest = super.createOutput(name, context);
-    if (useWriteCache(name, context)) {
-      return new CachedIndexOutput(this, dest, blockSize, name, cache, blockSize);
-    }
-    return dest;
-  }
-  
-  public void deleteFile(String name) throws IOException {
-    cache.delete(getFileCacheName(name));
-    super.deleteFile(name);
-  }
-    
-  public boolean isBlockCacheReadEnabled() {
-    return blockCacheReadEnabled;
-  }
-
-  public boolean isBlockCacheWriteEnabled() {
-    return blockCacheWriteEnabled;
-  }
-  
-}
diff --git a/solr/core/src/java/org/apache/solr/store/blockcache/BlockDirectoryCache.java b/solr/core/src/java/org/apache/solr/store/blockcache/BlockDirectoryCache.java
deleted file mode 100644
index 38089ee..0000000
--- a/solr/core/src/java/org/apache/solr/store/blockcache/BlockDirectoryCache.java
+++ /dev/null
@@ -1,125 +0,0 @@
-package org.apache.solr.store.blockcache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.Map;
-import java.util.Set;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.atomic.AtomicInteger;
-
-/**
- * @lucene.experimental
- */
-public class BlockDirectoryCache implements Cache {
-  private final BlockCache blockCache;
-  private final AtomicInteger counter = new AtomicInteger();
-  private final Map<String,Integer> names = new ConcurrentHashMap<>();
-  private Set<BlockCacheKey> keysToRelease;
-  private final String path;
-  private final Metrics metrics;
-  
-  public BlockDirectoryCache(BlockCache blockCache, String path, Metrics metrics) {
-    this(blockCache, path, metrics, false);
-  }
-  
-  public BlockDirectoryCache(BlockCache blockCache, String path, Metrics metrics, boolean releaseBlocks) {
-    this.blockCache = blockCache;
-    this.path = path;
-    this.metrics = metrics;
-    if (releaseBlocks) {
-      keysToRelease = Collections.synchronizedSet(new HashSet<BlockCacheKey>());
-    }
-  }
-  
-  /**
-   * Expert: mostly for tests
-   * 
-   * @lucene.experimental
-   */
-  public BlockCache getBlockCache() {
-    return blockCache;
-  }
-  
-  @Override
-  public void delete(String name) {
-    names.remove(name);
-  }
-  
-  @Override
-  public void update(String name, long blockId, int blockOffset, byte[] buffer,
-      int offset, int length) {
-    Integer file = names.get(name);
-    if (file == null) {
-      file = counter.incrementAndGet();
-      names.put(name, file);
-    }
-    BlockCacheKey blockCacheKey = new BlockCacheKey();
-    blockCacheKey.setPath(path);
-    blockCacheKey.setBlock(blockId);
-    blockCacheKey.setFile(file);
-    if (blockCache.store(blockCacheKey, blockOffset, buffer, offset, length) && keysToRelease != null) {
-      keysToRelease.add(blockCacheKey);
-    }
-  }
-  
-  @Override
-  public boolean fetch(String name, long blockId, int blockOffset, byte[] b,
-      int off, int lengthToReadInBlock) {
-    Integer file = names.get(name);
-    if (file == null) {
-      return false;
-    }
-    BlockCacheKey blockCacheKey = new BlockCacheKey();
-    blockCacheKey.setPath(path);
-    blockCacheKey.setBlock(blockId);
-    blockCacheKey.setFile(file);
-    boolean fetch = blockCache.fetch(blockCacheKey, b, blockOffset, off,
-        lengthToReadInBlock);
-    if (fetch) {
-      metrics.blockCacheHit.incrementAndGet();
-    } else {
-      metrics.blockCacheMiss.incrementAndGet();
-    }
-    return fetch;
-  }
-  
-  @Override
-  public long size() {
-    return blockCache.getSize();
-  }
-  
-  @Override
-  public void renameCacheFile(String source, String dest) {
-    Integer file = names.remove(source);
-    // possible if the file is empty
-    if (file != null) {
-      names.put(dest, file);
-    }
-  }
-
-  @Override
-  public void releaseResources() {
-    if (keysToRelease != null) {
-      for (BlockCacheKey key : keysToRelease) {
-        blockCache.release(key);
-      }
-    }
-  }
-}
diff --git a/solr/core/src/java/org/apache/solr/store/blockcache/BlockLocks.java b/solr/core/src/java/org/apache/solr/store/blockcache/BlockLocks.java
deleted file mode 100644
index ba69650..0000000
--- a/solr/core/src/java/org/apache/solr/store/blockcache/BlockLocks.java
+++ /dev/null
@@ -1,99 +0,0 @@
-package org.apache.solr.store.blockcache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.concurrent.atomic.AtomicLongArray;
-
-import org.apache.lucene.util.LongBitSet;
-
-/**
- * @lucene.experimental
- */
-public class BlockLocks {
-  
-  private AtomicLongArray bits;
-  private int wlen;
-  
-  public BlockLocks(long numBits) {
-    int length = LongBitSet.bits2words(numBits);
-    bits = new AtomicLongArray(length);
-    wlen = length;
-  }
-  
-  /**
-   * Find the next clear bit in the bit set.
-   * 
-   * @param index
-   *          index
-   * @return next next bit
-   */
-  public int nextClearBit(int index) {
-    int i = index >> 6;
-    if (i >= wlen) return -1;
-    int subIndex = index & 0x3f; // index within the word
-    long word = ~bits.get(i) >> subIndex; // skip all the bits to the right of
-                                          // index
-    if (word != 0) {
-      return (i << 6) + subIndex + Long.numberOfTrailingZeros(word);
-    }
-    while (++i < wlen) {
-      word = ~bits.get(i);
-      if (word != 0) {
-        return (i << 6) + Long.numberOfTrailingZeros(word);
-      }
-    }
-    return -1;
-  }
-  
-  /**
-   * Thread safe set operation that will set the bit if and only if the bit was
-   * not previously set.
-   * 
-   * @param index
-   *          the index position to set.
-   * @return returns true if the bit was set and false if it was already set.
-   */
-  public boolean set(int index) {
-    int wordNum = index >> 6; // div 64
-    int bit = index & 0x3f; // mod 64
-    long bitmask = 1L << bit;
-    long word, oword;
-    do {
-      word = bits.get(wordNum);
-      // if set another thread stole the lock
-      if ((word & bitmask) != 0) {
-        return false;
-      }
-      oword = word;
-      word |= bitmask;
-    } while (!bits.compareAndSet(wordNum, oword, word));
-    return true;
-  }
-  
-  public void clear(int index) {
-    int wordNum = index >> 6;
-    int bit = index & 0x03f;
-    long bitmask = 1L << bit;
-    long word, oword;
-    do {
-      word = bits.get(wordNum);
-      oword = word;
-      word &= ~bitmask;
-    } while (!bits.compareAndSet(wordNum, oword, word));
-  }
-}
diff --git a/solr/core/src/java/org/apache/solr/store/blockcache/BufferStore.java b/solr/core/src/java/org/apache/solr/store/blockcache/BufferStore.java
deleted file mode 100644
index dedbdb2..0000000
--- a/solr/core/src/java/org/apache/solr/store/blockcache/BufferStore.java
+++ /dev/null
@@ -1,134 +0,0 @@
-package org.apache.solr.store.blockcache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.concurrent.ArrayBlockingQueue;
-import java.util.concurrent.BlockingQueue;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.ConcurrentMap;
-import java.util.concurrent.atomic.AtomicLong;
-
-/**
- * @lucene.experimental
- */
-public class BufferStore implements Store {
-
-  private static final Store EMPTY = new Store() {
-
-    @Override
-    public byte[] takeBuffer(int bufferSize) {
-      return new byte[bufferSize];
-    }
-
-    @Override
-    public void putBuffer(byte[] buffer) {
-    }
-  };
-
-  private final static ConcurrentMap<Integer, BufferStore> bufferStores = new ConcurrentHashMap<>();
-
-  private final BlockingQueue<byte[]> buffers;
-
-  private final int bufferSize;
-
-  private final AtomicLong shardBuffercacheAllocate;
-  private final AtomicLong shardBuffercacheLost;
-
-  public synchronized static void initNewBuffer(int bufferSize, long totalAmount) {
-    initNewBuffer(bufferSize, totalAmount, null);
-  }
-
-  public synchronized static void initNewBuffer(int bufferSize, long totalAmount, Metrics metrics) {
-    if (totalAmount == 0) {
-      return;
-    }
-    BufferStore bufferStore = bufferStores.get(bufferSize);
-    if (bufferStore == null) {
-      long count = totalAmount / bufferSize;
-      if (count > Integer.MAX_VALUE) {
-        count = Integer.MAX_VALUE;
-      }
-      AtomicLong shardBuffercacheLost = new AtomicLong(0);
-      AtomicLong shardBuffercacheAllocate = new AtomicLong(0);
-      if (metrics != null) {
-        shardBuffercacheLost = metrics.shardBuffercacheLost;
-        shardBuffercacheAllocate = metrics.shardBuffercacheAllocate;
-      }
-      BufferStore store = new BufferStore(bufferSize, (int) count, shardBuffercacheAllocate, shardBuffercacheLost);
-      bufferStores.put(bufferSize, store);
-    }
-  }
-
-  private BufferStore(int bufferSize, int count, AtomicLong shardBuffercacheAllocate, AtomicLong shardBuffercacheLost) {
-    this.bufferSize = bufferSize;
-    this.shardBuffercacheAllocate = shardBuffercacheAllocate;
-    this.shardBuffercacheLost = shardBuffercacheLost;
-    buffers = setupBuffers(bufferSize, count);
-  }
-
-  private static BlockingQueue<byte[]> setupBuffers(int bufferSize, int count) {
-    BlockingQueue<byte[]> queue = new ArrayBlockingQueue<>(count);
-    for (int i = 0; i < count; i++) {
-      queue.add(new byte[bufferSize]);
-    }
-    return queue;
-  }
-
-  public static Store instance(int bufferSize) {
-    BufferStore bufferStore = bufferStores.get(bufferSize);
-    if (bufferStore == null) {
-      return EMPTY;
-    }
-    return bufferStore;
-  }
-
-  @Override
-  public byte[] takeBuffer(int bufferSize) {
-    if (this.bufferSize != bufferSize) {
-      throw new RuntimeException("Buffer with length [" + bufferSize + "] does not match buffer size of ["
-          + bufferSize + "]");
-    }
-    return newBuffer(buffers.poll());
-  }
-
-  @Override
-  public void putBuffer(byte[] buffer) {
-    if (buffer == null) {
-      return;
-    }
-    if (buffer.length != bufferSize) {
-      throw new RuntimeException("Buffer with length [" + buffer.length + "] does not match buffer size of ["
-          + bufferSize + "]");
-    }
-    checkReturn(buffers.offer(buffer));
-  }
-
-  private void checkReturn(boolean accepted) {
-    if (!accepted) {
-      shardBuffercacheLost.incrementAndGet();
-    }
-  }
-
-  private byte[] newBuffer(byte[] buf) {
-    if (buf != null) {
-      return buf;
-    }
-    shardBuffercacheAllocate.incrementAndGet();
-    return new byte[bufferSize];
-  }
-}
diff --git a/solr/core/src/java/org/apache/solr/store/blockcache/Cache.java b/solr/core/src/java/org/apache/solr/store/blockcache/Cache.java
deleted file mode 100644
index 1a691a4..0000000
--- a/solr/core/src/java/org/apache/solr/store/blockcache/Cache.java
+++ /dev/null
@@ -1,70 +0,0 @@
-package org.apache.solr.store.blockcache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * @lucene.experimental
- */
-public interface Cache {
-  
-  /**
-   * Remove a file from the cache.
-   * 
-   * @param name
-   *          cache file name
-   */
-  void delete(String name);
-  
-  /**
-   * Update the content of the specified cache file. Creates cache entry if
-   * necessary.
-   * 
-   */
-  void update(String name, long blockId, int blockOffset, byte[] buffer,
-      int offset, int length);
-  
-  /**
-   * Fetch the specified cache file content.
-   * 
-   * @return true if cached content found, otherwise return false
-   */
-  boolean fetch(String name, long blockId, int blockOffset, byte[] b, int off,
-      int lengthToReadInBlock);
-  
-  /**
-   * Number of entries in the cache.
-   */
-  long size();
-  
-  /**
-   * Expert: Rename the specified file in the cache. Allows a file to be moved
-   * without invalidating the cache.
-   * 
-   * @param source
-   *          original name
-   * @param dest
-   *          final name
-   */
-  void renameCacheFile(String source, String dest);
-
-  /**
-   * Release any resources associated with the cache.
-   */
-  void releaseResources();
-  
-}
diff --git a/solr/core/src/java/org/apache/solr/store/blockcache/CachedIndexOutput.java b/solr/core/src/java/org/apache/solr/store/blockcache/CachedIndexOutput.java
deleted file mode 100644
index d1e0bdc..0000000
--- a/solr/core/src/java/org/apache/solr/store/blockcache/CachedIndexOutput.java
+++ /dev/null
@@ -1,87 +0,0 @@
-package org.apache.solr.store.blockcache;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.store.IndexOutput;
-
-/**
- * Cache the blocks as they are written. The cache file name is the name of
- * the file until the file is closed, at which point the cache is updated
- * to include the last modified date (which is unknown until that point).
- * @lucene.experimental
- */
-public class CachedIndexOutput extends ReusedBufferedIndexOutput {
-  private final BlockDirectory directory;
-  private final IndexOutput dest;
-  private final int blockSize;
-  private final String name;
-  private final String location;
-  private final Cache cache;
-  
-  public CachedIndexOutput(BlockDirectory directory, IndexOutput dest,
-      int blockSize, String name, Cache cache, int bufferSize) {
-    super("dest=" + dest + " name=" + name, bufferSize);
-    this.directory = directory;
-    this.dest = dest;
-    this.blockSize = blockSize;
-    this.name = name;
-    this.location = directory.getFileCacheLocation(name);
-    this.cache = cache;
-  }
-
-  @Override
-  public void closeInternal() throws IOException {
-    dest.close();
-    cache.renameCacheFile(location, directory.getFileCacheName(name));
-  }
-  
-  private int writeBlock(long position, byte[] b, int offset, int length)
-      throws IOException {
-    // read whole block into cache and then provide needed data
-    long blockId = BlockDirectory.getBlock(position);
-    int blockOffset = (int) BlockDirectory.getPosition(position);
-    int lengthToWriteInBlock = Math.min(length, blockSize - blockOffset);
-    
-    // write the file and copy into the cache
-    dest.writeBytes(b, offset, lengthToWriteInBlock);
-    cache.update(location, blockId, blockOffset, b, offset,
-        lengthToWriteInBlock);
-    
-    return lengthToWriteInBlock;
-  }
-  
-  @Override
-  public void writeInternal(byte[] b, int offset, int length)
-      throws IOException {
-    long position = getBufferStart();
-    while (length > 0) {
-      int len = writeBlock(position, b, offset, length);
-      position += len;
-      length -= len;
-      offset += len;
-    }
-  }
-
-  @Override
-  public long getChecksum() throws IOException {
-    flushBufferToCache();
-    return dest.getChecksum();
-  }
-}
diff --git a/solr/core/src/java/org/apache/solr/store/blockcache/CustomBufferedIndexInput.java b/solr/core/src/java/org/apache/solr/store/blockcache/CustomBufferedIndexInput.java
deleted file mode 100644
index e21077a..0000000
--- a/solr/core/src/java/org/apache/solr/store/blockcache/CustomBufferedIndexInput.java
+++ /dev/null
@@ -1,284 +0,0 @@
-package org.apache.solr.store.blockcache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.EOFException;
-import java.io.IOException;
-
-import org.apache.lucene.store.BufferedIndexInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-
-/**
- * @lucene.experimental
- */
-public abstract class CustomBufferedIndexInput extends IndexInput {
-  
-  public static final int BUFFER_SIZE = 32768;
-  
-  private int bufferSize = BUFFER_SIZE;
-  
-  protected byte[] buffer;
-  
-  private long bufferStart = 0; // position in file of buffer
-  private int bufferLength = 0; // end of valid bytes
-  private int bufferPosition = 0; // next byte to read
-  
-  private Store store;
-  
-  @Override
-  public byte readByte() throws IOException {
-    if (bufferPosition >= bufferLength) refill();
-    return buffer[bufferPosition++];
-  }
-  
-  public CustomBufferedIndexInput(String resourceDesc) {
-    this(resourceDesc, BUFFER_SIZE);
-  }
-  
-  public CustomBufferedIndexInput(String resourceDesc, int bufferSize) {
-    super(resourceDesc);
-    checkBufferSize(bufferSize);
-    this.bufferSize = bufferSize;
-    this.store = BufferStore.instance(bufferSize);
-  }
-  
-  private void checkBufferSize(int bufferSize) {
-    if (bufferSize <= 0) throw new IllegalArgumentException(
-        "bufferSize must be greater than 0 (got " + bufferSize + ")");
-  }
-  
-  @Override
-  public void readBytes(byte[] b, int offset, int len) throws IOException {
-    readBytes(b, offset, len, true);
-  }
-  
-  @Override
-  public void readBytes(byte[] b, int offset, int len, boolean useBuffer)
-      throws IOException {
-    
-    if (len <= (bufferLength - bufferPosition)) {
-      // the buffer contains enough data to satisfy this request
-      if (len > 0) // to allow b to be null if len is 0...
-      System.arraycopy(buffer, bufferPosition, b, offset, len);
-      bufferPosition += len;
-    } else {
-      // the buffer does not have enough data. First serve all we've got.
-      int available = bufferLength - bufferPosition;
-      if (available > 0) {
-        System.arraycopy(buffer, bufferPosition, b, offset, available);
-        offset += available;
-        len -= available;
-        bufferPosition += available;
-      }
-      // and now, read the remaining 'len' bytes:
-      if (useBuffer && len < bufferSize) {
-        // If the amount left to read is small enough, and
-        // we are allowed to use our buffer, do it in the usual
-        // buffered way: fill the buffer and copy from it:
-        refill();
-        if (bufferLength < len) {
-          // Throw an exception when refill() could not read len bytes:
-          System.arraycopy(buffer, 0, b, offset, bufferLength);
-          throw new IOException("read past EOF");
-        } else {
-          System.arraycopy(buffer, 0, b, offset, len);
-          bufferPosition = len;
-        }
-      } else {
-        // The amount left to read is larger than the buffer
-        // or we've been asked to not use our buffer -
-        // there's no performance reason not to read it all
-        // at once. Note that unlike the previous code of
-        // this function, there is no need to do a seek
-        // here, because there's no need to reread what we
-        // had in the buffer.
-        long after = bufferStart + bufferPosition + len;
-        if (after > length()) throw new IOException("read past EOF");
-        readInternal(b, offset, len);
-        bufferStart = after;
-        bufferPosition = 0;
-        bufferLength = 0; // trigger refill() on read
-      }
-    }
-  }
-  
-  @Override
-  public int readInt() throws IOException {
-    if (4 <= (bufferLength - bufferPosition)) {
-      return ((buffer[bufferPosition++] & 0xFF) << 24)
-          | ((buffer[bufferPosition++] & 0xFF) << 16)
-          | ((buffer[bufferPosition++] & 0xFF) << 8)
-          | (buffer[bufferPosition++] & 0xFF);
-    } else {
-      return super.readInt();
-    }
-  }
-  
-  @Override
-  public long readLong() throws IOException {
-    if (8 <= (bufferLength - bufferPosition)) {
-      final int i1 = ((buffer[bufferPosition++] & 0xff) << 24)
-          | ((buffer[bufferPosition++] & 0xff) << 16)
-          | ((buffer[bufferPosition++] & 0xff) << 8)
-          | (buffer[bufferPosition++] & 0xff);
-      final int i2 = ((buffer[bufferPosition++] & 0xff) << 24)
-          | ((buffer[bufferPosition++] & 0xff) << 16)
-          | ((buffer[bufferPosition++] & 0xff) << 8)
-          | (buffer[bufferPosition++] & 0xff);
-      return (((long) i1) << 32) | (i2 & 0xFFFFFFFFL);
-    } else {
-      return super.readLong();
-    }
-  }
-  
-  @Override
-  public int readVInt() throws IOException {
-    if (5 <= (bufferLength - bufferPosition)) {
-      byte b = buffer[bufferPosition++];
-      int i = b & 0x7F;
-      for (int shift = 7; (b & 0x80) != 0; shift += 7) {
-        b = buffer[bufferPosition++];
-        i |= (b & 0x7F) << shift;
-      }
-      return i;
-    } else {
-      return super.readVInt();
-    }
-  }
-  
-  @Override
-  public long readVLong() throws IOException {
-    if (9 <= bufferLength - bufferPosition) {
-      byte b = buffer[bufferPosition++];
-      long i = b & 0x7F;
-      for (int shift = 7; (b & 0x80) != 0; shift += 7) {
-        b = buffer[bufferPosition++];
-        i |= (b & 0x7FL) << shift;
-      }
-      return i;
-    } else {
-      return super.readVLong();
-    }
-  }
-  
-  private void refill() throws IOException {
-    long start = bufferStart + bufferPosition;
-    long end = start + bufferSize;
-    if (end > length()) // don't read past EOF
-    end = length();
-    int newLength = (int) (end - start);
-    if (newLength <= 0) throw new EOFException("read past EOF");
-    
-    if (buffer == null) {
-      buffer = store.takeBuffer(bufferSize);
-      seekInternal(bufferStart);
-    }
-    readInternal(buffer, 0, newLength);
-    bufferLength = newLength;
-    bufferStart = start;
-    bufferPosition = 0;
-  }
-  
-  @Override
-  public final void close() throws IOException {
-    closeInternal();
-    store.putBuffer(buffer);
-    buffer = null;
-  }
-  
-  protected abstract void closeInternal() throws IOException;
-  
-  /**
-   * Expert: implements buffer refill. Reads bytes from the current position in
-   * the input.
-   * 
-   * @param b
-   *          the array to read bytes into
-   * @param offset
-   *          the offset in the array to start storing bytes
-   * @param length
-   *          the number of bytes to read
-   */
-  protected abstract void readInternal(byte[] b, int offset, int length)
-      throws IOException;
-  
-  @Override
-  public long getFilePointer() {
-    return bufferStart + bufferPosition;
-  }
-  
-  @Override
-  public void seek(long pos) throws IOException {
-    if (pos >= bufferStart && pos < (bufferStart + bufferLength)) bufferPosition = (int) (pos - bufferStart); // seek
-                                                                                                              // within
-                                                                                                              // buffer
-    else {
-      bufferStart = pos;
-      bufferPosition = 0;
-      bufferLength = 0; // trigger refill() on read()
-      seekInternal(pos);
-    }
-  }
-  
-  /**
-   * Expert: implements seek. Sets current position in this file, where the next
-   * {@link #readInternal(byte[],int,int)} will occur.
-   * 
-   * @see #readInternal(byte[],int,int)
-   */
-  protected abstract void seekInternal(long pos) throws IOException;
-  
-  @Override
-  public IndexInput clone() {
-    CustomBufferedIndexInput clone = (CustomBufferedIndexInput) super.clone();
-    
-    clone.buffer = null;
-    clone.bufferLength = 0;
-    clone.bufferPosition = 0;
-    clone.bufferStart = getFilePointer();
-    
-    return clone;
-  }
-  
-  @Override
-  public IndexInput slice(String sliceDescription, long offset, long length) throws IOException {
-    return BufferedIndexInput.wrap(sliceDescription, this, offset, length);
-  }
-
-  /**
-   * Flushes the in-memory bufer to the given output, copying at most
-   * <code>numBytes</code>.
-   * <p>
-   * <b>NOTE:</b> this method does not refill the buffer, however it does
-   * advance the buffer position.
-   * 
-   * @return the number of bytes actually flushed from the in-memory buffer.
-   */
-  protected int flushBuffer(IndexOutput out, long numBytes) throws IOException {
-    int toCopy = bufferLength - bufferPosition;
-    if (toCopy > numBytes) {
-      toCopy = (int) numBytes;
-    }
-    if (toCopy > 0) {
-      out.writeBytes(buffer, bufferPosition, toCopy);
-      bufferPosition += toCopy;
-    }
-    return toCopy;
-  }
-}
diff --git a/solr/core/src/java/org/apache/solr/store/blockcache/Metrics.java b/solr/core/src/java/org/apache/solr/store/blockcache/Metrics.java
deleted file mode 100644
index bfe4105..0000000
--- a/solr/core/src/java/org/apache/solr/store/blockcache/Metrics.java
+++ /dev/null
@@ -1,148 +0,0 @@
-package org.apache.solr.store.blockcache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.net.URL;
-import java.util.Map;
-import java.util.Map.Entry;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.solr.common.util.NamedList;
-import org.apache.solr.common.util.SimpleOrderedMap;
-import org.apache.solr.core.SolrCore;
-import org.apache.solr.core.SolrInfoMBean;
-import org.apache.solr.search.SolrCacheBase;
-
-/**
- * A {@link SolrInfoMBean} that provides metrics on block cache operations.
- *
- * @lucene.experimental
- */
-public class Metrics extends SolrCacheBase implements SolrInfoMBean {
-  
-  public static class MethodCall {
-    public AtomicLong invokes = new AtomicLong();
-    public AtomicLong times = new AtomicLong();
-  }
-
-  public AtomicLong blockCacheHit = new AtomicLong(0);
-  public AtomicLong blockCacheMiss = new AtomicLong(0);
-  public AtomicLong blockCacheEviction = new AtomicLong(0);
-  public AtomicLong blockCacheSize = new AtomicLong(0);
-  public AtomicLong rowReads = new AtomicLong(0);
-  public AtomicLong rowWrites = new AtomicLong(0);
-  public AtomicLong recordReads = new AtomicLong(0);
-  public AtomicLong recordWrites = new AtomicLong(0);
-  public AtomicLong queriesExternal = new AtomicLong(0);
-  public AtomicLong queriesInternal = new AtomicLong(0);
-  public AtomicLong shardBuffercacheAllocate = new AtomicLong(0);
-  public AtomicLong shardBuffercacheLost = new AtomicLong(0);
-  public Map<String,MethodCall> methodCalls = new ConcurrentHashMap<>();
-  
-  public AtomicLong tableCount = new AtomicLong(0);
-  public AtomicLong rowCount = new AtomicLong(0);
-  public AtomicLong recordCount = new AtomicLong(0);
-  public AtomicLong indexCount = new AtomicLong(0);
-  public AtomicLong indexMemoryUsage = new AtomicLong(0);
-  public AtomicLong segmentCount = new AtomicLong(0);
-
-  private long previous = System.nanoTime();
-
-  public static void main(String[] args) throws InterruptedException {
-    Metrics metrics = new Metrics();
-    MethodCall methodCall = new MethodCall();
-    metrics.methodCalls.put("test", methodCall);
-    for (int i = 0; i < 100; i++) {
-      metrics.blockCacheHit.incrementAndGet();
-      metrics.blockCacheMiss.incrementAndGet();
-      methodCall.invokes.incrementAndGet();
-      methodCall.times.addAndGet(56000000);
-      Thread.sleep(500);
-    }
-  }
-
-  public NamedList<Number> getStatistics() {
-    NamedList<Number> stats = new SimpleOrderedMap<>(21); // room for one method call before growing
-    
-    long now = System.nanoTime();
-    float seconds = (now - previous) / 1000000000.0f;
-    
-    long hits = blockCacheHit.getAndSet(0);
-    long lookups = hits + blockCacheMiss.getAndSet(0);
-    
-    stats.add("lookups", getPerSecond(lookups, seconds));
-    stats.add("hits", getPerSecond(hits, seconds));
-    stats.add("hitratio", calcHitRatio(lookups, hits));
-    stats.add("evictions", getPerSecond(blockCacheEviction.getAndSet(0), seconds));
-    stats.add("size", blockCacheSize.get());
-    stats.add("row.reads", getPerSecond(rowReads.getAndSet(0), seconds));
-    stats.add("row.writes", getPerSecond(rowWrites.getAndSet(0), seconds));
-    stats.add("record.reads", getPerSecond(recordReads.getAndSet(0), seconds));
-    stats.add("record.writes", getPerSecond(recordWrites.getAndSet(0), seconds));
-    stats.add("query.external", getPerSecond(queriesExternal.getAndSet(0), seconds));
-    stats.add("query.internal", getPerSecond(queriesInternal.getAndSet(0), seconds));
-    stats.add("buffercache.allocations", getPerSecond(shardBuffercacheAllocate.getAndSet(0), seconds));
-    stats.add("buffercache.lost", getPerSecond(shardBuffercacheLost.getAndSet(0), seconds));
-    for (Entry<String,MethodCall> entry : methodCalls.entrySet()) {
-      String key = entry.getKey();
-      MethodCall value = entry.getValue();
-      long invokes = value.invokes.getAndSet(0);
-      long times = value.times.getAndSet(0);
-      
-      float avgTimes = (times / (float) invokes) / 1000000000.0f;
-      stats.add("methodcalls." + key + ".count", getPerSecond(invokes, seconds));
-      stats.add("methodcalls." + key + ".time", avgTimes);
-    }
-    stats.add("tables", tableCount.get());
-    stats.add("rows", rowCount.get());
-    stats.add("records", recordCount.get());
-    stats.add("index.count", indexCount.get());
-    stats.add("index.memoryusage", indexMemoryUsage.get());
-    stats.add("index.segments", segmentCount.get());
-    previous = now;
-    
-    return stats;
-  }
-
-  private float getPerSecond(long value, float seconds) {
-    return (float) (value / seconds);
-  }
-
-  // SolrInfoMBean methods
-
-  @Override
-  public String getName() {
-    return "HdfsBlockCache";
-  }
-
-  @Override
-  public String getDescription() {
-    return "Provides metrics for the HdfsDirectoryFactory BlockCache.";
-  }
-
-  @Override
-  public String getSource() {
-    return null;
-  }
-
-  @Override
-  public URL[] getDocs() {
-    return null;
-  }
-}
diff --git a/solr/core/src/java/org/apache/solr/store/blockcache/ReusedBufferedIndexOutput.java b/solr/core/src/java/org/apache/solr/store/blockcache/ReusedBufferedIndexOutput.java
deleted file mode 100644
index f9e1c42..0000000
--- a/solr/core/src/java/org/apache/solr/store/blockcache/ReusedBufferedIndexOutput.java
+++ /dev/null
@@ -1,165 +0,0 @@
-package org.apache.solr.store.blockcache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.store.IndexOutput;
-
-/**
- * @lucene.experimental
- */
-public abstract class ReusedBufferedIndexOutput extends IndexOutput {
-  
-  public static final int BUFFER_SIZE = 1024;
-  
-  private int bufferSize = BUFFER_SIZE;
-  
-  protected byte[] buffer;
-  
-  /** position in the file of buffer */
-  private long bufferStart = 0;
-  /** end of valid bytes */
-  private int bufferLength = 0;
-  /** next byte to write */
-  private int bufferPosition = 0;
-  /** total length of the file */
-  private long fileLength = 0;
-  
-  private final Store store;
-  
-  public ReusedBufferedIndexOutput(String resourceDescription) {
-    this(resourceDescription, BUFFER_SIZE);
-  }
-  
-  public ReusedBufferedIndexOutput(String resourceDescription, int bufferSize) {
-    super(resourceDescription);
-    checkBufferSize(bufferSize);
-    this.bufferSize = bufferSize;
-    store = BufferStore.instance(bufferSize);
-    buffer = store.takeBuffer(this.bufferSize);
-  }
-  
-  protected long getBufferStart() {
-    return bufferStart;
-  }
-  
-  private void checkBufferSize(int bufferSize) {
-    if (bufferSize <= 0) throw new IllegalArgumentException(
-        "bufferSize must be greater than 0 (got " + bufferSize + ")");
-  }
-  
-  /** Write the buffered bytes to cache */
-  protected void flushBufferToCache() throws IOException {
-    writeInternal(buffer, 0, bufferLength);
-    
-    bufferStart += bufferLength;
-    bufferLength = 0;
-    bufferPosition = 0;
-  }
-  
-  protected abstract void closeInternal() throws IOException;
-  
-  @Override
-  public void close() throws IOException {
-    flushBufferToCache();
-    closeInternal();
-    store.putBuffer(buffer);
-    buffer = null;
-  }
-  
-  @Override
-  public long getFilePointer() {
-    return bufferStart + bufferPosition;
-  }
-  
-  @Override
-  public void writeByte(byte b) throws IOException {
-    if (bufferPosition >= bufferSize) {
-      flushBufferToCache();
-    }
-    if (getFilePointer() >= fileLength) {
-      fileLength++;
-    }
-    buffer[bufferPosition++] = b;
-    if (bufferPosition > bufferLength) {
-      bufferLength = bufferPosition;
-    }
-  }
-  
-  /**
-   * Expert: implements buffer flushing to cache. Writes bytes to the current
-   * position in the output.
-   * 
-   * @param b
-   *          the array of bytes to write
-   * @param offset
-   *          the offset in the array of bytes to write
-   * @param length
-   *          the number of bytes to write
-   */
-  protected abstract void writeInternal(byte[] b, int offset, int length)
-      throws IOException;
-  
-  @Override
-  public void writeBytes(byte[] b, int offset, int length) throws IOException {
-    if (getFilePointer() + length > fileLength) {
-      fileLength = getFilePointer() + length;
-    }
-    if (length <= bufferSize - bufferPosition) {
-      // the buffer contains enough space to satisfy this request
-      if (length > 0) { // to allow b to be null if len is 0...
-        System.arraycopy(b, offset, buffer, bufferPosition, length);
-      }
-      bufferPosition += length;
-      if (bufferPosition > bufferLength) {
-        bufferLength = bufferPosition;
-      }
-    } else {
-      // the buffer does not have enough space. First buffer all we've got.
-      int available = bufferSize - bufferPosition;
-      if (available > 0) {
-        System.arraycopy(b, offset, buffer, bufferPosition, available);
-        offset += available;
-        length -= available;
-        bufferPosition = bufferSize;
-        bufferLength = bufferSize;
-      }
-      
-      flushBufferToCache();
-      
-      // and now, write the remaining 'length' bytes:
-      if (length < bufferSize) {
-        // If the amount left to write is small enough do it in the usual
-        // buffered way:
-        System.arraycopy(b, offset, buffer, 0, length);
-        bufferPosition = length;
-        bufferLength = length;
-      } else {
-        // The amount left to write is larger than the buffer
-        // there's no performance reason not to write it all
-        // at once.
-        writeInternal(b, offset, length);
-        bufferStart += length;
-        bufferPosition = 0;
-        bufferLength = 0;
-      }
-      
-    }
-  }
-}
diff --git a/solr/core/src/java/org/apache/solr/store/blockcache/SolrMetrics.java b/solr/core/src/java/org/apache/solr/store/blockcache/SolrMetrics.java
new file mode 100644
index 0000000..6ccbd8c
--- /dev/null
+++ b/solr/core/src/java/org/apache/solr/store/blockcache/SolrMetrics.java
@@ -0,0 +1,118 @@
+package org.apache.solr.store.blockcache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.net.URL;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.lucene.store.blockcache.Metrics;
+import org.apache.solr.core.SolrInfoMBean;
+import org.apache.solr.search.SolrCacheBase;
+
+import org.apache.solr.common.util.NamedList;
+import org.apache.solr.common.util.SimpleOrderedMap;
+import org.apache.solr.core.SolrCore;
+
+/**
+ * Extension of the metrics interface that allows for publication of metrics in solr
+ */
+public class SolrMetrics extends Metrics implements SolrInfoMBean {
+
+  private long previous = System.nanoTime();
+
+  public NamedList<Number> getStatistics() {
+    NamedList<Number> stats = new SimpleOrderedMap<>(21); // room for one method call before growing
+
+    long now = System.nanoTime();
+    float seconds = (now - previous) / 1000000000.0f;
+
+    long hits = blockCacheHit.getAndSet(0);
+    long lookups = hits + blockCacheMiss.getAndSet(0);
+
+    stats.add("lookups", getPerSecond(lookups, seconds));
+    stats.add("hits", getPerSecond(hits, seconds));
+    stats.add("hitratio", SolrCacheBase.calcHitRatio(lookups, hits));
+    stats.add("evictions", getPerSecond(blockCacheEviction.getAndSet(0), seconds));
+    stats.add("size", blockCacheSize.get());
+    stats.add("row.reads", getPerSecond(rowReads.getAndSet(0), seconds));
+    stats.add("row.writes", getPerSecond(rowWrites.getAndSet(0), seconds));
+    stats.add("record.reads", getPerSecond(recordReads.getAndSet(0), seconds));
+    stats.add("record.writes", getPerSecond(recordWrites.getAndSet(0), seconds));
+    stats.add("query.external", getPerSecond(queriesExternal.getAndSet(0), seconds));
+    stats.add("query.internal", getPerSecond(queriesInternal.getAndSet(0), seconds));
+    stats.add("buffercache.allocations", getPerSecond(shardBuffercacheAllocate.getAndSet(0), seconds));
+    stats.add("buffercache.lost", getPerSecond(shardBuffercacheLost.getAndSet(0), seconds));
+
+    for (Map.Entry<String, MethodCall> entry : methodCalls.entrySet()) {
+      String key = entry.getKey();
+      MethodCall value = entry.getValue();
+      long invokes = value.invokes.getAndSet(0);
+      long times = value.times.getAndSet(0);
+
+      float avgTimes = (times / (float) invokes) / 1000000000.0f;
+      stats.add("methodcalls." + key + ".count", getPerSecond(invokes, seconds));
+      stats.add("methodcalls." + key + ".time", avgTimes);
+    }
+    stats.add("tables", tableCount.get());
+    stats.add("rows", rowCount.get());
+    stats.add("records", recordCount.get());
+    stats.add("index.count", indexCount.get());
+    stats.add("index.memoryusage", indexMemoryUsage.get());
+    stats.add("index.segments", segmentCount.get());
+    previous = now;
+
+    return stats;
+  }
+
+  private float getPerSecond(long value, float seconds) {
+    return (float) (value / seconds);
+  }
+
+  // SolrInfoMBean methods
+
+  @Override
+  public String getName() {
+    return "HdfsBlockCache";
+  }
+
+  @Override
+  public String getDescription() {
+    return "Provides metrics for the HdfsDirectoryFactory BlockCache.";
+  }
+
+  @Override
+  public String getSource() {
+    return null;
+  }
+
+  @Override
+  public URL[] getDocs() {
+    return null;
+  }
+
+  @Override
+  public String getVersion() {
+    return SolrCore.version;
+  }
+
+  @Override
+  public Category getCategory() {
+    return Category.CACHE;
+  }
+}
diff --git a/solr/core/src/java/org/apache/solr/store/blockcache/Store.java b/solr/core/src/java/org/apache/solr/store/blockcache/Store.java
deleted file mode 100644
index 8fb4e48..0000000
--- a/solr/core/src/java/org/apache/solr/store/blockcache/Store.java
+++ /dev/null
@@ -1,29 +0,0 @@
-package org.apache.solr.store.blockcache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * @lucene.experimental
- */
-public interface Store {
-
-  byte[] takeBuffer(int bufferSize);
-
-  void putBuffer(byte[] buffer);
-
-}
diff --git a/solr/core/src/java/org/apache/solr/store/blockcache/package-info.java b/solr/core/src/java/org/apache/solr/store/blockcache/package-info.java
deleted file mode 100644
index 6da8bb3..0000000
--- a/solr/core/src/java/org/apache/solr/store/blockcache/package-info.java
+++ /dev/null
@@ -1,23 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
- 
-/** 
- * An HDFS blockcache implementation.
- */
-package org.apache.solr.store.blockcache;
-
-
diff --git a/solr/core/src/java/org/apache/solr/store/hdfs/HdfsDirectory.java b/solr/core/src/java/org/apache/solr/store/hdfs/HdfsDirectory.java
deleted file mode 100644
index 70e4d55..0000000
--- a/solr/core/src/java/org/apache/solr/store/hdfs/HdfsDirectory.java
+++ /dev/null
@@ -1,244 +0,0 @@
-package org.apache.solr.store.hdfs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FileContext;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.ipc.RemoteException;
-import org.apache.lucene.store.BaseDirectory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.LockFactory;
-import org.apache.solr.store.blockcache.CustomBufferedIndexInput;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public class HdfsDirectory extends BaseDirectory {
-  public static Logger LOG = LoggerFactory.getLogger(HdfsDirectory.class);
-  
-  public static final int BUFFER_SIZE = 8192;
-  
-  private static final String LF_EXT = ".lf";
-  protected final Path hdfsDirPath;
-  protected final Configuration configuration;
-  
-  private final FileSystem fileSystem;
-  private final FileContext fileContext;
-  
-  public HdfsDirectory(Path hdfsDirPath, Configuration configuration) throws IOException {
-    this(hdfsDirPath, HdfsLockFactory.INSTANCE, configuration);
-  }
-
-  public HdfsDirectory(Path hdfsDirPath, LockFactory lockFactory, Configuration configuration)
-      throws IOException {
-    super(lockFactory);
-    this.hdfsDirPath = hdfsDirPath;
-    this.configuration = configuration;
-    fileSystem = FileSystem.get(hdfsDirPath.toUri(), configuration);
-    fileContext = FileContext.getFileContext(hdfsDirPath.toUri(), configuration);
-    
-    while (true) {
-      try {
-        if (!fileSystem.exists(hdfsDirPath)) {
-          boolean success = fileSystem.mkdirs(hdfsDirPath);
-          if (!success) {
-            throw new RuntimeException("Could not create directory: " + hdfsDirPath);
-          }
-        } else {
-          fileSystem.mkdirs(hdfsDirPath); // check for safe mode
-        }
-        
-        break;
-      } catch (RemoteException e) {
-        if (e.getClassName().equals("org.apache.hadoop.hdfs.server.namenode.SafeModeException")) {
-          LOG.warn("The NameNode is in SafeMode - Solr will wait 5 seconds and try again.");
-          try {
-            Thread.sleep(5000);
-          } catch (InterruptedException e1) {
-            Thread.interrupted();
-          }
-          continue;
-        }
-        org.apache.solr.common.util.IOUtils.closeQuietly(fileSystem);
-        throw new RuntimeException(
-            "Problem creating directory: " + hdfsDirPath, e);
-      } catch (Exception e) {
-        org.apache.solr.common.util.IOUtils.closeQuietly(fileSystem);
-        throw new RuntimeException(
-            "Problem creating directory: " + hdfsDirPath, e);
-      }
-    }
-  }
-  
-  @Override
-  public void close() throws IOException {
-    LOG.info("Closing hdfs directory {}", hdfsDirPath);
-    fileSystem.close();
-  }
-  
-  @Override
-  public IndexOutput createOutput(String name, IOContext context) throws IOException {
-    return new HdfsFileWriter(getFileSystem(), new Path(hdfsDirPath, name));
-  }
-  
-  private String[] getNormalNames(List<String> files) {
-    int size = files.size();
-    for (int i = 0; i < size; i++) {
-      String str = files.get(i);
-      files.set(i, toNormalName(str));
-    }
-    return files.toArray(new String[] {});
-  }
-  
-  private String toNormalName(String name) {
-    if (name.endsWith(LF_EXT)) {
-      return name.substring(0, name.length() - 3);
-    }
-    return name;
-  }
-  
-  @Override
-  public IndexInput openInput(String name, IOContext context)
-      throws IOException {
-    return openInput(name, BUFFER_SIZE);
-  }
-  
-  private IndexInput openInput(String name, int bufferSize) throws IOException {
-    return new HdfsIndexInput(name, getFileSystem(), new Path(
-        hdfsDirPath, name), BUFFER_SIZE);
-  }
-  
-  @Override
-  public void deleteFile(String name) throws IOException {
-    Path path = new Path(hdfsDirPath, name);
-    LOG.debug("Deleting {}", path);
-    getFileSystem().delete(path, false);
-  }
-  
-  @Override
-  public void renameFile(String source, String dest) throws IOException {
-    Path sourcePath = new Path(hdfsDirPath, source);
-    Path destPath = new Path(hdfsDirPath, dest);
-    fileContext.rename(sourcePath, destPath);
-  }
-
-  @Override
-  public long fileLength(String name) throws IOException {
-    return HdfsFileReader.getLength(getFileSystem(),
-        new Path(hdfsDirPath, name));
-  }
-  
-  public long fileModified(String name) throws IOException {
-    FileStatus fileStatus = getFileSystem().getFileStatus(
-        new Path(hdfsDirPath, name));
-    return fileStatus.getModificationTime();
-  }
-  
-  @Override
-  public String[] listAll() throws IOException {
-    FileStatus[] listStatus = getFileSystem().listStatus(hdfsDirPath);
-    List<String> files = new ArrayList<>();
-    if (listStatus == null) {
-      return new String[] {};
-    }
-    for (FileStatus status : listStatus) {
-      files.add(status.getPath().getName());
-    }
-    return getNormalNames(files);
-  }
-
-  public Path getHdfsDirPath() {
-    return hdfsDirPath;
-  }
-  
-  public FileSystem getFileSystem() {
-    return fileSystem;
-  }
-  
-  public Configuration getConfiguration() {
-    return configuration;
-  }
-  
-  static class HdfsIndexInput extends CustomBufferedIndexInput {
-    public static Logger LOG = LoggerFactory
-        .getLogger(HdfsIndexInput.class);
-    
-    private final Path path;
-    private final FSDataInputStream inputStream;
-    private final long length;
-    private boolean clone = false;
-    
-    public HdfsIndexInput(String name, FileSystem fileSystem, Path path,
-        int bufferSize) throws IOException {
-      super(name);
-      this.path = path;
-      LOG.debug("Opening normal index input on {}", path);
-      FileStatus fileStatus = fileSystem.getFileStatus(path);
-      length = fileStatus.getLen();
-      inputStream = fileSystem.open(path, bufferSize);
-    }
-    
-    @Override
-    protected void readInternal(byte[] b, int offset, int length)
-        throws IOException {
-      inputStream.readFully(getFilePointer(), b, offset, length);
-    }
-    
-    @Override
-    protected void seekInternal(long pos) throws IOException {
-
-    }
-    
-    @Override
-    protected void closeInternal() throws IOException {
-      LOG.debug("Closing normal index input on {}", path);
-      if (!clone) {
-        inputStream.close();
-      }
-    }
-    
-    @Override
-    public long length() {
-      return length;
-    }
-    
-    @Override
-    public IndexInput clone() {
-      HdfsIndexInput clone = (HdfsIndexInput) super.clone();
-      clone.clone = true;
-      return clone;
-    }
-  }
-  
-  @Override
-  public void sync(Collection<String> names) throws IOException {
-    LOG.debug("Sync called on {}", Arrays.toString(names.toArray()));
-  }
-  
-}
diff --git a/solr/core/src/java/org/apache/solr/store/hdfs/HdfsFileReader.java b/solr/core/src/java/org/apache/solr/store/hdfs/HdfsFileReader.java
deleted file mode 100644
index 0294496..0000000
--- a/solr/core/src/java/org/apache/solr/store/hdfs/HdfsFileReader.java
+++ /dev/null
@@ -1,105 +0,0 @@
-package org.apache.solr.store.hdfs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.FileNotFoundException;
-import java.io.IOException;
-
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.lucene.store.DataInput;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * @lucene.experimental
- */
-public class HdfsFileReader extends DataInput {
-  
-  public static Logger LOG = LoggerFactory.getLogger(HdfsFileReader.class);
-  
-  private final Path path;
-  private FSDataInputStream inputStream;
-  private long length;
-  private boolean isClone;
-  
-  public HdfsFileReader(FileSystem fileSystem, Path path, int bufferSize)
-      throws IOException {
-    this.path = path;
-    LOG.debug("Opening reader on {}", path);
-    if (!fileSystem.exists(path)) {
-      throw new FileNotFoundException(path.toString());
-    }
-    inputStream = fileSystem.open(path, bufferSize);
-    FileStatus fileStatus = fileSystem.getFileStatus(path);
-    length = fileStatus.getLen();
-  }
-  
-  public HdfsFileReader(FileSystem fileSystem, Path path) throws IOException {
-    this(fileSystem, path, HdfsDirectory.BUFFER_SIZE);
-  }
-  
-  public long length() {
-    return length;
-  }
-  
-  public void seek(long pos) throws IOException {
-    inputStream.seek(pos);
-  }
-  
-  public void close() throws IOException {
-    if (!isClone) {
-      inputStream.close();
-    }
-    LOG.debug("Closing reader on {}", path);
-  }
-  
-  /**
-   * This method should never be used!
-   */
-  @Override
-  public byte readByte() throws IOException {
-    LOG.warn("Should not be used!");
-    return inputStream.readByte();
-  }
-  
-  @Override
-  public void readBytes(byte[] b, int offset, int len) throws IOException {
-    while (len > 0) {
-      int lenRead = inputStream.read(b, offset, len);
-      offset += lenRead;
-      len -= lenRead;
-    }
-  }
-  
-  public static long getLength(FileSystem fileSystem, Path path)
-      throws IOException {
-    FileStatus fileStatus = fileSystem.getFileStatus(path);
-    return fileStatus.getLen();
-  }
-  
-  @Override
-  public DataInput clone() {
-    HdfsFileReader reader = (HdfsFileReader) super.clone();
-    reader.isClone = true;
-    return reader;
-  }
-  
-}
diff --git a/solr/core/src/java/org/apache/solr/store/hdfs/HdfsFileWriter.java b/solr/core/src/java/org/apache/solr/store/hdfs/HdfsFileWriter.java
deleted file mode 100644
index 2a467dd..0000000
--- a/solr/core/src/java/org/apache/solr/store/hdfs/HdfsFileWriter.java
+++ /dev/null
@@ -1,57 +0,0 @@
-package org.apache.solr.store.hdfs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.OutputStream;
-import java.util.EnumSet;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.CreateFlag;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FsServerDefaults;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.lucene.store.OutputStreamIndexOutput;
-
-/**
- * @lucene.experimental
- */
-public class HdfsFileWriter extends OutputStreamIndexOutput {
-  
-  public static final String HDFS_SYNC_BLOCK = "solr.hdfs.sync.block";
-  public static final int BUFFER_SIZE = 16384;
-  
-  public HdfsFileWriter(FileSystem fileSystem, Path path) throws IOException {
-    super("fileSystem=" + fileSystem + " path=" + path, getOutputStream(fileSystem, path), BUFFER_SIZE);
-  }
-  
-  private static final OutputStream getOutputStream(FileSystem fileSystem, Path path) throws IOException {
-    Configuration conf = fileSystem.getConf();
-    FsServerDefaults fsDefaults = fileSystem.getServerDefaults(path);
-    EnumSet<CreateFlag> flags = EnumSet.of(CreateFlag.CREATE,
-        CreateFlag.OVERWRITE);
-    if (Boolean.getBoolean(HDFS_SYNC_BLOCK)) {
-      flags.add(CreateFlag.SYNC_BLOCK);
-    }
-    return fileSystem.create(path, FsPermission.getDefault()
-        .applyUMask(FsPermission.getUMask(conf)), flags, fsDefaults
-        .getFileBufferSize(), fsDefaults.getReplication(), fsDefaults
-        .getBlockSize(), null);
-  }
-}
diff --git a/solr/core/src/java/org/apache/solr/store/hdfs/HdfsLockFactory.java b/solr/core/src/java/org/apache/solr/store/hdfs/HdfsLockFactory.java
deleted file mode 100644
index 940c78c..0000000
--- a/solr/core/src/java/org/apache/solr/store/hdfs/HdfsLockFactory.java
+++ /dev/null
@@ -1,124 +0,0 @@
-package org.apache.solr.store.hdfs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileAlreadyExistsException;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.ipc.RemoteException;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.Lock;
-import org.apache.lucene.store.LockFactory;
-import org.apache.lucene.store.LockObtainFailedException;
-import org.apache.lucene.store.LockReleaseFailedException;
-import org.apache.solr.common.util.IOUtils;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public class HdfsLockFactory extends LockFactory {
-  public static Logger log = LoggerFactory.getLogger(HdfsLockFactory.class);
-  
-  public static final HdfsLockFactory INSTANCE = new HdfsLockFactory();
-  
-  private HdfsLockFactory() {}
-  
-  @Override
-  public Lock obtainLock(Directory dir, String lockName) throws IOException {
-    if (!(dir instanceof HdfsDirectory)) {
-      throw new UnsupportedOperationException("HdfsLockFactory can only be used with HdfsDirectory subclasses, got: " + dir);
-    }
-    final HdfsDirectory hdfsDir = (HdfsDirectory) dir;
-    final Configuration conf = hdfsDir.getConfiguration();
-    final Path lockPath = hdfsDir.getHdfsDirPath();
-    final Path lockFile = new Path(lockPath, lockName);
-    
-    FSDataOutputStream file = null;
-    final FileSystem fs = FileSystem.get(lockPath.toUri(), conf);
-    while (true) {
-      try {
-        if (!fs.exists(lockPath)) {
-          boolean success = fs.mkdirs(lockPath);
-          if (!success) {
-            throw new RuntimeException("Could not create directory: " + lockPath);
-          }
-        } else {
-          // just to check for safe mode
-          fs.mkdirs(lockPath);
-        }
-        
-        file = fs.create(lockFile, false);
-        break;
-      } catch (FileAlreadyExistsException e) {
-        throw new LockObtainFailedException("Cannot obtain lock file: " + lockFile, e);
-      } catch (RemoteException e) {
-        if (e.getClassName().equals(
-            "org.apache.hadoop.hdfs.server.namenode.SafeModeException")) {
-          log.warn("The NameNode is in SafeMode - Solr will wait 5 seconds and try again.");
-          try {
-            Thread.sleep(5000);
-          } catch (InterruptedException e1) {
-            Thread.interrupted();
-          }
-          continue;
-        }
-        throw new LockObtainFailedException("Cannot obtain lock file: " + lockFile, e);
-      } catch (IOException e) {
-        throw new LockObtainFailedException("Cannot obtain lock file: " + lockFile, e);
-      } finally {
-        IOUtils.closeQuietly(file);
-      }
-    }
-
-    return new HdfsLock(fs, lockFile);
-  }
-  
-  private static final class HdfsLock extends Lock {
-    
-    private final FileSystem fs;
-    private final Path lockFile;
-    private volatile boolean closed;
-    
-    HdfsLock(FileSystem fs, Path lockFile) {
-      this.fs = fs;
-      this.lockFile = lockFile;
-    }
-    
-    @Override
-    public void close() throws IOException {
-      if (closed) {
-        return;
-      }
-      try {
-        if (fs.exists(lockFile) && !fs.delete(lockFile, false)) {
-          throw new LockReleaseFailedException("failed to delete: " + lockFile);
-        }
-      } finally {
-        IOUtils.closeQuietly(fs);
-      }
-    }
-
-    @Override
-    public void ensureValid() throws IOException {
-      // no idea how to implement this on HDFS
-    }
-  }
-}
diff --git a/solr/core/src/java/org/apache/solr/store/hdfs/package-info.java b/solr/core/src/java/org/apache/solr/store/hdfs/package-info.java
deleted file mode 100644
index 5064bcd..0000000
--- a/solr/core/src/java/org/apache/solr/store/hdfs/package-info.java
+++ /dev/null
@@ -1,22 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
- 
-/** 
- * An HDFS Directory implementation.
- */
-package org.apache.solr.store.hdfs;
-
diff --git a/solr/core/src/test/org/apache/solr/cloud/SharedFSAutoReplicaFailoverTest.java b/solr/core/src/test/org/apache/solr/cloud/SharedFSAutoReplicaFailoverTest.java
index 80b8f13..75141cf 100644
--- a/solr/core/src/test/org/apache/solr/cloud/SharedFSAutoReplicaFailoverTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/SharedFSAutoReplicaFailoverTest.java
@@ -46,7 +46,7 @@ import org.apache.solr.common.params.CollectionParams;
 import org.apache.solr.common.params.MapSolrParams;
 import org.apache.solr.common.util.ExecutorUtil;
 import org.apache.solr.util.DefaultSolrThreadFactory;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
diff --git a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsBasicDistributedZk2Test.java b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsBasicDistributedZk2Test.java
index fe23ea4..ff0c0b3 100644
--- a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsBasicDistributedZk2Test.java
+++ b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsBasicDistributedZk2Test.java
@@ -22,7 +22,7 @@ import java.io.IOException;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.apache.solr.cloud.BasicDistributedZk2Test;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 
diff --git a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsBasicDistributedZkTest.java b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsBasicDistributedZkTest.java
index 1f3fe3e..92216cb 100644
--- a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsBasicDistributedZkTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsBasicDistributedZkTest.java
@@ -22,7 +22,7 @@ import java.io.IOException;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.apache.solr.cloud.BasicDistributedZkTest;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 
diff --git a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsChaosMonkeySafeLeaderTest.java b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsChaosMonkeySafeLeaderTest.java
index 0f6e906..4486310 100644
--- a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsChaosMonkeySafeLeaderTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsChaosMonkeySafeLeaderTest.java
@@ -22,7 +22,7 @@ import java.io.IOException;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.apache.solr.cloud.ChaosMonkeySafeLeaderTest;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 
diff --git a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsCollectionsAPIDistributedZkTest.java b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsCollectionsAPIDistributedZkTest.java
index dd2691a..efc9c9b 100644
--- a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsCollectionsAPIDistributedZkTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsCollectionsAPIDistributedZkTest.java
@@ -23,7 +23,7 @@ import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.apache.solr.cloud.CollectionsAPIDistributedZkTest;
 import org.apache.solr.update.HdfsUpdateLog;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 
diff --git a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsNNFailoverTest.java b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsNNFailoverTest.java
index 9420736..94c2498 100644
--- a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsNNFailoverTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsNNFailoverTest.java
@@ -22,7 +22,7 @@ import java.io.IOException;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.apache.solr.cloud.BasicDistributedZkTest;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
diff --git a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsRecoverLeaseTest.java b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsRecoverLeaseTest.java
index 76b7f6c..7011430 100644
--- a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsRecoverLeaseTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsRecoverLeaseTest.java
@@ -28,7 +28,7 @@ import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.solr.SolrTestCaseJ4;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.apache.solr.util.FSHDFSUtils;
 import org.apache.solr.util.FSHDFSUtils.CallerInfo;
 import org.junit.After;
diff --git a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsRecoveryZkTest.java b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsRecoveryZkTest.java
index 759a32f..d695c80 100644
--- a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsRecoveryZkTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsRecoveryZkTest.java
@@ -22,7 +22,7 @@ import java.io.IOException;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.apache.solr.cloud.RecoveryZkTest;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 
diff --git a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsSyncSliceTest.java b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsSyncSliceTest.java
index baf3577..1139e85 100644
--- a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsSyncSliceTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsSyncSliceTest.java
@@ -22,7 +22,7 @@ import java.io.IOException;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.apache.solr.cloud.SyncSliceTest;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 
diff --git a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsThreadLeakTest.java b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsThreadLeakTest.java
index 7c87aa7..eaa6c6e 100644
--- a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsThreadLeakTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsThreadLeakTest.java
@@ -18,7 +18,6 @@ package org.apache.solr.cloud.hdfs;
  */
 
 import java.io.IOException;
-import java.net.URI;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataOutputStream;
@@ -27,7 +26,7 @@ import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hdfs.DistributedFileSystem;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.solr.SolrTestCaseJ4;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.junit.After;
 import org.junit.AfterClass;
 import org.junit.Before;
diff --git a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsUnloadDistributedZkTest.java b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsUnloadDistributedZkTest.java
index 7f6faec..95e33d2 100644
--- a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsUnloadDistributedZkTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsUnloadDistributedZkTest.java
@@ -22,7 +22,7 @@ import java.io.IOException;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.apache.solr.cloud.UnloadDistributedZkTest;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 
diff --git a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java
index 2c74c89..30d77c9 100644
--- a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java
@@ -19,6 +19,7 @@ package org.apache.solr.cloud.hdfs;
 
 import com.carrotsearch.randomizedtesting.annotations.ThreadLeakFilters;
 
+
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.store.NRTCachingDirectory;
@@ -34,11 +35,11 @@ import org.apache.solr.core.CoreContainer;
 import org.apache.solr.core.HdfsDirectoryFactory;
 import org.apache.solr.core.SolrCore;
 import org.apache.solr.servlet.SolrDispatchFilter;
-import org.apache.solr.store.blockcache.BlockCache;
-import org.apache.solr.store.blockcache.BlockDirectory;
-import org.apache.solr.store.blockcache.BlockDirectoryCache;
-import org.apache.solr.store.blockcache.Cache;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.store.blockcache.BlockCache;
+import org.apache.lucene.store.blockcache.BlockDirectory;
+import org.apache.lucene.store.blockcache.BlockDirectoryCache;
+import org.apache.lucene.store.blockcache.Cache;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.apache.solr.util.RefCounted;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
diff --git a/solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest.java b/solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest.java
index b47a474..97348b9 100644
--- a/solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest.java
@@ -36,7 +36,7 @@ import org.apache.solr.cloud.ChaosMonkey;
 import org.apache.solr.common.params.CollectionParams.CollectionAction;
 import org.apache.solr.common.params.ModifiableSolrParams;
 import org.apache.solr.common.util.NamedList;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.apache.zookeeper.KeeperException;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
diff --git a/solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest.java b/solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest.java
index 979222b..527f759 100644
--- a/solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest.java
+++ b/solr/core/src/test/org/apache/solr/core/HdfsDirectoryFactoryTest.java
@@ -31,7 +31,7 @@ import org.apache.solr.cloud.hdfs.HdfsTestUtil;
 import org.apache.solr.common.util.NamedList;
 import org.apache.solr.core.DirectoryFactory.DirContext;
 import org.apache.solr.handler.SnapShooter;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.apache.solr.util.MockCoreContainer.MockCoreDescriptor;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
diff --git a/solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java b/solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java
index 1ea45f6..216364a 100644
--- a/solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java
+++ b/solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java
@@ -39,7 +39,6 @@ import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FsStatus;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.solr.SolrTestCaseJ4;
@@ -51,7 +50,7 @@ import org.apache.solr.update.HdfsUpdateLog;
 import org.apache.solr.update.UpdateHandler;
 import org.apache.solr.update.UpdateLog;
 import org.apache.solr.update.processor.DistributedUpdateProcessor.DistribPhase;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Ignore;
diff --git a/solr/core/src/test/org/apache/solr/store/blockcache/BlockCacheTest.java b/solr/core/src/test/org/apache/solr/store/blockcache/BlockCacheTest.java
deleted file mode 100644
index 9024337..0000000
--- a/solr/core/src/test/org/apache/solr/store/blockcache/BlockCacheTest.java
+++ /dev/null
@@ -1,110 +0,0 @@
-package org.apache.solr.store.blockcache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Arrays;
-import java.util.Random;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.lucene.util.LuceneTestCase;
-import org.junit.Test;
-
-public class BlockCacheTest extends LuceneTestCase {
-  @Test
-  public void testBlockCache() {
-    int blocksInTest = 2000000;
-    int blockSize = 1024;
-    
-    int slabSize = blockSize * 4096;
-    long totalMemory = 2 * slabSize;
-    
-    BlockCache blockCache = new BlockCache(new Metrics(), true, totalMemory, slabSize, blockSize);
-    byte[] buffer = new byte[1024];
-    Random random = random();
-    byte[] newData = new byte[blockSize];
-    AtomicLong hitsInCache = new AtomicLong();
-    AtomicLong missesInCache = new AtomicLong();
-    long storeTime = 0;
-    long fetchTime = 0;
-    int passes = 10000;
-
-    BlockCacheKey blockCacheKey = new BlockCacheKey();
-
-    for (int j = 0; j < passes; j++) {
-      long block = random.nextInt(blocksInTest);
-      int file = 0;
-      blockCacheKey.setBlock(block);
-      blockCacheKey.setFile(file);
-      blockCacheKey.setPath("/");
-
-      if (blockCache.fetch(blockCacheKey, buffer)) {
-        hitsInCache.incrementAndGet();
-      } else {
-        missesInCache.incrementAndGet();
-      }
-
-      byte[] testData = testData(random, blockSize, newData);
-      long t1 = System.nanoTime();
-      blockCache.store(blockCacheKey, 0, testData, 0, blockSize);
-      storeTime += (System.nanoTime() - t1);
-
-      long t3 = System.nanoTime();
-      if (blockCache.fetch(blockCacheKey, buffer)) {
-        fetchTime += (System.nanoTime() - t3);
-        assertTrue(Arrays.equals(testData, buffer));
-      }
-    }
-    System.out.println("Cache Hits    = " + hitsInCache.get());
-    System.out.println("Cache Misses  = " + missesInCache.get());
-    System.out.println("Store         = " + (storeTime / (double) passes) / 1000000.0);
-    System.out.println("Fetch         = " + (fetchTime / (double) passes) / 1000000.0);
-    System.out.println("# of Elements = " + blockCache.getSize());
-  }
-
-  /**
-   * Verify checking of buffer size limits against the cached block size.
-   */
-  @Test
-  public void testLongBuffer() {
-    Random random = random();
-    int blockSize = BlockCache._32K;
-    int slabSize = blockSize * 1024;
-    long totalMemory = 2 * slabSize;
-
-    BlockCache blockCache = new BlockCache(new Metrics(), true, totalMemory, slabSize);
-    BlockCacheKey blockCacheKey = new BlockCacheKey();
-    blockCacheKey.setBlock(0);
-    blockCacheKey.setFile(0);
-    blockCacheKey.setPath("/");
-    byte[] newData = new byte[blockSize*3];
-    byte[] testData = testData(random, blockSize, newData);
-
-    assertTrue(blockCache.store(blockCacheKey, 0, testData, 0, blockSize));
-    assertTrue(blockCache.store(blockCacheKey, 0, testData, blockSize, blockSize));
-    assertTrue(blockCache.store(blockCacheKey, 0, testData, blockSize*2, blockSize));
-
-    assertTrue(blockCache.store(blockCacheKey, 1, testData, 0, blockSize - 1));
-    assertTrue(blockCache.store(blockCacheKey, 1, testData, blockSize, blockSize - 1));
-    assertTrue(blockCache.store(blockCacheKey, 1, testData, blockSize*2, blockSize - 1));
-  }
-
-  private static byte[] testData(Random random, int size, byte[] buf) {
-    random.nextBytes(buf);
-    return buf;
-  }
-}
diff --git a/solr/core/src/test/org/apache/solr/store/blockcache/BlockDirectoryTest.java b/solr/core/src/test/org/apache/solr/store/blockcache/BlockDirectoryTest.java
deleted file mode 100644
index fb043f0..0000000
--- a/solr/core/src/test/org/apache/solr/store/blockcache/BlockDirectoryTest.java
+++ /dev/null
@@ -1,273 +0,0 @@
-package org.apache.solr.store.blockcache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.io.IOException;
-import java.util.Map;
-import java.util.Random;
-
-import com.github.benmanes.caffeine.cache.Caffeine;
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.MergeInfo;
-import org.apache.lucene.util.IOUtils;
-import org.apache.solr.SolrTestCaseJ4;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-public class BlockDirectoryTest extends SolrTestCaseJ4 {
-
-  private class MapperCache implements Cache {
-    public Map<String, byte[]> map = Caffeine.newBuilder()
-        .maximumSize(8)
-        .<String, byte[]>build()
-        .asMap();
-
-    @Override
-    public void update(String name, long blockId, int blockOffset, byte[] buffer, int offset, int length) {
-      byte[] cached = map.get(name + blockId);
-      if (cached != null) {
-        int newlen = Math.max(cached.length, blockOffset + length);
-        byte[] b = new byte[newlen];
-        System.arraycopy(cached, 0, b, 0, cached.length);
-        System.arraycopy(buffer, offset, b, blockOffset, length);
-        cached = b;
-      } else {
-        cached = copy(blockOffset, buffer, offset, length);
-      }
-      map.put(name + blockId, cached);
-    }
-
-    private byte[] copy(int blockOffset, byte[] buffer, int offset, int length) {
-      byte[] b = new byte[length + blockOffset];
-      System.arraycopy(buffer, offset, b, blockOffset, length);
-      return b;
-    }
-
-    @Override
-    public boolean fetch(String name, long blockId, int blockOffset, byte[] b, int off, int lengthToReadInBlock) {
-      // return false;
-      byte[] data = map.get(name + blockId);
-      if (data == null) {
-        return false;
-      }
-      System.arraycopy(data, blockOffset, b, off, lengthToReadInBlock);
-      return true;
-    }
-
-    @Override
-    public void delete(String name) {
-
-    }
-
-    @Override
-    public long size() {
-      return map.size();
-    }
-
-    @Override
-    public void renameCacheFile(String source, String dest) {
-    }
-
-    @Override
-    public void releaseResources() {}
-  }
-
-  private static final int MAX_NUMBER_OF_WRITES = 10000;
-  private static final int MIN_FILE_SIZE = 100;
-  private static final int MAX_FILE_SIZE = 100000;
-  private static final int MIN_BUFFER_SIZE = 1;
-  private static final int MAX_BUFFER_SIZE = 12000;
-  private static final int MAX_NUMBER_OF_READS = 20000;
-  private BlockDirectory directory;
-  private File file;
-  private Random random;
-  private MapperCache mapperCache;
-
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-    file = createTempDir().toFile();
-    FSDirectory dir = FSDirectory.open(new File(file, "base").toPath());
-    mapperCache = new MapperCache();
-    directory = new BlockDirectory("test", dir, mapperCache, null, true, true);
-    random = random();
-  }
-  
-  @After
-  public void tearDown() throws Exception {
-    super.tearDown();
-    directory.close();
-  }
-
-  @Test
-  public void testEOF() throws IOException {
-    Directory fsDir = FSDirectory.open(new File(file, "normal").toPath());
-    String name = "test.eof";
-    createFile(name, fsDir, directory);
-    long fsLength = fsDir.fileLength(name);
-    long hdfsLength = directory.fileLength(name);
-    assertEquals(fsLength, hdfsLength);
-    testEof(name, fsDir, fsLength);
-    testEof(name, directory, hdfsLength);
-    fsDir.close();
-  }
-
-  private void testEof(String name, Directory directory, long length) throws IOException {
-    IndexInput input = directory.openInput(name, new IOContext());
-    try {
-    input.seek(length);
-      try {
-        input.readByte();
-        fail("should throw eof");
-      } catch (IOException e) {
-      }
-    } finally {
-      input.close();
-    }
-  }
-
-  @Test
-  public void testRandomAccessWrites() throws IOException {
-    long t1 = System.nanoTime();
-
-    int i = 0;
-    try {
-      for (; i < 10; i++) {
-        Directory fsDir = FSDirectory.open(new File(file, "normal").toPath());
-        String name = getName();
-        createFile(name, fsDir, directory);
-        assertInputsEquals(name, fsDir, directory);
-      }
-    } catch (Exception e) {
-      e.printStackTrace();
-      fail("Test failed on pass [" + i + "]");
-    }
-    long t2 = System.nanoTime();
-    System.out.println("Total time is " + ((t2 - t1)/1000000) + "ms");
-  }
-
-  @Test
-  public void testRandomAccessWritesLargeCache() throws IOException {
-    mapperCache.map = Caffeine.newBuilder()
-        .maximumSize(10_000)
-        .<String, byte[]>build()
-        .asMap();
-    testRandomAccessWrites();
-  }
-
-  private void assertInputsEquals(String name, Directory fsDir, Directory hdfs) throws IOException {
-    int reads = random.nextInt(MAX_NUMBER_OF_READS);
-    IndexInput fsInput = fsDir.openInput(name, new IOContext());
-    IndexInput hdfsInput = hdfs.openInput(name, new IOContext());
-    assertEquals(fsInput.length(), hdfsInput.length());
-    int fileLength = (int) fsInput.length();
-    for (int i = 0; i < reads; i++) {
-      int rnd;
-      if (fileLength == 0) {
-        rnd = 0;
-      } else {
-        rnd = random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE, fileLength));
-      }
-
-      byte[] fsBuf = new byte[rnd + MIN_BUFFER_SIZE];
-      byte[] hdfsBuf = new byte[fsBuf.length];
-      int offset = random.nextInt(fsBuf.length);
-      int length = random.nextInt(fsBuf.length - offset);
-      
-      int pos;
-      if (fileLength == 0) {
-        pos = 0;
-      } else {
-        pos = random.nextInt(fileLength - length);
-      }
-    
-      fsInput.seek(pos);
-      fsInput.readBytes(fsBuf, offset, length);
-      hdfsInput.seek(pos);
-      hdfsInput.readBytes(hdfsBuf, offset, length);
-      for (int f = offset; f < length; f++) {
-        if (fsBuf[f] != hdfsBuf[f]) {
-          fail("read [" + i + "]");
-        }
-      }
-    }
-    fsInput.close();
-    hdfsInput.close();
-  }
-
-  private void createFile(String name, Directory fsDir, Directory hdfs) throws IOException {
-    int writes = random.nextInt(MAX_NUMBER_OF_WRITES);
-    int fileLength = random.nextInt(MAX_FILE_SIZE - MIN_FILE_SIZE) + MIN_FILE_SIZE;
-    IndexOutput fsOutput = fsDir.createOutput(name, IOContext.DEFAULT);
-    IndexOutput hdfsOutput = hdfs.createOutput(name, IOContext.DEFAULT);
-    for (int i = 0; i < writes; i++) {
-      byte[] buf = new byte[random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE, fileLength)) + MIN_BUFFER_SIZE];
-      random.nextBytes(buf);
-      int offset = random.nextInt(buf.length);
-      int length = random.nextInt(buf.length - offset);
-      fsOutput.writeBytes(buf, offset, length);
-      hdfsOutput.writeBytes(buf, offset, length);
-    }
-    fsOutput.close();
-    hdfsOutput.close();
-  }
-
-  private String getName() {
-    return Long.toString(Math.abs(random.nextLong()));
-  }
-
-  public static void rm(File file) {
-    try {
-      IOUtils.rm(file.toPath());
-    } catch (Throwable ignored) {
-      // TODO: should this class care if a file couldnt be deleted?
-      // this just emulates previous behavior, where only SecurityException would be handled.
-    }
-  }
-
-  /**
-   * Verify the configuration options for the block cache are handled
-   * appropriately.
-   */
-  @Test
-  public void ensureCacheConfigurable() throws Exception {
-    IOContext mergeContext = new IOContext(new MergeInfo(1,1,false,1));
-
-    BlockDirectory d = directory;
-    assertTrue(d.useReadCache("", IOContext.DEFAULT));
-    assertTrue(d.useWriteCache("", IOContext.DEFAULT));
-    assertFalse(d.useWriteCache("", mergeContext));
-
-    d = new BlockDirectory("test", directory, mapperCache, null, true, false);
-    assertTrue(d.useReadCache("", IOContext.DEFAULT));
-    assertFalse(d.useWriteCache("", IOContext.DEFAULT));
-    assertFalse(d.useWriteCache("", mergeContext));
-
-    d = new BlockDirectory("test", directory, mapperCache, null, false, true);
-    assertFalse(d.useReadCache("", IOContext.DEFAULT));
-    assertTrue(d.useWriteCache("", IOContext.DEFAULT));
-    assertFalse(d.useWriteCache("", mergeContext));
-  }
-}
diff --git a/solr/core/src/test/org/apache/solr/store/blockcache/BufferStoreTest.java b/solr/core/src/test/org/apache/solr/store/blockcache/BufferStoreTest.java
deleted file mode 100644
index 6751414..0000000
--- a/solr/core/src/test/org/apache/solr/store/blockcache/BufferStoreTest.java
+++ /dev/null
@@ -1,93 +0,0 @@
-package org.apache.solr.store.blockcache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.math.BigDecimal;
-
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.solr.common.util.NamedList;
-import org.junit.Before;
-import org.junit.Test;
-
-public class BufferStoreTest extends LuceneTestCase {
-  private final static int blockSize = 1024;
-
-  private Metrics metrics;
-
-  private Store store;
-
-  @Before
-  public void setup() {
-    metrics = new Metrics();
-    BufferStore.initNewBuffer(blockSize, blockSize, metrics);
-    store = BufferStore.instance(blockSize);
-  }
-
-  @Test
-  public void testBufferTakePut() {
-    byte[] b1 = store.takeBuffer(blockSize);
-
-    assertGaugeMetricsChanged(false, false);
-
-    byte[] b2 = store.takeBuffer(blockSize);
-    byte[] b3 = store.takeBuffer(blockSize);
-
-    assertRawMetricCounts(2, 0);
-    assertGaugeMetricsChanged(true, false);
-
-    store.putBuffer(b1);
-
-    assertGaugeMetricsChanged(false, false);
-
-    store.putBuffer(b2);
-    store.putBuffer(b3);
-
-    assertRawMetricCounts(0, 2);
-    assertGaugeMetricsChanged(false, true);
-  }
-
-  private void assertRawMetricCounts(int allocated, int lost) {
-    assertEquals("Buffer allocation count is wrong.", allocated,
-        metrics.shardBuffercacheAllocate.get());
-    assertEquals("Lost buffer count is wrong", lost,
-        metrics.shardBuffercacheLost.get());
-  }
-
-  /**
-   * Stateful method to verify whether the amount of buffers allocated and lost
-   * since the last call has changed.
-   *
-   * @param allocated
-   *          whether buffers should have been allocated since the last call
-   * @param lost
-   *          whether buffers should have been lost since the last call
-   */
-  private void assertGaugeMetricsChanged(boolean allocated, boolean lost) {
-    NamedList<Number> stats = metrics.getStatistics();
-
-    assertEquals("Buffer allocation metric not updating correctly.",
-        allocated, isMetricPositive(stats, "buffercache.allocations"));
-    assertEquals("Buffer lost metric not updating correctly.",
-        lost, isMetricPositive(stats, "buffercache.lost"));
-  }
-
-  private boolean isMetricPositive(NamedList<Number> stats, String metric) {
-    return new BigDecimal(stats.get(metric).toString()).compareTo(BigDecimal.ZERO) > 0;
-  }
-
-}
diff --git a/solr/core/src/test/org/apache/solr/store/hdfs/HdfsDirectoryTest.java b/solr/core/src/test/org/apache/solr/store/hdfs/HdfsDirectoryTest.java
deleted file mode 100644
index 153d324..0000000
--- a/solr/core/src/test/org/apache/solr/store/hdfs/HdfsDirectoryTest.java
+++ /dev/null
@@ -1,237 +0,0 @@
-package org.apache.solr.store.hdfs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashSet;
-import java.util.Random;
-import java.util.Set;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.NoLockFactory;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.solr.SolrTestCaseJ4;
-import org.apache.solr.cloud.hdfs.HdfsTestUtil;
-import org.apache.solr.util.BadHdfsThreadsFilter;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import com.carrotsearch.randomizedtesting.annotations.ThreadLeakFilters;
-
-@ThreadLeakFilters(defaultFilters = true, filters = {
-    BadHdfsThreadsFilter.class // hdfs currently leaks thread(s)
-})
-public class HdfsDirectoryTest extends SolrTestCaseJ4 {
-  
-  private static final int MAX_NUMBER_OF_WRITES = 10000;
-  private static final int MIN_FILE_SIZE = 100;
-  private static final int MAX_FILE_SIZE = 100000;
-  private static final int MIN_BUFFER_SIZE = 1;
-  private static final int MAX_BUFFER_SIZE = 5000;
-  private static final int MAX_NUMBER_OF_READS = 10000;
-  private static MiniDFSCluster dfsCluster;
-  private HdfsDirectory directory;
-  private Random random;
-
-  @BeforeClass
-  public static void beforeClass() throws Exception {
-    dfsCluster = HdfsTestUtil.setupClass(createTempDir().toFile().getAbsolutePath());
-  }
-  
-  @AfterClass
-  public static void afterClass() throws Exception {
-    HdfsTestUtil.teardownClass(dfsCluster);
-    dfsCluster = null;
-  }
-  
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-    
-    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);
-    conf.set("dfs.permissions.enabled", "false");
-    
-    directory = new HdfsDirectory(new Path(HdfsTestUtil.getURI(dfsCluster) + createTempDir().toFile().getAbsolutePath() + "/hdfs"), NoLockFactory.INSTANCE, conf);
-    
-    random = random();
-  }
-  
-  @After
-  public void tearDown() throws Exception {
-    super.tearDown();
-  }
-  
-  @Test
-  public void testWritingAndReadingAFile() throws IOException {
-    String[] listAll = directory.listAll();
-    for (String file : listAll) {
-      directory.deleteFile(file);
-    }
-    
-    IndexOutput output = directory.createOutput("testing.test", new IOContext());
-    output.writeInt(12345);
-    output.close();
-
-    IndexInput input = directory.openInput("testing.test", new IOContext());
-    assertEquals(12345, input.readInt());
-    input.close();
-
-    listAll = directory.listAll();
-    assertEquals(1, listAll.length);
-    assertEquals("testing.test", listAll[0]);
-
-    assertEquals(4, directory.fileLength("testing.test"));
-
-    IndexInput input1 = directory.openInput("testing.test", new IOContext());
-
-    IndexInput input2 = (IndexInput) input1.clone();
-    assertEquals(12345, input2.readInt());
-    input2.close();
-
-    assertEquals(12345, input1.readInt());
-    input1.close();
-
-    assertFalse(slowFileExists(directory, "testing.test.other"));
-    assertTrue(slowFileExists(directory, "testing.test"));
-    directory.deleteFile("testing.test");
-    assertFalse(slowFileExists(directory, "testing.test"));
-  }
-  
-  public void testRename() throws IOException {
-    String[] listAll = directory.listAll();
-    for (String file : listAll) {
-      directory.deleteFile(file);
-    }
-    
-    IndexOutput output = directory.createOutput("testing.test", new IOContext());
-    output.writeInt(12345);
-    output.close();
-    directory.renameFile("testing.test", "testing.test.renamed");
-    assertFalse(slowFileExists(directory, "testing.test"));
-    assertTrue(slowFileExists(directory, "testing.test.renamed"));
-    IndexInput input = directory.openInput("testing.test.renamed", new IOContext());
-    assertEquals(12345, input.readInt());
-    assertEquals(input.getFilePointer(), input.length());
-    input.close();
-    directory.deleteFile("testing.test.renamed");
-    assertFalse(slowFileExists(directory, "testing.test.renamed"));
-  }
-  
-  @Test
-  public void testEOF() throws IOException {
-    Directory fsDir = new RAMDirectory();
-    String name = "test.eof";
-    createFile(name, fsDir, directory);
-    long fsLength = fsDir.fileLength(name);
-    long hdfsLength = directory.fileLength(name);
-    assertEquals(fsLength, hdfsLength);
-    testEof(name,fsDir,fsLength);
-    testEof(name,directory,hdfsLength);
-  }
-
-  private void testEof(String name, Directory directory, long length) throws IOException {
-    IndexInput input = directory.openInput(name, new IOContext());
-    input.seek(length);
-    try {
-      input.readByte();
-      fail("should throw eof");
-    } catch (IOException e) {
-    }
-  }
-
-  @Test
-  public void testRandomAccessWrites() throws IOException {
-    int i = 0;
-    try {
-      Set<String> names = new HashSet<>();
-      for (; i< 10; i++) {
-        Directory fsDir = new RAMDirectory();
-        String name = getName();
-        System.out.println("Working on pass [" + i  +"] contains [" + names.contains(name) + "]");
-        names.add(name);
-        createFile(name,fsDir,directory);
-        assertInputsEquals(name,fsDir,directory);
-        fsDir.close();
-      }
-    } catch (Exception e) {
-      e.printStackTrace();
-      fail("Test failed on pass [" + i + "]");
-    }
-  }
-
-  private void assertInputsEquals(String name, Directory fsDir, HdfsDirectory hdfs) throws IOException {
-    int reads = random.nextInt(MAX_NUMBER_OF_READS);
-    IndexInput fsInput = fsDir.openInput(name,new IOContext());
-    IndexInput hdfsInput = hdfs.openInput(name,new IOContext());
-    assertEquals(fsInput.length(), hdfsInput.length());
-    int fileLength = (int) fsInput.length();
-    for (int i = 0; i < reads; i++) {
-      int nextInt = Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE,fileLength);
-      byte[] fsBuf = new byte[random.nextInt(nextInt > 0 ? nextInt : 1) + MIN_BUFFER_SIZE];
-      byte[] hdfsBuf = new byte[fsBuf.length];
-      int offset = random.nextInt(fsBuf.length);
-      
-      nextInt = fsBuf.length - offset;
-      int length = random.nextInt(nextInt > 0 ? nextInt : 1);
-      nextInt = fileLength - length;
-      int pos = random.nextInt(nextInt > 0 ? nextInt : 1);
-      fsInput.seek(pos);
-      fsInput.readBytes(fsBuf, offset, length);
-      hdfsInput.seek(pos);
-      hdfsInput.readBytes(hdfsBuf, offset, length);
-      for (int f = offset; f < length; f++) {
-        if (fsBuf[f] != hdfsBuf[f]) {
-          fail();
-        }
-      }
-    }
-    fsInput.close();
-    hdfsInput.close();
-  }
-
-  private void createFile(String name, Directory fsDir, HdfsDirectory hdfs) throws IOException {
-    int writes = random.nextInt(MAX_NUMBER_OF_WRITES);
-    int fileLength = random.nextInt(MAX_FILE_SIZE - MIN_FILE_SIZE) + MIN_FILE_SIZE;
-    IndexOutput fsOutput = fsDir.createOutput(name, new IOContext());
-    IndexOutput hdfsOutput = hdfs.createOutput(name, new IOContext());
-    for (int i = 0; i < writes; i++) {
-      byte[] buf = new byte[random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE,fileLength)) + MIN_BUFFER_SIZE];
-      random.nextBytes(buf);
-      int offset = random.nextInt(buf.length);
-      int length = random.nextInt(buf.length - offset);
-      fsOutput.writeBytes(buf, offset, length);
-      hdfsOutput.writeBytes(buf, offset, length);
-    }
-    fsOutput.close();
-    hdfsOutput.close();
-  }
-
-  private String getName() {
-    return Long.toString(Math.abs(random.nextLong()));
-  }
-
-}
diff --git a/solr/core/src/test/org/apache/solr/store/hdfs/HdfsLockFactoryTest.java b/solr/core/src/test/org/apache/solr/store/hdfs/HdfsLockFactoryTest.java
deleted file mode 100644
index 6e3a1b6..0000000
--- a/solr/core/src/test/org/apache/solr/store/hdfs/HdfsLockFactoryTest.java
+++ /dev/null
@@ -1,82 +0,0 @@
-package org.apache.solr.store.hdfs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.lucene.store.Lock;
-import org.apache.lucene.store.LockObtainFailedException;
-import org.apache.solr.SolrTestCaseJ4;
-import org.apache.solr.cloud.hdfs.HdfsTestUtil;
-import org.apache.solr.util.BadHdfsThreadsFilter;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import com.carrotsearch.randomizedtesting.annotations.ThreadLeakFilters;
-
-@ThreadLeakFilters(defaultFilters = true, filters = {
-    BadHdfsThreadsFilter.class // hdfs currently leaks thread(s)
-})
-public class HdfsLockFactoryTest extends SolrTestCaseJ4 {
-  
-  private static MiniDFSCluster dfsCluster;
-
-  @BeforeClass
-  public static void beforeClass() throws Exception {
-    dfsCluster = HdfsTestUtil.setupClass(createTempDir().toFile().getAbsolutePath());
-  }
-
-  @AfterClass
-  public static void afterClass() throws Exception {
-    HdfsTestUtil.teardownClass(dfsCluster);
-    dfsCluster = null;
-  }
-  
-  @Test
-  public void testBasic() throws IOException {
-    String uri = HdfsTestUtil.getURI(dfsCluster);
-    Path lockPath = new Path(uri, "/basedir/lock");
-    Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);
-    HdfsDirectory dir = new HdfsDirectory(lockPath, conf);
-    
-    try (Lock lock = dir.obtainLock("testlock")) {
-      assert lock != null;
-      try (Lock lock2 = dir.obtainLock("testlock")) {
-        assert lock2 != null;
-        fail("Locking should fail");
-      } catch (LockObtainFailedException lofe) {
-        // pass
-      }
-    }
-    // now repeat after close()
-    try (Lock lock = dir.obtainLock("testlock")) {
-      assert lock != null;
-      try (Lock lock2 = dir.obtainLock("testlock")) {
-        assert lock2 != null;
-        fail("Locking should fail");
-      } catch (LockObtainFailedException lofe) {
-        // pass
-      }
-    }
-    dir.close();
-  }
-}
diff --git a/solr/core/src/test/org/apache/solr/update/TestHdfsUpdateLog.java b/solr/core/src/test/org/apache/solr/update/TestHdfsUpdateLog.java
index 5021d84..27bf133 100644
--- a/solr/core/src/test/org/apache/solr/update/TestHdfsUpdateLog.java
+++ b/solr/core/src/test/org/apache/solr/update/TestHdfsUpdateLog.java
@@ -29,7 +29,7 @@ import org.apache.solr.SolrTestCaseJ4.SuppressObjectReleaseTracker;
 import org.apache.solr.cloud.hdfs.HdfsTestUtil;
 import org.apache.solr.common.util.IOUtils;
 import org.apache.solr.request.SolrQueryRequest;
-import org.apache.solr.util.BadHdfsThreadsFilter;
+import org.apache.lucene.util.BadHdfsThreadsFilter;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
diff --git a/solr/test-framework/src/java/org/apache/solr/util/BadHdfsThreadsFilter.java b/solr/test-framework/src/java/org/apache/solr/util/BadHdfsThreadsFilter.java
deleted file mode 100644
index f1eea7a..0000000
--- a/solr/test-framework/src/java/org/apache/solr/util/BadHdfsThreadsFilter.java
+++ /dev/null
@@ -1,36 +0,0 @@
-package org.apache.solr.util;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import com.carrotsearch.randomizedtesting.ThreadFilter;
-
-public class BadHdfsThreadsFilter implements ThreadFilter {
-
-  @Override
-  public boolean reject(Thread t) {
-    String name = t.getName();
-    if (name.startsWith("IPC Parameter Sending Thread ")) { // SOLR-5007
-      return true;
-    } else if (name.startsWith("org.apache.hadoop.hdfs.PeerCache")) { // SOLR-7288
-      return true;
-    } else if (name.startsWith("LeaseRenewer")) { // SOLR-7287
-      return true;
-    }
-    return false;
-  }
-}
-- 
2.3.6

