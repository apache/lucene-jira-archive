diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene50/Lucene50Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene50/Lucene50Codec.java
index 08ea3e9..001439c 100644
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene50/Lucene50Codec.java
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene50/Lucene50Codec.java
@@ -21,12 +21,12 @@ import java.util.Objects;
 
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.CompoundFormat;
-import org.apache.lucene.codecs.PointFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PointsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
@@ -154,8 +154,8 @@ public class Lucene50Codec extends Codec {
   }
 
   @Override
-  public final PointFormat pointFormat() {
-    return PointFormat.EMPTY;
+  public final PointsFormat pointsFormat() {
+    return PointsFormat.EMPTY;
   }
 
   private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene50");
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene53/Lucene53Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene53/Lucene53Codec.java
index ab90306..7630194 100644
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene53/Lucene53Codec.java
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene53/Lucene53Codec.java
@@ -21,12 +21,12 @@ import java.util.Objects;
 
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.CompoundFormat;
-import org.apache.lucene.codecs.PointFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PointsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
@@ -35,8 +35,8 @@ import org.apache.lucene.codecs.lucene50.Lucene50CompoundFormat;
 import org.apache.lucene.codecs.lucene50.Lucene50FieldInfosFormat;
 import org.apache.lucene.codecs.lucene50.Lucene50LiveDocsFormat;
 import org.apache.lucene.codecs.lucene50.Lucene50SegmentInfoFormat;
-import org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat;
 import org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat.Mode;
+import org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat;
 import org.apache.lucene.codecs.lucene50.Lucene50TermVectorsFormat;
 import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
 import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
@@ -160,8 +160,8 @@ public class Lucene53Codec extends Codec {
   }
 
   @Override
-  public final PointFormat pointFormat() {
-    return PointFormat.EMPTY;
+  public final PointsFormat pointsFormat() {
+    return PointsFormat.EMPTY;
   }
 
   private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene50");
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene54/Lucene54Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene54/Lucene54Codec.java
index 62ef89c..2dde0cf 100644
--- a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene54/Lucene54Codec.java
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene54/Lucene54Codec.java
@@ -21,12 +21,12 @@ import java.util.Objects;
 
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.CompoundFormat;
-import org.apache.lucene.codecs.PointFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PointsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
@@ -35,8 +35,8 @@ import org.apache.lucene.codecs.lucene50.Lucene50CompoundFormat;
 import org.apache.lucene.codecs.lucene50.Lucene50FieldInfosFormat;
 import org.apache.lucene.codecs.lucene50.Lucene50LiveDocsFormat;
 import org.apache.lucene.codecs.lucene50.Lucene50SegmentInfoFormat;
-import org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat;
 import org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat.Mode;
+import org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat;
 import org.apache.lucene.codecs.lucene50.Lucene50TermVectorsFormat;
 import org.apache.lucene.codecs.lucene53.Lucene53NormsFormat;
 import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
@@ -160,8 +160,8 @@ public class Lucene54Codec extends Codec {
   }
 
   @Override
-  public final PointFormat pointFormat() {
-    return PointFormat.EMPTY;
+  public final PointsFormat pointsFormat() {
+    return PointsFormat.EMPTY;
   }
 
   private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene50");
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDReader.java
index 09c40ec..6e2e1ac 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDReader.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDReader.java
@@ -27,9 +27,9 @@ import org.apache.lucene.util.BytesRefBuilder;
 import org.apache.lucene.util.StringHelper;
 import org.apache.lucene.util.bkd.BKDReader;
 
-import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.BLOCK_COUNT;
-import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.BLOCK_DOC_ID;
-import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.BLOCK_VALUE;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointsWriter.BLOCK_COUNT;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointsWriter.BLOCK_DOC_ID;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointsWriter.BLOCK_VALUE;
 
 class SimpleTextBKDReader extends BKDReader {
 
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
index 4cec13d..109fec9 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
@@ -19,11 +19,11 @@ package org.apache.lucene.codecs.simpletext;
 
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.CompoundFormat;
-import org.apache.lucene.codecs.PointFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PointsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
@@ -45,7 +45,7 @@ public final class SimpleTextCodec extends Codec {
   private final LiveDocsFormat liveDocs = new SimpleTextLiveDocsFormat();
   private final DocValuesFormat dvFormat = new SimpleTextDocValuesFormat();
   private final CompoundFormat compoundFormat = new SimpleTextCompoundFormat();
-  private final PointFormat pointFormat = new SimpleTextPointFormat();
+  private final PointsFormat pointsFormat = new SimpleTextPointsFormat();
   
   public SimpleTextCodec() {
     super("SimpleText");
@@ -97,7 +97,7 @@ public final class SimpleTextCodec extends Codec {
   }
 
   @Override
-  public PointFormat pointFormat() {
-    return pointFormat;
+  public PointsFormat pointsFormat() {
+    return pointsFormat;
   }
 }
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointFormat.java
deleted file mode 100644
index 87b8310..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointFormat.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.codecs.simpletext;
-
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.PointFormat;
-import org.apache.lucene.codecs.PointReader;
-import org.apache.lucene.codecs.PointWriter;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-/** For debugging, curiosity, transparency only!!  Do not
- *  use this codec in production.
- *
- *  <p>This codec stores all dimensional data in a single
- *  human-readable text file (_N.dim).  You can view this in
- *  any text editor, and even edit it to alter your index.
- *
- *  @lucene.experimental */
-public final class SimpleTextPointFormat extends PointFormat {
-  
-  @Override
-  public PointWriter fieldsWriter(SegmentWriteState state) throws IOException {
-    return new SimpleTextPointWriter(state);
-  }
-
-  @Override
-  public PointReader fieldsReader(SegmentReadState state) throws IOException {
-    return new SimpleTextPointReader(state);
-  }
-
-  /** Extension of points data file */
-  static final String POINT_EXTENSION = "dim";
-
-  /** Extension of points index file */
-  static final String POINT_INDEX_EXTENSION = "dii";
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointReader.java
deleted file mode 100644
index 05afd93..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointReader.java
+++ /dev/null
@@ -1,302 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.codecs.simpletext;
-
-
-import java.io.IOException;
-import java.nio.charset.StandardCharsets;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.codecs.PointReader;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.store.BufferedChecksumIndexInput;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.bkd.BKDReader;
-
-import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.BLOCK_FP;
-import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.BYTES_PER_DIM;
-import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.DOC_COUNT;
-import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.FIELD_COUNT;
-import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.FIELD_FP;
-import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.FIELD_FP_NAME;
-import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.INDEX_COUNT;
-import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.MAX_LEAF_POINTS;
-import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.MAX_VALUE;
-import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.MIN_VALUE;
-import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.NUM_DIMS;
-import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.POINT_COUNT;
-import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.SPLIT_COUNT;
-import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.SPLIT_DIM;
-import static org.apache.lucene.codecs.simpletext.SimpleTextPointWriter.SPLIT_VALUE;
-
-class SimpleTextPointReader extends PointReader {
-
-  private final IndexInput dataIn;
-  final SegmentReadState readState;
-  final Map<String,BKDReader> readers = new HashMap<>();
-  final BytesRefBuilder scratch = new BytesRefBuilder();
-
-  public SimpleTextPointReader(SegmentReadState readState) throws IOException {
-    // Initialize readers now:
-
-    // Read index:
-    Map<String,Long> fieldToFileOffset = new HashMap<>();
-
-    String indexFileName = IndexFileNames.segmentFileName(readState.segmentInfo.name, readState.segmentSuffix, SimpleTextPointFormat.POINT_INDEX_EXTENSION);
-    try (ChecksumIndexInput in = readState.directory.openChecksumInput(indexFileName, IOContext.DEFAULT)) {
-      readLine(in);
-      int count = parseInt(FIELD_COUNT);
-      for(int i=0;i<count;i++) {
-        readLine(in);
-        String fieldName = stripPrefix(FIELD_FP_NAME);
-        readLine(in);
-        long fp = parseLong(FIELD_FP);
-        fieldToFileOffset.put(fieldName, fp);
-      }
-      SimpleTextUtil.checkFooter(in);
-    }
-
-    boolean success = false;
-    String fileName = IndexFileNames.segmentFileName(readState.segmentInfo.name, readState.segmentSuffix, SimpleTextPointFormat.POINT_EXTENSION);
-    dataIn = readState.directory.openInput(fileName, IOContext.DEFAULT);
-    try {
-      for(Map.Entry<String,Long> ent : fieldToFileOffset.entrySet()) {
-        readers.put(ent.getKey(), initReader(ent.getValue()));
-      }
-      success = true;
-    } finally {
-      if (success == false) {
-        IOUtils.closeWhileHandlingException(this);
-      }
-    }
-        
-    this.readState = readState;
-  }
-
-  private BKDReader initReader(long fp) throws IOException {
-    // NOTE: matches what writeIndex does in SimpleTextPointWriter
-    dataIn.seek(fp);
-    readLine(dataIn);
-    int numDims = parseInt(NUM_DIMS);
-
-    readLine(dataIn);
-    int bytesPerDim = parseInt(BYTES_PER_DIM);
-
-    readLine(dataIn);
-    int maxPointsInLeafNode = parseInt(MAX_LEAF_POINTS);
-
-    readLine(dataIn);
-    int count = parseInt(INDEX_COUNT);
-
-    readLine(dataIn);
-    assert startsWith(MIN_VALUE);
-    BytesRef minValue = SimpleTextUtil.fromBytesRefString(stripPrefix(MIN_VALUE));
-    assert minValue.length == numDims*bytesPerDim;
-
-    readLine(dataIn);
-    assert startsWith(MAX_VALUE);
-    BytesRef maxValue = SimpleTextUtil.fromBytesRefString(stripPrefix(MAX_VALUE));
-    assert maxValue.length == numDims*bytesPerDim;
-
-    readLine(dataIn);
-    assert startsWith(POINT_COUNT);
-    long pointCount = parseLong(POINT_COUNT);
-
-    readLine(dataIn);
-    assert startsWith(DOC_COUNT);
-    int docCount = parseInt(DOC_COUNT);
-    
-    long[] leafBlockFPs = new long[count];
-    for(int i=0;i<count;i++) {
-      readLine(dataIn);
-      leafBlockFPs[i] = parseLong(BLOCK_FP);
-    }
-    readLine(dataIn);
-    count = parseInt(SPLIT_COUNT);
-
-    byte[] splitPackedValues = new byte[count * (1 + bytesPerDim)];
-    for(int i=0;i<count;i++) {
-      readLine(dataIn);
-      splitPackedValues[(1 + bytesPerDim) * i] = (byte) parseInt(SPLIT_DIM);
-      readLine(dataIn);
-      assert startsWith(SPLIT_VALUE);
-      BytesRef br = SimpleTextUtil.fromBytesRefString(stripPrefix(SPLIT_VALUE));
-      assert br.length == bytesPerDim;
-      System.arraycopy(br.bytes, br.offset, splitPackedValues, (1 + bytesPerDim) * i + 1, bytesPerDim);
-    }
-
-    return new SimpleTextBKDReader(dataIn, numDims, maxPointsInLeafNode, bytesPerDim, leafBlockFPs, splitPackedValues, minValue.bytes, maxValue.bytes, pointCount, docCount);
-  }
-
-  private void readLine(IndexInput in) throws IOException {
-    SimpleTextUtil.readLine(in, scratch);
-  }
-
-  private boolean startsWith(BytesRef prefix) {
-    return StringHelper.startsWith(scratch.get(), prefix);
-  }
-
-  private int parseInt(BytesRef prefix) {
-    assert startsWith(prefix);
-    return Integer.parseInt(stripPrefix(prefix));
-  }
-
-  private long parseLong(BytesRef prefix) {
-    assert startsWith(prefix);
-    return Long.parseLong(stripPrefix(prefix));
-  }
-
-  private String stripPrefix(BytesRef prefix) {
-    return new String(scratch.bytes(), prefix.length, scratch.length() - prefix.length, StandardCharsets.UTF_8);
-  }
-
-  private BKDReader getBKDReader(String fieldName) {
-    FieldInfo fieldInfo = readState.fieldInfos.fieldInfo(fieldName);
-    if (fieldInfo == null) {
-      throw new IllegalArgumentException("field=\"" + fieldName + "\" is unrecognized");
-    }
-    if (fieldInfo.getPointDimensionCount() == 0) {
-      throw new IllegalArgumentException("field=\"" + fieldName + "\" did not index points");
-    }
-    return readers.get(fieldName);
-  }
-
-  /** Finds all documents and points matching the provided visitor */
-  @Override
-  public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index points in the past, but
-      // now all docs having this field were deleted in this segment:
-      return;
-    }
-    bkdReader.intersect(visitor);
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {
-    BytesRefBuilder scratch = new BytesRefBuilder();
-    IndexInput clone = dataIn.clone();
-    clone.seek(0);
-
-    // checksum is fixed-width encoded with 20 bytes, plus 1 byte for newline (the space is included in SimpleTextUtil.CHECKSUM):
-    long footerStartPos = dataIn.length() - (SimpleTextUtil.CHECKSUM.length + 21);
-    ChecksumIndexInput input = new BufferedChecksumIndexInput(clone);
-    while (true) {
-      SimpleTextUtil.readLine(input, scratch);
-      if (input.getFilePointer() >= footerStartPos) {
-        // Make sure we landed at precisely the right location:
-        if (input.getFilePointer() != footerStartPos) {
-          throw new CorruptIndexException("SimpleText failure: footer does not start at expected position current=" + input.getFilePointer() + " vs expected=" + footerStartPos, input);
-        }
-        SimpleTextUtil.checkFooter(input);
-        break;
-      }
-    }
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return 0L;
-  }
-
-  @Override
-  public void close() throws IOException {
-    dataIn.close();
-  }
-
-  @Override
-  public String toString() {
-    return "SimpleTextPointReader(segment=" + readState.segmentInfo.name + " maxDoc=" + readState.segmentInfo.maxDoc() + ")";
-  }
-
-  @Override
-  public byte[] getMinPackedValue(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index points in the past, but
-      // now all docs having this field were deleted in this segment:
-      return null;
-    }
-    return bkdReader.getMinPackedValue();
-  }
-
-  @Override
-  public byte[] getMaxPackedValue(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index points in the past, but
-      // now all docs having this field were deleted in this segment:
-      return null;
-    }
-    return bkdReader.getMaxPackedValue();
-  }
-
-  @Override
-  public int getNumDimensions(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index points in the past, but
-      // now all docs having this field were deleted in this segment:
-      return 0;
-    }
-    return bkdReader.getNumDimensions();
-  }
-
-  @Override
-  public int getBytesPerDimension(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index points in the past, but
-      // now all docs having this field were deleted in this segment:
-      return 0;
-    }
-    return bkdReader.getBytesPerDimension();
-  }
-
-  @Override
-  public long size(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index points in the past, but
-      // now all docs having this field were deleted in this segment:
-      return 0;
-    }
-    return bkdReader.getPointCount();
-  }
-
-  @Override
-  public int getDocCount(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index points in the past, but
-      // now all docs having this field were deleted in this segment:
-      return 0;
-    }
-    return bkdReader.getDocCount();
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointWriter.java
deleted file mode 100644
index 1554c0c..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointWriter.java
+++ /dev/null
@@ -1,244 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.codecs.simpletext;
-
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.codecs.PointReader;
-import org.apache.lucene.codecs.PointWriter;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.PointValues.IntersectVisitor;
-import org.apache.lucene.index.PointValues.Relation;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.bkd.BKDWriter;
-
-class SimpleTextPointWriter extends PointWriter {
-
-  final static BytesRef NUM_DIMS      = new BytesRef("num dims ");
-  final static BytesRef BYTES_PER_DIM = new BytesRef("bytes per dim ");
-  final static BytesRef MAX_LEAF_POINTS = new BytesRef("max leaf points ");
-  final static BytesRef INDEX_COUNT = new BytesRef("index count ");
-  final static BytesRef BLOCK_COUNT   = new BytesRef("block count ");
-  final static BytesRef BLOCK_DOC_ID  = new BytesRef("  doc ");
-  final static BytesRef BLOCK_FP      = new BytesRef("  block fp ");
-  final static BytesRef BLOCK_VALUE   = new BytesRef("  block value ");
-  final static BytesRef SPLIT_COUNT   = new BytesRef("split count ");
-  final static BytesRef SPLIT_DIM     = new BytesRef("  split dim ");
-  final static BytesRef SPLIT_VALUE   = new BytesRef("  split value ");
-  final static BytesRef FIELD_COUNT   = new BytesRef("field count ");
-  final static BytesRef FIELD_FP_NAME = new BytesRef("  field fp name ");
-  final static BytesRef FIELD_FP      = new BytesRef("  field fp ");
-  final static BytesRef MIN_VALUE     = new BytesRef("min value ");
-  final static BytesRef MAX_VALUE     = new BytesRef("max value ");
-  final static BytesRef POINT_COUNT   = new BytesRef("point count ");
-  final static BytesRef DOC_COUNT     = new BytesRef("doc count ");
-  final static BytesRef END           = new BytesRef("END");
-
-  private IndexOutput dataOut;
-  final BytesRefBuilder scratch = new BytesRefBuilder();
-  final SegmentWriteState writeState;
-  final Map<String,Long> indexFPs = new HashMap<>();
-
-  public SimpleTextPointWriter(SegmentWriteState writeState) throws IOException {
-    String fileName = IndexFileNames.segmentFileName(writeState.segmentInfo.name, writeState.segmentSuffix, SimpleTextPointFormat.POINT_EXTENSION);
-    dataOut = writeState.directory.createOutput(fileName, writeState.context);
-    this.writeState = writeState;
-  }
-
-  @Override
-  public void writeField(FieldInfo fieldInfo, PointReader values) throws IOException {
-
-    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:
-    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),
-                                          writeState.directory,
-                                          writeState.segmentInfo.name,
-                                          fieldInfo.getPointDimensionCount(),
-                                          fieldInfo.getPointNumBytes(),
-                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,
-                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP) {
-
-        @Override
-        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {
-          write(out, NUM_DIMS);
-          writeInt(out, numDims);
-          newline(out);
-
-          write(out, BYTES_PER_DIM);
-          writeInt(out, bytesPerDim);
-          newline(out);
-
-          write(out, MAX_LEAF_POINTS);
-          writeInt(out, maxPointsInLeafNode);
-          newline(out);
-
-          write(out, INDEX_COUNT);
-          writeInt(out, leafBlockFPs.length);
-          newline(out);
-
-          write(out, MIN_VALUE);
-          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);
-          write(out, br.toString());
-          newline(out);
-
-          write(out, MAX_VALUE);
-          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);
-          write(out, br.toString());
-          newline(out);
-
-          write(out, POINT_COUNT);
-          writeLong(out, pointCount);
-          newline(out);
-
-          write(out, DOC_COUNT);
-          writeInt(out, docsSeen.cardinality());
-          newline(out);
-
-          for(int i=0;i<leafBlockFPs.length;i++) {
-            write(out, BLOCK_FP);
-            writeLong(out, leafBlockFPs[i]);
-            newline(out);
-          }
-
-          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;
-          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());
-          assert count == leafBlockFPs.length;
-
-          write(out, SPLIT_COUNT);
-          writeInt(out, count);
-          newline(out);
-
-          for(int i=0;i<count;i++) {
-            write(out, SPLIT_DIM);
-            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);
-            newline(out);
-            write(out, SPLIT_VALUE);
-            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());
-            write(out, br.toString());
-            newline(out);
-          }
-        }
-
-        @Override
-        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {
-          write(out, BLOCK_COUNT);
-          writeInt(out, count);
-          newline(out);
-          for(int i=0;i<count;i++) {
-            write(out, BLOCK_DOC_ID);
-            writeInt(out, docIDs[start+i]);
-            newline(out);
-          }
-        }
-
-        @Override
-        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {
-          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths
-        }
-
-        @Override
-        protected void writeLeafBlockPackedValue(IndexOutput out, int[] commonPrefixLengths, byte[] bytes) throws IOException {
-          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths
-          assert bytes.length == packedBytesLength;
-          write(out, BLOCK_VALUE);
-          write(out, new BytesRef(bytes, 0, bytes.length).toString());
-          newline(out);
-        }          
-      }) {
-
-      values.intersect(fieldInfo.name, new IntersectVisitor() {
-          @Override
-          public void visit(int docID) {
-            throw new IllegalStateException();
-          }
-
-          public void visit(int docID, byte[] packedValue) throws IOException {
-            writer.add(packedValue, docID);
-          }
-
-          @Override
-          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
-            return Relation.CELL_CROSSES_QUERY;
-          }
-        });
-
-      // We could have 0 points on merge since all docs with points may be deleted:
-      if (writer.getPointCount() > 0) {
-        indexFPs.put(fieldInfo.name, writer.finish(dataOut));
-      }
-    }
-  }
-
-  private void write(IndexOutput out, String s) throws IOException {
-    SimpleTextUtil.write(out, s, scratch);
-  }
-
-  private void writeInt(IndexOutput out, int x) throws IOException {
-    SimpleTextUtil.write(out, Integer.toString(x), scratch);
-  }
-
-  private void writeLong(IndexOutput out, long x) throws IOException {
-    SimpleTextUtil.write(out, Long.toString(x), scratch);
-  }
-
-  private void write(IndexOutput out, BytesRef b) throws IOException {
-    SimpleTextUtil.write(out, b);
-  }
-
-  private void newline(IndexOutput out) throws IOException {
-    SimpleTextUtil.writeNewline(out);
-  }
-
-  @Override
-  public void finish() throws IOException {
-    SimpleTextUtil.write(dataOut, END);
-    SimpleTextUtil.writeNewline(dataOut);
-    SimpleTextUtil.writeChecksum(dataOut, scratch);
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (dataOut != null) {
-      dataOut.close();
-      dataOut = null;
-
-      // Write index file
-      String fileName = IndexFileNames.segmentFileName(writeState.segmentInfo.name, writeState.segmentSuffix, SimpleTextPointFormat.POINT_INDEX_EXTENSION);
-      try (IndexOutput indexOut = writeState.directory.createOutput(fileName, writeState.context)) {
-        int count = indexFPs.size();
-        write(indexOut, FIELD_COUNT);
-        write(indexOut, Integer.toString(count));
-        newline(indexOut);
-        for(Map.Entry<String,Long> ent : indexFPs.entrySet()) {
-          write(indexOut, FIELD_FP_NAME);
-          write(indexOut, ent.getKey());
-          newline(indexOut);
-          write(indexOut, FIELD_FP);
-          write(indexOut, Long.toString(ent.getValue()));
-          newline(indexOut);
-        }
-        SimpleTextUtil.writeChecksum(indexOut, scratch);
-      }
-    }
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsFormat.java
new file mode 100644
index 0000000..5e3f57a
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsFormat.java
@@ -0,0 +1,53 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.simpletext;
+
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.PointsFormat;
+import org.apache.lucene.codecs.PointsReader;
+import org.apache.lucene.codecs.PointsWriter;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+/** For debugging, curiosity, transparency only!!  Do not
+ *  use this codec in production.
+ *
+ *  <p>This codec stores all dimensional data in a single
+ *  human-readable text file (_N.dim).  You can view this in
+ *  any text editor, and even edit it to alter your index.
+ *
+ *  @lucene.experimental */
+public final class SimpleTextPointsFormat extends PointsFormat {
+  
+  @Override
+  public PointsWriter fieldsWriter(SegmentWriteState state) throws IOException {
+    return new SimpleTextPointsWriter(state);
+  }
+
+  @Override
+  public PointsReader fieldsReader(SegmentReadState state) throws IOException {
+    return new SimpleTextPointsReader(state);
+  }
+
+  /** Extension of points data file */
+  static final String POINT_EXTENSION = "dim";
+
+  /** Extension of points index file */
+  static final String POINT_INDEX_EXTENSION = "dii";
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsReader.java
new file mode 100644
index 0000000..1477f17
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsReader.java
@@ -0,0 +1,302 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.simpletext;
+
+
+import java.io.IOException;
+import java.nio.charset.StandardCharsets;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.codecs.PointsReader;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.store.BufferedChecksumIndexInput;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefBuilder;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.bkd.BKDReader;
+
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointsWriter.BLOCK_FP;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointsWriter.BYTES_PER_DIM;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointsWriter.DOC_COUNT;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointsWriter.FIELD_COUNT;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointsWriter.FIELD_FP;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointsWriter.FIELD_FP_NAME;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointsWriter.INDEX_COUNT;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointsWriter.MAX_LEAF_POINTS;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointsWriter.MAX_VALUE;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointsWriter.MIN_VALUE;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointsWriter.NUM_DIMS;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointsWriter.POINT_COUNT;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointsWriter.SPLIT_COUNT;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointsWriter.SPLIT_DIM;
+import static org.apache.lucene.codecs.simpletext.SimpleTextPointsWriter.SPLIT_VALUE;
+
+class SimpleTextPointsReader extends PointsReader {
+
+  private final IndexInput dataIn;
+  final SegmentReadState readState;
+  final Map<String,BKDReader> readers = new HashMap<>();
+  final BytesRefBuilder scratch = new BytesRefBuilder();
+
+  public SimpleTextPointsReader(SegmentReadState readState) throws IOException {
+    // Initialize readers now:
+
+    // Read index:
+    Map<String,Long> fieldToFileOffset = new HashMap<>();
+
+    String indexFileName = IndexFileNames.segmentFileName(readState.segmentInfo.name, readState.segmentSuffix, SimpleTextPointsFormat.POINT_INDEX_EXTENSION);
+    try (ChecksumIndexInput in = readState.directory.openChecksumInput(indexFileName, IOContext.DEFAULT)) {
+      readLine(in);
+      int count = parseInt(FIELD_COUNT);
+      for(int i=0;i<count;i++) {
+        readLine(in);
+        String fieldName = stripPrefix(FIELD_FP_NAME);
+        readLine(in);
+        long fp = parseLong(FIELD_FP);
+        fieldToFileOffset.put(fieldName, fp);
+      }
+      SimpleTextUtil.checkFooter(in);
+    }
+
+    boolean success = false;
+    String fileName = IndexFileNames.segmentFileName(readState.segmentInfo.name, readState.segmentSuffix, SimpleTextPointsFormat.POINT_EXTENSION);
+    dataIn = readState.directory.openInput(fileName, IOContext.DEFAULT);
+    try {
+      for(Map.Entry<String,Long> ent : fieldToFileOffset.entrySet()) {
+        readers.put(ent.getKey(), initReader(ent.getValue()));
+      }
+      success = true;
+    } finally {
+      if (success == false) {
+        IOUtils.closeWhileHandlingException(this);
+      }
+    }
+        
+    this.readState = readState;
+  }
+
+  private BKDReader initReader(long fp) throws IOException {
+    // NOTE: matches what writeIndex does in SimpleTextPointsWriter
+    dataIn.seek(fp);
+    readLine(dataIn);
+    int numDims = parseInt(NUM_DIMS);
+
+    readLine(dataIn);
+    int bytesPerDim = parseInt(BYTES_PER_DIM);
+
+    readLine(dataIn);
+    int maxPointsInLeafNode = parseInt(MAX_LEAF_POINTS);
+
+    readLine(dataIn);
+    int count = parseInt(INDEX_COUNT);
+
+    readLine(dataIn);
+    assert startsWith(MIN_VALUE);
+    BytesRef minValue = SimpleTextUtil.fromBytesRefString(stripPrefix(MIN_VALUE));
+    assert minValue.length == numDims*bytesPerDim;
+
+    readLine(dataIn);
+    assert startsWith(MAX_VALUE);
+    BytesRef maxValue = SimpleTextUtil.fromBytesRefString(stripPrefix(MAX_VALUE));
+    assert maxValue.length == numDims*bytesPerDim;
+
+    readLine(dataIn);
+    assert startsWith(POINT_COUNT);
+    long pointCount = parseLong(POINT_COUNT);
+
+    readLine(dataIn);
+    assert startsWith(DOC_COUNT);
+    int docCount = parseInt(DOC_COUNT);
+    
+    long[] leafBlockFPs = new long[count];
+    for(int i=0;i<count;i++) {
+      readLine(dataIn);
+      leafBlockFPs[i] = parseLong(BLOCK_FP);
+    }
+    readLine(dataIn);
+    count = parseInt(SPLIT_COUNT);
+
+    byte[] splitPackedValues = new byte[count * (1 + bytesPerDim)];
+    for(int i=0;i<count;i++) {
+      readLine(dataIn);
+      splitPackedValues[(1 + bytesPerDim) * i] = (byte) parseInt(SPLIT_DIM);
+      readLine(dataIn);
+      assert startsWith(SPLIT_VALUE);
+      BytesRef br = SimpleTextUtil.fromBytesRefString(stripPrefix(SPLIT_VALUE));
+      assert br.length == bytesPerDim;
+      System.arraycopy(br.bytes, br.offset, splitPackedValues, (1 + bytesPerDim) * i + 1, bytesPerDim);
+    }
+
+    return new SimpleTextBKDReader(dataIn, numDims, maxPointsInLeafNode, bytesPerDim, leafBlockFPs, splitPackedValues, minValue.bytes, maxValue.bytes, pointCount, docCount);
+  }
+
+  private void readLine(IndexInput in) throws IOException {
+    SimpleTextUtil.readLine(in, scratch);
+  }
+
+  private boolean startsWith(BytesRef prefix) {
+    return StringHelper.startsWith(scratch.get(), prefix);
+  }
+
+  private int parseInt(BytesRef prefix) {
+    assert startsWith(prefix);
+    return Integer.parseInt(stripPrefix(prefix));
+  }
+
+  private long parseLong(BytesRef prefix) {
+    assert startsWith(prefix);
+    return Long.parseLong(stripPrefix(prefix));
+  }
+
+  private String stripPrefix(BytesRef prefix) {
+    return new String(scratch.bytes(), prefix.length, scratch.length() - prefix.length, StandardCharsets.UTF_8);
+  }
+
+  private BKDReader getBKDReader(String fieldName) {
+    FieldInfo fieldInfo = readState.fieldInfos.fieldInfo(fieldName);
+    if (fieldInfo == null) {
+      throw new IllegalArgumentException("field=\"" + fieldName + "\" is unrecognized");
+    }
+    if (fieldInfo.getPointDimensionCount() == 0) {
+      throw new IllegalArgumentException("field=\"" + fieldName + "\" did not index points");
+    }
+    return readers.get(fieldName);
+  }
+
+  /** Finds all documents and points matching the provided visitor */
+  @Override
+  public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this field were deleted in this segment:
+      return;
+    }
+    bkdReader.intersect(visitor);
+  }
+
+  @Override
+  public void checkIntegrity() throws IOException {
+    BytesRefBuilder scratch = new BytesRefBuilder();
+    IndexInput clone = dataIn.clone();
+    clone.seek(0);
+
+    // checksum is fixed-width encoded with 20 bytes, plus 1 byte for newline (the space is included in SimpleTextUtil.CHECKSUM):
+    long footerStartPos = dataIn.length() - (SimpleTextUtil.CHECKSUM.length + 21);
+    ChecksumIndexInput input = new BufferedChecksumIndexInput(clone);
+    while (true) {
+      SimpleTextUtil.readLine(input, scratch);
+      if (input.getFilePointer() >= footerStartPos) {
+        // Make sure we landed at precisely the right location:
+        if (input.getFilePointer() != footerStartPos) {
+          throw new CorruptIndexException("SimpleText failure: footer does not start at expected position current=" + input.getFilePointer() + " vs expected=" + footerStartPos, input);
+        }
+        SimpleTextUtil.checkFooter(input);
+        break;
+      }
+    }
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    return 0L;
+  }
+
+  @Override
+  public void close() throws IOException {
+    dataIn.close();
+  }
+
+  @Override
+  public String toString() {
+    return "SimpleTextPointsReader(segment=" + readState.segmentInfo.name + " maxDoc=" + readState.segmentInfo.maxDoc() + ")";
+  }
+
+  @Override
+  public byte[] getMinPackedValue(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this field were deleted in this segment:
+      return null;
+    }
+    return bkdReader.getMinPackedValue();
+  }
+
+  @Override
+  public byte[] getMaxPackedValue(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this field were deleted in this segment:
+      return null;
+    }
+    return bkdReader.getMaxPackedValue();
+  }
+
+  @Override
+  public int getNumDimensions(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this field were deleted in this segment:
+      return 0;
+    }
+    return bkdReader.getNumDimensions();
+  }
+
+  @Override
+  public int getBytesPerDimension(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this field were deleted in this segment:
+      return 0;
+    }
+    return bkdReader.getBytesPerDimension();
+  }
+
+  @Override
+  public long size(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this field were deleted in this segment:
+      return 0;
+    }
+    return bkdReader.getPointCount();
+  }
+
+  @Override
+  public int getDocCount(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this field were deleted in this segment:
+      return 0;
+    }
+    return bkdReader.getDocCount();
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter.java
new file mode 100644
index 0000000..d2b848d
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPointsWriter.java
@@ -0,0 +1,244 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.simpletext;
+
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.codecs.PointsReader;
+import org.apache.lucene.codecs.PointsWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.PointValues.IntersectVisitor;
+import org.apache.lucene.index.PointValues.Relation;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefBuilder;
+import org.apache.lucene.util.bkd.BKDWriter;
+
+class SimpleTextPointsWriter extends PointsWriter {
+
+  final static BytesRef NUM_DIMS      = new BytesRef("num dims ");
+  final static BytesRef BYTES_PER_DIM = new BytesRef("bytes per dim ");
+  final static BytesRef MAX_LEAF_POINTS = new BytesRef("max leaf points ");
+  final static BytesRef INDEX_COUNT = new BytesRef("index count ");
+  final static BytesRef BLOCK_COUNT   = new BytesRef("block count ");
+  final static BytesRef BLOCK_DOC_ID  = new BytesRef("  doc ");
+  final static BytesRef BLOCK_FP      = new BytesRef("  block fp ");
+  final static BytesRef BLOCK_VALUE   = new BytesRef("  block value ");
+  final static BytesRef SPLIT_COUNT   = new BytesRef("split count ");
+  final static BytesRef SPLIT_DIM     = new BytesRef("  split dim ");
+  final static BytesRef SPLIT_VALUE   = new BytesRef("  split value ");
+  final static BytesRef FIELD_COUNT   = new BytesRef("field count ");
+  final static BytesRef FIELD_FP_NAME = new BytesRef("  field fp name ");
+  final static BytesRef FIELD_FP      = new BytesRef("  field fp ");
+  final static BytesRef MIN_VALUE     = new BytesRef("min value ");
+  final static BytesRef MAX_VALUE     = new BytesRef("max value ");
+  final static BytesRef POINT_COUNT   = new BytesRef("point count ");
+  final static BytesRef DOC_COUNT     = new BytesRef("doc count ");
+  final static BytesRef END           = new BytesRef("END");
+
+  private IndexOutput dataOut;
+  final BytesRefBuilder scratch = new BytesRefBuilder();
+  final SegmentWriteState writeState;
+  final Map<String,Long> indexFPs = new HashMap<>();
+
+  public SimpleTextPointsWriter(SegmentWriteState writeState) throws IOException {
+    String fileName = IndexFileNames.segmentFileName(writeState.segmentInfo.name, writeState.segmentSuffix, SimpleTextPointsFormat.POINT_EXTENSION);
+    dataOut = writeState.directory.createOutput(fileName, writeState.context);
+    this.writeState = writeState;
+  }
+
+  @Override
+  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {
+
+    // We use the normal BKDWriter, but subclass to customize how it writes the index and blocks to disk:
+    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),
+                                          writeState.directory,
+                                          writeState.segmentInfo.name,
+                                          fieldInfo.getPointDimensionCount(),
+                                          fieldInfo.getPointNumBytes(),
+                                          BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE,
+                                          BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP) {
+
+        @Override
+        protected void writeIndex(IndexOutput out, long[] leafBlockFPs, byte[] splitPackedValues) throws IOException {
+          write(out, NUM_DIMS);
+          writeInt(out, numDims);
+          newline(out);
+
+          write(out, BYTES_PER_DIM);
+          writeInt(out, bytesPerDim);
+          newline(out);
+
+          write(out, MAX_LEAF_POINTS);
+          writeInt(out, maxPointsInLeafNode);
+          newline(out);
+
+          write(out, INDEX_COUNT);
+          writeInt(out, leafBlockFPs.length);
+          newline(out);
+
+          write(out, MIN_VALUE);
+          BytesRef br = new BytesRef(minPackedValue, 0, minPackedValue.length);
+          write(out, br.toString());
+          newline(out);
+
+          write(out, MAX_VALUE);
+          br = new BytesRef(maxPackedValue, 0, maxPackedValue.length);
+          write(out, br.toString());
+          newline(out);
+
+          write(out, POINT_COUNT);
+          writeLong(out, pointCount);
+          newline(out);
+
+          write(out, DOC_COUNT);
+          writeInt(out, docsSeen.cardinality());
+          newline(out);
+
+          for(int i=0;i<leafBlockFPs.length;i++) {
+            write(out, BLOCK_FP);
+            writeLong(out, leafBlockFPs[i]);
+            newline(out);
+          }
+
+          assert (splitPackedValues.length % (1 + fieldInfo.getPointNumBytes())) == 0;
+          int count = splitPackedValues.length / (1 + fieldInfo.getPointNumBytes());
+          assert count == leafBlockFPs.length;
+
+          write(out, SPLIT_COUNT);
+          writeInt(out, count);
+          newline(out);
+
+          for(int i=0;i<count;i++) {
+            write(out, SPLIT_DIM);
+            writeInt(out, splitPackedValues[i * (1 + fieldInfo.getPointNumBytes())] & 0xff);
+            newline(out);
+            write(out, SPLIT_VALUE);
+            br = new BytesRef(splitPackedValues, 1+(i * (1+fieldInfo.getPointNumBytes())), fieldInfo.getPointNumBytes());
+            write(out, br.toString());
+            newline(out);
+          }
+        }
+
+        @Override
+        protected void writeLeafBlockDocs(IndexOutput out, int[] docIDs, int start, int count) throws IOException {
+          write(out, BLOCK_COUNT);
+          writeInt(out, count);
+          newline(out);
+          for(int i=0;i<count;i++) {
+            write(out, BLOCK_DOC_ID);
+            writeInt(out, docIDs[start+i]);
+            newline(out);
+          }
+        }
+
+        @Override
+        protected void writeCommonPrefixes(IndexOutput out, int[] commonPrefixLengths, byte[] packedValue) {
+          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths
+        }
+
+        @Override
+        protected void writeLeafBlockPackedValue(IndexOutput out, int[] commonPrefixLengths, byte[] bytes) throws IOException {
+          // NOTE: we don't do prefix coding, so we ignore commonPrefixLengths
+          assert bytes.length == packedBytesLength;
+          write(out, BLOCK_VALUE);
+          write(out, new BytesRef(bytes, 0, bytes.length).toString());
+          newline(out);
+        }          
+      }) {
+
+      values.intersect(fieldInfo.name, new IntersectVisitor() {
+          @Override
+          public void visit(int docID) {
+            throw new IllegalStateException();
+          }
+
+          public void visit(int docID, byte[] packedValue) throws IOException {
+            writer.add(packedValue, docID);
+          }
+
+          @Override
+          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
+            return Relation.CELL_CROSSES_QUERY;
+          }
+        });
+
+      // We could have 0 points on merge since all docs with points may be deleted:
+      if (writer.getPointCount() > 0) {
+        indexFPs.put(fieldInfo.name, writer.finish(dataOut));
+      }
+    }
+  }
+
+  private void write(IndexOutput out, String s) throws IOException {
+    SimpleTextUtil.write(out, s, scratch);
+  }
+
+  private void writeInt(IndexOutput out, int x) throws IOException {
+    SimpleTextUtil.write(out, Integer.toString(x), scratch);
+  }
+
+  private void writeLong(IndexOutput out, long x) throws IOException {
+    SimpleTextUtil.write(out, Long.toString(x), scratch);
+  }
+
+  private void write(IndexOutput out, BytesRef b) throws IOException {
+    SimpleTextUtil.write(out, b);
+  }
+
+  private void newline(IndexOutput out) throws IOException {
+    SimpleTextUtil.writeNewline(out);
+  }
+
+  @Override
+  public void finish() throws IOException {
+    SimpleTextUtil.write(dataOut, END);
+    SimpleTextUtil.writeNewline(dataOut);
+    SimpleTextUtil.writeChecksum(dataOut, scratch);
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (dataOut != null) {
+      dataOut.close();
+      dataOut = null;
+
+      // Write index file
+      String fileName = IndexFileNames.segmentFileName(writeState.segmentInfo.name, writeState.segmentSuffix, SimpleTextPointsFormat.POINT_INDEX_EXTENSION);
+      try (IndexOutput indexOut = writeState.directory.createOutput(fileName, writeState.context)) {
+        int count = indexFPs.size();
+        write(indexOut, FIELD_COUNT);
+        write(indexOut, Integer.toString(count));
+        newline(indexOut);
+        for(Map.Entry<String,Long> ent : indexFPs.entrySet()) {
+          write(indexOut, FIELD_FP_NAME);
+          write(indexOut, ent.getKey());
+          newline(indexOut);
+          write(indexOut, FIELD_FP);
+          write(indexOut, Long.toString(ent.getValue()));
+          newline(indexOut);
+        }
+        SimpleTextUtil.writeChecksum(indexOut, scratch);
+      }
+    }
+  }
+}
diff --git a/lucene/codecs/src/test/org/apache/lucene/codecs/simpletext/TestSimpleTextPointFormat.java b/lucene/codecs/src/test/org/apache/lucene/codecs/simpletext/TestSimpleTextPointFormat.java
deleted file mode 100644
index 7637584..0000000
--- a/lucene/codecs/src/test/org/apache/lucene/codecs/simpletext/TestSimpleTextPointFormat.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.codecs.simpletext;
-
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BasePointFormatTestCase;
-
-/**
- * Tests SimpleText's point format
- */
-public class TestSimpleTextPointFormat extends BasePointFormatTestCase {
-  private final Codec codec = new SimpleTextCodec();
-
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-}
diff --git a/lucene/codecs/src/test/org/apache/lucene/codecs/simpletext/TestSimpleTextPointsFormat.java b/lucene/codecs/src/test/org/apache/lucene/codecs/simpletext/TestSimpleTextPointsFormat.java
new file mode 100644
index 0000000..4594caf
--- /dev/null
+++ b/lucene/codecs/src/test/org/apache/lucene/codecs/simpletext/TestSimpleTextPointsFormat.java
@@ -0,0 +1,33 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.simpletext;
+
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BasePointsFormatTestCase;
+
+/**
+ * Tests SimpleText's point format
+ */
+public class TestSimpleTextPointsFormat extends BasePointsFormatTestCase {
+  private final Codec codec = new SimpleTextCodec();
+
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/Codec.java b/lucene/core/src/java/org/apache/lucene/codecs/Codec.java
index a263acd..5d704ca 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/Codec.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/Codec.java
@@ -109,7 +109,7 @@ public abstract class Codec implements NamedSPILoader.NamedSPI {
   public abstract CompoundFormat compoundFormat();
 
   /** Encodes/decodes points index */
-  public abstract PointFormat pointFormat();
+  public abstract PointsFormat pointsFormat();
   
   /** looks up a codec by name */
   public static Codec forName(String name) {
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/FilterCodec.java b/lucene/core/src/java/org/apache/lucene/codecs/FilterCodec.java
index 4fe236e..9abd8d4 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/FilterCodec.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/FilterCodec.java
@@ -105,7 +105,7 @@ public abstract class FilterCodec extends Codec {
   }
 
   @Override
-  public PointFormat pointFormat() {
-    return delegate.pointFormat();
+  public PointsFormat pointsFormat() {
+    return delegate.pointsFormat();
   }
 }
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/PointFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/PointFormat.java
deleted file mode 100644
index 964f8f0..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/PointFormat.java
+++ /dev/null
@@ -1,111 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.codecs;
-
-
-import java.io.IOException;
-
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-/** 
- * Encodes/decodes indexed points.
- *
- * @lucene.experimental */
-public abstract class PointFormat {
-
-  /**
-   * Creates a new point format.
-   */
-  protected PointFormat() {
-  }
-
-  /** Writes a new segment */
-  public abstract PointWriter fieldsWriter(SegmentWriteState state) throws IOException;
-
-  /** Reads a segment.  NOTE: by the time this call
-   *  returns, it must hold open any files it will need to
-   *  use; else, those files may be deleted. 
-   *  Additionally, required files may be deleted during the execution of 
-   *  this call before there is a chance to open them. Under these 
-   *  circumstances an IOException should be thrown by the implementation. 
-   *  IOExceptions are expected and will automatically cause a retry of the 
-   *  segment opening logic with the newly revised segments.
-   *  */
-  public abstract PointReader fieldsReader(SegmentReadState state) throws IOException;
-
-  /** A {@code PointFormat} that has nothing indexed */
-  public static final PointFormat EMPTY = new PointFormat() {
-      @Override
-      public PointWriter fieldsWriter(SegmentWriteState state) {
-        throw new UnsupportedOperationException();
-      }
-
-      @Override
-      public PointReader fieldsReader(SegmentReadState state) {
-        return new PointReader() {
-          @Override
-          public void close() {
-          }
-
-          @Override
-          public long ramBytesUsed() {
-            return 0L;
-          }
-
-          @Override
-          public void checkIntegrity() {
-          }
-
-          @Override
-          public void intersect(String fieldName, IntersectVisitor visitor) {
-            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
-          }
-
-          @Override
-          public byte[] getMinPackedValue(String fieldName) {
-            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
-          }
-
-          @Override
-          public byte[] getMaxPackedValue(String fieldName) {
-            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
-          }
-
-          @Override
-          public int getNumDimensions(String fieldName) {
-            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
-          }
-
-          @Override
-          public int getBytesPerDimension(String fieldName) {
-            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
-          }
-
-          @Override
-          public long size(String fieldName) {
-            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
-          }
-
-          @Override
-          public int getDocCount(String fieldName) {
-            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
-          }
-        };
-      }
-    };
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/PointReader.java b/lucene/core/src/java/org/apache/lucene/codecs/PointReader.java
deleted file mode 100644
index 7087840..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/PointReader.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.codecs;
-
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.index.PointValues;
-import org.apache.lucene.util.Accountable;
-
-/** Abstract API to visit point values.
- *
- * @lucene.experimental
- */
-public abstract class PointReader extends PointValues implements Closeable, Accountable {
-
-  /** Sole constructor. (For invocation by subclass constructors, typically implicit.) */
-  protected PointReader() {}
-
-  /** 
-   * Checks consistency of this reader.
-   * <p>
-   * Note that this may be costly in terms of I/O, e.g. 
-   * may involve computing a checksum value against large data files.
-   * @lucene.internal
-   */
-  public abstract void checkIntegrity() throws IOException;
-
-  /** 
-   * Returns an instance optimized for merging.
-   * <p>
-   * The default implementation returns {@code this} */
-  public PointReader getMergeInstance() throws IOException {
-    return this;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/PointWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/PointWriter.java
deleted file mode 100644
index c2972a8..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/PointWriter.java
+++ /dev/null
@@ -1,151 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.codecs;
-
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.MergeState;
-
-/** Abstract API to write points
- *
- * @lucene.experimental
- */
-
-public abstract class PointWriter implements Closeable {
-  /** Sole constructor. (For invocation by subclass 
-   *  constructors, typically implicit.) */
-  protected PointWriter() {
-  }
-
-  /** Write all values contained in the provided reader */
-  public abstract void writeField(FieldInfo fieldInfo, PointReader values) throws IOException;
-
-  /** Default naive merge implementation for one field: it just re-indexes all the values
-   *  from the incoming segment.  The default codec overrides this for 1D fields and uses
-   *  a faster but more complex implementation. */
-  protected void mergeOneField(MergeState mergeState, FieldInfo fieldInfo) throws IOException {
-    writeField(fieldInfo,
-               new PointReader() {
-                 @Override
-                 public void intersect(String fieldName, IntersectVisitor mergedVisitor) throws IOException {
-                   if (fieldName.equals(fieldInfo.name) == false) {
-                     throw new IllegalArgumentException("field name must match the field being merged");
-                   }
-                   for (int i=0;i<mergeState.pointReaders.length;i++) {
-                     PointReader pointReader = mergeState.pointReaders[i];
-                     if (pointReader == null) {
-                       // This segment has no points
-                       continue;
-                     }
-                     MergeState.DocMap docMap = mergeState.docMaps[i];
-                     int docBase = mergeState.docBase[i];
-                     pointReader.intersect(fieldInfo.name,
-                                                 new IntersectVisitor() {
-                                                   @Override
-                                                   public void visit(int docID) {
-                                                     // Should never be called because our compare method never returns Relation.CELL_INSIDE_QUERY
-                                                     throw new IllegalStateException();
-                                                   }
-
-                                                   @Override
-                                                   public void visit(int docID, byte[] packedValue) throws IOException {
-                                                     int newDocID = docMap.get(docID);
-                                                     if (newDocID != -1) {
-                                                       // Not deleted:
-                                                       mergedVisitor.visit(docBase + newDocID, packedValue);
-                                                     }
-                                                   }
-
-                                                   @Override
-                                                   public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
-                                                     // Forces this segment's PointReader to always visit all docs + values:
-                                                     return Relation.CELL_CROSSES_QUERY;
-                                                   }
-                                                 });
-                   }
-                 }
-
-                 @Override
-                 public void checkIntegrity() {
-                   throw new UnsupportedOperationException();
-                 }
-
-                 @Override
-                 public long ramBytesUsed() {
-                   return 0L;
-                 }
-
-                 @Override
-                 public void close() {
-                 }
-
-                 @Override
-                 public byte[] getMinPackedValue(String fieldName) {
-                   throw new UnsupportedOperationException();
-                 }
-
-                 @Override
-                 public byte[] getMaxPackedValue(String fieldName) {
-                   throw new UnsupportedOperationException();
-                 }
-
-                 @Override
-                 public int getNumDimensions(String fieldName) {
-                   throw new UnsupportedOperationException();
-                 }
-
-                 @Override
-                 public int getBytesPerDimension(String fieldName) {
-                   throw new UnsupportedOperationException();
-                 }
-
-                 @Override
-                 public long size(String fieldName) {
-                   throw new UnsupportedOperationException();
-                 }
-
-                 @Override
-                 public int getDocCount(String fieldName) {
-                   throw new UnsupportedOperationException();
-                 }
-               });
-  }
-
-  /** Default merge implementation to merge incoming points readers by visiting all their points and
-   *  adding to this writer */
-  public void merge(MergeState mergeState) throws IOException {
-    // check each incoming reader
-    for (PointReader reader : mergeState.pointReaders) {
-      if (reader != null) {
-        reader.checkIntegrity();
-      }
-    }
-    // merge field at a time
-    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {
-      if (fieldInfo.getPointDimensionCount() != 0) {
-        mergeOneField(mergeState, fieldInfo);
-      }
-    }
-    finish();
-  }
-
-  /** Called once at the end before close */
-  public abstract void finish() throws IOException;
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/PointsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/PointsFormat.java
new file mode 100644
index 0000000..e49bf53
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/PointsFormat.java
@@ -0,0 +1,111 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs;
+
+
+import java.io.IOException;
+
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+/** 
+ * Encodes/decodes indexed points.
+ *
+ * @lucene.experimental */
+public abstract class PointsFormat {
+
+  /**
+   * Creates a new point format.
+   */
+  protected PointsFormat() {
+  }
+
+  /** Writes a new segment */
+  public abstract PointsWriter fieldsWriter(SegmentWriteState state) throws IOException;
+
+  /** Reads a segment.  NOTE: by the time this call
+   *  returns, it must hold open any files it will need to
+   *  use; else, those files may be deleted. 
+   *  Additionally, required files may be deleted during the execution of 
+   *  this call before there is a chance to open them. Under these 
+   *  circumstances an IOException should be thrown by the implementation. 
+   *  IOExceptions are expected and will automatically cause a retry of the 
+   *  segment opening logic with the newly revised segments.
+   *  */
+  public abstract PointsReader fieldsReader(SegmentReadState state) throws IOException;
+
+  /** A {@code PointsFormat} that has nothing indexed */
+  public static final PointsFormat EMPTY = new PointsFormat() {
+      @Override
+      public PointsWriter fieldsWriter(SegmentWriteState state) {
+        throw new UnsupportedOperationException();
+      }
+
+      @Override
+      public PointsReader fieldsReader(SegmentReadState state) {
+        return new PointsReader() {
+          @Override
+          public void close() {
+          }
+
+          @Override
+          public long ramBytesUsed() {
+            return 0L;
+          }
+
+          @Override
+          public void checkIntegrity() {
+          }
+
+          @Override
+          public void intersect(String fieldName, IntersectVisitor visitor) {
+            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
+          }
+
+          @Override
+          public byte[] getMinPackedValue(String fieldName) {
+            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
+          }
+
+          @Override
+          public byte[] getMaxPackedValue(String fieldName) {
+            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
+          }
+
+          @Override
+          public int getNumDimensions(String fieldName) {
+            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
+          }
+
+          @Override
+          public int getBytesPerDimension(String fieldName) {
+            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
+          }
+
+          @Override
+          public long size(String fieldName) {
+            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
+          }
+
+          @Override
+          public int getDocCount(String fieldName) {
+            throw new IllegalArgumentException("field=\"" + fieldName + "\" was not indexed with points");
+          }
+        };
+      }
+    };
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/PointsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/PointsReader.java
new file mode 100644
index 0000000..ab21431
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/PointsReader.java
@@ -0,0 +1,51 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs;
+
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.index.PointValues;
+import org.apache.lucene.util.Accountable;
+
+/** Abstract API to visit point values.
+ *
+ * @lucene.experimental
+ */
+public abstract class PointsReader extends PointValues implements Closeable, Accountable {
+
+  /** Sole constructor. (For invocation by subclass constructors, typically implicit.) */
+  protected PointsReader() {}
+
+  /** 
+   * Checks consistency of this reader.
+   * <p>
+   * Note that this may be costly in terms of I/O, e.g. 
+   * may involve computing a checksum value against large data files.
+   * @lucene.internal
+   */
+  public abstract void checkIntegrity() throws IOException;
+
+  /** 
+   * Returns an instance optimized for merging.
+   * <p>
+   * The default implementation returns {@code this} */
+  public PointsReader getMergeInstance() throws IOException {
+    return this;
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/PointsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/PointsWriter.java
new file mode 100644
index 0000000..56689ec
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/PointsWriter.java
@@ -0,0 +1,151 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs;
+
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.MergeState;
+
+/** Abstract API to write points
+ *
+ * @lucene.experimental
+ */
+
+public abstract class PointsWriter implements Closeable {
+  /** Sole constructor. (For invocation by subclass 
+   *  constructors, typically implicit.) */
+  protected PointsWriter() {
+  }
+
+  /** Write all values contained in the provided reader */
+  public abstract void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException;
+
+  /** Default naive merge implementation for one field: it just re-indexes all the values
+   *  from the incoming segment.  The default codec overrides this for 1D fields and uses
+   *  a faster but more complex implementation. */
+  protected void mergeOneField(MergeState mergeState, FieldInfo fieldInfo) throws IOException {
+    writeField(fieldInfo,
+               new PointsReader() {
+                 @Override
+                 public void intersect(String fieldName, IntersectVisitor mergedVisitor) throws IOException {
+                   if (fieldName.equals(fieldInfo.name) == false) {
+                     throw new IllegalArgumentException("field name must match the field being merged");
+                   }
+                   for (int i=0;i<mergeState.pointsReaders.length;i++) {
+                     PointsReader pointsReader = mergeState.pointsReaders[i];
+                     if (pointsReader == null) {
+                       // This segment has no points
+                       continue;
+                     }
+                     MergeState.DocMap docMap = mergeState.docMaps[i];
+                     int docBase = mergeState.docBase[i];
+                     pointsReader.intersect(fieldInfo.name,
+                                            new IntersectVisitor() {
+                                              @Override
+                                              public void visit(int docID) {
+                                                // Should never be called because our compare method never returns Relation.CELL_INSIDE_QUERY
+                                                throw new IllegalStateException();
+                                              }
+
+                                              @Override
+                                              public void visit(int docID, byte[] packedValue) throws IOException {
+                                                int newDocID = docMap.get(docID);
+                                                if (newDocID != -1) {
+                                                  // Not deleted:
+                                                  mergedVisitor.visit(docBase + newDocID, packedValue);
+                                                }
+                                              }
+
+                                              @Override
+                                              public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
+                                                // Forces this segment's PointsReader to always visit all docs + values:
+                                                return Relation.CELL_CROSSES_QUERY;
+                                              }
+                                            });
+                   }
+                 }
+
+                 @Override
+                 public void checkIntegrity() {
+                   throw new UnsupportedOperationException();
+                 }
+
+                 @Override
+                 public long ramBytesUsed() {
+                   return 0L;
+                 }
+
+                 @Override
+                 public void close() {
+                 }
+
+                 @Override
+                 public byte[] getMinPackedValue(String fieldName) {
+                   throw new UnsupportedOperationException();
+                 }
+
+                 @Override
+                 public byte[] getMaxPackedValue(String fieldName) {
+                   throw new UnsupportedOperationException();
+                 }
+
+                 @Override
+                 public int getNumDimensions(String fieldName) {
+                   throw new UnsupportedOperationException();
+                 }
+
+                 @Override
+                 public int getBytesPerDimension(String fieldName) {
+                   throw new UnsupportedOperationException();
+                 }
+
+                 @Override
+                 public long size(String fieldName) {
+                   throw new UnsupportedOperationException();
+                 }
+
+                 @Override
+                 public int getDocCount(String fieldName) {
+                   throw new UnsupportedOperationException();
+                 }
+               });
+  }
+
+  /** Default merge implementation to merge incoming points readers by visiting all their points and
+   *  adding to this writer */
+  public void merge(MergeState mergeState) throws IOException {
+    // check each incoming reader
+    for (PointsReader reader : mergeState.pointsReaders) {
+      if (reader != null) {
+        reader.checkIntegrity();
+      }
+    }
+    // merge field at a time
+    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {
+      if (fieldInfo.getPointDimensionCount() != 0) {
+        mergeOneField(mergeState, fieldInfo);
+      }
+    }
+    finish();
+  }
+
+  /** Called once at the end before close */
+  public abstract void finish() throws IOException;
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60Codec.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60Codec.java
index e9ef9a8..9f0d546 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60Codec.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60Codec.java
@@ -21,12 +21,12 @@ import java.util.Objects;
 
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.CompoundFormat;
-import org.apache.lucene.codecs.PointFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PointsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
@@ -129,8 +129,8 @@ public class Lucene60Codec extends Codec {
   }
 
   @Override
-  public final PointFormat pointFormat() {
-    return new Lucene60PointFormat();
+  public final PointsFormat pointsFormat() {
+    return new Lucene60PointsFormat();
   }
 
   /** Returns the postings format that should be used for writing 
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointFormat.java
deleted file mode 100644
index 8f43187..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointFormat.java
+++ /dev/null
@@ -1,106 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.codecs.lucene60;
-
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PointFormat;
-import org.apache.lucene.codecs.PointReader;
-import org.apache.lucene.codecs.PointWriter;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Lucene 6.0 point format, which encodes dimensional values in a block KD-tree structure
- * for fast shape intersection filtering. See <a href="https://www.cs.duke.edu/~pankaj/publications/papers/bkd-sstd.pdf">this paper</a> for details.
- *
- * <p>This data structure is written as a series of blocks on disk, with an in-memory perfectly balanced
- * binary tree of split values referencing those blocks at the leaves.
- *
- * <p>The <code>.dim</code> file has both blocks and the index split
- * values, for each field.  The file starts with {@link CodecUtil#writeIndexHeader}.
- *
- * <p>The blocks are written like this:
- *
- * <ul>
- *  <li> count (vInt)
- *  <li> delta-docID (vInt) <sup>count</sup> (delta coded docIDs, in sorted order)
- *  <li> packedValue<sup>count</sup> (the <code>byte[]</code> value of each dimension packed into a single <code>byte[]</code>)
- * </ul>
- *
- * <p>After all blocks for a field are written, then the index is written:
- * <ul>
- *  <li> numDims (vInt)
- *  <li> maxPointsInLeafNode (vInt)
- *  <li> bytesPerDim (vInt)
- *  <li> count (vInt)
- *  <li> byte[bytesPerDim]<sup>count</sup> (packed <code>byte[]</code> all split values)
- *  <li> delta-blockFP (vLong)<sup>count</sup> (delta-coded file pointers to the on-disk leaf blocks))
- * </ul>
- *
- * <p>After all fields blocks + index data are written, {@link CodecUtil#writeFooter} writes the checksum.
- *
- * <p>The <code>.dii</code> file records the file pointer in the <code>.dim</code> file where each field's
- * index data was written.  It starts with {@link CodecUtil#writeIndexHeader}, then has:
- *
- * <ul>
- *   <li> fieldCount (vInt)
- *   <li> (fieldNumber (vInt), fieldFilePointer (vLong))<sup>fieldCount</sup>
- * </ul>
- *
- * <p>After all fields blocks + index data are written, {@link CodecUtil#writeFooter} writes the checksum.
- *
- * @lucene.experimental
- */
-
-public final class Lucene60PointFormat extends PointFormat {
-
-  static final String DATA_CODEC_NAME = "Lucene60PointFormatData";
-  static final String META_CODEC_NAME = "Lucene60PointFormatMeta";
-
-  /**
-   * Filename extension for the leaf blocks
-   */
-  public static final String DATA_EXTENSION = "dim";
-
-  /**
-   * Filename extension for the index per field
-   */
-  public static final String INDEX_EXTENSION = "dii";
-
-  static final int DATA_VERSION_START = 0;
-  static final int DATA_VERSION_CURRENT = DATA_VERSION_START;
-
-  static final int INDEX_VERSION_START = 0;
-  static final int INDEX_VERSION_CURRENT = INDEX_VERSION_START;
-
-  /** Sole constructor */
-  public Lucene60PointFormat() {
-  }
-
-  @Override
-  public PointWriter fieldsWriter(SegmentWriteState state) throws IOException {
-    return new Lucene60PointWriter(state);
-  }
-
-  @Override
-  public PointReader fieldsReader(SegmentReadState state) throws IOException {
-    return new Lucene60PointReader(state);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointReader.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointReader.java
deleted file mode 100644
index 91a1e6c..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointReader.java
+++ /dev/null
@@ -1,241 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.codecs.lucene60;
-
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PointReader;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.Accountables;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.bkd.BKDReader;
-
-/** Reads point values previously written with {@link Lucene60PointWriter} */
-public class Lucene60PointReader extends PointReader implements Closeable {
-  final IndexInput dataIn;
-  final SegmentReadState readState;
-  final Map<Integer,BKDReader> readers = new HashMap<>();
-
-  /** Sole constructor */
-  public Lucene60PointReader(SegmentReadState readState) throws IOException {
-    this.readState = readState;
-
-
-    String indexFileName = IndexFileNames.segmentFileName(readState.segmentInfo.name,
-                                                          readState.segmentSuffix,
-                                                          Lucene60PointFormat.INDEX_EXTENSION);
-
-    Map<Integer,Long> fieldToFileOffset = new HashMap<>();
-
-    // Read index file
-    try (ChecksumIndexInput indexIn = readState.directory.openChecksumInput(indexFileName, readState.context)) {
-      Throwable priorE = null;
-      try {
-        CodecUtil.checkIndexHeader(indexIn,
-                                   Lucene60PointFormat.META_CODEC_NAME,
-                                   Lucene60PointFormat.INDEX_VERSION_START,
-                                   Lucene60PointFormat.INDEX_VERSION_START,
-                                   readState.segmentInfo.getId(),
-                                   readState.segmentSuffix);
-        int count = indexIn.readVInt();
-        for(int i=0;i<count;i++) {
-          int fieldNumber = indexIn.readVInt();
-          long fp = indexIn.readVLong();
-          fieldToFileOffset.put(fieldNumber, fp);
-        }
-      } catch (Throwable t) {
-        priorE = t;
-      } finally {
-        CodecUtil.checkFooter(indexIn, priorE);
-      }
-    }
-
-    String dataFileName = IndexFileNames.segmentFileName(readState.segmentInfo.name,
-                                                         readState.segmentSuffix,
-                                                         Lucene60PointFormat.DATA_EXTENSION);
-    boolean success = false;
-    dataIn = readState.directory.openInput(dataFileName, readState.context);
-    try {
-
-      CodecUtil.checkIndexHeader(dataIn,
-                                 Lucene60PointFormat.DATA_CODEC_NAME,
-                                 Lucene60PointFormat.DATA_VERSION_START,
-                                 Lucene60PointFormat.DATA_VERSION_START,
-                                 readState.segmentInfo.getId(),
-                                 readState.segmentSuffix);
-
-      // NOTE: data file is too costly to verify checksum against all the bytes on open,
-      // but for now we at least verify proper structure of the checksum footer: which looks
-      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-      // such as file truncation.
-      CodecUtil.retrieveChecksum(dataIn);
-
-      for(Map.Entry<Integer,Long> ent : fieldToFileOffset.entrySet()) {
-        int fieldNumber = ent.getKey();
-        long fp = ent.getValue();
-        dataIn.seek(fp);
-        BKDReader reader = new BKDReader(dataIn);
-        readers.put(fieldNumber, reader);
-      }
-
-      success = true;
-    } finally {
-      if (success == false) {
-        IOUtils.closeWhileHandlingException(this);
-      }
-    }
-  }
-
-  private BKDReader getBKDReader(String fieldName) {
-    FieldInfo fieldInfo = readState.fieldInfos.fieldInfo(fieldName);
-    if (fieldInfo == null) {
-      throw new IllegalArgumentException("field=\"" + fieldName + "\" is unrecognized");
-    }
-    if (fieldInfo.getPointDimensionCount() == 0) {
-      throw new IllegalArgumentException("field=\"" + fieldName + "\" did not index point values");
-    }
-
-    return readers.get(fieldInfo.number);
-  }
-
-  @Override
-  public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
-    BKDReader bkdReader = getBKDReader(fieldName);
-
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index points in the past, but
-      // now all docs having this point field were deleted in this segment:
-      return;
-    }
-
-    bkdReader.intersect(visitor);
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    long sizeInBytes = 0;
-    for(BKDReader reader : readers.values()) {
-      sizeInBytes += reader.ramBytesUsed();
-    }
-    return sizeInBytes;
-  }
-
-  @Override
-  public Collection<Accountable> getChildResources() {
-    List<Accountable> resources = new ArrayList<>();
-    for(Map.Entry<Integer,BKDReader> ent : readers.entrySet()) {
-      resources.add(Accountables.namedAccountable(readState.fieldInfos.fieldInfo(ent.getKey()).name,
-                                                  ent.getValue()));
-    }
-    return Collections.unmodifiableList(resources);
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {
-    CodecUtil.checksumEntireFile(dataIn);
-  }
-
-  @Override
-  public void close() throws IOException {
-    dataIn.close();
-    // Free up heap:
-    readers.clear();
-  }
-
-  @Override
-  public byte[] getMinPackedValue(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index points in the past, but
-      // now all docs having this point field were deleted in this segment:
-      return null;
-    }
-
-    return bkdReader.getMinPackedValue();
-  }
-
-  @Override
-  public byte[] getMaxPackedValue(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index points in the past, but
-      // now all docs having this point field were deleted in this segment:
-      return null;
-    }
-
-    return bkdReader.getMaxPackedValue();
-  }
-
-  @Override
-  public int getNumDimensions(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index points in the past, but
-      // now all docs having this point field were deleted in this segment:
-      return 0;
-    }
-    return bkdReader.getNumDimensions();
-  }
-
-  @Override
-  public int getBytesPerDimension(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index points in the past, but
-      // now all docs having this point field were deleted in this segment:
-      return 0;
-    }
-    return bkdReader.getBytesPerDimension();
-  }
-
-  @Override
-  public long size(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index points in the past, but
-      // now all docs having this point field were deleted in this segment:
-      return 0;
-    }
-    return bkdReader.getPointCount();
-  }
-
-  @Override
-  public int getDocCount(String fieldName) {
-    BKDReader bkdReader = getBKDReader(fieldName);
-    if (bkdReader == null) {
-      // Schema ghost corner case!  This field did index points in the past, but
-      // now all docs having this point field were deleted in this segment:
-      return 0;
-    }
-    return bkdReader.getDocCount();
-  }
-}
-  
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter.java
deleted file mode 100644
index 8a00d4c..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter.java
+++ /dev/null
@@ -1,225 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.codecs.lucene60;
-
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PointReader;
-import org.apache.lucene.codecs.PointWriter;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.PointValues.IntersectVisitor;
-import org.apache.lucene.index.PointValues.Relation;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.bkd.BKDReader;
-import org.apache.lucene.util.bkd.BKDWriter;
-
-/** Writes dimensional values */
-public class Lucene60PointWriter extends PointWriter implements Closeable {
-  
-  final IndexOutput dataOut;
-  final Map<String,Long> indexFPs = new HashMap<>();
-  final SegmentWriteState writeState;
-  final int maxPointsInLeafNode;
-  final double maxMBSortInHeap;
-  private boolean finished;
-
-  /** Full constructor */
-  public Lucene60PointWriter(SegmentWriteState writeState, int maxPointsInLeafNode, double maxMBSortInHeap) throws IOException {
-    assert writeState.fieldInfos.hasPointValues();
-    this.writeState = writeState;
-    this.maxPointsInLeafNode = maxPointsInLeafNode;
-    this.maxMBSortInHeap = maxMBSortInHeap;
-    String dataFileName = IndexFileNames.segmentFileName(writeState.segmentInfo.name,
-                                                         writeState.segmentSuffix,
-                                                         Lucene60PointFormat.DATA_EXTENSION);
-    dataOut = writeState.directory.createOutput(dataFileName, writeState.context);
-    boolean success = false;
-    try {
-      CodecUtil.writeIndexHeader(dataOut,
-                                 Lucene60PointFormat.DATA_CODEC_NAME,
-                                 Lucene60PointFormat.DATA_VERSION_CURRENT,
-                                 writeState.segmentInfo.getId(),
-                                 writeState.segmentSuffix);
-      success = true;
-    } finally {
-      if (success == false) {
-        IOUtils.closeWhileHandlingException(dataOut);
-      }
-    }
-  }
-
-  /** Uses the defaults values for {@code maxPointsInLeafNode} (1024) and {@code maxMBSortInHeap} (16.0) */
-  public Lucene60PointWriter(SegmentWriteState writeState) throws IOException {
-    this(writeState, BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE, BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP);
-  }
-
-  @Override
-  public void writeField(FieldInfo fieldInfo, PointReader values) throws IOException {
-
-    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),
-                                          writeState.directory,
-                                          writeState.segmentInfo.name,
-                                          fieldInfo.getPointDimensionCount(),
-                                          fieldInfo.getPointNumBytes(),
-                                          maxPointsInLeafNode,
-                                          maxMBSortInHeap)) {
-
-      values.intersect(fieldInfo.name, new IntersectVisitor() {
-          @Override
-          public void visit(int docID) {
-            throw new IllegalStateException();
-          }
-
-          public void visit(int docID, byte[] packedValue) throws IOException {
-            writer.add(packedValue, docID);
-          }
-
-          @Override
-          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
-            return Relation.CELL_CROSSES_QUERY;
-          }
-        });
-
-      // We could have 0 points on merge since all docs with dimensional fields may be deleted:
-      if (writer.getPointCount() > 0) {
-        indexFPs.put(fieldInfo.name, writer.finish(dataOut));
-      }
-    }
-  }
-
-  @Override
-  public void merge(MergeState mergeState) throws IOException {
-    for(PointReader reader : mergeState.pointReaders) {
-      if (reader instanceof Lucene60PointReader == false) {
-        // We can only bulk merge when all to-be-merged segments use our format:
-        super.merge(mergeState);
-        return;
-      }
-    }
-    for (PointReader reader : mergeState.pointReaders) {
-      if (reader != null) {
-        reader.checkIntegrity();
-      }
-    }
-
-    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {
-      if (fieldInfo.getPointDimensionCount() != 0) {
-        if (fieldInfo.getPointDimensionCount() == 1) {
-          //System.out.println("MERGE: field=" + fieldInfo.name);
-          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the
-          // already sorted incoming segments, instead of trying to sort all points again as if
-          // we were simply reindexing them:
-          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),
-                                                writeState.directory,
-                                                writeState.segmentInfo.name,
-                                                fieldInfo.getPointDimensionCount(),
-                                                fieldInfo.getPointNumBytes(),
-                                                maxPointsInLeafNode,
-                                                maxMBSortInHeap)) {
-            List<BKDReader> bkdReaders = new ArrayList<>();
-            List<MergeState.DocMap> docMaps = new ArrayList<>();
-            List<Integer> docIDBases = new ArrayList<>();
-            for(int i=0;i<mergeState.pointReaders.length;i++) {
-              PointReader reader = mergeState.pointReaders[i];
-
-              if (reader != null) {
-
-                // we confirmed this up above
-                assert reader instanceof Lucene60PointReader;
-                Lucene60PointReader reader60 = (Lucene60PointReader) reader;
-
-                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this
-                // reader's FieldInfo as we do below) because field numbers can easily be different
-                // when addIndexes(Directory...) copies over segments from another index:
-
-
-                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];
-                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);
-                if (readerFieldInfo != null) {
-                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);
-                  if (bkdReader != null) {
-                    docIDBases.add(mergeState.docBase[i]);
-                    bkdReaders.add(bkdReader);
-                    docMaps.add(mergeState.docMaps[i]);
-                  }
-                }
-              }
-            }
-
-            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);
-            if (fp != -1) {
-              indexFPs.put(fieldInfo.name, fp);
-            }
-          }
-        } else {
-          mergeOneField(mergeState, fieldInfo);
-        }
-      }
-    }
-
-    finish();
-  }
-
-  @Override
-  public void finish() throws IOException {
-    if (finished) {
-      throw new IllegalStateException("already finished");
-    }
-    finished = true;
-    CodecUtil.writeFooter(dataOut);
-
-    String indexFileName = IndexFileNames.segmentFileName(writeState.segmentInfo.name,
-                                                          writeState.segmentSuffix,
-                                                          Lucene60PointFormat.INDEX_EXTENSION);
-    // Write index file
-    try (IndexOutput indexOut = writeState.directory.createOutput(indexFileName, writeState.context)) {
-      CodecUtil.writeIndexHeader(indexOut,
-                                 Lucene60PointFormat.META_CODEC_NAME,
-                                 Lucene60PointFormat.INDEX_VERSION_CURRENT,
-                                 writeState.segmentInfo.getId(),
-                                 writeState.segmentSuffix);
-      int count = indexFPs.size();
-      indexOut.writeVInt(count);
-      for(Map.Entry<String,Long> ent : indexFPs.entrySet()) {
-        FieldInfo fieldInfo = writeState.fieldInfos.fieldInfo(ent.getKey());
-        if (fieldInfo == null) {
-          throw new IllegalStateException("wrote field=\"" + ent.getKey() + "\" but that field doesn't exist in FieldInfos");
-        }
-        indexOut.writeVInt(fieldInfo.number);
-        indexOut.writeVLong(ent.getValue());
-      }
-      CodecUtil.writeFooter(indexOut);
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    dataOut.close();
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsFormat.java
new file mode 100644
index 0000000..e558d0d
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsFormat.java
@@ -0,0 +1,106 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.lucene60;
+
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PointsFormat;
+import org.apache.lucene.codecs.PointsReader;
+import org.apache.lucene.codecs.PointsWriter;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+/**
+ * Lucene 6.0 point format, which encodes dimensional values in a block KD-tree structure
+ * for fast shape intersection filtering. See <a href="https://www.cs.duke.edu/~pankaj/publications/papers/bkd-sstd.pdf">this paper</a> for details.
+ *
+ * <p>This data structure is written as a series of blocks on disk, with an in-memory perfectly balanced
+ * binary tree of split values referencing those blocks at the leaves.
+ *
+ * <p>The <code>.dim</code> file has both blocks and the index split
+ * values, for each field.  The file starts with {@link CodecUtil#writeIndexHeader}.
+ *
+ * <p>The blocks are written like this:
+ *
+ * <ul>
+ *  <li> count (vInt)
+ *  <li> delta-docID (vInt) <sup>count</sup> (delta coded docIDs, in sorted order)
+ *  <li> packedValue<sup>count</sup> (the <code>byte[]</code> value of each dimension packed into a single <code>byte[]</code>)
+ * </ul>
+ *
+ * <p>After all blocks for a field are written, then the index is written:
+ * <ul>
+ *  <li> numDims (vInt)
+ *  <li> maxPointsInLeafNode (vInt)
+ *  <li> bytesPerDim (vInt)
+ *  <li> count (vInt)
+ *  <li> byte[bytesPerDim]<sup>count</sup> (packed <code>byte[]</code> all split values)
+ *  <li> delta-blockFP (vLong)<sup>count</sup> (delta-coded file pointers to the on-disk leaf blocks))
+ * </ul>
+ *
+ * <p>After all fields blocks + index data are written, {@link CodecUtil#writeFooter} writes the checksum.
+ *
+ * <p>The <code>.dii</code> file records the file pointer in the <code>.dim</code> file where each field's
+ * index data was written.  It starts with {@link CodecUtil#writeIndexHeader}, then has:
+ *
+ * <ul>
+ *   <li> fieldCount (vInt)
+ *   <li> (fieldNumber (vInt), fieldFilePointer (vLong))<sup>fieldCount</sup>
+ * </ul>
+ *
+ * <p>After all fields blocks + index data are written, {@link CodecUtil#writeFooter} writes the checksum.
+ *
+ * @lucene.experimental
+ */
+
+public final class Lucene60PointsFormat extends PointsFormat {
+
+  static final String DATA_CODEC_NAME = "Lucene60PointsFormatData";
+  static final String META_CODEC_NAME = "Lucene60PointsFormatMeta";
+
+  /**
+   * Filename extension for the leaf blocks
+   */
+  public static final String DATA_EXTENSION = "dim";
+
+  /**
+   * Filename extension for the index per field
+   */
+  public static final String INDEX_EXTENSION = "dii";
+
+  static final int DATA_VERSION_START = 0;
+  static final int DATA_VERSION_CURRENT = DATA_VERSION_START;
+
+  static final int INDEX_VERSION_START = 0;
+  static final int INDEX_VERSION_CURRENT = INDEX_VERSION_START;
+
+  /** Sole constructor */
+  public Lucene60PointsFormat() {
+  }
+
+  @Override
+  public PointsWriter fieldsWriter(SegmentWriteState state) throws IOException {
+    return new Lucene60PointsWriter(state);
+  }
+
+  @Override
+  public PointsReader fieldsReader(SegmentReadState state) throws IOException {
+    return new Lucene60PointsReader(state);
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsReader.java
new file mode 100644
index 0000000..e7a612c
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsReader.java
@@ -0,0 +1,241 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.lucene60;
+
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PointsReader;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Accountable;
+import org.apache.lucene.util.Accountables;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.bkd.BKDReader;
+
+/** Reads point values previously written with {@link Lucene60PointsWriter} */
+public class Lucene60PointsReader extends PointsReader implements Closeable {
+  final IndexInput dataIn;
+  final SegmentReadState readState;
+  final Map<Integer,BKDReader> readers = new HashMap<>();
+
+  /** Sole constructor */
+  public Lucene60PointsReader(SegmentReadState readState) throws IOException {
+    this.readState = readState;
+
+
+    String indexFileName = IndexFileNames.segmentFileName(readState.segmentInfo.name,
+                                                          readState.segmentSuffix,
+                                                          Lucene60PointsFormat.INDEX_EXTENSION);
+
+    Map<Integer,Long> fieldToFileOffset = new HashMap<>();
+
+    // Read index file
+    try (ChecksumIndexInput indexIn = readState.directory.openChecksumInput(indexFileName, readState.context)) {
+      Throwable priorE = null;
+      try {
+        CodecUtil.checkIndexHeader(indexIn,
+                                   Lucene60PointsFormat.META_CODEC_NAME,
+                                   Lucene60PointsFormat.INDEX_VERSION_START,
+                                   Lucene60PointsFormat.INDEX_VERSION_START,
+                                   readState.segmentInfo.getId(),
+                                   readState.segmentSuffix);
+        int count = indexIn.readVInt();
+        for(int i=0;i<count;i++) {
+          int fieldNumber = indexIn.readVInt();
+          long fp = indexIn.readVLong();
+          fieldToFileOffset.put(fieldNumber, fp);
+        }
+      } catch (Throwable t) {
+        priorE = t;
+      } finally {
+        CodecUtil.checkFooter(indexIn, priorE);
+      }
+    }
+
+    String dataFileName = IndexFileNames.segmentFileName(readState.segmentInfo.name,
+                                                         readState.segmentSuffix,
+                                                         Lucene60PointsFormat.DATA_EXTENSION);
+    boolean success = false;
+    dataIn = readState.directory.openInput(dataFileName, readState.context);
+    try {
+
+      CodecUtil.checkIndexHeader(dataIn,
+                                 Lucene60PointsFormat.DATA_CODEC_NAME,
+                                 Lucene60PointsFormat.DATA_VERSION_START,
+                                 Lucene60PointsFormat.DATA_VERSION_START,
+                                 readState.segmentInfo.getId(),
+                                 readState.segmentSuffix);
+
+      // NOTE: data file is too costly to verify checksum against all the bytes on open,
+      // but for now we at least verify proper structure of the checksum footer: which looks
+      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
+      // such as file truncation.
+      CodecUtil.retrieveChecksum(dataIn);
+
+      for(Map.Entry<Integer,Long> ent : fieldToFileOffset.entrySet()) {
+        int fieldNumber = ent.getKey();
+        long fp = ent.getValue();
+        dataIn.seek(fp);
+        BKDReader reader = new BKDReader(dataIn);
+        readers.put(fieldNumber, reader);
+      }
+
+      success = true;
+    } finally {
+      if (success == false) {
+        IOUtils.closeWhileHandlingException(this);
+      }
+    }
+  }
+
+  private BKDReader getBKDReader(String fieldName) {
+    FieldInfo fieldInfo = readState.fieldInfos.fieldInfo(fieldName);
+    if (fieldInfo == null) {
+      throw new IllegalArgumentException("field=\"" + fieldName + "\" is unrecognized");
+    }
+    if (fieldInfo.getPointDimensionCount() == 0) {
+      throw new IllegalArgumentException("field=\"" + fieldName + "\" did not index point values");
+    }
+
+    return readers.get(fieldInfo.number);
+  }
+
+  @Override
+  public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
+    BKDReader bkdReader = getBKDReader(fieldName);
+
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this point field were deleted in this segment:
+      return;
+    }
+
+    bkdReader.intersect(visitor);
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    long sizeInBytes = 0;
+    for(BKDReader reader : readers.values()) {
+      sizeInBytes += reader.ramBytesUsed();
+    }
+    return sizeInBytes;
+  }
+
+  @Override
+  public Collection<Accountable> getChildResources() {
+    List<Accountable> resources = new ArrayList<>();
+    for(Map.Entry<Integer,BKDReader> ent : readers.entrySet()) {
+      resources.add(Accountables.namedAccountable(readState.fieldInfos.fieldInfo(ent.getKey()).name,
+                                                  ent.getValue()));
+    }
+    return Collections.unmodifiableList(resources);
+  }
+
+  @Override
+  public void checkIntegrity() throws IOException {
+    CodecUtil.checksumEntireFile(dataIn);
+  }
+
+  @Override
+  public void close() throws IOException {
+    dataIn.close();
+    // Free up heap:
+    readers.clear();
+  }
+
+  @Override
+  public byte[] getMinPackedValue(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this point field were deleted in this segment:
+      return null;
+    }
+
+    return bkdReader.getMinPackedValue();
+  }
+
+  @Override
+  public byte[] getMaxPackedValue(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this point field were deleted in this segment:
+      return null;
+    }
+
+    return bkdReader.getMaxPackedValue();
+  }
+
+  @Override
+  public int getNumDimensions(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this point field were deleted in this segment:
+      return 0;
+    }
+    return bkdReader.getNumDimensions();
+  }
+
+  @Override
+  public int getBytesPerDimension(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this point field were deleted in this segment:
+      return 0;
+    }
+    return bkdReader.getBytesPerDimension();
+  }
+
+  @Override
+  public long size(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this point field were deleted in this segment:
+      return 0;
+    }
+    return bkdReader.getPointCount();
+  }
+
+  @Override
+  public int getDocCount(String fieldName) {
+    BKDReader bkdReader = getBKDReader(fieldName);
+    if (bkdReader == null) {
+      // Schema ghost corner case!  This field did index points in the past, but
+      // now all docs having this point field were deleted in this segment:
+      return 0;
+    }
+    return bkdReader.getDocCount();
+  }
+}
+  
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter.java
new file mode 100644
index 0000000..3d09c45
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter.java
@@ -0,0 +1,225 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.lucene60;
+
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PointsReader;
+import org.apache.lucene.codecs.PointsWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.PointValues.IntersectVisitor;
+import org.apache.lucene.index.PointValues.Relation;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.bkd.BKDReader;
+import org.apache.lucene.util.bkd.BKDWriter;
+
+/** Writes dimensional values */
+public class Lucene60PointsWriter extends PointsWriter implements Closeable {
+  
+  final IndexOutput dataOut;
+  final Map<String,Long> indexFPs = new HashMap<>();
+  final SegmentWriteState writeState;
+  final int maxPointsInLeafNode;
+  final double maxMBSortInHeap;
+  private boolean finished;
+
+  /** Full constructor */
+  public Lucene60PointsWriter(SegmentWriteState writeState, int maxPointsInLeafNode, double maxMBSortInHeap) throws IOException {
+    assert writeState.fieldInfos.hasPointValues();
+    this.writeState = writeState;
+    this.maxPointsInLeafNode = maxPointsInLeafNode;
+    this.maxMBSortInHeap = maxMBSortInHeap;
+    String dataFileName = IndexFileNames.segmentFileName(writeState.segmentInfo.name,
+                                                         writeState.segmentSuffix,
+                                                         Lucene60PointsFormat.DATA_EXTENSION);
+    dataOut = writeState.directory.createOutput(dataFileName, writeState.context);
+    boolean success = false;
+    try {
+      CodecUtil.writeIndexHeader(dataOut,
+                                 Lucene60PointsFormat.DATA_CODEC_NAME,
+                                 Lucene60PointsFormat.DATA_VERSION_CURRENT,
+                                 writeState.segmentInfo.getId(),
+                                 writeState.segmentSuffix);
+      success = true;
+    } finally {
+      if (success == false) {
+        IOUtils.closeWhileHandlingException(dataOut);
+      }
+    }
+  }
+
+  /** Uses the defaults values for {@code maxPointsInLeafNode} (1024) and {@code maxMBSortInHeap} (16.0) */
+  public Lucene60PointsWriter(SegmentWriteState writeState) throws IOException {
+    this(writeState, BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE, BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP);
+  }
+
+  @Override
+  public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {
+
+    try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),
+                                          writeState.directory,
+                                          writeState.segmentInfo.name,
+                                          fieldInfo.getPointDimensionCount(),
+                                          fieldInfo.getPointNumBytes(),
+                                          maxPointsInLeafNode,
+                                          maxMBSortInHeap)) {
+
+      values.intersect(fieldInfo.name, new IntersectVisitor() {
+          @Override
+          public void visit(int docID) {
+            throw new IllegalStateException();
+          }
+
+          public void visit(int docID, byte[] packedValue) throws IOException {
+            writer.add(packedValue, docID);
+          }
+
+          @Override
+          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
+            return Relation.CELL_CROSSES_QUERY;
+          }
+        });
+
+      // We could have 0 points on merge since all docs with dimensional fields may be deleted:
+      if (writer.getPointCount() > 0) {
+        indexFPs.put(fieldInfo.name, writer.finish(dataOut));
+      }
+    }
+  }
+
+  @Override
+  public void merge(MergeState mergeState) throws IOException {
+    for(PointsReader reader : mergeState.pointsReaders) {
+      if (reader instanceof Lucene60PointsReader == false) {
+        // We can only bulk merge when all to-be-merged segments use our format:
+        super.merge(mergeState);
+        return;
+      }
+    }
+    for (PointsReader reader : mergeState.pointsReaders) {
+      if (reader != null) {
+        reader.checkIntegrity();
+      }
+    }
+
+    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {
+      if (fieldInfo.getPointDimensionCount() != 0) {
+        if (fieldInfo.getPointDimensionCount() == 1) {
+          //System.out.println("MERGE: field=" + fieldInfo.name);
+          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the
+          // already sorted incoming segments, instead of trying to sort all points again as if
+          // we were simply reindexing them:
+          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),
+                                                writeState.directory,
+                                                writeState.segmentInfo.name,
+                                                fieldInfo.getPointDimensionCount(),
+                                                fieldInfo.getPointNumBytes(),
+                                                maxPointsInLeafNode,
+                                                maxMBSortInHeap)) {
+            List<BKDReader> bkdReaders = new ArrayList<>();
+            List<MergeState.DocMap> docMaps = new ArrayList<>();
+            List<Integer> docIDBases = new ArrayList<>();
+            for(int i=0;i<mergeState.pointsReaders.length;i++) {
+              PointsReader reader = mergeState.pointsReaders[i];
+
+              if (reader != null) {
+
+                // we confirmed this up above
+                assert reader instanceof Lucene60PointsReader;
+                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;
+
+                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this
+                // reader's FieldInfo as we do below) because field numbers can easily be different
+                // when addIndexes(Directory...) copies over segments from another index:
+
+
+                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];
+                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);
+                if (readerFieldInfo != null) {
+                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);
+                  if (bkdReader != null) {
+                    docIDBases.add(mergeState.docBase[i]);
+                    bkdReaders.add(bkdReader);
+                    docMaps.add(mergeState.docMaps[i]);
+                  }
+                }
+              }
+            }
+
+            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);
+            if (fp != -1) {
+              indexFPs.put(fieldInfo.name, fp);
+            }
+          }
+        } else {
+          mergeOneField(mergeState, fieldInfo);
+        }
+      }
+    }
+
+    finish();
+  }
+
+  @Override
+  public void finish() throws IOException {
+    if (finished) {
+      throw new IllegalStateException("already finished");
+    }
+    finished = true;
+    CodecUtil.writeFooter(dataOut);
+
+    String indexFileName = IndexFileNames.segmentFileName(writeState.segmentInfo.name,
+                                                          writeState.segmentSuffix,
+                                                          Lucene60PointsFormat.INDEX_EXTENSION);
+    // Write index file
+    try (IndexOutput indexOut = writeState.directory.createOutput(indexFileName, writeState.context)) {
+      CodecUtil.writeIndexHeader(indexOut,
+                                 Lucene60PointsFormat.META_CODEC_NAME,
+                                 Lucene60PointsFormat.INDEX_VERSION_CURRENT,
+                                 writeState.segmentInfo.getId(),
+                                 writeState.segmentSuffix);
+      int count = indexFPs.size();
+      indexOut.writeVInt(count);
+      for(Map.Entry<String,Long> ent : indexFPs.entrySet()) {
+        FieldInfo fieldInfo = writeState.fieldInfos.fieldInfo(ent.getKey());
+        if (fieldInfo == null) {
+          throw new IllegalStateException("wrote field=\"" + ent.getKey() + "\" but that field doesn't exist in FieldInfos");
+        }
+        indexOut.writeVInt(fieldInfo.number);
+        indexOut.writeVLong(ent.getValue());
+      }
+      CodecUtil.writeFooter(indexOut);
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    dataOut.close();
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/package-info.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/package-info.java
index a52d6f6..64531f5 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene60/package-info.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene60/package-info.java
@@ -193,7 +193,7 @@
  * An optional file indicating which documents are live.
  * </li>
  * <li>
- * {@link org.apache.lucene.codecs.lucene60.Lucene60PointFormat Point values}.
+ * {@link org.apache.lucene.codecs.lucene60.Lucene60PointsFormat Point values}.
  * Optional pair of files, recording dimesionally indexed fields, to enable fast
  * numeric range filtering and large numeric values like BigInteger and BigDecimal (1D)
  * and geo shape intersection (2D, 3D).
@@ -322,7 +322,7 @@
  * <td>Info about what files are live</td>
  * </tr>
  * <tr>
- * <td>{@link org.apache.lucene.codecs.lucene60.Lucene60PointFormat Point values}</td>
+ * <td>{@link org.apache.lucene.codecs.lucene60.Lucene60PointsFormat Point values}</td>
  * <td>.dii, .dim</td>
  * <td>Holds indexed points, if any</td>
  * </tr>
diff --git a/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java b/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
index 0bfa350..3c437c1 100644
--- a/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
+++ b/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
@@ -33,9 +33,9 @@ import java.util.Locale;
 import java.util.Map;
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.PointReader;
 import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.NormsProducer;
+import org.apache.lucene.codecs.PointsReader;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.codecs.TermVectorsReader;
@@ -1687,9 +1687,9 @@ public final class CheckIndex implements Closeable {
     Status.PointsStatus status = new Status.PointsStatus();
     try {
       if (fieldInfos.hasPointValues()) {
-        PointReader values = reader.getPointReader();
+        PointsReader values = reader.getPointsReader();
         if (values == null) {
-          throw new RuntimeException("there are fields with points, but reader.getPointReader() is null");
+          throw new RuntimeException("there are fields with points, but reader.getPointsReader() is null");
         }
         for (FieldInfo fieldInfo : fieldInfos) {
           if (fieldInfo.getPointDimensionCount() > 0) {
diff --git a/lucene/core/src/java/org/apache/lucene/index/CodecReader.java b/lucene/core/src/java/org/apache/lucene/index/CodecReader.java
index eb53648..194acd8e 100644
--- a/lucene/core/src/java/org/apache/lucene/index/CodecReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/CodecReader.java
@@ -25,10 +25,10 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import org.apache.lucene.codecs.PointReader;
 import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.NormsProducer;
+import org.apache.lucene.codecs.PointsReader;
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.util.Accountable;
@@ -77,10 +77,10 @@ public abstract class CodecReader extends LeafReader implements Accountable {
   public abstract FieldsProducer getPostingsReader();
 
   /**
-   * Expert: retrieve underlying PointReader
+   * Expert: retrieve underlying PointsReader
    * @lucene.internal
    */
-  public abstract PointReader getPointReader();
+  public abstract PointsReader getPointsReader();
   
   @Override
   public final void document(int docID, StoredFieldVisitor visitor) throws IOException {
@@ -323,8 +323,8 @@ public abstract class CodecReader extends LeafReader implements Accountable {
     }
 
     // points
-    if (getPointReader() != null) {
-      ramBytesUsed += getPointReader().ramBytesUsed();
+    if (getPointsReader() != null) {
+      ramBytesUsed += getPointsReader().ramBytesUsed();
     }
     
     return ramBytesUsed;
@@ -359,8 +359,8 @@ public abstract class CodecReader extends LeafReader implements Accountable {
     }
 
     // points
-    if (getPointReader() != null) {
-      resources.add(Accountables.namedAccountable("points", getPointReader()));
+    if (getPointsReader() != null) {
+      resources.add(Accountables.namedAccountable("points", getPointsReader()));
     }
     
     return Collections.unmodifiableList(resources);
@@ -394,8 +394,8 @@ public abstract class CodecReader extends LeafReader implements Accountable {
     }
 
     // points
-    if (getPointReader() != null) {
-      getPointReader().checkIntegrity();
+    if (getPointsReader() != null) {
+      getPointsReader().checkIntegrity();
     }
   }
 }
diff --git a/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java b/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java
index d1a68da..408f0bb 100644
--- a/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java
+++ b/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java
@@ -23,12 +23,12 @@ import java.util.HashMap;
 import java.util.Map;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.codecs.PointFormat;
-import org.apache.lucene.codecs.PointWriter;
 import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.NormsConsumer;
 import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PointsFormat;
+import org.apache.lucene.codecs.PointsWriter;
 import org.apache.lucene.codecs.StoredFieldsWriter;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.search.similarities.Similarity;
@@ -149,7 +149,7 @@ final class DefaultIndexingChain extends DocConsumer {
 
   /** Writes all buffered points. */
   private void writePoints(SegmentWriteState state) throws IOException {
-    PointWriter pointWriter = null;
+    PointsWriter pointsWriter = null;
     boolean success = false;
     try {
       for (int i=0;i<fieldHash.length;i++) {
@@ -160,16 +160,16 @@ final class DefaultIndexingChain extends DocConsumer {
               // BUG
               throw new AssertionError("segment=" + state.segmentInfo + ": field=\"" + perField.fieldInfo.name + "\" has no points but wrote them");
             }
-            if (pointWriter == null) {
+            if (pointsWriter == null) {
               // lazy init
-              PointFormat fmt = state.segmentInfo.getCodec().pointFormat();
+              PointsFormat fmt = state.segmentInfo.getCodec().pointsFormat();
               if (fmt == null) {
                 throw new IllegalStateException("field=\"" + perField.fieldInfo.name + "\" was indexed as points but codec does not support points");
               }
-              pointWriter = fmt.fieldsWriter(state);
+              pointsWriter = fmt.fieldsWriter(state);
             }
 
-            perField.pointValuesWriter.flush(state, pointWriter);
+            perField.pointValuesWriter.flush(state, pointsWriter);
             perField.pointValuesWriter = null;
           } else if (perField.fieldInfo.getPointDimensionCount() != 0) {
             // BUG
@@ -178,15 +178,15 @@ final class DefaultIndexingChain extends DocConsumer {
           perField = perField.next;
         }
       }
-      if (pointWriter != null) {
-        pointWriter.finish();
+      if (pointsWriter != null) {
+        pointsWriter.finish();
       }
       success = true;
     } finally {
       if (success) {
-        IOUtils.close(pointWriter);
+        IOUtils.close(pointsWriter);
       } else {
-        IOUtils.closeWhileHandlingException(pointWriter);
+        IOUtils.closeWhileHandlingException(pointsWriter);
       }
     }
   }
diff --git a/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java b/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java
index 2574bce..0dccf31 100644
--- a/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java
+++ b/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java
@@ -49,7 +49,7 @@ public final class FieldInfo {
   private long dvGen;
 
   /** If both of these are positive it means this field indexed points
-   *  (see {@link org.apache.lucene.codecs.PointFormat}). */
+   *  (see {@link org.apache.lucene.codecs.PointsFormat}). */
   private int pointDimensionCount;
   private int pointNumBytes;
 
diff --git a/lucene/core/src/java/org/apache/lucene/index/FilterCodecReader.java b/lucene/core/src/java/org/apache/lucene/index/FilterCodecReader.java
index 8b2a55f..41f0984 100644
--- a/lucene/core/src/java/org/apache/lucene/index/FilterCodecReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/FilterCodecReader.java
@@ -19,10 +19,10 @@ package org.apache.lucene.index;
 
 import java.util.Objects;
 
-import org.apache.lucene.codecs.PointReader;
 import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.NormsProducer;
+import org.apache.lucene.codecs.PointsReader;
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.util.Bits;
@@ -82,8 +82,8 @@ public class FilterCodecReader extends CodecReader {
   }
 
   @Override
-  public PointReader getPointReader() {
-    return in.getPointReader();
+  public PointsReader getPointsReader() {
+    return in.getPointsReader();
   }
 
   @Override
diff --git a/lucene/core/src/java/org/apache/lucene/index/LeafReader.java b/lucene/core/src/java/org/apache/lucene/index/LeafReader.java
index 76db600..9622d4e 100644
--- a/lucene/core/src/java/org/apache/lucene/index/LeafReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/LeafReader.java
@@ -300,7 +300,7 @@ public abstract class LeafReader extends IndexReader {
    */
   public abstract Bits getLiveDocs();
 
-  /** Returns the {@link org.apache.lucene.codecs.PointReader} used for numeric or
+  /** Returns the {@link PointValues} used for numeric or
    *  spatial searches, or null if there are no point fields. */
   public abstract PointValues getPointValues();
 
diff --git a/lucene/core/src/java/org/apache/lucene/index/MergeState.java b/lucene/core/src/java/org/apache/lucene/index/MergeState.java
index 531c799..7242785 100644
--- a/lucene/core/src/java/org/apache/lucene/index/MergeState.java
+++ b/lucene/core/src/java/org/apache/lucene/index/MergeState.java
@@ -20,10 +20,10 @@ package org.apache.lucene.index;
 import java.io.IOException;
 import java.util.List;
 
-import org.apache.lucene.codecs.PointReader;
 import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.NormsProducer;
+import org.apache.lucene.codecs.PointsReader;
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.util.Bits;
@@ -67,7 +67,7 @@ public class MergeState {
   public final FieldsProducer[] fieldsProducers;
 
   /** Point readers to merge */
-  public final PointReader[] pointReaders;
+  public final PointsReader[] pointsReaders;
 
   /** New docID base per reader. */
   public final int[] docBase;
@@ -90,7 +90,7 @@ public class MergeState {
     storedFieldsReaders = new StoredFieldsReader[numReaders];
     termVectorsReaders = new TermVectorsReader[numReaders];
     docValuesProducers = new DocValuesProducer[numReaders];
-    pointReaders = new PointReader[numReaders];
+    pointsReaders = new PointsReader[numReaders];
     fieldInfos = new FieldInfos[numReaders];
     liveDocs = new Bits[numReaders];
 
@@ -122,9 +122,9 @@ public class MergeState {
       }
       
       fieldsProducers[i] = reader.getPostingsReader().getMergeInstance();
-      pointReaders[i] = reader.getPointReader();
-      if (pointReaders[i] != null) {
-        pointReaders[i] = pointReaders[i].getMergeInstance();
+      pointsReaders[i] = reader.getPointsReader();
+      if (pointsReaders[i] != null) {
+        pointsReaders[i] = pointsReaders[i].getMergeInstance();
       }
     }
 
diff --git a/lucene/core/src/java/org/apache/lucene/index/PointValuesWriter.java b/lucene/core/src/java/org/apache/lucene/index/PointValuesWriter.java
index 546bf71..35b9a90 100644
--- a/lucene/core/src/java/org/apache/lucene/index/PointValuesWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/PointValuesWriter.java
@@ -18,8 +18,8 @@ package org.apache.lucene.index;
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.PointReader;
-import org.apache.lucene.codecs.PointWriter;
+import org.apache.lucene.codecs.PointsReader;
+import org.apache.lucene.codecs.PointsWriter;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.ByteBlockPool;
 import org.apache.lucene.util.BytesRef;
@@ -60,10 +60,10 @@ class PointValuesWriter {
     numDocs++;
   }
 
-  public void flush(SegmentWriteState state, PointWriter writer) throws IOException {
+  public void flush(SegmentWriteState state, PointsWriter writer) throws IOException {
 
     writer.writeField(fieldInfo,
-                      new PointReader() {
+                      new PointsReader() {
                         @Override
                         public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
                           if (fieldName.equals(fieldInfo.name) == false) {
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java b/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
index 296fbbc..e99c1ad 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
@@ -24,9 +24,9 @@ import java.util.Set;
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.PointReader;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.NormsProducer;
+import org.apache.lucene.codecs.PointsReader;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.codecs.TermVectorsReader;
@@ -54,7 +54,7 @@ final class SegmentCoreReaders {
 
   final StoredFieldsReader fieldsReaderOrig;
   final TermVectorsReader termVectorsReaderOrig;
-  final PointReader pointReader;
+  final PointsReader pointsReader;
   final Directory cfsReader;
   /** 
    * fieldinfos for this core: means gen=-1.
@@ -125,9 +125,9 @@ final class SegmentCoreReaders {
       }
 
       if (coreFieldInfos.hasPointValues()) {
-        pointReader = codec.pointFormat().fieldsReader(segmentReadState);
+        pointsReader = codec.pointsFormat().fieldsReader(segmentReadState);
       } else {
-        pointReader = null;
+        pointsReader = null;
       }
       success = true;
     } finally {
@@ -157,7 +157,7 @@ final class SegmentCoreReaders {
       Throwable th = null;
       try {
         IOUtils.close(termVectorsLocal, fieldsReaderLocal, fields, termVectorsReaderOrig, fieldsReaderOrig,
-                      cfsReader, normsProducer, pointReader);
+                      cfsReader, normsProducer, pointsReader);
       } catch (Throwable throwable) {
         th = throwable;
       } finally {
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java b/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
index bb8e256..b0d9bcf 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -21,10 +21,10 @@ import java.io.IOException;
 import java.util.List;
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.PointWriter;
 import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.NormsConsumer;
+import org.apache.lucene.codecs.PointsWriter;
 import org.apache.lucene.codecs.StoredFieldsWriter;
 import org.apache.lucene.codecs.TermVectorsWriter;
 import org.apache.lucene.store.Directory;
@@ -164,7 +164,7 @@ final class SegmentMerger {
   }
 
   private void mergePoints(SegmentWriteState segmentWriteState) throws IOException {
-    try (PointWriter writer = codec.pointFormat().fieldsWriter(segmentWriteState)) {
+    try (PointsWriter writer = codec.pointsFormat().fieldsWriter(segmentWriteState)) {
       writer.merge(mergeState);
     }
   }
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java b/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
index 205017b..8ed93e3 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
@@ -21,11 +21,11 @@ import java.io.IOException;
 import java.util.Collections;
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.PointReader;
 import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.NormsProducer;
+import org.apache.lucene.codecs.PointsReader;
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.store.Directory;
@@ -220,7 +220,7 @@ public final class SegmentReader extends CodecReader {
   @Override
   public PointValues getPointValues() {
     ensureOpen();
-    return core.pointReader;
+    return core.pointsReader;
   }
 
   @Override
@@ -242,9 +242,9 @@ public final class SegmentReader extends CodecReader {
   }
 
   @Override
-  public PointReader getPointReader() {
+  public PointsReader getPointsReader() {
     ensureOpen();
-    return core.pointReader;
+    return core.pointsReader;
   }
 
   @Override
diff --git a/lucene/core/src/java/org/apache/lucene/index/SlowCodecReaderWrapper.java b/lucene/core/src/java/org/apache/lucene/index/SlowCodecReaderWrapper.java
index a741111..3a73701 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SlowCodecReaderWrapper.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SlowCodecReaderWrapper.java
@@ -20,10 +20,10 @@ package org.apache.lucene.index;
 import java.io.IOException;
 import java.util.Iterator;
 
-import org.apache.lucene.codecs.PointReader;
 import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.NormsProducer;
+import org.apache.lucene.codecs.PointsReader;
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.util.Bits;
@@ -97,7 +97,7 @@ public final class SlowCodecReaderWrapper {
         }
 
         @Override
-        public PointReader getPointReader() {
+        public PointsReader getPointsReader() {
           return pointValuesToReader(reader.getPointValues());
         }
 
@@ -129,11 +129,11 @@ public final class SlowCodecReaderWrapper {
     }
   }
 
-  private static PointReader pointValuesToReader(PointValues values) {
+  private static PointsReader pointValuesToReader(PointValues values) {
     if (values == null) {
       return null;
     }
-    return new PointReader() {
+    return new PointsReader() {
       @Override
       public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
         values.intersect(fieldName, visitor);
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene60/TestLucene60PointFormat.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene60/TestLucene60PointFormat.java
deleted file mode 100644
index 272fc6b..0000000
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene60/TestLucene60PointFormat.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.codecs.lucene60;
-
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.FilterCodec;
-import org.apache.lucene.codecs.PointFormat;
-import org.apache.lucene.codecs.PointReader;
-import org.apache.lucene.codecs.PointWriter;
-import org.apache.lucene.index.BasePointFormatTestCase;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.TestUtil;
-
-/**
- * Tests Lucene60PointFormat
- */
-public class TestLucene60PointFormat extends BasePointFormatTestCase {
-  private final Codec codec;
-  
-  public TestLucene60PointFormat() {
-    // standard issue
-    Codec defaultCodec = TestUtil.getDefaultCodec();
-    if (random().nextBoolean()) {
-      // randomize parameters
-      int maxPointsInLeafNode = TestUtil.nextInt(random(), 50, 500);
-      double maxMBSortInHeap = 3.0 + (3*random().nextDouble());
-      if (VERBOSE) {
-        System.out.println("TEST: using Lucene60PointFormat with maxPointsInLeafNode=" + maxPointsInLeafNode + " and maxMBSortInHeap=" + maxMBSortInHeap);
-      }
-
-      // sneaky impersonation!
-      codec = new FilterCodec(defaultCodec.getName(), defaultCodec) {
-        @Override
-        public PointFormat pointFormat() {
-          return new PointFormat() {
-            @Override
-            public PointWriter fieldsWriter(SegmentWriteState writeState) throws IOException {
-              return new Lucene60PointWriter(writeState, maxPointsInLeafNode, maxMBSortInHeap);
-            }
-
-            @Override
-            public PointReader fieldsReader(SegmentReadState readState) throws IOException {
-              return new Lucene60PointReader(readState);
-            }
-          };
-        }
-      };
-    } else {
-      // standard issue
-      codec = defaultCodec;
-    }
-  }
-
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-
-  @Override
-  public void testMergeStability() throws Exception {
-    assumeFalse("TODO: mess with the parameters and test gets angry!", codec instanceof FilterCodec);
-    super.testMergeStability();
-  }
-  
-}
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene60/TestLucene60PointsFormat.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene60/TestLucene60PointsFormat.java
new file mode 100644
index 0000000..afa8ec4
--- /dev/null
+++ b/lucene/core/src/test/org/apache/lucene/codecs/lucene60/TestLucene60PointsFormat.java
@@ -0,0 +1,83 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.lucene60;
+
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.FilterCodec;
+import org.apache.lucene.codecs.PointsFormat;
+import org.apache.lucene.codecs.PointsReader;
+import org.apache.lucene.codecs.PointsWriter;
+import org.apache.lucene.index.BasePointsFormatTestCase;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.TestUtil;
+
+/**
+ * Tests Lucene60PointsFormat
+ */
+public class TestLucene60PointsFormat extends BasePointsFormatTestCase {
+  private final Codec codec;
+  
+  public TestLucene60PointsFormat() {
+    // standard issue
+    Codec defaultCodec = TestUtil.getDefaultCodec();
+    if (random().nextBoolean()) {
+      // randomize parameters
+      int maxPointsInLeafNode = TestUtil.nextInt(random(), 50, 500);
+      double maxMBSortInHeap = 3.0 + (3*random().nextDouble());
+      if (VERBOSE) {
+        System.out.println("TEST: using Lucene60PointsFormat with maxPointsInLeafNode=" + maxPointsInLeafNode + " and maxMBSortInHeap=" + maxMBSortInHeap);
+      }
+
+      // sneaky impersonation!
+      codec = new FilterCodec(defaultCodec.getName(), defaultCodec) {
+        @Override
+        public PointsFormat pointsFormat() {
+          return new PointsFormat() {
+            @Override
+            public PointsWriter fieldsWriter(SegmentWriteState writeState) throws IOException {
+              return new Lucene60PointsWriter(writeState, maxPointsInLeafNode, maxMBSortInHeap);
+            }
+
+            @Override
+            public PointsReader fieldsReader(SegmentReadState readState) throws IOException {
+              return new Lucene60PointsReader(readState);
+            }
+          };
+        }
+      };
+    } else {
+      // standard issue
+      codec = defaultCodec;
+    }
+  }
+
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+
+  @Override
+  public void testMergeStability() throws Exception {
+    assumeFalse("TODO: mess with the parameters and test gets angry!", codec instanceof FilterCodec);
+    super.testMergeStability();
+  }
+  
+}
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestPointValues.java b/lucene/core/src/test/org/apache/lucene/index/TestPointValues.java
index 9aafb3f..9faa0bc 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestPointValues.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestPointValues.java
@@ -22,11 +22,11 @@ import java.io.IOException;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.FilterCodec;
-import org.apache.lucene.codecs.PointFormat;
-import org.apache.lucene.codecs.PointReader;
-import org.apache.lucene.codecs.PointWriter;
-import org.apache.lucene.codecs.lucene60.Lucene60PointReader;
-import org.apache.lucene.codecs.lucene60.Lucene60PointWriter;
+import org.apache.lucene.codecs.PointsFormat;
+import org.apache.lucene.codecs.PointsReader;
+import org.apache.lucene.codecs.PointsWriter;
+import org.apache.lucene.codecs.lucene60.Lucene60PointsReader;
+import org.apache.lucene.codecs.lucene60.Lucene60PointsWriter;
 import org.apache.lucene.document.BinaryPoint;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.DoublePoint;
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestPointQueries.java b/lucene/core/src/test/org/apache/lucene/search/TestPointQueries.java
index 66a1593..cd0d719 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestPointQueries.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestPointQueries.java
@@ -32,11 +32,11 @@ import java.util.concurrent.atomic.AtomicBoolean;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.FilterCodec;
-import org.apache.lucene.codecs.PointFormat;
-import org.apache.lucene.codecs.PointReader;
-import org.apache.lucene.codecs.PointWriter;
-import org.apache.lucene.codecs.lucene60.Lucene60PointReader;
-import org.apache.lucene.codecs.lucene60.Lucene60PointWriter;
+import org.apache.lucene.codecs.PointsFormat;
+import org.apache.lucene.codecs.PointsReader;
+import org.apache.lucene.codecs.PointsWriter;
+import org.apache.lucene.codecs.lucene60.Lucene60PointsReader;
+import org.apache.lucene.codecs.lucene60.Lucene60PointsWriter;
 import org.apache.lucene.document.BinaryPoint;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.DoublePoint;
@@ -1157,21 +1157,21 @@ public class TestPointQueries extends LuceneTestCase {
       int maxPointsInLeafNode = TestUtil.nextInt(random(), 16, 2048);
       double maxMBSortInHeap = 4.0 + (3*random().nextDouble());
       if (VERBOSE) {
-        System.out.println("TEST: using Lucene60PointFormat with maxPointsInLeafNode=" + maxPointsInLeafNode + " and maxMBSortInHeap=" + maxMBSortInHeap);
+        System.out.println("TEST: using Lucene60PointsFormat with maxPointsInLeafNode=" + maxPointsInLeafNode + " and maxMBSortInHeap=" + maxMBSortInHeap);
       }
 
       return new FilterCodec("Lucene60", Codec.getDefault()) {
         @Override
-        public PointFormat pointFormat() {
-          return new PointFormat() {
+        public PointsFormat pointsFormat() {
+          return new PointsFormat() {
             @Override
-            public PointWriter fieldsWriter(SegmentWriteState writeState) throws IOException {
-              return new Lucene60PointWriter(writeState, maxPointsInLeafNode, maxMBSortInHeap);
+            public PointsWriter fieldsWriter(SegmentWriteState writeState) throws IOException {
+              return new Lucene60PointsWriter(writeState, maxPointsInLeafNode, maxMBSortInHeap);
             }
 
             @Override
-            public PointReader fieldsReader(SegmentReadState readState) throws IOException {
-              return new Lucene60PointReader(readState);
+            public PointsReader fieldsReader(SegmentReadState readState) throws IOException {
+              return new Lucene60PointsReader(readState);
             }
           };
         }
diff --git a/lucene/sandbox/src/test/org/apache/lucene/document/TestLatLonPointDistanceQuery.java b/lucene/sandbox/src/test/org/apache/lucene/document/TestLatLonPointDistanceQuery.java
index e37d75e..fa95710 100644
--- a/lucene/sandbox/src/test/org/apache/lucene/document/TestLatLonPointDistanceQuery.java
+++ b/lucene/sandbox/src/test/org/apache/lucene/document/TestLatLonPointDistanceQuery.java
@@ -20,11 +20,11 @@ import java.io.IOException;
 import java.util.BitSet;
 
 import org.apache.lucene.codecs.FilterCodec;
-import org.apache.lucene.codecs.PointFormat;
-import org.apache.lucene.codecs.PointReader;
-import org.apache.lucene.codecs.PointWriter;
-import org.apache.lucene.codecs.lucene60.Lucene60PointReader;
-import org.apache.lucene.codecs.lucene60.Lucene60PointWriter;
+import org.apache.lucene.codecs.PointsFormat;
+import org.apache.lucene.codecs.PointsReader;
+import org.apache.lucene.codecs.PointsWriter;
+import org.apache.lucene.codecs.lucene60.Lucene60PointsReader;
+import org.apache.lucene.codecs.lucene60.Lucene60PointsWriter;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -36,8 +36,8 @@ import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.spatial.util.GeoDistanceUtils;
 import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.TestUtil;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
 import org.apache.lucene.util.bkd.BKDWriter;
 
 /** Simple tests for {@link LatLonPoint#newDistanceQuery} */
@@ -119,16 +119,16 @@ public class TestLatLonPointDistanceQuery extends LuceneTestCase {
     int pointsInLeaf = 2 + random().nextInt(4);
     iwc.setCodec(new FilterCodec("Lucene60", TestUtil.getDefaultCodec()) {
       @Override
-      public PointFormat pointFormat() {
-        return new PointFormat() {
+      public PointsFormat pointsFormat() {
+        return new PointsFormat() {
           @Override
-          public PointWriter fieldsWriter(SegmentWriteState writeState) throws IOException {
-            return new Lucene60PointWriter(writeState, pointsInLeaf, BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP);
+          public PointsWriter fieldsWriter(SegmentWriteState writeState) throws IOException {
+            return new Lucene60PointsWriter(writeState, pointsInLeaf, BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP);
           }
 
           @Override
-          public PointReader fieldsReader(SegmentReadState readState) throws IOException {
-            return new Lucene60PointReader(readState);
+          public PointsReader fieldsReader(SegmentReadState readState) throws IOException {
+            return new Lucene60PointsReader(readState);
           }
         };
       }
diff --git a/lucene/spatial3d/src/test/org/apache/lucene/geo3d/TestGeo3DPoint.java b/lucene/spatial3d/src/test/org/apache/lucene/geo3d/TestGeo3DPoint.java
index 675d642..9d00d3e 100644
--- a/lucene/spatial3d/src/test/org/apache/lucene/geo3d/TestGeo3DPoint.java
+++ b/lucene/spatial3d/src/test/org/apache/lucene/geo3d/TestGeo3DPoint.java
@@ -27,12 +27,12 @@ import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.atomic.AtomicBoolean;
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.PointFormat;
-import org.apache.lucene.codecs.PointReader;
-import org.apache.lucene.codecs.PointWriter;
 import org.apache.lucene.codecs.FilterCodec;
-import org.apache.lucene.codecs.lucene60.Lucene60PointReader;
-import org.apache.lucene.codecs.lucene60.Lucene60PointWriter;
+import org.apache.lucene.codecs.PointsFormat;
+import org.apache.lucene.codecs.PointsReader;
+import org.apache.lucene.codecs.PointsWriter;
+import org.apache.lucene.codecs.lucene60.Lucene60PointsReader;
+import org.apache.lucene.codecs.lucene60.Lucene60PointsWriter;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.NumericDocValuesField;
@@ -76,21 +76,21 @@ public class TestGeo3DPoint extends LuceneTestCase {
       int maxPointsInLeafNode = TestUtil.nextInt(random(), 16, 2048);
       double maxMBSortInHeap = 3.0 + (3*random().nextDouble());
       if (VERBOSE) {
-        System.out.println("TEST: using Lucene60PointFormat with maxPointsInLeafNode=" + maxPointsInLeafNode + " and maxMBSortInHeap=" + maxMBSortInHeap);
+        System.out.println("TEST: using Lucene60PointsFormat with maxPointsInLeafNode=" + maxPointsInLeafNode + " and maxMBSortInHeap=" + maxMBSortInHeap);
       }
 
       return new FilterCodec("Lucene60", Codec.getDefault()) {
         @Override
-        public PointFormat pointFormat() {
-          return new PointFormat() {
+        public PointsFormat pointsFormat() {
+          return new PointsFormat() {
             @Override
-            public PointWriter fieldsWriter(SegmentWriteState writeState) throws IOException {
-              return new Lucene60PointWriter(writeState, maxPointsInLeafNode, maxMBSortInHeap);
+            public PointsWriter fieldsWriter(SegmentWriteState writeState) throws IOException {
+              return new Lucene60PointsWriter(writeState, maxPointsInLeafNode, maxMBSortInHeap);
             }
 
             @Override
-            public PointReader fieldsReader(SegmentReadState readState) throws IOException {
-              return new Lucene60PointReader(readState);
+            public PointsReader fieldsReader(SegmentReadState readState) throws IOException {
+              return new Lucene60PointsReader(readState);
             }
           };
         }
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingCodec.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingCodec.java
index 9e06307..15bcfa2 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingCodec.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingCodec.java
@@ -16,11 +16,11 @@
  */
 package org.apache.lucene.codecs.asserting;
 
-import org.apache.lucene.codecs.PointFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PointsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
@@ -53,7 +53,7 @@ public class AssertingCodec extends FilterCodec {
   private final LiveDocsFormat liveDocs = new AssertingLiveDocsFormat();
   private final PostingsFormat defaultFormat = new AssertingPostingsFormat();
   private final DocValuesFormat defaultDVFormat = new AssertingDocValuesFormat();
-  private final PointFormat pointFormat = new AssertingPointFormat();
+  private final PointsFormat pointsFormat = new AssertingPointsFormat();
 
   public AssertingCodec() {
     super("Asserting", TestUtil.getDefaultCodec());
@@ -90,8 +90,8 @@ public class AssertingCodec extends FilterCodec {
   }
 
   @Override
-  public PointFormat pointFormat() {
-    return pointFormat;
+  public PointsFormat pointsFormat() {
+    return pointsFormat;
   }
 
   @Override
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPointFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPointFormat.java
deleted file mode 100644
index 07365e0..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPointFormat.java
+++ /dev/null
@@ -1,276 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.codecs.asserting;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collection;
-
-import org.apache.lucene.codecs.PointFormat;
-import org.apache.lucene.codecs.PointReader;
-import org.apache.lucene.codecs.PointWriter;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.PointValues.IntersectVisitor;
-import org.apache.lucene.index.PointValues.Relation;
-import org.apache.lucene.index.PointValues;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.TestUtil;
-
-/**
- * Just like the default point format but with additional asserts.
- */
-
-public final class AssertingPointFormat extends PointFormat {
-  private final PointFormat in;
-
-  /** Create a new AssertingPointFormat */
-  public AssertingPointFormat() {
-    this(TestUtil.getDefaultCodec().pointFormat());
-  }
-
-  /**
-   * Expert: Create an AssertingPointFormat.
-   * This is only intended to pass special parameters for testing.
-   */
-  // TODO: can we randomize this a cleaner way? e.g. stored fields and vectors do
-  // this with a separate codec...
-  public AssertingPointFormat(PointFormat in) {
-    this.in = in;
-  }
-  
-  @Override
-  public PointWriter fieldsWriter(SegmentWriteState state) throws IOException {
-    return new AssertingPointWriter(state, in.fieldsWriter(state));
-  }
-
-  @Override
-  public PointReader fieldsReader(SegmentReadState state) throws IOException {
-    return new AssertingPointReader(state.segmentInfo.maxDoc(), in.fieldsReader(state));
-  }
-
-  /** Validates in the 1D case that all points are visited in order, and point values are in bounds of the last cell checked */
-  static class AssertingIntersectVisitor implements IntersectVisitor {
-    final IntersectVisitor in;
-    final int numDims;
-    final int bytesPerDim;
-    final byte[] lastDocValue;
-    final byte[] lastMinPackedValue;
-    final byte[] lastMaxPackedValue;
-    private Relation lastCompareResult;
-    private int lastDocID = -1;
-
-    public AssertingIntersectVisitor(int numDims, int bytesPerDim, IntersectVisitor in) {
-      this.in = in;
-      this.numDims = numDims;
-      this.bytesPerDim = bytesPerDim;
-      lastMaxPackedValue = new byte[numDims*bytesPerDim];
-      lastMinPackedValue = new byte[numDims*bytesPerDim];
-      if (numDims == 1) {
-        lastDocValue = new byte[bytesPerDim];
-      } else {
-        lastDocValue = null;
-      }
-    }
-
-    @Override
-    public void visit(int docID) throws IOException {
-      // This method, not filtering each hit, should only be invoked when the cell is inside the query shape:
-      assert lastCompareResult == Relation.CELL_INSIDE_QUERY;
-      in.visit(docID);
-    }
-
-    @Override
-    public void visit(int docID, byte[] packedValue) throws IOException {
-
-      // This method, to filter each doc's value, should only be invoked when the cell crosses the query shape:
-      assert lastCompareResult == PointValues.Relation.CELL_CROSSES_QUERY;
-
-      // This doc's packed value should be contained in the last cell passed to compare:
-      for(int dim=0;dim<numDims;dim++) {
-        assert StringHelper.compare(bytesPerDim, lastMinPackedValue, dim*bytesPerDim, packedValue, dim*bytesPerDim) <= 0: "dim=" + dim + " of " +  numDims + " value=" + new BytesRef(packedValue);
-        assert StringHelper.compare(bytesPerDim, lastMaxPackedValue, dim*bytesPerDim, packedValue, dim*bytesPerDim) >= 0: "dim=" + dim + " of " +  numDims + " value=" + new BytesRef(packedValue);
-      }
-
-      // TODO: we should assert that this "matches" whatever relation the last call to compare had returned
-      assert packedValue.length == numDims * bytesPerDim;
-      if (numDims == 1) {
-        int cmp = StringHelper.compare(bytesPerDim, lastDocValue, 0, packedValue, 0);
-        if (cmp < 0) {
-          // ok
-        } else if (cmp == 0) {
-          assert lastDocID <= docID: "doc ids are out of order when point values are the same!";
-        } else {
-          // out of order!
-          assert false: "point values are out of order";
-        }
-        System.arraycopy(packedValue, 0, lastDocValue, 0, bytesPerDim);
-      }
-      in.visit(docID, packedValue);
-    }
-
-    @Override
-    public void grow(int count) {
-      in.grow(count);
-    }
-
-    @Override
-    public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
-      for(int dim=0;dim<numDims;dim++) {
-        assert StringHelper.compare(bytesPerDim, minPackedValue, dim*bytesPerDim, maxPackedValue, dim*bytesPerDim) <= 0;
-      }
-      System.arraycopy(maxPackedValue, 0, lastMaxPackedValue, 0, numDims*bytesPerDim);
-      System.arraycopy(minPackedValue, 0, lastMinPackedValue, 0, numDims*bytesPerDim);
-      lastCompareResult = in.compare(minPackedValue, maxPackedValue);
-      return lastCompareResult;
-    }
-  }
-  
-  static class AssertingPointReader extends PointReader {
-    private final PointReader in;
-    private final int maxDoc;
-    
-    AssertingPointReader(int maxDoc, PointReader in) {
-      this.in = in;
-      this.maxDoc = maxDoc;
-      // do a few simple checks on init
-      assert toString() != null;
-      assert ramBytesUsed() >= 0;
-      assert getChildResources() != null;
-    }
-    
-    @Override
-    public void close() throws IOException {
-      in.close();
-      in.close(); // close again
-    }
-
-    @Override
-    public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
-      in.intersect(fieldName,
-                   new AssertingIntersectVisitor(in.getNumDimensions(fieldName), in.getBytesPerDimension(fieldName), visitor));
-    }
-
-    @Override
-    public long ramBytesUsed() {
-      long v = in.ramBytesUsed();
-      assert v >= 0;
-      return v;
-    }
-    
-    @Override
-    public Collection<Accountable> getChildResources() {
-      Collection<Accountable> res = in.getChildResources();
-      TestUtil.checkReadOnly(res);
-      return res;
-    }
-
-    @Override
-    public void checkIntegrity() throws IOException {
-      in.checkIntegrity();
-    }
-    
-    @Override
-    public PointReader getMergeInstance() throws IOException {
-      return new AssertingPointReader(maxDoc, in.getMergeInstance());
-    }
-
-    @Override
-    public String toString() {
-      return getClass().getSimpleName() + "(" + in.toString() + ")";
-    }
-
-    @Override
-    public byte[] getMinPackedValue(String fieldName) throws IOException {
-      assertStats(fieldName);
-      return in.getMinPackedValue(fieldName);
-    }
-
-    @Override
-    public byte[] getMaxPackedValue(String fieldName) throws IOException {
-      assertStats(fieldName);
-      return in.getMaxPackedValue(fieldName);
-    }
-
-    @Override
-    public int getNumDimensions(String fieldName) throws IOException {
-      assertStats(fieldName);
-      return in.getNumDimensions(fieldName);
-    }
-
-    @Override
-    public int getBytesPerDimension(String fieldName) throws IOException {
-      assertStats(fieldName);
-      return in.getBytesPerDimension(fieldName);
-    }
-
-    @Override
-    public long size(String fieldName) {
-      assertStats(fieldName);
-      return in.size(fieldName);
-    }
-
-    @Override
-    public int getDocCount(String fieldName) {
-      assertStats(fieldName);
-      return in.getDocCount(fieldName);
-    }
-
-    private void assertStats(String fieldName) {
-      assert in.size(fieldName) >= 0;
-      assert in.getDocCount(fieldName) >= 0;
-      assert in.getDocCount(fieldName) <= in.size(fieldName);
-      assert in.getDocCount(fieldName) <= maxDoc;
-    }
-  }
-
-  static class AssertingPointWriter extends PointWriter {
-    private final PointWriter in;
-
-    AssertingPointWriter(SegmentWriteState writeState, PointWriter in) {
-      this.in = in;
-    }
-    
-    @Override
-    public void writeField(FieldInfo fieldInfo, PointReader values) throws IOException {
-      if (fieldInfo.getPointDimensionCount() == 0) {
-        throw new IllegalArgumentException("writing field=\"" + fieldInfo.name + "\" but pointDimensionalCount is 0");
-      }
-      in.writeField(fieldInfo, values);
-    }
-
-    @Override
-    public void merge(MergeState mergeState) throws IOException {
-      in.merge(mergeState);
-    }
-
-    @Override
-    public void finish() throws IOException {
-      in.finish();
-    }
-
-    @Override
-    public void close() throws IOException {
-      in.close();
-      in.close(); // close again
-    }
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPointsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPointsFormat.java
new file mode 100644
index 0000000..061d5b6
--- /dev/null
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPointsFormat.java
@@ -0,0 +1,276 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.asserting;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collection;
+
+import org.apache.lucene.codecs.PointsFormat;
+import org.apache.lucene.codecs.PointsReader;
+import org.apache.lucene.codecs.PointsWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.PointValues.IntersectVisitor;
+import org.apache.lucene.index.PointValues.Relation;
+import org.apache.lucene.index.PointValues;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.Accountable;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.TestUtil;
+
+/**
+ * Just like the default point format but with additional asserts.
+ */
+
+public final class AssertingPointsFormat extends PointsFormat {
+  private final PointsFormat in;
+
+  /** Create a new AssertingPointsFormat */
+  public AssertingPointsFormat() {
+    this(TestUtil.getDefaultCodec().pointsFormat());
+  }
+
+  /**
+   * Expert: Create an AssertingPointsFormat.
+   * This is only intended to pass special parameters for testing.
+   */
+  // TODO: can we randomize this a cleaner way? e.g. stored fields and vectors do
+  // this with a separate codec...
+  public AssertingPointsFormat(PointsFormat in) {
+    this.in = in;
+  }
+  
+  @Override
+  public PointsWriter fieldsWriter(SegmentWriteState state) throws IOException {
+    return new AssertingPointsWriter(state, in.fieldsWriter(state));
+  }
+
+  @Override
+  public PointsReader fieldsReader(SegmentReadState state) throws IOException {
+    return new AssertingPointsReader(state.segmentInfo.maxDoc(), in.fieldsReader(state));
+  }
+
+  /** Validates in the 1D case that all points are visited in order, and point values are in bounds of the last cell checked */
+  static class AssertingIntersectVisitor implements IntersectVisitor {
+    final IntersectVisitor in;
+    final int numDims;
+    final int bytesPerDim;
+    final byte[] lastDocValue;
+    final byte[] lastMinPackedValue;
+    final byte[] lastMaxPackedValue;
+    private Relation lastCompareResult;
+    private int lastDocID = -1;
+
+    public AssertingIntersectVisitor(int numDims, int bytesPerDim, IntersectVisitor in) {
+      this.in = in;
+      this.numDims = numDims;
+      this.bytesPerDim = bytesPerDim;
+      lastMaxPackedValue = new byte[numDims*bytesPerDim];
+      lastMinPackedValue = new byte[numDims*bytesPerDim];
+      if (numDims == 1) {
+        lastDocValue = new byte[bytesPerDim];
+      } else {
+        lastDocValue = null;
+      }
+    }
+
+    @Override
+    public void visit(int docID) throws IOException {
+      // This method, not filtering each hit, should only be invoked when the cell is inside the query shape:
+      assert lastCompareResult == Relation.CELL_INSIDE_QUERY;
+      in.visit(docID);
+    }
+
+    @Override
+    public void visit(int docID, byte[] packedValue) throws IOException {
+
+      // This method, to filter each doc's value, should only be invoked when the cell crosses the query shape:
+      assert lastCompareResult == PointValues.Relation.CELL_CROSSES_QUERY;
+
+      // This doc's packed value should be contained in the last cell passed to compare:
+      for(int dim=0;dim<numDims;dim++) {
+        assert StringHelper.compare(bytesPerDim, lastMinPackedValue, dim*bytesPerDim, packedValue, dim*bytesPerDim) <= 0: "dim=" + dim + " of " +  numDims + " value=" + new BytesRef(packedValue);
+        assert StringHelper.compare(bytesPerDim, lastMaxPackedValue, dim*bytesPerDim, packedValue, dim*bytesPerDim) >= 0: "dim=" + dim + " of " +  numDims + " value=" + new BytesRef(packedValue);
+      }
+
+      // TODO: we should assert that this "matches" whatever relation the last call to compare had returned
+      assert packedValue.length == numDims * bytesPerDim;
+      if (numDims == 1) {
+        int cmp = StringHelper.compare(bytesPerDim, lastDocValue, 0, packedValue, 0);
+        if (cmp < 0) {
+          // ok
+        } else if (cmp == 0) {
+          assert lastDocID <= docID: "doc ids are out of order when point values are the same!";
+        } else {
+          // out of order!
+          assert false: "point values are out of order";
+        }
+        System.arraycopy(packedValue, 0, lastDocValue, 0, bytesPerDim);
+      }
+      in.visit(docID, packedValue);
+    }
+
+    @Override
+    public void grow(int count) {
+      in.grow(count);
+    }
+
+    @Override
+    public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {
+      for(int dim=0;dim<numDims;dim++) {
+        assert StringHelper.compare(bytesPerDim, minPackedValue, dim*bytesPerDim, maxPackedValue, dim*bytesPerDim) <= 0;
+      }
+      System.arraycopy(maxPackedValue, 0, lastMaxPackedValue, 0, numDims*bytesPerDim);
+      System.arraycopy(minPackedValue, 0, lastMinPackedValue, 0, numDims*bytesPerDim);
+      lastCompareResult = in.compare(minPackedValue, maxPackedValue);
+      return lastCompareResult;
+    }
+  }
+  
+  static class AssertingPointsReader extends PointsReader {
+    private final PointsReader in;
+    private final int maxDoc;
+    
+    AssertingPointsReader(int maxDoc, PointsReader in) {
+      this.in = in;
+      this.maxDoc = maxDoc;
+      // do a few simple checks on init
+      assert toString() != null;
+      assert ramBytesUsed() >= 0;
+      assert getChildResources() != null;
+    }
+    
+    @Override
+    public void close() throws IOException {
+      in.close();
+      in.close(); // close again
+    }
+
+    @Override
+    public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
+      in.intersect(fieldName,
+                   new AssertingIntersectVisitor(in.getNumDimensions(fieldName), in.getBytesPerDimension(fieldName), visitor));
+    }
+
+    @Override
+    public long ramBytesUsed() {
+      long v = in.ramBytesUsed();
+      assert v >= 0;
+      return v;
+    }
+    
+    @Override
+    public Collection<Accountable> getChildResources() {
+      Collection<Accountable> res = in.getChildResources();
+      TestUtil.checkReadOnly(res);
+      return res;
+    }
+
+    @Override
+    public void checkIntegrity() throws IOException {
+      in.checkIntegrity();
+    }
+    
+    @Override
+    public PointsReader getMergeInstance() throws IOException {
+      return new AssertingPointsReader(maxDoc, in.getMergeInstance());
+    }
+
+    @Override
+    public String toString() {
+      return getClass().getSimpleName() + "(" + in.toString() + ")";
+    }
+
+    @Override
+    public byte[] getMinPackedValue(String fieldName) throws IOException {
+      assertStats(fieldName);
+      return in.getMinPackedValue(fieldName);
+    }
+
+    @Override
+    public byte[] getMaxPackedValue(String fieldName) throws IOException {
+      assertStats(fieldName);
+      return in.getMaxPackedValue(fieldName);
+    }
+
+    @Override
+    public int getNumDimensions(String fieldName) throws IOException {
+      assertStats(fieldName);
+      return in.getNumDimensions(fieldName);
+    }
+
+    @Override
+    public int getBytesPerDimension(String fieldName) throws IOException {
+      assertStats(fieldName);
+      return in.getBytesPerDimension(fieldName);
+    }
+
+    @Override
+    public long size(String fieldName) {
+      assertStats(fieldName);
+      return in.size(fieldName);
+    }
+
+    @Override
+    public int getDocCount(String fieldName) {
+      assertStats(fieldName);
+      return in.getDocCount(fieldName);
+    }
+
+    private void assertStats(String fieldName) {
+      assert in.size(fieldName) >= 0;
+      assert in.getDocCount(fieldName) >= 0;
+      assert in.getDocCount(fieldName) <= in.size(fieldName);
+      assert in.getDocCount(fieldName) <= maxDoc;
+    }
+  }
+
+  static class AssertingPointsWriter extends PointsWriter {
+    private final PointsWriter in;
+
+    AssertingPointsWriter(SegmentWriteState writeState, PointsWriter in) {
+      this.in = in;
+    }
+    
+    @Override
+    public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {
+      if (fieldInfo.getPointDimensionCount() == 0) {
+        throw new IllegalArgumentException("writing field=\"" + fieldInfo.name + "\" but pointDimensionalCount is 0");
+      }
+      in.writeField(fieldInfo, values);
+    }
+
+    @Override
+    public void merge(MergeState mergeState) throws IOException {
+      in.merge(mergeState);
+    }
+
+    @Override
+    public void finish() throws IOException {
+      in.finish();
+    }
+
+    @Override
+    public void close() throws IOException {
+      in.close();
+      in.close(); // close again
+    }
+  }
+}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/CrankyCodec.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/CrankyCodec.java
index 314ff95..086a574 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/CrankyCodec.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/CrankyCodec.java
@@ -25,7 +25,7 @@ import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PointFormat;
+import org.apache.lucene.codecs.PointsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
@@ -92,8 +92,8 @@ public class CrankyCodec extends FilterCodec {
   }
 
   @Override
-  public PointFormat pointFormat() {
-    return new CrankyPointFormat(delegate.pointFormat(), random);
+  public PointsFormat pointsFormat() {
+    return new CrankyPointsFormat(delegate.pointsFormat(), random);
   }
 
   @Override
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/CrankyPointFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/CrankyPointFormat.java
deleted file mode 100644
index b663a80..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/CrankyPointFormat.java
+++ /dev/null
@@ -1,185 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.codecs.cranky;
-
-import java.io.IOException;
-import java.util.Random;
-
-import org.apache.lucene.codecs.PointFormat;
-import org.apache.lucene.codecs.PointReader;
-import org.apache.lucene.codecs.PointWriter;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-class CrankyPointFormat extends PointFormat {
-  PointFormat delegate;
-  Random random;
-  
-  CrankyPointFormat(PointFormat delegate, Random random) {
-    this.delegate = delegate;
-    this.random = random;
-  }
-
-  @Override
-  public PointWriter fieldsWriter(SegmentWriteState state) throws IOException {
-    return new CrankyPointWriter(delegate.fieldsWriter(state), random);
-  }
-
-  @Override
-  public PointReader fieldsReader(SegmentReadState state) throws IOException {
-    return new CrankyPointReader(delegate.fieldsReader(state), random);
-  }
-
-  static class CrankyPointWriter extends PointWriter {
-    final PointWriter delegate;
-    final Random random;
-
-    public CrankyPointWriter(PointWriter delegate, Random random) {
-      this.delegate = delegate;
-      this.random = random;
-    }
-
-    @Override
-    public void writeField(FieldInfo fieldInfo, PointReader values) throws IOException {
-      if (random.nextInt(100) == 0) {
-        throw new IOException("Fake IOException");
-      }  
-      delegate.writeField(fieldInfo, values);
-    }
-
-    @Override
-    public void finish() throws IOException {
-      if (random.nextInt(100) == 0) {
-        throw new IOException("Fake IOException");
-      }  
-      delegate.finish();
-      if (random.nextInt(100) == 0) {
-        throw new IOException("Fake IOException");
-      }  
-    }
-
-    @Override
-    public void merge(MergeState mergeState) throws IOException {
-      if (random.nextInt(100) == 0) {
-        throw new IOException("Fake IOException");
-      }  
-      delegate.merge(mergeState);
-      if (random.nextInt(100) == 0) {
-        throw new IOException("Fake IOException");
-      }  
-    }
-
-    @Override
-    public void close() throws IOException {
-      delegate.close();
-      if (random.nextInt(100) == 0) {
-        throw new IOException("Fake IOException");
-      }  
-    }
-  }
-
-  static class CrankyPointReader extends PointReader {
-    final PointReader delegate;
-    final Random random;
-    public CrankyPointReader(PointReader delegate, Random random) {
-      this.delegate = delegate;
-      this.random = random;
-    }
-
-    @Override
-    public void checkIntegrity() throws IOException {
-      if (random.nextInt(100) == 0) {
-        throw new IOException("Fake IOException");
-      }
-      delegate.checkIntegrity();
-      if (random.nextInt(100) == 0) {
-        throw new IOException("Fake IOException");
-      }  
-    }
-
-    @Override
-    public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
-      if (random.nextInt(100) == 0) {
-        throw new IOException("Fake IOException");
-      }
-      delegate.intersect(fieldName, visitor);
-      if (random.nextInt(100) == 0) {
-        throw new IOException("Fake IOException");
-      }  
-    }
-
-    @Override
-    public byte[] getMinPackedValue(String fieldName) throws IOException {
-      if (random.nextInt(100) == 0) {
-        throw new IOException("Fake IOException");
-      }
-      return delegate.getMinPackedValue(fieldName);
-    }
-
-    @Override
-    public byte[] getMaxPackedValue(String fieldName) throws IOException {
-      if (random.nextInt(100) == 0) {
-        throw new IOException("Fake IOException");
-      }
-      return delegate.getMaxPackedValue(fieldName);
-    }
-
-    @Override
-    public int getNumDimensions(String fieldName) throws IOException {
-      if (random.nextInt(100) == 0) {
-        throw new IOException("Fake IOException");
-      }
-      return delegate.getNumDimensions(fieldName);
-    }
-
-    @Override
-    public int getBytesPerDimension(String fieldName) throws IOException {
-      if (random.nextInt(100) == 0) {
-        throw new IOException("Fake IOException");
-      }
-      return delegate.getBytesPerDimension(fieldName);
-    }
-
-    @Override
-    public void close() throws IOException {
-      delegate.close();
-      if (random.nextInt(100) == 0) {
-        throw new IOException("Fake IOException");
-      }  
-    }
-
-    @Override
-    public long ramBytesUsed() {
-      return delegate.ramBytesUsed();
-    }
-
-    @Override
-    public long size(String fieldName) {
-      return delegate.size(fieldName);
-    }
-
-    @Override
-    public int getDocCount(String fieldName) {
-      return delegate.getDocCount(fieldName);
-    }
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/CrankyPointsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/CrankyPointsFormat.java
new file mode 100644
index 0000000..4d16461
--- /dev/null
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/CrankyPointsFormat.java
@@ -0,0 +1,185 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.cranky;
+
+import java.io.IOException;
+import java.util.Random;
+
+import org.apache.lucene.codecs.PointsFormat;
+import org.apache.lucene.codecs.PointsReader;
+import org.apache.lucene.codecs.PointsWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+class CrankyPointsFormat extends PointsFormat {
+  PointsFormat delegate;
+  Random random;
+  
+  CrankyPointsFormat(PointsFormat delegate, Random random) {
+    this.delegate = delegate;
+    this.random = random;
+  }
+
+  @Override
+  public PointsWriter fieldsWriter(SegmentWriteState state) throws IOException {
+    return new CrankyPointsWriter(delegate.fieldsWriter(state), random);
+  }
+
+  @Override
+  public PointsReader fieldsReader(SegmentReadState state) throws IOException {
+    return new CrankyPointsReader(delegate.fieldsReader(state), random);
+  }
+
+  static class CrankyPointsWriter extends PointsWriter {
+    final PointsWriter delegate;
+    final Random random;
+
+    public CrankyPointsWriter(PointsWriter delegate, Random random) {
+      this.delegate = delegate;
+      this.random = random;
+    }
+
+    @Override
+    public void writeField(FieldInfo fieldInfo, PointsReader values) throws IOException {
+      if (random.nextInt(100) == 0) {
+        throw new IOException("Fake IOException");
+      }  
+      delegate.writeField(fieldInfo, values);
+    }
+
+    @Override
+    public void finish() throws IOException {
+      if (random.nextInt(100) == 0) {
+        throw new IOException("Fake IOException");
+      }  
+      delegate.finish();
+      if (random.nextInt(100) == 0) {
+        throw new IOException("Fake IOException");
+      }  
+    }
+
+    @Override
+    public void merge(MergeState mergeState) throws IOException {
+      if (random.nextInt(100) == 0) {
+        throw new IOException("Fake IOException");
+      }  
+      delegate.merge(mergeState);
+      if (random.nextInt(100) == 0) {
+        throw new IOException("Fake IOException");
+      }  
+    }
+
+    @Override
+    public void close() throws IOException {
+      delegate.close();
+      if (random.nextInt(100) == 0) {
+        throw new IOException("Fake IOException");
+      }  
+    }
+  }
+
+  static class CrankyPointsReader extends PointsReader {
+    final PointsReader delegate;
+    final Random random;
+    public CrankyPointsReader(PointsReader delegate, Random random) {
+      this.delegate = delegate;
+      this.random = random;
+    }
+
+    @Override
+    public void checkIntegrity() throws IOException {
+      if (random.nextInt(100) == 0) {
+        throw new IOException("Fake IOException");
+      }
+      delegate.checkIntegrity();
+      if (random.nextInt(100) == 0) {
+        throw new IOException("Fake IOException");
+      }  
+    }
+
+    @Override
+    public void intersect(String fieldName, IntersectVisitor visitor) throws IOException {
+      if (random.nextInt(100) == 0) {
+        throw new IOException("Fake IOException");
+      }
+      delegate.intersect(fieldName, visitor);
+      if (random.nextInt(100) == 0) {
+        throw new IOException("Fake IOException");
+      }  
+    }
+
+    @Override
+    public byte[] getMinPackedValue(String fieldName) throws IOException {
+      if (random.nextInt(100) == 0) {
+        throw new IOException("Fake IOException");
+      }
+      return delegate.getMinPackedValue(fieldName);
+    }
+
+    @Override
+    public byte[] getMaxPackedValue(String fieldName) throws IOException {
+      if (random.nextInt(100) == 0) {
+        throw new IOException("Fake IOException");
+      }
+      return delegate.getMaxPackedValue(fieldName);
+    }
+
+    @Override
+    public int getNumDimensions(String fieldName) throws IOException {
+      if (random.nextInt(100) == 0) {
+        throw new IOException("Fake IOException");
+      }
+      return delegate.getNumDimensions(fieldName);
+    }
+
+    @Override
+    public int getBytesPerDimension(String fieldName) throws IOException {
+      if (random.nextInt(100) == 0) {
+        throw new IOException("Fake IOException");
+      }
+      return delegate.getBytesPerDimension(fieldName);
+    }
+
+    @Override
+    public void close() throws IOException {
+      delegate.close();
+      if (random.nextInt(100) == 0) {
+        throw new IOException("Fake IOException");
+      }  
+    }
+
+    @Override
+    public long ramBytesUsed() {
+      return delegate.ramBytesUsed();
+    }
+
+    @Override
+    public long size(String fieldName) {
+      return delegate.size(fieldName);
+    }
+
+    @Override
+    public int getDocCount(String fieldName) {
+      return delegate.getDocCount(fieldName);
+    }
+  }
+}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/BasePointFormatTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/index/BasePointFormatTestCase.java
deleted file mode 100644
index 16a6b6b..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/index/BasePointFormatTestCase.java
+++ /dev/null
@@ -1,950 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.index;
-
-import java.io.IOException;
-import java.math.BigInteger;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.BitSet;
-import java.util.List;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.document.BinaryPoint;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.IntPoint;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.PointValues.IntersectVisitor;
-import org.apache.lucene.index.PointValues.Relation;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.NumericUtils;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.TestUtil;
-
-/**
- * Abstract class to do basic tests for a points format.
- * NOTE: This test focuses on the points impl, nothing else.
- * The [stretch] goal is for this test to be
- * so thorough in testing a new PointFormat that if this
- * test passes, then all Lucene/Solr tests should also pass.  Ie,
- * if there is some bug in a given PointFormat that this
- * test fails to catch then this test needs to be improved! */
-public abstract class BasePointFormatTestCase extends BaseIndexFileFormatTestCase {
-
-  @Override
-  protected void addRandomFields(Document doc) {
-    final int numValues = random().nextInt(3);
-    for (int i = 0; i < numValues; i++) {
-      doc.add(new IntPoint("f", random().nextInt()));
-    }
-  }
-  
-  public void testBasic() throws Exception {
-    Directory dir = getDirectory(20);
-    IndexWriterConfig iwc = newIndexWriterConfig();
-    iwc.setMergePolicy(newLogMergePolicy());
-    IndexWriter w = new IndexWriter(dir, iwc);
-    byte[] point = new byte[4];
-    for(int i=0;i<20;i++) {
-      Document doc = new Document();
-      NumericUtils.intToSortableBytes(i, point, 0);
-      doc.add(new BinaryPoint("dim", point));
-      w.addDocument(doc);
-    }
-    w.forceMerge(1);
-    w.close();
-
-    DirectoryReader r = DirectoryReader.open(dir);
-    LeafReader sub = getOnlySegmentReader(r);
-    PointValues values = sub.getPointValues();
-
-    // Simple test: make sure intersect can visit every doc:
-    BitSet seen = new BitSet();
-    values.intersect("dim",
-                     new IntersectVisitor() {
-                       @Override
-                       public Relation compare(byte[] minPacked, byte[] maxPacked) {
-                         return Relation.CELL_CROSSES_QUERY;
-                       }
-                       public void visit(int docID) {
-                         throw new IllegalStateException();
-                       }
-                       public void visit(int docID, byte[] packedValue) {
-                         seen.set(docID);
-                         assertEquals(docID, NumericUtils.sortableBytesToInt(packedValue, 0));
-                       }
-                     });
-    assertEquals(20, seen.cardinality());
-    IOUtils.close(r, dir);
-  }
-
-  public void testMerge() throws Exception {
-    Directory dir = getDirectory(20);
-    IndexWriterConfig iwc = newIndexWriterConfig();
-    iwc.setMergePolicy(newLogMergePolicy());
-    IndexWriter w = new IndexWriter(dir, iwc);
-    byte[] point = new byte[4];
-    for(int i=0;i<20;i++) {
-      Document doc = new Document();
-      NumericUtils.intToSortableBytes(i, point, 0);
-      doc.add(new BinaryPoint("dim", point));
-      w.addDocument(doc);
-      if (i == 10) {
-        w.commit();
-      }
-    }
-    w.forceMerge(1);
-    w.close();
-
-    DirectoryReader r = DirectoryReader.open(dir);
-    LeafReader sub = getOnlySegmentReader(r);
-    PointValues values = sub.getPointValues();
-
-    // Simple test: make sure intersect can visit every doc:
-    BitSet seen = new BitSet();
-    values.intersect("dim",
-                     new IntersectVisitor() {
-                       @Override
-                       public Relation compare(byte[] minPacked, byte[] maxPacked) {
-                         return Relation.CELL_CROSSES_QUERY;
-                       }
-                       public void visit(int docID) {
-                         throw new IllegalStateException();
-                       }
-                       public void visit(int docID, byte[] packedValue) {
-                         seen.set(docID);
-                         assertEquals(docID, NumericUtils.sortableBytesToInt(packedValue, 0));
-                       }
-                     });
-    assertEquals(20, seen.cardinality());
-    IOUtils.close(r, dir);
-  }
-
-  public void testAllPointDocsDeletedInSegment() throws Exception {
-    Directory dir = getDirectory(20);
-    IndexWriterConfig iwc = newIndexWriterConfig();
-    IndexWriter w = new IndexWriter(dir, iwc);
-    byte[] point = new byte[4];
-    for(int i=0;i<10;i++) {
-      Document doc = new Document();
-      NumericUtils.intToSortableBytes(i, point, 0);
-      doc.add(new BinaryPoint("dim", point));
-      doc.add(new NumericDocValuesField("id", i));
-      doc.add(newStringField("x", "x", Field.Store.NO));
-      w.addDocument(doc);
-    }
-    w.addDocument(new Document());
-    w.deleteDocuments(new Term("x", "x"));
-    if (random().nextBoolean()) {
-      w.forceMerge(1);
-    }
-    w.close();
-    DirectoryReader r = DirectoryReader.open(dir);
-    assertEquals(1, r.numDocs());
-    Bits liveDocs = MultiFields.getLiveDocs(r);
-
-    for(LeafReaderContext ctx : r.leaves()) {
-      PointValues values = ctx.reader().getPointValues();
-      NumericDocValues idValues = ctx.reader().getNumericDocValues("id");
-      if (values != null) {
-        BitSet seen = new BitSet();
-        values.intersect("dim",
-                         new IntersectVisitor() {
-                           @Override
-                           public Relation compare(byte[] minPacked, byte[] maxPacked) {
-                             return Relation.CELL_CROSSES_QUERY;
-                           }
-                           public void visit(int docID) {
-                             throw new IllegalStateException();
-                           }
-                           public void visit(int docID, byte[] packedValue) {
-                             if (liveDocs.get(docID)) {
-                               seen.set(docID);
-                             }
-                             assertEquals(idValues.get(docID), NumericUtils.sortableBytesToInt(packedValue, 0));
-                           }
-                         });
-        assertEquals(0, seen.cardinality());
-      }
-    }
-    IOUtils.close(r, dir);
-  }
-
-  /** Make sure we close open files, delete temp files, etc., on exception */
-  public void testWithExceptions() throws Exception {
-    int numDocs = atLeast(10000);
-    int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
-    int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
-
-    byte[][][] docValues = new byte[numDocs][][];
-
-    for(int docID=0;docID<numDocs;docID++) {
-      byte[][] values = new byte[numDims][];
-      for(int dim=0;dim<numDims;dim++) {
-        values[dim] = new byte[numBytesPerDim];
-        random().nextBytes(values[dim]);
-      }
-      docValues[docID] = values;
-    }
-
-    // Keep retrying until we 1) we allow a big enough heap, and 2) we hit a random IOExc from MDW:
-    boolean done = false;
-    while (done == false) {
-      try (MockDirectoryWrapper dir = newMockFSDirectory(createTempDir())) {
-        try {
-          dir.setRandomIOExceptionRate(0.05);
-          dir.setRandomIOExceptionRateOnOpen(0.05);
-          verify(dir, docValues, null, numDims, numBytesPerDim, true);
-        } catch (IllegalStateException ise) {
-          if (ise.getMessage().contains("this writer hit an unrecoverable error")) {
-            Throwable cause = ise.getCause();
-            if (cause != null && cause.getMessage().contains("a random IOException")) {
-              done = true;
-            } else {
-              throw ise;
-            }
-          } else {
-            throw ise;
-          }
-        } catch (AssertionError ae) {
-          if (ae.getMessage().contains("does not exist; files=")) {
-            // OK: likely we threw the random IOExc when IW was asserting the commit files exist
-            done = true;
-          } else {
-            throw ae;
-          }
-        } catch (IllegalArgumentException iae) {
-          // This just means we got a too-small maxMB for the maxPointsInLeafNode; just retry w/ more heap
-          assertTrue(iae.getMessage().contains("either increase maxMBSortInHeap or decrease maxPointsInLeafNode"));
-        } catch (IOException ioe) {
-          String message = ioe.getMessage();
-          if (message.contains("a random IOException") || message.contains("background merge hit exception")) {
-            // BKDWriter should fully clean up after itself:
-            done = true;
-          } else {
-            throw ioe;
-          }
-        }
-      }
-    }
-  }
-
-  public void testMultiValued() throws Exception {
-    int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
-    int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
-
-    int numDocs = atLeast(1000);
-    List<byte[][]> docValues = new ArrayList<>();
-    List<Integer> docIDs = new ArrayList<>();
-
-    for(int docID=0;docID<numDocs;docID++) {
-      int numValuesInDoc = TestUtil.nextInt(random(), 1, 5);
-      for(int ord=0;ord<numValuesInDoc;ord++) {
-        docIDs.add(docID);
-        byte[][] values = new byte[numDims][];
-        for(int dim=0;dim<numDims;dim++) {
-          values[dim] = new byte[numBytesPerDim];
-          random().nextBytes(values[dim]);
-        }
-        docValues.add(values);
-      }
-    }
-
-    byte[][][] docValuesArray = docValues.toArray(new byte[docValues.size()][][]);
-    int[] docIDsArray = new int[docIDs.size()];
-    for(int i=0;i<docIDsArray.length;i++) {
-      docIDsArray[i] = docIDs.get(i);
-    }
-
-    verify(docValuesArray, docIDsArray, numDims, numBytesPerDim);
-  }
-
-  public void testAllEqual() throws Exception {
-    int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
-    int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
-
-    int numDocs = atLeast(1000);
-    byte[][][] docValues = new byte[numDocs][][];
-
-    for(int docID=0;docID<numDocs;docID++) {
-      if (docID == 0) {
-        byte[][] values = new byte[numDims][];
-        for(int dim=0;dim<numDims;dim++) {
-          values[dim] = new byte[numBytesPerDim];
-          random().nextBytes(values[dim]);
-        }
-        docValues[docID] = values;
-      } else {
-        docValues[docID] = docValues[0];
-      }
-    }
-
-    verify(docValues, null, numDims, numBytesPerDim);
-  }
-
-  public void testOneDimEqual() throws Exception {
-    int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
-    int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
-
-    int numDocs = atLeast(1000);
-    int theEqualDim = random().nextInt(numDims);
-    byte[][][] docValues = new byte[numDocs][][];
-
-    for(int docID=0;docID<numDocs;docID++) {
-      byte[][] values = new byte[numDims][];
-      for(int dim=0;dim<numDims;dim++) {
-        values[dim] = new byte[numBytesPerDim];
-        random().nextBytes(values[dim]);
-      }
-      docValues[docID] = values;
-      if (docID > 0) {
-        docValues[docID][theEqualDim] = docValues[0][theEqualDim];
-      }
-    }
-
-    verify(docValues, null, numDims, numBytesPerDim);
-  }
-
-  // Tests on N-dimensional points where each dimension is a BigInteger
-  public void testBigIntNDims() throws Exception {
-
-    int numDocs = atLeast(1000);
-    try (Directory dir = getDirectory(numDocs)) {
-      int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
-      int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
-      IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
-      // We rely on docIDs not changing:
-      iwc.setMergePolicy(newLogMergePolicy());
-      RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-      BigInteger[][] docs = new BigInteger[numDocs][];
-
-      for(int docID=0;docID<numDocs;docID++) {
-        BigInteger[] values = new BigInteger[numDims];
-        if (VERBOSE) {
-          System.out.println("  docID=" + docID);
-        }
-        byte[][] bytes = new byte[numDims][];
-        for(int dim=0;dim<numDims;dim++) {
-          values[dim] = randomBigInt(numBytesPerDim);
-          bytes[dim] = new byte[numBytesPerDim];
-          NumericUtils.bigIntToSortableBytes(values[dim], numBytesPerDim, bytes[dim], 0);
-          if (VERBOSE) {
-            System.out.println("    " + dim + " -> " + values[dim]);
-          }
-        }
-        docs[docID] = values;
-        Document doc = new Document();
-        doc.add(new BinaryPoint("field", bytes));
-        w.addDocument(doc);
-      }
-
-      DirectoryReader r = w.getReader();
-      w.close();
-
-      int iters = atLeast(100);
-      for(int iter=0;iter<iters;iter++) {
-        if (VERBOSE) {
-          System.out.println("\nTEST: iter=" + iter);
-        }
-
-        // Random N dims rect query:
-        BigInteger[] queryMin = new BigInteger[numDims];
-        BigInteger[] queryMax = new BigInteger[numDims];    
-        for(int dim=0;dim<numDims;dim++) {
-          queryMin[dim] = randomBigInt(numBytesPerDim);
-          queryMax[dim] = randomBigInt(numBytesPerDim);
-          if (queryMin[dim].compareTo(queryMax[dim]) > 0) {
-            BigInteger x = queryMin[dim];
-            queryMin[dim] = queryMax[dim];
-            queryMax[dim] = x;
-          }
-          if (VERBOSE) {
-            System.out.println("  " + dim + "\n    min=" + queryMin[dim] + "\n    max=" + queryMax[dim]);
-          }
-        }
-
-        final BitSet hits = new BitSet();
-        for(LeafReaderContext ctx : r.leaves()) {
-          PointValues dimValues = ctx.reader().getPointValues();
-          if (dimValues == null) {
-            continue;
-          }
-
-          final int docBase = ctx.docBase;
-          
-          dimValues.intersect("field", new IntersectVisitor() {
-              @Override
-              public void visit(int docID) {
-                hits.set(docBase+docID);
-                //System.out.println("visit docID=" + docID);
-              }
-
-              @Override
-              public void visit(int docID, byte[] packedValue) {
-                //System.out.println("visit check docID=" + docID);
-                for(int dim=0;dim<numDims;dim++) {
-                  BigInteger x = NumericUtils.sortableBytesToBigInt(packedValue, dim * numBytesPerDim, numBytesPerDim);
-                  if (x.compareTo(queryMin[dim]) < 0 || x.compareTo(queryMax[dim]) > 0) {
-                    //System.out.println("  no");
-                    return;
-                  }
-                }
-
-                //System.out.println("  yes");
-                hits.set(docBase+docID);
-              }
-
-              @Override
-              public Relation compare(byte[] minPacked, byte[] maxPacked) {
-                boolean crosses = false;
-                for(int dim=0;dim<numDims;dim++) {
-                  BigInteger min = NumericUtils.sortableBytesToBigInt(minPacked, dim * numBytesPerDim, numBytesPerDim);
-                  BigInteger max = NumericUtils.sortableBytesToBigInt(maxPacked, dim * numBytesPerDim, numBytesPerDim);
-                  assert max.compareTo(min) >= 0;
-
-                  if (max.compareTo(queryMin[dim]) < 0 || min.compareTo(queryMax[dim]) > 0) {
-                    return Relation.CELL_OUTSIDE_QUERY;
-                  } else if (min.compareTo(queryMin[dim]) < 0 || max.compareTo(queryMax[dim]) > 0) {
-                    crosses = true;
-                  }
-                }
-
-                if (crosses) {
-                  return Relation.CELL_CROSSES_QUERY;
-                } else {
-                  return Relation.CELL_INSIDE_QUERY;
-                }
-              }
-            });
-        }
-
-        for(int docID=0;docID<numDocs;docID++) {
-          BigInteger[] docValues = docs[docID];
-          boolean expected = true;
-          for(int dim=0;dim<numDims;dim++) {
-            BigInteger x = docValues[dim];
-            if (x.compareTo(queryMin[dim]) < 0 || x.compareTo(queryMax[dim]) > 0) {
-              expected = false;
-              break;
-            }
-          }
-          boolean actual = hits.get(docID);
-          assertEquals("docID=" + docID, expected, actual);
-        }
-      }
-      r.close();
-      }
-  }
-
-  public void testRandomBinaryTiny() throws Exception {
-    doTestRandomBinary(10);
-  }
-
-  public void testRandomBinaryMedium() throws Exception {
-    doTestRandomBinary(10000);
-  }
-
-  @Nightly
-  public void testRandomBinaryBig() throws Exception {
-    assumeFalse("too slow with SimpleText", Codec.getDefault().getName().equals("SimpleText"));
-    doTestRandomBinary(200000);
-  }
-
-  private void doTestRandomBinary(int count) throws Exception {
-    int numDocs = TestUtil.nextInt(random(), count, count*2);
-    int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
-    int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
-
-    byte[][][] docValues = new byte[numDocs][][];
-
-    for(int docID=0;docID<numDocs;docID++) {
-      byte[][] values = new byte[numDims][];
-      for(int dim=0;dim<numDims;dim++) {
-        values[dim] = new byte[numBytesPerDim];
-        // TODO: sometimes test on a "small" volume too, so we test the high density cases, higher chance of boundary, etc. cases:
-        random().nextBytes(values[dim]);
-      }
-      docValues[docID] = values;
-    }
-
-    verify(docValues, null, numDims, numBytesPerDim);
-  }
-
-  /** docIDs can be null, for the single valued case, else it maps value to docID, but all values for one doc must be adjacent */
-  private void verify(byte[][][] docValues, int[] docIDs, int numDims, int numBytesPerDim) throws Exception {
-    try (Directory dir = getDirectory(docValues.length)) {
-      while (true) {
-        try {
-          verify(dir, docValues, docIDs, numDims, numBytesPerDim, false);
-          return;
-        } catch (IllegalArgumentException iae) {
-          // This just means we got a too-small maxMB for the maxPointsInLeafNode; just retry
-          assertTrue(iae.getMessage().contains("either increase maxMBSortInHeap or decrease maxPointsInLeafNode"));
-        }
-      }
-    }
-  }
-
-  private void verify(Directory dir, byte[][][] docValues, int[] ids, int numDims, int numBytesPerDim, boolean expectExceptions) throws Exception {
-    int numValues = docValues.length;
-    if (VERBOSE) {
-      System.out.println("TEST: numValues=" + numValues + " numDims=" + numDims + " numBytesPerDim=" + numBytesPerDim);
-    }
-
-    // RandomIndexWriter is too slow:
-    boolean useRealWriter = docValues.length > 10000;
-
-    IndexWriterConfig iwc;
-    if (useRealWriter) {
-      iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-    } else {
-      iwc = newIndexWriterConfig();
-    }
-
-    if (expectExceptions) {
-      MergeScheduler ms = iwc.getMergeScheduler();
-      if (ms instanceof ConcurrentMergeScheduler) {
-        ((ConcurrentMergeScheduler) ms).setSuppressExceptions();
-      }
-    }
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    DirectoryReader r = null;
-
-    // Compute actual min/max values:
-    byte[][] expectedMinValues = new byte[numDims][];
-    byte[][] expectedMaxValues = new byte[numDims][];
-    for(int ord=0;ord<docValues.length;ord++) {
-      for(int dim=0;dim<numDims;dim++) {
-        if (ord == 0) {
-          expectedMinValues[dim] = new byte[numBytesPerDim];
-          System.arraycopy(docValues[ord][dim], 0, expectedMinValues[dim], 0, numBytesPerDim);
-          expectedMaxValues[dim] = new byte[numBytesPerDim];
-          System.arraycopy(docValues[ord][dim], 0, expectedMaxValues[dim], 0, numBytesPerDim);
-        } else {
-          // TODO: it's cheating that we use StringHelper.compare for "truth": what if it's buggy?
-          if (StringHelper.compare(numBytesPerDim, docValues[ord][dim], 0, expectedMinValues[dim], 0) < 0) {
-            System.arraycopy(docValues[ord][dim], 0, expectedMinValues[dim], 0, numBytesPerDim);
-          }
-          if (StringHelper.compare(numBytesPerDim, docValues[ord][dim], 0, expectedMaxValues[dim], 0) > 0) {
-            System.arraycopy(docValues[ord][dim], 0, expectedMaxValues[dim], 0, numBytesPerDim);
-          }
-        }
-      }
-    }
-
-    // 20% of the time we add into a separate directory, then at some point use
-    // addIndexes to bring the indexed point values to the main directory:
-    Directory saveDir;
-    RandomIndexWriter saveW;
-    int addIndexesAt;
-    if (random().nextInt(5) == 1) {
-      saveDir = dir;
-      saveW = w;
-      dir = getDirectory(numValues);
-      if (useRealWriter) {
-        iwc = new IndexWriterConfig(new MockAnalyzer(random()));
-      } else {
-        iwc = newIndexWriterConfig();
-      }
-      if (expectExceptions) {
-        MergeScheduler ms = iwc.getMergeScheduler();
-        if (ms instanceof ConcurrentMergeScheduler) {
-          ((ConcurrentMergeScheduler) ms).setSuppressExceptions();
-        }
-      }
-      w = new RandomIndexWriter(random(), dir, iwc);
-      addIndexesAt = TestUtil.nextInt(random(), 1, numValues-1);
-    } else {
-      saveW = null;
-      saveDir = null;
-      addIndexesAt = 0;
-    }
-
-    try {
-
-      Document doc = null;
-      int lastID = -1;
-      for(int ord=0;ord<numValues;ord++) {
-        int id;
-        if (ids == null) {
-          id = ord;
-        } else {
-          id = ids[ord];
-        }
-        if (id != lastID) {
-          if (doc != null) {
-            if (useRealWriter) {
-              w.w.addDocument(doc);
-            } else {
-              w.addDocument(doc);
-            }
-          }
-          doc = new Document();
-          doc.add(new NumericDocValuesField("id", id));
-        }
-        doc.add(new BinaryPoint("field", docValues[ord]));
-        lastID = id;
-
-        if (random().nextInt(30) == 17) {
-          // randomly index some documents without this field
-          if (useRealWriter) {
-            w.w.addDocument(new Document());
-          } else {
-            w.addDocument(new Document());
-          }
-          if (VERBOSE) {
-            System.out.println("add empty doc");
-          }
-        }
-
-        if (random().nextInt(30) == 17) {
-          // randomly index some documents with this field, but we will delete them:
-          Document xdoc = new Document();
-          xdoc.add(new BinaryPoint("field", docValues[ord]));
-          xdoc.add(new StringField("nukeme", "yes", Field.Store.NO));
-          if (useRealWriter) {
-            w.w.addDocument(xdoc);
-          } else {
-            w.addDocument(xdoc);
-          }
-          if (VERBOSE) {
-            System.out.println("add doc doc-to-delete");
-          }
-
-          if (random().nextInt(5) == 1) {
-            if (useRealWriter) {
-              w.w.deleteDocuments(new Term("nukeme", "yes"));
-            } else {
-              w.deleteDocuments(new Term("nukeme", "yes"));
-            }
-          }
-        }
-
-        if (VERBOSE) {
-          System.out.println("  ord=" + ord + " id=" + id);
-          for(int dim=0;dim<numDims;dim++) {
-            System.out.println("    dim=" + dim + " value=" + new BytesRef(docValues[ord][dim]));
-          }
-        }
-
-        if (saveW != null && ord >= addIndexesAt) {
-          switchIndex(w, dir, saveW);
-          w = saveW;
-          dir = saveDir;
-          saveW = null;
-          saveDir = null;
-        }
-      }
-      w.addDocument(doc);
-      w.deleteDocuments(new Term("nukeme", "yes"));
-
-      if (random().nextBoolean()) {
-        if (VERBOSE) {
-          System.out.println("\nTEST: now force merge");
-        }
-        w.forceMerge(1);
-      }
-
-      r = w.getReader();
-      w.close();
-
-      if (VERBOSE) {
-        System.out.println("TEST: reader=" + r);
-      }
-
-      NumericDocValues idValues = MultiDocValues.getNumericValues(r, "id");
-      Bits liveDocs = MultiFields.getLiveDocs(r);
-
-      // Verify min/max values are correct:
-      byte[] minValues = new byte[numDims*numBytesPerDim];
-      Arrays.fill(minValues, (byte) 0xff);
-
-      byte[] maxValues = new byte[numDims*numBytesPerDim];
-
-      for(LeafReaderContext ctx : r.leaves()) {
-        PointValues dimValues = ctx.reader().getPointValues();
-        if (dimValues == null) {
-          continue;
-        }
-
-        byte[] leafMinValues = dimValues.getMinPackedValue("field");
-        byte[] leafMaxValues = dimValues.getMaxPackedValue("field");
-        for(int dim=0;dim<numDims;dim++) {
-          if (StringHelper.compare(numBytesPerDim, leafMinValues, dim*numBytesPerDim, minValues, dim*numBytesPerDim) < 0) {
-            System.arraycopy(leafMinValues, dim*numBytesPerDim, minValues, dim*numBytesPerDim, numBytesPerDim);
-          }
-          if (StringHelper.compare(numBytesPerDim, leafMaxValues, dim*numBytesPerDim, maxValues, dim*numBytesPerDim) > 0) {
-            System.arraycopy(leafMaxValues, dim*numBytesPerDim, maxValues, dim*numBytesPerDim, numBytesPerDim);
-          }
-        }
-      }
-
-      byte[] scratch = new byte[numBytesPerDim];
-      for(int dim=0;dim<numDims;dim++) {
-        System.arraycopy(minValues, dim*numBytesPerDim, scratch, 0, numBytesPerDim);
-        //System.out.println("dim=" + dim + " expectedMin=" + new BytesRef(expectedMinValues[dim]) + " min=" + new BytesRef(scratch));
-        assertTrue(Arrays.equals(expectedMinValues[dim], scratch));
-        System.arraycopy(maxValues, dim*numBytesPerDim, scratch, 0, numBytesPerDim);
-        //System.out.println("dim=" + dim + " expectedMax=" + new BytesRef(expectedMaxValues[dim]) + " max=" + new BytesRef(scratch));
-        assertTrue(Arrays.equals(expectedMaxValues[dim], scratch));
-      }
-
-      int iters = atLeast(100);
-      for(int iter=0;iter<iters;iter++) {
-        if (VERBOSE) {
-          System.out.println("\nTEST: iter=" + iter);
-        }
-
-        // Random N dims rect query:
-        byte[][] queryMin = new byte[numDims][];
-        byte[][] queryMax = new byte[numDims][];    
-        for(int dim=0;dim<numDims;dim++) {    
-          queryMin[dim] = new byte[numBytesPerDim];
-          random().nextBytes(queryMin[dim]);
-          queryMax[dim] = new byte[numBytesPerDim];
-          random().nextBytes(queryMax[dim]);
-          if (StringHelper.compare(numBytesPerDim, queryMin[dim], 0, queryMax[dim], 0) > 0) {
-            byte[] x = queryMin[dim];
-            queryMin[dim] = queryMax[dim];
-            queryMax[dim] = x;
-          }
-        }
-
-        if (VERBOSE) {
-          for(int dim=0;dim<numDims;dim++) {
-            System.out.println("  dim=" + dim + "\n    queryMin=" + new BytesRef(queryMin[dim]) + "\n    queryMax=" + new BytesRef(queryMax[dim]));
-          }
-        }
-
-        final BitSet hits = new BitSet();
-
-        for(LeafReaderContext ctx : r.leaves()) {
-          PointValues dimValues = ctx.reader().getPointValues();
-          if (dimValues == null) {
-            continue;
-          }
-
-          final int docBase = ctx.docBase;
-
-          dimValues.intersect("field", new PointValues.IntersectVisitor() {
-              @Override
-              public void visit(int docID) {
-                if (liveDocs == null || liveDocs.get(docBase+docID)) {
-                  hits.set((int) idValues.get(docBase+docID));
-                }
-                //System.out.println("visit docID=" + docID);
-              }
-
-              @Override
-              public void visit(int docID, byte[] packedValue) {
-                if (liveDocs != null && liveDocs.get(docBase+docID) == false) {
-                  return;
-                }
-
-                //System.out.println("visit check docID=" + docID + " id=" + idValues.get(docID));
-                for(int dim=0;dim<numDims;dim++) {
-                  //System.out.println("  dim=" + dim + " value=" + new BytesRef(packedValue, dim*numBytesPerDim, numBytesPerDim));
-                  if (StringHelper.compare(numBytesPerDim, packedValue, dim*numBytesPerDim, queryMin[dim], 0) < 0 ||
-                      StringHelper.compare(numBytesPerDim, packedValue, dim*numBytesPerDim, queryMax[dim], 0) > 0) {
-                    //System.out.println("  no");
-                    return;
-                  }
-                }
-
-                //System.out.println("  yes");
-                hits.set((int) idValues.get(docBase+docID));
-              }
-
-              @Override
-              public Relation compare(byte[] minPacked, byte[] maxPacked) {
-                boolean crosses = false;
-                //System.out.println("compare");
-                for(int dim=0;dim<numDims;dim++) {
-                  if (StringHelper.compare(numBytesPerDim, maxPacked, dim*numBytesPerDim, queryMin[dim], 0) < 0 ||
-                      StringHelper.compare(numBytesPerDim, minPacked, dim*numBytesPerDim, queryMax[dim], 0) > 0) {
-                    //System.out.println("  query_outside_cell");
-                    return Relation.CELL_OUTSIDE_QUERY;
-                  } else if (StringHelper.compare(numBytesPerDim, minPacked, dim*numBytesPerDim, queryMin[dim], 0) < 0 ||
-                             StringHelper.compare(numBytesPerDim, maxPacked, dim*numBytesPerDim, queryMax[dim], 0) > 0) {
-                    crosses = true;
-                  }
-                }
-
-                if (crosses) {
-                  //System.out.println("  query_crosses_cell");
-                  return Relation.CELL_CROSSES_QUERY;
-                } else {
-                  //System.out.println("  cell_inside_query");
-                  return Relation.CELL_INSIDE_QUERY;
-                }
-              }
-            });
-        }
-
-        BitSet expected = new BitSet();
-        for(int ord=0;ord<numValues;ord++) {
-          boolean matches = true;
-          for(int dim=0;dim<numDims;dim++) {
-            byte[] x = docValues[ord][dim];
-            if (StringHelper.compare(numBytesPerDim, x, 0, queryMin[dim], 0) < 0 ||
-                StringHelper.compare(numBytesPerDim, x, 0, queryMax[dim], 0) > 0) {
-              matches = false;
-              break;
-            }
-          }
-
-          if (matches) {
-            int id;
-            if (ids == null) {
-              id = ord;
-            } else {
-              id = ids[ord];
-            }
-            expected.set(id);
-          }
-        }
-
-        int limit = Math.max(expected.length(), hits.length());
-        int failCount = 0;
-        int successCount = 0;
-        for(int id=0;id<limit;id++) {
-          if (expected.get(id) != hits.get(id)) {
-            System.out.println("FAIL: id=" + id);
-            failCount++;
-          } else {
-            successCount++;
-          }
-        }
-
-        if (failCount != 0) {
-          for(int docID=0;docID<r.maxDoc();docID++) {
-            System.out.println("  docID=" + docID + " id=" + idValues.get(docID));
-          }
-
-          fail(failCount + " docs failed; " + successCount + " docs succeeded");
-        }
-      }
-    } finally {
-      IOUtils.closeWhileHandlingException(r, w, saveW, saveDir == null ? null : dir);
-    }
-  }
-
-  public void testAddIndexes() throws IOException {
-    Directory dir1 = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir1);
-    Document doc = new Document();
-    doc.add(new IntPoint("int1", 17));
-    w.addDocument(doc);
-    doc = new Document();
-    doc.add(new IntPoint("int2", 42));
-    w.addDocument(doc);
-    w.close();
-
-    // Different field number assigments:
-    Directory dir2 = newDirectory();
-    w = new RandomIndexWriter(random(), dir2);
-    doc = new Document();
-    doc.add(new IntPoint("int2", 42));
-    w.addDocument(doc);
-    doc = new Document();
-    doc.add(new IntPoint("int1", 17));
-    w.addDocument(doc);
-    w.close();
-
-    Directory dir = newDirectory();
-    w = new RandomIndexWriter(random(), dir);
-    w.addIndexes(new Directory[] {dir1, dir2});
-    w.forceMerge(1);
-
-    DirectoryReader r = w.getReader();
-    IndexSearcher s = newSearcher(r, false);
-    assertEquals(2, s.count(IntPoint.newExactQuery("int1", 17)));
-    assertEquals(2, s.count(IntPoint.newExactQuery("int2", 42)));
-    r.close();
-    w.close();
-    dir.close();
-    dir1.close();
-    dir2.close();
-  }
-
-  private void switchIndex(RandomIndexWriter w, Directory dir, RandomIndexWriter saveW) throws IOException {
-    if (random().nextBoolean()) {
-      // Add via readers:
-      try (DirectoryReader r = w.getReader()) {
-        if (random().nextBoolean()) {
-          // Add via CodecReaders:
-          List<CodecReader> subs = new ArrayList<>();
-          for (LeafReaderContext context : r.leaves()) {
-            subs.add((CodecReader) context.reader());
-          }
-          if (VERBOSE) {
-            System.out.println("TEST: now use addIndexes(CodecReader[]) to switch writers");
-          }
-          saveW.addIndexes(subs.toArray(new CodecReader[subs.size()]));
-        } else {
-          if (VERBOSE) {
-            System.out.println("TEST: now use TestUtil.addIndexesSlowly(DirectoryReader[]) to switch writers");
-          }
-          TestUtil.addIndexesSlowly(saveW.w, r);
-        }
-      }
-    } else {
-      // Add via directory:
-      if (VERBOSE) {
-        System.out.println("TEST: now use addIndexes(Directory[]) to switch writers");
-      }
-      w.close();
-      saveW.addIndexes(new Directory[] {dir});
-    }
-    w.close();
-    dir.close();
-  }
-
-  private BigInteger randomBigInt(int numBytes) {
-    BigInteger x = new BigInteger(numBytes*8-1, random());
-    if (random().nextBoolean()) {
-      x = x.negate();
-    }
-    return x;
-  }
-
-  private Directory getDirectory(int numPoints) throws IOException {
-    Directory dir;
-    if (numPoints > 100000) {
-      dir = newFSDirectory(createTempDir("TestBKDTree"));
-    } else {
-      dir = newDirectory();
-    }
-    //dir = FSDirectory.open(createTempDir());
-    return dir;
-  }
-
-  @Override
-  protected boolean mergeIsStable() {
-    // suppress this test from base class: merges for BKD trees are not stable because the tree created by merge will have a different
-    // structure than the tree created by adding points separately
-    return false;
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/BasePointsFormatTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/index/BasePointsFormatTestCase.java
new file mode 100644
index 0000000..24753aa
--- /dev/null
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/BasePointsFormatTestCase.java
@@ -0,0 +1,950 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.index;
+
+import java.io.IOException;
+import java.math.BigInteger;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.BitSet;
+import java.util.List;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.document.BinaryPoint;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.IntPoint;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.PointValues.IntersectVisitor;
+import org.apache.lucene.index.PointValues.Relation;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.NumericUtils;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.TestUtil;
+
+/**
+ * Abstract class to do basic tests for a points format.
+ * NOTE: This test focuses on the points impl, nothing else.
+ * The [stretch] goal is for this test to be
+ * so thorough in testing a new PointsFormat that if this
+ * test passes, then all Lucene/Solr tests should also pass.  Ie,
+ * if there is some bug in a given PointsFormat that this
+ * test fails to catch then this test needs to be improved! */
+public abstract class BasePointsFormatTestCase extends BaseIndexFileFormatTestCase {
+
+  @Override
+  protected void addRandomFields(Document doc) {
+    final int numValues = random().nextInt(3);
+    for (int i = 0; i < numValues; i++) {
+      doc.add(new IntPoint("f", random().nextInt()));
+    }
+  }
+  
+  public void testBasic() throws Exception {
+    Directory dir = getDirectory(20);
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    iwc.setMergePolicy(newLogMergePolicy());
+    IndexWriter w = new IndexWriter(dir, iwc);
+    byte[] point = new byte[4];
+    for(int i=0;i<20;i++) {
+      Document doc = new Document();
+      NumericUtils.intToSortableBytes(i, point, 0);
+      doc.add(new BinaryPoint("dim", point));
+      w.addDocument(doc);
+    }
+    w.forceMerge(1);
+    w.close();
+
+    DirectoryReader r = DirectoryReader.open(dir);
+    LeafReader sub = getOnlySegmentReader(r);
+    PointValues values = sub.getPointValues();
+
+    // Simple test: make sure intersect can visit every doc:
+    BitSet seen = new BitSet();
+    values.intersect("dim",
+                     new IntersectVisitor() {
+                       @Override
+                       public Relation compare(byte[] minPacked, byte[] maxPacked) {
+                         return Relation.CELL_CROSSES_QUERY;
+                       }
+                       public void visit(int docID) {
+                         throw new IllegalStateException();
+                       }
+                       public void visit(int docID, byte[] packedValue) {
+                         seen.set(docID);
+                         assertEquals(docID, NumericUtils.sortableBytesToInt(packedValue, 0));
+                       }
+                     });
+    assertEquals(20, seen.cardinality());
+    IOUtils.close(r, dir);
+  }
+
+  public void testMerge() throws Exception {
+    Directory dir = getDirectory(20);
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    iwc.setMergePolicy(newLogMergePolicy());
+    IndexWriter w = new IndexWriter(dir, iwc);
+    byte[] point = new byte[4];
+    for(int i=0;i<20;i++) {
+      Document doc = new Document();
+      NumericUtils.intToSortableBytes(i, point, 0);
+      doc.add(new BinaryPoint("dim", point));
+      w.addDocument(doc);
+      if (i == 10) {
+        w.commit();
+      }
+    }
+    w.forceMerge(1);
+    w.close();
+
+    DirectoryReader r = DirectoryReader.open(dir);
+    LeafReader sub = getOnlySegmentReader(r);
+    PointValues values = sub.getPointValues();
+
+    // Simple test: make sure intersect can visit every doc:
+    BitSet seen = new BitSet();
+    values.intersect("dim",
+                     new IntersectVisitor() {
+                       @Override
+                       public Relation compare(byte[] minPacked, byte[] maxPacked) {
+                         return Relation.CELL_CROSSES_QUERY;
+                       }
+                       public void visit(int docID) {
+                         throw new IllegalStateException();
+                       }
+                       public void visit(int docID, byte[] packedValue) {
+                         seen.set(docID);
+                         assertEquals(docID, NumericUtils.sortableBytesToInt(packedValue, 0));
+                       }
+                     });
+    assertEquals(20, seen.cardinality());
+    IOUtils.close(r, dir);
+  }
+
+  public void testAllPointDocsDeletedInSegment() throws Exception {
+    Directory dir = getDirectory(20);
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    IndexWriter w = new IndexWriter(dir, iwc);
+    byte[] point = new byte[4];
+    for(int i=0;i<10;i++) {
+      Document doc = new Document();
+      NumericUtils.intToSortableBytes(i, point, 0);
+      doc.add(new BinaryPoint("dim", point));
+      doc.add(new NumericDocValuesField("id", i));
+      doc.add(newStringField("x", "x", Field.Store.NO));
+      w.addDocument(doc);
+    }
+    w.addDocument(new Document());
+    w.deleteDocuments(new Term("x", "x"));
+    if (random().nextBoolean()) {
+      w.forceMerge(1);
+    }
+    w.close();
+    DirectoryReader r = DirectoryReader.open(dir);
+    assertEquals(1, r.numDocs());
+    Bits liveDocs = MultiFields.getLiveDocs(r);
+
+    for(LeafReaderContext ctx : r.leaves()) {
+      PointValues values = ctx.reader().getPointValues();
+      NumericDocValues idValues = ctx.reader().getNumericDocValues("id");
+      if (values != null) {
+        BitSet seen = new BitSet();
+        values.intersect("dim",
+                         new IntersectVisitor() {
+                           @Override
+                           public Relation compare(byte[] minPacked, byte[] maxPacked) {
+                             return Relation.CELL_CROSSES_QUERY;
+                           }
+                           public void visit(int docID) {
+                             throw new IllegalStateException();
+                           }
+                           public void visit(int docID, byte[] packedValue) {
+                             if (liveDocs.get(docID)) {
+                               seen.set(docID);
+                             }
+                             assertEquals(idValues.get(docID), NumericUtils.sortableBytesToInt(packedValue, 0));
+                           }
+                         });
+        assertEquals(0, seen.cardinality());
+      }
+    }
+    IOUtils.close(r, dir);
+  }
+
+  /** Make sure we close open files, delete temp files, etc., on exception */
+  public void testWithExceptions() throws Exception {
+    int numDocs = atLeast(10000);
+    int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
+    int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
+
+    byte[][][] docValues = new byte[numDocs][][];
+
+    for(int docID=0;docID<numDocs;docID++) {
+      byte[][] values = new byte[numDims][];
+      for(int dim=0;dim<numDims;dim++) {
+        values[dim] = new byte[numBytesPerDim];
+        random().nextBytes(values[dim]);
+      }
+      docValues[docID] = values;
+    }
+
+    // Keep retrying until we 1) we allow a big enough heap, and 2) we hit a random IOExc from MDW:
+    boolean done = false;
+    while (done == false) {
+      try (MockDirectoryWrapper dir = newMockFSDirectory(createTempDir())) {
+        try {
+          dir.setRandomIOExceptionRate(0.05);
+          dir.setRandomIOExceptionRateOnOpen(0.05);
+          verify(dir, docValues, null, numDims, numBytesPerDim, true);
+        } catch (IllegalStateException ise) {
+          if (ise.getMessage().contains("this writer hit an unrecoverable error")) {
+            Throwable cause = ise.getCause();
+            if (cause != null && cause.getMessage().contains("a random IOException")) {
+              done = true;
+            } else {
+              throw ise;
+            }
+          } else {
+            throw ise;
+          }
+        } catch (AssertionError ae) {
+          if (ae.getMessage().contains("does not exist; files=")) {
+            // OK: likely we threw the random IOExc when IW was asserting the commit files exist
+            done = true;
+          } else {
+            throw ae;
+          }
+        } catch (IllegalArgumentException iae) {
+          // This just means we got a too-small maxMB for the maxPointsInLeafNode; just retry w/ more heap
+          assertTrue(iae.getMessage().contains("either increase maxMBSortInHeap or decrease maxPointsInLeafNode"));
+        } catch (IOException ioe) {
+          String message = ioe.getMessage();
+          if (message.contains("a random IOException") || message.contains("background merge hit exception")) {
+            // BKDWriter should fully clean up after itself:
+            done = true;
+          } else {
+            throw ioe;
+          }
+        }
+      }
+    }
+  }
+
+  public void testMultiValued() throws Exception {
+    int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
+    int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
+
+    int numDocs = atLeast(1000);
+    List<byte[][]> docValues = new ArrayList<>();
+    List<Integer> docIDs = new ArrayList<>();
+
+    for(int docID=0;docID<numDocs;docID++) {
+      int numValuesInDoc = TestUtil.nextInt(random(), 1, 5);
+      for(int ord=0;ord<numValuesInDoc;ord++) {
+        docIDs.add(docID);
+        byte[][] values = new byte[numDims][];
+        for(int dim=0;dim<numDims;dim++) {
+          values[dim] = new byte[numBytesPerDim];
+          random().nextBytes(values[dim]);
+        }
+        docValues.add(values);
+      }
+    }
+
+    byte[][][] docValuesArray = docValues.toArray(new byte[docValues.size()][][]);
+    int[] docIDsArray = new int[docIDs.size()];
+    for(int i=0;i<docIDsArray.length;i++) {
+      docIDsArray[i] = docIDs.get(i);
+    }
+
+    verify(docValuesArray, docIDsArray, numDims, numBytesPerDim);
+  }
+
+  public void testAllEqual() throws Exception {
+    int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
+    int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
+
+    int numDocs = atLeast(1000);
+    byte[][][] docValues = new byte[numDocs][][];
+
+    for(int docID=0;docID<numDocs;docID++) {
+      if (docID == 0) {
+        byte[][] values = new byte[numDims][];
+        for(int dim=0;dim<numDims;dim++) {
+          values[dim] = new byte[numBytesPerDim];
+          random().nextBytes(values[dim]);
+        }
+        docValues[docID] = values;
+      } else {
+        docValues[docID] = docValues[0];
+      }
+    }
+
+    verify(docValues, null, numDims, numBytesPerDim);
+  }
+
+  public void testOneDimEqual() throws Exception {
+    int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
+    int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
+
+    int numDocs = atLeast(1000);
+    int theEqualDim = random().nextInt(numDims);
+    byte[][][] docValues = new byte[numDocs][][];
+
+    for(int docID=0;docID<numDocs;docID++) {
+      byte[][] values = new byte[numDims][];
+      for(int dim=0;dim<numDims;dim++) {
+        values[dim] = new byte[numBytesPerDim];
+        random().nextBytes(values[dim]);
+      }
+      docValues[docID] = values;
+      if (docID > 0) {
+        docValues[docID][theEqualDim] = docValues[0][theEqualDim];
+      }
+    }
+
+    verify(docValues, null, numDims, numBytesPerDim);
+  }
+
+  // Tests on N-dimensional points where each dimension is a BigInteger
+  public void testBigIntNDims() throws Exception {
+
+    int numDocs = atLeast(1000);
+    try (Directory dir = getDirectory(numDocs)) {
+      int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
+      int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
+      IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
+      // We rely on docIDs not changing:
+      iwc.setMergePolicy(newLogMergePolicy());
+      RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+      BigInteger[][] docs = new BigInteger[numDocs][];
+
+      for(int docID=0;docID<numDocs;docID++) {
+        BigInteger[] values = new BigInteger[numDims];
+        if (VERBOSE) {
+          System.out.println("  docID=" + docID);
+        }
+        byte[][] bytes = new byte[numDims][];
+        for(int dim=0;dim<numDims;dim++) {
+          values[dim] = randomBigInt(numBytesPerDim);
+          bytes[dim] = new byte[numBytesPerDim];
+          NumericUtils.bigIntToSortableBytes(values[dim], numBytesPerDim, bytes[dim], 0);
+          if (VERBOSE) {
+            System.out.println("    " + dim + " -> " + values[dim]);
+          }
+        }
+        docs[docID] = values;
+        Document doc = new Document();
+        doc.add(new BinaryPoint("field", bytes));
+        w.addDocument(doc);
+      }
+
+      DirectoryReader r = w.getReader();
+      w.close();
+
+      int iters = atLeast(100);
+      for(int iter=0;iter<iters;iter++) {
+        if (VERBOSE) {
+          System.out.println("\nTEST: iter=" + iter);
+        }
+
+        // Random N dims rect query:
+        BigInteger[] queryMin = new BigInteger[numDims];
+        BigInteger[] queryMax = new BigInteger[numDims];    
+        for(int dim=0;dim<numDims;dim++) {
+          queryMin[dim] = randomBigInt(numBytesPerDim);
+          queryMax[dim] = randomBigInt(numBytesPerDim);
+          if (queryMin[dim].compareTo(queryMax[dim]) > 0) {
+            BigInteger x = queryMin[dim];
+            queryMin[dim] = queryMax[dim];
+            queryMax[dim] = x;
+          }
+          if (VERBOSE) {
+            System.out.println("  " + dim + "\n    min=" + queryMin[dim] + "\n    max=" + queryMax[dim]);
+          }
+        }
+
+        final BitSet hits = new BitSet();
+        for(LeafReaderContext ctx : r.leaves()) {
+          PointValues dimValues = ctx.reader().getPointValues();
+          if (dimValues == null) {
+            continue;
+          }
+
+          final int docBase = ctx.docBase;
+          
+          dimValues.intersect("field", new IntersectVisitor() {
+              @Override
+              public void visit(int docID) {
+                hits.set(docBase+docID);
+                //System.out.println("visit docID=" + docID);
+              }
+
+              @Override
+              public void visit(int docID, byte[] packedValue) {
+                //System.out.println("visit check docID=" + docID);
+                for(int dim=0;dim<numDims;dim++) {
+                  BigInteger x = NumericUtils.sortableBytesToBigInt(packedValue, dim * numBytesPerDim, numBytesPerDim);
+                  if (x.compareTo(queryMin[dim]) < 0 || x.compareTo(queryMax[dim]) > 0) {
+                    //System.out.println("  no");
+                    return;
+                  }
+                }
+
+                //System.out.println("  yes");
+                hits.set(docBase+docID);
+              }
+
+              @Override
+              public Relation compare(byte[] minPacked, byte[] maxPacked) {
+                boolean crosses = false;
+                for(int dim=0;dim<numDims;dim++) {
+                  BigInteger min = NumericUtils.sortableBytesToBigInt(minPacked, dim * numBytesPerDim, numBytesPerDim);
+                  BigInteger max = NumericUtils.sortableBytesToBigInt(maxPacked, dim * numBytesPerDim, numBytesPerDim);
+                  assert max.compareTo(min) >= 0;
+
+                  if (max.compareTo(queryMin[dim]) < 0 || min.compareTo(queryMax[dim]) > 0) {
+                    return Relation.CELL_OUTSIDE_QUERY;
+                  } else if (min.compareTo(queryMin[dim]) < 0 || max.compareTo(queryMax[dim]) > 0) {
+                    crosses = true;
+                  }
+                }
+
+                if (crosses) {
+                  return Relation.CELL_CROSSES_QUERY;
+                } else {
+                  return Relation.CELL_INSIDE_QUERY;
+                }
+              }
+            });
+        }
+
+        for(int docID=0;docID<numDocs;docID++) {
+          BigInteger[] docValues = docs[docID];
+          boolean expected = true;
+          for(int dim=0;dim<numDims;dim++) {
+            BigInteger x = docValues[dim];
+            if (x.compareTo(queryMin[dim]) < 0 || x.compareTo(queryMax[dim]) > 0) {
+              expected = false;
+              break;
+            }
+          }
+          boolean actual = hits.get(docID);
+          assertEquals("docID=" + docID, expected, actual);
+        }
+      }
+      r.close();
+      }
+  }
+
+  public void testRandomBinaryTiny() throws Exception {
+    doTestRandomBinary(10);
+  }
+
+  public void testRandomBinaryMedium() throws Exception {
+    doTestRandomBinary(10000);
+  }
+
+  @Nightly
+  public void testRandomBinaryBig() throws Exception {
+    assumeFalse("too slow with SimpleText", Codec.getDefault().getName().equals("SimpleText"));
+    doTestRandomBinary(200000);
+  }
+
+  private void doTestRandomBinary(int count) throws Exception {
+    int numDocs = TestUtil.nextInt(random(), count, count*2);
+    int numBytesPerDim = TestUtil.nextInt(random(), 2, PointValues.MAX_NUM_BYTES);
+    int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);
+
+    byte[][][] docValues = new byte[numDocs][][];
+
+    for(int docID=0;docID<numDocs;docID++) {
+      byte[][] values = new byte[numDims][];
+      for(int dim=0;dim<numDims;dim++) {
+        values[dim] = new byte[numBytesPerDim];
+        // TODO: sometimes test on a "small" volume too, so we test the high density cases, higher chance of boundary, etc. cases:
+        random().nextBytes(values[dim]);
+      }
+      docValues[docID] = values;
+    }
+
+    verify(docValues, null, numDims, numBytesPerDim);
+  }
+
+  /** docIDs can be null, for the single valued case, else it maps value to docID, but all values for one doc must be adjacent */
+  private void verify(byte[][][] docValues, int[] docIDs, int numDims, int numBytesPerDim) throws Exception {
+    try (Directory dir = getDirectory(docValues.length)) {
+      while (true) {
+        try {
+          verify(dir, docValues, docIDs, numDims, numBytesPerDim, false);
+          return;
+        } catch (IllegalArgumentException iae) {
+          // This just means we got a too-small maxMB for the maxPointsInLeafNode; just retry
+          assertTrue(iae.getMessage().contains("either increase maxMBSortInHeap or decrease maxPointsInLeafNode"));
+        }
+      }
+    }
+  }
+
+  private void verify(Directory dir, byte[][][] docValues, int[] ids, int numDims, int numBytesPerDim, boolean expectExceptions) throws Exception {
+    int numValues = docValues.length;
+    if (VERBOSE) {
+      System.out.println("TEST: numValues=" + numValues + " numDims=" + numDims + " numBytesPerDim=" + numBytesPerDim);
+    }
+
+    // RandomIndexWriter is too slow:
+    boolean useRealWriter = docValues.length > 10000;
+
+    IndexWriterConfig iwc;
+    if (useRealWriter) {
+      iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    } else {
+      iwc = newIndexWriterConfig();
+    }
+
+    if (expectExceptions) {
+      MergeScheduler ms = iwc.getMergeScheduler();
+      if (ms instanceof ConcurrentMergeScheduler) {
+        ((ConcurrentMergeScheduler) ms).setSuppressExceptions();
+      }
+    }
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    DirectoryReader r = null;
+
+    // Compute actual min/max values:
+    byte[][] expectedMinValues = new byte[numDims][];
+    byte[][] expectedMaxValues = new byte[numDims][];
+    for(int ord=0;ord<docValues.length;ord++) {
+      for(int dim=0;dim<numDims;dim++) {
+        if (ord == 0) {
+          expectedMinValues[dim] = new byte[numBytesPerDim];
+          System.arraycopy(docValues[ord][dim], 0, expectedMinValues[dim], 0, numBytesPerDim);
+          expectedMaxValues[dim] = new byte[numBytesPerDim];
+          System.arraycopy(docValues[ord][dim], 0, expectedMaxValues[dim], 0, numBytesPerDim);
+        } else {
+          // TODO: it's cheating that we use StringHelper.compare for "truth": what if it's buggy?
+          if (StringHelper.compare(numBytesPerDim, docValues[ord][dim], 0, expectedMinValues[dim], 0) < 0) {
+            System.arraycopy(docValues[ord][dim], 0, expectedMinValues[dim], 0, numBytesPerDim);
+          }
+          if (StringHelper.compare(numBytesPerDim, docValues[ord][dim], 0, expectedMaxValues[dim], 0) > 0) {
+            System.arraycopy(docValues[ord][dim], 0, expectedMaxValues[dim], 0, numBytesPerDim);
+          }
+        }
+      }
+    }
+
+    // 20% of the time we add into a separate directory, then at some point use
+    // addIndexes to bring the indexed point values to the main directory:
+    Directory saveDir;
+    RandomIndexWriter saveW;
+    int addIndexesAt;
+    if (random().nextInt(5) == 1) {
+      saveDir = dir;
+      saveW = w;
+      dir = getDirectory(numValues);
+      if (useRealWriter) {
+        iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+      } else {
+        iwc = newIndexWriterConfig();
+      }
+      if (expectExceptions) {
+        MergeScheduler ms = iwc.getMergeScheduler();
+        if (ms instanceof ConcurrentMergeScheduler) {
+          ((ConcurrentMergeScheduler) ms).setSuppressExceptions();
+        }
+      }
+      w = new RandomIndexWriter(random(), dir, iwc);
+      addIndexesAt = TestUtil.nextInt(random(), 1, numValues-1);
+    } else {
+      saveW = null;
+      saveDir = null;
+      addIndexesAt = 0;
+    }
+
+    try {
+
+      Document doc = null;
+      int lastID = -1;
+      for(int ord=0;ord<numValues;ord++) {
+        int id;
+        if (ids == null) {
+          id = ord;
+        } else {
+          id = ids[ord];
+        }
+        if (id != lastID) {
+          if (doc != null) {
+            if (useRealWriter) {
+              w.w.addDocument(doc);
+            } else {
+              w.addDocument(doc);
+            }
+          }
+          doc = new Document();
+          doc.add(new NumericDocValuesField("id", id));
+        }
+        doc.add(new BinaryPoint("field", docValues[ord]));
+        lastID = id;
+
+        if (random().nextInt(30) == 17) {
+          // randomly index some documents without this field
+          if (useRealWriter) {
+            w.w.addDocument(new Document());
+          } else {
+            w.addDocument(new Document());
+          }
+          if (VERBOSE) {
+            System.out.println("add empty doc");
+          }
+        }
+
+        if (random().nextInt(30) == 17) {
+          // randomly index some documents with this field, but we will delete them:
+          Document xdoc = new Document();
+          xdoc.add(new BinaryPoint("field", docValues[ord]));
+          xdoc.add(new StringField("nukeme", "yes", Field.Store.NO));
+          if (useRealWriter) {
+            w.w.addDocument(xdoc);
+          } else {
+            w.addDocument(xdoc);
+          }
+          if (VERBOSE) {
+            System.out.println("add doc doc-to-delete");
+          }
+
+          if (random().nextInt(5) == 1) {
+            if (useRealWriter) {
+              w.w.deleteDocuments(new Term("nukeme", "yes"));
+            } else {
+              w.deleteDocuments(new Term("nukeme", "yes"));
+            }
+          }
+        }
+
+        if (VERBOSE) {
+          System.out.println("  ord=" + ord + " id=" + id);
+          for(int dim=0;dim<numDims;dim++) {
+            System.out.println("    dim=" + dim + " value=" + new BytesRef(docValues[ord][dim]));
+          }
+        }
+
+        if (saveW != null && ord >= addIndexesAt) {
+          switchIndex(w, dir, saveW);
+          w = saveW;
+          dir = saveDir;
+          saveW = null;
+          saveDir = null;
+        }
+      }
+      w.addDocument(doc);
+      w.deleteDocuments(new Term("nukeme", "yes"));
+
+      if (random().nextBoolean()) {
+        if (VERBOSE) {
+          System.out.println("\nTEST: now force merge");
+        }
+        w.forceMerge(1);
+      }
+
+      r = w.getReader();
+      w.close();
+
+      if (VERBOSE) {
+        System.out.println("TEST: reader=" + r);
+      }
+
+      NumericDocValues idValues = MultiDocValues.getNumericValues(r, "id");
+      Bits liveDocs = MultiFields.getLiveDocs(r);
+
+      // Verify min/max values are correct:
+      byte[] minValues = new byte[numDims*numBytesPerDim];
+      Arrays.fill(minValues, (byte) 0xff);
+
+      byte[] maxValues = new byte[numDims*numBytesPerDim];
+
+      for(LeafReaderContext ctx : r.leaves()) {
+        PointValues dimValues = ctx.reader().getPointValues();
+        if (dimValues == null) {
+          continue;
+        }
+
+        byte[] leafMinValues = dimValues.getMinPackedValue("field");
+        byte[] leafMaxValues = dimValues.getMaxPackedValue("field");
+        for(int dim=0;dim<numDims;dim++) {
+          if (StringHelper.compare(numBytesPerDim, leafMinValues, dim*numBytesPerDim, minValues, dim*numBytesPerDim) < 0) {
+            System.arraycopy(leafMinValues, dim*numBytesPerDim, minValues, dim*numBytesPerDim, numBytesPerDim);
+          }
+          if (StringHelper.compare(numBytesPerDim, leafMaxValues, dim*numBytesPerDim, maxValues, dim*numBytesPerDim) > 0) {
+            System.arraycopy(leafMaxValues, dim*numBytesPerDim, maxValues, dim*numBytesPerDim, numBytesPerDim);
+          }
+        }
+      }
+
+      byte[] scratch = new byte[numBytesPerDim];
+      for(int dim=0;dim<numDims;dim++) {
+        System.arraycopy(minValues, dim*numBytesPerDim, scratch, 0, numBytesPerDim);
+        //System.out.println("dim=" + dim + " expectedMin=" + new BytesRef(expectedMinValues[dim]) + " min=" + new BytesRef(scratch));
+        assertTrue(Arrays.equals(expectedMinValues[dim], scratch));
+        System.arraycopy(maxValues, dim*numBytesPerDim, scratch, 0, numBytesPerDim);
+        //System.out.println("dim=" + dim + " expectedMax=" + new BytesRef(expectedMaxValues[dim]) + " max=" + new BytesRef(scratch));
+        assertTrue(Arrays.equals(expectedMaxValues[dim], scratch));
+      }
+
+      int iters = atLeast(100);
+      for(int iter=0;iter<iters;iter++) {
+        if (VERBOSE) {
+          System.out.println("\nTEST: iter=" + iter);
+        }
+
+        // Random N dims rect query:
+        byte[][] queryMin = new byte[numDims][];
+        byte[][] queryMax = new byte[numDims][];    
+        for(int dim=0;dim<numDims;dim++) {    
+          queryMin[dim] = new byte[numBytesPerDim];
+          random().nextBytes(queryMin[dim]);
+          queryMax[dim] = new byte[numBytesPerDim];
+          random().nextBytes(queryMax[dim]);
+          if (StringHelper.compare(numBytesPerDim, queryMin[dim], 0, queryMax[dim], 0) > 0) {
+            byte[] x = queryMin[dim];
+            queryMin[dim] = queryMax[dim];
+            queryMax[dim] = x;
+          }
+        }
+
+        if (VERBOSE) {
+          for(int dim=0;dim<numDims;dim++) {
+            System.out.println("  dim=" + dim + "\n    queryMin=" + new BytesRef(queryMin[dim]) + "\n    queryMax=" + new BytesRef(queryMax[dim]));
+          }
+        }
+
+        final BitSet hits = new BitSet();
+
+        for(LeafReaderContext ctx : r.leaves()) {
+          PointValues dimValues = ctx.reader().getPointValues();
+          if (dimValues == null) {
+            continue;
+          }
+
+          final int docBase = ctx.docBase;
+
+          dimValues.intersect("field", new PointValues.IntersectVisitor() {
+              @Override
+              public void visit(int docID) {
+                if (liveDocs == null || liveDocs.get(docBase+docID)) {
+                  hits.set((int) idValues.get(docBase+docID));
+                }
+                //System.out.println("visit docID=" + docID);
+              }
+
+              @Override
+              public void visit(int docID, byte[] packedValue) {
+                if (liveDocs != null && liveDocs.get(docBase+docID) == false) {
+                  return;
+                }
+
+                //System.out.println("visit check docID=" + docID + " id=" + idValues.get(docID));
+                for(int dim=0;dim<numDims;dim++) {
+                  //System.out.println("  dim=" + dim + " value=" + new BytesRef(packedValue, dim*numBytesPerDim, numBytesPerDim));
+                  if (StringHelper.compare(numBytesPerDim, packedValue, dim*numBytesPerDim, queryMin[dim], 0) < 0 ||
+                      StringHelper.compare(numBytesPerDim, packedValue, dim*numBytesPerDim, queryMax[dim], 0) > 0) {
+                    //System.out.println("  no");
+                    return;
+                  }
+                }
+
+                //System.out.println("  yes");
+                hits.set((int) idValues.get(docBase+docID));
+              }
+
+              @Override
+              public Relation compare(byte[] minPacked, byte[] maxPacked) {
+                boolean crosses = false;
+                //System.out.println("compare");
+                for(int dim=0;dim<numDims;dim++) {
+                  if (StringHelper.compare(numBytesPerDim, maxPacked, dim*numBytesPerDim, queryMin[dim], 0) < 0 ||
+                      StringHelper.compare(numBytesPerDim, minPacked, dim*numBytesPerDim, queryMax[dim], 0) > 0) {
+                    //System.out.println("  query_outside_cell");
+                    return Relation.CELL_OUTSIDE_QUERY;
+                  } else if (StringHelper.compare(numBytesPerDim, minPacked, dim*numBytesPerDim, queryMin[dim], 0) < 0 ||
+                             StringHelper.compare(numBytesPerDim, maxPacked, dim*numBytesPerDim, queryMax[dim], 0) > 0) {
+                    crosses = true;
+                  }
+                }
+
+                if (crosses) {
+                  //System.out.println("  query_crosses_cell");
+                  return Relation.CELL_CROSSES_QUERY;
+                } else {
+                  //System.out.println("  cell_inside_query");
+                  return Relation.CELL_INSIDE_QUERY;
+                }
+              }
+            });
+        }
+
+        BitSet expected = new BitSet();
+        for(int ord=0;ord<numValues;ord++) {
+          boolean matches = true;
+          for(int dim=0;dim<numDims;dim++) {
+            byte[] x = docValues[ord][dim];
+            if (StringHelper.compare(numBytesPerDim, x, 0, queryMin[dim], 0) < 0 ||
+                StringHelper.compare(numBytesPerDim, x, 0, queryMax[dim], 0) > 0) {
+              matches = false;
+              break;
+            }
+          }
+
+          if (matches) {
+            int id;
+            if (ids == null) {
+              id = ord;
+            } else {
+              id = ids[ord];
+            }
+            expected.set(id);
+          }
+        }
+
+        int limit = Math.max(expected.length(), hits.length());
+        int failCount = 0;
+        int successCount = 0;
+        for(int id=0;id<limit;id++) {
+          if (expected.get(id) != hits.get(id)) {
+            System.out.println("FAIL: id=" + id);
+            failCount++;
+          } else {
+            successCount++;
+          }
+        }
+
+        if (failCount != 0) {
+          for(int docID=0;docID<r.maxDoc();docID++) {
+            System.out.println("  docID=" + docID + " id=" + idValues.get(docID));
+          }
+
+          fail(failCount + " docs failed; " + successCount + " docs succeeded");
+        }
+      }
+    } finally {
+      IOUtils.closeWhileHandlingException(r, w, saveW, saveDir == null ? null : dir);
+    }
+  }
+
+  public void testAddIndexes() throws IOException {
+    Directory dir1 = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir1);
+    Document doc = new Document();
+    doc.add(new IntPoint("int1", 17));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(new IntPoint("int2", 42));
+    w.addDocument(doc);
+    w.close();
+
+    // Different field number assigments:
+    Directory dir2 = newDirectory();
+    w = new RandomIndexWriter(random(), dir2);
+    doc = new Document();
+    doc.add(new IntPoint("int2", 42));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(new IntPoint("int1", 17));
+    w.addDocument(doc);
+    w.close();
+
+    Directory dir = newDirectory();
+    w = new RandomIndexWriter(random(), dir);
+    w.addIndexes(new Directory[] {dir1, dir2});
+    w.forceMerge(1);
+
+    DirectoryReader r = w.getReader();
+    IndexSearcher s = newSearcher(r, false);
+    assertEquals(2, s.count(IntPoint.newExactQuery("int1", 17)));
+    assertEquals(2, s.count(IntPoint.newExactQuery("int2", 42)));
+    r.close();
+    w.close();
+    dir.close();
+    dir1.close();
+    dir2.close();
+  }
+
+  private void switchIndex(RandomIndexWriter w, Directory dir, RandomIndexWriter saveW) throws IOException {
+    if (random().nextBoolean()) {
+      // Add via readers:
+      try (DirectoryReader r = w.getReader()) {
+        if (random().nextBoolean()) {
+          // Add via CodecReaders:
+          List<CodecReader> subs = new ArrayList<>();
+          for (LeafReaderContext context : r.leaves()) {
+            subs.add((CodecReader) context.reader());
+          }
+          if (VERBOSE) {
+            System.out.println("TEST: now use addIndexes(CodecReader[]) to switch writers");
+          }
+          saveW.addIndexes(subs.toArray(new CodecReader[subs.size()]));
+        } else {
+          if (VERBOSE) {
+            System.out.println("TEST: now use TestUtil.addIndexesSlowly(DirectoryReader[]) to switch writers");
+          }
+          TestUtil.addIndexesSlowly(saveW.w, r);
+        }
+      }
+    } else {
+      // Add via directory:
+      if (VERBOSE) {
+        System.out.println("TEST: now use addIndexes(Directory[]) to switch writers");
+      }
+      w.close();
+      saveW.addIndexes(new Directory[] {dir});
+    }
+    w.close();
+    dir.close();
+  }
+
+  private BigInteger randomBigInt(int numBytes) {
+    BigInteger x = new BigInteger(numBytes*8-1, random());
+    if (random().nextBoolean()) {
+      x = x.negate();
+    }
+    return x;
+  }
+
+  private Directory getDirectory(int numPoints) throws IOException {
+    Directory dir;
+    if (numPoints > 100000) {
+      dir = newFSDirectory(createTempDir("TestBKDTree"));
+    } else {
+      dir = newDirectory();
+    }
+    //dir = FSDirectory.open(createTempDir());
+    return dir;
+  }
+
+  @Override
+  protected boolean mergeIsStable() {
+    // suppress this test from base class: merges for BKD trees are not stable because the tree created by merge will have a different
+    // structure than the tree created by adding points separately
+    return false;
+  }
+}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java b/lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java
index 5b04b00..662001e 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java
@@ -28,21 +28,21 @@ import java.util.Random;
 import java.util.Set;
 
 import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.PointFormat;
-import org.apache.lucene.codecs.PointReader;
-import org.apache.lucene.codecs.PointWriter;
+import org.apache.lucene.codecs.PointsFormat;
+import org.apache.lucene.codecs.PointsReader;
+import org.apache.lucene.codecs.PointsWriter;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.asserting.AssertingCodec;
 import org.apache.lucene.codecs.asserting.AssertingDocValuesFormat;
-import org.apache.lucene.codecs.asserting.AssertingPointFormat;
+import org.apache.lucene.codecs.asserting.AssertingPointsFormat;
 import org.apache.lucene.codecs.asserting.AssertingPostingsFormat;
 import org.apache.lucene.codecs.blockterms.LuceneFixedGap;
 import org.apache.lucene.codecs.blockterms.LuceneVarGapDocFreqInterval;
 import org.apache.lucene.codecs.blockterms.LuceneVarGapFixedInterval;
 import org.apache.lucene.codecs.blocktreeords.BlockTreeOrdsPostingsFormat;
 import org.apache.lucene.codecs.bloom.TestBloomFilteredLucenePostings;
-import org.apache.lucene.codecs.lucene60.Lucene60PointReader;
-import org.apache.lucene.codecs.lucene60.Lucene60PointWriter;
+import org.apache.lucene.codecs.lucene60.Lucene60PointsReader;
+import org.apache.lucene.codecs.lucene60.Lucene60PointsWriter;
 import org.apache.lucene.codecs.memory.DirectDocValuesFormat;
 import org.apache.lucene.codecs.memory.DirectPostingsFormat;
 import org.apache.lucene.codecs.memory.FSTOrdPostingsFormat;
@@ -95,16 +95,16 @@ public class RandomCodec extends AssertingCodec {
   private final double maxMBSortInHeap;
 
   @Override
-  public PointFormat pointFormat() {
-    return new AssertingPointFormat(new PointFormat() {
+  public PointsFormat pointsFormat() {
+    return new AssertingPointsFormat(new PointsFormat() {
       @Override
-      public PointWriter fieldsWriter(SegmentWriteState writeState) throws IOException {
-        return new Lucene60PointWriter(writeState, maxPointsInLeafNode, maxMBSortInHeap);
+      public PointsWriter fieldsWriter(SegmentWriteState writeState) throws IOException {
+        return new Lucene60PointsWriter(writeState, maxPointsInLeafNode, maxMBSortInHeap);
       }
 
       @Override
-      public PointReader fieldsReader(SegmentReadState readState) throws IOException {
-        return new Lucene60PointReader(readState);
+      public PointsReader fieldsReader(SegmentReadState readState) throws IOException {
+        return new Lucene60PointsReader(readState);
       }
     });
   }
diff --git a/lucene/test-framework/src/test/org/apache/lucene/codecs/asserting/TestAssertingPointFormat.java b/lucene/test-framework/src/test/org/apache/lucene/codecs/asserting/TestAssertingPointFormat.java
deleted file mode 100644
index 9f79b70..0000000
--- a/lucene/test-framework/src/test/org/apache/lucene/codecs/asserting/TestAssertingPointFormat.java
+++ /dev/null
@@ -1,30 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.codecs.asserting;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BasePointFormatTestCase;
-
-/** Test AssertingPointFormat directly */
-public class TestAssertingPointFormat extends BasePointFormatTestCase {
-  private final Codec codec = new AssertingCodec();
-
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-}
diff --git a/lucene/test-framework/src/test/org/apache/lucene/codecs/asserting/TestAssertingPointsFormat.java b/lucene/test-framework/src/test/org/apache/lucene/codecs/asserting/TestAssertingPointsFormat.java
new file mode 100644
index 0000000..cc05e3d
--- /dev/null
+++ b/lucene/test-framework/src/test/org/apache/lucene/codecs/asserting/TestAssertingPointsFormat.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.asserting;
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BasePointsFormatTestCase;
+
+/** Test AssertingPointsFormat directly */
+public class TestAssertingPointsFormat extends BasePointsFormatTestCase {
+  private final Codec codec = new AssertingCodec();
+
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+}
