

diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/appending/AppendingCodec.java lucene-3661/lucene/src/java/org/apache/lucene/codecs/appending/AppendingCodec.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/appending/AppendingCodec.java	2012-01-12 13:57:22.437157289 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/codecs/appending/AppendingCodec.java	2011-12-20 23:08:23.010150632 -0500
@@ -20,6 +20,7 @@
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfosFormat;
@@ -28,6 +29,7 @@
 import org.apache.lucene.codecs.lucene40.Lucene40Codec;
 import org.apache.lucene.codecs.lucene40.Lucene40DocValuesFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40NormsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40TermVectorsFormat;
@@ -50,6 +52,7 @@
   private final TermVectorsFormat vectors = new Lucene40TermVectorsFormat();
   private final DocValuesFormat docValues = new Lucene40DocValuesFormat();
   private final NormsFormat norms = new Lucene40NormsFormat();
+  private final LiveDocsFormat liveDocs = new Lucene40LiveDocsFormat();
   
   @Override
   public PostingsFormat postingsFormat() {
@@ -85,4 +88,9 @@
   public NormsFormat normsFormat() {
     return norms;
   }
+  
+  @Override
+  public LiveDocsFormat liveDocsFormat() {
+    return liveDocs;
+  }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/Codec.java lucene-3661/lucene/src/java/org/apache/lucene/codecs/Codec.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/Codec.java	2012-01-12 13:57:22.513157292 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/codecs/Codec.java	2012-01-28 09:58:02.192906337 -0500
@@ -22,6 +22,7 @@
 
 import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.util.NamedSPILoader;
+import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 
 /**
@@ -43,7 +44,11 @@
     return name;
   }
   
+  /** Populates <code>files</code> with all filenames needed for 
+   * the <code>info</code> segment.
+   */
   public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    assert (dir instanceof CompoundFileDirectory) == false;
     postingsFormat().files(dir, info, "", files);
     storedFieldsFormat().files(dir, info, files);
     termVectorsFormat().files(dir, info, files);
@@ -54,6 +59,14 @@
     normsFormat().files(dir, info, files);
   }
   
+  /** Populates <code>files</code> with any filenames that are
+   * stored outside of CFS for the <code>info</code> segment.
+   */
+  public void separateFiles(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    liveDocsFormat().separateFiles(dir, info, files);
+    normsFormat().separateFiles(dir, info, files);
+  }
+  
   /** Encodes/decodes postings */
   public abstract PostingsFormat postingsFormat();
   
@@ -75,6 +88,9 @@
   /** Encodes/decodes document normalization values */
   public abstract NormsFormat normsFormat();
   
+  /** Encodes/decodes live docs */
+  public abstract LiveDocsFormat liveDocsFormat();
+  
   /** looks up a codec by name */
   public static Codec forName(String name) {
     return loader.lookup(name);


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/LiveDocsFormat.java lucene-3661/lucene/src/java/org/apache/lucene/codecs/LiveDocsFormat.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/LiveDocsFormat.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/codecs/LiveDocsFormat.java	2012-01-28 09:52:46.960907070 -0500
@@ -0,0 +1,41 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.MutableBits;
+
+/** Format for live/deleted documents
+ * @lucene.experimental */
+public abstract class LiveDocsFormat {
+  /** creates a new mutablebits, with all bits set, for the specified size */
+  public abstract MutableBits newLiveDocs(int size) throws IOException;
+  /** creates a new mutablebits of the same bits set and size of existing */
+  public abstract MutableBits newLiveDocs(Bits existing) throws IOException;
+  /** reads bits from a file */
+  public abstract Bits readLiveDocs(Directory dir, SegmentInfo info, IOContext context) throws IOException;
+  /** writes bits to a file */
+  public abstract void writeLiveDocs(MutableBits bits, Directory dir, SegmentInfo info, IOContext context) throws IOException;
+  public abstract void separateFiles(Directory dir, SegmentInfo info, Set<String> files) throws IOException;
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xCodec.java lucene-3661/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xCodec.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xCodec.java	2012-01-19 13:28:40.682685692 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xCodec.java	2012-01-28 10:10:53.060904544 -0500
@@ -23,18 +23,23 @@
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.PerDocConsumer;
 import org.apache.lucene.codecs.PerDocProducer;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfosFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.StoredFieldsWriter;
 import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
 import org.apache.lucene.index.PerDocWriteState;
 import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.util.MutableBits;
 
 /**
  * Supports the Lucene 3.x index format (readonly)
@@ -47,7 +52,12 @@
   private final PostingsFormat postingsFormat = new Lucene3xPostingsFormat();
   
   // TODO: this should really be a different impl
-  private final StoredFieldsFormat fieldsFormat = new Lucene40StoredFieldsFormat();
+  private final StoredFieldsFormat fieldsFormat = new Lucene40StoredFieldsFormat() {
+    @Override
+    public StoredFieldsWriter fieldsWriter(Directory directory, String segment, IOContext context) throws IOException {
+      throw new UnsupportedOperationException("this codec can only be used for reading");
+    }
+  };
   
   private final TermVectorsFormat vectorsFormat = new Lucene3xTermVectorsFormat();
   
@@ -57,6 +67,14 @@
   
   private final NormsFormat normsFormat = new Lucene3xNormsFormat();
   
+  // TODO: this should really be a different impl
+  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat() {
+    @Override
+    public void writeLiveDocs(MutableBits bits, Directory dir, SegmentInfo info, IOContext context) throws IOException {
+      throw new UnsupportedOperationException("this codec can only be used for reading");
+    }
+  };
+  
   // 3.x doesn't support docvalues
   private final DocValuesFormat docValuesFormat = new DocValuesFormat() {
     @Override
@@ -107,4 +125,9 @@
   public NormsFormat normsFormat() {
     return normsFormat;
   }
+  
+  @Override
+  public LiveDocsFormat liveDocsFormat() {
+    return liveDocsFormat;
+  }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/lucene40/BitVector.java lucene-3661/lucene/src/java/org/apache/lucene/codecs/lucene40/BitVector.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/lucene40/BitVector.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/codecs/lucene40/BitVector.java	2012-01-20 13:03:15.830488266 -0500
@@ -0,0 +1,420 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.store.CompoundFileDirectory;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.MutableBits;
+
+/** Optimized implementation of a vector of bits.  This is more-or-less like
+ *  java.util.BitSet, but also includes the following:
+ *  <ul>
+ *  <li>a count() method, which efficiently computes the number of one bits;</li>
+ *  <li>optimized read from and write to disk;</li>
+ *  <li>inlinable get() method;</li>
+ *  <li>store and load, as bit set or d-gaps, depending on sparseness;</li> 
+ *  </ul>
+ *
+ *  @lucene.internal
+ */
+// pkg-private: if this thing is generally useful then it can go back in .util,
+// but the serialization must be here underneath the codec.
+final class BitVector implements Cloneable, MutableBits {
+
+  private byte[] bits;
+  private int size;
+  private int count;
+  private int version;
+
+  /** Constructs a vector capable of holding <code>n</code> bits. */
+  public BitVector(int n) {
+    size = n;
+    bits = new byte[getNumBytes(size)];
+    count = 0;
+  }
+
+  BitVector(byte[] bits, int size) {
+    this.bits = bits;
+    this.size = size;
+    count = -1;
+  }
+  
+  private int getNumBytes(int size) {
+    int bytesLength = size >>> 3;
+    if ((size & 7) != 0) {
+      bytesLength++;
+    }
+    return bytesLength;
+  }
+  
+  @Override
+  public BitVector clone() {
+    byte[] copyBits = new byte[bits.length];
+    System.arraycopy(bits, 0, copyBits, 0, bits.length);
+    BitVector clone = new BitVector(copyBits, size);
+    clone.count = count;
+    return clone;
+  }
+  
+  /** Sets the value of <code>bit</code> to one. */
+  public final void set(int bit) {
+    if (bit >= size) {
+      throw new ArrayIndexOutOfBoundsException("bit=" + bit + " size=" + size);
+    }
+    bits[bit >> 3] |= 1 << (bit & 7);
+    count = -1;
+  }
+
+  /** Sets the value of <code>bit</code> to true, and
+   *  returns true if bit was already set */
+  public final boolean getAndSet(int bit) {
+    if (bit >= size) {
+      throw new ArrayIndexOutOfBoundsException("bit=" + bit + " size=" + size);
+    }
+    final int pos = bit >> 3;
+    final int v = bits[pos];
+    final int flag = 1 << (bit & 7);
+    if ((flag & v) != 0)
+      return true;
+    else {
+      bits[pos] = (byte) (v | flag);
+      if (count != -1) {
+        count++;
+        assert count <= size;
+      }
+      return false;
+    }
+  }
+
+  /** Sets the value of <code>bit</code> to zero. */
+  public final void clear(int bit) {
+    if (bit >= size) {
+      throw new ArrayIndexOutOfBoundsException(bit);
+    }
+    bits[bit >> 3] &= ~(1 << (bit & 7));
+    count = -1;
+  }
+
+  public final boolean getAndClear(int bit) {
+    if (bit >= size) {
+      throw new ArrayIndexOutOfBoundsException(bit);
+    }
+    final int pos = bit >> 3;
+    final int v = bits[pos];
+    final int flag = 1 << (bit & 7);
+    if ((flag & v) == 0) {
+      return false;
+    } else {
+      bits[pos] &= ~flag;
+      if (count != -1) {
+        count--;
+        assert count >= 0;
+      }
+      return true;
+    }
+  }
+
+  /** Returns <code>true</code> if <code>bit</code> is one and
+    <code>false</code> if it is zero. */
+  public final boolean get(int bit) {
+    assert bit >= 0 && bit < size: "bit " + bit + " is out of bounds 0.." + (size-1);
+    return (bits[bit >> 3] & (1 << (bit & 7))) != 0;
+  }
+
+  /** Returns the number of bits in this vector.  This is also one greater than
+    the number of the largest valid bit number. */
+  public final int size() {
+    return size;
+  }
+
+  @Override
+  public int length() {
+    return size;
+  }
+
+  /** Returns the total number of one bits in this vector.  This is efficiently
+    computed and cached, so that, if the vector is not changed, no
+    recomputation is done for repeated calls. */
+  public final int count() {
+    // if the vector has been modified
+    if (count == -1) {
+      int c = 0;
+      int end = bits.length;
+      for (int i = 0; i < end; i++) {
+        c += BYTE_COUNTS[bits[i] & 0xFF];	  // sum bits per byte
+      }
+      count = c;
+    }
+    assert count <= size: "count=" + count + " size=" + size;
+    return count;
+  }
+
+  /** For testing */
+  public final int getRecomputedCount() {
+    int c = 0;
+    int end = bits.length;
+    for (int i = 0; i < end; i++) {
+      c += BYTE_COUNTS[bits[i] & 0xFF];	  // sum bits per byte
+    }
+    return c;
+  }
+
+  private static final byte[] BYTE_COUNTS = {	  // table of bits/byte
+    0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,
+    1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5,
+    1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5,
+    2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
+    1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5,
+    2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
+    2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
+    3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7,
+    1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5,
+    2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
+    2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
+    3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7,
+    2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
+    3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7,
+    3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7,
+    4, 5, 5, 6, 5, 6, 6, 7, 5, 6, 6, 7, 6, 7, 7, 8
+  };
+
+  private static String CODEC = "BitVector";
+
+  // Version before version tracking was added:
+  public final static int VERSION_PRE = -1;
+
+  // First version:
+  public final static int VERSION_START = 0;
+
+  // Changed DGaps to encode gaps between cleared bits, not
+  // set:
+  public final static int VERSION_DGAPS_CLEARED = 1;
+
+  // Increment version to change it:
+  public final static int VERSION_CURRENT = VERSION_DGAPS_CLEARED;
+
+  public int getVersion() {
+    return version;
+  }
+
+  /** Writes this vector to the file <code>name</code> in Directory
+    <code>d</code>, in a format that can be read by the constructor {@link
+    #BitVector(Directory, String, IOContext)}.  */
+  public final void write(Directory d, String name, IOContext context) throws IOException {
+    assert !(d instanceof CompoundFileDirectory);
+    IndexOutput output = d.createOutput(name, context);
+    try {
+      output.writeInt(-2);
+      CodecUtil.writeHeader(output, CODEC, VERSION_CURRENT);
+      if (isSparse()) { 
+        // sparse bit-set more efficiently saved as d-gaps.
+        writeClearedDgaps(output);
+      } else {
+        writeBits(output);
+      }
+      assert verifyCount();
+    } finally {
+      output.close();
+    }
+  }
+
+  /** Invert all bits */
+  public void invertAll() {
+    if (count != -1) {
+      count = size - count;
+    }
+    if (bits.length > 0) {
+      for(int idx=0;idx<bits.length;idx++) {
+        bits[idx] = (byte) (~bits[idx]);
+      }
+      clearUnusedBits();
+    }
+  }
+
+  private void clearUnusedBits() {
+    // Take care not to invert the "unused" bits in the
+    // last byte:
+    if (bits.length > 0) {
+      final int lastNBits = size & 7;
+      if (lastNBits != 0) {
+        final int mask = (1 << lastNBits)-1;
+        bits[bits.length-1] &= mask;
+      }
+    }
+  }
+
+  /** Set all bits */
+  public void setAll() {
+    Arrays.fill(bits, (byte) 0xff);
+    clearUnusedBits();
+    count = size;
+  }
+     
+  /** Write as a bit set */
+  private void writeBits(IndexOutput output) throws IOException {
+    output.writeInt(size());        // write size
+    output.writeInt(count());       // write count
+    output.writeBytes(bits, bits.length);
+  }
+  
+  /** Write as a d-gaps list */
+  private void writeClearedDgaps(IndexOutput output) throws IOException {
+    output.writeInt(-1);            // mark using d-gaps                         
+    output.writeInt(size());        // write size
+    output.writeInt(count());       // write count
+    int last=0;
+    int numCleared = size()-count();
+    for (int i=0; i<bits.length && numCleared>0; i++) {
+      if (bits[i] != (byte) 0xff) {
+        output.writeVInt(i-last);
+        output.writeByte(bits[i]);
+        last = i;
+        numCleared -= (8-BYTE_COUNTS[bits[i] & 0xFF]);
+        assert numCleared >= 0 || (i == (bits.length-1) && numCleared == -(8-(size&7)));
+      }
+    }
+  }
+
+  /** Indicates if the bit vector is sparse and should be saved as a d-gaps list, or dense, and should be saved as a bit set. */
+  private boolean isSparse() {
+
+    final int clearedCount = size() - count();
+    if (clearedCount == 0) {
+      return true;
+    }
+
+    final int avgGapLength = bits.length / clearedCount;
+
+    // expected number of bytes for vInt encoding of each gap
+    final int expectedDGapBytes;
+    if (avgGapLength <= (1<< 7)) {
+      expectedDGapBytes = 1;
+    } else if (avgGapLength <= (1<<14)) {
+      expectedDGapBytes = 2;
+    } else if (avgGapLength <= (1<<21)) {
+      expectedDGapBytes = 3;
+    } else if (avgGapLength <= (1<<28)) {
+      expectedDGapBytes = 4;
+    } else {
+      expectedDGapBytes = 5;
+    }
+
+    // +1 because we write the byte itself that contains the
+    // set bit
+    final int bytesPerSetBit = expectedDGapBytes + 1;
+    
+    // note: adding 32 because we start with ((int) -1) to indicate d-gaps format.
+    final long expectedBits = 32 + 8 * bytesPerSetBit * clearedCount;
+
+    // note: factor is for read/write of byte-arrays being faster than vints.  
+    final long factor = 10;  
+    return factor * expectedBits < size();
+  }
+
+  /** Constructs a bit vector from the file <code>name</code> in Directory
+    <code>d</code>, as written by the {@link #write} method.
+    */
+  public BitVector(Directory d, String name, IOContext context) throws IOException {
+    IndexInput input = d.openInput(name, context);
+
+    try {
+      final int firstInt = input.readInt();
+
+      if (firstInt == -2) {
+        // New format, with full header & version:
+        version = CodecUtil.checkHeader(input, CODEC, VERSION_START, VERSION_CURRENT);
+        size = input.readInt();
+      } else {
+        version = VERSION_PRE;
+        size = firstInt;
+      }
+      if (size == -1) {
+        if (version >= VERSION_DGAPS_CLEARED) {
+          readClearedDgaps(input);
+        } else {
+          readSetDgaps(input);
+        }
+      } else {
+        readBits(input);
+      }
+
+      if (version < VERSION_DGAPS_CLEARED) {
+        invertAll();
+      }
+
+      assert verifyCount();
+    } finally {
+      input.close();
+    }
+  }
+
+  // asserts only
+  private boolean verifyCount() {
+    assert count != -1;
+    final int countSav = count;
+    count = -1;
+    assert countSav == count(): "saved count was " + countSav + " but recomputed count is " + count;
+    return true;
+  }
+
+  /** Read as a bit set */
+  private void readBits(IndexInput input) throws IOException {
+    count = input.readInt();        // read count
+    bits = new byte[getNumBytes(size)];     // allocate bits
+    input.readBytes(bits, 0, bits.length);
+  }
+
+  /** read as a d-gaps list */ 
+  private void readSetDgaps(IndexInput input) throws IOException {
+    size = input.readInt();       // (re)read size
+    count = input.readInt();        // read count
+    bits = new byte[getNumBytes(size)];     // allocate bits
+    int last=0;
+    int n = count();
+    while (n>0) {
+      last += input.readVInt();
+      bits[last] = input.readByte();
+      n -= BYTE_COUNTS[bits[last] & 0xFF];
+      assert n >= 0;
+    }          
+  }
+
+  /** read as a d-gaps cleared bits list */ 
+  private void readClearedDgaps(IndexInput input) throws IOException {
+    size = input.readInt();       // (re)read size
+    count = input.readInt();        // read count
+    bits = new byte[getNumBytes(size)];     // allocate bits
+    Arrays.fill(bits, (byte) 0xff);
+    clearUnusedBits();
+    int last=0;
+    int numCleared = size()-count();
+    while (numCleared>0) {
+      last += input.readVInt();
+      bits[last] = input.readByte();
+      numCleared -= 8-BYTE_COUNTS[bits[last] & 0xFF];
+      assert numCleared >= 0 || (last == (bits.length-1) && numCleared == -(8-(size&7)));
+    }
+  }
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java lucene-3661/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java	2012-01-12 13:57:22.381157290 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java	2011-12-20 23:07:08.258149330 -0500
@@ -20,6 +20,7 @@
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfosFormat;
@@ -42,6 +43,8 @@
   private final DocValuesFormat docValuesFormat = new Lucene40DocValuesFormat();
   private final SegmentInfosFormat infosFormat = new Lucene40SegmentInfosFormat();
   private final NormsFormat normsFormat = new Lucene40NormsFormat();
+  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
+  
   private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
     @Override
     public PostingsFormat getPostingsFormatForField(String field) {
@@ -87,6 +90,11 @@
   public NormsFormat normsFormat() {
     return normsFormat;
   }
+  
+  @Override
+  public LiveDocsFormat liveDocsFormat() {
+    return liveDocsFormat;
+  }
 
   /** Returns the postings format that should be used for writing 
    *  new segments of <code>field</code>.


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40LiveDocsFormat.java lucene-3661/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40LiveDocsFormat.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40LiveDocsFormat.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40LiveDocsFormat.java	2012-01-28 09:32:34.204909889 -0500
@@ -0,0 +1,56 @@
+package org.apache.lucene.codecs.lucene40;
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.LiveDocsFormat;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.MutableBits;
+
+public class Lucene40LiveDocsFormat extends LiveDocsFormat {
+
+  /** Extension of deletes */
+  static final String DELETES_EXTENSION = "del";
+  
+  @Override
+  public MutableBits newLiveDocs(int size) throws IOException {
+    BitVector bitVector = new BitVector(size);
+    bitVector.invertAll();
+    return bitVector;
+  }
+
+  @Override
+  public MutableBits newLiveDocs(Bits existing) throws IOException {
+    final BitVector liveDocs = (BitVector) existing;
+    return liveDocs.clone();
+  }
+
+  @Override
+  public Bits readLiveDocs(Directory dir, SegmentInfo info, IOContext context) throws IOException {
+    String filename = IndexFileNames.fileNameFromGeneration(info.name, DELETES_EXTENSION, info.getDelGen());
+    final BitVector liveDocs = new BitVector(dir, filename, context);
+    assert liveDocs.count() == info.docCount - info.getDelCount();
+    assert liveDocs.length() == info.docCount;
+    return liveDocs;
+  }
+
+  @Override
+  public void writeLiveDocs(MutableBits bits, Directory dir, SegmentInfo info, IOContext context) throws IOException {
+    String filename = IndexFileNames.fileNameFromGeneration(info.name, DELETES_EXTENSION, info.getDelGen());
+    final BitVector liveDocs = (BitVector) bits;
+    assert liveDocs.count() == info.docCount - info.getDelCount();
+    assert liveDocs.length() == info.docCount;
+    liveDocs.write(dir, filename, context);
+  }
+
+  @Override
+  public void separateFiles(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    if (info.hasDeletions()) {
+      files.add(IndexFileNames.fileNameFromGeneration(info.name, DELETES_EXTENSION, info.getDelGen()));
+    }
+  }
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Floats.java lucene-3661/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Floats.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Floats.java	2012-01-15 22:00:26.038175848 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Floats.java	2012-01-28 09:13:41.812912525 -0500
@@ -109,7 +109,7 @@
         throws IOException {
       super(dir, id, CODEC_NAME, VERSION_CURRENT, maxDoc, context, type);
       arrayTemplate = DocValuesArray.TEMPLATES.get(type);
-      assert size == 4 || size == 8;
+      assert size == 4 || size == 8: "wrong size=" + size + " type=" + type + " id=" + id;
     }
     
     @Override


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/MappingMultiDocsEnum.java lucene-3661/lucene/src/java/org/apache/lucene/codecs/MappingMultiDocsEnum.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/MappingMultiDocsEnum.java	2012-01-12 13:57:22.425157290 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/codecs/MappingMultiDocsEnum.java	2012-01-28 09:13:41.816912525 -0500
@@ -78,6 +78,7 @@
           current = subs[upto].docsEnum;
           currentBase = mergeState.docBase[reader];
           currentMap = mergeState.docMaps[reader];
+          assert currentMap == null || currentMap.length == subs[upto].slice.length: "readerIndex=" + reader + " subs.len=" + subs.length + " len1=" + currentMap.length + " vs " + subs[upto].slice.length;
         }
       }
 


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java lucene-3661/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java	2012-01-12 13:57:22.429157290 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java	2012-01-20 10:53:22.014506393 -0500
@@ -20,6 +20,7 @@
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfosFormat;
@@ -43,6 +44,7 @@
   private final DocValuesFormat docValues = new Lucene40DocValuesFormat();
   // TODO: need a plain-text impl (using the above)
   private final NormsFormat normsFormat = new SimpleTextNormsFormat();
+  private final LiveDocsFormat liveDocs = new SimpleTextLiveDocsFormat();
   
   public SimpleTextCodec() {
     super("SimpleText");
@@ -82,4 +84,9 @@
   public NormsFormat normsFormat() {
     return normsFormat;
   }
+  
+  @Override
+  public LiveDocsFormat liveDocsFormat() {
+    return liveDocs;
+  }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextLiveDocsFormat.java lucene-3661/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextLiveDocsFormat.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextLiveDocsFormat.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextLiveDocsFormat.java	2012-01-28 13:21:05.140877997 -0500
@@ -0,0 +1,185 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.BitSet;
+import java.util.Set;
+
+import org.apache.lucene.codecs.LiveDocsFormat;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CharsRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.MutableBits;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.UnicodeUtil;
+
+/**
+ * reads/writes plaintext live docs
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextLiveDocsFormat extends LiveDocsFormat {
+
+  static final String LIVEDOCS_EXTENSION = "liv";
+  
+  final static BytesRef SIZE             = new BytesRef("size ");
+  final static BytesRef DOC              = new BytesRef("  doc ");
+  final static BytesRef END              = new BytesRef("END");
+  
+  @Override
+  public MutableBits newLiveDocs(int size) throws IOException {
+    return new SimpleTextMutableBits(size);
+  }
+
+  @Override
+  public MutableBits newLiveDocs(Bits existing) throws IOException {
+    final SimpleTextBits bits = (SimpleTextBits) existing;
+    return new SimpleTextMutableBits((BitSet)bits.bits.clone(), bits.size);
+  }
+
+  @Override
+  public Bits readLiveDocs(Directory dir, SegmentInfo info, IOContext context) throws IOException {
+    assert info.hasDeletions();
+    BytesRef scratch = new BytesRef();
+    CharsRef scratchUTF16 = new CharsRef();
+    
+    String fileName = IndexFileNames.fileNameFromGeneration(info.name, LIVEDOCS_EXTENSION, info.getDelGen());
+    IndexInput in = null;
+    boolean success = false;
+    try {
+      in = dir.openInput(fileName, context);
+      
+      SimpleTextUtil.readLine(in, scratch);
+      assert StringHelper.startsWith(scratch, SIZE);
+      int size = parseIntAt(scratch, SIZE.length, scratchUTF16);
+      
+      BitSet bits = new BitSet(size);
+      
+      SimpleTextUtil.readLine(in, scratch);
+      while (!scratch.equals(END)) {
+        assert StringHelper.startsWith(scratch, DOC);
+        int docid = parseIntAt(scratch, DOC.length, scratchUTF16);
+        bits.set(docid);
+        SimpleTextUtil.readLine(in, scratch);
+      }
+      
+      success = true;
+      return new SimpleTextBits(bits, size);
+    } finally {
+      if (success) {
+        IOUtils.close(in);
+      } else {
+        IOUtils.closeWhileHandlingException(in);
+      }
+    }
+  }
+  
+  private int parseIntAt(BytesRef bytes, int offset, CharsRef scratch) throws IOException {
+    UnicodeUtil.UTF8toUTF16(bytes.bytes, bytes.offset+offset, bytes.length-offset, scratch);
+    return ArrayUtil.parseInt(scratch.chars, 0, scratch.length);
+  }
+
+  @Override
+  public void writeLiveDocs(MutableBits bits, Directory dir, SegmentInfo info, IOContext context) throws IOException {
+    BitSet set = ((SimpleTextBits) bits).bits;
+    int size = bits.length();
+    BytesRef scratch = new BytesRef();
+    
+    String fileName = IndexFileNames.fileNameFromGeneration(info.name, LIVEDOCS_EXTENSION, info.getDelGen());
+    IndexOutput out = null;
+    boolean success = false;
+    try {
+      out = dir.createOutput(fileName, context);
+      SimpleTextUtil.write(out, SIZE);
+      SimpleTextUtil.write(out, Integer.toString(size), scratch);
+      SimpleTextUtil.writeNewline(out);
+      
+      for (int i = set.nextSetBit(0); i >= 0; i=set.nextSetBit(i + 1)) { 
+        SimpleTextUtil.write(out, DOC);
+        SimpleTextUtil.write(out, Integer.toString(i), scratch);
+        SimpleTextUtil.writeNewline(out);
+      }
+      
+      SimpleTextUtil.write(out, END);
+      SimpleTextUtil.writeNewline(out);
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(out);
+      } else {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+
+  @Override
+  public void separateFiles(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    if (info.hasDeletions()) {
+      files.add(IndexFileNames.fileNameFromGeneration(info.name, LIVEDOCS_EXTENSION, info.getDelGen()));
+    }
+  }
+  
+  // read-only
+  static class SimpleTextBits implements Bits {
+    final BitSet bits;
+    final int size;
+    
+    SimpleTextBits(BitSet bits, int size) {
+      this.bits = bits;
+      this.size = size;
+    }
+    
+    @Override
+    public boolean get(int index) {
+      return bits.get(index);
+    }
+
+    @Override
+    public int length() {
+      return size;
+    }
+  }
+  
+  // read-write
+  static class SimpleTextMutableBits extends SimpleTextBits implements MutableBits {
+
+    SimpleTextMutableBits(int size) {
+      this(new BitSet(size), size);
+      bits.set(0, size);
+    }
+    
+    SimpleTextMutableBits(BitSet bits, int size) {
+      super(bits, size);
+    }
+    
+    @Override
+    public void clear(int bit) {
+      bits.clear(bit);
+    }
+  }
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/BufferedDeletesStream.java lucene-3661/lucene/src/java/org/apache/lucene/index/BufferedDeletesStream.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/BufferedDeletesStream.java	2012-01-12 13:57:22.953157299 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/index/BufferedDeletesStream.java	2012-01-28 09:13:41.824912525 -0500
@@ -147,7 +147,7 @@
   };
   
   /** Resolves the buffered deleted Term/Query/docIDs, into
-   *  actual deleted docIDs in the liveDocs BitVector for
+   *  actual deleted docIDs in the liveDocs MutableBits for
    *  each SegmentReader. */
   public synchronized ApplyDeletesResult applyDeletes(IndexWriter.ReaderPool readerPool, List<SegmentInfo> infos) throws IOException {
     final long t0 = System.currentTimeMillis();
@@ -206,7 +206,7 @@
 
         delIDX--;
       } else if (packet != null && segGen == packet.delGen()) {
-        assert packet.isSegmentPrivate : "Packet and Segments deletegen can only match on a segment private del packet";
+        assert packet.isSegmentPrivate : "Packet and Segments deletegen can only match on a segment private del packet gen=" + segGen;
         //System.out.println("  eq");
 
         // Lock order: IW -> BD -> RP


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/CheckIndex.java lucene-3661/lucene/src/java/org/apache/lucene/index/CheckIndex.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/CheckIndex.java	2012-01-17 18:57:50.236994277 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/index/CheckIndex.java	2012-01-19 20:35:33.286626113 -0500
@@ -174,8 +174,8 @@
       /** True if this segment has pending deletions. */
       public boolean hasDeletions;
 
-      /** Name of the current deletions file name. */
-      public String deletionsFileName;
+      /** Current deletions generation. */
+      public long deletionsGen;
     
       /** Number of deleted documents. */
       public int numDeleted;
@@ -526,15 +526,14 @@
           segInfoStat.docStoreCompoundFile = info.getDocStoreIsCompoundFile();
         }
 
-        final String delFileName = info.getDelFileName();
-        if (delFileName == null){
+        if (info.hasDeletions()) {
           msg("    no deletions");
           segInfoStat.hasDeletions = false;
         }
         else{
-          msg("    has deletions [delFileName=" + delFileName + "]");
+          msg("    has deletions [delGen=" + info.getDelGen() + "]");
           segInfoStat.hasDeletions = true;
-          segInfoStat.deletionsFileName = delFileName;
+          segInfoStat.deletionsGen = info.getDelGen();
         }
         if (infoStream != null)
           infoStream.print("    test: open reader.........");


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/DirectoryReader.java lucene-3661/lucene/src/java/org/apache/lucene/index/DirectoryReader.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/DirectoryReader.java	2012-01-12 13:57:22.941157299 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/index/DirectoryReader.java	2012-01-28 10:45:18.120899740 -0500
@@ -166,7 +166,9 @@
           } else {
             readerShared[i] = false;
             // Steal the ref returned by SegmentReader ctor:
-            newReaders[i] = new SegmentReader(infos.info(i), newReaders[i], IOContext.READ);
+            assert infos.info(i).dir == newReaders[i].getSegmentInfo().dir;
+            assert infos.info(i).hasDeletions();
+            newReaders[i] = new SegmentReader(infos.info(i), newReaders[i].core, IOContext.READ);
           }
         }
         success = true;


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java lucene-3661/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java	2012-01-12 13:57:22.945157299 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java	2012-01-28 09:13:41.824912525 -0500
@@ -30,12 +30,12 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.FlushInfo;
 import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.BitVector;
 import org.apache.lucene.util.Counter;
 import org.apache.lucene.util.ByteBlockPool.Allocator;
 import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
 import org.apache.lucene.util.InfoStream;
 import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.MutableBits;
 
 public class DocumentsWriterPerThread {
 
@@ -114,13 +114,15 @@
   static class FlushedSegment {
     final SegmentInfo segmentInfo;
     final BufferedDeletes segmentDeletes;
-    final BitVector liveDocs;
+    final MutableBits liveDocs;
+    final int delCount;
 
     private FlushedSegment(SegmentInfo segmentInfo,
-        BufferedDeletes segmentDeletes, BitVector liveDocs) {
+                           BufferedDeletes segmentDeletes, MutableBits liveDocs, int delCount) {
       this.segmentInfo = segmentInfo;
       this.segmentDeletes = segmentDeletes;
       this.liveDocs = liveDocs;
+      this.delCount = delCount;
     }
   }
 
@@ -448,11 +450,11 @@
     // happens when an exception is hit processing that
     // doc, eg if analyzer has some problem w/ the text):
     if (pendingDeletes.docIDs.size() > 0) {
-      flushState.liveDocs = new BitVector(numDocsInRAM);
-      flushState.liveDocs.invertAll();
+      flushState.liveDocs = codec.liveDocsFormat().newLiveDocs(numDocsInRAM);
       for(int delDocID : pendingDeletes.docIDs) {
         flushState.liveDocs.clear(delDocID);
       }
+      flushState.delCountOnFlush = pendingDeletes.docIDs.size();
       pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);
       pendingDeletes.docIDs.clear();
     }
@@ -475,7 +477,7 @@
       pendingDeletes.terms.clear();
       final SegmentInfo newSegment = new SegmentInfo(segment, flushState.numDocs, directory, false, flushState.codec, fieldInfos.asReadOnly());
       if (infoStream.isEnabled("DWPT")) {
-        infoStream.message("DWPT", "new segment has " + (flushState.liveDocs == null ? 0 : (flushState.numDocs - flushState.liveDocs.count())) + " deleted docs");
+        infoStream.message("DWPT", "new segment has " + (flushState.liveDocs == null ? 0 : (flushState.numDocs - flushState.delCountOnFlush)) + " deleted docs");
         infoStream.message("DWPT", "new segment has " + (newSegment.getHasVectors() ? "vectors" : "no vectors"));
         infoStream.message("DWPT", "flushedFiles=" + newSegment.files());
         infoStream.message("DWPT", "flushed codec=" + newSegment.getCodec());
@@ -504,7 +506,7 @@
       doAfterFlush();
       success = true;
 
-      return new FlushedSegment(newSegment, segmentDeletes, flushState.liveDocs);
+      return new FlushedSegment(newSegment, segmentDeletes, flushState.liveDocs, flushState.delCountOnFlush);
     } finally {
       if (!success) {
         if (segment != null) {


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java lucene-3661/lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java	2012-01-19 08:21:20.890728582 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java	2012-01-28 09:35:30.212909481 -0500
@@ -28,7 +28,6 @@
 import org.apache.lucene.codecs.TermStats;
 import org.apache.lucene.codecs.TermsConsumer;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.util.BitVector;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.RamUsageEstimator;
@@ -461,11 +460,15 @@
           // Mark it deleted.  TODO: we could also skip
           // writing its postings; this would be
           // deterministic (just for this Term's docs).
+          
+          // TODO: can we do this reach-around in a cleaner way????
           if (state.liveDocs == null) {
-            state.liveDocs = new BitVector(state.numDocs);
-            state.liveDocs.invertAll();
+            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);
+          }
+          if (state.liveDocs.get(docID)) {
+            state.delCountOnFlush++;
+            state.liveDocs.clear(docID);
           }
-          state.liveDocs.clear(docID);
         }
 
         totTF += termDocFreq;


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/IndexFileNames.java lucene-3661/lucene/src/java/org/apache/lucene/index/IndexFileNames.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/IndexFileNames.java	2012-01-12 13:57:22.945157299 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/index/IndexFileNames.java	2012-01-19 21:15:29.170620540 -0500
@@ -57,9 +57,6 @@
   /** Extension of compound file for doc store files*/
   public static final String COMPOUND_FILE_STORE_EXTENSION = "cfx";
 
-  /** Extension of deletes */
-  public static final String DELETES_EXTENSION = "del";
-
   /**
    * This array contains all filename extensions used by
    * Lucene's index files, with one exception, namely the
@@ -70,7 +67,6 @@
   public static final String INDEX_EXTENSIONS[] = new String[] {
     COMPOUND_FILE_EXTENSION,
     COMPOUND_FILE_ENTRIES_EXTENSION,
-    DELETES_EXTENSION,
     GEN_EXTENSION,
     COMPOUND_FILE_STORE_EXTENSION,
   };


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/IndexWriter.java lucene-3661/lucene/src/java/org/apache/lucene/index/IndexWriter.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/IndexWriter.java	2012-01-26 15:46:00.605259716 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/index/IndexWriter.java	2012-01-28 13:04:21.044880334 -0500
@@ -30,10 +30,10 @@
 import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.atomic.AtomicInteger;
-import java.util.regex.Pattern;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.index.DocumentsWriterPerThread.FlushedSegment;
 import org.apache.lucene.index.FieldInfos.FieldNumberBiMap;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
@@ -48,10 +48,11 @@
 import org.apache.lucene.store.Lock;
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.store.MergeInfo;
-import org.apache.lucene.util.BitVector;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.Constants;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.InfoStream;
+import org.apache.lucene.util.MutableBits;
 import org.apache.lucene.util.ThreadInterruptedException;
 import org.apache.lucene.util.TwoPhaseCommit;
 
@@ -416,7 +417,7 @@
     // docs, and it's copy-on-write (cloned whenever we need
     // to change it but it's been shared to an external NRT
     // reader).
-    public BitVector liveDocs;
+    public Bits liveDocs;
 
     // How many further deletions we've done against
     // liveDocs vs when we loaded it or last wrote it:
@@ -446,6 +447,24 @@
       return rc > myRefCounts;
     }
 
+    // Call only from assert!
+    public synchronized boolean verifyDocCounts() {
+      int count;
+      if (liveDocs != null) {
+        count = 0;
+        for(int docID=0;docID<info.docCount;docID++) {
+          if (liveDocs.get(docID)) {
+            count++;
+          }
+        }
+      } else {
+        count = info.docCount;
+      }
+
+      assert info.docCount - info.getDelCount() - pendingDeleteCount == count: "info.docCount=" + info.docCount + " info.getDelCount()=" + info.getDelCount() + " pendingDeleteCount=" + pendingDeleteCount + " count=" + count;;
+      return true;
+    }
+
     // Returns true if any reader remains
     public synchronized boolean removeReader(SegmentReader sr, boolean drop) throws IOException {
       if (sr == reader) {
@@ -468,17 +487,6 @@
       return reader != null || mergeReader != null;
     }
 
-    // Called only from assert
-    private boolean countsMatch() {
-      if (liveDocs == null) {
-        assert pendingDeleteCount == 0;
-      } else {
-        assert liveDocs.count() == info.docCount - info.getDelCount() - pendingDeleteCount :
-        "liveDocs.count()=" + liveDocs.count() + " info.docCount=" + info.docCount + " info.delCount=" + info.getDelCount() + " pendingDelCount=" + pendingDeleteCount;
-      }
-      return true;
-    }
-
     // Get reader for searching/deleting
     public synchronized SegmentReader getReader(IOContext context) throws IOException {
       //System.out.println("  livedocs=" + rld.liveDocs);
@@ -486,7 +494,7 @@
       if (reader == null) {
         reader = new SegmentReader(info, config.getReaderTermsIndexDivisor(), context);
         if (liveDocs == null) {
-          liveDocs = (BitVector) reader.getLiveDocs();
+          liveDocs = reader.getLiveDocs();
         }
         //System.out.println("ADD seg=" + rld.info + " isMerge=" + isMerge + " " + readerMap.size() + " in pool");
       }
@@ -513,7 +521,7 @@
         } else {
           mergeReader = new SegmentReader(info, -1, context);
           if (liveDocs == null) {
-            liveDocs = (BitVector) mergeReader.getLiveDocs();
+            liveDocs = mergeReader.getLiveDocs();
           }
         }
       }
@@ -526,8 +534,10 @@
     public synchronized boolean delete(int docID) {
       assert liveDocs != null;
       assert docID >= 0 && docID < liveDocs.length();
-      final boolean didDelete = liveDocs.getAndClear(docID);
+      assert !shared;
+      final boolean didDelete = liveDocs.get(docID);
       if (didDelete) {
+       ((MutableBits) liveDocs).clear(docID);
         pendingDeleteCount++;
         //System.out.println("  new del seg=" + info + " docID=" + docID + " pendingDelCount=" + pendingDeleteCount + " totDelCount=" + (info.docCount-liveDocs.count()));
       }
@@ -557,17 +567,16 @@
         getReader(context).decRef();
         assert reader != null;
       }
-      assert countsMatch();
       shared = true;
       if (liveDocs != null) {
-        return new SegmentReader(reader, liveDocs, info.docCount - info.getDelCount() - pendingDeleteCount);
+        return new SegmentReader(reader.getSegmentInfo(), reader.core, liveDocs, info.docCount - info.getDelCount() - pendingDeleteCount);
       } else {
         reader.incRef();
         return reader;
       }
     }
 
-    public synchronized void initWritableLiveDocs() {
+    public synchronized void initWritableLiveDocs() throws IOException {
       assert Thread.holdsLock(IndexWriter.this);
       //System.out.println("initWritableLivedocs seg=" + info + " liveDocs=" + liveDocs + " shared=" + shared);
       if (shared) {
@@ -575,12 +584,12 @@
         // SegmentReader sharing the current liveDocs
         // instance; must now make a private clone so we can
         // change it:
+        LiveDocsFormat liveDocsFormat = info.getCodec().liveDocsFormat();
         if (liveDocs == null) {
           //System.out.println("create BV seg=" + info);
-          liveDocs = new BitVector(info.docCount);
-          liveDocs.setAll();
+          liveDocs = liveDocsFormat.newLiveDocs(info.docCount);
         } else {
-          liveDocs = (BitVector) liveDocs.clone();
+          liveDocs = liveDocsFormat.newLiveDocs(liveDocs);
         }
         shared = false;
       } else {
@@ -588,11 +597,10 @@
       }
     }
 
-    public synchronized BitVector getReadOnlyLiveDocs() {
+    public synchronized Bits getReadOnlyLiveDocs() {
       //System.out.println("getROLiveDocs seg=" + info);
       assert Thread.holdsLock(IndexWriter.this);
       shared = true;
-      assert countsMatch();
       //if (liveDocs != null) {
       //System.out.println("  liveCount=" + liveDocs.count());
       //}
@@ -611,29 +619,20 @@
         // Save in case we need to rollback on failure:
         final SegmentInfo sav = (SegmentInfo) info.clone();
         info.advanceDelGen();
+        info.setDelCount(info.getDelCount() + pendingDeleteCount);
 
         // We can write directly to the actual name (vs to a
         // .tmp & renaming it) because the file is not live
         // until segments file is written:
-        final String delFileName = info.getDelFileName();
         boolean success = false;
         try {
-          liveDocs.write(dir, delFileName, IOContext.DEFAULT);
+          info.getCodec().liveDocsFormat().writeLiveDocs((MutableBits)liveDocs, dir, info, IOContext.DEFAULT);
           success = true;
         } finally {
           if (!success) {
             info.reset(sav);
-            try {
-              dir.deleteFile(delFileName);
-            } catch (Throwable t) {
-              // Suppress this so we keep throwing the
-              // original exception
-            }
           }
         }
-        assert (info.docCount - liveDocs.count()) == info.getDelCount() + pendingDeleteCount:
-           "delete count mismatch during commit: seg=" + info + " info.delCount=" + info.getDelCount() + " vs BitVector=" + (info.docCount-liveDocs.count() + " pendingDelCount=" + pendingDeleteCount);
-        info.setDelCount(info.getDelCount() + pendingDeleteCount);
         pendingDeleteCount = 0;
         return true;
       } else {
@@ -2205,7 +2204,7 @@
 
   /**
    * Prepares the {@link SegmentInfo} for the new flushed segment and persists
-   * the deleted documents {@link BitVector}. Use
+   * the deleted documents {@link MutableBits}. Use
    * {@link #publishFlushedSegment(SegmentInfo, FrozenBufferedDeletes)} to
    * publish the returned {@link SegmentInfo} together with its segment private
    * delete packet.
@@ -2252,33 +2251,23 @@
       // Must write deleted docs after the CFS so we don't
       // slurp the del file into CFS:
       if (flushedSegment.liveDocs != null) {
-        final int delCount = flushedSegment.segmentInfo.docCount - flushedSegment.liveDocs.count();
+        final int delCount = flushedSegment.delCount;
         assert delCount > 0;
         newSegment.setDelCount(delCount);
         newSegment.advanceDelGen();
-        final String delFileName = newSegment.getDelFileName();
         if (infoStream.isEnabled("IW")) {
-          infoStream.message("IW", "flush: write " + delCount + " deletes to " + delFileName);
-        }
-        boolean success2 = false;
-        try {
-          // TODO: in the NRT case it'd be better to hand
-          // this del vector over to the
-          // shortly-to-be-opened SegmentReader and let it
-          // carry the changes; there's no reason to use
-          // filesystem as intermediary here.
-          flushedSegment.liveDocs.write(directory, delFileName, context);
-          success2 = true;
-        } finally {
-          if (!success2) {
-            try {
-              directory.deleteFile(delFileName);
-            } catch (Throwable t) {
-              // suppress this so we keep throwing the
-              // original exception
-            }
-          }
+          infoStream.message("IW", "flush: write " + delCount + " deletes gen=" + flushedSegment.segmentInfo.getDelGen());
         }
+
+        // TODO: in the NRT case it'd be better to hand
+        // this del vector over to the
+        // shortly-to-be-opened SegmentReader and let it
+        // carry the changes; there's no reason to use
+        // filesystem as intermediary here.
+          
+        SegmentInfo info = flushedSegment.segmentInfo;
+        Codec codec = info.getCodec();
+        codec.liveDocsFormat().writeLiveDocs(flushedSegment.liveDocs, directory, info, context);
       }
 
       success = true;
@@ -3032,8 +3021,8 @@
       SegmentInfo info = sourceSegments.get(i);
       minGen = Math.min(info.getBufferedDeletesGen(), minGen);
       final int docCount = info.docCount;
-      final BitVector prevLiveDocs = merge.readerLiveDocs.get(i);
-      final BitVector currentLiveDocs;
+      final Bits prevLiveDocs = merge.readerLiveDocs.get(i);
+      final Bits currentLiveDocs;
       ReadersAndLiveDocs rld = readerPool.get(info, false);
       // We enrolled in mergeInit:
       assert rld != null;
@@ -3052,7 +3041,13 @@
         // newly flushed deletes but mapping them to the new
         // docIDs.
 
-        if (currentLiveDocs.count() < prevLiveDocs.count()) {
+        // Since we copy-on-write, if any new deletes were
+        // applied after merging has started, we can just
+        // check if the before/after liveDocs have changed.
+        // If so, we must carefully merge the liveDocs one
+        // doc at a time:
+        if (currentLiveDocs != prevLiveDocs) {
+
           // This means this segment received new deletes
           // since we started the merge, so we
           // must merge them:
@@ -3071,8 +3066,7 @@
             }
           }
         } else {
-          assert currentLiveDocs.count() == prevLiveDocs.count(): "currentLiveDocs.count()==" + currentLiveDocs.count() + " vs prevLiveDocs.count()=" + prevLiveDocs.count() + " info=" + info;
-          docUpto += currentLiveDocs.count();
+          docUpto += info.docCount - info.getDelCount() - rld.pendingDeleteCount;
         }
       } else if (currentLiveDocs != null) {
         // This segment had no deletes before but now it
@@ -3576,13 +3570,12 @@
     }
 
     merge.readers = new ArrayList<SegmentReader>();
-    merge.readerLiveDocs = new ArrayList<BitVector>();
+    merge.readerLiveDocs = new ArrayList<Bits>();
 
     // This is try/finally to make sure merger's readers are
     // closed:
     boolean success = false;
     try {
-      int totDocCount = 0;
       int segUpto = 0;
       while(segUpto < sourceSegments.size()) {
 
@@ -3595,13 +3588,15 @@
         assert reader != null;
 
         // Carefully pull the most recent live docs:
-        final BitVector liveDocs;
+        final Bits liveDocs;
         synchronized(this) {
           // Must sync to ensure BufferedDeletesStream
           // cannot change liveDocs/pendingDeleteCount while
           // we pull a copy:
           liveDocs = rld.getReadOnlyLiveDocs();
 
+          assert rld.verifyDocCounts();
+
           if (infoStream.isEnabled("IW")) {
             if (rld.pendingDeleteCount != 0) {
               infoStream.message("IW", "seg=" + info + " delCount=" + info.getDelCount() + " pendingDelCount=" + rld.pendingDeleteCount);
@@ -3612,23 +3607,16 @@
             }
           }
         }
-
         merge.readerLiveDocs.add(liveDocs);
         merge.readers.add(reader);
-
-        if (liveDocs == null || liveDocs.count() > 0) {
+        final int delCount = rld.pendingDeleteCount + info.getDelCount();
+        assert delCount <= info.docCount;
+        if (delCount < info.docCount) {
           merger.add(reader, liveDocs);
-          totDocCount += liveDocs == null ? reader.maxDoc() : liveDocs.count();
-        } else {
-          //System.out.println("  skip seg: fully deleted");
         }
         segUpto++;
       }
 
-      if (infoStream.isEnabled("IW")) {
-        infoStream.message("IW", "merge: total " + totDocCount + " docs");
-      }
-
       merge.checkAborted(directory);
 
       // This is where all the work happens:
@@ -3639,11 +3627,9 @@
       merge.info.setCodec(codec);
 
       if (infoStream.isEnabled("IW")) {
-        infoStream.message("IW", "merge codec=" + codec);
+        infoStream.message("IW", "merge codec=" + codec + " docCount=" + mergedDocCount);
       }
 
-      assert mergedDocCount == totDocCount: "mergedDocCount=" + mergedDocCount + " vs " + totDocCount;
-
       // Very important to do this before opening the reader
       // because codec must know if prox was written for
       // this segment:
@@ -4089,11 +4075,8 @@
     Collection<String> files = info.files();
     CompoundFileDirectory cfsDir = new CompoundFileDirectory(directory, fileName, context, true);
     try {
+      assert assertNoSeparateFiles(files, directory, info);
       for (String file : files) {
-        assert !IndexFileNames.matchesExtension(file, IndexFileNames.DELETES_EXTENSION) 
-                  : ".del file is not allowed in .cfs: " + file;
-        assert !isSeparateNormsFile(file) 
-                  : "separate norms file (.s[0-9]+) is not allowed in .cfs: " + file;
         directory.copy(cfsDir, file, file, context);
         checkAbort.work(directory.fileLength(file));
       }
@@ -4106,15 +4089,17 @@
   
   
   /**
-   * Returns true if the given filename ends with the separate norms file
-   * pattern: {@code SEPARATE_NORMS_EXTENSION + "[0-9]+"}.
-   * @deprecated only for asserting
-   */
-  @Deprecated
-  private static boolean isSeparateNormsFile(String filename) {
-    int idx = filename.lastIndexOf('.');
-    if (idx == -1) return false;
-    String ext = filename.substring(idx + 1);
-    return Pattern.matches("s[0-9]+", ext);
+   * used only by assert: checks that filenames about to be put in cfs belong.
+   */
+  private static boolean assertNoSeparateFiles(Collection<String> files, 
+      Directory dir, SegmentInfo info) throws IOException {
+    // maybe this is overkill, but codec naming clashes would be bad.
+    Set<String> separateFiles = new HashSet<String>();
+    info.getCodec().separateFiles(dir, info, separateFiles);
+    
+    for (String file : files) {
+      assert !separateFiles.contains(file) : file + " should not go in CFS!";
+    }
+    return true;
   }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/MergePolicy.java lucene-3661/lucene/src/java/org/apache/lucene/index/MergePolicy.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/MergePolicy.java	2012-01-12 13:57:22.953157299 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/index/MergePolicy.java	2012-01-20 09:45:34.578515857 -0500
@@ -24,7 +24,7 @@
 
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.MergeInfo;
-import org.apache.lucene.util.BitVector;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.SetOnce.AlreadySetException;
 import org.apache.lucene.util.SetOnce;
 
@@ -74,7 +74,7 @@
     int maxNumSegments = -1;        // used by IndexWriter
     public long estimatedMergeBytes;       // used by IndexWriter
     List<SegmentReader> readers;        // used by IndexWriter
-    List<BitVector> readerLiveDocs;   // used by IndexWriter
+    List<Bits> readerLiveDocs;      // used by IndexWriter
     public final List<SegmentInfo> segments;
     public final int totalDocCount;
     boolean aborted;


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/MergeState.java lucene-3661/lucene/src/java/org/apache/lucene/index/MergeState.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/MergeState.java	2012-01-12 13:57:22.945157299 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/index/MergeState.java	2012-01-28 13:04:21.044880334 -0500
@@ -41,10 +41,10 @@
   }
 
   public FieldInfos fieldInfos;
-  public List<IndexReaderAndLiveDocs> readers;    // Readers & liveDocs being merged
-  public int[][] docMaps;                         // Maps docIDs around deletions
-  public int[] docBase;                           // New docID base per reader
-  public int mergedDocCount;                      // Total # merged docs
+  public List<IndexReaderAndLiveDocs> readers;        // Readers & liveDocs being merged
+  public int[][] docMaps;                             // Maps docIDs around deletions
+  public int[] docBase;                               // New docID base per reader
+  public int mergedDocCount;                          // Total # merged docs
   public CheckAbort checkAbort;
   public InfoStream infoStream;
 


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/SegmentInfo.java lucene-3661/lucene/src/java/org/apache/lucene/index/SegmentInfo.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/SegmentInfo.java	2012-01-12 13:57:22.949157299 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/index/SegmentInfo.java	2012-01-20 13:07:50.010487627 -0500
@@ -326,16 +326,6 @@
     return si;
   }
 
-  public String getDelFileName() {
-    if (delGen == NO) {
-      // In this case we know there is no deletion filename
-      // against this segment
-      return null;
-    } else {
-      return IndexFileNames.fileNameFromGeneration(name, IndexFileNames.DELETES_EXTENSION, delGen);
-    }
-  }
-
   /**
    * @deprecated separate norms are not supported in >= 4.0
    */
@@ -494,6 +484,9 @@
     } else {
       codec.files(dir, this, fileSet);
     }
+    
+    // regardless of compound file setting: these files are always in the directory
+    codec.separateFiles(dir, this, fileSet);
 
     if (docStoreOffset != -1) {
       // We are sharing doc stores (stored fields, term
@@ -505,18 +498,6 @@
       }
     }
 
-    String delFileName = IndexFileNames.fileNameFromGeneration(name, IndexFileNames.DELETES_EXTENSION, delGen);
-    if (delFileName != null && (delGen >= YES || dir.fileExists(delFileName))) {
-      fileSet.add(delFileName);
-    }
-
-    // because separate norm files are unconditionally stored outside cfs,
-    // we must explicitly ask for their filenames if we might have separate norms:
-    // remove this when 3.x indexes are no longer supported
-    if (normGen != null) {
-      codec.normsFormat().separateFiles(dir, this, fileSet);
-    }
-
     files = new ArrayList<String>(fileSet);
 
     return files;


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/SegmentMerger.java lucene-3661/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/SegmentMerger.java	2012-01-16 18:06:56.155436483 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/index/SegmentMerger.java	2012-01-28 13:04:21.044880334 -0500
@@ -104,16 +104,12 @@
     // IndexWriter.close(false) takes to actually stop the
     // threads.
     
-    final int numReaders = mergeState.readers.size();
-    // Remap docIDs
-    mergeState.docMaps = new int[numReaders][];
-    mergeState.docBase = new int[numReaders];
-    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[numReaders];
-    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];
+    mergeState.mergedDocCount = setDocMaps();
 
     mergeFieldInfos();
     setMatchingSegmentReaders();
-    mergeState.mergedDocCount = mergeFields();
+    int numMerged = mergeFields();
+    assert numMerged == mergeState.mergedDocCount;
 
     final SegmentWriteState segmentWriteState = new SegmentWriteState(mergeState.infoStream, directory, segment, mergeState.fieldInfos, mergeState.mergedDocCount, termIndexInterval, codec, null, context);
     mergeTerms(segmentWriteState);
@@ -124,7 +120,7 @@
     }
 
     if (mergeState.fieldInfos.hasVectors()) {
-      int numMerged = mergeVectors();
+      numMerged = mergeVectors();
       assert numMerged == mergeState.mergedDocCount;
     }
 
@@ -283,37 +279,31 @@
     }
   }
 
-  private final void mergeTerms(SegmentWriteState segmentWriteState) throws CorruptIndexException, IOException {
-    int docBase = 0;
-    
-    final List<Fields> fields = new ArrayList<Fields>();
-    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();
-
-    for(MergeState.IndexReaderAndLiveDocs r : mergeState.readers) {
-      final Fields f = r.reader.fields();
-      final int maxDoc = r.reader.maxDoc();
-      if (f != null) {
-        slices.add(new ReaderUtil.Slice(docBase, maxDoc, fields.size()));
-        fields.add(f);
-      }
-      docBase += maxDoc;
-    }
-
+  // NOTE: removes any "all deleted" readers from mergeState.readers
+  private int setDocMaps() throws IOException {
     final int numReaders = mergeState.readers.size();
 
-    docBase = 0;
+    // Remap docIDs
+    mergeState.docMaps = new int[numReaders][];
+    mergeState.docBase = new int[numReaders];
+    mergeState.dirPayloadProcessor = new PayloadProcessorProvider.DirPayloadProcessor[numReaders];
+    mergeState.currentPayloadProcessor = new PayloadProcessorProvider.PayloadProcessor[numReaders];
+
+    int docBase = 0;
 
-    for(int i=0;i<numReaders;i++) {
+    int i = 0;
+    while(i < mergeState.readers.size()) {
 
       final MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(i);
 
       mergeState.docBase[i] = docBase;
       final int maxDoc = reader.reader.maxDoc();
-      if (reader.liveDocs != null) {
+      final int docCount;
+      final Bits liveDocs = reader.liveDocs;
+      final int[] docMap;
+      if (liveDocs != null) {
         int delCount = 0;
-        final Bits liveDocs = reader.liveDocs;
-        assert liveDocs != null;
-        final int[] docMap = mergeState.docMaps[i] = new int[maxDoc];
+        docMap = new int[maxDoc];
         int newDocID = 0;
         for(int j=0;j<maxDoc;j++) {
           if (!liveDocs.get(j)) {
@@ -323,14 +313,41 @@
             docMap[j] = newDocID++;
           }
         }
-        docBase += maxDoc - delCount;
+        docCount = maxDoc - delCount;
       } else {
-        docBase += maxDoc;
+        docCount = maxDoc;
+        docMap = null;
       }
 
+      mergeState.docMaps[i] = docMap;
+      docBase += docCount;
+
       if (mergeState.payloadProcessorProvider != null) {
         mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(reader.reader.directory());
       }
+
+      i++;
+    }
+
+    return docBase;
+  }
+
+  private final void mergeTerms(SegmentWriteState segmentWriteState) throws CorruptIndexException, IOException {
+    
+    final List<Fields> fields = new ArrayList<Fields>();
+    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();
+
+    int docBase = 0;
+
+    for(int readerIndex=0;readerIndex<mergeState.readers.size();readerIndex++) {
+      final MergeState.IndexReaderAndLiveDocs r = mergeState.readers.get(readerIndex);
+      final Fields f = r.reader.fields();
+      final int maxDoc = r.reader.maxDoc();
+      if (f != null) {
+        slices.add(new ReaderUtil.Slice(docBase, maxDoc, readerIndex));
+        fields.add(f);
+      }
+      docBase += maxDoc;
     }
 
     final FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(segmentWriteState);


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/SegmentReader.java lucene-3661/lucene/src/java/org/apache/lucene/index/SegmentReader.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/SegmentReader.java	2012-01-16 18:06:56.155436483 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/index/SegmentReader.java	2012-01-28 10:59:58.556897691 -0500
@@ -25,7 +25,6 @@
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.search.FieldCache; // javadocs
 import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.BitVector;
 import org.apache.lucene.util.Bits;
 
 /**
@@ -36,14 +35,14 @@
   private final SegmentInfo si;
   private final ReaderContext readerContext = new AtomicReaderContext(this);
   
-  private final BitVector liveDocs;
+  private final Bits liveDocs;
 
   // Normally set to si.docCount - si.delDocCount, unless we
   // were created as an NRT reader from IW, in which case IW
   // tells us the docCount:
   private final int numDocs;
 
-  private final SegmentCoreReaders core;
+  final SegmentCoreReaders core;
 
   /**
    * @throws CorruptIndexException if the index is corrupt
@@ -56,13 +55,12 @@
     try {
       if (si.hasDeletions()) {
         // NOTE: the bitvector is stored using the regular directory, not cfs
-        liveDocs = new BitVector(directory(), si.getDelFileName(), new IOContext(IOContext.READ, true));
+        liveDocs = si.getCodec().liveDocsFormat().readLiveDocs(directory(), si, new IOContext(IOContext.READ, true));
       } else {
         assert si.getDelCount() == 0;
         liveDocs = null;
       }
       numDocs = si.docCount - si.getDelCount();
-      assert checkLiveCounts(false);
       success = true;
     } finally {
       // With lock-less commits, it's entirely possible (and
@@ -76,46 +74,26 @@
     }
   }
 
-  // TODO: really these next 2 ctors could take
-  // SegmentCoreReaders... that's all we do w/ the parent
-  // SR:
-
   // Create new SegmentReader sharing core from a previous
   // SegmentReader and loading new live docs from a new
   // deletes file.  Used by openIfChanged.
-  SegmentReader(SegmentInfo si, SegmentReader parent, IOContext context) throws IOException {
-    assert si.dir == parent.getSegmentInfo().dir;
-    this.si = si;
-
-    // It's no longer possible to unDeleteAll, so, we can
-    // only be created if we have deletions:
-    assert si.hasDeletions();
-
-    // ... but load our own deleted docs:
-    liveDocs = new BitVector(si.dir, si.getDelFileName(), context);
-    numDocs = si.docCount - si.getDelCount();
-    assert checkLiveCounts(false);
-
-    // We share core w/ parent:
-    parent.core.incRef();
-    core = parent.core;
+  SegmentReader(SegmentInfo si, SegmentCoreReaders core, IOContext context) throws IOException {
+    this(si, core, si.getCodec().liveDocsFormat().readLiveDocs(si.dir, si, context), si.docCount - si.getDelCount());
   }
 
   // Create new SegmentReader sharing core from a previous
   // SegmentReader and using the provided in-memory
   // liveDocs.  Used by IndexWriter to provide a new NRT
   // reader:
-  SegmentReader(SegmentReader parent, BitVector liveDocs, int numDocs) throws IOException {
-    this.si = parent.si;
-    parent.core.incRef();
-    this.core = parent.core;
+  SegmentReader(SegmentInfo si, SegmentCoreReaders core, Bits liveDocs, int numDocs) throws IOException {
+    this.si = si;
+    this.core = core;
+    core.incRef();
 
     assert liveDocs != null;
     this.liveDocs = liveDocs;
 
     this.numDocs = numDocs;
-
-    assert checkLiveCounts(true);
   }
 
   @Override
@@ -124,27 +102,6 @@
     return liveDocs;
   }
 
-  private boolean checkLiveCounts(boolean isNRT) throws IOException {
-    if (liveDocs != null) {
-      if (liveDocs.size() != si.docCount) {
-        throw new CorruptIndexException("document count mismatch: deleted docs count " + liveDocs.size() + " vs segment doc count " + si.docCount + " segment=" + si.name);
-      }
-
-      final int recomputedCount = liveDocs.getRecomputedCount();
-      // Verify BitVector is self consistent:
-      assert liveDocs.count() == recomputedCount : "live count=" + liveDocs.count() + " vs recomputed count=" + recomputedCount;
-
-      // Verify our docCount matches:
-      assert numDocs == recomputedCount :
-      "delete count mismatch: numDocs=" + numDocs + " vs BitVector=" + (si.docCount-recomputedCount);
-
-      assert isNRT || si.docCount - si.getDelCount() == recomputedCount :
-        "si.docCount=" + si.docCount + "si.getDelCount()=" + si.getDelCount() + " recomputedCount=" + recomputedCount;
-    }
-  
-    return true;
-  }
-
   @Override
   protected void doClose() throws IOException {
     //System.out.println("SR.close seg=" + si);


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/SegmentWriteState.java lucene-3661/lucene/src/java/org/apache/lucene/index/SegmentWriteState.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/SegmentWriteState.java	2012-01-12 13:57:22.957157298 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/index/SegmentWriteState.java	2012-01-25 19:25:20.737430077 -0500
@@ -20,8 +20,8 @@
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.BitVector;
 import org.apache.lucene.util.InfoStream;
+import org.apache.lucene.util.MutableBits;
 
 /**
  * @lucene.experimental
@@ -32,6 +32,7 @@
   public final String segmentName;
   public final FieldInfos fieldInfos;
   public final int numDocs;
+  public int delCountOnFlush;
 
   // Deletes to apply while we are flushing the segment.  A
   // Term is enrolled in here if it was deleted at one
@@ -41,7 +42,7 @@
   public final BufferedDeletes segDeletes;
 
   // Lazily created:
-  public BitVector liveDocs;
+  public MutableBits liveDocs;
 
   public final Codec codec;
   public final String segmentSuffix;
@@ -83,5 +84,6 @@
     codec = state.codec;
     this.segmentSuffix = segmentSuffix;
     segDeletes = state.segDeletes;
+    delCountOnFlush = state.delCountOnFlush;
   }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/util/BitVector.java lucene-3661/lucene/src/java/org/apache/lucene/util/BitVector.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/util/BitVector.java	2012-01-12 13:57:23.009157300 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/util/BitVector.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,416 +0,0 @@
-package org.apache.lucene.util;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.store.CompoundFileDirectory;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-
-/** Optimized implementation of a vector of bits.  This is more-or-less like
- *  java.util.BitSet, but also includes the following:
- *  <ul>
- *  <li>a count() method, which efficiently computes the number of one bits;</li>
- *  <li>optimized read from and write to disk;</li>
- *  <li>inlinable get() method;</li>
- *  <li>store and load, as bit set or d-gaps, depending on sparseness;</li> 
- *  </ul>
- *
- *  @lucene.internal
- */
-public final class BitVector implements Cloneable, Bits {
-
-  private byte[] bits;
-  private int size;
-  private int count;
-  private int version;
-
-  /** Constructs a vector capable of holding <code>n</code> bits. */
-  public BitVector(int n) {
-    size = n;
-    bits = new byte[getNumBytes(size)];
-    count = 0;
-  }
-
-  BitVector(byte[] bits, int size) {
-    this.bits = bits;
-    this.size = size;
-    count = -1;
-  }
-  
-  private int getNumBytes(int size) {
-    int bytesLength = size >>> 3;
-    if ((size & 7) != 0) {
-      bytesLength++;
-    }
-    return bytesLength;
-  }
-  
-  @Override
-  public Object clone() {
-    byte[] copyBits = new byte[bits.length];
-    System.arraycopy(bits, 0, copyBits, 0, bits.length);
-    BitVector clone = new BitVector(copyBits, size);
-    clone.count = count;
-    return clone;
-  }
-  
-  /** Sets the value of <code>bit</code> to one. */
-  public final void set(int bit) {
-    if (bit >= size) {
-      throw new ArrayIndexOutOfBoundsException("bit=" + bit + " size=" + size);
-    }
-    bits[bit >> 3] |= 1 << (bit & 7);
-    count = -1;
-  }
-
-  /** Sets the value of <code>bit</code> to true, and
-   *  returns true if bit was already set */
-  public final boolean getAndSet(int bit) {
-    if (bit >= size) {
-      throw new ArrayIndexOutOfBoundsException("bit=" + bit + " size=" + size);
-    }
-    final int pos = bit >> 3;
-    final int v = bits[pos];
-    final int flag = 1 << (bit & 7);
-    if ((flag & v) != 0)
-      return true;
-    else {
-      bits[pos] = (byte) (v | flag);
-      if (count != -1) {
-        count++;
-        assert count <= size;
-      }
-      return false;
-    }
-  }
-
-  /** Sets the value of <code>bit</code> to zero. */
-  public final void clear(int bit) {
-    if (bit >= size) {
-      throw new ArrayIndexOutOfBoundsException(bit);
-    }
-    bits[bit >> 3] &= ~(1 << (bit & 7));
-    count = -1;
-  }
-
-  public final boolean getAndClear(int bit) {
-    if (bit >= size) {
-      throw new ArrayIndexOutOfBoundsException(bit);
-    }
-    final int pos = bit >> 3;
-    final int v = bits[pos];
-    final int flag = 1 << (bit & 7);
-    if ((flag & v) == 0) {
-      return false;
-    } else {
-      bits[pos] &= ~flag;
-      if (count != -1) {
-        count--;
-        assert count >= 0;
-      }
-      return true;
-    }
-  }
-
-  /** Returns <code>true</code> if <code>bit</code> is one and
-    <code>false</code> if it is zero. */
-  public final boolean get(int bit) {
-    assert bit >= 0 && bit < size: "bit " + bit + " is out of bounds 0.." + (size-1);
-    return (bits[bit >> 3] & (1 << (bit & 7))) != 0;
-  }
-
-  /** Returns the number of bits in this vector.  This is also one greater than
-    the number of the largest valid bit number. */
-  public final int size() {
-    return size;
-  }
-
-  @Override
-  public int length() {
-    return size;
-  }
-
-  /** Returns the total number of one bits in this vector.  This is efficiently
-    computed and cached, so that, if the vector is not changed, no
-    recomputation is done for repeated calls. */
-  public final int count() {
-    // if the vector has been modified
-    if (count == -1) {
-      int c = 0;
-      int end = bits.length;
-      for (int i = 0; i < end; i++) {
-        c += BYTE_COUNTS[bits[i] & 0xFF];	  // sum bits per byte
-      }
-      count = c;
-    }
-    assert count <= size: "count=" + count + " size=" + size;
-    return count;
-  }
-
-  /** For testing */
-  public final int getRecomputedCount() {
-    int c = 0;
-    int end = bits.length;
-    for (int i = 0; i < end; i++) {
-      c += BYTE_COUNTS[bits[i] & 0xFF];	  // sum bits per byte
-    }
-    return c;
-  }
-
-  private static final byte[] BYTE_COUNTS = {	  // table of bits/byte
-    0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,
-    1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5,
-    1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5,
-    2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
-    1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5,
-    2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
-    2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
-    3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7,
-    1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5,
-    2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
-    2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
-    3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7,
-    2, 3, 3, 4, 3, 4, 4, 5, 3, 4, 4, 5, 4, 5, 5, 6,
-    3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7,
-    3, 4, 4, 5, 4, 5, 5, 6, 4, 5, 5, 6, 5, 6, 6, 7,
-    4, 5, 5, 6, 5, 6, 6, 7, 5, 6, 6, 7, 6, 7, 7, 8
-  };
-
-  private static String CODEC = "BitVector";
-
-  // Version before version tracking was added:
-  public final static int VERSION_PRE = -1;
-
-  // First version:
-  public final static int VERSION_START = 0;
-
-  // Changed DGaps to encode gaps between cleared bits, not
-  // set:
-  public final static int VERSION_DGAPS_CLEARED = 1;
-
-  // Increment version to change it:
-  public final static int VERSION_CURRENT = VERSION_DGAPS_CLEARED;
-
-  public int getVersion() {
-    return version;
-  }
-
-  /** Writes this vector to the file <code>name</code> in Directory
-    <code>d</code>, in a format that can be read by the constructor {@link
-    #BitVector(Directory, String, IOContext)}.  */
-  public final void write(Directory d, String name, IOContext context) throws IOException {
-    assert !(d instanceof CompoundFileDirectory);
-    IndexOutput output = d.createOutput(name, context);
-    try {
-      output.writeInt(-2);
-      CodecUtil.writeHeader(output, CODEC, VERSION_CURRENT);
-      if (isSparse()) { 
-        // sparse bit-set more efficiently saved as d-gaps.
-        writeClearedDgaps(output);
-      } else {
-        writeBits(output);
-      }
-      assert verifyCount();
-    } finally {
-      output.close();
-    }
-  }
-
-  /** Invert all bits */
-  public void invertAll() {
-    if (count != -1) {
-      count = size - count;
-    }
-    if (bits.length > 0) {
-      for(int idx=0;idx<bits.length;idx++) {
-        bits[idx] = (byte) (~bits[idx]);
-      }
-      clearUnusedBits();
-    }
-  }
-
-  private void clearUnusedBits() {
-    // Take care not to invert the "unused" bits in the
-    // last byte:
-    if (bits.length > 0) {
-      final int lastNBits = size & 7;
-      if (lastNBits != 0) {
-        final int mask = (1 << lastNBits)-1;
-        bits[bits.length-1] &= mask;
-      }
-    }
-  }
-
-  /** Set all bits */
-  public void setAll() {
-    Arrays.fill(bits, (byte) 0xff);
-    clearUnusedBits();
-    count = size;
-  }
-     
-  /** Write as a bit set */
-  private void writeBits(IndexOutput output) throws IOException {
-    output.writeInt(size());        // write size
-    output.writeInt(count());       // write count
-    output.writeBytes(bits, bits.length);
-  }
-  
-  /** Write as a d-gaps list */
-  private void writeClearedDgaps(IndexOutput output) throws IOException {
-    output.writeInt(-1);            // mark using d-gaps                         
-    output.writeInt(size());        // write size
-    output.writeInt(count());       // write count
-    int last=0;
-    int numCleared = size()-count();
-    for (int i=0; i<bits.length && numCleared>0; i++) {
-      if (bits[i] != (byte) 0xff) {
-        output.writeVInt(i-last);
-        output.writeByte(bits[i]);
-        last = i;
-        numCleared -= (8-BYTE_COUNTS[bits[i] & 0xFF]);
-        assert numCleared >= 0 || (i == (bits.length-1) && numCleared == -(8-(size&7)));
-      }
-    }
-  }
-
-  /** Indicates if the bit vector is sparse and should be saved as a d-gaps list, or dense, and should be saved as a bit set. */
-  private boolean isSparse() {
-
-    final int clearedCount = size() - count();
-    if (clearedCount == 0) {
-      return true;
-    }
-
-    final int avgGapLength = bits.length / clearedCount;
-
-    // expected number of bytes for vInt encoding of each gap
-    final int expectedDGapBytes;
-    if (avgGapLength <= (1<< 7)) {
-      expectedDGapBytes = 1;
-    } else if (avgGapLength <= (1<<14)) {
-      expectedDGapBytes = 2;
-    } else if (avgGapLength <= (1<<21)) {
-      expectedDGapBytes = 3;
-    } else if (avgGapLength <= (1<<28)) {
-      expectedDGapBytes = 4;
-    } else {
-      expectedDGapBytes = 5;
-    }
-
-    // +1 because we write the byte itself that contains the
-    // set bit
-    final int bytesPerSetBit = expectedDGapBytes + 1;
-    
-    // note: adding 32 because we start with ((int) -1) to indicate d-gaps format.
-    final long expectedBits = 32 + 8 * bytesPerSetBit * clearedCount;
-
-    // note: factor is for read/write of byte-arrays being faster than vints.  
-    final long factor = 10;  
-    return factor * expectedBits < size();
-  }
-
-  /** Constructs a bit vector from the file <code>name</code> in Directory
-    <code>d</code>, as written by the {@link #write} method.
-    */
-  public BitVector(Directory d, String name, IOContext context) throws IOException {
-    IndexInput input = d.openInput(name, context);
-
-    try {
-      final int firstInt = input.readInt();
-
-      if (firstInt == -2) {
-        // New format, with full header & version:
-        version = CodecUtil.checkHeader(input, CODEC, VERSION_START, VERSION_CURRENT);
-        size = input.readInt();
-      } else {
-        version = VERSION_PRE;
-        size = firstInt;
-      }
-      if (size == -1) {
-        if (version >= VERSION_DGAPS_CLEARED) {
-          readClearedDgaps(input);
-        } else {
-          readSetDgaps(input);
-        }
-      } else {
-        readBits(input);
-      }
-
-      if (version < VERSION_DGAPS_CLEARED) {
-        invertAll();
-      }
-
-      assert verifyCount();
-    } finally {
-      input.close();
-    }
-  }
-
-  // asserts only
-  private boolean verifyCount() {
-    assert count != -1;
-    final int countSav = count;
-    count = -1;
-    assert countSav == count(): "saved count was " + countSav + " but recomputed count is " + count;
-    return true;
-  }
-
-  /** Read as a bit set */
-  private void readBits(IndexInput input) throws IOException {
-    count = input.readInt();        // read count
-    bits = new byte[getNumBytes(size)];     // allocate bits
-    input.readBytes(bits, 0, bits.length);
-  }
-
-  /** read as a d-gaps list */ 
-  private void readSetDgaps(IndexInput input) throws IOException {
-    size = input.readInt();       // (re)read size
-    count = input.readInt();        // read count
-    bits = new byte[getNumBytes(size)];     // allocate bits
-    int last=0;
-    int n = count();
-    while (n>0) {
-      last += input.readVInt();
-      bits[last] = input.readByte();
-      n -= BYTE_COUNTS[bits[last] & 0xFF];
-      assert n >= 0;
-    }          
-  }
-
-  /** read as a d-gaps cleared bits list */ 
-  private void readClearedDgaps(IndexInput input) throws IOException {
-    size = input.readInt();       // (re)read size
-    count = input.readInt();        // read count
-    bits = new byte[getNumBytes(size)];     // allocate bits
-    Arrays.fill(bits, (byte) 0xff);
-    clearUnusedBits();
-    int last=0;
-    int numCleared = size()-count();
-    while (numCleared>0) {
-      last += input.readVInt();
-      bits[last] = input.readByte();
-      numCleared -= 8-BYTE_COUNTS[bits[last] & 0xFF];
-      assert numCleared >= 0 || (last == (bits.length-1) && numCleared == -(8-(size&7)));
-    }
-  }
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/util/MutableBits.java lucene-3661/lucene/src/java/org/apache/lucene/util/MutableBits.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/util/MutableBits.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3661/lucene/src/java/org/apache/lucene/util/MutableBits.java	2012-01-28 09:25:12.292910919 -0500
@@ -0,0 +1,22 @@
+package org.apache.lucene.util;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public interface MutableBits extends Bits {
+  public void clear(int bit);
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test/org/apache/lucene/codecs/lucene40/TestBitVector.java lucene-3661/lucene/src/test/org/apache/lucene/codecs/lucene40/TestBitVector.java
--- lucene-clean-trunk/lucene/src/test/org/apache/lucene/codecs/lucene40/TestBitVector.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3661/lucene/src/test/org/apache/lucene/codecs/lucene40/TestBitVector.java	2011-12-20 18:35:52.921865904 -0500
@@ -0,0 +1,287 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+/**
+ * <code>TestBitVector</code> tests the <code>BitVector</code>, obviously.
+ */
+public class TestBitVector extends LuceneTestCase
+{
+
+    /**
+     * Test the default constructor on BitVectors of various sizes.
+     * @throws Exception
+     */
+    public void testConstructSize() throws Exception {
+        doTestConstructOfSize(8);
+        doTestConstructOfSize(20);
+        doTestConstructOfSize(100);
+        doTestConstructOfSize(1000);
+    }
+
+    private void doTestConstructOfSize(int n) {
+        BitVector bv = new BitVector(n);
+        assertEquals(n,bv.size());
+    }
+
+    /**
+     * Test the get() and set() methods on BitVectors of various sizes.
+     * @throws Exception
+     */
+    public void testGetSet() throws Exception {
+        doTestGetSetVectorOfSize(8);
+        doTestGetSetVectorOfSize(20);
+        doTestGetSetVectorOfSize(100);
+        doTestGetSetVectorOfSize(1000);
+    }
+
+    private void doTestGetSetVectorOfSize(int n) {
+        BitVector bv = new BitVector(n);
+        for(int i=0;i<bv.size();i++) {
+            // ensure a set bit can be git'
+            assertFalse(bv.get(i));
+            bv.set(i);
+            assertTrue(bv.get(i));
+        }
+    }
+
+    /**
+     * Test the clear() method on BitVectors of various sizes.
+     * @throws Exception
+     */
+    public void testClear() throws Exception {
+        doTestClearVectorOfSize(8);
+        doTestClearVectorOfSize(20);
+        doTestClearVectorOfSize(100);
+        doTestClearVectorOfSize(1000);
+    }
+
+    private void doTestClearVectorOfSize(int n) {
+        BitVector bv = new BitVector(n);
+        for(int i=0;i<bv.size();i++) {
+            // ensure a set bit is cleared
+            assertFalse(bv.get(i));
+            bv.set(i);
+            assertTrue(bv.get(i));
+            bv.clear(i);
+            assertFalse(bv.get(i));
+        }
+    }
+
+    /**
+     * Test the count() method on BitVectors of various sizes.
+     * @throws Exception
+     */
+    public void testCount() throws Exception {
+        doTestCountVectorOfSize(8);
+        doTestCountVectorOfSize(20);
+        doTestCountVectorOfSize(100);
+        doTestCountVectorOfSize(1000);
+    }
+
+    private void doTestCountVectorOfSize(int n) {
+        BitVector bv = new BitVector(n);
+        // test count when incrementally setting bits
+        for(int i=0;i<bv.size();i++) {
+            assertFalse(bv.get(i));
+            assertEquals(i,bv.count());
+            bv.set(i);
+            assertTrue(bv.get(i));
+            assertEquals(i+1,bv.count());
+        }
+
+        bv = new BitVector(n);
+        // test count when setting then clearing bits
+        for(int i=0;i<bv.size();i++) {
+            assertFalse(bv.get(i));
+            assertEquals(0,bv.count());
+            bv.set(i);
+            assertTrue(bv.get(i));
+            assertEquals(1,bv.count());
+            bv.clear(i);
+            assertFalse(bv.get(i));
+            assertEquals(0,bv.count());
+        }
+    }
+
+    /**
+     * Test writing and construction to/from Directory.
+     * @throws Exception
+     */
+    public void testWriteRead() throws Exception {
+        doTestWriteRead(8);
+        doTestWriteRead(20);
+        doTestWriteRead(100);
+        doTestWriteRead(1000);
+    }
+
+    private void doTestWriteRead(int n) throws Exception {
+        MockDirectoryWrapper d = new  MockDirectoryWrapper(random, new RAMDirectory());
+        d.setPreventDoubleWrite(false);
+        BitVector bv = new BitVector(n);
+        // test count when incrementally setting bits
+        for(int i=0;i<bv.size();i++) {
+            assertFalse(bv.get(i));
+            assertEquals(i,bv.count());
+            bv.set(i);
+            assertTrue(bv.get(i));
+            assertEquals(i+1,bv.count());
+            bv.write(d, "TESTBV", newIOContext(random));
+            BitVector compare = new BitVector(d, "TESTBV", newIOContext(random));
+            // compare bit vectors with bits set incrementally
+            assertTrue(doCompare(bv,compare));
+        }
+    }
+    
+    /**
+     * Test r/w when size/count cause switching between bit-set and d-gaps file formats.  
+     */
+    public void testDgaps() throws IOException {
+      doTestDgaps(1,0,1);
+      doTestDgaps(10,0,1);
+      doTestDgaps(100,0,1);
+      doTestDgaps(1000,4,7);
+      doTestDgaps(10000,40,43);
+      doTestDgaps(100000,415,418);
+      doTestDgaps(1000000,3123,3126);
+      // now exercise skipping of fully populated byte in the bitset (they are omitted if bitset is sparse)
+      MockDirectoryWrapper d = new  MockDirectoryWrapper(random, new RAMDirectory());
+      d.setPreventDoubleWrite(false);
+      BitVector bv = new BitVector(10000);
+      bv.set(0);
+      for (int i = 8; i < 16; i++) {
+        bv.set(i);
+      } // make sure we have once byte full of set bits
+      for (int i = 32; i < 40; i++) {
+        bv.set(i);
+      } // get a second byte full of set bits
+      // add some more bits here 
+      for (int i = 40; i < 10000; i++) {
+        if (random.nextInt(1000) == 0) {
+          bv.set(i);
+        }
+      }
+      bv.write(d, "TESTBV", newIOContext(random));
+      BitVector compare = new BitVector(d, "TESTBV", newIOContext(random));
+      assertTrue(doCompare(bv,compare));
+    }
+    
+    private void doTestDgaps(int size, int count1, int count2) throws IOException {
+      MockDirectoryWrapper d = new  MockDirectoryWrapper(random, new RAMDirectory());
+      d.setPreventDoubleWrite(false);
+      BitVector bv = new BitVector(size);
+      bv.invertAll();
+      for (int i=0; i<count1; i++) {
+        bv.clear(i);
+        assertEquals(i+1,size-bv.count());
+      }
+      bv.write(d, "TESTBV", newIOContext(random));
+      // gradually increase number of set bits
+      for (int i=count1; i<count2; i++) {
+        BitVector bv2 = new BitVector(d, "TESTBV", newIOContext(random));
+        assertTrue(doCompare(bv,bv2));
+        bv = bv2;
+        bv.clear(i);
+        assertEquals(i+1, size-bv.count());
+        bv.write(d, "TESTBV", newIOContext(random));
+      }
+      // now start decreasing number of set bits
+      for (int i=count2-1; i>=count1; i--) {
+        BitVector bv2 = new BitVector(d, "TESTBV", newIOContext(random));
+        assertTrue(doCompare(bv,bv2));
+        bv = bv2;
+        bv.set(i);
+        assertEquals(i,size-bv.count());
+        bv.write(d, "TESTBV", newIOContext(random));
+      }
+    }
+
+    public void testSparseWrite() throws IOException {
+      Directory d = newDirectory();
+      final int numBits = 10240;
+      BitVector bv = new BitVector(numBits);
+      bv.invertAll();
+      int numToClear = random.nextInt(5);
+      for(int i=0;i<numToClear;i++) {
+        bv.clear(random.nextInt(numBits));
+      }
+      bv.write(d, "test", newIOContext(random));
+      final long size = d.fileLength("test");
+      assertTrue("size=" + size, size < 100);
+      d.close();
+    }
+
+    public void testClearedBitNearEnd() throws IOException {
+      Directory d = newDirectory();
+      final int numBits = _TestUtil.nextInt(random, 7, 1000);
+      BitVector bv = new BitVector(numBits);
+      bv.invertAll();
+      bv.clear(numBits-_TestUtil.nextInt(random, 1, 7));
+      bv.write(d, "test", newIOContext(random));
+      assertEquals(numBits-1, bv.count());
+      d.close();
+    }
+
+    public void testMostlySet() throws IOException {
+      Directory d = newDirectory();
+      final int numBits = _TestUtil.nextInt(random, 30, 1000);
+      for(int numClear=0;numClear<20;numClear++) {
+        BitVector bv = new BitVector(numBits);
+        bv.invertAll();
+        int count = 0;
+        while(count < numClear) {
+          final int bit = random.nextInt(numBits);
+          // Don't use getAndClear, so that count is recomputed
+          if (bv.get(bit)) {
+            bv.clear(bit);
+            count++;
+            assertEquals(numBits-count, bv.count());
+          }
+        }
+      }
+
+      d.close();
+    }
+
+    /**
+     * Compare two BitVectors.
+     * This should really be an equals method on the BitVector itself.
+     * @param bv One bit vector
+     * @param compare The second to compare
+     */
+    private boolean doCompare(BitVector bv, BitVector compare) {
+        boolean equal = true;
+        for(int i=0;i<bv.size();i++) {
+            // bits must be equal
+            if(bv.get(i)!=compare.get(i)) {
+                equal = false;
+                break;
+            }
+        }
+        assertEquals(bv.count(), compare.count());
+        return equal;
+    }
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java lucene-3661/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java
--- lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java	2012-01-15 22:00:26.018175849 -0500
+++ lucene-3661/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java	2012-01-19 12:07:33.642697011 -0500
@@ -27,6 +27,7 @@
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfosFormat;
@@ -35,6 +36,7 @@
 import org.apache.lucene.codecs.lucene40.Lucene40Codec;
 import org.apache.lucene.codecs.lucene40.Lucene40DocValuesFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40NormsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfosFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
@@ -1156,6 +1158,11 @@
     public NormsFormat normsFormat() {
       return new Lucene40NormsFormat();
     }
+    
+    @Override
+    public LiveDocsFormat liveDocsFormat() {
+      return new Lucene40LiveDocsFormat();
+    }
   }
   
   /*


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java lucene-3661/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
--- lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	2012-01-18 10:59:31.041999105 -0500
+++ lucene-3661/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	2012-01-20 11:14:41.554503419 -0500
@@ -497,17 +497,25 @@
       writer.deleteDocuments(searchTerm);
       writer.close();
 
-      // Now verify file names:
+      // Now verify file names... TODO: fix this test better, we could populate from 
+      // separateFiles() or something.
       String[] expected = new String[] {"_0.cfs", "_0.cfe",
                                "_0_1.del",
                                "segments_2",
                                "segments.gen"};
+      
+      String[] expectedSimpleText = new String[] {"_0.cfs", "_0.cfe",
+          "_0_1.liv",
+          "segments_2",
+          "segments.gen"};
 
       String[] actual = dir.listAll();
       Arrays.sort(expected);
+      Arrays.sort(expectedSimpleText);
       Arrays.sort(actual);
-      if (!Arrays.equals(expected, actual)) {
-        fail("incorrect filenames in index: expected:\n    " + asString(expected) + "\n  actual:\n    " + asString(actual));
+      if (!Arrays.equals(expected, actual) && !Arrays.equals(expectedSimpleText, actual)) {
+        fail("incorrect filenames in index: expected:\n    " + asString(expected) 
+            + "\n or " + asString(expectedSimpleText) + "\n actual:\n    " + asString(actual));
       }
       dir.close();
     } finally {


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter.java lucene-3661/lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
--- lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter.java	2012-01-12 13:57:22.273157288 -0500
+++ lucene-3661/lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter.java	2012-01-20 11:39:19.314499981 -0500
@@ -87,17 +87,20 @@
     }
     */
 
+    // TODO: fix this test better
+    String ext = Codec.getDefault().getName().equals("SimpleText") ? ".liv" : ".del";
+    
     // Create a bogus separate del file for a
     // segment that already has a separate del file: 
-    copyFile(dir, "_0_1.del", "_0_2.del");
+    copyFile(dir, "_0_1" + ext, "_0_2" + ext);
 
     // Create a bogus separate del file for a
     // segment that does not yet have a separate del file:
-    copyFile(dir, "_0_1.del", "_1_1.del");
+    copyFile(dir, "_0_1" + ext, "_1_1" + ext);
 
     // Create a bogus separate del file for a
     // non-existent segment:
-    copyFile(dir, "_0_1.del", "_188_1.del");
+    copyFile(dir, "_0_1" + ext, "_188_1" + ext);
 
     // Create a bogus segment file:
     copyFile(dir, "_0.cfs", "_188.cfs");


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestIndexReaderReopen.java lucene-3661/lucene/src/test/org/apache/lucene/index/TestIndexReaderReopen.java
--- lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestIndexReaderReopen.java	2012-01-12 13:57:22.277157288 -0500
+++ lucene-3661/lucene/src/test/org/apache/lucene/index/TestIndexReaderReopen.java	2011-12-20 18:35:11.389865181 -0500
@@ -42,7 +42,6 @@
 import org.apache.lucene.search.similarities.DefaultSimilarity;
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BitVector;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete.java lucene-3661/lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
--- lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete.java	2012-01-12 13:57:22.273157288 -0500
+++ lucene-3661/lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete.java	2012-01-20 11:54:11.878497904 -0500
@@ -961,7 +961,8 @@
       }
       w.updateDocument(delTerm, doc);
       // Eventually segment 0 should get a del docs:
-      if (dir.fileExists("_0_1.del")) {
+      // TODO: fix this test
+      if (dir.fileExists("_0_1.del") || dir.fileExists("_0_1.liv") ) {
         if (VERBOSE) {
           System.out.println("TEST: deletes created @ count=" + count);
         }
@@ -1006,7 +1007,8 @@
       }
       w.updateDocument(delTerm, doc);
       // Eventually segment 0 should get a del docs:
-      if (dir.fileExists("_0_1.del")) {
+      // TODO: fix this test
+      if (dir.fileExists("_0_1.del") || dir.fileExists("_0_1.liv")) {
         break;
       }
       count++;
@@ -1052,7 +1054,8 @@
       doc.add(newField("body", sb.toString(), TextField.TYPE_UNSTORED));
       w.updateDocument(new Term("id", ""+id), doc);
       docsInSegment.incrementAndGet();
-      if (dir.fileExists("_0_1.del")) {
+      // TODO: fix this test
+      if (dir.fileExists("_0_1.del") || dir.fileExists("_0_1.liv")) {
         if (VERBOSE) {
           System.out.println("TEST: deletes created @ id=" + id);
         }


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java lucene-3661/lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java
--- lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java	2012-01-27 07:53:48.157124648 -0500
+++ lucene-3661/lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java	2012-01-27 13:15:05.049079806 -0500
@@ -20,6 +20,7 @@
 import java.io.IOException;
 
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
@@ -459,13 +460,13 @@
       }
       StackTraceElement[] trace = new Exception().getStackTrace();
       for (int i = 0; i < trace.length; i++) {
-        if ("org.apache.lucene.index.SegmentMerger".equals(trace[i].getClassName()) && "mergeTerms".equals(trace[i].getMethodName()) && !didFail1) {
+        if (SegmentMerger.class.getName().equals(trace[i].getClassName()) && "mergeTerms".equals(trace[i].getMethodName()) && !didFail1) {
           didFail1 = true;
           throw new IOException("fake disk full during mergeTerms");
         }
-        if ("org.apache.lucene.util.BitVector".equals(trace[i].getClassName()) && "write".equals(trace[i].getMethodName()) && !didFail2) {
+        if (LiveDocsFormat.class.getName().equals(trace[i].getClassName()) && "writeLiveDocs".equals(trace[i].getMethodName()) && !didFail2) {
           didFail2 = true;
-          throw new IOException("fake disk full while writing BitVector");
+          throw new IOException("fake disk full while writing LiveDocs");
         }
       }
     }


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestMixedCodecs.java lucene-3661/lucene/src/test/org/apache/lucene/index/TestMixedCodecs.java
--- lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestMixedCodecs.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3661/lucene/src/test/org/apache/lucene/index/TestMixedCodecs.java	2012-01-28 09:13:41.804912525 -0500
@@ -0,0 +1,87 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.preflexrw.PreFlexRWCodec;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+import org.junit.Assume;
+
+public class TestMixedCodecs extends LuceneTestCase {
+
+  public void test() throws Exception {
+
+    Assume.assumeTrue(!(Codec.getDefault() instanceof PreFlexRWCodec));
+
+    final int NUM_DOCS = atLeast(1000);
+
+    final Directory dir = newDirectory();
+    RandomIndexWriter w = null;
+
+    int docsLeftInThisSegment = 0;
+    
+    int docUpto = 0;
+    while (docUpto < NUM_DOCS) {
+      if (docsLeftInThisSegment == 0) {
+        final IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));
+        if (random.nextBoolean()) {
+          // Make sure we aggressively mix in SimpleText
+          // since it has different impls for all codec
+          // formats...
+          iwc.setCodec(Codec.forName("SimpleText"));
+        }
+        if (w != null) {
+          w.close();
+        }
+        w = new RandomIndexWriter(random, dir, iwc);
+        docsLeftInThisSegment = _TestUtil.nextInt(random, 10, 100);
+      }
+      final Document doc = new Document();
+      doc.add(newField("id", String.valueOf(docUpto), StringField.TYPE_STORED));
+      w.addDocument(doc);
+      docUpto++;
+      docsLeftInThisSegment--;
+    }
+
+    // Random delete half the docs:
+    final Set<Integer> deleted = new HashSet<Integer>();
+    while(deleted.size() < NUM_DOCS/2) {
+      final Integer toDelete = random.nextInt(NUM_DOCS);
+      if (!deleted.contains(toDelete)) {
+        deleted.add(toDelete);
+        w.deleteDocuments(new Term("id", String.valueOf(toDelete)));
+        if (random.nextInt(17) == 6) {
+          final IndexReader r = w.getReader();
+          assertEquals(NUM_DOCS - deleted.size(), r.numDocs());
+          r.close();
+        }
+      }
+    }
+
+    w.close();
+    dir.close();
+  }
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test/org/apache/lucene/util/TestBitVector.java lucene-3661/lucene/src/test/org/apache/lucene/util/TestBitVector.java
--- lucene-clean-trunk/lucene/src/test/org/apache/lucene/util/TestBitVector.java	2012-01-12 13:57:22.297157288 -0500
+++ lucene-3661/lucene/src/test/org/apache/lucene/util/TestBitVector.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,285 +0,0 @@
-package org.apache.lucene.util;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.store.RAMDirectory;
-
-/**
- * <code>TestBitVector</code> tests the <code>BitVector</code>, obviously.
- */
-public class TestBitVector extends LuceneTestCase
-{
-
-    /**
-     * Test the default constructor on BitVectors of various sizes.
-     * @throws Exception
-     */
-    public void testConstructSize() throws Exception {
-        doTestConstructOfSize(8);
-        doTestConstructOfSize(20);
-        doTestConstructOfSize(100);
-        doTestConstructOfSize(1000);
-    }
-
-    private void doTestConstructOfSize(int n) {
-        BitVector bv = new BitVector(n);
-        assertEquals(n,bv.size());
-    }
-
-    /**
-     * Test the get() and set() methods on BitVectors of various sizes.
-     * @throws Exception
-     */
-    public void testGetSet() throws Exception {
-        doTestGetSetVectorOfSize(8);
-        doTestGetSetVectorOfSize(20);
-        doTestGetSetVectorOfSize(100);
-        doTestGetSetVectorOfSize(1000);
-    }
-
-    private void doTestGetSetVectorOfSize(int n) {
-        BitVector bv = new BitVector(n);
-        for(int i=0;i<bv.size();i++) {
-            // ensure a set bit can be git'
-            assertFalse(bv.get(i));
-            bv.set(i);
-            assertTrue(bv.get(i));
-        }
-    }
-
-    /**
-     * Test the clear() method on BitVectors of various sizes.
-     * @throws Exception
-     */
-    public void testClear() throws Exception {
-        doTestClearVectorOfSize(8);
-        doTestClearVectorOfSize(20);
-        doTestClearVectorOfSize(100);
-        doTestClearVectorOfSize(1000);
-    }
-
-    private void doTestClearVectorOfSize(int n) {
-        BitVector bv = new BitVector(n);
-        for(int i=0;i<bv.size();i++) {
-            // ensure a set bit is cleared
-            assertFalse(bv.get(i));
-            bv.set(i);
-            assertTrue(bv.get(i));
-            bv.clear(i);
-            assertFalse(bv.get(i));
-        }
-    }
-
-    /**
-     * Test the count() method on BitVectors of various sizes.
-     * @throws Exception
-     */
-    public void testCount() throws Exception {
-        doTestCountVectorOfSize(8);
-        doTestCountVectorOfSize(20);
-        doTestCountVectorOfSize(100);
-        doTestCountVectorOfSize(1000);
-    }
-
-    private void doTestCountVectorOfSize(int n) {
-        BitVector bv = new BitVector(n);
-        // test count when incrementally setting bits
-        for(int i=0;i<bv.size();i++) {
-            assertFalse(bv.get(i));
-            assertEquals(i,bv.count());
-            bv.set(i);
-            assertTrue(bv.get(i));
-            assertEquals(i+1,bv.count());
-        }
-
-        bv = new BitVector(n);
-        // test count when setting then clearing bits
-        for(int i=0;i<bv.size();i++) {
-            assertFalse(bv.get(i));
-            assertEquals(0,bv.count());
-            bv.set(i);
-            assertTrue(bv.get(i));
-            assertEquals(1,bv.count());
-            bv.clear(i);
-            assertFalse(bv.get(i));
-            assertEquals(0,bv.count());
-        }
-    }
-
-    /**
-     * Test writing and construction to/from Directory.
-     * @throws Exception
-     */
-    public void testWriteRead() throws Exception {
-        doTestWriteRead(8);
-        doTestWriteRead(20);
-        doTestWriteRead(100);
-        doTestWriteRead(1000);
-    }
-
-    private void doTestWriteRead(int n) throws Exception {
-        MockDirectoryWrapper d = new  MockDirectoryWrapper(random, new RAMDirectory());
-        d.setPreventDoubleWrite(false);
-        BitVector bv = new BitVector(n);
-        // test count when incrementally setting bits
-        for(int i=0;i<bv.size();i++) {
-            assertFalse(bv.get(i));
-            assertEquals(i,bv.count());
-            bv.set(i);
-            assertTrue(bv.get(i));
-            assertEquals(i+1,bv.count());
-            bv.write(d, "TESTBV", newIOContext(random));
-            BitVector compare = new BitVector(d, "TESTBV", newIOContext(random));
-            // compare bit vectors with bits set incrementally
-            assertTrue(doCompare(bv,compare));
-        }
-    }
-    
-    /**
-     * Test r/w when size/count cause switching between bit-set and d-gaps file formats.  
-     */
-    public void testDgaps() throws IOException {
-      doTestDgaps(1,0,1);
-      doTestDgaps(10,0,1);
-      doTestDgaps(100,0,1);
-      doTestDgaps(1000,4,7);
-      doTestDgaps(10000,40,43);
-      doTestDgaps(100000,415,418);
-      doTestDgaps(1000000,3123,3126);
-      // now exercise skipping of fully populated byte in the bitset (they are omitted if bitset is sparse)
-      MockDirectoryWrapper d = new  MockDirectoryWrapper(random, new RAMDirectory());
-      d.setPreventDoubleWrite(false);
-      BitVector bv = new BitVector(10000);
-      bv.set(0);
-      for (int i = 8; i < 16; i++) {
-        bv.set(i);
-      } // make sure we have once byte full of set bits
-      for (int i = 32; i < 40; i++) {
-        bv.set(i);
-      } // get a second byte full of set bits
-      // add some more bits here 
-      for (int i = 40; i < 10000; i++) {
-        if (random.nextInt(1000) == 0) {
-          bv.set(i);
-        }
-      }
-      bv.write(d, "TESTBV", newIOContext(random));
-      BitVector compare = new BitVector(d, "TESTBV", newIOContext(random));
-      assertTrue(doCompare(bv,compare));
-    }
-    
-    private void doTestDgaps(int size, int count1, int count2) throws IOException {
-      MockDirectoryWrapper d = new  MockDirectoryWrapper(random, new RAMDirectory());
-      d.setPreventDoubleWrite(false);
-      BitVector bv = new BitVector(size);
-      bv.invertAll();
-      for (int i=0; i<count1; i++) {
-        bv.clear(i);
-        assertEquals(i+1,size-bv.count());
-      }
-      bv.write(d, "TESTBV", newIOContext(random));
-      // gradually increase number of set bits
-      for (int i=count1; i<count2; i++) {
-        BitVector bv2 = new BitVector(d, "TESTBV", newIOContext(random));
-        assertTrue(doCompare(bv,bv2));
-        bv = bv2;
-        bv.clear(i);
-        assertEquals(i+1, size-bv.count());
-        bv.write(d, "TESTBV", newIOContext(random));
-      }
-      // now start decreasing number of set bits
-      for (int i=count2-1; i>=count1; i--) {
-        BitVector bv2 = new BitVector(d, "TESTBV", newIOContext(random));
-        assertTrue(doCompare(bv,bv2));
-        bv = bv2;
-        bv.set(i);
-        assertEquals(i,size-bv.count());
-        bv.write(d, "TESTBV", newIOContext(random));
-      }
-    }
-
-    public void testSparseWrite() throws IOException {
-      Directory d = newDirectory();
-      final int numBits = 10240;
-      BitVector bv = new BitVector(numBits);
-      bv.invertAll();
-      int numToClear = random.nextInt(5);
-      for(int i=0;i<numToClear;i++) {
-        bv.clear(random.nextInt(numBits));
-      }
-      bv.write(d, "test", newIOContext(random));
-      final long size = d.fileLength("test");
-      assertTrue("size=" + size, size < 100);
-      d.close();
-    }
-
-    public void testClearedBitNearEnd() throws IOException {
-      Directory d = newDirectory();
-      final int numBits = _TestUtil.nextInt(random, 7, 1000);
-      BitVector bv = new BitVector(numBits);
-      bv.invertAll();
-      bv.clear(numBits-_TestUtil.nextInt(random, 1, 7));
-      bv.write(d, "test", newIOContext(random));
-      assertEquals(numBits-1, bv.count());
-      d.close();
-    }
-
-    public void testMostlySet() throws IOException {
-      Directory d = newDirectory();
-      final int numBits = _TestUtil.nextInt(random, 30, 1000);
-      for(int numClear=0;numClear<20;numClear++) {
-        BitVector bv = new BitVector(numBits);
-        bv.invertAll();
-        int count = 0;
-        while(count < numClear) {
-          final int bit = random.nextInt(numBits);
-          // Don't use getAndClear, so that count is recomputed
-          if (bv.get(bit)) {
-            bv.clear(bit);
-            count++;
-            assertEquals(numBits-count, bv.count());
-          }
-        }
-      }
-
-      d.close();
-    }
-
-    /**
-     * Compare two BitVectors.
-     * This should really be an equals method on the BitVector itself.
-     * @param bv One bit vector
-     * @param compare The second to compare
-     */
-    private boolean doCompare(BitVector bv, BitVector compare) {
-        boolean equal = true;
-        for(int i=0;i<bv.size();i++) {
-            // bits must be equal
-            if(bv.get(i)!=compare.get(i)) {
-                equal = false;
-                break;
-            }
-        }
-        assertEquals(bv.count(), compare.count());
-        return equal;
-    }
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWCodec.java lucene-3661/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWCodec.java
--- lucene-clean-trunk/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWCodec.java	2012-01-19 13:28:40.774685692 -0500
+++ lucene-3661/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWCodec.java	2012-01-28 10:12:02.512904380 -0500
@@ -18,11 +18,15 @@
  */
 
 import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfosFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
 import org.apache.lucene.codecs.lucene3x.Lucene3xCodec;
+import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
 import org.apache.lucene.util.LuceneTestCase;
 
 /**
@@ -35,6 +39,10 @@
   private final FieldInfosFormat fieldInfos = new PreFlexRWFieldInfosFormat();
   private final TermVectorsFormat termVectors = new PreFlexRWTermVectorsFormat();
   private final SegmentInfosFormat segmentInfos = new PreFlexRWSegmentInfosFormat();
+  // TODO: this should really be a different impl
+  private final LiveDocsFormat liveDocs = new Lucene40LiveDocsFormat();
+  // TODO: this should really be a different impl
+  private final StoredFieldsFormat storedFields = new Lucene40StoredFieldsFormat();
   
   @Override
   public PostingsFormat postingsFormat() {
@@ -80,4 +88,22 @@
       return super.termVectorsFormat();
     }
   }
+
+  @Override
+  public LiveDocsFormat liveDocsFormat() {
+    if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
+      return liveDocs;
+    } else {
+      return super.liveDocsFormat();
+    }
+  }
+
+  @Override
+  public StoredFieldsFormat storedFieldsFormat() {
+    if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
+      return storedFields;
+    } else {
+      return super.storedFieldsFormat();
+    }
+  }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/solr/CHANGES.txt lucene-3661/solr/CHANGES.txt
--- lucene-clean-trunk/solr/CHANGES.txt	2012-01-25 15:22:30.529463968 -0500
+++ lucene-3661/solr/CHANGES.txt	2012-01-25 15:32:50.205462528 -0500
@@ -19,7 +19,7 @@
 See the tutorial at http://lucene.apache.org/solr/tutorial.html
 
 
-$Id: CHANGES.txt 1235888 2012-01-25 19:49:26Z markrmiller $
+$Id: CHANGES.txt 1235919 2012-01-25 20:32:44Z rmuir $
 
 ==================  4.0.0-dev ==================
 Versions of Major Components


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/core/RequestHandlers.java lucene-3661/solr/core/src/java/org/apache/solr/core/RequestHandlers.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/core/RequestHandlers.java	2012-01-25 15:22:30.885463969 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/core/RequestHandlers.java	2012-01-25 15:32:50.217462527 -0500
@@ -277,7 +277,7 @@
     }
     
     public String getVersion() {
-        String rev = "$Revision: 1235888 $";
+        String rev = "$Revision: 1235919 $";
         if( _handler != null ) {
           rev += " :: " + _handler.getVersion();
         }
@@ -285,7 +285,7 @@
     }
 
     public String getSourceId() {
-      String rev = "$Id: RequestHandlers.java 1235888 2012-01-25 19:49:26Z markrmiller $";
+      String rev = "$Id: RequestHandlers.java 1235919 2012-01-25 20:32:44Z rmuir $";
       if( _handler != null ) {
         rev += " :: " + _handler.getSourceId();
       }
@@ -293,7 +293,7 @@
     }
 
     public String getSource() {
-      String rev = "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/core/RequestHandlers.java $";
+      String rev = "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3661/solr/core/src/java/org/apache/solr/core/RequestHandlers.java $";
       if( _handler != null ) {
         rev += "\n" + _handler.getSource();
       }


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/core/SolrCore.java lucene-3661/solr/core/src/java/org/apache/solr/core/SolrCore.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/core/SolrCore.java	2012-01-27 13:14:19.053079918 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/core/SolrCore.java	2012-01-27 13:15:57.673079684 -0500
@@ -1878,11 +1878,11 @@
   }
 
   public String getSourceId() {
-    return "$Id: SolrCore.java 1236748 2012-01-27 16:40:49Z markrmiller $";
+    return "$Id: SolrCore.java 1236796 2012-01-27 18:15:52Z rmuir $";
   }
 
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/core/SolrCore.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3661/solr/core/src/java/org/apache/solr/core/SolrCore.java $";
   }
 
   public URL[] getDocs() {


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/admin/AdminHandlers.java lucene-3661/solr/core/src/java/org/apache/solr/handler/admin/AdminHandlers.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/admin/AdminHandlers.java	2012-01-12 13:56:26.429156317 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/admin/AdminHandlers.java	2011-09-11 11:01:09.366123944 -0400
@@ -121,7 +121,7 @@
   }
 
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/AdminHandlers.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring/solr/core/src/java/org/apache/solr/handler/admin/AdminHandlers.java $";
   }
 
   public Category getCategory() {


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java lucene-3661/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java	2012-01-25 15:22:30.893463969 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java	2012-01-25 15:32:50.221462526 -0500
@@ -779,16 +779,16 @@
 
   @Override
   public String getVersion() {
-    return "$Revision: 1235888 $";
+    return "$Revision: 1235919 $";
   }
 
   @Override
   public String getSourceId() {
-    return "$Id: CoreAdminHandler.java 1235888 2012-01-25 19:49:26Z markrmiller $";
+    return "$Id: CoreAdminHandler.java 1235919 2012-01-25 20:32:44Z rmuir $";
   }
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3661/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java lucene-3661/solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java	2012-01-12 13:56:26.429156317 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java	2011-09-11 11:01:09.366123944 -0400
@@ -111,6 +111,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring/solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java lucene-3661/solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java	2012-01-12 13:56:26.429156317 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java	2011-09-11 11:01:09.366123944 -0400
@@ -66,6 +66,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring/solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java lucene-3661/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java	2012-01-25 15:22:30.893463969 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java	2012-01-25 15:32:50.221462526 -0500
@@ -314,16 +314,16 @@
 
   @Override
   public String getVersion() {
-      return "$Revision: 1235888 $";
+      return "$Revision: 1235919 $";
   }
 
   @Override
   public String getSourceId() {
-    return "$Id: ShowFileRequestHandler.java 1235888 2012-01-25 19:49:26Z markrmiller $";
+    return "$Id: ShowFileRequestHandler.java 1235919 2012-01-25 20:32:44Z rmuir $";
   }
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3661/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java lucene-3661/solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java	2012-01-12 13:56:26.433156316 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java	2011-09-11 11:01:09.366123944 -0400
@@ -107,7 +107,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring/solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java lucene-3661/solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java	2012-01-12 13:56:26.433156316 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java	2011-09-11 11:01:09.366123944 -0400
@@ -292,7 +292,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring/solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java $";
   }
   
   private static final long ONE_KB = 1024;


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java lucene-3661/solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java	2012-01-12 13:56:26.433156316 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java	2011-09-11 11:01:09.366123944 -0400
@@ -144,6 +144,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring/solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java lucene-3661/solr/core/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java	2012-01-25 15:22:30.905463969 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java	2012-01-25 15:32:50.217462527 -0500
@@ -148,16 +148,16 @@
 
   @Override
   public String getSourceId() {
-    return "$Id: BinaryUpdateRequestHandler.java 1235888 2012-01-25 19:49:26Z markrmiller $";
+    return "$Id: BinaryUpdateRequestHandler.java 1235919 2012-01-25 20:32:44Z rmuir $";
   }
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3661/solr/core/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java $";
   }
 
   @Override
   public String getVersion() {
-    return "$Revision: 1235888 $";
+    return "$Revision: 1235919 $";
   }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java lucene-3661/solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java	2012-01-12 13:56:26.449156315 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java	2011-09-11 11:01:09.366123944 -0400
@@ -260,7 +260,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring/solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java lucene-3661/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java	2012-01-12 13:56:26.449156315 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java	2011-09-11 11:01:09.366123944 -0400
@@ -593,17 +593,17 @@
 
   @Override
   public String getVersion() {
-    return "$Revision: 1152530 $";
+    return "$Revision: 1160700 $";
   }
 
   @Override
   public String getSourceId() {
-    return "$Id: FacetComponent.java 1152530 2011-07-31 00:30:19Z koji $";
+    return "$Id: FacetComponent.java 1160700 2011-08-23 14:06:58Z rmuir $";
   }
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java lucene-3661/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java	2012-01-12 13:56:26.453156316 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java	2011-11-09 10:42:02.804271391 -0500
@@ -214,7 +214,7 @@
   
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene2621/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java $";
   }
   
   @Override


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java lucene-3661/solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java	2012-01-12 13:56:26.453156316 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java	2011-09-11 11:01:09.366123944 -0400
@@ -128,7 +128,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring/solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java lucene-3661/solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java	2012-01-12 13:56:26.449156315 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java	2011-09-11 11:01:09.366123944 -0400
@@ -249,7 +249,7 @@
 //  }
 //
 //  public String getSource() {
-//    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java $";
+//    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring/solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java $";
 //  }
 //
 //  public String getVersion() {


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java lucene-3661/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java	2012-01-27 10:28:09.889103100 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java	2012-01-25 15:32:50.221462526 -0500
@@ -1081,17 +1081,17 @@
 
   @Override
   public String getVersion() {
-    return "$Revision: 1235888 $";
+    return "$Revision: 1235919 $";
   }
 
   @Override
   public String getSourceId() {
-    return "$Id: QueryComponent.java 1235888 2012-01-25 19:49:26Z markrmiller $";
+    return "$Id: QueryComponent.java 1235919 2012-01-25 20:32:44Z rmuir $";
   }
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3661/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java lucene-3661/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java	2012-01-25 15:22:30.901463969 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java	2012-01-25 15:32:50.221462526 -0500
@@ -241,17 +241,17 @@
 
   @Override
   public String getVersion() {
-    return "$Revision: 1235888 $";
+    return "$Revision: 1235919 $";
   }
 
   @Override
   public String getSourceId() {
-    return "$Id: RealTimeGetComponent.java 1235888 2012-01-25 19:49:26Z markrmiller $";
+    return "$Id: RealTimeGetComponent.java 1235919 2012-01-25 20:32:44Z rmuir $";
   }
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3661/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java lucene-3661/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java	2012-01-27 10:28:09.889103100 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java	2012-01-25 15:32:50.221462526 -0500
@@ -329,17 +329,17 @@
 
   @Override
   public String getVersion() {
-    return "$Revision: 1235888 $";
+    return "$Revision: 1235919 $";
   }
 
   @Override
   public String getSourceId() {
-    return "$Id: SearchHandler.java 1235888 2012-01-25 19:49:26Z markrmiller $";
+    return "$Id: SearchHandler.java 1235919 2012-01-25 20:32:44Z rmuir $";
   }
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3661/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java lucene-3661/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java	2012-01-12 13:56:26.457156316 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java	2011-11-06 07:27:12.236410948 -0500
@@ -132,7 +132,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene2621/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java $";
   }
 
 


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java lucene-3661/solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java	2012-01-12 13:56:26.449156315 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java	2011-09-11 11:01:09.376123944 -0400
@@ -79,6 +79,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring/solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java lucene-3661/solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java	2012-01-12 13:56:26.461156314 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java	2011-09-11 11:01:09.376123944 -0400
@@ -118,7 +118,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring/solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java $";
   }
 
   // ================================================= Helper methods ================================================


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/JsonUpdateRequestHandler.java lucene-3661/solr/core/src/java/org/apache/solr/handler/JsonUpdateRequestHandler.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/JsonUpdateRequestHandler.java	2012-01-12 13:56:26.457156316 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/JsonUpdateRequestHandler.java	2011-09-11 11:01:09.376123944 -0400
@@ -59,7 +59,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/JsonUpdateRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring/solr/core/src/java/org/apache/solr/handler/JsonUpdateRequestHandler.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java lucene-3661/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java	2012-01-25 15:22:30.893463969 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java	2012-01-25 15:32:50.217462527 -0500
@@ -470,17 +470,17 @@
 
   @Override
   public String getSourceId() {
-    return "$Id: ReplicationHandler.java 1235888 2012-01-25 19:49:26Z markrmiller $";
+    return "$Id: ReplicationHandler.java 1235919 2012-01-25 20:32:44Z rmuir $";
   }
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3661/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java $";
   }
 
   @Override
   public String getVersion() {
-    return "$Revision: 1235888 $";
+    return "$Revision: 1235919 $";
   }
 
   private long[] getIndexVersion() {


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/StandardRequestHandler.java lucene-3661/solr/core/src/java/org/apache/solr/handler/StandardRequestHandler.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/StandardRequestHandler.java	2012-01-12 13:56:26.429156317 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/StandardRequestHandler.java	2011-09-11 11:01:09.376123944 -0400
@@ -62,7 +62,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/StandardRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring/solr/core/src/java/org/apache/solr/handler/StandardRequestHandler.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java lucene-3661/solr/core/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java	2012-01-25 15:22:30.901463969 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java	2012-01-25 15:32:50.221462526 -0500
@@ -89,17 +89,17 @@
 
   @Override
   public String getVersion() {
-    return "$Revision: 1235888 $";
+    return "$Revision: 1235919 $";
   }
 
   @Override
   public String getSourceId() {
-    return "$Id: XmlUpdateRequestHandler.java 1235888 2012-01-25 19:49:26Z markrmiller $";
+    return "$Id: XmlUpdateRequestHandler.java 1235919 2012-01-25 20:32:44Z rmuir $";
   }
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3661/solr/core/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/highlight/DefaultEncoder.java lucene-3661/solr/core/src/java/org/apache/solr/highlight/DefaultEncoder.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/highlight/DefaultEncoder.java	2012-01-12 13:56:26.241156312 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/highlight/DefaultEncoder.java	2011-09-11 11:01:09.386123944 -0400
@@ -42,7 +42,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/DefaultEncoder.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring/solr/core/src/java/org/apache/solr/highlight/DefaultEncoder.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/highlight/HtmlEncoder.java lucene-3661/solr/core/src/java/org/apache/solr/highlight/HtmlEncoder.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/highlight/HtmlEncoder.java	2012-01-12 13:56:26.237156312 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/highlight/HtmlEncoder.java	2011-09-11 11:01:09.386123944 -0400
@@ -42,7 +42,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/HtmlEncoder.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring/solr/core/src/java/org/apache/solr/highlight/HtmlEncoder.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/LRUCache.java lucene-3661/solr/core/src/java/org/apache/solr/search/LRUCache.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/LRUCache.java	2012-01-12 13:56:26.205156311 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/search/LRUCache.java	2011-09-11 11:01:09.386123944 -0400
@@ -231,7 +231,7 @@
   }
 
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/LRUCache.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring/solr/core/src/java/org/apache/solr/search/LRUCache.java $";
   }
 
   public URL[] getDocs() {


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java lucene-3661/solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java	2012-01-12 13:56:26.129156310 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java	2011-09-11 11:01:09.396123944 -0400
@@ -50,7 +50,7 @@
     return "$Id: SolrFieldCacheMBean.java 1144761 2011-07-09 23:01:53Z sarowe $"; 
   }
   public String getSource() { 
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring/solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java $";
   }
   public URL[] getDocs() {
     return null;


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java lucene-3661/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java	2012-01-27 10:28:09.881103104 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java	2012-01-25 15:32:50.225462526 -0500
@@ -1958,11 +1958,11 @@
   }
 
   public String getSourceId() {
-    return "$Id: SolrIndexSearcher.java 1235888 2012-01-25 19:49:26Z markrmiller $";
+    return "$Id: SolrIndexSearcher.java 1235919 2012-01-25 20:32:44Z rmuir $";
   }
 
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3661/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java $";
   }
 
   public URL[] getDocs() {


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java lucene-3661/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java	2012-01-25 15:22:30.801463967 -0500
+++ lucene-3661/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java	2012-01-25 15:32:50.225462526 -0500
@@ -568,11 +568,11 @@
   }
 
   public String getSourceId() {
-    return "$Id: DirectUpdateHandler2.java 1235888 2012-01-25 19:49:26Z markrmiller $";
+    return "$Id: DirectUpdateHandler2.java 1235919 2012-01-25 20:32:44Z rmuir $";
   }
 
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3661/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java $";
   }
 
   public URL[] getDocs() {


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/test-files/solr/conf/schema12.xml lucene-3661/solr/core/src/test-files/solr/conf/schema12.xml
--- lucene-clean-trunk/solr/core/src/test-files/solr/conf/schema12.xml	2012-01-25 15:22:30.721463970 -0500
+++ lucene-3661/solr/core/src/test-files/solr/conf/schema12.xml	2012-01-25 15:32:50.245462526 -0500
@@ -23,7 +23,7 @@
      kitchen sink thrown in. See example/solr/conf/schema.xml for a 
      more concise example.
 
-     $Id: schema12.xml 1235888 2012-01-25 19:49:26Z markrmiller $
+     $Id: schema12.xml 1235919 2012-01-25 20:32:44Z rmuir $
      $Source: /cvs/main/searching/solr-configs/test/WEB-INF/classes/schema.xml,v $
      $Name:  $
   -->


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/test-files/solr/conf/solrconfig-nocache.xml lucene-3661/solr/core/src/test-files/solr/conf/solrconfig-nocache.xml
--- lucene-clean-trunk/solr/core/src/test-files/solr/conf/solrconfig-nocache.xml	2012-01-25 15:22:30.721463970 -0500
+++ lucene-3661/solr/core/src/test-files/solr/conf/solrconfig-nocache.xml	2012-01-25 15:32:50.245462526 -0500
@@ -17,7 +17,7 @@
  limitations under the License.
 -->
 
-<!-- $Id: solrconfig-nocache.xml 1235888 2012-01-25 19:49:26Z markrmiller $
+<!-- $Id: solrconfig-nocache.xml 1235919 2012-01-25 20:32:44Z rmuir $
      $Source$
      $Name$
   -->


diff -ruN -x .svn -x build lucene-clean-trunk/solr/solrj/src/test-files/solrj/solr/conf/schema-replication1.xml lucene-3661/solr/solrj/src/test-files/solrj/solr/conf/schema-replication1.xml
--- lucene-clean-trunk/solr/solrj/src/test-files/solrj/solr/conf/schema-replication1.xml	2012-01-12 13:56:26.689156320 -0500
+++ lucene-3661/solr/solrj/src/test-files/solrj/solr/conf/schema-replication1.xml	2011-09-11 11:01:09.406123944 -0400
@@ -23,7 +23,7 @@
      kitchen sink thrown in. See example/solr/conf/schema.xml for a 
      more concise example.
 
-     $Id: schema-replication1.xml 1144761 2011-07-09 23:01:53Z sarowe $
+     $Id: schema-replication1.xml 1160700 2011-08-23 14:06:58Z rmuir $
      $Source$
      $Name$
   -->
