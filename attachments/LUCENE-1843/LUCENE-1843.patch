Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/SingleTokenTokenStream.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/SingleTokenTokenStream.java	(revision 807118)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/SingleTokenTokenStream.java	(working copy)
@@ -19,14 +19,10 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.util.AttributeImpl;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 
 /**
  * A {@link TokenStream} containing a single token.
@@ -34,45 +30,37 @@
 public class SingleTokenTokenStream extends TokenStream {
 
   private boolean exhausted = false;
+  
   // The token needs to be immutable, so work with clones!
   private Token singleToken;
+  private final AttributeImpl tokenAtt;
 
-  private TermAttribute termAtt;
-  private OffsetAttribute offsetAtt;
-  private FlagsAttribute flagsAtt;
-  private PositionIncrementAttribute posIncAtt;
-  private TypeAttribute typeAtt;
-  private PayloadAttribute payloadAtt;
+  private static final AttributeFactory TOKEN_ATTRIBUTE_FACTORY = new AttributeFactory() {
+    public AttributeImpl createAttributeInstance(Class attClass) {
+      return attClass.isAssignableFrom(Token.class)
+        ? new Token() : DEFAULT_ATTRIBUTE_FACTORY.createAttributeInstance(attClass);
+    }
+  };
 
   public SingleTokenTokenStream(Token token) {
+    super(TOKEN_ATTRIBUTE_FACTORY);
+    
     assert token != null;
     this.singleToken = (Token) token.clone();
-     
-    termAtt = (TermAttribute) addAttribute(TermAttribute.class);
-    offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);
-    flagsAtt = (FlagsAttribute) addAttribute(FlagsAttribute.class);
-    posIncAtt = (PositionIncrementAttribute) addAttribute(PositionIncrementAttribute.class);
-    typeAtt = (TypeAttribute) addAttribute(TypeAttribute.class);
-    payloadAtt = (PayloadAttribute) addAttribute(PayloadAttribute.class);
+    
+    tokenAtt = (AttributeImpl) addAttribute(TermAttribute.class);
+    assert (tokenAtt instanceof Token || tokenAtt.getClass().getName().equals("org.apache.lucene.analysis.TokenWrapper"));
   }
 
-
   public final boolean incrementToken() throws IOException {
     if (exhausted) {
       return false;
+    } else {
+      clearAttributes();
+      singleToken.copyTo(tokenAtt);
+      exhausted = true;
+      return true;
     }
-    
-    Token clone = (Token) singleToken.clone();
-    
-    clearAttributes();
-    termAtt.setTermBuffer(clone.termBuffer(), 0, clone.termLength());
-    offsetAtt.setOffset(clone.startOffset(), clone.endOffset());
-    flagsAtt.setFlags(clone.getFlags());
-    typeAtt.setType(clone.type());
-    posIncAtt.setPositionIncrement(clone.getPositionIncrement());
-    payloadAtt.setPayload(clone.getPayload());
-    exhausted = true;
-    return true;
   }
   
   /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java	(working copy)
@@ -21,15 +21,13 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 
-import junit.framework.TestCase;
-
 /**
  * Test the Arabic Analyzer
  *
  */
-public class TestArabicAnalyzer extends TestCase {
+public class TestArabicAnalyzer extends BaseTokenStreamTestCase {
   
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
@@ -74,33 +72,4 @@
     assertAnalyzesTo(new ArabicAnalyzer(), "English text.", new String[] {
         "english", "text" });
   }
-  
-  private void assertAnalyzesTo(Analyzer a, String input, String[] output)
-      throws Exception {
-    TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts
-        .getAttribute(TermAttribute.class);
-
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(output[i], termAtt.term());
-    }
-
-    assertFalse(ts.incrementToken());
-    ts.close();
-  }
-  
-  private void assertAnalyzesToReuse(Analyzer a, String input, String[] output)
-      throws Exception {
-    TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts
-        .getAttribute(TermAttribute.class);
-
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(output[i], termAtt.term());
-    }
-
-    assertFalse(ts.incrementToken());
-  }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java	(working copy)
@@ -20,15 +20,14 @@
 import java.io.IOException;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * Test the Arabic Normalization Filter
  *
  */
-public class TestArabicNormalizationFilter extends TestCase {
+public class TestArabicNormalizationFilter extends BaseTokenStreamTestCase {
 
   public void testAlifMadda() throws IOException {
     check("آجن", "اجن");
@@ -89,11 +88,7 @@
   private void check(final String input, final String expected) throws IOException {
     ArabicLetterTokenizer tokenStream = new ArabicLetterTokenizer(new StringReader(input));
     ArabicNormalizationFilter filter = new ArabicNormalizationFilter(tokenStream);
-    TermAttribute termAtt = (TermAttribute) filter.getAttribute(TermAttribute.class);
-    
-    assertTrue(filter.incrementToken());
-    assertEquals(expected, termAtt.term());
-    filter.close();
+    assertTokenStreamContents(filter, new String[]{expected});
   }
 
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java	(working copy)
@@ -20,15 +20,14 @@
 import java.io.IOException;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * Test the Arabic Normalization Filter
  *
  */
-public class TestArabicStemFilter extends TestCase {
+public class TestArabicStemFilter extends BaseTokenStreamTestCase {
   
   public void testAlPrefix() throws IOException {
     check("الحسن", "حسن");
@@ -117,11 +116,7 @@
   private void check(final String input, final String expected) throws IOException {
     ArabicLetterTokenizer tokenStream  = new ArabicLetterTokenizer(new StringReader(input));
     ArabicStemFilter filter = new ArabicStemFilter(tokenStream);
-    TermAttribute termAtt = (TermAttribute) filter.getAttribute(TermAttribute.class);
-    
-    assertTrue(filter.incrementToken());
-    assertEquals(expected, termAtt.term());
-    filter.close();
+    assertTokenStreamContents(filter, new String[]{expected});
   }
 
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java	(working copy)
@@ -17,11 +17,7 @@
  * limitations under the License.
  */
 
-import java.io.IOException;
-import java.io.StringReader;
-
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
@@ -32,9 +28,9 @@
  * It is very similar to the snowball portuguese algorithm but not exactly the same.
  *
  */
-public class TestBrazilianStemmer extends TestCase {
+public class TestBrazilianStemmer extends BaseTokenStreamTestCase {
   
-  public void testWithSnowballExamples() throws IOException {
+  public void testWithSnowballExamples() throws Exception {
 	 check("boa", "boa");
 	 check("boainain", "boainain");
 	 check("boas", "boas");
@@ -150,23 +146,13 @@
     a.setStemExclusionTable(new String[] { "quintessência" });
     checkReuse(a, "quintessência", "quintessência");
   }
-
-  private void check(final String input, final String expected) throws IOException {
-    Analyzer analyzer = new BrazilianAnalyzer(); 
-    TokenStream stream = analyzer.tokenStream("dummy", new StringReader(input));
-    TermAttribute text = (TermAttribute) stream.getAttribute(TermAttribute.class);
-    assertTrue(stream.incrementToken());
-    assertEquals(expected, text.term());
-    assertFalse(stream.incrementToken());
-    stream.close();
+  
+  private void check(final String input, final String expected) throws Exception {
+    checkOneTerm(new BrazilianAnalyzer(), input, expected);
   }
   
-  private void checkReuse(Analyzer analyzer, final String input, final String expected) throws IOException {
-    TokenStream stream = analyzer.reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute text = (TermAttribute) stream.getAttribute(TermAttribute.class);
-    assertTrue(stream.incrementToken());
-    assertEquals(expected, text.term());
-    assertFalse(stream.incrementToken());
+  private void checkReuse(Analyzer a, String input, String expected) throws Exception {
+    checkOneTermReuse(a, input, expected);
   }
 
 }
\ No newline at end of file
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java	(working copy)
@@ -20,8 +20,7 @@
 import java.io.IOException;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
@@ -29,7 +28,7 @@
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 
 
-public class TestCJKTokenizer extends TestCase{
+public class TestCJKTokenizer extends BaseTokenStreamTestCase {
   
   class TestToken {
     String termText;
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java	(working copy)
@@ -21,17 +21,15 @@
 import java.io.Reader;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 
-public class TestChineseTokenizer extends TestCase
+public class TestChineseTokenizer extends BaseTokenStreamTestCase
 {
     public void testOtherLetterOffset() throws IOException
     {
@@ -116,34 +114,5 @@
       assertAnalyzesTo(justFilter, "This is a Test. b c d", 
           new String[] { "This", "Test." });
     }
-    
-    private void assertAnalyzesTo(Analyzer a, String input, String[] output)
-      throws Exception {
-      TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-      TermAttribute termAtt = (TermAttribute) ts
-      .getAttribute(TermAttribute.class);
 
-     for (int i = 0; i < output.length; i++) {
-       assertTrue(ts.incrementToken());
-       assertEquals(output[i], termAtt.term());
-     }
-
-     assertFalse(ts.incrementToken());
-     ts.close();
-    }
-    
-    private void assertAnalyzesToReuse(Analyzer a, String input, String[] output,
-      int startOffsets[], int endOffsets[])
-      throws Exception {
-      TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-      TermAttribute termAtt = (TermAttribute) ts
-        .getAttribute(TermAttribute.class);
-
-      for (int i = 0; i < output.length; i++) {
-        assertTrue(ts.incrementToken());
-        assertEquals(output[i], termAtt.term());
-      }
-
-      assertFalse(ts.incrementToken());
-    }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java	(working copy)
@@ -31,8 +31,7 @@
 import java.util.zip.ZipEntry;
 import java.util.zip.ZipInputStream;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
@@ -41,7 +40,7 @@
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
-public class TestCompoundWordTokenFilter extends TestCase {
+public class TestCompoundWordTokenFilter extends BaseTokenStreamTestCase {
   private static String[] locations = {
       "http://dfn.dl.sourceforge.net/sourceforge/offo/offo-hyphenation.zip",
       "http://surfnet.dl.sourceforge.net/sourceforge/offo/offo-hyphenation.zip",
@@ -76,7 +75,7 @@
         dict, CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,
         CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,
         CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE, false);
-    assertFiltersTo(tf, new String[] { "Rindfleischüberwachungsgesetz", "Rind",
+    assertTokenStreamContents(tf, new String[] { "Rindfleischüberwachungsgesetz", "Rind",
         "fleisch", "überwachung", "gesetz", "Drahtschere", "Draht", "schere",
         "abba" }, new int[] { 0, 0, 4, 11, 23, 30, 30, 35, 42 }, new int[] {
         29, 4, 11, 22, 29, 41, 35, 41, 46 }, new int[] { 1, 0, 0, 0, 0, 1, 0,
@@ -101,7 +100,7 @@
             "Rindfleischüberwachungsgesetz")), hyphenator, dict,
         CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,
         CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE, 40, true);
-    assertFiltersTo(tf, new String[] { "Rindfleischüberwachungsgesetz",
+    assertTokenStreamContents(tf, new String[] { "Rindfleischüberwachungsgesetz",
         "Rindfleisch", "fleisch", "überwachungsgesetz", "gesetz" }, new int[] {
         0, 0, 4, 11, 23 }, new int[] { 29, 11, 11, 29, 29 }, new int[] { 1, 0,
         0, 0, 0 });
@@ -118,7 +117,7 @@
                 "Bildörr Bilmotor Biltak Slagborr Hammarborr Pelarborr Glasögonfodral Basfiolsfodral Basfiolsfodralmakaregesäll Skomakare Vindrutetorkare Vindrutetorkarblad abba")),
         dict);
 
-    assertFiltersTo(tf, new String[] { "Bildörr", "Bil", "dörr", "Bilmotor",
+    assertTokenStreamContents(tf, new String[] { "Bildörr", "Bil", "dörr", "Bilmotor",
         "Bil", "motor", "Biltak", "Bil", "tak", "Slagborr", "Slag", "borr",
         "Hammarborr", "Hammar", "borr", "Pelarborr", "Pelar", "borr",
         "Glasögonfodral", "Glas", "ögon", "fodral", "Basfiolsfodral", "Bas",
@@ -147,7 +146,7 @@
         CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,
         CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE, true);
 
-    assertFiltersTo(tf, new String[] { "Basfiolsfodralmakaregesäll", "Bas",
+    assertTokenStreamContents(tf, new String[] { "Basfiolsfodralmakaregesäll", "Bas",
         "fiolsfodral", "fodral", "makare", "gesäll" }, new int[] { 0, 0, 3, 8,
         14, 20 }, new int[] { 26, 3, 14, 14, 20, 26 }, new int[] { 1, 0, 0, 0,
         0, 0 });
@@ -185,22 +184,6 @@
     assertEquals("Rindfleischüberwachungsgesetz", termAtt.term());
   }
 
-  private void assertFiltersTo(TokenFilter tf, String[] s, int[] startOffset,
-      int[] endOffset, int[] posIncr) throws Exception {
-    TermAttribute termAtt = (TermAttribute) tf.getAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = (OffsetAttribute) tf.getAttribute(OffsetAttribute.class);
-    PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) tf.getAttribute(PositionIncrementAttribute.class);
-    
-    for (int i = 0; i < s.length; ++i) {
-      assertTrue(tf.incrementToken());
-      assertEquals(s[i], termAtt.term());
-      assertEquals(startOffset[i], offsetAtt.startOffset());
-      assertEquals(endOffset[i], offsetAtt.endOffset());
-      assertEquals(posIncr[i], posIncAtt.getPositionIncrement());
-    }
-    assertFalse(tf.incrementToken());
-  }
-
   private void getHyphenationPatternFileContents() {
     if (patternsFileContent == null) {
       try {
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java	(working copy)
@@ -21,13 +21,10 @@
 import java.io.FileInputStream;
 import java.io.IOException;
 import java.io.InputStream;
-import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * Test the CzechAnalyzer
@@ -35,7 +32,7 @@
  * CzechAnalyzer is like a StandardAnalyzer with a custom stopword list.
  *
  */
-public class TestCzechAnalyzer extends TestCase {
+public class TestCzechAnalyzer extends BaseTokenStreamTestCase {
   File dataDir = new File(System.getProperty("dataDir", "./bin"));
   File customStopFile = new File(dataDir, "org/apache/lucene/analysis/cz/customStopWordFile.txt");
   
@@ -85,24 +82,4 @@
     assertAnalyzesToReuse(cz, "Česká Republika", new String[] { "česká" });
   }
 
-  private void assertAnalyzesTo(Analyzer a, String input, String[] output) throws Exception {
-    TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-    TermAttribute text = (TermAttribute) ts.getAttribute(TermAttribute.class);
-    for (int i=0; i<output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(text.term(), output[i]);
-    }
-    assertFalse(ts.incrementToken());
-    ts.close();
-  }
-  
-  private void assertAnalyzesToReuse(Analyzer a, String input, String[] output) throws Exception {
-    TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute text = (TermAttribute) ts.getAttribute(TermAttribute.class);
-    for (int i=0; i<output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(text.term(), output[i]);
-    }
-    assertFalse(ts.incrementToken());
-  }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java	(working copy)
@@ -20,18 +20,14 @@
 import java.io.BufferedReader;
 import java.io.File;
 import java.io.FileInputStream;
-import java.io.IOException;
 import java.io.InputStreamReader;
 import java.io.Reader;
-import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * Test the German stemmer. The stemming algorithm is known to work less 
@@ -39,34 +35,29 @@
  * also check some of the cases where the algorithm is wrong.
  *
  */
-public class TestGermanStemFilter extends TestCase {
+public class TestGermanStemFilter extends BaseTokenStreamTestCase {
 
-  public void testStemming() {
-    try {
-      // read test cases from external file:
-      File dataDir = new File(System.getProperty("dataDir", "./bin"));
-      File testFile = new File(dataDir, "org/apache/lucene/analysis/de/data.txt");
-      FileInputStream fis = new FileInputStream(testFile);
-      InputStreamReader isr = new InputStreamReader(fis, "iso-8859-1");
-      BufferedReader breader = new BufferedReader(isr);
-      while(true) {
-        String line = breader.readLine();
-        if (line == null)
-          break;
-        line = line.trim();
-        if (line.startsWith("#") || line.equals(""))
-          continue;    // ignore comments and empty lines
-        String[] parts = line.split(";");
-        //System.out.println(parts[0] + " -- " + parts[1]);
-        check(parts[0], parts[1]);
-      }
-      breader.close();
-      isr.close();
-      fis.close();
-    } catch (IOException e) {
-       e.printStackTrace();
-       fail();
+  public void testStemming() throws Exception {
+    // read test cases from external file:
+    File dataDir = new File(System.getProperty("dataDir", "./bin"));
+    File testFile = new File(dataDir, "org/apache/lucene/analysis/de/data.txt");
+    FileInputStream fis = new FileInputStream(testFile);
+    InputStreamReader isr = new InputStreamReader(fis, "iso-8859-1");
+    BufferedReader breader = new BufferedReader(isr);
+    while(true) {
+      String line = breader.readLine();
+      if (line == null)
+        break;
+      line = line.trim();
+      if (line.startsWith("#") || line.equals(""))
+        continue;    // ignore comments and empty lines
+      String[] parts = line.split(";");
+      //System.out.println(parts[0] + " -- " + parts[1]);
+      check(parts[0], parts[1]);
     }
+    breader.close();
+    isr.close();
+    fis.close();
   }
   
   public void testReusableTokenStream() throws Exception {
@@ -100,20 +91,11 @@
     checkReuse(a, "tischen", "tischen");
   }
   
-  private void check(final String input, final String expected) throws IOException {
-    Analyzer a = new GermanAnalyzer();
-    TokenStream tokenStream = a.tokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) tokenStream.getAttribute(TermAttribute.class);
-    assertTrue(tokenStream.incrementToken());
-    assertEquals(expected, termAtt.term());
-    tokenStream.close();
+  private void check(final String input, final String expected) throws Exception {
+    checkOneTerm(new GermanAnalyzer(), input, expected);
   }
   
-  private void checkReuse(Analyzer a, String input, String expected) throws IOException {
-    TokenStream stream = a.reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute text = (TermAttribute) stream.getAttribute(TermAttribute.class);
-    assertTrue(stream.incrementToken());
-    assertEquals(expected, text.term());
-    assertFalse(stream.incrementToken());
+  private void checkReuse(Analyzer a, String input, String expected) throws Exception {
+    checkOneTermReuse(a, input, expected);
   }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java	(working copy)
@@ -16,51 +16,18 @@
  * limitations under the License.
  */
 
-import java.io.StringReader;
-
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 
 /**
  * A unit test class for verifying the correct operation of the GreekAnalyzer.
  *
  */
-public class GreekAnalyzerTest extends TestCase {
+public class GreekAnalyzerTest extends BaseTokenStreamTestCase {
 
 	/**
-	 * A helper method copied from org.apache.lucene.analysis.TestAnalyzers.
-	 *
-	 * @param a			the Analyzer to test
-	 * @param input		an input String to analyze
-	 * @param output	a String[] with the results of the analysis
-	 * @throws Exception in case an error occurs
-	 */
-	private void assertAnalyzesTo(Analyzer a, String input, String[] output) throws Exception {
-		TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-		TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-		for (int i=0; i<output.length; i++) {
-			assertTrue(ts.incrementToken());
-			assertEquals(termAtt.term(), output[i]);
-		}
-		assertFalse(ts.incrementToken());
-		ts.close();
-	}
-	
-	private void assertAnalyzesToReuse(Analyzer a, String input, String[] output) throws Exception {
-	    TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-	    TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-	    for (int i=0; i<output.length; i++) {
-	        assertTrue(ts.incrementToken());
-	        assertEquals(termAtt.term(), output[i]);
-	    }
-	    assertFalse(ts.incrementToken());
-	}
-
-	/**
 	 * Test the analysis of various greek strings.
 	 *
 	 * @throws Exception in case an error occurs
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/fa/TestPersianAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/fa/TestPersianAnalyzer.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/fa/TestPersianAnalyzer.java	(working copy)
@@ -19,17 +19,15 @@
 
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * Test the Persian Analyzer
  * 
  */
-public class TestPersianAnalyzer extends TestCase {
+public class TestPersianAnalyzer extends BaseTokenStreamTestCase {
 
   /**
    * This test fails with NPE when the stopwords file is missing in classpath
@@ -216,33 +214,4 @@
     assertAnalyzesToReuse(a, "برگ‌ها", new String[] { "برگ" });
   }
 
-  private void assertAnalyzesTo(Analyzer a, String input, String[] output)
-      throws Exception {
-	TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-	TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-
-	for (int i = 0; i < output.length; i++) {
-		assertTrue(ts.incrementToken());
-		assertEquals(output[i], termAtt.term());
-	}
-	
-	assertFalse(ts.incrementToken());
-    ts.close();
-  }
-  
-  private void assertAnalyzesToReuse(Analyzer a, String input, String[] output)
-      throws Exception {
-    TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts
-        .getAttribute(TermAttribute.class);
-
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(output[i], termAtt.term());
-    }
-
-    assertFalse(ts.incrementToken());
-    ts.close();
-  }
-
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java	(working copy)
@@ -20,16 +20,14 @@
 import java.io.IOException;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.ar.ArabicLetterTokenizer;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * Test the Arabic Normalization Filter
  * 
  */
-public class TestPersianNormalizationFilter extends TestCase {
+public class TestPersianNormalizationFilter extends BaseTokenStreamTestCase {
 
   public void testFarsiYeh() throws IOException {
     check("های", "هاي");
@@ -55,17 +53,12 @@
     check("زادہ", "زاده");
   }
 
-  private void check(final String input, final String expected)
-      throws IOException {
+  private void check(final String input, final String expected) throws IOException {
     ArabicLetterTokenizer tokenStream = new ArabicLetterTokenizer(
         new StringReader(input));
     PersianNormalizationFilter filter = new PersianNormalizationFilter(
         tokenStream);
-    TermAttribute termAtt = (TermAttribute) filter.getAttribute(TermAttribute.class);
-    assertTrue(filter.incrementToken());
-    assertEquals(expected, termAtt.term());
-    assertFalse(filter.incrementToken());
-    filter.close();
+    assertTokenStreamContents(filter, new String[]{expected});
   }
 
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/fr/TestElision.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/fr/TestElision.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/fr/TestElision.java	(working copy)
@@ -24,8 +24,7 @@
 import java.util.List;
 import java.util.Set;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
@@ -34,9 +33,9 @@
 /**
  * 
  */
-public class TestElision extends TestCase {
+public class TestElision extends BaseTokenStreamTestCase {
 
-  public void testElision() {
+  public void testElision() throws Exception {
     String test = "Plop, juste pour voir l'embrouille avec O'brian. M'enfin.";
     Tokenizer tokenizer = new StandardTokenizer(new StringReader(test));
     Set articles = new HashSet();
@@ -49,15 +48,11 @@
     assertEquals("enfin", tas.get(7));
   }
 
-  private List filtre(TokenFilter filter) {
+  private List filtre(TokenFilter filter) throws IOException {
     List tas = new ArrayList();
-    try {
-      TermAttribute termAtt = (TermAttribute) filter.getAttribute(TermAttribute.class);
-      while (filter.incrementToken()) {
-        tas.add(termAtt.term());
-      }
-    } catch (IOException e) {
-      e.printStackTrace();
+    TermAttribute termAtt = (TermAttribute) filter.getAttribute(TermAttribute.class);
+    while (filter.incrementToken()) {
+      tas.add(termAtt.term());
     }
     return tas;
   }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java	(working copy)
@@ -56,11 +56,9 @@
 
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * Test case for FrenchAnalyzer.
@@ -68,36 +66,8 @@
  * @version   $version$
  */
 
-public class TestFrenchAnalyzer extends TestCase {
+public class TestFrenchAnalyzer extends BaseTokenStreamTestCase {
 
-	// Method copied from TestAnalyzers, maybe should be refactored
-	public void assertAnalyzesTo(Analyzer a, String input, String[] output)
-		throws Exception {
-
-		TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-
-		TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-		for (int i = 0; i < output.length; i++) {
-			assertTrue(ts.incrementToken());
-			assertEquals(termAtt.term(), output[i]);
-		}
-		assertFalse(ts.incrementToken());
-		ts.close();
-	}
-	
-   public void assertAnalyzesToReuse(Analyzer a, String input, String[] output)
-       throws Exception {
-
-       TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-
-       TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-       for (int i = 0; i < output.length; i++) {
-           assertTrue(ts.incrementToken());
-           assertEquals(termAtt.term(), output[i]);
-       }
-       assertFalse(ts.incrementToken());
-   }
-
 	public void testAnalyzer() throws Exception {
 		FrenchAnalyzer fa = new FrenchAnalyzer();
 	
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestEmptyTokenStream.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestEmptyTokenStream.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestEmptyTokenStream.java	(working copy)
@@ -19,11 +19,10 @@
 
 import java.io.IOException;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.analysis.TokenStream;
 
-public class TestEmptyTokenStream extends TestCase {
+public class TestEmptyTokenStream extends LuceneTestCase {
 
   public void test() throws IOException {
     TokenStream ts = new EmptyTokenStream();
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java	(working copy)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
@@ -27,7 +27,7 @@
 import java.io.IOException;
 import java.io.StringReader;
 
-public class TestPrefixAndSuffixAwareTokenFilter extends TestCase {
+public class TestPrefixAndSuffixAwareTokenFilter extends BaseTokenStreamTestCase {
 
   public void test() throws IOException {
 
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java	(working copy)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
@@ -27,7 +27,7 @@
 import java.io.IOException;
 import java.io.StringReader;
 
-public class TestPrefixAwareTokenFilter extends TestCase {
+public class TestPrefixAwareTokenFilter extends BaseTokenStreamTestCase {
 
   public void test() throws IOException {
 
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestSingleTokenTokenFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestSingleTokenTokenFilter.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestSingleTokenTokenFilter.java	(working copy)
@@ -17,21 +17,28 @@
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
-
 import java.io.IOException;
 
+import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.analysis.Token;
 
-public class TestSingleTokenTokenFilter extends TestCase {
+public class TestSingleTokenTokenFilter extends LuceneTestCase {
 
   public void test() throws IOException {
+    final Token reusableToken = new Token();
+    
     Token token = new Token();
-
     SingleTokenTokenStream ts = new SingleTokenTokenStream(token);
+    ts.reset();
 
-    final Token reusableToken = new Token();
     assertEquals(token, ts.next(reusableToken));
     assertNull(ts.next(reusableToken));
+    
+    token = new Token("hallo", 10, 20, "someType");
+    ts.setToken(token);
+    ts.reset();
+
+    assertEquals(token, ts.next(reusableToken));
+    assertNull(ts.next(reusableToken));
   }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java	(working copy)
@@ -19,19 +19,18 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
 /**
  * Tests {@link EdgeNGramTokenFilter} for correctness.
  */
-public class EdgeNGramTokenFilterTest extends TestCase {
+public class EdgeNGramTokenFilterTest extends BaseTokenStreamTestCase {
   private TokenStream input;
 
-  public void setUp() {
+  public void setUp() throws Exception {
+    super.setUp();
     input = new WhitespaceTokenizer(new StringReader("abcde"));
   }
 
@@ -67,71 +66,40 @@
 
   public void testFrontUnigram() throws Exception {
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.FRONT, 1, 1);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(a,0,1)", termAtt.toString());
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[]{"a"}, new int[]{0}, new int[]{1});
   }
 
   public void testBackUnigram() throws Exception {
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.BACK, 1, 1);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(e,4,5)", termAtt.toString());
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[]{"e"}, new int[]{4}, new int[]{5});
   }
 
   public void testOversizedNgrams() throws Exception {
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.FRONT, 6, 6);
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[0], new int[0], new int[0]);
   }
 
   public void testFrontRangeOfNgrams() throws Exception {
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.FRONT, 1, 3);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(a,0,1)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(ab,0,2)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(abc,0,3)", termAtt.toString());
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3});
   }
 
   public void testBackRangeOfNgrams() throws Exception {
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.BACK, 1, 3);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(e,4,5)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(de,3,5)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(cde,2,5)", termAtt.toString());
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[]{"e","de","cde"}, new int[]{4,3,2}, new int[]{5,5,5});
   }
   
   public void testSmallTokenInStream() throws Exception {
     input = new WhitespaceTokenizer(new StringReader("abc de fgh"));
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.FRONT, 3, 3);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(abc,0,3)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(fgh,0,3)", termAtt.toString());
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[]{"abc","fgh"}, new int[]{0,0}, new int[]{3,3});
   }
   
   public void testReset() throws Exception {
     WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(new StringReader("abcde"));
     EdgeNGramTokenFilter filter = new EdgeNGramTokenFilter(tokenizer, EdgeNGramTokenFilter.Side.FRONT, 1, 3);
-    TermAttribute termAtt = (TermAttribute) filter.getAttribute(TermAttribute.class);
-    assertTrue(filter.incrementToken());
-    assertEquals("(a,0,1)", termAtt.toString());
-    assertTrue(filter.incrementToken());
-    assertEquals("(ab,0,2)", termAtt.toString());
+    assertTokenStreamContents(filter, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3});
     tokenizer.reset(new StringReader("abcde"));
-    filter.reset();
-    assertTrue(filter.incrementToken());
-    assertEquals("(a,0,1)", termAtt.toString());
+    assertTokenStreamContents(filter, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3});
   }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java	(working copy)
@@ -20,17 +20,16 @@
 
 import java.io.StringReader;
 
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 
-import junit.framework.TestCase;
-
 /**
  * Tests {@link EdgeNGramTokenizer} for correctness.
  */
-public class EdgeNGramTokenizerTest extends TestCase {
+public class EdgeNGramTokenizerTest extends BaseTokenStreamTestCase {
   private StringReader input;
 
-  public void setUp() {
+  public void setUp() throws Exception {
+    super.setUp();
     input = new StringReader("abcde");
   }
 
@@ -66,58 +65,33 @@
 
   public void testFrontUnigram() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.FRONT, 1, 1);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(a,0,1)", termAtt.toString());
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[]{"a"}, new int[]{0}, new int[]{1});
   }
 
   public void testBackUnigram() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.BACK, 1, 1);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(e,4,5)", termAtt.toString());
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[]{"e"}, new int[]{4}, new int[]{5});
   }
 
   public void testOversizedNgrams() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.FRONT, 6, 6);
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[0], new int[0], new int[0]);
   }
 
   public void testFrontRangeOfNgrams() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.FRONT, 1, 3);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(a,0,1)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(ab,0,2)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(abc,0,3)", termAtt.toString());
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3});
   }
 
   public void testBackRangeOfNgrams() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.BACK, 1, 3);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(e,4,5)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(de,3,5)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(cde,2,5)", termAtt.toString());
-    assertFalse(tokenizer.incrementToken());
+    assertTokenStreamContents(tokenizer, new String[]{"e","de","cde"}, new int[]{4,3,2}, new int[]{5,5,5});
   }
   
   public void testReset() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.FRONT, 1, 3);
-    TermAttribute termAtt = (TermAttribute) tokenizer.addAttribute(TermAttribute.class);
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(a,0,1)", termAtt.toString());
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(ab,0,2)", termAtt.toString());
+    assertTokenStreamContents(tokenizer, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3});
     tokenizer.reset(new StringReader("abcde"));
-    assertTrue(tokenizer.incrementToken());
-    assertEquals("(a,0,1)", termAtt.toString());
+    assertTokenStreamContents(tokenizer, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3});
   }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java	(working copy)
@@ -19,20 +19,19 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 
 import java.io.IOException;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
 /**
  * Tests {@link NGramTokenFilter} for correctness.
  */
-public class NGramTokenFilterTest extends TestCase {
+public class NGramTokenFilterTest extends BaseTokenStreamTestCase {
     private TokenStream input;
     
-    public void setUp() {
+    public void setUp() throws Exception {
+        super.setUp();
         input = new WhitespaceTokenizer(new StringReader("abcde"));
     }
 
@@ -56,70 +55,41 @@
         assertTrue(gotException);
     }
 
-    private void checkStream(TokenStream stream, String[] exp) throws IOException {
-      TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);
-      for (int i = 0; i < exp.length; i++) {
-        assertTrue(stream.incrementToken());
-        assertEquals(exp[i], termAtt.toString());
-      }
-      assertFalse(stream.incrementToken());
-    }
-    
     public void testUnigrams() throws Exception {
       NGramTokenFilter filter = new NGramTokenFilter(input, 1, 1);
-      String[] exp = new String[] {
-        "(a,0,1)", "(b,1,2)", "(c,2,3)", "(d,3,4)", "(e,4,5)"
-      };
-      
-      checkStream(filter, exp);
+      assertTokenStreamContents(filter, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5});
     }
 
     public void testBigrams() throws Exception {
       NGramTokenFilter filter = new NGramTokenFilter(input, 2, 2);
-      String[] exp = new String[] {
-          "(ab,0,2)", "(bc,1,3)", "(cd,2,4)", "(de,3,5)"
-        };
-        
-      checkStream(filter, exp);
+      assertTokenStreamContents(filter, new String[]{"ab","bc","cd","de"}, new int[]{0,1,2,3}, new int[]{2,3,4,5});
     }
 
     public void testNgrams() throws Exception {
       NGramTokenFilter filter = new NGramTokenFilter(input, 1, 3);
-      String[] exp = new String[] {
-          "(a,0,1)", "(b,1,2)", "(c,2,3)", "(d,3,4)", "(e,4,5)",
-          "(ab,0,2)", "(bc,1,3)", "(cd,2,4)", "(de,3,5)",
-          "(abc,0,3)", "(bcd,1,4)", "(cde,2,5)"
-      };
-        
-      checkStream(filter, exp);
+      assertTokenStreamContents(filter,
+        new String[]{"a","b","c","d","e", "ab","bc","cd","de", "abc","bcd","cde"}, 
+        new int[]{0,1,2,3,4, 0,1,2,3, 0,1,2},
+        new int[]{1,2,3,4,5, 2,3,4,5, 3,4,5}
+      );
     }
 
     public void testOversizedNgrams() throws Exception {
       NGramTokenFilter filter = new NGramTokenFilter(input, 6, 7);
-      assertFalse(filter.incrementToken());
+      assertTokenStreamContents(filter, new String[0], new int[0], new int[0]);
     }
     
     public void testSmallTokenInStream() throws Exception {
       input = new WhitespaceTokenizer(new StringReader("abc de fgh"));
       NGramTokenFilter filter = new NGramTokenFilter(input, 3, 3);
-      String[] exp = new String[] {
-          "(abc,0,3)", "(fgh,0,3)"
-        };
-        
-      checkStream(filter, exp);
+      assertTokenStreamContents(filter, new String[]{"abc","fgh"}, new int[]{0,0}, new int[]{3,3});
     }
     
     public void testReset() throws Exception {
       WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(new StringReader("abcde"));
-      NGramTokenFilter filter = new NGramTokenFilter(tokenizer, 1, 3);
-      TermAttribute termAtt = (TermAttribute) filter.addAttribute(TermAttribute.class);
-      assertTrue(filter.incrementToken());
-      assertEquals("(a,0,1)", termAtt.toString());
-      assertTrue(filter.incrementToken());
-      assertEquals("(b,1,2)", termAtt.toString());
+      NGramTokenFilter filter = new NGramTokenFilter(tokenizer, 1, 1);
+      assertTokenStreamContents(filter, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5});
       tokenizer.reset(new StringReader("abcde"));
-      filter.reset();
-      assertTrue(filter.incrementToken());
-      assertEquals("(a,0,1)", termAtt.toString());
+      assertTokenStreamContents(filter, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5});
     }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java	(working copy)
@@ -22,17 +22,16 @@
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 
-import junit.framework.TestCase;
-
 /**
  * Tests {@link NGramTokenizer} for correctness.
  */
-public class NGramTokenizerTest extends TestCase {
+public class NGramTokenizerTest extends BaseTokenStreamTestCase {
     private StringReader input;
     
-    public void setUp() {
+    public void setUp() throws Exception {
+        super.setUp();
         input = new StringReader("abcde");
     }
 
@@ -55,60 +54,35 @@
         }
         assertTrue(gotException);
     }
-    
-    private void checkStream(TokenStream stream, String[] exp) throws IOException {
-      TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);
-      for (int i = 0; i < exp.length; i++) {
-        assertTrue(stream.incrementToken());
-        assertEquals(exp[i], termAtt.toString());
-      }
-      assertFalse(stream.incrementToken());
-    }
 
     public void testUnigrams() throws Exception {
         NGramTokenizer tokenizer = new NGramTokenizer(input, 1, 1);
-        
-        String[] exp = new String[] {
-            "(a,0,1)", "(b,1,2)", "(c,2,3)", "(d,3,4)", "(e,4,5)"
-          };
-          
-        checkStream(tokenizer, exp);
+        assertTokenStreamContents(tokenizer, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5});
     }
 
     public void testBigrams() throws Exception {
         NGramTokenizer tokenizer = new NGramTokenizer(input, 2, 2);
-        String[] exp = new String[] {
-            "(ab,0,2)", "(bc,1,3)", "(cd,2,4)", "(de,3,5)"
-          };
-          
-        checkStream(tokenizer, exp);
+        assertTokenStreamContents(tokenizer, new String[]{"ab","bc","cd","de"}, new int[]{0,1,2,3}, new int[]{2,3,4,5});
     }
 
     public void testNgrams() throws Exception {
         NGramTokenizer tokenizer = new NGramTokenizer(input, 1, 3);
-        String[] exp = new String[] {
-            "(a,0,1)", "(b,1,2)", "(c,2,3)", "(d,3,4)", "(e,4,5)",
-            "(ab,0,2)", "(bc,1,3)", "(cd,2,4)", "(de,3,5)",
-            "(abc,0,3)", "(bcd,1,4)", "(cde,2,5)"
-        };
-          
-        checkStream(tokenizer, exp);
+        assertTokenStreamContents(tokenizer,
+          new String[]{"a","b","c","d","e", "ab","bc","cd","de", "abc","bcd","cde"}, 
+          new int[]{0,1,2,3,4, 0,1,2,3, 0,1,2},
+          new int[]{1,2,3,4,5, 2,3,4,5, 3,4,5}
+        );
     }
 
     public void testOversizedNgrams() throws Exception {
         NGramTokenizer tokenizer = new NGramTokenizer(input, 6, 7);
-        assertFalse(tokenizer.incrementToken());
+        assertTokenStreamContents(tokenizer, new String[0], new int[0], new int[0]);
     }
     
     public void testReset() throws Exception {
-      NGramTokenizer tokenizer = new NGramTokenizer(input, 1, 3);
-      TermAttribute termAtt = (TermAttribute) tokenizer.getAttribute(TermAttribute.class);
-      assertTrue(tokenizer.incrementToken());
-      assertEquals("(a,0,1)", termAtt.toString());
-      assertTrue(tokenizer.incrementToken());
-      assertEquals("(b,1,2)", termAtt.toString());
+      NGramTokenizer tokenizer = new NGramTokenizer(input, 1, 1);
+      assertTokenStreamContents(tokenizer, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5});
       tokenizer.reset(new StringReader("abcde"));
-      assertTrue(tokenizer.incrementToken());
-      assertEquals("(a,0,1)", termAtt.toString());
+      assertTokenStreamContents(tokenizer, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5});
     }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java	(working copy)
@@ -18,16 +18,12 @@
  */
 
 import java.io.File;
-import java.io.IOException;
 import java.io.Reader;
-import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * Test the Dutch Stem Filter, which only modifies the term text.
@@ -35,11 +31,11 @@
  * The code states that it uses the snowball algorithm, but tests reveal some differences.
  * 
  */
-public class TestDutchStemmer extends TestCase {
+public class TestDutchStemmer extends BaseTokenStreamTestCase {
   File dataDir = new File(System.getProperty("dataDir", "./bin"));
   File customDictFile = new File(dataDir, "org/apache/lucene/analysis/nl/customStemDict.txt");
   
-  public void testWithSnowballExamples() throws IOException {
+  public void testWithSnowballExamples() throws Exception {
 	 check("lichaamsziek", "lichaamsziek");
 	 check("lichamelijk", "licham");
 	 check("lichamelijke", "licham");
@@ -124,10 +120,10 @@
   
   public void testReusableTokenStream() throws Exception {
     Analyzer a = new DutchAnalyzer(); 
-    checkReuse(a, "lichaamsziek", "lichaamsziek");
-    checkReuse(a, "lichamelijk", "licham");
-    checkReuse(a, "lichamelijke", "licham");
-    checkReuse(a, "lichamelijkheden", "licham");
+    checkOneTermReuse(a, "lichaamsziek", "lichaamsziek");
+    checkOneTermReuse(a, "lichamelijk", "licham");
+    checkOneTermReuse(a, "lichamelijke", "licham");
+    checkOneTermReuse(a, "lichamelijkheden", "licham");
   }
   
   /**
@@ -141,10 +137,10 @@
   
   public void testLUCENE1678BWComp() throws Exception {
     Analyzer a = new DutchSubclassAnalyzer();
-    checkReuse(a, "lichaamsziek", "lichaamsziek");
-    checkReuse(a, "lichamelijk", "lichamelijk");
-    checkReuse(a, "lichamelijke", "lichamelijke");
-    checkReuse(a, "lichamelijkheden", "lichamelijkheden");
+    checkOneTermReuse(a, "lichaamsziek", "lichaamsziek");
+    checkOneTermReuse(a, "lichamelijk", "lichamelijk");
+    checkOneTermReuse(a, "lichamelijke", "lichamelijke");
+    checkOneTermReuse(a, "lichamelijkheden", "lichamelijkheden");
   }
  
   /* 
@@ -153,9 +149,9 @@
    */
   public void testExclusionTableReuse() throws Exception {
     DutchAnalyzer a = new DutchAnalyzer();
-    checkReuse(a, "lichamelijk", "licham");
+    checkOneTermReuse(a, "lichamelijk", "licham");
     a.setStemExclusionTable(new String[] { "lichamelijk" });
-    checkReuse(a, "lichamelijk", "lichamelijk");
+    checkOneTermReuse(a, "lichamelijk", "lichamelijk");
   }
   
   /* 
@@ -164,30 +160,13 @@
    */
   public void testStemDictionaryReuse() throws Exception {
     DutchAnalyzer a = new DutchAnalyzer();
-    checkReuse(a, "lichamelijk", "licham");
+    checkOneTermReuse(a, "lichamelijk", "licham");
     a.setStemDictionary(customDictFile);
-    checkReuse(a, "lichamelijk", "somethingentirelydifferent");
+    checkOneTermReuse(a, "lichamelijk", "somethingentirelydifferent");
   }
   
-  private void check(final String input, final String expected) throws IOException {
-    Analyzer analyzer = new DutchAnalyzer(); 
-    TokenStream stream = analyzer.tokenStream("dummy", new StringReader(input));
-    TermAttribute text = (TermAttribute) stream.getAttribute(TermAttribute.class);
-    assertTrue(stream.incrementToken());
-    assertEquals(expected, text.term());
-    assertFalse(stream.incrementToken());
-    stream.close();
+  private void check(final String input, final String expected) throws Exception {
+    checkOneTerm(new DutchAnalyzer(), input, expected); 
   }
   
-  private void checkReuse(Analyzer a, final String input, final String expected)
-      throws IOException {
-    TokenStream stream = a
-        .reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute text = (TermAttribute) stream
-        .getAttribute(TermAttribute.class);
-    assertTrue(stream.incrementToken());
-    assertEquals(expected, text.term());
-    assertFalse(stream.incrementToken());
-  }
-
 }
\ No newline at end of file
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java	(working copy)
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
@@ -27,20 +27,13 @@
 import java.io.IOException;
 import java.io.StringReader;
 
-public class NumericPayloadTokenFilterTest extends TestCase {
+public class NumericPayloadTokenFilterTest extends BaseTokenStreamTestCase {
 
 
   public NumericPayloadTokenFilterTest(String s) {
     super(s);
   }
 
-  protected void setUp() {
-  }
-
-  protected void tearDown() {
-
-  }
-
   public void test() throws IOException {
     String test = "The quick red fox jumped over the lazy brown dogs";
 
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java	(working copy)
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
@@ -25,20 +25,13 @@
 import java.io.IOException;
 import java.io.StringReader;
 
-public class TokenOffsetPayloadTokenFilterTest extends TestCase {
+public class TokenOffsetPayloadTokenFilterTest extends BaseTokenStreamTestCase {
 
 
   public TokenOffsetPayloadTokenFilterTest(String s) {
     super(s);
   }
 
-  protected void setUp() {
-  }
-
-  protected void tearDown() {
-
-  }
-
   public void test() throws IOException {
     String test = "The quick red fox jumped over the lazy brown dogs";
 
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java	(working copy)
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
@@ -27,21 +27,13 @@
 import java.io.IOException;
 import java.io.StringReader;
 
-public class TypeAsPayloadTokenFilterTest extends TestCase {
+public class TypeAsPayloadTokenFilterTest extends BaseTokenStreamTestCase {
 
 
   public TypeAsPayloadTokenFilterTest(String s) {
     super(s);
   }
 
-  protected void setUp() {
-  }
-
-  protected void tearDown() {
-
-  }
-
-
   public void test() throws IOException {
     String test = "The quick red fox jumped over the lazy brown dogs";
 
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/position/PositionFilterTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/position/PositionFilterTest.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/position/PositionFilterTest.java	(working copy)
@@ -19,13 +19,12 @@
 
 import java.io.IOException;
 
-import junit.framework.TestCase;
-import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.shingle.ShingleFilter;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
-public class PositionFilterTest extends TestCase {
+public class PositionFilterTest extends BaseTokenStreamTestCase {
 
   public class TestTokenStream extends TokenStream {
 
@@ -40,6 +39,7 @@
     }
 
     public final boolean incrementToken() throws IOException {
+      clearAttributes();
       if (index < testToken.length) {
         termAtt.setTermBuffer(testToken[index++]);
         return true;
@@ -52,9 +52,6 @@
     }
   }
 
-  public static void main(String[] args) {
-    junit.textui.TestRunner.run(PositionFilterTest.class);
-  }
   public static final String[] TEST_TOKEN = new String[]{
     "please",
     "divide",
@@ -105,65 +102,39 @@
     "word"
   };
 
-  public void testFilter() throws IOException {
+  public void testFilter() throws Exception {
 
-    filterTest(new PositionFilter(new TestTokenStream(TEST_TOKEN)),
+    assertTokenStreamContents(new PositionFilter(new TestTokenStream(TEST_TOKEN)),
                TEST_TOKEN,
                TEST_TOKEN_POSITION_INCREMENTS);
   }
 
-  public void testNonZeroPositionIncrement() throws IOException {
+  public void testNonZeroPositionIncrement() throws Exception {
     
-    filterTest(new PositionFilter(new TestTokenStream(TEST_TOKEN), 5),
+    assertTokenStreamContents(new PositionFilter(new TestTokenStream(TEST_TOKEN), 5),
                TEST_TOKEN,
                TEST_TOKEN_NON_ZERO_POSITION_INCREMENTS);
   }
   
-  public void testReset() throws IOException {
+  public void testReset() throws Exception {
 
     PositionFilter filter = new PositionFilter(new TestTokenStream(TEST_TOKEN));
-    filterTest(filter, TEST_TOKEN, TEST_TOKEN_POSITION_INCREMENTS);
+    assertTokenStreamContents(filter, TEST_TOKEN, TEST_TOKEN_POSITION_INCREMENTS);
     filter.reset();
     // Make sure that the reset filter provides correct position increments
-    filterTest(filter, TEST_TOKEN, TEST_TOKEN_POSITION_INCREMENTS);
+    assertTokenStreamContents(filter, TEST_TOKEN, TEST_TOKEN_POSITION_INCREMENTS);
   }
   
   /** Tests ShingleFilter up to six shingles against six terms.
    *  Tests PositionFilter setting all but the first positionIncrement to zero.
    * @throws java.io.IOException @see Token#next(Token)
    */
-  public void test6GramFilterNoPositions() throws IOException {
+  public void test6GramFilterNoPositions() throws Exception {
 
     ShingleFilter filter = new ShingleFilter(new TestTokenStream(TEST_TOKEN), 6);
-    filterTest(new PositionFilter(filter),
+    assertTokenStreamContents(new PositionFilter(filter),
                SIX_GRAM_NO_POSITIONS_TOKENS,
                SIX_GRAM_NO_POSITIONS_INCREMENTS);
   }
 
-  protected TokenStream filterTest(final TokenStream filter,
-                                   final String[] tokensToCompare,
-                                   final int[] positionIncrements)
-      throws IOException {
-
-    int i = 0;
-    final Token reusableToken = new Token();
-
-    for (Token nextToken = filter.next(reusableToken)
-        ; i < tokensToCompare.length
-        ; nextToken = filter.next(reusableToken)) {
-
-      if (null != nextToken) {
-        final String termText = nextToken.term();
-        final String goldText = tokensToCompare[i];
-
-        assertEquals("Wrong termText", goldText, termText);
-        assertEquals("Wrong positionIncrement for token \"" + termText + "\"",
-                     positionIncrements[i], nextToken.getPositionIncrement());
-      }else{
-        assertNull(tokensToCompare[i]);
-      }
-      i++;
-    }
-    return filter;
-  }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java	(working copy)
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.LetterTokenizer;
 import org.apache.lucene.analysis.TokenStream;
@@ -39,7 +39,7 @@
 import java.io.Reader;
 import java.io.StringReader;
 
-public class QueryAutoStopWordAnalyzerTest extends TestCase {
+public class QueryAutoStopWordAnalyzerTest extends BaseTokenStreamTestCase {
   String variedFieldValues[] = {"the", "quick", "brown", "fox", "jumped", "over", "the", "lazy", "boring", "dog"};
   String repetitiveFieldValues[] = {"boring", "boring", "vaguelyboring"};
   RAMDirectory dir;
@@ -67,8 +67,8 @@
   }
 
   protected void tearDown() throws Exception {
+    reader.close();
     super.tearDown();
-    reader.close();
   }
 
   //Helper method to query
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java	(working copy)
@@ -22,9 +22,9 @@
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 
-public class TestReverseStringFilter extends LuceneTestCase {
+public class TestReverseStringFilter extends BaseTokenStreamTestCase {
   public void testFilter() throws Exception {
     TokenStream stream = new WhitespaceTokenizer(
         new StringReader("Do have a nice day"));     // 1-4 length string
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java	(working copy)
@@ -24,8 +24,7 @@
 import java.io.Reader;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
@@ -37,7 +36,7 @@
  * @version   $Id$
  */
 
-public class TestRussianAnalyzer extends TestCase
+public class TestRussianAnalyzer extends BaseTokenStreamTestCase
 {
     private InputStreamReader inWords;
 
@@ -55,6 +54,7 @@
 
     protected void setUp() throws Exception
     {
+      super.setUp();
       dataDir = new File(System.getProperty("dataDir", "./bin"));
     }
 
@@ -195,14 +195,4 @@
       assertAnalyzesToReuse(a, "Но знание это хранилось в тайне",
           new String[] { "знан", "хран", "тайн" });
     }
-
-    private void assertAnalyzesToReuse(Analyzer a, String input, String[] output) throws Exception {
-      TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-      TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-      for (int i=0; i<output.length; i++) {
-          assertTrue(ts.incrementToken());
-          assertEquals(termAtt.term(), output[i]);
-      }
-      assertFalse(ts.incrementToken());
-    }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianStem.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianStem.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianStem.java	(working copy)
@@ -17,15 +17,14 @@
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.util.LuceneTestCase;
 import java.io.BufferedReader;
 import java.io.File;
 import java.io.InputStreamReader;
 import java.io.FileInputStream;
 import java.util.ArrayList;
 
-public class TestRussianStem extends TestCase
+public class TestRussianStem extends LuceneTestCase
 {
     private ArrayList words = new ArrayList();
     private ArrayList stems = new ArrayList();
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java	(working copy)
@@ -20,15 +20,14 @@
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.LetterTokenizer;
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.analysis.tokenattributes.*;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexWriter;
@@ -44,19 +43,13 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
 
-import junit.framework.TestCase;
-
 /**
  * A test class for ShingleAnalyzerWrapper as regards queries and scoring.
  */
-public class ShingleAnalyzerWrapperTest extends TestCase {
+public class ShingleAnalyzerWrapperTest extends BaseTokenStreamTestCase {
 
   public IndexSearcher searcher;
 
-  public static void main(String[] args) {
-    junit.textui.TestRunner.run(ShingleAnalyzerWrapperTest.class);
-  }
-
   /**
    * Set up a new index in RAM with three test phrases and the supplied Analyzer.
    *
@@ -233,8 +226,7 @@
     assertAnalyzesToReuse(a, "this is a test",
         new String[] { "this", "is", "a", "test" },
         new int[] { 0, 5, 8, 10 },
-        new int[] { 4, 7, 9, 14 },
-        new int[] { 1, 1, 1, 1 });
+        new int[] { 4, 7, 9, 14 });
   }
   
   /*
@@ -269,25 +261,4 @@
         new int[] { 6, 13, 13, 18, 18, 27, 27 },
         new int[] { 1, 0, 1, 0, 1, 0, 1 });
   }
-  
-  private void assertAnalyzesToReuse(Analyzer a, String input, String[] output,
-      int[] startOffsets, int[] endOffsets, int[] posIncr) throws Exception {
-    TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts
-        .getAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = (OffsetAttribute) ts
-        .getAttribute(OffsetAttribute.class);
-    PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) ts
-        .getAttribute(PositionIncrementAttribute.class);
-
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(output[i], termAtt.term());
-      assertEquals(startOffsets[i], offsetAtt.startOffset());
-      assertEquals(endOffsets[i], offsetAtt.endOffset());
-      assertEquals(posIncr[i], posIncAtt.getPositionIncrement());
-    }
-
-    assertFalse(ts.incrementToken());
-  }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java	(working copy)
@@ -20,18 +20,14 @@
 import java.io.IOException;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttributeImpl;
+import org.apache.lucene.analysis.tokenattributes.*;
 
-public class ShingleFilterTest extends TestCase {
+public class ShingleFilterTest extends BaseTokenStreamTestCase {
 
   public class TestTokenStream extends TokenStream {
 
@@ -53,6 +49,7 @@
     }
 
     public final boolean incrementToken() throws IOException {
+      clearAttributes();
       if (index < testToken.length) {
         Token t = testToken[index++];
         termAtt.setTermBuffer(t.termBuffer(), 0, t.termLength());
@@ -66,10 +63,6 @@
     }
   }
 
-  public static void main(String[] args) {
-    junit.textui.TestRunner.run(ShingleFilterTest.class);
-  }
-
   public static final Token[] TEST_TOKEN = new Token[] {
       createToken("please", 0, 6),
       createToken("divide", 7, 13),
@@ -188,15 +181,19 @@
   public void testReset() throws Exception {
     Tokenizer wsTokenizer = new WhitespaceTokenizer(new StringReader("please divide this sentence"));
     TokenStream filter = new ShingleFilter(wsTokenizer, 2);
-    TermAttribute termAtt = (TermAttribute) filter.getAttribute(TermAttribute.class);
-    assertTrue(filter.incrementToken());
-    assertEquals("(please,0,6)", termAtt.toString());
-    assertTrue(filter.incrementToken());
-    assertEquals("(please divide,0,13,type=shingle,posIncr=0)", termAtt.toString());
+    assertTokenStreamContents(filter,
+      new String[]{"please","please divide","divide","divide this","this","this sentence","sentence"},
+      new int[]{0,0,7,7,14,14,19}, new int[]{6,13,13,18,18,27,27},
+      new String[]{TypeAttributeImpl.DEFAULT_TYPE,"shingle",TypeAttributeImpl.DEFAULT_TYPE,"shingle",TypeAttributeImpl.DEFAULT_TYPE,"shingle",TypeAttributeImpl.DEFAULT_TYPE},
+      new int[]{1,0,1,0,1,0,1}
+    );
     wsTokenizer.reset(new StringReader("please divide this sentence"));
-    filter.reset();
-    assertTrue(filter.incrementToken());
-    assertEquals("(please,0,6)", termAtt.toString());
+    assertTokenStreamContents(filter,
+      new String[]{"please","please divide","divide","divide this","this","this sentence","sentence"},
+      new int[]{0,0,7,7,14,14,19}, new int[]{6,13,13,18,18,27,27},
+      new String[]{TypeAttributeImpl.DEFAULT_TYPE,"shingle",TypeAttributeImpl.DEFAULT_TYPE,"shingle",TypeAttributeImpl.DEFAULT_TYPE,"shingle",TypeAttributeImpl.DEFAULT_TYPE},
+      new int[]{1,0,1,0,1,0,1}
+    );
   }
   
   protected void shingleFilterTest(int maxSize, Token[] tokensToShingle, Token[] tokensToCompare,
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java	(working copy)
@@ -21,9 +21,10 @@
 import java.util.Collection;
 import java.util.Iterator;
 import java.util.LinkedList;
+import java.util.HashSet;
+import java.util.Arrays;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CachingTokenFilter;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
@@ -33,15 +34,16 @@
 import org.apache.lucene.analysis.payloads.PayloadHelper;
 import org.apache.lucene.analysis.shingle.ShingleMatrixFilter.Matrix;
 import org.apache.lucene.analysis.shingle.ShingleMatrixFilter.Matrix.Column;
-import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.analysis.tokenattributes.*;
 
-public class TestShingleMatrixFilter extends TestCase {
+public class TestShingleMatrixFilter extends BaseTokenStreamTestCase {
 
+  public TestShingleMatrixFilter(String name) {
+    // use this ctor, because SingleTokenTokenStream only uses next(Token), so exclude it
+    super(name, new HashSet(Arrays.asList(new String[]{
+      "testBehavingAsShingleFilter", "testMatrix"
+    })));
+  }
 
   public void testBehavingAsShingleFilter() throws IOException {
 
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java	(working copy)
@@ -21,26 +21,18 @@
 import java.text.SimpleDateFormat;
 import java.util.Locale;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TeeSinkTokenFilter;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.TeeSinkTokenFilter.SinkTokenStream;
 
-public class DateRecognizerSinkTokenizerTest extends TestCase {
+public class DateRecognizerSinkTokenizerTest extends BaseTokenStreamTestCase {
 
 
   public DateRecognizerSinkTokenizerTest(String s) {
     super(s);
   }
 
-  protected void setUp() {
-  }
-
-  protected void tearDown() {
-
-  }
-
   public void test() throws IOException {
     DateRecognizerSinkFilter sinkFilter = new DateRecognizerSinkFilter(new SimpleDateFormat("MM/dd/yyyy", Locale.US));
     String test = "The quick red fox jumped over the lazy brown dogs on 7/11/2006  The dogs finally reacted on 7/12/2006";
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java	(working copy)
@@ -19,26 +19,18 @@
 import java.io.IOException;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TeeSinkTokenFilter;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.TeeSinkTokenFilter.SinkTokenStream;
 
-public class TokenRangeSinkTokenizerTest extends TestCase {
+public class TokenRangeSinkTokenizerTest extends BaseTokenStreamTestCase {
 
 
   public TokenRangeSinkTokenizerTest(String s) {
     super(s);
   }
 
-  protected void setUp() {
-  }
-
-  protected void tearDown() {
-
-  }
-
   public void test() throws IOException {
     TokenRangeSinkFilter sinkFilter = new TokenRangeSinkFilter(2, 4);
     String test = "The quick red fox jumped over the lazy brown dogs";
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java	(working copy)
@@ -19,8 +19,7 @@
 import java.io.IOException;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TeeSinkTokenFilter;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
@@ -29,20 +28,13 @@
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 
-public class TokenTypeSinkTokenizerTest extends TestCase {
+public class TokenTypeSinkTokenizerTest extends BaseTokenStreamTestCase {
 
 
   public TokenTypeSinkTokenizerTest(String s) {
     super(s);
   }
 
-  protected void setUp() {
-  }
-
-  protected void tearDown() {
-
-  }
-
   public void test() throws IOException {
     TokenTypeSinkFilter sinkFilter = new TokenTypeSinkFilter("D");
     String test = "The quick red fox jumped over the lazy brown dogs";
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java	(revision 807118)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java	(working copy)
@@ -18,16 +18,11 @@
  */
 
 import java.io.Reader;
-import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 
 /**
  * Test case for ThaiAnalyzer, modified from TestFrenchAnalyzer
@@ -35,7 +30,7 @@
  * @version   0.1
  */
 
-public class TestThaiAnalyzer extends TestCase {
+public class TestThaiAnalyzer extends BaseTokenStreamTestCase {
 	
 	/* 
 	 * testcase for offsets
@@ -71,57 +66,7 @@
 				new String[] { "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>", "<NUM>" });
 	}
 	*/
-	
-	public void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[])
-		throws Exception {
 
-		TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-		TermAttribute termAtt = (TermAttribute) ts.addAttribute(TermAttribute.class);
-		OffsetAttribute offsetAtt = (OffsetAttribute) ts.addAttribute(OffsetAttribute.class);
-		TypeAttribute typeAtt = (TypeAttribute) ts.addAttribute(TypeAttribute.class);
-		for (int i = 0; i < output.length; i++) {
-			assertTrue(ts.incrementToken());
-			assertEquals(termAtt.term(), output[i]);
-			if (startOffsets != null)
-				assertEquals(offsetAtt.startOffset(), startOffsets[i]);
-			if (endOffsets != null)
-				assertEquals(offsetAtt.endOffset(), endOffsets[i]);
-			if (types != null)
-				assertEquals(typeAtt.type(), types[i]);
-		}
-		assertFalse(ts.incrementToken());
-		ts.close();
-	}
-	
-	public void assertAnalyzesToReuse(Analyzer a, String input, String[] output)
-      throws Exception {
-
-      TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-      TermAttribute termAtt = (TermAttribute) ts
-        .addAttribute(TermAttribute.class);
-      OffsetAttribute offsetAtt = (OffsetAttribute) ts
-        .addAttribute(OffsetAttribute.class);
-      TypeAttribute typeAtt = (TypeAttribute) ts
-        .addAttribute(TypeAttribute.class);
-      for (int i = 0; i < output.length; i++) {
-        assertTrue(ts.incrementToken());
-        assertEquals(termAtt.term(), output[i]);
-      }
-      assertFalse(ts.incrementToken());
-    }
-	
-	public void assertAnalyzesTo(Analyzer a, String input, String[] output) throws Exception {
-		assertAnalyzesTo(a, input, output, null, null, null);
-	}
-	
-	public void assertAnalyzesTo(Analyzer a, String input, String[] output, String[] types) throws Exception {
-		assertAnalyzesTo(a, input, output, null, null, types);
-	}
-	
-	public void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[]) throws Exception {
-		assertAnalyzesTo(a, input, output, startOffsets, endOffsets, null);
-	}
-
 	public void testAnalyzer() throws Exception {
 		ThaiAnalyzer analyzer = new ThaiAnalyzer();
 	
Index: contrib/analyzers/smartcn/src/test/org/apache/lucene/analysis/cn/TestSmartChineseAnalyzer.java
===================================================================
--- contrib/analyzers/smartcn/src/test/org/apache/lucene/analysis/cn/TestSmartChineseAnalyzer.java	(revision 807118)
+++ contrib/analyzers/smartcn/src/test/org/apache/lucene/analysis/cn/TestSmartChineseAnalyzer.java	(working copy)
@@ -20,20 +20,13 @@
 import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.io.Reader;
-import java.io.StringReader;
 import java.io.UnsupportedEncodingException;
 import java.util.Date;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 
-public class TestSmartChineseAnalyzer extends TestCase {
+public class TestSmartChineseAnalyzer extends BaseTokenStreamTestCase {
   
   public void testChineseStopWordsDefault() throws Exception {
     Analyzer ca = new SmartChineseAnalyzer(); /* will load stopwords */
@@ -77,20 +70,11 @@
     assertAnalyzesTo(ca, sentence, result);
   }
   
-  public void testChineseAnalyzer() throws IOException {
-    Token nt = new Token();
+  public void testChineseAnalyzer() throws Exception {
     Analyzer ca = new SmartChineseAnalyzer(true);
-    Reader sentence = new StringReader("我购买了道具和服装。");
+    String sentence = "我购买了道具和服装。";
     String[] result = { "我", "购买", "了", "道具", "和", "服装" };
-    TokenStream ts = ca.tokenStream("sentence", sentence);
-    int i = 0;
-    nt = ts.next(nt);
-    while (nt != null) {
-      assertEquals(result[i], nt.term());
-      i++;
-      nt = ts.next(nt);
-    }
-    ts.close();
+    assertAnalyzesTo(ca, sentence, result);
   }
   
   /*
@@ -165,90 +149,4 @@
         new int[] { 0, 1, 3, 4, 6, 7 },
         new int[] { 1, 3, 4, 6, 7, 9 });
   }
-  
-  public void assertAnalyzesToReuse(Analyzer a, String input, String[] output,
-      int startOffsets[], int endOffsets[]) throws Exception {
-
-    TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = (OffsetAttribute) ts.getAttribute(OffsetAttribute.class);
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(termAtt.term(), output[i]);
-      assertEquals(offsetAtt.startOffset(), startOffsets[i]);
-      assertEquals(offsetAtt.endOffset(), endOffsets[i]);
-    }
-    assertFalse(ts.incrementToken());
-  }
-  
-  public void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[])
-  throws Exception {
-
-    TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = (OffsetAttribute) ts.getAttribute(OffsetAttribute.class);
-    TypeAttribute typeAtt = (TypeAttribute) ts.getAttribute(TypeAttribute.class);
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(termAtt.term(), output[i]);
-      if (startOffsets != null)
-        assertEquals(offsetAtt.startOffset(), startOffsets[i]);
-      if (endOffsets != null)
-        assertEquals(offsetAtt.endOffset(), endOffsets[i]);
-      if (types != null)
-        assertEquals(typeAtt.type(), types[i]);
-    }
-    assertFalse(ts.incrementToken());
-    ts.close();
-  }
-
-public void assertAnalyzesTo(Analyzer a, String input, String[] output) throws Exception {
-  assertAnalyzesTo(a, input, output, null, null, null);
 }
-
-public void assertAnalyzesTo(Analyzer a, String input, String[] output, String[] types) throws Exception {
-  assertAnalyzesTo(a, input, output, null, null, types);
-}
-
-public void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[]) throws Exception {
-  assertAnalyzesTo(a, input, output, startOffsets, endOffsets, null);
-}
-
-
-  /**
-   * @param args
-   * @throws IOException
-   */
-  public static void main(String[] args) throws IOException {
-    new TestSmartChineseAnalyzer().sampleMethod();
-  }
-
-  /**
-   * @throws UnsupportedEncodingException
-   * @throws FileNotFoundException
-   * @throws IOException
-   */
-  private void sampleMethod() throws UnsupportedEncodingException,
-      FileNotFoundException, IOException {
-    Token nt = new Token();
-    Analyzer ca = new SmartChineseAnalyzer(true);
-    Reader sentence = new StringReader(
-        "我从小就不由自主地认为自己长大以后一定得成为一个象我父亲一样的画家, 可能是父母潜移默化的影响。其实我根本不知道作为画家意味着什么，我是否喜欢，最重要的是否适合我，我是否有这个才华。其实人到中年的我还是不确定我最喜欢什么，最想做的是什么？我相信很多人和我一样有同样的烦恼。毕竟不是每个人都能成为作文里的宇航员，科学家和大教授。知道自己适合做什么，喜欢做什么，能做好什么其实是个非常困难的问题。"
-            + "幸运的是，我想我的孩子不会为这个太过烦恼。通过老大，我慢慢发现美国高中的一个重要功能就是帮助学生分析他们的专长和兴趣，从而帮助他们选择大学的专业和未来的职业。我觉得帮助一个未成形的孩子找到她未来成长的方向是个非常重要的过程。"
-            + "美国高中都有专门的职业顾问，通过接触不同的课程，和各种心理，个性，兴趣很多方面的问答来帮助每个学生找到最感兴趣的专业。这样的教育一般是要到高年级才开始， 可老大因为今年上计算机的课程就是研究一个职业走向的软件项目，所以她提前做了这些考试和面试。看来以后这样的教育会慢慢由电脑来测试了。老大带回家了一些试卷，我挑出一些给大家看看。这门课她花了2个多月才做完，这里只是很小的一部分。"
-            + "在测试里有这样的一些问题："
-            + "你是个喜欢动手的人吗？ 你喜欢修东西吗？你喜欢体育运动吗？你喜欢在室外工作吗？你是个喜欢思考的人吗？你喜欢数学和科学课吗？你喜欢一个人工作吗？你对自己的智力自信吗？你的创造能力很强吗？你喜欢艺术，音乐和戏剧吗？  你喜欢自由自在的工作环境吗？你喜欢尝试新的东西吗？ 你喜欢帮助别人吗？你喜欢教别人吗？你喜欢和机器和工具打交道吗？你喜欢当领导吗？你喜欢组织活动吗？你什么和数字打交道吗？");
-    TokenStream ts = ca.tokenStream("sentence", sentence);
-
-    System.out.println("start: " + (new Date()));
-    long before = System.currentTimeMillis();
-    nt = ts.next(nt);
-    while (nt != null) {
-      System.out.println(nt.term());
-      nt = ts.next(nt);
-    }
-    ts.close();
-    long now = System.currentTimeMillis();
-    System.out.println("time: " + (now - before) / 1000.0 + " s");
-  }
-}
Index: contrib/memory/src/test/org/apache/lucene/index/memory/TestSynonymTokenFilter.java
===================================================================
--- contrib/memory/src/test/org/apache/lucene/index/memory/TestSynonymTokenFilter.java	(revision 807118)
+++ contrib/memory/src/test/org/apache/lucene/index/memory/TestSynonymTokenFilter.java	(working copy)
@@ -28,9 +28,6 @@
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 
 public class TestSynonymTokenFilter extends BaseTokenStreamTestCase {
@@ -117,44 +114,4 @@
     }
   }
   
-  public void assertAnalyzesTo(Analyzer a, String input, String[] output,
-      int startOffsets[], int endOffsets[], int posIncs[]) throws Exception {
-
-    TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts
-        .getAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = (OffsetAttribute) ts
-        .getAttribute(OffsetAttribute.class);
-    PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) ts
-        .getAttribute(PositionIncrementAttribute.class);
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(termAtt.term(), output[i]);
-      assertEquals(offsetAtt.startOffset(), startOffsets[i]);
-      assertEquals(offsetAtt.endOffset(), endOffsets[i]);
-      assertEquals(posIncAtt.getPositionIncrement(), posIncs[i]);
-    }
-    assertFalse(ts.incrementToken());
-    ts.close();
-  }
-
-  public void assertAnalyzesToReuse(Analyzer a, String input, String[] output,
-      int startOffsets[], int endOffsets[], int posIncs[]) throws Exception {
-
-    TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts
-        .getAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = (OffsetAttribute) ts
-        .getAttribute(OffsetAttribute.class);
-    PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) ts
-        .getAttribute(PositionIncrementAttribute.class);
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(termAtt.term(), output[i]);
-      assertEquals(offsetAtt.startOffset(), startOffsets[i]);
-      assertEquals(offsetAtt.endOffset(), endOffsets[i]);
-      assertEquals(posIncAtt.getPositionIncrement(), posIncs[i]);
-    }
-    assertFalse(ts.incrementToken());
-  }
 }
Index: contrib/snowball/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java
===================================================================
--- contrib/snowball/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java	(revision 807118)
+++ contrib/snowball/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java	(working copy)
@@ -20,8 +20,7 @@
 import java.io.Reader;
 import java.io.StringReader;
 
-import junit.framework.TestCase;
-
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.index.Payload;
@@ -33,33 +32,8 @@
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 
-public class TestSnowball extends TestCase {
+public class TestSnowball extends BaseTokenStreamTestCase {
 
-  public void assertAnalyzesTo(Analyzer a,
-                               String input,
-                               String[] output) throws Exception {
-    TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(output[i], termAtt.term());
-    }
-    assertFalse(ts.incrementToken());
-    ts.close();
-  }
-  
-  public void assertAnalyzesToReuse(Analyzer a,
-                               String input,
-                               String[] output) throws Exception {
-    TokenStream ts = a.reusableTokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-    for (int i = 0; i < output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(output[i], termAtt.term());
-    }
-    assertFalse(ts.incrementToken());
-  }
-
   public void testEnglish() throws Exception {
     Analyzer a = new SnowballAnalyzer("English");
     assertAnalyzesTo(a, "he abhorred accents",
Index: contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java
===================================================================
--- contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java	(revision 807118)
+++ contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java	(working copy)
@@ -27,6 +27,7 @@
 import java.util.Set;
 import java.util.HashSet;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
@@ -38,22 +39,13 @@
  *
  *
  **/
-public class WikipediaTokenizerTest extends TestCase {
+public class WikipediaTokenizerTest extends BaseTokenStreamTestCase {
   protected static final String LINK_PHRASES = "click [[link here again]] click [http://lucene.apache.org here again] [[Category:a b c d]]";
 
-
   public WikipediaTokenizerTest(String s) {
     super(s);
   }
 
-  protected void setUp() {
-  }
-
-  protected void tearDown() {
-
-  }
-
-
   public void testHandwritten() throws Exception {
     //make sure all tokens are in only one type
     String test = "[[link]] This is a [[Category:foo]] Category  This is a linked [[:Category:bar none withstanding]] " +
Index: src/java/org/apache/lucene/analysis/Token.java
===================================================================
--- src/java/org/apache/lucene/analysis/Token.java	(revision 807118)
+++ src/java/org/apache/lucene/analysis/Token.java	(working copy)
@@ -866,6 +866,9 @@
       if (payload !=null) {
         to.payload = (Payload) payload.clone();
       }
+    // remove the following optimization in 3.0 when old TokenStream API removed:
+    } else if (target instanceof TokenWrapper) {
+      ((TokenWrapper) target).delegate = (Token) this.clone();
     } else {
       initTermBuffer();
       ((TermAttribute) target).setTermBuffer(termBuffer, 0, termLength);
Index: src/test/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
===================================================================
--- src/test/org/apache/lucene/analysis/BaseTokenStreamTestCase.java	(revision 807118)
+++ src/test/org/apache/lucene/analysis/BaseTokenStreamTestCase.java	(working copy)
@@ -18,7 +18,10 @@
  */
 
 import java.util.Set;
+import java.io.StringReader;
+import java.io.IOException;
  
+import org.apache.lucene.analysis.tokenattributes.*;
 import org.apache.lucene.util.LuceneTestCase;
 
 /** 
@@ -59,12 +62,6 @@
   }
 
   // @Override
-  protected void tearDown() throws Exception {
-    TokenStream.setOnlyUseNewAPI(false);
-    super.tearDown();
-  }
-
-  // @Override
   public void runBare() throws Throwable {
     // Do the test with onlyUseNewAPI=false (default)
     try {
@@ -86,5 +83,127 @@
       }
     }
   }
+  
+  // some helpers to test Analyzers and TokenStreams:
+  
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
+    assertNotNull(output);
+    assertTrue("has TermAttribute", ts.hasAttribute(TermAttribute.class));
+    TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
+    
+    OffsetAttribute offsetAtt = null;
+    if (startOffsets != null || endOffsets != null) {
+      assertTrue("has OffsetAttribute", ts.hasAttribute(OffsetAttribute.class));
+      offsetAtt = (OffsetAttribute) ts.getAttribute(OffsetAttribute.class);
+    }
+    
+    TypeAttribute typeAtt = null;
+    if (types != null) {
+      assertTrue("has TypeAttribute", ts.hasAttribute(TypeAttribute.class));
+      typeAtt = (TypeAttribute) ts.getAttribute(TypeAttribute.class);
+    }
+    
+    PositionIncrementAttribute posIncrAtt = null;
+    if (posIncrements != null) {
+      assertTrue("has PositionIncrementAttribute", ts.hasAttribute(PositionIncrementAttribute.class));
+      posIncrAtt = (PositionIncrementAttribute) ts.getAttribute(PositionIncrementAttribute.class);
+    }
+    
+    ts.reset();
+    for (int i = 0; i < output.length; i++) {
+      assertTrue("token "+i+" exists", ts.incrementToken());
+      assertEquals("term "+i, output[i], termAtt.term());
+      if (startOffsets != null)
+        assertEquals("startOffset "+i, startOffsets[i], offsetAtt.startOffset());
+      if (endOffsets != null)
+        assertEquals("endOffset "+i, endOffsets[i], offsetAtt.endOffset());
+      if (types != null)
+        assertEquals("type "+i, types[i], typeAtt.type());
+      if (posIncrements != null)
+        assertEquals("posIncrement "+i, posIncrements[i], posIncrAtt.getPositionIncrement());
+    }
+    assertFalse("end of stream", ts.incrementToken());
+    ts.close();
+  }
+  
+  public static void assertTokenStreamContents(TokenStream ts, String[] output) throws IOException {
+    assertTokenStreamContents(ts, output, null, null, null, null);
+  }
+  
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, String[] types) throws IOException {
+    assertTokenStreamContents(ts, output, null, null, types, null);
+  }
+  
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int[] posIncrements) throws IOException {
+    assertTokenStreamContents(ts, output, null, null, null, posIncrements);
+  }
+  
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[]) throws IOException {
+    assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, null);
+  }
+  
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], int[] posIncrements) throws IOException {
+    assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, posIncrements);
+  }
 
+  
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
+    assertTokenStreamContents(a.tokenStream("dummy", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements);
+  }
+  
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output) throws IOException {
+    assertAnalyzesTo(a, input, output, null, null, null, null);
+  }
+  
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, String[] types) throws IOException {
+    assertAnalyzesTo(a, input, output, null, null, types, null);
+  }
+  
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int[] posIncrements) throws IOException {
+    assertAnalyzesTo(a, input, output, null, null, null, posIncrements);
+  }
+  
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[]) throws IOException {
+    assertAnalyzesTo(a, input, output, startOffsets, endOffsets, null, null);
+  }
+  
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], int[] posIncrements) throws IOException {
+    assertAnalyzesTo(a, input, output, startOffsets, endOffsets, null, posIncrements);
+  }
+  
+
+  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
+    assertTokenStreamContents(a.reusableTokenStream("dummy", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements);
+  }
+  
+  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output) throws IOException {
+    assertAnalyzesToReuse(a, input, output, null, null, null, null);
+  }
+  
+  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, String[] types) throws IOException {
+    assertAnalyzesToReuse(a, input, output, null, null, types, null);
+  }
+  
+  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int[] posIncrements) throws IOException {
+    assertAnalyzesToReuse(a, input, output, null, null, null, posIncrements);
+  }
+  
+  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[]) throws IOException {
+    assertAnalyzesToReuse(a, input, output, startOffsets, endOffsets, null, null);
+  }
+  
+  public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], int[] posIncrements) throws IOException {
+    assertAnalyzesToReuse(a, input, output, startOffsets, endOffsets, null, posIncrements);
+  }
+
+  // simple utility method for testing stemmers
+  
+  public static void checkOneTerm(Analyzer a, final String input, final String expected) throws IOException {
+    assertAnalyzesTo(a, input, new String[]{expected});
+  }
+  
+  public static void checkOneTermReuse(Analyzer a, final String input, final String expected) throws IOException {
+    assertAnalyzesToReuse(a, input, new String[]{expected});
+  }
+  
 }
Index: src/test/org/apache/lucene/analysis/BaseTokenTestCase.java
===================================================================
--- src/test/org/apache/lucene/analysis/BaseTokenTestCase.java	(revision 807118)
+++ src/test/org/apache/lucene/analysis/BaseTokenTestCase.java	(working copy)
@@ -1,157 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.analysis;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-
-import org.apache.lucene.util.AttributeImpl;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-
-public abstract class BaseTokenTestCase extends BaseTokenStreamTestCase {
-
-  public static String tsToString(TokenStream in) throws IOException {
-    final TermAttribute termAtt = (TermAttribute) in.addAttribute(TermAttribute.class);
-    final StringBuffer out = new StringBuffer();
-    in.reset();
-    while (in.incrementToken()) {
-      if (out.length()>0) out.append(' ');
-      out.append(termAtt.term());
-    }
-    in.close();
-    return out.toString();
-  }
-
-  public void assertTokEqual(List/*<Token>*/ a, List/*<Token>*/ b) {
-    assertTokEq(a,b,false);
-    assertTokEq(b,a,false);
-  }
-
-  public void assertTokEqualOff(List/*<Token>*/ a, List/*<Token>*/ b) {
-    assertTokEq(a,b,true);
-    assertTokEq(b,a,true);
-  }
-
-  private void assertTokEq(List/*<Token>*/ a, List/*<Token>*/ b, boolean checkOff) {
-    int pos=0;
-    for (Iterator iter = a.iterator(); iter.hasNext();) {
-      Token tok = (Token)iter.next();
-      pos += tok.getPositionIncrement();
-      if (!tokAt(b, tok.term(), pos
-              , checkOff ? tok.startOffset() : -1
-              , checkOff ? tok.endOffset() : -1
-              )) 
-      {
-        fail(a + "!=" + b);
-      }
-    }
-  }
-
-  public boolean tokAt(List/*<Token>*/ lst, String val, int tokPos, int startOff, int endOff) {
-    int pos=0;
-    for (Iterator iter = lst.iterator(); iter.hasNext();) {
-      Token tok = (Token)iter.next();
-      pos += tok.getPositionIncrement();
-      if (pos==tokPos && tok.term().equals(val)
-          && (startOff==-1 || tok.startOffset()==startOff)
-          && (endOff  ==-1 || tok.endOffset()  ==endOff  )
-           )
-      {
-        return true;
-      }
-    }
-    return false;
-  }
-
-
-  /***
-   * Return a list of tokens according to a test string format:
-   * a b c  =>  returns List<Token> [a,b,c]
-   * a/b   => tokens a and b share the same spot (b.positionIncrement=0)
-   * a,3/b/c => a,b,c all share same position (a.positionIncrement=3, b.positionIncrement=0, c.positionIncrement=0)
-   * a,1,10,11  => "a" with positionIncrement=1, startOffset=10, endOffset=11
-   */
-  public List/*<Token>*/ tokens(String str) {
-    String[] arr = str.split(" ");
-    List/*<Token>*/ result = new ArrayList/*<Token>*/();
-    for (int i=0; i<arr.length; i++) {
-      String[] toks = arr[i].split("/");
-      String[] params = toks[0].split(",");
-
-      int posInc;
-      int start;
-      int end;
-
-      if (params.length > 1) {
-        posInc = Integer.parseInt(params[1]);
-      } else {
-        posInc = 1;
-      }
-
-      if (params.length > 2) {
-        start = Integer.parseInt(params[2]);
-      } else {
-        start = 0;
-      }
-
-      if (params.length > 3) {
-        end = Integer.parseInt(params[3]);
-      } else {
-        end = start + params[0].length();
-      }
-
-      Token t = new Token(params[0],start,end,"TEST");
-      t.setPositionIncrement(posInc);
-      
-      result.add(t);
-      for (int j=1; j<toks.length; j++) {
-        t = new Token(toks[j],0,0,"TEST");
-        t.setPositionIncrement(0);
-        result.add(t);
-      }
-    }
-    return result;
-  }
-
-  //------------------------------------------------------------------------
-  // These may be useful beyond test cases...
-  //------------------------------------------------------------------------
-
-  static List/*<Token>*/ getTokens(TokenStream tstream) throws IOException {
-    List/*<Token>*/ tokens = new ArrayList/*<Token>*/();
-    tstream.reset();
-    while (tstream.incrementToken()) {
-      final Token t = new Token();
-      for (Iterator it = tstream.getAttributeImplsIterator(); it.hasNext();) {
-        final AttributeImpl att = (AttributeImpl) it.next();
-        try {
-          att.copyTo(t);
-        } catch (ClassCastException ce) {
-          // ignore Attributes unsupported by Token
-        }
-      }
-      tokens.add(t);
-    }
-    tstream.close();
-    
-    return tokens;
-  }
-
-}
Index: src/test/org/apache/lucene/analysis/TestAnalyzers.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestAnalyzers.java	(revision 807118)
+++ src/test/org/apache/lucene/analysis/TestAnalyzers.java	(working copy)
@@ -33,19 +33,6 @@
       super(name);
    }
 
-  public void assertAnalyzesTo(Analyzer a, 
-                               String input, 
-                               String[] output) throws Exception {
-    TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-    TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-    for (int i=0; i<output.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(termAtt.term(), output[i]);
-    }
-    assertFalse(ts.incrementToken());
-    ts.close();
-  }
-
   public void testSimple() throws Exception {
     Analyzer a = new SimpleAnalyzer();
     assertAnalyzesTo(a, "foo bar FOO BAR", 
Index: src/test/org/apache/lucene/analysis/TestMappingCharFilter.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestMappingCharFilter.java	(revision 807118)
+++ src/test/org/apache/lucene/analysis/TestMappingCharFilter.java	(working copy)
@@ -20,7 +20,7 @@
 import java.io.StringReader;
 import java.util.List;
 
-public class TestMappingCharFilter extends BaseTokenTestCase {
+public class TestMappingCharFilter extends BaseTokenStreamTestCase {
 
   NormalizeCharMap normMap;
 
@@ -59,72 +59,55 @@
   public void testNothingChange() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "x" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "x" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts, new String[]{"x"}, new int[]{0}, new int[]{1});
   }
 
   public void test1to1() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "h" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "i" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts, new String[]{"i"}, new int[]{0}, new int[]{1});
   }
 
   public void test1to2() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "j" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "jj,1,0,1" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts, new String[]{"jj"}, new int[]{0}, new int[]{1});
   }
 
   public void test1to3() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "k" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "kkk,1,0,1" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts, new String[]{"kkk"}, new int[]{0}, new int[]{1});
   }
 
   public void test2to4() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "ll" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "llll,1,0,2" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts, new String[]{"llll"}, new int[]{0}, new int[]{2});
   }
 
   public void test2to1() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "aa" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "a,1,0,2" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts, new String[]{"a"}, new int[]{0}, new int[]{2});
   }
 
   public void test3to1() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "bbb" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "b,1,0,3" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts, new String[]{"b"}, new int[]{0}, new int[]{3});
   }
 
   public void test4to2() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "cccc" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "cc,1,0,4" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts, new String[]{"cc"}, new int[]{0}, new int[]{4});
   }
 
   public void test5to0() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "empty" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    assertEquals( 0, real.size() );
+    assertTokenStreamContents(ts, new String[0]);
   }
 
   //
@@ -148,9 +131,11 @@
   public void testTokenStream() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "h i j k ll cccc bbb aa" ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "i,1,0,1 i,1,2,3 jj,1,4,5 kkk,1,6,7 llll,1,8,10 cc,1,11,15 b,1,16,19 a,1,20,22" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts,
+      new String[]{"i","i","jj","kkk","llll","cc","b","a"},
+      new int[]{0,2,4,6,8,11,16,20},
+      new int[]{1,3,5,7,10,15,19,22}
+    );
   }
 
   //
@@ -167,8 +152,10 @@
     CharStream cs = new MappingCharFilter( normMap,
         new MappingCharFilter( normMap, CharReader.get( new StringReader( "aaaa ll h" ) ) ) );
     TokenStream ts = new WhitespaceTokenizer( cs );
-    List real = getTokens( ts );
-    List expect = tokens( "a,1,0,4 llllllll,1,5,7 i,1,8,9" );
-    assertTokEqualOff( expect, real );
+    assertTokenStreamContents(ts,
+      new String[]{"a","llllllll","i"},
+      new int[]{0,5,8},
+      new int[]{4,7,9}
+    );
   }
 }
Index: src/test/org/apache/lucene/analysis/TestStandardAnalyzer.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestStandardAnalyzer.java	(revision 807118)
+++ src/test/org/apache/lucene/analysis/TestStandardAnalyzer.java	(working copy)
@@ -28,39 +28,6 @@
 
   private Analyzer a = new StandardAnalyzer();
 
-  public void assertAnalyzesTo(Analyzer a, String input, String[] expected) throws Exception {
-    assertAnalyzesTo(a, input, expected, null);
-  }
-
-  public void assertAnalyzesTo(Analyzer a, String input, String[] expectedImages, String[] expectedTypes) throws Exception {
-    assertAnalyzesTo(a, input, expectedImages, expectedTypes, null);
-  }
-
-  public void assertAnalyzesTo(Analyzer a, String input, String[] expectedImages, String[] expectedTypes, int[] expectedPosIncrs) throws Exception {
-    TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-    // TODO Java 1.5
-    //final TypeAttribute typeAtt = reusableToken.getAttribute(TypeAttribute.class);
-    //final PositionIncrementAttribute posIncrAtt = reusableToken.getAttribute(PositionIncrementAttribute.class);
-
-    final TermAttribute termAtt = (TermAttribute) ts.getAttribute(TermAttribute.class);
-    final TypeAttribute typeAtt = (TypeAttribute) ts.getAttribute(TypeAttribute.class);
-    final PositionIncrementAttribute posIncrAtt = (PositionIncrementAttribute) ts.getAttribute(PositionIncrementAttribute.class);
-    
-    for (int i = 0; i < expectedImages.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(expectedImages[i], new String(termAtt.termBuffer(), 0, termAtt.termLength()));
-      if (expectedTypes != null) {
-        assertEquals(expectedTypes[i], typeAtt.type());
-      }
-      if (expectedPosIncrs != null) {
-        assertEquals(expectedPosIncrs[i], posIncrAtt.getPositionIncrement());
-      }
-    }
-    assertFalse(ts.incrementToken());
-    ts.close();
-  }
-
-
   public void testMaxTermLength() throws Exception {
     StandardAnalyzer sa = new StandardAnalyzer();
     sa.setMaxTokenLength(5);
@@ -72,7 +39,7 @@
     assertAnalyzesTo(sa, "ab cd toolong xy z", new String[]{"ab", "cd", "toolong", "xy", "z"});
     sa.setMaxTokenLength(5);
     
-    assertAnalyzesTo(sa, "ab cd toolong xy z", new String[]{"ab", "cd", "xy", "z"}, null, new int[]{1, 1, 2, 1});
+    assertAnalyzesTo(sa, "ab cd toolong xy z", new String[]{"ab", "cd", "xy", "z"}, new int[]{1, 1, 2, 1});
   }
 
   public void testMaxTermLength3() throws Exception {
Index: src/test/org/apache/lucene/analysis/TestToken.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestToken.java	(revision 807118)
+++ src/test/org/apache/lucene/analysis/TestToken.java	(working copy)
@@ -185,11 +185,16 @@
   }
   
   public void testCopyTo() throws Exception {
-    Token t = new Token(0, 5);
+    Token t = new Token();
+    Token copy = (Token) TestSimpleAttributeImpls.assertCopyIsEqual(t);
+    assertEquals("", t.term());
+    assertEquals("", copy.term());
+
+    t = new Token(0, 5);
     char[] content = "hello".toCharArray();
     t.setTermBuffer(content, 0, 5);
     char[] buf = t.termBuffer();
-    Token copy = (Token) TestSimpleAttributeImpls.assertCopyIsEqual(t);
+    copy = (Token) TestSimpleAttributeImpls.assertCopyIsEqual(t);
     assertEquals(t.term(), copy.term());
     assertNotSame(buf, copy.termBuffer());
 
Index: src/test/org/apache/lucene/analysis/tokenattributes/TestTermAttributeImpl.java
===================================================================
--- src/test/org/apache/lucene/analysis/tokenattributes/TestTermAttributeImpl.java	(revision 807118)
+++ src/test/org/apache/lucene/analysis/tokenattributes/TestTermAttributeImpl.java	(working copy)
@@ -148,10 +148,15 @@
   
   public void testCopyTo() throws Exception {
     TermAttributeImpl t = new TermAttributeImpl();
+    TermAttributeImpl copy = (TermAttributeImpl) TestSimpleAttributeImpls.assertCopyIsEqual(t);
+    assertEquals("", t.term());
+    assertEquals("", copy.term());
+
+    t = new TermAttributeImpl();
     char[] content = "hello".toCharArray();
     t.setTermBuffer(content, 0, 5);
     char[] buf = t.termBuffer();
-    TermAttributeImpl copy = (TermAttributeImpl) TestSimpleAttributeImpls.assertCopyIsEqual(t);
+    copy = (TermAttributeImpl) TestSimpleAttributeImpls.assertCopyIsEqual(t);
     assertEquals(t.term(), copy.term());
     assertNotSame(buf, copy.termBuffer());
   }
Index: src/test/org/apache/lucene/util/LuceneTestCase.java
===================================================================
--- src/test/org/apache/lucene/util/LuceneTestCase.java	(revision 807118)
+++ src/test/org/apache/lucene/util/LuceneTestCase.java	(working copy)
@@ -50,6 +50,8 @@
  */
 public abstract class LuceneTestCase extends TestCase {
 
+  private boolean savedAPISetting = false;
+
   public LuceneTestCase() {
     super();
   }
@@ -61,6 +63,8 @@
   protected void setUp() throws Exception {
     super.setUp();
     ConcurrentMergeScheduler.setTestMode();
+    
+    savedAPISetting = TokenStream.getOnlyUseNewAPI();
     TokenStream.setOnlyUseNewAPI(false);
   }
 
@@ -99,6 +103,8 @@
     } finally {
       purgeFieldCache(FieldCache.DEFAULT);
     }
+    
+    TokenStream.setOnlyUseNewAPI(savedAPISetting);
     super.tearDown();
   }
 
