Index: src/test/org/apache/lucene/search/TestTermRangeQuery.java
===================================================================
--- src/test/org/apache/lucene/search/TestTermRangeQuery.java	(revision 822139)
+++ src/test/org/apache/lucene/search/TestTermRangeQuery.java	(working copy)
@@ -319,26 +319,26 @@
                                  false, false);
     initializeIndex(new String[] {"A", "B", "", "C", "D"}, analyzer);
     IndexSearcher searcher = new IndexSearcher(dir, true);
-    Hits hits = searcher.search(query);
+    int numHits = searcher.search(query, null, 1000).totalHits;
     // When Lucene-38 is fixed, use the assert on the next line:
-    assertEquals("A,B,<empty string>,C,D => A, B & <empty string> are in range", 3, hits.length());
+    assertEquals("A,B,<empty string>,C,D => A, B & <empty string> are in range", 3, numHits);
     // until Lucene-38 is fixed, use this assert:
     //assertEquals("A,B,<empty string>,C,D => A, B & <empty string> are in range", 2, hits.length());
 
     searcher.close();
     initializeIndex(new String[] {"A", "B", "", "D"}, analyzer);
     searcher = new IndexSearcher(dir, true);
-    hits = searcher.search(query);
+    numHits = searcher.search(query, null, 1000).totalHits;
     // When Lucene-38 is fixed, use the assert on the next line:
-    assertEquals("A,B,<empty string>,D => A, B & <empty string> are in range", 3, hits.length());
+    assertEquals("A,B,<empty string>,D => A, B & <empty string> are in range", 3, numHits);
     // until Lucene-38 is fixed, use this assert:
     //assertEquals("A,B,<empty string>,D => A, B & <empty string> are in range", 2, hits.length());
     searcher.close();
     addDoc("C");
     searcher = new IndexSearcher(dir, true);
-    hits = searcher.search(query);
+    numHits = searcher.search(query, null, 1000).totalHits;
     // When Lucene-38 is fixed, use the assert on the next line:
-    assertEquals("C added, still A, B & <empty string> are in range", 3, hits.length());
+    assertEquals("C added, still A, B & <empty string> are in range", 3, numHits);
     // until Lucene-38 is fixed, use this assert
     //assertEquals("C added, still A, B & <empty string> are in range", 2, hits.length());
     searcher.close();
@@ -351,25 +351,25 @@
     Query query = new TermRangeQuery("content", null, "C", true, true);
     initializeIndex(new String[]{"A", "B", "","C", "D"}, analyzer);
     IndexSearcher searcher = new IndexSearcher(dir, true);
-    Hits hits = searcher.search(query);
+    int numHits = searcher.search(query, null, 1000).totalHits;
     // When Lucene-38 is fixed, use the assert on the next line:
-    assertEquals("A,B,<empty string>,C,D => A,B,<empty string>,C in range", 4, hits.length());
+    assertEquals("A,B,<empty string>,C,D => A,B,<empty string>,C in range", 4, numHits);
     // until Lucene-38 is fixed, use this assert
     //assertEquals("A,B,<empty string>,C,D => A,B,<empty string>,C in range", 3, hits.length());
     searcher.close();
     initializeIndex(new String[]{"A", "B", "", "D"}, analyzer);
     searcher = new IndexSearcher(dir, true);
-    hits = searcher.search(query);
+    numHits = searcher.search(query, null, 1000).totalHits;
     // When Lucene-38 is fixed, use the assert on the next line:
-    assertEquals("A,B,<empty string>,D - A, B and <empty string> in range", 3, hits.length());
+    assertEquals("A,B,<empty string>,D - A, B and <empty string> in range", 3, numHits);
     // until Lucene-38 is fixed, use this assert
     //assertEquals("A,B,<empty string>,D => A, B and <empty string> in range", 2, hits.length());
     searcher.close();
     addDoc("C");
     searcher = new IndexSearcher(dir, true);
-    hits = searcher.search(query);
+    numHits = searcher.search(query, null, 1000).totalHits;
     // When Lucene-38 is fixed, use the assert on the next line:
-    assertEquals("C added => A,B,<empty string>,C in range", 4, hits.length());
+    assertEquals("C added => A,B,<empty string>,C in range", 4, numHits);
     // until Lucene-38 is fixed, use this assert
     //assertEquals("C added => A,B,<empty string>,C in range", 3, hits.length());
      searcher.close();
Index: src/test/org/apache/lucene/search/TestTermRangeFilter.java
===================================================================
--- src/test/org/apache/lucene/search/TestTermRangeFilter.java	(revision 822139)
+++ src/test/org/apache/lucene/search/TestTermRangeFilter.java	(working copy)
@@ -146,70 +146,68 @@
 
         assertEquals("num of docs", numDocs, 1+ maxId - minId);
 
-        Hits result;
         Query q = new TermQuery(new Term("body","body"));
 
         // test id, bounded on both ends
+        int numHits = search.search(q,new TermRangeFilter("id",minIP,maxIP,T,T,c), 1000).totalHits;
+        assertEquals("find all", numDocs, numHits);
 
-        result = search.search(q,new TermRangeFilter("id",minIP,maxIP,T,T,c));
-        assertEquals("find all", numDocs, result.length());
+        numHits = search.search(q,new TermRangeFilter("id",minIP,maxIP,T,F,c), 1000).totalHits;
+        assertEquals("all but last", numDocs-1, numHits);
 
-        result = search.search(q,new TermRangeFilter("id",minIP,maxIP,T,F,c));
-        assertEquals("all but last", numDocs-1, result.length());
+        numHits = search.search(q,new TermRangeFilter("id",minIP,maxIP,F,T,c), 1000).totalHits;
+        assertEquals("all but first", numDocs-1, numHits);
 
-        result = search.search(q,new TermRangeFilter("id",minIP,maxIP,F,T,c));
-        assertEquals("all but first", numDocs-1, result.length());
+        numHits = search.search(q,new TermRangeFilter("id",minIP,maxIP,F,F,c), 1000).totalHits;
+        assertEquals("all but ends", numDocs-2, numHits);
 
-        result = search.search(q,new TermRangeFilter("id",minIP,maxIP,F,F,c));
-        assertEquals("all but ends", numDocs-2, result.length());
+        numHits = search.search(q,new TermRangeFilter("id",medIP,maxIP,T,T,c), 1000).totalHits;
+        assertEquals("med and up", 1+ maxId-medId, numHits);
 
-        result = search.search(q,new TermRangeFilter("id",medIP,maxIP,T,T,c));
-        assertEquals("med and up", 1+ maxId-medId, result.length());
+        numHits = search.search(q,new TermRangeFilter("id",minIP,medIP,T,T,c), 1000).totalHits;
+        assertEquals("up to med", 1+ medId-minId, numHits);
 
-        result = search.search(q,new TermRangeFilter("id",minIP,medIP,T,T,c));
-        assertEquals("up to med", 1+ medId-minId, result.length());
-
         // unbounded id
 
-        result = search.search(q,new TermRangeFilter("id",minIP,null,T,F,c));
-        assertEquals("min and up", numDocs, result.length());
+        numHits = search.search(q,new TermRangeFilter("id",minIP,null,T,F,c), 1000).totalHits;
+        assertEquals("min and up", numDocs, numHits);
 
-        result = search.search(q,new TermRangeFilter("id",null,maxIP,F,T,c));
-        assertEquals("max and down", numDocs, result.length());
+        numHits = search.search(q,new TermRangeFilter("id",null,maxIP,F,T,c), 1000).totalHits;
+        assertEquals("max and down", numDocs, numHits);
 
-        result = search.search(q,new TermRangeFilter("id",minIP,null,F,F,c));
-        assertEquals("not min, but up", numDocs-1, result.length());
+        numHits = search.search(q,new TermRangeFilter("id",minIP,null,F,F,c), 1000).totalHits;
+        assertEquals("not min, but up", numDocs-1, numHits);
 
-        result = search.search(q,new TermRangeFilter("id",null,maxIP,F,F,c));
-        assertEquals("not max, but down", numDocs-1, result.length());
+        numHits = search.search(q,new TermRangeFilter("id",null,maxIP,F,F,c), 1000).totalHits;
+        assertEquals("not max, but down", numDocs-1, numHits);
 
-        result = search.search(q,new TermRangeFilter("id",medIP,maxIP,T,F,c));
-        assertEquals("med and up, not max", maxId-medId, result.length());
+        numHits = search.search(q,new TermRangeFilter("id",medIP,maxIP,T,F,c), 1000).totalHits;
+        assertEquals("med and up, not max", maxId-medId, numHits);
 
-        result = search.search(q,new TermRangeFilter("id",minIP,medIP,F,T,c));
-        assertEquals("not min, up to med", medId-minId, result.length());
+        numHits = search.search(q,new TermRangeFilter("id",minIP,medIP,F,T,c), 1000).totalHits;
+        assertEquals("not min, up to med", medId-minId, numHits);
 
         // very small sets
 
-        result = search.search(q,new TermRangeFilter("id",minIP,minIP,F,F,c));
-        assertEquals("min,min,F,F", 0, result.length());
-        result = search.search(q,new TermRangeFilter("id",medIP,medIP,F,F,c));
-        assertEquals("med,med,F,F", 0, result.length());
-        result = search.search(q,new TermRangeFilter("id",maxIP,maxIP,F,F,c));
-        assertEquals("max,max,F,F", 0, result.length());
+        numHits = search.search(q,new TermRangeFilter("id",minIP,minIP,F,F,c), 1000).totalHits;
+        assertEquals("min,min,F,F", 0, numHits);
+        numHits = search.search(q,new TermRangeFilter("id",medIP,medIP,F,F,c), 1000).totalHits;
+        assertEquals("med,med,F,F", 0, numHits);
+        numHits = search.search(q,new TermRangeFilter("id",maxIP,maxIP,F,F,c), 1000).totalHits;
+        assertEquals("max,max,F,F", 0, numHits);
 
-        result = search.search(q,new TermRangeFilter("id",minIP,minIP,T,T,c));
-        assertEquals("min,min,T,T", 1, result.length());
-        result = search.search(q,new TermRangeFilter("id",null,minIP,F,T,c));
-        assertEquals("nul,min,F,T", 1, result.length());
+        numHits = search.search(q,new TermRangeFilter("id",minIP,minIP,T,T,c), 1000).totalHits;
+        assertEquals("min,min,T,T", 1, numHits);
+        numHits = search.search(q,new TermRangeFilter("id",null,minIP,F,T,c), 1000).totalHits;
+        assertEquals("nul,min,F,T", 1, numHits);
 
-        result = search.search(q,new TermRangeFilter("id",maxIP,maxIP,T,T,c));
-        assertEquals("max,max,T,T", 1, result.length());
-        result = search.search(q,new TermRangeFilter("id",maxIP,null,T,F,c));
-        assertEquals("max,nul,T,T", 1, result.length());
+        numHits = search.search(q,new TermRangeFilter("id",maxIP,maxIP,T,T,c), 1000).totalHits;
+        assertEquals("max,max,T,T", 1, numHits);
+        numHits = search.search(q,new TermRangeFilter("id",maxIP,null,T,F,c), 1000).totalHits;
+        assertEquals("max,nul,T,T", 1, numHits);
 
-        result = search.search(q,new TermRangeFilter("id",medIP,medIP,T,T,c));
-        assertEquals("med,med,T,T", 1, result.length());
+        numHits = search.search(q,new TermRangeFilter("id",medIP,medIP,T,T,c), 1000).totalHits;
+        assertEquals("med,med,T,T", 1, numHits);
     }
 
     public void testRangeFilterRand() throws IOException {
@@ -289,53 +287,52 @@
 
         assertEquals("num of docs", numDocs, 1+ maxId - minId);
 
-        Hits result;
         Query q = new TermQuery(new Term("body","body"));
 
         // test extremes, bounded on both ends
 
-        result = search.search(q,new TermRangeFilter("rand",minRP,maxRP,T,T,c));
-        assertEquals("find all", numDocs, result.length());
+        int numHits = search.search(q,new TermRangeFilter("rand",minRP,maxRP,T,T,c), 1000).totalHits;
+        assertEquals("find all", numDocs, numHits);
 
-        result = search.search(q,new TermRangeFilter("rand",minRP,maxRP,T,F,c));
-        assertEquals("all but biggest", numDocs-1, result.length());
+        numHits = search.search(q,new TermRangeFilter("rand",minRP,maxRP,T,F,c), 1000).totalHits;
+        assertEquals("all but biggest", numDocs-1, numHits);
 
-        result = search.search(q,new TermRangeFilter("rand",minRP,maxRP,F,T,c));
-        assertEquals("all but smallest", numDocs-1, result.length());
+        numHits = search.search(q,new TermRangeFilter("rand",minRP,maxRP,F,T,c), 1000).totalHits;
+        assertEquals("all but smallest", numDocs-1, numHits);
 
-        result = search.search(q,new TermRangeFilter("rand",minRP,maxRP,F,F,c));
-        assertEquals("all but extremes", numDocs-2, result.length());
+        numHits = search.search(q,new TermRangeFilter("rand",minRP,maxRP,F,F,c), 1000).totalHits;
+        assertEquals("all but extremes", numDocs-2, numHits);
 
         // unbounded
 
-        result = search.search(q,new TermRangeFilter("rand",minRP,null,T,F,c));
-        assertEquals("smallest and up", numDocs, result.length());
+        numHits = search.search(q,new TermRangeFilter("rand",minRP,null,T,F,c), 1000).totalHits;
+        assertEquals("smallest and up", numDocs, numHits);
 
-        result = search.search(q,new TermRangeFilter("rand",null,maxRP,F,T,c));
-        assertEquals("biggest and down", numDocs, result.length());
+        numHits = search.search(q,new TermRangeFilter("rand",null,maxRP,F,T,c), 1000).totalHits;
+        assertEquals("biggest and down", numDocs, numHits);
 
-        result = search.search(q,new TermRangeFilter("rand",minRP,null,F,F,c));
-        assertEquals("not smallest, but up", numDocs-1, result.length());
+        numHits = search.search(q,new TermRangeFilter("rand",minRP,null,F,F,c), 1000).totalHits;
+        assertEquals("not smallest, but up", numDocs-1, numHits);
 
-        result = search.search(q,new TermRangeFilter("rand",null,maxRP,F,F,c));
-        assertEquals("not biggest, but down", numDocs-1, result.length());
+        numHits = search.search(q,new TermRangeFilter("rand",null,maxRP,F,F,c), 1000).totalHits;
+        assertEquals("not biggest, but down", numDocs-1, numHits);
 
         // very small sets
 
-        result = search.search(q,new TermRangeFilter("rand",minRP,minRP,F,F,c));
-        assertEquals("min,min,F,F", 0, result.length());
-        result = search.search(q,new TermRangeFilter("rand",maxRP,maxRP,F,F,c));
-        assertEquals("max,max,F,F", 0, result.length());
+        numHits = search.search(q,new TermRangeFilter("rand",minRP,minRP,F,F,c), 1000).totalHits;
+        assertEquals("min,min,F,F", 0, numHits);
+        numHits = search.search(q,new TermRangeFilter("rand",maxRP,maxRP,F,F,c), 1000).totalHits;
+        assertEquals("max,max,F,F", 0, numHits);
 
-        result = search.search(q,new TermRangeFilter("rand",minRP,minRP,T,T,c));
-        assertEquals("min,min,T,T", 1, result.length());
-        result = search.search(q,new TermRangeFilter("rand",null,minRP,F,T,c));
-        assertEquals("nul,min,F,T", 1, result.length());
+        numHits = search.search(q,new TermRangeFilter("rand",minRP,minRP,T,T,c), 1000).totalHits;
+        assertEquals("min,min,T,T", 1, numHits);
+        numHits = search.search(q,new TermRangeFilter("rand",null,minRP,F,T,c), 1000).totalHits;
+        assertEquals("nul,min,F,T", 1, numHits);
 
-        result = search.search(q,new TermRangeFilter("rand",maxRP,maxRP,T,T,c));
-        assertEquals("max,max,T,T", 1, result.length());
-        result = search.search(q,new TermRangeFilter("rand",maxRP,null,T,F,c));
-        assertEquals("max,nul,T,T", 1, result.length());
+        numHits = search.search(q,new TermRangeFilter("rand",maxRP,maxRP,T,T,c), 1000).totalHits;
+        assertEquals("max,max,T,T", 1, numHits);
+        numHits = search.search(q,new TermRangeFilter("rand",maxRP,null,T,F,c), 1000).totalHits;
+        assertEquals("max,nul,T,T", 1, numHits);
     }
     
     public void testFarsi() throws Exception {
@@ -367,13 +364,13 @@
         // orders the U+0698 character before the U+0633 character, so the single
         // index Term below should NOT be returned by a TermRangeFilter with a Farsi
         // Collator (or an Arabic one for the case when Farsi is not supported).
-        Hits result = search.search
-            (q, new TermRangeFilter("content", "\u062F", "\u0698", T, T, collator));
-        assertEquals("The index Term should not be included.", 0, result.length());
+        int numHits = search.search
+            (q, new TermRangeFilter("content", "\u062F", "\u0698", T, T, collator), 1000).totalHits;
+        assertEquals("The index Term should not be included.", 0, numHits);
 
-        result = search.search
-            (q, new TermRangeFilter("content", "\u0633", "\u0638", T, T, collator));
-        assertEquals("The index Term should be included.", 1, result.length());
+        numHits = search.search
+            (q, new TermRangeFilter("content", "\u0633", "\u0638", T, T, collator), 1000).totalHits;
+        assertEquals("The index Term should be included.", 1, numHits);
         search.close();
     }
 
@@ -408,14 +405,14 @@
 
         // Unicode order would not include "H\u00C5T" in [ "H\u00D8T", "MAND" ],
         // but Danish collation does.
-        Hits result = search.search
-            (q, new TermRangeFilter("content", "H\u00D8T", "MAND", F, F, collator));
-        assertEquals("The index Term should be included.", 1, result.length());
+        int numHits = search.search
+            (q, new TermRangeFilter("content", "H\u00D8T", "MAND", F, F, collator), 1000).totalHits;
+        assertEquals("The index Term should be included.", 1, numHits);
 
-        result = search.search
-            (q, new TermRangeFilter("content", "H\u00C5T", "MAND", F, F, collator));
+        numHits = search.search
+            (q, new TermRangeFilter("content", "H\u00C5T", "MAND", F, F, collator), 1000).totalHits;
         assertEquals
-            ("The index Term should not be included.", 0, result.length());
+            ("The index Term should not be included.", 0, numHits);
         search.close();
     }
 }
Index: src/java/org/apache/lucene/search/Searcher.java
===================================================================
--- src/java/org/apache/lucene/search/Searcher.java	(revision 822139)
+++ src/java/org/apache/lucene/search/Searcher.java	(working copy)
@@ -32,48 +32,6 @@
  * closed, otherwise an IOException will be thrown.
  */
 public abstract class Searcher implements Searchable {
-
-  /** Returns the documents matching <code>query</code>. 
-   * @throws BooleanQuery.TooManyClauses
-   * @deprecated Hits will be removed in Lucene 3.0. Use
-   * {@link #search(Query, Filter, int)} instead.
-   */
-  public final Hits search(Query query) throws IOException {
-    return search(query, (Filter)null);
-  }
-
-  /** Returns the documents matching <code>query</code> and
-   * <code>filter</code>.
-   * @throws BooleanQuery.TooManyClauses
-   * @deprecated Hits will be removed in Lucene 3.0. Use
-   * {@link #search(Query, Filter, int)} instead.
-   */
-  public Hits search(Query query, Filter filter) throws IOException {
-    return new Hits(this, query, filter);
-  }
-
-  /** Returns documents matching <code>query</code> sorted by
-   * <code>sort</code>.
-   * @throws BooleanQuery.TooManyClauses
-   * @deprecated Hits will be removed in Lucene 3.0. Use 
-   * {@link #search(Query, Filter, int, Sort)} instead.
-   */
-  public Hits search(Query query, Sort sort)
-    throws IOException {
-    return new Hits(this, query, null, sort);
-  }
-
-  /** Returns documents matching <code>query</code> and <code>filter</code>,
-   * sorted by <code>sort</code>.
-   * @throws BooleanQuery.TooManyClauses
-   * @deprecated Hits will be removed in Lucene 3.0. Use 
-   * {@link #search(Query, Filter, int, Sort)} instead.
-   */
-  public Hits search(Query query, Filter filter, Sort sort)
-    throws IOException {
-    return new Hits(this, query, filter, sort);
-  }
-
   /** Search implementation with arbitrary sorting.  Finds
    * the top <code>n</code> hits for <code>query</code>, applying
    * <code>filter</code> if non-null, and sorting the hits by the criteria in
Index: contrib/wordnet/src/java/org/apache/lucene/wordnet/SynLookup.java
===================================================================
--- contrib/wordnet/src/java/org/apache/lucene/wordnet/SynLookup.java	(revision 822139)
+++ contrib/wordnet/src/java/org/apache/lucene/wordnet/SynLookup.java	(working copy)
@@ -17,9 +17,9 @@
  * limitations under the License.
  */
 
+import java.io.File;
 import java.io.IOException;
 import java.io.StringReader;
-import java.io.File;
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.LinkedList;
@@ -30,12 +30,15 @@
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.document.Document;
+import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Hits;
+import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.Searcher;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.store.FSDirectory;
@@ -46,6 +49,20 @@
  */
 public class SynLookup {
 
+  final static class CountingCollector extends Collector {
+    public int numHits = 0;
+    
+    public void setScorer(Scorer scorer) throws IOException {}
+    public void collect(int doc) throws IOException {
+      numHits++;
+    }
+
+    public void setNextReader(IndexReader reader, int docBase) {}
+    public boolean acceptsDocsOutOfOrder() {
+      return true;
+    }    
+  }
+  
 	public static void main(String[] args) throws IOException {
 		if (args.length != 2) {
 			System.out.println(
@@ -56,17 +73,20 @@
 		IndexSearcher searcher = new IndexSearcher(directory, true);
 
 		String word = args[1];
-		Hits hits = searcher.search(
-									new TermQuery(new Term(Syns2Index.F_WORD, word)));
+		Query query = new TermQuery(new Term(Syns2Index.F_WORD, word));
+		CountingCollector countingCollector = new CountingCollector();
+		searcher.search(query, countingCollector);
 
-		if (hits.length() == 0) {
+		if (countingCollector.numHits == 0) {
 			System.out.println("No synonyms found for " + word);
 		} else {
 			System.out.println("Synonyms found for \"" + word + "\":");
 		}
 
-		for (int i = 0; i < hits.length(); i++) {
-			Document doc = hits.doc(i);
+		ScoreDoc[] hits = searcher.search(query, countingCollector.numHits).scoreDocs;
+		
+		for (int i = 0; i < hits.length; i++) {
+			Document doc = searcher.doc(hits[i].doc);
 
 			String[] values = doc.getValues(Syns2Index.F_SYN);
 
@@ -92,11 +112,11 @@
 	public static Query expand( String query,
 								Searcher syns,
 								Analyzer a,
-								String field,
-								float boost)
+								final String field,
+								final float boost)
 		throws IOException
 	{
-		Set already = new HashSet(); // avoid dups		
+		final Set already = new HashSet(); // avoid dups		
 		List top = new LinkedList(); // needs to be separately listed..
 
 		// [1] Parse query into separate words so that when we expand we can avoid dups
@@ -108,7 +128,7 @@
 			if ( already.add( word))
 				top.add( word);
 		}
-		BooleanQuery tmp = new BooleanQuery();
+		final BooleanQuery tmp = new BooleanQuery();
 		
 		// [2] form query
 		Iterator it = top.iterator();
@@ -120,23 +140,40 @@
 			tmp.add( tq, BooleanClause.Occur.SHOULD);
 
 			// [2b] add in unique synonums
-			Hits hits = syns.search( new TermQuery( new Term(Syns2Index.F_WORD, word)));
-			for (int i = 0; i < hits.length(); i++)
-			{
-				Document doc = hits.doc(i);
-				String[] values = doc.getValues( Syns2Index.F_SYN);
-				for ( int j = 0; j < values.length; j++)
-				{
-					String syn = values[ j];
-					if ( already.add( syn))
-					{
-						tq = new TermQuery( new Term( field, syn));
-						if ( boost > 0) // else keep normal 1.0
-							tq.setBoost( boost);
-						tmp.add( tq, BooleanClause.Occur.SHOULD); 
-					}
-				}
-			}
+			syns.search(new TermQuery( new Term(Syns2Index.F_WORD, word)), new Collector() {
+			  IndexReader reader;
+			  
+        @Override
+        public boolean acceptsDocsOutOfOrder() {
+          return true;
+        }
+
+        @Override
+        public void collect(int doc) throws IOException {
+          Document d = reader.document(doc);
+          String[] values = d.getValues( Syns2Index.F_SYN);
+          for ( int j = 0; j < values.length; j++)
+          {
+            String syn = values[ j];
+            if ( already.add( syn))
+            {
+              TermQuery tq = new TermQuery( new Term( field, syn));
+              if ( boost > 0) // else keep normal 1.0
+                tq.setBoost( boost);
+              tmp.add( tq, BooleanClause.Occur.SHOULD); 
+            }
+          }
+        }
+
+        @Override
+        public void setNextReader(IndexReader reader, int docBase)
+            throws IOException {
+          this.reader = reader;
+        }
+
+        @Override
+        public void setScorer(Scorer scorer) throws IOException {}
+			});
 		}
 
 
Index: contrib/wordnet/src/java/org/apache/lucene/wordnet/SynExpand.java
===================================================================
--- contrib/wordnet/src/java/org/apache/lucene/wordnet/SynExpand.java	(revision 822139)
+++ contrib/wordnet/src/java/org/apache/lucene/wordnet/SynExpand.java	(working copy)
@@ -17,9 +17,9 @@
  * limitations under the License.
  */
 
+import java.io.File;
 import java.io.IOException;
 import java.io.StringReader;
-import java.io.File;
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.LinkedList;
@@ -31,12 +31,14 @@
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.document.Document;
+import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Hits;
+import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.Searcher;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.store.FSDirectory;
@@ -103,13 +105,13 @@
 	public static Query expand( String query,
 								Searcher syns,
 								Analyzer a,
-								String field,
-								float boost)
+								String f,
+								final float boost)
 		throws IOException
 	{
-		Set already = new HashSet(); // avoid dups 
+		final Set already = new HashSet(); // avoid dups 
 		List top = new LinkedList(); // needs to be separately listed..
-		if ( field == null) field = "contents";
+		final String field = ( f == null) ? "contents" : f;
 		if ( a == null) a = new StandardAnalyzer();
 
 		// [1] Parse query into separate words so that when we expand we can avoid dups
@@ -121,7 +123,7 @@
 			if ( already.add( word))
 				top.add( word);
 		}
-		BooleanQuery tmp = new BooleanQuery();
+		final BooleanQuery tmp = new BooleanQuery();
 		
 		// [2] form query
 		Iterator it = top.iterator();
@@ -132,24 +134,42 @@
 			TermQuery tq = new TermQuery( new Term( field, word));
 			tmp.add( tq, BooleanClause.Occur.SHOULD);
 
+			syns.search(new TermQuery( new Term(Syns2Index.F_WORD, word)), new Collector() {
+			  IndexReader reader;
+			  
+        @Override
+        public boolean acceptsDocsOutOfOrder() {
+          return true;
+        }
+
+        @Override
+        public void collect(int doc) throws IOException {
+          Document d = reader.document(doc);
+          String[] values = d.getValues( Syns2Index.F_SYN);
+          for ( int j = 0; j < values.length; j++)
+          {
+            String syn = values[ j];
+            if ( already.add( syn)) // avoid dups of top level words and synonyms
+            {
+              TermQuery tq = new TermQuery( new Term( field, syn));
+              if ( boost > 0) // else keep normal 1.0
+                tq.setBoost( boost);
+              tmp.add( tq, BooleanClause.Occur.SHOULD); 
+            }
+          }
+        }
+
+        @Override
+        public void setNextReader(IndexReader reader, int docBase)
+            throws IOException {
+          this.reader = reader;
+        }
+
+        @Override
+        public void setScorer(Scorer scorer) throws IOException {}
+			});
+			
 			// [2b] add in unique synonums
-			Hits hits = syns.search( new TermQuery( new Term(Syns2Index.F_WORD, word)));
-			for (int i = 0; i < hits.length(); i++)
-			{
-				Document doc = hits.doc(i);
-				String[] values = doc.getValues( Syns2Index.F_SYN);
-				for ( int j = 0; j < values.length; j++)
-				{
-					String syn = values[ j];
-					if ( already.add( syn)) // avoid dups of top level words and synonyms
-					{
-						tq = new TermQuery( new Term( field, syn));
-						if ( boost > 0) // else keep normal 1.0
-							tq.setBoost( boost);
-						tmp.add( tq, BooleanClause.Occur.SHOULD); 
-					}
-				}
-			}
 		}
 
 
Index: contrib/ant/src/test/org/apache/lucene/ant/IndexTaskTest.java
===================================================================
--- contrib/ant/src/test/org/apache/lucene/ant/IndexTaskTest.java	(revision 822139)
+++ contrib/ant/src/test/org/apache/lucene/ant/IndexTaskTest.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.File;
-
 import java.io.IOException;
 
 import junit.framework.TestCase;
@@ -26,12 +25,10 @@
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.StopAnalyzer;
 import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.search.Hits;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Searcher;
 import org.apache.lucene.store.FSDirectory;
-
 import org.apache.tools.ant.Project;
 import org.apache.tools.ant.types.FileSet;
 
@@ -79,9 +76,9 @@
     public void testSearch() throws Exception {
         Query query = new QueryParser("contents",analyzer).parse("test");
 
-        Hits hits = searcher.search(query);
+        int numHits = searcher.search(query, null, 1000).totalHits;
 
-        assertEquals("Find document(s)", 2, hits.length());
+        assertEquals("Find document(s)", 2, numHits);
     }
 
     /**
Index: contrib/ant/src/java/org/apache/lucene/ant/IndexTask.java
===================================================================
--- contrib/ant/src/java/org/apache/lucene/ant/IndexTask.java	(revision 822139)
+++ contrib/ant/src/java/org/apache/lucene/ant/IndexTask.java	(working copy)
@@ -17,44 +17,42 @@
  * limitations under the License.
  */
 
+import java.io.File;
+import java.io.IOException;
+import java.text.ParseException;
+import java.util.Date;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Properties;
+import java.util.Set;
+import java.util.Vector;
+
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.StopAnalyzer;
 import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.analysis.StopAnalyzer;
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.document.DateTools;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.document.DateTools;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.search.Hits;
 import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.Searcher;
 import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.store.FSDirectory;
 import org.apache.tools.ant.BuildException;
-import org.apache.tools.ant.DirectoryScanner;
 import org.apache.tools.ant.DynamicConfigurator;
 import org.apache.tools.ant.Project;
 import org.apache.tools.ant.Task;
-import org.apache.tools.ant.types.FileSet;
 import org.apache.tools.ant.types.EnumeratedAttribute;
+import org.apache.tools.ant.types.FileSet;
 import org.apache.tools.ant.types.Resource;
 import org.apache.tools.ant.types.ResourceCollection;
 import org.apache.tools.ant.types.resources.FileResource;
 
-import java.io.File;
-import java.io.IOException;
-import java.util.Date;
-import java.util.Iterator;
-import java.util.Properties;
-import java.util.Map;
-import java.util.HashMap;
-import java.util.Set;
-import java.util.ArrayList;
-import java.util.Vector;
-import java.text.ParseException;
-
 /**
  *  Ant task to index files with Lucene
  *
@@ -311,14 +309,14 @@
                   new Term("path", file.getPath());
                 TermQuery query =
                   new TermQuery(pathTerm);
-                Hits hits = searcher.search(query);
+                ScoreDoc[] hits = searcher.search(query, null, 1).scoreDocs;
 
                 // if document is found, compare the
                 // indexed last modified time with the
                 // current file
                 // - don't index if up to date
-                if (hits.length() > 0) {
-                  Document doc = hits.doc(0);
+                if (hits.length > 0) {
+                  Document doc = searcher.doc(hits[0].doc);
                   String indexModified =
                     doc.get("modified").trim();
                   if (indexModified != null) {
Index: contrib/swing/src/java/org/apache/lucene/swing/models/TableSearcher.java
===================================================================
--- contrib/swing/src/java/org/apache/lucene/swing/models/TableSearcher.java	(revision 822139)
+++ contrib/swing/src/java/org/apache/lucene/swing/models/TableSearcher.java	(working copy)
@@ -16,6 +16,13 @@
  * limitations under the License.
  */
 
+import java.util.ArrayList;
+
+import javax.swing.event.TableModelEvent;
+import javax.swing.event.TableModelListener;
+import javax.swing.table.AbstractTableModel;
+import javax.swing.table.TableModel;
+
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
@@ -23,18 +30,13 @@
 import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.queryParser.MultiFieldQueryParser;
-import org.apache.lucene.search.Hits;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.swing.models.ListSearcher.CountingCollector;
 
-import javax.swing.event.TableModelEvent;
-import javax.swing.event.TableModelListener;
-import javax.swing.table.AbstractTableModel;
-import javax.swing.table.TableModel;
-import java.util.ArrayList;
 
-
 /**
  * This is a TableModel that encapsulates Lucene
  * search logic within a TableModel implementation.
@@ -244,10 +246,8 @@
             // has some weirdness.
             MultiFieldQueryParser parser = new MultiFieldQueryParser(fields, analyzer);
             Query query = parser.parse(searchString);
-            //run the search
-            Hits hits = is.search(query);
             //reset this table model with the new results
-            resetSearchResults(hits);
+            resetSearchResults(is, query);
         } catch (Exception e){
             e.printStackTrace();
         }
@@ -260,17 +260,22 @@
      *
      * @param hits The new result set to set this table to.
      */
-    private void resetSearchResults(Hits hits) {
+    private void resetSearchResults(IndexSearcher searcher, Query query) {
         try {
             //clear our index mapping this table model rows to
             //the decorated inner table model
             rowToModelIndex.clear();
+            
+            CountingCollector countingCollector = new CountingCollector();
+            searcher.search(query, countingCollector);
+            ScoreDoc[] hits = searcher.search(query, countingCollector.numHits).scoreDocs;
+            
             //iterate through the hits
             //get the row number stored at the index
             //that number is the row number of the decorated
             //table model row that we are mapping to
-            for (int t=0; t<hits.length(); t++){
-                Document document = hits.doc(t);
+            for (int t=0; t<hits.length; t++){
+                Document document = searcher.doc(hits[t].doc);
                 Fieldable field = document.getField(ROW_NUMBER);
                 rowToModelIndex.add(Integer.valueOf(field.stringValue()));
             }
Index: contrib/swing/src/java/org/apache/lucene/swing/models/ListSearcher.java
===================================================================
--- contrib/swing/src/java/org/apache/lucene/swing/models/ListSearcher.java	(revision 822139)
+++ contrib/swing/src/java/org/apache/lucene/swing/models/ListSearcher.java	(working copy)
@@ -16,23 +16,29 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.store.RAMDirectory;
+import java.io.IOException;
+import java.util.ArrayList;
+
+import javax.swing.AbstractListModel;
+import javax.swing.ListModel;
+import javax.swing.event.ListDataEvent;
+import javax.swing.event.ListDataListener;
+
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.Fieldable;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.queryParser.MultiFieldQueryParser;
+import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
-import org.apache.lucene.search.Hits;
-import org.apache.lucene.queryParser.MultiFieldQueryParser;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.store.RAMDirectory;
 
-import javax.swing.*;
-import javax.swing.event.ListDataListener;
-import javax.swing.event.ListDataEvent;
-import java.util.ArrayList;
-
 /**
  * See table searcher explanation.
  *
@@ -163,10 +169,8 @@
             // has some weirdness.
             MultiFieldQueryParser parser = new MultiFieldQueryParser(fields, analyzer);
             Query query =parser.parse(searchString);
-            //run the search
-            Hits hits = is.search(query);
             //reset this list model with the new results
-            resetSearchResults(hits);
+            resetSearchResults(is, query);
         } catch (Exception e){
             e.printStackTrace();
         }
@@ -175,21 +179,41 @@
         fireContentsChanged(this, 0, getSize());
     }
 
+    final static class CountingCollector extends Collector {
+      public int numHits = 0;
+      
+      public void setScorer(Scorer scorer) throws IOException {}
+      public void collect(int doc) throws IOException {
+        numHits++;
+      }
+
+      public void setNextReader(IndexReader reader, int docBase) {}
+      public boolean acceptsDocsOutOfOrder() {
+        return true;
+      }    
+    }
+
+    
     /**
      *
      * @param hits The new result set to set this list to.
      */
-    private void resetSearchResults(Hits hits) {
+    private void resetSearchResults(IndexSearcher searcher, Query query) {
         try {
             //clear our index mapping this list model rows to
             //the decorated inner list model
             rowToModelIndex.clear();
+            
+            CountingCollector countingCollector = new CountingCollector();
+            searcher.search(query, countingCollector);
+            ScoreDoc[] hits = searcher.search(query, countingCollector.numHits).scoreDocs;
+            
             //iterate through the hits
             //get the row number stored at the index
             //that number is the row number of the decorated
             //table model row that we are mapping to
-            for (int t=0; t<hits.length(); t++){
-                Document document = hits.doc(t);
+            for (int t=0; t<hits.length; t++){
+                Document document = searcher.doc(hits[t].doc);
                 Fieldable field = document.getField(ROW_NUMBER);
                 rowToModelIndex.add(Integer.valueOf(field.stringValue()));
             }
Index: contrib/misc/src/test/org/apache/lucene/misc/ChainedFilterTest.java
===================================================================
--- contrib/misc/src/test/org/apache/lucene/misc/ChainedFilterTest.java	(revision 822139)
+++ contrib/misc/src/test/org/apache/lucene/misc/ChainedFilterTest.java	(working copy)
@@ -132,22 +132,24 @@
       
       ChainedFilter chain = getChainedFilter(new Filter[] {dateFilter}, null, old);
   
-      Hits hits = searcher.search(query, chain);
-      assertEquals(MAX, hits.length());
+      int numHits = searcher.search(query, chain, 1000).totalHits;
+      assertEquals(MAX, numHits);
   
       chain = new ChainedFilter(new Filter[] {bobFilter});
-      hits = searcher.search(query, chain);
-      assertEquals(MAX / 2, hits.length());
+      numHits = searcher.search(query, chain, 1000).totalHits;
+      assertEquals(MAX / 2, numHits);
       
       chain = getChainedFilter(new Filter[] {bobFilter}, new int[] {ChainedFilter.AND}, old);
-      hits = searcher.search(query, chain);
-      assertEquals(MAX / 2, hits.length());
-      assertEquals("bob", hits.doc(0).get("owner"));
+      TopDocs hits = searcher.search(query, chain, 1000);
+      numHits = hits.totalHits;
+      assertEquals(MAX / 2, numHits);
+      assertEquals("bob", searcher.doc(hits.scoreDocs[0].doc).get("owner"));
       
       chain = getChainedFilter(new Filter[] {bobFilter}, new int[] {ChainedFilter.ANDNOT}, old);
-      hits = searcher.search(query, chain);
-      assertEquals(MAX / 2, hits.length());
-      assertEquals("sue", hits.doc(0).get("owner"));
+      hits = searcher.search(query, chain, 1000);
+      numHits = hits.totalHits;
+      assertEquals(MAX / 2, numHits);
+      assertEquals("sue", searcher.doc(hits.scoreDocs[0].doc).get("owner"));
     }
   }
 
@@ -157,8 +159,8 @@
       ChainedFilter chain = getChainedFilter(
         new Filter[] {sueFilter, bobFilter}, null, old);
   
-      Hits hits = searcher.search(query, chain);
-      assertEquals("OR matches all", MAX, hits.length());
+      int numHits = searcher.search(query, chain, 1000).totalHits;
+      assertEquals("OR matches all", MAX, numHits);
     }
   }
 
@@ -168,9 +170,9 @@
       ChainedFilter chain = getChainedFilter(
         new Filter[] {dateFilter, bobFilter}, ChainedFilter.AND, old);
   
-      Hits hits = searcher.search(query, chain);
-      assertEquals("AND matches just bob", MAX / 2, hits.length());
-      assertEquals("bob", hits.doc(0).get("owner"));
+      TopDocs hits = searcher.search(query, chain, 1000);
+      assertEquals("AND matches just bob", MAX / 2, hits.totalHits);
+      assertEquals("bob", searcher.doc(hits.scoreDocs[0].doc).get("owner"));
     }
   }
 
@@ -180,9 +182,9 @@
       ChainedFilter chain = getChainedFilter(
         new Filter[]{dateFilter, bobFilter}, ChainedFilter.XOR, old);
   
-      Hits hits = searcher.search(query, chain);
-      assertEquals("XOR matches sue", MAX / 2, hits.length());
-      assertEquals("sue", hits.doc(0).get("owner"));
+      TopDocs hits = searcher.search(query, chain, 1000);
+      assertEquals("XOR matches sue", MAX / 2, hits.totalHits);
+      assertEquals("sue", searcher.doc(hits.scoreDocs[0].doc).get("owner"));
     }
   }
 
@@ -193,19 +195,19 @@
         new Filter[]{dateFilter, sueFilter},
           new int[] {ChainedFilter.AND, ChainedFilter.ANDNOT}, old);
   
-      Hits hits = searcher.search(query, chain);
+      TopDocs hits = searcher.search(query, chain, 1000);
       assertEquals("ANDNOT matches just bob",
-          MAX / 2, hits.length());
-      assertEquals("bob", hits.doc(0).get("owner"));
+          MAX / 2, hits.totalHits);
+      assertEquals("bob", searcher.doc(hits.scoreDocs[0].doc).get("owner"));
       
       chain = getChainedFilter(
           new Filter[]{bobFilter, bobFilter},
             new int[] {ChainedFilter.ANDNOT, ChainedFilter.ANDNOT}, old);
   
-        hits = searcher.search(query, chain);
+        hits = searcher.search(query, chain, 1000);
         assertEquals("ANDNOT bob ANDNOT bob matches all sues",
-            MAX / 2, hits.length());
-        assertEquals("sue", hits.doc(0).get("owner"));
+            MAX / 2, hits.totalHits);
+        assertEquals("sue", searcher.doc(hits.scoreDocs[0].doc).get("owner"));
     }
   }
 
Index: contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestParser.java
===================================================================
--- contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestParser.java	(revision 822139)
+++ contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestParser.java	(working copy)
@@ -1,6 +1,9 @@
 package org.apache.lucene.xmlparser;
 
-import java.io.*;
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
 
 import junit.framework.TestCase;
 
@@ -9,9 +12,10 @@
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.search.Hits;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
 /**
@@ -116,8 +120,8 @@
 	public void testCustomFieldUserQueryXML() throws ParserException, IOException
 	{
 			Query q=parse("UserInputQueryCustomField.xml");
-			Hits h = searcher.search(q);
-			assertEquals("UserInputQueryCustomField should produce 0 result ", 0,h.length());
+			int h = searcher.search(q, null, 1000).totalHits;
+			assertEquals("UserInputQueryCustomField should produce 0 result ", 0,h);
 	}
 	
 	public void testLikeThisQueryXML() throws Exception
@@ -183,8 +187,8 @@
 	public void testDuplicateFilterQueryXML() throws ParserException, IOException
 	{
 			Query q=parse("DuplicateFilterQuery.xml");
-			Hits h = searcher.search(q);
-			assertEquals("DuplicateFilterQuery should produce 1 result ", 1,h.length());
+			int h = searcher.search(q, null, 1000).totalHits;
+			assertEquals("DuplicateFilterQuery should produce 1 result ", 1,h);
 	}
 	
 
@@ -199,14 +203,15 @@
 	}
 	private void dumpResults(String qType,Query q, int numDocs) throws IOException
 	{
-		Hits h = searcher.search(q);
-		assertTrue(qType +" should produce results ", h.length()>0);
+		TopDocs hits = searcher.search(q, null, numDocs);
+		assertTrue(qType +" should produce results ", hits.totalHits>0);
 		if(printResults)
 		{
 			System.out.println("========="+qType+"============");
-			for(int i=0;i<Math.min(numDocs,h.length());i++)
+			ScoreDoc[] scoreDocs = hits.scoreDocs;
+			for(int i=0;i<Math.min(numDocs,hits.totalHits);i++)
 			{
-				org.apache.lucene.document.Document ldoc=h.doc(i);
+				org.apache.lucene.document.Document ldoc=searcher.doc(scoreDocs[i].doc);
 				System.out.println("["+ldoc.get("date")+"]"+ldoc.get("contents"));
 			}
 			System.out.println();
Index: contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestQueryTemplateManager.java
===================================================================
--- contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestQueryTemplateManager.java	(revision 822139)
+++ contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestQueryTemplateManager.java	(working copy)
@@ -13,7 +13,6 @@
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.search.Hits;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.store.RAMDirectory;
@@ -90,11 +89,11 @@
 			Query q=builder.getQuery(doc.getDocumentElement());
 			
 			//Run the query
-			Hits h=searcher.search(q);
+			int h=searcher.search(q, null, 1000).totalHits;
 			
 			//Check we have the expected number of results
 			int expectedHits=Integer.parseInt(queryFormProperties.getProperty("expectedMatches"));
-			assertEquals("Number of results should match for query "+queryForms[i],expectedHits,h.length());
+			assertEquals("Number of results should match for query "+queryForms[i],expectedHits,h);
 			
 		}
 	}
Index: contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestCartesian.java
===================================================================
--- contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestCartesian.java	(revision 822139)
+++ contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestCartesian.java	(working copy)
@@ -28,25 +28,26 @@
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.search.Hits;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.SortField;
 import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.search.function.CustomScoreQuery;
 import org.apache.lucene.search.function.FieldScoreQuery;
 import org.apache.lucene.search.function.FieldScoreQuery.Type;
-import org.apache.lucene.util.NumericUtils;
 import org.apache.lucene.spatial.geohash.GeoHashUtils;
+import org.apache.lucene.spatial.geometry.DistanceUnits;
+import org.apache.lucene.spatial.geometry.FloatLatLng;
+import org.apache.lucene.spatial.geometry.LatLng;
 import org.apache.lucene.spatial.tier.projections.CartesianTierPlotter;
 import org.apache.lucene.spatial.tier.projections.IProjector;
 import org.apache.lucene.spatial.tier.projections.SinusoidalProjector;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.spatial.geometry.LatLng;
-import org.apache.lucene.spatial.geometry.FloatLatLng;
-import org.apache.lucene.spatial.geometry.DistanceUnits;
+import org.apache.lucene.util.NumericUtils;
 
 /**
  *
@@ -210,10 +211,10 @@
 
     // Perform the search, using the term query, the serial chain filter, and the
     // distance sort
-    Hits hits = searcher.search(customScore,null,sort);
-
-    int results = hits.length();
-
+    TopDocs hits = searcher.search(customScore.createWeight(searcher),null, 1000, sort);
+    int results = hits.totalHits;
+    ScoreDoc[] scoreDocs = hits.scoreDocs; 
+    
     // Get a list of distances
     Map<Integer,Double> distances = dq.distanceFilter.getDistances();
 
@@ -234,12 +235,12 @@
     assertEquals(2, results);
     double lastDistance = 0;
     for(int i =0 ; i < results; i++){
-      Document d = hits.doc(i);
+      Document d = searcher.doc(scoreDocs[i].doc);
 
       String name = d.get("name");
       double rsLat = NumericUtils.prefixCodedToDouble(d.get(latField));
       double rsLng = NumericUtils.prefixCodedToDouble(d.get(lngField));
-      Double geo_distance = distances.get(hits.id(i));
+      Double geo_distance = distances.get(scoreDocs[i].doc);
 
       double distance = DistanceUtils.getInstance().getDistanceMi(lat, lng, rsLat, rsLng);
       double llm = DistanceUtils.getInstance().getLLMDistance(lat, lng, rsLat, rsLng);
@@ -297,10 +298,10 @@
 
     // Perform the search, using the term query, the serial chain filter, and the
     // distance sort
-    Hits hits = searcher.search(customScore,null,sort);
+    TopDocs hits = searcher.search(customScore.createWeight(searcher),null, 1000, sort);
+    int results = hits.totalHits;
+    ScoreDoc[] scoreDocs = hits.scoreDocs; 
 
-    int results = hits.length();
-
     // Get a list of distances
     Map<Integer,Double> distances = dq.distanceFilter.getDistances();
 
@@ -321,11 +322,11 @@
     assertEquals(18, results);
     double lastDistance = 0;
     for(int i =0 ; i < results; i++){
-      Document d = hits.doc(i);
+      Document d = searcher.doc(scoreDocs[i].doc);
       String name = d.get("name");
       double rsLat = NumericUtils.prefixCodedToDouble(d.get(latField));
       double rsLng = NumericUtils.prefixCodedToDouble(d.get(lngField));
-      Double geo_distance = distances.get(hits.id(i));
+      Double geo_distance = distances.get(scoreDocs[i].doc);
 
       double distance = DistanceUtils.getInstance().getDistanceMi(lat, lng, rsLat, rsLng);
       double llm = DistanceUtils.getInstance().getLLMDistance(lat, lng, rsLat, rsLng);
@@ -386,9 +387,9 @@
     
       // Perform the search, using the term query, the serial chain filter, and the
       // distance sort
-      Hits hits = searcher.search(customScore,null,sort);
-
-      int results = hits.length();
+      TopDocs hits = searcher.search(customScore.createWeight(searcher),null, 1000, sort);
+      int results = hits.totalHits;
+      ScoreDoc[] scoreDocs = hits.scoreDocs; 
     
       // Get a list of distances 
       Map<Integer,Double> distances = dq.distanceFilter.getDistances();
@@ -410,12 +411,12 @@
       assertEquals(expected[x], results);
       double lastDistance = 0;
       for(int i =0 ; i < results; i++){
-        Document d = hits.doc(i);
+        Document d = searcher.doc(scoreDocs[i].doc);
       
         String name = d.get("name");
         double rsLat = NumericUtils.prefixCodedToDouble(d.get(latField));
         double rsLng = NumericUtils.prefixCodedToDouble(d.get(lngField)); 
-        Double geo_distance = distances.get(hits.id(i));
+        Double geo_distance = distances.get(scoreDocs[i].doc);
       
         double distance = DistanceUtils.getInstance().getDistanceMi(lat, lng, rsLat, rsLng);
         double llm = DistanceUtils.getInstance().getLLMDistance(lat, lng, rsLat, rsLng);
@@ -475,9 +476,9 @@
 	    
       // Perform the search, using the term query, the serial chain filter, and the
       // distance sort
-      Hits hits = searcher.search(customScore, dq.getFilter()); //,sort);
-
-      int results = hits.length();
+      TopDocs hits = searcher.search(customScore.createWeight(searcher),dq.getFilter(), 1000); //,sort);
+      int results = hits.totalHits;
+      ScoreDoc[] scoreDocs = hits.scoreDocs; 
 	    
       // Get a list of distances 
       Map<Integer,Double> distances = dq.distanceFilter.getDistances();
@@ -499,16 +500,16 @@
       assertEquals(expected[x], results);
 	    
       for(int i =0 ; i < results; i++){
-        Document d = hits.doc(i);
+        Document d = searcher.doc(scoreDocs[i].doc);
 	      
         String name = d.get("name");
         double rsLat = NumericUtils.prefixCodedToDouble(d.get(latField));
         double rsLng = NumericUtils.prefixCodedToDouble(d.get(lngField)); 
-        Double geo_distance = distances.get(hits.id(i));
+        Double geo_distance = distances.get(scoreDocs[i].doc);
 	      
         double distance = DistanceUtils.getInstance().getDistanceMi(lat, lng, rsLat, rsLng);
         double llm = DistanceUtils.getInstance().getLLMDistance(lat, lng, rsLat, rsLng);
-        System.out.println("Name: "+ name +", Distance (res, ortho, harvesine):"+ distance +" |"+ geo_distance +"|"+ llm +" | score "+ hits.score(i));
+        System.out.println("Name: "+ name +", Distance (res, ortho, harvesine):"+ distance +" |"+ geo_distance +"|"+ llm +" | score "+ scoreDocs[i].score);
         assertTrue(Math.abs((distance - llm)) < 1);
         assertTrue((distance < miles ));
 	      
Index: contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java
===================================================================
--- contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java	(revision 822139)
+++ contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java	(working copy)
@@ -34,6 +34,7 @@
 import javax.xml.parsers.DocumentBuilderFactory;
 
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.LowerCaseTokenizer;
 import org.apache.lucene.analysis.SimpleAnalyzer;
 import org.apache.lucene.analysis.Token;
@@ -56,7 +57,6 @@
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.ConstantScoreRangeQuery;
 import org.apache.lucene.search.FilteredQuery;
-import org.apache.lucene.search.Hits;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MultiPhraseQuery;
 import org.apache.lucene.search.MultiSearcher;
@@ -76,7 +76,6 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.Version;
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.w3c.dom.Element;
 import org.w3c.dom.NodeList;
 
@@ -90,9 +89,9 @@
   private Query query;
   RAMDirectory ramDir;
   public IndexSearcher searcher = null;
-  public Hits hits = null;
   int numHighlights = 0;
   Analyzer analyzer = new StandardAnalyzer();
+  TopDocs hits;
 
   String[] texts = {
       "Hello this is a piece of text that is very long and contains too much preamble and the meat is really here which says kennedy has been shot",
@@ -193,8 +192,8 @@
     QueryScorer scorer = new QueryScorer(query, FIELD_NAME);
     Highlighter highlighter = new Highlighter(scorer);
     
-    for (int i = 0; i < hits.length(); i++) {
-      String text = hits.doc(i).get(FIELD_NAME);
+    for (int i = 0; i < hits.totalHits; i++) {
+      String text = searcher.doc(hits.scoreDocs[i].doc).get(FIELD_NAME);
       TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME,
           new StringReader(text));
       highlighter.setTextFragmenter(new SimpleFragmenter(40));
@@ -242,8 +241,8 @@
     QueryScorer scorer = new QueryScorer(query, FIELD_NAME);
     Highlighter highlighter = new Highlighter(this, scorer);
     
-    for (int i = 0; i < hits.length(); i++) {
-      String text = hits.doc(i).get(FIELD_NAME);
+    for (int i = 0; i < hits.totalHits; i++) {
+      String text = searcher.doc(hits.scoreDocs[i].doc).get(FIELD_NAME);
       TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(text));
 
       highlighter.setTextFragmenter(new SimpleFragmenter(40));
@@ -266,8 +265,8 @@
     Highlighter highlighter = new Highlighter(this,scorer);
     highlighter.setTextFragmenter(new SimpleFragmenter(40));
     
-    for (int i = 0; i < hits.length(); i++) {
-      String text = hits.doc(i).get(FIELD_NAME);
+    for (int i = 0; i < hits.totalHits; i++) {
+      String text = searcher.doc(hits.scoreDocs[i].doc).get(FIELD_NAME);
       TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(text));
 
       String result = highlighter.getBestFragments(tokenStream, text, maxNumFragmentsRequired,
@@ -284,8 +283,8 @@
 
     int maxNumFragmentsRequired = 2;
 
-    for (int i = 0; i < hits.length(); i++) {
-      String text = hits.doc(i).get(FIELD_NAME);
+    for (int i = 0; i < hits.totalHits; i++) {
+      String text = searcher.doc(hits.scoreDocs[i].doc).get(FIELD_NAME);
       TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(text));
       QueryScorer scorer = new QueryScorer(query, FIELD_NAME);
       Highlighter highlighter = new Highlighter(this, scorer);
@@ -309,8 +308,8 @@
     QueryScorer scorer = new QueryScorer(query, FIELD_NAME);
     Highlighter highlighter = new Highlighter(this, scorer);
     
-    for (int i = 0; i < hits.length(); i++) {
-      String text = hits.doc(i).get(FIELD_NAME);
+    for (int i = 0; i < hits.totalHits; i++) {
+      String text = searcher.doc(hits.scoreDocs[i].doc).get(FIELD_NAME);
       TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(text));
 
       highlighter.setTextFragmenter(new SimpleSpanFragmenter(scorer, 5));
@@ -328,8 +327,8 @@
     scorer = new QueryScorer(query, FIELD_NAME);
     highlighter = new Highlighter(this, scorer);
 
-    for (int i = 0; i < hits.length(); i++) {
-      String text = hits.doc(i).get(FIELD_NAME);
+    for (int i = 0; i < hits.totalHits; i++) {
+      String text = searcher.doc(hits.scoreDocs[i].doc).get(FIELD_NAME);
       TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(text));
 
       highlighter.setTextFragmenter(new SimpleSpanFragmenter(scorer, 20));
@@ -350,8 +349,8 @@
     QueryScorer scorer = new QueryScorer(query, FIELD_NAME);
     Highlighter highlighter = new Highlighter(this,scorer);
     
-    for (int i = 0; i < hits.length(); i++) {
-      String text = hits.doc(i).get(FIELD_NAME);
+    for (int i = 0; i < hits.totalHits; i++) {
+      String text = searcher.doc(hits.scoreDocs[i].doc).get(FIELD_NAME);
       TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME,new StringReader(text));
 
       highlighter.setTextFragmenter(new SimpleFragmenter(40));
@@ -405,7 +404,7 @@
 
       public void run() throws Exception {
         mode = QUERY;
-        doStandardHighlights(analyzer, hits, query, HighlighterTest.this);
+        doStandardHighlights(analyzer, searcher, hits, query, HighlighterTest.this);
       }
     };
 
@@ -420,8 +419,8 @@
     Highlighter highlighter = new Highlighter(new QueryTermScorer(query));
     highlighter.setTextFragmenter(new SimpleFragmenter(40));
     int maxNumFragmentsRequired = 2;
-    for (int i = 0; i < hits.length(); i++) {
-      String text = hits.doc(i).get(FIELD_NAME);
+    for (int i = 0; i < hits.totalHits; i++) {
+      String text = searcher.doc(hits.scoreDocs[i].doc).get(FIELD_NAME);
       TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(text));
 
       String result = highlighter.getBestFragments(tokenStream, text, maxNumFragmentsRequired,
@@ -447,7 +446,7 @@
 
       public void run() throws Exception {
         mode = QUERY;
-        doStandardHighlights(analyzer, hits, query, HighlighterTest.this);
+        doStandardHighlights(analyzer, searcher, hits, query, HighlighterTest.this);
       }
     };
 
@@ -465,7 +464,7 @@
 
       public void run() throws Exception {
         mode = QUERY;
-        doStandardHighlights(analyzer, hits, query, HighlighterTest.this);
+        doStandardHighlights(analyzer, searcher, hits, query, HighlighterTest.this);
       }
     };
 
@@ -480,7 +479,7 @@
       public void run() throws Exception {
         numHighlights = 0;
         doSearching("Kennedy");
-        doStandardHighlights(analyzer, hits, query, HighlighterTest.this);
+        doStandardHighlights(analyzer, searcher, hits, query, HighlighterTest.this);
         assertTrue("Failed to find correct number of highlights " + numHighlights + " found",
             numHighlights == 4);
       }
@@ -495,7 +494,7 @@
       public void run() throws Exception {
         numHighlights = 0;
         doSearching("Kinnedy~");
-        doStandardHighlights(analyzer, hits, query, HighlighterTest.this, true);
+        doStandardHighlights(analyzer, searcher, hits, query, HighlighterTest.this, true);
         assertTrue("Failed to find correct number of highlights " + numHighlights + " found",
             numHighlights == 5);
       }
@@ -510,7 +509,7 @@
       public void run() throws Exception {
         numHighlights = 0;
         doSearching("K?nnedy");
-        doStandardHighlights(analyzer, hits, query, HighlighterTest.this);
+        doStandardHighlights(analyzer, searcher, hits, query, HighlighterTest.this);
         assertTrue("Failed to find correct number of highlights " + numHighlights + " found",
             numHighlights == 4);
       }
@@ -525,7 +524,7 @@
       public void run() throws Exception {
         numHighlights = 0;
         doSearching("K*dy");
-        doStandardHighlights(analyzer, hits, query, HighlighterTest.this);
+        doStandardHighlights(analyzer, searcher, hits, query, HighlighterTest.this);
         assertTrue("Failed to find correct number of highlights " + numHighlights + " found",
             numHighlights == 5);
       }
@@ -549,7 +548,7 @@
         query = parser.parse(queryString);
         doSearching(query);
 
-        doStandardHighlights(analyzer, hits, query, HighlighterTest.this);
+        doStandardHighlights(analyzer, searcher, hits, query, HighlighterTest.this);
         assertTrue("Failed to find correct number of highlights " + numHighlights + " found",
             numHighlights == 5);
       }
@@ -569,10 +568,10 @@
     // it rewrites to ConstantScoreQuery which cannot be highlighted
     // query = unReWrittenQuery.rewrite(reader);
     System.out.println("Searching for: " + query.toString(FIELD_NAME));
-    hits = searcher.search(query);
+    hits = searcher.search(query, null, 1000);
 
-    for (int i = 0; i < hits.length(); i++) {
-      String text = hits.doc(i).get(HighlighterTest.FIELD_NAME);
+    for (int i = 0; i < hits.totalHits; i++) {
+      String text = searcher.doc(hits.scoreDocs[i].doc).get(HighlighterTest.FIELD_NAME);
       int maxNumFragmentsRequired = 2;
       String fragmentSeparator = "...";
       QueryScorer scorer = null;
@@ -605,10 +604,10 @@
     // it rewrites to ConstantScoreQuery which cannot be highlighted
     // query = unReWrittenQuery.rewrite(reader);
     System.out.println("Searching for: " + query.toString(FIELD_NAME));
-    hits = searcher.search(query);
+    hits = searcher.search(query, null, 1000);
 
-    for (int i = 0; i < hits.length(); i++) {
-      String text = hits.doc(i).get(HighlighterTest.FIELD_NAME);
+    for (int i = 0; i < hits.totalHits; i++) {
+      String text = searcher.doc(hits.scoreDocs[i].doc).get(HighlighterTest.FIELD_NAME);
       int maxNumFragmentsRequired = 2;
       String fragmentSeparator = "...";
       QueryScorer scorer = null;
@@ -631,12 +630,12 @@
     
     // try null field
     
-    hits = searcher.search(query);
+    hits = searcher.search(query, null, 1000);
     
     numHighlights = 0;
 
-    for (int i = 0; i < hits.length(); i++) {
-      String text = hits.doc(i).get(HighlighterTest.FIELD_NAME);
+    for (int i = 0; i < hits.totalHits; i++) {
+      String text = searcher.doc(hits.scoreDocs[i].doc).get(HighlighterTest.FIELD_NAME);
       int maxNumFragmentsRequired = 2;
       String fragmentSeparator = "...";
       QueryScorer scorer = null;
@@ -659,12 +658,12 @@
     
     // try default field
     
-    hits = searcher.search(query);
+    hits = searcher.search(query, null, 1000);
     
     numHighlights = 0;
 
-    for (int i = 0; i < hits.length(); i++) {
-      String text = hits.doc(i).get(HighlighterTest.FIELD_NAME);
+    for (int i = 0; i < hits.totalHits; i++) {
+      String text = searcher.doc(hits.scoreDocs[i].doc).get(HighlighterTest.FIELD_NAME);
       int maxNumFragmentsRequired = 2;
       String fragmentSeparator = "...";
       QueryScorer scorer = null;
@@ -692,7 +691,7 @@
       public void run() throws Exception {
         numHighlights = 0;
         doSearching("\"John Kennedy\"");
-        doStandardHighlights(analyzer, hits, query, HighlighterTest.this);
+        doStandardHighlights(analyzer, searcher, hits, query, HighlighterTest.this);
         // Currently highlights "John" and "Kennedy" separately
         assertTrue("Failed to find correct number of highlights " + numHighlights + " found",
             numHighlights == 2);
@@ -712,7 +711,7 @@
 
         SpanNearQuery snq = new SpanNearQuery(clauses, 1, true);
         doSearching(snq);
-        doStandardHighlights(analyzer, hits, query, HighlighterTest.this);
+        doStandardHighlights(analyzer, searcher, hits, query, HighlighterTest.this);
         // Currently highlights "John" and "Kennedy" separately
         assertTrue("Failed to find correct number of highlights " + numHighlights + " found",
             numHighlights == 2);
@@ -752,7 +751,7 @@
         FilteredQuery fq = new FilteredQuery(snq, rf);
 
         doSearching(fq);
-        doStandardHighlights(analyzer, hits, query, HighlighterTest.this);
+        doStandardHighlights(analyzer, searcher, hits, query, HighlighterTest.this);
         // Currently highlights "John" and "Kennedy" separately
         assertTrue("Failed to find correct number of highlights " + numHighlights + " found",
             numHighlights == 2);
@@ -774,7 +773,7 @@
         FilteredQuery fq = new FilteredQuery(pq, rf);
 
         doSearching(fq);
-        doStandardHighlights(analyzer, hits, query, HighlighterTest.this);
+        doStandardHighlights(analyzer, searcher, hits, query, HighlighterTest.this);
         // Currently highlights "John" and "Kennedy" separately
         assertTrue("Failed to find correct number of highlights " + numHighlights + " found",
             numHighlights == 2);
@@ -790,7 +789,7 @@
       public void run() throws Exception {
         numHighlights = 0;
         doSearching("John Kenn*");
-        doStandardHighlights(analyzer, hits, query, HighlighterTest.this);
+        doStandardHighlights(analyzer, searcher, hits, query, HighlighterTest.this);
         assertTrue("Failed to find correct number of highlights " + numHighlights + " found",
             numHighlights == 5);
       }
@@ -805,7 +804,7 @@
       public void run() throws Exception {
         numHighlights = 0;
         doSearching("JFK OR Kennedy");
-        doStandardHighlights(analyzer, hits, query, HighlighterTest.this);
+        doStandardHighlights(analyzer, searcher, hits, query, HighlighterTest.this);
         assertTrue("Failed to find correct number of highlights " + numHighlights + " found",
             numHighlights == 5);
       }
@@ -820,8 +819,8 @@
       public void run() throws Exception {
         doSearching("Kennedy");
         numHighlights = 0;
-        for (int i = 0; i < hits.length(); i++) {
-          String text = hits.doc(i).get(FIELD_NAME);
+        for (int i = 0; i < hits.totalHits; i++) {
+          String text = searcher.doc(hits.scoreDocs[i].doc).get(FIELD_NAME);
           TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(text));
 
           Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream,
@@ -834,8 +833,8 @@
             numHighlights == 4);
 
         numHighlights = 0;
-        for (int i = 0; i < hits.length(); i++) {
-          String text = hits.doc(i).get(FIELD_NAME);
+        for (int i = 0; i < hits.totalHits; i++) {
+          String text = searcher.doc(hits.scoreDocs[i].doc).get(FIELD_NAME);
           TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(text));
           Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream,
               HighlighterTest.this);
@@ -845,8 +844,8 @@
             numHighlights == 4);
 
         numHighlights = 0;
-        for (int i = 0; i < hits.length(); i++) {
-          String text = hits.doc(i).get(FIELD_NAME);
+        for (int i = 0; i < hits.totalHits; i++) {
+          String text = searcher.doc(hits.scoreDocs[i].doc).get(FIELD_NAME);
 
           TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(text));
           Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream,
@@ -950,8 +949,8 @@
         doSearching("Kennedy");
         // new Highlighter(HighlighterTest.this, new QueryTermScorer(query));
 
-        for (int i = 0; i < hits.length(); i++) {
-          String text = hits.doc(i).get(FIELD_NAME);
+        for (int i = 0; i < hits.totalHits; i++) {
+          String text = searcher.doc(hits.scoreDocs[i].doc).get(FIELD_NAME);
           TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(text));
           Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream,
               HighlighterTest.this);
@@ -972,8 +971,8 @@
 
         doSearching("Kennedy");
 
-        for (int i = 0; i < hits.length(); i++) {
-          String text = hits.doc(i).get(FIELD_NAME);
+        for (int i = 0; i < hits.totalHits; i++) {
+          String text = searcher.doc(hits.scoreDocs[i].doc).get(FIELD_NAME);
           TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(text));
 
           Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream,
@@ -1106,7 +1105,7 @@
         System.out.println("Searching with primitive query");
         // forget to set this and...
         // query=query.rewrite(reader);
-        Hits hits = searcher.search(query);
+        TopDocs hits = searcher.search(query, null, 1000);
 
         // create an instance of the highlighter with the tags used to surround
         // highlighted text
@@ -1116,8 +1115,8 @@
 
         int maxNumFragmentsRequired = 3;
 
-        for (int i = 0; i < hits.length(); i++) {
-          String text = hits.doc(i).get(FIELD_NAME);
+        for (int i = 0; i < hits.totalHits; i++) {
+          String text = searcher.doc(hits.scoreDocs[i].doc).get(FIELD_NAME);
           TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(text));
           Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream, HighlighterTest.this, false);
 
@@ -1240,7 +1239,7 @@
     query = parser.parse("multi*");
     System.out.println("Searching for: " + query.toString(FIELD_NAME));
     // at this point the multisearcher calls combine(query[])
-    hits = multiSearcher.search(query);
+    hits = multiSearcher.search(query, null, 1000);
 
     // query = QueryParser.parse("multi*", FIELD_NAME, new StandardAnalyzer());
     Query expandedQueries[] = new Query[2];
@@ -1252,8 +1251,8 @@
     // highlighted text
     Highlighter highlighter = new Highlighter(this, new QueryTermScorer(query));
 
-    for (int i = 0; i < hits.length(); i++) {
-      String text = hits.doc(i).get(FIELD_NAME);
+    for (int i = 0; i < hits.totalHits; i++) {
+      String text = multiSearcher.doc(hits.scoreDocs[i].doc).get(FIELD_NAME);
       TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(text));
       String highlightedText = highlighter.getBestFragment(tokenStream, text);
       System.out.println(highlightedText);
@@ -1549,8 +1548,8 @@
    * Highlighter highlighter = new Highlighter(this,new
    * QueryFragmentScorer(query));
    * 
-   * for (int i = 0; i < hits.length(); i++) { String text =
-   * hits.doc(i).get(FIELD_NAME); TokenStream
+   * for (int i = 0; i < hits.totalHits; i++) { String text =
+   * searcher.doc(hits.scoreDocs[i].doc).get(FIELD_NAME); TokenStream
    * tokenStream=bigramAnalyzer.tokenStream(FIELD_NAME,new StringReader(text));
    * String highlightedText = highlighter.getBestFragment(tokenStream,text);
    * System.out.println(highlightedText); } }
@@ -1577,13 +1576,13 @@
     // you must use a rewritten query!
     query = unReWrittenQuery.rewrite(reader);
     System.out.println("Searching for: " + query.toString(FIELD_NAME));
-    hits = searcher.search(query);
+    hits = searcher.search(query, null, 1000);
   }
 
   public void assertExpectedHighlightCount(final int maxNumFragmentsRequired,
       final int expectedHighlights) throws Exception {
-    for (int i = 0; i < hits.length(); i++) {
-      String text = hits.doc(i).get(FIELD_NAME);
+    for (int i = 0; i < hits.totalHits; i++) {
+      String text = searcher.doc(hits.scoreDocs[i].doc).get(FIELD_NAME);
       TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(text));
       QueryScorer scorer = new QueryScorer(query, FIELD_NAME);
       Highlighter highlighter = new Highlighter(this, scorer);
@@ -1770,16 +1769,16 @@
       }
     }
 
-    void doStandardHighlights(Analyzer analyzer, Hits hits, Query query, Formatter formatter)
+    void doStandardHighlights(Analyzer analyzer, IndexSearcher searcher, TopDocs hits, Query query, Formatter formatter)
     throws Exception {
-      doStandardHighlights(analyzer, hits, query, formatter, false);
+      doStandardHighlights(analyzer, searcher, hits, query, formatter, false);
     }
     
-    void doStandardHighlights(Analyzer analyzer, Hits hits, Query query, Formatter formatter, boolean expandMT)
+    void doStandardHighlights(Analyzer analyzer, IndexSearcher searcher, TopDocs hits, Query query, Formatter formatter, boolean expandMT)
         throws Exception {
 
-      for (int i = 0; i < hits.length(); i++) {
-        String text = hits.doc(i).get(HighlighterTest.FIELD_NAME);
+      for (int i = 0; i < hits.totalHits; i++) {
+        String text = searcher.doc(hits.scoreDocs[i].doc).get(HighlighterTest.FIELD_NAME);
         int maxNumFragmentsRequired = 2;
         String fragmentSeparator = "...";
         Scorer scorer = null;
Index: contrib/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java
===================================================================
--- contrib/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java	(revision 822139)
+++ contrib/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java	(working copy)
@@ -17,6 +17,9 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+import java.util.Iterator;
+
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -25,15 +28,12 @@
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Hits;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.store.Directory;
 
-import java.io.IOException;
-import java.util.Iterator;
-
 /**
  * <p>
  *   Spell Checker class  (Main class) <br/>
@@ -214,17 +214,19 @@
       }
     }
 
+    int maxHits = 10 * numSug;
+    
 //    System.out.println("Q: " + query);
-    Hits hits = searcher.search(query);
+    ScoreDoc[] hits = searcher.search(query, null, maxHits).scoreDocs;
 //    System.out.println("HITS: " + hits.length());
     SuggestWordQueue sugQueue = new SuggestWordQueue(numSug);
 
     // go thru more than 'maxr' matches in case the distance filter triggers
-    int stop = Math.min(hits.length(), 10 * numSug);
+    int stop = Math.min(hits.length, maxHits);
     SuggestWord sugWord = new SuggestWord();
     for (int i = 0; i < stop; i++) {
 
-      sugWord.string = hits.doc(i).get(F_WORD); // get orig word
+      sugWord.string = searcher.doc(hits[i].doc).get(F_WORD); // get orig word
 
       // don't suggest a word for itself, that would be silly
       if (sugWord.string.equals(word)) {
Index: contrib/lucli/src/java/lucli/LuceneMethods.java
===================================================================
--- contrib/lucli/src/java/lucli/LuceneMethods.java	(revision 822139)
+++ contrib/lucli/src/java/lucli/LuceneMethods.java	(working copy)
@@ -17,13 +17,13 @@
  * limitations under the License.
  */
 
+import java.io.File;
 import java.io.IOException;
 import java.io.Reader;
 import java.io.StringReader;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Comparator;
-import java.util.Enumeration;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
@@ -31,7 +31,6 @@
 import java.util.Set;
 import java.util.TreeMap;
 import java.util.Map.Entry;
-import java.io.File;
 
 import jline.ConsoleReader;
 
@@ -50,10 +49,12 @@
 import org.apache.lucene.index.IndexReader.FieldOption;
 import org.apache.lucene.queryParser.MultiFieldQueryParser;
 import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Explanation;
-import org.apache.lucene.search.Hits;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.Searcher;
 import org.apache.lucene.store.FSDirectory;
 
@@ -117,31 +118,33 @@
 
   public void search(String queryString, boolean explain, boolean showTokens, ConsoleReader cr)
   		throws java.io.IOException, org.apache.lucene.queryParser.ParseException {
-    Hits hits = initSearch(queryString);
-    System.out.println(hits.length() + " total matching documents");
+    initSearch(queryString);
+    int numHits = computeCount(query);
+    System.out.println(numHits + " total matching documents");
     if (explain) {
       query = explainQuery(queryString);
     }
 
     final int HITS_PER_PAGE = 10;
     message("--------------------------------------");
-    for (int start = 0; start < hits.length(); start += HITS_PER_PAGE) {
-      int end = Math.min(hits.length(), start + HITS_PER_PAGE);
+    for (int start = 0; start < numHits; start += HITS_PER_PAGE) {
+      int end = Math.min(numHits, start + HITS_PER_PAGE);
+      ScoreDoc[] hits = search(query, end);
       for (int ii = start; ii < end; ii++) {
-        Document doc = hits.doc(ii);
-        message("---------------- " + (ii + 1) + " score:" + hits.score(ii) + "---------------------");
+        Document doc = searcher.doc(hits[ii].doc);
+        message("---------------- " + (ii + 1) + " score:" + hits[ii].score + "---------------------");
         printHit(doc);
         if (showTokens) {
           invertDocument(doc);
         }
         if (explain) {
-          Explanation exp = searcher.explain(query, hits.id(ii));
+          Explanation exp = searcher.explain(query, hits[ii].doc);
           message("Explanation:" + exp.toString());
         }
       }
       message("#################################################");
 
-      if (hits.length() > end) {
+      if (numHits > end) {
       	// TODO: don't let the input end up in the command line history
       	queryString = cr.readLine("more (y/n) ? ");
         if (queryString.length() == 0 || queryString.charAt(0) == 'n')
@@ -201,7 +204,7 @@
   /**
    * TODO: Allow user to specify analyzer
    */
-  private Hits initSearch(String queryString) throws IOException, ParseException {
+  private void initSearch(String queryString) throws IOException, ParseException {
 
     searcher = new IndexSearcher(indexName, true);
     Analyzer analyzer = createAnalyzer();
@@ -215,16 +218,38 @@
     MultiFieldQueryParser parser = new MultiFieldQueryParser(fieldsArray, analyzer);
     query = parser.parse(queryString);
     System.out.println("Searching for: " + query.toString());
-    Hits hits = searcher.search(query);
-    return (hits);
+  }
+  
+  final static class CountingCollector extends Collector {
+    public int numHits = 0;
+    
+    public void setScorer(Scorer scorer) throws IOException {}
+    public void collect(int doc) throws IOException {
+      numHits++;
+    }
 
+    public void setNextReader(IndexReader reader, int docBase) {}
+    public boolean acceptsDocsOutOfOrder() {
+      return true;
+    }    
   }
+  
+  private int computeCount(Query q) throws IOException {
+    CountingCollector countingCollector = new CountingCollector();
+    
+    searcher.search(q, countingCollector);    
+    return countingCollector.numHits;
+  }
 
   public void count(String queryString) throws java.io.IOException, ParseException {
-    Hits hits = initSearch(queryString);
-    System.out.println(hits.length() + " total documents");
+    initSearch(queryString);
+    System.out.println(computeCount(query) + " total documents");
     searcher.close();
   }
+  
+  private ScoreDoc[] search(Query q, int numHits) throws IOException {
+    return searcher.search(query, numHits).scoreDocs;
+  }
 
   static public void message(String s) {
     System.out.println(s);
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java	(revision 822139)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java	(working copy)
@@ -20,26 +20,26 @@
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.LetterTokenizer;
+import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
-import org.apache.lucene.analysis.tokenattributes.*;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Hits;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.PhraseQuery;
 import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
 
@@ -82,20 +82,20 @@
     return new IndexSearcher(dir, true);
   }
 
-  protected Hits queryParsingTest(Analyzer analyzer, String qs) throws Exception {
+  protected ScoreDoc[] queryParsingTest(Analyzer analyzer, String qs) throws Exception {
     searcher = setUpSearcher(analyzer);
 
     QueryParser qp = new QueryParser("content", analyzer);
 
     Query q = qp.parse(qs);
 
-    return searcher.search(q);
+    return searcher.search(q, null, 1000).scoreDocs;
   }
 
-  protected void compareRanks(Hits hits, int[] ranks) throws Exception {
-    assertEquals(ranks.length, hits.length());
+  protected void compareRanks(ScoreDoc[] hits, int[] ranks) throws Exception {
+    assertEquals(ranks.length, hits.length);
     for (int i = 0; i < ranks.length; i++) {
-      assertEquals(ranks[i], hits.id(i));
+      assertEquals(ranks[i], hits[i].doc);
     }
   }
 
@@ -104,7 +104,7 @@
    * tokenizes on whitespace.
    */
   public void testShingleAnalyzerWrapperQueryParsing() throws Exception {
-    Hits hits = queryParsingTest(new ShingleAnalyzerWrapper
+    ScoreDoc[] hits = queryParsingTest(new ShingleAnalyzerWrapper
                                      (new WhitespaceAnalyzer(), 2),
                                  "test sentence");
     int[] ranks = new int[] { 1, 2, 0 };
@@ -115,7 +115,7 @@
    * This one fails with an exception.
    */
   public void testShingleAnalyzerWrapperPhraseQueryParsingFails() throws Exception {
-    Hits hits = queryParsingTest(new ShingleAnalyzerWrapper
+    ScoreDoc[] hits = queryParsingTest(new ShingleAnalyzerWrapper
                                      (new WhitespaceAnalyzer(), 2),
                                  "\"this sentence\"");
     int[] ranks = new int[] { 0 };
@@ -126,7 +126,7 @@
    * This one works, actually.
    */
   public void testShingleAnalyzerWrapperPhraseQueryParsing() throws Exception {
-    Hits hits = queryParsingTest(new ShingleAnalyzerWrapper
+    ScoreDoc[] hits = queryParsingTest(new ShingleAnalyzerWrapper
                                      (new WhitespaceAnalyzer(), 2),
                                  "\"test sentence\"");
     int[] ranks = new int[] { 1 };
@@ -137,7 +137,7 @@
    * Same as above, is tokenized without using the analyzer.
    */
   public void testShingleAnalyzerWrapperRequiredQueryParsing() throws Exception {
-    Hits hits = queryParsingTest(new ShingleAnalyzerWrapper
+    ScoreDoc[] hits = queryParsingTest(new ShingleAnalyzerWrapper
                                      (new WhitespaceAnalyzer(), 2),
                                  "+test +sentence");
     int[] ranks = new int[] { 1, 2 };
@@ -166,7 +166,7 @@
       q.add(new Term("content", termText), j);
     }
 
-    Hits hits = searcher.search(q);
+    ScoreDoc[] hits = searcher.search(q, null, 1000).scoreDocs;
     int[] ranks = new int[] { 0 };
     compareRanks(hits, ranks);
   }
@@ -193,7 +193,7 @@
             BooleanClause.Occur.SHOULD);
     }
 
-    Hits hits = searcher.search(q);
+    ScoreDoc[] hits = searcher.search(q, null, 1000).scoreDocs;
     int[] ranks = new int[] { 1, 2, 0 };
     compareRanks(hits, ranks);
   }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java	(revision 822139)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java	(working copy)
@@ -16,8 +16,12 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import java.io.IOException;
+import java.io.Reader;
+import java.io.StringReader;
+
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.LetterTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
@@ -30,15 +34,10 @@
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queryParser.ParseException;
 import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.search.Hits;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.store.RAMDirectory;
 
-import java.io.IOException;
-import java.io.Reader;
-import java.io.StringReader;
-
 public class QueryAutoStopWordAnalyzerTest extends BaseTokenStreamTestCase {
   String variedFieldValues[] = {"the", "quick", "brown", "fox", "jumped", "over", "the", "lazy", "boring", "dog"};
   String repetitiveFieldValues[] = {"boring", "boring", "vaguelyboring"};
@@ -72,18 +71,18 @@
   }
 
   //Helper method to query
-  private Hits search(Analyzer a, String queryString) throws IOException, ParseException {
+  private int search(Analyzer a, String queryString) throws IOException, ParseException {
     QueryParser qp = new QueryParser("repetitiveField", a);
     Query q = qp.parse(queryString);
-    return new IndexSearcher(reader).search(q);
+    return new IndexSearcher(reader).search(q, null, 1000).totalHits;
   }
 
   public void testUninitializedAnalyzer() throws Exception {
     //Note: no calls to "addStopWord"
     String query = "variedField:quick repetitiveField:boring";
-    Hits h = search(protectedAnalyzer, query);
-    Hits h2 = search(appAnalyzer, query);
-    assertEquals("No filtering test", h.length(), h2.length());
+    int numHits1 = search(protectedAnalyzer, query);
+    int numHits2 = search(appAnalyzer, query);
+    assertEquals("No filtering test", numHits1, numHits2);
   }
 
   /*
@@ -91,8 +90,8 @@
     */
   public void testDefaultAddStopWordsIndexReader() throws Exception {
     protectedAnalyzer.addStopWords(reader);
-    Hits h = search(protectedAnalyzer, "repetitiveField:boring");
-    assertEquals("Default filter should remove all docs", 0, h.length());
+    int numHits = search(protectedAnalyzer, "repetitiveField:boring");
+    assertEquals("Default filter should remove all docs", 0, numHits);
   }
 
 
@@ -101,26 +100,26 @@
     */
   public void testAddStopWordsIndexReaderInt() throws Exception {
     protectedAnalyzer.addStopWords(reader, 1f / 2f);
-    Hits h = search(protectedAnalyzer, "repetitiveField:boring");
-    assertEquals("A filter on terms in > one half of docs remove boring docs", 0, h.length());
+    int numHits = search(protectedAnalyzer, "repetitiveField:boring");
+    assertEquals("A filter on terms in > one half of docs remove boring docs", 0, numHits);
 
-    h = search(protectedAnalyzer, "repetitiveField:vaguelyboring");
-    assertTrue("A filter on terms in > half of docs should not remove vaguelyBoring docs", h.length() > 1);
+    numHits = search(protectedAnalyzer, "repetitiveField:vaguelyboring");
+    assertTrue("A filter on terms in > half of docs should not remove vaguelyBoring docs", numHits > 1);
 
     protectedAnalyzer.addStopWords(reader, 1f / 4f);
-    h = search(protectedAnalyzer, "repetitiveField:vaguelyboring");
-    assertEquals("A filter on terms in > quarter of docs should remove vaguelyBoring docs", 0, h.length());
+    numHits = search(protectedAnalyzer, "repetitiveField:vaguelyboring");
+    assertEquals("A filter on terms in > quarter of docs should remove vaguelyBoring docs", 0, numHits);
   }
 
 
   public void testAddStopWordsIndexReaderStringFloat() throws Exception {
     protectedAnalyzer.addStopWords(reader, "variedField", 1f / 2f);
-    Hits h = search(protectedAnalyzer, "repetitiveField:boring");
-    assertTrue("A filter on one Field should not affect queris on another", h.length() > 0);
+    int numHits = search(protectedAnalyzer, "repetitiveField:boring");
+    assertTrue("A filter on one Field should not affect queris on another", numHits > 0);
 
     protectedAnalyzer.addStopWords(reader, "repetitiveField", 1f / 2f);
-    h = search(protectedAnalyzer, "repetitiveField:boring");
-    assertEquals("A filter on the right Field should affect queries on it", h.length(), 0);
+    numHits = search(protectedAnalyzer, "repetitiveField:boring");
+    assertEquals("A filter on the right Field should affect queries on it", numHits, 0);
   }
 
   public void testAddStopWordsIndexReaderStringInt() throws Exception {
@@ -138,11 +137,11 @@
 
   public void testNoFieldNamePollution() throws Exception {
     protectedAnalyzer.addStopWords(reader, "repetitiveField", 10);
-    Hits h = search(protectedAnalyzer, "repetitiveField:boring");
-    assertEquals("Check filter set up OK", 0, h.length());
+    int numHits = search(protectedAnalyzer, "repetitiveField:boring");
+    assertEquals("Check filter set up OK", 0, numHits);
 
-    h = search(protectedAnalyzer, "variedField:boring");
-    assertTrue("Filter should not prevent stopwords in one field being used in another ", h.length() > 0);
+    numHits = search(protectedAnalyzer, "variedField:boring");
+    assertTrue("Filter should not prevent stopwords in one field being used in another ", numHits > 0);
 
   }
   
@@ -162,8 +161,8 @@
   public void testLUCENE1678BWComp() throws Exception {
     QueryAutoStopWordAnalyzer a = new QueryAutoStopWordSubclassAnalyzer();
     a.addStopWords(reader, "repetitiveField", 10);
-    Hits h = search(a, "repetitiveField:boring");
-    assertFalse(h.length() == 0);
+    int numHits = search(a, "repetitiveField:boring");
+    assertFalse(numHits == 0);
   }
   
   /*
@@ -183,10 +182,10 @@
   public void testWrappingNonReusableAnalyzer() throws Exception {
     QueryAutoStopWordAnalyzer a = new QueryAutoStopWordAnalyzer(new NonreusableAnalyzer());
     a.addStopWords(reader, 10);
-    Hits h = search(a, "repetitiveField:boring");
-    assertTrue(h.length() == 0);
-    h = search(a, "repetitiveField:vaguelyboring");
-    assertTrue(h.length() == 0);
+    int numHits = search(a, "repetitiveField:boring");
+    assertTrue(numHits == 0);
+    numHits = search(a, "repetitiveField:vaguelyboring");
+    assertTrue(numHits == 0);
   }
   
   public void testTokenStream() throws Exception {
Index: contrib/regex/src/test/org/apache/lucene/search/regex/TestRegexQuery.java
===================================================================
--- contrib/regex/src/test/org/apache/lucene/search/regex/TestRegexQuery.java	(revision 822139)
+++ contrib/regex/src/test/org/apache/lucene/search/regex/TestRegexQuery.java	(working copy)
@@ -67,7 +67,7 @@
     if ( capability != null )
       query.setRegexImplementation(capability);
     
-    return searcher.search(query).length();
+    return searcher.search(query, null, 1000).totalHits;
   }
 
   private int  spanRegexQueryNrHits(String regex1, String regex2, int slop, boolean ordered) throws Exception {
@@ -75,7 +75,7 @@
     SpanRegexQuery srq2 = new SpanRegexQuery( newTerm(regex2));
     SpanNearQuery query = new SpanNearQuery( new SpanQuery[]{srq1, srq2}, slop, ordered);
     
-    return searcher.search(query).length();
+    return searcher.search(query, null, 1000).totalHits;
   }
 
   public void testMatchAll() throws Exception {
Index: contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java
===================================================================
--- contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java	(revision 822139)
+++ contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java	(working copy)
@@ -28,7 +28,6 @@
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.search.Hits;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MultiSearcher;
 import org.apache.lucene.search.spans.SpanFirstQuery;
@@ -66,8 +65,8 @@
     SpanFirstQuery sfq = new SpanFirstQuery(srq, 1);
     // SpanNearQuery query = new SpanNearQuery(new SpanQuery[] {srq, stq}, 6,
     // true);
-    Hits hits = searcher.search(sfq);
-    assertEquals(1, hits.length());
+    int numHits = searcher.search(sfq, null, 1000).totalHits;
+    assertEquals(1, numHits);
   }
 
   public void testSpanRegexBug() throws CorruptIndexException, IOException {
@@ -83,7 +82,7 @@
     arrSearcher[0] = new IndexSearcher(indexStoreA, true);
     arrSearcher[1] = new IndexSearcher(indexStoreB, true);
     MultiSearcher searcher = new MultiSearcher(arrSearcher);
-    Hits hits = searcher.search(query);
+    int numHits = searcher.search(query, null, 1000).totalHits;
     arrSearcher[0].close();
     arrSearcher[1].close();
 
@@ -92,7 +91,7 @@
     // The rewriter function only write it once on the first IndexSearcher
     // So it's using term: a1 b1 to search on the second IndexSearcher
     // As a result, it won't match the document in the second IndexSearcher
-    assertEquals(2, hits.length());
+    assertEquals(2, numHits);
     indexStoreA.close();
     indexStoreB.close();
   }
Index: contrib/queries/src/test/org/apache/lucene/search/DuplicateFilterTest.java
===================================================================
--- contrib/queries/src/test/org/apache/lucene/search/DuplicateFilterTest.java	(revision 822139)
+++ contrib/queries/src/test/org/apache/lucene/search/DuplicateFilterTest.java	(working copy)
@@ -80,10 +80,10 @@
 	{
 		DuplicateFilter df=new DuplicateFilter(KEY_FIELD);		
 		HashSet results=new HashSet();
-		Hits h = searcher.search(tq,df);
-		for(int i=0;i<h.length();i++)
+		ScoreDoc[] hits = searcher.search(tq,df, 1000).scoreDocs;
+		for(int i=0;i<hits.length;i++)
 		{
-			Document d=h.doc(i);
+			Document d=searcher.doc(hits[i].doc);
 			String url=d.get(KEY_FIELD);
 			assertFalse("No duplicate urls should be returned",results.contains(url));
 			results.add(url);
@@ -92,12 +92,12 @@
 	public void testNoFilter() throws Throwable
 	{
 		HashSet results=new HashSet();
-		Hits h = searcher.search(tq);
-		assertTrue("Default searching should have found some matches",h.length()>0);
+		ScoreDoc[] hits = searcher.search(tq, null, 1000).scoreDocs;
+		assertTrue("Default searching should have found some matches",hits.length>0);
 		boolean dupsFound=false;
-		for(int i=0;i<h.length();i++)
+		for(int i=0;i<hits.length;i++)
 		{
-			Document d=h.doc(i);
+			Document d=searcher.doc(hits[i].doc);
 			String url=d.get(KEY_FIELD);
 			if(!dupsFound)
 				dupsFound=results.contains(url);
@@ -111,11 +111,11 @@
 		DuplicateFilter df=new DuplicateFilter(KEY_FIELD);
 		df.setProcessingMode(DuplicateFilter.PM_FAST_INVALIDATION);
 		HashSet results=new HashSet();
-		Hits h = searcher.search(tq,df);
-		assertTrue("Filtered searching should have found some matches",h.length()>0);
-		for(int i=0;i<h.length();i++)
+		ScoreDoc[] hits = searcher.search(tq,df, 1000).scoreDocs;
+		assertTrue("Filtered searching should have found some matches",hits.length>0);
+		for(int i=0;i<hits.length;i++)
 		{
-			Document d=h.doc(i);
+			Document d=searcher.doc(hits[i].doc);
 			String url=d.get(KEY_FIELD);
 			assertFalse("No duplicate urls should be returned",results.contains(url));
 			results.add(url);
@@ -126,11 +126,11 @@
 	{
 		DuplicateFilter df=new DuplicateFilter(KEY_FIELD);
 		df.setKeepMode(DuplicateFilter.KM_USE_LAST_OCCURRENCE);
-		Hits h = searcher.search(tq,df);
-		assertTrue("Filtered searching should have found some matches",h.length()>0);
-		for(int i=0;i<h.length();i++)
+		ScoreDoc[] hits = searcher.search(tq,df, 1000).scoreDocs;
+		assertTrue("Filtered searching should have found some matches",hits.length>0);
+		for(int i=0;i<hits.length;i++)
 		{
-			Document d=h.doc(i);
+			Document d=searcher.doc(hits[i].doc);
 			String url=d.get(KEY_FIELD);
 			TermDocs td = reader.termDocs(new Term(KEY_FIELD,url));
 			int lastDoc=0;
@@ -138,7 +138,7 @@
 			{
 				lastDoc=td.doc();
 			}
-			assertEquals("Duplicate urls should return last doc",lastDoc, h.id((i)));
+			assertEquals("Duplicate urls should return last doc",lastDoc, hits[i].doc);
 		}
 	}	
 	
@@ -147,17 +147,17 @@
 	{
 		DuplicateFilter df=new DuplicateFilter(KEY_FIELD);
 		df.setKeepMode(DuplicateFilter.KM_USE_FIRST_OCCURRENCE);
-		Hits h = searcher.search(tq,df);
-		assertTrue("Filtered searching should have found some matches",h.length()>0);
-		for(int i=0;i<h.length();i++)
+		ScoreDoc[] hits = searcher.search(tq,df, 1000).scoreDocs;
+		assertTrue("Filtered searching should have found some matches",hits.length>0);
+		for(int i=0;i<hits.length;i++)
 		{
-			Document d=h.doc(i);
+			Document d=searcher.doc(hits[i].doc);
 			String url=d.get(KEY_FIELD);
 			TermDocs td = reader.termDocs(new Term(KEY_FIELD,url));
 			int lastDoc=0;
 			td.next();
 			lastDoc=td.doc();
-			assertEquals("Duplicate urls should return first doc",lastDoc, h.id((i)));
+			assertEquals("Duplicate urls should return first doc",lastDoc, hits[i].doc);
 		}
 	}	
 	
Index: contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis.java
===================================================================
--- contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis.java	(revision 822139)
+++ contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis.java	(working copy)
@@ -15,41 +15,42 @@
  */
 package org.apache.lucene.search.similar;
 
-import org.apache.lucene.util.PriorityQueue;
+import java.io.File;
+import java.io.FileReader;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.io.PrintStream;
+import java.io.Reader;
+import java.io.StringReader;
+import java.net.URL;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.document.Document;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermFreqVector;
-import org.apache.lucene.search.BooleanClause;	
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.DefaultSimilarity;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.Similarity;
 import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.Hits;
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.document.Document;
+import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.util.PriorityQueue;
 
-import java.util.Set;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Collection;
-import java.util.Iterator;
-import java.io.IOException;
-import java.io.Reader;
-import java.io.File;
-import java.io.PrintStream;
-import java.io.StringReader;
-import java.io.FileReader;
-import java.io.InputStreamReader;
-import java.net.URL;
-import java.util.ArrayList;
 
-
 /**
  * Generate "more like this" similarity queries. 
  * Based on this mail:
@@ -745,14 +746,15 @@
         o.println();
         IndexSearcher searcher = new IndexSearcher(dir, true);
 
-        Hits hits = searcher.search(query);
-        int len = hits.length();
+        TopDocs hits = searcher.search(query, null, 25);
+        int len = hits.totalHits;
         o.println("found: " + len + " documents matching");
         o.println();
+        ScoreDoc[] scoreDocs = hits.scoreDocs;
         for (int i = 0; i < Math.min(25, len); i++) {
-            Document d = hits.doc(i);
+            Document d = searcher.doc(scoreDocs[i].doc);
 			String summary = d.get( "summary");
-            o.println("score  : " + hits.score(i));
+            o.println("score  : " + scoreDocs[i].score);
             o.println("url    : " + d.get("url"));
             o.println("\ttitle  : " + d.get("title"));
 			if ( summary != null)
