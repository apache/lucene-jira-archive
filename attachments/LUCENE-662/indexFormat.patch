Index: src/test/org/apache/lucene/store/IndexInputTest.java
===================================================================
--- src/test/org/apache/lucene/store/IndexInputTest.java	(révision 0)
+++ src/test/org/apache/lucene/store/IndexInputTest.java	(révision 0)
@@ -0,0 +1,121 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.store;
+
+import junit.framework.TestCase;
+
+public class IndexInputTest extends TestCase {
+
+  public void testInt() throws Exception {
+    genericTestInt(0);
+    genericTestInt(1);
+    genericTestInt(-1);
+    genericTestInt(Integer.MAX_VALUE);
+    genericTestInt(Integer.MIN_VALUE);
+  }
+
+  public void testVInt() throws Exception {
+    genericTestVInt(0);
+    genericTestVInt(1);
+    genericTestVInt(-1);
+    genericTestVInt(Integer.MAX_VALUE);
+    genericTestVInt(Integer.MIN_VALUE);
+  }
+
+  public void testLong() throws Exception {
+    genericTestLong(0);
+    genericTestLong(1);
+    genericTestLong(-1);
+    genericTestLong(Long.MAX_VALUE);
+    genericTestLong(Long.MIN_VALUE);
+  }
+
+  public void testVLong() throws Exception {
+    genericTestVLong(0);
+    genericTestVLong(1);
+    genericTestVLong(-1);
+    genericTestVLong(Long.MAX_VALUE);
+    genericTestVLong(Long.MIN_VALUE);
+  }
+
+  public void testString() throws Exception {
+    genericTestString("");
+    genericTestString("a");
+    genericTestString("GiyNNKHhnivNKKHgcNiCniCH716534912��_��'-(��(_����-��$*��!:;,!:;,");
+  }
+
+  private void genericTestInt(int i) throws Exception {
+    RAMFile fileA = new RAMFile();
+    RAMFile fileB = new RAMFile();
+    RAMOutputStream outA = new RAMOutputStream(fileA);
+    outA.writeInt(i);
+    outA.close();
+    RAMOutputStream outB = new RAMOutputStream(fileB);
+    outB.writeInt(new RAMInputStream(fileA));
+    outB.close();
+    assertEquals(i, new RAMInputStream(fileB).readInt());
+  }
+
+  private void genericTestVInt(int i) throws Exception {
+    RAMFile fileA = new RAMFile();
+    RAMFile fileB = new RAMFile();
+    RAMOutputStream outA = new RAMOutputStream(fileA);
+    outA.writeVInt(i);
+    outA.close();
+    RAMOutputStream outB = new RAMOutputStream(fileB);
+    outB.writeVInt(new RAMInputStream(fileA));
+    outB.close();
+    assertEquals(i, new RAMInputStream(fileB).readVInt());
+  }
+
+  private void genericTestLong(long l) throws Exception {
+    RAMFile fileA = new RAMFile();
+    RAMFile fileB = new RAMFile();
+    RAMOutputStream outA = new RAMOutputStream(fileA);
+    outA.writeLong(l);
+    outA.close();
+    RAMOutputStream outB = new RAMOutputStream(fileB);
+    outB.writeLong(new RAMInputStream(fileA));
+    outB.close();
+    assertEquals(l, new RAMInputStream(fileB).readLong());
+  }
+
+  private void genericTestVLong(long l) throws Exception {
+    RAMFile fileA = new RAMFile();
+    RAMFile fileB = new RAMFile();
+    RAMOutputStream outA = new RAMOutputStream(fileA);
+    outA.writeVLong(l);
+    outA.close();
+    RAMOutputStream outB = new RAMOutputStream(fileB);
+    outB.writeVLong(new RAMInputStream(fileA));
+    outB.close();
+    assertEquals(l, new RAMInputStream(fileB).readVLong());
+  }
+
+  private void genericTestString(String s) throws Exception {
+    RAMFile fileA = new RAMFile();
+    RAMFile fileB = new RAMFile();
+    RAMOutputStream outA = new RAMOutputStream(fileA);
+    outA.writeString(s);
+    outA.close();
+    RAMOutputStream outB = new RAMOutputStream(fileB);
+    outB.writeString(new RAMInputStream(fileA));
+    outB.close();
+    assertEquals(s, new RAMInputStream(fileB).readString());
+  }
+}
Index: src/test/org/apache/lucene/index/DocHelper.java
===================================================================
--- src/test/org/apache/lucene/index/DocHelper.java	(révision 524580)
+++ src/test/org/apache/lucene/index/DocHelper.java	(copie de travail)
@@ -30,7 +30,7 @@
 import org.apache.lucene.search.Similarity;
 import org.apache.lucene.store.Directory;
 
-class DocHelper {
+public class DocHelper {
   public static final String FIELD_1_TEXT = "field one text";
   public static final String TEXT_FIELD_1_KEY = "textField1";
   public static Field textField1 = new Field(TEXT_FIELD_1_KEY, FIELD_1_TEXT,
Index: src/test/org/apache/lucene/index/TestIndexFormat.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexFormat.java	(révision 0)
+++ src/test/org/apache/lucene/index/TestIndexFormat.java	(révision 0)
@@ -0,0 +1,211 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.index;
+
+import junit.framework.TestCase;
+
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMDirectory;
+
+/**
+ * Tests related to the {@link IndexFormat} management
+ * 
+ * $Id$
+ */
+public class TestIndexFormat extends TestCase {
+
+  public void testSetIndexFmt() throws Exception {
+    RAMDirectory dir = new RAMDirectory();
+    try {
+      dir.setIndexFormat(new FakeIndexFmt());
+      fail("We must not be able to set the index format twice");
+    } catch (IllegalStateException e) {
+      // catched : OK
+    }
+  }
+
+  public void testWriteIndexFmt() throws Exception {
+    RAMDirectory dir = new RAMDirectory(new DefaultIndexFormat());
+    SegmentInfos infos = new SegmentInfos();
+    infos.writeIndexFormat(dir);
+    assertEquals(DefaultIndexFormat.ID, infos.readIndexFormat(dir));
+
+    dir = new RAMDirectory(new FakeIndexFmt());
+    infos = new SegmentInfos();
+    infos.writeIndexFormat(dir);
+    assertEquals(FakeIndexFmt.ID, infos.readIndexFormat(dir));
+  }
+
+  public void testInit() throws Exception {
+    RAMDirectory dir = new RAMDirectory(new DefaultIndexFormat());
+    new IndexWriter(dir, new StandardAnalyzer(), true).close();
+    SegmentInfos infos = new SegmentInfos();
+    assertEquals(DefaultIndexFormat.ID, infos.readIndexFormat(dir));
+    infos.checkIndexFormat(dir);
+  }
+
+  public void testConflictIndexFmt() throws Exception {
+    RAMDirectory dir = new RAMDirectory(new DefaultIndexFormat());
+    new IndexWriter(dir, new StandardAnalyzer(), true).close();
+    //simulate another index format
+    if (dir.fileExists(IndexFileNames.INDEXFORMAT)) {
+      dir.deleteFile(IndexFileNames.INDEXFORMAT);
+    }
+    IndexOutput out = dir.createOutput(IndexFileNames.INDEXFORMAT);
+    out.writeString(FakeIndexFmt.ID);
+    out.close();
+
+    SegmentInfos infos = new SegmentInfos();
+    assertEquals(FakeIndexFmt.ID, infos.readIndexFormat(dir));
+    try {
+      infos.checkIndexFormat(dir);
+      fail("checking the index format must fail if the index format is not compatible");
+    } catch (IllegalStateException e) {
+      // catched  : OK
+    }
+  }
+
+  public void testExtendingIndexFmt() throws Exception {
+    RAMDirectory dir = new RAMDirectory(new ExtendDefaultIndexFmt());
+    new IndexWriter(dir, new StandardAnalyzer(), true).close();
+    //simulate another index format
+    if (dir.fileExists(IndexFileNames.INDEXFORMAT)) {
+      dir.deleteFile(IndexFileNames.INDEXFORMAT);
+    }
+    IndexOutput out = dir.createOutput(IndexFileNames.INDEXFORMAT);
+    out.writeString(DefaultIndexFormat.ID);
+    out.close();
+
+    SegmentInfos infos = new SegmentInfos();
+    assertEquals(DefaultIndexFormat.ID, infos.readIndexFormat(dir));
+    infos.checkIndexFormat(dir); // ExtendDefaultIndexFmt extends DefaultIndexFormat : should be OK
+
+    //now check the reverse : must fail
+    dir = new RAMDirectory(new DefaultIndexFormat());
+    new IndexWriter(dir, new StandardAnalyzer(), true).close();
+    //simulate another index format
+    if (dir.fileExists(IndexFileNames.INDEXFORMAT)) {
+      dir.deleteFile(IndexFileNames.INDEXFORMAT);
+    }
+    out = dir.createOutput(IndexFileNames.INDEXFORMAT);
+    out.writeString("org.apache.lucene.index.TestIndexFormat.ExtendDefaultIndexFmt");
+    out.close();
+
+    infos = new SegmentInfos();
+    assertEquals("org.apache.lucene.index.TestIndexFormat.ExtendDefaultIndexFmt", infos.readIndexFormat(dir));
+    try {
+      infos.checkIndexFormat(dir);
+      fail("checking the index format must fail if the index format is not compatible");
+    } catch (IllegalStateException e) {
+      // catched  : OK
+    }
+  }
+
+  public void testMerge() throws Exception {
+    RAMDirectory dir = new RAMDirectory(new DefaultIndexFormat());
+    new IndexWriter(dir, new StandardAnalyzer(), true).close();
+
+    RAMDirectory dirDefault = new RAMDirectory(new DefaultIndexFormat());
+    new IndexWriter(dirDefault, new StandardAnalyzer(), true).close();
+
+    RAMDirectory dirFake = new RAMDirectory(new FakeIndexFmt());
+    new IndexWriter(dirFake, new StandardAnalyzer(), true).close();
+
+    RAMDirectory dirExtend = new RAMDirectory(new ExtendDefaultIndexFmt());
+    new IndexWriter(dirExtend, new StandardAnalyzer(), true).close();
+
+    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), false);
+    writer.addIndexes(new Directory[] { dirDefault });
+    writer.close();
+
+    writer = new IndexWriter(dir, new StandardAnalyzer(), false);
+    try {
+      writer.addIndexes(new Directory[] { dirFake });
+      fail("merging incomaptible directory must fail");
+    } catch (IncompatibleFormatException e) {
+      //catched : OK
+    }
+    writer.close();
+
+    writer = new IndexWriter(dirExtend, new StandardAnalyzer(), false);
+    writer.addIndexes(new Directory[] { dir });
+    writer.close();
+
+    //now test with readers
+
+    writer = new IndexWriter(dir, new StandardAnalyzer(), false);
+    writer.addIndexes(new IndexReader[] { IndexReader.open(dirDefault) });
+    writer.close();
+
+    writer = new IndexWriter(dir, new StandardAnalyzer(), false);
+    try {
+      writer.addIndexes(new IndexReader[] { IndexReader.open(dirFake) });
+      fail("merging incomaptible directory must fail");
+    } catch (IncompatibleFormatException e) {
+      //catched : OK
+    }
+    writer.close();
+
+    writer = new IndexWriter(dirExtend, new StandardAnalyzer(), false);
+    writer.addIndexes(new IndexReader[] { IndexReader.open(dir) });
+    writer.close();
+
+    //now test without optimization
+
+    writer = new IndexWriter(dir, new StandardAnalyzer(), false);
+    writer.addIndexesNoOptimize(new Directory[] { dirDefault });
+    writer.close();
+
+    writer = new IndexWriter(dir, new StandardAnalyzer(), false);
+    try {
+      writer.addIndexesNoOptimize(new Directory[] { dirFake });
+      fail("merging incomaptible directory must fail");
+    } catch (IncompatibleFormatException e) {
+      //catched : OK
+    }
+    writer.close();
+
+    writer = new IndexWriter(dirExtend, new StandardAnalyzer(), false);
+    writer.addIndexesNoOptimize(new Directory[] { dir });
+    writer.close();
+  }
+
+  class FakeIndexFmt extends DefaultIndexFormat {
+
+    public static final String ID = "This is fake !!";
+
+    public boolean canRead(String indexFmtName) {
+      return getID().equals(indexFmtName);
+    }
+
+    public String getID() {
+      return ID;
+    }
+
+  }
+
+  class ExtendDefaultIndexFmt extends DefaultIndexFormat {
+
+    public boolean canRead(String indexFmtName) {
+      return getID().equals(indexFmtName) || DefaultIndexFormat.ID.equals(indexFmtName);
+    }
+
+  }
+}

Modification de propriétés sur src/test/org/apache/lucene/index/TestIndexFormat.java
___________________________________________________________________
Nom : svn:keywords
   + Date Revision Author HeadURL Id
Nom : svn:eol-style
   + native

Index: src/test/org/apache/lucene/index/TestFieldsReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestFieldsReader.java	(révision 524580)
+++ src/test/org/apache/lucene/index/TestFieldsReader.java	(copie de travail)
@@ -52,7 +52,7 @@
   public void test() throws IOException {
     assertTrue(dir != null);
     assertTrue(fieldInfos != null);
-    FieldsReader reader = new FieldsReader(dir, "test", fieldInfos);
+    FieldsReader reader = dir.getIndexFormat().getFieldsReader(dir, "test", fieldInfos);
     assertTrue(reader != null);
     assertTrue(reader.size() == 1);
     Document doc = reader.doc(0, null);
@@ -82,7 +82,7 @@
   public void testLazyFields() throws Exception {
     assertTrue(dir != null);
     assertTrue(fieldInfos != null);
-    FieldsReader reader = new FieldsReader(dir, "test", fieldInfos);
+    FieldsReader reader = dir.getIndexFormat().getFieldsReader(dir, "test", fieldInfos);
     assertTrue(reader != null);
     assertTrue(reader.size() == 1);
     Set loadFieldNames = new HashSet();
@@ -137,7 +137,7 @@
   public void testLazyFieldsAfterClose() throws Exception {
     assertTrue(dir != null);
     assertTrue(fieldInfos != null);
-    FieldsReader reader = new FieldsReader(dir, "test", fieldInfos);
+    FieldsReader reader = dir.getIndexFormat().getFieldsReader(dir, "test", fieldInfos);
     assertTrue(reader != null);
     assertTrue(reader.size() == 1);
     Set loadFieldNames = new HashSet();
@@ -167,7 +167,7 @@
   public void testLoadFirst() throws Exception {
     assertTrue(dir != null);
     assertTrue(fieldInfos != null);
-    FieldsReader reader = new FieldsReader(dir, "test", fieldInfos);
+    FieldsReader reader = dir.getIndexFormat().getFieldsReader(dir, "test", fieldInfos);
     assertTrue(reader != null);
     assertTrue(reader.size() == 1);
     LoadFirstFieldSelector fieldSelector = new LoadFirstFieldSelector();
@@ -214,7 +214,7 @@
     SetBasedFieldSelector fieldSelector = new SetBasedFieldSelector(Collections.EMPTY_SET, lazyFieldNames);
 
     for (int i = 0; i < length; i++) {
-      reader = new FieldsReader(tmpDir, "test", fieldInfos);
+      reader = tmpDir.getIndexFormat().getFieldsReader(tmpDir, "test", fieldInfos);
       assertTrue(reader != null);
       assertTrue(reader.size() == 1);
 
@@ -238,7 +238,7 @@
       doc = null;
       //Hmmm, are we still in cache???
       System.gc();
-      reader = new FieldsReader(tmpDir, "test", fieldInfos);
+      reader = tmpDir.getIndexFormat().getFieldsReader(tmpDir, "test", fieldInfos);
       doc = reader.doc(0, fieldSelector);
       field = doc.getFieldable(DocHelper.LARGE_LAZY_FIELD_KEY);
       assertTrue("field is not lazy", field.isLazy() == true);
@@ -256,7 +256,7 @@
   }
   
   public void testLoadSize() throws IOException {
-    FieldsReader reader = new FieldsReader(dir, "test", fieldInfos);
+    FieldsReader reader = dir.getIndexFormat().getFieldsReader(dir, "test", fieldInfos);
     Document doc;
     
     doc = reader.doc(0, new FieldSelector(){
Index: src/test/org/apache/lucene/index/TestSegmentTermDocs.java
===================================================================
--- src/test/org/apache/lucene/index/TestSegmentTermDocs.java	(révision 524580)
+++ src/test/org/apache/lucene/index/TestSegmentTermDocs.java	(copie de travail)
@@ -17,15 +17,17 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+
 import junit.framework.TestCase;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.store.Directory;
+
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.RAMDirectory;
 
-import java.io.IOException;
-
 public class TestSegmentTermDocs extends TestCase {
   private Document testDoc = new Document();
   private Directory dir = new RAMDirectory();
@@ -50,9 +52,10 @@
   
   public void testTermDocs() throws IOException {
     //After adding the document, we should be able to read it back in
-    SegmentReader reader = SegmentReader.get(new SegmentInfo("test", 1, dir));
-    assertTrue(reader != null);
-    SegmentTermDocs segTermDocs = new SegmentTermDocs(reader);
+    IndexInput freqStrem = dir.openInput("test.frq");
+    FieldInfos fieldInfos = new FieldInfos(dir, "test.fnm");
+    TermInfosReader tis = new TermInfosReader(dir, "test", fieldInfos);
+    SegmentTermDocs segTermDocs = new SegmentTermDocs(freqStrem, tis, null, fieldInfos);
     assertTrue(segTermDocs != null);
     segTermDocs.seek(new Term(DocHelper.TEXT_FIELD_2_KEY, "field"));
     if (segTermDocs.next() == true)
@@ -62,29 +65,34 @@
       int freq = segTermDocs.freq();
       assertTrue(freq == 3);  
     }
-    reader.close();
+    tis.close();
+    segTermDocs.close();
   }  
   
   public void testBadSeek() throws IOException {
     {
       //After adding the document, we should be able to read it back in
-      SegmentReader reader = SegmentReader.get(new SegmentInfo("test", 1, dir));
-      assertTrue(reader != null);
-      SegmentTermDocs segTermDocs = new SegmentTermDocs(reader);
+      IndexInput freqStrem = dir.openInput("test.frq");
+      FieldInfos fieldInfos = new FieldInfos(dir, "test.fnm");
+      TermInfosReader tis = new TermInfosReader(dir, "test", fieldInfos);
+      SegmentTermDocs segTermDocs = new SegmentTermDocs(freqStrem, tis, null, fieldInfos);
       assertTrue(segTermDocs != null);
       segTermDocs.seek(new Term("textField2", "bad"));
       assertTrue(segTermDocs.next() == false);
-      reader.close();
+      tis.close();
+      segTermDocs.close();
     }
     {
       //After adding the document, we should be able to read it back in
-      SegmentReader reader = SegmentReader.get(new SegmentInfo("test", 1, dir));
-      assertTrue(reader != null);
-      SegmentTermDocs segTermDocs = new SegmentTermDocs(reader);
+      IndexInput freqStrem = dir.openInput("test.frq");
+      FieldInfos fieldInfos = new FieldInfos(dir, "test.fnm");
+      TermInfosReader tis = new TermInfosReader(dir, "test", fieldInfos);
+      SegmentTermDocs segTermDocs = new SegmentTermDocs(freqStrem, tis, null, fieldInfos);
       assertTrue(segTermDocs != null);
       segTermDocs.seek(new Term("junk", "bad"));
       assertTrue(segTermDocs.next() == false);
-      reader.close();
+      tis.close();
+      segTermDocs.close();
     }
   }
   
Index: src/test/org/apache/lucene/index/TestLazyProxSkipping.java
===================================================================
--- src/test/org/apache/lucene/index/TestLazyProxSkipping.java	(révision 524580)
+++ src/test/org/apache/lucene/index/TestLazyProxSkipping.java	(copie de travail)
@@ -29,6 +29,7 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.BitVector;
 
 import junit.framework.TestCase;
 
@@ -48,7 +49,18 @@
     private void createIndex(int numHits) throws IOException {
         int numDocs = 500;
         
-        Directory directory = new RAMDirectory();
+        Directory directory = new RAMDirectory(new DefaultIndexFormat() {
+        
+          public PostingReader getPostingReader(Directory d, String segment) throws IOException {
+            return new DefaultPostingReader(d, segment) {
+              public DefaultPostingReader changeProxStream() {
+                // we decorate the proxStream with a wrapper class that allows to count the number of calls of seek()
+                proxStream = new SeeksCountingStream(proxStream);
+                return this;
+              }
+            }.changeProxStream();
+          }
+        });
         IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true);
         
         for (int i = 0; i < numDocs; i++) {
@@ -76,9 +88,6 @@
         // the index is a single segment, thus IndexReader.open() returns an instance of SegmentReader
         SegmentReader reader = (SegmentReader) IndexReader.open(directory);
 
-        // we decorate the proxStream with a wrapper class that allows to count the number of calls of seek()
-        reader.proxStream = new SeeksCountingStream(reader.proxStream);
-        
         this.searcher = new IndexSearcher(reader);        
     }
     
Index: src/java/org/apache/lucene/index/FieldInfo.java
===================================================================
--- src/java/org/apache/lucene/index/FieldInfo.java	(révision 524580)
+++ src/java/org/apache/lucene/index/FieldInfo.java	(copie de travail)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-final class FieldInfo {
+final public class FieldInfo {
   String name;
   boolean isIndexed;
   int number;
@@ -43,4 +43,28 @@
     this.omitNorms = omitNorms;
     this.storePayloads = storePayloads;
   }
+
+  public String getName() {
+    return name;
+  }
+
+  public boolean storeTermVector() {
+    return storeTermVector;
+  }
+
+  public boolean storeOffsetWithTermVector() {
+    return storeOffsetWithTermVector;
+  }
+
+  public boolean storePositionWithTermVector() {
+    return storePositionWithTermVector;
+  }
+
+  public boolean omitNorms() {
+    return omitNorms;
+  }
+
+  public boolean isIndexed() {
+    return isIndexed;
+  }
 }
Index: src/java/org/apache/lucene/index/PostingReader.java
===================================================================
--- src/java/org/apache/lucene/index/PostingReader.java	(révision 0)
+++ src/java/org/apache/lucene/index/PostingReader.java	(révision 0)
@@ -0,0 +1,38 @@
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.index;
+
+import java.io.IOException;
+
+import org.apache.lucene.util.BitVector;
+
+/**
+ * The reader of postings
+ * 
+ * $Id$
+ */
+public interface PostingReader {
+
+  public void close() throws IOException ;
+
+  public TermDocs termDocs(BitVector deletedDocs, TermInfosReader tis, FieldInfos fieldInfos) throws IOException;
+
+  public TermPositions termPositions(BitVector deletedDocs, TermInfosReader tis, FieldInfos fieldInfos) throws IOException;
+
+}

Modification de propriétés sur src/java/org/apache/lucene/index/PostingReader.java
___________________________________________________________________
Nom : svn:mime-type
   + text/plain
Nom : svn:keywords
   + Date Revision Author HeadURL Id
Nom : svn:eol-style
   + native

Index: src/java/org/apache/lucene/index/CompoundFileReader.java
===================================================================
--- src/java/org/apache/lucene/index/CompoundFileReader.java	(révision 524580)
+++ src/java/org/apache/lucene/index/CompoundFileReader.java	(copie de travail)
@@ -35,7 +35,7 @@
  * @author Dmitry Serebrennikov
  * @version $Id$
  */
-class CompoundFileReader extends Directory {
+public class CompoundFileReader extends Directory {
 
     private static final class FileEntry {
         long offset;
@@ -55,6 +55,7 @@
     throws IOException
     {
         directory = dir;
+        setIndexFormat(dir.getIndexFormat());
         fileName = name;
 
         boolean success = false;
Index: src/java/org/apache/lucene/index/DefaultFieldsWriter.java
===================================================================
--- src/java/org/apache/lucene/index/DefaultFieldsWriter.java	(révision 0)
+++ src/java/org/apache/lucene/index/DefaultFieldsWriter.java	(révision 0)
@@ -0,0 +1,152 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.index;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.util.zip.Deflater;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexOutput;
+
+/**
+ * The default implementation of FieldsWriter
+ * 
+ * $Id$
+ */
+public class DefaultFieldsWriter extends FieldsWriter {
+
+  protected DefaultFieldsWriter(Directory d, String segment, FieldInfos fn) throws IOException {
+    super(d, segment, fn);
+  }
+
+  /**
+   * There no data stored at the document level
+   */
+  protected void writeDocumentData(IndexOutput out, Document doc) throws IOException {
+    //nothing to write
+  }
+
+  /**
+   * If a the field to write has been load lazily, it does a direct copy from the
+   * source to the output.
+   */
+  protected void writeFieldData(FieldData fieldData, IndexOutput out) throws IOException {
+    if (fieldData.isLazy() && isBinaryCompatible(fieldData)) {
+      fieldData.writeFromLazyLoading(out);
+    } else {
+      byte bits = 0;
+      if (fieldData.isTokenized())
+        bits |= DefaultFieldData.FIELD_IS_TOKENIZED;
+      if (fieldData.isBinary())
+        bits |= DefaultFieldData.FIELD_IS_BINARY;
+      if (fieldData instanceof DefaultFieldData && ((DefaultFieldData) fieldData).isCompressed()) {
+        bits |= DefaultFieldData.FIELD_IS_COMPRESSED;
+      }
+
+      out.writeByte(bits);
+
+      if (fieldData instanceof DefaultFieldData && ((DefaultFieldData) fieldData).isCompressed()) {
+        // compression is enabled for the current field
+        byte[] bdata = null;
+        // check if it is a binary field
+        if (fieldData.isBinary()) {
+          bdata = compress(fieldData.binaryValue());
+        } else {
+          bdata = compress(fieldData.stringValue().getBytes("UTF-8"));
+        }
+        final int len = bdata.length;
+        out.writeVInt(len);
+        out.writeBytes(bdata, len);
+      } else {
+        // compression is disabled for the current field
+        if (fieldData.isBinary()) {
+          byte[] bdata = fieldData.binaryValue();
+          final int len = bdata.length;
+          out.writeVInt(len);
+          out.writeBytes(bdata, len);
+        } else {
+          out.writeString(fieldData.stringValue());
+        }
+      }
+    }
+  }
+
+  /**
+   * Test if the specified field is binary compatible with the current format, so
+   * it allow us to do a direct copy from the lazy loaded field into an index
+   * 
+   * @param field the field to test
+   * @return true if it is compatible
+   */
+  protected boolean isBinaryCompatible(FieldData field) {
+    return field instanceof DefaultFieldData;
+  }
+
+  /**
+   * To be overriden by subclasses to choose a different level of compression
+   * 
+   * @return the compression level
+   */
+  protected int getCompressionLevel() {
+    return Deflater.BEST_COMPRESSION;
+  }
+
+  /**
+   * Do the compression of data
+   * 
+   * To be overiden by subclasses to use a different format of compression. If overriden, you
+   * probably should also override isBinaryCompatible and and decompress function of
+   * DefaultFieldsReader.
+   * 
+   * @param input the data to compress
+   * @return the compressed data
+   */
+  protected byte[] compress(byte[] input) {
+
+    // Create the compressor with highest level of compression
+    Deflater compressor = new Deflater();
+    compressor.setLevel(getCompressionLevel());
+
+    // Give the compressor the data to compress
+    compressor.setInput(input);
+    compressor.finish();
+
+    /*
+     * Create an expandable byte array to hold the compressed data.
+     * You cannot use an array that's the same size as the orginal because
+     * there is no guarantee that the compressed data will be smaller than
+     * the uncompressed data.
+     */
+    ByteArrayOutputStream bos = new ByteArrayOutputStream(input.length);
+
+    // Compress the data
+    byte[] buf = new byte[1024];
+    while (!compressor.finished()) {
+      int count = compressor.deflate(buf);
+      bos.write(buf, 0, count);
+    }
+
+    compressor.end();
+
+    // Get the compressed data
+    return bos.toByteArray();
+  }
+
+}

Modification de propriétés sur src/java/org/apache/lucene/index/DefaultFieldsWriter.java
___________________________________________________________________
Nom : svn:mime-type
   + text/plain
Nom : svn:keywords
   + Date Revision Author HeadURL Id
Nom : svn:eol-style
   + native

Index: src/java/org/apache/lucene/index/DefaultPostingReader.java
===================================================================
--- src/java/org/apache/lucene/index/DefaultPostingReader.java	(révision 0)
+++ src/java/org/apache/lucene/index/DefaultPostingReader.java	(révision 0)
@@ -0,0 +1,59 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.index;
+
+import java.io.IOException;
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BitVector;
+
+/**
+ * The default implementation of the {@link PostingReader}
+ * 
+ * $Id$
+ */
+public class DefaultPostingReader implements PostingReader {
+
+  protected IndexInput freqStream;
+
+  protected IndexInput proxStream;
+
+  public DefaultPostingReader(Directory dir, String segment) throws IOException {
+    freqStream = dir.openInput(segment + ".frq");
+    proxStream = dir.openInput(segment + ".prx");
+  }
+
+  public void close() throws IOException {
+    if (freqStream != null) {
+      freqStream.close();
+    }
+    if (proxStream != null) {
+      proxStream.close();
+    }
+  }
+
+  public TermDocs termDocs(BitVector deletedDocs, TermInfosReader tis, FieldInfos fieldInfos) throws IOException {
+    return new SegmentTermDocs((IndexInput) freqStream.clone(), tis, deletedDocs, fieldInfos);
+  }
+
+  public TermPositions termPositions(BitVector deletedDocs, TermInfosReader tis, FieldInfos fieldInfos) throws IOException {
+    return new SegmentTermPositions((IndexInput) freqStream.clone(), (IndexInput) proxStream.clone(), tis, deletedDocs, fieldInfos);
+  }
+
+}

Modification de propriétés sur src/java/org/apache/lucene/index/DefaultPostingReader.java
___________________________________________________________________
Nom : svn:mime-type
   + text/plain
Nom : svn:keywords
   + Date Revision Author HeadURL Id
Nom : svn:eol-style
   + native

Index: src/java/org/apache/lucene/index/FieldData.java
===================================================================
--- src/java/org/apache/lucene/index/FieldData.java	(révision 0)
+++ src/java/org/apache/lucene/index/FieldData.java	(révision 0)
@@ -0,0 +1,254 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.Reader;
+import java.io.Serializable;
+
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+
+/**
+ * The data of field
+ */
+public abstract class FieldData implements Serializable {
+
+  private boolean isBinary = false;
+
+  private boolean isTokenized = false;
+
+  // the one and only data object for all different kind of field values    
+  private Object fieldData = null;
+
+  private boolean isLazy = false;
+
+  private IndexInput fieldsStream;
+
+  private long pointer;
+
+  private long toRead;
+
+  /**
+   * This contructor should only be used when retreiving data form an index
+   * 
+   */
+  protected FieldData() {
+    // nothing to initialized yet. The properties should be be via #setLazyData() and then
+    // by #readLazyData()
+  }
+
+  protected FieldData(String text) {
+    this((Object) text);
+    isBinary = false;
+  }
+
+  protected FieldData(byte[] data) {
+    this((Object) data);
+    isBinary = true;
+  }
+
+  protected FieldData(Reader reader) {
+    this((Object) reader);
+    isBinary = false;
+  }
+
+  private FieldData(Object data) {
+    if (data == null)
+      throw new NullPointerException("data cannot be null");
+
+    fieldData = data;
+  }
+
+  /**
+   * @return true iff the value of the filed is stored as binary
+   */
+  public final boolean isBinary() {
+    return isBinary;
+  }
+
+  protected final void setBinary(boolean isBinary) {
+    this.isBinary = isBinary;
+  }
+
+  /**
+   * @return true iff the value of the filed is stored as binary
+   */
+  public final boolean isTokenized() {
+    return isTokenized;
+  }
+
+  /**
+   * Set the tokenization status of the field data
+   * 
+   * @param isTokenized
+   */
+  public final void setTokenized(boolean isTokenized) {
+    this.isTokenized = isTokenized;
+  }
+
+  /**
+   * Indicates whether a Field is Lazy or not.  The semantics of Lazy loading are such that if a Field is lazily loaded, retrieving
+   * it's values via {@link #stringValue()} or {@link #binaryValue()} is only valid as long as the {@link org.apache.lucene.index.IndexReader} that
+   * retrieved the {@link Document} is still open.
+   *  
+   * @return true if this field can be loaded lazily
+   */
+  public boolean isLazy() {
+    return isLazy;
+  }
+
+  /**
+   * The value of the field as a {@link String}, or <code>null</code>. If <code>null</code>,
+   * the {@link Reader} value or binary value is used. Exactly one of {@link #stringValue()},
+   * {@link #readerValue()}, and {@link #binaryValue()} must be set.
+   * 
+   * @return the string value fo the field
+   */
+  public final String stringValue() {
+    if (isLazy && fieldData == null) {
+      readLazyData();
+    }
+    return fieldData instanceof String ? (String) fieldData : null;
+  }
+
+  /**
+   * The value of the field as a {@link Reader}, or <code>null</code>. If <code>null</code>,
+   * the {@link String} value or binary value is used. Exactly one of {@link #stringValue()},
+   * {@link #readerValue()}, and {@link #binaryValue()} must be set.
+   * 
+   * @return the reader value
+   */
+  public final Reader readerValue() {
+    if (isLazy && fieldData == null) {
+      readLazyData();
+    }
+    return fieldData instanceof Reader ? (Reader) fieldData : null;
+  }
+
+  /**
+   * The value of the field in Binary, or <code>null</code>.  If <code>null</code>, the
+   * {@link Reader} or {@link String} value is used.  Exactly one of {@link #stringValue()},
+   * {@link #readerValue()} and {@link #binaryValue()} must be set.
+   * 
+   * @return the binary value
+   */
+  public final byte[] binaryValue() {
+    if (isLazy && fieldData == null) {
+      readLazyData();
+    }
+    return fieldData instanceof byte[] ? (byte[]) fieldData : null;
+  }
+
+  /**
+   * 
+   * @param fieldData the new data of the field
+   */
+  protected void setData(Object fieldData) {
+    this.fieldData = fieldData;
+  }
+
+  /**
+   * 
+   * @return the data of the field
+   */
+  protected Object getData() {
+    return fieldData;
+  }
+
+  /**
+   * Load the field data from the stream
+   * 
+   * @param in the stream to read
+   * @param skip if the data have to be stored, or just skipped from the stream
+   * @throws IOException
+   */
+  public abstract void readStream(IndexInput in, boolean skip) throws CorruptIndexException, IOException;
+
+  private final void readLazyData() {
+    try {
+      fieldsStream.seek(pointer);
+      readStream(fieldsStream, false);
+    } catch (IOException e) {
+      throw new FieldReaderException(e);
+    } finally {
+      try {
+        fieldsStream.close();
+      } catch (IOException e) {
+        throw new FieldReaderException(e);
+      } finally {
+        fieldsStream = null;
+      }
+    }
+  }
+
+  /**
+   * Set this field as lazy loaded, and save the stream status
+   * 
+   * @param fieldsStream the field stream
+   * @param pointer the pointer of the field data
+   * @param toRead the number of byte of the field data
+   */
+  final void setLazyData(IndexInput fieldsStream, long pointer, long toRead) {
+    isLazy = true;
+    this.fieldsStream = fieldsStream;
+    this.pointer = pointer;
+    this.toRead = toRead;
+  }
+
+  /**
+   * If the data was loaded lazily, close the kept opened stream. This should be used 
+   * 
+   * @throws IOException
+   */
+  public void close() throws IOException {
+    if (fieldsStream != null) {
+      fieldsStream.close();
+    }
+  }
+
+  /**
+   * Write the lazy loaded field data directly in the specified output stream.
+   * If the field has not been loaded lazily, it throws an UnsupportedOperationException.
+   * 
+   * @param out the stream to write in
+   * @throws IOException in case of write error
+   */
+  public void writeFromLazyLoading(IndexOutput out) throws IOException {
+    if (!isLazy) {
+      throw new UnsupportedOperationException("The field have to be load lazily to copy it directly");
+    }
+    try {
+      fieldsStream.seek(pointer);
+      out.writeBytes(fieldsStream, toRead);
+    } finally {
+      try {
+        fieldsStream.close();
+      } finally {
+        fieldsStream = null;
+      }
+    }
+  }
+
+  public String toString() {
+    if (isLazy) {
+      return null;
+    }
+    return stringValue();
+  }
+}
Index: src/java/org/apache/lucene/index/SegmentTermEnum.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentTermEnum.java	(révision 524580)
+++ src/java/org/apache/lucene/index/SegmentTermEnum.java	(copie de travail)
@@ -20,7 +20,7 @@
 import java.io.IOException;
 import org.apache.lucene.store.IndexInput;
 
-final class SegmentTermEnum extends TermEnum implements Cloneable {
+public final class SegmentTermEnum extends TermEnum implements Cloneable {
   private IndexInput input;
   FieldInfos fieldInfos;
   long size;
@@ -30,7 +30,7 @@
   private TermBuffer prevBuffer = new TermBuffer();
   private TermBuffer scratch;                     // used for scanning
 
-  private TermInfo termInfo = new TermInfo();
+  private TermInfo termInfo;
 
   private int format;
   private boolean isIndex = false;
@@ -73,10 +73,15 @@
         // switch off skipTo optimization for file format prior to 1.4rc2 in order to avoid a bug in 
         // skipTo implementation of these versions
         skipInterval = Integer.MAX_VALUE;
-      }
-      else{
+        termInfo = new TermInfo(2);
+      } else {
         indexInterval = input.readInt();
         skipInterval = input.readInt();
+        if (format < -2) {
+          termInfo = new TermInfo(input.readInt());
+        } else {
+          termInfo = new TermInfo(2);
+        }
       }
     }
 
@@ -118,9 +123,11 @@
     termBuffer.read(input, fieldInfos);
 
     termInfo.docFreq = input.readVInt();	  // read doc freq
-    termInfo.freqPointer += input.readVLong();	  // read freq pointer
-    termInfo.proxPointer += input.readVLong();	  // read prox pointer
-    
+    long[] pointers = termInfo.getPointers();
+    for (int i = 0; i < pointers.length; i++) {
+      pointers[i] += input.readVLong();	  // read pointer
+    }
+
     if(format == -1){
     //  just read skipOffset in order to increment  file pointer; 
     // value is never used since skipTo is switched off
@@ -162,7 +169,7 @@
 
   /** Returns the current TermInfo in the enumeration.
    Initially invalid, valid after next() called for the first time.*/
-  final TermInfo termInfo() {
+  public final TermInfo termInfo() {
     return new TermInfo(termInfo);
   }
 
@@ -178,20 +185,12 @@
     return termInfo.docFreq;
   }
 
-  /* Returns the freqPointer from the current TermInfo in the enumeration.
-    Initially invalid, valid after next() called for the first time.*/
-  final long freqPointer() {
-    return termInfo.freqPointer;
-  }
-
-  /* Returns the proxPointer from the current TermInfo in the enumeration.
-    Initially invalid, valid after next() called for the first time.*/
-  final long proxPointer() {
-    return termInfo.proxPointer;
-  }
-
   /** Closes the enumeration to further activity, freeing resources. */
   public final void close() throws IOException {
     input.close();
   }
+
+  public FieldInfos getFieldInfos() {
+    return fieldInfos;
+  }
 }
Index: src/java/org/apache/lucene/index/IndexFormat.java
===================================================================
--- src/java/org/apache/lucene/index/IndexFormat.java	(révision 0)
+++ src/java/org/apache/lucene/index/IndexFormat.java	(révision 0)
@@ -0,0 +1,138 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.document.Fieldable;
+import org.apache.lucene.store.Directory;
+
+/**
+ * Specify the format of index.
+ * <p>
+ * The implementation of the {@link FieldsReader} and {@link FieldsWriter} returned by
+ * the function getFieldsReader and getFieldsWriter will specify how the data of fields are
+ * serialized, and also the kind of {@link Fieldable} used.
+ * <p>
+ * The implementation of the {@link PostingReader} and {@link PostingWriter} returned by
+ * the function getPostingReader and getPostingWriter will specify how the indexed data are
+ * serialized and retreived.
+ *
+ * $Id$
+ */
+public interface IndexFormat {
+
+  /**
+   * Get the ID of the index format. The ID must be unique, the use of the canonical
+   * class name is prefered.
+   * 
+   * @return the ID
+   */
+  String getID();
+
+  /**
+   * State if the current index format can read a segment from the other specified index format
+   * 
+   * @param indexFmtID the other index format ID 
+   * @return <code>true</code> if it can read it
+   */
+  boolean canRead(String indexFmtID);
+
+  /**
+   * This array contains all filename extensions used by
+   * Lucene's index files, with two exceptions, namely the
+   * extension made up from <code>.f</code> + a number and
+   * from <code>.s</code> + a number.  Also note that
+   * Lucene's <code>segments_N</code> files do not have any
+   * filename extension.
+   * 
+   * @return a List of String
+   */
+  Set getIndexExtensions();
+
+  /**
+   * File extensions that are added to a compound file
+   * (same as above, minus "del", "gen", "cfs").
+   * 
+   * @return a List of String
+   */
+  Set getIndexExtensionsInCounpoundFile();
+
+  /**
+   * File extensions of old-style index files
+   * 
+   * @return a List of String
+   */
+  Set getCompoundExtensions();
+
+  /**
+   * File extensions for term vector support
+   * 
+   * @return a List of String
+   */
+  Set getVectorExtensions();
+
+  /**
+   * Return an implementation of {@link FieldsReader} for this format
+   * 
+   * @param d the directory to use
+   * @param segment the segment name
+   * @param fn the infos on fields
+   * @return the implemetation of {@link FieldsReader}
+   * @throws IOException
+   */
+  FieldsReader getFieldsReader(Directory d, String segment, FieldInfos fn) throws IOException;
+
+  /**
+   * Return an implementation of {@link FieldsWriter} for this format
+   * 
+   * @param d the directory to use
+   * @param segment the segment name
+   * @param fn the infos on fields
+   * @return the implemetation of {@link FieldsWriter}
+   * @throws IOException
+   */
+  FieldsWriter getFieldsWriter(Directory d, String segment, FieldInfos fn) throws IOException;
+
+  /**
+   * Return an implementation of {@link PostingReader} for this format
+   * 
+   * @param d the directory to use
+   * @param segment the segment name
+   * @return the implemetation of {@link PostingReader}
+   * @throws IOException
+   */
+  public PostingReader getPostingReader(Directory d, String segment) throws IOException;
+
+  /**
+   * Return an implementation of {@link PostingWriter} for this format
+   * 
+   * @param d the directory to use
+   * @param segment the segment name
+   * @return the implemetation of {@link PostingWriter}
+   * @throws IOException
+   */
+  public PostingWriter getPostingWriter(Directory d, String segment) throws IOException;
+
+  /**
+   * 
+   * @return the index file name filter associated to this index format
+   */
+  IndexFileNameFilter getIndexFileNameFilter();
+}
Index: src/java/org/apache/lucene/index/PostingWriter.java
===================================================================
--- src/java/org/apache/lucene/index/PostingWriter.java	(révision 0)
+++ src/java/org/apache/lucene/index/PostingWriter.java	(révision 0)
@@ -0,0 +1,41 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.index;
+
+import java.io.IOException;
+
+import org.apache.lucene.store.RAMOutputStream;
+
+/**
+ * The writer of postings
+ * 
+ * $Id$
+ */
+public interface PostingWriter {
+
+  public void close() throws IOException;
+
+  public long[] getPointers();
+
+  public int getNbPointer();
+
+  public long writeSkip(RAMOutputStream skipBuffer) throws IOException;
+
+  public void write(int doc, int lastDoc, int nbPos, int[] positions, Payload[] payloads, boolean writePayload) throws IOException;
+
+}

Modification de propriétés sur src/java/org/apache/lucene/index/PostingWriter.java
___________________________________________________________________
Nom : svn:mime-type
   + text/plain
Nom : svn:keywords
   + Date Revision Author HeadURL Id
Nom : svn:eol-style
   + native

Index: src/java/org/apache/lucene/index/DefaultFieldData.java
===================================================================
--- src/java/org/apache/lucene/index/DefaultFieldData.java	(révision 0)
+++ src/java/org/apache/lucene/index/DefaultFieldData.java	(révision 0)
@@ -0,0 +1,156 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.index;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.Reader;
+import java.util.zip.DataFormatException;
+import java.util.zip.Inflater;
+
+import org.apache.lucene.store.IndexInput;
+
+/**
+ * The default impelmentation of {@link FieldData}
+ * 
+ * $Id$
+ */
+public class DefaultFieldData extends FieldData {
+
+  DefaultFieldData() {
+    super();
+  }
+
+  /**
+   * Contructor for string data
+   * 
+   * @param text the string data
+   */
+  public DefaultFieldData(String text) {
+    super(text);
+  }
+
+  /**
+   * Contructor for blob data
+   * 
+   * @param data the blob data
+   */
+  public DefaultFieldData(byte[] data) {
+    super(data);
+  }
+
+  /**
+   * Contructor for data got from a reader
+   * 
+   * @param reader the data's reader
+   */
+  public DefaultFieldData(Reader reader) {
+    super(reader);
+  }
+
+  private boolean isCompressed;
+
+  /**
+   * @return true if the value of the field is stored and compressed within the index
+   */
+  public final boolean isCompressed() {
+    return isCompressed;
+  }
+
+  /**
+   * Set the compress status of the data
+   * 
+   * @param isCompressed
+   */
+  public void setCompressed(boolean isCompressed) {
+    this.isCompressed = isCompressed;
+  }
+
+  public static final byte FIELD_IS_TOKENIZED = 0x1;
+
+  public static final byte FIELD_IS_BINARY = 0x2;
+
+  public static final byte FIELD_IS_COMPRESSED = 0x4;
+
+  public void readStream(IndexInput in, boolean skip) throws IOException {
+    byte bits = in.readByte();
+    isCompressed = (bits & FIELD_IS_COMPRESSED) != 0;
+    setTokenized((bits & FIELD_IS_TOKENIZED) != 0);
+    setBinary((bits & FIELD_IS_BINARY) != 0);
+
+    if (skip) {
+      int toRead = in.readVInt();
+      if (isBinary() || isCompressed()) {
+        long pointer = in.getFilePointer();
+        //Need to move the pointer ahead by toRead positions
+        in.seek(pointer + toRead);
+      } else {
+        //Skip ahead of where we are by the length of what is stored
+        in.skipChars(toRead);
+      }
+    } else {
+      if (isBinary()) {
+        int toRead = in.readVInt();
+        final byte[] b = new byte[toRead];
+        in.readBytes(b, 0, b.length);
+        if (isCompressed()) {
+          setData(uncompress(b));
+        } else {
+          setData(b);
+        }
+      } else {
+        if (isCompressed()) {
+          int toRead = in.readVInt();
+          final byte[] b = new byte[toRead];
+          in.readBytes(b, 0, b.length);
+          setData(new String(uncompress(b), "UTF-8"));
+        } else {
+          setData(in.readString()); // read value
+        }
+      }
+    }
+  }
+
+  protected byte[] uncompress(final byte[] input) throws IOException {
+
+    Inflater decompressor = new Inflater();
+    decompressor.setInput(input);
+
+    // Create an expandable byte array to hold the decompressed data
+    ByteArrayOutputStream bos = new ByteArrayOutputStream(input.length);
+
+    // Decompress the data
+    byte[] buf = new byte[1024];
+    while (!decompressor.finished()) {
+      try {
+        int count = decompressor.inflate(buf);
+        bos.write(buf, 0, count);
+      } catch (DataFormatException e) {
+        // this will happen if the field is not compressed
+        CorruptIndexException newException = new CorruptIndexException("field data are in wrong format: " + e.toString());
+        newException.initCause(e);
+        throw newException;
+      }
+    }
+
+    decompressor.end();
+
+    // Get the decompressed data
+    return bos.toByteArray();
+  }
+}

Modification de propriétés sur src/java/org/apache/lucene/index/DefaultFieldData.java
___________________________________________________________________
Nom : svn:mime-type
   + text/plain
Nom : svn:keywords
   + Date Revision Author HeadURL Id
Nom : svn:eol-style
   + native

Index: src/java/org/apache/lucene/index/FieldInfos.java
===================================================================
--- src/java/org/apache/lucene/index/FieldInfos.java	(révision 524580)
+++ src/java/org/apache/lucene/index/FieldInfos.java	(copie de travail)
@@ -32,7 +32,7 @@
  *  be adding documents at a time, with no other reader or writer threads
  *  accessing this object.
  */
-final class FieldInfos {
+public class FieldInfos {
   
   static final byte IS_INDEXED = 0x1;
   static final byte STORE_TERMVECTOR = 0x2;
@@ -53,7 +53,7 @@
    * @param name The name of the file to open the IndexInput from in the Directory
    * @throws IOException
    */
-  FieldInfos(Directory d, String name) throws IOException {
+  public FieldInfos(Directory d, String name) throws IOException {
     IndexInput input = d.openInput(name);
     try {
       read(input);
Index: src/java/org/apache/lucene/index/Posting.java
===================================================================
--- src/java/org/apache/lucene/index/Posting.java	(révision 0)
+++ src/java/org/apache/lucene/index/Posting.java	(révision 0)
@@ -0,0 +1,88 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.index;
+
+/**
+ * Info about the indexed data of a term
+ * 
+ * $Id$
+ */
+public class Posting { // info about a Term in a doc
+  Term term; // the Term
+
+  int freq; // its frequency in doc
+
+  int[] positions; // positions it occurs at
+
+  Payload[] payloads; // the payloads of the terms
+
+  TermVectorOffsetInfo[] offsets;
+
+  Posting(Term t, int position, Payload payload, TermVectorOffsetInfo offset) {
+    term = t;
+    freq = 1;
+    positions = new int[1];
+    positions[0] = position;
+
+    if (payload != null) {
+      payloads = new Payload[1];
+      payloads[0] = payload;
+    } else {
+      payloads = null;
+    }
+
+    if (offset != null) {
+      offsets = new TermVectorOffsetInfo[1];
+      offsets[0] = offset;
+    } else {
+      offsets = null;
+    }
+  }
+
+  public int getFreq() {
+    return freq;
+  }
+
+  public TermVectorOffsetInfo[] getOffsets() {
+    return offsets;
+  }
+
+  public int[] getPositions() {
+    return positions;
+  }
+
+  public Term getTerm() {
+    return term;
+  }
+
+  public Payload[] getPayloads() {
+    return payloads;
+  }
+
+  public String toString() {
+    StringBuilder builder = new StringBuilder();
+    builder.append(term.toString());
+    builder.append('(');
+    for (int i = 0; i < freq; i++) {
+      builder.append(positions[i]);
+      builder.append(',');
+    }
+    builder.append(')');
+    return builder.toString();
+  }
+}

Modification de propriétés sur src/java/org/apache/lucene/index/Posting.java
___________________________________________________________________
Nom : svn:mime-type
   + text/plain
Nom : svn:keywords
   + Date Revision Author HeadURL Id
Nom : svn:eol-style
   + native

Index: src/java/org/apache/lucene/index/TermInfo.java
===================================================================
--- src/java/org/apache/lucene/index/TermInfo.java	(révision 524580)
+++ src/java/org/apache/lucene/index/TermInfo.java	(copie de travail)
@@ -19,41 +19,55 @@
 
 /** A TermInfo is the record of information stored for a term.*/
 
-final class TermInfo {
+public class TermInfo {
   /** The number of documents which contain the term. */
   int docFreq = 0;
 
-  long freqPointer = 0;
-  long proxPointer = 0;
   int skipOffset;
 
-  TermInfo() {}
+  private long[] pointers;
+  
+  TermInfo(int nbPointers) {
+    pointers = new long[nbPointers];
+  }
 
-  TermInfo(int df, long fp, long pp) {
+  TermInfo(int df, long[] pointers) {
     docFreq = df;
-    freqPointer = fp;
-    proxPointer = pp;
+    this.pointers = pointers;
   }
 
   TermInfo(TermInfo ti) {
     docFreq = ti.docFreq;
-    freqPointer = ti.freqPointer;
-    proxPointer = ti.proxPointer;
+    pointers = new long[ti.pointers.length];
+    System.arraycopy(ti.pointers, 0, pointers, 0, ti.pointers.length);
     skipOffset = ti.skipOffset;
   }
 
   final void set(int docFreq,
-                 long freqPointer, long proxPointer, int skipOffset) {
+                 long[] pointers, int skipOffset) {
     this.docFreq = docFreq;
-    this.freqPointer = freqPointer;
-    this.proxPointer = proxPointer;
+    this.pointers = new long[pointers.length];
+    System.arraycopy(pointers, 0, this.pointers, 0, pointers.length);
     this.skipOffset = skipOffset;
   }
 
   final void set(TermInfo ti) {
     docFreq = ti.docFreq;
-    freqPointer = ti.freqPointer;
-    proxPointer = ti.proxPointer;
+    pointers = new long[ti.pointers.length];
+    System.arraycopy(ti.pointers, 0, pointers, 0, ti.pointers.length);
     skipOffset = ti.skipOffset;
   }
+
+  public int getDocFreq() {
+    return docFreq;
+  }
+
+  public long[] getPointers() {
+    return pointers;
+  }
+
+  public int getSkipOffset() {
+    return skipOffset;
+  }
+
 }
Index: src/java/org/apache/lucene/index/DefaultIndexFormat.java
===================================================================
--- src/java/org/apache/lucene/index/DefaultIndexFormat.java	(révision 0)
+++ src/java/org/apache/lucene/index/DefaultIndexFormat.java	(révision 0)
@@ -0,0 +1,116 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.index;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.lucene.store.Directory;
+
+/**
+ * The default implementation of the index format
+ * 
+ * $Id$
+ */
+public class DefaultIndexFormat implements IndexFormat {
+
+  protected static final Set INDEX_EXTENSIONS = new HashSet(Arrays.asList(new String[] {
+      "cfs", "fnm", "fdx", "fdt", "tii", "tis", "frq", "prx", "del",
+      "tvx", "tvd", "tvf", "gen", "nrm" 
+  }));
+
+  protected static final Set INDEX_EXTENSIONS_IN_COMPOUND_FILE = new HashSet(Arrays.asList(new String[] {
+      "fnm", "fdx", "fdt", "tii", "tis", "frq", "prx",
+      "tvx", "tvd", "tvf", "nrm" 
+  }));
+
+  protected static final Set COMPOUND_EXTENSIONS = new HashSet(Arrays.asList(new String[] {
+      "fnm", "frq", "prx", "fdx", "fdt", "tii", "tis"
+  }));
+
+  protected static final Set VECTOR_EXTENSIONS = new HashSet(Arrays.asList(new String[] {
+      "tvx", "tvd", "tvf"
+  }));
+
+  private IndexFileNameFilter indexFileNameFilter;
+
+  /** the ID of the default format */
+  public static final String ID = DefaultIndexFormat.class.getName();
+
+  /**
+   * Can only read indexes of the exact same format.
+   */
+  public boolean canRead(String indexFmtName) {
+    return getID().equals(indexFmtName);
+  }
+
+  /**
+   * The ID is the canonical class name
+   */
+  public String getID() {
+    return getClass().getName();
+  }
+
+  public Set getIndexExtensions() {
+    return INDEX_EXTENSIONS;
+  }
+
+  public Set getIndexExtensionsInCounpoundFile() {
+    return INDEX_EXTENSIONS_IN_COMPOUND_FILE;
+  }
+
+  public Set getCompoundExtensions() {
+    return COMPOUND_EXTENSIONS;
+  }
+
+  public Set getVectorExtensions() {
+    return VECTOR_EXTENSIONS;
+  }
+
+  /**
+   * Use the default implementation of {@link FieldsReader} : {@link DefaultFieldsReader}
+   */
+  public FieldsReader getFieldsReader(Directory d, String segment, FieldInfos fn) throws IOException {
+    return new DefaultFieldsReader(d, segment, fn);
+  }
+
+  /**
+   * Use the default implementation of {@link FieldsWriter} : {@link DefaultFieldsWriter}
+   */
+  public FieldsWriter getFieldsWriter(Directory d, String segment, FieldInfos fn) throws IOException {
+    return new DefaultFieldsWriter(d, segment, fn);
+  }
+
+  public PostingReader getPostingReader(Directory d, String segment) throws IOException {
+    return new DefaultPostingReader(d, segment);
+  }
+
+  public PostingWriter getPostingWriter(Directory d, String segment) throws IOException {
+    return new DefaultPostingWriter(d, segment);
+  }
+
+  public IndexFileNameFilter getIndexFileNameFilter() {
+    if (indexFileNameFilter == null) {
+      indexFileNameFilter = new IndexFileNameFilter(this);
+    }
+    return indexFileNameFilter;
+  }
+
+}

Modification de propriétés sur src/java/org/apache/lucene/index/DefaultIndexFormat.java
___________________________________________________________________
Nom : svn:mime-type
   + text/plain
Nom : svn:keywords
   + Date Revision Author HeadURL Id
Nom : svn:eol-style
   + native

Index: src/java/org/apache/lucene/index/DefaultPostingWriter.java
===================================================================
--- src/java/org/apache/lucene/index/DefaultPostingWriter.java	(révision 0)
+++ src/java/org/apache/lucene/index/DefaultPostingWriter.java	(révision 0)
@@ -0,0 +1,151 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.index;
+
+import java.io.IOException;
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
+
+/**
+ * The default implementation of the {@link PostingWriter}
+ * 
+ * $Id$
+ */
+public class DefaultPostingWriter implements PostingWriter {
+
+  private IndexOutput freq;
+
+  private IndexOutput prox;
+
+  public DefaultPostingWriter(Directory directory, String segment)
+      throws IOException {
+    //open files for inverse index storage
+    freq = directory.createOutput(segment + ".frq");
+    prox = directory.createOutput(segment + ".prx");
+  }
+
+  public void close() throws IOException {
+    // make an effort to close all streams we can but remember and re-throw
+    // the first exception encountered in this process
+    IOException keep = null;
+    if (freq != null) {
+      try {
+        freq.close();
+      } catch (IOException e) {
+        keep = e;
+      }
+    }
+    if (prox != null) {
+      try {
+        prox.close();
+      } catch (IOException e) {
+        if (keep == null) {
+          keep = e;
+        }
+      }
+    }
+    if (keep != null) {
+      throw (IOException) keep.fillInStackTrace();
+    }
+  }
+
+  public int getNbPointer() {
+    return 2;
+  }
+
+  public long[] getPointers() {
+    return new long[] {freq.getFilePointer(), prox.getFilePointer()};
+  }
+
+  public long writeSkip(RAMOutputStream skipBuffer) throws IOException {
+    long skipPointer = freq.getFilePointer();
+    skipBuffer.writeTo(freq);
+    return skipPointer;
+  }
+
+  public void write(int doc, int lastDoc, int nbPos, int[] positions, Payload[] payloads, boolean writePayload) throws IOException {
+    int docCode = (doc - lastDoc) << 1;   // use low bit to flag freq=1
+
+    if (nbPos == 1) {
+      freq.writeVInt(docCode | 1);      // write doc & freq=1
+    } else {
+      freq.writeVInt(docCode);      // write doc
+      freq.writeVInt(nbPos);         // write frequency in doc
+    }
+
+    int lastPosition = 0;             // write position deltas
+    int lastPayloadLength = -1;
+
+    
+    // The following encoding is being used for positions and payloads:
+    // Case 1: current field does not store payloads
+    //           Positions     -> <PositionDelta>^freq
+    //           PositionDelta -> VInt
+    //         The PositionDelta is the difference between the current
+    //         and the previous position
+    // Case 2: current field stores payloads
+    //           Positions     -> <PositionDelta, Payload>^freq
+    //           Payload       ->  <PayloadLength?, PayloadData>
+    //           PositionDelta -> VInt
+    //           PayloadLength -> VInt
+    //           PayloadData   -> byte^PayloadLength
+    //         In this case PositionDelta/2 is the difference between
+    //         the current and the previous position. If PositionDelta
+    //         is odd, then a PayloadLength encoded as VInt follows,
+    //         if PositionDelta is even, then it is assumed that the
+    //         length of the current Payload equals the length of the
+    //         previous Payload.        
+    for (int j = 0; j < nbPos; j++) {
+      int position = positions[j];
+      int delta = position - lastPosition;
+      if (writePayload) {
+        int payloadLength = 0;
+        Payload payload = null;
+        if (payloads != null) {
+          payload = payloads[j];
+          if (payload != null) {
+            payloadLength = payload.length;
+          }
+        }
+        if (payloadLength == lastPayloadLength) {
+            // the length of the current payload equals the length
+            // of the previous one. So we do not have to store the length
+            // again and we only shift the position delta by one bit
+          prox.writeVInt(delta * 2);
+        } else {
+            // the length of the current payload is different from the
+            // previous one. We shift the position delta, set the lowest
+            // bit and store the current payload length as VInt.
+          prox.writeVInt(delta * 2 + 1);
+          prox.writeVInt(payloadLength);
+          lastPayloadLength = payloadLength;
+        }
+        if (payloadLength > 0) {
+            // write current payload
+          prox.writeBytes(payload.data, payload.offset, payload.length);
+        }
+      } else {
+        // field does not store payloads, just write position delta as VInt
+        prox.writeVInt(delta);
+      }
+      lastPosition = position;
+    }
+  }
+}

Modification de propriétés sur src/java/org/apache/lucene/index/DefaultPostingWriter.java
___________________________________________________________________
Nom : svn:mime-type
   + text/plain
Nom : svn:keywords
   + Date Revision Author HeadURL Id
Nom : svn:eol-style
   + native

Index: src/java/org/apache/lucene/index/FieldsReader.java
===================================================================
--- src/java/org/apache/lucene/index/FieldsReader.java	(révision 524586)
+++ src/java/org/apache/lucene/index/FieldsReader.java	(copie de travail)
@@ -17,17 +17,16 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.document.*;
+import java.io.IOException;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.FieldSelector;
+import org.apache.lucene.document.FieldSelectorResult;
+import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.AlreadyClosedException;
 
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.Reader;
-import java.util.zip.DataFormatException;
-import java.util.zip.Inflater;
-
 /**
  * Class responsible for access to stored document fields.
  * <p/>
@@ -35,7 +34,7 @@
  *
  * @version $Id$
  */
-final class FieldsReader {
+public abstract class FieldsReader {
   private final FieldInfos fieldInfos;
 
   // The main fieldStream, used only for cloning.
@@ -51,7 +50,7 @@
 
   private ThreadLocal fieldsStreamTL = new ThreadLocal();
 
-  FieldsReader(Directory d, String segment, FieldInfos fn) throws IOException {
+  protected FieldsReader(Directory d, String segment, FieldInfos fn) throws IOException {
     fieldInfos = fn;
 
     cloneableFieldsStream = d.openInput(segment + ".fdt");
@@ -75,7 +74,7 @@
    *
    * @throws IOException
    */
-  final void close() throws IOException {
+  protected void close() throws IOException {
     if (!closed) {
       fieldsStream.close();
       cloneableFieldsStream.close();
@@ -98,385 +97,53 @@
     long position = indexStream.readLong();
     fieldsStream.seek(position);
 
-    Document doc = new Document();
+    Document doc = createDocument(fieldsStream);
+
     int numFields = fieldsStream.readVInt();
     for (int i = 0; i < numFields; i++) {
       int fieldNumber = fieldsStream.readVInt();
       FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);
       FieldSelectorResult acceptField = fieldSelector == null ? FieldSelectorResult.LOAD : fieldSelector.accept(fi.name);
-      
-      byte bits = fieldsStream.readByte();
-      boolean compressed = (bits & FieldsWriter.FIELD_IS_COMPRESSED) != 0;
-      boolean tokenize = (bits & FieldsWriter.FIELD_IS_TOKENIZED) != 0;
-      boolean binary = (bits & FieldsWriter.FIELD_IS_BINARY) != 0;
-      //TODO: Find an alternative approach here if this list continues to grow beyond the
-      //list of 5 or 6 currently here.  See Lucene 762 for discussion
-      if (acceptField.equals(FieldSelectorResult.LOAD)) {
-        addField(doc, fi, binary, compressed, tokenize);
-      }
-      else if (acceptField.equals(FieldSelectorResult.LOAD_FOR_MERGE)) {
-        addFieldForMerge(doc, fi, binary, compressed, tokenize);
-      }
-      else if (acceptField.equals(FieldSelectorResult.LOAD_AND_BREAK)){
-        addField(doc, fi, binary, compressed, tokenize);
-        break;//Get out of this loop
-      }
-      else if (acceptField.equals(FieldSelectorResult.LAZY_LOAD)) {
-        addFieldLazy(doc, fi, binary, compressed, tokenize);
-      }
-      else if (acceptField.equals(FieldSelectorResult.SIZE)){
-        skipField(binary, compressed, addFieldSize(doc, fi, binary, compressed));
-      }
-      else if (acceptField.equals(FieldSelectorResult.SIZE_AND_BREAK)){
-        addFieldSize(doc, fi, binary, compressed);
-        break;
-      }
-      else {
-        skipField(binary, compressed);
-      }
-    }
 
-    return doc;
-  }
+      FieldData fieldData = createFieldData(fi);
 
-  /**
-   * Skip the field.  We still have to read some of the information about the field, but can skip past the actual content.
-   * This will have the most payoff on large fields.
-   */
-  private void skipField(boolean binary, boolean compressed) throws IOException {
-    skipField(binary, compressed, fieldsStream.readVInt());
-  }
-  
-  private void skipField(boolean binary, boolean compressed, int toRead) throws IOException {
-    if (binary || compressed) {
-      long pointer = fieldsStream.getFilePointer();
-      fieldsStream.seek(pointer + toRead);
-    } else {
-      //We need to skip chars.  This will slow us down, but still better
-      fieldsStream.skipChars(toRead);
-    }
-  }
+      boolean lazy = acceptField.equals(FieldSelectorResult.LAZY_LOAD);
+      boolean skip = acceptField.equals(FieldSelectorResult.NO_LOAD);
+      boolean size = acceptField.equals(FieldSelectorResult.SIZE);
 
-  private void addFieldLazy(Document doc, FieldInfo fi, boolean binary, boolean compressed, boolean tokenize) throws IOException {
-    if (binary == true) {
-      int toRead = fieldsStream.readVInt();
       long pointer = fieldsStream.getFilePointer();
-      if (compressed) {
-        //was: doc.add(new Fieldable(fi.name, uncompress(b), Fieldable.Store.COMPRESS));
-        doc.add(new LazyField(fi.name, Field.Store.COMPRESS, toRead, pointer));
-      } else {
-        //was: doc.add(new Fieldable(fi.name, b, Fieldable.Store.YES));
-        doc.add(new LazyField(fi.name, Field.Store.YES, toRead, pointer));
-      }
-      //Need to move the pointer ahead by toRead positions
-      fieldsStream.seek(pointer + toRead);
-    } else {
-      Field.Store store = Field.Store.YES;
-      Field.Index index = getIndexType(fi, tokenize);
-      Field.TermVector termVector = getTermVectorType(fi);
 
-      Fieldable f;
-      if (compressed) {
-        store = Field.Store.COMPRESS;
-        int toRead = fieldsStream.readVInt();
-        long pointer = fieldsStream.getFilePointer();
-        f = new LazyField(fi.name, store, toRead, pointer);
-        //skip over the part that we aren't loading
-        fieldsStream.seek(pointer + toRead);
-        f.setOmitNorms(fi.omitNorms);
-      } else {
-        int length = fieldsStream.readVInt();
-        long pointer = fieldsStream.getFilePointer();
-        //Skip ahead of where we are by the length of what is stored
-        fieldsStream.skipChars(length);
-        f = new LazyField(fi.name, store, index, termVector, length, pointer);
-        f.setOmitNorms(fi.omitNorms);
-      }
-      doc.add(f);
-    }
+      fieldData.readStream(fieldsStream, skip || lazy || size);
 
-  }
-
-  // in merge mode we don't uncompress the data of a compressed field
-  private void addFieldForMerge(Document doc, FieldInfo fi, boolean binary, boolean compressed, boolean tokenize) throws IOException {
-    Object data;
-      
-    if (binary || compressed) {
-      int toRead = fieldsStream.readVInt();
-      final byte[] b = new byte[toRead];
-      fieldsStream.readBytes(b, 0, b.length);
-      data = b;
-    } else {
-      data = fieldsStream.readString();
-    }
-      
-    doc.add(new FieldForMerge(data, fi, binary, compressed, tokenize));
-  }
-  
-  private void addField(Document doc, FieldInfo fi, boolean binary, boolean compressed, boolean tokenize) throws CorruptIndexException, IOException {
-
-    //we have a binary stored field, and it may be compressed
-    if (binary) {
-      int toRead = fieldsStream.readVInt();
-      final byte[] b = new byte[toRead];
-      fieldsStream.readBytes(b, 0, b.length);
-      if (compressed)
-        doc.add(new Field(fi.name, uncompress(b), Field.Store.COMPRESS));
-      else
-        doc.add(new Field(fi.name, b, Field.Store.YES));
-
-    } else {
-      Field.Store store = Field.Store.YES;
-      Field.Index index = getIndexType(fi, tokenize);
-      Field.TermVector termVector = getTermVectorType(fi);
-
-      Fieldable f;
-      if (compressed) {
-        store = Field.Store.COMPRESS;
-        int toRead = fieldsStream.readVInt();
-
-        final byte[] b = new byte[toRead];
-        fieldsStream.readBytes(b, 0, b.length);
-        f = new Field(fi.name,      // field name
-                new String(uncompress(b), "UTF-8"), // uncompress the value and add as string
-                store,
-                index,
-                termVector);
-        f.setOmitNorms(fi.omitNorms);
-      } else {
-        f = new Field(fi.name,     // name
-                fieldsStream.readString(), // read value
-                store,
-                index,
-                termVector);
-        f.setOmitNorms(fi.omitNorms);
-      }
-      doc.add(f);
-    }
-  }
-  
-  // Add the size of field as a byte[] containing the 4 bytes of the integer byte size (high order byte first; char = 2 bytes)
-  // Read just the size -- caller must skip the field content to continue reading fields
-  // Return the size in bytes or chars, depending on field type
-  private int addFieldSize(Document doc, FieldInfo fi, boolean binary, boolean compressed) throws IOException {
-    int size = fieldsStream.readVInt(), bytesize = binary || compressed ? size : 2*size;
-    byte[] sizebytes = new byte[4];
-    sizebytes[0] = (byte) (bytesize>>>24);
-    sizebytes[1] = (byte) (bytesize>>>16);
-    sizebytes[2] = (byte) (bytesize>>> 8);
-    sizebytes[3] = (byte)  bytesize      ;
-    doc.add(new Field(fi.name, sizebytes, Field.Store.YES));
-    return size;
-  }
-
-  private Field.TermVector getTermVectorType(FieldInfo fi) {
-    Field.TermVector termVector = null;
-    if (fi.storeTermVector) {
-      if (fi.storeOffsetWithTermVector) {
-        if (fi.storePositionWithTermVector) {
-          termVector = Field.TermVector.WITH_POSITIONS_OFFSETS;
-        } else {
-          termVector = Field.TermVector.WITH_OFFSETS;
+      if (!skip) {
+        if (size) {
+          int bytesize = (int) (fieldsStream.getFilePointer() - pointer);
+          byte[] sizebytes = new byte[4];
+          sizebytes[0] = (byte) (bytesize>>>24);
+          sizebytes[1] = (byte) (bytesize>>>16);
+          sizebytes[2] = (byte) (bytesize>>> 8);
+          sizebytes[3] = (byte)  bytesize      ;
+          fieldData.setData(sizebytes);
+          fieldData.setBinary(true);
+          fieldData.setTokenized(false);
+        } else if (lazy) {
+          fieldData.setLazyData((IndexInput) fieldsStream.clone(), pointer, fieldsStream.getFilePointer() - pointer);
         }
-      } else if (fi.storePositionWithTermVector) {
-        termVector = Field.TermVector.WITH_POSITIONS;
-      } else {
-        termVector = Field.TermVector.YES;
-      }
-    } else {
-      termVector = Field.TermVector.NO;
-    }
-    return termVector;
-  }
-
-  private Field.Index getIndexType(FieldInfo fi, boolean tokenize) {
-    Field.Index index;
-    if (fi.isIndexed && tokenize)
-      index = Field.Index.TOKENIZED;
-    else if (fi.isIndexed && !tokenize)
-      index = Field.Index.UN_TOKENIZED;
-    else
-      index = Field.Index.NO;
-    return index;
-  }
-
-  /**
-   * A Lazy implementation of Fieldable that differs loading of fields until asked for, instead of when the Document is
-   * loaded.
-   */
-  private class LazyField extends AbstractField implements Fieldable {
-    private int toRead;
-    private long pointer;
-
-    public LazyField(String name, Field.Store store, int toRead, long pointer) {
-      super(name, store, Field.Index.NO, Field.TermVector.NO);
-      this.toRead = toRead;
-      this.pointer = pointer;
-      lazy = true;
-    }
-
-    public LazyField(String name, Field.Store store, Field.Index index, Field.TermVector termVector, int toRead, long pointer) {
-      super(name, store, index, termVector);
-      this.toRead = toRead;
-      this.pointer = pointer;
-      lazy = true;
-    }
-
-    private IndexInput getFieldStream() {
-      IndexInput localFieldsStream = (IndexInput) fieldsStreamTL.get();
-      if (localFieldsStream == null) {
-        localFieldsStream = (IndexInput) cloneableFieldsStream.clone();
-        fieldsStreamTL.set(localFieldsStream);
-      }
-      return localFieldsStream;
-    }
-
-    /**
-     * The value of the field in Binary, or null.  If null, the Reader or
-     * String value is used.  Exactly one of stringValue(), readerValue() and
-     * binaryValue() must be set.
-     */
-    public byte[] binaryValue() {
-      ensureOpen();
-      if (fieldsData == null) {
-        final byte[] b = new byte[toRead];
-        IndexInput localFieldsStream = getFieldStream();
-        //Throw this IO Exception since IndexREader.document does so anyway, so probably not that big of a change for people
-        //since they are already handling this exception when getting the document
-        try {
-          localFieldsStream.seek(pointer);
-          localFieldsStream.readBytes(b, 0, b.length);
-          if (isCompressed == true) {
-            fieldsData = uncompress(b);
-          } else {
-            fieldsData = b;
-          }
-        } catch (IOException e) {
-          throw new FieldReaderException(e);
+        Fieldable field = createFieldable(fi, fieldData);
+        doc.add(field);
+        if (acceptField.equals(FieldSelectorResult.LOAD_AND_BREAK)) {
+          break;
         }
       }
-      return fieldsData instanceof byte[] ? (byte[]) fieldsData : null;
     }
 
-    /**
-     * The value of the field as a Reader, or null.  If null, the String value
-     * or binary value is  used.  Exactly one of stringValue(), readerValue(),
-     * and binaryValue() must be set.
-     */
-    public Reader readerValue() {
-      ensureOpen();
-      return fieldsData instanceof Reader ? (Reader) fieldsData : null;
-    }
-
-    /**
-     * The value of the field as a String, or null.  If null, the Reader value
-     * or binary value is used.  Exactly one of stringValue(), readerValue(), and
-     * binaryValue() must be set.
-     */
-    public String stringValue() {
-      ensureOpen();
-      if (fieldsData == null) {
-        IndexInput localFieldsStream = getFieldStream();
-        try {
-          localFieldsStream.seek(pointer);
-          if (isCompressed) {
-            final byte[] b = new byte[toRead];
-            localFieldsStream.readBytes(b, 0, b.length);
-            fieldsData = new String(uncompress(b), "UTF-8");
-          } else {
-            //read in chars b/c we already know the length we need to read
-            char[] chars = new char[toRead];
-            localFieldsStream.readChars(chars, 0, toRead);
-            fieldsData = new String(chars);
-          }
-        } catch (IOException e) {
-          throw new FieldReaderException(e);
-        }
-      }
-      return fieldsData instanceof String ? (String) fieldsData : null;
-    }
-
-    public long getPointer() {
-      ensureOpen();
-      return pointer;
-    }
-
-    public void setPointer(long pointer) {
-      ensureOpen();
-      this.pointer = pointer;
-    }
-
-    public int getToRead() {
-      ensureOpen();
-      return toRead;
-    }
-
-    public void setToRead(int toRead) {
-      ensureOpen();
-      this.toRead = toRead;
-    }
+    return doc;
   }
 
-  private final byte[] uncompress(final byte[] input)
-          throws CorruptIndexException, IOException {
+  protected abstract Document createDocument(IndexInput in);
 
-    Inflater decompressor = new Inflater();
-    decompressor.setInput(input);
+  protected abstract FieldData createFieldData(FieldInfo fi);
 
-    // Create an expandable byte array to hold the decompressed data
-    ByteArrayOutputStream bos = new ByteArrayOutputStream(input.length);
+  protected abstract Fieldable createFieldable(FieldInfo fi, FieldData fieldData);
 
-    // Decompress the data
-    byte[] buf = new byte[1024];
-    while (!decompressor.finished()) {
-      try {
-        int count = decompressor.inflate(buf);
-        bos.write(buf, 0, count);
-      }
-      catch (DataFormatException e) {
-        // this will happen if the field is not compressed
-        CorruptIndexException newException = new CorruptIndexException("field data are in wrong format: " + e.toString());
-        newException.initCause(e);
-        throw newException;
-      }
-    }
-  
-    decompressor.end();
-    
-    // Get the decompressed data
-    return bos.toByteArray();
-  }
-  
-  // Instances of this class hold field properties and data
-  // for merge
-  final static class FieldForMerge extends AbstractField {
-    public String stringValue() {
-      return (String) this.fieldsData;
-    }
-
-    public Reader readerValue() {
-      // not needed for merge
-      return null;
-    }
-
-    public byte[] binaryValue() {
-      return (byte[]) this.fieldsData;
-    }
-    
-    public FieldForMerge(Object value, FieldInfo fi, boolean binary, boolean compressed, boolean tokenize) {
-      this.isStored = true;  
-      this.fieldsData = value;
-      this.isCompressed = compressed;
-      this.isBinary = binary;
-      this.isTokenized = tokenize;
-
-      this.name = fi.name.intern();
-      this.isIndexed = fi.isIndexed;
-      this.omitNorms = fi.omitNorms;          
-      this.storeOffsetWithTermVector = fi.storeOffsetWithTermVector;
-      this.storePositionWithTermVector = fi.storePositionWithTermVector;
-      this.storeTermVector = fi.storeTermVector;            
-    }
-     
-  }
 }
Index: src/java/org/apache/lucene/index/TermInfosReader.java
===================================================================
--- src/java/org/apache/lucene/index/TermInfosReader.java	(révision 524580)
+++ src/java/org/apache/lucene/index/TermInfosReader.java	(copie de travail)
@@ -25,7 +25,7 @@
  * Directory.  Pairs are accessed either by Term or by ordinal position the
  * set.  */
 
-final class TermInfosReader {
+public class TermInfosReader {
   private Directory directory;
   private String segment;
   private FieldInfos fieldInfos;
@@ -40,7 +40,7 @@
   
   private SegmentTermEnum indexEnum;
 
-  TermInfosReader(Directory dir, String seg, FieldInfos fis)
+  public TermInfosReader(Directory dir, String seg, FieldInfos fis)
        throws CorruptIndexException, IOException {
     directory = dir;
     segment = seg;
@@ -127,7 +127,7 @@
   }
 
   /** Returns the TermInfo for a Term in the set, or null. */
-  TermInfo get(Term term) throws IOException {
+  public TermInfo get(Term term) throws IOException {
     if (size == 0) return null;
 
     ensureIndexIsRead();
Index: src/java/org/apache/lucene/index/IndexReader.java
===================================================================
--- src/java/org/apache/lucene/index/IndexReader.java	(révision 524580)
+++ src/java/org/apache/lucene/index/IndexReader.java	(copie de travail)
@@ -17,6 +17,12 @@
  * limitations under the License.
  */
 
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collection;
+
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldSelector;
 import org.apache.lucene.search.Similarity;
@@ -27,12 +33,6 @@
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.store.AlreadyClosedException;
 
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collection;
-
 /** IndexReader is an abstract class, providing an interface for accessing an
  index.  Search of an index is done entirely through this abstract interface,
  so that any subclass which implements it is searchable.
@@ -486,7 +486,7 @@
 
   /** Expert: Resets the normalization factor for the named field of the named
    * document.  The norm represents the product of the field's {@link
-   * org.apache.lucene.document.Fieldable#setBoost(float) boost} and its {@link Similarity#lengthNorm(String,
+   * org.apache.lucene.document.FielData#setBoost(float) boost} and its {@link Similarity#lengthNorm(String,
    * int) length normalization}.  Thus, to preserve the length normalization
    * values when resetting this, one should base the new value upon the old.
    *
Index: src/java/org/apache/lucene/index/IndexFileNames.java
===================================================================
--- src/java/org/apache/lucene/index/IndexFileNames.java	(révision 524583)
+++ src/java/org/apache/lucene/index/IndexFileNames.java	(copie de travail)
@@ -49,36 +49,9 @@
 
   /** Extension of separate norms */
   static final String SEPARATE_NORMS_EXTENSION = "s";
-
-  /**
-   * This array contains all filename extensions used by
-   * Lucene's index files, with two exceptions, namely the
-   * extension made up from <code>.f</code> + a number and
-   * from <code>.s</code> + a number.  Also note that
-   * Lucene's <code>segments_N</code> files do not have any
-   * filename extension.
-   */
-  static final String INDEX_EXTENSIONS[] = new String[] {
-      "cfs", "fnm", "fdx", "fdt", "tii", "tis", "frq", "prx", "del",
-      "tvx", "tvd", "tvf", "gen", "nrm" 
-  };
-
-  /** File extensions that are added to a compound file
-   * (same as above, minus "del", "gen", "cfs"). */
-  static final String[] INDEX_EXTENSIONS_IN_COMPOUND_FILE = new String[] {
-      "fnm", "fdx", "fdt", "tii", "tis", "frq", "prx",
-      "tvx", "tvd", "tvf", "nrm" 
-  };
   
-  /** File extensions of old-style index files */
-  static final String COMPOUND_EXTENSIONS[] = new String[] {
-    "fnm", "frq", "prx", "fdx", "fdt", "tii", "tis"
-  };
-  
-  /** File extensions for term vector support */
-  static final String VECTOR_EXTENSIONS[] = new String[] {
-    "tvx", "tvd", "tvf"
-  };
+  /** Name of the index format file */
+  static public final String INDEXFORMAT = "indexformat";
 
   /**
    * Computes the full file name from base, extension and
Index: src/java/org/apache/lucene/index/IncompatibleFormatException.java
===================================================================
--- src/java/org/apache/lucene/index/IncompatibleFormatException.java	(révision 0)
+++ src/java/org/apache/lucene/index/IncompatibleFormatException.java	(révision 0)
@@ -0,0 +1,36 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.index;
+
+
+/**
+ * Exception raised when some Lucene operation cannot be realized due
+ * to incompatible index formats. It can appear while trying to merge.
+ * 
+ * $Id$
+ */
+public class IncompatibleFormatException extends RuntimeException {
+
+  public IncompatibleFormatException() {
+    super();
+  }
+
+  public IncompatibleFormatException(String message) {
+    super(message);
+  }
+}

Modification de propriétés sur src/java/org/apache/lucene/index/IncompatibleFormatException.java
___________________________________________________________________
Nom : svn:mime-type
   + text/plain
Nom : svn:keywords
   + Date Revision Author HeadURL Id
Nom : svn:eol-style
   + native

Index: src/java/org/apache/lucene/index/SegmentInfos.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentInfos.java	(révision 524580)
+++ src/java/org/apache/lucene/index/SegmentInfos.java	(copie de travail)
@@ -17,16 +17,16 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-
 import java.io.File;
 import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.io.PrintStream;
 import java.util.Vector;
 
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+
 final class SegmentInfos extends Vector {
   
   
@@ -179,6 +179,65 @@
   }
 
   /**
+   * Check the index format of the directory.
+   * <ul>
+   *   <li>if there is no index format written in the directory : write the instanciated one</li>
+   *   <li>else : check that the instanciated one can read the written one</li>
+   * </ul>
+   * 
+   * @param dir the directory to check
+   * @throws IOException
+   */
+  void checkIndexFormat(Directory dir) throws IOException {
+    String indexFormatName = readIndexFormat(dir);
+    if (dir.getIndexFormat() == null) {
+      throw new IllegalStateException("A directory must have an index format declared");
+    }
+    if (indexFormatName == null) { // no written index format
+      writeIndexFormat(dir); // so write it
+    } else {
+      // an format has been written, check the compatibility
+      if (!dir.getIndexFormat().canRead(indexFormatName)) {
+        throw new IllegalStateException("The IndexFormat instanciated with the directory '"
+            + dir.getIndexFormat().getClass().getCanonicalName()
+            + "' is not compatible with the actual format of the index : '" + indexFormatName + "'");
+      }
+    }
+  }
+
+  /**
+   * Write the index format in the directory
+   * 
+   * @param dir the directory to write in
+   * @throws IOException
+   */
+  void writeIndexFormat(Directory dir) throws IOException {
+    if (dir.fileExists(IndexFileNames.INDEXFORMAT)) {
+      dir.deleteFile(IndexFileNames.INDEXFORMAT);
+    }
+    IndexOutput out = dir.createOutput(IndexFileNames.INDEXFORMAT);
+    out.writeString(dir.getIndexFormat().getID());
+    out.close();
+  }
+
+  /**
+   * Read the index format from the directory
+   * 
+   * @param dir the directory to read
+   * @return the canonical class name of the index format
+   * @throws IOException
+   */
+  String readIndexFormat(Directory dir) throws IOException {
+    if (!dir.fileExists(IndexFileNames.INDEXFORMAT)) {
+      return null;
+    }
+    IndexInput in = dir.openInput(IndexFileNames.INDEXFORMAT);
+    String fmt = in.readString();
+    in.close();
+    return fmt;
+  }
+
+  /**
    * Read a particular segmentFileName.  Note that this may
    * throw an IOException if a commit is in process.
    *
@@ -235,9 +294,12 @@
    * commits) to find the right segments file to load.
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
+   * @throws ClassNotFoundException when the written index format doesn't exist in the classpath
    */
   public final void read(Directory directory) throws CorruptIndexException, IOException {
 
+    checkIndexFormat(directory);
+
     generation = lastGeneration = -1;
 
     new FindSegmentsFile(directory) {
Index: src/java/org/apache/lucene/index/SegmentTermPositions.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentTermPositions.java	(révision 524584)
+++ src/java/org/apache/lucene/index/SegmentTermPositions.java	(copie de travail)
@@ -21,6 +21,9 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BitVector;
+
 final class SegmentTermPositions
 extends SegmentTermDocs implements TermPositions {
   private IndexInput proxStream;
@@ -38,15 +41,15 @@
   private long lazySkipPointer = 0;
   private int lazySkipProxCount = 0;
   
-  SegmentTermPositions(SegmentReader p) {
-    super(p);
-    this.proxStream = null;  // the proxStream will be cloned lazily when nextPosition() is called for the first time
+  SegmentTermPositions(IndexInput freqStream, IndexInput proxStream, TermInfosReader tis, BitVector deletedDocs, FieldInfos fieldInfos) {
+    super(freqStream, tis, deletedDocs, fieldInfos);
+    this.proxStream = proxStream;
   }
 
   final void seek(TermInfo ti, Term term) throws IOException {
     super.seek(ti, term);
     if (ti != null)
-      lazySkipPointer = ti.proxPointer;
+      lazySkipPointer = ti.getPointers()[1];
     
     lazySkipProxCount = 0;
     proxCount = 0;
@@ -143,15 +146,10 @@
   // So we move the prox pointer lazily to the document
   // as soon as positions are requested.
   private void lazySkip() throws IOException {
-    if (proxStream == null) {
-      // clone lazily
-      proxStream = (IndexInput)parent.proxStream.clone();
-    }
-    
     // we might have to skip the current payload
     // if it was not read yet
     skipPayload();
-      
+
     if (lazySkipPointer != 0) {
       proxStream.seek(lazySkipPointer);
       lazySkipPointer = 0;
Index: src/java/org/apache/lucene/index/FieldsWriter.java
===================================================================
--- src/java/org/apache/lucene/index/FieldsWriter.java	(révision 524580)
+++ src/java/org/apache/lucene/index/FieldsWriter.java	(copie de travail)
@@ -16,35 +16,29 @@
  * the License.
  */
 
-import java.io.ByteArrayOutputStream;
 import java.io.IOException;
 import java.util.Iterator;
-import java.util.zip.Deflater;
 
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexOutput;
 
-final class FieldsWriter
-{
-  static final byte FIELD_IS_TOKENIZED = 0x1;
-  static final byte FIELD_IS_BINARY = 0x2;
-  static final byte FIELD_IS_COMPRESSED = 0x4;
-  
+public abstract class FieldsWriter {
+
     private FieldInfos fieldInfos;
 
     private IndexOutput fieldsStream;
 
     private IndexOutput indexStream;
 
-    FieldsWriter(Directory d, String segment, FieldInfos fn) throws IOException {
+    protected FieldsWriter(Directory d, String segment, FieldInfos fn) throws IOException {
         fieldInfos = fn;
         fieldsStream = d.createOutput(segment + ".fdt");
         indexStream = d.createOutput(segment + ".fdx");
     }
 
-    final void close() throws IOException {
+    protected void close() throws IOException {
         fieldsStream.close();
         indexStream.close();
     }
@@ -52,100 +46,29 @@
     final void addDocument(Document doc) throws IOException {
         indexStream.writeLong(fieldsStream.getFilePointer());
 
+        writeDocumentData(fieldsStream, doc);
+
         int storedCount = 0;
-        Iterator fieldIterator = doc.getFields().iterator();
-        while (fieldIterator.hasNext()) {
-            Fieldable field = (Fieldable) fieldIterator.next();
+        Iterator fields = doc.getFields().iterator();
+        while (fields.hasNext()) {
+            Fieldable field = (Fieldable) fields.next();
             if (field.isStored())
                 storedCount++;
         }
         fieldsStream.writeVInt(storedCount);
 
-        fieldIterator = doc.getFields().iterator();
-        while (fieldIterator.hasNext()) {
-            Fieldable field = (Fieldable) fieldIterator.next();
-            // if the field as an instanceof FieldsReader.FieldForMerge, we're in merge mode
-            // and field.binaryValue() already returns the compressed value for a field
-            // with isCompressed()==true, so we disable compression in that case
-            boolean disableCompression = (field instanceof FieldsReader.FieldForMerge);
+        fields = doc.getFields().iterator();
+        while (fields.hasNext()) {
+            Fieldable field = (Fieldable) fields.next();
             if (field.isStored()) {
                 fieldsStream.writeVInt(fieldInfos.fieldNumber(field.name()));
-
-                byte bits = 0;
-                if (field.isTokenized())
-                    bits |= FieldsWriter.FIELD_IS_TOKENIZED;
-                if (field.isBinary())
-                    bits |= FieldsWriter.FIELD_IS_BINARY;
-                if (field.isCompressed())
-                    bits |= FieldsWriter.FIELD_IS_COMPRESSED;
-                
-                fieldsStream.writeByte(bits);
-                
-                if (field.isCompressed()) {
-                  // compression is enabled for the current field
-                  byte[] data = null;
-                  
-                  if (disableCompression) {
-                      // optimized case for merging, the data
-                      // is already compressed
-                      data = field.binaryValue();
-                  } else {
-                      // check if it is a binary field
-                      if (field.isBinary()) {
-                        data = compress(field.binaryValue());
-                      }
-                      else {
-                        data = compress(field.stringValue().getBytes("UTF-8"));
-                      }
-                  }
-                  final int len = data.length;
-                  fieldsStream.writeVInt(len);
-                  fieldsStream.writeBytes(data, len);
-                }
-                else {
-                  // compression is disabled for the current field
-                  if (field.isBinary()) {
-                    byte[] data = field.binaryValue();
-                    final int len = data.length;
-                    fieldsStream.writeVInt(len);
-                    fieldsStream.writeBytes(data, len);
-                  }
-                  else {
-                    fieldsStream.writeString(field.stringValue());
-                  }
-                }
+                writeFieldData(field.getFieldData(), fieldsStream);
             }
         }
     }
 
-    private final byte[] compress (byte[] input) {
+    abstract protected void writeDocumentData(IndexOutput out, Document doc) throws IOException;
 
-      // Create the compressor with highest level of compression
-      Deflater compressor = new Deflater();
-      compressor.setLevel(Deflater.BEST_COMPRESSION);
+    abstract protected void writeFieldData(FieldData fieldData, IndexOutput out) throws IOException;
 
-      // Give the compressor the data to compress
-      compressor.setInput(input);
-      compressor.finish();
-
-      /*
-       * Create an expandable byte array to hold the compressed data.
-       * You cannot use an array that's the same size as the orginal because
-       * there is no guarantee that the compressed data will be smaller than
-       * the uncompressed data.
-       */
-      ByteArrayOutputStream bos = new ByteArrayOutputStream(input.length);
-
-      // Compress the data
-      byte[] buf = new byte[1024];
-      while (!compressor.finished()) {
-        int count = compressor.deflate(buf);
-        bos.write(buf, 0, count);
-      }
-      
-      compressor.end();
-
-      // Get the compressed data
-      return bos.toByteArray();
-    }
 }
Index: src/java/org/apache/lucene/index/SegmentTermDocs.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentTermDocs.java	(révision 524584)
+++ src/java/org/apache/lucene/index/SegmentTermDocs.java	(copie de travail)
@@ -22,11 +22,9 @@
 import org.apache.lucene.store.IndexInput;
 
 class SegmentTermDocs implements TermDocs {
-  protected SegmentReader parent;
   protected IndexInput freqStream;
   protected int count;
   protected int df;
-  protected BitVector deletedDocs;
   int doc = 0;
   int freq;
 
@@ -39,19 +37,23 @@
   private long proxPointer;
   private long skipPointer;
   private boolean haveSkipped;
+  private BitVector deletedDocs;
+  private TermInfosReader tis;
+  private FieldInfos fieldInfos;
   
   private int payloadLengthAtLastSkip;
   protected boolean currentFieldStoresPayloads;
 
-  protected SegmentTermDocs(SegmentReader parent) {
-    this.parent = parent;
-    this.freqStream = (IndexInput) parent.freqStream.clone();
-    this.deletedDocs = parent.deletedDocs;
-    this.skipInterval = parent.tis.getSkipInterval();
+  protected SegmentTermDocs(IndexInput freqStream, TermInfosReader tis, BitVector deletedDocs, FieldInfos fieldInfos) {
+    this.freqStream = freqStream;
+    this.deletedDocs = deletedDocs;
+    this.tis = tis;
+    this.skipInterval = tis.getSkipInterval();
+    this.fieldInfos = fieldInfos;
   }
 
   public void seek(Term term) throws IOException {
-    TermInfo ti = parent.tis.get(term);
+    TermInfo ti = tis.get(term);
     seek(ti, term);
   }
 
@@ -60,13 +62,13 @@
     Term term;
     
     // use comparison of fieldinfos to verify that termEnum belongs to the same segment as this SegmentTermDocs
-    if (termEnum instanceof SegmentTermEnum && ((SegmentTermEnum) termEnum).fieldInfos == parent.fieldInfos) {        // optimized case
+    if (termEnum instanceof SegmentTermEnum && ((SegmentTermEnum) termEnum).fieldInfos == fieldInfos) {        // optimized case
       SegmentTermEnum segmentTermEnum = ((SegmentTermEnum) termEnum);
       term = segmentTermEnum.term();
       ti = segmentTermEnum.termInfo();
     } else  {                                         // punt case
       term = termEnum.term();
-      ti = parent.tis.get(term);        
+      ti = tis.get(term);        
     }
     
     seek(ti, term);
@@ -75,7 +77,7 @@
   void seek(TermInfo ti, Term term) throws IOException {
     count = 0;
     payloadLengthAtLastSkip = 0;
-    FieldInfo fi = parent.fieldInfos.fieldInfo(term.field);
+    FieldInfo fi = fieldInfos.fieldInfo(term.field);
     currentFieldStoresPayloads = (fi != null) ? fi.storePayloads : false;
     if (ti == null) {
       df = 0;
@@ -85,8 +87,8 @@
       skipDoc = 0;
       skipCount = 0;
       numSkips = df / skipInterval;
-      freqPointer = ti.freqPointer;
-      proxPointer = ti.proxPointer;
+      freqPointer = ti.getPointers()[0];
+      proxPointer = ti.getPointers()[1];
       skipPointer = freqPointer + ti.skipOffset;
       freqStream.seek(freqPointer);
       haveSkipped = false;
Index: src/java/org/apache/lucene/index/TermInfosWriter.java
===================================================================
--- src/java/org/apache/lucene/index/TermInfosWriter.java	(révision 524580)
+++ src/java/org/apache/lucene/index/TermInfosWriter.java	(copie de travail)
@@ -28,12 +28,12 @@
 
 final class TermInfosWriter {
   /** The file format version, a negative number. */
-  public static final int FORMAT = -2;
+  public static final int FORMAT = -3;
 
   private FieldInfos fieldInfos;
   private IndexOutput output;
   private Term lastTerm = new Term("", "");
-  private TermInfo lastTi = new TermInfo();
+  private TermInfo lastTi;
   private long size = 0;
 
   // TODO: the default values for these two parameters should be settable from
@@ -63,20 +63,20 @@
   private TermInfosWriter other = null;
 
   TermInfosWriter(Directory directory, String segment, FieldInfos fis,
-                  int interval)
+                  int interval, int nbPointer)
        throws IOException {
-    initialize(directory, segment, fis, interval, false);
-    other = new TermInfosWriter(directory, segment, fis, interval, true);
+    initialize(directory, segment, fis, interval, nbPointer, false);
+    other = new TermInfosWriter(directory, segment, fis, interval, nbPointer, true);
     other.other = this;
   }
 
   private TermInfosWriter(Directory directory, String segment, FieldInfos fis,
-                          int interval, boolean isIndex) throws IOException {
-    initialize(directory, segment, fis, interval, isIndex);
+                          int interval, int nbPointer, boolean isIndex) throws IOException {
+    initialize(directory, segment, fis, interval, nbPointer, isIndex);
   }
 
   private void initialize(Directory directory, String segment, FieldInfos fis,
-                          int interval, boolean isi) throws IOException {
+                          int interval, int nbPointer, boolean isi) throws IOException {
     indexInterval = interval;
     fieldInfos = fis;
     isIndex = isi;
@@ -85,6 +85,8 @@
     output.writeLong(0);                          // leave space for size
     output.writeInt(indexInterval);             // write indexInterval
     output.writeInt(skipInterval);              // write skipInterval
+    output.writeInt(nbPointer);              // write nbPointer     
+    lastTi = new TermInfo(nbPointer);
   }
 
   /** Adds a new <Term, TermInfo> pair to the set.
@@ -95,20 +97,24 @@
     if (!isIndex && term.compareTo(lastTerm) <= 0)
       throw new CorruptIndexException("term out of order (\"" + term + 
           "\".compareTo(\"" + lastTerm + "\") <= 0)");
-    if (ti.freqPointer < lastTi.freqPointer)
-      throw new CorruptIndexException("freqPointer out of order (" + ti.freqPointer +
-          " < " + lastTi.freqPointer + ")");
-    if (ti.proxPointer < lastTi.proxPointer)
-      throw new CorruptIndexException("proxPointer out of order (" + ti.proxPointer + 
-          " < " + lastTi.proxPointer + ")");
+    
+    long[] pointers = ti.getPointers();
+    long[] lastPointers = lastTi.getPointers();
+    for (int i = 0; i < pointers.length; i++) {
+      if (pointers[i] < lastPointers[i]) {
+        throw new CorruptIndexException("the " + i + "th pointer out of order (" + pointers[i] +
+            " < " + lastPointers[i] + ")");
+      }
+    }
 
     if (!isIndex && size % indexInterval == 0)
       other.add(lastTerm, lastTi);                      // add an index term
 
     writeTerm(term);                                    // write term
     output.writeVInt(ti.docFreq);                       // write doc freq
-    output.writeVLong(ti.freqPointer - lastTi.freqPointer); // write pointers
-    output.writeVLong(ti.proxPointer - lastTi.proxPointer);
+    for (int i = 0; i < pointers.length; i++) {         // write pointers
+      output.writeVLong(pointers[i] - lastPointers[i]);
+    }
 
     if (ti.docFreq >= skipInterval) {
       output.writeVInt(ti.skipOffset);
Index: src/java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- src/java/org/apache/lucene/index/IndexWriter.java	(révision 524584)
+++ src/java/org/apache/lucene/index/IndexWriter.java	(copie de travail)
@@ -209,7 +209,7 @@
 
   SegmentInfos segmentInfos = new SegmentInfos();       // the segments
   SegmentInfos ramSegmentInfos = new SegmentInfos();    // the segments in ramDirectory
-  private final RAMDirectory ramDirectory = new RAMDirectory(); // for temp segs
+  private RAMDirectory ramDirectory; // for temp segs
   private IndexFileDeleter deleter;
 
   private Lock writeLock;
@@ -565,6 +565,7 @@
 
   private void init(Directory d, Analyzer a, final boolean create, boolean closeDir, IndexDeletionPolicy deletionPolicy, boolean autoCommit)
     throws CorruptIndexException, LockObtainFailedException, IOException {
+    ramDirectory = new RAMDirectory(d.getIndexFormat());
     this.closeDir = closeDir;
     directory = d;
     analyzer = a;
@@ -1370,11 +1371,23 @@
    * for details.</p>
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
+   * @throws IncompatibleFormatException if one of the directory
+   * cannot be read by the index format of the writer
    */
   public synchronized void addIndexes(Directory[] dirs)
-    throws CorruptIndexException, IOException {
+    throws CorruptIndexException, IOException, IncompatibleFormatException {
 
     ensureOpen();
+
+    //check that every reader can be read 
+    for (int i = 0; i < dirs.length; i++) {
+      String dirFmtID = dirs[i].getIndexFormat().getID();
+      if (!directory.getIndexFormat().canRead(dirFmtID)) {
+        throw new IncompatibleFormatException("The " + i + "th directory has the index format '" + dirFmtID
+            + "' which is incompatible with the writer's index format + '" + directory.getIndexFormat().getID() + "'");
+      }
+    }
+
     optimize();					  // start with zero or 1 seg
 
     int start = segmentInfos.size();
@@ -1429,9 +1442,11 @@
    * on an Exception.</p>
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
+   * @throws IncompatibleFormatException if one of the directory
+   * cannot be read by the index format of the writer
    */
   public synchronized void addIndexesNoOptimize(Directory[] dirs)
-      throws CorruptIndexException, IOException {
+      throws CorruptIndexException, IOException, IncompatibleFormatException {
     // Adding indexes can be viewed as adding a sequence of segments S to
     // a sequence of segments T. Segments in T follow the invariants but
     // segments in S may not since they could come from multiple indexes.
@@ -1458,6 +1473,15 @@
     // copy a segment, which may cause doc count to change because deleted
     // docs are garbage collected.
 
+    // 0 check that every reader can be read 
+    for (int i = 0; i < dirs.length; i++) {
+      String dirFmtID = dirs[i].getIndexFormat().getID();
+      if (!directory.getIndexFormat().canRead(dirFmtID)) {
+        throw new IncompatibleFormatException("The " + i + "th directory has the index format '" + dirFmtID
+            + "' which is incompatible with the writer's index format + '" + directory.getIndexFormat().getID() + "'");
+      }
+    }
+
     // 1 flush ram segments
 
     ensureOpen();
@@ -1560,11 +1584,23 @@
    * on an Exception.</p>
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
+   * @throws IncompatibleFormatException if one of the reader
+   * cannot be read by the index format of the writer
    */
   public synchronized void addIndexes(IndexReader[] readers)
-    throws CorruptIndexException, IOException {
+    throws CorruptIndexException, IOException, IncompatibleFormatException {
 
     ensureOpen();
+
+    //check that every reader can be read 
+    for (int i = 0; i < readers.length; i++) {
+      String readerFmtID = readers[i].directory().getIndexFormat().getID();
+      if (!directory.getIndexFormat().canRead(readerFmtID)) {
+        throw new IncompatibleFormatException("The " + i + "th reader has the index format '" + readerFmtID
+            + "' which is incompatible with the writer's index format + '" + directory.getIndexFormat().getID() + "'");
+      }
+    }
+
     optimize();					  // start with zero or 1 seg
 
     final String mergedName = newSegmentName();
Index: src/java/org/apache/lucene/index/SegmentMerger.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentMerger.java	(révision 524584)
+++ src/java/org/apache/lucene/index/SegmentMerger.java	(copie de travail)
@@ -17,10 +17,11 @@
  * limitations under the License.
  */
 
-import java.util.Vector;
-import java.util.Iterator;
-import java.util.Collection;
 import java.io.IOException;
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.Set;
+import java.util.Vector;
 
 import org.apache.lucene.document.FieldSelector;
 import org.apache.lucene.document.FieldSelectorResult;
@@ -51,7 +52,7 @@
   private Vector readers = new Vector();
   private FieldInfos fieldInfos;
 
-  /** This ctor used only by test code.
+  /** This constructor is used only by test code.
    * 
    * @param dir The Directory to merge the other segments into
    * @param name The name of the new segment
@@ -120,12 +121,14 @@
     CompoundFileWriter cfsWriter =
             new CompoundFileWriter(directory, fileName);
 
-    Vector files =
-      new Vector(IndexFileNames.COMPOUND_EXTENSIONS.length + 1);    
-    
+    Set compoundExtensions = directory.getIndexFormat().getCompoundExtensions();
+
+    Vector files = new Vector(compoundExtensions.size() + 1);
+
     // Basic files
-    for (int i = 0; i < IndexFileNames.COMPOUND_EXTENSIONS.length; i++) {
-      files.add(segment + "." + IndexFileNames.COMPOUND_EXTENSIONS[i]);
+    Iterator cpompoundIt = compoundExtensions.iterator();
+    while (cpompoundIt.hasNext()) {
+      files.add(segment + "." + cpompoundIt.next());
     }
 
     // Fieldable norm files
@@ -137,10 +140,13 @@
       }
     }
 
+    Set vectorExtensions = directory.getIndexFormat().getVectorExtensions();
+
     // Vector files
     if (fieldInfos.hasVectors()) {
-      for (int i = 0; i < IndexFileNames.VECTOR_EXTENSIONS.length; i++) {
-        files.add(segment + "." + IndexFileNames.VECTOR_EXTENSIONS[i]);
+      Iterator vectorIt = vectorExtensions.iterator();
+      while (vectorIt.hasNext()) {
+        files.add(segment + "." + vectorIt.next());
       }
     }
 
@@ -186,14 +192,13 @@
     }
     fieldInfos.write(directory, segment + ".fnm");
 
-    FieldsWriter fieldsWriter = // merge field values
-            new FieldsWriter(directory, segment, fieldInfos);
-    
+    FieldsWriter fieldsWriter = directory.getIndexFormat().getFieldsWriter(directory, segment, fieldInfos);
+
     // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're
     // in  merge mode, we use this FieldSelector
     FieldSelector fieldSelectorMerge = new FieldSelector() {
       public FieldSelectorResult accept(String fieldName) {
-        return FieldSelectorResult.LOAD_FOR_MERGE;
+        return FieldSelectorResult.LAZY_LOAD;
       }        
     };
     
@@ -237,28 +242,27 @@
     }
   }
 
-  private IndexOutput freqOutput = null;
-  private IndexOutput proxOutput = null;
   private TermInfosWriter termInfosWriter = null;
   private int skipInterval;
   private SegmentMergeQueue queue = null;
+  private PostingWriter postingWriter;
 
   private final void mergeTerms() throws CorruptIndexException, IOException {
     try {
-      freqOutput = directory.createOutput(segment + ".frq");
-      proxOutput = directory.createOutput(segment + ".prx");
+      postingWriter = directory.getIndexFormat().getPostingWriter(directory, segment);
       termInfosWriter =
               new TermInfosWriter(directory, segment, fieldInfos,
-                                  termIndexInterval);
+                                  termIndexInterval, postingWriter.getNbPointer());
       skipInterval = termInfosWriter.skipInterval;
       queue = new SegmentMergeQueue(readers.size());
+      termInfo = new TermInfo(postingWriter.getNbPointer());
+      lastSkipPointers = new long[postingWriter.getNbPointer()];
 
       mergeTermInfos();
 
     } finally {
-      if (freqOutput != null) freqOutput.close();
-      if (proxOutput != null) proxOutput.close();
       if (termInfosWriter != null) termInfosWriter.close();
+      if (postingWriter != null) postingWriter.close();
       if (queue != null) queue.close();
     }
   }
@@ -301,7 +305,7 @@
     }
   }
 
-  private final TermInfo termInfo = new TermInfo(); // minimize consing
+  private TermInfo termInfo; // minimize consing
 
   /** Merge one term found in one or more segments. The array <code>smis</code>
    *  contains segments that are positioned at the same term. <code>N</code>
@@ -314,22 +318,19 @@
    */
   private final void mergeTermInfo(SegmentMergeInfo[] smis, int n)
           throws CorruptIndexException, IOException {
-    long freqPointer = freqOutput.getFilePointer();
-    long proxPointer = proxOutput.getFilePointer();
+    long[] pointers = postingWriter.getPointers();
 
     int df = appendPostings(smis, n);		  // append posting data
 
-    long skipPointer = writeSkip();
+    long skipPointer = postingWriter.writeSkip(skipBuffer);
 
     if (df > 0) {
       // add an entry to the dictionary with pointers to prox and freq files
-      termInfo.set(df, freqPointer, proxPointer, (int) (skipPointer - freqPointer));
+      termInfo.set(df, pointers, (int) (skipPointer - pointers[0]));
       termInfosWriter.add(smis[0].term, termInfo);
     }
   }
   
-  private byte[] payloadBuffer = null;
-
   /** Process postings from multiple segments all positioned on the
    *  same term. Writes out merged entries into freqOutput and
    *  the proxOutput streams.
@@ -369,45 +370,21 @@
           bufferSkip(lastDoc, storePayloads, lastPayloadLength);
         }
 
-        int docCode = (doc - lastDoc) << 1;	  // use low bit to flag freq=1
-        lastDoc = doc;
-
-        int freq = postings.freq();
-        if (freq == 1) {
-          freqOutput.writeVInt(docCode | 1);	  // write doc & freq=1
-        } else {
-          freqOutput.writeVInt(docCode);	  // write doc
-          freqOutput.writeVInt(freq);		  // write frequency in doc
-        }
-        
-        /** See {@link DocumentWriter#writePostings(Posting[], String) for 
-         *  documentation about the encoding of positions and payloads
-         */
-        int lastPosition = 0;			  // write position deltas
-        for (int j = 0; j < freq; j++) {
-          int position = postings.nextPosition();
-          int delta = position - lastPosition;
+        int[] positions = new int[postings.freq()];
+        Payload[] payloads = new Payload[postings.freq()];
+        for (int j = 0; j < positions.length; j++) {
+          positions[j] = postings.nextPosition();
           if (storePayloads) {
             int payloadLength = postings.getPayloadLength();
-            if (payloadLength == lastPayloadLength) {
-              proxOutput.writeVInt(delta * 2);
-            } else {
-              proxOutput.writeVInt(delta * 2 + 1);
-              proxOutput.writeVInt(payloadLength);
-              lastPayloadLength = payloadLength;
-            }
             if (payloadLength > 0) {
-              if (payloadBuffer == null || payloadBuffer.length < payloadLength) {
-                payloadBuffer = new byte[payloadLength];
-              }
+              byte[] payloadBuffer = new byte[payloadLength];
               postings.getPayload(payloadBuffer, 0);
-              proxOutput.writeBytes(payloadBuffer, 0, payloadLength);
+              payloads[j] = new Payload(payloadBuffer, 0, payloadLength);
             }
-          } else {
-            proxOutput.writeVInt(delta);
           }
-          lastPosition = position;
         }
+        postingWriter.write(doc, lastDoc, postings.freq(), positions, payloads, storePayloads);
+        lastDoc = doc;
       }
     }
     return df;
@@ -416,20 +393,17 @@
   private RAMOutputStream skipBuffer = new RAMOutputStream();
   private int lastSkipDoc;
   private int lastSkipPayloadLength;
-  private long lastSkipFreqPointer;
-  private long lastSkipProxPointer;
+  private long[] lastSkipPointers;
 
   private void resetSkip() {
     skipBuffer.reset();
     lastSkipDoc = 0;
     lastSkipPayloadLength = -1;  // we don't have to write the first length in the skip list
-    lastSkipFreqPointer = freqOutput.getFilePointer();
-    lastSkipProxPointer = proxOutput.getFilePointer();
+    System.arraycopy(postingWriter.getPointers(), 0, lastSkipPointers, 0, lastSkipPointers.length);
   }
 
   private void bufferSkip(int doc, boolean storePayloads, int payloadLength) throws IOException {
-    long freqPointer = freqOutput.getFilePointer();
-    long proxPointer = proxOutput.getFilePointer();
+    long[] pointers = postingWriter.getPointers();
 
     // To efficiently store payloads in the posting lists we do not store the length of
     // every payload. Instead we omit the length for a payload if the previous payload had
@@ -468,20 +442,15 @@
       // current field does not store payloads
       skipBuffer.writeVInt(doc - lastSkipDoc);
     }
-    skipBuffer.writeVInt((int) (freqPointer - lastSkipFreqPointer));
-    skipBuffer.writeVInt((int) (proxPointer - lastSkipProxPointer));
 
+    for (int i = 0 ; i < pointers.length; i++) {
+      skipBuffer.writeVInt((int) (pointers[i] - lastSkipPointers[i]));
+    }
+
     lastSkipDoc = doc;
-    lastSkipFreqPointer = freqPointer;
-    lastSkipProxPointer = proxPointer;
+    System.arraycopy(pointers, 0, lastSkipPointers, 0, lastSkipPointers.length);
   }
 
-  private long writeSkip() throws IOException {
-    long skipPointer = freqOutput.getFilePointer();
-    skipBuffer.writeTo(freqOutput);
-    return skipPointer;
-  }
-
   private void mergeNorms() throws IOException {
     byte[] normBuffer = null;
     IndexOutput output = null;
Index: src/java/org/apache/lucene/index/DocumentWriter.java
===================================================================
--- src/java/org/apache/lucene/index/DocumentWriter.java	(révision 524585)
+++ src/java/org/apache/lucene/index/DocumentWriter.java	(copie de travail)
@@ -73,7 +73,7 @@
     // create field infos
     fieldInfos = new FieldInfos();
     fieldInfos.add(doc);
-    
+
     // invert doc into postingTable
     postingTable.clear();			  // clear postingTable
     fieldLengths = new int[fieldInfos.size()];    // init fieldLengths
@@ -97,8 +97,7 @@
     fieldInfos.write(directory, segment + ".fnm");
 
     // write field values
-    FieldsWriter fieldsWriter =
-            new FieldsWriter(directory, segment, fieldInfos);
+    FieldsWriter fieldsWriter = directory.getIndexFormat().getFieldsWriter(directory, segment, fieldInfos);
     try {
       fieldsWriter.addDocument(doc);
     } finally {
@@ -338,16 +337,15 @@
 
   private final void writePostings(Posting[] postings, String segment)
           throws CorruptIndexException, IOException {
-    IndexOutput freq = null, prox = null;
     TermInfosWriter tis = null;
     TermVectorsWriter termVectorWriter = null;
+    PostingWriter postingWriter = null;
     try {
+      postingWriter = directory.getIndexFormat().getPostingWriter(directory, segment);
       //open files for inverse index storage
-      freq = directory.createOutput(segment + ".frq");
-      prox = directory.createOutput(segment + ".prx");
       tis = new TermInfosWriter(directory, segment, fieldInfos,
-                                termIndexInterval);
-      TermInfo ti = new TermInfo();
+                                termIndexInterval, postingWriter.getNbPointer());
+      TermInfo ti = new TermInfo(postingWriter.getNbPointer());
       String currentField = null;
       boolean currentFieldHasPayloads = false;
       
@@ -375,79 +373,13 @@
         }
         
         // add an entry to the dictionary with pointers to prox and freq files
-        ti.set(1, freq.getFilePointer(), prox.getFilePointer(), -1);
+        ti.set(1, postingWriter.getPointers(), -1);
         tis.add(posting.term, ti);
 
-        // add an entry to the freq file
-        int postingFreq = posting.freq;
-        if (postingFreq == 1)				  // optimize freq=1
-          freq.writeVInt(1);			  // set low bit of doc num.
-        else {
-          freq.writeVInt(0);			  // the document number
-          freq.writeVInt(postingFreq);			  // frequency in doc
-        }
+        postingWriter.write(0, 0, posting.getFreq(), posting.getPositions(), posting.getPayloads(), currentFieldHasPayloads);
 
-        int lastPosition = 0;			  // write positions
-        int[] positions = posting.positions;
-        Payload[] payloads = posting.payloads;
-        int lastPayloadLength = -1;
-        
-        
-        // The following encoding is being used for positions and payloads:
-        // Case 1: current field does not store payloads
-        //           Positions     -> <PositionDelta>^freq
-        //           PositionDelta -> VInt
-        //         The PositionDelta is the difference between the current
-        //         and the previous position
-        // Case 2: current field stores payloads
-        //           Positions     -> <PositionDelta, Payload>^freq
-        //           Payload       ->  <PayloadLength?, PayloadData>
-        //           PositionDelta -> VInt
-        //           PayloadLength -> VInt
-        //           PayloadData   -> byte^PayloadLength
-        //         In this case PositionDelta/2 is the difference between
-        //         the current and the previous position. If PositionDelta
-        //         is odd, then a PayloadLength encoded as VInt follows,
-        //         if PositionDelta is even, then it is assumed that the
-        //         length of the current Payload equals the length of the
-        //         previous Payload.        
-        for (int j = 0; j < postingFreq; j++) {		  // use delta-encoding
-          int position = positions[j];
-          int delta = position - lastPosition;
-          if (currentFieldHasPayloads) {
-            int payloadLength = 0;
-            Payload payload = null;
-            if (payloads != null) {
-              payload = payloads[j];
-              if (payload != null) {
-                payloadLength = payload.length;
-              }
-            }
-            if (payloadLength == lastPayloadLength) {
-            	// the length of the current payload equals the length
-            	// of the previous one. So we do not have to store the length
-            	// again and we only shift the position delta by one bit
-              prox.writeVInt(delta * 2);
-            } else {
-            	// the length of the current payload is different from the
-            	// previous one. We shift the position delta, set the lowest
-            	// bit and store the current payload length as VInt.
-              prox.writeVInt(delta * 2 + 1);
-              prox.writeVInt(payloadLength);
-              lastPayloadLength = payloadLength;
-            }
-            if (payloadLength > 0) {
-            	// write current payload
-              prox.writeBytes(payload.data, payload.offset, payload.length);
-            }
-          } else {
-          	// field does not store payloads, just write position delta as VInt
-            prox.writeVInt(delta);
-          }
-          lastPosition = position;
-        }
         if (termVectorWriter != null && termVectorWriter.isFieldOpen()) {
-            termVectorWriter.addTerm(posting.term.text(), postingFreq, posting.positions, posting.offsets);
+            termVectorWriter.addTerm(posting.term.text(), posting.freq, posting.positions, posting.offsets);
         }
       }
       if (termVectorWriter != null)
@@ -456,9 +388,8 @@
       // make an effort to close all streams we can but remember and re-throw
       // the first exception encountered in this process
       IOException keep = null;
-      if (freq != null) try { freq.close(); } catch (IOException e) { if (keep == null) keep = e; }
-      if (prox != null) try { prox.close(); } catch (IOException e) { if (keep == null) keep = e; }
       if (tis  != null) try {  tis.close(); } catch (IOException e) { if (keep == null) keep = e; }
+      if (postingWriter  != null) try {  postingWriter.close(); } catch (IOException e) { if (keep == null) keep = e; }
       if (termVectorWriter  != null) try {  termVectorWriter.close(); } catch (IOException e) { if (keep == null) keep = e; }
       if (keep != null) throw (IOException) keep.fillInStackTrace();
     }
@@ -489,32 +420,3 @@
     return fieldInfos.size();
   }
 }
-
-final class Posting {				  // info about a Term in a doc
-  Term term;					  // the Term
-  int freq;					  // its frequency in doc
-  int[] positions;				  // positions it occurs at
-  Payload[] payloads; // the payloads of the terms
-  TermVectorOffsetInfo [] offsets;
-  
-
-  Posting(Term t, int position, Payload payload, TermVectorOffsetInfo offset) {
-    term = t;
-    freq = 1;
-    positions = new int[1];
-    positions[0] = position;
-    
-    if (payload != null) {
-      payloads = new Payload[1];
-      payloads[0] = payload;
-    } else 
-      payloads = null;    
-    
-
-    if(offset != null){
-      offsets = new TermVectorOffsetInfo[1];
-      offsets[0] = offset;
-    } else
-      offsets = null;
-  }
-}
Index: src/java/org/apache/lucene/index/IndexFileDeleter.java
===================================================================
--- src/java/org/apache/lucene/index/IndexFileDeleter.java	(révision 524580)
+++ src/java/org/apache/lucene/index/IndexFileDeleter.java	(copie de travail)
@@ -126,7 +126,7 @@
     // First pass: walk the files and initialize our ref
     // counts:
     long currentGen = segmentInfos.getGeneration();
-    IndexFileNameFilter filter = IndexFileNameFilter.getFilter();
+    IndexFileNameFilter filter = directory.getIndexFormat().getIndexFileNameFilter();
 
     String[] files = directory.list();
     if (files == null)
@@ -258,7 +258,7 @@
     String[] files = directory.list();
     if (files == null)
       throw new IOException("cannot read directory " + directory + ": list() returned null");
-    IndexFileNameFilter filter = IndexFileNameFilter.getFilter();
+    IndexFileNameFilter filter = directory.getIndexFormat().getIndexFileNameFilter();
     for(int i=0;i<files.length;i++) {
       String fileName = files[i];
       if (filter.accept(null, fileName) && !refCounts.containsKey(fileName) && !fileName.equals(IndexFileNames.SEGMENTS_GEN)) {
Index: src/java/org/apache/lucene/index/IndexFileNameFilter.java
===================================================================
--- src/java/org/apache/lucene/index/IndexFileNameFilter.java	(révision 524580)
+++ src/java/org/apache/lucene/index/IndexFileNameFilter.java	(copie de travail)
@@ -19,7 +19,7 @@
 
 import java.io.File;
 import java.io.FilenameFilter;
-import java.util.HashSet;
+import java.util.Set;
 
 /**
  * Filename filter that accept filenames and extensions only created by Lucene.
@@ -29,19 +29,17 @@
  */
 public class IndexFileNameFilter implements FilenameFilter {
 
-  static IndexFileNameFilter singleton = new IndexFileNameFilter();
-  private HashSet extensions;
-  private HashSet extensionsInCFS;
+  private Set extensions;
+  private Set extensionsInCFS;
 
-  public IndexFileNameFilter() {
-    extensions = new HashSet();
-    for (int i = 0; i < IndexFileNames.INDEX_EXTENSIONS.length; i++) {
-      extensions.add(IndexFileNames.INDEX_EXTENSIONS[i]);
-    }
-    extensionsInCFS = new HashSet();
-    for (int i = 0; i < IndexFileNames.INDEX_EXTENSIONS_IN_COMPOUND_FILE.length; i++) {
-      extensionsInCFS.add(IndexFileNames.INDEX_EXTENSIONS_IN_COMPOUND_FILE[i]);
-    }
+  /**
+   * Contructor
+   * 
+   * @param indexFormat the format of the index
+   */
+  public IndexFileNameFilter(IndexFormat indexFormat) {
+    extensions = indexFormat.getIndexExtensions();
+    extensionsInCFS = indexFormat.getIndexExtensionsInCounpoundFile();
   }
 
   /* (non-Javadoc)
@@ -87,8 +85,4 @@
     }
     return false;
   }
-
-  public static IndexFileNameFilter getFilter() {
-    return singleton;
-  }
 }
Index: src/java/org/apache/lucene/index/SegmentInfo.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentInfo.java	(révision 524580)
+++ src/java/org/apache/lucene/index/SegmentInfo.java	(copie de travail)
@@ -21,8 +21,10 @@
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.store.IndexInput;
 import java.io.IOException;
+import java.util.Iterator;
 import java.util.List;
 import java.util.ArrayList;
+import java.util.Set;
 
 final class SegmentInfo {
 
@@ -409,8 +411,9 @@
     if (useCompoundFile) {
       files.add(name + "." + IndexFileNames.COMPOUND_FILE_EXTENSION);
     } else {
-      for (int i = 0; i < IndexFileNames.INDEX_EXTENSIONS_IN_COMPOUND_FILE.length; i++) {
-        String ext = IndexFileNames.INDEX_EXTENSIONS_IN_COMPOUND_FILE[i];
+      Iterator extensionsIt = dir.getIndexFormat().getIndexExtensionsInCounpoundFile().iterator();
+      while (extensionsIt.hasNext()) {
+        String ext = (String) extensionsIt.next();
         String fileName = name + "." + ext;
         if (dir.fileExists(fileName)) {
           files.add(fileName);
Index: src/java/org/apache/lucene/index/SegmentReader.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentReader.java	(révision 524584)
+++ src/java/org/apache/lucene/index/SegmentReader.java	(copie de travail)
@@ -38,11 +38,11 @@
   FieldInfos fieldInfos;
   private FieldsReader fieldsReader;
 
-  TermInfosReader tis;
-  TermVectorsReader termVectorsReaderOrig = null;
-  ThreadLocal termVectorsLocal = new ThreadLocal();
+  private TermInfosReader tis;
+  private TermVectorsReader termVectorsReaderOrig = null;
+  private ThreadLocal termVectorsLocal = new ThreadLocal();
 
-  BitVector deletedDocs = null;
+  private BitVector deletedDocs = null;
   private boolean deletedDocsDirty = false;
   private boolean normsDirty = false;
   private boolean undeleteAll = false;
@@ -51,8 +51,7 @@
   private boolean rollbackNormsDirty = false;
   private boolean rollbackUndeleteAll = false;
 
-  IndexInput freqStream;
-  IndexInput proxStream;
+  private PostingReader postingReader;
 
   // optionally used for the .nrm file shared by multiple norms
   private IndexInput singleNormStream;
@@ -174,8 +173,9 @@
 
       // No compound file exists - use the multi-file format
       fieldInfos = new FieldInfos(cfsDir, segment + ".fnm");
-      fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos);
 
+      fieldsReader = cfsDir.getIndexFormat().getFieldsReader(cfsDir, segment, fieldInfos);
+
       // Verify two sources of "maxDoc" agree:
       if (fieldsReader.size() != si.docCount) {
         throw new CorruptIndexException("doc counts differ for segment " + si.name + ": fieldsReader shows " + fieldsReader.size() + " but segmentInfo shows " + si.docCount);
@@ -195,8 +195,8 @@
 
       // make sure that all index files have been read or are kept open
       // so that if an index update removes them we'll still have them
-      freqStream = cfsDir.openInput(segment + ".frq");
-      proxStream = cfsDir.openInput(segment + ".prx");
+      postingReader = cfsDir.getIndexFormat().getPostingReader(cfsDir, segment);
+
       openNorms(cfsDir);
 
       if (fieldInfos.hasVectors()) { // open term vector files only as needed
@@ -250,12 +250,10 @@
     if (tis != null) {
       tis.close();
     }
+    if (postingReader != null) {
+      postingReader.close();
+    }
 
-    if (freqStream != null)
-      freqStream.close();
-    if (proxStream != null)
-      proxStream.close();
-
     closeNorms();
 
     if (termVectorsReaderOrig != null)
@@ -329,12 +327,12 @@
 
   public TermDocs termDocs() throws IOException {
     ensureOpen();
-    return new SegmentTermDocs(this);
+    return postingReader.termDocs(deletedDocs, tis, fieldInfos);
   }
 
   public TermPositions termPositions() throws IOException {
     ensureOpen();
-    return new SegmentTermPositions(this);
+    return postingReader.termPositions(deletedDocs, tis, fieldInfos);
   }
 
   public int docFreq(Term t) throws IOException {
Index: src/java/org/apache/lucene/index/DefaultFieldsReader.java
===================================================================
--- src/java/org/apache/lucene/index/DefaultFieldsReader.java	(révision 0)
+++ src/java/org/apache/lucene/index/DefaultFieldsReader.java	(révision 0)
@@ -0,0 +1,53 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.index;
+
+import java.io.IOException;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Fieldable;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexInput;
+
+/**
+ * The default implementation of a {@link FieldsReader}
+ * 
+ * $Id$
+ */
+public class DefaultFieldsReader extends FieldsReader {
+
+  protected DefaultFieldsReader(Directory d, String segment, FieldInfos fn) throws IOException {
+    super(d, segment, fn);
+  }
+
+  /**
+   * There is no information stored at the document level
+   */
+  protected Document createDocument(IndexInput fieldsStream) {
+    return new Document();
+  }
+  
+  protected FieldData createFieldData(FieldInfo fi) {
+    return new DefaultFieldData();
+  }
+  
+  protected Fieldable createFieldable(FieldInfo fi, FieldData fieldData) {
+    return new Field(fi, (DefaultFieldData) fieldData);
+  }
+}

Modification de propriétés sur src/java/org/apache/lucene/index/DefaultFieldsReader.java
___________________________________________________________________
Nom : svn:mime-type
   + text/plain
Nom : svn:keywords
   + Date Revision Author HeadURL Id
Nom : svn:eol-style
   + native

Index: src/java/org/apache/lucene/store/RAMDirectory.java
===================================================================
--- src/java/org/apache/lucene/store/RAMDirectory.java	(révision 524580)
+++ src/java/org/apache/lucene/store/RAMDirectory.java	(copie de travail)
@@ -17,14 +17,17 @@
  * limitations under the License.
  */
 
+import java.io.File;
+import java.io.FileNotFoundException;
 import java.io.IOException;
-import java.io.FileNotFoundException;
-import java.io.File;
 import java.io.Serializable;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.Set;
 
+import org.apache.lucene.index.DefaultIndexFormat;
+import org.apache.lucene.index.IndexFormat;
+
 /**
  * A memory-resident {@link Directory} implementation.  Locking
  * implementation is by default the {@link SingleInstanceLockFactory}
@@ -43,8 +46,21 @@
   // Lock acquisition sequence:  RAMDirectory, then RAMFile
   // *****
 
-  /** Constructs an empty {@link Directory}. */
+  /**
+   * Constructs an empty {@link Directory}.
+   * The index format used the the default one 
+   */
   public RAMDirectory() {
+    this(new DefaultIndexFormat());
+  }
+
+  /**
+   * Contructor
+   * 
+   * @param indexFormat the format of the index
+   */
+  public RAMDirectory(IndexFormat indexFormat) {
+    setIndexFormat(indexFormat);
     setLockFactory(new SingleInstanceLockFactory());
   }
 
@@ -67,9 +83,9 @@
   public RAMDirectory(Directory dir) throws IOException {
     this(dir, false);
   }
-  
+
   private RAMDirectory(Directory dir, boolean closeDir) throws IOException {
-    this();
+    this(dir.getIndexFormat());
     Directory.copy(dir, this, closeDir);
   }
 
@@ -239,6 +255,22 @@
   public void close() {
     fileMap = null;
   }
+  
+  /**
+   * For debug purpose, list every files name of this directory.
+   * The code was commented because the lockID is based on the toString() function
+   */
+//  public String toString() {
+//    String[] f = list();
+//    StringBuffer buffer = new StringBuffer();
+//    for (int i = 0; i< f.length; i++) {
+//      buffer.append(f[i]);
+//      if (i != f.length - 1) {
+//        buffer.append(", ");
+//      }
+//    }
+//    return buffer.toString();
+//  }
 
   /**
    * @throws AlreadyClosedException if this IndexReader is closed
Index: src/java/org/apache/lucene/store/Directory.java
===================================================================
--- src/java/org/apache/lucene/store/Directory.java	(révision 524580)
+++ src/java/org/apache/lucene/store/Directory.java	(copie de travail)
@@ -19,6 +19,8 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.index.IndexFormat;
+
 /** A Directory is a flat list of files.  Files may be written once, when they
  * are created.  Once a file is created it may only be opened for read, or
  * deleted.  Random access is permitted both when reading and writing.
@@ -42,6 +44,19 @@
    * this Directory instance). */
   protected LockFactory lockFactory;
 
+  private IndexFormat indexFormat;
+
+  public IndexFormat getIndexFormat() {
+    return indexFormat;
+  }
+
+  public void setIndexFormat(IndexFormat indexFormat) {
+    if (this.indexFormat != null) {
+      throw new IllegalStateException("The index format has already been set on the directory");
+    }
+    this.indexFormat = indexFormat;
+  }
+
   /** Returns an array of strings, one for each file in the
    * directory.  This method may return null (for example for
    * {@link FSDirectory} if the underlying directory doesn't
Index: src/java/org/apache/lucene/store/IndexOutput.java
===================================================================
--- src/java/org/apache/lucene/store/IndexOutput.java	(révision 524586)
+++ src/java/org/apache/lucene/store/IndexOutput.java	(copie de travail)
@@ -31,6 +31,17 @@
    */
   public abstract void writeByte(byte b) throws IOException;
 
+  /**
+   * Write a byte directly from an input stream.
+   * 
+   * @param in the stream to read
+   * @throws IOException
+   * @see #writeByte(byte)
+   */
+  public void writeByte(IndexInput in) throws IOException {
+    writeByte(in.readByte());
+  }
+
   /** Writes an array of bytes.
    * @param b the bytes to write
    * @param length the number of bytes to write
@@ -40,6 +51,7 @@
     writeBytes(b, 0, length);
   }
 
+
   /** Writes an array of bytes.
    * @param b the bytes to write
    * @param offset the offset in the byte array
@@ -48,6 +60,20 @@
    */
   public abstract void writeBytes(byte[] b, int offset, int length) throws IOException;
 
+  /**
+   * Write a batch of bytes directly from an input stream.
+   * 
+   * @param in the stream to read
+   * @param length the number of bytes to write
+   * @throws IOException
+   * @see #writeBytes(byte[], int)
+   */
+  public void writeBytes(IndexInput in, long length) throws IOException {
+    while (length-- > 0) {
+      writeByte(in.readByte());
+    }
+  }
+  
   /** Writes an int as four bytes.
    * @see IndexInput#readInt()
    */
@@ -58,6 +84,20 @@
     writeByte((byte) i);
   }
 
+  /**
+   * Writes an int as four bytes directly from an input stream.
+   * 
+   * @param in the stream to read
+   * @throws IOException 
+   * @see #writeInt(int)
+   */
+  public void writeInt(IndexInput in) throws IOException {
+    writeByte(in.readByte());
+    writeByte(in.readByte());
+    writeByte(in.readByte());
+    writeByte(in.readByte());
+  }
+
   /** Writes an int in a variable-length format.  Writes between one and
    * five bytes.  Smaller values take fewer bytes.  Negative numbers are not
    * supported.
@@ -71,6 +111,22 @@
     writeByte((byte)i);
   }
 
+  /**
+   * Writes an int in a variable-length format directly from an input stream.
+   * 
+   * @param in the stream to read
+   * @throws IOException 
+   * @see #writeVInt(int)
+   */
+  public void writeVInt(IndexInput in) throws IOException {
+    byte b = in.readByte();
+    writeByte(b);
+    while ((b & 0x80) != 0) {
+      b = in.readByte();
+      writeByte(b);
+    }
+  }
+
   /** Writes a long as eight bytes.
    * @see IndexInput#readLong()
    */
@@ -79,6 +135,24 @@
     writeInt((int) i);
   }
 
+  /**
+   * Writes a long as eight bytes directly from an input stream.
+   * 
+   * @param in the stream to read
+   * @throws IOException 
+   * @see #writeLong(long)
+   */
+  public void writeLong(IndexInput in) throws IOException {
+    writeByte(in.readByte());
+    writeByte(in.readByte());
+    writeByte(in.readByte());
+    writeByte(in.readByte());
+    writeByte(in.readByte());
+    writeByte(in.readByte());
+    writeByte(in.readByte());
+    writeByte(in.readByte());
+  }
+
   /** Writes an long in a variable-length format.  Writes between one and five
    * bytes.  Smaller values take fewer bytes.  Negative numbers are not
    * supported.
@@ -92,6 +166,22 @@
     writeByte((byte)i);
   }
 
+  /**
+   * Writes an long in a variable-length format directly from an input stream.
+   * 
+   * @param in the stream to read
+   * @throws IOException 
+   * @see #writeVLong(long)
+   */
+  public void writeVLong(IndexInput in) throws IOException {
+    byte b = in.readByte();
+    writeByte(b);
+    while ((b & 0x80) != 0) {
+      b = in.readByte();
+      writeByte(b);
+    }
+  }
+
   /** Writes a string.
    * @see IndexInput#readString()
    */
@@ -101,6 +191,19 @@
     writeChars(s, 0, length);
   }
 
+  /**
+   * Writes a string directly from an input stream.
+   * 
+   * @param in the stream to read
+   * @throws IOException 
+   * @see #writeString(String)
+   */
+  public void writeString(IndexInput in) throws IOException {
+    int length = in.readVInt();
+    writeVInt(length);
+    writeChars(in, length);
+  }
+
   /** Writes a sequence of UTF-8 encoded characters from a string.
    * @param s the source of the characters
    * @param start the first character in the sequence
@@ -113,18 +216,40 @@
     for (int i = start; i < end; i++) {
       final int code = (int)s.charAt(i);
       if (code >= 0x01 && code <= 0x7F)
-	writeByte((byte)code);
+        writeByte((byte)code);
       else if (((code >= 0x80) && (code <= 0x7FF)) || code == 0) {
-	writeByte((byte)(0xC0 | (code >> 6)));
-	writeByte((byte)(0x80 | (code & 0x3F)));
+        writeByte((byte)(0xC0 | (code >> 6)));
+        writeByte((byte)(0x80 | (code & 0x3F)));
       } else {
-	writeByte((byte)(0xE0 | (code >>> 12)));
-	writeByte((byte)(0x80 | ((code >> 6) & 0x3F)));
-	writeByte((byte)(0x80 | (code & 0x3F)));
+        writeByte((byte)(0xE0 | (code >>> 12)));
+        writeByte((byte)(0x80 | ((code >> 6) & 0x3F)));
+        writeByte((byte)(0x80 | (code & 0x3F)));
       }
     }
   }
 
+  /**
+   * Writes a sequence of UTF-8 encoded characters directly from an input stream.
+   * 
+   * @param in the stream to read
+   * @param length the number of characters in the sequence
+   * @throws IOException 
+   * @see #writeChars(String,int,int)
+   */
+  public void writeChars(IndexInput in, int length)
+       throws IOException {
+    for (int i = 0; i < length; i++) {
+      byte b = in.readByte();
+      writeByte(b);
+      if ((b & 0x80) != 0) {
+        writeByte(in.readByte());
+        if ((b & 0xE0) == 0xE0) {
+          writeByte(in.readByte());
+        }
+      }
+    }
+  }
+
   /** Forces any buffered output to be written. */
   public abstract void flush() throws IOException;
 
Index: src/java/org/apache/lucene/store/FSDirectory.java
===================================================================
--- src/java/org/apache/lucene/store/FSDirectory.java	(révision 524580)
+++ src/java/org/apache/lucene/store/FSDirectory.java	(copie de travail)
@@ -26,9 +26,9 @@
 import java.security.NoSuchAlgorithmException;
 import java.util.Hashtable;
 
-import org.apache.lucene.index.IndexFileNameFilter;
-
-// Used only for WRITE_LOCK_NAME in deprecated create=true case:
+import org.apache.lucene.index.DefaultIndexFormat;
+import org.apache.lucene.index.IndexFormat;
+//Used only for WRITE_LOCK_NAME in deprecated create=true case:
 import org.apache.lucene.index.IndexWriter;
 
 /**
@@ -165,6 +165,16 @@
   public static FSDirectory getDirectory(File file, LockFactory lockFactory)
     throws IOException
   {
+    return getDirectory(file, lockFactory, new DefaultIndexFormat());
+  }
+  /** Returns the directory instance for the named location.
+   * @param file the path to the directory.
+   * @param lockFactory instance of {@link LockFactory} providing the
+   *        locking implementation.
+   * @return the FSDirectory for the named file.  */
+  public static FSDirectory getDirectory(File file, LockFactory lockFactory, IndexFormat indexFormat)
+    throws IOException
+  {
     file = new File(file.getCanonicalPath());
 
     if (file.exists() && !file.isDirectory())
@@ -183,7 +193,7 @@
         } catch (Exception e) {
           throw new RuntimeException("cannot load FSDirectory class: " + e.toString(), e);
         }
-        dir.init(file, lockFactory);
+        dir.init(file, lockFactory, indexFormat);
         DIRECTORIES.put(file, dir);
       } else {
         // Catch the case where a Directory is pulled from the cache, but has a
@@ -237,7 +247,7 @@
 
   private void create() throws IOException {
     if (directory.exists()) {
-      String[] files = directory.list(IndexFileNameFilter.getFilter());            // clear old files
+      String[] files = directory.list(getIndexFormat().getIndexFileNameFilter());            // clear old files
       if (files == null)
         throw new IOException("cannot read directory " + directory.getAbsolutePath() + ": list() returned null");
       for (int i = 0; i < files.length; i++) {
@@ -254,13 +264,14 @@
 
   protected FSDirectory() {};                     // permit subclassing
 
-  private void init(File path, LockFactory lockFactory) throws IOException {
+  private void init(File path, LockFactory lockFactory, IndexFormat indexFormat) throws IOException {
 
     // Set up lockFactory with cascaded defaults: if an instance was passed in,
     // use that; else if locks are disabled, use NoLockFactory; else if the
     // system property org.apache.lucene.store.FSDirectoryLockFactoryClass is set,
     // instantiate that; else, use SimpleFSLockFactory:
 
+    setIndexFormat(indexFormat);
     directory = path;
 
     boolean doClearLockID = false;
@@ -317,7 +328,7 @@
 
   /** Returns an array of strings, one for each Lucene index file in the directory. */
   public String[] list() {
-    return directory.list(IndexFileNameFilter.getFilter());
+    return directory.list(getIndexFormat().getIndexFileNameFilter());
   }
 
   /** Returns true iff a file with the given name exists. */
Index: src/java/org/apache/lucene/document/Field.java
===================================================================
--- src/java/org/apache/lucene/document/Field.java	(révision 524580)
+++ src/java/org/apache/lucene/document/Field.java	(copie de travail)
@@ -17,11 +17,13 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.util.Parameter;
-
 import java.io.Reader;
 import java.io.Serializable;
 
+import org.apache.lucene.index.DefaultFieldData;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.util.Parameter;
+
 /**
   A field is a section of a Document.  Each field has two parts, a name and a
   value.  Values may be free text, provided as a String or as a Reader, or they
@@ -31,7 +33,7 @@
   */
 
 public final class Field extends AbstractField implements Fieldable, Serializable {
-  
+
   /** Specifies whether and how a field should be stored. */
   public static final class Store extends Parameter implements Serializable {
 
@@ -131,24 +133,18 @@
      */ 
     public static final TermVector WITH_POSITIONS_OFFSETS = new TermVector("WITH_POSITIONS_OFFSETS");
   }
-  
-  
-  /** The value of the field as a String, or null.  If null, the Reader value
-   * or binary value is used.  Exactly one of stringValue(), readerValue(), and
-   * binaryValue() must be set. */
-  public String stringValue()   { return fieldsData instanceof String ? (String)fieldsData : null; }
-  
-  /** The value of the field as a Reader, or null.  If null, the String value
-   * or binary value is  used.  Exactly one of stringValue(), readerValue(),
-   * and binaryValue() must be set. */
-  public Reader readerValue()   { return fieldsData instanceof Reader ? (Reader)fieldsData : null; }
-  
-  /** The value of the field in Binary, or null.  If null, the Reader or
-   * String value is used.  Exactly one of stringValue(), readerValue() and
-   * binaryValue() must be set. */
-  public byte[] binaryValue()   { return fieldsData instanceof byte[] ? (byte[])fieldsData : null; }
-  
+
   /**
+   * Contructor used when getting data from the index
+   * 
+   * @param fi the info of the field
+   * @param fieldData the data of the field
+   */
+  public Field(FieldInfo fi, DefaultFieldData fieldData) {
+    super(fi, fieldData);
+  }
+
+  /**
    * Create a field by specifying its name, value and how it will
    * be saved in the index. Term vectors will not be stored in the index.
    * 
@@ -182,57 +178,9 @@
    * </ul> 
    */ 
   public Field(String name, String value, Store store, Index index, TermVector termVector) {
-    if (name == null)
-      throw new NullPointerException("name cannot be null");
-    if (value == null)
-      throw new NullPointerException("value cannot be null");
+    super(name, new DefaultFieldData(value), store, index, termVector);
     if (name.length() == 0 && value.length() == 0)
       throw new IllegalArgumentException("name and value cannot both be empty");
-    if (index == Index.NO && store == Store.NO)
-      throw new IllegalArgumentException("it doesn't make sense to have a field that "
-         + "is neither indexed nor stored");
-    if (index == Index.NO && termVector != TermVector.NO)
-      throw new IllegalArgumentException("cannot store term vector information "
-         + "for a field that is not indexed");
-          
-    this.name = name.intern();        // field names are interned
-    this.fieldsData = value;
-
-    if (store == Store.YES){
-      this.isStored = true;
-      this.isCompressed = false;
-    }
-    else if (store == Store.COMPRESS) {
-      this.isStored = true;
-      this.isCompressed = true;
-    }
-    else if (store == Store.NO){
-      this.isStored = false;
-      this.isCompressed = false;
-    }
-    else
-      throw new IllegalArgumentException("unknown store parameter " + store);
-   
-    if (index == Index.NO) {
-      this.isIndexed = false;
-      this.isTokenized = false;
-    } else if (index == Index.TOKENIZED) {
-      this.isIndexed = true;
-      this.isTokenized = true;
-    } else if (index == Index.UN_TOKENIZED) {
-      this.isIndexed = true;
-      this.isTokenized = false;
-    } else if (index == Index.NO_NORMS) {
-      this.isIndexed = true;
-      this.isTokenized = false;
-      this.omitNorms = true;
-    } else {
-      throw new IllegalArgumentException("unknown index parameter " + index);
-    }
-    
-    this.isBinary = false;
-
-    setStoreTermVector(termVector);
   }
 
   /**
@@ -257,23 +205,7 @@
    * @throws NullPointerException if name or reader is <code>null</code>
    */ 
   public Field(String name, Reader reader, TermVector termVector) {
-    if (name == null)
-      throw new NullPointerException("name cannot be null");
-    if (reader == null)
-      throw new NullPointerException("reader cannot be null");
-    
-    this.name = name.intern();        // field names are interned
-    this.fieldsData = reader;
-    
-    this.isStored = false;
-    this.isCompressed = false;
-    
-    this.isIndexed = true;
-    this.isTokenized = true;
-    
-    this.isBinary = false;
-    
-    setStoreTermVector(termVector);
+    super(name, new DefaultFieldData(reader), Store.NO, Index.TOKENIZED, termVector);
   }
   
   /**
@@ -285,34 +217,91 @@
    * @throws IllegalArgumentException if store is <code>Store.NO</code> 
    */
   public Field(String name, byte[] value, Store store) {
-    if (name == null)
-      throw new IllegalArgumentException("name cannot be null");
-    if (value == null)
-      throw new IllegalArgumentException("value cannot be null");
-    
-    this.name = name.intern();
-    this.fieldsData = value;
-    
-    if (store == Store.YES){
-      this.isStored = true;
-      this.isCompressed = false;
+    super(name, new DefaultFieldData(value), store, Index.NO, TermVector.NO);
+  }
+
+  /**
+   * Override the store management to handle compression
+   */
+  protected void setStore(Field.Store store) {
+    if (store == Field.Store.YES) {
+      isStored = true;
+      ((DefaultFieldData) fieldData).setCompressed(false);
+    } else if (store == Field.Store.COMPRESS) {
+      isStored = true;
+      ((DefaultFieldData) fieldData).setCompressed(true);
+    } else if (store == Field.Store.NO) {
+      if (isBinary()) {
+        throw new IllegalArgumentException("binary values can't be unstored");
+      }
+      isStored = false;
+      ((DefaultFieldData) fieldData).setCompressed(false);
+    } else {
+      throw new IllegalArgumentException("unknown store parameter " + store);
     }
-    else if (store == Store.COMPRESS) {
-      this.isStored = true;
-      this.isCompressed = true;
+  }
+
+  /** Prints a Field for human consumption. */
+  public String toString() {
+    StringBuffer result = new StringBuffer();
+    if (isStored()) {
+      result.append("stored");
+      if (((DefaultFieldData) fieldData).isCompressed())
+        result.append("/compressed");
+      else
+        result.append("/uncompressed");
     }
-    else if (store == Store.NO)
-      throw new IllegalArgumentException("binary values can't be unstored");
-    else
-      throw new IllegalArgumentException("unknown store parameter " + store);
-    
-    this.isIndexed   = false;
-    this.isTokenized = false;
-    
-    this.isBinary    = true;
-    
-    setStoreTermVector(TermVector.NO);
+    if (isIndexed()) {
+      if (result.length() > 0)
+        result.append(",");
+      result.append("indexed");
+    }
+    if (isTokenized()) {
+      if (result.length() > 0)
+        result.append(",");
+      result.append("tokenized");
+    }
+    if (isTermVectorStored()) {
+      if (result.length() > 0)
+        result.append(",");
+      result.append("termVector");
+    }
+    if (isStoreOffsetWithTermVector()) {
+      if (result.length() > 0)
+        result.append(",");
+      result.append("termVectorOffsets");
+    }
+    if (isStorePositionWithTermVector()) {
+      if (result.length() > 0)
+        result.append(",");
+      result.append("termVectorPosition");
+    }
+    if (isBinary()) {
+      if (result.length() > 0)
+        result.append(",");
+      result.append("binary");
+    }
+    if (getOmitNorms()) {
+      result.append(",omitNorms");
+    }
+    if (isLazy()) {
+      result.append(",lazy");
+    }
+    result.append('<');
+    result.append(name());
+    result.append(':');
+
+    result.append(getFieldData());
+
+    result.append('>');
+    return result.toString();
   }
 
+  /**
+   * @return true if the value of the field is stored and compressed within the index
+   */
+  public final boolean isCompressed() {
+    return ((DefaultFieldData) fieldData).isCompressed();
+  }
 
 }
Index: src/java/org/apache/lucene/document/AbstractField.java
===================================================================
--- src/java/org/apache/lucene/document/AbstractField.java	(révision 524580)
+++ src/java/org/apache/lucene/document/AbstractField.java	(copie de travail)
@@ -1,274 +1,322 @@
-package org.apache.lucene.document;
-/**
- * Copyright 2006 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/**
- *
- *
- **/
-public abstract class AbstractField implements Fieldable {
-
-  protected String name = "body";
-  protected boolean storeTermVector = false;
-  protected boolean storeOffsetWithTermVector = false;
-  protected boolean storePositionWithTermVector = false;
-  protected boolean omitNorms = false;
-  protected boolean isStored = false;
-  protected boolean isIndexed = true;
-  protected boolean isTokenized = true;
-  protected boolean isBinary = false;
-  protected boolean isCompressed = false;
-  protected boolean lazy = false;
-  protected float boost = 1.0f;
-  // the one and only data object for all different kind of field values
-  protected Object fieldsData = null;
-
-  protected AbstractField()
-  {
-    
-  }
-
-  protected AbstractField(String name, Field.Store store, Field.Index index, Field.TermVector termVector) {
-    if (name == null)
-      throw new NullPointerException("name cannot be null");
-    this.name = name.intern();        // field names are interned
-
-    if (store == Field.Store.YES){
-      this.isStored = true;
-      this.isCompressed = false;
-    }
-    else if (store == Field.Store.COMPRESS) {
-      this.isStored = true;
-      this.isCompressed = true;
-    }
-    else if (store == Field.Store.NO){
-      this.isStored = false;
-      this.isCompressed = false;
-    }
-    else
-      throw new IllegalArgumentException("unknown store parameter " + store);
-
-    if (index == Field.Index.NO) {
-      this.isIndexed = false;
-      this.isTokenized = false;
-    } else if (index == Field.Index.TOKENIZED) {
-      this.isIndexed = true;
-      this.isTokenized = true;
-    } else if (index == Field.Index.UN_TOKENIZED) {
-      this.isIndexed = true;
-      this.isTokenized = false;
-    } else if (index == Field.Index.NO_NORMS) {
-      this.isIndexed = true;
-      this.isTokenized = false;
-      this.omitNorms = true;
-    } else {
-      throw new IllegalArgumentException("unknown index parameter " + index);
-    }
-
-    this.isBinary = false;
-
-    setStoreTermVector(termVector);
-  }
-
-  /** Sets the boost factor hits on this field.  This value will be
-   * multiplied into the score of all hits on this this field of this
-   * document.
-   *
-   * <p>The boost is multiplied by {@link org.apache.lucene.document.Document#getBoost()} of the document
-   * containing this field.  If a document has multiple fields with the same
-   * name, all such values are multiplied together.  This product is then
-   * multipled by the value {@link org.apache.lucene.search.Similarity#lengthNorm(String,int)}, and
-   * rounded by {@link org.apache.lucene.search.Similarity#encodeNorm(float)} before it is stored in the
-   * index.  One should attempt to ensure that this product does not overflow
-   * the range of that encoding.
-   *
-   * @see org.apache.lucene.document.Document#setBoost(float)
-   * @see org.apache.lucene.search.Similarity#lengthNorm(String, int)
-   * @see org.apache.lucene.search.Similarity#encodeNorm(float)
-   */
-  public void setBoost(float boost) {
-    this.boost = boost;
-  }
-
-  /** Returns the boost factor for hits for this field.
-   *
-   * <p>The default value is 1.0.
-   *
-   * <p>Note: this value is not stored directly with the document in the index.
-   * Documents returned from {@link org.apache.lucene.index.IndexReader#document(int)} and
-   * {@link org.apache.lucene.search.Hits#doc(int)} may thus not have the same value present as when
-   * this field was indexed.
-   *
-   * @see #setBoost(float)
-   */
-  public float getBoost() {
-    return boost;
-  }
-
-  /** Returns the name of the field as an interned string.
-   * For example "date", "title", "body", ...
-   */
-  public String name()    { return name; }
-
-  protected void setStoreTermVector(Field.TermVector termVector) {
-    if (termVector == Field.TermVector.NO) {
-      this.storeTermVector = false;
-      this.storePositionWithTermVector = false;
-      this.storeOffsetWithTermVector = false;
-    }
-    else if (termVector == Field.TermVector.YES) {
-      this.storeTermVector = true;
-      this.storePositionWithTermVector = false;
-      this.storeOffsetWithTermVector = false;
-    }
-    else if (termVector == Field.TermVector.WITH_POSITIONS) {
-      this.storeTermVector = true;
-      this.storePositionWithTermVector = true;
-      this.storeOffsetWithTermVector = false;
-    }
-    else if (termVector == Field.TermVector.WITH_OFFSETS) {
-      this.storeTermVector = true;
-      this.storePositionWithTermVector = false;
-      this.storeOffsetWithTermVector = true;
-    }
-    else if (termVector == Field.TermVector.WITH_POSITIONS_OFFSETS) {
-      this.storeTermVector = true;
-      this.storePositionWithTermVector = true;
-      this.storeOffsetWithTermVector = true;
-    }
-    else {
-      throw new IllegalArgumentException("unknown termVector parameter " + termVector);
-    }
-  }
-
-  /** True iff the value of the field is to be stored in the index for return
-    with search hits.  It is an error for this to be true if a field is
-    Reader-valued. */
-  public final boolean  isStored()  { return isStored; }
-
-  /** True iff the value of the field is to be indexed, so that it may be
-    searched on. */
-  public final boolean  isIndexed()   { return isIndexed; }
-
-  /** True iff the value of the field should be tokenized as text prior to
-    indexing.  Un-tokenized fields are indexed as a single word and may not be
-    Reader-valued. */
-  public final boolean  isTokenized()   { return isTokenized; }
-
-  /** True if the value of the field is stored and compressed within the index */
-  public final boolean  isCompressed()   { return isCompressed; }
-
-  /** True iff the term or terms used to index this field are stored as a term
-   *  vector, available from {@link org.apache.lucene.index.IndexReader#getTermFreqVector(int,String)}.
-   *  These methods do not provide access to the original content of the field,
-   *  only to terms used to index it. If the original content must be
-   *  preserved, use the <code>stored</code> attribute instead.
-   *
-   * @see org.apache.lucene.index.IndexReader#getTermFreqVector(int, String)
-   */
-  public final boolean isTermVectorStored() { return storeTermVector; }
-
-  /**
-   * True iff terms are stored as term vector together with their offsets 
-   * (start and end positon in source text).
-   */
-  public boolean isStoreOffsetWithTermVector(){
-    return storeOffsetWithTermVector;
-  }
-
-  /**
-   * True iff terms are stored as term vector together with their token positions.
-   */
-  public boolean isStorePositionWithTermVector(){
-    return storePositionWithTermVector;
-  }
-
-  /** True iff the value of the filed is stored as binary */
-  public final boolean  isBinary()      { return isBinary; }
-
-  /** True if norms are omitted for this indexed field */
-  public boolean getOmitNorms() { return omitNorms; }
-
-  /** Expert:
-   *
-   * If set, omit normalization factors associated with this indexed field.
-   * This effectively disables indexing boosts and length normalization for this field.
-   */
-  public void setOmitNorms(boolean omitNorms) { this.omitNorms=omitNorms; }
-
-  public boolean isLazy() {
-    return lazy;
-  }
-
-  /** Prints a Field for human consumption. */
-  public final String toString() {
-    StringBuffer result = new StringBuffer();
-    if (isStored) {
-      result.append("stored");
-      if (isCompressed)
-        result.append("/compressed");
-      else
-        result.append("/uncompressed");
-    }
-    if (isIndexed) {
-      if (result.length() > 0)
-        result.append(",");
-      result.append("indexed");
-    }
-    if (isTokenized) {
-      if (result.length() > 0)
-        result.append(",");
-      result.append("tokenized");
-    }
-    if (storeTermVector) {
-      if (result.length() > 0)
-        result.append(",");
-      result.append("termVector");
-    }
-    if (storeOffsetWithTermVector) {
-      if (result.length() > 0)
-        result.append(",");
-      result.append("termVectorOffsets");
-    }
-    if (storePositionWithTermVector) {
-      if (result.length() > 0)
-        result.append(",");
-      result.append("termVectorPosition");
-    }
-    if (isBinary) {
-      if (result.length() > 0)
-        result.append(",");
-      result.append("binary");
-    }
-    if (omitNorms) {
-      result.append(",omitNorms");
-    }
-    if (lazy){
-      result.append(",lazy");
-    }
-    result.append('<');
-    result.append(name);
-    result.append(':');
-
-    if (fieldsData != null && lazy == false) {
-      result.append(fieldsData);
-    }
-
-    result.append('>');
-    return result.toString();
-  }
-}
+package org.apache.lucene.document;
+/**
+ * Copyright 2006 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Reader;
+
+import org.apache.lucene.index.FieldData;
+import org.apache.lucene.index.FieldInfo;
+
+/**
+ * Default abstract implementation of a {@link Fieldable}
+ * 
+ * $Id$
+ */
+public abstract class AbstractField implements Fieldable {
+
+  protected String name = "body";
+  protected boolean storeTermVector = false;
+  protected boolean storeOffsetWithTermVector = false;
+  protected boolean storePositionWithTermVector = false;
+  protected boolean omitNorms = false;
+  protected boolean isStored = false;
+  protected boolean isIndexed = true;
+  protected boolean isTokenized = true;
+  protected float boost = 1.0f;
+  // the one and only data object for all different kind of field values
+  protected FieldData fieldData = null;
+
+  protected AbstractField(FieldInfo fi, FieldData fieldData) {
+    this.name = fi.getName();
+    storeTermVector = fi.storeTermVector();
+    storeOffsetWithTermVector = fi.storeOffsetWithTermVector();
+    storePositionWithTermVector = fi.storePositionWithTermVector();
+    omitNorms = fi.omitNorms();
+    isStored = true;
+    isIndexed = fi.isIndexed();
+    isTokenized = fieldData.isTokenized();
+    this.fieldData = fieldData;
+  }
+
+  protected AbstractField(String name, FieldData data, Field.Store store, Field.Index index, Field.TermVector termVector) {
+    if (name == null)
+      throw new NullPointerException("name cannot be null");
+    if (data == null)
+      throw new NullPointerException("data cannot be null");
+
+    this.name = name.intern(); // field names are interned
+
+    fieldData = data;
+
+    if (index == Field.Index.NO && store == Field.Store.NO) {
+      throw new IllegalArgumentException("it doesn't make sense to have a field that " + "is neither indexed nor stored");
+    }
+    if (index == Field.Index.NO && termVector != Field.TermVector.NO) {
+      throw new IllegalArgumentException("cannot store term vector information " + "for a field that is not indexed");
+    }
+
+    setStore(store);
+    setIndex(index);
+    setStoreTermVector(termVector);
+  }
+
+  /** Sets the boost factor hits on this field.  This value will be
+   * multiplied into the score of all hits on this this field of this
+   * document.
+   *
+   * <p>The boost is multiplied by {@link org.apache.lucene.document.Document#getBoost()} of the document
+   * containing this field.  If a document has multiple fields with the same
+   * name, all such values are multiplied together.  This product is then
+   * multipled by the value {@link org.apache.lucene.search.Similarity#lengthNorm(String,int)}, and
+   * rounded by {@link org.apache.lucene.search.Similarity#encodeNorm(float)} before it is stored in the
+   * index.  One should attempt to ensure that this product does not overflow
+   * the range of that encoding.
+   *
+   * @see org.apache.lucene.document.Document#setBoost(float)
+   * @see org.apache.lucene.search.Similarity#lengthNorm(String, int)
+   * @see org.apache.lucene.search.Similarity#encodeNorm(float)
+   */
+  public void setBoost(float boost) {
+    this.boost = boost;
+  }
+
+  /** Returns the boost factor for hits for this field.
+   *
+   * <p>The default value is 1.0.
+   *
+   * <p>Note: this value is not stored directly with the document in the index.
+   * Documents returned from {@link org.apache.lucene.index.IndexReader#document(int)} and
+   * {@link org.apache.lucene.search.Hits#doc(int)} may thus not have the same value present as when
+   * this field was indexed.
+   *
+   * @see #setBoost(float)
+   */
+  public float getBoost() {
+    return boost;
+  }
+
+  /** Returns the name of the field as an interned string.
+   * For example "date", "title", "body", ...
+   */
+  public String name()    { return name; }
+
+  protected void setStoreTermVector(Field.TermVector termVector) {
+    if (termVector == Field.TermVector.NO) {
+      this.storeTermVector = false;
+      this.storePositionWithTermVector = false;
+      this.storeOffsetWithTermVector = false;
+    }
+    else if (termVector == Field.TermVector.YES) {
+      this.storeTermVector = true;
+      this.storePositionWithTermVector = false;
+      this.storeOffsetWithTermVector = false;
+    }
+    else if (termVector == Field.TermVector.WITH_POSITIONS) {
+      this.storeTermVector = true;
+      this.storePositionWithTermVector = true;
+      this.storeOffsetWithTermVector = false;
+    }
+    else if (termVector == Field.TermVector.WITH_OFFSETS) {
+      this.storeTermVector = true;
+      this.storePositionWithTermVector = false;
+      this.storeOffsetWithTermVector = true;
+    }
+    else if (termVector == Field.TermVector.WITH_POSITIONS_OFFSETS) {
+      this.storeTermVector = true;
+      this.storePositionWithTermVector = true;
+      this.storeOffsetWithTermVector = true;
+    }
+    else {
+      throw new IllegalArgumentException("unknown termVector parameter " + termVector);
+    }
+  }
+
+  protected void setIndex(Field.Index index) {
+    if (index == Field.Index.NO) {
+      isIndexed = false;
+      isTokenized = false;
+    } else if (index == Field.Index.TOKENIZED) {
+      isIndexed = true;
+      isTokenized = true;
+    } else if (index == Field.Index.UN_TOKENIZED) {
+      isIndexed = true;
+      isTokenized = false;
+    } else if (index == Field.Index.NO_NORMS) {
+      isIndexed = true;
+      isTokenized = false;
+      omitNorms = true;
+    } else {
+      throw new IllegalArgumentException("unknown index parameter " + index);
+    }
+    fieldData.setTokenized(isTokenized);
+  }
+
+  protected void setStore(Field.Store store) {
+    if (store == Field.Store.YES) {
+      isStored = true;
+    } else if (store == Field.Store.NO) {
+      if (isBinary()) {
+        throw new IllegalArgumentException("binary values can't be unstored");
+      }
+      isStored = false;
+    } else {
+      throw new IllegalArgumentException("unknown store parameter " + store);
+    }
+  }
+
+  /** True iff the value of the field is to be stored in the index for return
+    with search hits.  It is an error for this to be true if a field is
+    Reader-valued. */
+  public final boolean  isStored()  { return isStored; }
+
+  /** True iff the value of the field is to be indexed, so that it may be
+    searched on. */
+  public final boolean  isIndexed()   { return isIndexed; }
+
+  /** True iff the value of the field should be tokenized as text prior to
+    indexing.  Un-tokenized fields are indexed as a single word and may not be
+    Reader-valued. */
+  public final boolean  isTokenized()   { return isTokenized; }
+
+
+  /** True iff the term or terms used to index this field are stored as a term
+   *  vector, available from {@link org.apache.lucene.index.IndexReader#getTermFreqVector(int,String)}.
+   *  These methods do not provide access to the original content of the field,
+   *  only to terms used to index it. If the original content must be
+   *  preserved, use the <code>stored</code> attribute instead.
+   *
+   * @see org.apache.lucene.index.IndexReader#getTermFreqVector(int, String)
+   */
+  public final boolean isTermVectorStored() { return storeTermVector; }
+
+  /**
+   * True iff terms are stored as term vector together with their offsets 
+   * (start and end positon in source text).
+   */
+  public boolean isStoreOffsetWithTermVector(){
+    return storeOffsetWithTermVector;
+  }
+
+  /**
+   * True iff terms are stored as term vector together with their token positions.
+   */
+  public boolean isStorePositionWithTermVector(){
+    return storePositionWithTermVector;
+  }
+
+  /** True iff the value of the filed is stored as binary */
+  public final boolean isBinary() {
+    return fieldData.isBinary();
+  }
+
+  /** True if norms are omitted for this indexed field */
+  public boolean getOmitNorms() { return omitNorms; }
+
+  /** Expert:
+   *
+   * If set, omit normalization factors associated with this indexed field.
+   * This effectively disables indexing boosts and length normalization for this field.
+   */
+  public void setOmitNorms(boolean omitNorms) { this.omitNorms=omitNorms; }
+
+  public boolean isLazy() {
+    return fieldData.isLazy();
+  }
+
+  /** Prints a Field for human consumption. */
+  public String toString() {
+    StringBuffer result = new StringBuffer();
+    if (isStored) {
+      result.append("stored");
+    }
+    if (isIndexed) {
+      if (result.length() > 0)
+        result.append(",");
+      result.append("indexed");
+    }
+    if (isTokenized) {
+      if (result.length() > 0)
+        result.append(",");
+      result.append("tokenized");
+    }
+    if (storeTermVector) {
+      if (result.length() > 0)
+        result.append(",");
+      result.append("termVector");
+    }
+    if (storeOffsetWithTermVector) {
+      if (result.length() > 0)
+        result.append(",");
+      result.append("termVectorOffsets");
+    }
+    if (storePositionWithTermVector) {
+      if (result.length() > 0)
+        result.append(",");
+      result.append("termVectorPosition");
+    }
+    if (isBinary()) {
+      if (result.length() > 0)
+        result.append(",");
+      result.append("binary");
+    }
+    if (omitNorms) {
+      result.append(",omitNorms");
+    }
+    if (isLazy()) {
+      result.append(",lazy");
+    }
+    result.append('<');
+    result.append(name);
+    result.append(':');
+
+    if (fieldData != null && !isLazy()) {
+      result.append(fieldData);
+    }
+
+    result.append('>');
+    return result.toString();
+  }
+
+  /** The value of the field as a String, or null.  If null, the Reader value
+   * or binary value is used.  Exactly one of stringValue(), readerValue(), and
+   * binaryValue() must be set. */
+  public final String stringValue() {
+    return fieldData.stringValue();
+  }
+
+  /** The value of the field as a Reader, or null.  If null, the String value
+   * or binary value is  used.  Exactly one of stringValue(), readerValue(),
+   * and binaryValue() must be set. */
+  public final Reader readerValue() {
+    return fieldData.readerValue();
+  }
+
+  /** The value of the field in Binary, or null.  If null, the Reader or
+   * String value is used.  Exactly one of stringValue(), readerValue() and
+   * binaryValue() must be set. */
+  public final byte[] binaryValue() {
+    return fieldData.binaryValue();
+  }
+
+  /**
+   * 
+   * @return the data of the field
+   */
+  public FieldData getFieldData() {
+    return fieldData;
+  }
+
+}
Index: src/java/org/apache/lucene/document/Fieldable.java
===================================================================
--- src/java/org/apache/lucene/document/Fieldable.java	(révision 524580)
+++ src/java/org/apache/lucene/document/Fieldable.java	(copie de travail)
@@ -19,6 +19,8 @@
 import java.io.Reader;
 import java.io.Serializable;
 
+import org.apache.lucene.index.FieldData;
+
 /**
  * Synonymous with {@link Field}.
  *
@@ -134,4 +136,10 @@
    * @return true if this field can be loaded lazily
    */
   boolean isLazy();
+
+  /**
+   * 
+   * @return the data of the field
+   */
+  FieldData getFieldData();
 }
Index: src/java/org/apache/lucene/document/Document.java
===================================================================
--- src/java/org/apache/lucene/document/Document.java	(révision 524580)
+++ src/java/org/apache/lucene/document/Document.java	(copie de travail)
@@ -17,12 +17,12 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.Hits;
-import org.apache.lucene.search.Searcher;
+import java.util.ArrayList;
+import java.util.Enumeration;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Vector;
 
-import java.util.*;             // for javadoc
-
 /** Documents are the unit of indexing and search.
  *
  * A Document is a set of fields.  Each field has a name and a textual value.
@@ -37,8 +37,8 @@
  * IndexReader#document(int)}.
  */
 
-public final class Document implements java.io.Serializable {
-  List fields = new Vector();
+public class Document implements java.io.Serializable {
+  protected List fields = new Vector();
   private float boost = 1.0f;
 
   /** Constructs a new document with no fields. */
