Index: contrib/analyzers/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java	(revision 637499)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java	(working copy)
@@ -17,10 +17,23 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TermPositions;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Hits;
 import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.queryParser.QueryParser;
 
+import java.io.Reader;
 import java.io.StringReader;
 import java.util.ArrayList;
 
@@ -35,7 +48,7 @@
     private ArrayList tokens = new ArrayList();
     
     public void setUp() {
-        input = new WhitespaceTokenizer(new StringReader("abcde"));
+        input = new WhitespaceTokenizer(new StringReader("abcde abcde"));
     }
 
     public void testInvalidInput() throws Exception {
@@ -61,63 +74,78 @@
     public void testUnigrams() throws Exception {
       NGramTokenFilter filter = new NGramTokenFilter(input, 1, 1);
         
+        // ArrayList<Token> tokens=new ArrayList();
+        ArrayList tokens = new ArrayList();
         Token token = null;
         do { 
             token = filter.next();
             if (token != null) {
-                tokens.add(token.toString());
-//                System.out.println(token.termText());
-//                System.out.println(token);
-//                Thread.sleep(1000);
+                tokens.add(token);
             }
         } while (token != null);
 
-        assertEquals(5, tokens.size());
-        ArrayList exp = new ArrayList();
-        exp.add("(a,0,1)"); exp.add("(b,1,2)"); exp.add("(c,2,3)"); exp.add("(d,3,4)"); exp.add("(e,4,5)");
-        assertEquals(exp, tokens);
+        assertEquals(10, tokens.size());
+        String[] texts=new String[]{"a","b","c","d","e","a","b","c","d","e"};
+        int[] starts=new int[]{0,0,0,0,0,6,6,6,6,6};
+        int[] ends  =new int[]{5,5,5,5,5,11,11,11,11,11};
+        for(int i=0;i<tokens.size();i++){
+          token=(Token)tokens.get(i);
+          assertEquals(texts[i], token.termText());
+          assertEquals(starts[i], token.startOffset());
+          assertEquals(token.toString(),ends[i], token.endOffset());
+        }
     }
 
     public void testBigrams() throws Exception {
       NGramTokenFilter filter = new NGramTokenFilter(input, 2, 2);
         
+        // ArrayList<Token> tokens=new ArrayList();
+        ArrayList tokens = new ArrayList();
         Token token = null;
         do { 
             token = filter.next();
             if (token != null) {
-                tokens.add(token.toString());
-//                System.out.println(token.termText());
-//                System.out.println(token);
-//                Thread.sleep(1000);
+                tokens.add(token);
             }
         } while (token != null);
 
-        assertEquals(4, tokens.size());
-        ArrayList exp = new ArrayList();
-        exp.add("(ab,0,2)"); exp.add("(bc,1,3)"); exp.add("(cd,2,4)"); exp.add("(de,3,5)");
-        assertEquals(exp, tokens);
+        assertEquals(8, tokens.size());
+        String[] texts=new String[]{"ab","bc","cd","de","ab","bc","cd","de"};
+        int[] starts=new int[]{0,0,0,0,6,6,6,6};
+        int[] ends  =new int[]{5,5,5,5,11,11,11,11};
+        for(int i=0;i<tokens.size();i++){
+          token=(Token)tokens.get(i);
+          assertEquals(texts[i], token.termText());
+          assertEquals(starts[i], token.startOffset());
+          assertEquals(ends[i],   token.endOffset());
+        }
     }
 
     public void testNgrams() throws Exception {
-      NGramTokenFilter filter = new NGramTokenFilter(input, 1, 3);
+        RAMDirectory ramDir = new RAMDirectory();
         
-        Token token = null;
-        do { 
-            token = filter.next();
-            if (token != null) {
-                tokens.add(token.toString());
-//                System.out.println(token.termText());
-//                System.out.println(token);
-//                Thread.sleep(1000);
-            }
-        } while (token != null);
-
-        assertEquals(12, tokens.size());
-        ArrayList exp = new ArrayList();
-        exp.add("(a,0,1)"); exp.add("(b,1,2)"); exp.add("(c,2,3)"); exp.add("(d,3,4)"); exp.add("(e,4,5)");
-        exp.add("(ab,0,2)"); exp.add("(bc,1,3)"); exp.add("(cd,2,4)"); exp.add("(de,3,5)");
-        exp.add("(abc,0,3)"); exp.add("(bcd,1,4)"); exp.add("(cde,2,5)");
-        assertEquals(exp, tokens);
+        IndexWriter writer = new IndexWriter(ramDir, new Analyzer(){
+            public TokenStream tokenStream(String fieldName, Reader reader){
+              return new NGramTokenFilter(new WhitespaceTokenizer(reader),1,3); }
+            }, true, IndexWriter.MaxFieldLength.LIMITED);
+        
+        Document d=new Document();
+        d.add(new Field("content", "abcde abcde", Field.Store.YES, Field.Index.TOKENIZED));
+        writer.addDocument(d);
+        writer.close();
+        
+        IndexReader reader = IndexReader.open(ramDir);
+        
+        
+        TermPositions termPositions = reader.termPositions();
+        
+        String[] texts=new String[]{"a","b","c","d","e","ab","bc","cd","de","ab","bc","cd","de","abc","bcd","cde"};
+        for(int i=0;i<texts.length;i++){
+          termPositions.seek(new Term("content", texts[i]));
+          assertTrue(termPositions.next());
+          assertEquals(2, termPositions.freq());
+        }
+        reader.close();
     }
 
     public void testOversizedNgrams() throws Exception {
@@ -136,4 +164,32 @@
 
         assertTrue(tokens.isEmpty());
     }
+
+  public void testIndexAndQuery() throws Exception {
+    RAMDirectory ramDir = new RAMDirectory();
+    
+    IndexWriter writer = new IndexWriter(ramDir, new Analyzer(){
+            public TokenStream tokenStream(String fieldName, Reader reader){
+              return new NGramTokenFilter(new WhitespaceTokenizer(reader),2,3); }
+            }, true, IndexWriter.MaxFieldLength.LIMITED);
+    
+    Document d=new Document();
+    d.add(new Field("content", "z abcdef", Field.Store.YES, Field.Index.TOKENIZED));
+    writer.addDocument(d);
+    writer.close();
+    
+    IndexSearcher searcher = new IndexSearcher(ramDir);
+    QueryParser parser = new QueryParser("content", new Analyzer(){
+            public TokenStream tokenStream(String fieldName, Reader reader){
+              return new NGramTokenFilter(new WhitespaceTokenizer(reader),2,3); }
+    	    });
+    
+    String[] queryConds=new String[]{"ab","abc","abcd"};
+    for(int i=0; i<queryConds.length; i++){
+	    Query query=parser.parse(queryConds[i]);
+	    Hits hits=searcher.search(query);
+	    assertEquals(1,hits.length());
+    }
+    searcher.close();
+  }
 }
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java	(revision 637582)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java	(working copy)
@@ -34,7 +34,7 @@
 
   private int minGram, maxGram;
   private LinkedList ngrams;
-
+  
   /**
    * Creates NGramTokenFilter with given min and max n-grams.
    * @param input TokenStream holding the input to be tokenized
@@ -69,8 +69,10 @@
     }
 
     Token token = input.next();
-    if (token == null) {
-      return null;
+    if (token == null) return null;
+    while(token.termText().length()<minGram){
+      token = input.next();
+      if (token == null) return null;
     }
 
     ngram(token);
@@ -83,17 +85,28 @@
   private void ngram(Token token) { 
     String inStr = token.termText();
     int inLen = inStr.length();
-    int gramSize = minGram;
-    while (gramSize <= maxGram) {
-      int pos = 0;                        // reset to beginning of string
-      while (pos+gramSize <= inLen) {     // while there is input
+    
+    int start=token.startOffset();
+    int end  =token.endOffset();
+    
+    int pos=0;
+    int increment=1;
+    while(pos<inLen){
+      int gramSize=minGram;
+      while(gramSize<=maxGram && pos+gramSize<=inLen){
         String gram = inStr.substring(pos, pos+gramSize);
-        Token tok = new Token(gram, pos, pos+gramSize);
-//        tok.setPositionIncrement(pos);
+        Token tok = new Token(gram, start, end);
+        if(pos==0 && token.getPositionIncrement()!=1){
+          tok.setPositionIncrement(token.getPositionIncrement());
+        }else{
+          tok.setPositionIncrement(increment);
+        }
         ngrams.add(tok);
-        pos++;
+        gramSize++;
+        increment=0;
       }
-      gramSize++;                         // increase n-gram size
+      increment=1;
+      pos++;
     }
   }
 }
