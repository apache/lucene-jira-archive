diff --git a/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java b/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java
index 422292b..cfef241 100644
--- a/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java
+++ b/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java
@@ -264,7 +264,7 @@ public final class FieldInfo {
     assert checkConsistency();
   }
   
-  void setStorePayloads() {
+  public void setStorePayloads() {
     if (indexOptions != IndexOptions.NONE && indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
       storePayloads = true;
     }
diff --git a/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java b/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
index fb94bdd..6e73391 100644
--- a/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
+++ b/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
@@ -182,7 +182,6 @@ public class MemoryIndex {
   private final SortedMap<String,Info> fields = new TreeMap<>();
   
   private final boolean storeOffsets;
-  private final boolean storePayloads;
 
   private final ByteBlockPool byteBlockPool;
   private final IntBlockPool intBlockPool;
@@ -208,37 +207,24 @@ public class MemoryIndex {
   /**
    * Constructs an empty instance that can optionally store the start and end
    * character offset of each token term in the text. This can be useful for
-   * highlighting of hit locations with the Lucene highlighter package.  But
-   * it will not store payloads; use another constructor for that.
+   * highlighting of hit locations with the Lucene highlighter package.
    * 
    * @param storeOffsets
    *            whether or not to store the start and end character offset of
    *            each token term in the text
    */
   public MemoryIndex(boolean storeOffsets) {
-    this(storeOffsets, false);
-  }
-
-  /**
-   * Constructs an empty instance with the option of storing offsets and payloads.
-   *
-   * @param storeOffsets store term offsets at each position
-   * @param storePayloads store term payloads at each position
-   */
-  public MemoryIndex(boolean storeOffsets, boolean storePayloads) {
-    this(storeOffsets, storePayloads, 0);
+    this(storeOffsets, 0);
   }
 
   /**
    * Expert: This constructor accepts an upper limit for the number of bytes that should be reused if this instance is {@link #reset()}.
    * The payload storage, if used, is unaffected by maxReusuedBytes, however.
    * @param storeOffsets <code>true</code> if offsets should be stored
-   * @param storePayloads <code>true</code> if payloads should be stored
    * @param maxReusedBytes the number of bytes that should remain in the internal memory pools after {@link #reset()} is called
    */
-  MemoryIndex(boolean storeOffsets, boolean storePayloads, long maxReusedBytes) {
+  MemoryIndex(boolean storeOffsets, long maxReusedBytes) {
     this.storeOffsets = storeOffsets;
-    this.storePayloads = storePayloads;
     this.defaultFieldType.setIndexOptions(storeOffsets ?
         IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
     this.defaultFieldType.setStoreTermVectors(true);
@@ -250,7 +236,7 @@ public class MemoryIndex {
     intBlockPool = new IntBlockPool(new RecyclingIntBlockAllocator(IntBlockPool.INT_BLOCK_SIZE, maxBufferedIntBlocks, bytesUsed));
     postingsWriter = new SliceWriter(intBlockPool);
     //TODO refactor BytesRefArray to allow us to apply maxReusedBytes option
-    payloadsBytesRefs = storePayloads ? new BytesRefArray(bytesUsed) : null;
+    payloadsBytesRefs = new BytesRefArray(bytesUsed);
   }
   
   /**
@@ -275,8 +261,7 @@ public class MemoryIndex {
       throw new IllegalArgumentException("analyzer must not be null");
     
     TokenStream stream = analyzer.tokenStream(fieldName, text);
-    storeTerms(getInfo(fieldName, defaultFieldType), stream, 1.0f,
-        analyzer.getPositionIncrementGap(fieldName), analyzer.getOffsetGap(fieldName));
+    addField(fieldName, stream, 1.0f, analyzer.getPositionIncrementGap(fieldName), analyzer.getOffsetGap(fieldName));
   }
 
   /**
@@ -287,7 +272,7 @@ public class MemoryIndex {
    * @return a MemoryIndex
    */
   public static MemoryIndex fromDocument(Iterable<? extends IndexableField> document, Analyzer analyzer) {
-    return fromDocument(document, analyzer, false, false, 0);
+    return fromDocument(document, analyzer, false, 0);
   }
 
   /**
@@ -295,11 +280,10 @@ public class MemoryIndex {
    * @param document the document to index
    * @param analyzer the analyzer to use
    * @param storeOffsets <code>true</code> if offsets should be stored
-   * @param storePayloads <code>true</code> if payloads should be stored
    * @return a MemoryIndex
    */
-  public static MemoryIndex fromDocument(Iterable<? extends IndexableField> document, Analyzer analyzer, boolean storeOffsets, boolean storePayloads) {
-    return fromDocument(document, analyzer, storeOffsets, storePayloads, 0);
+  public static MemoryIndex fromDocument(Iterable<? extends IndexableField> document, Analyzer analyzer, boolean storeOffsets) {
+    return fromDocument(document, analyzer, storeOffsets, 0);
   }
 
   /**
@@ -307,12 +291,11 @@ public class MemoryIndex {
    * @param document the document to index
    * @param analyzer the analyzer to use
    * @param storeOffsets <code>true</code> if offsets should be stored
-   * @param storePayloads <code>true</code> if payloads should be stored
    * @param maxReusedBytes the number of bytes that should remain in the internal memory pools after {@link #reset()} is called
    * @return a MemoryIndex
    */
-  public static MemoryIndex fromDocument(Iterable<? extends IndexableField> document, Analyzer analyzer, boolean storeOffsets, boolean storePayloads, long maxReusedBytes) {
-    MemoryIndex mi = new MemoryIndex(storeOffsets, storePayloads, maxReusedBytes);
+  public static MemoryIndex fromDocument(Iterable<? extends IndexableField> document, Analyzer analyzer, boolean storeOffsets, long maxReusedBytes) {
+    MemoryIndex mi = new MemoryIndex(storeOffsets, maxReusedBytes);
     for (IndexableField field : document) {
       mi.addField(field, analyzer);
     }
@@ -408,6 +391,7 @@ public class MemoryIndex {
       tokenStream = field.tokenStream(null, null);
       positionIncrementGap = 0;
     }
+
     if (tokenStream != null) {
       storeTerms(info, tokenStream, boost, positionIncrementGap, offsetGap);
     }
@@ -534,7 +518,7 @@ public class MemoryIndex {
 
   private FieldInfo createFieldInfo(String fieldName, int ord, IndexableFieldType fieldType) {
     IndexOptions indexOptions = storeOffsets ? IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-    return new FieldInfo(fieldName, ord, fieldType.storeTermVectors(), fieldType.omitNorms(), storePayloads,
+    return new FieldInfo(fieldName, ord, fieldType.storeTermVectors(), fieldType.omitNorms(), false,
         indexOptions, fieldType.docValuesType(), -1, Collections.emptyMap(),
         fieldType.pointDimensionCount(), fieldType.pointNumBytes());
   }
@@ -620,7 +604,12 @@ public class MemoryIndex {
       TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);
       PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);
       OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);
-      PayloadAttribute payloadAtt = storePayloads ? stream.addAttribute(PayloadAttribute.class) : null;
+      PayloadAttribute payloadAtt = stream.getAttribute(PayloadAttribute.class);
+      if (payloadAtt != null && info.fieldInfo.hasPayloads() == false) {
+        if (info.numTokens != 0)
+          throw new IllegalArgumentException("Cannot add payloads to field " + info.fieldInfo.name + " as it has already been added with no payloads");
+        info.fieldInfo.setStorePayloads();
+      }
       stream.reset();
 
       while (stream.incrementToken()) {
@@ -645,7 +634,7 @@ public class MemoryIndex {
           postingsWriter.writeInt(offsetAtt.startOffset() + offset);
           postingsWriter.writeInt(offsetAtt.endOffset() + offset);
         }
-        if (storePayloads) {
+        if (payloadAtt != null) {
           final BytesRef payload = payloadAtt.getPayload();
           final int pIndex;
           if (payload == null || payload.length == 0) {
@@ -763,7 +752,7 @@ public class MemoryIndex {
     int sumPositions = 0;
     int sumTerms = 0;
     final BytesRef spare = new BytesRef();
-    final BytesRefBuilder payloadBuilder = storePayloads ? new BytesRefBuilder() : null;
+    final BytesRefBuilder payloadBuilder = new BytesRefBuilder();
     for (Map.Entry<String, Info> entry : fields.entrySet()) {
       String fieldName = entry.getKey();
       Info info = entry.getValue();
@@ -789,7 +778,7 @@ public class MemoryIndex {
               result.append(", ");
             }
           }
-          if (storePayloads) {
+          if (info.fieldInfo.hasPayloads()) {
             int payloadIndex = postingsReader.readInt();
             if (payloadIndex != -1) {
                 result.append(", " + payloadsBytesRefs.get(payloadBuilder, payloadIndex));
@@ -1356,7 +1345,7 @@ public class MemoryIndex {
 
           @Override
           public boolean hasPayloads() {
-            return storePayloads;
+            return info.fieldInfo.hasPayloads();
           }
         };
       }
@@ -1467,7 +1456,7 @@ public class MemoryIndex {
           reuse = new MemoryPostingsEnum();
         }
         final int ord = info.sortedTerms[termUpto];
-        return ((MemoryPostingsEnum) reuse).reset(info.sliceArray.start[ord], info.sliceArray.end[ord], info.sliceArray.freq[ord]);
+        return ((MemoryPostingsEnum) reuse).reset(info, info.sliceArray.start[ord], info.sliceArray.end[ord], info.sliceArray.freq[ord]);
       }
 
       @Override
@@ -1495,15 +1484,17 @@ public class MemoryIndex {
       private int startOffset;
       private int endOffset;
       private int payloadIndex;
+      private boolean hasPayloads;
       private final BytesRefBuilder payloadBuilder;//only non-null when storePayloads
 
       public MemoryPostingsEnum() {
         this.sliceReader = new SliceReader(intBlockPool);
-        this.payloadBuilder = storePayloads ? new BytesRefBuilder() : null;
+        this.payloadBuilder = new BytesRefBuilder();
       }
 
-      public PostingsEnum reset(int start, int end, int freq) {
+      public PostingsEnum reset(Info info, int start, int end, int freq) {
         this.sliceReader.reset(start, end);
+        hasPayloads = info.fieldInfo.hasPayloads();
         posUpto = 0; // for assert
         hasNext = true;
         doc = -1;
@@ -1549,7 +1540,7 @@ public class MemoryIndex {
           startOffset = sliceReader.readInt();
           endOffset = sliceReader.readInt();
         }
-        if (storePayloads) {
+        if (hasPayloads) {
           payloadIndex = sliceReader.readInt();
         }
         return pos;
@@ -1567,7 +1558,7 @@ public class MemoryIndex {
 
       @Override
       public BytesRef getPayload() {
-        if (payloadBuilder == null || payloadIndex == -1) {
+        if (hasPayloads == false || payloadIndex == -1) {
           return null;
         }
         return payloadsBytesRefs.get(payloadBuilder, payloadIndex);
diff --git a/lucene/memory/src/test/org/apache/lucene/index/memory/TestMemoryIndex.java b/lucene/memory/src/test/org/apache/lucene/index/memory/TestMemoryIndex.java
index 9630c1f..1db8307 100644
--- a/lucene/memory/src/test/org/apache/lucene/index/memory/TestMemoryIndex.java
+++ b/lucene/memory/src/test/org/apache/lucene/index/memory/TestMemoryIndex.java
@@ -27,6 +27,7 @@ import java.util.stream.LongStream;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockPayloadAnalyzer;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.BinaryPoint;
 import org.apache.lucene.document.Document;
@@ -55,6 +56,7 @@ import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.SortedNumericDocValues;
 import org.apache.lucene.index.SortedSetDocValues;
 import org.apache.lucene.index.Term;
+import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MatchAllDocsQuery;
@@ -155,22 +157,14 @@ public class TestMemoryIndex extends LuceneTestCase {
     TestUtil.checkReader(mi.createSearcher().getIndexReader());
     
     // all combinations of offsets/payloads options
-    mi = new MemoryIndex(true, true);
+    mi = new MemoryIndex(true);
     mi.addField("field", "some terms be here", analyzer);
     TestUtil.checkReader(mi.createSearcher().getIndexReader());
-    
-    mi = new MemoryIndex(true, false);
-    mi.addField("field", "some terms be here", analyzer);
-    TestUtil.checkReader(mi.createSearcher().getIndexReader());
-    
-    mi = new MemoryIndex(false, true);
-    mi.addField("field", "some terms be here", analyzer);
-    TestUtil.checkReader(mi.createSearcher().getIndexReader());
-    
-    mi = new MemoryIndex(false, false);
+
+    mi = new MemoryIndex(false);
     mi.addField("field", "some terms be here", analyzer);
     TestUtil.checkReader(mi.createSearcher().getIndexReader());
-    
+
     analyzer.close();
   }
 
@@ -340,7 +334,7 @@ public class TestMemoryIndex extends LuceneTestCase {
     Document doc = new Document();
     doc.add(new BinaryDocValuesField("text", new BytesRef("quick brown fox")));
     doc.add(new TextField("text", "quick brown fox", Field.Store.NO));
-    MemoryIndex mi = MemoryIndex.fromDocument(doc, analyzer, true, true);
+    MemoryIndex mi = MemoryIndex.fromDocument(doc, analyzer, true);
     LeafReader leafReader = mi.createSearcher().getIndexReader().leaves().get(0).reader();
     TermsEnum tenum = leafReader.terms("text").iterator();
 
@@ -434,7 +428,7 @@ public class TestMemoryIndex extends LuceneTestCase {
   }
 
   public void testPointValuesDoNotAffectBoostPositionsOrOffset() throws Exception {
-    MemoryIndex mi = new MemoryIndex(true, true);
+    MemoryIndex mi = new MemoryIndex(true);
     mi.addField(new TextField("text", "quick brown fox", Field.Store.NO), analyzer, 5f);
     mi.addField(new BinaryPoint("text", "quick".getBytes(StandardCharsets.UTF_8)), analyzer, 5f);
     mi.addField(new BinaryPoint("text", "brown".getBytes(StandardCharsets.UTF_8)), analyzer, 5f);
@@ -516,7 +510,7 @@ public class TestMemoryIndex extends LuceneTestCase {
   }
 
   public void testToStringDebug() {
-    MemoryIndex mi = new MemoryIndex(true, true);
+    MemoryIndex mi = new MemoryIndex(true);
     Analyzer analyzer = new MockPayloadAnalyzer();
 
     mi.addField("analyzedField", "aa bb aa", analyzer);
@@ -537,4 +531,20 @@ public class TestMemoryIndex extends LuceneTestCase {
         "fields=2, terms=2, positions=3", mi.toStringDebug());
   }
 
+  public void testPayloadFields() throws IOException {
+    MemoryIndex mi = new MemoryIndex();
+    mi.addField("payloads", "a b c d", new MockPayloadAnalyzer());
+    mi.addField("nopayloads", "a b c d", new StandardAnalyzer());
+    mi.freeze();
+
+    IndexReader r = mi.createSearcher().getIndexReader();
+    TestUtil.checkReader(r);
+    LeafReader reader = r.leaves().get(0).reader();
+
+    Terms payloadTerms = reader.terms("payloads");
+    assertTrue(payloadTerms.hasPayloads());
+    Terms nopayloadTerms = reader.terms("nopayloads");
+    assertFalse(nopayloadTerms.hasPayloads());
+  }
+
 }
diff --git a/lucene/memory/src/test/org/apache/lucene/index/memory/TestMemoryIndexAgainstRAMDir.java b/lucene/memory/src/test/org/apache/lucene/index/memory/TestMemoryIndexAgainstRAMDir.java
index 03c17a5..ccdcb72 100644
--- a/lucene/memory/src/test/org/apache/lucene/index/memory/TestMemoryIndexAgainstRAMDir.java
+++ b/lucene/memory/src/test/org/apache/lucene/index/memory/TestMemoryIndexAgainstRAMDir.java
@@ -317,7 +317,7 @@ public class TestMemoryIndexAgainstRAMDir extends BaseTokenStreamTestCase {
   
   public void testDocsEnumStart() throws Exception {
     Analyzer analyzer = new MockAnalyzer(random());
-    MemoryIndex memory = new MemoryIndex(random().nextBoolean(), false, random().nextInt(50) * 1024 * 1024);
+    MemoryIndex memory = new MemoryIndex(random().nextBoolean(), random().nextInt(50) * 1024 * 1024);
     memory.addField("foo", "bar", analyzer);
     LeafReader reader = (LeafReader) memory.createSearcher().getIndexReader();
     TestUtil.checkReader(reader);
@@ -345,13 +345,13 @@ public class TestMemoryIndexAgainstRAMDir extends BaseTokenStreamTestCase {
   }
 
   private MemoryIndex randomMemoryIndex() {
-    return new MemoryIndex(random().nextBoolean(), random().nextBoolean(), random().nextInt(50) * 1024 * 1024);
+    return new MemoryIndex(random().nextBoolean(), random().nextInt(50) * 1024 * 1024);
   }
 
   public void testDocsAndPositionsEnumStart() throws Exception {
     Analyzer analyzer = new MockAnalyzer(random());
     int numIters = atLeast(3);
-    MemoryIndex memory = new MemoryIndex(true, false, random().nextInt(50) * 1024 * 1024);
+    MemoryIndex memory = new MemoryIndex(true, random().nextInt(50) * 1024 * 1024);
     for (int i = 0; i < numIters; i++) { // check reuse
       memory.addField("foo", "bar", analyzer);
       LeafReader reader = (LeafReader) memory.createSearcher().getIndexReader();
@@ -533,7 +533,7 @@ public class TestMemoryIndexAgainstRAMDir extends BaseTokenStreamTestCase {
   }
 
   public void testNormsWithDocValues() throws Exception {
-    MemoryIndex mi = new MemoryIndex(true, true);
+    MemoryIndex mi = new MemoryIndex(true);
     MockAnalyzer mockAnalyzer = new MockAnalyzer(random());
 
     mi.addField(new BinaryDocValuesField("text", new BytesRef("quick brown fox")), mockAnalyzer, 5f);
