Index: lucene/src/test/org/apache/lucene/TestMergeSchedulerExternal.java
===================================================================
--- lucene/src/test/org/apache/lucene/TestMergeSchedulerExternal.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/TestMergeSchedulerExternal.java	(working copy)
@@ -86,7 +86,7 @@
     dir.failOn(new FailOnlyOnMerge());
 
     Document doc = new Document();
-    Field idField = new Field("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
+    Field idField = newField("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
     doc.add(idField);
     
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(
Index: lucene/src/test/org/apache/lucene/queryParser/TestMultiFieldQueryParser.java
===================================================================
--- lucene/src/test/org/apache/lucene/queryParser/TestMultiFieldQueryParser.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/queryParser/TestMultiFieldQueryParser.java	(working copy)
@@ -284,7 +284,7 @@
     Directory ramDir = newDirectory();
     IndexWriter iw =  new IndexWriter(ramDir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
     Document doc = new Document();
-    doc.add(new Field("body", "blah the footest blah", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("body", "blah the footest blah", Field.Store.NO, Field.Index.ANALYZED));
     iw.addDocument(doc);
     iw.close();
     
Index: lucene/src/test/org/apache/lucene/queryParser/TestQueryParser.java
===================================================================
--- lucene/src/test/org/apache/lucene/queryParser/TestQueryParser.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/queryParser/TestQueryParser.java	(working copy)
@@ -571,7 +571,7 @@
     Directory ramDir = newDirectory();
     IndexWriter iw = new IndexWriter(ramDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));
     Document doc = new Document();
-    doc.add(new Field("content","\u0633\u0627\u0628", 
+    doc.add(newField("content","\u0633\u0627\u0628", 
                       Field.Store.YES, Field.Index.NOT_ANALYZED));
     iw.addDocument(doc);
     iw.close();
@@ -1131,13 +1131,13 @@
     assertEquals(expected, hits.length);
   }
 
-  private static void addDateDoc(String content, int year, int month,
+  private void addDateDoc(String content, int year, int month,
       int day, int hour, int minute, int second, IndexWriter iw) throws IOException {
     Document d = new Document();
-    d.add(new Field("f", content, Field.Store.YES, Field.Index.ANALYZED));
+    d.add(newField("f", content, Field.Store.YES, Field.Index.ANALYZED));
     Calendar cal = Calendar.getInstance(Locale.ENGLISH);
     cal.set(year, month-1, day, hour, minute, second);
-    d.add(new Field("date", DateField.dateToString(cal.getTime()), Field.Store.YES, Field.Index.NOT_ANALYZED));
+    d.add(newField("date", DateField.dateToString(cal.getTime()), Field.Store.YES, Field.Index.NOT_ANALYZED));
     iw.addDocument(d);
   }
 
@@ -1155,7 +1155,7 @@
     Analyzer a = new MockAnalyzer(MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, a));
     Document doc = new Document();
-    doc.add(new Field("f", "the wizard of ozzy", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("f", "the wizard of ozzy", Field.Store.NO, Field.Index.ANALYZED));
     w.addDocument(doc);
     IndexReader r = w.getReader();
     w.close();
Index: lucene/src/test/org/apache/lucene/TestExternalCodecs.java
===================================================================
--- lucene/src/test/org/apache/lucene/TestExternalCodecs.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/TestExternalCodecs.java	(working copy)
@@ -607,11 +607,11 @@
     w.setMergeFactor(3);
     Document doc = new Document();
     // uses default codec:
-    doc.add(new Field("field1", "this field uses the standard codec as the test", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("field1", "this field uses the standard codec as the test", Field.Store.NO, Field.Index.ANALYZED));
     // uses pulsing codec:
-    doc.add(new Field("field2", "this field uses the pulsing codec as the test", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("field2", "this field uses the pulsing codec as the test", Field.Store.NO, Field.Index.ANALYZED));
     
-    Field idField = new Field("id", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
+    Field idField = newField("id", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
     doc.add(idField);
     for(int i=0;i<NUM_DOCS;i++) {
       idField.setValue(""+i);
Index: lucene/src/test/org/apache/lucene/TestDemo.java
===================================================================
--- lucene/src/test/org/apache/lucene/TestDemo.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/TestDemo.java	(working copy)
@@ -57,7 +57,7 @@
     Document doc = new Document();
     String longTerm = "longtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongterm";
     String text = "This is the text to be indexed. " + longTerm;
-    doc.add(new Field("fieldname", text, Field.Store.YES,
+    doc.add(newField("fieldname", text, Field.Store.YES,
         Field.Index.ANALYZED));
     iwriter.addDocument(doc);
     iwriter.close();
Index: lucene/src/test/org/apache/lucene/search/TestFuzzyQuery2.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestFuzzyQuery2.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestFuzzyQuery2.java	(working copy)
@@ -82,7 +82,7 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));
     
     Document doc = new Document();
-    Field field = new Field("field", "", Field.Store.NO, Field.Index.ANALYZED);
+    Field field = newField("field", "", Field.Store.NO, Field.Index.ANALYZED);
     doc.add(field);
     
     for (int i = 0; i < terms; i++) {
@@ -136,7 +136,7 @@
         IndexWriter.MaxFieldLength.UNLIMITED);
     
     Document doc = new Document();
-    Field field = new Field("field", "", Field.Store.NO, Field.Index.ANALYZED);
+    Field field = newField("field", "", Field.Store.NO, Field.Index.ANALYZED);
     doc.add(field);
 
     for (int i = 0; i < terms; i++) {
Index: lucene/src/test/org/apache/lucene/search/TestNot.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestNot.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestNot.java	(working copy)
@@ -42,7 +42,7 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, store);
 
     Document d1 = new Document();
-    d1.add(new Field("field", "a b", Field.Store.YES, Field.Index.ANALYZED));
+    d1.add(newField("field", "a b", Field.Store.YES, Field.Index.ANALYZED));
 
     writer.addDocument(d1);
     IndexReader reader = writer.getReader();
Index: lucene/src/test/org/apache/lucene/search/TestTimeLimitingCollector.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestTimeLimitingCollector.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestTimeLimitingCollector.java	(working copy)
@@ -108,7 +108,7 @@
 
   private void add(String value, RandomIndexWriter iw) throws IOException {
     Document d = new Document();
-    d.add(new Field(FIELD_NAME, value, Field.Store.NO, Field.Index.ANALYZED));
+    d.add(newField(FIELD_NAME, value, Field.Store.NO, Field.Index.ANALYZED));
     iw.addDocument(d);
   }
 
Index: lucene/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java	(working copy)
@@ -164,7 +164,7 @@
 
     // add a doc, refresh the reader, and check that its there
     Document doc = new Document();
-    doc.add(new Field("id", "1", Field.Store.YES, Field.Index.NOT_ANALYZED));
+    doc.add(newField("id", "1", Field.Store.YES, Field.Index.NOT_ANALYZED));
     writer.addDocument(doc);
 
     reader = refreshReader(reader);
Index: lucene/src/test/org/apache/lucene/search/TestPrefixFilter.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestPrefixFilter.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestPrefixFilter.java	(working copy)
@@ -40,7 +40,7 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, directory);
     for (int i = 0; i < categories.length; i++) {
       Document doc = new Document();
-      doc.add(new Field("category", categories[i], Field.Store.YES, Field.Index.NOT_ANALYZED));
+      doc.add(newField("category", categories[i], Field.Store.YES, Field.Index.NOT_ANALYZED));
       writer.addDocument(doc);
     }
     IndexReader reader = writer.getReader();
Index: lucene/src/test/org/apache/lucene/search/TestAutomatonQueryUnicode.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestAutomatonQueryUnicode.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestAutomatonQueryUnicode.java	(working copy)
@@ -46,11 +46,11 @@
     directory = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random, directory);
     Document doc = new Document();
-    Field titleField = new Field("title", "some title", Field.Store.NO,
+    Field titleField = newField("title", "some title", Field.Store.NO,
         Field.Index.ANALYZED);
-    Field field = new Field(FN, "", Field.Store.NO,
+    Field field = newField(FN, "", Field.Store.NO,
         Field.Index.ANALYZED);
-    Field footerField = new Field("footer", "a footer", Field.Store.NO,
+    Field footerField = newField("footer", "a footer", Field.Store.NO,
         Field.Index.ANALYZED);
     doc.add(titleField);
     doc.add(field);
Index: lucene/src/test/org/apache/lucene/search/TestBooleanQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestBooleanQuery.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestBooleanQuery.java	(working copy)
@@ -61,7 +61,7 @@
     Directory dir = newDirectory();
     RandomIndexWriter w = new RandomIndexWriter(random, dir);
     Document doc = new Document();
-    doc.add(new Field("field", "a b c d", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("field", "a b c d", Field.Store.NO, Field.Index.ANALYZED));
     w.addDocument(doc);
 
     IndexReader r = w.getReader();
Index: lucene/src/test/org/apache/lucene/search/TestPhraseQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestPhraseQuery.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestPhraseQuery.java	(working copy)
@@ -68,19 +68,19 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, directory, analyzer);
     
     Document doc = new Document();
-    doc.add(new Field("field", "one two three four five", Field.Store.YES, Field.Index.ANALYZED));
-    doc.add(new Field("repeated", "this is a repeated field - first part", Field.Store.YES, Field.Index.ANALYZED));
-    Fieldable repeatedField = new Field("repeated", "second part of a repeated field", Field.Store.YES, Field.Index.ANALYZED);
+    doc.add(newField("field", "one two three four five", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("repeated", "this is a repeated field - first part", Field.Store.YES, Field.Index.ANALYZED));
+    Fieldable repeatedField = newField("repeated", "second part of a repeated field", Field.Store.YES, Field.Index.ANALYZED);
     doc.add(repeatedField);
-    doc.add(new Field("palindrome", "one two three two one", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("palindrome", "one two three two one", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
     
     doc = new Document();
-    doc.add(new Field("nonexist", "phrase exist notexist exist found", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("nonexist", "phrase exist notexist exist found", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
     
     doc = new Document();
-    doc.add(new Field("nonexist", "phrase exist notexist exist found", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("nonexist", "phrase exist notexist exist found", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
 
     reader = writer.getReader();
@@ -216,7 +216,7 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, directory, 
         newIndexWriterConfig( Version.LUCENE_24, stopAnalyzer));
     Document doc = new Document();
-    doc.add(new Field("field", "the stop words are here", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("field", "the stop words are here", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
     IndexReader reader = writer.getReader();
     writer.close();
@@ -251,12 +251,12 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, directory);
     
     Document doc = new Document();
-    doc.add(new Field("source", "marketing info", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("source", "marketing info", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
     
     doc = new Document();
-    doc.add(new Field("contents", "foobar", Field.Store.YES, Field.Index.ANALYZED));
-    doc.add(new Field("source", "marketing info", Field.Store.YES, Field.Index.ANALYZED)); 
+    doc.add(newField("contents", "foobar", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("source", "marketing info", Field.Store.YES, Field.Index.ANALYZED)); 
     writer.addDocument(doc);
     
     IndexReader reader = writer.getReader();
@@ -287,15 +287,15 @@
     writer = new RandomIndexWriter(random, directory, 
         newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.CREATE));
     doc = new Document();
-    doc.add(new Field("contents", "map entry woo", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("contents", "map entry woo", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
 
     doc = new Document();
-    doc.add(new Field("contents", "woo map entry", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("contents", "woo map entry", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
 
     doc = new Document();
-    doc.add(new Field("contents", "map foobarword entry woo", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("contents", "map foobarword entry woo", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
 
     reader = writer.getReader();
@@ -338,15 +338,15 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, directory);
 
     Document doc = new Document();
-    doc.add(new Field("field", "foo firstname lastname foo", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("field", "foo firstname lastname foo", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
     
     Document doc2 = new Document();
-    doc2.add(new Field("field", "foo firstname zzz lastname foo", Field.Store.YES, Field.Index.ANALYZED));
+    doc2.add(newField("field", "foo firstname zzz lastname foo", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc2);
     
     Document doc3 = new Document();
-    doc3.add(new Field("field", "foo firstname zzz yyy lastname foo", Field.Store.YES, Field.Index.ANALYZED));
+    doc3.add(newField("field", "foo firstname zzz yyy lastname foo", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc3);
     
     IndexReader reader = writer.getReader();
@@ -601,7 +601,7 @@
     RandomIndexWriter w  = new RandomIndexWriter(random, dir, analyzer);
     List<List<String>> docs = new ArrayList<List<String>>();
     Document d = new Document();
-    Field f = new Field("f", "", Field.Store.NO, Field.Index.ANALYZED);
+    Field f = newField("f", "", Field.Store.NO, Field.Index.ANALYZED);
     d.add(f);
 
     Random r = random;
Index: lucene/src/test/org/apache/lucene/search/TestPositionIncrement.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestPositionIncrement.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestPositionIncrement.java	(working copy)
@@ -93,7 +93,7 @@
     Directory store = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random, store, analyzer);
     Document d = new Document();
-    d.add(new Field("field", "bogus", Field.Store.YES, Field.Index.ANALYZED));
+    d.add(newField("field", "bogus", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(d);
     IndexReader reader = writer.getReader();
     writer.close();
Index: lucene/src/test/org/apache/lucene/search/TestTermRangeQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestTermRangeQuery.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestTermRangeQuery.java	(working copy)
@@ -355,8 +355,8 @@
   private void insertDoc(IndexWriter writer, String content) throws IOException {
     Document doc = new Document();
 
-    doc.add(new Field("id", "id" + docCount, Field.Store.YES, Field.Index.NOT_ANALYZED));
-    doc.add(new Field("content", content, Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("id", "id" + docCount, Field.Store.YES, Field.Index.NOT_ANALYZED));
+    doc.add(newField("content", content, Field.Store.NO, Field.Index.ANALYZED));
 
     writer.addDocument(doc);
     docCount++;
Index: lucene/src/test/org/apache/lucene/search/TestBooleanMinShouldMatch.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestBooleanMinShouldMatch.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestBooleanMinShouldMatch.java	(working copy)
@@ -56,10 +56,10 @@
 
         for (int i = 0; i < data.length; i++) {
             Document doc = new Document();
-            doc.add(new Field("id", String.valueOf(i), Field.Store.YES, Field.Index.NOT_ANALYZED));//Field.Keyword("id",String.valueOf(i)));
-            doc.add(new Field("all", "all", Field.Store.YES, Field.Index.NOT_ANALYZED));//Field.Keyword("all","all"));
+            doc.add(newField("id", String.valueOf(i), Field.Store.YES, Field.Index.NOT_ANALYZED));//Field.Keyword("id",String.valueOf(i)));
+            doc.add(newField("all", "all", Field.Store.YES, Field.Index.NOT_ANALYZED));//Field.Keyword("all","all"));
             if (null != data[i]) {
-                doc.add(new Field("data", data[i], Field.Store.YES, Field.Index.ANALYZED));//Field.Text("data",data[i]));
+                doc.add(newField("data", data[i], Field.Store.YES, Field.Index.ANALYZED));//Field.Text("data",data[i]));
             }
             w.addDocument(doc);
         }
Index: lucene/src/test/org/apache/lucene/search/TestBooleanOr.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestBooleanOr.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestBooleanOr.java	(working copy)
@@ -143,12 +143,12 @@
 
     //
     Document d = new Document();
-    d.add(new Field(
+    d.add(newField(
         FIELD_T,
         "Optimize not deleting all files",
         Field.Store.YES,
         Field.Index.ANALYZED));
-    d.add(new Field(
+    d.add(newField(
         FIELD_C,
         "Deleted When I run an optimize in our production environment.",
         Field.Store.YES,
Index: lucene/src/test/org/apache/lucene/search/TestDateSort.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestDateSort.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestDateSort.java	(working copy)
@@ -106,16 +106,16 @@
     assertEquals(Arrays.asList(expectedOrder), Arrays.asList(actualOrder));
   }
 
-  private static Document createDocument(String text, long time) {
+  private Document createDocument(String text, long time) {
     Document document = new Document();
 
     // Add the text field.
-    Field textField = new Field(TEXT_FIELD, text, Field.Store.YES, Field.Index.ANALYZED);
+    Field textField = newField(TEXT_FIELD, text, Field.Store.YES, Field.Index.ANALYZED);
     document.add(textField);
 
     // Add the date/time field.
     String dateTimeString = DateTools.timeToString(time, DateTools.Resolution.SECOND);
-    Field dateTimeField = new Field(DATE_TIME_FIELD, dateTimeString, Field.Store.YES,
+    Field dateTimeField = newField(DATE_TIME_FIELD, dateTimeString, Field.Store.YES,
         Field.Index.NOT_ANALYZED);
     document.add(dateTimeField);
 
Index: lucene/src/test/org/apache/lucene/search/TestSort.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestSort.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestSort.java	(working copy)
@@ -1099,12 +1099,12 @@
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(
                         TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    doc.add(new Field("f", "", Field.Store.NO, Field.Index.NOT_ANALYZED));
-    doc.add(new Field("t", "1", Field.Store.NO, Field.Index.NOT_ANALYZED));
+    doc.add(newField("f", "", Field.Store.NO, Field.Index.NOT_ANALYZED));
+    doc.add(newField("t", "1", Field.Store.NO, Field.Index.NOT_ANALYZED));
     w.addDocument(doc);
     w.commit();
     doc = new Document();
-    doc.add(new Field("t", "1", Field.Store.NO, Field.Index.NOT_ANALYZED));
+    doc.add(newField("t", "1", Field.Store.NO, Field.Index.NOT_ANALYZED));
     w.addDocument(doc);
 
     IndexReader r = w.getReader();
Index: lucene/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java	(working copy)
@@ -59,13 +59,13 @@
 
     for (int i = 0; i < data.length; i++) {
       Document doc = new Document();
-      doc.add(new Field("id", String.valueOf(i), Field.Store.YES,
+      doc.add(newField("id", String.valueOf(i), Field.Store.YES,
           Field.Index.NOT_ANALYZED));// Field.Keyword("id",String.valueOf(i)));
       doc
-          .add(new Field("all", "all", Field.Store.YES,
+          .add(newField("all", "all", Field.Store.YES,
               Field.Index.NOT_ANALYZED));// Field.Keyword("all","all"));
       if (null != data[i]) {
-        doc.add(new Field("data", data[i], Field.Store.YES,
+        doc.add(newField("data", data[i], Field.Store.YES,
             Field.Index.ANALYZED));// Field.Text("data",data[i]));
       }
       writer.addDocument(doc);
@@ -611,10 +611,10 @@
     Directory farsiIndex = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random, farsiIndex, new MockAnalyzer(MockTokenizer.SIMPLE, true));
     Document doc = new Document();
-    doc.add(new Field("content", "\u0633\u0627\u0628", Field.Store.YES,
+    doc.add(newField("content", "\u0633\u0627\u0628", Field.Store.YES,
         Field.Index.NOT_ANALYZED));
     doc
-        .add(new Field("body", "body", Field.Store.YES,
+        .add(newField("body", "body", Field.Store.YES,
             Field.Index.NOT_ANALYZED));
     writer.addDocument(doc);
 
@@ -656,9 +656,9 @@
     String[] words = { "H\u00D8T", "H\u00C5T", "MAND" };
     for (int docnum = 0 ; docnum < words.length ; ++docnum) {   
       Document doc = new Document();
-      doc.add(new Field("content", words[docnum], 
+      doc.add(newField("content", words[docnum], 
                         Field.Store.YES, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("body", "body",
+      doc.add(newField("body", "body",
                         Field.Store.YES, Field.Index.NOT_ANALYZED));
       writer.addDocument(doc);
     }
Index: lucene/src/test/org/apache/lucene/search/TestWildcardRandom.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestWildcardRandom.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestWildcardRandom.java	(working copy)
@@ -47,7 +47,7 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, dir);
     
     Document doc = new Document();
-    Field field = new Field("field", "", Field.Store.NO, Field.Index.ANALYZED);
+    Field field = newField("field", "", Field.Store.NO, Field.Index.ANALYZED);
     doc.add(field);
     
     NumberFormat df = new DecimalFormat("0000", new DecimalFormatSymbols(Locale.ENGLISH));
Index: lucene/src/test/org/apache/lucene/search/payloads/TestPayloadNearQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/payloads/TestPayloadNearQuery.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/payloads/TestPayloadNearQuery.java	(working copy)
@@ -113,9 +113,9 @@
     //writer.infoStream = System.out;
     for (int i = 0; i < 1000; i++) {
       Document doc = new Document();
-      doc.add(new Field("field", English.intToEnglish(i), Field.Store.YES, Field.Index.ANALYZED));
+      doc.add(newField("field", English.intToEnglish(i), Field.Store.YES, Field.Index.ANALYZED));
       String txt = English.intToEnglish(i) +' '+English.intToEnglish(i+1);
-      doc.add(new Field("field2",  txt, Field.Store.YES, Field.Index.ANALYZED));
+      doc.add(newField("field2",  txt, Field.Store.YES, Field.Index.ANALYZED));
       writer.addDocument(doc);
     }
     reader = writer.getReader();
Index: lucene/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery.java	(working copy)
@@ -118,11 +118,11 @@
     //writer.infoStream = System.out;
     for (int i = 0; i < 1000; i++) {
       Document doc = new Document();
-      Field noPayloadField = new Field(PayloadHelper.NO_PAYLOAD_FIELD, English.intToEnglish(i), Field.Store.YES, Field.Index.ANALYZED);
+      Field noPayloadField = newField(PayloadHelper.NO_PAYLOAD_FIELD, English.intToEnglish(i), Field.Store.YES, Field.Index.ANALYZED);
       //noPayloadField.setBoost(0);
       doc.add(noPayloadField);
-      doc.add(new Field("field", English.intToEnglish(i), Field.Store.YES, Field.Index.ANALYZED));
-      doc.add(new Field("multiField", English.intToEnglish(i) + "  " + English.intToEnglish(i), Field.Store.YES, Field.Index.ANALYZED));
+      doc.add(newField("field", English.intToEnglish(i), Field.Store.YES, Field.Index.ANALYZED));
+      doc.add(newField("multiField", English.intToEnglish(i) + "  " + English.intToEnglish(i), Field.Store.YES, Field.Index.ANALYZED));
       writer.addDocument(doc);
     }
     reader = writer.getReader();
Index: lucene/src/test/org/apache/lucene/search/TestBooleanScorer.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestBooleanScorer.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestBooleanScorer.java	(working copy)
@@ -46,7 +46,7 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, directory);
     for (int i = 0; i < values.length; i++) {
       Document doc = new Document();
-      doc.add(new Field(FIELD, values[i], Field.Store.YES, Field.Index.NOT_ANALYZED));
+      doc.add(newField(FIELD, values[i], Field.Store.YES, Field.Index.NOT_ANALYZED));
       writer.addDocument(doc);
     }
     IndexReader ir = writer.getReader();
Index: lucene/src/test/org/apache/lucene/search/TestDocIdSet.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestDocIdSet.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestDocIdSet.java	(working copy)
@@ -103,7 +103,7 @@
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random, dir);
     Document doc = new Document();
-    doc.add(new Field("c", "val", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
+    doc.add(newField("c", "val", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
     writer.addDocument(doc);
     IndexReader reader = writer.getReader();
     writer.close();
Index: lucene/src/test/org/apache/lucene/search/spans/TestNearSpansOrdered.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/spans/TestNearSpansOrdered.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/spans/TestNearSpansOrdered.java	(working copy)
@@ -57,7 +57,7 @@
     RandomIndexWriter writer= new RandomIndexWriter(random, directory);
     for (int i = 0; i < docFields.length; i++) {
       Document doc = new Document();
-      doc.add(new Field(FIELD, docFields[i], Field.Store.NO, Field.Index.ANALYZED));
+      doc.add(newField(FIELD, docFields[i], Field.Store.NO, Field.Index.ANALYZED));
       writer.addDocument(doc);
     }
     reader = writer.getReader();
Index: lucene/src/test/org/apache/lucene/search/spans/TestBasics.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/spans/TestBasics.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/spans/TestBasics.java	(working copy)
@@ -65,7 +65,7 @@
     //writer.infoStream = System.out;
     for (int i = 0; i < 1000; i++) {
       Document doc = new Document();
-      doc.add(new Field("field", English.intToEnglish(i), Field.Store.YES, Field.Index.ANALYZED));
+      doc.add(newField("field", English.intToEnglish(i), Field.Store.YES, Field.Index.ANALYZED));
       writer.addDocument(doc);
     }
     reader = writer.getReader();
Index: lucene/src/test/org/apache/lucene/search/spans/TestSpans.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/spans/TestSpans.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/spans/TestSpans.java	(working copy)
@@ -53,7 +53,7 @@
     RandomIndexWriter writer= new RandomIndexWriter(random, directory);
     for (int i = 0; i < docFields.length; i++) {
       Document doc = new Document();
-      doc.add(new Field(field, docFields[i], Field.Store.YES, Field.Index.ANALYZED));
+      doc.add(newField(field, docFields[i], Field.Store.YES, Field.Index.ANALYZED));
       writer.addDocument(doc);
     }
     reader = writer.getReader();
@@ -433,8 +433,8 @@
   // LUCENE-1404
   private void addDoc(IndexWriter writer, String id, String text) throws IOException {
     final Document doc = new Document();
-    doc.add( new Field("id", id, Field.Store.YES, Field.Index.NOT_ANALYZED) );
-    doc.add( new Field("text", text, Field.Store.YES, Field.Index.ANALYZED) );
+    doc.add( newField("id", id, Field.Store.YES, Field.Index.NOT_ANALYZED) );
+    doc.add( newField("text", text, Field.Store.YES, Field.Index.ANALYZED) );
     writer.addDocument(doc);
   }
 
Index: lucene/src/test/org/apache/lucene/search/spans/TestFieldMaskingSpanQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/spans/TestFieldMaskingSpanQuery.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/spans/TestFieldMaskingSpanQuery.java	(working copy)
@@ -43,8 +43,8 @@
     return doc;
   }
   
-  protected static Field field(String name, String value) {
-    return new Field(name, value, Field.Store.NO, Field.Index.ANALYZED);
+  protected Field field(String name, String value) {
+    return newField(name, value, Field.Store.NO, Field.Index.ANALYZED);
   }
 
   protected IndexSearcher searcher;
Index: lucene/src/test/org/apache/lucene/search/spans/TestSpansAdvanced.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/spans/TestSpansAdvanced.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/spans/TestSpansAdvanced.java	(working copy)
@@ -89,9 +89,9 @@
       final String text) throws IOException {
     
     final Document document = new Document();
-    document.add(new Field(FIELD_ID, id, Field.Store.YES,
+    document.add(newField(FIELD_ID, id, Field.Store.YES,
         Field.Index.NOT_ANALYZED));
-    document.add(new Field(FIELD_TEXT, text, Field.Store.YES,
+    document.add(newField(FIELD_TEXT, text, Field.Store.YES,
         Field.Index.ANALYZED));
     writer.addDocument(document);
   }
Index: lucene/src/test/org/apache/lucene/search/spans/TestPayloadSpans.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/spans/TestPayloadSpans.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/spans/TestPayloadSpans.java	(working copy)
@@ -118,7 +118,7 @@
                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new PayloadAnalyzer()).setSimilarity(similarity));
 
     Document doc = new Document();
-    doc.add(new Field(PayloadHelper.FIELD, "one two three one four three",
+    doc.add(newField(PayloadHelper.FIELD, "one two three one four three",
         Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
     IndexReader reader = writer.getReader();
@@ -377,7 +377,7 @@
                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new PayloadAnalyzer()).setSimilarity(similarity));
 
     Document doc = new Document();
-    doc.add(new Field(PayloadHelper.FIELD,"xx rr yy mm  pp", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField(PayloadHelper.FIELD,"xx rr yy mm  pp", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
   
     IndexReader reader = writer.getReader();
@@ -440,7 +440,7 @@
     for(int i = 0; i < docs.length; i++) {
       doc = new Document();
       String docText = docs[i];
-      doc.add(new Field(PayloadHelper.FIELD,docText, Field.Store.YES, Field.Index.ANALYZED));
+      doc.add(newField(PayloadHelper.FIELD,docText, Field.Store.YES, Field.Index.ANALYZED));
       writer.addDocument(doc);
     }
 
Index: lucene/src/test/org/apache/lucene/search/TestMultiSearcher.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestMultiSearcher.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestMultiSearcher.java	(working copy)
@@ -63,23 +63,23 @@
 
         // creating a document to store
         Document lDoc = new Document();
-        lDoc.add(new Field("fulltext", "Once upon a time.....", Field.Store.YES, Field.Index.ANALYZED));
-        lDoc.add(new Field("id", "doc1", Field.Store.YES, Field.Index.NOT_ANALYZED));
-        lDoc.add(new Field("handle", "1", Field.Store.YES, Field.Index.NOT_ANALYZED));
+        lDoc.add(newField("fulltext", "Once upon a time.....", Field.Store.YES, Field.Index.ANALYZED));
+        lDoc.add(newField("id", "doc1", Field.Store.YES, Field.Index.NOT_ANALYZED));
+        lDoc.add(newField("handle", "1", Field.Store.YES, Field.Index.NOT_ANALYZED));
 
         // creating a document to store
         Document lDoc2 = new Document();
-        lDoc2.add(new Field("fulltext", "in a galaxy far far away.....",
+        lDoc2.add(newField("fulltext", "in a galaxy far far away.....",
             Field.Store.YES, Field.Index.ANALYZED));
-        lDoc2.add(new Field("id", "doc2", Field.Store.YES, Field.Index.NOT_ANALYZED));
-        lDoc2.add(new Field("handle", "1", Field.Store.YES, Field.Index.NOT_ANALYZED));
+        lDoc2.add(newField("id", "doc2", Field.Store.YES, Field.Index.NOT_ANALYZED));
+        lDoc2.add(newField("handle", "1", Field.Store.YES, Field.Index.NOT_ANALYZED));
 
         // creating a document to store
         Document lDoc3 = new Document();
-        lDoc3.add(new Field("fulltext", "a bizarre bug manifested itself....",
+        lDoc3.add(newField("fulltext", "a bizarre bug manifested itself....",
             Field.Store.YES, Field.Index.ANALYZED));
-        lDoc3.add(new Field("id", "doc3", Field.Store.YES, Field.Index.NOT_ANALYZED));
-        lDoc3.add(new Field("handle", "1", Field.Store.YES, Field.Index.NOT_ANALYZED));
+        lDoc3.add(newField("id", "doc3", Field.Store.YES, Field.Index.NOT_ANALYZED));
+        lDoc3.add(newField("handle", "1", Field.Store.YES, Field.Index.NOT_ANALYZED));
 
         // creating an index writer for the first index
         IndexWriter writerA = new IndexWriter(indexStoreA, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
@@ -205,19 +205,19 @@
         indexStoreB.close();
     }
     
-    private static Document createDocument(String contents1, String contents2) {
+    private Document createDocument(String contents1, String contents2) {
         Document document=new Document();
         
-        document.add(new Field("contents", contents1, Field.Store.YES, Field.Index.NOT_ANALYZED));
-      document.add(new Field("other", "other contents", Field.Store.YES, Field.Index.NOT_ANALYZED));
+        document.add(newField("contents", contents1, Field.Store.YES, Field.Index.NOT_ANALYZED));
+      document.add(newField("other", "other contents", Field.Store.YES, Field.Index.NOT_ANALYZED));
         if (contents2!=null) {
-            document.add(new Field("contents", contents2, Field.Store.YES, Field.Index.NOT_ANALYZED));
+            document.add(newField("contents", contents2, Field.Store.YES, Field.Index.NOT_ANALYZED));
         }
         
         return document;
     }
     
-    private static void initIndex(Random random, Directory directory, int nDocs, boolean create, String contents2) throws IOException {
+    private void initIndex(Random random, Directory directory, int nDocs, boolean create, String contents2) throws IOException {
         IndexWriter indexWriter=null;
         
         try {
Index: lucene/src/test/org/apache/lucene/search/TestSubScorerFreqs.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestSubScorerFreqs.java	(revision 995688)
+++ lucene/src/test/org/apache/lucene/search/TestSubScorerFreqs.java	(working copy)
@@ -41,17 +41,18 @@
   @BeforeClass
   public static void makeIndex() throws Exception {
     dir = new RAMDirectory();
+    Random random = newStaticRandom(TestSubScorerFreqs.class);
     RandomIndexWriter w = new RandomIndexWriter(
-        newStaticRandom(TestSubScorerFreqs.class), dir);
+        random, dir);
     // make sure we have more than one segment occationally
     for (int i = 0; i < 31 * RANDOM_MULTIPLIER; i++) {
       Document doc = new Document();
-      doc.add(new Field("f", "a b c d b c d c d d", Field.Store.NO,
+      doc.add(newField(random, "f", "a b c d b c d c d d", Field.Store.NO,
           Field.Index.ANALYZED));
       w.addDocument(doc);
 
       doc = new Document();
-      doc.add(new Field("f", "a b c d", Field.Store.NO, Field.Index.ANALYZED));
+      doc.add(newField(random, "f", "a b c d", Field.Store.NO, Field.Index.ANALYZED));
       w.addDocument(doc);
     }
 
Index: lucene/src/test/org/apache/lucene/search/TestQueryWrapperFilter.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestQueryWrapperFilter.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestQueryWrapperFilter.java	(working copy)
@@ -34,7 +34,7 @@
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random, dir);
     Document doc = new Document();
-    doc.add(new Field("field", "value", Store.NO, Index.ANALYZED));
+    doc.add(newField("field", "value", Store.NO, Index.ANALYZED));
     writer.addDocument(doc);
     IndexReader reader = writer.getReader();
     writer.close();
Index: lucene/src/test/org/apache/lucene/search/TestSimpleExplanations.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestSimpleExplanations.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestSimpleExplanations.java	(working copy)
@@ -309,11 +309,11 @@
     Directory indexStoreB = newDirectory();
 
     Document lDoc = new Document();
-    lDoc.add(new Field("handle", "1 2", Field.Store.YES, Field.Index.ANALYZED));
+    lDoc.add(newField("handle", "1 2", Field.Store.YES, Field.Index.ANALYZED));
     Document lDoc2 = new Document();
-    lDoc2.add(new Field("handle", "1 2", Field.Store.YES, Field.Index.ANALYZED));
+    lDoc2.add(newField("handle", "1 2", Field.Store.YES, Field.Index.ANALYZED));
     Document lDoc3 = new Document();
-    lDoc3.add(new Field("handle", "1 2", Field.Store.YES, Field.Index.ANALYZED));
+    lDoc3.add(newField("handle", "1 2", Field.Store.YES, Field.Index.ANALYZED));
 
     IndexWriter writerA = new IndexWriter(indexStoreA, newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer()));
Index: lucene/src/test/org/apache/lucene/search/TestMultiValuedNumericRangeQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestMultiValuedNumericRangeQuery.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestMultiValuedNumericRangeQuery.java	(working copy)
@@ -47,7 +47,7 @@
       Document doc = new Document();
       for (int m=0, c=random.nextInt(10); m<=c; m++) {
         int value = random.nextInt(Integer.MAX_VALUE);
-        doc.add(new Field("asc", format.format(value), Field.Store.NO, Field.Index.NOT_ANALYZED));
+        doc.add(newField("asc", format.format(value), Field.Store.NO, Field.Index.NOT_ANALYZED));
         doc.add(new NumericField("trie", Field.Store.NO, true).setIntValue(value));
       }
       writer.addDocument(doc);
Index: lucene/src/test/org/apache/lucene/search/TestTermScorer.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestTermScorer.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestTermScorer.java	(working copy)
@@ -52,7 +52,7 @@
     for (int i = 0; i < values.length; i++) {
       Document doc = new Document();
       doc
-          .add(new Field(FIELD, values[i], Field.Store.YES,
+          .add(newField(FIELD, values[i], Field.Store.YES,
               Field.Index.ANALYZED));
       writer.addDocument(doc);
     }
Index: lucene/src/test/org/apache/lucene/search/TestPhrasePrefixQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestPhrasePrefixQuery.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestPhrasePrefixQuery.java	(working copy)
@@ -50,15 +50,15 @@
     Document doc3 = new Document();
     Document doc4 = new Document();
     Document doc5 = new Document();
-    doc1.add(new Field("body", "blueberry pie", Field.Store.YES,
+    doc1.add(newField("body", "blueberry pie", Field.Store.YES,
         Field.Index.ANALYZED));
-    doc2.add(new Field("body", "blueberry strudel", Field.Store.YES,
+    doc2.add(newField("body", "blueberry strudel", Field.Store.YES,
         Field.Index.ANALYZED));
-    doc3.add(new Field("body", "blueberry pizza", Field.Store.YES,
+    doc3.add(newField("body", "blueberry pizza", Field.Store.YES,
         Field.Index.ANALYZED));
-    doc4.add(new Field("body", "blueberry chewing gum", Field.Store.YES,
+    doc4.add(newField("body", "blueberry chewing gum", Field.Store.YES,
         Field.Index.ANALYZED));
-    doc5.add(new Field("body", "piccadilly circus", Field.Store.YES,
+    doc5.add(newField("body", "piccadilly circus", Field.Store.YES,
         Field.Index.ANALYZED));
     writer.addDocument(doc1);
     writer.addDocument(doc2);
Index: lucene/src/test/org/apache/lucene/search/TestSetNorm.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestSetNorm.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestSetNorm.java	(working copy)
@@ -42,7 +42,7 @@
     IndexWriter writer = new IndexWriter(store, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
 
     // add the same document four times
-    Fieldable f1 = new Field("field", "word", Field.Store.YES, Field.Index.ANALYZED);
+    Fieldable f1 = newField("field", "word", Field.Store.YES, Field.Index.ANALYZED);
     Document d1 = new Document();
     d1.add(f1);
     writer.addDocument(d1);
Index: lucene/src/test/org/apache/lucene/search/TestWildcard.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestWildcard.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestWildcard.java	(working copy)
@@ -211,7 +211,7 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, indexStore);
     for (int i = 0; i < contents.length; ++i) {
       Document doc = new Document();
-      doc.add(new Field(field, contents[i], Field.Store.YES, Field.Index.ANALYZED));
+      doc.add(newField(field, contents[i], Field.Store.YES, Field.Index.ANALYZED));
       writer.addDocument(doc);
     }
     writer.close();
@@ -267,7 +267,7 @@
     RandomIndexWriter iw = new RandomIndexWriter(random, dir);
     for (int i = 0; i < docs.length; i++) {
       Document doc = new Document();
-      doc.add(new Field(field,docs[i],Store.NO,Index.ANALYZED));
+      doc.add(newField(field,docs[i],Store.NO,Index.ANALYZED));
       iw.addDocument(doc);
     }
     iw.close();
Index: lucene/src/test/org/apache/lucene/search/BaseTestRangeFilter.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/BaseTestRangeFilter.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/BaseTestRangeFilter.java	(working copy)
@@ -111,7 +111,7 @@
     
     for (int d = minId; d <= maxId; d++) {
       Document doc = new Document();
-      doc.add(new Field("id", pad(d), Field.Store.YES,
+      doc.add(newField("id", pad(d), Field.Store.YES,
           Field.Index.NOT_ANALYZED));
       int r = index.allowNegativeRandomInts ? random.nextInt() : random
           .nextInt(Integer.MAX_VALUE);
@@ -121,9 +121,9 @@
       if (r < index.minR) {
         index.minR = r;
       }
-      doc.add(new Field("rand", pad(r), Field.Store.YES,
+      doc.add(newField("rand", pad(r), Field.Store.YES,
           Field.Index.NOT_ANALYZED));
-      doc.add(new Field("body", "body", Field.Store.YES,
+      doc.add(newField("body", "body", Field.Store.YES,
           Field.Index.NOT_ANALYZED));
       writer.addDocument(doc);
     }
Index: lucene/src/test/org/apache/lucene/search/TestAutomatonQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestAutomatonQuery.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestAutomatonQuery.java	(working copy)
@@ -43,11 +43,11 @@
     directory = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random, directory);
     Document doc = new Document();
-    Field titleField = new Field("title", "some title", Field.Store.NO,
+    Field titleField = newField("title", "some title", Field.Store.NO,
         Field.Index.ANALYZED);
-    Field field = new Field(FN, "this is document one 2345", Field.Store.NO,
+    Field field = newField(FN, "this is document one 2345", Field.Store.NO,
         Field.Index.ANALYZED);
-    Field footerField = new Field("footer", "a footer", Field.Store.NO,
+    Field footerField = newField("footer", "a footer", Field.Store.NO,
         Field.Index.ANALYZED);
     doc.add(titleField);
     doc.add(field);
Index: lucene/src/test/org/apache/lucene/search/TestBoolean2.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestBoolean2.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestBoolean2.java	(working copy)
@@ -54,7 +54,7 @@
     RandomIndexWriter writer= new RandomIndexWriter(random, directory);
     for (int i = 0; i < docFields.length; i++) {
       Document doc = new Document();
-      doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));
+      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));
       writer.addDocument(doc);
     }
     writer.close();
@@ -77,12 +77,12 @@
 
     RandomIndexWriter w = new RandomIndexWriter(random, dir2);
     Document doc = new Document();
-    doc.add(new Field("field2", "xxx", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("field2", "xxx", Field.Store.NO, Field.Index.ANALYZED));
     for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {
       w.addDocument(doc);
     }
     doc = new Document();
-    doc.add(new Field("field2", "big bad bug", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("field2", "big bad bug", Field.Store.NO, Field.Index.ANALYZED));
     for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {
       w.addDocument(doc);
     }
Index: lucene/src/test/org/apache/lucene/search/function/FunctionTestSetup.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/function/FunctionTestSetup.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/function/FunctionTestSetup.java	(working copy)
@@ -125,19 +125,19 @@
     Fieldable f;
     int scoreAndID = i + 1;
 
-    f = new Field(ID_FIELD, id2String(scoreAndID), Field.Store.YES, Field.Index.NOT_ANALYZED); // for debug purposes
+    f = newField(ID_FIELD, id2String(scoreAndID), Field.Store.YES, Field.Index.NOT_ANALYZED); // for debug purposes
     f.setOmitNorms(true);
     d.add(f);
 
-    f = new Field(TEXT_FIELD, "text of doc" + scoreAndID + textLine(i), Field.Store.NO, Field.Index.ANALYZED); // for regular search
+    f = newField(TEXT_FIELD, "text of doc" + scoreAndID + textLine(i), Field.Store.NO, Field.Index.ANALYZED); // for regular search
     f.setOmitNorms(true);
     d.add(f);
 
-    f = new Field(INT_FIELD, "" + scoreAndID, Field.Store.NO, Field.Index.NOT_ANALYZED); // for function scoring
+    f = newField(INT_FIELD, "" + scoreAndID, Field.Store.NO, Field.Index.NOT_ANALYZED); // for function scoring
     f.setOmitNorms(true);
     d.add(f);
 
-    f = new Field(FLOAT_FIELD, scoreAndID + ".000", Field.Store.NO, Field.Index.NOT_ANALYZED); // for function scoring
+    f = newField(FLOAT_FIELD, scoreAndID + ".000", Field.Store.NO, Field.Index.NOT_ANALYZED); // for function scoring
     f.setOmitNorms(true);
     d.add(f);
 
Index: lucene/src/test/org/apache/lucene/search/function/TestValueSource.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/function/TestValueSource.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/function/TestValueSource.java	(working copy)
@@ -30,7 +30,7 @@
     Directory dir = newDirectory();
     IndexWriter w = new IndexWriter(dir, new MockAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
-    Field f = new Field("field", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
+    Field f = newField("field", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
     doc.add(f);
 
     for(int i=0;i<17;i++) {
Index: lucene/src/test/org/apache/lucene/search/TestExplanations.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestExplanations.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestExplanations.java	(working copy)
@@ -71,8 +71,8 @@
     RandomIndexWriter writer= new RandomIndexWriter(random, directory);
     for (int i = 0; i < docFields.length; i++) {
       Document doc = new Document();
-      doc.add(new Field(KEY, ""+i, Field.Store.NO, Field.Index.NOT_ANALYZED));
-      doc.add(new Field(FIELD, docFields[i], Field.Store.NO, Field.Index.ANALYZED));
+      doc.add(newField(KEY, ""+i, Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField(FIELD, docFields[i], Field.Store.NO, Field.Index.ANALYZED));
       writer.addDocument(doc);
     }
     reader = writer.getReader();
Index: lucene/src/test/org/apache/lucene/search/TestThreadSafe.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestThreadSafe.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestThreadSafe.java	(working copy)
@@ -114,7 +114,7 @@
         sb.append(" $");
         Field.Store store = Field.Store.YES;  // make random later
         Field.Index index = Field.Index.ANALYZED;  // make random later
-        d.add(new Field("f"+i, sb.toString(), store, index));
+        d.add(newField("f"+i, sb.toString(), store, index));
       }
       iw.addDocument(d);
     }
Index: lucene/src/test/org/apache/lucene/search/TestFieldCache.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestFieldCache.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestFieldCache.java	(working copy)
@@ -55,12 +55,12 @@
     unicodeStrings = new String[NUM_DOCS];
     for (int i = 0; i < NUM_DOCS; i++){
       Document doc = new Document();
-      doc.add(new Field("theLong", String.valueOf(theLong--), Field.Store.NO, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("theDouble", String.valueOf(theDouble--), Field.Store.NO, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("theByte", String.valueOf(theByte--), Field.Store.NO, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("theShort", String.valueOf(theShort--), Field.Store.NO, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("theInt", String.valueOf(theInt--), Field.Store.NO, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("theFloat", String.valueOf(theFloat--), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("theLong", String.valueOf(theLong--), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("theDouble", String.valueOf(theDouble--), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("theByte", String.valueOf(theByte--), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("theShort", String.valueOf(theShort--), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("theInt", String.valueOf(theInt--), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("theFloat", String.valueOf(theFloat--), Field.Store.NO, Field.Index.NOT_ANALYZED));
 
       // sometimes skip the field:
       if (random.nextInt(40) != 17) {
@@ -77,7 +77,7 @@
           s = _TestUtil.randomUnicodeString(random, 250);
         }
         unicodeStrings[i] = s;
-        doc.add(new Field("theRandomUnicodeString", unicodeStrings[i], Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
+        doc.add(newField("theRandomUnicodeString", unicodeStrings[i], Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
       }
       writer.addDocument(doc);
     }
Index: lucene/src/test/org/apache/lucene/search/TestScorerPerf.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestScorerPerf.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestScorerPerf.java	(working copy)
@@ -64,7 +64,7 @@
       Document d = new Document();
       for (int j=0; j<nTerms; j++) {
         if (random.nextInt(freq[j]) == 0) {
-          d.add(new Field("f", terms[j].text(), Field.Store.NO, Field.Index.NOT_ANALYZED));
+          d.add(newField("f", terms[j].text(), Field.Store.NO, Field.Index.NOT_ANALYZED));
           //System.out.println(d);
         }
       }
Index: lucene/src/test/org/apache/lucene/search/TestFuzzyQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestFuzzyQuery.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestFuzzyQuery.java	(working copy)
@@ -381,7 +381,7 @@
 
   private void addDoc(String text, RandomIndexWriter writer) throws IOException {
     Document doc = new Document();
-    doc.add(new Field("field", text, Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("field", text, Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
   }
 }
Index: lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java	(working copy)
@@ -56,7 +56,7 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));
     
     Document doc = new Document();
-    Field field = new Field("field", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
+    Field field = newField("field", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
     doc.add(field);
     List<String> terms = new ArrayList<String>();
     int num = 2000 * RANDOM_MULTIPLIER;
Index: lucene/src/test/org/apache/lucene/search/TestMultiThreadTermVectors.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestMultiThreadTermVectors.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestMultiThreadTermVectors.java	(working copy)
@@ -47,7 +47,7 @@
     //writer.infoStream = System.out;
     for (int i = 0; i < numDocs; i++) {
       Document doc = new Document();
-      Fieldable fld = new Field("field", English.intToEnglish(i), Field.Store.YES, Field.Index.NOT_ANALYZED, Field.TermVector.YES);
+      Fieldable fld = newField("field", English.intToEnglish(i), Field.Store.YES, Field.Index.NOT_ANALYZED, Field.TermVector.YES);
       doc.add(fld);
       writer.addDocument(doc);
     }
Index: lucene/src/test/org/apache/lucene/search/TestRegexpQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestRegexpQuery.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestRegexpQuery.java	(working copy)
@@ -47,7 +47,7 @@
     directory = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random, directory);
     Document doc = new Document();
-    doc.add(new Field(FN,
+    doc.add(newField(FN,
         "the quick brown fox jumps over the lazy ??? dog 493432 49344",
         Field.Store.NO, Field.Index.ANALYZED));
     writer.addDocument(doc);
Index: lucene/src/test/org/apache/lucene/search/TestDateFilter.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestDateFilter.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestDateFilter.java	(working copy)
@@ -51,10 +51,10 @@
     
     Document doc = new Document();
     // add time that is in the past
-    doc.add(new Field("datefield", DateTools.timeToString(now - 1000,
+    doc.add(newField("datefield", DateTools.timeToString(now - 1000,
         DateTools.Resolution.MILLISECOND), Field.Store.YES,
         Field.Index.NOT_ANALYZED));
-    doc.add(new Field("body", "Today is a very sunny day in New York City",
+    doc.add(newField("body", "Today is a very sunny day in New York City",
         Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
     
@@ -117,10 +117,10 @@
     
     Document doc = new Document();
     // add time that is in the future
-    doc.add(new Field("datefield", DateTools.timeToString(now + 888888,
+    doc.add(newField("datefield", DateTools.timeToString(now + 888888,
         DateTools.Resolution.MILLISECOND), Field.Store.YES,
         Field.Index.NOT_ANALYZED));
-    doc.add(new Field("body", "Today is a very sunny day in New York City",
+    doc.add(newField("body", "Today is a very sunny day in New York City",
         Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
     
Index: lucene/src/test/org/apache/lucene/search/TestFilteredSearch.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestFilteredSearch.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestFilteredSearch.java	(working copy)
@@ -67,7 +67,7 @@
     try {
       for (int i = 0; i < 60; i++) {//Simple docs
         Document doc = new Document();
-        doc.add(new Field(FIELD, Integer.toString(i), Field.Store.YES, Field.Index.NOT_ANALYZED));
+        doc.add(newField(FIELD, Integer.toString(i), Field.Store.YES, Field.Index.NOT_ANALYZED));
         writer.addDocument(doc);
       }
       if(optimize)
Index: lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestCachingSpanFilter.java	(working copy)
@@ -42,7 +42,7 @@
 
     // add a doc, refresh the reader, and check that its there
     Document doc = new Document();
-    doc.add(new Field("id", "1", Field.Store.YES, Field.Index.NOT_ANALYZED));
+    doc.add(newField("id", "1", Field.Store.YES, Field.Index.NOT_ANALYZED));
     writer.addDocument(doc);
 
     reader = refreshReader(reader);
Index: lucene/src/test/org/apache/lucene/search/TestMatchAllDocsQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestMatchAllDocsQuery.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestMatchAllDocsQuery.java	(working copy)
@@ -124,7 +124,7 @@
   
   private void addDoc(String text, IndexWriter iw, float boost) throws IOException {
     Document doc = new Document();
-    Field f = new Field("key", text, Field.Store.YES, Field.Index.ANALYZED);
+    Field f = newField("key", text, Field.Store.YES, Field.Index.ANALYZED);
     f.setBoost(boost);
     doc.add(f);
     iw.addDocument(doc);
Index: lucene/src/test/org/apache/lucene/search/TestDisjunctionMaxQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestDisjunctionMaxQuery.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestDisjunctionMaxQuery.java	(working copy)
@@ -88,13 +88,13 @@
     // d1 is an "ok" match for: albino elephant
     {
       Document d1 = new Document();
-      d1.add(new Field("id", "d1", Field.Store.YES, Field.Index.NOT_ANALYZED));// Field.Keyword("id",
+      d1.add(newField("id", "d1", Field.Store.YES, Field.Index.NOT_ANALYZED));// Field.Keyword("id",
                                                                                // "d1"));
       d1
-          .add(new Field("hed", "elephant", Field.Store.YES,
+          .add(newField("hed", "elephant", Field.Store.YES,
               Field.Index.ANALYZED));// Field.Text("hed", "elephant"));
       d1
-          .add(new Field("dek", "elephant", Field.Store.YES,
+          .add(newField("dek", "elephant", Field.Store.YES,
               Field.Index.ANALYZED));// Field.Text("dek", "elephant"));
       writer.addDocument(d1);
     }
@@ -102,15 +102,15 @@
     // d2 is a "good" match for: albino elephant
     {
       Document d2 = new Document();
-      d2.add(new Field("id", "d2", Field.Store.YES, Field.Index.NOT_ANALYZED));// Field.Keyword("id",
+      d2.add(newField("id", "d2", Field.Store.YES, Field.Index.NOT_ANALYZED));// Field.Keyword("id",
                                                                                // "d2"));
       d2
-          .add(new Field("hed", "elephant", Field.Store.YES,
+          .add(newField("hed", "elephant", Field.Store.YES,
               Field.Index.ANALYZED));// Field.Text("hed", "elephant"));
-      d2.add(new Field("dek", "albino", Field.Store.YES, Field.Index.ANALYZED));// Field.Text("dek",
+      d2.add(newField("dek", "albino", Field.Store.YES, Field.Index.ANALYZED));// Field.Text("dek",
                                                                                 // "albino"));
       d2
-          .add(new Field("dek", "elephant", Field.Store.YES,
+          .add(newField("dek", "elephant", Field.Store.YES,
               Field.Index.ANALYZED));// Field.Text("dek", "elephant"));
       writer.addDocument(d2);
     }
@@ -118,12 +118,12 @@
     // d3 is a "better" match for: albino elephant
     {
       Document d3 = new Document();
-      d3.add(new Field("id", "d3", Field.Store.YES, Field.Index.NOT_ANALYZED));// Field.Keyword("id",
+      d3.add(newField("id", "d3", Field.Store.YES, Field.Index.NOT_ANALYZED));// Field.Keyword("id",
                                                                                // "d3"));
-      d3.add(new Field("hed", "albino", Field.Store.YES, Field.Index.ANALYZED));// Field.Text("hed",
+      d3.add(newField("hed", "albino", Field.Store.YES, Field.Index.ANALYZED));// Field.Text("hed",
                                                                                 // "albino"));
       d3
-          .add(new Field("hed", "elephant", Field.Store.YES,
+          .add(newField("hed", "elephant", Field.Store.YES,
               Field.Index.ANALYZED));// Field.Text("hed", "elephant"));
       writer.addDocument(d3);
     }
@@ -131,14 +131,14 @@
     // d4 is the "best" match for: albino elephant
     {
       Document d4 = new Document();
-      d4.add(new Field("id", "d4", Field.Store.YES, Field.Index.NOT_ANALYZED));// Field.Keyword("id",
+      d4.add(newField("id", "d4", Field.Store.YES, Field.Index.NOT_ANALYZED));// Field.Keyword("id",
                                                                                // "d4"));
-      d4.add(new Field("hed", "albino", Field.Store.YES, Field.Index.ANALYZED));// Field.Text("hed",
+      d4.add(newField("hed", "albino", Field.Store.YES, Field.Index.ANALYZED));// Field.Text("hed",
                                                                                 // "albino"));
       d4
-          .add(new Field("hed", "elephant", Field.Store.YES,
+          .add(newField("hed", "elephant", Field.Store.YES,
               Field.Index.ANALYZED));// Field.Text("hed", "elephant"));
-      d4.add(new Field("dek", "albino", Field.Store.YES, Field.Index.ANALYZED));// Field.Text("dek",
+      d4.add(newField("dek", "albino", Field.Store.YES, Field.Index.ANALYZED));// Field.Text("dek",
                                                                                 // "albino"));
       writer.addDocument(d4);
     }
Index: lucene/src/test/org/apache/lucene/search/TestSimilarity.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestSimilarity.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestSimilarity.java	(working copy)
@@ -68,10 +68,10 @@
         .setSimilarity(new SimpleSimilarity()));
     
     Document d1 = new Document();
-    d1.add(new Field("field", "a c", Field.Store.YES, Field.Index.ANALYZED));
+    d1.add(newField("field", "a c", Field.Store.YES, Field.Index.ANALYZED));
 
     Document d2 = new Document();
-    d2.add(new Field("field", "a b c", Field.Store.YES, Field.Index.ANALYZED));
+    d2.add(newField("field", "a b c", Field.Store.YES, Field.Index.ANALYZED));
     
     writer.addDocument(d1);
     writer.addDocument(d2);
Index: lucene/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java	(working copy)
@@ -520,8 +520,8 @@
 
     for (int d = -20; d <= 20; d++) {
       Document doc = new Document();
-      doc.add(new Field("id",Integer.toString(d), Field.Store.NO, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("body","body", Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("id",Integer.toString(d), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("body","body", Field.Store.NO, Field.Index.NOT_ANALYZED));
       writer.addDocument(doc);
     }
     
Index: lucene/src/test/org/apache/lucene/search/TestRegexpRandom.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestRegexpRandom.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestRegexpRandom.java	(working copy)
@@ -47,7 +47,7 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, dir);
     
     Document doc = new Document();
-    Field field = new Field("field", "", Field.Store.NO, Field.Index.ANALYZED);
+    Field field = newField("field", "", Field.Store.NO, Field.Index.ANALYZED);
     doc.add(field);
     
     NumberFormat df = new DecimalFormat("0000", new DecimalFormatSymbols(Locale.ENGLISH));
Index: lucene/src/test/org/apache/lucene/search/TestSpanQueryFilter.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestSpanQueryFilter.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestSpanQueryFilter.java	(working copy)
@@ -41,7 +41,7 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, dir);
     for (int i = 0; i < 500; i++) {
       Document document = new Document();
-      document.add(new Field("field", English.intToEnglish(i) + " equals " + English.intToEnglish(i),
+      document.add(newField("field", English.intToEnglish(i) + " equals " + English.intToEnglish(i),
               Field.Store.NO, Field.Index.ANALYZED));
       writer.addDocument(document);
     }
Index: lucene/src/test/org/apache/lucene/search/TestPrefixRandom.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestPrefixRandom.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestPrefixRandom.java	(working copy)
@@ -48,7 +48,7 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, dir, new MockAnalyzer(MockTokenizer.KEYWORD, false));
     
     Document doc = new Document();
-    Field field = new Field("field", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
+    Field field = newField("field", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
     doc.add(field);
 
     int num = 2000 * RANDOM_MULTIPLIER;
Index: lucene/src/test/org/apache/lucene/search/TestCustomSearcherSort.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestCustomSearcherSort.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestCustomSearcherSort.java	(working copy)
@@ -56,15 +56,15 @@
       Document doc = new Document();
       if ((i % 5) != 0) { // some documents must not have an entry in the first
                           // sort field
-        doc.add(new Field("publicationDate_", random.getLuceneDate(),
+        doc.add(newField("publicationDate_", random.getLuceneDate(),
             Field.Store.YES, Field.Index.NOT_ANALYZED));
       }
       if ((i % 7) == 0) { // some documents to match the query (see below)
-        doc.add(new Field("content", "test", Field.Store.YES,
+        doc.add(newField("content", "test", Field.Store.YES,
             Field.Index.ANALYZED));
       }
       // every document has a defined 'mandant' field
-      doc.add(new Field("mandant", Integer.toString(i % 3), Field.Store.YES,
+      doc.add(newField("mandant", Integer.toString(i % 3), Field.Store.YES,
           Field.Index.NOT_ANALYZED));
       writer.addDocument(doc);
     }
Index: lucene/src/test/org/apache/lucene/search/TestPrefixInBooleanQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestPrefixInBooleanQuery.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestPrefixInBooleanQuery.java	(working copy)
@@ -50,26 +50,26 @@
 
     for (int i = 0; i < 5137; ++i) {
       Document doc = new Document();
-      doc.add(new Field(FIELD, "meaninglessnames", Field.Store.YES,
+      doc.add(newField(FIELD, "meaninglessnames", Field.Store.YES,
                         Field.Index.NOT_ANALYZED));
       writer.addDocument(doc);
     }
     { 
       Document doc = new Document();
-      doc.add(new Field(FIELD, "tangfulin", Field.Store.YES,
+      doc.add(newField(FIELD, "tangfulin", Field.Store.YES,
                         Field.Index.NOT_ANALYZED));
       writer.addDocument(doc);
     }
 
     for (int i = 5138; i < 11377; ++i) {
       Document doc = new Document();
-      doc.add(new Field(FIELD, "meaninglessnames", Field.Store.YES,
+      doc.add(newField(FIELD, "meaninglessnames", Field.Store.YES,
                         Field.Index.NOT_ANALYZED));
       writer.addDocument(doc);
     }
     {
       Document doc = new Document();
-      doc.add(new Field(FIELD, "tangfulin", Field.Store.YES,
+      doc.add(newField(FIELD, "tangfulin", Field.Store.YES,
                         Field.Index.NOT_ANALYZED));
       writer.addDocument(doc);
     }
Index: lucene/src/test/org/apache/lucene/search/TestDocBoost.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestDocBoost.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestDocBoost.java	(working copy)
@@ -40,8 +40,8 @@
     Directory store = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random, store);
 
-    Fieldable f1 = new Field("field", "word", Field.Store.YES, Field.Index.ANALYZED);
-    Fieldable f2 = new Field("field", "word", Field.Store.YES, Field.Index.ANALYZED);
+    Fieldable f1 = newField("field", "word", Field.Store.YES, Field.Index.ANALYZED);
+    Fieldable f2 = newField("field", "word", Field.Store.YES, Field.Index.ANALYZED);
     f2.setBoost(2.0f);
 
     Document d1 = new Document();
Index: lucene/src/test/org/apache/lucene/search/TestElevationComparator.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestElevationComparator.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestElevationComparator.java	(working copy)
@@ -119,7 +119,7 @@
  private Document adoc(String[] vals) {
    Document doc = new Document();
    for (int i = 0; i < vals.length - 2; i += 2) {
-     doc.add(new Field(vals[i], vals[i + 1], Field.Store.YES, Field.Index.ANALYZED));
+     doc.add(newField(vals[i], vals[i + 1], Field.Store.YES, Field.Index.ANALYZED));
    }
    return doc;
  }
Index: lucene/src/test/org/apache/lucene/search/TestMultiSearcherRanking.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestMultiSearcherRanking.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestMultiSearcherRanking.java	(working copy)
@@ -166,7 +166,7 @@
   
   private void add(String value, IndexWriter iw) throws IOException {
     Document d = new Document();
-    d.add(new Field(FIELD_NAME, value, Field.Store.YES, Field.Index.ANALYZED));
+    d.add(newField(FIELD_NAME, value, Field.Store.YES, Field.Index.ANALYZED));
     iw.addDocument(d);
   }
   
Index: lucene/src/test/org/apache/lucene/search/TestPrefixQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestPrefixQuery.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestPrefixQuery.java	(working copy)
@@ -39,7 +39,7 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, directory);
     for (int i = 0; i < categories.length; i++) {
       Document doc = new Document();
-      doc.add(new Field("category", categories[i], Field.Store.YES, Field.Index.NOT_ANALYZED));
+      doc.add(newField("category", categories[i], Field.Store.YES, Field.Index.NOT_ANALYZED));
       writer.addDocument(doc);
     }
     IndexReader reader = writer.getReader();
Index: lucene/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestMultiPhraseQuery.java	(working copy)
@@ -155,7 +155,7 @@
   
   private void add(String s, RandomIndexWriter writer) throws IOException {
     Document doc = new Document();
-    doc.add(new Field("body", s, Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("body", s, Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
   }
   
@@ -278,8 +278,8 @@
   private void add(String s, String type, RandomIndexWriter writer)
       throws IOException {
     Document doc = new Document();
-    doc.add(new Field("body", s, Field.Store.YES, Field.Index.ANALYZED));
-    doc.add(new Field("type", type, Field.Store.YES, Field.Index.NOT_ANALYZED));
+    doc.add(newField("body", s, Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("type", type, Field.Store.YES, Field.Index.NOT_ANALYZED));
     writer.addDocument(doc);
   }
   
Index: lucene/src/test/org/apache/lucene/search/TestBooleanPrefixQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestBooleanPrefixQuery.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestBooleanPrefixQuery.java	(working copy)
@@ -78,7 +78,7 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, directory);
     for (int i = 0; i < categories.length; i++) {
       Document doc = new Document();
-      doc.add(new Field("category", categories[i], Field.Store.YES, Field.Index.NOT_ANALYZED));
+      doc.add(newField("category", categories[i], Field.Store.YES, Field.Index.NOT_ANALYZED));
       writer.addDocument(doc);
     }
     reader = writer.getReader();
Index: lucene/src/test/org/apache/lucene/search/TestFilteredQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestFilteredQuery.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestFilteredQuery.java	(working copy)
@@ -51,23 +51,23 @@
     RandomIndexWriter writer = new RandomIndexWriter (random, directory);
 
     Document doc = new Document();
-    doc.add (new Field("field", "one two three four five", Field.Store.YES, Field.Index.ANALYZED));
-    doc.add (new Field("sorter", "b", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add (newField("field", "one two three four five", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add (newField("sorter", "b", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument (doc);
 
     doc = new Document();
-    doc.add (new Field("field", "one two three four", Field.Store.YES, Field.Index.ANALYZED));
-    doc.add (new Field("sorter", "d", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add (newField("field", "one two three four", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add (newField("sorter", "d", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument (doc);
 
     doc = new Document();
-    doc.add (new Field("field", "one two three y", Field.Store.YES, Field.Index.ANALYZED));
-    doc.add (new Field("sorter", "a", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add (newField("field", "one two three y", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add (newField("sorter", "a", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument (doc);
 
     doc = new Document();
-    doc.add (new Field("field", "one two x", Field.Store.YES, Field.Index.ANALYZED));
-    doc.add (new Field("sorter", "c", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add (newField("field", "one two x", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add (newField("sorter", "c", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument (doc);
 
     // tests here require single segment (eg try seed
Index: lucene/src/test/org/apache/lucene/search/TestFieldCacheTermsFilter.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestFieldCacheTermsFilter.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestFieldCacheTermsFilter.java	(working copy)
@@ -41,7 +41,7 @@
     for (int i = 0; i < 100; i++) {
       Document doc = new Document();
       int term = i * 10; //terms are units of 10;
-      doc.add(new Field(fieldName, "" + term, Field.Store.YES, Field.Index.NOT_ANALYZED));
+      doc.add(newField(fieldName, "" + term, Field.Store.YES, Field.Index.NOT_ANALYZED));
       w.addDocument(doc);
     }
     IndexReader reader = w.getReader();
Index: lucene/src/test/org/apache/lucene/search/TestTermRangeFilter.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestTermRangeFilter.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/search/TestTermRangeFilter.java	(working copy)
@@ -401,10 +401,10 @@
     Directory farsiIndex = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random, farsiIndex);
     Document doc = new Document();
-    doc.add(new Field("content", "\u0633\u0627\u0628", Field.Store.YES,
+    doc.add(newField("content", "\u0633\u0627\u0628", Field.Store.YES,
         Field.Index.NOT_ANALYZED));
     doc
-        .add(new Field("body", "body", Field.Store.YES,
+        .add(newField("body", "body", Field.Store.YES,
             Field.Index.NOT_ANALYZED));
     writer.addDocument(doc);
     
@@ -445,9 +445,9 @@
     String[] words = {"H\u00D8T", "H\u00C5T", "MAND"};
     for (int docnum = 0; docnum < words.length; ++docnum) {
       Document doc = new Document();
-      doc.add(new Field("content", words[docnum], Field.Store.YES,
+      doc.add(newField("content", words[docnum], Field.Store.YES,
           Field.Index.NOT_ANALYZED));
-      doc.add(new Field("body", "body", Field.Store.YES,
+      doc.add(newField("body", "body", Field.Store.YES,
           Field.Index.NOT_ANALYZED));
       writer.addDocument(doc);
     }
Index: lucene/src/test/org/apache/lucene/TestSearch.java
===================================================================
--- lucene/src/test/org/apache/lucene/TestSearch.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/TestSearch.java	(working copy)
@@ -91,7 +91,7 @@
       };
       for (int j = 0; j < docs.length; j++) {
         Document d = new Document();
-        d.add(new Field("contents", docs[j], Field.Store.YES, Field.Index.ANALYZED));
+        d.add(newField("contents", docs[j], Field.Store.YES, Field.Index.ANALYZED));
         writer.addDocument(d);
       }
       writer.close();
Index: lucene/src/test/org/apache/lucene/index/TestSnapshotDeletionPolicy.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestSnapshotDeletionPolicy.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestSnapshotDeletionPolicy.java	(working copy)
@@ -111,7 +111,7 @@
         @Override
         public void run() {
           Document doc = new Document();
-          doc.add(new Field("content", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+          doc.add(newField("content", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
           do {
             for(int i=0;i<27;i++) {
               try {
@@ -152,7 +152,7 @@
     // final segment, so deletion policy has a chance to
     // delete again:
     Document doc = new Document();
-    doc.add(new Field("content", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(newField("content", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
     writer.addDocument(doc);
 
     // Make sure we don't have any leftover files in the
Index: lucene/src/test/org/apache/lucene/index/TestPayloadProcessorProvider.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestPayloadProcessorProvider.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestPayloadProcessorProvider.java	(working copy)
@@ -140,8 +140,8 @@
     TokenStream payloadTS2 = new PayloadTokenStream("p2");
     for (int i = 0; i < NUM_DOCS; i++) {
       Document doc = new Document();
-      doc.add(new Field("id", "doc" + i, Store.NO, Index.NOT_ANALYZED_NO_NORMS));
-      doc.add(new Field("content", "doc content " + i, Store.NO, Index.ANALYZED));
+      doc.add(newField("id", "doc" + i, Store.NO, Index.NOT_ANALYZED_NO_NORMS));
+      doc.add(newField("content", "doc content " + i, Store.NO, Index.ANALYZED));
       doc.add(new Field("p", payloadTS1));
       doc.add(new Field("p", payloadTS2));
       writer.addDocument(doc);
Index: lucene/src/test/org/apache/lucene/index/TestParallelTermEnum.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestParallelTermEnum.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestParallelTermEnum.java	(working copy)
@@ -42,11 +42,11 @@
         IndexWriter iw1 = new IndexWriter(rd1, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
 
         doc = new Document();
-        doc.add(new Field("field1", "the quick brown fox jumps", Store.YES,
+        doc.add(newField("field1", "the quick brown fox jumps", Store.YES,
             Index.ANALYZED));
-        doc.add(new Field("field2", "the quick brown fox jumps", Store.YES,
+        doc.add(newField("field2", "the quick brown fox jumps", Store.YES,
             Index.ANALYZED));
-        doc.add(new Field("field4", "", Store.NO, Index.ANALYZED));
+        doc.add(newField("field4", "", Store.NO, Index.ANALYZED));
         iw1.addDocument(doc);
 
         iw1.close();
@@ -54,10 +54,10 @@
         IndexWriter iw2 = new IndexWriter(rd2, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
 
         doc = new Document();
-        doc.add(new Field("field0", "", Store.NO, Index.ANALYZED));
-        doc.add(new Field("field1", "the fox jumps over the lazy dog",
+        doc.add(newField("field0", "", Store.NO, Index.ANALYZED));
+        doc.add(newField("field1", "the fox jumps over the lazy dog",
             Store.YES, Index.ANALYZED));
-        doc.add(new Field("field3", "the fox jumps over the lazy dog",
+        doc.add(newField("field3", "the fox jumps over the lazy dog",
             Store.YES, Index.ANALYZED));
         iw2.addDocument(doc);
 
Index: lucene/src/test/org/apache/lucene/index/TestIndexReaderClone.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexReaderClone.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestIndexReaderClone.java	(working copy)
@@ -495,7 +495,7 @@
     ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundFile(false);
     ((LogMergePolicy) w.getConfig().getMergePolicy()).setUseCompoundDocStore(false);
     Document doc = new Document();
-    doc.add(new Field("field", "yes it's stored", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("field", "yes it's stored", Field.Store.YES, Field.Index.ANALYZED));
     w.addDocument(doc);
     w.close();
     IndexReader r1 = IndexReader.open(dir, false);
Index: lucene/src/test/org/apache/lucene/index/TestIsCurrent.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIsCurrent.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestIsCurrent.java	(working copy)
@@ -45,7 +45,7 @@
 
     // write document
     Document doc = new Document();
-    doc.add(new Field("UUID", "1", Store.YES, Index.ANALYZED));
+    doc.add(newField("UUID", "1", Store.YES, Index.ANALYZED));
     writer.addDocument(doc);
     writer.commit();
   }
Index: lucene/src/test/org/apache/lucene/index/TestSegmentTermEnum.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestSegmentTermEnum.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestSegmentTermEnum.java	(working copy)
@@ -127,7 +127,7 @@
   private void addDoc(IndexWriter writer, String value) throws IOException
   {
     Document doc = new Document();
-    doc.add(new Field("content", value, Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("content", value, Field.Store.NO, Field.Index.ANALYZED));
     writer.addDocument(doc);
   }
 }
Index: lucene/src/test/org/apache/lucene/index/TestRollback.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestRollback.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestRollback.java	(working copy)
@@ -33,7 +33,7 @@
     RandomIndexWriter rw = new RandomIndexWriter(random, dir);
     for (int i = 0; i < 5; i++) {
       Document doc = new Document();
-      doc.add(new Field("pk", Integer.toString(i), Store.YES, Index.ANALYZED_NO_NORMS));
+      doc.add(newField("pk", Integer.toString(i), Store.YES, Index.ANALYZED_NO_NORMS));
       rw.addDocument(doc);
     }
     rw.close();
@@ -45,8 +45,8 @@
     for (int i = 0; i < 3; i++) {
       Document doc = new Document();
       String value = Integer.toString(i);
-      doc.add(new Field("pk", value, Store.YES, Index.ANALYZED_NO_NORMS));
-      doc.add(new Field("text", "foo", Store.YES, Index.ANALYZED_NO_NORMS));
+      doc.add(newField("pk", value, Store.YES, Index.ANALYZED_NO_NORMS));
+      doc.add(newField("text", "foo", Store.YES, Index.ANALYZED_NO_NORMS));
       w.updateDocument(pkTerm.createTerm(value), doc);
     }
     w.rollback();
Index: lucene/src/test/org/apache/lucene/index/TestIndexReader.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexReader.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestIndexReader.java	(working copy)
@@ -163,7 +163,14 @@
         // set up writer
         IndexWriter writer = new IndexWriter(d, newIndexWriterConfig(
             TEST_VERSION_CURRENT, new MockAnalyzer()));
-        addDocumentWithFields(writer);
+
+        Document doc = new Document();
+        doc.add(new Field("keyword","test1", Field.Store.YES, Field.Index.NOT_ANALYZED));
+        doc.add(new Field("text","test1", Field.Store.YES, Field.Index.ANALYZED));
+        doc.add(new Field("unindexed","test1", Field.Store.YES, Field.Index.NO));
+        doc.add(new Field("unstored","test1", Field.Store.NO, Field.Index.ANALYZED));
+        writer.addDocument(doc);
+
         writer.close();
         // set up reader
         IndexReader reader = IndexReader.open(d, false);
@@ -179,15 +186,31 @@
         // want to get some more segments here
         int mergeFactor = ((LogMergePolicy) writer.getConfig().getMergePolicy()).getMergeFactor();
         for (int i = 0; i < 5*mergeFactor; i++) {
-            addDocumentWithFields(writer);
+          doc = new Document();
+          doc.add(new Field("keyword","test1", Field.Store.YES, Field.Index.NOT_ANALYZED));
+          doc.add(new Field("text","test1", Field.Store.YES, Field.Index.ANALYZED));
+          doc.add(new Field("unindexed","test1", Field.Store.YES, Field.Index.NO));
+          doc.add(new Field("unstored","test1", Field.Store.NO, Field.Index.ANALYZED));
+          writer.addDocument(doc);
         }
         // new fields are in some different segments (we hope)
         for (int i = 0; i < 5*mergeFactor; i++) {
-            addDocumentWithDifferentFields(writer);
+          doc = new Document();
+          doc.add(new Field("keyword2","test1", Field.Store.YES, Field.Index.NOT_ANALYZED));
+          doc.add(new Field("text2","test1", Field.Store.YES, Field.Index.ANALYZED));
+          doc.add(new Field("unindexed2","test1", Field.Store.YES, Field.Index.NO));
+          doc.add(new Field("unstored2","test1", Field.Store.NO, Field.Index.ANALYZED));
+          writer.addDocument(doc);
         }
         // new termvector fields
         for (int i = 0; i < 5*mergeFactor; i++) {
-          addDocumentWithTermVectorFields(writer);
+          doc = new Document();
+          doc.add(new Field("tvnot","tvnot", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));
+          doc.add(new Field("termvector","termvector", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));
+          doc.add(new Field("tvoffset","tvoffset", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));
+          doc.add(new Field("tvposition","tvposition", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));
+          doc.add(newField("tvpositionoffset","tvpositionoffset", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+          writer.addDocument(doc);
         }
         
         writer.close();
@@ -869,8 +892,8 @@
       IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));
       for(int i=0;i<157;i++) {
         Document d = new Document();
-        d.add(new Field("id", Integer.toString(i), Field.Store.YES, Field.Index.NOT_ANALYZED));
-        d.add(new Field("content", "aaa " + i, Field.Store.NO, Field.Index.ANALYZED));
+        d.add(newField("id", Integer.toString(i), Field.Store.YES, Field.Index.NOT_ANALYZED));
+        d.add(newField("content", "aaa " + i, Field.Store.NO, Field.Index.ANALYZED));
         writer.addDocument(d);
         if (0==i%10)
           writer.commit();
@@ -1137,11 +1160,11 @@
       Directory dir = newDirectory();
       RandomIndexWriter w = new RandomIndexWriter(random, dir);
       Document doc = new Document();
-      doc.add(new Field("f", "doctor", Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("f", "doctor", Field.Store.NO, Field.Index.NOT_ANALYZED));
       w.addDocument(doc);
       doc = new Document();
       w.commit();
-      doc.add(new Field("f", "who", Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("f", "who", Field.Store.NO, Field.Index.NOT_ANALYZED));
       w.addDocument(doc);
       IndexReader r = w.getReader();
       IndexReader wr = SlowMultiReaderWrapper.wrap(r);
@@ -1267,31 +1290,31 @@
     private void addDocumentWithFields(IndexWriter writer) throws IOException
     {
         Document doc = new Document();
-        doc.add(new Field("keyword","test1", Field.Store.YES, Field.Index.NOT_ANALYZED));
-        doc.add(new Field("text","test1", Field.Store.YES, Field.Index.ANALYZED));
-        doc.add(new Field("unindexed","test1", Field.Store.YES, Field.Index.NO));
-        doc.add(new Field("unstored","test1", Field.Store.NO, Field.Index.ANALYZED));
+        doc.add(newField("keyword","test1", Field.Store.YES, Field.Index.NOT_ANALYZED));
+        doc.add(newField("text","test1", Field.Store.YES, Field.Index.ANALYZED));
+        doc.add(newField("unindexed","test1", Field.Store.YES, Field.Index.NO));
+        doc.add(newField("unstored","test1", Field.Store.NO, Field.Index.ANALYZED));
         writer.addDocument(doc);
     }
 
     private void addDocumentWithDifferentFields(IndexWriter writer) throws IOException
     {
         Document doc = new Document();
-        doc.add(new Field("keyword2","test1", Field.Store.YES, Field.Index.NOT_ANALYZED));
-        doc.add(new Field("text2","test1", Field.Store.YES, Field.Index.ANALYZED));
-        doc.add(new Field("unindexed2","test1", Field.Store.YES, Field.Index.NO));
-        doc.add(new Field("unstored2","test1", Field.Store.NO, Field.Index.ANALYZED));
+        doc.add(newField("keyword2","test1", Field.Store.YES, Field.Index.NOT_ANALYZED));
+        doc.add(newField("text2","test1", Field.Store.YES, Field.Index.ANALYZED));
+        doc.add(newField("unindexed2","test1", Field.Store.YES, Field.Index.NO));
+        doc.add(newField("unstored2","test1", Field.Store.NO, Field.Index.ANALYZED));
         writer.addDocument(doc);
     }
 
     private void addDocumentWithTermVectorFields(IndexWriter writer) throws IOException
     {
         Document doc = new Document();
-        doc.add(new Field("tvnot","tvnot", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));
-        doc.add(new Field("termvector","termvector", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));
-        doc.add(new Field("tvoffset","tvoffset", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));
-        doc.add(new Field("tvposition","tvposition", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));
-        doc.add(new Field("tvpositionoffset","tvpositionoffset", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+        doc.add(newField("tvnot","tvnot", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));
+        doc.add(newField("termvector","termvector", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));
+        doc.add(newField("tvoffset","tvoffset", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));
+        doc.add(newField("tvposition","tvposition", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));
+        doc.add(newField("tvpositionoffset","tvpositionoffset", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
         
         writer.addDocument(doc);
     }
@@ -1299,7 +1322,7 @@
     private void addDoc(IndexWriter writer, String value) throws IOException
     {
         Document doc = new Document();
-        doc.add(new Field("content", value, Field.Store.NO, Field.Index.ANALYZED));
+        doc.add(newField("content", value, Field.Store.NO, Field.Index.ANALYZED));
         writer.addDocument(doc);
     }
     private void rmDir(File dir) {
@@ -1558,7 +1581,7 @@
 
   private Document createDocument(String id) {
     Document doc = new Document();
-    doc.add(new Field("id", id, Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
+    doc.add(newField("id", id, Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
     return doc;
   }
 
@@ -1608,7 +1631,7 @@
     Directory dir = newDirectory();
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    doc.add(new Field("number", "17", Field.Store.NO, Field.Index.NOT_ANALYZED));
+    doc.add(newField("number", "17", Field.Store.NO, Field.Index.NOT_ANALYZED));
     writer.addDocument(doc);
     writer.close();
 
@@ -1639,7 +1662,7 @@
     Directory dir = newDirectory();
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    doc.add(new Field("number", "17", Field.Store.NO, Field.Index.NOT_ANALYZED));
+    doc.add(newField("number", "17", Field.Store.NO, Field.Index.NOT_ANALYZED));
     ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(10);
     writer.addDocument(doc);
     writer.commit();
@@ -1673,7 +1696,7 @@
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(-1));
     ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(10);
     Document doc = new Document();
-    doc.add(new Field("number", "17", Field.Store.NO, Field.Index.NOT_ANALYZED));
+    doc.add(newField("number", "17", Field.Store.NO, Field.Index.NOT_ANALYZED));
     writer.addDocument(doc);
     writer.commit();
 
@@ -1714,8 +1737,8 @@
     Directory dir = newDirectory();
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setCodecProvider(_TestUtil.alwaysCodec("Standard")));
     Document doc = new Document();
-    doc.add(new Field("field", "a b c d e f g h i j k l m n o p q r s t u v w x y z", Field.Store.NO, Field.Index.ANALYZED));
-    doc.add(new Field("number", "0 1 2 3 4 5 6 7 8 9", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("field", "a b c d e f g h i j k l m n o p q r s t u v w x y z", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("number", "0 1 2 3 4 5 6 7 8 9", Field.Store.NO, Field.Index.ANALYZED));
     writer.addDocument(doc);
     writer.addDocument(doc);
     writer.commit();
@@ -1747,8 +1770,8 @@
     Directory dir = newDirectory();
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setCodecProvider(_TestUtil.alwaysCodec("Standard")));
     Document doc = new Document();
-    doc.add(new Field("field", "a b c d e f g h i j k l m n o p q r s t u v w x y z", Field.Store.NO, Field.Index.ANALYZED));
-    doc.add(new Field("number", "0 1 2 3 4 5 6 7 8 9", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("field", "a b c d e f g h i j k l m n o p q r s t u v w x y z", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("number", "0 1 2 3 4 5 6 7 8 9", Field.Store.NO, Field.Index.ANALYZED));
     writer.addDocument(doc);
     writer.addDocument(doc);
     writer.close();
Index: lucene/src/test/org/apache/lucene/index/TestStressIndexing2.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestStressIndexing2.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestStressIndexing2.java	(working copy)
@@ -530,20 +530,26 @@
         for(int j=0;j<numTerms;j++) {
           int[] pos1 = tpv1.getTermPositions(j);
           int[] pos2 = tpv2.getTermPositions(j);
-          assertEquals(pos1.length, pos2.length);
-          TermVectorOffsetInfo[] offsets1 = tpv1.getOffsets(j);
-          TermVectorOffsetInfo[] offsets2 = tpv2.getOffsets(j);
-          if (offsets1 == null)
-            assertTrue(offsets2 == null);
-          else
-            assertTrue(offsets2 != null);
-          for(int k=0;k<pos1.length;k++) {
-            assertEquals(pos1[k], pos2[k]);
-            if (offsets1 != null) {
-              assertEquals(offsets1[k].getStartOffset(),
-                           offsets2[k].getStartOffset());
-              assertEquals(offsets1[k].getEndOffset(),
-                           offsets2[k].getEndOffset());
+          if (pos1 == null) {
+            assertNull(pos2);
+          } else {
+            assertNotNull(pos1);
+            assertNotNull(pos2);
+            assertEquals(pos1.length, pos2.length);
+            TermVectorOffsetInfo[] offsets1 = tpv1.getOffsets(j);
+            TermVectorOffsetInfo[] offsets2 = tpv2.getOffsets(j);
+            if (offsets1 == null)
+              assertTrue(offsets2 == null);
+            else
+              assertTrue(offsets2 != null);
+            for(int k=0;k<pos1.length;k++) {
+              assertEquals(pos1[k], pos2[k]);
+              if (offsets1 != null) {
+                assertEquals(offsets1[k].getStartOffset(),
+                             offsets2[k].getStartOffset());
+                assertEquals(offsets1[k].getEndOffset(),
+                             offsets2[k].getEndOffset());
+              }
             }
           }
         }
@@ -551,7 +557,7 @@
     }
   }
 
-  private static class IndexingThread extends Thread {
+  private class IndexingThread extends Thread {
     IndexWriter w;
     int base;
     int range;
@@ -639,7 +645,7 @@
 
       ArrayList<Field> fields = new ArrayList<Field>();      
       String idString = getIdString();
-      Field idField =  new Field(idTerm.field(), idString, Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS);
+      Field idField =  newField(idTerm.field(), idString, Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS);
       fields.add(idField);
 
       int nFields = nextInt(maxFields);
@@ -663,16 +669,16 @@
         
         switch (nextInt(4)) {
           case 0:
-            fields.add(new Field("f" + nextInt(100), getString(1), Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS, tvVal));
+            fields.add(newField("f" + nextInt(100), getString(1), Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS, tvVal));
             break;
           case 1:
-            fields.add(new Field("f" + nextInt(100), getString(0), Field.Store.NO, Field.Index.ANALYZED, tvVal));
+            fields.add(newField("f" + nextInt(100), getString(0), Field.Store.NO, Field.Index.ANALYZED, tvVal));
             break;
           case 2:
-            fields.add(new Field("f" + nextInt(100), getString(0), Field.Store.YES, Field.Index.NO, Field.TermVector.NO));
+            fields.add(newField("f" + nextInt(100), getString(0), Field.Store.YES, Field.Index.NO, Field.TermVector.NO));
             break;
           case 3:
-            fields.add(new Field("f" + nextInt(100), getString(bigFieldSize), Field.Store.YES, Field.Index.ANALYZED, tvVal));
+            fields.add(newField("f" + nextInt(100), getString(bigFieldSize), Field.Store.YES, Field.Index.ANALYZED, tvVal));
             break;          
         }
       }
Index: lucene/src/test/org/apache/lucene/index/TestFlex.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestFlex.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestFlex.java	(working copy)
@@ -37,10 +37,10 @@
       if (iter == 0) {
         w.setMaxBufferedDocs(7);
         Document doc = new Document();
-        doc.add(new Field("field1", "this is field1", Field.Store.NO, Field.Index.ANALYZED));
-        doc.add(new Field("field2", "this is field2", Field.Store.NO, Field.Index.ANALYZED));
-        doc.add(new Field("field3", "aaa", Field.Store.NO, Field.Index.ANALYZED));
-        doc.add(new Field("field4", "bbb", Field.Store.NO, Field.Index.ANALYZED));
+        doc.add(newField("field1", "this is field1", Field.Store.NO, Field.Index.ANALYZED));
+        doc.add(newField("field2", "this is field2", Field.Store.NO, Field.Index.ANALYZED));
+        doc.add(newField("field3", "aaa", Field.Store.NO, Field.Index.ANALYZED));
+        doc.add(newField("field4", "bbb", Field.Store.NO, Field.Index.ANALYZED));
         for(int i=0;i<DOC_COUNT;i++) {
           w.addDocument(doc);
         }
@@ -64,7 +64,7 @@
     IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT,
                                                              new MockAnalyzer()).setCodecProvider(_TestUtil.alwaysCodec("Standard")));
     Document doc = new Document();
-    doc.add(new Field("f", "a b c", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("f", "a b c", Field.Store.NO, Field.Index.ANALYZED));
     w.addDocument(doc);
     IndexReader r = w.getReader();
     TermsEnum terms = r.getSequentialSubReaders()[0].fields().terms("f").iterator();
Index: lucene/src/test/org/apache/lucene/index/TestSegmentTermDocs.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestSegmentTermDocs.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestSegmentTermDocs.java	(working copy)
@@ -264,7 +264,7 @@
   private void addDoc(IndexWriter writer, String value) throws IOException
   {
       Document doc = new Document();
-      doc.add(new Field("content", value, Field.Store.NO, Field.Index.ANALYZED));
+      doc.add(newField("content", value, Field.Store.NO, Field.Index.ANALYZED));
       writer.addDocument(doc);
   }
 }
Index: lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexWriter.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestIndexWriter.java	(working copy)
@@ -148,18 +148,18 @@
         dir.close();
     }
 
-    private static void addDoc(IndexWriter writer) throws IOException
+    private void addDoc(IndexWriter writer) throws IOException
     {
         Document doc = new Document();
-        doc.add(new Field("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
+        doc.add(newField("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
         writer.addDocument(doc);
     }
 
     private void addDocWithIndex(IndexWriter writer, int index) throws IOException
     {
         Document doc = new Document();
-        doc.add(new Field("content", "aaa " + index, Field.Store.YES, Field.Index.ANALYZED));
-        doc.add(new Field("id", "" + index, Field.Store.YES, Field.Index.ANALYZED));
+        doc.add(newField("content", "aaa " + index, Field.Store.YES, Field.Index.ANALYZED));
+        doc.add(newField("id", "" + index, Field.Store.YES, Field.Index.ANALYZED));
         writer.addDocument(doc);
     }
 
@@ -555,7 +555,7 @@
       MockDirectoryWrapper dir = newDirectory();
 
       final Document doc = new Document();
-      doc.add(new Field("content", "aaa", Field.Store.YES, Field.Index.ANALYZED));
+      doc.add(newField("content", "aaa", Field.Store.YES, Field.Index.ANALYZED));
 
       for(int numDocs=38;numDocs<500;numDocs += 38) {
         LogDocMergePolicy ldmp = new LogDocMergePolicy();
@@ -596,7 +596,7 @@
       MockDirectoryWrapper dir = newDirectory();
 
       final Document doc = new Document();
-      doc.add(new Field("content", "aaa", Field.Store.YES, Field.Index.ANALYZED));
+      doc.add(newField("content", "aaa", Field.Store.YES, Field.Index.ANALYZED));
 
       LogDocMergePolicy ldmp = new LogDocMergePolicy();
       ldmp.setMinMergeDocs(1);
@@ -1137,12 +1137,12 @@
       IndexWriter writer  = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10));
       for(int j=0;j<100;j++) {
         Document doc = new Document();
-        doc.add(new Field("a"+j, "aaa" + j, Field.Store.YES, Field.Index.ANALYZED));
-        doc.add(new Field("b"+j, "aaa" + j, Field.Store.YES, Field.Index.ANALYZED));
-        doc.add(new Field("c"+j, "aaa" + j, Field.Store.YES, Field.Index.ANALYZED));
-        doc.add(new Field("d"+j, "aaa", Field.Store.YES, Field.Index.ANALYZED));
-        doc.add(new Field("e"+j, "aaa", Field.Store.YES, Field.Index.ANALYZED));
-        doc.add(new Field("f"+j, "aaa", Field.Store.YES, Field.Index.ANALYZED));
+        doc.add(newField("a"+j, "aaa" + j, Field.Store.YES, Field.Index.ANALYZED));
+        doc.add(newField("b"+j, "aaa" + j, Field.Store.YES, Field.Index.ANALYZED));
+        doc.add(newField("c"+j, "aaa" + j, Field.Store.YES, Field.Index.ANALYZED));
+        doc.add(newField("d"+j, "aaa", Field.Store.YES, Field.Index.ANALYZED));
+        doc.add(newField("e"+j, "aaa", Field.Store.YES, Field.Index.ANALYZED));
+        doc.add(newField("f"+j, "aaa", Field.Store.YES, Field.Index.ANALYZED));
         writer.addDocument(doc);
       }
       writer.close();
@@ -1169,7 +1169,7 @@
       int lastNumFile = dir.listAll().length;
       for(int j=0;j<9;j++) {
         Document doc = new Document();
-        doc.add(new Field("field", "aaa" + j, Field.Store.YES, Field.Index.ANALYZED));
+        doc.add(newField("field", "aaa" + j, Field.Store.YES, Field.Index.ANALYZED));
         writer.addDocument(doc);
         int numFile = dir.listAll().length;
         // Verify that with a tiny RAM buffer we see new
@@ -1198,7 +1198,7 @@
       int lastFlushCount = -1;
       for(int j=1;j<52;j++) {
         Document doc = new Document();
-        doc.add(new Field("field", "aaa" + j, Field.Store.YES, Field.Index.ANALYZED));
+        doc.add(newField("field", "aaa" + j, Field.Store.YES, Field.Index.ANALYZED));
         writer.addDocument(doc);
         _TestUtil.syncConcurrentMerges(writer);
         int flushCount = writer.getFlushCount();
@@ -1255,7 +1255,7 @@
 
       for(int j=1;j<52;j++) {
         Document doc = new Document();
-        doc.add(new Field("field", "aaa" + j, Field.Store.YES, Field.Index.ANALYZED));
+        doc.add(newField("field", "aaa" + j, Field.Store.YES, Field.Index.ANALYZED));
         writer.addDocument(doc);
       }
       
@@ -1315,7 +1315,7 @@
         for(int j=0;j<100;j++) {
           Document doc = new Document();
           for(int k=0;k<100;k++) {
-            doc.add(new Field("field", Integer.toString(random.nextInt()), Field.Store.YES, Field.Index.ANALYZED));
+            doc.add(newField("field", Integer.toString(random.nextInt()), Field.Store.YES, Field.Index.ANALYZED));
           }
           writer.addDocument(doc);
         }
@@ -1324,7 +1324,7 @@
         // occurs (heavy on byte blocks)
         for(int j=0;j<100;j++) {
           Document doc = new Document();
-          doc.add(new Field("field", "aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa", Field.Store.YES, Field.Index.ANALYZED));
+          doc.add(newField("field", "aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa", Field.Store.YES, Field.Index.ANALYZED));
           writer.addDocument(doc);
         }
 
@@ -1339,7 +1339,7 @@
           String longTerm = b.toString();
 
           Document doc = new Document();
-          doc.add(new Field("field", longTerm, Field.Store.YES, Field.Index.ANALYZED));
+          doc.add(newField("field", longTerm, Field.Store.YES, Field.Index.ANALYZED));
           writer.addDocument(doc);
         }
       }
@@ -1359,7 +1359,7 @@
       // Enable norms for only 1 doc, pre flush
       for(int j=0;j<10;j++) {
         Document doc = new Document();
-        Field f = new Field("field", "aaa", Field.Store.YES, Field.Index.ANALYZED); 
+        Field f = newField("field", "aaa", Field.Store.YES, Field.Index.ANALYZED); 
         if (j != 8) {
           f.setOmitNorms(true);
         }
@@ -1380,7 +1380,7 @@
       // Enable norms for only 1 doc, post flush
       for(int j=0;j<27;j++) {
         Document doc = new Document();
-        Field f = new Field("field", "aaa", Field.Store.YES, Field.Index.ANALYZED); 
+        Field f = newField("field", "aaa", Field.Store.YES, Field.Index.ANALYZED); 
         if (j != 26) {
           f.setOmitNorms(true);
         }
@@ -1412,7 +1412,7 @@
         b.append(" a a a a a a a a");
       }
       Document doc = new Document();
-      doc.add(new Field("field", b.toString(), Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+      doc.add(newField("field", b.toString(), Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
       writer.addDocument(doc);
       writer.close();
 
@@ -1477,7 +1477,7 @@
         TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(2));
       ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(10);
       Document doc = new Document();
-      doc.add(new Field("field", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+      doc.add(newField("field", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
       for(int i=0;i<19;i++)
         writer.addDocument(doc);
       writer.flush(false, true, true);
@@ -1496,7 +1496,7 @@
       Directory dir = newDirectory();
       IndexWriter writer  = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
       Document doc = new Document();
-      doc.add(new Field("field", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+      doc.add(newField("field", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
       writer.addDocument(doc);
       writer.commit();
       writer.addDocument(new Document());
@@ -1520,7 +1520,7 @@
           .setOpenMode(OpenMode.CREATE).setMaxBufferedDocs(2));
         ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(101);
         Document doc = new Document();
-        doc.add(new Field("field", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+        doc.add(newField("field", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
         for(int i=0;i<200;i++)
           writer.addDocument(doc);
         writer.optimize(false);
@@ -1573,7 +1573,7 @@
         TEST_VERSION_CURRENT, new MockAnalyzer()));
     
     Document document = new Document();
-    document.add(new Field("tvtest", "", Store.NO, Index.ANALYZED, TermVector.YES));
+    document.add(newField("tvtest", "", Store.NO, Index.ANALYZED, TermVector.YES));
     iw.addDocument(document);
     iw.close();
     dir.close();
@@ -1585,17 +1585,17 @@
     IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document document = new Document();
-    document.add(new Field("tvtest", "a b c", Field.Store.NO, Field.Index.ANALYZED,
+    document.add(newField("tvtest", "a b c", Field.Store.NO, Field.Index.ANALYZED,
         Field.TermVector.YES));
     iw.addDocument(document);
     document = new Document();
-    document.add(new Field("tvtest", "x y z", Field.Store.NO, Field.Index.ANALYZED,
+    document.add(newField("tvtest", "x y z", Field.Store.NO, Field.Index.ANALYZED,
                            Field.TermVector.NO));
     iw.addDocument(document);
     // Make first segment
     iw.commit();
 
-    document.add(new Field("tvtest", "a b c", Field.Store.NO, Field.Index.ANALYZED,
+    document.add(newField("tvtest", "a b c", Field.Store.NO, Field.Index.ANALYZED,
         Field.TermVector.YES));
     iw.addDocument(document);
     // Make 2nd segment
@@ -1612,13 +1612,13 @@
     IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document document = new Document();
-    document.add(new Field("tvtest", "a b c", Field.Store.NO, Field.Index.ANALYZED,
+    document.add(newField("tvtest", "a b c", Field.Store.NO, Field.Index.ANALYZED,
         Field.TermVector.YES));
     iw.addDocument(document);
     iw.commit();
 
     document = new Document();
-    document.add(new Field("tvtest", "x y z", Field.Store.NO, Field.Index.ANALYZED,
+    document.add(newField("tvtest", "x y z", Field.Store.NO, Field.Index.ANALYZED,
                            Field.TermVector.NO));
     iw.addDocument(document);
     // Make first segment
@@ -1626,7 +1626,7 @@
 
     iw.optimize();
 
-    document.add(new Field("tvtest", "a b c", Field.Store.NO, Field.Index.ANALYZED,
+    document.add(newField("tvtest", "a b c", Field.Store.NO, Field.Index.ANALYZED,
         Field.TermVector.YES));
     iw.addDocument(document);
     // Make 2nd segment
@@ -1648,7 +1648,7 @@
       ((LogMergePolicy) conf.getMergePolicy()).setMergeFactor(2);
       IndexWriter iw = new IndexWriter(dir, conf);
       Document document = new Document();
-      document.add(new Field("tvtest", "a b c", Field.Store.NO, Field.Index.ANALYZED,
+      document.add(newField("tvtest", "a b c", Field.Store.NO, Field.Index.ANALYZED,
                              Field.TermVector.YES));
       Thread.currentThread().setPriority(Thread.MAX_PRIORITY);
       for(int i=0;i<4;i++)
@@ -1692,7 +1692,7 @@
     lmp.setMergeFactor(2);
     IndexWriter iw = new IndexWriter(dir, conf);
     Document document = new Document();
-    document.add(new Field("tvtest", "a b c", Field.Store.NO, Field.Index.ANALYZED,
+    document.add(newField("tvtest", "a b c", Field.Store.NO, Field.Index.ANALYZED,
                            Field.TermVector.YES));
     for(int i=0;i<177;i++)
       iw.addDocument(document);
@@ -1725,7 +1725,7 @@
 
     Document doc = new Document();
     String contents = "aa bb cc dd ee ff gg hh ii jj kk";
-    doc.add(new Field("content", contents, Field.Store.NO,
+    doc.add(newField("content", contents, Field.Store.NO,
         Field.Index.ANALYZED));
     try {
       writer.addDocument(doc);
@@ -1735,13 +1735,13 @@
 
     // Make sure we can add another normal document
     doc = new Document();
-    doc.add(new Field("content", "aa bb cc dd", Field.Store.NO,
+    doc.add(newField("content", "aa bb cc dd", Field.Store.NO,
         Field.Index.ANALYZED));
     writer.addDocument(doc);
 
     // Make sure we can add another normal document
     doc = new Document();
-    doc.add(new Field("content", "aa bb cc dd", Field.Store.NO,
+    doc.add(newField("content", "aa bb cc dd", Field.Store.NO,
         Field.Index.ANALYZED));
     writer.addDocument(doc);
 
@@ -1813,7 +1813,7 @@
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(2));
     Document doc = new Document();
     String contents = "aa bb cc dd ee ff gg hh ii jj kk";
-    doc.add(new Field("content", contents, Field.Store.NO,
+    doc.add(newField("content", contents, Field.Store.NO,
         Field.Index.ANALYZED));
     boolean hitError = false;
     for(int i=0;i<200;i++) {
@@ -1869,13 +1869,13 @@
       IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));
       //writer.setInfoStream(System.out);
       Document doc = new Document();
-      doc.add(new Field("contents", "here are some contents", Field.Store.YES,
+      doc.add(newField("contents", "here are some contents", Field.Store.YES,
                         Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
       writer.addDocument(doc);
       writer.addDocument(doc);
-      doc.add(new Field("crash", "this should crash after 4 terms", Field.Store.YES,
+      doc.add(newField("crash", "this should crash after 4 terms", Field.Store.YES,
                         Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
-      doc.add(new Field("other", "this will not get indexed", Field.Store.YES,
+      doc.add(newField("other", "this will not get indexed", Field.Store.YES,
                         Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
       try {
         writer.addDocument(doc);
@@ -1885,7 +1885,7 @@
 
       if (0 == i) {
         doc = new Document();
-        doc.add(new Field("contents", "here are some contents", Field.Store.YES,
+        doc.add(newField("contents", "here are some contents", Field.Store.YES,
                           Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
         writer.addDocument(doc);
         writer.addDocument(doc);
@@ -1914,7 +1914,7 @@
       writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT,
           analyzer).setMaxBufferedDocs(10));
       doc = new Document();
-      doc.add(new Field("contents", "here are some contents", Field.Store.YES,
+      doc.add(newField("contents", "here are some contents", Field.Store.YES,
                         Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
       for(int j=0;j<17;j++)
         writer.addDocument(doc);
@@ -1965,13 +1965,13 @@
                 try {
                   for(int iter=0;iter<NUM_ITER;iter++) {
                     Document doc = new Document();
-                    doc.add(new Field("contents", "here are some contents", Field.Store.YES,
+                    doc.add(newField("contents", "here are some contents", Field.Store.YES,
                                       Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
                     writer.addDocument(doc);
                     writer.addDocument(doc);
-                    doc.add(new Field("crash", "this should crash after 4 terms", Field.Store.YES,
+                    doc.add(newField("crash", "this should crash after 4 terms", Field.Store.YES,
                                       Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
-                    doc.add(new Field("other", "this will not get indexed", Field.Store.YES,
+                    doc.add(newField("other", "this will not get indexed", Field.Store.YES,
                                       Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
                     try {
                       writer.addDocument(doc);
@@ -1981,7 +1981,7 @@
 
                     if (0 == finalI) {
                       doc = new Document();
-                      doc.add(new Field("contents", "here are some contents", Field.Store.YES,
+                      doc.add(newField("contents", "here are some contents", Field.Store.YES,
                                         Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
                       writer.addDocument(doc);
                       writer.addDocument(doc);
@@ -2027,7 +2027,7 @@
       IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(
           TEST_VERSION_CURRENT, analyzer).setMaxBufferedDocs(10));
       Document doc = new Document();
-      doc.add(new Field("contents", "here are some contents", Field.Store.YES,
+      doc.add(newField("contents", "here are some contents", Field.Store.YES,
                         Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
       for(int j=0;j<17;j++)
         writer.addDocument(doc);
@@ -2063,21 +2063,21 @@
 
       if (i == 7) {
         // Add empty docs here
-        doc.add(new Field("content3", "", Field.Store.NO,
+        doc.add(newField("content3", "", Field.Store.NO,
                           Field.Index.ANALYZED));
       } else {
         Field.Store storeVal;
         if (i%2 == 0) {
-          doc.add(new Field("content4", contents, Field.Store.YES,
+          doc.add(newField("content4", contents, Field.Store.YES,
                             Field.Index.ANALYZED));
           storeVal = Field.Store.YES;
         } else
           storeVal = Field.Store.NO;
-        doc.add(new Field("content1", contents, storeVal,
+        doc.add(newField("content1", contents, storeVal,
                           Field.Index.ANALYZED));
-        doc.add(new Field("content3", "", Field.Store.YES,
+        doc.add(newField("content3", "", Field.Store.YES,
                           Field.Index.ANALYZED));
-        doc.add(new Field("content5", "", storeVal,
+        doc.add(newField("content5", "", storeVal,
                           Field.Index.ANALYZED));
       }
 
@@ -2105,7 +2105,7 @@
     Directory directory = newDirectory();
 
     final Document doc = new Document();
-    Field idField = new Field("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
+    Field idField = newField("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
     doc.add(idField);
 
     for(int pass=0;pass<2;pass++) {
@@ -2206,7 +2206,7 @@
     public void run() {
 
       final Document doc = new Document();
-      doc.add(new Field("field", "aaa bbb ccc ddd eee fff ggg hhh iii jjj", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+      doc.add(newField("field", "aaa bbb ccc ddd eee fff ggg hhh iii jjj", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
 
       int idUpto = 0;
       int fullCount = 0;
@@ -2322,7 +2322,7 @@
         .setMaxBufferedDocs(2).setMergeScheduler(new ConcurrentMergeScheduler()));
     dir.setMaxSizeInBytes(Math.max(1, dir.getRecomputedActualSizeInBytes()));
     final Document doc = new Document();
-    doc.add(new Field("field", "aaa bbb ccc ddd eee fff ggg hhh iii jjj", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(newField("field", "aaa bbb ccc ddd eee fff ggg hhh iii jjj", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
     try {
       writer.addDocument(doc);
       fail("did not hit disk full");
@@ -2419,7 +2419,7 @@
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer())
       .setMaxBufferedDocs(2).setMergeScheduler(new ConcurrentMergeScheduler()));
     final Document doc = new Document();
-    doc.add(new Field("field", "aaa bbb ccc ddd eee fff ggg hhh iii jjj", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(newField("field", "aaa bbb ccc ddd eee fff ggg hhh iii jjj", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
 
     for(int i=0;i<6;i++)
       writer.addDocument(doc);
@@ -2614,7 +2614,7 @@
     for(int i=0;i<10000;i++)
       b.append(" a");
     b.append(" x");
-    doc.add(new Field("field", b.toString(), Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("field", b.toString(), Field.Store.NO, Field.Index.ANALYZED));
     writer.addDocument(doc);
     writer.close();
 
@@ -2765,7 +2765,7 @@
 
       Document document = new Document();
 
-      Field storedField = new Field("stored", "stored", Field.Store.YES,
+      Field storedField = newField("stored", "stored", Field.Store.YES,
                                     Field.Index.NO);
       document.add(storedField);
       writer.addDocument(document);
@@ -2773,7 +2773,7 @@
 
       document = new Document();
       document.add(storedField);
-      Field termVectorField = new Field("termVector", "termVector",
+      Field termVectorField = newField("termVector", "termVector",
                                         Field.Store.NO, Field.Index.NOT_ANALYZED,
                                         Field.TermVector.WITH_POSITIONS_OFFSETS);
 
@@ -2816,7 +2816,7 @@
 
       Document document = new Document();
 
-      Field storedField = new Field("stored", "stored", Field.Store.YES,
+      Field storedField = newField("stored", "stored", Field.Store.YES,
                                     Field.Index.NO);
       document.add(storedField);
       writer.addDocument(document);
@@ -2824,7 +2824,7 @@
 
       document = new Document();
       document.add(storedField);
-      Field termVectorField = new Field("termVector", "termVector",
+      Field termVectorField = newField("termVector", "termVector",
                                         Field.Store.NO, Field.Index.NOT_ANALYZED,
                                         Field.TermVector.WITH_POSITIONS_OFFSETS);
       document.add(termVectorField);
@@ -2853,10 +2853,10 @@
     Document document = new Document();
 
     document = new Document();
-    Field storedField = new Field("stored", "stored", Field.Store.YES,
+    Field storedField = newField("stored", "stored", Field.Store.YES,
                                   Field.Index.NO);
     document.add(storedField);
-    Field termVectorField = new Field("termVector", "termVector",
+    Field termVectorField = newField("termVector", "termVector",
                                       Field.Store.NO, Field.Index.NOT_ANALYZED,
                                       Field.TermVector.WITH_POSITIONS_OFFSETS);
     document.add(termVectorField);
@@ -2896,7 +2896,7 @@
     for(int i=0;i<10000;i++)
       b.append(" a");
     b.append(" x");
-    doc.add(new Field("field", b.toString(), Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("field", b.toString(), Field.Store.NO, Field.Index.ANALYZED));
     writer.addDocument(doc);
     writer.close();
 
@@ -2919,10 +2919,10 @@
     Document document = new Document();
 
     document = new Document();
-    Field storedField = new Field("stored", "stored", Field.Store.YES,
+    Field storedField = newField("stored", "stored", Field.Store.YES,
                                   Field.Index.NO);
     document.add(storedField);
-    Field termVectorField = new Field("termVector", "termVector",
+    Field termVectorField = newField("termVector", "termVector",
                                       Field.Store.NO, Field.Index.NOT_ANALYZED,
                                       Field.TermVector.WITH_POSITIONS_OFFSETS);
     document.add(termVectorField);
@@ -2963,10 +2963,10 @@
     Document document = new Document();
 
     document = new Document();
-    Field storedField = new Field("stored", "stored", Field.Store.YES,
+    Field storedField = newField("stored", "stored", Field.Store.YES,
                                   Field.Index.NO);
     document.add(storedField);
-    Field termVectorField = new Field("termVector", "termVector",
+    Field termVectorField = newField("termVector", "termVector",
                                       Field.Store.NO, Field.Index.NOT_ANALYZED,
                                       Field.TermVector.WITH_POSITIONS_OFFSETS);
     document.add(termVectorField);
@@ -3008,10 +3008,10 @@
     Document document = new Document();
 
     document = new Document();
-    Field storedField = new Field("stored", "stored", Field.Store.YES,
+    Field storedField = newField("stored", "stored", Field.Store.YES,
                                   Field.Index.NO);
     document.add(storedField);
-    Field termVectorField = new Field("termVector", "termVector",
+    Field termVectorField = newField("termVector", "termVector",
                                       Field.Store.NO, Field.Index.NOT_ANALYZED,
                                       Field.TermVector.WITH_POSITIONS_OFFSETS);
     document.add(termVectorField);
@@ -3044,7 +3044,7 @@
     Directory dir = newDirectory();
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    doc.add(new Field("", "a b c", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("", "a b c", Field.Store.NO, Field.Index.ANALYZED));
     writer.addDocument(doc);
     writer.close();
     dir.close();
@@ -3072,7 +3072,7 @@
     Directory dir = newDirectory();
     MockIndexWriter w = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    doc.add(new Field("field", "a field", Field.Store.YES,
+    doc.add(newField("field", "a field", Field.Store.YES,
                       Field.Index.ANALYZED));
     w.addDocument(doc);
     w.doFail = true;
@@ -3092,7 +3092,7 @@
     Directory dir = newDirectory();
     MockIndexWriter w = new MockIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(2));
     Document doc = new Document();
-    doc.add(new Field("field", "a field", Field.Store.YES,
+    doc.add(newField("field", "a field", Field.Store.YES,
                       Field.Index.ANALYZED));
     w.addDocument(doc);
 
@@ -3104,7 +3104,7 @@
     };
 
     Document crashDoc = new Document();
-    crashDoc.add(new Field("crash", "do it on token 4", Field.Store.YES,
+    crashDoc.add(newField("crash", "do it on token 4", Field.Store.YES,
                            Field.Index.ANALYZED));
     try {
       w.addDocument(crashDoc, analyzer);
@@ -3146,7 +3146,7 @@
     MockIndexWriter2 w = new MockIndexWriter2(dir, conf);
     w.doFail = true;
     Document doc = new Document();
-    doc.add(new Field("field", "a field", Field.Store.YES,
+    doc.add(newField("field", "a field", Field.Store.YES,
                       Field.Index.ANALYZED));
     for(int i=0;i<10;i++)
       try {
@@ -3187,7 +3187,7 @@
     Directory dir = newDirectory();
     MockIndexWriter3 w = new MockIndexWriter3(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    doc.add(new Field("field", "a field", Field.Store.YES,
+    doc.add(newField("field", "a field", Field.Store.YES,
                       Field.Index.ANALYZED));
     w.addDocument(doc);
     w.commit();
@@ -3244,7 +3244,7 @@
     FailOnlyInCommit failure = new FailOnlyInCommit();
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    doc.add(new Field("field", "a field", Field.Store.YES,
+    doc.add(newField("field", "a field", Field.Store.YES,
                       Field.Index.ANALYZED));
     w.addDocument(doc);
     dir.failOn(failure);
@@ -3295,7 +3295,7 @@
 
     final int count = utf8Data.length/2;
     for(int i=0;i<count;i++)
-      doc.add(new Field("f" + i, utf8Data[2*i], Field.Store.YES, Field.Index.ANALYZED));
+      doc.add(newField("f" + i, utf8Data[2*i], Field.Store.YES, Field.Index.ANALYZED));
     w.addDocument(doc);
     w.close();
 
@@ -3652,7 +3652,7 @@
     dir.close();
   }
 
-  private abstract static class RunAddIndexesThreads {
+  private abstract class RunAddIndexesThreads {
 
     Directory dir, dir2;
     final static int NUM_INIT_DOCS = 17;
@@ -4094,10 +4094,10 @@
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( 
         TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    Field f = new Field("field", "abcd", Field.Store.NO, Field.Index.NOT_ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
+    Field f = newField("field", "abcd", Field.Store.NO, Field.Index.NOT_ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
     doc.add(f);
     doc.add(f);
-    Field f2 = new Field("field", "", Field.Store.NO, Field.Index.NOT_ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
+    Field f2 = newField("field", "", Field.Store.NO, Field.Index.NOT_ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
     doc.add(f2);
     doc.add(f);
     w.addDocument(doc);
@@ -4129,7 +4129,7 @@
     Directory dir = newDirectory();
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    Field f = new Field("field", "abcd", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
+    Field f = newField("field", "abcd", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
     doc.add(f);
     doc.add(f);
     w.addDocument(doc);
@@ -4151,7 +4151,7 @@
     Directory dir = newDirectory();
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    Field f = new Field("field", "abcd   ", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
+    Field f = newField("field", "abcd   ", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
     doc.add(f);
     doc.add(f);
     w.addDocument(doc);
@@ -4198,7 +4198,7 @@
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( 
         TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true)));
     Document doc = new Document();
-    Field f = new Field("field", "abcd the", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
+    Field f = newField("field", "abcd the", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
     doc.add(f);
     doc.add(f);
     w.addDocument(doc);
@@ -4221,9 +4221,9 @@
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( 
         TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    Field f = new Field("field", "abcd the  ", Field.Store.NO,
+    Field f = newField("field", "abcd the  ", Field.Store.NO,
         Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
-    Field f2 = new Field("field", "crunch man", Field.Store.NO,
+    Field f2 = newField("field", "crunch man", Field.Store.NO,
         Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
     doc.add(f);
     doc.add(f2);
@@ -4252,9 +4252,9 @@
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( 
         TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    Field f = new Field("field", "", Field.Store.NO,
+    Field f = newField("field", "", Field.Store.NO,
                         Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
-    Field f2 = new Field("field", "crunch man", Field.Store.NO,
+    Field f2 = newField("field", "crunch man", Field.Store.NO,
         Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
     doc.add(f);
     doc.add(f2);
@@ -4281,12 +4281,12 @@
         TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
 
-    Field f = new Field("field", "abcd", Field.Store.NO,
+    Field f = newField("field", "abcd", Field.Store.NO,
                         Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
     doc.add(f);
-    doc.add(new Field("field", "", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(newField("field", "", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
 
-    Field f2 = new Field("field", "crunch", Field.Store.NO,
+    Field f2 = newField("field", "crunch", Field.Store.NO,
         Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
     doc.add(f2);
 
@@ -4333,7 +4333,7 @@
     Directory dir = newDirectory();
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(2));
     Document doc = new Document();
-    doc.add(new Field("content", "aaa bbb ccc ddd eee fff ggg hhh iii", Field.Store.YES,
+    doc.add(newField("content", "aaa bbb ccc ddd eee fff ggg hhh iii", Field.Store.YES,
                       Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
     writer.addDocument(doc);
     writer.addDocument(doc);
@@ -4386,7 +4386,7 @@
             w = new IndexWriter(dir, conf);
 
             Document doc = new Document();
-            doc.add(new Field("field", "some text contents", Field.Store.YES, Field.Index.ANALYZED));
+            doc.add(newField("field", "some text contents", Field.Store.YES, Field.Index.ANALYZED));
             for(int i=0;i<100;i++) {
               w.addDocument(doc);
               if (i%10 == 0) {
@@ -4495,7 +4495,7 @@
     Document doc = new Document();
     Field f = new Field("binary", b, 10, 17);
     f.setTokenStream(new MockTokenizer(new StringReader("doc1field1"), MockTokenizer.WHITESPACE, false));
-    Field f2 = new Field("string", "value", Field.Store.YES,Field.Index.ANALYZED);
+    Field f2 = newField("string", "value", Field.Store.YES,Field.Index.ANALYZED);
     f2.setTokenStream(new MockTokenizer(new StringReader("doc1field2"), MockTokenizer.WHITESPACE, false));
     doc.add(f);
     doc.add(f2);
@@ -4552,9 +4552,9 @@
     Directory d = newDirectory();
     IndexWriter w = new IndexWriter(d, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    doc.add(new Field("zzz", "a b c", Field.Store.YES, Field.Index.NO));
-    doc.add(new Field("aaa", "a b c", Field.Store.YES, Field.Index.NO));
-    doc.add(new Field("zzz", "1 2 3", Field.Store.YES, Field.Index.NO));
+    doc.add(newField("zzz", "a b c", Field.Store.YES, Field.Index.NO));
+    doc.add(newField("aaa", "a b c", Field.Store.YES, Field.Index.NO));
+    doc.add(newField("zzz", "1 2 3", Field.Store.YES, Field.Index.NO));
     w.addDocument(doc);
     IndexReader r = w.getReader();
     doc = r.document(0);
@@ -4584,10 +4584,10 @@
     Directory d = newDirectory();
     IndexWriter w = new IndexWriter(d, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    doc.add(new Field("field", "a a\uffffb", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("field", "a a\uffffb", Field.Store.NO, Field.Index.ANALYZED));
     w.addDocument(doc);
     doc = new Document();
-    doc.add(new Field("field", "a", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("field", "a", Field.Store.NO, Field.Index.ANALYZED));
     w.addDocument(doc);
     IndexReader r = w.getReader();
     assertEquals(1, r.docFreq(new Term("field", "a\uffffb")));
@@ -4634,7 +4634,7 @@
             try {
               final Document doc = new Document();
               IndexReader r = IndexReader.open(dir);
-              Field f = new Field("f", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
+              Field f = newField("f", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
               doc.add(f);
               int count = 0;
               do {
@@ -4731,7 +4731,7 @@
     RandomIndexWriter writer = new RandomIndexWriter(rnd, dir);
     Document d = new Document();
     // Single segment
-    Field f = new Field("f", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
+    Field f = newField("f", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
     d.add(f);
     char[] chars = new char[2];
     final Set<String> allTerms = new HashSet<String>();
@@ -4799,7 +4799,7 @@
       s.append(' ').append(""+i);
     }
     Document d = new Document();
-    Field f = new Field("field", s.toString(), Field.Store.NO, Field.Index.ANALYZED);
+    Field f = newField("field", s.toString(), Field.Store.NO, Field.Index.ANALYZED);
     d.add(f);
     w.addDocument(d);
     IndexReader r = w.getReader(2).getSequentialSubReaders()[0];
@@ -4824,7 +4824,7 @@
       IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
       ((LogMergePolicy) w.getMergePolicy()).setUseCompoundFile(true);
       Document doc = new Document();
-      doc.add(new Field("field", "go", Field.Store.NO, Field.Index.ANALYZED));
+      doc.add(newField("field", "go", Field.Store.NO, Field.Index.ANALYZED));
       w.addDocument(doc);
       IndexReader r;
       if (iter == 0) {
@@ -4887,7 +4887,7 @@
     
     // First commit
     Document doc = new Document();
-    doc.add(new Field("c", "val", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(newField("c", "val", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));
     writer.addDocument(doc);
     writer.commit();
     assertEquals(1, IndexReader.listCommits(dir).size());
@@ -4897,7 +4897,7 @@
     
     // Second commit - now KeepOnlyLastCommit cannot delete the prev commit.
     doc = new Document();
-    doc.add(new Field("c", "val", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(newField("c", "val", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));
     writer.addDocument(doc);
     writer.commit();
     assertEquals(2, IndexReader.listCommits(dir).size());
@@ -4928,7 +4928,7 @@
     FlushCountingIndexWriter w = new FlushCountingIndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setRAMBufferSizeMB(0.5).setMaxBufferedDocs(-1).setMaxBufferedDeleteTerms(-1));
     //w.setInfoStream(System.out);
     Document doc = new Document();
-    doc.add(new Field("field", "go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("field", "go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20", Field.Store.NO, Field.Index.ANALYZED));
     int num = 6 * RANDOM_MULTIPLIER;
     for (int iter = 0; iter < num; iter++) {
       int count = 0;
@@ -5004,13 +5004,13 @@
 
     Document doc = new Document();
     // create as many files as possible
-    doc.add(new Field("c", "val", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(newField("c", "val", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));
     writer.addDocument(doc);
     // Adding just one document does not call flush yet.
     assertEquals("only the stored and term vector files should exist in the directory", 5 + extraFileCount, dir.listAll().length);
     
     doc = new Document();
-    doc.add(new Field("c", "val", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(newField("c", "val", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));
     writer.addDocument(doc);
     // The second document should cause a flush.
     assertTrue("flush should have occurred and files created", dir.listAll().length > 5 + extraFileCount);
@@ -5034,7 +5034,7 @@
           TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(2));
 
       Document doc = new Document();
-      doc.add(new Field("c", "val", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));
+      doc.add(newField("c", "val", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));
       w.addDocument(doc);
       w.addDocument(doc);
       IndexWriter w2 = new IndexWriter(dir, newIndexWriterConfig( 
@@ -5140,7 +5140,7 @@
       
     final List<Integer> fieldIDs = new ArrayList<Integer>();
 
-    Field idField = new Field("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
+    Field idField = newField("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
 
     for(int i=0;i<fieldCount;i++) {
       fieldIDs.add(i);
@@ -5163,7 +5163,7 @@
         final String s;
         if (rand.nextInt(4) != 3) {
           s = _TestUtil.randomUnicodeString(rand, 1000);
-          doc.add(new Field("f"+field, s, Field.Store.YES, Field.Index.NO));
+          doc.add(newField("f"+field, s, Field.Store.YES, Field.Index.NO));
         } else {
           s = null;
         }
@@ -5248,7 +5248,7 @@
     ((LogMergePolicy) w.getMergePolicy()).setMergeFactor(2);
 
     Document doc = new Document();
-    doc.add(new Field("f", "doctor who", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("f", "doctor who", Field.Store.YES, Field.Index.ANALYZED));
     w.addDocument(doc);
 
     w.commit();
Index: lucene/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java	(working copy)
@@ -66,7 +66,7 @@
     Term term = new Term("test", "a");
     for (int i = 0; i < 5000; i++) {
       Document d1 = new Document();
-      d1.add(new Field(term.field(), term.text(), Store.NO, Index.ANALYZED));
+      d1.add(newField(term.field(), term.text(), Store.NO, Index.ANALYZED));
       writer.addDocument(d1);
     }
     writer.commit();
Index: lucene/src/test/org/apache/lucene/index/TestCheckIndex.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestCheckIndex.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestCheckIndex.java	(working copy)
@@ -36,7 +36,7 @@
     Directory dir = newDirectory();
     IndexWriter writer  = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(2));
     Document doc = new Document();
-    doc.add(new Field("field", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(newField("field", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
     for(int i=0;i<19;i++) {
       writer.addDocument(doc);
     }
Index: lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java	(working copy)
@@ -47,17 +47,17 @@
 
       final Document doc = new Document();
 
-      doc.add(new Field("content1", "aaa bbb ccc ddd", Field.Store.YES, Field.Index.ANALYZED));
-      doc.add(new Field("content6", "aaa bbb ccc ddd", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
-      doc.add(new Field("content2", "aaa bbb ccc ddd", Field.Store.YES, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("content3", "aaa bbb ccc ddd", Field.Store.YES, Field.Index.NO));
+      doc.add(newField("content1", "aaa bbb ccc ddd", Field.Store.YES, Field.Index.ANALYZED));
+      doc.add(newField("content6", "aaa bbb ccc ddd", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+      doc.add(newField("content2", "aaa bbb ccc ddd", Field.Store.YES, Field.Index.NOT_ANALYZED));
+      doc.add(newField("content3", "aaa bbb ccc ddd", Field.Store.YES, Field.Index.NO));
 
-      doc.add(new Field("content4", "aaa bbb ccc ddd", Field.Store.NO, Field.Index.ANALYZED));
-      doc.add(new Field("content5", "aaa bbb ccc ddd", Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("content4", "aaa bbb ccc ddd", Field.Store.NO, Field.Index.ANALYZED));
+      doc.add(newField("content5", "aaa bbb ccc ddd", Field.Store.NO, Field.Index.NOT_ANALYZED));
 
-      doc.add(new Field("content7", "aaa bbb ccc ddd", Field.Store.NO, Field.Index.NOT_ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+      doc.add(newField("content7", "aaa bbb ccc ddd", Field.Store.NO, Field.Index.NOT_ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
 
-      final Field idField = new Field("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
+      final Field idField = newField("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
       doc.add(idField);
 
       final long stopTime = System.currentTimeMillis() + 500;
Index: lucene/src/test/org/apache/lucene/index/TestTransactions.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestTransactions.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestTransactions.java	(working copy)
@@ -140,8 +140,8 @@
       for(int j=0; j<10; j++) {
         Document d = new Document();
         int n = random.nextInt();
-        d.add(new Field("id", Integer.toString(nextID++), Field.Store.YES, Field.Index.NOT_ANALYZED));
-        d.add(new Field("contents", English.intToEnglish(n), Field.Store.NO, Field.Index.ANALYZED));
+        d.add(newField("id", Integer.toString(nextID++), Field.Store.YES, Field.Index.NOT_ANALYZED));
+        d.add(newField("contents", English.intToEnglish(n), Field.Store.NO, Field.Index.ANALYZED));
         writer.addDocument(d);
       }
 
@@ -185,7 +185,7 @@
     for(int j=0; j<7; j++) {
       Document d = new Document();
       int n = random.nextInt();
-      d.add(new Field("contents", English.intToEnglish(n), Field.Store.NO, Field.Index.ANALYZED));
+      d.add(newField("contents", English.intToEnglish(n), Field.Store.NO, Field.Index.ANALYZED));
       writer.addDocument(d);
     }
     writer.close();
Index: lucene/src/test/org/apache/lucene/index/TestDocumentWriter.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestDocumentWriter.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestDocumentWriter.java	(working copy)
@@ -127,8 +127,8 @@
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
 
     Document doc = new Document();
-    doc.add(new Field("repeated", "repeated one", Field.Store.YES, Field.Index.ANALYZED));
-    doc.add(new Field("repeated", "repeated two", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("repeated", "repeated one", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("repeated", "repeated two", Field.Store.YES, Field.Index.ANALYZED));
 
     writer.addDocument(doc);
     writer.commit();
@@ -192,7 +192,7 @@
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
 
     Document doc = new Document();
-    doc.add(new Field("f1", "a 5 a a", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("f1", "a 5 a a", Field.Store.YES, Field.Index.ANALYZED));
 
     writer.addDocument(doc);
     writer.commit();
@@ -269,11 +269,11 @@
   public void testMixedTermVectorSettingsSameField() throws Exception {
     Document doc = new Document();
     // f1 first without tv then with tv
-    doc.add(new Field("f1", "v1", Store.YES, Index.NOT_ANALYZED, TermVector.NO));
-    doc.add(new Field("f1", "v2", Store.YES, Index.NOT_ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(newField("f1", "v1", Store.YES, Index.NOT_ANALYZED, TermVector.NO));
+    doc.add(newField("f1", "v2", Store.YES, Index.NOT_ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));
     // f2 first with tv then without tv
-    doc.add(new Field("f2", "v1", Store.YES, Index.NOT_ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));
-    doc.add(new Field("f2", "v2", Store.YES, Index.NOT_ANALYZED, TermVector.NO));
+    doc.add(newField("f2", "v1", Store.YES, Index.NOT_ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(newField("f2", "v2", Store.YES, Index.NOT_ANALYZED, TermVector.NO));
 
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer()));
@@ -302,13 +302,13 @@
   public void testLUCENE_1590() throws Exception {
     Document doc = new Document();
     // f1 has no norms
-    doc.add(new Field("f1", "v1", Store.NO, Index.ANALYZED_NO_NORMS));
-    doc.add(new Field("f1", "v2", Store.YES, Index.NO));
+    doc.add(newField("f1", "v1", Store.NO, Index.ANALYZED_NO_NORMS));
+    doc.add(newField("f1", "v2", Store.YES, Index.NO));
     // f2 has no TF
-    Field f = new Field("f2", "v1", Store.NO, Index.ANALYZED);
+    Field f = newField("f2", "v1", Store.NO, Index.ANALYZED);
     f.setOmitTermFreqAndPositions(true);
     doc.add(f);
-    doc.add(new Field("f2", "v2", Store.YES, Index.NO));
+    doc.add(newField("f2", "v2", Store.YES, Index.NO));
 
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer()));
Index: lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter.java	(working copy)
@@ -219,8 +219,8 @@
   private void addDoc(IndexWriter writer, int id) throws IOException
   {
     Document doc = new Document();
-    doc.add(new Field("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
-    doc.add(new Field("id", Integer.toString(id), Field.Store.YES, Field.Index.NOT_ANALYZED));
+    doc.add(newField("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("id", Integer.toString(id), Field.Store.YES, Field.Index.NOT_ANALYZED));
     writer.addDocument(doc);
   }
 }
Index: lucene/src/test/org/apache/lucene/index/TestMultiFields.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestMultiFields.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestMultiFields.java	(working copy)
@@ -39,9 +39,9 @@
 
       int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);
       Document doc = new Document();
-      Field f = new Field("field", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
+      Field f = newField("field", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
       doc.add(f);
-      Field id = new Field("id", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
+      Field id = newField("id", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
       doc.add(id);
 
       boolean onlyUniqueTerms = random.nextBoolean();
@@ -132,7 +132,7 @@
     Directory dir = newDirectory();
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document d = new Document();
-    d.add(new Field("f", "j", Field.Store.NO, Field.Index.NOT_ANALYZED));
+    d.add(newField("f", "j", Field.Store.NO, Field.Index.NOT_ANALYZED));
     w.addDocument(d);
     w.commit();
     w.addDocument(d);
Index: lucene/src/test/org/apache/lucene/index/TestIndexReaderCloneNorms.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexReaderCloneNorms.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestIndexReaderCloneNorms.java	(working copy)
@@ -305,7 +305,7 @@
     Document d = new Document();
     float boost = nextNorm();
     for (int i = 0; i < 10; i++) {
-      Field f = new Field("f" + i, "v" + i, Store.NO, Index.NOT_ANALYZED);
+      Field f = newField("f" + i, "v" + i, Store.NO, Index.NOT_ANALYZED);
       f.setBoost(boost);
       d.add(f);
     }
Index: lucene/src/test/org/apache/lucene/index/TestCrash.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestCrash.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestCrash.java	(working copy)
@@ -44,8 +44,8 @@
     }
     
     Document doc = new Document();
-    doc.add(new Field("content", "aaa", Field.Store.YES, Field.Index.ANALYZED));
-    doc.add(new Field("id", "0", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("content", "aaa", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("id", "0", Field.Store.YES, Field.Index.ANALYZED));
     for(int i=0;i<157;i++)
       writer.addDocument(doc);
 
Index: lucene/src/test/org/apache/lucene/index/TestCodecs.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestCodecs.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestCodecs.java	(working copy)
@@ -355,7 +355,7 @@
       pq.add(new Term("content", "ccc"));
 
       final Document doc = new Document();
-      doc.add(new Field("content", "aaa bbb ccc ddd", Store.NO, Field.Index.ANALYZED_NO_NORMS));
+      doc.add(newField("content", "aaa bbb ccc ddd", Store.NO, Field.Index.ANALYZED_NO_NORMS));
 
       // add document and force commit for creating a first segment
       writer.addDocument(doc);
Index: lucene/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java	(working copy)
@@ -65,7 +65,7 @@
 
     IndexWriter writer = new IndexWriter(directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(2));
     Document doc = new Document();
-    Field idField = new Field("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
+    Field idField = newField("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
     doc.add(idField);
     int extraCount = 0;
 
@@ -116,7 +116,7 @@
         .setMergePolicy(mp));
 
     Document doc = new Document();
-    Field idField = new Field("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
+    Field idField = newField("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
     doc.add(idField);
     for(int i=0;i<10;i++) {
       for(int j=0;j<100;j++) {
@@ -151,7 +151,7 @@
 
       for(int j=0;j<21;j++) {
         Document doc = new Document();
-        doc.add(new Field("content", "a b c", Field.Store.NO, Field.Index.ANALYZED));
+        doc.add(newField("content", "a b c", Field.Store.NO, Field.Index.ANALYZED));
         writer.addDocument(doc);
       }
         
@@ -172,7 +172,7 @@
   public void testNoWaitClose() throws IOException {
     MockDirectoryWrapper directory = newDirectory();
     Document doc = new Document();
-    Field idField = new Field("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
+    Field idField = newField("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
     doc.add(idField);
 
     IndexWriter writer = new IndexWriter(directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(2));
Index: lucene/src/test/org/apache/lucene/index/TestThreadedOptimize.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestThreadedOptimize.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestThreadedOptimize.java	(working copy)
@@ -66,8 +66,8 @@
 
       for(int i=0;i<200;i++) {
         Document d = new Document();
-        d.add(new Field("id", Integer.toString(i), Field.Store.YES, Field.Index.NOT_ANALYZED));
-        d.add(new Field("contents", English.intToEnglish(i), Field.Store.NO, Field.Index.ANALYZED));
+        d.add(newField("id", Integer.toString(i), Field.Store.YES, Field.Index.NOT_ANALYZED));
+        d.add(newField("contents", English.intToEnglish(i), Field.Store.NO, Field.Index.ANALYZED));
         writer.addDocument(d);
       }
 
@@ -87,8 +87,8 @@
                 writerFinal.optimize(false);
                 for(int k=0;k<17*(1+iFinal);k++) {
                   Document d = new Document();
-                  d.add(new Field("id", iterFinal + "_" + iFinal + "_" + j + "_" + k, Field.Store.YES, Field.Index.NOT_ANALYZED));
-                  d.add(new Field("contents", English.intToEnglish(iFinal+k), Field.Store.NO, Field.Index.ANALYZED));
+                  d.add(newField("id", iterFinal + "_" + iFinal + "_" + j + "_" + k, Field.Store.YES, Field.Index.NOT_ANALYZED));
+                  d.add(newField("contents", English.intToEnglish(iFinal+k), Field.Store.NO, Field.Index.ANALYZED));
                   writerFinal.addDocument(d);
                 }
                 for(int k=0;k<9*(1+iFinal);k++)
Index: lucene/src/test/org/apache/lucene/index/TestOmitTf.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestOmitTf.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestOmitTf.java	(working copy)
@@ -64,11 +64,11 @@
     Document d = new Document();
         
     // this field will have Tf
-    Field f1 = new Field("f1", "This field has term freqs", Field.Store.NO, Field.Index.ANALYZED);
+    Field f1 = newField("f1", "This field has term freqs", Field.Store.NO, Field.Index.ANALYZED);
     d.add(f1);
        
     // this field will NOT have Tf
-    Field f2 = new Field("f2", "This field has NO Tf in all docs", Field.Store.NO, Field.Index.ANALYZED);
+    Field f2 = newField("f2", "This field has NO Tf in all docs", Field.Store.NO, Field.Index.ANALYZED);
     f2.setOmitTermFreqAndPositions(true);
     d.add(f2);
         
@@ -113,11 +113,11 @@
     Document d = new Document();
         
     // this field will have Tf
-    Field f1 = new Field("f1", "This field has term freqs", Field.Store.NO, Field.Index.ANALYZED);
+    Field f1 = newField("f1", "This field has term freqs", Field.Store.NO, Field.Index.ANALYZED);
     d.add(f1);
        
     // this field will NOT have Tf
-    Field f2 = new Field("f2", "This field has NO Tf in all docs", Field.Store.NO, Field.Index.ANALYZED);
+    Field f2 = newField("f2", "This field has NO Tf in all docs", Field.Store.NO, Field.Index.ANALYZED);
     f2.setOmitTermFreqAndPositions(true);
     d.add(f2);
 
@@ -166,11 +166,11 @@
     Document d = new Document();
         
     // this field will have Tf
-    Field f1 = new Field("f1", "This field has term freqs", Field.Store.NO, Field.Index.ANALYZED);
+    Field f1 = newField("f1", "This field has term freqs", Field.Store.NO, Field.Index.ANALYZED);
     d.add(f1);
        
     // this field will NOT have Tf
-    Field f2 = new Field("f2", "This field has NO Tf in all docs", Field.Store.NO, Field.Index.ANALYZED);
+    Field f2 = newField("f2", "This field has NO Tf in all docs", Field.Store.NO, Field.Index.ANALYZED);
     d.add(f2);
 
     for(int i=0;i<5;i++)
@@ -216,7 +216,7 @@
     lmp.setUseCompoundDocStore(false);
     Document d = new Document();
         
-    Field f1 = new Field("f1", "This field has term freqs", Field.Store.NO, Field.Index.ANALYZED);
+    Field f1 = newField("f1", "This field has term freqs", Field.Store.NO, Field.Index.ANALYZED);
     f1.setOmitTermFreqAndPositions(true);
     d.add(f1);
 
@@ -252,11 +252,11 @@
       Document d = new Document();
       sb.append(term).append(" ");
       String content  = sb.toString();
-      Field noTf = new Field("noTf", content + (i%2==0 ? "" : " notf"), Field.Store.NO, Field.Index.ANALYZED);
+      Field noTf = newField("noTf", content + (i%2==0 ? "" : " notf"), Field.Store.NO, Field.Index.ANALYZED);
       noTf.setOmitTermFreqAndPositions(true);
       d.add(noTf);
           
-      Field tf = new Field("tf", content + (i%2==0 ? " tf" : ""), Field.Store.NO, Field.Index.ANALYZED);
+      Field tf = newField("tf", content + (i%2==0 ? " tf" : ""), Field.Store.NO, Field.Index.ANALYZED);
       d.add(tf);
           
       writer.addDocument(d);
Index: lucene/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java	(working copy)
@@ -214,7 +214,7 @@
 
   private void addDoc(IndexWriter writer) throws IOException {
     Document doc = new Document();
-    doc.add(new Field("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
     writer.addDocument(doc);
   }
 
Index: lucene/src/test/org/apache/lucene/index/TestAtomicUpdate.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestAtomicUpdate.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestAtomicUpdate.java	(working copy)
@@ -135,8 +135,8 @@
     // Establish a base index of 100 docs:
     for(int i=0;i<100;i++) {
       Document d = new Document();
-      d.add(new Field("id", Integer.toString(i), Field.Store.YES, Field.Index.NOT_ANALYZED));
-      d.add(new Field("contents", English.intToEnglish(i), Field.Store.NO, Field.Index.ANALYZED));
+      d.add(newField("id", Integer.toString(i), Field.Store.YES, Field.Index.NOT_ANALYZED));
+      d.add(newField("contents", English.intToEnglish(i), Field.Store.NO, Field.Index.ANALYZED));
       if ((i-1)%7 == 0) {
         writer.commit();
       }
Index: lucene/src/test/org/apache/lucene/index/TestNoDeletionPolicy.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestNoDeletionPolicy.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestNoDeletionPolicy.java	(working copy)
@@ -79,7 +79,7 @@
         .setIndexDeletionPolicy(NoDeletionPolicy.INSTANCE));
     for (int i = 0; i < 10; i++) {
       Document doc = new Document();
-      doc.add(new Field("c", "a" + i, Store.YES, Index.ANALYZED));
+      doc.add(newField("c", "a" + i, Store.YES, Index.ANALYZED));
       writer.addDocument(doc);
       writer.commit();
       assertEquals("wrong number of commits !", i + 1, IndexReader.listCommits(dir).size());
Index: lucene/src/test/org/apache/lucene/index/TestDirectoryReader.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestDirectoryReader.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestDirectoryReader.java	(working copy)
@@ -206,7 +206,7 @@
         new MockAnalyzer()).setOpenMode(
         create ? OpenMode.CREATE : OpenMode.APPEND));
     Document doc = new Document();
-    doc.add(new Field("body", s, Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("body", s, Field.Store.YES, Field.Index.ANALYZED));
     iw.addDocument(doc);
     iw.close();
   }
Index: lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete.java	(working copy)
@@ -48,14 +48,14 @@
 
     for (int i = 0; i < keywords.length; i++) {
       Document doc = new Document();
-      doc.add(new Field("id", keywords[i], Field.Store.YES,
+      doc.add(newField("id", keywords[i], Field.Store.YES,
                         Field.Index.NOT_ANALYZED));
-      doc.add(new Field("country", unindexed[i], Field.Store.YES,
+      doc.add(newField("country", unindexed[i], Field.Store.YES,
                         Field.Index.NO));
-      doc.add(new Field("contents", unstored[i], Field.Store.NO,
+      doc.add(newField("contents", unstored[i], Field.Store.NO,
                         Field.Index.ANALYZED));
       doc
-        .add(new Field("city", text[i], Field.Store.YES,
+        .add(newField("city", text[i], Field.Store.YES,
                        Field.Index.ANALYZED));
       modifier.addDocument(doc);
     }
@@ -370,10 +370,10 @@
   private void updateDoc(IndexWriter modifier, int id, int value)
       throws IOException {
     Document doc = new Document();
-    doc.add(new Field("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
-    doc.add(new Field("id", String.valueOf(id), Field.Store.YES,
+    doc.add(newField("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("id", String.valueOf(id), Field.Store.YES,
         Field.Index.NOT_ANALYZED));
-    doc.add(new Field("value", String.valueOf(value), Field.Store.NO,
+    doc.add(newField("value", String.valueOf(value), Field.Store.NO,
         Field.Index.NOT_ANALYZED));
     modifier.updateDocument(new Term("id", String.valueOf(id)), doc);
   }
@@ -382,10 +382,10 @@
   private void addDoc(IndexWriter modifier, int id, int value)
       throws IOException {
     Document doc = new Document();
-    doc.add(new Field("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
-    doc.add(new Field("id", String.valueOf(id), Field.Store.YES,
+    doc.add(newField("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("id", String.valueOf(id), Field.Store.YES,
         Field.Index.NOT_ANALYZED));
-    doc.add(new Field("value", String.valueOf(value), Field.Store.NO,
+    doc.add(newField("value", String.valueOf(value), Field.Store.NO,
         Field.Index.NOT_ANALYZED));
     modifier.addDocument(doc);
   }
@@ -422,9 +422,9 @@
     IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));
     for (int i = 0; i < 157; i++) {
       Document d = new Document();
-      d.add(new Field("id", Integer.toString(i), Field.Store.YES,
+      d.add(newField("id", Integer.toString(i), Field.Store.YES,
                       Field.Index.NOT_ANALYZED));
-      d.add(new Field("content", "aaa " + i, Field.Store.NO,
+      d.add(newField("content", "aaa " + i, Field.Store.NO,
                       Field.Index.ANALYZED));
       writer.addDocument(d);
     }
@@ -496,9 +496,9 @@
             for (int i = 0; i < 13; i++) {
               if (updates) {
                 Document d = new Document();
-                d.add(new Field("id", Integer.toString(i), Field.Store.YES,
+                d.add(newField("id", Integer.toString(i), Field.Store.YES,
                                 Field.Index.NOT_ANALYZED));
-                d.add(new Field("content", "bbb " + i, Field.Store.NO,
+                d.add(newField("content", "bbb " + i, Field.Store.NO,
                                 Field.Index.ANALYZED));
                 modifier.updateDocument(new Term("id", Integer.toString(docId)), d);
               } else { // deletes
@@ -666,13 +666,13 @@
 
     for (int i = 0; i < keywords.length; i++) {
       Document doc = new Document();
-      doc.add(new Field("id", keywords[i], Field.Store.YES,
+      doc.add(newField("id", keywords[i], Field.Store.YES,
                         Field.Index.NOT_ANALYZED));
-      doc.add(new Field("country", unindexed[i], Field.Store.YES,
+      doc.add(newField("country", unindexed[i], Field.Store.YES,
                         Field.Index.NO));
-      doc.add(new Field("contents", unstored[i], Field.Store.NO,
+      doc.add(newField("contents", unstored[i], Field.Store.NO,
                         Field.Index.ANALYZED));
-      doc.add(new Field("city", text[i], Field.Store.YES,
+      doc.add(newField("city", text[i], Field.Store.YES,
                         Field.Index.ANALYZED));
       modifier.addDocument(doc);
     }
@@ -772,13 +772,13 @@
 
     for (int i = 0; i < keywords.length; i++) {
       Document doc = new Document();
-      doc.add(new Field("id", keywords[i], Field.Store.YES,
+      doc.add(newField("id", keywords[i], Field.Store.YES,
                         Field.Index.NOT_ANALYZED));
-      doc.add(new Field("country", unindexed[i], Field.Store.YES,
+      doc.add(newField("country", unindexed[i], Field.Store.YES,
                         Field.Index.NO));
-      doc.add(new Field("contents", unstored[i], Field.Store.NO,
+      doc.add(newField("contents", unstored[i], Field.Store.NO,
                         Field.Index.ANALYZED));
-      doc.add(new Field("city", text[i], Field.Store.YES,
+      doc.add(newField("city", text[i], Field.Store.YES,
                         Field.Index.ANALYZED));
       try {
         modifier.addDocument(doc);
Index: lucene/src/test/org/apache/lucene/index/TestIndexWriterReader.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexWriterReader.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestIndexWriterReader.java	(working copy)
@@ -81,7 +81,7 @@
     
     Document newDoc = r1.document(10);
     newDoc.removeField("id");
-    newDoc.add(new Field("id", Integer.toString(8000), Store.YES, Index.NOT_ANALYZED));
+    newDoc.add(newField("id", Integer.toString(8000), Store.YES, Index.NOT_ANALYZED));
     writer.updateDocument(new Term("id", id10), newDoc);
     assertFalse(r1.isCurrent());
 
@@ -102,7 +102,7 @@
 
     writer = new IndexWriter(dir1, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    doc.add(new Field("field", "a b c", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("field", "a b c", Field.Store.NO, Field.Index.ANALYZED));
     writer.addDocument(doc);
     assertTrue(r2.isCurrent());
     assertTrue(r3.isCurrent());
@@ -776,8 +776,8 @@
     Directory dir = newDirectory();
     final IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    doc.add(new Field("field", "a b c", Field.Store.NO, Field.Index.ANALYZED));
-    Field id = new Field("id", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
+    doc.add(newField("field", "a b c", Field.Store.NO, Field.Index.ANALYZED));
+    Field id = newField("id", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
     doc.add(id);
     id.setValue("0");
     w.addDocument(doc);
@@ -800,8 +800,8 @@
     Directory dir = newDirectory();
     final IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    doc.add(new Field("field", "a b c", Field.Store.NO, Field.Index.ANALYZED));
-    Field id = new Field("id", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
+    doc.add(newField("field", "a b c", Field.Store.NO, Field.Index.ANALYZED));
+    Field id = newField("id", "", Field.Store.NO, Field.Index.NOT_ANALYZED);
     doc.add(id);
     id.setValue("0");
     w.addDocument(doc);
@@ -850,7 +850,7 @@
       });
     
     Document doc = new Document();
-    doc.add(new Field("foo", "bar", Field.Store.YES, Field.Index.NOT_ANALYZED));
+    doc.add(newField("foo", "bar", Field.Store.YES, Field.Index.NOT_ANALYZED));
     for(int i=0;i<20;i++) {
       w.addDocument(doc);
     }
Index: lucene/src/test/org/apache/lucene/index/TestTransactionRollback.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestTransactionRollback.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestTransactionRollback.java	(working copy)
@@ -130,7 +130,7 @@
     IndexWriter w=new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setIndexDeletionPolicy(sdp));
     for(int currentRecordId=1;currentRecordId<=100;currentRecordId++) {
       Document doc=new Document();
-      doc.add(new Field(FIELD_RECORD_ID,""+currentRecordId,Field.Store.YES,Field.Index.ANALYZED));
+      doc.add(newField(FIELD_RECORD_ID,""+currentRecordId,Field.Store.YES,Field.Index.ANALYZED));
       w.addDocument(doc);
 			
       if (currentRecordId%10 == 0) {
Index: lucene/src/test/org/apache/lucene/index/TestFilterIndexReader.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestFilterIndexReader.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestFilterIndexReader.java	(working copy)
@@ -133,15 +133,15 @@
     IndexWriter writer = new IndexWriter(directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));
 
     Document d1 = new Document();
-    d1.add(new Field("default","one two", Field.Store.YES, Field.Index.ANALYZED));
+    d1.add(newField("default","one two", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(d1);
 
     Document d2 = new Document();
-    d2.add(new Field("default","one three", Field.Store.YES, Field.Index.ANALYZED));
+    d2.add(newField("default","one three", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(d2);
 
     Document d3 = new Document();
-    d3.add(new Field("default","two four", Field.Store.YES, Field.Index.ANALYZED));
+    d3.add(newField("default","two four", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(d3);
 
     writer.close();
Index: lucene/src/test/org/apache/lucene/index/TestIndexReaderReopen.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexReaderReopen.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestIndexReaderReopen.java	(working copy)
@@ -181,9 +181,9 @@
       for (int i=0; i<4; i++) {
         for (int j=0; j<M; j++) {
           Document doc = new Document();
-          doc.add(new Field("id", i+"_"+j, Store.YES, Index.NOT_ANALYZED));
-          doc.add(new Field("id2", i+"_"+j, Store.YES, Index.NOT_ANALYZED_NO_NORMS));
-          doc.add(new Field("id3", i+"_"+j, Store.YES, Index.NO));
+          doc.add(newField("id", i+"_"+j, Store.YES, Index.NOT_ANALYZED));
+          doc.add(newField("id2", i+"_"+j, Store.YES, Index.NOT_ANALYZED_NO_NORMS));
+          doc.add(newField("id3", i+"_"+j, Store.YES, Index.NO));
           iwriter.addDocument(doc);
           if (i>0) {
             int k = i-1;
@@ -1196,7 +1196,7 @@
     ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(10);
     for(int i=0;i<4;i++) {
       Document doc = new Document();
-      doc.add(new Field("id", ""+i, Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("id", ""+i, Field.Store.NO, Field.Index.NOT_ANALYZED));
       writer.addDocument(doc);
       Map<String,String> data = new HashMap<String,String>();
       data.put("index", i+"");
Index: lucene/src/test/org/apache/lucene/index/TestAddIndexes.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestAddIndexes.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestAddIndexes.java	(working copy)
@@ -150,8 +150,8 @@
     // docs, so 10 pending deletes:
     for (int i = 0; i < 20; i++) {
       Document doc = new Document();
-      doc.add(new Field("id", "" + (i % 10), Field.Store.NO, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("content", "bbb " + i, Field.Store.NO,
+      doc.add(newField("id", "" + (i % 10), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("content", "bbb " + i, Field.Store.NO,
                         Field.Index.ANALYZED));
       writer.updateDocument(new Term("id", "" + (i%10)), doc);
     }
@@ -186,8 +186,8 @@
     // docs, so 10 pending deletes:
     for (int i = 0; i < 20; i++) {
       Document doc = new Document();
-      doc.add(new Field("id", "" + (i % 10), Field.Store.NO, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("content", "bbb " + i, Field.Store.NO, Field.Index.ANALYZED));
+      doc.add(newField("id", "" + (i % 10), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("content", "bbb " + i, Field.Store.NO, Field.Index.ANALYZED));
       writer.updateDocument(new Term("id", "" + (i%10)), doc);
     }
     
@@ -224,8 +224,8 @@
     // docs, so 10 pending deletes:
     for (int i = 0; i < 20; i++) {
       Document doc = new Document();
-      doc.add(new Field("id", "" + (i % 10), Field.Store.NO, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("content", "bbb " + i, Field.Store.NO,
+      doc.add(newField("id", "" + (i % 10), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("content", "bbb " + i, Field.Store.NO,
                         Field.Index.ANALYZED));
       writer.updateDocument(new Term("id", "" + (i%10)), doc);
     }
@@ -456,7 +456,7 @@
   private void addDocs(IndexWriter writer, int numDocs) throws IOException {
     for (int i = 0; i < numDocs; i++) {
       Document doc = new Document();
-      doc.add(new Field("content", "aaa", Field.Store.NO,
+      doc.add(newField("content", "aaa", Field.Store.NO,
                         Field.Index.ANALYZED));
       writer.addDocument(doc);
     }
@@ -465,7 +465,7 @@
   private void addDocs2(IndexWriter writer, int numDocs) throws IOException {
     for (int i = 0; i < numDocs; i++) {
       Document doc = new Document();
-      doc.add(new Field("content", "bbb", Field.Store.NO,
+      doc.add(newField("content", "bbb", Field.Store.NO,
                         Field.Index.ANALYZED));
       writer.addDocument(doc);
     }
@@ -530,19 +530,19 @@
         .setMaxBufferedDocs(5).setMergePolicy(lmp));
 
     Document doc = new Document();
-    doc.add(new Field("content", "aaa bbb ccc ddd eee fff ggg hhh iii", Field.Store.YES,
+    doc.add(newField("content", "aaa bbb ccc ddd eee fff ggg hhh iii", Field.Store.YES,
                       Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
     for(int i=0;i<60;i++)
       writer.addDocument(doc);
 
     Document doc2 = new Document();
-    doc2.add(new Field("content", "aaa bbb ccc ddd eee fff ggg hhh iii", Field.Store.YES,
+    doc2.add(newField("content", "aaa bbb ccc ddd eee fff ggg hhh iii", Field.Store.YES,
                       Field.Index.NO));
-    doc2.add(new Field("content", "aaa bbb ccc ddd eee fff ggg hhh iii", Field.Store.YES,
+    doc2.add(newField("content", "aaa bbb ccc ddd eee fff ggg hhh iii", Field.Store.YES,
                       Field.Index.NO));
-    doc2.add(new Field("content", "aaa bbb ccc ddd eee fff ggg hhh iii", Field.Store.YES,
+    doc2.add(newField("content", "aaa bbb ccc ddd eee fff ggg hhh iii", Field.Store.YES,
                       Field.Index.NO));
-    doc2.add(new Field("content", "aaa bbb ccc ddd eee fff ggg hhh iii", Field.Store.YES,
+    doc2.add(newField("content", "aaa bbb ccc ddd eee fff ggg hhh iii", Field.Store.YES,
                       Field.Index.NO));
     for(int i=0;i<10;i++)
       writer.addDocument(doc2);
Index: lucene/src/test/org/apache/lucene/index/TestStressIndexing.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestStressIndexing.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestStressIndexing.java	(working copy)
@@ -80,8 +80,8 @@
       for(int j=0; j<10; j++) {
         Document d = new Document();
         int n = random.nextInt();
-        d.add(new Field("id", Integer.toString(nextID++), Field.Store.YES, Field.Index.NOT_ANALYZED));
-        d.add(new Field("contents", English.intToEnglish(n), Field.Store.NO, Field.Index.ANALYZED));
+        d.add(newField("id", Integer.toString(nextID++), Field.Store.YES, Field.Index.NOT_ANALYZED));
+        d.add(newField("contents", English.intToEnglish(n), Field.Store.NO, Field.Index.ANALYZED));
         writer.addDocument(d);
       }
 
Index: lucene/src/test/org/apache/lucene/index/TestTermdocPerf.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestTermdocPerf.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestTermdocPerf.java	(working copy)
@@ -69,7 +69,7 @@
     };
 
     Document doc = new Document();
-    doc.add(new Field(field,val, Field.Store.NO, Field.Index.NOT_ANALYZED_NO_NORMS));
+    doc.add(newField(field,val, Field.Store.NO, Field.Index.NOT_ANALYZED_NO_NORMS));
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(
         TEST_VERSION_CURRENT, analyzer)
         .setOpenMode(OpenMode.CREATE).setMaxBufferedDocs(100));
Index: lucene/src/test/org/apache/lucene/index/TestPayloads.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestPayloads.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestPayloads.java	(working copy)
@@ -103,15 +103,15 @@
         IndexWriter writer = new IndexWriter(ram, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer));
         Document d = new Document();
         // this field won't have any payloads
-        d.add(new Field("f1", "This field has no payloads", Field.Store.NO, Field.Index.ANALYZED));
+        d.add(newField("f1", "This field has no payloads", Field.Store.NO, Field.Index.ANALYZED));
         // this field will have payloads in all docs, however not for all term positions,
         // so this field is used to check if the DocumentWriter correctly enables the payloads bit
         // even if only some term positions have payloads
-        d.add(new Field("f2", "This field has payloads in all docs", Field.Store.NO, Field.Index.ANALYZED));
-        d.add(new Field("f2", "This field has payloads in all docs", Field.Store.NO, Field.Index.ANALYZED));
+        d.add(newField("f2", "This field has payloads in all docs", Field.Store.NO, Field.Index.ANALYZED));
+        d.add(newField("f2", "This field has payloads in all docs", Field.Store.NO, Field.Index.ANALYZED));
         // this field is used to verify if the SegmentMerger enables payloads for a field if it has payloads 
         // enabled in only some documents
-        d.add(new Field("f3", "This field has payloads in some docs", Field.Store.NO, Field.Index.ANALYZED));
+        d.add(newField("f3", "This field has payloads in some docs", Field.Store.NO, Field.Index.ANALYZED));
         // only add payload data for field f2
         analyzer.setPayloadData("f2", 1, "somedata".getBytes(), 0, 1);
         writer.addDocument(d);
@@ -130,10 +130,10 @@
         writer = new IndexWriter(ram, newIndexWriterConfig( TEST_VERSION_CURRENT,
             analyzer).setOpenMode(OpenMode.CREATE));
         d = new Document();
-        d.add(new Field("f1", "This field has no payloads", Field.Store.NO, Field.Index.ANALYZED));
-        d.add(new Field("f2", "This field has payloads in all docs", Field.Store.NO, Field.Index.ANALYZED));
-        d.add(new Field("f2", "This field has payloads in all docs", Field.Store.NO, Field.Index.ANALYZED));
-        d.add(new Field("f3", "This field has payloads in some docs", Field.Store.NO, Field.Index.ANALYZED));
+        d.add(newField("f1", "This field has no payloads", Field.Store.NO, Field.Index.ANALYZED));
+        d.add(newField("f2", "This field has payloads in all docs", Field.Store.NO, Field.Index.ANALYZED));
+        d.add(newField("f2", "This field has payloads in all docs", Field.Store.NO, Field.Index.ANALYZED));
+        d.add(newField("f3", "This field has payloads in some docs", Field.Store.NO, Field.Index.ANALYZED));
         // add payload data for field f2 and f3
         analyzer.setPayloadData("f2", "somedata".getBytes(), 0, 1);
         analyzer.setPayloadData("f3", "somedata".getBytes(), 0, 3);
@@ -196,7 +196,7 @@
         byte[] payloadData = generateRandomData(payloadDataLength);
         
         Document d = new Document();
-        d.add(new Field(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));
+        d.add(newField(fieldName, content, Field.Store.NO, Field.Index.ANALYZED));
         // add the same document multiple times to have the same payload lengths for all
         // occurrences within two consecutive skip intervals
         int offset = 0;
@@ -317,7 +317,7 @@
         String singleTerm = "lucene";
         
         d = new Document();
-        d.add(new Field(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));
+        d.add(newField(fieldName, singleTerm, Field.Store.NO, Field.Index.ANALYZED));
         // add a payload whose length is greater than the buffer size of BufferedIndexOutput
         payloadData = generateRandomData(2000);
         analyzer.setPayloadData(fieldName, payloadData, 100, 1500);
Index: lucene/src/test/org/apache/lucene/index/TestLazyBug.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestLazyBug.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestLazyBug.java	(working copy)
@@ -78,7 +78,7 @@
       for (int d = 1; d <= NUM_DOCS; d++) {
         Document doc = new Document();
         for (int f = 1; f <= NUM_FIELDS; f++ ) {
-          doc.add(new Field("f"+f, 
+          doc.add(newField("f"+f, 
                             data[f % data.length] 
                             + '#' + data[random.nextInt(data.length)], 
                             Field.Store.YES, 
Index: lucene/src/test/org/apache/lucene/index/TestIndexWriterMerging.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexWriterMerging.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestIndexWriterMerging.java	(working copy)
@@ -103,7 +103,7 @@
     for (int i = start; i < (start + numDocs); i++)
     {
       Document temp = new Document();
-      temp.add(new Field("count", (""+i), Field.Store.YES, Field.Index.NOT_ANALYZED));
+      temp.add(newField("count", (""+i), Field.Store.YES, Field.Index.NOT_ANALYZED));
 
       writer.addDocument(temp);
     }
Index: lucene/src/test/org/apache/lucene/index/TestParallelReader.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestParallelReader.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestParallelReader.java	(working copy)
@@ -123,7 +123,7 @@
     Directory dir2 = newDirectory();
     IndexWriter w2 = new IndexWriter(dir2, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document d3 = new Document();
-    d3.add(new Field("f3", "v1", Field.Store.YES, Field.Index.ANALYZED));
+    d3.add(newField("f3", "v1", Field.Store.YES, Field.Index.ANALYZED));
     w2.addDocument(d3);
     w2.close();
     
@@ -177,14 +177,14 @@
     IndexWriter modifier = new IndexWriter(dir1, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
     ((LogMergePolicy) modifier.getMergePolicy()).setMergeFactor(10);
     Document d = new Document();
-    d.add(new Field("f1", "v1", Field.Store.YES, Field.Index.ANALYZED));
+    d.add(newField("f1", "v1", Field.Store.YES, Field.Index.ANALYZED));
     modifier.addDocument(d);
     modifier.close();
     
     modifier = new IndexWriter(dir2, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
     ((LogMergePolicy) modifier.getMergePolicy()).setMergeFactor(10);
     d = new Document();
-    d.add(new Field("f2", "v2", Field.Store.YES, Field.Index.ANALYZED));
+    d.add(newField("f2", "v2", Field.Store.YES, Field.Index.ANALYZED));
     modifier.addDocument(d);
     modifier.close();
 
@@ -241,16 +241,16 @@
     dir = newDirectory();
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document d1 = new Document();
-    d1.add(new Field("f1", "v1", Field.Store.YES, Field.Index.ANALYZED));
-    d1.add(new Field("f2", "v1", Field.Store.YES, Field.Index.ANALYZED));
-    d1.add(new Field("f3", "v1", Field.Store.YES, Field.Index.ANALYZED));
-    d1.add(new Field("f4", "v1", Field.Store.YES, Field.Index.ANALYZED));
+    d1.add(newField("f1", "v1", Field.Store.YES, Field.Index.ANALYZED));
+    d1.add(newField("f2", "v1", Field.Store.YES, Field.Index.ANALYZED));
+    d1.add(newField("f3", "v1", Field.Store.YES, Field.Index.ANALYZED));
+    d1.add(newField("f4", "v1", Field.Store.YES, Field.Index.ANALYZED));
     w.addDocument(d1);
     Document d2 = new Document();
-    d2.add(new Field("f1", "v2", Field.Store.YES, Field.Index.ANALYZED));
-    d2.add(new Field("f2", "v2", Field.Store.YES, Field.Index.ANALYZED));
-    d2.add(new Field("f3", "v2", Field.Store.YES, Field.Index.ANALYZED));
-    d2.add(new Field("f4", "v2", Field.Store.YES, Field.Index.ANALYZED));
+    d2.add(newField("f1", "v2", Field.Store.YES, Field.Index.ANALYZED));
+    d2.add(newField("f2", "v2", Field.Store.YES, Field.Index.ANALYZED));
+    d2.add(newField("f3", "v2", Field.Store.YES, Field.Index.ANALYZED));
+    d2.add(newField("f4", "v2", Field.Store.YES, Field.Index.ANALYZED));
     w.addDocument(d2);
     w.close();
 
@@ -271,12 +271,12 @@
     Directory dir1 = newDirectory();
     IndexWriter w1 = new IndexWriter(dir1, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document d1 = new Document();
-    d1.add(new Field("f1", "v1", Field.Store.YES, Field.Index.ANALYZED));
-    d1.add(new Field("f2", "v1", Field.Store.YES, Field.Index.ANALYZED));
+    d1.add(newField("f1", "v1", Field.Store.YES, Field.Index.ANALYZED));
+    d1.add(newField("f2", "v1", Field.Store.YES, Field.Index.ANALYZED));
     w1.addDocument(d1);
     Document d2 = new Document();
-    d2.add(new Field("f1", "v2", Field.Store.YES, Field.Index.ANALYZED));
-    d2.add(new Field("f2", "v2", Field.Store.YES, Field.Index.ANALYZED));
+    d2.add(newField("f1", "v2", Field.Store.YES, Field.Index.ANALYZED));
+    d2.add(newField("f2", "v2", Field.Store.YES, Field.Index.ANALYZED));
     w1.addDocument(d2);
     w1.close();
     return dir1;
@@ -286,12 +286,12 @@
     Directory dir2 = newDirectory();
     IndexWriter w2 = new IndexWriter(dir2, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document d3 = new Document();
-    d3.add(new Field("f3", "v1", Field.Store.YES, Field.Index.ANALYZED));
-    d3.add(new Field("f4", "v1", Field.Store.YES, Field.Index.ANALYZED));
+    d3.add(newField("f3", "v1", Field.Store.YES, Field.Index.ANALYZED));
+    d3.add(newField("f4", "v1", Field.Store.YES, Field.Index.ANALYZED));
     w2.addDocument(d3);
     Document d4 = new Document();
-    d4.add(new Field("f3", "v2", Field.Store.YES, Field.Index.ANALYZED));
-    d4.add(new Field("f4", "v2", Field.Store.YES, Field.Index.ANALYZED));
+    d4.add(newField("f3", "v2", Field.Store.YES, Field.Index.ANALYZED));
+    d4.add(newField("f4", "v2", Field.Store.YES, Field.Index.ANALYZED));
     w2.addDocument(d4);
     w2.close();
     return dir2;
Index: lucene/src/test/org/apache/lucene/index/codecs/preflex/TestSurrogates.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/codecs/preflex/TestSurrogates.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/codecs/preflex/TestSurrogates.java	(working copy)
@@ -297,7 +297,7 @@
         uniqueTerms.add(term);
         fieldTerms.add(new Term(field, term));
         Document doc = new Document();
-        doc.add(new Field(field, term, Field.Store.NO, Field.Index.NOT_ANALYZED));
+        doc.add(newField(field, term, Field.Store.NO, Field.Index.NOT_ANALYZED));
         w.addDocument(doc);
       }
       uniqueTermCount += uniqueTerms.size();
Index: lucene/src/test/org/apache/lucene/index/TestDeletionPolicy.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestDeletionPolicy.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestDeletionPolicy.java	(working copy)
@@ -795,7 +795,7 @@
   private void addDoc(IndexWriter writer) throws IOException
   {
     Document doc = new Document();
-    doc.add(new Field("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
     writer.addDocument(doc);
   }
 }
Index: lucene/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java	(working copy)
@@ -79,10 +79,10 @@
     {
       IndexWriter iw = new IndexWriter(rd1, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
       Document doc = new Document();
-      doc.add(new Field("test", "", Store.NO, Index.ANALYZED,
+      doc.add(newField("test", "", Store.NO, Index.ANALYZED,
                         TermVector.YES));
       iw.addDocument(doc);
-      doc.add(new Field("test", "", Store.NO, Index.ANALYZED,
+      doc.add(newField("test", "", Store.NO, Index.ANALYZED,
                         TermVector.NO));
       iw.addDocument(doc);
       iw.close();
Index: lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	(working copy)
@@ -479,7 +479,7 @@
     dirName = fullDir(dirName);
 
     Directory dir = FSDirectory.open(new File(dirName));
-    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);
+    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);
     ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);
     ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);
     IndexWriter writer = new IndexWriter(dir, conf);
@@ -491,7 +491,7 @@
     writer.close();
 
     // open fresh writer so we get no prx file in the added segment
-    conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);
+    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);
     ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);
     ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);
     writer = new IndexWriter(dir, conf);
Index: lucene/src/test/org/apache/lucene/index/TestLazyProxSkipping.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestLazyProxSkipping.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestLazyProxSkipping.java	(working copy)
@@ -85,7 +85,7 @@
                 content = this.term3 + " " + this.term2;
             }
 
-            doc.add(new Field(this.field, content, Field.Store.YES, Field.Index.ANALYZED));
+            doc.add(newField(this.field, content, Field.Store.YES, Field.Index.ANALYZED));
             writer.addDocument(doc);
         }
         
@@ -129,7 +129,7 @@
         IndexWriter writer = new IndexWriter(directory, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));
         for (int i = 0; i < 10; i++) {
             Document doc = new Document();
-            doc.add(new Field(this.field, "a b", Field.Store.YES, Field.Index.ANALYZED));
+            doc.add(newField(this.field, "a b", Field.Store.YES, Field.Index.ANALYZED));
             writer.addDocument(doc);
         }
         
Index: lucene/src/test/org/apache/lucene/index/TestNorms.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestNorms.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/index/TestNorms.java	(working copy)
@@ -208,7 +208,7 @@
     Document d = new Document();
     float boost = nextNorm();
     for (int i = 0; i < 10; i++) {
-      Field f = new Field("f"+i,"v"+i,Store.NO,Index.NOT_ANALYZED);
+      Field f = newField("f"+i,"v"+i,Store.NO,Index.NOT_ANALYZED);
       f.setBoost(boost);
       d.add(f);
     }
Index: lucene/src/test/org/apache/lucene/TestSearchForDuplicates.java
===================================================================
--- lucene/src/test/org/apache/lucene/TestSearchForDuplicates.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/TestSearchForDuplicates.java	(working copy)
@@ -89,11 +89,11 @@
 
       for (int j = 0; j < MAX_DOCS; j++) {
         Document d = new Document();
-        d.add(new Field(PRIORITY_FIELD, HIGH_PRIORITY, Field.Store.YES, Field.Index.ANALYZED));
+        d.add(newField(PRIORITY_FIELD, HIGH_PRIORITY, Field.Store.YES, Field.Index.ANALYZED));
 
         // NOTE: this ID_FIELD produces no tokens since
         // MockAnalyzer discards numbers
-        d.add(new Field(ID_FIELD, Integer.toString(j), Field.Store.YES, Field.Index.ANALYZED));
+        d.add(newField(ID_FIELD, Integer.toString(j), Field.Store.YES, Field.Index.ANALYZED));
         writer.addDocument(d);
       }
       writer.close();
Index: lucene/src/test/org/apache/lucene/store/TestLockFactory.java
===================================================================
--- lucene/src/test/org/apache/lucene/store/TestLockFactory.java	(revision 995688)
+++ lucene/src/test/org/apache/lucene/store/TestLockFactory.java	(working copy)
@@ -405,7 +405,7 @@
 
     private void addDoc(IndexWriter writer) throws IOException {
         Document doc = new Document();
-        doc.add(new Field("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
+        doc.add(newField("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
         writer.addDocument(doc);
     }
 }
Index: lucene/src/test/org/apache/lucene/store/TestRAMDirectory.java
===================================================================
--- lucene/src/test/org/apache/lucene/store/TestRAMDirectory.java	(revision 995688)
+++ lucene/src/test/org/apache/lucene/store/TestRAMDirectory.java	(working copy)
@@ -58,7 +58,7 @@
     Document doc = null;
     for (int i = 0; i < docsToAdd; i++) {
       doc = new Document();
-      doc.add(new Field("content", English.intToEnglish(i).trim(), Field.Store.YES, Field.Index.NOT_ANALYZED));
+      doc.add(newField("content", English.intToEnglish(i).trim(), Field.Store.YES, Field.Index.NOT_ANALYZED));
       writer.addDocument(doc);
     }
     assertEquals(docsToAdd, writer.maxDoc());
@@ -118,7 +118,7 @@
         public void run() {
           for (int j=1; j<docsPerThread; j++) {
             Document doc = new Document();
-            doc.add(new Field("sizeContent", English.intToEnglish(num*docsPerThread+j).trim(), Field.Store.YES, Field.Index.NOT_ANALYZED));
+            doc.add(newField("sizeContent", English.intToEnglish(num*docsPerThread+j).trim(), Field.Store.YES, Field.Index.NOT_ANALYZED));
             try {
               writer.addDocument(doc);
             } catch (IOException e) {
Index: lucene/src/test/org/apache/lucene/store/TestMultiMMap.java
===================================================================
--- lucene/src/test/org/apache/lucene/store/TestMultiMMap.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/store/TestMultiMMap.java	(working copy)
@@ -60,8 +60,8 @@
       dir.setUseUnmap(true);
     RandomIndexWriter writer = new RandomIndexWriter(random, dir);
     Document doc = new Document();
-    Field docid = new Field("docid", "0", Field.Store.YES, Field.Index.NOT_ANALYZED);
-    Field junk = new Field("junk", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
+    Field docid = newField("docid", "0", Field.Store.YES, Field.Index.NOT_ANALYZED);
+    Field junk = newField("junk", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
     doc.add(docid);
     doc.add(junk);
     
Index: lucene/src/test/org/apache/lucene/store/TestWindowsMMap.java
===================================================================
--- lucene/src/test/org/apache/lucene/store/TestWindowsMMap.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/store/TestWindowsMMap.java	(working copy)
@@ -83,7 +83,7 @@
     for(int dx = 0; dx < num; dx ++) {
       String f = randomField();
       Document doc = new Document();
-      doc.add(new Field("data", f, Field.Store.YES, Field.Index.ANALYZED));	
+      doc.add(newField("data", f, Field.Store.YES, Field.Index.ANALYZED));	
       writer.addDocument(doc);
     }
     
Index: lucene/src/test/org/apache/lucene/store/TestBufferedIndexInput.java
===================================================================
--- lucene/src/test/org/apache/lucene/store/TestBufferedIndexInput.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/store/TestBufferedIndexInput.java	(working copy)
@@ -249,8 +249,8 @@
         ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);
         for(int i=0;i<37;i++) {
           Document doc = new Document();
-          doc.add(new Field("content", "aaa bbb ccc ddd" + i, Field.Store.YES, Field.Index.ANALYZED));
-          doc.add(new Field("id", "" + i, Field.Store.YES, Field.Index.ANALYZED));
+          doc.add(newField("content", "aaa bbb ccc ddd" + i, Field.Store.YES, Field.Index.ANALYZED));
+          doc.add(newField("id", "" + i, Field.Store.YES, Field.Index.ANALYZED));
           writer.addDocument(doc);
         }
         writer.close();
Index: lucene/src/test/org/apache/lucene/util/LuceneTestCaseJ4.java
===================================================================
--- lucene/src/test/org/apache/lucene/util/LuceneTestCaseJ4.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/util/LuceneTestCaseJ4.java	(working copy)
@@ -18,6 +18,10 @@
  */
 
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Field.Index;
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.document.Field.TermVector;
 import org.apache.lucene.index.ConcurrentMergeScheduler;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.LogDocMergePolicy;
@@ -610,6 +614,55 @@
     return dir;
   }
   
+  public Field newField(String name, String value, Index index) {
+    return newField(random, name, value, index);
+  }
+  
+  public Field newField(String name, String value, Store store, Index index) {
+    return newField(random, name, value, store, index);
+  }
+  
+  public Field newField(String name, String value, Store store, Index index, TermVector tv) {
+    return newField(random, name, value, store, index, tv);
+  }
+  
+  public static Field newField(Random random, String name, String value, Index index) {
+    return newField(random, name, value, Store.NO, index);
+  }
+  
+  public static Field newField(Random random, String name, String value, Store store, Index index) {
+    return newField(random, name, value, store, index, TermVector.NO);
+  }
+  
+  public static Field newField(Random random, String name, String value, Store store, Index index, TermVector tv) {
+    if (!index.isIndexed())
+      return new Field(name, value, store, index);
+    
+    if (!store.isStored() && random.nextBoolean())
+      store = Store.YES; // randomly store it
+    
+    tv = randomTVSetting(random, tv);
+    
+    return new Field(name, value, store, index, tv);
+  }
+  
+  static final TermVector tvSettings[] = { 
+    TermVector.NO, TermVector.YES, TermVector.WITH_OFFSETS, 
+    TermVector.WITH_POSITIONS, TermVector.WITH_POSITIONS_OFFSETS 
+  };
+  
+  private static TermVector randomTVSetting(Random random, TermVector minimum) {
+    switch(minimum) {
+      case NO: return tvSettings[_TestUtil.nextInt(random, 0, tvSettings.length-1)];
+      case YES: return tvSettings[_TestUtil.nextInt(random, 1, tvSettings.length-1)];
+      case WITH_OFFSETS: return random.nextBoolean() ? TermVector.WITH_OFFSETS 
+          : TermVector.WITH_POSITIONS_OFFSETS;
+      case WITH_POSITIONS: return random.nextBoolean() ? TermVector.WITH_POSITIONS 
+          : TermVector.WITH_POSITIONS_OFFSETS;
+      default: return TermVector.WITH_POSITIONS_OFFSETS;
+    }
+  }
+  
   /** return a random Locale from the available locales on the system */
   public static Locale randomLocale(Random random) {
     Locale locales[] = Locale.getAvailableLocales();
Index: lucene/src/test/org/apache/lucene/util/LuceneTestCase.java
===================================================================
--- lucene/src/test/org/apache/lucene/util/LuceneTestCase.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/util/LuceneTestCase.java	(working copy)
@@ -35,6 +35,10 @@
 import junit.framework.TestResult;
 
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Field.Index;
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.document.Field.TermVector;
 import org.apache.lucene.index.ConcurrentMergeScheduler;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.codecs.Codec;
@@ -339,6 +343,18 @@
     return dir;
   }
   
+  public Field newField(String name, String value, Index index) {
+    return LuceneTestCaseJ4.newField(random, name, value, index);
+  }
+  
+  public Field newField(String name, String value, Store store, Index index) {
+    return LuceneTestCaseJ4.newField(random, name, value, store, index);
+  }
+  
+  public Field newField(String name, String value, Store store, Index index, TermVector tv) {
+    return LuceneTestCaseJ4.newField(random, name, value, store, index, tv);
+  }
+  
   /** Gets a resource from the classpath as {@link File}. This method should only be used,
    * if a real file is needed. To get a stream, code should prefer
    * {@link Class#getResourceAsStream} using {@code this.getClass()}.
Index: lucene/src/test/org/apache/lucene/util/TestFieldCacheSanityChecker.java
===================================================================
--- lucene/src/test/org/apache/lucene/util/TestFieldCacheSanityChecker.java	(revision 995772)
+++ lucene/src/test/org/apache/lucene/util/TestFieldCacheSanityChecker.java	(working copy)
@@ -54,12 +54,12 @@
     float theFloat = Float.MAX_VALUE;
     for (int i = 0; i < NUM_DOCS; i++){
       Document doc = new Document();
-      doc.add(new Field("theLong", String.valueOf(theLong--), Field.Store.NO, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("theDouble", String.valueOf(theDouble--), Field.Store.NO, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("theByte", String.valueOf(theByte--), Field.Store.NO, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("theShort", String.valueOf(theShort--), Field.Store.NO, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("theInt", String.valueOf(theInt--), Field.Store.NO, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("theFloat", String.valueOf(theFloat--), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("theLong", String.valueOf(theLong--), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("theDouble", String.valueOf(theDouble--), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("theByte", String.valueOf(theByte--), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("theShort", String.valueOf(theShort--), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("theInt", String.valueOf(theInt--), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("theFloat", String.valueOf(theFloat--), Field.Store.NO, Field.Index.NOT_ANALYZED));
       if (0 == i % 3) {
         wA.addDocument(doc);
       } else {
Index: lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestMultiFieldQPHelper.java
===================================================================
--- lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestMultiFieldQPHelper.java	(revision 995772)
+++ lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestMultiFieldQPHelper.java	(working copy)
@@ -320,7 +320,7 @@
     Directory ramDir = newDirectory();
     IndexWriter iw = new IndexWriter(ramDir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
     Document doc = new Document();
-    doc.add(new Field("body", "blah the footest blah", Field.Store.NO,
+    doc.add(newField("body", "blah the footest blah", Field.Store.NO,
         Field.Index.ANALYZED));
     iw.addDocument(doc);
     iw.close();
Index: lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQPHelper.java
===================================================================
--- lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQPHelper.java	(revision 995772)
+++ lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQPHelper.java	(working copy)
@@ -656,7 +656,7 @@
     Directory ramDir = newDirectory();
     IndexWriter iw = new IndexWriter(ramDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));
     Document doc = new Document();
-    doc.add(new Field("content", "\u0633\u0627\u0628", Field.Store.YES,
+    doc.add(newField("content", "\u0633\u0627\u0628", Field.Store.YES,
         Field.Index.NOT_ANALYZED));
     iw.addDocument(doc);
     iw.close();
@@ -1250,13 +1250,13 @@
     assertEquals(expected, hits.length);
   }
 
-  private static void addDateDoc(String content, int year, int month, int day,
+  private void addDateDoc(String content, int year, int month, int day,
       int hour, int minute, int second, IndexWriter iw) throws IOException {
     Document d = new Document();
-    d.add(new Field("f", content, Field.Store.YES, Field.Index.ANALYZED));
+    d.add(newField("f", content, Field.Store.YES, Field.Index.ANALYZED));
     Calendar cal = Calendar.getInstance(Locale.ENGLISH);
     cal.set(year, month - 1, day, hour, minute, second);
-    d.add(new Field("date", DateField.dateToString(cal.getTime()),
+    d.add(newField("date", DateField.dateToString(cal.getTime()),
         Field.Store.YES, Field.Index.NOT_ANALYZED));
     iw.addDocument(d);
   }
@@ -1306,7 +1306,7 @@
     Directory dir = newDirectory();
     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new CannedAnalyzer()));
     Document doc = new Document();
-    doc.add(new Field("field", "", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField("field", "", Field.Store.NO, Field.Index.ANALYZED));
     w.addDocument(doc);
     IndexReader r = w.getReader();
     IndexSearcher s = new IndexSearcher(r);
Index: lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestMultiFieldQueryParserWrapper.java
===================================================================
--- lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestMultiFieldQueryParserWrapper.java	(revision 995772)
+++ lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestMultiFieldQueryParserWrapper.java	(working copy)
@@ -324,7 +324,7 @@
     IndexWriter iw = new IndexWriter(ramDir, analyzer, true,
         IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
-    doc.add(new Field("body", "blah the footest blah", Field.Store.NO,
+    doc.add(newField("body", "blah the footest blah", Field.Store.NO,
         Field.Index.ANALYZED));
     iw.addDocument(doc);
     iw.close();
Index: lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQueryParserWrapper.java
===================================================================
--- lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQueryParserWrapper.java	(revision 995772)
+++ lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQueryParserWrapper.java	(working copy)
@@ -658,7 +658,7 @@
     IndexWriter iw = new IndexWriter(ramDir, new MockAnalyzer(MockTokenizer.WHITESPACE, false), true,
         IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
-    doc.add(new Field("content", "\u0633\u0627\u0628", Field.Store.YES,
+    doc.add(newField("content", "\u0633\u0627\u0628", Field.Store.YES,
         Field.Index.NOT_ANALYZED));
     iw.addDocument(doc);
     iw.close();
@@ -1201,13 +1201,13 @@
     assertEquals(expected, hits.length);
   }
 
-  private static void addDateDoc(String content, int year, int month, int day,
+  private void addDateDoc(String content, int year, int month, int day,
       int hour, int minute, int second, IndexWriter iw) throws IOException {
     Document d = new Document();
-    d.add(new Field("f", content, Field.Store.YES, Field.Index.ANALYZED));
+    d.add(newField("f", content, Field.Store.YES, Field.Index.ANALYZED));
     Calendar cal = Calendar.getInstance(Locale.ENGLISH);
     cal.set(year, month - 1, day, hour, minute, second);
-    d.add(new Field("date", DateField.dateToString(cal.getTime()),
+    d.add(newField("date", DateField.dateToString(cal.getTime()),
         Field.Store.YES, Field.Index.NOT_ANALYZED));
     iw.addDocument(d);
   }
Index: lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java
===================================================================
--- lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java	(revision 995772)
+++ lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java	(working copy)
@@ -115,9 +115,9 @@
     IndexWriter w = new IndexWriter(rd, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
     for (int i = 0; i < docsContent.length; i++) {
       Document doc = new Document();
-      doc.add(new Field("name", docsContent[i].name, Field.Store.YES,
+      doc.add(newField("name", docsContent[i].name, Field.Store.YES,
           Field.Index.ANALYZED));
-      doc.add(new Field("id", docsContent[i].id, Field.Store.YES,
+      doc.add(newField("id", docsContent[i].id, Field.Store.YES,
           Field.Index.ANALYZED));
       w.addDocument(doc);
     }
Index: lucene/contrib/remote/src/test/org/apache/lucene/search/TestRemoteCachingWrapperFilter.java
===================================================================
--- lucene/contrib/remote/src/test/org/apache/lucene/search/TestRemoteCachingWrapperFilter.java	(revision 995688)
+++ lucene/contrib/remote/src/test/org/apache/lucene/search/TestRemoteCachingWrapperFilter.java	(working copy)
@@ -46,15 +46,15 @@
     IndexWriter writer = new IndexWriter(indexStore, newIndexWriterConfig(random,
         TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    doc.add(new Field("test", "test text", Field.Store.YES, Field.Index.ANALYZED));
-    doc.add(new Field("type", "A", Field.Store.YES, Field.Index.ANALYZED));
-    doc.add(new Field("other", "other test text", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField(random, "test", "test text", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField(random, "type", "A", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField(random, "other", "other test text", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
     //Need a second document to search for
     doc = new Document();
-    doc.add(new Field("test", "test text", Field.Store.YES, Field.Index.ANALYZED));
-    doc.add(new Field("type", "B", Field.Store.YES, Field.Index.ANALYZED));
-    doc.add(new Field("other", "other test text", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField(random, "test", "test text", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField(random, "type", "B", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField(random, "other", "other test text", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
     writer.optimize();
     writer.close();
Index: lucene/contrib/remote/src/test/org/apache/lucene/search/TestRemoteSearchable.java
===================================================================
--- lucene/contrib/remote/src/test/org/apache/lucene/search/TestRemoteSearchable.java	(revision 995688)
+++ lucene/contrib/remote/src/test/org/apache/lucene/search/TestRemoteSearchable.java	(working copy)
@@ -45,8 +45,8 @@
     IndexWriter writer = new IndexWriter(indexStore, newIndexWriterConfig(random,
         TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    doc.add(new Field("test", "test text", Field.Store.YES, Field.Index.ANALYZED));
-    doc.add(new Field("other", "other test text", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField(random, "test", "test text", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField(random, "other", "other test text", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
     writer.optimize();
     writer.close();
Index: lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec.java
===================================================================
--- lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec.java	(revision 995688)
+++ lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec.java	(working copy)
@@ -143,7 +143,7 @@
     ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundDocStore(false);
     IndexWriter writer = new IndexWriter(dir, cfg);
     Document doc = new Document();
-    doc.add(new Field("f", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(newField("f", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));
     writer.addDocument(doc);
     writer.commit();
     writer.addDocument(doc);
Index: lucene/contrib/misc/src/test/org/apache/lucene/index/TestMultiPassIndexSplitter.java
===================================================================
--- lucene/contrib/misc/src/test/org/apache/lucene/index/TestMultiPassIndexSplitter.java	(revision 995772)
+++ lucene/contrib/misc/src/test/org/apache/lucene/index/TestMultiPassIndexSplitter.java	(working copy)
@@ -36,8 +36,8 @@
     Document doc;
     for (int i = 0; i < NUM_DOCS; i++) {
       doc = new Document();
-      doc.add(new Field("id", i + "", Field.Store.YES, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("f", i + " " + i, Field.Store.YES, Field.Index.ANALYZED));
+      doc.add(newField("id", i + "", Field.Store.YES, Field.Index.NOT_ANALYZED));
+      doc.add(newField("f", i + " " + i, Field.Store.YES, Field.Index.ANALYZED));
       w.addDocument(doc);
     }
     w.close();
Index: lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor.java
===================================================================
--- lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor.java	(revision 995772)
+++ lucene/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor.java	(working copy)
@@ -30,33 +30,33 @@
     Document doc;
 
     doc = new Document();
-    doc.add(new Field("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
-    doc.add(new Field("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
-    doc.add(new Field("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(newField("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(newField("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(newField("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
     iw.addDocument(doc);
 
     doc = new Document();
-    doc.add(new Field("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));
-    doc.add(new Field("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));
-    doc.add(new Field("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));
+    doc.add(newField("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));
+    doc.add(newField("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));
+    doc.add(newField("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));
     iw.addDocument(doc);
 
     doc = new Document();
-    doc.add(new Field("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));
-    doc.add(new Field("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));
-    doc.add(new Field("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));
+    doc.add(newField("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));
+    doc.add(newField("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));
+    doc.add(newField("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));
     iw.addDocument(doc);
 
     doc = new Document();
-    doc.add(new Field("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));
-    doc.add(new Field("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));
-    doc.add(new Field("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));
+    doc.add(newField("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));
+    doc.add(newField("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));
+    doc.add(newField("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));
     iw.addDocument(doc);
 
     doc = new Document();
-    doc.add(new Field("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
-    doc.add(new Field("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));
-    doc.add(new Field("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));
+    doc.add(newField("a", "a b a c a d a e a f a g a h a", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+    doc.add(newField("b", "a b c b d b e b f b g b h b", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));
+    doc.add(newField("c", "a c b c d c e c f c g c h c", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));
     iw.addDocument(doc);
 
     iw.close();
Index: lucene/contrib/misc/src/test/org/apache/lucene/index/TestFieldNormModifier.java
===================================================================
--- lucene/contrib/misc/src/test/org/apache/lucene/index/TestFieldNormModifier.java	(revision 995772)
+++ lucene/contrib/misc/src/test/org/apache/lucene/index/TestFieldNormModifier.java	(working copy)
@@ -62,13 +62,13 @@
     
     for (int i = 0; i < NUM_DOCS; i++) {
       Document d = new Document();
-      d.add(new Field("field", "word", Field.Store.YES, Field.Index.ANALYZED));
-      d.add(new Field("nonorm", "word", Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
-      d.add(new Field("untokfield", "20061212 20071212", Field.Store.YES, Field.Index.ANALYZED));
+      d.add(newField("field", "word", Field.Store.YES, Field.Index.ANALYZED));
+      d.add(newField("nonorm", "word", Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
+      d.add(newField("untokfield", "20061212 20071212", Field.Store.YES, Field.Index.ANALYZED));
       
       for (int j = 1; j <= i; j++) {
-        d.add(new Field("field", "crap", Field.Store.YES, Field.Index.ANALYZED));
-        d.add(new Field("nonorm", "more words", Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
+        d.add(newField("field", "crap", Field.Store.YES, Field.Index.ANALYZED));
+        d.add(newField("nonorm", "more words", Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
       }
       writer.addDocument(d);
     }
Index: lucene/contrib/misc/src/test/org/apache/lucene/misc/TestHighFreqTerms.java
===================================================================
--- lucene/contrib/misc/src/test/org/apache/lucene/misc/TestHighFreqTerms.java	(revision 995772)
+++ lucene/contrib/misc/src/test/org/apache/lucene/misc/TestHighFreqTerms.java	(working copy)
@@ -189,7 +189,7 @@
   }
   /********************Testing Utils**********************************/
     
-  private static void indexDocs(IndexWriter writer) throws Exception {
+  private void indexDocs(IndexWriter writer) throws Exception {
 
     /**
      * Generate 10 documents where term n  has a docFreq of n and a totalTermFreq of n*2 (squared). 
@@ -198,9 +198,9 @@
       Document doc = new Document();
       String content = getContent(i);
     
-      doc.add(new Field("FIELD_1", content, Field.Store.YES,Field.Index.ANALYZED, Field.TermVector.NO));
+      doc.add(newField("FIELD_1", content, Field.Store.YES,Field.Index.ANALYZED, Field.TermVector.NO));
       //add a different field
-      doc.add(new Field("different_field", "diff", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));
+      doc.add(newField("different_field", "diff", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));
       writer.addDocument(doc);
     }
     
@@ -208,7 +208,7 @@
     //highest freq terms for a specific field.
     for (int i = 1; i <= 10; i++) {
       Document doc = new Document();
-      doc.add(new Field("different_field", "diff", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));
+      doc.add(newField("different_field", "diff", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));
       writer.addDocument(doc);
     }
     // add some docs where tf < df so we can see if sorting works
@@ -219,7 +219,7 @@
     for (int i = 0; i < highTF; i++) {
       content += "highTF ";
     }
-    doc.add(new Field("FIELD_1", content, Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));
+    doc.add(newField("FIELD_1", content, Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));
     writer.addDocument(doc);
     // highTF medium df =5
     int medium_df = 5;
@@ -230,7 +230,7 @@
       for (int j = 0; j < tf; j++) {
         newcontent += "highTFmedDF ";
       }
-      newdoc.add(new Field("FIELD_1", newcontent, Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));
+      newdoc.add(newField("FIELD_1", newcontent, Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));
       writer.addDocument(newdoc);
     }
     // add a doc with high tf in field different_field
@@ -240,7 +240,7 @@
     for (int i = 0; i < targetTF; i++) {
       content += "TF150 ";
     }
-    doc.add(new Field("different_field", content, Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));
+    doc.add(newField("different_field", content, Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));
     writer.addDocument(doc);
     writer.close();
     
Index: lucene/contrib/misc/src/test/org/apache/lucene/misc/TestLengthNormModifier.java
===================================================================
--- lucene/contrib/misc/src/test/org/apache/lucene/misc/TestLengthNormModifier.java	(revision 995772)
+++ lucene/contrib/misc/src/test/org/apache/lucene/misc/TestLengthNormModifier.java	(working copy)
@@ -65,15 +65,15 @@
 	
 	for (int i = 0; i < NUM_DOCS; i++) {
 	    Document d = new Document();
-	    d.add(new Field("field", "word",
+	    d.add(newField("field", "word",
 			    Field.Store.YES, Field.Index.ANALYZED));
-	    d.add(new Field("nonorm", "word",
+	    d.add(newField("nonorm", "word",
 			    Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
 		
 	    for (int j = 1; j <= i; j++) {
-		d.add(new Field("field", "crap",
+		d.add(newField("field", "crap",
 				Field.Store.YES, Field.Index.ANALYZED));
-		d.add(new Field("nonorm", "more words",
+		d.add(newField("nonorm", "more words",
 				Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
 	    }
 	    writer.addDocument(d);
Index: lucene/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestCartesian.java
===================================================================
--- lucene/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestCartesian.java	(revision 995772)
+++ lucene/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestCartesian.java	(working copy)
@@ -98,14 +98,14 @@
     
     Document doc = new Document();
     
-    doc.add(new Field("name", name,Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("name", name,Field.Store.YES, Field.Index.ANALYZED));
     
     // convert the lat / long to lucene fields
     doc.add(new NumericField(latField, Integer.MAX_VALUE, Field.Store.YES, true).setDoubleValue(lat));
     doc.add(new NumericField(lngField, Integer.MAX_VALUE, Field.Store.YES, true).setDoubleValue(lng));
     
     // add a default meta field to make searching all documents easy 
-    doc.add(new Field("metafile", "doc",Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("metafile", "doc",Field.Store.YES, Field.Index.ANALYZED));
     
     int ctpsize = ctps.size();
     for (int i =0; i < ctpsize; i++){
@@ -114,7 +114,7 @@
           Field.Store.YES, 
           true).setDoubleValue(ctp.getTierBoxId(lat,lng)));
       
-      doc.add(new Field(geoHashPrefix, GeoHashUtils.encode(lat,lng), 
+      doc.add(newField(geoHashPrefix, GeoHashUtils.encode(lat,lng), 
     		  Field.Store.YES, 
     		  Field.Index.NOT_ANALYZED_NO_NORMS));
     }
Index: lucene/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java
===================================================================
--- lucene/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java	(revision 995772)
+++ lucene/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java	(working copy)
@@ -60,14 +60,14 @@
     
     Document doc = new Document();
     
-    doc.add(new Field("name", name,Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("name", name,Field.Store.YES, Field.Index.ANALYZED));
     
     // convert the lat / long to lucene fields
     doc.add(new NumericField(latField, Integer.MAX_VALUE, Field.Store.YES, true).setDoubleValue(lat));
     doc.add(new NumericField(lngField, Integer.MAX_VALUE,Field.Store.YES, true).setDoubleValue(lng));
     
     // add a default meta field to make searching all documents easy 
-    doc.add(new Field("metafile", "doc",Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("metafile", "doc",Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
     
   }
Index: lucene/contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestParser.java
===================================================================
--- lucene/contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestParser.java	(revision 995772)
+++ lucene/contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestParser.java	(working copy)
@@ -66,8 +66,8 @@
 				String date=line.substring(0,endOfDate).trim();
 				String content=line.substring(endOfDate).trim();
 				org.apache.lucene.document.Document doc =new org.apache.lucene.document.Document();
-				doc.add(new Field("date",date,Field.Store.YES,Field.Index.ANALYZED));
-				doc.add(new Field("contents",content,Field.Store.YES,Field.Index.ANALYZED));
+				doc.add(newField("date",date,Field.Store.YES,Field.Index.ANALYZED));
+				doc.add(newField("contents",content,Field.Store.YES,Field.Index.ANALYZED));
 				NumericField numericField = new NumericField("date2");
 				numericField.setIntValue(Integer.valueOf(date));
 				doc.add(numericField);
Index: lucene/contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestQueryTemplateManager.java
===================================================================
--- lucene/contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestQueryTemplateManager.java	(revision 995772)
+++ lucene/contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestQueryTemplateManager.java	(working copy)
@@ -126,7 +126,7 @@
 			if(st.hasMoreTokens())
 			{
 				String value=st.nextToken().trim();
-				result.add(new Field(name,value,Field.Store.YES,Field.Index.ANALYZED));
+				result.add(newField(name,value,Field.Store.YES,Field.Index.ANALYZED));
 			}
 		}
 		return result;
Index: lucene/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java
===================================================================
--- lucene/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java	(revision 995772)
+++ lucene/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java	(working copy)
@@ -51,11 +51,11 @@
     Document doc;
 
     doc = new  Document();
-    doc.add(new Field("aaa", "foo", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("aaa", "foo", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
 
     doc = new  Document();
-    doc.add(new Field("aaa", "foo", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("aaa", "foo", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
 
     doc = new  Document();
@@ -67,7 +67,7 @@
     writer.addDocument(doc);
 
     doc = new Document();
-    doc.add(new Field("zzz", "bar", Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("zzz", "bar", Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
 
     writer.optimize();
Index: lucene/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
===================================================================
--- lucene/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java	(revision 995772)
+++ lucene/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java	(working copy)
@@ -58,9 +58,9 @@
 
     for (int i = 0; i < 1000; i++) {
       Document doc = new Document();
-      doc.add(new Field("field1", English.intToEnglish(i), Field.Store.YES, Field.Index.ANALYZED));
-      doc.add(new Field("field2", English.intToEnglish(i + 1), Field.Store.YES, Field.Index.ANALYZED)); // + word thousand
-      doc.add(new Field("field3", "fvei" + (i % 2 == 0 ? " five" : ""), Field.Store.YES, Field.Index.ANALYZED)); // + word thousand
+      doc.add(newField("field1", English.intToEnglish(i), Field.Store.YES, Field.Index.ANALYZED));
+      doc.add(newField("field2", English.intToEnglish(i + 1), Field.Store.YES, Field.Index.ANALYZED)); // + word thousand
+      doc.add(newField("field3", "fvei" + (i % 2 == 0 ? " five" : ""), Field.Store.YES, Field.Index.ANALYZED)); // + word thousand
       writer.addDocument(doc);
     }
     writer.close();
Index: lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
===================================================================
--- lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java	(revision 995772)
+++ lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java	(working copy)
@@ -108,8 +108,8 @@
     IndexWriter writer = new IndexWriter(ramdir,
                                          new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setCodecProvider(_TestUtil.alwaysCodec("Standard")));
     Document doc = new Document();
-    Field field1 = new Field("foo", fooField.toString(), Field.Store.NO, Field.Index.ANALYZED);
-    Field field2 = new Field("term", termField.toString(), Field.Store.NO, Field.Index.ANALYZED);
+    Field field1 = newField("foo", fooField.toString(), Field.Store.NO, Field.Index.ANALYZED);
+    Field field2 = newField("term", termField.toString(), Field.Store.NO, Field.Index.ANALYZED);
     doc.add(field1);
     doc.add(field2);
     writer.addDocument(doc);
Index: lucene/contrib/queries/src/test/org/apache/lucene/search/similar/TestMoreLikeThis.java
===================================================================
--- lucene/contrib/queries/src/test/org/apache/lucene/search/similar/TestMoreLikeThis.java	(revision 995772)
+++ lucene/contrib/queries/src/test/org/apache/lucene/search/similar/TestMoreLikeThis.java	(working copy)
@@ -66,7 +66,7 @@
   
   private void addDoc(RandomIndexWriter writer, String text) throws IOException {
     Document doc = new Document();
-    doc.add(new Field("text", text, Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(newField("text", text, Field.Store.YES, Field.Index.ANALYZED));
     writer.addDocument(doc);
   }
   
Index: lucene/contrib/queries/src/test/org/apache/lucene/search/DuplicateFilterTest.java
===================================================================
--- lucene/contrib/queries/src/test/org/apache/lucene/search/DuplicateFilterTest.java	(revision 995772)
+++ lucene/contrib/queries/src/test/org/apache/lucene/search/DuplicateFilterTest.java	(working copy)
@@ -75,9 +75,9 @@
 	private void addDoc(RandomIndexWriter writer, String url, String text, String date) throws IOException
 	{
 		Document doc=new Document();
-		doc.add(new Field(KEY_FIELD,url,Field.Store.YES,Field.Index.NOT_ANALYZED));
-		doc.add(new Field("text",text,Field.Store.YES,Field.Index.ANALYZED));
-		doc.add(new Field("date",date,Field.Store.YES,Field.Index.ANALYZED));
+		doc.add(newField(KEY_FIELD,url,Field.Store.YES,Field.Index.NOT_ANALYZED));
+		doc.add(newField("text",text,Field.Store.YES,Field.Index.ANALYZED));
+		doc.add(newField("date",date,Field.Store.YES,Field.Index.ANALYZED));
 		writer.addDocument(doc);
 	}
 		
Index: lucene/contrib/queries/src/test/org/apache/lucene/search/TermsFilterTest.java
===================================================================
--- lucene/contrib/queries/src/test/org/apache/lucene/search/TermsFilterTest.java	(revision 995772)
+++ lucene/contrib/queries/src/test/org/apache/lucene/search/TermsFilterTest.java	(working copy)
@@ -57,7 +57,7 @@
 		for (int i = 0; i < 100; i++) {
 			Document doc=new Document();
 			int term=i*10; //terms are units of 10;
-			doc.add(new Field(fieldName,""+term,Field.Store.YES,Field.Index.NOT_ANALYZED));
+			doc.add(newField(fieldName,""+term,Field.Store.YES,Field.Index.NOT_ANALYZED));
 			w.addDocument(doc);			
 		}
 		IndexReader mainReader = w.getReader();
Index: lucene/contrib/queries/src/test/org/apache/lucene/search/ChainedFilterTest.java
===================================================================
--- lucene/contrib/queries/src/test/org/apache/lucene/search/ChainedFilterTest.java	(revision 995772)
+++ lucene/contrib/queries/src/test/org/apache/lucene/search/ChainedFilterTest.java	(working copy)
@@ -63,9 +63,9 @@
 
     for (int i = 0; i < MAX; i++) {
       Document doc = new Document();
-      doc.add(new Field("key", "" + (i + 1), Field.Store.YES, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("owner", (i < MAX / 2) ? "bob" : "sue", Field.Store.YES, Field.Index.NOT_ANALYZED));
-      doc.add(new Field("date", cal.getTime().toString(), Field.Store.YES, Field.Index.NOT_ANALYZED));
+      doc.add(newField("key", "" + (i + 1), Field.Store.YES, Field.Index.NOT_ANALYZED));
+      doc.add(newField("owner", (i < MAX / 2) ? "bob" : "sue", Field.Store.YES, Field.Index.NOT_ANALYZED));
+      doc.add(newField("date", cal.getTime().toString(), Field.Store.YES, Field.Index.NOT_ANALYZED));
       writer.addDocument(doc);
 
       cal.add(Calendar.DATE, 1);
Index: lucene/contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java
===================================================================
--- lucene/contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java	(revision 995772)
+++ lucene/contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java	(working copy)
@@ -62,10 +62,10 @@
 	private void addDoc(RandomIndexWriter writer, String accessRights, String price, String date, String inStock) throws IOException
 	{
 		Document doc=new Document();
-		doc.add(new Field("accessRights",accessRights,Field.Store.YES,Field.Index.ANALYZED));
-		doc.add(new Field("price",price,Field.Store.YES,Field.Index.ANALYZED));
-		doc.add(new Field("date",date,Field.Store.YES,Field.Index.ANALYZED));
-		doc.add(new Field("inStock",inStock,Field.Store.YES,Field.Index.ANALYZED));
+		doc.add(newField("accessRights",accessRights,Field.Store.YES,Field.Index.ANALYZED));
+		doc.add(newField("price",price,Field.Store.YES,Field.Index.ANALYZED));
+		doc.add(newField("date",date,Field.Store.YES,Field.Index.ANALYZED));
+		doc.add(newField("inStock",inStock,Field.Store.YES,Field.Index.ANALYZED));
 		writer.addDocument(doc);
 	}
 	
Index: lucene/contrib/queries/src/test/org/apache/lucene/search/regex/TestRegexQuery.java
===================================================================
--- lucene/contrib/queries/src/test/org/apache/lucene/search/regex/TestRegexQuery.java	(revision 995772)
+++ lucene/contrib/queries/src/test/org/apache/lucene/search/regex/TestRegexQuery.java	(working copy)
@@ -43,7 +43,7 @@
     directory = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random, directory);
     Document doc = new Document();
-    doc.add(new Field(FN, "the quick brown fox jumps over the lazy dog", Field.Store.NO, Field.Index.ANALYZED));
+    doc.add(newField(FN, "the quick brown fox jumps over the lazy dog", Field.Store.NO, Field.Index.ANALYZED));
     writer.addDocument(doc);
     reader = writer.getReader();
     writer.close();
Index: lucene/contrib/queries/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java
===================================================================
--- lucene/contrib/queries/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java	(revision 995772)
+++ lucene/contrib/queries/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java	(working copy)
@@ -59,15 +59,15 @@
     IndexWriter writer = new IndexWriter(directory, newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer()));
     Document doc = new Document();
-    // doc.add(new Field("field", "the quick brown fox jumps over the lazy dog",
+    // doc.add(newField("field", "the quick brown fox jumps over the lazy dog",
     // Field.Store.NO, Field.Index.ANALYZED));
     // writer.addDocument(doc);
     // doc = new Document();
-    doc.add(new Field("field", "auto update", Field.Store.NO,
+    doc.add(newField("field", "auto update", Field.Store.NO,
         Field.Index.ANALYZED));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new Field("field", "first auto update", Field.Store.NO,
+    doc.add(newField("field", "first auto update", Field.Store.NO,
         Field.Index.ANALYZED));
     writer.addDocument(doc);
     writer.optimize();
@@ -115,12 +115,12 @@
       LockObtainFailedException, IOException {
     // creating a document to store
     Document lDoc = new Document();
-    lDoc.add(new Field("field", "a1 b1", Field.Store.NO,
+    lDoc.add(newField("field", "a1 b1", Field.Store.NO,
         Field.Index.ANALYZED_NO_NORMS));
 
     // creating a document to store
     Document lDoc2 = new Document();
-    lDoc2.add(new Field("field", "a2 b2", Field.Store.NO,
+    lDoc2.add(newField("field", "a2 b2", Field.Store.NO,
         Field.Index.ANALYZED_NO_NORMS));
 
     // creating first index writer
Index: lucene/contrib/queries/src/test/org/apache/lucene/search/FuzzyLikeThisQueryTest.java
===================================================================
--- lucene/contrib/queries/src/test/org/apache/lucene/search/FuzzyLikeThisQueryTest.java	(revision 995772)
+++ lucene/contrib/queries/src/test/org/apache/lucene/search/FuzzyLikeThisQueryTest.java	(working copy)
@@ -65,8 +65,8 @@
 	private void addDoc(RandomIndexWriter writer, String name, String id) throws IOException
 	{
 		Document doc=new Document();
-		doc.add(new Field("name",name,Field.Store.YES,Field.Index.ANALYZED));
-		doc.add(new Field("id",id,Field.Store.YES,Field.Index.ANALYZED));
+		doc.add(newField("name",name,Field.Store.YES,Field.Index.ANALYZED));
+		doc.add(newField("id",id,Field.Store.YES,Field.Index.ANALYZED));
 		writer.addDocument(doc);
 	}
 	
