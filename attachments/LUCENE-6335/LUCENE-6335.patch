Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilterFactory.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilterFactory.java	(revision 1663853)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilterFactory.java	(working copy)
@@ -138,7 +138,7 @@
       };
     }
 
-    try {
+    try (Analyzer a = analyzer) {
       String formatClass = format;
       if (format == null || format.equals("solr")) {
         formatClass = SolrSynonymParser.class.getName();
@@ -146,7 +146,7 @@
         formatClass = WordnetSynonymParser.class.getName();
       }
       // TODO: expose dedup as a parameter?
-      map = loadSynonyms(loader, formatClass, true, analyzer);
+      map = loadSynonyms(loader, formatClass, true, a);
     } catch (ParseException e) {
       throw new IOException("Error parsing synonyms file:", e);
     }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java	(working copy)
@@ -31,7 +31,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new ArabicAnalyzer();
+    new ArabicAnalyzer().close();
   }
   
   /**
@@ -53,6 +53,7 @@
     
     assertAnalyzesTo(a, "ما ملكت أيمانكم", new String[] { "ملكت", "ايمانكم"});
     assertAnalyzesTo(a, "الذين ملكت أيمانكم", new String[] { "ملكت", "ايمانكم" }); // stopwords
+    a.close();
   }
   
   /**
@@ -62,6 +63,7 @@
     ArabicAnalyzer a = new ArabicAnalyzer();
     assertAnalyzesTo(a, "كبير", new String[] { "كبير" });
     assertAnalyzesTo(a, "كبيرة", new String[] { "كبير" }); // feminine marker
+    a.close();
   }
 
   /**
@@ -68,8 +70,10 @@
    * Non-arabic text gets treated in a similar way as SimpleAnalyzer.
    */
   public void testEnglishInput() throws Exception {
-    assertAnalyzesTo(new ArabicAnalyzer(), "English text.", new String[] {
+    ArabicAnalyzer a = new ArabicAnalyzer();
+    assertAnalyzesTo(a, "English text.", new String[] {
         "english", "text" });
+    a.close();
   }
   
   /**
@@ -80,6 +84,7 @@
     ArabicAnalyzer a = new ArabicAnalyzer(set);
     assertAnalyzesTo(a, "The quick brown fox.", new String[] { "quick",
         "brown", "fox" });
+    a.close();
   }
   
   public void testWithStemExclusionSet() throws IOException {
@@ -87,15 +92,18 @@
     ArabicAnalyzer a = new ArabicAnalyzer(CharArraySet.EMPTY_SET, set);
     assertAnalyzesTo(a, "كبيرة the quick ساهدهات", new String[] { "كبير","the", "quick", "ساهدهات" });
     assertAnalyzesTo(a, "كبيرة the quick ساهدهات", new String[] { "كبير","the", "quick", "ساهدهات" });
-
+    a.close();
     
     a = new ArabicAnalyzer(CharArraySet.EMPTY_SET, CharArraySet.EMPTY_SET);
     assertAnalyzesTo(a, "كبيرة the quick ساهدهات", new String[] { "كبير","the", "quick", "ساهد" });
     assertAnalyzesTo(a, "كبيرة the quick ساهدهات", new String[] { "كبير","the", "quick", "ساهد" });
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new ArabicAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    ArabicAnalyzer a = new ArabicAnalyzer();
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java	(working copy)
@@ -104,6 +104,7 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java	(working copy)
@@ -141,5 +141,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianAnalyzer.java	(working copy)
@@ -32,12 +32,13 @@
    * This test fails with NPE when the stopwords file is missing in classpath
    */
   public void testResourcesAvailable() {
-    new BulgarianAnalyzer();
+    new BulgarianAnalyzer().close();
   }
   
   public void testStopwords() throws IOException {
     Analyzer a = new BulgarianAnalyzer();
     assertAnalyzesTo(a, "Как се казваш?", new String[] {"казваш"});
+    a.close();
   }
   
   public void testCustomStopwords() throws IOException {
@@ -44,6 +45,7 @@
     Analyzer a = new BulgarianAnalyzer(CharArraySet.EMPTY_SET);
     assertAnalyzesTo(a, "Как се казваш?", 
         new String[] {"как", "се", "казваш"});
+    a.close();
   }
   
   public void testReusableTokenStream() throws IOException {
@@ -50,6 +52,7 @@
     Analyzer a = new BulgarianAnalyzer();
     assertAnalyzesTo(a, "документи", new String[] {"документ"});
     assertAnalyzesTo(a, "документ", new String[] {"документ"});
+    a.close();
   }
   
   /**
@@ -64,6 +67,7 @@
     assertAnalyzesTo(a, "компютър", new String[] {"компютр"});
     
     assertAnalyzesTo(a, "градове", new String[] {"град"});
+    a.close();
   }
   
   public void testWithStemExclusionSet() throws IOException {
@@ -71,10 +75,13 @@
     set.add("строеве");
     Analyzer a = new BulgarianAnalyzer(CharArraySet.EMPTY_SET, set);
     assertAnalyzesTo(a, "строевете строеве", new String[] { "строй", "строеве" });
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new BulgarianAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    BulgarianAnalyzer a = new BulgarianAnalyzer();
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java	(working copy)
@@ -97,6 +97,8 @@
     assertAnalyzesTo(a, "братя", new String[] {"брат"});
     assertAnalyzesTo(a, "братята", new String[] {"брат"});
     assertAnalyzesTo(a, "брате", new String[] {"брат"});
+    
+    a.close();
   }
   
   /**
@@ -109,6 +111,8 @@
     assertAnalyzesTo(a, "вестта", new String[] {"вест"});
     assertAnalyzesTo(a, "вести", new String[] {"вест"});
     assertAnalyzesTo(a, "вестите", new String[] {"вест"});
+    
+    a.close();
   }
   
   /**
@@ -138,6 +142,8 @@
     assertAnalyzesTo(a, "изключенията", new String[] {"изключени"});
     /* note the below form in this example does not conflate with the rest */
     assertAnalyzesTo(a, "изключения", new String[] {"изключн"});
+    
+    a.close();
   }
   
   /**
@@ -154,6 +160,7 @@
     assertAnalyzesTo(a, "красивото", new String[] {"красив"});
     assertAnalyzesTo(a, "красиви", new String[] {"красив"});
     assertAnalyzesTo(a, "красивите", new String[] {"красив"});
+    a.close();
   }
   
   /**
@@ -212,6 +219,8 @@
     /* note the below forms conflate with each other, but not the rest */
     assertAnalyzesTo(a, "строя", new String[] {"стр"});
     assertAnalyzesTo(a, "строят", new String[] {"стр"});
+    
+    a.close();
   }
 
   public void testWithKeywordAttribute() throws IOException {
@@ -234,5 +243,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianAnalyzer.java	(working copy)
@@ -135,6 +135,7 @@
     checkReuse(a, "boainain", "boainain");
     checkReuse(a, "boas", "boas");
     checkReuse(a, "bôas", "boas"); // removes diacritic: different from snowball portugese
+    a.close();
   }
  
   public void testStemExclusionTable() throws Exception {
@@ -141,6 +142,7 @@
     BrazilianAnalyzer a = new BrazilianAnalyzer(
         CharArraySet.EMPTY_SET, new CharArraySet(asSet("quintessência"), false));
     checkReuse(a, "quintessência", "quintessência"); // excluded words will be completely unchanged.
+    a.close();
   }
   
   public void testWithKeywordAttribute() throws IOException {
@@ -154,7 +156,9 @@
   }
 
   private void check(final String input, final String expected) throws Exception {
-    checkOneTerm(new BrazilianAnalyzer(), input, expected);
+    BrazilianAnalyzer a = new BrazilianAnalyzer();
+    checkOneTerm(a, input, expected);
+    a.close();
   }
   
   private void checkReuse(Analyzer a, String input, String expected) throws Exception {
@@ -163,7 +167,9 @@
 
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new BrazilianAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    BrazilianAnalyzer a = new BrazilianAnalyzer();
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
   
   public void testEmptyTerm() throws IOException {
@@ -175,5 +181,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ca/TestCatalanAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ca/TestCatalanAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ca/TestCatalanAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new CatalanAnalyzer();
+    new CatalanAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -38,6 +38,7 @@
     checkOneTerm(a, "llengua", "llengu");
     // stopword
     assertAnalyzesTo(a, "un", new String[] { });
+    a.close();
   }
   
   /** test use of elisionfilter */
@@ -45,6 +46,7 @@
     Analyzer a = new CatalanAnalyzer();
     assertAnalyzesTo(a, "Diccionari de l'Institut d'Estudis Catalans",
         new String[] { "diccion", "inst", "estud", "catalan" });
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -53,10 +55,13 @@
     Analyzer a = new CatalanAnalyzer(CatalanAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "llengües", "llengües");
     checkOneTerm(a, "llengua", "llengu");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new CatalanAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    CatalanAnalyzer a = new CatalanAnalyzer();
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/HTMLStripCharFilterTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/HTMLStripCharFilterTest.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/HTMLStripCharFilterTest.java	(working copy)
@@ -25,7 +25,6 @@
 import java.nio.charset.StandardCharsets;
 import java.util.Arrays;
 import java.util.HashSet;
-import java.util.List;
 import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
@@ -403,16 +402,22 @@
 
   public void testRandom() throws Exception {
     int numRounds = RANDOM_MULTIPLIER * 1000;
-    checkRandomData(random(), newTestAnalyzer(), numRounds);
+    Analyzer a = newTestAnalyzer();
+    checkRandomData(random(), a, numRounds);
+    a.close();
   }
   
   public void testRandomHugeStrings() throws Exception {
     int numRounds = RANDOM_MULTIPLIER * 100;
-    checkRandomData(random(), newTestAnalyzer(), numRounds, 8192);
+    Analyzer a = newTestAnalyzer();
+    checkRandomData(random(), a, numRounds, 8192);
+    a.close();
   }
 
   public void testCloseBR() throws Exception {
-    checkAnalysisConsistency(random(), newTestAnalyzer(), random().nextBoolean(), " Secretary)</br> [[M");
+    Analyzer a = newTestAnalyzer();
+    checkAnalysisConsistency(random(), a, random().nextBoolean(), " Secretary)</br> [[M");
+    a.close();
   }
   
   public void testServerSideIncludes() throws Exception {
@@ -549,7 +554,9 @@
   public void testRandomBrokenHTML() throws Exception {
     int maxNumElements = 10000;
     String text = TestUtil.randomHtmlishString(random(), maxNumElements);
-    checkAnalysisConsistency(random(), newTestAnalyzer(), random().nextBoolean(), text);
+    Analyzer a = newTestAnalyzer();
+    checkAnalysisConsistency(random(), a, random().nextBoolean(), text);
+    a.close();
   }
 
   public void testRandomText() throws Exception {
@@ -617,6 +624,7 @@
     assertAnalyzesTo(analyzer, " &#57209;", new String[] { "\uFFFD" } );
     assertAnalyzesTo(analyzer, " &#57209", new String[] { "\uFFFD" } );
     assertAnalyzesTo(analyzer, " &#57209<br>", new String[] { "&#57209" } );
+    analyzer.close();
   }
 
 
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter.java	(working copy)
@@ -216,6 +216,7 @@
     
     int numRounds = RANDOM_MULTIPLIER * 10000;
     checkRandomData(random(), analyzer, numRounds);
+    analyzer.close();
   }
 
   //@Ignore("wrong finalOffset: https://issues.apache.org/jira/browse/LUCENE-3971")
@@ -242,6 +243,7 @@
     
     String text = "gzw f quaxot";
     checkAnalysisConsistency(random(), analyzer, false, text);
+    analyzer.close();
   }
   
   //@Ignore("wrong finalOffset: https://issues.apache.org/jira/browse/LUCENE-3971")
@@ -263,6 +265,7 @@
       };
       int numRounds = 100;
       checkRandomData(random(), analyzer, numRounds);
+      analyzer.close();
     }
   }
   
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKAnalyzer.java	(working copy)
@@ -19,7 +19,6 @@
 
 import java.io.IOException;
 import java.io.Reader;
-import java.util.Random;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -39,8 +38,20 @@
  * Most tests adopted from TestCJKTokenizer
  */
 public class TestCJKAnalyzer extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new CJKAnalyzer();
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new CJKAnalyzer();
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   public void testJa1() throws IOException {
     assertAnalyzesTo(analyzer, "一二三四五六七八九十",
       new String[] { "一二", "二三", "三四", "四五", "五六", "六七", "七八", "八九", "九十" },
@@ -228,6 +239,8 @@
     // before bigramming, the 4 tokens look like:
     //   { 0, 0, 1, 1 },
     //   { 0, 1, 1, 2 }
+    
+    analyzer.close();
   }
 
   private static class FakeStandardTokenizer extends TokenFilter {
@@ -267,17 +280,21 @@
         new int[] { 1 },
         new String[] { "<SINGLE>" },
         new int[] { 1 });
+    analyzer.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new CJKAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer a = new CJKAnalyzer();
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomHugeStrings() throws Exception {
-    Random random = random();
-    checkRandomData(random, new CJKAnalyzer(), 100*RANDOM_MULTIPLIER, 8192);
+    Analyzer a = new CJKAnalyzer();
+    checkRandomData(random(), a, 100*RANDOM_MULTIPLIER, 8192);
+    a.close();
   }
   
   public void testEmptyTerm() throws IOException {
@@ -289,5 +306,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKBigramFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKBigramFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKBigramFilter.java	(working copy)
@@ -17,7 +17,6 @@
  * limitations under the License.
  */
 
-import java.io.Reader;
 import java.util.Random;
 
 import org.apache.lucene.analysis.Analyzer;
@@ -24,25 +23,37 @@
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.util.IOUtils;
 
 public class TestCJKBigramFilter extends BaseTokenStreamTestCase {
-  Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer t = new StandardTokenizer();
-      return new TokenStreamComponents(t, new CJKBigramFilter(t));
-    }
-  };
+  Analyzer analyzer, unibiAnalyzer;
   
-  Analyzer unibiAnalyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer t = new StandardTokenizer();
-      return new TokenStreamComponents(t, 
-          new CJKBigramFilter(t, 0xff, true));
-    }
-  };
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer t = new StandardTokenizer();
+        return new TokenStreamComponents(t, new CJKBigramFilter(t));
+      }
+    };
+    unibiAnalyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer t = new StandardTokenizer();
+        return new TokenStreamComponents(t, 
+            new CJKBigramFilter(t, 0xff, true));
+      }
+    };
+  }
   
+  @Override
+  public void tearDown() throws Exception {
+    IOUtils.close(analyzer, unibiAnalyzer);
+    super.tearDown();
+  }
+  
   public void testHuge() throws Exception {
     assertAnalyzesTo(analyzer, "多くの学生が試験に落ちた" + "多くの学生が試験に落ちた" + "多くの学生が試験に落ちた"
      + "多くの学生が試験に落ちた" + "多くの学生が試験に落ちた" + "多くの学生が試験に落ちた" + "多くの学生が試験に落ちた"
@@ -79,6 +90,7 @@
                        "<HIRAGANA>", "<SINGLE>", "<HIRAGANA>", "<HIRAGANA>", "<SINGLE>" },
         new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 },
         new int[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 });
+    a.close();
   }
   
   public void testAllScripts() throws Exception {
@@ -92,6 +104,7 @@
     };
     assertAnalyzesTo(a, "多くの学生が試験に落ちた。",
         new String[] { "多く", "くの", "の学", "学生", "生が", "が試", "試験", "験に", "に落", "落ち", "ちた" });
+    a.close();
   }
   
   public void testUnigramsAndBigramsAllScripts() throws Exception {
@@ -132,6 +145,7 @@
                        "<HIRAGANA>", "<SINGLE>", "<HIRAGANA>", "<HIRAGANA>", "<SINGLE>" },
         new int[] { 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1 },
         new int[] { 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1 });
+    a.close();
   }
   
   public void testUnigramsAndBigramsHuge() throws Exception {
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKWidthFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKWidthFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKWidthFilter.java	(working copy)
@@ -29,14 +29,26 @@
  * Tests for {@link CJKWidthFilter}
  */
 public class TestCJKWidthFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(source, new CJKWidthFilter(source));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(source, new CJKWidthFilter(source));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /**
    * Full-width ASCII forms normalized to half-width (basic latin)
    */
@@ -74,5 +86,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ckb/TestSoraniAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ckb/TestSoraniAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ckb/TestSoraniAnalyzer.java	(working copy)
@@ -32,12 +32,13 @@
    * This test fails with NPE when the stopwords file is missing in classpath
    */
   public void testResourcesAvailable() {
-    new SoraniAnalyzer();
+    new SoraniAnalyzer().close();
   }
   
   public void testStopwords() throws IOException {
     Analyzer a = new SoraniAnalyzer();
     assertAnalyzesTo(a, "ئەم پیاوە", new String[] {"پیاو"});
+    a.close();
   }
   
   public void testCustomStopwords() throws IOException {
@@ -44,6 +45,7 @@
     Analyzer a = new SoraniAnalyzer(CharArraySet.EMPTY_SET);
     assertAnalyzesTo(a, "ئەم پیاوە", 
         new String[] {"ئەم", "پیاو"});
+    a.close();
   }
   
   public void testReusableTokenStream() throws IOException {
@@ -50,6 +52,7 @@
     Analyzer a = new SoraniAnalyzer();
     assertAnalyzesTo(a, "پیاوە", new String[] {"پیاو"});
     assertAnalyzesTo(a, "پیاو", new String[] {"پیاو"});
+    a.close();
   }
   
   public void testWithStemExclusionSet() throws IOException {
@@ -57,10 +60,13 @@
     set.add("پیاوە");
     Analyzer a = new SoraniAnalyzer(CharArraySet.EMPTY_SET, set);
     assertAnalyzesTo(a, "پیاوە", new String[] { "پیاوە" });
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new SoraniAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer a = new SoraniAnalyzer();
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ckb/TestSoraniNormalizationFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ckb/TestSoraniNormalizationFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ckb/TestSoraniNormalizationFilter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -30,14 +29,26 @@
  * Tests normalization for Sorani (this is more critical than stemming...)
  */
 public class TestSoraniNormalizationFilter extends BaseTokenStreamTestCase {
-  Analyzer a = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new MockTokenizer(MockTokenizer.KEYWORD, false);
-      return new TokenStreamComponents(tokenizer, new SoraniNormalizationFilter(tokenizer));
-    }
-  };
+  Analyzer a;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.KEYWORD, false);
+        return new TokenStreamComponents(tokenizer, new SoraniNormalizationFilter(tokenizer));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    a.close();
+    super.tearDown();
+  }
+  
   public void testY() throws Exception {
     checkOneTerm(a, "\u064A", "\u06CC");
     checkOneTerm(a, "\u0649", "\u06CC");
@@ -96,5 +107,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ckb/TestSoraniStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ckb/TestSoraniStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ckb/TestSoraniStemFilter.java	(working copy)
@@ -20,7 +20,6 @@
 import static org.apache.lucene.analysis.VocabularyAssert.assertVocabulary;
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -31,8 +30,20 @@
  * Test the Sorani Stemmer.
  */
 public class TestSoraniStemFilter extends BaseTokenStreamTestCase {
-  SoraniAnalyzer a = new SoraniAnalyzer();
+  Analyzer a;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    a = new SoraniAnalyzer();
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    a.close();
+    super.tearDown();
+  }
+  
   public void testIndefiniteSingular() throws Exception {
     checkOneTerm(a, "پیاوێک", "پیاو"); // -ek
     checkOneTerm(a, "دەرگایەک", "دەرگا"); // -yek
@@ -90,6 +101,7 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
   
   /** test against a basic vocabulary file */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/commongrams/CommonGramsFilterTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/commongrams/CommonGramsFilterTest.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/commongrams/CommonGramsFilterTest.java	(working copy)
@@ -156,6 +156,7 @@
         new String[] { "s_s", "s_s" });
     assertAnalyzesTo(a, "of the of", 
         new String[] { "of_the", "the_of" });
+    a.close();
   }
   
   public void testCommonGramsFilter() throws Exception {
@@ -242,6 +243,7 @@
     assertAnalyzesTo(a, "of the of", 
         new String[] { "of", "of_the", "the", "the_of", "of" }, 
         new int[] { 1, 0, 1, 0, 1 });
+    a.close();
   }
   
   /**
@@ -330,6 +332,7 @@
     };
     
     checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
     
     Analyzer b = new Analyzer() {
 
@@ -342,5 +345,6 @@
     };
     
     checkRandomData(random(), b, 1000*RANDOM_MULTIPLIER);
+    b.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java	(working copy)
@@ -336,6 +336,7 @@
         new String[] { "bankueberfall", "fall" },
         new int[] { 0,  0 },
         new int[] { 12, 12 });
+    analyzer.close();
   }
   
   /** blast some random strings through the analyzer */
@@ -350,6 +351,7 @@
       }
     };
     checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
     
     InputSource is = new InputSource(getClass().getResource("da_UTF8.xml").toExternalForm());
     final HyphenationTree hyphenator = HyphenationCompoundWordTokenFilter.getHyphenationTree(is);
@@ -363,6 +365,7 @@
       }
     };
     checkRandomData(random(), b, 1000*RANDOM_MULTIPLIER);
+    b.close();
   }
   
   public void testEmptyTerm() throws Exception {
@@ -376,6 +379,7 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
     
     InputSource is = new InputSource(getClass().getResource("da_UTF8.xml").toExternalForm());
     final HyphenationTree hyphenator = HyphenationCompoundWordTokenFilter.getHyphenationTree(is);
@@ -389,5 +393,6 @@
       }
     };
     checkOneTerm(b, "", "");
+    b.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestAnalyzers.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestAnalyzers.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestAnalyzers.java	(working copy)
@@ -18,15 +18,18 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 import java.io.StringReader;
-import java.util.Random;
 
-import org.apache.lucene.analysis.*;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 
 public class TestAnalyzers extends BaseTokenStreamTestCase {
 
@@ -48,6 +51,7 @@
                      new String[] { "b" });
     assertAnalyzesTo(a, "\"QUOTED\" word", 
                      new String[] { "quoted", "word" });
+    a.close();
   }
 
   public void testNull() throws Exception {
@@ -68,6 +72,7 @@
                      new String[] { "2B" });
     assertAnalyzesTo(a, "\"QUOTED\" word", 
                      new String[] { "\"QUOTED\"", "word" });
+    a.close();
   }
 
   public void testStop() throws Exception {
@@ -76,6 +81,7 @@
                      new String[] { "foo", "bar", "foo", "bar" });
     assertAnalyzesTo(a, "foo a bar such FOO THESE BAR", 
                      new String[] { "foo", "bar", "foo", "bar" });
+    a.close();
   }
 
   void verifyPayload(TokenStream ts) throws IOException {
@@ -159,6 +165,7 @@
     // unpaired trail surrogate
     assertAnalyzesTo(a, "AbaC\uDC16AdaBa", 
         new String [] { "abac\uDC16adaba" });
+    a.close();
   }
 
   /**
@@ -179,9 +186,9 @@
     // unpaired trail surrogate
     assertAnalyzesTo(a, "AbaC\uDC16AdaBa", 
         new String [] { "ABAC\uDC16ADABA" });
+    a.close();
   }
   
-  
   /**
    * Test that LowercaseFilter handles the lowercasing correctly if the term
    * buffer has a trailing surrogate character leftover and the current term in
@@ -223,17 +230,20 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new WhitespaceAnalyzer(), 1000*RANDOM_MULTIPLIER);
-    checkRandomData(random(), new SimpleAnalyzer(), 1000*RANDOM_MULTIPLIER);
-    checkRandomData(random(), new StopAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzers[] = new Analyzer[] { new WhitespaceAnalyzer(), new SimpleAnalyzer(), new StopAnalyzer() };
+    for (Analyzer analyzer : analyzers) {
+      checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    }
+    IOUtils.close(analyzers);
   }
   
   /** blast some random large strings through the analyzer */
   public void testRandomHugeStrings() throws Exception {
-    Random random = random();
-    checkRandomData(random, new WhitespaceAnalyzer(), 100*RANDOM_MULTIPLIER, 8192);
-    checkRandomData(random, new SimpleAnalyzer(), 100*RANDOM_MULTIPLIER, 8192);
-    checkRandomData(random, new StopAnalyzer(), 100*RANDOM_MULTIPLIER, 8192);
+    Analyzer analyzers[] = new Analyzer[] { new WhitespaceAnalyzer(), new SimpleAnalyzer(), new StopAnalyzer() };
+    for (Analyzer analyzer : analyzers) {
+      checkRandomData(random(), analyzer, 100*RANDOM_MULTIPLIER, 8192);
+    }
+    IOUtils.close(analyzers);
   } 
 }
 
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething.java	(working copy)
@@ -75,6 +75,7 @@
       }
     };
     checkAnalysisConsistency(random(), a, false, "wmgddzunizdomqyj");
+    a.close();
   }
   
   CharFilter wrappedStream = new CharFilter(new StringReader("bogus")) {
@@ -261,6 +262,7 @@
       }  
     };
     checkRandomData(random(), analyzer, 2000);
+    analyzer.close();
   }
   
   public void testCuriousWikipediaString() throws Exception {
@@ -285,5 +287,6 @@
       }  
     };
     checkAnalysisConsistency(random(), a, false, "B\u28c3\ue0f8[ \ud800\udfc2 </p> jb");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestDuelingAnalyzers.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestDuelingAnalyzers.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestDuelingAnalyzers.java	(working copy)
@@ -30,6 +30,7 @@
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.TestUtil;
 import org.apache.lucene.util.automaton.Operations;
 import org.apache.lucene.util.automaton.CharacterRunAutomaton;
@@ -78,6 +79,7 @@
       assertEquals(s, left.tokenStream("foo", newStringReader(s)), 
                    right.tokenStream("foo", newStringReader(s)));
     }
+    IOUtils.close(left, right);
   }
   
   // not so useful since it's all one token?!
@@ -99,6 +101,7 @@
       assertEquals(s, left.tokenStream("foo", newStringReader(s)), 
                    right.tokenStream("foo", newStringReader(s)));
     }
+    IOUtils.close(left, right);
   }
   
   public void testLetterHtmlish() throws Exception {
@@ -116,6 +119,7 @@
       assertEquals(s, left.tokenStream("foo", newStringReader(s)), 
                    right.tokenStream("foo", newStringReader(s)));
     }
+    IOUtils.close(left, right);
   }
   
   public void testLetterHtmlishHuge() throws Exception {
@@ -136,6 +140,7 @@
       assertEquals(s, left.tokenStream("foo", newStringReader(s)), 
                    right.tokenStream("foo", newStringReader(s)));
     }
+    IOUtils.close(left, right);
   }
   
   public void testLetterUnicode() throws Exception {
@@ -153,6 +158,7 @@
       assertEquals(s, left.tokenStream("foo", newStringReader(s)), 
                    right.tokenStream("foo", newStringReader(s)));
     }
+    IOUtils.close(left, right);
   }
   
   public void testLetterUnicodeHuge() throws Exception {
@@ -173,6 +179,7 @@
       assertEquals(s, left.tokenStream("foo", newStringReader(s)), 
                    right.tokenStream("foo", newStringReader(s)));
     }
+    IOUtils.close(left, right);
   }
   
   // we only check a few core attributes here.
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFactories.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFactories.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFactories.java	(working copy)
@@ -46,6 +46,8 @@
 // TODO: move this, TestRandomChains, and TestAllAnalyzersHaveFactories
 // to an integration test module that sucks in all analysis modules.
 // currently the only way to do this is via eclipse etc (LUCENE-3974)
+
+// TODO: fix this to use CustomAnalyzer instead of its own FactoryAnalyzer
 public class TestFactories extends BaseTokenStreamTestCase {
   public void test() throws IOException {
     for (String tokenizer : TokenizerFactory.availableTokenizers()) {
@@ -77,7 +79,9 @@
       
       // beast it just a little, it shouldnt throw exceptions:
       // (it should have thrown them in initialize)
-      checkRandomData(random(), new FactoryAnalyzer(factory, null, null), 20, 20, false, false);
+      Analyzer a = new FactoryAnalyzer(factory, null, null);
+      checkRandomData(random(), a, 20, 20, false, false);
+      a.close();
     }
   }
   
@@ -97,7 +101,9 @@
       
       // beast it just a little, it shouldnt throw exceptions:
       // (it should have thrown them in initialize)
-      checkRandomData(random(), new FactoryAnalyzer(assertingTokenizer, factory, null), 20, 20, false, false);
+      Analyzer a = new FactoryAnalyzer(assertingTokenizer, factory, null);
+      checkRandomData(random(), a, 20, 20, false, false);
+      a.close();
     }
   }
   
@@ -117,7 +123,9 @@
       
       // beast it just a little, it shouldnt throw exceptions:
       // (it should have thrown them in initialize)
-      checkRandomData(random(), new FactoryAnalyzer(assertingTokenizer, null, factory), 20, 20, false, false);
+      Analyzer a = new FactoryAnalyzer(assertingTokenizer, null, factory);
+      checkRandomData(random(), a, 20, 20, false, false);
+      a.close();
     }
   }
   
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordAnalyzer.java	(working copy)
@@ -19,6 +19,7 @@
 
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
@@ -33,23 +34,24 @@
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.TestUtil;
 
 public class TestKeywordAnalyzer extends BaseTokenStreamTestCase {
   
   private Directory directory;
-  private IndexSearcher searcher;
   private IndexReader reader;
+  private Analyzer analyzer;
 
   @Override
   public void setUp() throws Exception {
     super.setUp();
     directory = newDirectory();
-    IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(new SimpleAnalyzer()));
+    analyzer = new SimpleAnalyzer();
+    IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(analyzer));
 
     Document doc = new Document();
     doc.add(new StringField("partnum", "Q36", Field.Store.YES));
@@ -59,13 +61,11 @@
     writer.close();
 
     reader = DirectoryReader.open(directory);
-    searcher = newSearcher(reader);
   }
   
   @Override
   public void tearDown() throws Exception {
-    reader.close();
-    directory.close();
+    IOUtils.close(analyzer, reader, directory);
     super.tearDown();
   }
 
@@ -86,7 +86,8 @@
 
   public void testMutipleDocument() throws Exception {
     RAMDirectory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new KeywordAnalyzer()));
+    Analyzer analyzer = new KeywordAnalyzer();
+    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(analyzer));
     Document doc = new Document();
     doc.add(new TextField("partnum", "Q36", Field.Store.YES));
     writer.addDocument(doc);
@@ -112,11 +113,13 @@
         null,
         0);
     assertTrue(td.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
+    analyzer.close();
   }
 
   // LUCENE-1441
   public void testOffsets() throws Exception {
-    try (TokenStream stream = new KeywordAnalyzer().tokenStream("field", new StringReader("abcd"))) {
+    try (Analyzer analyzer = new KeywordAnalyzer();
+         TokenStream stream = analyzer.tokenStream("field", new StringReader("abcd"))) {
       OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);
       stream.reset();
       assertTrue(stream.incrementToken());
@@ -129,6 +132,8 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new KeywordAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new KeywordAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java	(working copy)
@@ -901,17 +901,18 @@
     int numIterations = TEST_NIGHTLY ? atLeast(20) : 3;
     Random random = random();
     for (int i = 0; i < numIterations; i++) {
-      MockRandomAnalyzer a = new MockRandomAnalyzer(random.nextLong());
-      if (VERBOSE) {
-        System.out.println("Creating random analyzer:" + a);
+      try (MockRandomAnalyzer a = new MockRandomAnalyzer(random.nextLong())) {
+        if (VERBOSE) {
+          System.out.println("Creating random analyzer:" + a);
+        }
+        try {
+          checkRandomData(random, a, 500*RANDOM_MULTIPLIER, 20, false,
+              false /* We already validate our own offsets... */);
+        } catch (Throwable e) {
+          System.err.println("Exception from random analyzer: " + a);
+          throw e;
+        }
       }
-      try {
-        checkRandomData(random, a, 500*RANDOM_MULTIPLIER, 20, false,
-                        false /* We already validate our own offsets... */);
-      } catch (Throwable e) {
-        System.err.println("Exception from random analyzer: " + a);
-        throw e;
-      }
     }
   }
   
@@ -920,17 +921,18 @@
     int numIterations = TEST_NIGHTLY ? atLeast(20) : 3;
     Random random = random();
     for (int i = 0; i < numIterations; i++) {
-      MockRandomAnalyzer a = new MockRandomAnalyzer(random.nextLong());
-      if (VERBOSE) {
-        System.out.println("Creating random analyzer:" + a);
+      try (MockRandomAnalyzer a = new MockRandomAnalyzer(random.nextLong())) {
+        if (VERBOSE) {
+          System.out.println("Creating random analyzer:" + a);
+        }
+        try {
+          checkRandomData(random, a, 50*RANDOM_MULTIPLIER, 80, false,
+              false /* We already validate our own offsets... */);
+        } catch (Throwable e) {
+          System.err.println("Exception from random analyzer: " + a);
+          throw e;
+        }
       }
-      try {
-        checkRandomData(random, a, 50*RANDOM_MULTIPLIER, 80, false,
-                        false /* We already validate our own offsets... */);
-      } catch (Throwable e) {
-        System.err.println("Exception from random analyzer: " + a);
-        throw e;
-      }
     }
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopAnalyzer.java	(working copy)
@@ -30,7 +30,7 @@
 
 public class TestStopAnalyzer extends BaseTokenStreamTestCase {
   
-  private StopAnalyzer stop = new StopAnalyzer();
+  private StopAnalyzer stop;
   private Set<Object> inValidTokens = new HashSet<>();
 
   @Override
@@ -41,7 +41,14 @@
     while(it.hasNext()) {
       inValidTokens.add(it.next());
     }
+    stop = new StopAnalyzer();
   }
+  
+  @Override
+  public void tearDown() throws Exception {
+    stop.close();
+    super.tearDown();
+  }
 
   public void testDefaults() throws IOException {
     assertTrue(stop != null);
@@ -71,6 +78,7 @@
       }
       stream.end();
     }
+    newStop.close();
   }
 
   public void testStopListPositions() throws IOException {
@@ -92,6 +100,7 @@
       }
       stream.end();
     }
+    newStop.close();
   }
 
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/custom/TestCustomAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/custom/TestCustomAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/custom/TestCustomAnalyzer.java	(working copy)
@@ -62,6 +62,7 @@
     assertAnalyzesTo(a, "föó bär FÖÖ BAR", 
         new String[] { "foo", "föó", "bar", "bär", "foo", "föö", "bar" },
         new int[]    { 1,     0,     1,     0,     1,     0,     1});
+    a.close();
   }
 
   public void testHtmlStripClassicFolding() throws Exception {
@@ -93,6 +94,7 @@
     assertAnalyzesTo(a, "<p><b>föó</b> bär     FÖÖ BAR</p>", 
         new String[] { "foo", "föó", "bar", "bär", "foo", "föö", "bar" },
         new int[]    { 1,     0,     1,     0,     1,     0,     1});
+    a.close();
   }
   
   public void testStopWordsFromClasspath() throws Exception {
@@ -114,6 +116,7 @@
     assertSame(Version.LATEST, a.getVersion());
 
     assertAnalyzesTo(a, "foo Foo Bar", new String[0]);
+    a.close();
   }
   
   public void testStopWordsFromClasspathWithMap() throws Exception {
@@ -141,6 +144,7 @@
     } catch (IllegalArgumentException | UnsupportedOperationException e) {
       // pass
     }
+    a.close();
   }
   
   public void testStopWordsFromFile() throws Exception {
@@ -152,6 +156,7 @@
             "format", "wordset")
         .build();
     assertAnalyzesTo(a, "foo Foo Bar", new String[0]);
+    a.close();
   }
   
   public void testStopWordsFromFileAbsolute() throws Exception {
@@ -163,6 +168,7 @@
             "format", "wordset")
         .build();
     assertAnalyzesTo(a, "foo Foo Bar", new String[0]);
+    a.close();
   }
   
   // Now test misconfigurations:
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 import java.io.IOException;
+
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.util.CharArraySet;
@@ -31,9 +32,17 @@
  */
 public class TestCzechAnalyzer extends BaseTokenStreamTestCase {
   
+  /** This test fails with NPE when the 
+   * stopwords file is missing in classpath */
+  public void testResourcesAvailable() {
+    new CzechAnalyzer().close();
+  }
+  
   public void testStopWord() throws Exception {
-    assertAnalyzesTo(new CzechAnalyzer(), "Pokud mluvime o volnem", 
+    Analyzer analyzer = new CzechAnalyzer();
+    assertAnalyzesTo(analyzer, "Pokud mluvime o volnem", 
         new String[] { "mluvim", "voln" });
+    analyzer.close();
   }
   
   public void testReusableTokenStream() throws Exception {
@@ -40,6 +49,7 @@
     Analyzer analyzer = new CzechAnalyzer();
     assertAnalyzesTo(analyzer, "Pokud mluvime o volnem", new String[] { "mluvim", "voln" });
     assertAnalyzesTo(analyzer, "Česká Republika", new String[] { "česk", "republik" });
+    analyzer.close();
   }
 
   public void testWithStemExclusionSet() throws IOException{
@@ -47,10 +57,13 @@
     set.add("hole");
     CzechAnalyzer cz = new CzechAnalyzer(CharArraySet.EMPTY_SET, set);
     assertAnalyzesTo(cz, "hole desek", new String[] {"hole", "desk"});
+    cz.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new CzechAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new CzechAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.Analyzer;
@@ -103,6 +102,8 @@
     assertAnalyzesTo(cz, "soudcům", new String[] { "soudk" });
     assertAnalyzesTo(cz, "soudcích", new String[] { "soudk" });
     assertAnalyzesTo(cz, "soudcem", new String[] { "soudk" });
+    
+    cz.close();
   }
   
   /**
@@ -147,6 +148,8 @@
     assertAnalyzesTo(cz, "ženách", new String[] { "žn" });
     assertAnalyzesTo(cz, "ženou", new String[] { "žn" });
     assertAnalyzesTo(cz, "ženami", new String[] { "žn" });
+    
+    cz.close();
   }
 
   /**
@@ -189,7 +192,9 @@
     assertAnalyzesTo(cz, "stavení", new String[] { "stavn" });
     assertAnalyzesTo(cz, "stavením", new String[] { "stavn" });
     assertAnalyzesTo(cz, "staveních", new String[] { "stavn" });
-    assertAnalyzesTo(cz, "staveními", new String[] { "stavn" });    
+    assertAnalyzesTo(cz, "staveními", new String[] { "stavn" }); 
+    
+    cz.close();
   }
   
   /**
@@ -218,6 +223,8 @@
     assertAnalyzesTo(cz, "jarnímu", new String[] { "jarn" });
     assertAnalyzesTo(cz, "jarním", new String[] { "jarn" });
     assertAnalyzesTo(cz, "jarními", new String[] { "jarn" });  
+    
+    cz.close();
   }
   
   /**
@@ -227,6 +234,7 @@
     CzechAnalyzer cz = new CzechAnalyzer();
     assertAnalyzesTo(cz, "Karlův", new String[] { "karl" });
     assertAnalyzesTo(cz, "jazykový", new String[] { "jazyk" });
+    cz.close();
   }
   
   /**
@@ -267,6 +275,8 @@
     /* rewrite of e* -> * */
     assertAnalyzesTo(cz, "deska", new String[] { "desk" });
     assertAnalyzesTo(cz, "desek", new String[] { "desk" });
+    
+    cz.close();
   }
   
   /**
@@ -276,6 +286,7 @@
     CzechAnalyzer cz = new CzechAnalyzer();
     assertAnalyzesTo(cz, "e", new String[] { "e" });
     assertAnalyzesTo(cz, "zi", new String[] { "zi" });
+    cz.close();
   }
   
   public void testWithKeywordAttribute() throws IOException {
@@ -297,6 +308,7 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
   
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/da/TestDanishAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/da/TestDanishAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/da/TestDanishAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new DanishAnalyzer();
+    new DanishAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -38,6 +38,7 @@
     checkOneTerm(a, "undersøgelse", "undersøg");
     // stopword
     assertAnalyzesTo(a, "på", new String[] {});
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -47,10 +48,13 @@
         DanishAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "undersøgelse", "undersøgelse");
     checkOneTerm(a, "undersøg", "undersøg");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new DanishAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new DanishAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java	(working copy)
@@ -32,6 +32,7 @@
     checkOneTerm(a, "Tisch", "tisch");
     checkOneTerm(a, "Tische", "tisch");
     checkOneTerm(a, "Tischen", "tisch");
+    a.close();
   }
   
   public void testWithKeywordAttribute() throws IOException {
@@ -48,6 +49,7 @@
     GermanAnalyzer a = new GermanAnalyzer( CharArraySet.EMPTY_SET, 
         new CharArraySet( asSet("tischen"), false));
     checkOneTerm(a, "tischen", "tischen");
+    a.close();
   }
   
   /** test some features of the new snowball filter
@@ -58,10 +60,13 @@
     // a/o/u + e is equivalent to the umlaut form
     checkOneTerm(a, "Schaltflächen", "schaltflach");
     checkOneTerm(a, "Schaltflaechen", "schaltflach");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new GermanAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    GermanAnalyzer a = new GermanAnalyzer();
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanLightStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanLightStemFilter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -35,14 +34,26 @@
  * Simple tests for {@link GermanLightStemFilter}
  */
 public class TestGermanLightStemFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(source, new GermanLightStemFilter(source));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(source, new GermanLightStemFilter(source));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /** Test against a vocabulary from the reference impl */
   public void testVocabulary() throws IOException {
     assertVocabulary(analyzer, getDataPath("delighttestdata.zip"), "delight.txt");
@@ -59,6 +70,7 @@
       }
     };
     checkOneTerm(a, "sängerinnen", "sängerinnen");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
@@ -75,5 +87,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanMinimalStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanMinimalStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanMinimalStemFilter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -35,14 +34,26 @@
  * Simple tests for {@link GermanMinimalStemFilter}
  */
 public class TestGermanMinimalStemFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(source, new GermanMinimalStemFilter(source));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(source, new GermanMinimalStemFilter(source));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /** Test some examples from the paper */
   public void testExamples() throws IOException {
     checkOneTerm(analyzer, "sängerinnen", "sangerin");
@@ -66,6 +77,7 @@
       }
     };
     checkOneTerm(a, "sängerinnen", "sängerinnen");
+    a.close();
   }
   
   /** Test against a vocabulary from the reference impl */
@@ -87,5 +99,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanNormalizationFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanNormalizationFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanNormalizationFilter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -31,15 +30,27 @@
  * Tests {@link GermanNormalizationFilter}
  */
 public class TestGermanNormalizationFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String field) {
-      final Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      final TokenStream stream = new GermanNormalizationFilter(tokenizer);
-      return new TokenStreamComponents(tokenizer, stream);
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String field) {
+        final Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        final TokenStream stream = new GermanNormalizationFilter(tokenizer);
+        return new TokenStreamComponents(tokenizer, stream);
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /**
    * Tests that a/o/u + e is equivalent to the umlaut form
    */
@@ -76,5 +87,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java	(working copy)
@@ -25,6 +25,7 @@
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.Analyzer.TokenStreamComponents;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.core.LowerCaseFilter;
 import org.apache.lucene.analysis.miscellaneous.SetKeywordMarkerFilter;
@@ -39,14 +40,26 @@
  *
  */
 public class TestGermanStemFilter extends BaseTokenStreamTestCase {
-  Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer t = new MockTokenizer(MockTokenizer.KEYWORD, false);
-      return new TokenStreamComponents(t,
-          new GermanStemFilter(new LowerCaseFilter(t)));
-    }
-  };
+  private Analyzer analyzer;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer t = new MockTokenizer(MockTokenizer.KEYWORD, false);
+        return new TokenStreamComponents(t,
+            new GermanStemFilter(new LowerCaseFilter(t)));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
 
   public void testStemming() throws Exception {  
     InputStream vocOut = getClass().getResourceAsStream("data.txt");
@@ -65,6 +78,7 @@
       }
     };
     checkOneTerm(a, "sängerinnen", "sängerinnen");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
@@ -81,5 +95,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java	(working copy)
@@ -45,6 +45,7 @@
     // as well as the elimination of stop words
     assertAnalyzesTo(a, "ΠΡΟΫΠΟΘΕΣΕΙΣ  Άψογος, ο μεστός και οι άλλοι",
         new String[] { "προυποθεσ", "αψογ", "μεστ", "αλλ" });
+    a.close();
   }
 
   public void testReusableTokenStream() throws Exception {
@@ -62,10 +63,13 @@
     // as well as the elimination of stop words
     assertAnalyzesTo(a, "ΠΡΟΫΠΟΘΕΣΕΙΣ  Άψογος, ο μεστός και οι άλλοι",
         new String[] { "προυποθεσ", "αψογ", "μεστ", "αλλ" });
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new GreekAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer a = new GreekAnalyzer();
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/el/TestGreekStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/el/TestGreekStemmer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/el/TestGreekStemmer.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -26,7 +25,19 @@
 import org.apache.lucene.analysis.core.KeywordTokenizer;
 
 public class TestGreekStemmer extends BaseTokenStreamTestCase {
-  Analyzer a = new GreekAnalyzer();
+  private Analyzer a;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    a = new GreekAnalyzer();
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    a.close();
+    super.tearDown();
+  }
 
   public void testMasculineNouns() throws Exception {
     // -ος
@@ -537,5 +548,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestEnglishAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestEnglishAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestEnglishAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new EnglishAnalyzer();
+    new EnglishAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -42,6 +42,7 @@
     checkOneTerm(a, "steven's", "steven");
     checkOneTerm(a, "steven\u2019s", "steven");
     checkOneTerm(a, "steven\uFF07s", "steven");
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -51,10 +52,13 @@
         EnglishAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "books", "books");
     checkOneTerm(a, "book", "book");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new EnglishAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer a = new EnglishAnalyzer();
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestEnglishMinimalStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestEnglishMinimalStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestEnglishMinimalStemFilter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -30,14 +29,26 @@
  * Simple tests for {@link EnglishMinimalStemFilter}
  */
 public class TestEnglishMinimalStemFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(source, new EnglishMinimalStemFilter(source));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(source, new EnglishMinimalStemFilter(source));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+    
   /** Test some examples from various papers about this technique */
   public void testExamples() throws IOException {
     checkOneTerm(analyzer, "queries", "query");
@@ -65,5 +76,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestKStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestKStemmer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestKStemmer.java	(working copy)
@@ -20,7 +20,6 @@
 import static org.apache.lucene.analysis.VocabularyAssert.assertVocabulary;
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -32,13 +31,25 @@
  * Tests for {@link KStemmer}
  */
 public class TestKStemmer extends BaseTokenStreamTestCase {
-  Analyzer a = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);
-      return new TokenStreamComponents(tokenizer, new KStemFilter(tokenizer));
-    }
-  };
+  private Analyzer a;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);
+        return new TokenStreamComponents(tokenizer, new KStemFilter(tokenizer));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    a.close();
+    super.tearDown();
+  }
  
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
@@ -63,6 +74,7 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 
   /****** requires original java kstem source code to create map
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestPorterStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestPorterStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/en/TestPorterStemFilter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -36,14 +35,26 @@
  * Test the PorterStemFilter with Martin Porter's test data.
  */
 public class TestPorterStemFilter extends BaseTokenStreamTestCase {
-  Analyzer a = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer t = new MockTokenizer( MockTokenizer.KEYWORD, false);
-      return new TokenStreamComponents(t, new PorterStemFilter(t));
-    }
-  };
+  private Analyzer a;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer t = new MockTokenizer( MockTokenizer.KEYWORD, false);
+        return new TokenStreamComponents(t, new PorterStemFilter(t));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    a.close();
+    super.tearDown();
+  }
+  
   /**
    * Run the stemmer against all strings in voc.txt
    * The output should be the same as the string in output.txt
@@ -75,5 +86,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/es/TestSpanishAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/es/TestSpanishAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/es/TestSpanishAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new SpanishAnalyzer();
+    new SpanishAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -38,6 +38,7 @@
     checkOneTerm(a, "chicano", "chican");
     // stopword
     assertAnalyzesTo(a, "los", new String[] {});
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -47,10 +48,13 @@
         SpanishAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "chicana", "chican");
     checkOneTerm(a, "chicano", "chicano");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new SpanishAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer a = new SpanishAnalyzer();
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/es/TestSpanishLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/es/TestSpanishLightStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/es/TestSpanishLightStemFilter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -32,14 +31,26 @@
  * Simple tests for {@link SpanishLightStemFilter}
  */
 public class TestSpanishLightStemFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(source, new SpanishLightStemFilter(source));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(source, new SpanishLightStemFilter(source));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+    
   /** Test against a vocabulary from the reference impl */
   public void testVocabulary() throws IOException {
     assertVocabulary(analyzer, getDataPath("eslighttestdata.zip"), "eslight.txt");
@@ -59,5 +70,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/eu/TestBasqueAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/eu/TestBasqueAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/eu/TestBasqueAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new BasqueAnalyzer();
+    new BasqueAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -38,6 +38,7 @@
     checkOneTerm(a, "zaldiak", "zaldi");
     // stopword
     assertAnalyzesTo(a, "izan", new String[] { });
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -47,10 +48,13 @@
         BasqueAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "zaldiak", "zaldiak");
     checkOneTerm(a, "mendiari", "mendi");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new BasqueAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer a = new BasqueAnalyzer();
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianAnalyzer.java	(working copy)
@@ -31,7 +31,7 @@
    * This test fails with NPE when the stopwords file is missing in classpath
    */
   public void testResourcesAvailable() {
-    new PersianAnalyzer();
+    new PersianAnalyzer().close();
   }
 
   /**
@@ -105,6 +105,7 @@
 
     // active present subjunctive
     assertAnalyzesTo(a, "بخورد", new String[] { "بخورد" });
+    a.close();
   }
 
   /**
@@ -181,6 +182,7 @@
 
     // active present subjunctive
     assertAnalyzesTo(a, "بخورد", new String[] { "بخورد" });
+    a.close();
   }
 
   /**
@@ -192,6 +194,7 @@
     Analyzer a = new PersianAnalyzer();
     assertAnalyzesTo(a, "برگ ها", new String[] { "برگ" });
     assertAnalyzesTo(a, "برگ‌ها", new String[] { "برگ" });
+    a.close();
   }
 
   /**
@@ -201,6 +204,7 @@
   public void testBehaviorNonPersian() throws Exception {
     Analyzer a = new PersianAnalyzer();
     assertAnalyzesTo(a, "English test.", new String[] { "english", "test" });
+    a.close();
   }
   
   /**
@@ -210,6 +214,7 @@
     Analyzer a = new PersianAnalyzer();
     assertAnalyzesTo(a, "خورده مي شده بوده باشد", new String[] { "خورده" });
     assertAnalyzesTo(a, "برگ‌ها", new String[] { "برگ" });
+    a.close();
   }
   
   /**
@@ -220,10 +225,13 @@
         new CharArraySet( asSet("the", "and", "a"), false));
     assertAnalyzesTo(a, "The quick brown fox.", new String[] { "quick",
         "brown", "fox" });
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new PersianAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    PersianAnalyzer a = new PersianAnalyzer();
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianCharFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianCharFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianCharFilter.java	(working copy)
@@ -24,18 +24,30 @@
 import org.apache.lucene.analysis.MockTokenizer;
 
 public class TestPersianCharFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      return new TokenStreamComponents(new MockTokenizer());
-    }
+  private Analyzer analyzer;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        return new TokenStreamComponents(new MockTokenizer());
+      }
 
-    @Override
-    protected Reader initReader(String fieldName, Reader reader) {
-      return new PersianCharFilter(reader);
-    }
-  };
+      @Override
+      protected Reader initReader(String fieldName, Reader reader) {
+        return new PersianCharFilter(reader);
+      }
+    };
+  }
   
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   public void testBasics() throws Exception {
     assertAnalyzesTo(analyzer, "this is a\u200Ctest",
         new String[] { "this", "is", "a", "test" });
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java	(working copy)
@@ -18,8 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
-import java.io.StringReader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -73,6 +71,7 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/fi/TestFinnishAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/fi/TestFinnishAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/fi/TestFinnishAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new FinnishAnalyzer();
+    new FinnishAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -38,6 +38,7 @@
     checkOneTerm(a, "edeltäjistään", "edeltäj");
     // stopword
     assertAnalyzesTo(a, "olla", new String[] {});
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -47,10 +48,13 @@
         FinnishAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "edeltäjiinsä", "edeltäj");
     checkOneTerm(a, "edeltäjistään", "edeltäjistään");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new FinnishAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer a = new FinnishAnalyzer();
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/fi/TestFinnishLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/fi/TestFinnishLightStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/fi/TestFinnishLightStemFilter.java	(working copy)
@@ -34,14 +34,26 @@
  * Simple tests for {@link FinnishLightStemFilter}
  */
 public class TestFinnishLightStemFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(source, new FinnishLightStemFilter(source));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(source, new FinnishLightStemFilter(source));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /** Test against a vocabulary from the reference impl */
   public void testVocabulary() throws IOException {
     assertVocabulary(analyzer, getDataPath("filighttestdata.zip"), "filight.txt");
@@ -58,6 +70,7 @@
       }
     };
     checkOneTerm(a, "edeltäjistään", "edeltäjistään");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
@@ -74,5 +87,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java	(working copy)
@@ -110,7 +110,7 @@
       fa,
       "33Bis 1940-1945 1940:1945 (---i+++)*",
       new String[] { "33bi", "1940", "1945", "1940", "1945", "i" });
-
+    fa.close();
   }
   
   public void testReusableTokenStream() throws Exception {
@@ -132,6 +132,7 @@
               "chist",
               "element",
               "captif" });
+      fa.close();
   }
 
   public void testExclusionTableViaCtor() throws Exception {
@@ -141,15 +142,18 @@
         CharArraySet.EMPTY_SET, set);
     assertAnalyzesTo(fa, "habitable chiste", new String[] { "habitable",
         "chist" });
+    fa.close();
 
     fa = new FrenchAnalyzer( CharArraySet.EMPTY_SET, set);
     assertAnalyzesTo(fa, "habitable chiste", new String[] { "habitable",
         "chist" });
+    fa.close();
   }
   
   public void testElision() throws Exception {
     FrenchAnalyzer fa = new FrenchAnalyzer();
     assertAnalyzesTo(fa, "voir l'embrouille", new String[] { "voir", "embrouil" });
+    fa.close();
   }
   
   /**
@@ -158,11 +162,14 @@
   public void testStopwordsCasing() throws IOException {
     FrenchAnalyzer a = new FrenchAnalyzer();
     assertAnalyzesTo(a, "Votre", new String[] { });
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new FrenchAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer a = new FrenchAnalyzer();
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
   
   /** test accent-insensitive */
@@ -170,5 +177,6 @@
     Analyzer a = new FrenchAnalyzer();
     checkOneTerm(a, "sécuritaires", "securitair");
     checkOneTerm(a, "securitaires", "securitair");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchLightStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchLightStemFilter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -35,14 +34,26 @@
  * Simple tests for {@link FrenchLightStemFilter}
  */
 public class TestFrenchLightStemFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new MockTokenizer( MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(source, new FrenchLightStemFilter(source));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new MockTokenizer( MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(source, new FrenchLightStemFilter(source));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /** Test some examples from the paper */
   public void testExamples() throws IOException {
     checkOneTerm(analyzer, "chevaux", "cheval");
@@ -189,6 +200,7 @@
       }
     };
     checkOneTerm(a, "chevaux", "chevaux");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
@@ -205,5 +217,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchMinimalStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchMinimalStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchMinimalStemFilter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -35,14 +34,26 @@
  * Simple tests for {@link FrenchMinimalStemFilter}
  */
 public class TestFrenchMinimalStemFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(source, new FrenchMinimalStemFilter(source));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(source, new FrenchMinimalStemFilter(source));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /** Test some examples from the paper */
   public void testExamples() throws IOException {
     checkOneTerm(analyzer, "chevaux", "cheval");
@@ -68,6 +79,7 @@
       }
     };
     checkOneTerm(a, "chevaux", "chevaux");
+    a.close();
   }
   
   /** Test against a vocabulary from the reference impl */
@@ -89,5 +101,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ga/TestIrishAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ga/TestIrishAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ga/TestIrishAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new IrishAnalyzer();
+    new IrishAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -38,6 +38,7 @@
     checkOneTerm(a, "síceapatacha", "síceapaite");
     // stopword
     assertAnalyzesTo(a, "le", new String[] { });
+    a.close();
   }
   
   /** test use of elisionfilter */
@@ -45,6 +46,7 @@
     Analyzer a = new IrishAnalyzer();
     assertAnalyzesTo(a, "b'fhearr m'athair",
         new String[] { "fearr", "athair" });
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -54,6 +56,7 @@
         IrishAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "feirmeoireacht", "feirmeoireacht");
     checkOneTerm(a, "siopadóireacht", "siopadóir");
+    a.close();
   }
   
   /** test special hyphen handling */
@@ -62,10 +65,13 @@
     assertAnalyzesTo(a, "n-athair",
         new String[] { "athair" },
         new int[] { 2 });
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new IrishAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer a = new IrishAnalyzer();
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ga/TestIrishLowerCaseFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ga/TestIrishLowerCaseFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ga/TestIrishLowerCaseFilter.java	(working copy)
@@ -48,5 +48,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/gl/TestGalicianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/gl/TestGalicianAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/gl/TestGalicianAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new GalicianAnalyzer();
+    new GalicianAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -38,6 +38,7 @@
     checkOneTerm(a, "corresponderá", "correspond");
     // stopword
     assertAnalyzesTo(a, "e", new String[] {});
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -47,10 +48,13 @@
         GalicianAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "correspondente", "correspondente");
     checkOneTerm(a, "corresponderá", "correspond");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new GalicianAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new GalicianAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/gl/TestGalicianMinimalStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/gl/TestGalicianMinimalStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/gl/TestGalicianMinimalStemFilter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -33,14 +32,26 @@
  * Simple tests for {@link GalicianMinimalStemmer}
  */
 public class TestGalicianMinimalStemFilter extends BaseTokenStreamTestCase {
-  Analyzer a = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(tokenizer, new GalicianMinimalStemFilter(tokenizer));
-    }
-  };
+  private Analyzer a;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(tokenizer, new GalicianMinimalStemFilter(tokenizer));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    a.close();
+    super.tearDown();
+  }
+  
   public void testPlural() throws Exception {
     checkOneTerm(a, "elefantes", "elefante");
     checkOneTerm(a, "elefante", "elefante");
@@ -64,6 +75,7 @@
       }
     };
     checkOneTerm(a, "elefantes", "elefantes");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
@@ -80,5 +92,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/gl/TestGalicianStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/gl/TestGalicianStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/gl/TestGalicianStemFilter.java	(working copy)
@@ -20,29 +20,36 @@
 import static org.apache.lucene.analysis.VocabularyAssert.assertVocabulary;
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
-import org.apache.lucene.analysis.core.LowerCaseFilter;
-import org.apache.lucene.analysis.standard.StandardTokenizer;
 
 /**
  * Simple tests for {@link GalicianStemFilter}
  */
 public class TestGalicianStemFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(source, new GalicianStemFilter(source));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(source, new GalicianStemFilter(source));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
  
   /** Test against a vocabulary from the reference impl */
   public void testVocabulary() throws IOException {
@@ -58,5 +65,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiAnalyzer.java	(working copy)
@@ -28,7 +28,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new HindiAnalyzer();
+    new HindiAnalyzer().close();
   }
   
   public void testBasics() throws Exception {
@@ -36,6 +36,7 @@
     // two ways to write 'hindi' itself.
     checkOneTerm(a, "हिन्दी", "हिंद");
     checkOneTerm(a, "हिंदी", "हिंद");
+    a.close();
   }
   
   public void testExclusionSet() throws Exception {
@@ -43,10 +44,13 @@
     Analyzer a = new HindiAnalyzer( 
         HindiAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "हिंदी", "हिंदी");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new HindiAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new HindiAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiNormalizer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiNormalizer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiNormalizer.java	(working copy)
@@ -76,5 +76,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiStemmer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiStemmer.java	(working copy)
@@ -95,5 +95,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/hu/TestHungarianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/hu/TestHungarianAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/hu/TestHungarianAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new HungarianAnalyzer();
+    new HungarianAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -38,6 +38,7 @@
     checkOneTerm(a, "babakocsijáért", "babakocs");
     // stopword
     assertAnalyzesTo(a, "által", new String[] {});
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -47,10 +48,13 @@
         HungarianAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "babakocsi", "babakocsi");
     checkOneTerm(a, "babakocsijáért", "babakocs");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new HungarianAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new HungarianAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/hu/TestHungarianLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/hu/TestHungarianLightStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/hu/TestHungarianLightStemFilter.java	(working copy)
@@ -34,14 +34,26 @@
  * Simple tests for {@link HungarianLightStemFilter}
  */
 public class TestHungarianLightStemFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(source, new HungarianLightStemFilter(source));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(source, new HungarianLightStemFilter(source));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /** Test against a vocabulary from the reference impl */
   public void testVocabulary() throws IOException {
     assertVocabulary(analyzer, getDataPath("hulighttestdata.zip"), "hulight.txt");
@@ -58,6 +70,7 @@
       }
     };
     checkOneTerm(a, "babakocsi", "babakocsi");
+    a.close();
   }
   
   public void testEmptyTerm() throws IOException {
@@ -69,5 +82,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/hunspell/TestHunspellStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/hunspell/TestHunspellStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/hunspell/TestHunspellStemFilter.java	(working copy)
@@ -87,6 +87,7 @@
       }  
     };
     checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
   
   public void testEmptyTerm() throws IOException {
@@ -98,6 +99,7 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
   
   public void testIgnoreCaseNoSideEffects() throws Exception {
@@ -118,5 +120,6 @@
       }
     };
     checkOneTerm(a, "NoChAnGy", "NoChAnGy");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/hy/TestArmenianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/hy/TestArmenianAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/hy/TestArmenianAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new ArmenianAnalyzer();
+    new ArmenianAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -38,6 +38,7 @@
     checkOneTerm(a, "արծիվներ", "արծ");
     // stopword
     assertAnalyzesTo(a, "է", new String[] { });
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -47,10 +48,13 @@
         ArmenianAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "արծիվներ", "արծիվներ");
     checkOneTerm(a, "արծիվ", "արծ");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new ArmenianAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new ArmenianAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new IndonesianAnalyzer();
+    new IndonesianAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -38,6 +38,7 @@
     checkOneTerm(a, "pembunuhan", "bunuh");
     // stopword
     assertAnalyzesTo(a, "bahwa", new String[] {});
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -47,10 +48,13 @@
         IndonesianAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "peledakan", "peledakan");
     checkOneTerm(a, "pembunuhan", "bunuh");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new IndonesianAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new IndonesianAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianStemmer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianStemmer.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -25,20 +24,41 @@
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
+import org.apache.lucene.util.IOUtils;
 
 /**
  * Tests {@link IndonesianStemmer}
  */
 public class TestIndonesianStemmer extends BaseTokenStreamTestCase {
-  /* full stemming, no stopwords */
-  Analyzer a = new Analyzer() {
-    @Override
-    public TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new MockTokenizer(MockTokenizer.KEYWORD, false);
-      return new TokenStreamComponents(tokenizer, new IndonesianStemFilter(tokenizer));
-    }
-  };
+  private Analyzer a, b;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    /* full stemming, no stopwords */
+    a = new Analyzer() {
+      @Override
+      public TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.KEYWORD, false);
+        return new TokenStreamComponents(tokenizer, new IndonesianStemFilter(tokenizer));
+      }
+    };
+    /* inflectional-only stemming */
+    b = new Analyzer() {
+      @Override
+      public TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.KEYWORD, false);
+        return new TokenStreamComponents(tokenizer, new IndonesianStemFilter(tokenizer, false));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    IOUtils.close(a, b);
+    super.tearDown();
+  }
+  
   /** Some examples from the paper */
   public void testExamples() throws IOException {
     checkOneTerm(a, "bukukah", "buku");
@@ -111,15 +131,6 @@
     checkOneTerm(a, "kecelakaan", "celaka");
   }
   
-  /* inflectional-only stemming */
-  Analyzer b = new Analyzer() {
-    @Override
-    public TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new MockTokenizer(MockTokenizer.KEYWORD, false);
-      return new TokenStreamComponents(tokenizer, new IndonesianStemFilter(tokenizer, false));
-    }
-  };
-  
   /** Test stemming only inflectional suffixes */
   public void testInflectionalOnly() throws IOException {
     checkOneTerm(b, "bukunya", "buku");
@@ -143,5 +154,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/in/TestIndicNormalizer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/in/TestIndicNormalizer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/in/TestIndicNormalizer.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.Analyzer;
@@ -62,5 +61,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new ItalianAnalyzer();
+    new ItalianAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -38,6 +38,7 @@
     checkOneTerm(a, "abbandonati", "abbandonat");
     // stopword
     assertAnalyzesTo(a, "dallo", new String[] {});
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -47,11 +48,14 @@
         ItalianAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "abbandonata", "abbandonata");
     checkOneTerm(a, "abbandonati", "abbandonat");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new ItalianAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new ItalianAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
   
   /** test that the elisionfilter is working */
@@ -59,5 +63,6 @@
     Analyzer a = new ItalianAnalyzer();
     assertAnalyzesTo(a, "dell'Italia", new String[] { "ital" });
     assertAnalyzesTo(a, "l'Italiano", new String[] { "italian" });
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianLightStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianLightStemFilter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -32,14 +31,26 @@
  * Simple tests for {@link ItalianLightStemFilter}
  */
 public class TestItalianLightStemFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(source, new ItalianLightStemFilter(source));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(source, new ItalianLightStemFilter(source));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /** Test against a vocabulary from the reference impl */
   public void testVocabulary() throws IOException {
     assertVocabulary(analyzer, getDataPath("itlighttestdata.zip"), "itlight.txt");
@@ -59,5 +70,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/lv/TestLatvianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/lv/TestLatvianAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/lv/TestLatvianAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new LatvianAnalyzer();
+    new LatvianAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -38,6 +38,7 @@
     checkOneTerm(a, "tirgus", "tirg");
     // stopword
     assertAnalyzesTo(a, "un", new String[] {});
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -47,10 +48,13 @@
         LatvianAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "tirgiem", "tirgiem");
     checkOneTerm(a, "tirgus", "tirg");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new LatvianAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new LatvianAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/lv/TestLatvianStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/lv/TestLatvianStemmer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/lv/TestLatvianStemmer.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -30,14 +29,26 @@
  * Basic tests for {@link LatvianStemmer}
  */
 public class TestLatvianStemmer extends BaseTokenStreamTestCase {
-  private Analyzer a = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(tokenizer, new LatvianStemFilter(tokenizer));
-    }
-  };
+  private Analyzer a;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(tokenizer, new LatvianStemFilter(tokenizer));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    a.close();
+    super.tearDown();
+  }
+  
   public void testNouns1() throws IOException {
     // decl. I
     checkOneTerm(a, "tēvs",   "tēv"); // nom. sing.
@@ -279,5 +290,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestASCIIFoldingFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestASCIIFoldingFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestASCIIFoldingFilter.java	(working copy)
@@ -1933,6 +1933,7 @@
       } 
     };
     checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
   
   public void testEmptyTerm() throws IOException {
@@ -1945,5 +1946,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestCapitalizationFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestCapitalizationFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestCapitalizationFilter.java	(working copy)
@@ -137,6 +137,7 @@
     };
     
     checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
   
   public void testEmptyTerm() throws IOException {
@@ -148,6 +149,7 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 
   /**
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestCodepointCountFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestCodepointCountFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestCodepointCountFilter.java	(working copy)
@@ -47,6 +47,7 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
   
   public void testRandomStrings() throws IOException {
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java	(working copy)
@@ -18,7 +18,6 @@
 package org.apache.lucene.analysis.miscellaneous;
 
 import java.io.IOException;
-import java.io.Reader;
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.Analyzer;
@@ -78,6 +77,7 @@
     };
     
     checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
   
   public void testEmptyTerm() throws IOException {
@@ -89,5 +89,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeepWordFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeepWordFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeepWordFilter.java	(working copy)
@@ -66,5 +66,6 @@
     };
     
     checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java	(working copy)
@@ -18,12 +18,9 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
-import java.io.StringReader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
@@ -49,6 +46,7 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 
   /**
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer.java	(working copy)
@@ -52,6 +52,7 @@
     
       // equal to limit
       assertTokenStreamContents(a.tokenStream("dummy", "1  2  "), new String[] { "1", "2" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);
+      a.close();
     }
   }
 
@@ -86,6 +87,7 @@
       assertEquals(0, reader.docFreq(t));
       reader.close();
       dir.close();
+      a.close();
     }
   }
 
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter.java	(working copy)
@@ -56,6 +56,7 @@
       // equal to limit
       assertTokenStreamContents(a.tokenStream("dummy", "1  2  "),
           new String[]{"1", "2"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 6 : null);
+      a.close();
     }
   }
 
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalyzerWrapper.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalyzerWrapper.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalyzerWrapper.java	(working copy)
@@ -15,6 +15,7 @@
 import org.apache.lucene.analysis.core.SimpleAnalyzer;
 import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.Rethrow;
 
 /*
@@ -40,9 +41,11 @@
 
     Map<String,Analyzer> analyzerPerField =
         Collections.<String,Analyzer>singletonMap("special", new SimpleAnalyzer());
+    
+    Analyzer defaultAnalyzer = new WhitespaceAnalyzer();
 
     PerFieldAnalyzerWrapper analyzer =
-              new PerFieldAnalyzerWrapper(new WhitespaceAnalyzer(), analyzerPerField);
+              new PerFieldAnalyzerWrapper(defaultAnalyzer, analyzerPerField);
 
     try (TokenStream tokenStream = analyzer.tokenStream("field", text)) {
       CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);
@@ -67,6 +70,10 @@
       assertFalse(tokenStream.incrementToken());
       tokenStream.end();
     }
+    // TODO: fix this about PFAW, this is crazy
+    analyzer.close();
+    defaultAnalyzer.close();
+    IOUtils.close(analyzerPerField.values());    
   }
   
   public void testReuseWrapped() throws Exception {
@@ -124,6 +131,7 @@
     ts4 = wrapper3.tokenStream("moreSpecial", text);
     assertSame(ts3, ts4);
     assertSame(ts2, ts3);
+    IOUtils.close(wrapper3, wrapper2, wrapper1, specialAnalyzer, defaultAnalyzer);
   }
   
   public void testCharFilters() throws Exception {
@@ -152,5 +160,7 @@
         new int[] { 0 },
         new int[] { 2 }
     );
+    p.close();
+    a.close(); // TODO: fix this about PFAW, its a trap
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestRemoveDuplicatesTokenFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestRemoveDuplicatesTokenFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestRemoveDuplicatesTokenFilter.java	(working copy)
@@ -163,6 +163,7 @@
       };
 
       checkRandomData(random(), analyzer, 200);
+      analyzer.close();
     }
   }
   
@@ -175,6 +176,7 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestScandinavianFoldingFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestScandinavianFoldingFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestScandinavianFoldingFilter.java	(working copy)
@@ -24,20 +24,28 @@
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
 
-import java.io.Reader;
-
 public class TestScandinavianFoldingFilter extends BaseTokenStreamTestCase {
+  private Analyzer analyzer;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String field) {
+        final Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        final TokenStream stream = new ScandinavianFoldingFilter(tokenizer);
+        return new TokenStreamComponents(tokenizer, stream);
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
 
-
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String field) {
-      final Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      final TokenStream stream = new ScandinavianFoldingFilter(tokenizer);
-      return new TokenStreamComponents(tokenizer, stream);
-    }
-  };
-
   public void test() throws Exception {
 
     checkOneTerm(analyzer, "aeäaeeea", "aaaeea"); // should not cause ArrayOutOfBoundsException
@@ -117,6 +125,7 @@
       } 
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestScandinavianNormalizationFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestScandinavianNormalizationFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestScandinavianNormalizationFilter.java	(working copy)
@@ -24,21 +24,28 @@
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
 
-import java.io.Reader;
-
-
 public class TestScandinavianNormalizationFilter extends BaseTokenStreamTestCase {
+  private Analyzer analyzer;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String field) {
+        final Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        final TokenStream stream = new ScandinavianNormalizationFilter(tokenizer);
+        return new TokenStreamComponents(tokenizer, stream);
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
 
-
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String field) {
-      final Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      final TokenStream stream = new ScandinavianNormalizationFilter(tokenizer);
-      return new TokenStreamComponents(tokenizer, stream);
-    }
-  };
-
   public void test() throws Exception {
 
     checkOneTerm(analyzer, "aeäaeeea", "æææeea"); // should not cause ArrayIndexOutOfBoundsException
@@ -116,6 +123,7 @@
       } 
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestTrimFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestTrimFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestTrimFilter.java	(working copy)
@@ -18,7 +18,6 @@
 package org.apache.lucene.analysis.miscellaneous;
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -103,6 +102,7 @@
       } 
     };
     checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
   
   public void testEmptyTerm() throws IOException {
@@ -114,5 +114,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java	(working copy)
@@ -24,6 +24,7 @@
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.util.CharArraySet;
+import org.apache.lucene.util.IOUtils;
 import org.junit.Test;
 
 import java.io.IOException;
@@ -292,6 +293,7 @@
         new int[] { 4, 4, 11 }, 
         new int[] { 10, 15, 15 },
         new int[] { 2, 0, 1 });
+    IOUtils.close(a, a2, a3);
   }
   
   /** concat numbers + words + all */
@@ -312,6 +314,7 @@
         new int[] { 0, 0, 0, 4, 8, 8, 12 }, 
         new int[] { 3, 7, 15, 7, 11, 15, 15 },
         new int[] { 1, 0, 0, 1, 1, 0, 1 });
+    a.close();
   }
   
   /** concat numbers + words + all + preserve original */
@@ -332,6 +335,7 @@
         new int[] { 0, 0, 0, 0, 4, 8, 8, 12 }, 
         new int[] { 15, 3, 7, 15, 7, 11, 15, 15 },
         new int[] { 1, 0, 0, 0, 1, 1, 0, 1 });
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
@@ -356,6 +360,7 @@
       };
       // TODO: properly support positionLengthAttribute
       checkRandomData(random(), a, 200*RANDOM_MULTIPLIER, 20, false, false);
+      a.close();
     }
   }
   
@@ -381,6 +386,7 @@
       };
       // TODO: properly support positionLengthAttribute
       checkRandomData(random(), a, 20*RANDOM_MULTIPLIER, 8192, false, false);
+      a.close();
     }
   }
   
@@ -404,6 +410,7 @@
       };
       // depending upon options, this thing may or may not preserve the empty term
       checkAnalysisConsistency(random, a, random.nextBoolean(), "");
+      a.close();
     }
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java	(working copy)
@@ -35,7 +35,6 @@
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.util.TestUtil;
-import org.apache.lucene.util.TestUtil;
 
 /**
  * Tests {@link EdgeNGramTokenFilter} for correctness.
@@ -183,6 +182,7 @@
         }    
       };
       checkRandomData(random(), a, 100*RANDOM_MULTIPLIER);
+      a.close();
     }
   }
   
@@ -197,6 +197,7 @@
       }    
     };
     checkAnalysisConsistency(random, a, random.nextBoolean(), "");
+    a.close();
   }
 
   public void testGraphs() throws IOException {
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java	(working copy)
@@ -113,6 +113,7 @@
       };
       checkRandomData(random(), a, 100*RANDOM_MULTIPLIER, 20);
       checkRandomData(random(), a, 10*RANDOM_MULTIPLIER, 8192);
+      a.close();
     }
   }
 
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java	(working copy)
@@ -29,7 +29,6 @@
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.util.TestUtil;
-import org.apache.lucene.util.Version;
 
 import java.io.IOException;
 import java.io.StringReader;
@@ -140,6 +139,7 @@
         new int[]    {    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0 },
         new int[]    {   11,   11,   11,   11,   11,   11,   11,   11,   11,   11,   11 },
         new int[]    {     1,   0,    0,    0,    0,    0,    0,    0,    0,    0,    0  });
+    analyzer.close();
   }
   
   /** blast some random strings through the analyzer */
@@ -156,6 +156,7 @@
         }    
       };
       checkRandomData(random(), a, 200*RANDOM_MULTIPLIER, 20);
+      a.close();
     }
   }
   
@@ -170,6 +171,7 @@
       }    
     };
     checkAnalysisConsistency(random, a, random.nextBoolean(), "");
+    a.close();
   }
 
   public void testSupplementaryCharacters() throws IOException {
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java	(working copy)
@@ -123,6 +123,7 @@
       };
       checkRandomData(random(), a, 200*RANDOM_MULTIPLIER, 20);
       checkRandomData(random(), a, 10*RANDOM_MULTIPLIER, 1027);
+      a.close();
     }
   }
 
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchAnalyzer.java	(working copy)
@@ -117,6 +117,7 @@
     checkOneTerm(a, "opheffen", "opheff");
     checkOneTerm(a, "opheffende", "opheff");
     checkOneTerm(a, "opheffing", "opheff");
+    a.close();
   }
   
   public void testReusableTokenStream() throws Exception {
@@ -125,6 +126,7 @@
     checkOneTerm(a, "lichamelijk", "licham");
     checkOneTerm(a, "lichamelijke", "licham");
     checkOneTerm(a, "lichamelijkheden", "licham");
+    a.close();
   }
   
   public void testExclusionTableViaCtor() throws IOException {
@@ -132,10 +134,11 @@
     set.add("lichamelijk");
     DutchAnalyzer a = new DutchAnalyzer( CharArraySet.EMPTY_SET, set);
     assertAnalyzesTo(a, "lichamelijk lichamelijke", new String[] { "lichamelijk", "licham" });
-    
+    a.close();
+
     a = new DutchAnalyzer( CharArraySet.EMPTY_SET, set);
     assertAnalyzesTo(a, "lichamelijk lichamelijke", new String[] { "lichamelijk", "licham" });
-
+    a.close();
   }
   
   /** 
@@ -145,6 +148,7 @@
   public void testStemOverrides() throws IOException {
     DutchAnalyzer a = new DutchAnalyzer( CharArraySet.EMPTY_SET);
     checkOneTerm(a, "fiets", "fiets");
+    a.close();
   }
   
   public void testEmptyStemDictionary() throws IOException {
@@ -151,6 +155,7 @@
     DutchAnalyzer a = new DutchAnalyzer( CharArraySet.EMPTY_SET, 
         CharArraySet.EMPTY_SET, CharArrayMap.<String>emptyMap());
     checkOneTerm(a, "fiets", "fiet");
+    a.close();
   }
   
   /**
@@ -159,15 +164,20 @@
   public void testStopwordsCasing() throws IOException {
     DutchAnalyzer a = new DutchAnalyzer();
     assertAnalyzesTo(a, "Zelf", new String[] { });
+    a.close();
   }
   
   private void check(final String input, final String expected) throws Exception {
-    checkOneTerm(new DutchAnalyzer(), input, expected); 
+    Analyzer analyzer = new DutchAnalyzer();
+    checkOneTerm(analyzer, input, expected);
+    analyzer.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new DutchAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new DutchAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
   
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/no/TestNorwegianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/no/TestNorwegianAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/no/TestNorwegianAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new NorwegianAnalyzer();
+    new NorwegianAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -38,6 +38,7 @@
     checkOneTerm(a, "havnedistrikter", "havnedistrikt");
     // stopword
     assertAnalyzesTo(a, "det", new String[] {});
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -47,10 +48,13 @@
         NorwegianAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "havnedistriktene", "havnedistriktene");
     checkOneTerm(a, "havnedistrikter", "havnedistrikt");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new NorwegianAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new NorwegianAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/no/TestNorwegianLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/no/TestNorwegianLightStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/no/TestNorwegianLightStemFilter.java	(working copy)
@@ -17,9 +17,7 @@
  * limitations under the License.
  */
 
-import java.io.FileInputStream;
 import java.io.IOException;
-import java.io.Reader;
 import java.nio.file.Files;
 import java.util.Random;
 
@@ -36,19 +34,30 @@
 import static org.apache.lucene.analysis.no.NorwegianLightStemmer.BOKMAAL;
 import static org.apache.lucene.analysis.no.NorwegianLightStemmer.NYNORSK;
 
-
 /**
  * Simple tests for {@link NorwegianLightStemFilter}
  */
 public class TestNorwegianLightStemFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(source, new NorwegianLightStemFilter(source, BOKMAAL));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(source, new NorwegianLightStemFilter(source, BOKMAAL));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /** Test against a vocabulary file */
   public void testVocabulary() throws IOException {
     assertVocabulary(analyzer, Files.newInputStream(getDataPath("nb_light.txt")));
@@ -64,6 +73,7 @@
       }
     };
     assertVocabulary(analyzer, Files.newInputStream(getDataPath("nn_light.txt")));
+    analyzer.close();
   }
   
   public void testKeyword() throws IOException {
@@ -77,6 +87,7 @@
       }
     };
     checkOneTerm(a, "sekretæren", "sekretæren");
+    a.close();
   }
 
   /** blast some random strings through the analyzer */
@@ -94,5 +105,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/no/TestNorwegianMinimalStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/no/TestNorwegianMinimalStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/no/TestNorwegianMinimalStemFilter.java	(working copy)
@@ -17,9 +17,7 @@
  * limitations under the License.
  */
 
-import java.io.FileInputStream;
 import java.io.IOException;
-import java.io.Reader;
 import java.nio.file.Files;
 import java.util.Random;
 
@@ -40,14 +38,26 @@
  * Simple tests for {@link NorwegianMinimalStemFilter}
  */
 public class TestNorwegianMinimalStemFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(source, new NorwegianMinimalStemFilter(source, BOKMAAL));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(source, new NorwegianMinimalStemFilter(source, BOKMAAL));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /** Test against a Bokmål vocabulary file */
   public void testVocabulary() throws IOException {
     assertVocabulary(analyzer, Files.newInputStream(getDataPath("nb_minimal.txt")));
@@ -63,6 +73,7 @@
       }
     };
     assertVocabulary(analyzer, Files.newInputStream(getDataPath("nn_minimal.txt")));
+    analyzer.close();
   }
   
   public void testKeyword() throws IOException {
@@ -76,6 +87,7 @@
       }
     };
     checkOneTerm(a, "sekretæren", "sekretæren");
+    a.close();
   }
 
   /** blast some random strings through the analyzer */
@@ -93,5 +105,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/path/TestPathHierarchyTokenizer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/path/TestPathHierarchyTokenizer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/path/TestPathHierarchyTokenizer.java	(working copy)
@@ -227,6 +227,7 @@
     };
     // TODO: properly support positionLengthAttribute
     checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER, 20, false, false);
+    a.close();
   }
   
   /** blast some random large strings through the analyzer */
@@ -241,5 +242,6 @@
     };
     // TODO: properly support positionLengthAttribute
     checkRandomData(random, a, 100*RANDOM_MULTIPLIER, 1027, false, false);
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/path/TestReversePathHierarchyTokenizer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/path/TestReversePathHierarchyTokenizer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/path/TestReversePathHierarchyTokenizer.java	(working copy)
@@ -17,7 +17,6 @@
  * limitations under the License.
  */
 
-import java.io.Reader;
 import java.io.StringReader;
 import java.util.Random;
 
@@ -24,7 +23,6 @@
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.Analyzer.TokenStreamComponents;
 
 import static org.apache.lucene.analysis.path.ReversePathHierarchyTokenizer.DEFAULT_DELIMITER;
 import static org.apache.lucene.analysis.path.ReversePathHierarchyTokenizer.DEFAULT_SKIP;
@@ -187,6 +185,7 @@
     };
     // TODO: properly support positionLengthAttribute
     checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER, 20, false, false);
+    a.close();
   }
   
   /** blast some random large strings through the analyzer */
@@ -201,5 +200,6 @@
     };
     // TODO: properly support positionLengthAttribute
     checkRandomData(random, a, 100*RANDOM_MULTIPLIER, 1027, false, false);
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternCaptureGroupTokenFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternCaptureGroupTokenFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternCaptureGroupTokenFilter.java	(working copy)
@@ -16,7 +16,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-import java.io.Reader;
+
 import java.io.StringReader;
 import java.util.regex.Pattern;
 
@@ -606,6 +606,7 @@
     };
 
     checkRandomData(random(), a, 1000 * RANDOM_MULTIPLIER);
+    a.close();
   }
 
   private void testPatterns(String input, String[] regexes, String[] tokens,
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceCharFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceCharFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceCharFilter.java	(working copy)
@@ -324,6 +324,7 @@
       /* ASCII only input?: */
       final boolean asciiOnly = true;
       checkRandomData(random, a, 250 * RANDOM_MULTIPLIER, maxInputLength, asciiOnly);
+      a.close();
     }
   }
  }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceFilter.java	(working copy)
@@ -27,8 +27,6 @@
 import java.io.IOException;
 import java.util.regex.Pattern;
 
-/**
- */
 public class TestPatternReplaceFilter extends BaseTokenStreamTestCase {
   
   public void testReplaceAll() throws Exception {
@@ -92,6 +90,7 @@
       }    
     };
     checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
     
     Analyzer b = new Analyzer() {
       @Override
@@ -102,6 +101,7 @@
       }    
     };
     checkRandomData(random(), b, 1000*RANDOM_MULTIPLIER);
+    b.close();
   }
   
   public void testEmptyTerm() throws IOException {
@@ -113,6 +113,7 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer.java	(working copy)
@@ -18,7 +18,6 @@
 package org.apache.lucene.analysis.pattern;
 
 import java.io.IOException;
-import java.io.Reader;
 import java.io.StringReader;
 import java.util.ArrayList;
 import java.util.List;
@@ -29,10 +28,8 @@
 import org.apache.lucene.analysis.CharFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.Analyzer.TokenStreamComponents;
 import org.apache.lucene.analysis.charfilter.MappingCharFilter;
 import org.apache.lucene.analysis.charfilter.NormalizeCharMap;
-import org.apache.lucene.analysis.path.PathHierarchyTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 
 public class TestPatternTokenizer extends BaseTokenStreamTestCase 
@@ -137,6 +134,7 @@
       }    
     };
     checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
     
     Analyzer b = new Analyzer() {
       @Override
@@ -146,5 +144,6 @@
       }    
     };
     checkRandomData(random(), b, 1000*RANDOM_MULTIPLIER);
+    b.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new PortugueseAnalyzer();
+    new PortugueseAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -38,6 +38,7 @@
     checkOneTerm(a, "quilométricos", "quilometric");
     // stopword
     assertAnalyzesTo(a, "não", new String[] {});
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -47,10 +48,13 @@
         PortugueseAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "quilométricas", "quilométricas");
     checkOneTerm(a, "quilométricos", "quilometric");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new PortugueseAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new PortugueseAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseLightStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseLightStemFilter.java	(working copy)
@@ -34,14 +34,26 @@
  * Simple tests for {@link PortugueseLightStemFilter}
  */
 public class TestPortugueseLightStemFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new MockTokenizer(MockTokenizer.SIMPLE, true);
-      return new TokenStreamComponents(source, new PortugueseLightStemFilter(source));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new MockTokenizer(MockTokenizer.SIMPLE, true);
+        return new TokenStreamComponents(source, new PortugueseLightStemFilter(source));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /**
    * Test the example from the paper "Assessing the impact of stemming accuracy
    * on information retrieval"
@@ -102,6 +114,7 @@
       }
     };
     checkOneTerm(a, "quilométricas", "quilométricas");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
@@ -118,5 +131,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseMinimalStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseMinimalStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseMinimalStemFilter.java	(working copy)
@@ -34,14 +34,26 @@
  * Simple tests for {@link PortugueseMinimalStemFilter}
  */
 public class TestPortugueseMinimalStemFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new MockTokenizer(MockTokenizer.SIMPLE, true);
-      return new TokenStreamComponents(source, new PortugueseMinimalStemFilter(source));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new MockTokenizer(MockTokenizer.SIMPLE, true);
+        return new TokenStreamComponents(source, new PortugueseMinimalStemFilter(source));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /**
    * Test the example from the paper "Assessing the impact of stemming accuracy
    * on information retrieval"
@@ -76,6 +88,7 @@
       }
     };
     checkOneTerm(a, "quilométricas", "quilométricas");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
@@ -92,5 +105,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/pt/TestPortugueseStemFilter.java	(working copy)
@@ -34,14 +34,26 @@
  * Simple tests for {@link PortugueseStemFilter}
  */
 public class TestPortugueseStemFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new MockTokenizer(MockTokenizer.SIMPLE, true);
-      return new TokenStreamComponents(source, new PortugueseStemFilter(source));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new MockTokenizer(MockTokenizer.SIMPLE, true);
+        return new TokenStreamComponents(source, new PortugueseStemFilter(source));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /**
    * Test the example from the paper "Assessing the impact of stemming accuracy
    * on information retrieval"
@@ -76,6 +88,7 @@
       }
     };
     checkOneTerm(a, "quilométricas", "quilométricas");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
@@ -92,5 +105,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java	(working copy)
@@ -58,6 +58,7 @@
 
   @Override
   public void tearDown() throws Exception {
+    appAnalyzer.close();
     reader.close();
     super.tearDown();
   }
@@ -70,6 +71,7 @@
 
     protectedTokenStream = protectedAnalyzer.tokenStream("repetitiveField", "boring");
     assertTokenStreamContents(protectedTokenStream, new String[]{"boring"});
+    protectedAnalyzer.close();
   }
 
   public void testDefaultStopwordsAllFields() throws Exception {
@@ -76,6 +78,7 @@
     protectedAnalyzer = new QueryAutoStopWordAnalyzer( appAnalyzer, reader);
     TokenStream protectedTokenStream = protectedAnalyzer.tokenStream("repetitiveField", "boring");
     assertTokenStreamContents(protectedTokenStream, new String[0]); // Default stop word filtering will remove boring
+    protectedAnalyzer.close();
   }
 
   public void testStopwordsAllFieldsMaxPercentDocs() throws Exception {
@@ -88,11 +91,13 @@
     protectedTokenStream = protectedAnalyzer.tokenStream("repetitiveField", "vaguelyboring");
      // A filter on terms in > half of docs should not remove vaguelyBoring
     assertTokenStreamContents(protectedTokenStream, new String[]{"vaguelyboring"});
+    protectedAnalyzer.close();
 
     protectedAnalyzer = new QueryAutoStopWordAnalyzer( appAnalyzer, reader, 1f / 4f);
     protectedTokenStream = protectedAnalyzer.tokenStream("repetitiveField", "vaguelyboring");
      // A filter on terms in > quarter of docs should remove vaguelyBoring
     assertTokenStreamContents(protectedTokenStream, new String[0]);
+    protectedAnalyzer.close();
   }
 
   public void testStopwordsPerFieldMaxPercentDocs() throws Exception {
@@ -100,11 +105,13 @@
     TokenStream protectedTokenStream = protectedAnalyzer.tokenStream("repetitiveField", "boring");
     // A filter on one Field should not affect queries on another
     assertTokenStreamContents(protectedTokenStream, new String[]{"boring"});
+    protectedAnalyzer.close();
 
     protectedAnalyzer = new QueryAutoStopWordAnalyzer( appAnalyzer, reader, Arrays.asList("variedField", "repetitiveField"), 1f / 2f);
     protectedTokenStream = protectedAnalyzer.tokenStream("repetitiveField", "boring");
     // A filter on the right Field should affect queries on it
     assertTokenStreamContents(protectedTokenStream, new String[0]);
+    protectedAnalyzer.close();
   }
 
   public void testStopwordsPerFieldMaxDocFreq() throws Exception {
@@ -111,10 +118,12 @@
     protectedAnalyzer = new QueryAutoStopWordAnalyzer( appAnalyzer, reader, Arrays.asList("repetitiveField"), 10);
     int numStopWords = protectedAnalyzer.getStopWords("repetitiveField").length;
     assertTrue("Should have identified stop words", numStopWords > 0);
+    protectedAnalyzer.close();
 
     protectedAnalyzer = new QueryAutoStopWordAnalyzer( appAnalyzer, reader, Arrays.asList("repetitiveField", "variedField"), 10);
     int numNewStopWords = protectedAnalyzer.getStopWords("repetitiveField").length + protectedAnalyzer.getStopWords("variedField").length;
     assertTrue("Should have identified more stop words", numNewStopWords > numStopWords);
+    protectedAnalyzer.close();
   }
 
   public void testNoFieldNamePollution() throws Exception {
@@ -127,6 +136,7 @@
     protectedTokenStream = protectedAnalyzer.tokenStream("variedField", "boring");
     // Filter should not prevent stopwords in one field being used in another
     assertTokenStreamContents(protectedTokenStream, new String[]{"boring"});
+    protectedAnalyzer.close();
   }
   
   public void testTokenStream() throws Exception {
@@ -134,5 +144,6 @@
         new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false), reader, 10);
     TokenStream ts = a.tokenStream("repetitiveField", "this boring");
     assertTokenStreamContents(ts, new String[] { "this" });
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java	(working copy)
@@ -101,6 +101,7 @@
       }
     };
     checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
   
   public void testEmptyTerm() throws IOException {
@@ -112,5 +113,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ro/TestRomanianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ro/TestRomanianAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ro/TestRomanianAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new RomanianAnalyzer();
+    new RomanianAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -38,6 +38,7 @@
     checkOneTerm(a, "absenţi", "absenţ");
     // stopword
     assertAnalyzesTo(a, "îl", new String[] {});
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -47,10 +48,13 @@
         RomanianAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "absenţa", "absenţa");
     checkOneTerm(a, "absenţi", "absenţ");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new RomanianAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new RomanianAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java	(working copy)
@@ -28,34 +28,38 @@
  */
 
 public class TestRussianAnalyzer extends BaseTokenStreamTestCase {
-
-     /** Check that RussianAnalyzer doesnt discard any numbers */
-    public void testDigitsInRussianCharset() throws IOException
-    {
-      RussianAnalyzer ra = new RussianAnalyzer();
-      assertAnalyzesTo(ra, "text 1000", new String[] { "text", "1000" });
-    }
-    
-    public void testReusableTokenStream() throws Exception {
-      Analyzer a = new RussianAnalyzer();
-      assertAnalyzesTo(a, "Вместе с тем о силе электромагнитной энергии имели представление еще",
-          new String[] { "вмест", "сил", "электромагнитн", "энерг", "имел", "представлен" });
-      assertAnalyzesTo(a, "Но знание это хранилось в тайне",
-          new String[] { "знан", "эт", "хран", "тайн" });
-    }
-    
-    
-    public void testWithStemExclusionSet() throws Exception {
-      CharArraySet set = new CharArraySet( 1, true);
-      set.add("представление");
-      Analyzer a = new RussianAnalyzer( RussianAnalyzer.getDefaultStopSet() , set);
-      assertAnalyzesTo(a, "Вместе с тем о силе электромагнитной энергии имели представление еще",
-          new String[] { "вмест", "сил", "электромагнитн", "энерг", "имел", "представление" });
-     
-    }
-    
-    /** blast some random strings through the analyzer */
-    public void testRandomStrings() throws Exception {
-      checkRandomData(random(), new RussianAnalyzer(), 1000*RANDOM_MULTIPLIER);
-    }
+  
+  /** Check that RussianAnalyzer doesnt discard any numbers */
+  public void testDigitsInRussianCharset() throws IOException
+  {
+    RussianAnalyzer ra = new RussianAnalyzer();
+    assertAnalyzesTo(ra, "text 1000", new String[] { "text", "1000" });
+    ra.close();
+  }
+  
+  public void testReusableTokenStream() throws Exception {
+    Analyzer a = new RussianAnalyzer();
+    assertAnalyzesTo(a, "Вместе с тем о силе электромагнитной энергии имели представление еще",
+        new String[] { "вмест", "сил", "электромагнитн", "энерг", "имел", "представлен" });
+    assertAnalyzesTo(a, "Но знание это хранилось в тайне",
+        new String[] { "знан", "эт", "хран", "тайн" });
+    a.close();
+  }
+  
+  
+  public void testWithStemExclusionSet() throws Exception {
+    CharArraySet set = new CharArraySet( 1, true);
+    set.add("представление");
+    Analyzer a = new RussianAnalyzer( RussianAnalyzer.getDefaultStopSet() , set);
+    assertAnalyzesTo(a, "Вместе с тем о силе электромагнитной энергии имели представление еще",
+        new String[] { "вмест", "сил", "электромагнитн", "энерг", "имел", "представление" });
+    a.close();
+  }
+  
+  /** blast some random strings through the analyzer */
+  public void testRandomStrings() throws Exception {
+    Analyzer analyzer = new RussianAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
+  }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianLightStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianLightStemFilter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -35,14 +34,26 @@
  * Simple tests for {@link RussianLightStemFilter}
  */
 public class TestRussianLightStemFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(source, new RussianLightStemFilter(source));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(source, new RussianLightStemFilter(source));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /** Test against a vocabulary from the reference impl */
   public void testVocabulary() throws IOException {
     assertVocabulary(analyzer, getDataPath("rulighttestdata.zip"), "rulight.txt");
@@ -59,6 +70,7 @@
       }
     };
     checkOneTerm(a, "энергии", "энергии");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
@@ -75,5 +87,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java	(working copy)
@@ -83,6 +83,7 @@
   public void tearDown() throws Exception {
     reader.close();
     directory.close();
+    analyzer.close();
     super.tearDown();
   }
 
@@ -156,6 +157,7 @@
         new int[] { 0, 0, 7, 7, 10, 10, 13 },
         new int[] { 6, 9, 9, 12, 12, 18, 18 },
         new int[] { 1, 0, 1, 0, 1, 0, 1 });
+    a.close();
   }
 
   public void testNonDefaultMinShingleSize() throws Exception {
@@ -171,6 +173,7 @@
                           new int[] { 0,  0,  0,  7,  7,  7, 14, 14, 14, 19, 19, 28, 33 },
                           new int[] { 6, 18, 27, 13, 27, 32, 18, 32, 41, 27, 41, 32, 41 },
                           new int[] { 1,  0,  0,  1,  0,  0,  1,  0,  0,  1,  0,  1,  1 });
+    analyzer.close();
 
     analyzer = new ShingleAnalyzerWrapper(
         new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false), 3, 4,
@@ -183,6 +186,7 @@
                           new int[] {  0,  0,  7,  7, 14, 14, 19 },
                           new int[] { 18, 27, 27, 32, 32, 41, 41 },
                           new int[] {  1,  0,  1,  0,  1,  0,  1 });
+    analyzer.close();
   }
   
   public void testNonDefaultMinAndSameMaxShingleSize() throws Exception {
@@ -198,6 +202,7 @@
                           new int[] { 0,  0,  7,  7, 14, 14, 19, 19, 28, 33 },
                           new int[] { 6, 18, 13, 27, 18, 32, 27, 41, 32, 41 },
                           new int[] { 1,  0,  1,  0,  1,  0,  1,  0,  1,  1 });
+    analyzer.close();
 
     analyzer = new ShingleAnalyzerWrapper(
         new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false), 3, 3,
@@ -210,6 +215,7 @@
                           new int[] {  0,  7, 14, 19 },
                           new int[] { 18, 27, 32, 41 },
                           new int[] {  1,  1,  1,  1 });
+    analyzer.close();
   }
 
   public void testNoTokenSeparator() throws Exception {
@@ -227,6 +233,7 @@
                           new int[] { 0,  0,  7,  7, 14, 14, 19 },
                           new int[] { 6, 13, 13, 18, 18, 27, 27 },
                           new int[] { 1,  0,  1,  0,  1,  0,  1 });
+    analyzer.close();
 
     analyzer = new ShingleAnalyzerWrapper(
         new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false),
@@ -241,6 +248,7 @@
                           new int[] {  0,  7, 14 },
                           new int[] { 13, 18, 27 },
                           new int[] {  1,  1,  1 });
+    analyzer.close();
   }
 
   public void testNullTokenSeparator() throws Exception {
@@ -258,6 +266,7 @@
                           new int[] { 0,  0,  7,  7, 14, 14, 19 },
                           new int[] { 6, 13, 13, 18, 18, 27, 27 },
                           new int[] { 1,  0,  1,  0,  1,  0,  1 });
+    analyzer.close();
 
     analyzer = new ShingleAnalyzerWrapper(
         new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false),
@@ -272,6 +281,7 @@
                           new int[] {  0,  7, 14 },
                           new int[] { 13, 18, 27 },
                           new int[] {  1,  1,  1 });
+    analyzer.close();
   }
 
   public void testAltTokenSeparator() throws Exception {
@@ -289,6 +299,7 @@
                           new int[] { 0,  0,  7,  7, 14, 14, 19 },
                           new int[] { 6, 13, 13, 18, 18, 27, 27 },
                           new int[] { 1,  0,  1,  0,  1,  0,  1 });
+    analyzer.close();
 
     analyzer = new ShingleAnalyzerWrapper(
         new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false),
@@ -303,6 +314,7 @@
                           new int[] {  0,  7, 14 },
                           new int[] { 13, 18, 27 },
                           new int[] {  1,  1,  1 });
+    analyzer.close();
   }
 
   public void testAltFillerToken() throws Exception {
@@ -329,7 +341,17 @@
                      new int[] { 0,  0,  7,  7, 19, 19 },
                      new int[] { 6, 13, 13, 19, 27, 27 },
                      new int[] { 1,  0,  1,  0,  1,  1 });
+    analyzer.close();
 
+    delegate = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        CharArraySet stopSet = StopFilter.makeStopSet("into");
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        TokenFilter filter = new StopFilter(tokenizer, stopSet);
+        return new TokenStreamComponents(tokenizer, filter);
+      }
+    };
     analyzer = new ShingleAnalyzerWrapper(
         delegate,
         ShingleFilter.DEFAULT_MIN_SHINGLE_SIZE,
@@ -341,7 +363,17 @@
                      new int[] {  0,  7, 19 },
                      new int[] { 13, 19, 27 },
                      new int[] {  1,  1,  1 });
+    analyzer.close();
 
+    delegate = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        CharArraySet stopSet = StopFilter.makeStopSet("into");
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        TokenFilter filter = new StopFilter(tokenizer, stopSet);
+        return new TokenStreamComponents(tokenizer, filter);
+      }
+    };
     analyzer = new ShingleAnalyzerWrapper(
         delegate,
         ShingleFilter.DEFAULT_MIN_SHINGLE_SIZE,
@@ -353,6 +385,7 @@
                      new int[] {  0,  7, 19 },
                      new int[] { 13, 19, 27 },
                      new int[] {  1,  1,  1 });
+    analyzer.close();
   }
 
   public void testOutputUnigramsIfNoShinglesSingleToken() throws Exception {
@@ -367,5 +400,6 @@
                           new int[] { 0 },
                           new int[] { 6 },
                           new int[] { 1 });
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java	(working copy)
@@ -1113,6 +1113,7 @@
       }
     };
     checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
   
   /** blast some random large strings through the analyzer */
@@ -1126,6 +1127,7 @@
       }
     };
     checkRandomData(random, a, 100*RANDOM_MULTIPLIER, 8192);
+    a.close();
   }
   
   public void testEmptyTerm() throws IOException {
@@ -1137,6 +1139,7 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 
   public void testTrailingHole1() throws IOException {
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter.java	(working copy)
@@ -123,6 +123,7 @@
     assertEquals(DocIdSetIterator.NO_MORE_DOCS, positions.nextDoc());
     r.close();
     dir.close();
+    analyzer.close();
   }
   
   public void testGeneral() throws IOException {
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java	(working copy)
@@ -48,6 +48,7 @@
     
     assertAnalyzesTo(a, "he abhorred accents",
         new String[]{"he", "abhor", "accent"});
+    a.close();
   }
   
   public void testFilterTokens() throws Exception {
@@ -113,6 +114,7 @@
         }
       };
       checkOneTerm(a, "", "");
+      a.close();
     }
   }
   
@@ -131,5 +133,6 @@
       }  
     };
     checkRandomData(random(), a, 100*RANDOM_MULTIPLIER);
+    a.close();
   }
 }
\ No newline at end of file
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowballVocab.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowballVocab.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowballVocab.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.Tokenizer;
@@ -80,5 +79,6 @@
     
     assertVocabulary(a, getDataPath("TestSnowballVocabData.zip"), 
         dataDirectory + "/voc.txt", dataDirectory + "/output.txt");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/sr/TestSerbianNormalizationFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/sr/TestSerbianNormalizationFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/sr/TestSerbianNormalizationFilter.java	(working copy)
@@ -30,15 +30,27 @@
  * Tests {@link SerbianNormalizationFilter}
  */
 public class TestSerbianNormalizationFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String field) {
-      final Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      final TokenStream stream = new SerbianNormalizationFilter(tokenizer);
-      return new TokenStreamComponents(tokenizer, stream);
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        final Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        final TokenStream stream = new SerbianNormalizationFilter(tokenizer);
+        return new TokenStreamComponents(tokenizer, stream);
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /**
    * Tests Cyrillic text.
    */
@@ -67,5 +79,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer.java	(working copy)
@@ -35,17 +35,29 @@
 
 import java.io.IOException;
 import java.util.Arrays;
-import java.util.Random;
 
 /** tests for classicanalyzer */
 public class TestClassicAnalyzer extends BaseTokenStreamTestCase {
 
-  private Analyzer  a = new ClassicAnalyzer();
+  private Analyzer a;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    a = new ClassicAnalyzer();
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    a.close();
+    super.tearDown();
+  }
 
   public void testMaxTermLength() throws Exception {
     ClassicAnalyzer sa = new ClassicAnalyzer();
     sa.setMaxTokenLength(5);
     assertAnalyzesTo(sa, "ab cd toolong xy z", new String[]{"ab", "cd", "xy", "z"});
+    sa.close();
   }
 
   public void testMaxTermLength2() throws Exception {
@@ -54,6 +66,7 @@
     sa.setMaxTokenLength(5);
     
     assertAnalyzesTo(sa, "ab cd toolong xy z", new String[]{"ab", "cd", "xy", "z"}, new int[]{1, 1, 2, 1});
+    sa.close();
   }
 
   public void testMaxTermLength3() throws Exception {
@@ -115,6 +128,7 @@
     try {
       ClassicAnalyzer analyzer = new ClassicAnalyzer();
       assertAnalyzesTo(analyzer, "www.nutch.org.", new String[]{ "www.nutch.org" }, new String[] { "<HOST>" });
+      analyzer.close();
     } catch (NullPointerException e) {
       fail("Should not throw an NPE and it did");
     }
@@ -137,8 +151,10 @@
 
     // 2.4 should not show the bug. But, alas, it's also obsolete,
     // so we check latest released (Robert's gonna break this on 4.0 soon :) )
+    a2.close();
     a2 = new ClassicAnalyzer();
     assertAnalyzesTo(a2, "www.nutch.org.", new String[]{ "www.nutch.org" }, new String[] { "<HOST>" });
+    a2.close();
   }
 
   public void testEMailAddresses() throws Exception {
@@ -246,6 +262,7 @@
   public void testJava14BWCompatibility() throws Exception {
     ClassicAnalyzer sa = new ClassicAnalyzer();
     assertAnalyzesTo(sa, "test\u02C6test", new String[] { "test", "test" });
+    sa.close();
   }
 
   /**
@@ -253,7 +270,8 @@
   */
   public void testWickedLongTerm() throws IOException {
     RAMDirectory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new ClassicAnalyzer()));
+    Analyzer analyzer = new ClassicAnalyzer();
+    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(analyzer));
 
     char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];
     Arrays.fill(chars, 'x');
@@ -309,16 +327,21 @@
     reader.close();
 
     dir.close();
+    analyzer.close();
+    sa.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new ClassicAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new ClassicAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
   
   /** blast some random large strings through the analyzer */
   public void testRandomHugeStrings() throws Exception {
-    Random random = random();
-    checkRandomData(random, new ClassicAnalyzer(), 100*RANDOM_MULTIPLIER, 8192);
+    Analyzer analyzer = new ClassicAnalyzer();
+    checkRandomData(random(), analyzer, 100*RANDOM_MULTIPLIER, 8192);
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestStandardAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestStandardAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestStandardAnalyzer.java	(working copy)
@@ -114,15 +114,26 @@
     BaseTokenStreamTestCase.assertTokenStreamContents(tokenizer, new String[] { "testing", "1234" });
   }
 
-  private Analyzer a = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
+  private Analyzer a;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new StandardTokenizer(newAttributeFactory());
+        return new TokenStreamComponents(tokenizer);
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    a.close();
+    super.tearDown();
+  }
 
-      Tokenizer tokenizer = new StandardTokenizer(newAttributeFactory());
-      return new TokenStreamComponents(tokenizer);
-    }
-  };
-
   public void testArmenian() throws Exception {
     BaseTokenStreamTestCase.assertAnalyzesTo(a, "Վիքիպեդիայի 13 միլիոն հոդվածները (4,600` հայերեն վիքիպեդիայում) գրվել են կամավորների կողմից ու համարյա բոլոր հոդվածները կարող է խմբագրել ցանկաց մարդ ով կարող է բացել Վիքիպեդիայի կայքը։",
         new String[] { "Վիքիպեդիայի", "13", "միլիոն", "հոդվածները", "4,600", "հայերեն", "վիքիպեդիայում", "գրվել", "են", "կամավորների", "կողմից", 
@@ -350,27 +361,30 @@
 
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new StandardAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new StandardAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
   
   /** blast some random large strings through the analyzer */
   public void testRandomHugeStrings() throws Exception {
-    Random random = random();
-    checkRandomData(random, new StandardAnalyzer(), 100*RANDOM_MULTIPLIER, 8192);
+    Analyzer analyzer = new StandardAnalyzer();
+    checkRandomData(random(), analyzer, 100*RANDOM_MULTIPLIER, 8192);
+    analyzer.close();
   }
 
   // Adds random graph after:
   public void testRandomHugeStringsGraphAfter() throws Exception {
     Random random = random();
-    checkRandomData(random,
-                    new Analyzer() {
-                      @Override
-                      protected TokenStreamComponents createComponents(String fieldName) {
-                        Tokenizer tokenizer = new StandardTokenizer(newAttributeFactory());
-                        TokenStream tokenStream = new MockGraphTokenFilter(random(), tokenizer);
-                        return new TokenStreamComponents(tokenizer, tokenStream);
-                      }
-                    },
-                    100*RANDOM_MULTIPLIER, 8192);
+    Analyzer analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new StandardTokenizer(newAttributeFactory());
+        TokenStream tokenStream = new MockGraphTokenFilter(random(), tokenizer);
+        return new TokenStreamComponents(tokenizer, tokenStream);
+      }
+    };
+    checkRandomData(random, analyzer, 100*RANDOM_MULTIPLIER, 8192);
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestUAX29URLEmailAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestUAX29URLEmailAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestUAX29URLEmailAnalyzer.java	(working copy)
@@ -26,7 +26,19 @@
 
 public class TestUAX29URLEmailAnalyzer extends BaseTokenStreamTestCase {
 
-  private Analyzer a = new UAX29URLEmailAnalyzer();
+  private Analyzer a;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    a = new UAX29URLEmailAnalyzer();
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    a.close();
+    super.tearDown();
+  }
 
   public void testHugeDoc() throws IOException {
     StringBuilder sb = new StringBuilder();
@@ -343,6 +355,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new UAX29URLEmailAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestUAX29URLEmailTokenizer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestUAX29URLEmailTokenizer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestUAX29URLEmailTokenizer.java	(working copy)
@@ -8,6 +8,7 @@
 import org.apache.lucene.analysis.standard.UAX29URLEmailTokenizer;
 import org.apache.lucene.analysis.standard.WordBreakTestUnicode_6_3_0;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.TestUtil;
 
 import java.io.BufferedReader;
@@ -88,16 +89,43 @@
     BaseTokenStreamTestCase.assertTokenStreamContents(tokenizer, new String[] { "testing", "1234" });
   }
 
-  private Analyzer a = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
+  private Analyzer a, urlAnalyzer, emailAnalyzer;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new UAX29URLEmailTokenizer(newAttributeFactory());
+        return new TokenStreamComponents(tokenizer);
+      }
+    };
+    urlAnalyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        UAX29URLEmailTokenizer tokenizer = new UAX29URLEmailTokenizer(newAttributeFactory());
+        tokenizer.setMaxTokenLength(Integer.MAX_VALUE);  // Tokenize arbitrary length URLs
+        TokenFilter filter = new URLFilter(tokenizer);
+        return new TokenStreamComponents(tokenizer, filter);
+      }
+    };
+    emailAnalyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        UAX29URLEmailTokenizer tokenizer = new UAX29URLEmailTokenizer(newAttributeFactory());
+        TokenFilter filter = new EmailFilter(tokenizer);
+        return new TokenStreamComponents(tokenizer, filter);
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    IOUtils.close(a, urlAnalyzer, emailAnalyzer);
+    super.tearDown();
+  }
 
-      Tokenizer tokenizer = new UAX29URLEmailTokenizer(newAttributeFactory());
-      return new TokenStreamComponents(tokenizer);
-    }
-  };
-
-
   /** Passes through tokens with type "<URL>" and blocks all other types. */
   private class URLFilter extends TokenFilter {
     private final TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
@@ -134,28 +162,8 @@
       }
       return isTokenAvailable;
     }
-  }
-
-  private Analyzer urlAnalyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      UAX29URLEmailTokenizer tokenizer = new UAX29URLEmailTokenizer(newAttributeFactory());
-      tokenizer.setMaxTokenLength(Integer.MAX_VALUE);  // Tokenize arbitrary length URLs
-      TokenFilter filter = new URLFilter(tokenizer);
-      return new TokenStreamComponents(tokenizer, filter);
-    }
-  };
-
-  private Analyzer emailAnalyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      UAX29URLEmailTokenizer tokenizer = new UAX29URLEmailTokenizer(newAttributeFactory());
-      TokenFilter filter = new EmailFilter(tokenizer);
-      return new TokenStreamComponents(tokenizer, filter);
-    }
-  };
+  }  
   
-  
   public void testArmenian() throws Exception {
     BaseTokenStreamTestCase.assertAnalyzesTo(a, "Վիքիպեդիայի 13 միլիոն հոդվածները (4,600` հայերեն վիքիպեդիայում) գրվել են կամավորների կողմից ու համարյա բոլոր հոդվածները կարող է խմբագրել ցանկաց մարդ ով կարող է բացել Վիքիպեդիայի կայքը։",
         new String[] { "Վիքիպեդիայի", "13", "միլիոն", "հոդվածները", "4,600", "հայերեն", "վիքիպեդիայում", "գրվել", "են", "կամավորների", "կողմից", 
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/sv/TestSwedishAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/sv/TestSwedishAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/sv/TestSwedishAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new SwedishAnalyzer();
+    new SwedishAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -38,6 +38,7 @@
     checkOneTerm(a, "jaktkarlens", "jaktkarl");
     // stopword
     assertAnalyzesTo(a, "och", new String[] {});
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -47,10 +48,13 @@
         SwedishAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "jaktkarlarne", "jaktkarlarne");
     checkOneTerm(a, "jaktkarlens", "jaktkarl");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new SwedishAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new SwedishAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/sv/TestSwedishLightStemFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/sv/TestSwedishLightStemFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/sv/TestSwedishLightStemFilter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -35,14 +34,26 @@
  * Simple tests for {@link SwedishLightStemFilter}
  */
 public class TestSwedishLightStemFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(source, new SwedishLightStemFilter(source));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(source, new SwedishLightStemFilter(source));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /** Test against a vocabulary from the reference impl */
   public void testVocabulary() throws IOException {
     assertVocabulary(analyzer, getDataPath("svlighttestdata.zip"), "svlight.txt");
@@ -59,6 +70,7 @@
       }
     };
     checkOneTerm(a, "jaktkarlens", "jaktkarlens");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
@@ -75,5 +87,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSolrSynonymParser.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSolrSynonymParser.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSolrSynonymParser.java	(working copy)
@@ -17,7 +17,6 @@
  * limitations under the License.
  */
 
-import java.io.Reader;
 import java.io.StringReader;
 import java.text.ParseException;
 
@@ -27,7 +26,6 @@
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.en.EnglishAnalyzer;
-import org.junit.Test;
 
 /**
  * Tests parser for the Solr synonyms format
@@ -43,11 +41,13 @@
     "foo => baz\n" +
     "this test, that testing";
     
-    SolrSynonymParser parser = new SolrSynonymParser(true, true, new MockAnalyzer(random()));
+    Analyzer analyzer = new MockAnalyzer(random());
+    SolrSynonymParser parser = new SolrSynonymParser(true, true, analyzer);
     parser.parse(new StringReader(testFile));
     final SynonymMap map = parser.build();
+    analyzer.close();
     
-    Analyzer analyzer = new Analyzer() {
+    analyzer = new Analyzer() {
       @Override
       protected TokenStreamComponents createComponents(String fieldName) {
         Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);
@@ -70,46 +70,77 @@
     assertAnalyzesTo(analyzer, "this test",
         new String[] { "this", "that", "test", "testing" },
         new int[] { 1, 0, 1, 0 });
+    analyzer.close();
   }
   
   /** parse a syn file with bad syntax */
-  @Test(expected=ParseException.class)
   public void testInvalidDoubleMap() throws Exception {
-    String testFile = "a => b => c"; 
-    SolrSynonymParser parser = new SolrSynonymParser(true, true, new MockAnalyzer(random()));
-    parser.parse(new StringReader(testFile));
+    String testFile = "a => b => c";
+    Analyzer analyzer = new MockAnalyzer(random());
+    SolrSynonymParser parser = new SolrSynonymParser(true, true, analyzer);
+    try {
+      parser.parse(new StringReader(testFile));
+      fail("didn't get expected exception");
+    } catch (ParseException expected) {
+      // expected exc
+    }
+    analyzer.close();
   }
   
   /** parse a syn file with bad syntax */
-  @Test(expected=ParseException.class)
   public void testInvalidAnalyzesToNothingOutput() throws Exception {
     String testFile = "a => 1"; 
-    SolrSynonymParser parser = new SolrSynonymParser(true, true, new MockAnalyzer(random(), MockTokenizer.SIMPLE, false));
-    parser.parse(new StringReader(testFile));
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, false);
+    SolrSynonymParser parser = new SolrSynonymParser(true, true, analyzer);
+    try {
+      parser.parse(new StringReader(testFile));
+      fail("didn't get expected exception");
+    } catch (ParseException expected) {
+      // expected exc
+    }
+    analyzer.close();
   }
   
   /** parse a syn file with bad syntax */
-  @Test(expected=ParseException.class)
   public void testInvalidAnalyzesToNothingInput() throws Exception {
-    String testFile = "1 => a"; 
-    SolrSynonymParser parser = new SolrSynonymParser(true, true, new MockAnalyzer(random(), MockTokenizer.SIMPLE, false));
-    parser.parse(new StringReader(testFile));
+    String testFile = "1 => a";
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, false);
+    SolrSynonymParser parser = new SolrSynonymParser(true, true, analyzer);
+    try {
+      parser.parse(new StringReader(testFile));
+      fail("didn't get expected exception");
+    } catch (ParseException expected) {
+      // expected exc
+    }
+    analyzer.close();
   }
   
   /** parse a syn file with bad syntax */
-  @Test(expected=ParseException.class)
   public void testInvalidPositionsInput() throws Exception {
     String testFile = "testola => the test";
-    SolrSynonymParser parser = new SolrSynonymParser(true, true, new EnglishAnalyzer());
-    parser.parse(new StringReader(testFile));
+    Analyzer analyzer = new EnglishAnalyzer();
+    SolrSynonymParser parser = new SolrSynonymParser(true, true, analyzer);
+    try {
+      parser.parse(new StringReader(testFile));
+      fail("didn't get expected exception");
+    } catch (ParseException expected) {
+      // expected exc
+    }
+    analyzer.close();
   }
   
   /** parse a syn file with bad syntax */
-  @Test(expected=ParseException.class)
   public void testInvalidPositionsOutput() throws Exception {
     String testFile = "the test => testola";
-    SolrSynonymParser parser = new SolrSynonymParser(true, true, new EnglishAnalyzer());
-    parser.parse(new StringReader(testFile));
+    Analyzer analyzer = new EnglishAnalyzer();
+    SolrSynonymParser parser = new SolrSynonymParser(true, true, analyzer);
+    try {
+      parser.parse(new StringReader(testFile));
+      fail("didn't get expected exception");
+    } catch (ParseException expected) {
+      // expected exc
+    }
+    analyzer.close();
   }
   
   /** parse a syn file with some escaped syntax chars */
@@ -117,10 +148,12 @@
     String testFile = 
       "a\\=>a => b\\=>b\n" +
       "a\\,a => b\\,b";
-    SolrSynonymParser parser = new SolrSynonymParser(true, true, new MockAnalyzer(random(), MockTokenizer.KEYWORD, false));
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);
+    SolrSynonymParser parser = new SolrSynonymParser(true, true, analyzer);
     parser.parse(new StringReader(testFile));
     final SynonymMap map = parser.build();
-    Analyzer analyzer = new Analyzer() {
+    analyzer.close();
+    analyzer = new Analyzer() {
       @Override
       protected TokenStreamComponents createComponents(String fieldName) {
         Tokenizer tokenizer = new MockTokenizer(MockTokenizer.KEYWORD, false);
@@ -139,5 +172,6 @@
     assertAnalyzesTo(analyzer, "a,a",
         new String[] { "b,b" },
         new int[] { 1 });
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymMapFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymMapFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymMapFilter.java	(working copy)
@@ -37,7 +37,6 @@
 import org.apache.lucene.analysis.MockGraphTokenFilter;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.tokenattributes.*;
-import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.CharsRefBuilder;
 import org.apache.lucene.util.TestUtil;
 
@@ -166,6 +165,7 @@
                      new int[] {1, 1},
                      true);
     checkAnalysisConsistency(random(), analyzer, false, "a b c");
+    analyzer.close();
   }
 
   public void testDoKeepOrig() throws Exception {
@@ -191,6 +191,7 @@
                      new int[] {1, 2, 1, 1},
                      true);
     checkAnalysisConsistency(random(), analyzer, false, "a b c");
+    analyzer.close();
   }
 
   public void testBasic() throws Exception {
@@ -502,6 +503,7 @@
       };
 
       checkRandomData(random(), analyzer, 100);
+      analyzer.close();
     }
   }
 
@@ -560,6 +562,7 @@
       };
 
       checkRandomData(random, analyzer, 100);
+      analyzer.close();
     }
   }
   
@@ -584,6 +587,7 @@
       };
 
       checkAnalysisConsistency(random, analyzer, random.nextBoolean(), "");
+      analyzer.close();
     }
   }
   
@@ -613,6 +617,7 @@
       };
 
       checkRandomData(random, analyzer, 100, 1024);
+      analyzer.close();
     }
   }
   
@@ -621,10 +626,11 @@
     String testFile = 
       "aaa => aaaa1 aaaa2 aaaa3\n" + 
       "bbb => bbbb1 bbbb2\n";
-      
-    SolrSynonymParser parser = new SolrSynonymParser(true, true, new MockAnalyzer(random()));
+    Analyzer synAnalyzer = new MockAnalyzer(random());
+    SolrSynonymParser parser = new SolrSynonymParser(true, true, synAnalyzer);
     parser.parse(new StringReader(testFile));
     final SynonymMap map = parser.build();
+    synAnalyzer.close();
       
     Analyzer analyzer = new Analyzer() {
       @Override
@@ -642,6 +648,7 @@
     // xyzzy aaa pot of gold -> xyzzy aaaa1 aaaa2 aaaa3 gold
     assertAnalyzesTo(analyzer, "xyzzy aaa pot of gold",
                      new String[] { "xyzzy", "aaaa1", "pot", "aaaa2", "of", "aaaa3", "gold" });
+    analyzer.close();
   }
 
   public void testBasic2() throws Exception {
@@ -716,6 +723,7 @@
     assertAnalyzesTo(a, "z x c $",
         new String[] { "z", "xc", "$" },
         new int[] { 1, 1, 1 });
+    a.close();
   }
   
   public void testRepeatsOff() throws Exception {
@@ -736,6 +744,7 @@
     assertAnalyzesTo(a, "a b",
         new String[] { "ab" },
         new int[] { 1 });
+    a.close();
   }
   
   public void testRepeatsOn() throws Exception {
@@ -756,6 +765,7 @@
     assertAnalyzesTo(a, "a b",
         new String[] { "ab", "ab", "ab" },
         new int[] { 1, 0, 0 });
+    a.close();
   }
   
   public void testRecursion() throws Exception {
@@ -774,6 +784,7 @@
     assertAnalyzesTo(a, "zoo zoo $ zoo",
         new String[] { "zoo", "zoo", "$", "zoo" },
         new int[] { 1, 1, 1, 1 });
+    a.close();
   }
  
   public void testRecursion2() throws Exception {
@@ -794,6 +805,7 @@
     assertAnalyzesTo(a, "zoo zoo $ zoo",
         new String[] { "zoo", "zoo", "zoo", "zoo", "zoo", "$", "zoo", "zoo", "zoo", "zoo" },
         new int[] { 1, 0, 1, 0, 0, 1, 0, 1, 0, 1 });
+    a.close();
   }
 
   public void testOutputHangsOffEnd() throws Exception {
@@ -869,6 +881,7 @@
     assertAnalyzesTo(a, "z x c $",
         new String[] { "z", "x", "xc", "c", "$" },
         new int[] { 1, 1, 0, 1, 1 });
+    a.close();
   }
   
   public void testRecursion3() throws Exception {
@@ -887,6 +900,7 @@
     assertAnalyzesTo(a, "zoo zoo $ zoo",
         new String[] { "zoo", "zoo", "zoo", "$", "zoo" },
         new int[] { 1, 0, 1, 1, 1 });
+    a.close();
   }
   
   public void testRecursion4() throws Exception {
@@ -906,6 +920,7 @@
     assertAnalyzesTo(a, "zoo zoo $ zoo",
         new String[] { "zoo", "zoo", "zoo", "$", "zoo", "zoo", "zoo" },
         new int[] { 1, 0, 1, 1, 1, 0, 1 });
+    a.close();
   }
   
   public void testMultiwordOffsets() throws Exception {
@@ -926,6 +941,7 @@
         new int[] { 0, 0, 9, 16 },
         new int[] { 8, 22, 15, 22 },
         new int[] { 1, 0, 1, 1 });
+    a.close();
   }
 
   public void testEmpty() throws Exception {
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestWordnetSynonymParser.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestWordnetSynonymParser.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestWordnetSynonymParser.java	(working copy)
@@ -17,7 +17,6 @@
 
 package org.apache.lucene.analysis.synonym;
 
-import java.io.Reader;
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.Analyzer;
@@ -40,11 +39,13 @@
     "s(100000004,2,'king''s meany',n,1,1).\n";
   
   public void testSynonyms() throws Exception {
-    WordnetSynonymParser parser = new WordnetSynonymParser(true, true, new MockAnalyzer(random()));
+    Analyzer analyzer = new MockAnalyzer(random());
+    WordnetSynonymParser parser = new WordnetSynonymParser(true, true, analyzer);
     parser.parse(new StringReader(synonymsFile));
     final SynonymMap map = parser.build();
+    analyzer.close();
     
-    Analyzer analyzer = new Analyzer() {
+    analyzer = new Analyzer() {
       @Override
       protected TokenStreamComponents createComponents(String fieldName) {
         Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
@@ -66,5 +67,6 @@
     /* multi words */
     assertAnalyzesTo(analyzer, "king's evil",
         new String[] { "king's", "king's", "evil", "meany" });
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java	(working copy)
@@ -17,15 +17,9 @@
  * limitations under the License.
  */
 
-import java.io.IOException;
-import java.io.Reader;
-import java.util.Random;
-
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.core.StopAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
 import org.apache.lucene.analysis.util.CharArraySet;
@@ -46,18 +40,22 @@
    * testcase for offsets
    */
   public void testOffsets() throws Exception {
-    assertAnalyzesTo(new ThaiAnalyzer(CharArraySet.EMPTY_SET), "การที่ได้ต้องแสดงว่างานดี",
+    Analyzer analyzer = new ThaiAnalyzer(CharArraySet.EMPTY_SET);
+    assertAnalyzesTo(analyzer, "การที่ได้ต้องแสดงว่างานดี",
         new String[] { "การ", "ที่", "ได้", "ต้อง", "แสดง", "ว่า", "งาน", "ดี" },
         new int[] { 0, 3, 6, 9, 13, 17, 20, 23 },
         new int[] { 3, 6, 9, 13, 17, 20, 23, 25 });
+    analyzer.close();
   }
   
   public void testStopWords() throws Exception {
-    assertAnalyzesTo(new ThaiAnalyzer(), "การที่ได้ต้องแสดงว่างานดี",
+    Analyzer analyzer = new ThaiAnalyzer();
+    assertAnalyzesTo(analyzer, "การที่ได้ต้องแสดงว่างานดี",
         new String[] { "แสดง", "งาน", "ดี" },
         new int[] { 13, 20, 23 },
         new int[] { 17, 23, 25 },
         new int[] { 5, 2, 1 });
+    analyzer.close();
   }
   
   /*
@@ -78,32 +76,37 @@
         new int[] { 0, 3, 6, 9, 17, 21, 24, 27 },
         new int[] { 3, 6, 9, 13, 21, 24, 27, 29 },
         new int[] { 1, 1, 1, 1, 2, 1, 1, 1 });
+    analyzer.close();
   }
   
   public void testReusableTokenStream() throws Exception {
     ThaiAnalyzer analyzer = new ThaiAnalyzer(CharArraySet.EMPTY_SET);
     assertAnalyzesTo(analyzer, "", new String[] {});
-
-      assertAnalyzesTo(
-          analyzer,
-          "การที่ได้ต้องแสดงว่างานดี",
-          new String[] { "การ", "ที่", "ได้", "ต้อง", "แสดง", "ว่า", "งาน", "ดี"});
-
-      assertAnalyzesTo(
-          analyzer,
-          "บริษัทชื่อ XY&Z - คุยกับ xyz@demo.com",
-          new String[] { "บริษัท", "ชื่อ", "xy", "z", "คุย", "กับ", "xyz", "demo.com" });
+    
+    assertAnalyzesTo(
+        analyzer,
+        "การที่ได้ต้องแสดงว่างานดี",
+        new String[] { "การ", "ที่", "ได้", "ต้อง", "แสดง", "ว่า", "งาน", "ดี"});
+    
+    assertAnalyzesTo(
+        analyzer,
+        "บริษัทชื่อ XY&Z - คุยกับ xyz@demo.com",
+        new String[] { "บริษัท", "ชื่อ", "xy", "z", "คุย", "กับ", "xyz", "demo.com" });
+    analyzer.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new ThaiAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new ThaiAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
   
   /** blast some random large strings through the analyzer */
   public void testRandomHugeStrings() throws Exception {
-    Random random = random();
-    checkRandomData(random, new ThaiAnalyzer(), 100*RANDOM_MULTIPLIER, 8192);
+    Analyzer analyzer = new ThaiAnalyzer();
+    checkRandomData(random(), analyzer, 100*RANDOM_MULTIPLIER, 8192);
+    analyzer.close();
   }
   
   // LUCENE-3044
@@ -116,12 +119,15 @@
     ts = analyzer.tokenStream("dummy", "ภาษาไทย");
     ts.addAttribute(FlagsAttribute.class);
     assertTokenStreamContents(ts, new String[] { "ภาษา", "ไทย" });
+    analyzer.close();
   }
   
   public void testTwoSentences() throws Exception {
-    assertAnalyzesTo(new ThaiAnalyzer(CharArraySet.EMPTY_SET), "This is a test. การที่ได้ต้องแสดงว่างานดี",
+    Analyzer analyzer = new ThaiAnalyzer(CharArraySet.EMPTY_SET);
+    assertAnalyzesTo(analyzer, "This is a test. การที่ได้ต้องแสดงว่างานดี",
           new String[] { "this", "is", "a", "test", "การ", "ที่", "ได้", "ต้อง", "แสดง", "ว่า", "งาน", "ดี" },
           new int[] { 0, 5, 8, 10, 16, 19, 22, 25, 29, 33, 36, 39 },
           new int[] { 4, 7, 9, 14, 19, 22, 25, 29, 33, 36, 39, 41 });
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new TurkishAnalyzer();
+    new TurkishAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -41,6 +41,7 @@
     // apostrophes
     checkOneTerm(a, "Kıbrıs'ta", "kıbrıs");
     assertAnalyzesTo(a, "Van Gölü'ne", new String[]{"van", "göl"});
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -49,10 +50,13 @@
     Analyzer a = new TurkishAnalyzer(TurkishAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "ağacı", "ağacı");
     checkOneTerm(a, "ağaç", "ağaç");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new TurkishAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new TurkishAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishLowerCaseFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishLowerCaseFilter.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishLowerCaseFilter.java	(working copy)
@@ -18,12 +18,9 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
-import java.io.StringReader;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
@@ -80,5 +77,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers.java	(working copy)
@@ -141,6 +141,7 @@
     }
     // just for fun
     checkRandomData(random(), analyzer, num);
+    analyzer.close();
   }
   
   // LUCENE-3642: normalize BMP->SMP and check that offsets are correct
@@ -179,5 +180,6 @@
     }
     // just for fun
     checkRandomData(random(), analyzer, num);
+    analyzer.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestElision.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestElision.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestElision.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 import java.io.StringReader;
 import java.util.ArrayList;
 import java.util.List;
@@ -71,6 +70,7 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestSegmentingTokenizerBase.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestSegmentingTokenizerBase.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestSegmentingTokenizerBase.java	(working copy)
@@ -27,23 +27,36 @@
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.util.IOUtils;
 
 /** Basic tests for {@link SegmentingTokenizerBase} */
 public class TestSegmentingTokenizerBase extends BaseTokenStreamTestCase {
-  private Analyzer sentence = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      return new TokenStreamComponents(new WholeSentenceTokenizer());
-    }
-  };
+  private Analyzer sentence, sentenceAndWord;
   
-  private Analyzer sentenceAndWord = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      return new TokenStreamComponents(new SentenceAndWordTokenizer());
-    }
-  };
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    sentence = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        return new TokenStreamComponents(new WholeSentenceTokenizer());
+      }
+    };
+    sentenceAndWord = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        return new TokenStreamComponents(new SentenceAndWordTokenizer());
+      }
+    };
+  }
   
+  @Override
+  public void tearDown() throws Exception {
+    IOUtils.close(sentence, sentenceAndWord);
+    super.tearDown();
+  }
+
+  
   /** Some simple examples, just outputting the whole sentence boundaries as "terms" */
   public void testBasics() throws IOException {
     assertAnalyzesTo(sentence, "The acronym for United States is U.S. but this doesn't end a sentence",
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest.java	(working copy)
@@ -194,6 +194,7 @@
     };
     // TODO: properly support positionLengthAttribute
     checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER, 20, false, false);
+    a.close();
   }
   
   /** blast some random large strings through the analyzer */
@@ -209,5 +210,6 @@
     };
     // TODO: properly support positionLengthAttribute
     checkRandomData(random, a, 100*RANDOM_MULTIPLIER, 8192, false, false);
+    a.close();
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationDocValuesField.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationDocValuesField.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationDocValuesField.java	(working copy)
@@ -27,7 +27,6 @@
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.ConstantScoreQuery;
 import org.apache.lucene.search.DocValuesRangeQuery;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MatchAllDocsQuery;
Index: lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java	(revision 1663853)
+++ lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java	(working copy)
@@ -17,7 +17,6 @@
  * limitations under the License.
  */
 
-
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CollationTestBase;
 import org.apache.lucene.util.BytesRef;
@@ -25,17 +24,24 @@
 import java.text.Collator;
 import java.util.Locale;
 
-public class TestCollationKeyAnalyzer extends CollationTestBase {
-  // the sort order of Ø versus U depends on the version of the rules being used
-  // for the inherited root locale: Ø's order isnt specified in Locale.US since 
-  // it's not used in english.
-  private boolean oStrokeFirst = Collator.getInstance(new Locale("")).compare("Ø", "U") < 0;
-  
+public class TestCollationKeyAnalyzer extends CollationTestBase { 
   // Neither Java 1.4.2 nor 1.5.0 has Farsi Locale collation available in
   // RuleBasedCollator.  However, the Arabic Locale seems to order the Farsi
   // characters properly.
   private Collator collator = Collator.getInstance(new Locale("ar"));
-  private Analyzer analyzer = new CollationKeyAnalyzer(collator);
+  private Analyzer analyzer;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new CollationKeyAnalyzer(collator);
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
 
   private BytesRef firstRangeBeginning = new BytesRef(collator.getCollationKey(firstRangeBeginningOriginal).toByteArray());
   private BytesRef firstRangeEnd = new BytesRef(collator.getCollationKey(firstRangeEndOriginal).toByteArray());
@@ -65,7 +71,9 @@
     for (int i = 0; i < iters; i++) {
       Collator collator = Collator.getInstance(Locale.GERMAN);
       collator.setStrength(Collator.PRIMARY);
-      assertThreadSafe(new CollationKeyAnalyzer(collator));
+      Analyzer analyzer = new CollationKeyAnalyzer(collator);
+      assertThreadSafe(analyzer);
+      analyzer.close();
     }
   }
 }
Index: lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUFoldingFilter.java
===================================================================
--- lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUFoldingFilter.java	(revision 1663853)
+++ lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUFoldingFilter.java	(working copy)
@@ -18,9 +18,11 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 
-import org.apache.lucene.analysis.*;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
 
 /**
@@ -27,13 +29,26 @@
  * Tests ICUFoldingFilter
  */
 public class TestICUFoldingFilter extends BaseTokenStreamTestCase {
-  Analyzer a = new Analyzer() {
-    @Override
-    public TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(tokenizer, new ICUFoldingFilter(tokenizer));
-    }
-  };
+  Analyzer a;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    a = new Analyzer() {
+      @Override
+      public TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(tokenizer, new ICUFoldingFilter(tokenizer));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    a.close();
+    super.tearDown();
+  }
+  
   public void testDefaults() throws IOException {
     // case folding
     assertAnalyzesTo(a, "This is a test", new String[] { "this", "is", "a", "test" });
@@ -88,5 +103,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2CharFilter.java
===================================================================
--- lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2CharFilter.java	(revision 1663853)
+++ lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2CharFilter.java	(working copy)
@@ -129,6 +129,7 @@
       }
       checkOneTerm(a, input, normalized);
     }
+    a.close();
   }
 
   public void testNFC() throws Exception {
@@ -187,6 +188,7 @@
     checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
     // huge strings
     checkRandomData(random(), a, 25*RANDOM_MULTIPLIER, 8192);
+    a.close();
 
     // nfkd
     a = new Analyzer() {
@@ -203,6 +205,7 @@
     checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
     // huge strings
     checkRandomData(random(), a, 25*RANDOM_MULTIPLIER, 8192);
+    a.close();
   }
   
   public void testCuriousString() throws Exception {
@@ -221,6 +224,7 @@
     for (int i = 0; i < 1000; i++) {
       checkAnalysisConsistency(random(), a, false, text);
     }
+    a.close();
   }
   
   public void testCuriousMassiveString() throws Exception {
@@ -411,5 +415,6 @@
     for (int i = 0; i < 25; i++) {
       checkAnalysisConsistency(random(), a, false, text);
     }
+    a.close();
   }
 }
Index: lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2Filter.java
===================================================================
--- lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2Filter.java	(revision 1663853)
+++ lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2Filter.java	(working copy)
@@ -18,9 +18,11 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 
-import org.apache.lucene.analysis.*;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
 
 import com.ibm.icu.text.Normalizer2;
@@ -29,13 +31,25 @@
  * Tests the ICUNormalizer2Filter
  */
 public class TestICUNormalizer2Filter extends BaseTokenStreamTestCase {
-  Analyzer a = new Analyzer() {
-    @Override
-    public TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(tokenizer, new ICUNormalizer2Filter(tokenizer));
-    }
-  };
+  Analyzer a;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    a = new Analyzer() {
+      @Override
+      public TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(tokenizer, new ICUNormalizer2Filter(tokenizer));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    a.close();
+    super.tearDown();
+  }
 
   public void testDefaults() throws IOException {
     // case folding
@@ -72,6 +86,7 @@
     
     // decompose EAcute into E + combining Acute
     assertAnalyzesTo(a, "\u00E9", new String[] { "\u0065\u0301" });
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
@@ -88,5 +103,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUTransformFilter.java
===================================================================
--- lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUTransformFilter.java	(revision 1663853)
+++ lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUTransformFilter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.Analyzer;
@@ -105,6 +104,7 @@
       }
     };
     checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+    a.close();
   }
   
   public void testEmptyTerm() throws IOException {
@@ -116,5 +116,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java
===================================================================
--- lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java	(revision 1663853)
+++ lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java	(working copy)
@@ -67,14 +67,26 @@
     assertTokenStreamContents(tokenizer, expected);
   }
   
-  private Analyzer a = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new ICUTokenizer(newAttributeFactory(), new DefaultICUTokenizerConfig(false));
-      TokenFilter filter = new ICUNormalizer2Filter(tokenizer);
-      return new TokenStreamComponents(tokenizer, filter);
-    }
-  };
+  private Analyzer a; 
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new ICUTokenizer(newAttributeFactory(), new DefaultICUTokenizerConfig(false));
+        TokenFilter filter = new ICUNormalizer2Filter(tokenizer);
+        return new TokenStreamComponents(tokenizer, filter);
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    a.close();
+    super.tearDown();
+  }
 
   public void testArmenian() throws Exception {
     assertAnalyzesTo(a, "Վիքիպեդիայի 13 միլիոն հոդվածները (4,600` հայերեն վիքիպեդիայում) գրվել են կամավորների կողմից ու համարյա բոլոր հոդվածները կարող է խմբագրել ցանկաց մարդ ով կարող է բացել Վիքիպեդիայի կայքը։",
Index: lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizerCJK.java
===================================================================
--- lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizerCJK.java	(revision 1663853)
+++ lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizerCJK.java	(working copy)
@@ -17,7 +17,6 @@
  * limitations under the License.
  */
 
-import java.io.Reader;
 import java.util.Random;
 
 import org.apache.lucene.analysis.Analyzer;
@@ -27,13 +26,25 @@
  * test ICUTokenizer with dictionary-based CJ segmentation
  */
 public class TestICUTokenizerCJK extends BaseTokenStreamTestCase {
-  Analyzer a = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      return new TokenStreamComponents(new ICUTokenizer(newAttributeFactory(), new DefaultICUTokenizerConfig(true)));
-    }
-  };
+  Analyzer a;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        return new TokenStreamComponents(new ICUTokenizer(newAttributeFactory(), new DefaultICUTokenizerConfig(true)));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    a.close();
+    super.tearDown();
+  }
+  
   /**
    * test stolen from smartcn
    */
Index: lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestWithCJKBigramFilter.java
===================================================================
--- lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestWithCJKBigramFilter.java	(revision 1663853)
+++ lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestWithCJKBigramFilter.java	(working copy)
@@ -27,6 +27,7 @@
 import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.icu.ICUNormalizer2Filter;
 import org.apache.lucene.analysis.util.CharArraySet;
+import org.apache.lucene.util.IOUtils;
 
 /**
  * Tests ICUTokenizer's ability to work with CJKBigramFilter.
@@ -34,36 +35,47 @@
  */
 public class TestWithCJKBigramFilter extends BaseTokenStreamTestCase {
   
-  /**
-   * ICUTokenizer+CJKBigramFilter
-   */
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new ICUTokenizer(newAttributeFactory(), new DefaultICUTokenizerConfig(false));
-      TokenStream result = new CJKBigramFilter(source);
-      return new TokenStreamComponents(source, new StopFilter(result, CharArraySet.EMPTY_SET));
-    }
-  };
+  Analyzer analyzer, analyzer2;
   
-  /**
-   * ICUTokenizer+ICUNormalizer2Filter+CJKBigramFilter.
-   * 
-   * ICUNormalizer2Filter uses nfkc_casefold by default, so this is a language-independent
-   * superset of CJKWidthFilter's foldings.
-   */
-  private Analyzer analyzer2 = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer source = new ICUTokenizer(newAttributeFactory(), new DefaultICUTokenizerConfig(false));
-      // we put this before the CJKBigramFilter, because the normalization might combine
-      // some halfwidth katakana forms, which will affect the bigramming.
-      TokenStream result = new ICUNormalizer2Filter(source);
-      result = new CJKBigramFilter(source);
-      return new TokenStreamComponents(source, new StopFilter(result, CharArraySet.EMPTY_SET));
-    }
-  };
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    /*
+     * ICUTokenizer+CJKBigramFilter
+     */
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new ICUTokenizer(newAttributeFactory(), new DefaultICUTokenizerConfig(false));
+        TokenStream result = new CJKBigramFilter(source);
+        return new TokenStreamComponents(source, new StopFilter(result, CharArraySet.EMPTY_SET));
+      }
+    };
+    /*
+     * ICUTokenizer+ICUNormalizer2Filter+CJKBigramFilter.
+     * 
+     * ICUNormalizer2Filter uses nfkc_casefold by default, so this is a language-independent
+     * superset of CJKWidthFilter's foldings.
+     */
+    analyzer2 = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer source = new ICUTokenizer(newAttributeFactory(), new DefaultICUTokenizerConfig(false));
+        // we put this before the CJKBigramFilter, because the normalization might combine
+        // some halfwidth katakana forms, which will affect the bigramming.
+        TokenStream result = new ICUNormalizer2Filter(source);
+        result = new CJKBigramFilter(result);
+        return new TokenStreamComponents(source, new StopFilter(result, CharArraySet.EMPTY_SET));
+      }
+    };
+  }
   
+  @Override
+  public void tearDown() throws Exception {
+    IOUtils.close(analyzer, analyzer2);
+    super.tearDown();
+  }
+  
   public void testJa1() throws IOException {
     assertAnalyzesTo(analyzer, "一二三四五六七八九十",
       new String[] { "一二", "二三", "三四", "四五", "五六", "六七", "七八", "八九", "九十" },
Index: lucene/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyAnalyzer.java
===================================================================
--- lucene/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyAnalyzer.java	(revision 1663853)
+++ lucene/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyAnalyzer.java	(working copy)
@@ -29,7 +29,19 @@
 public class TestICUCollationKeyAnalyzer extends CollationTestBase {
 
   private Collator collator = Collator.getInstance(new Locale("fa"));
-  private Analyzer analyzer = new ICUCollationKeyAnalyzer(collator);
+  private Analyzer analyzer;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new ICUCollationKeyAnalyzer(collator);
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
 
   private BytesRef firstRangeBeginning = new BytesRef
     (collator.getCollationKey(firstRangeBeginningOriginal).toByteArray());
@@ -62,7 +74,9 @@
       Locale locale = Locale.GERMAN;
       Collator collator = Collator.getInstance(locale);
       collator.setStrength(Collator.IDENTICAL);
-      assertThreadSafe(new ICUCollationKeyAnalyzer(collator));
+      Analyzer a = new ICUCollationKeyAnalyzer(collator);
+      assertThreadSafe(a);
+      a.close();
     }
   }
 }
Index: lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestExtendedMode.java
===================================================================
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestExtendedMode.java	(revision 1663853)
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestExtendedMode.java	(working copy)
@@ -30,15 +30,26 @@
 import org.apache.lucene.util.UnicodeUtil;
 
 public class TestExtendedMode extends BaseTokenStreamTestCase {
-  private final Analyzer analyzer = new Analyzer() {
-    
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, true, Mode.EXTENDED);
-      return new TokenStreamComponents(tokenizer, tokenizer);
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, true, Mode.EXTENDED);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /** simple test for supplementary characters */
   public void testSurrogates() throws IOException {
     assertAnalyzesTo(analyzer, "𩬅艱鍟䇹愯瀛",
Index: lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseAnalyzer.java
===================================================================
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseAnalyzer.java	(revision 1663853)
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseAnalyzer.java	(working copy)
@@ -31,7 +31,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new JapaneseAnalyzer();
+    new JapaneseAnalyzer().close();
   }
   
   /**
@@ -40,12 +40,14 @@
    * and offsets are correct.
    */
   public void testBasics() throws IOException {
-    assertAnalyzesTo(new JapaneseAnalyzer(), "多くの学生が試験に落ちた。",
+    Analyzer a = new JapaneseAnalyzer();
+    assertAnalyzesTo(a, "多くの学生が試験に落ちた。",
         new String[] { "多く", "学生", "試験", "落ちる" },
         new int[] { 0, 3, 6,  9 },
         new int[] { 2, 5, 8, 11 },
         new int[] { 1, 2, 2,  2 }
       );
+    a.close();
   }
 
   /**
@@ -53,7 +55,7 @@
    */
   public void testDecomposition() throws IOException {
 
-    final Analyzer a = new JapaneseAnalyzer(null, Mode.SEARCH,
+    Analyzer a = new JapaneseAnalyzer(null, Mode.SEARCH,
                                             JapaneseAnalyzer.getDefaultStopSet(),
                                             JapaneseAnalyzer.getDefaultStopTags());
 
@@ -108,7 +110,9 @@
                               );
 
     // Kyoto University Baseball Club
-    assertAnalyzesToPositions(new JapaneseAnalyzer(), "京都大学硬式野球部",
+    a.close();
+    a = new JapaneseAnalyzer();
+    assertAnalyzesToPositions(a, "京都大学硬式野球部",
                      new String[] { "京都大",
                                     "学",
                                     "硬式",
@@ -117,6 +121,7 @@
                               new int[] {1, 1, 1, 1, 1},
                               new int[] {1, 1, 1, 1, 1});
     // toDotFile(a, "成田空港", "/mnt/scratch/out.dot");
+    a.close();
   }
 
   
@@ -129,6 +134,7 @@
                                             JapaneseAnalyzer.getDefaultStopSet(),
                                             JapaneseAnalyzer.getDefaultStopTags());
     checkRandomData(random, a, atLeast(1000));
+    a.close();
   }
   
   /** blast some random large strings through the analyzer */
@@ -138,6 +144,7 @@
         JapaneseAnalyzer.getDefaultStopSet(),
         JapaneseAnalyzer.getDefaultStopTags());
     checkRandomData(random, a, 2*RANDOM_MULTIPLIER, 8192);
+    a.close();
   }
 
   // Copied from TestJapaneseTokenizer, to make sure passing
@@ -154,6 +161,7 @@
                               new int[] { 1, 2, 4 },
                               new Integer(4)
     );
+    a.close();
   }
 
   // LUCENE-3897: this string (found by running all jawiki
@@ -165,6 +173,7 @@
                                             JapaneseAnalyzer.getDefaultStopSet(),
                                             JapaneseAnalyzer.getDefaultStopTags());
     checkAnalysisConsistency(random, a, random.nextBoolean(), s);
+    a.close();
   }
 
   // LUCENE-3897: this string (found by
@@ -176,6 +185,7 @@
                                             JapaneseAnalyzer.getDefaultStopSet(),
                                             JapaneseAnalyzer.getDefaultStopTags());
     checkAnalysisConsistency(random, a, random.nextBoolean(), s);
+    a.close();
   }
 
   // LUCENE-3897: this string (found by
@@ -187,6 +197,7 @@
                                             JapaneseAnalyzer.getDefaultStopSet(),
                                             JapaneseAnalyzer.getDefaultStopTags());
     checkAnalysisConsistency(random, a, random.nextBoolean(), s);
+    a.close();
   }
 
   public void test4thCuriousString() throws Exception {
@@ -194,8 +205,8 @@
     final Analyzer a = new JapaneseAnalyzer(null, Mode.SEARCH,
                                             JapaneseAnalyzer.getDefaultStopSet(),
                                             JapaneseAnalyzer.getDefaultStopTags());
-    Random random = random();
-    checkAnalysisConsistency(random, a, true, s);
+    checkAnalysisConsistency(random(), a, true, s);
+    a.close();
   }
 
   public void test5thCuriousString() throws Exception {
@@ -203,7 +214,7 @@
     final Analyzer a = new JapaneseAnalyzer(null, Mode.SEARCH,
                                             JapaneseAnalyzer.getDefaultStopSet(),
                                             JapaneseAnalyzer.getDefaultStopTags());
-    Random random = random();
-    checkAnalysisConsistency(random, a, false, s);
+    checkAnalysisConsistency(random(), a, false, s);
+    a.close();
   }
 }
Index: lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseBaseFormFilter.java
===================================================================
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseBaseFormFilter.java	(revision 1663853)
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseBaseFormFilter.java	(working copy)
@@ -28,14 +28,26 @@
 import org.apache.lucene.analysis.util.CharArraySet;
 
 public class TestJapaneseBaseFormFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, true, JapaneseTokenizer.DEFAULT_MODE);
-      return new TokenStreamComponents(tokenizer, new JapaneseBaseFormFilter(tokenizer));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, true, JapaneseTokenizer.DEFAULT_MODE);
+        return new TokenStreamComponents(tokenizer, new JapaneseBaseFormFilter(tokenizer));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   public void testBasics() throws IOException {
     assertAnalyzesTo(analyzer, "それはまだ実験段階にあります",
         new String[] { "それ", "は", "まだ", "実験", "段階", "に", "ある", "ます"  }
@@ -55,6 +67,7 @@
     assertAnalyzesTo(a, "それはまだ実験段階にあります",
         new String[] { "それ", "は", "まだ", "実験", "段階", "に", "あり", "ます"  }
     );
+    a.close();
   }
   
   public void testEnglish() throws IOException {
@@ -75,5 +88,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseIterationMarkCharFilter.java
===================================================================
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseIterationMarkCharFilter.java	(revision 1663853)
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseIterationMarkCharFilter.java	(working copy)
@@ -22,6 +22,7 @@
 import org.apache.lucene.analysis.CharFilter;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.util.IOUtils;
 
 import java.io.IOException;
 import java.io.Reader;
@@ -28,33 +29,43 @@
 import java.io.StringReader;
 
 public class TestJapaneseIterationMarkCharFilter extends BaseTokenStreamTestCase {
+  private Analyzer keywordAnalyzer, japaneseAnalyzer;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    keywordAnalyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.KEYWORD, false);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
 
-  private Analyzer keywordAnalyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new MockTokenizer(MockTokenizer.KEYWORD, false);
-      return new TokenStreamComponents(tokenizer, tokenizer);
-    }
+      @Override
+      protected Reader initReader(String fieldName, Reader reader) {
+        return new JapaneseIterationMarkCharFilter(reader);
+      }
+    };
+    japaneseAnalyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, false, JapaneseTokenizer.Mode.SEARCH);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
 
-    @Override
-    protected Reader initReader(String fieldName, Reader reader) {
-      return new JapaneseIterationMarkCharFilter(reader);
-    }
-  };
-
-  private Analyzer japaneseAnalyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, false, JapaneseTokenizer.Mode.SEARCH);
-      return new TokenStreamComponents(tokenizer, tokenizer);
-    }
-
-    @Override
-    protected Reader initReader(String fieldName, Reader reader) {
-      return new JapaneseIterationMarkCharFilter(reader);
-    }
-  };
+      @Override
+      protected Reader initReader(String fieldName, Reader reader) {
+        return new JapaneseIterationMarkCharFilter(reader);
+      }
+    };
+  }
   
+  @Override
+  public void tearDown() throws Exception {
+    IOUtils.close(keywordAnalyzer, japaneseAnalyzer);
+    super.tearDown();
+  }
+  
   public void testKanji() throws IOException {
     // Test single repetition
     assertAnalyzesTo(keywordAnalyzer, "時々", new String[]{"時時"});
Index: lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseKatakanaStemFilter.java
===================================================================
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseKatakanaStemFilter.java	(revision 1663853)
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseKatakanaStemFilter.java	(working copy)
@@ -32,15 +32,27 @@
  * Tests for {@link JapaneseKatakanaStemFilter}
  */
 public class TestJapaneseKatakanaStemFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      // Use a MockTokenizer here since this filter doesn't really depend on Kuromoji
-      Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(source, new JapaneseKatakanaStemFilter(source));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        // Use a MockTokenizer here since this filter doesn't really depend on Kuromoji
+        Tokenizer source = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(source, new JapaneseKatakanaStemFilter(source));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /**
    * Test a few common katakana spelling variations.
    * <p>
@@ -73,6 +85,7 @@
       }
     };
     checkOneTerm(a, "コーヒー", "コーヒー");
+    a.close();
   }
 
   public void testUnsupportedHalfWidthVariants() throws IOException {
@@ -93,5 +106,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseNumberFilter.java
===================================================================
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseNumberFilter.java	(revision 1663853)
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseNumberFilter.java	(working copy)
@@ -36,15 +36,26 @@
 import org.junit.Test;
 
 public class TestJapaneseNumberFilter extends BaseTokenStreamTestCase {
-
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, false, JapaneseTokenizer.Mode.SEARCH);
-      return new TokenStreamComponents(tokenizer, new JapaneseNumberFilter(tokenizer));
-    }
-  };
-
+  private Analyzer analyzer;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, false, JapaneseTokenizer.Mode.SEARCH);
+        return new TokenStreamComponents(tokenizer, new JapaneseNumberFilter(tokenizer));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   @Test
   public void testBasics() throws IOException {
 
@@ -188,6 +199,7 @@
         new int[]{2, 4},
         new int[]{1, 1}
     );
+    keywordMarkingAnalyzer.close();
   }
 
   @Test
@@ -285,6 +297,7 @@
         Files.newBufferedReader(input, StandardCharsets.UTF_8),
         Files.newBufferedWriter(normalizedOutput, StandardCharsets.UTF_8)
     );
+    plainAnalyzer.close();
   }
 
   public void analyze(Analyzer analyzer, Reader reader, Writer writer) throws IOException {
Index: lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseReadingFormFilter.java
===================================================================
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseReadingFormFilter.java	(revision 1663853)
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseReadingFormFilter.java	(working copy)
@@ -23,9 +23,9 @@
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.cjk.CJKWidthFilter;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
+import org.apache.lucene.util.IOUtils;
 
 import java.io.IOException;
-import java.io.Reader;
 import java.util.Random;
 
 /**
@@ -32,23 +32,33 @@
  * Tests for {@link TestJapaneseReadingFormFilter}
  */
 public class TestJapaneseReadingFormFilter extends BaseTokenStreamTestCase {
-  private Analyzer katakanaAnalyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, true, JapaneseTokenizer.Mode.SEARCH);
-      return new TokenStreamComponents(tokenizer, new JapaneseReadingFormFilter(tokenizer, false));
-    }
-  };
+  private Analyzer katakanaAnalyzer, romajiAnalyzer;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    katakanaAnalyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, true, JapaneseTokenizer.Mode.SEARCH);
+        return new TokenStreamComponents(tokenizer, new JapaneseReadingFormFilter(tokenizer, false));
+      }
+    };
+    romajiAnalyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, true, JapaneseTokenizer.Mode.SEARCH);
+        return new TokenStreamComponents(tokenizer, new JapaneseReadingFormFilter(tokenizer, true));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    IOUtils.close(katakanaAnalyzer, romajiAnalyzer);
+    super.tearDown();
+  }
 
-  private Analyzer romajiAnalyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, true, JapaneseTokenizer.Mode.SEARCH);
-      return new TokenStreamComponents(tokenizer, new JapaneseReadingFormFilter(tokenizer, true));
-    }
-  };
-
-
   public void testKatakanaReadings() throws IOException {
     assertAnalyzesTo(katakanaAnalyzer, "今夜はロバート先生と話した",
         new String[] { "コンヤ", "ハ", "ロバート", "センセイ", "ト", "ハナシ", "タ" }
@@ -67,6 +77,7 @@
     assertAnalyzesTo(a, "今夜はﾛﾊﾞｰﾄ先生と話した",
         new String[] { "コンヤ", "ハ", "ロバート", "センセイ", "ト", "ハナシ", "タ" }
     );
+    a.close();
   }
 
   public void testRomajiReadings() throws IOException {
@@ -87,6 +98,7 @@
     assertAnalyzesTo(a, "今夜はﾛﾊﾞｰﾄ先生と話した",
         new String[] { "kon'ya", "ha", "robato", "sensei", "to", "hanashi", "ta" }
     );
+    a.close();
   }
 
   public void testRandomData() throws IOException {
@@ -104,5 +116,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java
===================================================================
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java	(revision 1663853)
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java	(working copy)
@@ -35,9 +35,9 @@
 import org.apache.lucene.analysis.ja.dict.UserDictionary;
 import org.apache.lucene.analysis.ja.tokenattributes.*;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.TestUtil;
 import org.apache.lucene.util.UnicodeUtil;
-import org.apache.lucene.util.LuceneTestCase.Slow;
 
 public class TestJapaneseTokenizer extends BaseTokenStreamTestCase {
 
@@ -57,39 +57,48 @@
       throw new RuntimeException(ioe);
     }
   }
+  
+  private Analyzer analyzer, analyzerNormal, analyzerNoPunct, extendedModeAnalyzerNoPunct;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), false, Mode.SEARCH);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+    };
+    analyzerNormal = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), false, Mode.NORMAL);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+    };
+    analyzerNoPunct = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), true, Mode.SEARCH);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+    };
+    extendedModeAnalyzerNoPunct = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), true, Mode.EXTENDED);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    IOUtils.close(analyzer, analyzerNormal, analyzerNoPunct, extendedModeAnalyzerNoPunct);
+    super.tearDown();
+  }
 
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), false, Mode.SEARCH);
-      return new TokenStreamComponents(tokenizer, tokenizer);
-    }
-  };
-
-  private Analyzer analyzerNormal = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), false, Mode.NORMAL);
-      return new TokenStreamComponents(tokenizer, tokenizer);
-    }
-  };
-
-  private Analyzer analyzerNoPunct = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), true, Mode.SEARCH);
-      return new TokenStreamComponents(tokenizer, tokenizer);
-    }
-  };
-
-  private Analyzer extendedModeAnalyzerNoPunct = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), true, Mode.EXTENDED);
-      return new TokenStreamComponents(tokenizer, tokenizer);
-    }
-  };
-
   public void testNormalMode() throws Exception {
     assertAnalyzesTo(analyzerNormal,
                      "シニアソフトウェアエンジニア",
@@ -197,16 +206,16 @@
   public void testRandomHugeStringsMockGraphAfter() throws Exception {
     // Randomly inject graph tokens after JapaneseTokenizer:
     Random random = random();
-    checkRandomData(random,
-                    new Analyzer() {
-                      @Override
-                      protected TokenStreamComponents createComponents(String fieldName) {
-                        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), false, Mode.SEARCH);
-                        TokenStream graph = new MockGraphTokenFilter(random(), tokenizer);
-                        return new TokenStreamComponents(tokenizer, graph);
-                      }
-                    },
-                    20*RANDOM_MULTIPLIER, 8192);
+    Analyzer analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), false, Mode.SEARCH);
+        TokenStream graph = new MockGraphTokenFilter(random(), tokenizer);
+        return new TokenStreamComponents(tokenizer, graph);
+      }
+    };
+    checkRandomData(random, analyzer, 20*RANDOM_MULTIPLIER, 8192);
+    analyzer.close();
   }
 
   public void testLargeDocReliability() throws Exception {
@@ -367,6 +376,7 @@
                      surfaceForms);
     
     assertTrue(gv2.finish().indexOf("22.0") != -1);
+    analyzer.close();
   }
 
   private void assertReadings(String input, String... readings) throws IOException {
Index: lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestSearchMode.java
===================================================================
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestSearchMode.java	(revision 1663853)
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestSearchMode.java	(working copy)
@@ -31,13 +31,25 @@
 
 public class TestSearchMode extends BaseTokenStreamTestCase {
   private final static String SEGMENTATION_FILENAME = "search-segmentation-tests.txt";
-  private final Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, true, Mode.SEARCH);
-      return new TokenStreamComponents(tokenizer, tokenizer);
-    }
-  };
+  private Analyzer analyzer;
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, true, Mode.SEARCH);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
 
   /** Test search mode segmentation */
   public void testSearchSegmentation() throws IOException {
Index: lucene/analysis/morfologik/src/test/org/apache/lucene/analysis/morfologik/TestMorfologikAnalyzer.java
===================================================================
--- lucene/analysis/morfologik/src/test/org/apache/lucene/analysis/morfologik/TestMorfologikAnalyzer.java	(revision 1663853)
+++ lucene/analysis/morfologik/src/test/org/apache/lucene/analysis/morfologik/TestMorfologikAnalyzer.java	(working copy)
@@ -46,6 +46,7 @@
     assertAnalyzesTo(a, "liście", new String[] { "liście", "liść", "list", "lista" });
     assertAnalyzesTo(a, "danych", new String[] { "dany", "dana", "dane", "dać" });
     assertAnalyzesTo(a, "ęóąśłżźćń", new String[] { "ęóąśłżźćń" });
+    a.close();
   }
 
   /** Test stemming of multiple tokens and proper term metrics. */
@@ -66,11 +67,13 @@
         new int[] { 0, 0, 3  },
         new int[] { 1, 1, 13 },
         new int[] { 1, 0, 1  });
+    a.close();
   }
 
   @SuppressWarnings("unused")
   private void dumpTokens(String input) throws IOException {
-    try (TokenStream ts = getTestAnalyzer().tokenStream("dummy", input)) {
+    try (Analyzer a = getTestAnalyzer();
+        TokenStream ts = a.tokenStream("dummy", input)) {
       ts.reset();
 
       MorphosyntacticTagsAttribute attribute = ts.getAttribute(MorphosyntacticTagsAttribute.class);
@@ -100,6 +103,7 @@
       assertEquals("second stream", "dany", termAtt_2.toString());
       ts_2.end();
     }
+    a.close();
   }
 
   /** Test stemming of mixed-case tokens. */
@@ -116,6 +120,7 @@
     assertAnalyzesTo(a, "aarona",   new String[] { "aarona" });
 
     assertAnalyzesTo(a, "Liście",   new String[] { "liście", "liść", "list", "lista" });
+    a.close();
   }
 
   private void assertPOSToken(TokenStream ts, String term, String... tags) throws IOException {
@@ -140,7 +145,8 @@
 
   /** Test morphosyntactic annotations. */
   public final void testPOSAttribute() throws IOException {
-    try (TokenStream ts = getTestAnalyzer().tokenStream("dummy", "liście")) {
+    try (Analyzer a = getTestAnalyzer();
+         TokenStream ts = a.tokenStream("dummy", "liście")) {
       ts.reset();
       assertPOSToken(ts, "liście",  
         "subst:sg:acc:n2",
@@ -187,10 +193,13 @@
       new int[] { 0, 7, 7, 7, 7 },
       new int[] { 6, 13, 13, 13, 13 },
       new int[] { 1, 1, 0, 0, 0 });
+    a.close();
   }
 
   /** blast some random strings through the analyzer */
   public void testRandom() throws Exception {
-    checkRandomData(random(), getTestAnalyzer(), 1000 * RANDOM_MULTIPLIER); 
+    Analyzer a = getTestAnalyzer();
+    checkRandomData(random(), a, 1000 * RANDOM_MULTIPLIER);
+    a.close();
   }
 }
Index: lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/DoubleMetaphoneFilterTest.java
===================================================================
--- lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/DoubleMetaphoneFilterTest.java	(revision 1663853)
+++ lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/DoubleMetaphoneFilterTest.java	(working copy)
@@ -81,6 +81,7 @@
       
     };
     checkRandomData(random(), a, 1000 * RANDOM_MULTIPLIER);
+    a.close();
     
     Analyzer b = new Analyzer() {
 
@@ -92,6 +93,7 @@
       
     };
     checkRandomData(random(), b, 1000 * RANDOM_MULTIPLIER); 
+    b.close();
   }
   
   public void testEmptyTerm() throws IOException {
@@ -103,5 +105,6 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 }
Index: lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestBeiderMorseFilter.java
===================================================================
--- lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestBeiderMorseFilter.java	(revision 1663853)
+++ lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestBeiderMorseFilter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 import java.io.StringReader;
 import java.util.HashSet;
 import java.util.regex.Pattern;
@@ -35,19 +34,30 @@
 import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.miscellaneous.PatternKeywordMarkerFilter;
 import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
-import org.junit.Ignore;
 
 /** Tests {@link BeiderMorseFilter} */
 public class TestBeiderMorseFilter extends BaseTokenStreamTestCase {
-  private Analyzer analyzer = new Analyzer() {
-    @Override
-    protected TokenStreamComponents createComponents(String fieldName) {
-      Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-      return new TokenStreamComponents(tokenizer, 
-          new BeiderMorseFilter(tokenizer, new PhoneticEngine(NameType.GENERIC, RuleType.EXACT, true)));
-    }
-  };
+  private Analyzer analyzer;
   
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(tokenizer, 
+            new BeiderMorseFilter(tokenizer, new PhoneticEngine(NameType.GENERIC, RuleType.EXACT, true)));
+      }
+    };
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    analyzer.close();
+    super.tearDown();
+  }
+  
   /** generic, "exact" configuration */
   public void testBasicUsage() throws Exception {    
     assertAnalyzesTo(analyzer, "Angelo",
@@ -83,6 +93,7 @@
         new int[] { 0, 0, 0, },
         new int[] { 6, 6, 6, },
         new int[] { 1, 0, 0, });
+    analyzer.close();
   }
   
   /** for convenience, if the input yields no output, we pass it thru as-is */
@@ -107,6 +118,7 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
   
   public void testCustomAttribute() throws IOException {
Index: lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestDaitchMokotoffSoundexFilter.java
===================================================================
--- lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestDaitchMokotoffSoundexFilter.java	(revision 1663853)
+++ lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestDaitchMokotoffSoundexFilter.java	(working copy)
@@ -58,6 +58,7 @@
     };
 
     checkRandomData(random(), a, 1000 * RANDOM_MULTIPLIER);
+    a.close();
 
     Analyzer b = new Analyzer() {
       @Override
@@ -68,6 +69,7 @@
     };
 
     checkRandomData(random(), b, 1000 * RANDOM_MULTIPLIER);
+    b.close();
   }
 
   public void testEmptyTerm() throws IOException {
@@ -79,6 +81,7 @@
       }
     };
     checkOneTerm(a, "", "");
+    a.close();
   }
 
 }
Index: lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestPhoneticFilter.java
===================================================================
--- lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestPhoneticFilter.java	(revision 1663853)
+++ lucene/analysis/phonetic/src/test/org/apache/lucene/analysis/phonetic/TestPhoneticFilter.java	(working copy)
@@ -96,6 +96,7 @@
       };
       
       checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
+      a.close();
       
       Analyzer b = new Analyzer() {
         @Override
@@ -106,6 +107,7 @@
       };
       
       checkRandomData(random(), b, 1000*RANDOM_MULTIPLIER);
+      b.close();
     }
   }
   
@@ -122,6 +124,7 @@
         }
       };
       checkOneTerm(a, "", "");
+      a.close();
     }
   }
 }
Index: lucene/analysis/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseAnalyzer.java
===================================================================
--- lucene/analysis/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseAnalyzer.java	(revision 1663853)
+++ lucene/analysis/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseAnalyzer.java	(working copy)
@@ -17,19 +17,10 @@
 
 package org.apache.lucene.analysis.cn.smart;
 
-import java.io.IOException;
-import java.io.Reader;
-import java.util.Random;
-
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.core.KeywordTokenizer;
-import org.apache.lucene.analysis.miscellaneous.ASCIIFoldingFilter;
-import org.apache.lucene.util.Version;
+import org.apache.lucene.util.IOUtils;
 
 public class TestSmartChineseAnalyzer extends BaseTokenStreamTestCase {
   
@@ -38,9 +29,11 @@
     String sentence = "我购买了道具和服装。";
     String result[] = { "我", "购买", "了", "道具", "和", "服装" };
     assertAnalyzesTo(ca, sentence, result);
+    ca.close();
     // set stop-words from the outer world - must yield same behavior
     ca = new SmartChineseAnalyzer(SmartChineseAnalyzer.getDefaultStopSet());
     assertAnalyzesTo(ca, sentence, result);
+    ca.close();
   }
   
   /*
@@ -52,6 +45,7 @@
     String sentence = "我购买了道具和服装。 我购买了道具和服装。";
     String result[] = { "我", "购买", "了", "道具", "和", "服装", "我", "购买", "了", "道具", "和", "服装" };
     assertAnalyzesTo(ca, sentence, result);
+    ca.close();
   }
   
   /*
@@ -63,6 +57,7 @@
     String sentence = "我购买了道具和服装　我购买了道具和服装。";
     String result[] = { "我", "购买", "了", "道具", "和", "服装", "我", "购买", "了", "道具", "和", "服装" };
     assertAnalyzesTo(ca, sentence, result);
+    ca.close();
   }
   
   /*
@@ -81,6 +76,7 @@
       assertAnalyzesTo(analyzer, sentence, result);
       assertAnalyzesTo(analyzer, sentence, result);
     }
+    IOUtils.close(analyzers);
   }
   
   /*
@@ -95,6 +91,7 @@
     int endOffsets[] = { 5, 9 };
     int posIncr[] = { 1, 2 };
     assertAnalyzesTo(ca, sentence, result, startOffsets, endOffsets, posIncr);
+    ca.close();
   }
   
   public void testChineseAnalyzer() throws Exception {
@@ -102,6 +99,7 @@
     String sentence = "我购买了道具和服装。";
     String[] result = { "我", "购买", "了", "道具", "和", "服装" };
     assertAnalyzesTo(ca, sentence, result);
+    ca.close();
   }
   
   /*
@@ -108,8 +106,10 @@
    * English words are lowercased and porter-stemmed.
    */
   public void testMixedLatinChinese() throws Exception {
-    assertAnalyzesTo(new SmartChineseAnalyzer(true), "我购买 Tests 了道具和服装",
+    Analyzer analyzer = new SmartChineseAnalyzer(true);
+    assertAnalyzesTo(analyzer, "我购买 Tests 了道具和服装",
         new String[] { "我", "购买", "test", "了", "道具", "和", "服装"});
+    analyzer.close();
   }
   
   /*
@@ -116,8 +116,10 @@
    * Numerics are parsed as their own tokens
    */
   public void testNumerics() throws Exception {
-    assertAnalyzesTo(new SmartChineseAnalyzer(true), "我购买 Tests 了道具和服装1234",
+    Analyzer analyzer = new SmartChineseAnalyzer(true);
+    assertAnalyzesTo(analyzer, "我购买 Tests 了道具和服装1234",
       new String[] { "我", "购买", "test", "了", "道具", "和", "服装", "1234"});
+    analyzer.close();
   }
   
   /*
@@ -124,8 +126,10 @@
    * Full width alphas and numerics are folded to half-width
    */
   public void testFullWidth() throws Exception {
-    assertAnalyzesTo(new SmartChineseAnalyzer(true), "我购买 Ｔｅｓｔｓ 了道具和服装１２３４",
+    Analyzer analyzer = new SmartChineseAnalyzer(true);
+    assertAnalyzesTo(analyzer, "我购买 Ｔｅｓｔｓ 了道具和服装１２３４",
         new String[] { "我", "购买", "test", "了", "道具", "和", "服装", "1234"});
+    analyzer.close();
   }
   
   /*
@@ -132,8 +136,10 @@
    * Presentation form delimiters are removed
    */
   public void testDelimiters() throws Exception {
-    assertAnalyzesTo(new SmartChineseAnalyzer(true), "我购买︱ Tests 了道具和服装",
+    Analyzer analyzer = new SmartChineseAnalyzer(true);
+    assertAnalyzesTo(analyzer, "我购买︱ Tests 了道具和服装",
         new String[] { "我", "购买", "test", "了", "道具", "和", "服装"});
+    analyzer.close();
   }
   
   /*
@@ -141,8 +147,10 @@
    * (regardless of Unicode category)
    */
   public void testNonChinese() throws Exception {
-    assertAnalyzesTo(new SmartChineseAnalyzer(true), "我购买 روبرتTests 了道具和服装",
+    Analyzer analyzer = new SmartChineseAnalyzer(true);
+    assertAnalyzesTo(analyzer, "我购买 روبرتTests 了道具和服装",
         new String[] { "我", "购买", "ر", "و", "ب", "ر", "ت", "test", "了", "道具", "和", "服装"});
+    analyzer.close();
   }
   
   /*
@@ -151,18 +159,22 @@
    * Currently it is being analyzed into single characters...
    */
   public void testOOV() throws Exception {
-    assertAnalyzesTo(new SmartChineseAnalyzer(true), "优素福·拉扎·吉拉尼",
+    Analyzer analyzer = new SmartChineseAnalyzer(true);
+    assertAnalyzesTo(analyzer, "优素福·拉扎·吉拉尼",
       new String[] { "优", "素", "福", "拉", "扎", "吉", "拉", "尼" });
     
-    assertAnalyzesTo(new SmartChineseAnalyzer(true), "优素福拉扎吉拉尼",
+    assertAnalyzesTo(analyzer, "优素福拉扎吉拉尼",
       new String[] { "优", "素", "福", "拉", "扎", "吉", "拉", "尼" });
+    analyzer.close();
   }
   
   public void testOffsets() throws Exception {
-    assertAnalyzesTo(new SmartChineseAnalyzer(true), "我购买了道具和服装",
+    Analyzer analyzer = new SmartChineseAnalyzer(true);
+    assertAnalyzesTo(analyzer, "我购买了道具和服装",
         new String[] { "我", "购买", "了", "道具", "和", "服装" },
         new int[] { 0, 1, 3, 4, 6, 7 },
         new int[] { 1, 3, 4, 6, 7, 9 });
+    analyzer.close();
   }
   
   public void testReusableTokenStream() throws Exception {
@@ -175,6 +187,7 @@
         new String[] { "我", "购买", "了", "道具", "和", "服装" },
         new int[] { 0, 1, 3, 4, 6, 7 },
         new int[] { 1, 3, 4, 6, 7, 9 });
+    a.close();
   }
   
   // LUCENE-3026
@@ -183,8 +196,8 @@
     for (int i = 0; i < 5000; i++) {
       sb.append("我购买了道具和服装。");
     }
-    Analyzer analyzer = new SmartChineseAnalyzer();
-    try (TokenStream stream = analyzer.tokenStream("", sb.toString())) {
+    try (Analyzer analyzer = new SmartChineseAnalyzer();
+         TokenStream stream = analyzer.tokenStream("", sb.toString())) {
       stream.reset();
       while (stream.incrementToken()) {
       }
@@ -198,8 +211,8 @@
     for (int i = 0; i < 5000; i++) {
       sb.append("我购买了道具和服装");
     }
-    Analyzer analyzer = new SmartChineseAnalyzer();
-    try (TokenStream stream = analyzer.tokenStream("", sb.toString())) {
+    try (Analyzer analyzer = new SmartChineseAnalyzer();
+         TokenStream stream = analyzer.tokenStream("", sb.toString())) {
       stream.reset();
       while (stream.incrementToken()) {
       }
@@ -209,12 +222,15 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new SmartChineseAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new SmartChineseAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
   
   /** blast some random large strings through the analyzer */
   public void testRandomHugeStrings() throws Exception {
-    Random random = random();
-    checkRandomData(random, new SmartChineseAnalyzer(), 100*RANDOM_MULTIPLIER, 8192);
+    Analyzer analyzer = new SmartChineseAnalyzer();
+    checkRandomData(random(), analyzer, 100*RANDOM_MULTIPLIER, 8192);
+    analyzer.close();
   }
 }
Index: lucene/analysis/stempel/src/test/org/apache/lucene/analysis/pl/TestPolishAnalyzer.java
===================================================================
--- lucene/analysis/stempel/src/test/org/apache/lucene/analysis/pl/TestPolishAnalyzer.java	(revision 1663853)
+++ lucene/analysis/stempel/src/test/org/apache/lucene/analysis/pl/TestPolishAnalyzer.java	(working copy)
@@ -27,7 +27,7 @@
   /** This test fails with NPE when the 
    * stopwords file is missing in classpath */
   public void testResourcesAvailable() {
-    new PolishAnalyzer();
+    new PolishAnalyzer().close();
   }
   
   /** test stopwords and stemming */
@@ -38,6 +38,7 @@
     checkOneTerm(a, "studenci", "student");
     // stopword
     assertAnalyzesTo(a, "był", new String[] {});
+    a.close();
   }
   
   /** test use of exclusion set */
@@ -46,10 +47,13 @@
     Analyzer a = new PolishAnalyzer(PolishAnalyzer.getDefaultStopSet(), exclusionSet);
     checkOneTerm(a, "studenta", "studenta");
     checkOneTerm(a, "studenci", "student");
+    a.close();
   }
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new PolishAnalyzer(), 1000*RANDOM_MULTIPLIER);
+    Analyzer analyzer = new PolishAnalyzer();
+    checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
+    analyzer.close();
   }
 }
Index: lucene/analysis/uima/src/test/org/apache/lucene/analysis/uima/UIMABaseAnalyzerTest.java
===================================================================
--- lucene/analysis/uima/src/test/org/apache/lucene/analysis/uima/UIMABaseAnalyzerTest.java	(revision 1663853)
+++ lucene/analysis/uima/src/test/org/apache/lucene/analysis/uima/UIMABaseAnalyzerTest.java	(working copy)
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.document.Document;
@@ -120,8 +121,9 @@
 
   @Test
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new UIMABaseAnalyzer("/uima/TestAggregateSentenceAE.xml", "org.apache.lucene.uima.ts.TokenAnnotation", null),
-        100 * RANDOM_MULTIPLIER);
+    Analyzer analyzer = new UIMABaseAnalyzer("/uima/TestAggregateSentenceAE.xml", "org.apache.lucene.uima.ts.TokenAnnotation", null);
+    checkRandomData(random(), analyzer, 100 * RANDOM_MULTIPLIER);
+    analyzer.close();
   }
 
   @Test
@@ -128,8 +130,9 @@
   public void testRandomStringsWithConfigurationParameters() throws Exception {
     Map<String, Object> cp = new HashMap<>();
     cp.put("line-end", "\r");
-    checkRandomData(random(), new UIMABaseAnalyzer("/uima/TestWSTokenizerAE.xml", "org.apache.lucene.uima.ts.TokenAnnotation", cp),
-        100 * RANDOM_MULTIPLIER);
+    Analyzer analyzer = new UIMABaseAnalyzer("/uima/TestWSTokenizerAE.xml", "org.apache.lucene.uima.ts.TokenAnnotation", cp);
+    checkRandomData(random(), analyzer, 100 * RANDOM_MULTIPLIER);
+    analyzer.close();
   }
 
 }
Index: lucene/analysis/uima/src/test/org/apache/lucene/analysis/uima/UIMATypeAwareAnalyzerTest.java
===================================================================
--- lucene/analysis/uima/src/test/org/apache/lucene/analysis/uima/UIMATypeAwareAnalyzerTest.java	(revision 1663853)
+++ lucene/analysis/uima/src/test/org/apache/lucene/analysis/uima/UIMATypeAwareAnalyzerTest.java	(working copy)
@@ -17,9 +17,9 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.util.LuceneTestCase.SuppressSysoutChecks;
 import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
@@ -61,8 +61,10 @@
 
   @Test
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new UIMATypeAwareAnalyzer("/uima/TestAggregateSentenceAE.xml",
-        "org.apache.lucene.uima.ts.TokenAnnotation", "pos", null), 100 * RANDOM_MULTIPLIER);
+    Analyzer analyzer = new UIMATypeAwareAnalyzer("/uima/TestAggregateSentenceAE.xml",
+        "org.apache.lucene.uima.ts.TokenAnnotation", "pos", null);
+    checkRandomData(random(), analyzer, 100 * RANDOM_MULTIPLIER);
+    analyzer.close();
   }
 
 }
Index: lucene/core/src/test/org/apache/lucene/TestDemo.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/TestDemo.java	(revision 1663853)
+++ lucene/core/src/test/org/apache/lucene/TestDemo.java	(working copy)
@@ -77,5 +77,6 @@
 
     ireader.close();
     directory.close();
+    analyzer.close();
   }
 }
Index: lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java
===================================================================
--- lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java	(revision 1663853)
+++ lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java	(working copy)
@@ -49,6 +49,7 @@
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LineFileDocs;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.PriorityQueue;
@@ -126,9 +127,7 @@
       assertEquals("3", r.document(search.scoreDocs[0].doc).get("id"));
       
     }
-    r.close();
-    w.close();
-    dir.close();
+    IOUtils.close(r, w, dir, analyzer);
   }
   
   public void testEqualsHashCode() {
@@ -321,9 +320,7 @@
               r.document(search.scoreDocs[0].doc).get("id"),
               r.document(search.scoreDocs[1].doc).get("id"))));
     }
-    r.close();
-    w.close();
-    dir.close();
+    IOUtils.close(r, w, dir, analyzer);
   }
   
   public void testIllegalOccur() {
@@ -395,9 +392,7 @@
       assertEquals("3", r.document(search.scoreDocs[1].doc).get("id"));
       assertEquals("0", r.document(search.scoreDocs[2].doc).get("id"));
     }
-    r.close();
-    w.close();
-    dir.close();
+    IOUtils.close(r, w, dir, analyzer);
   }
   
   public void testRandomIndex() throws IOException {
@@ -496,10 +491,7 @@
       QueryUtils.check(random(), cq, newSearcher(reader2));
       reader2.close();
     } finally {
-      reader.close();
-      wrapper.close();
-      w.close();
-      dir.close();
+      IOUtils.close(reader, wrapper, w, dir, analyzer);
     }
     
   }
Index: lucene/queries/src/test/org/apache/lucene/queries/function/FunctionTestSetup.java
===================================================================
--- lucene/queries/src/test/org/apache/lucene/queries/function/FunctionTestSetup.java	(revision 1663853)
+++ lucene/queries/src/test/org/apache/lucene/queries/function/FunctionTestSetup.java	(working copy)
@@ -84,6 +84,7 @@
   public static void afterClassFunctionTestSetup() throws Exception {
     dir.close();
     dir = null;
+    anlzr.close();
     anlzr = null;
   }
 
Index: lucene/queries/src/test/org/apache/lucene/queries/function/TestLongNormValueSource.java
===================================================================
--- lucene/queries/src/test/org/apache/lucene/queries/function/TestLongNormValueSource.java	(revision 1663853)
+++ lucene/queries/src/test/org/apache/lucene/queries/function/TestLongNormValueSource.java	(working copy)
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -46,12 +47,15 @@
   static Directory dir;
   static IndexReader reader;
   static IndexSearcher searcher;
+  static Analyzer analyzer;
+  
   private static Similarity sim = new PreciseDefaultSimilarity();
 
   @BeforeClass
   public static void beforeClass() throws Exception {
     dir = newDirectory();
-    IndexWriterConfig iwConfig = newIndexWriterConfig(new MockAnalyzer(random()));
+    analyzer = new MockAnalyzer(random());
+    IndexWriterConfig iwConfig = newIndexWriterConfig(analyzer);
     iwConfig.setMergePolicy(newLogMergePolicy());
     iwConfig.setSimilarity(sim);
     RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConfig);
@@ -76,6 +80,8 @@
     reader = null;
     dir.close();
     dir = null;
+    analyzer.close();
+    analyzer = null;
   }
 
   public void testNorm() throws Exception {
Index: lucene/queries/src/test/org/apache/lucene/queries/function/TestValueSources.java
===================================================================
--- lucene/queries/src/test/org/apache/lucene/queries/function/TestValueSources.java	(revision 1663853)
+++ lucene/queries/src/test/org/apache/lucene/queries/function/TestValueSources.java	(working copy)
@@ -22,6 +22,7 @@
 import java.util.Map;
 import java.io.IOException;
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.DoubleField;
@@ -92,6 +93,7 @@
  */
 public class TestValueSources extends LuceneTestCase {
   static Directory dir;
+  static Analyzer analyzer;
   static IndexReader reader;
   static IndexSearcher searcher;
   
@@ -109,7 +111,8 @@
   @BeforeClass
   public static void beforeClass() throws Exception {
     dir = newDirectory();
-    IndexWriterConfig iwConfig = newIndexWriterConfig(new MockAnalyzer(random()));
+    analyzer = new MockAnalyzer(random());
+    IndexWriterConfig iwConfig = newIndexWriterConfig(analyzer);
     iwConfig.setMergePolicy(newLogMergePolicy());
     RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConfig);
     Document document = new Document();
@@ -169,6 +172,8 @@
     reader = null;
     dir.close();
     dir = null;
+    analyzer.close();
+    analyzer = null;
   }
   
   public void testConst() throws Exception {
Index: lucene/queries/src/test/org/apache/lucene/queries/mlt/TestMoreLikeThis.java
===================================================================
--- lucene/queries/src/test/org/apache/lucene/queries/mlt/TestMoreLikeThis.java	(revision 1663853)
+++ lucene/queries/src/test/org/apache/lucene/queries/mlt/TestMoreLikeThis.java	(working copy)
@@ -24,6 +24,7 @@
 import java.util.List;
 import java.util.Map;
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
@@ -87,7 +88,8 @@
     Map<String,Float> originalValues = getOriginalValues();
     
     MoreLikeThis mlt = new MoreLikeThis(reader);
-    mlt.setAnalyzer(new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false));
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false);
+    mlt.setAnalyzer(analyzer);
     mlt.setMinDocFreq(1);
     mlt.setMinTermFreq(1);
     mlt.setMinWordLen(1);
@@ -116,12 +118,14 @@
           + tq.getTerm().text() + "' got " + tq.getBoost(), totalBoost, tq
           .getBoost(), 0.0001);
     }
+    analyzer.close();
   }
   
   private Map<String,Float> getOriginalValues() throws IOException {
     Map<String,Float> originalValues = new HashMap<>();
     MoreLikeThis mlt = new MoreLikeThis(reader);
-    mlt.setAnalyzer(new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false));
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false);
+    mlt.setAnalyzer(analyzer);
     mlt.setMinDocFreq(1);
     mlt.setMinTermFreq(1);
     mlt.setMinWordLen(1);
@@ -135,6 +139,7 @@
       TermQuery tq = (TermQuery) clause.getQuery();
       originalValues.put(tq.getTerm().text(), tq.getBoost());
     }
+    analyzer.close();
     return originalValues;
   }
   
@@ -141,18 +146,21 @@
   // LUCENE-3326
   public void testMultiFields() throws Exception {
     MoreLikeThis mlt = new MoreLikeThis(reader);
-    mlt.setAnalyzer(new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false));
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false);
+    mlt.setAnalyzer(analyzer);
     mlt.setMinDocFreq(1);
     mlt.setMinTermFreq(1);
     mlt.setMinWordLen(1);
     mlt.setFieldNames(new String[] {"text", "foobar"});
     mlt.like("foobar", new StringReader("this is a test"));
+    analyzer.close();
   }
 
   // LUCENE-5725
   public void testMultiValues() throws Exception {
     MoreLikeThis mlt = new MoreLikeThis(reader);
-    mlt.setAnalyzer(new MockAnalyzer(random(), MockTokenizer.KEYWORD, false));
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);
+    mlt.setAnalyzer(analyzer);
     mlt.setMinDocFreq(1);
     mlt.setMinTermFreq(1);
     mlt.setMinWordLen(1);
@@ -167,12 +175,15 @@
       Term term = ((TermQuery) clause.getQuery()).getTerm();
       assertTrue(Arrays.asList(new Term("text", "lucene"), new Term("text", "apache")).contains(term));
     }
+    analyzer.close();
   }
 
   // just basic equals/hashcode etc
   public void testMoreLikeThisQuery() throws Exception {
-    Query query = new MoreLikeThisQuery("this is a test", new String[] { "text" }, new MockAnalyzer(random()), "text");
+    Analyzer analyzer = new MockAnalyzer(random());
+    Query query = new MoreLikeThisQuery("this is a test", new String[] { "text" }, analyzer, "text");
     QueryUtils.check(random(), query, searcher);
+    analyzer.close();
   }
 
   public void testTopN() throws Exception {
@@ -190,7 +201,8 @@
 
     // setup MLT query
     MoreLikeThis mlt = new MoreLikeThis(reader);
-    mlt.setAnalyzer(new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false));
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false);
+    mlt.setAnalyzer(analyzer);
     mlt.setMaxQueryTerms(topN);
     mlt.setMinDocFreq(1);
     mlt.setMinTermFreq(1);
@@ -221,6 +233,7 @@
     // clean up
     reader.close();
     dir.close();
+    analyzer.close();
   }
 
   private String[] generateStrSeq(int from, int size) {
Index: lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/DuplicateFilterTest.java
===================================================================
--- lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/DuplicateFilterTest.java	(revision 1663853)
+++ lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/DuplicateFilterTest.java	(working copy)
@@ -20,6 +20,7 @@
 import java.io.IOException;
 import java.util.HashSet;
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -31,6 +32,7 @@
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
 
@@ -40,12 +42,14 @@
   private IndexReader reader;
   TermQuery tq = new TermQuery(new Term("text", "lucene"));
   private IndexSearcher searcher;
+  Analyzer analyzer;
 
   @Override
   public void setUp() throws Exception {
     super.setUp();
     directory = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
+    analyzer = new MockAnalyzer(random());
+    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(analyzer).setMergePolicy(newLogMergePolicy()));
 
     //Add series of docs with filterable fields : url, text and dates  flags
     addDoc(writer, "http://lucene.apache.org", "lucene 1.4.3 available", "20040101");
@@ -69,8 +73,7 @@
 
   @Override
   public void tearDown() throws Exception {
-    reader.close();
-    directory.close();
+    IOUtils.close(reader, directory, analyzer);
     super.tearDown();
   }
 
Index: lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/FuzzyLikeThisQueryTest.java
===================================================================
--- lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/FuzzyLikeThisQueryTest.java	(revision 1663853)
+++ lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/FuzzyLikeThisQueryTest.java	(working copy)
@@ -30,6 +30,7 @@
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 
 import java.io.IOException;
@@ -47,7 +48,7 @@
 
     analyzer = new MockAnalyzer(random());
     directory = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
+    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, newIndexWriterConfig(analyzer).setMergePolicy(newLogMergePolicy()));
 
     //Add series of docs with misspelt names
     addDoc(writer, "jonathon smythe", "1");
@@ -63,8 +64,7 @@
 
   @Override
   public void tearDown() throws Exception {
-    reader.close();
-    directory.close();
+    IOUtils.close(reader, directory, analyzer);
     super.tearDown();
   }
 
Index: lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSlowFuzzyQuery2.java
===================================================================
--- lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSlowFuzzyQuery2.java	(revision 1663853)
+++ lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSlowFuzzyQuery2.java	(working copy)
@@ -22,6 +22,7 @@
 import java.io.InputStreamReader;
 import java.nio.charset.StandardCharsets;
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
@@ -34,6 +35,7 @@
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.search.similarities.DefaultSimilarity;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 
 /** 
@@ -90,7 +92,8 @@
     int terms = (int) Math.pow(2, bits);
     
     Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random(), MockTokenizer.KEYWORD, false)).setMergePolicy(newLogMergePolicy()));
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(analyzer).setMergePolicy(newLogMergePolicy()));
 
     Document doc = new Document();
     Field field = newTextField("field", "", Field.Store.NO);
@@ -129,8 +132,7 @@
         assertEquals(Float.parseFloat(scoreDoc[1]), docs.scoreDocs[i].score, epsilon);
       }
     }
-    r.close();
-    dir.close();
+    IOUtils.close(r, dir, analyzer);
   }
   
   /* map bits to unicode codepoints */
Index: lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestSpanRegexQuery.java
===================================================================
--- lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestSpanRegexQuery.java	(revision 1663853)
+++ lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestSpanRegexQuery.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -30,6 +31,7 @@
 import org.apache.lucene.search.spans.SpanMultiTermQueryWrapper;
 import org.apache.lucene.search.spans.SpanQuery;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 
 public class TestSpanRegexQuery extends LuceneTestCase {
@@ -53,7 +55,8 @@
   
   public void testSpanRegex() throws Exception {
     Directory directory = newDirectory();
-    IndexWriter writer = new IndexWriter(directory, newIndexWriterConfig(new MockAnalyzer(random())));
+    Analyzer analyzer = new MockAnalyzer(random());
+    IndexWriter writer = new IndexWriter(directory, newIndexWriterConfig(analyzer));
     Document doc = new Document();
     // doc.add(newField("field", "the quick brown fox jumps over the lazy dog",
     // Field.Store.NO, Field.Index.ANALYZED));
@@ -75,7 +78,6 @@
     // true);
     int numHits = searcher.search(sfq, 1000).totalHits;
     assertEquals(1, numHits);
-    reader.close();
-    directory.close();
+    IOUtils.close(reader, directory, analyzer);
   }
 }
Index: lucene/sandbox/src/test/org/apache/lucene/search/TestTermAutomatonQuery.java
===================================================================
--- lucene/sandbox/src/test/org/apache/lucene/search/TestTermAutomatonQuery.java	(revision 1663853)
+++ lucene/sandbox/src/test/org/apache/lucene/search/TestTermAutomatonQuery.java	(working copy)
@@ -48,6 +48,7 @@
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BitDocIdSet;
 import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
 import org.apache.lucene.util.automaton.Automata;
@@ -610,9 +611,7 @@
       }
     }
 
-    w.close();
-    r.close();
-    dir.close();
+    IOUtils.close(w, r, dir, analyzer);
   }
 
   private Set<String> toDocIDs(IndexSearcher s, TopDocs hits) throws IOException {
Index: lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestCase.java
===================================================================
--- lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestCase.java	(revision 1663853)
+++ lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestCase.java	(working copy)
@@ -29,6 +29,8 @@
 import com.spatial4j.core.distance.DistanceUtils;
 import com.spatial4j.core.shape.Point;
 import com.spatial4j.core.shape.Rectangle;
+
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.DirectoryReader;
@@ -46,8 +48,6 @@
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.LuceneTestCase.SuppressSysoutChecks;
 import org.apache.lucene.util.TestUtil;
-import org.junit.After;
-import org.junit.Before;
 
 import static com.carrotsearch.randomizedtesting.RandomizedTest.randomDouble;
 import static com.carrotsearch.randomizedtesting.RandomizedTest.randomGaussian;
@@ -63,6 +63,7 @@
   private DirectoryReader indexReader;
   protected RandomIndexWriter indexWriter;
   private Directory directory;
+  private Analyzer analyzer;
   protected IndexSearcher indexSearcher;
 
   protected SpatialContext ctx;//subclass must initialize
@@ -70,7 +71,6 @@
   protected Map<String,Type> uninvertMap = new HashMap<>();
   
   @Override
-  @Before
   public void setUp() throws Exception {
     super.setUp();
     // TODO: change this module to index docvalues instead of uninverting
@@ -80,13 +80,14 @@
 
     directory = newDirectory();
     final Random random = random();
-    indexWriter = new RandomIndexWriter(random,directory, newIndexWriterConfig(random));
+    analyzer = new MockAnalyzer(random);
+    indexWriter = new RandomIndexWriter(random,directory, newIWConfig(random, analyzer));
     indexReader = UninvertingReader.wrap(indexWriter.getReader(), uninvertMap);
     indexSearcher = newSearcher(indexReader);
   }
 
-  protected IndexWriterConfig newIndexWriterConfig(Random random) {
-    final IndexWriterConfig indexWriterConfig = LuceneTestCase.newIndexWriterConfig(random, new MockAnalyzer(random));
+  protected IndexWriterConfig newIWConfig(Random random, Analyzer analyzer) {
+    final IndexWriterConfig indexWriterConfig = LuceneTestCase.newIndexWriterConfig(random, analyzer);
     //TODO can we randomly choose a doc-values supported format?
     if (needsDocValues())
       indexWriterConfig.setCodec( TestUtil.getDefaultCodec());
@@ -98,10 +99,8 @@
   }
 
   @Override
-  @After
   public void tearDown() throws Exception {
-    indexWriter.close();
-    IOUtils.close(indexReader,directory);
+    IOUtils.close(indexWriter, indexReader, analyzer, directory);
     super.tearDown();
   }
 
Index: lucene/suggest/src/test/org/apache/lucene/search/spell/TestDirectSpellChecker.java
===================================================================
--- lucene/suggest/src/test/org/apache/lucene/search/spell/TestDirectSpellChecker.java	(revision 1663853)
+++ lucene/suggest/src/test/org/apache/lucene/search/spell/TestDirectSpellChecker.java	(working copy)
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
@@ -26,6 +27,7 @@
 import org.apache.lucene.index.Term;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.English;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 
 public class TestDirectSpellChecker extends LuceneTestCase {
@@ -33,8 +35,8 @@
   public void testInternalLevenshteinDistance() throws Exception {
     DirectSpellChecker spellchecker = new DirectSpellChecker();
     Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, 
-        new MockAnalyzer(random(), MockTokenizer.KEYWORD, true));
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, analyzer);
 
     String[] termsToAdd = { "metanoia", "metanoian", "metanoiai", "metanoias", "metanoi𐑍" };
     for (int i = 0; i < termsToAdd.length; i++) {
@@ -55,16 +57,15 @@
       assertTrue(word.score==sd.getDistance(misspelled, word.string));
     }
     
-    ir.close();
-    writer.close();
-    dir.close();
+    IOUtils.close(ir, writer, dir, analyzer);
   }
+  
   public void testSimpleExamples() throws Exception {
     DirectSpellChecker spellChecker = new DirectSpellChecker();
     spellChecker.setMinQueryLength(0);
     Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, 
-        new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, analyzer);
 
     for (int i = 0; i < 20; i++) {
       Document doc = new Document();
@@ -121,15 +122,13 @@
     assertTrue(similar.length > 0); 
     assertEquals("thousand", similar[0].string);
 
-    ir.close();
-    writer.close();
-    dir.close();
+    IOUtils.close(ir, writer, dir, analyzer);
   }
   
   public void testOptions() throws Exception {
     Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, 
-        new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, analyzer);
 
     Document doc = new Document();
     doc.add(newTextField("text", "foobar", Field.Store.NO));
@@ -187,16 +186,14 @@
         SuggestMode.SUGGEST_ALWAYS);
     assertEquals(2, similar.length);
 
-    ir.close();
-    writer.close();
-    dir.close();
+    IOUtils.close(ir, writer, dir, analyzer);;
   }
   
   public void testBogusField() throws Exception {
     DirectSpellChecker spellChecker = new DirectSpellChecker();
     Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, 
-        new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, analyzer);
 
     for (int i = 0; i < 20; i++) {
       Document doc = new Document();
@@ -210,9 +207,8 @@
         "bogusFieldBogusField", "fvie"), 2, ir,
         SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX);
     assertEquals(0, similar.length);
-    ir.close();
-    writer.close();
-    dir.close();
+    
+    IOUtils.close(ir, writer, dir, analyzer);
   }
   
   // simple test that transpositions work, we suggest five for fvie with ed=1
@@ -219,8 +215,8 @@
   public void testTransposition() throws Exception {
     DirectSpellChecker spellChecker = new DirectSpellChecker();
     Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, 
-        new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, analyzer);
 
     for (int i = 0; i < 20; i++) {
       Document doc = new Document();
@@ -235,9 +231,8 @@
         SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX);
     assertEquals(1, similar.length);
     assertEquals("five", similar[0].string);
-    ir.close();
-    writer.close();
-    dir.close();
+    
+    IOUtils.close(ir, writer, dir, analyzer);
   }
   
   // simple test that transpositions work, we suggest seventeen for seevntene with ed=2
@@ -244,8 +239,8 @@
   public void testTransposition2() throws Exception {
     DirectSpellChecker spellChecker = new DirectSpellChecker();
     Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, 
-        new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, analyzer);
 
     for (int i = 0; i < 20; i++) {
       Document doc = new Document();
@@ -260,8 +255,7 @@
         SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX);
     assertEquals(1, similar.length);
     assertEquals("seventeen", similar[0].string);
-    ir.close();
-    writer.close();
-    dir.close();
+    
+    IOUtils.close(ir, writer, dir, analyzer);
   }
 }
Index: lucene/suggest/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java
===================================================================
--- lucene/suggest/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java	(revision 1663853)
+++ lucene/suggest/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java	(working copy)
@@ -19,6 +19,7 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
@@ -39,7 +40,7 @@
 public class TestLuceneDictionary extends LuceneTestCase {
 
   private Directory store;
-
+  private Analyzer analyzer;
   private IndexReader indexReader = null;
   private LuceneDictionary ld;
   private BytesRefIterator it;
@@ -49,7 +50,8 @@
   public void setUp() throws Exception {
     super.setUp();
     store = newDirectory();
-    IndexWriter writer = new IndexWriter(store, newIndexWriterConfig(new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
+    analyzer = new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false);
+    IndexWriter writer = new IndexWriter(store, newIndexWriterConfig(analyzer));
 
     Document doc;
 
@@ -82,6 +84,7 @@
     if (indexReader != null)
       indexReader.close();
     store.close();
+    analyzer.close();
     super.tearDown();
   }
   
Index: lucene/suggest/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
===================================================================
--- lucene/suggest/src/test/org/apache/lucene/search/spell/TestSpellChecker.java	(revision 1663853)
+++ lucene/suggest/src/test/org/apache/lucene/search/spell/TestSpellChecker.java	(working copy)
@@ -27,6 +27,7 @@
 import java.util.concurrent.Executors;
 import java.util.concurrent.TimeUnit;
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -47,6 +48,7 @@
 public class TestSpellChecker extends LuceneTestCase {
   private SpellCheckerMock spellChecker;
   private Directory userindex, spellindex;
+  private Analyzer analyzer;
   private List<IndexSearcher> searchers;
 
   @Override
@@ -55,7 +57,8 @@
     
     //create a user index
     userindex = newDirectory();
-    IndexWriter writer = new IndexWriter(userindex, new IndexWriterConfig(new MockAnalyzer(random())));
+    analyzer = new MockAnalyzer(random());
+    IndexWriter writer = new IndexWriter(userindex, new IndexWriterConfig(analyzer));
 
     for (int i = 0; i < 1000; i++) {
       Document doc = new Document();
@@ -99,6 +102,7 @@
     if (!spellChecker.isClosed())
       spellChecker.close();
     spellindex.close();
+    analyzer.close();
     super.tearDown();
   }
 
Index: lucene/suggest/src/test/org/apache/lucene/search/spell/TestWordBreakSpellChecker.java
===================================================================
--- lucene/suggest/src/test/org/apache/lucene/search/spell/TestWordBreakSpellChecker.java	(revision 1663853)
+++ lucene/suggest/src/test/org/apache/lucene/search/spell/TestWordBreakSpellChecker.java	(working copy)
@@ -23,6 +23,7 @@
 
 import junit.framework.Assert;
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
@@ -34,17 +35,20 @@
 import org.apache.lucene.search.spell.WordBreakSpellChecker.BreakSuggestionSortMethod;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.English;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
 
 public class TestWordBreakSpellChecker extends LuceneTestCase {
-  private Directory dir = null;
+  private Directory dir;
+  private Analyzer analyzer;
   
   @Override
   public void setUp() throws Exception {
     super.setUp();
     dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, true));
+    analyzer = new MockAnalyzer(random(), MockTokenizer.WHITESPACE, true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, analyzer);
 
     for (int i = 900; i < 1112; i++) {
       Document doc = new Document();
@@ -75,289 +79,262 @@
   
   @Override
   public void tearDown() throws Exception {
-    if(dir!=null) {
-      dir.close();
-      dir = null;
-    }
+    IOUtils.close(dir, analyzer);
     super.tearDown();
   } 
+
   public void testCombiningWords() throws Exception {
-    IndexReader ir = null;
-    try {
-      ir = DirectoryReader.open(dir);
-      WordBreakSpellChecker wbsp = new WordBreakSpellChecker();
+    IndexReader ir = DirectoryReader.open(dir);
+    WordBreakSpellChecker wbsp = new WordBreakSpellChecker();
+    
+    {        
+      Term[] terms = { 
+          new Term("numbers", "one"),
+          new Term("numbers", "hun"),
+          new Term("numbers", "dred"),
+          new Term("numbers", "eight"),
+          new Term("numbers", "y"),
+          new Term("numbers", "eight"),
+      };
+      wbsp.setMaxChanges(3);
+      wbsp.setMaxCombineWordLength(20);
+      wbsp.setMinSuggestionFrequency(1);
+      CombineSuggestion[] cs = wbsp.suggestWordCombinations(terms, 10, ir, SuggestMode.SUGGEST_ALWAYS);
+      Assert.assertTrue(cs.length==5);
       
-      {        
-        Term[] terms = { 
-            new Term("numbers", "one"),
-            new Term("numbers", "hun"),
-            new Term("numbers", "dred"),
-            new Term("numbers", "eight"),
-            new Term("numbers", "y"),
-            new Term("numbers", "eight"),
-        };
-        wbsp.setMaxChanges(3);
-        wbsp.setMaxCombineWordLength(20);
-        wbsp.setMinSuggestionFrequency(1);
-        CombineSuggestion[] cs = wbsp.suggestWordCombinations(terms, 10, ir, SuggestMode.SUGGEST_ALWAYS);
-        Assert.assertTrue(cs.length==5);
-        
-        Assert.assertTrue(cs[0].originalTermIndexes.length==2);
-        Assert.assertTrue(cs[0].originalTermIndexes[0]==1);
-        Assert.assertTrue(cs[0].originalTermIndexes[1]==2);
-        Assert.assertTrue(cs[0].suggestion.string.equals("hundred"));
-        Assert.assertTrue(cs[0].suggestion.score==1);
-        
-        Assert.assertTrue(cs[1].originalTermIndexes.length==2);
-        Assert.assertTrue(cs[1].originalTermIndexes[0]==3);
-        Assert.assertTrue(cs[1].originalTermIndexes[1]==4);
-        Assert.assertTrue(cs[1].suggestion.string.equals("eighty"));
-        Assert.assertTrue(cs[1].suggestion.score==1);        
-        
-        Assert.assertTrue(cs[2].originalTermIndexes.length==2);
-        Assert.assertTrue(cs[2].originalTermIndexes[0]==4);
-        Assert.assertTrue(cs[2].originalTermIndexes[1]==5);
-        Assert.assertTrue(cs[2].suggestion.string.equals("yeight"));
-        Assert.assertTrue(cs[2].suggestion.score==1);
-        
-        for(int i=3 ; i<5 ; i++) {
-          Assert.assertTrue(cs[i].originalTermIndexes.length==3);
-          Assert.assertTrue(cs[i].suggestion.score==2);
-          Assert.assertTrue(
-              (cs[i].originalTermIndexes[0]==1 && 
-               cs[i].originalTermIndexes[1]==2 && 
-               cs[i].originalTermIndexes[2]==3 && 
-               cs[i].suggestion.string.equals("hundredeight")) ||
-              (cs[i].originalTermIndexes[0]==3 &&
-               cs[i].originalTermIndexes[1]==4 &&
-               cs[i].originalTermIndexes[2]==5 &&
-               cs[i].suggestion.string.equals("eightyeight"))
-         );
-        }     
-        
-        cs = wbsp.suggestWordCombinations(terms, 5, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX);
-        Assert.assertTrue(cs.length==2);
-        Assert.assertTrue(cs[0].originalTermIndexes.length==2);
-        Assert.assertTrue(cs[0].suggestion.score==1);
-        Assert.assertTrue(cs[0].originalTermIndexes[0]==1);
-        Assert.assertTrue(cs[0].originalTermIndexes[1]==2);
-        Assert.assertTrue(cs[0].suggestion.string.equals("hundred"));
-        Assert.assertTrue(cs[0].suggestion.score==1);
-        
-        Assert.assertTrue(cs[1].originalTermIndexes.length==3);
-        Assert.assertTrue(cs[1].suggestion.score==2);
-        Assert.assertTrue(cs[1].originalTermIndexes[0] == 1);
-        Assert.assertTrue(cs[1].originalTermIndexes[1] == 2);
-        Assert.assertTrue(cs[1].originalTermIndexes[2] == 3);
-        Assert.assertTrue(cs[1].suggestion.string.equals("hundredeight"));
-      }
-    } catch(Exception e) {
-      throw e;
-    } finally {
-      try { ir.close(); } catch(Exception e1) { }
-    }    
+      Assert.assertTrue(cs[0].originalTermIndexes.length==2);
+      Assert.assertTrue(cs[0].originalTermIndexes[0]==1);
+      Assert.assertTrue(cs[0].originalTermIndexes[1]==2);
+      Assert.assertTrue(cs[0].suggestion.string.equals("hundred"));
+      Assert.assertTrue(cs[0].suggestion.score==1);
+      
+      Assert.assertTrue(cs[1].originalTermIndexes.length==2);
+      Assert.assertTrue(cs[1].originalTermIndexes[0]==3);
+      Assert.assertTrue(cs[1].originalTermIndexes[1]==4);
+      Assert.assertTrue(cs[1].suggestion.string.equals("eighty"));
+      Assert.assertTrue(cs[1].suggestion.score==1);        
+      
+      Assert.assertTrue(cs[2].originalTermIndexes.length==2);
+      Assert.assertTrue(cs[2].originalTermIndexes[0]==4);
+      Assert.assertTrue(cs[2].originalTermIndexes[1]==5);
+      Assert.assertTrue(cs[2].suggestion.string.equals("yeight"));
+      Assert.assertTrue(cs[2].suggestion.score==1);
+      
+      for(int i=3 ; i<5 ; i++) {
+        Assert.assertTrue(cs[i].originalTermIndexes.length==3);
+        Assert.assertTrue(cs[i].suggestion.score==2);
+        Assert.assertTrue(
+            (cs[i].originalTermIndexes[0]==1 && 
+            cs[i].originalTermIndexes[1]==2 && 
+            cs[i].originalTermIndexes[2]==3 && 
+            cs[i].suggestion.string.equals("hundredeight")) ||
+            (cs[i].originalTermIndexes[0]==3 &&
+            cs[i].originalTermIndexes[1]==4 &&
+            cs[i].originalTermIndexes[2]==5 &&
+            cs[i].suggestion.string.equals("eightyeight"))
+            );
+      }     
+      
+      cs = wbsp.suggestWordCombinations(terms, 5, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX);
+      Assert.assertTrue(cs.length==2);
+      Assert.assertTrue(cs[0].originalTermIndexes.length==2);
+      Assert.assertTrue(cs[0].suggestion.score==1);
+      Assert.assertTrue(cs[0].originalTermIndexes[0]==1);
+      Assert.assertTrue(cs[0].originalTermIndexes[1]==2);
+      Assert.assertTrue(cs[0].suggestion.string.equals("hundred"));
+      Assert.assertTrue(cs[0].suggestion.score==1);
+      
+      Assert.assertTrue(cs[1].originalTermIndexes.length==3);
+      Assert.assertTrue(cs[1].suggestion.score==2);
+      Assert.assertTrue(cs[1].originalTermIndexes[0] == 1);
+      Assert.assertTrue(cs[1].originalTermIndexes[1] == 2);
+      Assert.assertTrue(cs[1].originalTermIndexes[2] == 3);
+      Assert.assertTrue(cs[1].suggestion.string.equals("hundredeight"));
+    }
+    ir.close();
   }  
+ 
   public void testBreakingWords() throws Exception {
-    IndexReader ir = null;
-    try {
-      ir = DirectoryReader.open(dir);
-      WordBreakSpellChecker wbsp = new WordBreakSpellChecker();
+    IndexReader ir = DirectoryReader.open(dir);
+    WordBreakSpellChecker wbsp = new WordBreakSpellChecker();
+    
+    {
+      Term term = new Term("numbers", "ninetynine");
+      wbsp.setMaxChanges(1);
+      wbsp.setMinBreakWordLength(1);
+      wbsp.setMinSuggestionFrequency(1);
+      SuggestWord[][] sw = wbsp.suggestWordBreaks(term, 5, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX, BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
+      Assert.assertTrue(sw.length==1);
+      Assert.assertTrue(sw[0].length==2);
+      Assert.assertTrue(sw[0][0].string.equals("ninety"));
+      Assert.assertTrue(sw[0][1].string.equals("nine"));
+      Assert.assertTrue(sw[0][0].score == 1);
+      Assert.assertTrue(sw[0][1].score == 1);
+    }
+    {
+      Term term = new Term("numbers", "onethousand");
+      wbsp.setMaxChanges(1);
+      wbsp.setMinBreakWordLength(1);
+      wbsp.setMinSuggestionFrequency(1);
+      SuggestWord[][] sw = wbsp.suggestWordBreaks(term, 2, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX, BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
+      Assert.assertTrue(sw.length==1);
+      Assert.assertTrue(sw[0].length==2);
+      Assert.assertTrue(sw[0][0].string.equals("one"));
+      Assert.assertTrue(sw[0][1].string.equals("thousand"));
+      Assert.assertTrue(sw[0][0].score == 1);
+      Assert.assertTrue(sw[0][1].score == 1);
       
-      {
-        Term term = new Term("numbers", "ninetynine");
-        wbsp.setMaxChanges(1);
-        wbsp.setMinBreakWordLength(1);
-        wbsp.setMinSuggestionFrequency(1);
-        SuggestWord[][] sw = wbsp.suggestWordBreaks(term, 5, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX, BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
-        Assert.assertTrue(sw.length==1);
-        Assert.assertTrue(sw[0].length==2);
-        Assert.assertTrue(sw[0][0].string.equals("ninety"));
-        Assert.assertTrue(sw[0][1].string.equals("nine"));
-        Assert.assertTrue(sw[0][0].score == 1);
-        Assert.assertTrue(sw[0][1].score == 1);
-      }
-      {
-        Term term = new Term("numbers", "onethousand");
-        wbsp.setMaxChanges(1);
-        wbsp.setMinBreakWordLength(1);
-        wbsp.setMinSuggestionFrequency(1);
-        SuggestWord[][] sw = wbsp.suggestWordBreaks(term, 2, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX, BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
-        Assert.assertTrue(sw.length==1);
-        Assert.assertTrue(sw[0].length==2);
-        Assert.assertTrue(sw[0][0].string.equals("one"));
-        Assert.assertTrue(sw[0][1].string.equals("thousand"));
-        Assert.assertTrue(sw[0][0].score == 1);
-        Assert.assertTrue(sw[0][1].score == 1);
-        
-        wbsp.setMaxChanges(2);
-        wbsp.setMinSuggestionFrequency(1);
-        sw = wbsp.suggestWordBreaks(term, 1, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX, BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
-        Assert.assertTrue(sw.length==1);
-        Assert.assertTrue(sw[0].length==2);
-        
-        wbsp.setMaxChanges(2);
-        wbsp.setMinSuggestionFrequency(2);
-        sw = wbsp.suggestWordBreaks(term, 2, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX, BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
-        Assert.assertTrue(sw.length==1);
-        Assert.assertTrue(sw[0].length==2);
-        
-        wbsp.setMaxChanges(2);
-        wbsp.setMinSuggestionFrequency(1);
-        sw = wbsp.suggestWordBreaks(term, 2, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX, BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
-        Assert.assertTrue(sw.length==2);
-        Assert.assertTrue(sw[0].length==2);
-        Assert.assertTrue(sw[0][0].string.equals("one"));
-        Assert.assertTrue(sw[0][1].string.equals("thousand"));
-        Assert.assertTrue(sw[0][0].score == 1);
-        Assert.assertTrue(sw[0][1].score == 1);
-        Assert.assertTrue(sw[0][1].freq>1);
-        Assert.assertTrue(sw[0][0].freq>sw[0][1].freq);
-        Assert.assertTrue(sw[1].length==3);
-        Assert.assertTrue(sw[1][0].string.equals("one"));
-        Assert.assertTrue(sw[1][1].string.equals("thou"));
-        Assert.assertTrue(sw[1][2].string.equals("sand"));
-        Assert.assertTrue(sw[1][0].score == 2);
-        Assert.assertTrue(sw[1][1].score == 2);
-        Assert.assertTrue(sw[1][2].score == 2);
-        Assert.assertTrue(sw[1][0].freq>1);
-        Assert.assertTrue(sw[1][1].freq==1);
-        Assert.assertTrue(sw[1][2].freq==1);
-      }
-      {
-        Term term = new Term("numbers", "onethousandonehundredeleven");
-        wbsp.setMaxChanges(3);
-        wbsp.setMinBreakWordLength(1);
-        wbsp.setMinSuggestionFrequency(1);
-        SuggestWord[][] sw = wbsp.suggestWordBreaks(term, 5, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX, BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
-        Assert.assertTrue(sw.length==0);
-        
-        wbsp.setMaxChanges(4);
-        sw = wbsp.suggestWordBreaks(term, 5, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX, BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
-        Assert.assertTrue(sw.length==1);
-        Assert.assertTrue(sw[0].length==5);
-        
-        wbsp.setMaxChanges(5);
-        sw = wbsp.suggestWordBreaks(term, 5, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX, BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
-        Assert.assertTrue(sw.length==2);
-        Assert.assertTrue(sw[0].length==5);
-        Assert.assertTrue(sw[0][1].string.equals("thousand"));
-        Assert.assertTrue(sw[1].length==6);
-        Assert.assertTrue(sw[1][1].string.equals("thou"));
-        Assert.assertTrue(sw[1][2].string.equals("sand"));
-      }
-      {
-        //make sure we can handle 2-char codepoints
-        Term term = new Term("numbers", "\uD864\uDC79");
-        wbsp.setMaxChanges(1);
-        wbsp.setMinBreakWordLength(1);
-        wbsp.setMinSuggestionFrequency(1);
-        SuggestWord[][] sw = wbsp.suggestWordBreaks(term, 5, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX, BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
-        Assert.assertTrue(sw.length==0);        
-      }
+      wbsp.setMaxChanges(2);
+      wbsp.setMinSuggestionFrequency(1);
+      sw = wbsp.suggestWordBreaks(term, 1, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX, BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
+      Assert.assertTrue(sw.length==1);
+      Assert.assertTrue(sw[0].length==2);
       
-    } catch(Exception e) {
-      throw e;
-    } finally {
-      try { ir.close(); } catch(Exception e1) { }
-    }    
+      wbsp.setMaxChanges(2);
+      wbsp.setMinSuggestionFrequency(2);
+      sw = wbsp.suggestWordBreaks(term, 2, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX, BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
+      Assert.assertTrue(sw.length==1);
+      Assert.assertTrue(sw[0].length==2);
+      
+      wbsp.setMaxChanges(2);
+      wbsp.setMinSuggestionFrequency(1);
+      sw = wbsp.suggestWordBreaks(term, 2, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX, BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
+      Assert.assertTrue(sw.length==2);
+      Assert.assertTrue(sw[0].length==2);
+      Assert.assertTrue(sw[0][0].string.equals("one"));
+      Assert.assertTrue(sw[0][1].string.equals("thousand"));
+      Assert.assertTrue(sw[0][0].score == 1);
+      Assert.assertTrue(sw[0][1].score == 1);
+      Assert.assertTrue(sw[0][1].freq>1);
+      Assert.assertTrue(sw[0][0].freq>sw[0][1].freq);
+      Assert.assertTrue(sw[1].length==3);
+      Assert.assertTrue(sw[1][0].string.equals("one"));
+      Assert.assertTrue(sw[1][1].string.equals("thou"));
+      Assert.assertTrue(sw[1][2].string.equals("sand"));
+      Assert.assertTrue(sw[1][0].score == 2);
+      Assert.assertTrue(sw[1][1].score == 2);
+      Assert.assertTrue(sw[1][2].score == 2);
+      Assert.assertTrue(sw[1][0].freq>1);
+      Assert.assertTrue(sw[1][1].freq==1);
+      Assert.assertTrue(sw[1][2].freq==1);
+    }
+    {
+      Term term = new Term("numbers", "onethousandonehundredeleven");
+      wbsp.setMaxChanges(3);
+      wbsp.setMinBreakWordLength(1);
+      wbsp.setMinSuggestionFrequency(1);
+      SuggestWord[][] sw = wbsp.suggestWordBreaks(term, 5, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX, BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
+      Assert.assertTrue(sw.length==0);
+      
+      wbsp.setMaxChanges(4);
+      sw = wbsp.suggestWordBreaks(term, 5, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX, BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
+      Assert.assertTrue(sw.length==1);
+      Assert.assertTrue(sw[0].length==5);
+      
+      wbsp.setMaxChanges(5);
+      sw = wbsp.suggestWordBreaks(term, 5, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX, BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
+      Assert.assertTrue(sw.length==2);
+      Assert.assertTrue(sw[0].length==5);
+      Assert.assertTrue(sw[0][1].string.equals("thousand"));
+      Assert.assertTrue(sw[1].length==6);
+      Assert.assertTrue(sw[1][1].string.equals("thou"));
+      Assert.assertTrue(sw[1][2].string.equals("sand"));
+    }
+    {
+      //make sure we can handle 2-char codepoints
+      Term term = new Term("numbers", "\uD864\uDC79");
+      wbsp.setMaxChanges(1);
+      wbsp.setMinBreakWordLength(1);
+      wbsp.setMinSuggestionFrequency(1);
+      SuggestWord[][] sw = wbsp.suggestWordBreaks(term, 5, ir, SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX, BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
+      Assert.assertTrue(sw.length==0);        
+    }
+    
+    ir.close();
   }
+
   public void testRandom() throws Exception {
     int numDocs = TestUtil.nextInt(random(), (10 * RANDOM_MULTIPLIER),
         (100 * RANDOM_MULTIPLIER));
-    Directory dir = null;
-    RandomIndexWriter writer = null;
     IndexReader ir = null;
-    try {
-      dir = newDirectory();
-      writer = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(),
-          MockTokenizer.WHITESPACE, false));
-      int maxLength = TestUtil.nextInt(random(), 5, 50);
-      List<String> originals = new ArrayList<>(numDocs);
-      List<String[]> breaks = new ArrayList<>(numDocs);
-      for (int i = 0; i < numDocs; i++) {
-        String orig = "";
-        if (random().nextBoolean()) {
-          while (!goodTestString(orig)) {
-            orig = TestUtil.randomSimpleString(random(), maxLength);
-          }
-        } else {
-          while (!goodTestString(orig)) {
-            orig = TestUtil.randomUnicodeString(random(), maxLength);
-          }
+    
+    Directory dir = newDirectory();
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, analyzer);
+    int maxLength = TestUtil.nextInt(random(), 5, 50);
+    List<String> originals = new ArrayList<>(numDocs);
+    List<String[]> breaks = new ArrayList<>(numDocs);
+    for (int i = 0; i < numDocs; i++) {
+      String orig = "";
+      if (random().nextBoolean()) {
+        while (!goodTestString(orig)) {
+          orig = TestUtil.randomSimpleString(random(), maxLength);
         }
-        originals.add(orig);
-        int totalLength = orig.codePointCount(0, orig.length());
-        int breakAt = orig.offsetByCodePoints(0,
-            TestUtil.nextInt(random(), 1, totalLength - 1));
-        String[] broken = new String[2];
-        broken[0] = orig.substring(0, breakAt);
-        broken[1] = orig.substring(breakAt);
-        breaks.add(broken);
-        Document doc = new Document();
-        doc.add(newTextField("random_break", broken[0] + " " + broken[1],
-            Field.Store.NO));
-        doc.add(newTextField("random_combine", orig, Field.Store.NO));
-        writer.addDocument(doc);
+      } else {
+        while (!goodTestString(orig)) {
+          orig = TestUtil.randomUnicodeString(random(), maxLength);
+        }
       }
-      writer.commit();
-      writer.close();
-      
-      ir = DirectoryReader.open(dir);
-      WordBreakSpellChecker wbsp = new WordBreakSpellChecker();
-      wbsp.setMaxChanges(1);
-      wbsp.setMinBreakWordLength(1);
-      wbsp.setMinSuggestionFrequency(1);
-      wbsp.setMaxCombineWordLength(maxLength);
-      for (int i = 0; i < originals.size(); i++) {
-        String orig = originals.get(i);
-        String left = breaks.get(i)[0];
-        String right = breaks.get(i)[1];
-        {
-          Term term = new Term("random_break", orig);
-          
-          SuggestWord[][] sw = wbsp.suggestWordBreaks(term, originals.size(),
-              ir, SuggestMode.SUGGEST_ALWAYS,
-              BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
-          boolean failed = true;
-          for (SuggestWord[] sw1 : sw) {
-            Assert.assertTrue(sw1.length == 2);
-            if (sw1[0].string.equals(left) && sw1[1].string.equals(right)) {
-              failed = false;
-            }
+      originals.add(orig);
+      int totalLength = orig.codePointCount(0, orig.length());
+      int breakAt = orig.offsetByCodePoints(0,
+          TestUtil.nextInt(random(), 1, totalLength - 1));
+      String[] broken = new String[2];
+      broken[0] = orig.substring(0, breakAt);
+      broken[1] = orig.substring(breakAt);
+      breaks.add(broken);
+      Document doc = new Document();
+      doc.add(newTextField("random_break", broken[0] + " " + broken[1],
+          Field.Store.NO));
+      doc.add(newTextField("random_combine", orig, Field.Store.NO));
+      writer.addDocument(doc);
+    }
+    writer.commit();
+    writer.close();
+    
+    ir = DirectoryReader.open(dir);
+    WordBreakSpellChecker wbsp = new WordBreakSpellChecker();
+    wbsp.setMaxChanges(1);
+    wbsp.setMinBreakWordLength(1);
+    wbsp.setMinSuggestionFrequency(1);
+    wbsp.setMaxCombineWordLength(maxLength);
+    for (int i = 0; i < originals.size(); i++) {
+      String orig = originals.get(i);
+      String left = breaks.get(i)[0];
+      String right = breaks.get(i)[1];
+      {
+        Term term = new Term("random_break", orig);
+        
+        SuggestWord[][] sw = wbsp.suggestWordBreaks(term, originals.size(),
+            ir, SuggestMode.SUGGEST_ALWAYS,
+            BreakSuggestionSortMethod.NUM_CHANGES_THEN_MAX_FREQUENCY);
+        boolean failed = true;
+        for (SuggestWord[] sw1 : sw) {
+          Assert.assertTrue(sw1.length == 2);
+          if (sw1[0].string.equals(left) && sw1[1].string.equals(right)) {
+            failed = false;
           }
-          Assert.assertFalse("Failed getting break suggestions\n >Original: "
-              + orig + "\n >Left: " + left + "\n >Right: " + right, failed);
         }
-        {
-          Term[] terms = {new Term("random_combine", left),
-              new Term("random_combine", right)};
-          CombineSuggestion[] cs = wbsp.suggestWordCombinations(terms,
-              originals.size(), ir, SuggestMode.SUGGEST_ALWAYS);
-          boolean failed = true;
-          for (CombineSuggestion cs1 : cs) {
-            Assert.assertTrue(cs1.originalTermIndexes.length == 2);
-            if (cs1.suggestion.string.equals(left + right)) {
-              failed = false;
-            }
+        Assert.assertFalse("Failed getting break suggestions\n >Original: "
+            + orig + "\n >Left: " + left + "\n >Right: " + right, failed);
+      }
+      {
+        Term[] terms = {new Term("random_combine", left),
+            new Term("random_combine", right)};
+        CombineSuggestion[] cs = wbsp.suggestWordCombinations(terms,
+            originals.size(), ir, SuggestMode.SUGGEST_ALWAYS);
+        boolean failed = true;
+        for (CombineSuggestion cs1 : cs) {
+          Assert.assertTrue(cs1.originalTermIndexes.length == 2);
+          if (cs1.suggestion.string.equals(left + right)) {
+            failed = false;
           }
-          Assert.assertFalse("Failed getting combine suggestions\n >Original: "
-              + orig + "\n >Left: " + left + "\n >Right: " + right, failed);
         }
+        Assert.assertFalse("Failed getting combine suggestions\n >Original: "
+            + orig + "\n >Left: " + left + "\n >Right: " + right, failed);
       }
-      
-    } catch (Exception e) {
-      throw e;
-    } finally {
-      try {
-        ir.close();
-      } catch (Exception e1) {}
-      try {
-        writer.close();
-      } catch (Exception e1) {}
-      try {
-        dir.close();
-      } catch (Exception e1) {}
     }
+    IOUtils.close(ir, dir, analyzer);
   }
   
   private static final Pattern mockTokenizerWhitespacePattern = Pattern
Index: lucene/suggest/src/test/org/apache/lucene/search/suggest/DocumentDictionaryTest.java
===================================================================
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/DocumentDictionaryTest.java	(revision 1663853)
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/DocumentDictionaryTest.java	(working copy)
@@ -11,6 +11,7 @@
 import java.util.Random;
 import java.util.Set;
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -26,6 +27,7 @@
 import org.apache.lucene.search.spell.Dictionary;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 import org.junit.Test;
 
@@ -113,7 +115,8 @@
   @Test
   public void testEmptyReader() throws IOException {
     Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
+    Analyzer analyzer = new MockAnalyzer(random());
+    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);
     iwc.setMergePolicy(newLogMergePolicy());
     // Make sure the index is created?
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
@@ -127,14 +130,14 @@
     assertEquals(inputIterator.weight(), 0);
     assertNull(inputIterator.payload());
     
-    ir.close();
-    dir.close();
+    IOUtils.close(ir, analyzer, dir);
   }
   
   @Test
   public void testBasic() throws IOException {
     Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
+    Analyzer analyzer = new MockAnalyzer(random());
+    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);
     iwc.setMergePolicy(newLogMergePolicy());
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
     Map.Entry<List<String>, Map<String, Document>> res = generateIndexDocuments(atLeast(1000), true, false);
@@ -162,14 +165,14 @@
     }
     assertTrue(docs.isEmpty());
     
-    ir.close();
-    dir.close();
+    IOUtils.close(ir, analyzer, dir);
   }
  
   @Test
   public void testWithoutPayload() throws IOException {
     Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
+    Analyzer analyzer = new MockAnalyzer(random());
+    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);
     iwc.setMergePolicy(newLogMergePolicy());
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
     Map.Entry<List<String>, Map<String, Document>> res = generateIndexDocuments(atLeast(1000), false, false);
@@ -198,14 +201,14 @@
     
     assertTrue(docs.isEmpty());
     
-    ir.close();
-    dir.close();
+    IOUtils.close(ir, analyzer, dir);
   }
   
   @Test
   public void testWithContexts() throws IOException {
     Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
+    Analyzer analyzer = new MockAnalyzer(random());
+    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);
     iwc.setMergePolicy(newLogMergePolicy());
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
     Map.Entry<List<String>, Map<String, Document>> res = generateIndexDocuments(atLeast(1000), true, true);
@@ -239,14 +242,14 @@
     }
     assertTrue(docs.isEmpty());
     
-    ir.close();
-    dir.close();
+    IOUtils.close(ir, analyzer, dir);
   }
   
   @Test
   public void testWithDeletions() throws IOException {
     Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
+    Analyzer analyzer = new MockAnalyzer(random());
+    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);
     iwc.setMergePolicy(newLogMergePolicy());
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
     Map.Entry<List<String>, Map<String, Document>> res = generateIndexDocuments(atLeast(1000), false, false);
@@ -296,14 +299,14 @@
     }
     assertTrue(docs.isEmpty());
     
-    ir.close();
-    dir.close();
+    IOUtils.close(ir, analyzer, dir);
   }
 
   @Test
   public void testMultiValuedField() throws IOException {
     Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(random(), new MockAnalyzer(random()));
+    Analyzer analyzer = new MockAnalyzer(random());
+    IndexWriterConfig iwc = newIndexWriterConfig(random(), analyzer);
     iwc.setMergePolicy(newLogMergePolicy());
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
 
@@ -325,8 +328,7 @@
       assertTrue(inputIterator.contexts().equals(nextSuggestion.contexts));
     }
     assertFalse(suggestionsIter.hasNext());
-    ir.close();
-    dir.close();
+    IOUtils.close(ir, analyzer, dir);
   }
 
   private List<Suggestion> indexMultiValuedDocuments(int numDocs, RandomIndexWriter writer) throws IOException {
Index: lucene/suggest/src/test/org/apache/lucene/search/suggest/DocumentValueSourceDictionaryTest.java
===================================================================
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/DocumentValueSourceDictionaryTest.java	(revision 1663853)
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/DocumentValueSourceDictionaryTest.java	(working copy)
@@ -26,6 +26,7 @@
 import java.util.Random;
 import java.util.Set;
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -44,6 +45,7 @@
 import org.apache.lucene.search.spell.Dictionary;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 import org.junit.Test;
 
@@ -84,7 +86,8 @@
   @Test
   public void testEmptyReader() throws IOException {
     Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
+    Analyzer analyzer = new MockAnalyzer(random());
+    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);
     iwc.setMergePolicy(newLogMergePolicy());
     // Make sure the index is created?
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
@@ -98,14 +101,14 @@
     assertEquals(inputIterator.weight(), 0);
     assertNull(inputIterator.payload());
 
-    ir.close();
-    dir.close();
+    IOUtils.close(ir, analyzer, dir);
   }
   
   @Test
   public void testBasic() throws IOException {
     Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
+    Analyzer analyzer = new MockAnalyzer(random());
+    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);
     iwc.setMergePolicy(newLogMergePolicy());
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
     Map<String, Document> docs = generateIndexDocuments(atLeast(100));
@@ -130,14 +133,14 @@
       assertTrue(inputIterator.payload().equals(doc.getField(PAYLOAD_FIELD_NAME).binaryValue()));
     }
     assertTrue(docs.isEmpty());
-    ir.close();
-    dir.close();
+    IOUtils.close(ir, analyzer, dir);
   }
   
   @Test
   public void testWithContext() throws IOException {
     Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
+    Analyzer analyzer = new MockAnalyzer(random());
+    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);
     iwc.setMergePolicy(newLogMergePolicy());
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
     Map<String, Document> docs = generateIndexDocuments(atLeast(100));
@@ -167,14 +170,14 @@
       assertEquals(originalCtxs, inputIterator.contexts());
     }
     assertTrue(docs.isEmpty());
-    ir.close();
-    dir.close();
+    IOUtils.close(ir, analyzer, dir);
   }
 
   @Test
   public void testWithoutPayload() throws IOException {
     Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
+    Analyzer analyzer = new MockAnalyzer(random());
+    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);
     iwc.setMergePolicy(newLogMergePolicy());
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
     Map<String, Document> docs = generateIndexDocuments(atLeast(100));
@@ -199,14 +202,14 @@
       assertEquals(inputIterator.payload(), null);
     }
     assertTrue(docs.isEmpty());
-    ir.close();
-    dir.close();
+    IOUtils.close(ir, analyzer, dir);
   }
   
   @Test
   public void testWithDeletions() throws IOException {
     Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
+    Analyzer analyzer = new MockAnalyzer(random());
+    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);
     iwc.setMergePolicy(newLogMergePolicy());
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
     Map<String, Document> docs = generateIndexDocuments(atLeast(100));
@@ -252,15 +255,14 @@
       assertTrue(inputIterator.payload().equals(doc.getField(PAYLOAD_FIELD_NAME).binaryValue()));
     }
     assertTrue(docs.isEmpty());
-    ir.close();
-    dir.close();
+    IOUtils.close(ir, analyzer, dir);
   }
   
   @Test
   public void testWithValueSource() throws IOException {
-    
     Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
+    Analyzer analyzer = new MockAnalyzer(random());
+    IndexWriterConfig iwc = newIndexWriterConfig(analyzer);
     iwc.setMergePolicy(newLogMergePolicy());
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
     Map<String, Document> docs = generateIndexDocuments(atLeast(100));
@@ -281,8 +283,7 @@
       assertTrue(inputIterator.payload().equals(doc.getField(PAYLOAD_FIELD_NAME).binaryValue()));
     }
     assertTrue(docs.isEmpty());
-    ir.close();
-    dir.close();
+    IOUtils.close(ir, analyzer, dir);
   }
   
 }
Index: lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest.java
===================================================================
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest.java	(revision 1663853)
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest.java	(working copy)
@@ -43,6 +43,7 @@
 import org.apache.lucene.search.suggest.Lookup.LookupResult;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
 import org.junit.Test;
@@ -117,6 +118,7 @@
     testConstructorDefaults(suggester, keys, a, false, true);
     
     suggester.close();
+    a.close();
   }
 
   private void testConstructorDefaults(AnalyzingInfixSuggester suggester, Input[] keys, Analyzer a, 
@@ -158,6 +160,7 @@
     assertEquals(new BytesRef("foobaz"), results.get(0).payload);
     assertEquals(2, suggester.getCount());
     suggester.close();
+    a.close();
   }
 
   /** Used to return highlighted result; see {@link
@@ -239,6 +242,7 @@
     assertEquals(10, results.get(0).value);
     assertEquals(new BytesRef("foobaz"), results.get(0).payload);
     suggester.close();
+    a.close();
   }
 
   public String toString(List<LookupHighlightFragment> fragments) {
@@ -320,6 +324,7 @@
       suggester = new AnalyzingInfixSuggester(newFSDirectory(tempDir), a, a, minPrefixLength, false);
     }
     suggester.close();
+    a.close();
   }
 
   public void testHighlight() throws Exception {
@@ -335,6 +340,7 @@
     assertEquals("a penny saved is a penny earned", results.get(0).key);
     assertEquals("a <b>penn</b>y saved is a <b>penn</b>y earned", results.get(0).highlightKey);
     suggester.close();
+    a.close();
   }
 
   public void testHighlightCaseChange() throws Exception {
@@ -367,6 +373,7 @@
     assertEquals("a Penny saved is a penny earned", results.get(0).key);
     assertEquals("a <b>Penny</b> saved is a <b>penny</b> earned", results.get(0).highlightKey);
     suggester.close();
+    a.close();
   }
 
   public void testDoubleClose() throws Exception {
@@ -379,6 +386,7 @@
     suggester.build(new InputArrayIterator(keys));
     suggester.close();
     suggester.close();
+    a.close();
   }
 
   public void testSuggestStopFilter() throws Exception {
@@ -413,6 +421,7 @@
     assertEquals("a bob for apples", results.get(0).key);
     assertEquals("a bob for <b>a</b>pples", results.get(0).highlightKey);
     suggester.close();
+    IOUtils.close(suggester, indexAnalyzer, queryAnalyzer);
   }
 
   public void testEmptyAtStart() throws Exception {
@@ -456,6 +465,7 @@
     assertEquals(new BytesRef("foobaz"), results.get(0).payload);
 
     suggester.close();
+    a.close();
   }
 
   public void testBothExactAndPrefix() throws Exception {
@@ -472,6 +482,7 @@
     assertEquals(10, results.get(0).value);
     assertEquals(new BytesRef("foobaz"), results.get(0).payload);
     suggester.close();
+    a.close();
   }
 
   private static String randomText() {
@@ -741,6 +752,7 @@
 
     lookupThread.finish();
     suggester.close();
+    a.close();
   }
 
   private static String hilite(boolean lastPrefix, String[] inputTerms, String[] queryTerms) {
@@ -856,6 +868,7 @@
     assertEquals(10, results.get(1).value);
     assertEquals(new BytesRef("foobaz"), results.get(1).payload);
     suggester.close();
+    a.close();
   }
 
   public void testNRTWithParallelAdds() throws IOException, InterruptedException {
@@ -896,6 +909,7 @@
     assertEquals("python", results.get(0).key);
 
     suggester.close();
+    a.close();
   }
 
   private class IndexDocument implements Runnable {
@@ -1153,6 +1167,7 @@
       assertTrue(result.contexts.contains(new BytesRef("baz")));
 
       suggester.close();
+      a.close();
     }
   }
 
@@ -1170,6 +1185,7 @@
 
     dir.close();
     suggester.close();
+    a.close();
   }
 
   private String pfmToString(AnalyzingInfixSuggester suggester, String surface, String prefix) throws IOException {
Index: lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest.java
===================================================================
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest.java	(revision 1663853)
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest.java	(working copy)
@@ -53,6 +53,7 @@
 import org.apache.lucene.search.suggest.InputArrayIterator;
 import org.apache.lucene.util.AttributeFactory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LineFileDocs;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
@@ -71,7 +72,8 @@
         new Input("barbara", 1)
     );
 
-    AnalyzingSuggester suggester = new AnalyzingSuggester(new MockAnalyzer(random(), MockTokenizer.KEYWORD, false));
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);
+    AnalyzingSuggester suggester = new AnalyzingSuggester(analyzer);
     suggester.build(new InputArrayIterator(keys));
     
     // top N of 2, but only foo is available
@@ -104,6 +106,8 @@
     assertEquals(10, results.get(1).value, 0.01F);
     assertEquals("barbara", results.get(2).key.toString());
     assertEquals(6, results.get(2).value, 0.01F);
+    
+    analyzer.close();
   }
   
   public void testKeywordWithPayloads() throws Exception {
@@ -115,7 +119,8 @@
       new Input("bar", 8, new BytesRef("should also be deduplicated")),
       new Input("barbara", 6, new BytesRef("for all the fish")));
     
-    AnalyzingSuggester suggester = new AnalyzingSuggester(new MockAnalyzer(random(), MockTokenizer.KEYWORD, false));
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);
+    AnalyzingSuggester suggester = new AnalyzingSuggester(analyzer);
     suggester.build(new InputArrayIterator(keys));
     for (int i = 0; i < 2; i++) {
       // top N of 2, but only foo is available
@@ -156,6 +161,7 @@
       assertEquals(6, results.get(2).value, 0.01F);
       assertEquals(new BytesRef("for all the fish"), results.get(2).payload);
     }
+    analyzer.close();
   }
   
   public void testRandomRealisticKeys() throws IOException {
@@ -173,7 +179,9 @@
           mapping.put(title, Long.valueOf(randomWeight));
       }
     }
-    AnalyzingSuggester analyzingSuggester = new AnalyzingSuggester(new MockAnalyzer(random()), new MockAnalyzer(random()),
+    Analyzer indexAnalyzer = new MockAnalyzer(random());
+    Analyzer queryAnalyzer = new MockAnalyzer(random());
+    AnalyzingSuggester analyzingSuggester = new AnalyzingSuggester(indexAnalyzer, queryAnalyzer,
         AnalyzingSuggester.EXACT_FIRST | AnalyzingSuggester.PRESERVE_SEP, 256, -1, random().nextBoolean());
     boolean doPayloads = random().nextBoolean();
     if (doPayloads) {
@@ -198,7 +206,7 @@
       }
     }
     
-    lineFile.close();
+    IOUtils.close(lineFile, indexAnalyzer, queryAnalyzer);
   }
   
   // TODO: more tests
@@ -232,6 +240,8 @@
     assertEquals(1, results.size());
     assertEquals("the ghost of christmas past", results.get(0).key.toString());
     assertEquals(50, results.get(0).value, 0.01F);
+    
+    standard.close();
   }
 
   public void testEmpty() throws Exception {
@@ -241,6 +251,7 @@
 
     List<LookupResult> result = suggester.lookup("a", false, 20);
     assertTrue(result.isEmpty());
+    standard.close();
   }
 
   public void testNoSeps() throws Exception {
@@ -265,6 +276,7 @@
     // complete to "abcd", which has higher weight so should
     // appear first:
     assertEquals("abcd", r.get(0).key.toString());
+    a.close();
   }
 
   public void testGraphDups() throws Exception {
@@ -330,6 +342,7 @@
     assertEquals(50, results.get(0).value);
     assertEquals("wi fi network is fast", results.get(1).key);
     assertEquals(10, results.get(1).value);
+    analyzer.close();
   }
 
   public void testInputPathRequired() throws Exception {
@@ -388,6 +401,7 @@
     suggester.build(new InputArrayIterator(keys));
     List<LookupResult> results = suggester.lookup("ab x", false, 1);
     assertTrue(results.size() == 1);
+    analyzer.close();
   }
 
   private static Token token(String term, int posInc, int posLength) {
@@ -492,6 +506,7 @@
         }
       }
     }
+    a.close();
   }
 
   public void testNonExactFirst() throws Exception {
@@ -529,6 +544,7 @@
         }
       }
     }
+    a.close();
   }
   
   // Holds surface form separately:
@@ -867,6 +883,7 @@
         }
       }
     }
+    a.close();
   }
 
   public void testMaxSurfaceFormsPerAnalyzedForm() throws Exception {
@@ -881,6 +898,7 @@
     assertEquals(60, results.get(0).value);
     assertEquals("a ", results.get(1).key);
     assertEquals(50, results.get(1).value);
+    a.close();
   }
 
   public void testQueueExhaustion() throws Exception {
@@ -895,6 +913,7 @@
         }));
 
     suggester.lookup("a", false, 4);
+    a.close();
   }
 
   public void testExactFirstMissingResult() throws Exception {
@@ -941,6 +960,7 @@
     assertEquals(4, results.get(1).value);
     assertEquals("a b", results.get(2).key);
     assertEquals(3, results.get(2).value);
+    a.close();
   }
 
   public void testDupSurfaceFormsMissingResults() throws Exception {
@@ -999,6 +1019,7 @@
     assertEquals(6, results.get(0).value);
     assertEquals("nellie", results.get(1).key);
     assertEquals(5, results.get(1).value);
+    a.close();
   }
 
   public void testDupSurfaceFormsMissingResults2() throws Exception {
@@ -1068,6 +1089,7 @@
     assertEquals(6, results.get(0).value);
     assertEquals("b", results.get(1).key);
     assertEquals(5, results.get(1).value);
+    a.close();
   }
 
   public void test0ByteKeys() throws Exception {
@@ -1113,6 +1135,8 @@
           new Input("a a", 50),
           new Input("a b", 50),
         }));
+    
+    a.close();
   }
 
   public void testDupSurfaceFormsMissingResults3() throws Exception {
@@ -1126,6 +1150,7 @@
           new Input("a b", 5),
         }));
     assertEquals("[a a/7, a c/6, a b/5]", suggester.lookup("a", false, 3).toString());
+    a.close();
   }
 
   public void testEndingSpace() throws Exception {
@@ -1137,6 +1162,7 @@
         }));
     assertEquals("[isla de muerta/8, i love lucy/7]", suggester.lookup("i", false, 3).toString());
     assertEquals("[i love lucy/7]", suggester.lookup("i ", false, 3).toString());
+    a.close();
   }
 
   public void testTooManyExpansions() throws Exception {
@@ -1166,6 +1192,7 @@
     AnalyzingSuggester suggester = new AnalyzingSuggester(a, a, 0, 256, 1, true);
     suggester.build(new InputArrayIterator(new Input[] {new Input("a", 1)}));
     assertEquals("[a/1]", suggester.lookup("a", false, 1).toString());
+    a.close();
   }
   
   public void testIllegalLookupArgument() throws Exception {
@@ -1186,6 +1213,7 @@
     } catch (IllegalArgumentException e) {
       // expected
     }
+    a.close();
   }
 
   static final Iterable<Input> shuffle(Input...values) {
@@ -1209,5 +1237,6 @@
     } catch (IllegalArgumentException iae) {
       // expected
     }
+    a.close();
   }
 }
Index: lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/FuzzySuggesterTest.java
===================================================================
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/FuzzySuggesterTest.java	(revision 1663853)
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/FuzzySuggesterTest.java	(working copy)
@@ -72,6 +72,7 @@
       assertEquals("foo bar boo far", results.get(0).key.toString());
       assertEquals(12, results.get(0).value, 0.01F);  
     }
+    analyzer.close();
   }
   
   public void testNonLatinRandomEdits() throws IOException {
@@ -93,6 +94,7 @@
       assertEquals("фуу бар буу фар", results.get(0).key.toString());
       assertEquals(12, results.get(0).value, 0.01F);
     }
+    analyzer.close();
   }
 
   /** this is basically the WFST test ported to KeywordAnalyzer. so it acts the same */
@@ -104,7 +106,8 @@
         new Input("barbara", 6)
     };
     
-    FuzzySuggester suggester = new FuzzySuggester(new MockAnalyzer(random(), MockTokenizer.KEYWORD, false));
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);
+    FuzzySuggester suggester = new FuzzySuggester(analyzer);
     suggester.build(new InputArrayIterator(keys));
     
     List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence("bariar", random()), false, 2);
@@ -166,6 +169,8 @@
     assertEquals(10, results.get(1).value, 0.01F);
     assertEquals("barbara", results.get(2).key.toString());
     assertEquals(6, results.get(2).value, 0.01F);
+    
+    analyzer.close();
   }
   
   /**
@@ -197,6 +202,8 @@
     assertEquals(1, results.size());
     assertEquals("the ghost of christmas past", results.get(0).key.toString());
     assertEquals(50, results.get(0).value, 0.01F);
+    
+    standard.close();
   }
 
   public void testNoSeps() throws Exception {
@@ -221,6 +228,7 @@
     // complete to "abcd", which has higher weight so should
     // appear first:
     assertEquals("abcd", r.get(0).key.toString());
+    a.close();
   }
 
   public void testGraphDups() throws Exception {
@@ -286,14 +294,17 @@
     assertEquals(50, results.get(0).value);
     assertEquals("wi fi network is fast", results.get(1).key);
     assertEquals(10, results.get(1).value);
+    analyzer.close();
   }
 
   public void testEmpty() throws Exception {
-    FuzzySuggester suggester = new FuzzySuggester(new MockAnalyzer(random(), MockTokenizer.KEYWORD, false));
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);
+    FuzzySuggester suggester = new FuzzySuggester(analyzer);
     suggester.build(new InputArrayIterator(new Input[0]));
 
     List<LookupResult> result = suggester.lookup("a", false, 20);
     assertTrue(result.isEmpty());
+    analyzer.close();
   }
 
   public void testInputPathRequired() throws Exception {
@@ -352,6 +363,7 @@
     suggester.build(new InputArrayIterator(keys));
     List<LookupResult> results = suggester.lookup("ab x", false, 1);
     assertTrue(results.size() == 1);
+    analyzer.close();
   }
 
   private static Token token(String term, int posInc, int posLength) {
@@ -451,6 +463,7 @@
         }
       }
     }
+    a.close();
   }
 
   public void testNonExactFirst() throws Exception {
@@ -488,6 +501,7 @@
         }
       }
     }
+    a.close();
   }
   
   // Holds surface form separately:
@@ -820,6 +834,7 @@
         assertEquals(matches.get(hit).value, r.get(hit).value, 0f);
       }
     }
+    a.close();
   }
 
   public void testMaxSurfaceFormsPerAnalyzedForm() throws Exception {
@@ -841,6 +856,7 @@
     assertEquals(60, results.get(0).value);
     assertEquals("a ", results.get(1).key);
     assertEquals(50, results.get(1).value);
+    a.close();
   }
 
   public void testEditSeps() throws Exception {
@@ -861,6 +877,7 @@
     assertEquals("[foo bar baz/50]", suggester.lookup("foobarbaz", false, 5).toString());
     assertEquals("[barbaz/60, barbazfoo/10]", suggester.lookup("bar baz", false, 5).toString());
     assertEquals("[barbazfoo/10]", suggester.lookup("bar baz foo", false, 5).toString());
+    a.close();
   }
   
   @SuppressWarnings("fallthrough")
@@ -1003,6 +1020,7 @@
       }
       assertEquals(expected.size(), actual.size());
     }
+    a.close();
   }
 
   private List<LookupResult> slowFuzzyMatch(int prefixLen, int maxEdits, boolean allowTransposition, List<Input> answers, String frag) {
Index: lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester.java
===================================================================
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester.java	(revision 1663853)
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestFreeTextSuggester.java	(working copy)
@@ -95,6 +95,7 @@
       is.close();
       assertEquals(2, sug.getCount());
     }
+    a.close();
   }
 
   public void testIllegalByteDuringBuild() throws Exception {
@@ -103,7 +104,8 @@
     Iterable<Input> keys = AnalyzingSuggesterTest.shuffle(
         new Input("foo\u001ebar baz", 50)
     );
-    FreeTextSuggester sug = new FreeTextSuggester(new MockAnalyzer(random()));
+    Analyzer analyzer = new MockAnalyzer(random());
+    FreeTextSuggester sug = new FreeTextSuggester(analyzer);
     try {
       sug.build(new InputArrayIterator(keys));
       fail("did not hit expected exception");
@@ -110,6 +112,7 @@
     } catch (IllegalArgumentException iae) {
       // expected
     }
+    analyzer.close();
   }
 
   public void testIllegalByteDuringQuery() throws Exception {
@@ -118,7 +121,8 @@
     Iterable<Input> keys = AnalyzingSuggesterTest.shuffle(
         new Input("foo bar baz", 50)
     );
-    FreeTextSuggester sug = new FreeTextSuggester(new MockAnalyzer(random()));
+    Analyzer analyzer = new MockAnalyzer(random());
+    FreeTextSuggester sug = new FreeTextSuggester(analyzer);
     sug.build(new InputArrayIterator(keys));
 
     try {
@@ -127,6 +131,7 @@
     } catch (IllegalArgumentException iae) {
       // expected
     }
+    analyzer.close();
   }
 
   @Ignore
@@ -134,7 +139,8 @@
     final LineFileDocs lfd = new LineFileDocs(null, "/lucenedata/enwiki/enwiki-20120502-lines-1k.txt", false);
     // Skip header:
     lfd.nextDoc();
-    FreeTextSuggester sug = new FreeTextSuggester(new MockAnalyzer(random()));
+    Analyzer analyzer = new MockAnalyzer(random());
+    FreeTextSuggester sug = new FreeTextSuggester(analyzer);
     sug.build(new InputIterator() {
 
         private int count;
@@ -190,6 +196,7 @@
         System.out.println("  " + result);
       }
     }
+    analyzer.close();
   }
 
   // Make sure you can suggest based only on unigram model:
@@ -204,6 +211,7 @@
     // Sorts first by count, descending, second by term, ascending
     assertEquals("bar/0.22 baz/0.11 bee/0.11 blah/0.11 boo/0.11",
                  toString(sug.lookup("b", 10)));
+    a.close();
   }
 
   // Make sure the last token is not duplicated
@@ -216,6 +224,7 @@
     sug.build(new InputArrayIterator(keys));
     assertEquals("foo bar/1.00",
                  toString(sug.lookup("foo b", 10)));
+    a.close();
   }
 
   // Lookup of just empty string produces unicode only matches:
@@ -232,6 +241,7 @@
     } catch (IllegalArgumentException iae) {
       // expected
     }
+    a.close();
   }
 
   // With one ending hole, ShingleFilter produces "of _" and
@@ -259,6 +269,7 @@
     // prop 0.5:
     assertEquals("oz/0.20",
                  toString(sug.lookup("wizard o", 10)));
+    a.close();
   }
 
   // If the number of ending holes exceeds the ngrams window
@@ -282,6 +293,7 @@
     sug.build(new InputArrayIterator(keys));
     assertEquals("",
                  toString(sug.lookup("wizard of of", 10)));
+    a.close();
   }
 
   private static Comparator<LookupResult> byScoreThenKey = new Comparator<LookupResult>() {
@@ -578,6 +590,7 @@
 
       assertEquals(expected.toString(), actual.toString());
     }
+    a.close();
   }
 
   private static String getZipfToken(String[] tokens) {
Index: lucene/test-framework/src/java/org/apache/lucene/index/RandomIndexWriter.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/index/RandomIndexWriter.java	(revision 1663853)
+++ lucene/test-framework/src/java/org/apache/lucene/index/RandomIndexWriter.java	(working copy)
@@ -29,6 +29,7 @@
 import org.apache.lucene.search.Query;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.InfoStream;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.NullInfoStream;
@@ -49,6 +50,7 @@
   private double flushAtFactor = 1.0;
   private boolean getReaderCalled;
   private final Codec codec; // sugar
+  private final Analyzer analyzer; // only if WE created it (then we close it)
 
   /** Returns an indexwriter that randomly mixes up thread scheduling (by yielding at test points) */
   public static IndexWriter mockIndexWriter(Directory dir, IndexWriterConfig conf, Random r) throws IOException {
@@ -73,7 +75,7 @@
 
   /** create a RandomIndexWriter with a random config: Uses MockAnalyzer */
   public RandomIndexWriter(Random r, Directory dir) throws IOException {
-    this(r, dir, LuceneTestCase.newIndexWriterConfig(r, new MockAnalyzer(r)));
+    this(r, dir, LuceneTestCase.newIndexWriterConfig(r, new MockAnalyzer(r)), true);
   }
   
   /** create a RandomIndexWriter with a random config */
@@ -83,10 +85,19 @@
   
   /** create a RandomIndexWriter with the provided config */
   public RandomIndexWriter(Random r, Directory dir, IndexWriterConfig c) throws IOException {
+    this(r, dir, c, false);
+  }
+      
+  private RandomIndexWriter(Random r, Directory dir, IndexWriterConfig c, boolean closeAnalyzer) throws IOException {
     // TODO: this should be solved in a different way; Random should not be shared (!).
     this.r = new Random(r.nextLong());
     w = mockIndexWriter(dir, c, r);
     flushAt = TestUtil.nextInt(r, 10, 1000);
+    if (closeAnalyzer) {
+      analyzer = w.getAnalyzer();
+    } else {
+      analyzer = null;
+    }
     codec = w.getConfig().getCodec();
     if (LuceneTestCase.VERBOSE) {
       System.out.println("RIW dir=" + dir + " config=" + w.getConfig());
@@ -361,7 +372,7 @@
         w.commit();
       }
     }
-    w.close();
+    IOUtils.close(w, analyzer);
   }
 
   /**
Index: lucene/test-framework/src/java/org/apache/lucene/search/BaseExplanationTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/search/BaseExplanationTestCase.java	(revision 1663853)
+++ lucene/test-framework/src/java/org/apache/lucene/search/BaseExplanationTestCase.java	(working copy)
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -51,6 +52,7 @@
   protected static IndexSearcher searcher;
   protected static IndexReader reader;
   protected static Directory directory;
+  protected static Analyzer analyzer;
   
   public static final String KEY = "KEY";
   // boost on this field is the same as the iterator for the doc
@@ -65,12 +67,15 @@
     reader = null;
     directory.close();
     directory = null;
+    analyzer.close();
+    analyzer = null;
   }
   
   @BeforeClass
   public static void beforeClassTestExplanations() throws Exception {
     directory = newDirectory();
-    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
+    analyzer = new MockAnalyzer(random());
+    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(analyzer).setMergePolicy(newLogMergePolicy()));
     for (int i = 0; i < docFields.length; i++) {
       Document doc = new Document();
       doc.add(newStringField(KEY, ""+i, Field.Store.NO));
Index: lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java	(revision 1663853)
+++ lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java	(working copy)
@@ -73,10 +73,8 @@
 
   @Override
   public synchronized void close() throws IOException {
-    if (reader != null) {
-      reader.close();
-      reader = null;
-    }
+    IOUtils.close(reader, threadDocs);
+    reader = null;
   }
   
   private long randomSeekPos(Random random, long size) {
@@ -205,7 +203,7 @@
     }
   }
 
-  private final ThreadLocal<DocState> threadDocs = new ThreadLocal<>();
+  private final CloseableThreadLocal<DocState> threadDocs = new CloseableThreadLocal<>();
 
   /** Note: Document instance is re-used per-thread */
   public Document nextDoc() throws IOException {
Index: lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java	(revision 1663853)
+++ lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java	(working copy)
@@ -563,6 +563,7 @@
         return super.accept(field);
       }
     })
+    .around(new TestRuleThreadLocals(suiteFailureMarker))
     .around(new NoClassHooksShadowingRule())
     .around(new NoInstanceHooksOverridesRule() {
       @Override
Index: lucene/test-framework/src/java/org/apache/lucene/util/TestRuleThreadLocals.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/TestRuleThreadLocals.java	(revision 0)
+++ lucene/test-framework/src/java/org/apache/lucene/util/TestRuleThreadLocals.java	(working copy)
@@ -0,0 +1,258 @@
+package org.apache.lucene.util;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.lang.ref.Reference;
+import java.lang.reflect.Field;
+import java.lang.reflect.Method;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
+import com.carrotsearch.randomizedtesting.rules.TestRuleAdapter;
+
+public class TestRuleThreadLocals extends TestRuleAdapter {
+  private final TestRuleMarkFailure failureMarker;
+  
+  public TestRuleThreadLocals(TestRuleMarkFailure failureMarker) {
+    this.failureMarker = failureMarker;
+  }
+  
+  @Override
+  protected void afterAlways(List<Throwable> errors) throws Throwable {
+    if (failureMarker.wasSuccessful()) {
+      checkThreadLocalsForLeaks();
+    }
+  }
+  
+  // below is hacked up version of tomcat threadlocal leak detection code:
+  // http://svn.apache.org/viewvc/tomcat/trunk/java/org/apache/catalina/loader/WebappClassLoaderBase.java
+  
+  /*
+   * Get the set of current threads as an array.
+   */
+  // TODO: don't we have this somewhere????
+  private Thread[] getThreads() {
+    // Get the current thread group
+    ThreadGroup tg = Thread.currentThread().getThreadGroup();
+    // Find the root thread group
+    
+    // don't catch securityexception, if you are using this rule, you want it to work
+    while (tg.getParent() != null) {
+      tg = tg.getParent();
+    }
+    
+    int threadCountGuess = tg.activeCount() + 50;
+    Thread[] threads = new Thread[threadCountGuess];
+    int threadCountActual = tg.enumerate(threads);
+    // Make sure we don't miss any threads
+    while (threadCountActual == threadCountGuess) {
+      threadCountGuess *=2;
+      threads = new Thread[threadCountGuess];
+      // Note tg.enumerate(Thread[]) silently ignores any threads that
+      // can't fit into the array
+      threadCountActual = tg.enumerate(threads);
+    }
+    
+    return threads;
+  }
+  
+  private void checkThreadLocalsForLeaks() throws Exception {
+    Thread[] threads = getThreads();
+    
+    // Make the fields in the Thread class that store ThreadLocals
+    // accessible
+    Field threadLocalsField = Thread.class.getDeclaredField("threadLocals");
+    threadLocalsField.setAccessible(true);
+    Field inheritableThreadLocalsField = Thread.class.getDeclaredField("inheritableThreadLocals");
+    inheritableThreadLocalsField.setAccessible(true);
+    // Make the underlying array of ThreadLoad.ThreadLocalMap.Entry objects
+    // accessible
+    Class<?> tlmClass = Class.forName("java.lang.ThreadLocal$ThreadLocalMap");
+    Field tableField = tlmClass.getDeclaredField("table");
+    tableField.setAccessible(true);
+    Method expungeStaleEntriesMethod = tlmClass.getDeclaredMethod("expungeStaleEntries");
+    expungeStaleEntriesMethod.setAccessible(true);
+    
+    List<String> leaks = new ArrayList<>();
+    for (int i = 0; i < threads.length; i++) {
+      Object threadLocalMap;
+      if (threads[i] != null) {
+        
+        // Clear the first map
+        threadLocalMap = threadLocalsField.get(threads[i]);
+        if (null != threadLocalMap){
+          expungeStaleEntriesMethod.invoke(threadLocalMap);
+          List<String> found = checkThreadLocalMapForLeaks(threadLocalMap, tableField);
+          for (String l : found) {
+            leaks.add("thread=" + threads[i].getName() + " " + l);
+          }
+        }
+        
+        // Clear the second map
+        threadLocalMap = inheritableThreadLocalsField.get(threads[i]);
+        if (null != threadLocalMap){
+          expungeStaleEntriesMethod.invoke(threadLocalMap);
+          List<String> found = checkThreadLocalMapForLeaks(threadLocalMap, tableField);
+          for (String l : found) {
+            leaks.add("thread=" + threads[i].getName() + " " + l);
+          }
+        }
+      }
+    }
+    if (!leaks.isEmpty()) {
+      StringBuilder msg = new StringBuilder();
+      for (int i = 0; i < leaks.size(); i++) {
+        msg.append((i+1) + ". ");
+        msg.append(leaks.get(i));
+        msg.append(System.lineSeparator());
+      }
+      throw new IllegalStateException("ThreadLocal leaks were found: " + System.lineSeparator() + msg);
+    }
+  }
+  
+  /**
+   * Analyzes the given thread local map object. Also pass in the field that
+   * points to the internal table to save re-calculating it on every
+   * call to this method.
+   */
+  private List<String> checkThreadLocalMapForLeaks(Object map, Field internalTableField) throws Exception {
+    List<String> leaks = new ArrayList<>();
+    if (map != null) {
+      Object[] table = (Object[]) internalTableField.get(map);
+      if (table != null) {
+        for (int j = 0; j < table.length; j++) {
+          Object obj = table[j];
+          if (obj != null) {
+            // Check the key
+            Object key = ((Reference<?>) obj).get();
+            try {
+              check(key);
+            } catch (ThreadLocalLeakException e) {
+              leaks.add("key=" + e.clazz);
+            }
+            
+            // Check the value
+            Field valueField = obj.getClass().getDeclaredField("value");
+            valueField.setAccessible(true);
+            Object value = valueField.get(obj);
+            try {
+              check(value);
+            } catch (ThreadLocalLeakException e) {
+              leaks.add("value=" + e.clazz);
+            }
+          }
+        }
+      }
+    }
+    return leaks;
+  }
+  
+  private String getPrettyClassName(Class<?> clazz) {
+    String fullName = clazz.getName();
+
+    if (clazz.getEnclosingClass() != null) {
+      int index = fullName.lastIndexOf(".");
+      if (index >= 0 && index + 1 < fullName.length()) {
+        return fullName.substring(index + 1);
+      }
+    }
+    
+    if (clazz.getSimpleName() != null) {
+      return clazz.getSimpleName();
+    } else {
+      return fullName;
+    }
+  }
+  
+  private void check(Object o) {
+    if (o == null) {
+      return;
+    }
+    
+    Class<?> clazz;
+    if (o instanceof Class) {
+      clazz = (Class<?>) o;
+    } else {
+      clazz = o.getClass();
+    }
+    
+    if (clazz.getName().startsWith("com.carrotsearch.ant.tasks.junit4.dependencies.com.google.gson.internal.")) {
+      return; // no need to be alarmed, just the test runner
+    }
+    
+    ClassLoader cl = clazz.getClassLoader();
+    // our crazy heuristic: threadlocal contains something from non-bootstrap loader.
+    if (cl != null) {
+      throw new ThreadLocalLeakException(getPrettyClassName(clazz));
+    }
+    
+    if (o instanceof Reference<?>) {
+      try {
+        check(((Reference<?>)o).get());
+      } catch (ThreadLocalLeakException leak) {
+        throw new ThreadLocalLeakException(getPrettyClassName(clazz) + "<" + leak.clazz + ">");
+      }
+    }
+    
+    // TODO: add arrays!
+    
+    if (o instanceof Collection<?>) {
+      Iterator<?> iter = ((Collection<?>) o).iterator();
+      while (iter.hasNext()) {
+        Object entry = iter.next();
+        try {
+          check(entry);
+        } catch (ThreadLocalLeakException leak) {
+          throw new ThreadLocalLeakException(getPrettyClassName(clazz) + "<" + leak.clazz + ">");
+        }
+      }
+    }
+    
+    if (o instanceof Map<?,?>) {
+      Map<?,?> map = (Map<?,?>) o;
+      try {
+        for (Object key : map.keySet()) {
+          check(key);
+        }
+      } catch (ThreadLocalLeakException leak) {
+        throw new ThreadLocalLeakException(getPrettyClassName(clazz) + "<" + leak.clazz + ",?>");
+      }
+      try {
+        for (Object value : map.values()) {
+          check(value);
+        }
+      } catch (ThreadLocalLeakException leak) {
+        throw new ThreadLocalLeakException(getPrettyClassName(clazz) + "<?," + leak.clazz + ">");
+      }
+    }
+  }
+  
+  // we nest into structures like maps/lists/references looking for interesting stuff.
+  // we bubble exception back up to make a nice tostring
+  private static class ThreadLocalLeakException extends IllegalStateException {
+    final String clazz;
+    
+    ThreadLocalLeakException(String clazz) {
+      super("ThreadLocal leak: " + clazz);
+      this.clazz = clazz;
+    }
+  }
+}

Property changes on: lucene/test-framework/src/java/org/apache/lucene/util/TestRuleThreadLocals.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
