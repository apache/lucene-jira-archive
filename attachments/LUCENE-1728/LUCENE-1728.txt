Index: build.xml
===================================================================
--- build.xml	(revision 796589)
+++ build.xml	(working copy)
@@ -306,7 +306,8 @@
           <!-- make sure the group list below is updated.            -->
           <!-- Also remember to keep site.xml in sync.            -->		  
 
-          <packageset dir="contrib/analyzers/src/java"/>
+          <packageset dir="contrib/analyzers/common/src/java"/>
+          <packageset dir="contrib/analyzers/smartcn/src/java"/>
           <packageset dir="contrib/ant/src/java"/>
           <packageset dir="contrib/benchmark/src/java"/>
           <packageset dir="contrib/collation/src/java"/>
Index: contrib/analyzers/build.xml
===================================================================
--- contrib/analyzers/build.xml	(revision 796589)
+++ contrib/analyzers/build.xml	(working copy)
@@ -21,10 +21,52 @@
 
   <description>
     Additional Analyzers
+      - common:	Additional Analyzers
+      - smartcn:	Smart Analyzer for Simplified Chinese Text
   </description>
 
-  <property name="javac.source" value="1.4" />
-  <property name="javac.target" value="1.4" />
+  <target name="common">
+    <ant dir="common" />
+  </target>
 
-  <import file="../contrib-build.xml"/>
+  <target name="smartcn">
+    <ant dir="smartcn" />
+  </target>
+
+  <target name="default" depends="common,smartcn" />
+
+  <target name="clean">
+    <ant dir="common" target="clean" />
+    <ant dir="smartcn" target="clean" />
+  </target>
+  <target name="compile-core">
+    <ant dir="common" target="compile-core" />
+    <ant dir="smartcn" target="compile-core" />
+  </target>
+  <target name="compile-test">
+    <ant dir="common" target="compile-test" />
+    <ant dir="smartcn" target="compile-test" />
+  </target>
+  <target name="test">
+    <ant dir="common" target="test" />
+    <ant dir="smartcn" target="test" />
+  </target>
+
+  <target name="build-artifacts-and-tests" depends="default,compile-test" />
+
+  <target name="dist-maven" depends="default">
+    <ant dir="common" target="dist-maven" />
+    <ant dir="smartcn" target="dist-maven" />
+  </target>  	
+
+  <target name="javadocs">
+    <ant dir="common" target="javadocs" />
+    <ant dir="smartcn" target="javadocs" />
+  </target>  	
+
+  <target name="javadocs-index.html">
+    <ant dir="common" target="javadocs-index.html" />
+    <ant dir="smartcn" target="javadocs-index.html" />
+  </target>
+	
 </project>
Index: contrib/analyzers/common/build.xml
===================================================================
--- contrib/analyzers/common/build.xml	(revision 0)
+++ contrib/analyzers/common/build.xml	(working copy)
@@ -25,6 +25,17 @@
 
   <property name="javac.source" value="1.4" />
   <property name="javac.target" value="1.4" />
+	
+  <property name="build.dir" location="../../../build/contrib/analyzers/common" />
+  <property name="dist.dir" location="../../../dist/contrib/analyzers/common" />
+  <property name="maven.dist.dir" location="../../../dist/maven" />
 
-  <import file="../contrib-build.xml"/>
+  <import file="../../contrib-build.xml"/>
+	
+  <path id="test.classpath">
+    <path refid="classpath"/>
+    <pathelement location="../../../build/classes/test/"/>
+    <path refid="junit-path"/>
+    <pathelement location="${build.dir}/classes/java"/>
+  </path>	
 </project>
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/cjk/package.html
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/cjk/package.html	(revision 796589)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/cjk/package.html	(working copy)
@@ -1,5 +1,24 @@
-<html><head></head>
+<html>
+<head>
+<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
+</head>
 <body>
-Analyzer for Chinese, Japanese and Korean.
+Analyzer for Chinese, Japanese, and Korean, which indexes bigrams (overlapping groups of two adjacent Han characters).
+<p>
+Three analyzers are provided for Chinese, each of which treats Chinese text in a different way.
+<ul>
+	<li>ChineseAnalyzer (in the analyzers/cn package): Index unigrams (individual Chinese characters) as a token.
+	<li>CJKAnalyzer (in this package): Index bigrams (overlapping groups of two adjacent Chinese characters) as tokens.
+	<li>SmartChineseAnalyzer (in the analyzers/smartcn package): Index words (attempt to segment Chinese text into words) as tokens.
+</ul>
+
+Example phrase： "我是中国人"
+<ol>
+	<li>ChineseAnalyzer: 我－是－中－国－人</li>
+	<li>CJKAnalyzer: 我是－是中－中国－国人</li>
+	<li>SmartChineseAnalyzer: 我－是－中国－人</li>
+</ol>
+</p>
+
 </body>
-</html>
+</html>
\ No newline at end of file
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/package.html
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/package.html	(revision 796589)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/package.html	(working copy)
@@ -1,13 +1,15 @@
 <html>
-<head></head>
+<head>
+<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
+</head>
 <body>
-Analyzers for Chinese.
+Analyzer for Chinese, which indexes unigrams (individuals chinese characters).
 <p>
 Three analyzers are provided for Chinese, each of which treats Chinese text in a different way.
 <ul>
-	<li>ChineseAnalyzer: Index unigrams (individual Chinese characters) as a token.
-	<li>CJKAnalyzer: Index bigrams (overlapping groups of two adjacent Chinese characters) as tokens.
-	<li>SmartChineseAnalyzer: Index words (attempt to segment Chinese text into words) as tokens.
+	<li>ChineseAnalyzer (in this package): Index unigrams (individual Chinese characters) as a token.
+	<li>CJKAnalyzer (in the analyzers/cjk package): Index bigrams (overlapping groups of two adjacent Chinese characters) as tokens.
+	<li>SmartChineseAnalyzer (in the analyzers/smartcn package): Index words (attempt to segment Chinese text into words) as tokens.
 </ul>
 
 Example phrase： "我是中国人"
Index: contrib/analyzers/smartcn/build.xml
===================================================================
--- contrib/analyzers/smartcn/build.xml	(revision 0)
+++ contrib/analyzers/smartcn/build.xml	(revision 0)
@@ -0,0 +1,41 @@
+<?xml version="1.0"?>
+
+<!--
+    Licensed to the Apache Software Foundation (ASF) under one or more
+    contributor license agreements.  See the NOTICE file distributed with
+    this work for additional information regarding copyright ownership.
+    The ASF licenses this file to You under the Apache License, Version 2.0
+    the "License"); you may not use this file except in compliance with
+    the License.  You may obtain a copy of the License at
+ 
+        http://www.apache.org/licenses/LICENSE-2.0
+ 
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an "AS IS" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+ -->
+
+<project name="smartcn" default="default">
+
+  <description>
+    Smart Chinese Analyzer
+  </description>
+
+  <property name="javac.source" value="1.4" />
+  <property name="javac.target" value="1.4" />
+	
+  <property name="build.dir" location="../../../build/contrib/analyzers/smartcn" />
+  <property name="dist.dir" location="../../../dist/contrib/analyzers/smartcn" />
+  <property name="maven.dist.dir" location="../../../dist/maven" />
+
+  <import file="../../contrib-build.xml"/>
+	
+  <path id="test.classpath">
+    <path refid="classpath"/>
+    <pathelement location="../../../build/classes/test/"/>
+    <path refid="junit-path"/>
+    <pathelement location="${build.dir}/classes/java"/>
+  </path>	
+</project>
Index: contrib/analyzers/smartcn/pom.xml.template
===================================================================
--- contrib/analyzers/smartcn/pom.xml.template	(revision 0)
+++ contrib/analyzers/smartcn/pom.xml.template	(revision 0)
@@ -0,0 +1,35 @@
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+
+  <!--
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    "License"); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+    
+    http://www.apache.org/licenses/LICENSE-2.0
+    
+    Unless required by applicable law or agreed to in writing,
+    software distributed under the License is distributed on an
+    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+    KIND, either express or implied.  See the License for the
+    specific language governing permissions and limitations
+    under the License.
+  -->
+  <modelVersion>4.0.0</modelVersion>
+  <parent>
+    <groupId>org.apache.lucene</groupId>
+    <artifactId>lucene-contrib</artifactId>
+    <version>@version@</version>
+  </parent>
+  <groupId>org.apache.lucene</groupId>
+  <artifactId>lucene-smartcn</artifactId>
+  <name>Lucene Smart Chinese Analyzer</name>
+  <version>@version@</version>
+  <description>Smart Chinese Analyzer</description>
+  <packaging>jar</packaging>
+</project>
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/SmartChineseAnalyzer.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/SmartChineseAnalyzer.java	(revision 0)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/SmartChineseAnalyzer.java	(working copy)
@@ -17,24 +17,21 @@
 
 package org.apache.lucene.analysis.cn;
 
-import java.io.BufferedReader;
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.InputStreamReader;
 import java.io.Reader;
-import java.util.HashSet;
 import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.PorterStemFilter;
 import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.cn.smart.SentenceTokenizer;
-import org.apache.lucene.analysis.cn.smart.WordSegmenter;
-import org.apache.lucene.analysis.cn.smart.WordTokenizer;
+import org.apache.lucene.analysis.cn.smart.WordTokenFilter;
 
-import org.apache.lucene.analysis.cn.smart.AnalyzerProfile; // for javadoc
-
 /**
  * <p>
  * SmartChineseAnalyzer is an analyzer for Chinese or mixed Chinese-English text.
@@ -47,19 +44,35 @@
  * </p>
  * <p>
  * This analyzer requires a dictionary to provide statistical data. 
- * To specify the location of the dictionary data, refer to {@link AnalyzerProfile}
+ * SmartChineseAnalyzer has an included dictionary out-of-box.
  * </p>
  * <p>
  * The included dictionary data is from <a href="http://www.ictclas.org">ICTCLAS1.0</a>.
  * Thanks to ICTCLAS for their hard work, and for contributing the data under the Apache 2 License!
  * </p>
+ * <p>
+ * In special circumstances a user may wish to configure SmartChineseAnalyzer with a custom data directory location, containing bigramdict.dct and coredict.dct
+ * </p>
+ * The following order is used to determine the location of the data directory:
+ * 
+ * <ol>
+ * <li>System property： -Danalysis.data.dir=/path/to/analysis-data</li>
+ * <li>Relative path: analysis-data</li>
+ * <li>Relative path: lib/analysis-data</li>
+ * <li>Property file: analysis.data.dir property from relative path analysis.properties</li>
+ * <li>Property file: analysis.data.dir property from relative path lib/analysis.properties</li>
+ * </ol>
+ * 
+ * Example property file：
+ * 
+ * <pre>
+ * analysis.data.dir=D:/path/to/analysis-data/
+ * </pre>
  */
 public class SmartChineseAnalyzer extends Analyzer {
 
   private Set stopWords = null;
 
-  private WordSegmenter wordSegment;
-
   /**
    * Create a new SmartChineseAnalyzer, using the default stopword list.
    */
@@ -80,10 +93,15 @@
    */
   public SmartChineseAnalyzer(boolean useDefaultStopWords) {
     if (useDefaultStopWords) {
-      stopWords = loadStopWords(this.getClass().getResourceAsStream(
-          "stopwords.txt"));
+      try {
+      InputStream stream = this.getClass().getResourceAsStream("stopwords.txt");
+      InputStreamReader reader = new InputStreamReader(stream, "UTF-8");
+      stopWords = WordlistLoader.getWordSet(reader, "//");
+      } catch (IOException e) {
+        // TODO: throw IOException
+        throw new RuntimeException(e);
+      }
     }
-    wordSegment = new WordSegmenter();
   }
 
   /**
@@ -94,16 +112,14 @@
    * Note: the set should include punctuation, unless you want to index punctuation!
    * </p>
    * @param stopWords {@link Set} of stopwords to use.
-   * @see SmartChineseAnalyzer#loadStopWords(InputStream)
    */
   public SmartChineseAnalyzer(Set stopWords) {
     this.stopWords = stopWords;
-    wordSegment = new WordSegmenter();
   }
 
   public TokenStream tokenStream(String fieldName, Reader reader) {
     TokenStream result = new SentenceTokenizer(reader);
-    result = new WordTokenizer(result, wordSegment);
+    result = new WordTokenFilter(result);
     // result = new LowerCaseFilter(result);
     // LowerCaseFilter is not needed, as SegTokenFilter lowercases Basic Latin text.
     // The porter stemming is too strict, this is not a bug, this is a feature:)
@@ -113,37 +129,28 @@
     }
     return result;
   }
-
-  /**
-   * Utility function to return a {@link Set} of stopwords from a UTF-8 encoded {@link InputStream}.
-   * The comment "//" can be used in the stopword list.
-   * 
-   * @param input {@link InputStream} of UTF-8 encoded stopwords
-   * @return {@link Set} of stopwords.
-   */
-  public static Set loadStopWords(InputStream input) {
-    /*
-     * Note: WordListLoader is not used here because this method allows for inline "//" comments.
-     * WordListLoader will only filter out these comments if they are on a separate line.
-     */
-    String line;
-    Set stopWords = new HashSet();
-    try {
-      BufferedReader br = new BufferedReader(new InputStreamReader(input,
-          "UTF-8"));
-      while ((line = br.readLine()) != null) {
-        if (line.indexOf("//") != -1) {
-          line = line.substring(0, line.indexOf("//"));
-        }
-        line = line.trim();
-        if (line.length() != 0)
-          stopWords.add(line.toLowerCase());
+  
+  private static final class SavedStreams {
+    Tokenizer tokenStream;
+    TokenStream filteredTokenStream;
+  }
+  
+  public TokenStream reusableTokenStream(String fieldName, Reader reader)
+      throws IOException {
+    SavedStreams streams = (SavedStreams) getPreviousTokenStream();
+    if (streams == null) {
+      streams = new SavedStreams();
+      setPreviousTokenStream(streams);
+      streams.tokenStream = new SentenceTokenizer(reader);
+      streams.filteredTokenStream = new WordTokenFilter(streams.tokenStream);
+      streams.filteredTokenStream = new PorterStemFilter(streams.filteredTokenStream);
+      if (stopWords != null) {
+        streams.filteredTokenStream = new StopFilter(streams.filteredTokenStream, stopWords, false);
       }
-      br.close();
-    } catch (IOException e) {
-      System.err.println("WARNING: cannot open stop words list!");
+    } else {
+      streams.tokenStream.reset(reader);
     }
-    return stopWords;
+
+    return streams.filteredTokenStream;
   }
-
 }
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/package.html
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/package.html	(revision 0)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/package.html	(revision 0)
@@ -0,0 +1,24 @@
+<html>
+<head>
+<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
+</head>
+<body>
+Analyzer for Simplified Chinese, which indexes words.
+<p>
+Three analyzers are provided for Chinese, each of which treats Chinese text in a different way.
+<ul>
+	<li>ChineseAnalyzer (in the analyzers/cn package): Index unigrams (individual Chinese characters) as a token.
+	<li>CJKAnalyzer (in the analyzers/cjk package): Index bigrams (overlapping groups of two adjacent Chinese characters) as tokens.
+	<li>SmartChineseAnalyzer (in this package): Index words (attempt to segment Chinese text into words) as tokens.
+</ul>
+
+Example phrase： "我是中国人"
+<ol>
+	<li>ChineseAnalyzer: 我－是－中－国－人</li>
+	<li>CJKAnalyzer: 我是－是中－中国－国人</li>
+	<li>SmartChineseAnalyzer: 我－是－中国－人</li>
+</ol>
+</p>
+
+</body>
+</html>
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/AnalyzerProfile.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/AnalyzerProfile.java	(revision 796589)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/AnalyzerProfile.java	(working copy)
@@ -23,7 +23,7 @@
 import java.util.Properties;
 
 /**
- * Configure analysis data for SmartChineseAnalyzer
+ * Manages analysis data configuration for SmartChineseAnalyzer
  * <p>
  * SmartChineseAnalyzer has a built-in dictionary and stopword list out-of-box.
  * </p>
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java	(revision 796589)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java	(working copy)
@@ -17,7 +17,6 @@
 
 package org.apache.lucene.analysis.cn.smart;
 
-import java.io.BufferedReader;
 import java.io.IOException;
 import java.io.Reader;
 
@@ -25,35 +24,33 @@
 import org.apache.lucene.analysis.Tokenizer;
 
 /**
- * Tokenizes input into sentences.
+ * Tokenizes input text into sentences.
+ * <p>
+ * The output tokens can then be broken into words with {@link WordTokenFilter}
+ * </p>
  */
 public class SentenceTokenizer extends Tokenizer {
 
   /**
    * End of sentence punctuation: 。，！？；,!?;
    */
-  public final static String PUNCTION = "。，！？；,!?;";
+  private final static String PUNCTION = "。，！？；,!?;";
 
   private StringBuffer buffer = new StringBuffer();
 
-  private BufferedReader bufferInput;
-
   private int tokenStart = 0, tokenEnd = 0;
 
-  private Token t = new Token();
-
   public SentenceTokenizer(Reader reader) {
     super(reader);
-    bufferInput = new BufferedReader(reader, 2048);
   }
 
-  public Token next() throws IOException {
+  public Token next(final Token reusableToken) throws IOException {
     buffer.setLength(0);
     int ci;
     char ch, pch;
     boolean atBegin = true;
     tokenStart = tokenEnd;
-    ci = bufferInput.read();
+    ci = input.read();
     ch = (char) ci;
 
     while (true) {
@@ -67,14 +64,14 @@
       } else if (atBegin && Utility.SPACES.indexOf(ch) != -1) {
         tokenStart++;
         tokenEnd++;
-        ci = bufferInput.read();
+        ci = input.read();
         ch = (char) ci;
       } else {
         buffer.append(ch);
         atBegin = false;
         tokenEnd++;
         pch = ch;
-        ci = bufferInput.read();
+        ci = input.read();
         ch = (char) ci;
         // Two spaces, such as CR, LF
         if (Utility.SPACES.indexOf(ch) != -1
@@ -88,14 +85,10 @@
     if (buffer.length() == 0)
       return null;
     else {
-      t.clear();
-      t.reinit(buffer.toString(), input.correctOffset(tokenStart), input.correctOffset(tokenEnd), "sentence");
-      return t;
+      reusableToken.clear();
+      reusableToken.reinit(buffer.toString(), input.correctOffset(tokenStart), input.correctOffset(tokenEnd), "sentence");
+      return reusableToken;
     }
   }
 
-  public void close() throws IOException {
-    bufferInput.close();
-  }
-
 }
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/Utility.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/Utility.java	(revision 796589)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/Utility.java	(working copy)
@@ -17,7 +17,6 @@
 
 package org.apache.lucene.analysis.cn.smart;
 
-import org.apache.lucene.analysis.cn.smart.hhmm.BiSegGraph; // for javadoc
 import org.apache.lucene.analysis.cn.smart.hhmm.SegTokenFilter; // for javadoc
 
 /**
@@ -47,7 +46,7 @@
   public static final String SPACES = " 　\t\r\n";
 
   /**
-   * Maximum bigram frequency (used in the {@link BiSegGraph} smoothing function). 
+   * Maximum bigram frequency (used in the smoothing function). 
    */
   public static final int MAX_FREQUENCE = 2079997 + 80000;
 
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java	(revision 796589)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java	(working copy)
@@ -28,7 +28,7 @@
 /**
  * Segment a sentence of Chinese text into words.
  */
-public class WordSegmenter {
+class WordSegmenter {
 
   private HHMMSegmenter hhmmSegmenter = new HHMMSegmenter();
 
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordTokenFilter.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordTokenFilter.java	(revision 796589)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordTokenFilter.java	(working copy)
@@ -22,40 +22,36 @@
 import java.util.List;
 
 import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Tokenizer;
 
 /**
- * A {@link Tokenizer} that breaks sentences into words.
+ * A {@link TokenFilter} that breaks sentences into words.
  */
-public class WordTokenizer extends Tokenizer {
+public class WordTokenFilter extends TokenFilter {
 
   private WordSegmenter wordSegmenter;
 
-  private TokenStream in;
-
   private Iterator tokenIter;
 
   private List tokenBuffer;
 
-  private Token sentenceToken = new Token();
-
   /**
    * Construct a new WordTokenizer.
    * 
-   * @param in {@link TokenStream} of sentences
-   * @param wordSegmenter {@link WordSegmenter} to break sentences into words 
+   * @param in {@link TokenStream} of sentences 
    */
-  public WordTokenizer(TokenStream in, WordSegmenter wordSegmenter) {
-    this.in = in;
-    this.wordSegmenter = wordSegmenter;
+  public WordTokenFilter(TokenStream in) {
+    super(in);
+    this.wordSegmenter = new WordSegmenter();
   }
 
-  public Token next() throws IOException {
+  public Token next(final Token reusableSentenceToken) throws IOException {
     if (tokenIter != null && tokenIter.hasNext())
       return (Token) tokenIter.next();
     else {
-      if (processNextSentence()) {
+      Token nextToken = input.next(reusableSentenceToken);
+      if (processNextSentence(nextToken)) {
         return (Token) tokenIter.next();
       } else
         return null;
@@ -65,20 +61,15 @@
   /**
    * Process the next input sentence, placing tokens into tokenBuffer
    * 
+   * @param reusableSentenceToken input sentence
    * @return true if more tokens were placed into tokenBuffer.
    * @throws IOException
    */
-  private boolean processNextSentence() throws IOException {
-    sentenceToken = in.next(sentenceToken);
-    if (sentenceToken == null)
+  private boolean processNextSentence(final Token reusableSentenceToken) throws IOException {
+    if (reusableSentenceToken == null)
       return false;
-    tokenBuffer = wordSegmenter.segmentSentence(sentenceToken);
+    tokenBuffer = wordSegmenter.segmentSentence(reusableSentenceToken);
     tokenIter = tokenBuffer.iterator();
     return tokenBuffer != null && tokenIter.hasNext();
   }
-
-  public void close() throws IOException {
-    in.close();
-  }
-
 }
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/package.html
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/package.html	(revision 0)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/package.html	(revision 0)
@@ -0,0 +1,5 @@
+<html><head></head>
+<body>
+SmartChineseAnalyzer Tokenizers and TokenFilters
+</body>
+</html>
\ No newline at end of file
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java	(revision 796589)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java	(working copy)
@@ -27,7 +27,7 @@
  * Contains methods for dealing with GB2312 encoding.
  * </p>
  */
-public abstract class AbstractDictionary {
+abstract class AbstractDictionary {
   /**
    * First Chinese Character in GB2312 (15 * 94)
    * Characters in GB2312 are arranged in a grid of 94 * 94, 0-14 are unassigned or punctuation.
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph.java	(revision 796589)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph.java	(working copy)
@@ -32,7 +32,7 @@
  * For each start offset, a list of possible token pairs is stored.
  * </p>
  */
-public class BiSegGraph {
+class BiSegGraph {
 
   private Map tokenPairListTable = new HashMap();
 
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BigramDictionary.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BigramDictionary.java	(revision 796589)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BigramDictionary.java	(working copy)
@@ -35,7 +35,7 @@
 /**
  * SmartChineseAnalyzer Bigram dictionary.
  */
-public class BigramDictionary extends AbstractDictionary {
+class BigramDictionary extends AbstractDictionary {
 
   private BigramDictionary() {
   }
@@ -208,45 +208,6 @@
     // log.info("load dictionary done! " + dctFilePath + " total:" + total);
   }
 
-  /*
-   * public void test(String dctFilePath) throws IOException { int i, cnt,
-   * length, total = 0; int corrupt = 0, notFound = 0; //
-   * 文件中只统计了6763个汉字加5个空汉字符3756~3760，其中第3756个用来存储符号信息。 int[] buffer = new int[3];
-   * byte[] intBuffer = new byte[4]; String tmpword; RandomAccessFile dctFile =
-   * new RandomAccessFile(dctFilePath, "r");
-   * 
-   * // 字典文件中第一个汉字出现的位置是0，最后一个是6768 for (i = GB2312_FIRST_CHAR; i <
-   * GB2312_FIRST_CHAR + CHAR_NUM_IN_FILE; i++) { String currentStr =
-   * getCCByGB2312Id(i); // if (i == 5231) // System.out.println(i);
-   * 
-   * dctFile.read(intBuffer);// 原词库文件在c下开发，所以写入的文件为little // endian编码，而java为big
-   * endian，必须转换过来 cnt =
-   * ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN) .getInt(); if
-   * (cnt <= 0) { continue; } total += cnt; int j = 0; while (j < cnt) {
-   * dctFile.read(intBuffer); buffer[0] = ByteBuffer.wrap(intBuffer).order(
-   * ByteOrder.LITTLE_ENDIAN).getInt();// frequency dctFile.read(intBuffer);
-   * buffer[1] = ByteBuffer.wrap(intBuffer).order(
-   * ByteOrder.LITTLE_ENDIAN).getInt();// length dctFile.read(intBuffer); //
-   * buffer[2] = ByteBuffer.wrap(intBuffer).order( //
-   * ByteOrder.LITTLE_ENDIAN).getInt();// handle
-   * 
-   * length = buffer[1]; if (length > 0) { byte[] lchBuffer = new byte[length];
-   * dctFile.read(lchBuffer); tmpword = new String(lchBuffer, "GB2312"); if (i
-   * != 3755 + GB2312_FIRST_CHAR) { tmpword = currentStr + tmpword; } char
-   * carray[] = tmpword.toCharArray(); int index = getBigramItemIndex(carray);
-   * if (index != -1) { // if (!bigramStringTable[index].equals(tmpword)) { //
-   * System.out.println("corrupt: " + tmpword + "<->" // +
-   * bigramStringTable[index]); // corrupt++; // } } else {
-   * System.out.println("not found: " + tmpword); notFound++; } } j++; } }
-   * dctFile.close(); System.out.println("num not found:" + notFound);
-   * System.out.println("num corrupt:" + corrupt);
-   * 
-   * log.info("test dictionary done! " + dctFilePath + " total:" + total); cnt =
-   * 0; for (int j = 0; j < PRIME_BIGRAM_LENGTH; j++) { if (bigramHashTable[j]
-   * != 0) { cnt++; } } System.out.println("total num in bigramTable: " + cnt);
-   * }
-   */
-
   private int getAvaliableIndex(long hashId, char carray[]) {
     int hash1 = (int) (hashId % PRIME_BIGRAM_LENGTH);
     int hash2 = hash2(carray) % PRIME_BIGRAM_LENGTH;
@@ -307,13 +268,4 @@
     return 0;
   }
 
-  public static void main(String[] args) throws FileNotFoundException,
-      UnsupportedEncodingException, IOException {
-    BigramDictionary dic = new BigramDictionary();
-    dic.load("D:/analysis-data");
-    // dic.test("D:/analysis-data/BigramDict.dct");
-    System.out.println("max:" + dic.max);
-    System.out.println("average repeat:" + (double) dic.repeat / 328856);
-    System.out.println("end");
-  }
 }
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/PathNode.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/PathNode.java	(revision 796589)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/PathNode.java	(working copy)
@@ -23,7 +23,7 @@
  * Used by {@link BiSegGraph} to maximize the segmentation with the Viterbi algorithm.
  * </p>
  */
-public class PathNode implements Comparable {
+class PathNode implements Comparable {
   public double weight;
 
   public int preNode;
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java	(revision 796589)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java	(working copy)
@@ -29,7 +29,7 @@
  * For each start offset, a list of possible tokens is stored.
  * </p>
  */
-public class SegGraph {
+class SegGraph {
 
   /**
    * Map of start offsets to ArrayList of tokens at that position
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegToken.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegToken.java	(revision 796589)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegToken.java	(working copy)
@@ -17,30 +17,64 @@
 
 package org.apache.lucene.analysis.cn.smart.hhmm;
 
+import org.apache.lucene.analysis.cn.smart.WordType; // for javadocs
+
 /**
  * SmartChineseAnalyzer internal token
  */
 public class SegToken {
+  /**
+   * Character array containing token text
+   */
   public char[] charArray;
 
+  /**
+   * start offset into {@link #charArray}
+   */
   public int startOffset;
 
+  /**
+   * end offset into {@link #charArray}
+   */
   public int endOffset;
 
+  /**
+   * {@link WordType} of the text 
+   */
   public int wordType;
 
+  /**
+   * word frequency
+   */
   public int weight;
 
+  /**
+   * during segmentation, this is used to store the index of the token in the token list table
+   */
   public int index;
 
+  /**
+   * Create a new SegToken from a {@link String}
+   * 
+   * @param word String containing text
+   * @param start start offset into word
+   * @param end end offset of word
+   * @param wordType {@link WordType} of the text
+   * @param weight word frequency
+   */
   public SegToken(String word, int start, int end, int wordType, int weight) {
-    this.charArray = word.toCharArray();
-    this.startOffset = start;
-    this.endOffset = end;
-    this.wordType = wordType;
-    this.weight = weight;
+    this(word.toCharArray(), start, end, wordType, weight);
   }
 
+  /**
+   * Create a new SegToken from a character array.
+   * 
+   * @param idArray character array containing text
+   * @param start start offset into idArray
+   * @param end end offset of idArray
+   * @param wordType {@link WordType} of the text
+   * @param weight word frequency
+   */
   public SegToken(char[] idArray, int start, int end, int wordType, int weight) {
     this.charArray = idArray;
     this.startOffset = start;
@@ -49,13 +83,4 @@
     this.weight = weight;
   }
 
-  // public String toString() {
-  // return String.valueOf(charArray) + "/s(" + startOffset + ")e("
-  // + endOffset + ")/w(" + weight + ")t(" + wordType + ")";
-  // }
-
-  // public boolean equals(RawToken t) {
-  // return this.startOffset == t.startOffset
-  // && this.endOffset == t.endOffset;
-  // }
 }
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenPair.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenPair.java	(revision 796589)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenPair.java	(working copy)
@@ -20,7 +20,7 @@
 /**
  * A pair of tokens in {@link SegGraph}
  */
-public class SegTokenPair {
+class SegTokenPair {
 
   public char[] charArray;
 
@@ -43,13 +43,4 @@
     this.weight = weight;
   }
 
-  // public String toString() {
-  // return String.valueOf(charArray) + ":f(" + from + ")t(" + to + "):"
-  // + weight;
-  // }
-
-  // public boolean equals(SegTokenPair tp) {
-  // return this.from == tp.from && this.to == tp.to;
-  // }
-
 }
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/WordDictionary.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/WordDictionary.java	(revision 796589)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/WordDictionary.java	(working copy)
@@ -37,7 +37,7 @@
  * SmartChineseAnalyzer Word Dictionary
  *
  */
-public class WordDictionary extends AbstractDictionary {
+class WordDictionary extends AbstractDictionary {
 
   private WordDictionary() {
   }
@@ -550,18 +550,4 @@
         wordItem_charArrayTable[wordIndexTable[hashIndex]][itemIndex], 0) == 0;
   }
 
-  public static void main(String[] args) throws FileNotFoundException,
-      IOException {
-    WordDictionary dic = new WordDictionary();
-    dic.load("D:/analysis-data");
-    Utility.getCharType('。');
-    Utility.getCharType('汗');
-    Utility.getCharType(' ');// 0020
-    Utility.getCharType('　');// 3000
-    Utility.getCharType('');// E095
-    Utility.getCharType(' ');// 3000
-    Utility.getCharType('\r');// 000D
-    Utility.getCharType('\n');// 000A
-    Utility.getCharType('\t');// 0009
-  }
 }
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/package.html
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/package.html	(revision 0)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/package.html	(revision 0)
@@ -0,0 +1,5 @@
+<html><head></head>
+<body>
+SmartChineseAnalyzer Hidden Markov Model package
+</body>
+</html>
\ No newline at end of file
Index: contrib/analyzers/smartcn/src/resources/org/apache/lucene/analysis/cn/stopwords.txt
===================================================================
--- contrib/analyzers/smartcn/src/resources/org/apache/lucene/analysis/cn/stopwords.txt	(revision 796589)
+++ contrib/analyzers/smartcn/src/resources/org/apache/lucene/analysis/cn/stopwords.txt	(working copy)
@@ -51,7 +51,8 @@
 ［
 ］
 ●
-　//IDEOGRAPHIC SPACE character (Used as a space in Chinese)
+// the line below contains an IDEOGRAPHIC SPACE character (Used as a space in Chinese)
+　
 
 //////////////// English Stop Words ////////////////
 
Index: contrib/analyzers/smartcn/src/test/org/apache/lucene/analysis/cn/TestSmartChineseAnalyzer.java
===================================================================
--- contrib/analyzers/smartcn/src/test/org/apache/lucene/analysis/cn/TestSmartChineseAnalyzer.java	(revision 0)
+++ contrib/analyzers/smartcn/src/test/org/apache/lucene/analysis/cn/TestSmartChineseAnalyzer.java	(working copy)
@@ -40,6 +40,28 @@
   }
   
   /*
+   * This test is the same as the above, except with two phrases.
+   * This tests to ensure the SentenceTokenizer->WordTokenFilter chain works correctly.
+   */
+  public void testChineseStopWordsDefaultTwoPhrases() throws Exception {
+    Analyzer ca = new SmartChineseAnalyzer(); /* will load stopwords */
+    String sentence = "我购买了道具和服装。 我购买了道具和服装。";
+    String result[] = { "我", "购买", "了", "道具", "和", "服装", "我", "购买", "了", "道具", "和", "服装" };
+    assertAnalyzesTo(ca, sentence, result);
+  }
+  
+  /*
+   * This test is the same as the above, except using an ideographic space as a separator.
+   * This tests to ensure the stopwords are working correctly.
+   */
+  public void testChineseStopWordsDefaultTwoPhrasesIdeoSpache() throws Exception {
+    Analyzer ca = new SmartChineseAnalyzer(); /* will load stopwords */
+    String sentence = "我购买了道具和服装　我购买了道具和服装。";
+    String result[] = { "我", "购买", "了", "道具", "和", "服装", "我", "购买", "了", "道具", "和", "服装" };
+    assertAnalyzesTo(ca, sentence, result);
+  }
+  
+  /*
    * Punctuation is handled in a strange way if you disable stopwords
    * In this example the IDEOGRAPHIC FULL STOP is converted into a comma.
    * if you don't supply (true) to the constructor, or use a different stopwords list,
Index: docs/benchmarks.html
===================================================================
--- docs/benchmarks.html	(revision 796589)
+++ docs/benchmarks.html	(working copy)
@@ -127,6 +127,9 @@
 <a href="api/contrib-analyzers/index.html">Analyzers</a>
 </div>
 <div class="menuitem">
+<a href="api/contrib-smartcn/index.html">Smart Chinese Analyzer</a>
+</div>
+<div class="menuitem">
 <a href="api/contrib-ant/index.html">Ant</a>
 </div>
 <div class="menuitem">
Index: docs/contributions.html
===================================================================
--- docs/contributions.html	(revision 796589)
+++ docs/contributions.html	(working copy)
@@ -129,6 +129,9 @@
 <a href="api/contrib-analyzers/index.html">Analyzers</a>
 </div>
 <div class="menuitem">
+<a href="api/contrib-smartcn/index.html">Smart Chinese Analyzer</a>
+</div>
+<div class="menuitem">
 <a href="api/contrib-ant/index.html">Ant</a>
 </div>
 <div class="menuitem">
Index: docs/demo.html
===================================================================
--- docs/demo.html	(revision 796589)
+++ docs/demo.html	(working copy)
@@ -129,6 +129,9 @@
 <a href="api/contrib-analyzers/index.html">Analyzers</a>
 </div>
 <div class="menuitem">
+<a href="api/contrib-smartcn/index.html">Smart Chinese Analyzer</a>
+</div>
+<div class="menuitem">
 <a href="api/contrib-ant/index.html">Ant</a>
 </div>
 <div class="menuitem">
Index: docs/demo2.html
===================================================================
--- docs/demo2.html	(revision 796589)
+++ docs/demo2.html	(working copy)
@@ -129,6 +129,9 @@
 <a href="api/contrib-analyzers/index.html">Analyzers</a>
 </div>
 <div class="menuitem">
+<a href="api/contrib-smartcn/index.html">Smart Chinese Analyzer</a>
+</div>
+<div class="menuitem">
 <a href="api/contrib-ant/index.html">Ant</a>
 </div>
 <div class="menuitem">
Index: docs/demo3.html
===================================================================
--- docs/demo3.html	(revision 796589)
+++ docs/demo3.html	(working copy)
@@ -129,6 +129,9 @@
 <a href="api/contrib-analyzers/index.html">Analyzers</a>
 </div>
 <div class="menuitem">
+<a href="api/contrib-smartcn/index.html">Smart Chinese Analyzer</a>
+</div>
+<div class="menuitem">
 <a href="api/contrib-ant/index.html">Ant</a>
 </div>
 <div class="menuitem">
Index: docs/demo4.html
===================================================================
--- docs/demo4.html	(revision 796589)
+++ docs/demo4.html	(working copy)
@@ -129,6 +129,9 @@
 <a href="api/contrib-analyzers/index.html">Analyzers</a>
 </div>
 <div class="menuitem">
+<a href="api/contrib-smartcn/index.html">Smart Chinese Analyzer</a>
+</div>
+<div class="menuitem">
 <a href="api/contrib-ant/index.html">Ant</a>
 </div>
 <div class="menuitem">
Index: docs/fileformats.html
===================================================================
--- docs/fileformats.html	(revision 796589)
+++ docs/fileformats.html	(working copy)
@@ -129,6 +129,9 @@
 <a href="api/contrib-analyzers/index.html">Analyzers</a>
 </div>
 <div class="menuitem">
+<a href="api/contrib-smartcn/index.html">Smart Chinese Analyzer</a>
+</div>
+<div class="menuitem">
 <a href="api/contrib-ant/index.html">Ant</a>
 </div>
 <div class="menuitem">
Index: docs/gettingstarted.html
===================================================================
--- docs/gettingstarted.html	(revision 796589)
+++ docs/gettingstarted.html	(working copy)
@@ -129,6 +129,9 @@
 <a href="api/contrib-analyzers/index.html">Analyzers</a>
 </div>
 <div class="menuitem">
+<a href="api/contrib-smartcn/index.html">Smart Chinese Analyzer</a>
+</div>
+<div class="menuitem">
 <a href="api/contrib-ant/index.html">Ant</a>
 </div>
 <div class="menuitem">
Index: docs/index.html
===================================================================
--- docs/index.html	(revision 796589)
+++ docs/index.html	(working copy)
@@ -127,6 +127,9 @@
 <a href="api/contrib-analyzers/index.html">Analyzers</a>
 </div>
 <div class="menuitem">
+<a href="api/contrib-smartcn/index.html">Smart Chinese Analyzer</a>
+</div>
+<div class="menuitem">
 <a href="api/contrib-ant/index.html">Ant</a>
 </div>
 <div class="menuitem">
Index: docs/linkmap.html
===================================================================
--- docs/linkmap.html	(revision 796589)
+++ docs/linkmap.html	(working copy)
@@ -127,6 +127,9 @@
 <a href="api/contrib-analyzers/index.html">Analyzers</a>
 </div>
 <div class="menuitem">
+<a href="api/contrib-smartcn/index.html">Smart Chinese Analyzer</a>
+</div>
+<div class="menuitem">
 <a href="api/contrib-ant/index.html">Ant</a>
 </div>
 <div class="menuitem">
@@ -304,6 +307,12 @@
 		    
 <ul>
 <li>
+<a href="api/contrib-smartcn/index.html">Smart Chinese Analyzer</a>&nbsp;&nbsp;___________________&nbsp;&nbsp;<em>javadoc-contrib-smartcn</em>
+</li>
+</ul>
+		    
+<ul>
+<li>
 <a href="api/contrib-ant/index.html">Ant</a>&nbsp;&nbsp;___________________&nbsp;&nbsp;<em>javadoc-contrib-ant</em>
 </li>
 </ul>
Index: docs/linkmap.pdf
===================================================================
--- docs/linkmap.pdf	(revision 796589)
+++ docs/linkmap.pdf	(working copy)
@@ -5,10 +5,10 @@
 /Producer (FOP 0.20.5) >>
 endobj
 5 0 obj
-<< /Length 1101 /Filter [ /ASCII85Decode /FlateDecode ]
+<< /Length 1092 /Filter [ /ASCII85Decode /FlateDecode ]
  >>
 stream
-Gatn&?#Q2d'Sc)J/%BCKiqXP'CpI&FeS<eVm:Mn9$;Nr]+9a,upZlGo$(/q&F4[?"2X:0'[a24,ArFdC9B'@eTXS#uT`0frd!M-&&#p026J][2"G@$:@dSS.]1o+]pSQRRe=3f11k/\!S$$65h.OFq`Nji^(V(6jXNnL$hVq-f1CrHafs"u#%lgAEh-uXM>(YutBDq"lHYa"W7Ut+6bE_90EO"A26M>`*[,6c3Uus"M<g>5FZ.tWH*8$C-SV80p>4t%:fnmNhW2G>h#j?`Ef@sEg)Mu;apuh1n6'*/p@NGT&\1-*id11`<L7ef[BOFUJC'O^PLpq0-DNRVNg_GfF!l>M<7S+2a$3nFk#K,i1"mtnVPe0o-3)_Q[9N^2I3/ETW5n!f&fi>D;Rc1g&QWd!t>!+^3Vmo&6;)BD$rX?:hZHRa&Hp@rrj]eTrUij_f3:4S'^bmc;kHH>(WOW@0rB47f$]gEIAe0Br/tC%2e<'@rpLHiI4@Z3sdlM-P/CSri\,^IiRhjhe[G"YF9_>;M+h-UL736$D^I@CW"Ib:u^bco?:8?*GKTu\S4YLj?4HCX-MB=XsEB5.CZX]<2YHeW_-#:R4q<jI:Q.A$[/OpN4o[,[K.>!(kC-)7rZ*kLE,g_u51Qf_YF]:ObK+an;A]*PaZSNId<Pu8AcFh6(2k@EWSS6gAVTFoWmT`4`EJU]qEu@u`]./4BYq4$un0#TTWkJZY6L\L]YfbJ!']kgM6`l-;3um9Yd7ngBe9'78*#gW=L\q*^B.%0fT6R=un3?$"<*N0;8WgXh=C9tpT-8/g(l],;Hi0pkE0`P[_KKheM:g,mLG;83O8Bl=I[@8+RpK3(%1J"8jLV`9TgSpi*Z[1R'ap;h\L=oS#;K4[5>4eH,?X%aNlm29XhJU?R0n./Up#?H_iXD6D_Zu<WAU0ZM?i[p4Z:ieMjsQ1aKdP!2*NQ<cZMN[=-j3h^h4^j5p3te.ttI&I$r3#+g8lq6!MD8fVbfPa!8ShaI*hW_#22q?>Nq@]t8A$qAj1HCLIKoYHVC,b5.#j'_g@k`DXE<AZt:&f]GbR\:EGNe^[1EW4f6<>?%Wl&ddcVrWf_4&fC~>
+Gatn&?#Q2d'Sc)J/%BCKiqXP'9Y+5.eS1Hjm:Mn98t:X%5R'u\maobh&o:GNc!@)&fVmt]NrAh,ShDGZY"=-mJAG\PJbgmf^u03a!iS:2Lijf;&.fldMbW^>i\$GS177&T;K"1*Zf67oD#95]nrOgZb>nRi<>JIHZG$`r"H+4qbQ<H%AP8&;+lJHjEf5'@lOZb)\M0Y>rim\])m*ZJ/D$V7Uuc3X$Yf7_RFt2iV4f.DWEN?7<:24:NpOj'c=.>T*W&">lqF\jl="*@!d_r]G*e(Q,;IgVs#g6?#37-40ol?$g^`$"k_?lX6Vmn>['6G6[E60:e(mL<>9^DZn3nRdJ2kQWd'??kR/q<`_6Ee%5l7i'eNOY]]r;i_#L([GO$Uh-+9ms\lmCC5D0iqVV5=m[&l3AgF'Ptn'TGYuIKj&`4SKr!&.l_Gn/]t7d;d4:3:1QCi4:BgDqbrNe6#._qu^'T"<Je_b\_j7.D<hie<66i%qNj1n/.R"T$B&@A<f=Q_aT*2fTt`VXc)T-_aNr3'QW@TQk`3lnjc[9Ma3JG)VH;:_J.H!N3JHnFKUP69iB*Ii*WUm6IZ8#*5aHNQ\:+<%QXB`Lt$l^`uJ\nCUUSec]>VB4=d5::LZKrIQ@^SS;t+M4C@rs=>(=&ed=Y&HWZ:i=mZ;d-S)pqPBVGL#Qi;NXmc[,,;DNAA%.6VU.EnY/'m(FK]l.NdQ$4P7E//(I;tVN4Fbh*JuTi\LYNX_ZDP=X(oo^+\X(N`;B(n[Rn.*n]il7,2(\MWHX,b!=2(@dkipIc6-iY[V<FfR#&:_-29.JlDS,pEG'q=$s25<$`rB#f;O*n+O^e,%@!t;JFG@T!,r@rI,.IU&3Btj&;.2PpQG=%hD\^8/\_HkNebQ"enLo3.F7c^OL6.m"@Z41e\L=dF'l'-QU>5Pd1oNIh@VQ:A^ac05`eVOo=*%)RmX.Q.-=d-oJ0;WYW'mkYI%ALSoifTR"Pr`3_BEL1m2Dru2<HHG$".2BH<"ii._R2*Hh0C4W,H)M\T-G3)jfsF2G)0e?+H-5PKN+"/pG:?.>bk8\)pH42&bl9BWGW>[22K_Vu@M&p@k!~>
 endstream
 endobj
 6 0 obj
@@ -20,10 +20,10 @@
 >>
 endobj
 7 0 obj
-<< /Length 971 /Filter [ /ASCII85Decode /FlateDecode ]
+<< /Length 1023 /Filter [ /ASCII85Decode /FlateDecode ]
  >>
 stream
-GatUs?#SFN'Sc)P'u"K;nGElJjK_`pdaUkS5jG.fRS5Ds&JAhPC&e.%%j2(%O]&\ML:ltbk8YRj1`^r^1Ood"@(I0tYT.&$g_U&Z,<*[]LE&`j\cKQ`SdAi7@#-O)m\cNrH;dr5"rOB"5M)N3n/G'93Y-]139j#$5t["&n5m<'H'&N!,.Ani%8C?[W3@e;L6;l\"XkaDEL<^Vlci#"j5dqp-0O],dRB$>,U(U<ZDm+"MK'*Z[^ZB%H?=,33//j&WO"-Yl"$njDcdpq=Sar-XDOR^?kGaD!o=kpm.5N&'HiM/aS33=4dgDSEYL((aW\oq]ePY&>S,fN$F=p>@Z591TUsSS]%-COoj:Gpj-]TgQ^ii;9,FXU4/%g`.a1$,[XUl6=XniX/(11_(FPrnP5>"A_*t.0*r"C$2f^8P<bb+0Q%7bWfmTLnOf5/>8R>0ZD3GE98t+_I0al8=o*S<IF*j_SA@59H^$Ej83NJS&0!gj[(EElromK!b]hOEt5G2rtR.s.-YM@)f_q@/dHgFHrdA?V)]l_`]E6aKm9V7Ml>aUMV+*b31)+i_)i>dT1UMpGfNG5TtI*pNr)f%f&Z-$2)]-0uoh%%GQ:V4)iq1!="84=4DP)"I[o@C3B<'HohDnSF_LD4ge193Nl];'Ha/n;_tBst#Tm<`_NIooc8PXN5o^&@:9i@\*P_l'QR]q,/IPt&,AY'LkqULk;Y(;VaUgZcbYg/C48Em57#8/PX(A0dj6\2(rIXrj(-eL@'qf+A-5/d^!S]hr!PCoV*8s0#njL#iN=o2D?3Sdo),qa=[5@b5H"F8KDa+@m>')Cu)RK2Hmp_3^P5iDE04*'l%5A%PtArhMDGN1(-5S"5@;lPbC]1<#aa)7^(6>3`iS^705TZB_"4fDV*kI#\J!r#AF4qAe:-<RQmT`PN[\BoS(p;kPUaHe<YG\puV'jh5HKg@a#u4br4.qZgfZ;F(~>
+GatUs?#SFN'Sc)P'u"K;nGEmQa^n\98Q."=$W8LWdq"&]N0)Bt9)84&^*"bC/HrXFL:ltbk8YT@IQI<\@/pss^iV:t?oc((kq<6oH?tZ/%QX*qF9@!L]Fr)!_&-[:oM7DAH^2WdVahT(#6+<\\)7daIb,qEAC;B@QpF><LO\l#cEh1(ASI:IZgh78dAX:@8qT\=oIQBEe5C]'L."5dgsj)lb\I:s,5*1b,&X\Q?+#dU8p26OOL9dpOp(kS(<S>U%PsLuK/j7gSD<bHjP6;">biQo^5[4bMc^Go`OH->>4b'a%i6T`ed;bn.S6A"MUj7!$qg[>pngcX[HliSp9:rj_L8"gqfG"a"c+cYch-GKSkNCb>0U)Gi?:s_=&lpVVEka!G&T/[C+ZCIGTC_Q4I3mO;nsLuVTC.IiU#Q3omCL+WTPh)VlCS?;Iu0jVF?QQol+nkcWX4"59=r8c5l3&aIOSYWTgZ_!reG[mYV'6U7N;#,c<jb8_)%Fj)&:<SKm83+LUD+]h5'M7cs$Q]pEb?1O!,:*Ea1![V,8#4lq)(BBaA#Ta*8+N?V<S_+4ZT2G`b4#*Vo+.DCm^DAls0fh%^4F4*ghHm=P"?EMR)k*q)5m4_kR_Ceg-X:=*W.Z/O^D.pgRk(O`/&?,*k%`V5h@UE[AkIn8HV;,6"O8DJESRi7A3hKu9%t`<oOCE*2-C)Vrf1NYoZ*78'ZW3Q0O(fbm_s6N\`%qkgm:Y#V4&AKkm&-mT^Ra?.Q$TeuXr8(f<n#$ao>Oa"ANRT.WX\Vg-.b7FJ.K?:PDi9c&:/uG,cDdkj6@=SK^K+t':jer]&(N10fG<'`;E]F0Z%i4<U$JZ^J7:;e]fWn&[Ca^K'?lG"pbhGEWm5RVpTp<Q,ArU9]K!BK#!#Q#hjI22#U!YRM[n`5s7<gZ-J%7lOjXd"2:Z\N,,DJCjXCdjf59Or,UV#$,bq^+N2URFJ[:!h,[#$36W/W60=`#8U1Z-j_bSjO&[!X-E+S$SQP.5qQAuM]QX;UJKUbY~>
 endstream
 endobj
 8 0 obj
@@ -87,19 +87,19 @@
 xref
 0 14
 0000000000 65535 f 
-0000003095 00000 n 
-0000003159 00000 n 
-0000003209 00000 n 
+0000003139 00000 n 
+0000003203 00000 n 
+0000003253 00000 n 
 0000000015 00000 n 
 0000000071 00000 n 
-0000001264 00000 n 
-0000001370 00000 n 
-0000002432 00000 n 
-0000002538 00000 n 
-0000002650 00000 n 
-0000002760 00000 n 
-0000002871 00000 n 
-0000002979 00000 n 
+0000001255 00000 n 
+0000001361 00000 n 
+0000002476 00000 n 
+0000002582 00000 n 
+0000002694 00000 n 
+0000002804 00000 n 
+0000002915 00000 n 
+0000003023 00000 n 
 trailer
 <<
 /Size 14
@@ -107,5 +107,5 @@
 /Info 4 0 R
 >>
 startxref
-3331
+3375
 %%EOF
Index: docs/queryparsersyntax.html
===================================================================
--- docs/queryparsersyntax.html	(revision 796589)
+++ docs/queryparsersyntax.html	(working copy)
@@ -129,6 +129,9 @@
 <a href="api/contrib-analyzers/index.html">Analyzers</a>
 </div>
 <div class="menuitem">
+<a href="api/contrib-smartcn/index.html">Smart Chinese Analyzer</a>
+</div>
+<div class="menuitem">
 <a href="api/contrib-ant/index.html">Ant</a>
 </div>
 <div class="menuitem">
Index: docs/scoring.html
===================================================================
--- docs/scoring.html	(revision 796589)
+++ docs/scoring.html	(working copy)
@@ -129,6 +129,9 @@
 <a href="api/contrib-analyzers/index.html">Analyzers</a>
 </div>
 <div class="menuitem">
+<a href="api/contrib-smartcn/index.html">Smart Chinese Analyzer</a>
+</div>
+<div class="menuitem">
 <a href="api/contrib-ant/index.html">Ant</a>
 </div>
 <div class="menuitem">
Index: docs/lucene-sandbox/index.html
===================================================================
--- docs/lucene-sandbox/index.html	(revision 796589)
+++ docs/lucene-sandbox/index.html	(working copy)
@@ -129,6 +129,9 @@
 <a href="../api/contrib-analyzers/index.html">Analyzers</a>
 </div>
 <div class="menuitem">
+<a href="../api/contrib-smartcn/index.html">Smart Chinese Analyzer</a>
+</div>
+<div class="menuitem">
 <a href="../api/contrib-ant/index.html">Ant</a>
 </div>
 <div class="menuitem">
Index: docs/skin/basic.css
===================================================================
--- docs/skin/basic.css	(revision 796589)
+++ docs/skin/basic.css	(working copy)
@@ -163,4 +163,4 @@
 .codefrag {
   font-family: "Courier New", Courier, monospace;
   font-size: 110%;
-}
\ No newline at end of file
+}
Index: docs/skin/print.css
===================================================================
--- docs/skin/print.css	(revision 796589)
+++ docs/skin/print.css	(working copy)
@@ -51,4 +51,4 @@
 
 acronym {
   border: 0;
-}
\ No newline at end of file
+}
Index: docs/skin/profile.css
===================================================================
--- docs/skin/profile.css	(revision 796589)
+++ docs/skin/profile.css	(working copy)
@@ -172,4 +172,4 @@
     }
       
     
-  
\ No newline at end of file
+  
Index: docs/skin/screen.css
===================================================================
--- docs/skin/screen.css	(revision 796589)
+++ docs/skin/screen.css	(working copy)
@@ -584,4 +584,4 @@
   list-style-image: url('../images/instruction_arrow.png');
   list-style-position: outside;
   margin-left: 2em;
-} 
\ No newline at end of file
+} 
Index: src/site/src/documentation/content/xdocs/site.xml
===================================================================
--- src/site/src/documentation/content/xdocs/site.xml	(revision 796589)
+++ src/site/src/documentation/content/xdocs/site.xml	(working copy)
@@ -49,6 +49,7 @@
 		 <javadoc-demo label="Demo" href="ext:javadocs-demo"/>
 		 <javadoc-contrib label="Contrib">
 		    <javadoc-contrib-analyzers label="Analyzers" href="ext:javadocs-contrib-analyzers"/>
+		    <javadoc-contrib-smartcn label="Smart Chinese Analyzer" href="ext:javadocs-contrib-smartcn"/>
 		    <javadoc-contrib-ant label="Ant" href="ext:javadocs-contrib-ant"/>
 		    <javadoc-contrib-bdb label="Bdb" href="ext:javadocs-contrib-bdb"/>
 		    <javadoc-contrib-bdb-je label="Bdb-je" href="ext:javadocs-contrib-bdb-je"/>
@@ -100,6 +101,7 @@
 	<javadocs-core href="api/core/index.html"/>
 	<javadocs-demo href="api/demo/index.html"/>
 	<javadocs-contrib-analyzers href="api/contrib-analyzers/index.html"/>
+	<javadocs-contrib-smartcn href="api/contrib-smartcn/index.html"/>
 	<javadocs-contrib-ant href="api/contrib-ant/index.html"/>
 	<javadocs-contrib-bdb href="api/contrib-bdb/index.html"/>
 	<javadocs-contrib-bdb-je href="api/contrib-bdb-je/index.html"/>
