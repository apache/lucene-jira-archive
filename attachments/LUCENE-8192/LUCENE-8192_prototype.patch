diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFactories.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFactories.java
index 499774e..e647868 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFactories.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFactories.java
@@ -91,7 +91,7 @@ public class TestFactories extends BaseTokenStreamTestCase {
         // beast it just a little, it shouldnt throw exceptions:
         // (it should have thrown them in initialize)
         Analyzer a = new FactoryAnalyzer(factory, null, null);
-        checkRandomData(random(), a, 20, 20, false, false);
+        checkRandomData(random(), a, 20, 20, false);
         a.close();
       }
     }
@@ -115,7 +115,7 @@ public class TestFactories extends BaseTokenStreamTestCase {
         // beast it just a little, it shouldnt throw exceptions:
         // (it should have thrown them in initialize)
         Analyzer a = new FactoryAnalyzer(assertingTokenizer, factory, null);
-        checkRandomData(random(), a, 20, 20, false, false);
+        checkRandomData(random(), a, 20, 20, false);
         a.close();
       }
     }
@@ -139,7 +139,7 @@ public class TestFactories extends BaseTokenStreamTestCase {
         // beast it just a little, it shouldnt throw exceptions:
         // (it should have thrown them in initialize)
         Analyzer a = new FactoryAnalyzer(assertingTokenizer, null, factory);
-        checkRandomData(random(), a, 20, 20, false, false);
+        checkRandomData(random(), a, 20, 20, false);
         a.close();
       }
     }
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFlattenGraphFilter.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFlattenGraphFilter.java
index c69bcca..0c0c4e6 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFlattenGraphFilter.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFlattenGraphFilter.java
@@ -50,8 +50,7 @@ public class TestFlattenGraphFilter extends BaseTokenStreamTestCase {
                      new int[]    {    3,         12},
                      null,
                      new int[]    {    1,          1},
-                     new int[]    {    1,          1},
-                     true);
+                     new int[]    {    1,          1});
   }
 
   // Make sure graph is unchanged if it's already flat
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java
index 7839203..99be9d0 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java
@@ -807,8 +807,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
         }
         try {
           checkNormalize(a);
-          checkRandomData(random, a, 500*RANDOM_MULTIPLIER, 20, false,
-              false /* We already validate our own offsets... */);
+          checkRandomData(random, a, 500*RANDOM_MULTIPLIER, 20, false);
         } catch (Throwable e) {
           System.err.println("Exception from random analyzer: " + a);
           throw e;
@@ -834,8 +833,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
           System.out.println("Creating random analyzer:" + a);
         }
         try {
-          checkRandomData(random, a, 50*RANDOM_MULTIPLIER, 80, false,
-              false /* We already validate our own offsets... */);
+          checkRandomData(random, a, 50*RANDOM_MULTIPLIER, 80, false);
         } catch (Throwable e) {
           System.err.println("Exception from random analyzer: " + a);
           throw e;
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/minhash/MinHashFilterTest.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/minhash/MinHashFilterTest.java
index 1bc6ed7..311cd4c 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/minhash/MinHashFilterTest.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/minhash/MinHashFilterTest.java
@@ -182,15 +182,13 @@ public class MinHashFilterTest extends BaseTokenStreamTestCase {
     String[] hashes = new String[]{"℁팽徭聙↝ꇁ홱杯"};
     TokenStream ts = createTokenStream(5, "woof woof woof woof woof", 1, 1, 100, false);
     assertTokenStreamContents(ts, hashes, new int[]{0},
-        new int[]{24}, new String[]{MinHashFilter.MIN_HASH_TYPE}, new int[]{1}, new int[]{1}, 24, 0, null,
-        true, null);
+        new int[]{24}, new String[]{MinHashFilter.MIN_HASH_TYPE}, new int[]{1}, new int[]{1}, 24, 0, null, null);
 
     ts = createTokenStream(5, "woof woof woof woof woof", 2, 1, 1, false);
     assertTokenStreamContents(ts, new String[]{new String(new char[]{0, 0, 8449, 54077, 64133, 32857, 8605, 41409}),
             new String(new char[]{0, 1, 16887, 58164, 39536, 14926, 6529, 17276})}, new int[]{0, 0},
         new int[]{24, 24}, new String[]{MinHashFilter.MIN_HASH_TYPE, MinHashFilter.MIN_HASH_TYPE}, new int[]{1, 0},
-        new int[]{1, 1}, 24, 0, null,
-        true, null);
+        new int[]{1, 1}, 24, 0, null, null);
   }
 
   @Test
@@ -203,7 +201,7 @@ public class MinHashFilterTest extends BaseTokenStreamTestCase {
         false);
     assertTokenStreamContents(ts, hashes, new int[]{0, 0},
         new int[]{49, 49}, new String[]{MinHashFilter.MIN_HASH_TYPE, MinHashFilter.MIN_HASH_TYPE}, new int[]{1, 0},
-        new int[]{1, 1}, 49, 0, null, true, null);
+        new int[]{1, 1}, 49, 0, null, null);
   }
 
   private ArrayList<String> getTokens(TokenStream ts) throws IOException {
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java
index 2804bfd..5a4d490 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java
@@ -226,8 +226,7 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
         new int[] { 6, 13 },
         null,
         new int[] { 1, 1 },
-        null,
-        false);
+        null);
     
     /* only in this case, posInc of 2 ?! */
     assertAnalyzesTo(a, "LUCENE / solR", new String[] { "LUCENE", "sol", "solR", "R" },
@@ -235,16 +234,14 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
         new int[] { 6, 12, 13, 13 },
         null,                     
         new int[] { 1, 1, 0, 1 },
-        null,
-        false);
+        null);
     
     assertAnalyzesTo(a, "LUCENE / NUTCH SOLR", new String[] { "LUCENE", "NUTCH", "SOLR" },
         new int[] { 0, 9, 15 },
         new int[] { 6, 14, 19 },
         null,
         new int[] { 1, 1, 1 },
-        null,
-        false);
+        null);
     
     /* analyzer that will consume tokens with large position increments */
     Analyzer a2 = new Analyzer() {
@@ -263,8 +260,7 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
         new int[] { 6, 15, 20 },
         null,
         new int[] { 1, 10, 1 },
-        null,
-        false);
+        null);
     
     /* the "/" had a position increment of 10, where did it go?!?!! */
     assertAnalyzesTo(a2, "LUCENE / SOLR", new String[] { "LUCENE", "SOLR" },
@@ -272,8 +268,7 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
         new int[] { 6, 13 },
         null,
         new int[] { 1, 11 },
-        null,
-        false);
+        null);
     
     /* in this case, the increment of 10 from the "/" is carried over */
     assertAnalyzesTo(a2, "LUCENE / solR", new String[] { "LUCENE", "sol", "solR", "R" },
@@ -281,16 +276,14 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
         new int[] { 6, 12, 13, 13 },
         null,
         new int[] { 1, 11, 0, 1 },
-        null,
-        false);
+        null);
     
     assertAnalyzesTo(a2, "LUCENE / NUTCH SOLR", new String[] { "LUCENE", "NUTCH", "SOLR" },
         new int[] { 0, 9, 15 },
         new int[] { 6, 14, 19 },
         null,
         new int[] { 1, 11, 1 },
-        null,
-        false);
+        null);
 
     Analyzer a3 = new Analyzer() {
       @Override
@@ -307,8 +300,7 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
         new int[] { 6, 11, 11 },
         null,
         new int[] { 1, 0, 1 },
-        null,
-        false);
+        null);
 
     /* the stopword should add a gap here */
     assertAnalyzesTo(a3, "the lucene.solr", 
@@ -317,8 +309,7 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
         new int[] { 10, 15, 15 },
         null,
         new int[] { 2, 0, 1 },
-        null,
-        false);
+        null);
 
     IOUtils.close(a, a2, a3);
   }
@@ -342,8 +333,7 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
         new int[] { 3, 7, 15, 7, 11, 15, 15 },
         null,
         new int[] { 1, 0, 0, 1, 1, 0, 1 },
-        null,
-        false);
+        null);
     a.close();
   }
   
@@ -366,8 +356,7 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
         new int[] { 15, 3, 7, 15, 7, 11, 15, 15 },
         null,
         new int[] { 1, 0, 0, 0, 1, 1, 0, 1 },
-        null,
-        false);
+        null);
     a.close();
   }
   
@@ -392,7 +381,7 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
         }
       };
       // TODO: properly support positionLengthAttribute
-      checkRandomData(random(), a, 200*RANDOM_MULTIPLIER, 20, false, false);
+      checkRandomData(random(), a, 200*RANDOM_MULTIPLIER, 20, false);
       a.close();
     }
   }
@@ -418,7 +407,7 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
         }
       };
       // TODO: properly support positionLengthAttribute
-      checkRandomData(random(), a, 20*RANDOM_MULTIPLIER, 8192, false, false);
+      checkRandomData(random(), a, 20*RANDOM_MULTIPLIER, 8192, false);
       a.close();
     }
   }
@@ -480,8 +469,7 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
                      new int[] {},
                      null,
                      new int[] {},
-                     null,
-                     false);
+                     null);
   }
 
   public void testNumberPunct() throws Exception {
@@ -501,8 +489,7 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
                      new int[] {1},
                      null,
                      new int[] {1},
-                     null,
-                     false);
+                     null);
   }
 
   private Analyzer getAnalyzer(final int flags) {
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterGraphFilter.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterGraphFilter.java
index 7516a23..fcde74e 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterGraphFilter.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterGraphFilter.java
@@ -212,8 +212,7 @@ public class TestWordDelimiterGraphFilter extends BaseTokenStreamTestCase {
         new int[] { 6, 13 },
         null,
         new int[] { 1, 2 },
-        null,
-        false);
+        null);
 
     /* only in this case, posInc of 2 ?! */
     assertAnalyzesTo(a, "LUCENE / solR", new String[] { "LUCENE", "solR", "sol", "R" },
@@ -221,16 +220,14 @@ public class TestWordDelimiterGraphFilter extends BaseTokenStreamTestCase {
         new int[] { 6, 13, 12, 13 },
         null,                     
         new int[] { 1, 2, 0, 1 },
-        null,
-        false);
+        null);
     
     assertAnalyzesTo(a, "LUCENE / NUTCH SOLR", new String[] { "LUCENE", "NUTCH", "SOLR" },
         new int[] { 0, 9, 15 },
         new int[] { 6, 14, 19 },
         null,
         new int[] { 1, 2, 1 },
-        null,
-        false);
+        null);
     
     /* analyzer that will consume tokens with large position increments */
     Analyzer a2 = new Analyzer() {
@@ -249,8 +246,7 @@ public class TestWordDelimiterGraphFilter extends BaseTokenStreamTestCase {
         new int[] { 6, 15, 20 },
         null,
         new int[] { 1, 10, 1 },
-        null,
-        false);
+        null);
     
     /* the "/" had a position increment of 10, where did it go?!?!! */
     assertAnalyzesTo(a2, "LUCENE / SOLR", new String[] { "LUCENE", "SOLR" },
@@ -258,8 +254,7 @@ public class TestWordDelimiterGraphFilter extends BaseTokenStreamTestCase {
         new int[] { 6, 13 },
         null,
         new int[] { 1, 11 },
-        null,
-        false);
+        null);
     
     /* in this case, the increment of 10 from the "/" is carried over */
     assertAnalyzesTo(a2, "LUCENE / solR", new String[] { "LUCENE", "solR", "sol", "R" },
@@ -267,16 +262,14 @@ public class TestWordDelimiterGraphFilter extends BaseTokenStreamTestCase {
         new int[] { 6, 13, 12, 13 },
         null,
         new int[] { 1, 11, 0, 1 },
-        null,
-        false);
+        null);
     
     assertAnalyzesTo(a2, "LUCENE / NUTCH SOLR", new String[] { "LUCENE", "NUTCH", "SOLR" },
         new int[] { 0, 9, 15 },
         new int[] { 6, 14, 19 },
         null,
         new int[] { 1, 11, 1 },
-        null,
-        false);
+        null);
 
     Analyzer a3 = new Analyzer() {
       @Override
@@ -293,8 +286,7 @@ public class TestWordDelimiterGraphFilter extends BaseTokenStreamTestCase {
         new int[] { 11, 6, 11 },
         null,
         new int[] { 1, 0, 1 },
-        null,
-        false);
+        null);
 
     /* the stopword should add a gap here */
     assertAnalyzesTo(a3, "the lucene.solr", 
@@ -303,8 +295,7 @@ public class TestWordDelimiterGraphFilter extends BaseTokenStreamTestCase {
         new int[] { 15, 10, 15 },
         null,
         new int[] { 2, 0, 1 },
-        null,
-        false);
+        null);
 
     IOUtils.close(a, a2, a3);
   }
@@ -328,8 +319,7 @@ public class TestWordDelimiterGraphFilter extends BaseTokenStreamTestCase {
         new int[] { 15, 7, 3, 7, 15, 11, 15 },
         null,
         new int[] { 1, 0, 0, 1, 1, 0, 1 },
-        null,
-        false);
+        null);
     a.close();
   }
   
@@ -352,8 +342,7 @@ public class TestWordDelimiterGraphFilter extends BaseTokenStreamTestCase {
                      new int[] { 15, 15, 7, 3, 7, 15, 11, 15 },
                      null,
                      new int[] { 1, 0, 0, 0, 1, 1, 0, 1 },
-                     null,
-                     false);
+                     null);
     a.close();
   }
   
@@ -378,7 +367,7 @@ public class TestWordDelimiterGraphFilter extends BaseTokenStreamTestCase {
         }
       };
       // TODO: properly support positionLengthAttribute
-      checkRandomData(random(), a, 200*RANDOM_MULTIPLIER, 20, false, false);
+      checkRandomData(random(), a, 200*RANDOM_MULTIPLIER, 20, false);
       a.close();
     }
   }
@@ -405,7 +394,7 @@ public class TestWordDelimiterGraphFilter extends BaseTokenStreamTestCase {
         }
       };
       // TODO: properly support positionLengthAttribute
-      checkRandomData(random(), a, 20*RANDOM_MULTIPLIER, 8192, false, false);
+      checkRandomData(random(), a, 20*RANDOM_MULTIPLIER, 8192, false);
       a.close();
     }
   }
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
index d7536e7..b042771 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
@@ -91,8 +91,7 @@ public class EdgeNGramTokenFilterTest extends BaseTokenStreamTestCase {
                               null,
                               new int[]{1,0,0,1,0,0},
                               null,
-                              null,
-                              false);
+                              null);
   }
 
   private static class PositionFilter extends TokenFilter {
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java
index 7fbea5f..350af7d 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java
@@ -115,8 +115,7 @@ public class EdgeNGramTokenizerTest extends BaseTokenStreamTestCase {
                               null,
                               new int[]{1,1,1},
                               null,
-                              null,
-                              false);
+                              null);
   }
 
   private static void testNGrams(int minGram, int maxGram, int length, final String nonTokenChars) throws IOException {
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
index d8591a9..b3f0eec 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
@@ -76,8 +76,7 @@ public class NGramTokenFilterTest extends BaseTokenStreamTestCase {
         new int[]{5,5,5,5,5,5,5,5,5,5,5,5},
         null,
         new int[]{1,0,0,0,0,0,0,0,0,0,0,0},
-        null, null, false
-        );
+        null, null);
   }
 
   public void testNgramsNoIncrement() throws Exception {
@@ -88,8 +87,7 @@ public class NGramTokenFilterTest extends BaseTokenStreamTestCase {
         new int[]{5,5,5,5,5,5,5,5,5,5,5,5},
         null,
         new int[]{1,0,0,0,0,0,0,0,0,0,0,0},
-        null, null, false
-        );
+        null, null);
   }
 
   public void testOversizedNgrams() throws Exception {
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java
index 46a0c1c..44a954a 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java
@@ -80,9 +80,7 @@ public class NGramTokenizerTest extends BaseTokenStreamTestCase {
         null,
         null,
         null,
-        5 /* abcde */,
-        false
-        );
+        5 /* abcde */);
   }
   
   public void testOversizedNgrams() throws Exception {
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/path/TestPathHierarchyTokenizer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/path/TestPathHierarchyTokenizer.java
index afe1523..4af5fd3 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/path/TestPathHierarchyTokenizer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/path/TestPathHierarchyTokenizer.java
@@ -226,7 +226,7 @@ public class TestPathHierarchyTokenizer extends BaseTokenStreamTestCase {
       }    
     };
     // TODO: properly support positionLengthAttribute
-    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER, 20, false, false);
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER, 20, false);
     a.close();
   }
   
@@ -241,7 +241,7 @@ public class TestPathHierarchyTokenizer extends BaseTokenStreamTestCase {
       }    
     };
     // TODO: properly support positionLengthAttribute
-    checkRandomData(random, a, 100*RANDOM_MULTIPLIER, 1027, false, false);
+    checkRandomData(random, a, 100*RANDOM_MULTIPLIER, 1027, false);
     a.close();
   }
 }
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/path/TestReversePathHierarchyTokenizer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/path/TestReversePathHierarchyTokenizer.java
index 5d423a3..779ec8d 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/path/TestReversePathHierarchyTokenizer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/path/TestReversePathHierarchyTokenizer.java
@@ -184,7 +184,7 @@ public class TestReversePathHierarchyTokenizer extends BaseTokenStreamTestCase {
       }    
     };
     // TODO: properly support positionLengthAttribute
-    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER, 20, false, false);
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER, 20, false);
     a.close();
   }
   
@@ -199,7 +199,7 @@ public class TestReversePathHierarchyTokenizer extends BaseTokenStreamTestCase {
       }    
     };
     // TODO: properly support positionLengthAttribute
-    checkRandomData(random, a, 100*RANDOM_MULTIPLIER, 1027, false, false);
+    checkRandomData(random, a, 100*RANDOM_MULTIPLIER, 1027, false);
     a.close();
   }
 }
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java
index 5645900..9d9c98e 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java
@@ -1257,6 +1257,10 @@ public class ShingleFilterTest extends BaseTokenStreamTestCase {
         return new TokenStreamComponents(tokenizer, filter);
       }
     };
+    
+    // this assert previous disabled offsets with this remark:
+    // offsets are correct but assertTokenStreamContents does not handle multiple terms with different offsets
+    // finishing at the same position
     assertTokenStreamContents(a.tokenStream("", "to be or not to be"),
         new String[] {"to be or not", "be or not to", "or not to be"},
         new int[] {0, 3, 6},
@@ -1264,10 +1268,7 @@ public class ShingleFilterTest extends BaseTokenStreamTestCase {
         null,
         new int[] {1, 1, 1},
         new int[] {1, 1, 1},
-        18,
-        // offsets are correct but assertTokenStreamContents does not handle multiple terms with different offsets
-        // finishing at the same position
-        false);
+        18);
 
 
     a = new Analyzer() {
@@ -1279,6 +1280,10 @@ public class ShingleFilterTest extends BaseTokenStreamTestCase {
         return new TokenStreamComponents(tokenizer, filter);
       }
     };
+    
+    // this assert previous disabled offsets with this remark:
+    // offsets are correct but assertTokenStreamContents does not handle multiple terms with different offsets
+    // finishing at the same position
     assertTokenStreamContents(a.tokenStream("", "to be or not to be"),
         new String[] {"to be", "to be or", "to be or not", "be or", "be or not", "be or not to", "or not", "or not to",
             "or not to be", "not to", "not to be", "to be"},
@@ -1287,10 +1292,7 @@ public class ShingleFilterTest extends BaseTokenStreamTestCase {
         null,
         new int[] {1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1},
         new int[] {1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1},
-        18,
-        // offsets are correct but assertTokenStreamContents does not handle multiple terms with different offsets
-        // finishing at the same position
-        false);
+        18);
 
     a = new Analyzer() {
       @Override
@@ -1302,6 +1304,9 @@ public class ShingleFilterTest extends BaseTokenStreamTestCase {
       }
     };
 
+    // this assert previous disabled offsets with this remark:
+    // offsets are correct but assertTokenStreamContents does not handle multiple terms with different offsets
+    // finishing at the same position
     assertTokenStreamContents(a.tokenStream("", "to be or not to be"),
         new String[] {"to be or", "to be or not", "be or not", "be or not to", "or not to",
             "or not to be", "not to be"},
@@ -1310,10 +1315,7 @@ public class ShingleFilterTest extends BaseTokenStreamTestCase {
         null,
         new int[] {1, 0, 1, 0, 1, 0, 1, 0},
         new int[] {1, 2, 1, 2, 1, 2, 1, 2},
-        18,
-        // offsets are correct but assertTokenStreamContents does not handle multiple terms with different offsets
-        // finishing at the same position
-        false);
+        18);
 
     a = new Analyzer() {
       @Override
@@ -1324,6 +1326,9 @@ public class ShingleFilterTest extends BaseTokenStreamTestCase {
         return new TokenStreamComponents(tokenizer, filter);
       }
     };
+    // this assert previous disabled offsets with this remark:
+    // offsets are correct but assertTokenStreamContents does not handle multiple terms with different offsets
+    // finishing at the same position
     assertTokenStreamContents(a.tokenStream("", "to be or not to be"),
         new String[] {"to be or", "to be or not", "to be or not to", "be or not", "be or not to",
             "be or not to be", "or not to", "or not to be", "not to be"},
@@ -1332,9 +1337,6 @@ public class ShingleFilterTest extends BaseTokenStreamTestCase {
         null,
         new int[] {1, 0, 0, 1, 0, 0, 1, 0, 1, 0},
         new int[] {1, 2, 3, 1, 2, 3, 1, 2, 1},
-        18,
-        // offsets are correct but assertTokenStreamContents does not handle multiple terms with different offsets
-        // finishing at the same position
-        false);
+        18);
   }
 }
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymGraphFilter.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymGraphFilter.java
index 730d00a..84710a8 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymGraphFilter.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymGraphFilter.java
@@ -283,8 +283,7 @@ public class TestSynonymGraphFilter extends BaseTokenStreamTestCase {
                      new int[]    {1,    1,   3,   7,   5,   7},  
                      null,
                      new int[]    {1,    0,   1,   1,   0,   1},  
-                     new int[]    {1,    1,   1,   2,   1,   1},
-                     true);
+                     new int[]    {1,    1,   1,   2,   1,   1});
   }
 
   public void testLookaheadSecondParse() throws Exception {
@@ -300,8 +299,7 @@ public class TestSynonymGraphFilter extends BaseTokenStreamTestCase {
                      new int[]    { 1,   1,   3,   3},  
                      null,
                      new int[]    { 1,    0,   1,   0},  
-                     new int[]    { 1,    1,   1,   1},
-                     true);
+                     new int[]    { 1,    1,   1,   1});
   }
 
   public void testOneInputMultipleOutputNoKeepOrig() throws Exception {
@@ -582,8 +580,7 @@ public class TestSynonymGraphFilter extends BaseTokenStreamTestCase {
                      new int[]    { 1,   1},
                      null,
                      new int[]    { 1,   1},
-                     new int[]    { 1,   1},
-                     true);
+                     new int[]    { 1,   1});
     a.close();
   }
 
@@ -764,8 +761,7 @@ public class TestSynonymGraphFilter extends BaseTokenStreamTestCase {
                      new int[]    {    3,     3,      3,     3,       12},
                      null,
                      new int[]    {    1,     0,      1,     1,       1},
-                     new int[]    {    1,     3,      1,     1,       1},
-                     true);
+                     new int[]    {    1,     3,      1,     1,       1});
 
     Directory dir = newDirectory();
     RandomIndexWriter w = new RandomIndexWriter(random(), dir, a);
@@ -1851,8 +1847,7 @@ public class TestSynonymGraphFilter extends BaseTokenStreamTestCase {
                      new int[]      {3,    17,  10,       10,       10,  17,       17,       17,  17,   17,        17,   20,        28},
                      new String[]{"word", "SYNONYM", "SYNONYM", "SYNONYM", "word", "SYNONYM", "SYNONYM", "word", "SYNONYM", "SYNONYM", "SYNONYM", "word", "word"},
                      new int[]      {1,     1,   0,        0,        0,   1,        0,        0,   1,    0,         1,    1,         1},
-                     new int[]      {1,     4,   1,        1,        1,   1,        1,        3,   2,    1,         1,    1,         1},
-                     false);
+                     new int[]      {1,     4,   1,        1,        1,   1,        1,        3,   2,    1,         1,    1,         1});
 
     assertAnalyzesTo(analyzer, "the united states of balance",
                      new String[]{"the", "usa", "u", "united", "united", "s", "states", "states", "a", "of", "america", "of", "balance"},
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymMapFilter.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymMapFilter.java
index 0fbbd2e..8972233 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymMapFilter.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymMapFilter.java
@@ -161,8 +161,7 @@ public class TestSynonymMapFilter extends BaseTokenStreamTestCase {
                      new int[] {3, 5},
                      null,
                      new int[] {1, 1},
-                     new int[] {1, 1},
-                     true);
+                     new int[] {1, 1});
     checkAnalysisConsistency(random(), analyzer, false, "a b c");
     analyzer.close();
   }
@@ -187,8 +186,7 @@ public class TestSynonymMapFilter extends BaseTokenStreamTestCase {
                      new int[] {1, 3, 3, 5},
                      null,
                      new int[] {1, 0, 1, 1},
-                     new int[] {1, 2, 1, 1},
-                     true);
+                     new int[] {1, 2, 1, 1});
     checkAnalysisConsistency(random(), analyzer, false, "a b c");
     analyzer.close();
   }
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest.java
index 638faa8..5faf3e5 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest.java
@@ -192,7 +192,7 @@ public class WikipediaTokenizerTest extends BaseTokenStreamTestCase {
       } 
     };
     // TODO: properly support positionLengthAttribute
-    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER, 20, false, false);
+    checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER, 20, false);
     a.close();
   }
   
@@ -208,7 +208,7 @@ public class WikipediaTokenizerTest extends BaseTokenStreamTestCase {
       } 
     };
     // TODO: properly support positionLengthAttribute
-    checkRandomData(random, a, 100*RANDOM_MULTIPLIER, 8192, false, false);
+    checkRandomData(random, a, 100*RANDOM_MULTIPLIER, 8192, false);
     a.close();
   }
 }
diff --git a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestFactories.java b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestFactories.java
index 0a467a7..0664bdd 100644
--- a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestFactories.java
+++ b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestFactories.java
@@ -88,7 +88,7 @@ public class TestFactories extends BaseTokenStreamTestCase {
         // beast it just a little, it shouldnt throw exceptions:
         // (it should have thrown them in initialize)
         Analyzer a = new FactoryAnalyzer(factory, null, null);
-        checkRandomData(random(), a, 20, 20, false, false);
+        checkRandomData(random(), a, 20, 20, false);
         a.close();
       }
     }
@@ -112,7 +112,7 @@ public class TestFactories extends BaseTokenStreamTestCase {
         // beast it just a little, it shouldnt throw exceptions:
         // (it should have thrown them in initialize)
         Analyzer a = new FactoryAnalyzer(assertingTokenizer, factory, null);
-        checkRandomData(random(), a, 20, 20, false, false);
+        checkRandomData(random(), a, 20, 20, false);
         a.close();
       }
     }
@@ -136,7 +136,7 @@ public class TestFactories extends BaseTokenStreamTestCase {
         // beast it just a little, it shouldnt throw exceptions:
         // (it should have thrown them in initialize)
         Analyzer a = new FactoryAnalyzer(assertingTokenizer, null, factory);
-        checkRandomData(random(), a, 20, 20, false, false);
+        checkRandomData(random(), a, 20, 20, false);
         a.close();
       }
     }
diff --git a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseNumberFilter.java b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseNumberFilter.java
index b8a987a..2c2bff9 100644
--- a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseNumberFilter.java
+++ b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseNumberFilter.java
@@ -267,8 +267,7 @@ public class TestJapaneseNumberFilter extends BaseTokenStreamTestCase {
   @Test
   public void testFunnyIssue() throws Exception {
     BaseTokenStreamTestCase.checkAnalysisConsistency(
-        random(), analyzer, true, "〇〇\u302f\u3029\u3039\u3023\u3033\u302bB", true
-    );
+        random(), analyzer, true, "〇〇\u302f\u3029\u3039\u3023\u3033\u302bB");
   }
 
   @Ignore("This test is used during development when analyze normalizations in large amounts of text")
diff --git a/lucene/analysis/opennlp/src/test/org/apache/lucene/analysis/opennlp/TestOpenNLPChunkerFilterFactory.java b/lucene/analysis/opennlp/src/test/org/apache/lucene/analysis/opennlp/TestOpenNLPChunkerFilterFactory.java
index 013348c..4b94d11 100644
--- a/lucene/analysis/opennlp/src/test/org/apache/lucene/analysis/opennlp/TestOpenNLPChunkerFilterFactory.java
+++ b/lucene/analysis/opennlp/src/test/org/apache/lucene/analysis/opennlp/TestOpenNLPChunkerFilterFactory.java
@@ -58,7 +58,7 @@ public class TestOpenNLPChunkerFilterFactory extends BaseTokenStreamTestCase {
         .addTokenFilter("opennlpChunker", "chunkerModel", chunkerModelFile)
         .build();
     assertAnalyzesTo(analyzer, SENTENCES, SENTENCES_punc, SENTENCES_startOffsets, SENTENCES_endOffsets,
-        SENTENCES_chunks, null, null, true);
+        SENTENCES_chunks, null, null);
   }
 
   public void testPayloads() throws Exception {
@@ -69,6 +69,6 @@ public class TestOpenNLPChunkerFilterFactory extends BaseTokenStreamTestCase {
         .addTokenFilter(TypeAsPayloadTokenFilterFactory.class)
         .build();
     assertAnalyzesTo(analyzer, SENTENCES, SENTENCES_punc, SENTENCES_startOffsets, SENTENCES_endOffsets,
-        null, null, null, true, toPayloads(SENTENCES_chunks));
+        null, null, null, toPayloads(SENTENCES_chunks));
   }
 }
diff --git a/lucene/analysis/opennlp/src/test/org/apache/lucene/analysis/opennlp/TestOpenNLPLemmatizerFilterFactory.java b/lucene/analysis/opennlp/src/test/org/apache/lucene/analysis/opennlp/TestOpenNLPLemmatizerFilterFactory.java
index 0491b91..6005b5c 100644
--- a/lucene/analysis/opennlp/src/test/org/apache/lucene/analysis/opennlp/TestOpenNLPLemmatizerFilterFactory.java
+++ b/lucene/analysis/opennlp/src/test/org/apache/lucene/analysis/opennlp/TestOpenNLPLemmatizerFilterFactory.java
@@ -77,7 +77,7 @@ public class TestOpenNLPLemmatizerFilterFactory extends BaseTokenStreamTestCase
         .addTokenFilter("opennlplemmatizer", "dictionary", "en-test-lemmas.dict")
         .build();
     assertAnalyzesTo(analyzer, SENTENCE, SENTENCE_dict_punc, null, null,
-        SENTENCE_posTags, null, null, true);
+        SENTENCE_posTags, null, null);
   }
 
   public void test2SentencesDictionaryOnly() throws Exception {
@@ -87,7 +87,7 @@ public class TestOpenNLPLemmatizerFilterFactory extends BaseTokenStreamTestCase
         .addTokenFilter("opennlplemmatizer", "dictionary", lemmatizerDictFile)
         .build();
     assertAnalyzesTo(analyzer, SENTENCES, SENTENCES_dict_punc, null, null,
-        SENTENCES_posTags, null, null, true);
+        SENTENCES_posTags, null, null);
   }
 
   public void test1SentenceMaxEntOnly() throws Exception {
@@ -97,7 +97,7 @@ public class TestOpenNLPLemmatizerFilterFactory extends BaseTokenStreamTestCase
         .addTokenFilter("opennlplemmatizer", "lemmatizerModel", lemmatizerModelFile)
         .build();
     assertAnalyzesTo(analyzer, SENTENCE, SENTENCE_maxent_punc, null, null,
-        SENTENCE_posTags, null, null, true);
+        SENTENCE_posTags, null, null);
   }
 
   public void test2SentencesMaxEntOnly() throws Exception {
@@ -107,7 +107,7 @@ public class TestOpenNLPLemmatizerFilterFactory extends BaseTokenStreamTestCase
         .addTokenFilter("OpenNLPLemmatizer", "lemmatizerModel", lemmatizerModelFile)
         .build();
     assertAnalyzesTo(analyzer, SENTENCES, SENTENCES_maxent_punc, null, null,
-        SENTENCES_posTags, null, null, true);
+        SENTENCES_posTags, null, null);
   }
 
   public void test1SentenceDictionaryAndMaxEnt() throws Exception {
@@ -117,7 +117,7 @@ public class TestOpenNLPLemmatizerFilterFactory extends BaseTokenStreamTestCase
         .addTokenFilter("opennlplemmatizer", "dictionary", "en-test-lemmas.dict", "lemmatizerModel", lemmatizerModelFile)
         .build();
     assertAnalyzesTo(analyzer, SENTENCE_both, SENTENCE_both_punc, null, null,
-        SENTENCE_both_posTags, null, null, true);
+        SENTENCE_both_posTags, null, null);
   }
 
   public void test2SentencesDictionaryAndMaxEnt() throws Exception {
@@ -127,7 +127,7 @@ public class TestOpenNLPLemmatizerFilterFactory extends BaseTokenStreamTestCase
         .addTokenFilter("opennlplemmatizer", "dictionary", lemmatizerDictFile, "lemmatizerModel", lemmatizerModelFile)
         .build();
     assertAnalyzesTo(analyzer, SENTENCES_both, SENTENCES_both_punc, null, null,
-        SENTENCES_both_posTags, null, null, true);
+        SENTENCES_both_posTags, null, null);
   }
 
   public void testKeywordAttributeAwarenessDictionaryOnly() throws Exception {
@@ -139,7 +139,7 @@ public class TestOpenNLPLemmatizerFilterFactory extends BaseTokenStreamTestCase
         .addTokenFilter(RemoveDuplicatesTokenFilterFactory.class)
         .build();
     assertAnalyzesTo(analyzer, SENTENCES, SENTENCES_dict_keep_orig_punc, null, null,
-        SENTENCES_keep_orig_posTags, null, null, true);
+        SENTENCES_keep_orig_posTags, null, null);
   }
 
   public void testKeywordAttributeAwarenessMaxEntOnly() throws Exception {
@@ -151,7 +151,7 @@ public class TestOpenNLPLemmatizerFilterFactory extends BaseTokenStreamTestCase
         .addTokenFilter(RemoveDuplicatesTokenFilterFactory.class)
         .build();
     assertAnalyzesTo(analyzer, SENTENCES, SENTENCES_max_ent_keep_orig_punc, null, null,
-        SENTENCES_keep_orig_posTags, null, null, true);
+        SENTENCES_keep_orig_posTags, null, null);
   }
 
   public void testKeywordAttributeAwarenessDictionaryAndMaxEnt() throws Exception {
@@ -163,7 +163,7 @@ public class TestOpenNLPLemmatizerFilterFactory extends BaseTokenStreamTestCase
         .addTokenFilter(RemoveDuplicatesTokenFilterFactory.class)
         .build();
     assertAnalyzesTo(analyzer, SENTENCES_both, SENTENCES_both_keep_orig_punc, null, null,
-        SENTENCES_both_keep_orig_posTags, null, null, true);
+        SENTENCES_both_keep_orig_posTags, null, null);
   }
 
 }
diff --git a/lucene/analysis/opennlp/src/test/org/apache/lucene/analysis/opennlp/TestOpenNLPPOSFilterFactory.java b/lucene/analysis/opennlp/src/test/org/apache/lucene/analysis/opennlp/TestOpenNLPPOSFilterFactory.java
index 814f480..68a3cc7 100644
--- a/lucene/analysis/opennlp/src/test/org/apache/lucene/analysis/opennlp/TestOpenNLPPOSFilterFactory.java
+++ b/lucene/analysis/opennlp/src/test/org/apache/lucene/analysis/opennlp/TestOpenNLPPOSFilterFactory.java
@@ -70,7 +70,7 @@ public class TestOpenNLPPOSFilterFactory extends BaseTokenStreamTestCase {
         .addTokenFilter("opennlpPOS", "posTaggerModel", posTaggerModelFile)
         .build();
     assertAnalyzesTo(analyzer, SENTENCES, SENTENCES_punc, SENTENCES_startOffsets, SENTENCES_endOffsets,
-        SENTENCES_posTags, null, null, true);
+        SENTENCES_posTags, null, null);
 
     analyzer = CustomAnalyzer.builder(new ClasspathResourceLoader(getClass()))
         .withTokenizer("opennlp", "tokenizerModel", tokenizerModelFile, "sentenceModel", sentenceModelFile)
@@ -78,7 +78,7 @@ public class TestOpenNLPPOSFilterFactory extends BaseTokenStreamTestCase {
         .addTokenFilter(TypeAsPayloadTokenFilterFactory.class)
         .build();
     assertAnalyzesTo(analyzer, SENTENCES, SENTENCES_punc, SENTENCES_startOffsets, SENTENCES_endOffsets,
-        null, null, null, true, toPayloads(SENTENCES_posTags));
+        null, null, null, toPayloads(SENTENCES_posTags));
   }
 
   public void testNoBreak() throws Exception {
@@ -87,6 +87,6 @@ public class TestOpenNLPPOSFilterFactory extends BaseTokenStreamTestCase {
         .addTokenFilter("opennlpPOS", "posTaggerModel", posTaggerModelFile)
         .build();
     assertAnalyzesTo(analyzer, NO_BREAK, NO_BREAK_terms, NO_BREAK_startOffsets, NO_BREAK_endOffsets,
-        null, null, null, true);
+        null, null, null);
   }
 }
diff --git a/lucene/core/src/test/org/apache/lucene/analysis/TestStopFilter.java b/lucene/core/src/test/org/apache/lucene/analysis/TestStopFilter.java
index f17cd51..c5b8430 100644
--- a/lucene/core/src/test/org/apache/lucene/analysis/TestStopFilter.java
+++ b/lucene/core/src/test/org/apache/lucene/analysis/TestStopFilter.java
@@ -107,7 +107,6 @@ public class TestStopFilter extends BaseTokenStreamTestCase {
                               7,
                               1,
                               null,
-                              true,
                               null);
   }
 
diff --git a/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestSuggestStopFilter.java b/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestSuggestStopFilter.java
index 5ed84e0..5a1e6be 100644
--- a/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestSuggestStopFilter.java
+++ b/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/TestSuggestStopFilter.java
@@ -40,8 +40,7 @@ public class TestSuggestStopFilter extends BaseTokenStreamTestCase {
                               new int[] {1, 1},
                               null,
                               5,
-                              new boolean[] {false, true},
-                              true);
+                              new boolean[] {false, true});
   }
 
   public void testEndIsStopWord() throws Exception {
@@ -59,8 +58,7 @@ public class TestSuggestStopFilter extends BaseTokenStreamTestCase {
                               new int[] {1},
                               null,
                               6,
-                              new boolean[] {false},
-                              true);
+                              new boolean[] {false});
   }
 
   public void testMidStopWord() throws Exception {
@@ -79,8 +77,7 @@ public class TestSuggestStopFilter extends BaseTokenStreamTestCase {
                               new int[] {1, 2},
                               null,
                               12,
-                              new boolean[] {false, false},
-                              true);
+                              new boolean[] {false, false});
   }
 
   public void testMultipleStopWords() throws Exception {
@@ -99,8 +96,7 @@ public class TestSuggestStopFilter extends BaseTokenStreamTestCase {
                               new int[] {1, 4},
                               null,
                               18,
-                              new boolean[] {false, false},
-                              true);
+                              new boolean[] {false, false});
   }
 
   public void testMultipleStopWordsEnd() throws Exception {
@@ -119,8 +115,7 @@ public class TestSuggestStopFilter extends BaseTokenStreamTestCase {
                               new int[] {1, 3},
                               null,
                               11,
-                              new boolean[] {false, true},
-                              true);
+                              new boolean[] {false, true});
   }
 
   public void testMultipleStopWordsEnd2() throws Exception {
@@ -139,7 +134,6 @@ public class TestSuggestStopFilter extends BaseTokenStreamTestCase {
                               new int[] {1},
                               null,
                               12,
-                              new boolean[] {false},
-                              true);
+                              new boolean[] {false});
   }
 }
diff --git a/lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
index 3e1e375..c53a66e 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
@@ -120,15 +120,8 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
     }
   }
 
-  // offsetsAreCorrect also validates:
-  //   - graph offsets are correct (all tokens leaving from
-  //     pos X have the same startOffset; all tokens
-  //     arriving to pos Y have the same endOffset)
-  //   - offsets only move forwards (startOffset >=
-  //     lastStartOffset)
   public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],
-                                               int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts,
-                                               boolean offsetsAreCorrect, byte[][] payloads) throws IOException {
+                                               int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts, byte[][] payloads) throws IOException {
     assertNotNull(output);
     CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);
     
@@ -235,12 +228,10 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
                      endOffset <= finalOffset.intValue());
         }
 
-        if (offsetsAreCorrect) {
-          assertTrue("offsets must not go backwards startOffset=" + startOffset + " is < lastStartOffset=" + lastStartOffset + " term=" + termAtt, offsetAtt.startOffset() >= lastStartOffset);
-          lastStartOffset = offsetAtt.startOffset();
-        }
+        assertTrue("offsets must not go backwards startOffset=" + startOffset + " is < lastStartOffset=" + lastStartOffset + " term=" + termAtt, offsetAtt.startOffset() >= lastStartOffset);
+        lastStartOffset = offsetAtt.startOffset();
 
-        if (offsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {
+        if (posLengthAtt != null && posIncrAtt != null) {
           // Validate offset consistency in the graph, ie
           // all tokens leaving from a certain pos have the
           // same startOffset, and all tokens arriving to a
@@ -320,17 +311,12 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
   }
   
   public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],
-                                               int posLengths[], Integer finalOffset, boolean[] keywordAtts,
-                                               boolean offsetsAreCorrect) throws IOException {
-    assertTokenStreamContents(ts, output, startOffsets, endOffsets, types, posIncrements, posLengths, finalOffset, null, keywordAtts, offsetsAreCorrect, null);
-  }
-
-  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], Integer finalOffset, boolean offsetsAreCorrect) throws IOException {
-    assertTokenStreamContents(ts, output, startOffsets, endOffsets, types, posIncrements, posLengths, finalOffset, null, offsetsAreCorrect);
+                                               int posLengths[], Integer finalOffset, boolean[] keywordAtts) throws IOException {
+    assertTokenStreamContents(ts, output, startOffsets, endOffsets, types, posIncrements, posLengths, finalOffset, null, keywordAtts, null);
   }
 
   public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], Integer finalOffset) throws IOException {
-    assertTokenStreamContents(ts, output, startOffsets, endOffsets, types, posIncrements, posLengths, finalOffset, true);
+    assertTokenStreamContents(ts, output, startOffsets, endOffsets, types, posIncrements, posLengths, finalOffset, null);
   }
 
   public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], Integer finalOffset) throws IOException {
@@ -385,15 +371,9 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
     assertTokenStreamContents(a.tokenStream("dummy", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length());
   }
 
-  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean offsetsAreCorrect) throws IOException {
-    checkResetException(a, input);
-    checkAnalysisConsistency(random(), a, true, input, offsetsAreCorrect);
-    assertTokenStreamContents(a.tokenStream("dummy", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), offsetsAreCorrect);
-  }
-
-  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean offsetsAreCorrect, byte[][] payloads) throws IOException {
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], byte[][] payloads) throws IOException {
     checkResetException(a, input);
-    assertTokenStreamContents(a.tokenStream("dummy", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), null, null, offsetsAreCorrect, payloads);
+    assertTokenStreamContents(a.tokenStream("dummy", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), null, null, payloads);
   }
 
   public static void assertAnalyzesTo(Analyzer a, String input, String[] output) throws IOException {
@@ -467,12 +447,12 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
   
   /** utility method for blasting tokenstreams with data to make sure they don't do anything crazy */
   public static void checkRandomData(Random random, Analyzer a, int iterations) throws IOException {
-    checkRandomData(random, a, iterations, 20, false, true);
+    checkRandomData(random, a, iterations, 20, false);
   }
 
   /** utility method for blasting tokenstreams with data to make sure they don't do anything crazy */
   public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {
-    checkRandomData(random, a, iterations, maxWordLength, false, true);
+    checkRandomData(random, a, iterations, maxWordLength, false);
   }
   
   /** 
@@ -480,7 +460,7 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
    * @param simple true if only ascii strings will be used (try to avoid)
    */
   public static void checkRandomData(Random random, Analyzer a, int iterations, boolean simple) throws IOException {
-    checkRandomData(random, a, iterations, 20, simple, true);
+    checkRandomData(random, a, iterations, 20, simple);
   }
   
   /** Asserts that the given stream has expected number of tokens. */
@@ -501,7 +481,6 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
     final Analyzer a;
     final boolean useCharFilter;
     final boolean simple;
-    final boolean offsetsAreCorrect;
     final RandomIndexWriter iw;
     final CountDownLatch latch;
 
@@ -510,14 +489,13 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
     // interact)... so this is just "best effort":
     public boolean failed;
     
-    AnalysisThread(long seed, CountDownLatch latch, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple, boolean offsetsAreCorrect, RandomIndexWriter iw) {
+    AnalysisThread(long seed, CountDownLatch latch, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple, RandomIndexWriter iw) {
       this.seed = seed;
       this.a = a;
       this.iterations = iterations;
       this.maxWordLength = maxWordLength;
       this.useCharFilter = useCharFilter;
       this.simple = simple;
-      this.offsetsAreCorrect = offsetsAreCorrect;
       this.iw = iw;
       this.latch = latch;
     }
@@ -529,7 +507,7 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
         latch.await();
         // see the part in checkRandomData where it replays the same text again
         // to verify reproducability/reuse: hopefully this would catch thread hazards.
-        checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);
+        checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, iw);
         success = true;
       } catch (Exception e) {
         Rethrow.rethrow(e);
@@ -540,10 +518,6 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
   };
   
   public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple) throws IOException {
-    checkRandomData(random, a, iterations, maxWordLength, simple, true);
-  }
-
-  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {
     checkResetException(a, "best effort");
     long seed = random.nextLong();
     boolean useCharFilter = random.nextBoolean();
@@ -559,14 +533,14 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
     }
     boolean success = false;
     try {
-      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);
+      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, iw);
       // now test with multiple threads: note we do the EXACT same thing we did before in each thread,
       // so this should only really fail from another thread if it's an actual thread problem
       int numThreads = TestUtil.nextInt(random, 2, 4);
       final CountDownLatch startingGun = new CountDownLatch(1);
       AnalysisThread threads[] = new AnalysisThread[numThreads];
       for (int i = 0; i < threads.length; i++) {
-        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);
+        threads[i] = new AnalysisThread(seed, startingGun, a, iterations, maxWordLength, useCharFilter, simple, iw);
       }
       for (int i = 0; i < threads.length; i++) {
         threads[i].start();
@@ -597,7 +571,7 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
     }
   }
 
-  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple, boolean offsetsAreCorrect, RandomIndexWriter iw) throws IOException {
+  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple, RandomIndexWriter iw) throws IOException {
 
     final LineFileDocs docs = new LineFileDocs(random);
     Document doc = null;
@@ -622,11 +596,7 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
         case 1: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS); break;
         case 2: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS); break;
         default:
-          if (offsetsAreCorrect) {
-            ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-          } else {
-            ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
-          }
+          ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
       }
       currentField = field = new Field("dummy", bogus, ft);
       doc.add(currentField);
@@ -661,7 +631,7 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
         }
 
         try {
-          checkAnalysisConsistency(random, a, useCharFilter, text, offsetsAreCorrect, currentField);
+          checkAnalysisConsistency(random, a, useCharFilter, text, currentField);
           if (iw != null) {
             if (random.nextInt(7) == 0) {
               // pile up a multivalued field
@@ -720,14 +690,10 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
   }
 
   public static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text) throws IOException {
-    checkAnalysisConsistency(random, a, useCharFilter, text, true);
-  }
-
-  public static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect) throws IOException {
-    checkAnalysisConsistency(random, a, useCharFilter, text, offsetsAreCorrect, null);
+    checkAnalysisConsistency(random, a, useCharFilter, text, null);
   }
   
-  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {
+  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, Field field) throws IOException {
 
     if (VERBOSE) {
       System.out.println(Thread.currentThread().getName() + ": NOTE: BaseTokenStreamTestCase: get first token stream now text=" + text);
@@ -869,8 +835,7 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
                                 types.toArray(new String[types.size()]),
                                 toIntArray(positions),
                                 toIntArray(positionLengths),
-                                text.length(),
-                                offsetsAreCorrect);
+                                text.length());
     } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {
       // offset + pos + type
       assertTokenStreamContents(ts, 
@@ -880,8 +845,7 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
                                 types.toArray(new String[types.size()]),
                                 toIntArray(positions),
                                 null,
-                                text.length(),
-                                offsetsAreCorrect);
+                                text.length());
     } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {
       // offset + pos + posLength
       assertTokenStreamContents(ts, 
@@ -891,8 +855,7 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
                                 null,
                                 toIntArray(positions),
                                 toIntArray(positionLengths),
-                                text.length(),
-                                offsetsAreCorrect);
+                                text.length());
     } else if (posIncAtt != null && offsetAtt != null) {
       // offset + pos
       assertTokenStreamContents(ts, 
@@ -902,8 +865,7 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
                                 null,
                                 toIntArray(positions),
                                 null,
-                                text.length(),
-                                offsetsAreCorrect);
+                                text.length());
     } else if (offsetAtt != null) {
       // offset
       assertTokenStreamContents(ts, 
@@ -913,8 +875,7 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
                                 null,
                                 null,
                                 null,
-                                text.length(),
-                                offsetsAreCorrect);
+                                text.length());
     } else {
       // terms only
       assertTokenStreamContents(ts, 
@@ -1029,7 +990,7 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
    * @param expectedStrings all expected finite strings.
    */
   public static void assertGraphStrings(Analyzer analyzer, String text, String... expectedStrings) throws IOException {
-    checkAnalysisConsistency(random(), analyzer, true, text, true);
+    checkAnalysisConsistency(random(), analyzer, true, text);
     try (TokenStream tokenStream = analyzer.tokenStream("dummy", text)) {
       assertGraphStrings(tokenStream, expectedStrings);
     }
diff --git a/lucene/test-framework/src/test/org/apache/lucene/analysis/TestMockSynonymFilter.java b/lucene/test-framework/src/test/org/apache/lucene/analysis/TestMockSynonymFilter.java
index fb0d065..efb52ac 100644
--- a/lucene/test-framework/src/test/org/apache/lucene/analysis/TestMockSynonymFilter.java
+++ b/lucene/test-framework/src/test/org/apache/lucene/analysis/TestMockSynonymFilter.java
@@ -37,8 +37,7 @@ public class TestMockSynonymFilter extends BaseTokenStreamTestCase {
         new int[]{4, 4}, // end offset
         null,
         new int[]{1, 0}, // position increment
-        new int[]{1, 1}, // position length
-        true); // check that offsets are correct
+        new int[]{1, 1}); // position length
 
     assertAnalyzesTo(analyzer, "small dogs",
         new String[]{"small", "dogs", "dog"},
@@ -46,8 +45,7 @@ public class TestMockSynonymFilter extends BaseTokenStreamTestCase {
         new int[]{5, 10, 10}, // end offset
         null,
         new int[]{1, 1, 0},   // position increment
-        new int[]{1, 1, 1},   // position length
-        true); // check that offsets are correct
+        new int[]{1, 1, 1});  // position length
 
     assertAnalyzesTo(analyzer, "dogs running",
         new String[]{"dogs", "dog", "running"},
@@ -55,8 +53,7 @@ public class TestMockSynonymFilter extends BaseTokenStreamTestCase {
         new int[]{4, 4, 12}, // end offset
         null,
         new int[]{1, 0, 1},  // position increment
-        new int[]{1, 1, 1},  // position length
-        true); // check that offsets are correct
+        new int[]{1, 1, 1}); // position length
 
     assertAnalyzesTo(analyzer, "small dogs running",
         new String[]{"small", "dogs", "dog", "running"},
@@ -64,8 +61,7 @@ public class TestMockSynonymFilter extends BaseTokenStreamTestCase {
         new int[]{5, 10, 10, 18}, // end offset
         null,
         new int[]{1, 1, 0, 1},    // position increment
-        new int[]{1, 1, 1, 1},    // position length
-        true); // check that offsets are correct
+        new int[]{1, 1, 1, 1});   // position length
 
     assertAnalyzesTo(analyzer, "guinea",
         new String[]{"guinea"},
@@ -73,8 +69,7 @@ public class TestMockSynonymFilter extends BaseTokenStreamTestCase {
         new int[]{6}, // end offset
         null,
         new int[]{1}, // position increment
-        new int[]{1}, // position length
-        true); // check that offsets are correct
+        new int[]{1}); // position length
 
     assertAnalyzesTo(analyzer, "pig",
         new String[]{"pig"},
@@ -82,8 +77,7 @@ public class TestMockSynonymFilter extends BaseTokenStreamTestCase {
         new int[]{3}, // end offset
         null,
         new int[]{1}, // position increment
-        new int[]{1}, // position length
-        true); // check that offsets are correct
+        new int[]{1}); // position length
 
     assertAnalyzesTo(analyzer, "guinea pig",
         new String[]{"guinea", "cavy", "pig"},
@@ -91,8 +85,7 @@ public class TestMockSynonymFilter extends BaseTokenStreamTestCase {
         new int[]{6, 10, 10}, // end offset
         null,
         new int[]{1, 0, 1},   // position increment
-        new int[]{1, 2, 1},   // position length
-        true); // check that offsets are correct
+        new int[]{1, 2, 1});  // position length
 
     assertAnalyzesTo(analyzer, "guinea dogs",
         new String[]{"guinea", "dogs", "dog"},
@@ -100,8 +93,7 @@ public class TestMockSynonymFilter extends BaseTokenStreamTestCase {
         new int[]{6, 11, 11}, // end offset
         null,
         new int[]{1, 1, 0},   // position increment
-        new int[]{1, 1, 1},   // position length
-        true); // check that offsets are correct
+        new int[]{1, 1, 1});  // position length
 
     assertAnalyzesTo(analyzer, "dogs guinea",
         new String[]{"dogs", "dog", "guinea"},
@@ -109,8 +101,7 @@ public class TestMockSynonymFilter extends BaseTokenStreamTestCase {
         new int[]{4, 4, 11}, // end offset
         null,
         new int[]{1, 0, 1},  // position increment
-        new int[]{1, 1, 1},  // position length
-        true); // check that offsets are correct
+        new int[]{1, 1, 1}); // position length
 
     assertAnalyzesTo(analyzer, "dogs guinea pig",
         new String[]{"dogs", "dog", "guinea", "cavy", "pig"},
@@ -118,8 +109,7 @@ public class TestMockSynonymFilter extends BaseTokenStreamTestCase {
         new int[]{4, 4, 11, 15, 15}, // end offset
         null,
         new int[]{1, 0, 1, 0, 1},    // position increment
-        new int[]{1, 1, 1, 2, 1},    // position length
-        true); // check that offsets are correct
+        new int[]{1, 1, 1, 2, 1});   // position length
 
     assertAnalyzesTo(analyzer, "guinea pig dogs",
         new String[]{"guinea", "cavy", "pig", "dogs", "dog"},
@@ -127,8 +117,7 @@ public class TestMockSynonymFilter extends BaseTokenStreamTestCase {
         new int[]{6, 10, 10, 15, 15}, // end offset
         null,
         new int[]{1, 0, 1, 1, 0},     // position increment
-        new int[]{1, 2, 1, 1, 1},     // position length
-        true); // check that offsets are correct
+        new int[]{1, 2, 1, 1, 1});    // position length
 
     assertAnalyzesTo(analyzer, "small dogs and guinea pig running",
         new String[]{"small", "dogs", "dog", "and", "guinea", "cavy", "pig", "running"},
@@ -136,8 +125,7 @@ public class TestMockSynonymFilter extends BaseTokenStreamTestCase {
         new int[]{5, 10, 10, 14, 21, 25, 25, 33}, // end offset
         null,
         new int[]{1, 1, 0, 1, 1, 0, 1, 1},        // position increment
-        new int[]{1, 1, 1, 1, 1, 2, 1, 1},        // position length
-        true); // check that offsets are correct
+        new int[]{1, 1, 1, 1, 1, 2, 1, 1});       // position length
 
     assertAnalyzesTo(analyzer, "small guinea pig and dogs running",
         new String[]{"small", "guinea", "cavy", "pig", "and", "dogs", "dog", "running"},
@@ -145,7 +133,6 @@ public class TestMockSynonymFilter extends BaseTokenStreamTestCase {
         new int[]{5, 12, 16, 16, 20, 25, 25, 33}, // end offset
         null,
         new int[]{1, 1, 0, 1, 1, 1, 0, 1},        // position increment
-        new int[]{1, 1, 2, 1, 1, 1, 1, 1},        // position length
-        true); // check that offsets are correct
+        new int[]{1, 1, 2, 1, 1, 1, 1, 1});       // position length
   }
 }
