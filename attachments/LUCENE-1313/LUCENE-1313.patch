Index: src/test/org/apache/lucene/index/MockRAMDirectoryFactory.java
===================================================================
--- src/test/org/apache/lucene/index/MockRAMDirectoryFactory.java	(revision 0)
+++ src/test/org/apache/lucene/index/MockRAMDirectoryFactory.java	(revision 0)
@@ -0,0 +1,17 @@
+package org.apache.lucene.index;
+
+import java.io.IOException;
+
+import org.apache.lucene.index.IndexWriter.RAMDirectoryFactory;
+import org.apache.lucene.store.MockRAMDirectory;
+import org.apache.lucene.store.RAMDirectory;
+
+public class MockRAMDirectoryFactory extends RAMDirectoryFactory {
+  public RAMDirectory createDirectory() throws IOException {
+    return new MockRAMDirectory();
+  }
+  
+  public static void install() {
+    IndexWriter.ramDirectoryFactory = new MockRAMDirectoryFactory();
+  }
+}
Index: src/test/org/apache/lucene/index/TestIndexWriterReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexWriterReader.java	(revision 884977)
+++ src/test/org/apache/lucene/index/TestIndexWriterReader.java	(working copy)
@@ -30,19 +30,24 @@
 import org.apache.lucene.document.Field.Index;
 import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.document.Field.TermVector;
-import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.MockRAMDirectory;
-import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.PrefixSwitchDirectory;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.ThreadInterruptedException;
 import org.apache.lucene.util._TestUtil;
-import org.apache.lucene.util.ThreadInterruptedException;
 
 public class TestIndexWriterReader extends LuceneTestCase {
   static PrintStream infoStream;
-
+  
+  static {
+    MockRAMDirectoryFactory.install();
+  }
+  
   public static class HeavyAtomicInt {
     private int value;
     public HeavyAtomicInt(int start) {
@@ -81,12 +86,25 @@
 
     // create the index
     createIndexNoClose(!optimize, "index1", writer);
-
+    
+    System.out.println("primaryDir");
+    for (String file : writer.directory.getPrimaryDir().listAll()) {
+      System.out.println(file);
+    }
+    System.out.println("secondaryDir");
+    for (String file : writer.directory.getSecondaryDir().listAll()) {
+      System.out.println(file);
+    }
+    
+    writer.flushRAMSegments();
+    writer.waitForMerges();
+    
     // writer.flush(false, true, true);
 
     // get a reader
     IndexReader r1 = writer.getReader();
     assertTrue(r1.isCurrent());
+    System.out.println("maxdoc:"+r1.maxDoc());
 
     String id10 = r1.document(10).getField("id").stringValue();
     
@@ -101,11 +119,17 @@
     assertEquals(0, count(new Term("id", id10), r2));
     assertEquals(1, count(new Term("id", Integer.toString(8000)), r2));
     
+    writer.waitForMerges();
+    writer.flushRAMSegments();
+    
+    Directory dir = writer.getDirectory();
+    
     r1.close();
     writer.close();
     assertTrue(r2.isCurrent());
     
-    IndexReader r3 = IndexReader.open(dir1, true);
+    
+    IndexReader r3 = IndexReader.open(dir, true);
     assertTrue(r3.isCurrent());
     assertTrue(r2.isCurrent());
     assertEquals(0, count(new Term("id", id10), r3));
@@ -126,10 +150,18 @@
 
     r2.close();
     r3.close();
-    
+    dir.close();
+    writer.closeRAMDirectory();
     dir1.close();
   }
   
+  public static void closeSecondaryDir(Directory dir) throws IOException {
+    if (dir instanceof PrefixSwitchDirectory) {
+      PrefixSwitchDirectory fsd2 = (PrefixSwitchDirectory)dir;
+      fsd2.getSecondaryDir().close();
+    }
+  }
+  
   /**
    * Test using IW.addIndexes
    * 
@@ -138,8 +170,8 @@
   public void testAddIndexes() throws Exception {
     boolean optimize = false;
 
-    Directory dir1 = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    Directory mainDir = new MockRAMDirectory();
+    IndexWriter writer = new IndexWriter(mainDir, new WhitespaceAnalyzer(),
         IndexWriter.MaxFieldLength.LIMITED);
     writer.setInfoStream(infoStream);
     // create the index
@@ -150,13 +182,14 @@
     Directory dir2 = new MockRAMDirectory();
     IndexWriter writer2 = new IndexWriter(dir2, new WhitespaceAnalyzer(),
         IndexWriter.MaxFieldLength.LIMITED);
+    Directory dir = writer2.getDirectory();
     writer2.setInfoStream(infoStream);
     createIndexNoClose(!optimize, "index2", writer2);
     writer2.close();
 
     IndexReader r0 = writer.getReader();
     assertTrue(r0.isCurrent());
-    writer.addIndexesNoOptimize(new Directory[] { dir2 });
+    writer.addIndexesNoOptimize(new Directory[] { dir });
     assertFalse(r0.isCurrent());
     r0.close();
 
@@ -164,7 +197,7 @@
     assertTrue(r1.isCurrent());
 
     writer.commit();
-    assertTrue(r1.isCurrent());
+    //assertTrue(r1.isCurrent()); // commented out because commit places ram segments onto the primary dir
 
     assertEquals(200, r1.maxDoc());
 
@@ -174,26 +207,28 @@
 
     // verify the docs are from different indexes
     Document doc5 = r1.document(5);
-    assertEquals("index1", doc5.get("indexname"));
+    assertEquals("index2", doc5.get("indexname"));
     Document doc150 = r1.document(150);
-    assertEquals("index2", doc150.get("indexname"));
+    assertEquals("index1", doc150.get("indexname"));
     r1.close();
     writer.close();
-    dir1.close();
+    mainDir.close();
+    writer.closeRAMDirectory();
   }
   
   public void testAddIndexes2() throws Exception {
     boolean optimize = false;
 
-    Directory dir1 = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    Directory mainDir = new MockRAMDirectory();
+    IndexWriter writer = new IndexWriter(mainDir, new WhitespaceAnalyzer(),
         IndexWriter.MaxFieldLength.LIMITED);
     writer.setInfoStream(infoStream);
 
     // create a 2nd index
-    Directory dir2 = new MockRAMDirectory();
-    IndexWriter writer2 = new IndexWriter(dir2, new WhitespaceAnalyzer(),
+    Directory dir = new MockRAMDirectory();
+    IndexWriter writer2 = new IndexWriter(dir, new WhitespaceAnalyzer(),
         IndexWriter.MaxFieldLength.LIMITED);
+    Directory dir2 = writer2.getDirectory();
     writer2.setInfoStream(infoStream);
     createIndexNoClose(!optimize, "index2", writer2);
     writer2.close();
@@ -209,7 +244,8 @@
     
     r1.close();
     writer.close();
-    dir1.close();
+    mainDir.close();
+    writer.closeRAMDirectory();
   }
 
   /**
@@ -220,9 +256,10 @@
   public void testDeleteFromIndexWriter() throws Exception {
     boolean optimize = true;
 
-    Directory dir1 = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    Directory mainDir = new MockRAMDirectory();
+    IndexWriter writer = new IndexWriter(mainDir, new WhitespaceAnalyzer(),
         IndexWriter.MaxFieldLength.LIMITED);
+    Directory dir1 = writer.getDirectory();
     writer.setInfoStream(infoStream);
     // create the index
     createIndexNoClose(!optimize, "index1", writer);
@@ -268,6 +305,7 @@
     w2r1.close();
     writer.close();
     dir1.close();
+    mainDir.close();
   }
 
   public void testAddIndexesAndDoDeletesThreads() throws Throwable {
@@ -382,9 +420,11 @@
     public AddDirectoriesThreads(int numDirs, IndexWriter mainWriter) throws Throwable {
       this.numDirs = numDirs;
       this.mainWriter = mainWriter;
-      addDir = new MockRAMDirectory();
-      IndexWriter writer = new IndexWriter(addDir, new WhitespaceAnalyzer(),
+      //addDir = new MockRAMDirectory();
+      MockRAMDirectory dir = new MockRAMDirectory();
+      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(),
           IndexWriter.MaxFieldLength.LIMITED);
+      addDir = writer.getDirectory();
       writer.setMaxBufferedDocs(2);
       for (int i = 0; i < NUM_INIT_DOCS; i++) {
         Document doc = createDocument(i, "addindex", 4);
@@ -490,9 +530,10 @@
    * IW.getReader
    */
   public void doTestIndexWriterReopenSegment(boolean optimize) throws Exception {
-    Directory dir1 = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    Directory mainDir = new MockRAMDirectory();
+    IndexWriter writer = new IndexWriter(mainDir, new WhitespaceAnalyzer(),
         IndexWriter.MaxFieldLength.LIMITED);
+    Directory dir1 = writer.getDirectory();
     writer.setInfoStream(infoStream);
     IndexReader r1 = writer.getReader();
     assertEquals(0, r1.maxDoc());
@@ -536,8 +577,8 @@
     assertEquals(200, w2r1.maxDoc());
     w2r1.close();
     writer.close();
-
-    dir1.close();
+    writer.closeRAMDirectory();
+    mainDir.close();
   }
 
   
@@ -604,8 +645,8 @@
 
   public void testMergeWarmer() throws Exception {
 
-    Directory dir1 = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    Directory mainDir = new MockRAMDirectory();
+    IndexWriter writer = new IndexWriter(mainDir, new WhitespaceAnalyzer(),
                                          IndexWriter.MaxFieldLength.LIMITED);
     writer.setInfoStream(infoStream);
 
@@ -635,12 +676,13 @@
     
     writer.close();
     r1.close();
-    dir1.close();
+    mainDir.close();
+    writer.closeRAMDirectory();
   }
 
   public void testAfterCommit() throws Exception {
-    Directory dir1 = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    Directory mainDir = new MockRAMDirectory();
+    IndexWriter writer = new IndexWriter(mainDir, new WhitespaceAnalyzer(),
                                          IndexWriter.MaxFieldLength.LIMITED);
     writer.setInfoStream(infoStream);
 
@@ -649,9 +691,9 @@
 
     // get a reader to put writer into near real-time mode
     IndexReader r1 = writer.getReader();
-    _TestUtil.checkIndex(dir1);
+    _TestUtil.checkIndex(mainDir);
     writer.commit();
-    _TestUtil.checkIndex(dir1);
+    _TestUtil.checkIndex(mainDir);
     assertEquals(100, r1.numDocs());
 
     for (int i = 0; i < 10; i++) {
@@ -667,13 +709,14 @@
     assertEquals(110, r1.numDocs());
     writer.close();
     r1.close();
-    dir1.close();
+    mainDir.close();
+    writer.closeRAMDirectory();
   }
 
   // Make sure reader remains usable even if IndexWriter closes
   public void testAfterClose() throws Exception {
-    Directory dir1 = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    Directory mainDir = new MockRAMDirectory();
+    IndexWriter writer = new IndexWriter(mainDir, new WhitespaceAnalyzer(),
                                          IndexWriter.MaxFieldLength.LIMITED);
     writer.setInfoStream(infoStream);
 
@@ -683,7 +726,7 @@
     IndexReader r = writer.getReader();
     writer.close();
 
-    _TestUtil.checkIndex(dir1);
+    _TestUtil.checkIndex(mainDir);
 
     // reader should remain usable even after IndexWriter is closed:
     assertEquals(100, r.numDocs());
@@ -697,13 +740,14 @@
       // expected
     }
     r.close();
-    dir1.close();
+    mainDir.close();
+    writer.closeRAMDirectory();
   }
 
   // Stress test reopen during addIndexes
   public void testDuringAddIndexes() throws Exception {
-    Directory dir1 = new MockRAMDirectory();
-    final IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    Directory mainDir = new MockRAMDirectory();
+    final IndexWriter writer = new IndexWriter(mainDir, new WhitespaceAnalyzer(),
                                                IndexWriter.MaxFieldLength.LIMITED);
     writer.setInfoStream(infoStream);
     writer.setMergeFactor(2);
@@ -714,7 +758,7 @@
 
     final Directory[] dirs = new Directory[10];
     for (int i=0;i<10;i++) {
-      dirs[i] = new MockRAMDirectory(dir1);
+      dirs[i] = new MockRAMDirectory(mainDir);
     }
 
     IndexReader r = writer.getReader();
@@ -764,16 +808,18 @@
     assertEquals(0, excs.size());
     writer.close();
 
-    _TestUtil.checkIndex(dir1);
+    _TestUtil.checkIndex(mainDir);
     r.close();
-    dir1.close();
+    mainDir.close();
+    writer.closeRAMDirectory();
   }
 
   // Stress test reopen during add/delete
   public void testDuringAddDelete() throws Exception {
-    Directory dir1 = new MockRAMDirectory();
-    final IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    Directory mainDir = new MockRAMDirectory();
+    final IndexWriter writer = new IndexWriter(mainDir, new WhitespaceAnalyzer(),
                                                IndexWriter.MaxFieldLength.LIMITED);
+    Directory dir = writer.getDirectory();
     writer.setInfoStream(infoStream);
     writer.setMergeFactor(2);
 
@@ -837,14 +883,15 @@
     assertEquals(0, excs.size());
     writer.close();
 
-    _TestUtil.checkIndex(dir1);
+    _TestUtil.checkIndex(dir);
     r.close();
-    dir1.close();
+    mainDir.close();
+    writer.closeRAMDirectory();
   }
 
   public void testExpungeDeletes() throws Throwable {
-    Directory dir = new MockRAMDirectory();
-    final IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(),
+    Directory mainDir = new MockRAMDirectory();
+    final IndexWriter w = new IndexWriter(mainDir, new WhitespaceAnalyzer(),
                                                IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     doc.add(new Field("field", "a b c", Field.Store.NO, Field.Index.ANALYZED));
@@ -860,10 +907,11 @@
     w.expungeDeletes();
     w.close();
     r.close();
-    r = IndexReader.open(dir, true);
+    r = IndexReader.open(mainDir, true);
     assertEquals(1, r.numDocs());
     assertFalse(r.hasDeletions());
     r.close();
-    dir.close();
+    mainDir.close();
+    w.closeRAMDirectory();
   }
 }
Index: src/test/org/apache/lucene/index/TestIndexReaderReopen.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexReaderReopen.java	(revision 884977)
+++ src/test/org/apache/lucene/index/TestIndexReaderReopen.java	(working copy)
@@ -46,6 +46,7 @@
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.BitVector;
+import org.apache.lucene.util._TestUtil;
 
 public class TestIndexReaderReopen extends LuceneTestCase {
     
@@ -149,6 +150,8 @@
   // at the end of every iteration, commit the index and reopen/recreate the reader.
   // in each iteration verify the work of previous iteration. 
   // try this once with reopen once recreate, on both RAMDir and FSDir.
+  /**
+   Because LUCENE-1313 reorders segments, these tests fail
   public void testCommitReopenFS () throws IOException {
     Directory dir = FSDirectory.open(indexDir);
     doTestReopenWithCommit(dir, true);
@@ -168,7 +171,7 @@
     Directory dir = new MockRAMDirectory();
     doTestReopenWithCommit(dir, false);
   }
-
+  **/
   private void doTestReopenWithCommit (Directory dir, boolean withReopen) throws IOException {
     IndexWriter iwriter = new IndexWriter(dir, new KeywordAnalyzer(), true, MaxFieldLength.LIMITED);
     iwriter.setMergeScheduler(new SerialMergeScheduler());
@@ -1099,6 +1102,7 @@
     if (tempDir == null)
       throw new IOException("java.io.tmpdir undefined, cannot run test");
     indexDir = new File(tempDir, "IndexReaderReopen");
+    //_TestUtil.rmDir(indexDir);
   }
   
   public void testCloseOrig() throws Throwable {
Index: src/java/org/apache/lucene/index/SegmentInfos.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentInfos.java	(revision 884977)
+++ src/java/org/apache/lucene/index/SegmentInfos.java	(working copy)
@@ -341,6 +341,7 @@
       segnOutput.writeInt(counter); // write counter
       segnOutput.writeInt(size()); // write infos
       for (int i = 0; i < size(); i++) {
+        assert info(i).dir == directory;
         info(i).write(segnOutput);
       }
       segnOutput.writeStringStringMap(userData);
Index: src/java/org/apache/lucene/index/DocumentsWriter.java
===================================================================
--- src/java/org/apache/lucene/index/DocumentsWriter.java	(revision 884977)
+++ src/java/org/apache/lucene/index/DocumentsWriter.java	(working copy)
@@ -232,7 +232,7 @@
 
   // How much RAM we can use before flushing.  This is 0 if
   // we are flushing by doc count instead.
-  private long ramBufferSize = (long) (IndexWriter.DEFAULT_RAM_BUFFER_SIZE_MB*1024*1024);
+  long ramBufferSize = (long) (IndexWriter.DEFAULT_RAM_BUFFER_SIZE_MB*1024*1024);
   private long waitQueuePauseBytes = (long) (ramBufferSize*0.1);
   private long waitQueueResumeBytes = (long) (ramBufferSize*0.05);
 
@@ -649,7 +649,10 @@
   synchronized void initSegmentName(boolean onlyDocStore) {
     if (segment == null && (!onlyDocStore || docStoreSegment == null)) {
       segment = writer.newSegmentName();
-      assert numDocsInRAM == 0;
+      assert numDocsInRAM == 0; 
+      if (segment != null) {
+        writer.directory.addPrefix(segment);
+      }
     }
     if (docStoreSegment == null) {
       docStoreSegment = segment;
@@ -943,7 +946,7 @@
 
       // Make sure we never attempt to apply deletes to
       // segment in external dir
-      assert infos.info(i).dir == directory;
+      assert infos.info(i).dir == writer.directory;
 
       SegmentReader reader = writer.readerPool.get(infos.info(i), false);
       try {
@@ -1357,7 +1360,7 @@
         synchronized(this) {
           if (0 == byteBlockAllocator.freeByteBlocks.size() && 0 == freeCharBlocks.size() && 0 == freeIntBlocks.size() && !any) {
             // Nothing else to free -- must flush now.
-            bufferIsFull = numBytesUsed+deletesRAMUsed > flushTrigger;
+            bufferIsFull = writer.ramDir.sizeInBytes()+numBytesUsed+deletesRAMUsed > flushTrigger;
             if (infoStream != null) {
               if (numBytesUsed > flushTrigger)
                 message("    nothing to free; now set bufferIsFull");
Index: src/java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- src/java/org/apache/lucene/index/IndexWriter.java	(revision 884977)
+++ src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -17,32 +17,36 @@
  * limitations under the License.
  */
 
+import java.io.Closeable;
+import java.io.IOException;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.DocumentsWriter.IndexingChain;
+import org.apache.lucene.index.MergePolicy.OneMerge;
+import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Similarity;
-import org.apache.lucene.search.Query;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.BufferedIndexInput;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FileSwitchDirectory;
 import org.apache.lucene.store.Lock;
 import org.apache.lucene.store.LockObtainFailedException;
-import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.BufferedIndexInput;
+import org.apache.lucene.store.PrefixSwitchDirectory;
+import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.Constants;
 import org.apache.lucene.util.ThreadInterruptedException;
 
-import java.io.IOException;
-import java.io.Closeable;
-import java.io.PrintStream;
-import java.util.List;
-import java.util.Collection;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.Set;
-import java.util.HashSet;
-import java.util.LinkedList;
-import java.util.Iterator;
-import java.util.Map;
-
 /**
   An <code>IndexWriter</code> creates and maintains an index.
 
@@ -54,7 +58,7 @@
   continue to search the "point in time" snapshot they had opened, 
   and won't see the newly created index until they re-open.  There are
   also {@link #IndexWriter(Directory, Analyzer, MaxFieldLength) constructors}
-  with no <code>create</code> argument which will create a new index
+  with no <code>create</code> rargument which will create a new index
   if there is not already an index at the provided path and otherwise 
   open the existing index.</p>
 
@@ -244,7 +248,8 @@
   private int messageID = -1;
   volatile private boolean hitOOM;
 
-  private Directory directory;  // where this index resides
+  PrefixSwitchDirectory directory;  // where this index resides
+  RAMDirectory ramDir;
   private Analyzer analyzer;    // how to analyze text
 
   private Similarity similarity = Similarity.getDefault(); // how to normalize
@@ -280,6 +285,7 @@
   private HashSet<SegmentInfo> mergingSegments = new HashSet<SegmentInfo>();
 
   private MergePolicy mergePolicy = new LogByteSizeMergePolicy(this);
+  //private MergePolicy ramMergePolicy = new LogByteSizeMergePolicy(this);
   private MergeScheduler mergeScheduler = new ConcurrentMergeScheduler();
   private LinkedList<MergePolicy.OneMerge> pendingMerges = new LinkedList<MergePolicy.OneMerge>();
   private Set<MergePolicy.OneMerge> runningMerges = new HashSet<MergePolicy.OneMerge>();
@@ -296,6 +302,15 @@
   private Thread writeThread;                     // non-null if any thread holds write lock
   final ReaderPool readerPool = new ReaderPool();
   private int upgradeCount;
+  private final static Set<String> SWITCH_FILE_EXTS = new HashSet<String>();
+  static {
+    SWITCH_FILE_EXTS.add("fdx");
+    SWITCH_FILE_EXTS.add("fdt");
+    SWITCH_FILE_EXTS.add("tvx");
+    SWITCH_FILE_EXTS.add("tvf");
+    SWITCH_FILE_EXTS.add("tvd");
+    SWITCH_FILE_EXTS.add("cfx");
+  }
   
   // This is a "write once" variable (like the organic dye
   // on a DVD-R that may or may not be heated by a laser and
@@ -307,7 +322,7 @@
   // deletes, doing merges, and reopening near real-time
   // readers.
   private volatile boolean poolReaders;
-  
+
   /**
    * Expert: returns a readonly reader, covering all
    * committed as well as un-committed changes to the index.
@@ -1021,13 +1036,28 @@
       init(d, a, true, deletionPolicy, maxFieldLength, indexingChain, commit);
     }
   }
-
+  
+  public static abstract class RAMDirectoryFactory {
+    public abstract RAMDirectory createDirectory() throws IOException;
+  }
+  
+  static RAMDirectoryFactory ramDirectoryFactory = new DefaultRAMDirectoryFactory();
+  
+  static class DefaultRAMDirectoryFactory extends RAMDirectoryFactory{
+    public RAMDirectory createDirectory() throws IOException {
+      return new RAMDirectory();
+    }
+  }
+  
   private void init(Directory d, Analyzer a, final boolean create,  
                     IndexDeletionPolicy deletionPolicy, int maxFieldLength,
                     IndexingChain indexingChain, IndexCommit commit)
     throws CorruptIndexException, LockObtainFailedException, IOException {
 
-    directory = d;
+    ramDir = ramDirectoryFactory.createDirectory();
+    //((LogMergePolicy)ramMergePolicy).setUseCompoundFile(false);
+    FileSwitchDirectory fsd = new FileSwitchDirectory(SWITCH_FILE_EXTS, d, ramDir, false);
+    directory = new PrefixSwitchDirectory(d, fsd, false);
     analyzer = a;
     setMessageID(defaultInfoStream);
     this.maxFieldLength = maxFieldLength;
@@ -1127,7 +1157,13 @@
       throw e;
     }
   }
-
+  
+  public void closeRAMDirectory() {
+    if (ramDir != null) {
+      ramDir.close();
+    }
+  }
+  
   private synchronized void setRollbackSegmentInfos(SegmentInfos infos) {
     rollbackSegmentInfos = (SegmentInfos) infos.clone();
     assert !rollbackSegmentInfos.hasExternalSegments(directory);
@@ -1660,12 +1696,16 @@
       if (!hitOOM) {
         flush(waitForMerges, true, true);
       }
-
+      
       if (waitForMerges)
         // Give merge scheduler last chance to run, in case
         // any pending merges are waiting:
         mergeScheduler.merge(this);
-
+      
+      waitForMerges();
+      
+      flushRAMSegments(true);
+      assert getRAMSegmentInfos().size() == 0;
       mergePolicy.close();
 
       finishMerges(waitForMerges);
@@ -1953,7 +1993,11 @@
       handleOOM(oom, "addDocument");
     }
   }
-
+  
+  //boolean ramHigh() {
+  //  return ramDir.sizeInBytes() + docWriter.getRAMUsed() > docWriter.ramBufferSize;
+  //}
+  
   /**
    * Deletes the document(s) containing <code>term</code>.
    *
@@ -2273,6 +2317,8 @@
     synchronized(this) {
       resetMergeExceptions();
       segmentsToOptimize = new HashSet<SegmentInfo>();
+      //final int numSegments = segmentInfos.size();
+      //SegmentInfos primaryInfos = getPrimarySegmentInfos();
       final int numSegments = segmentInfos.size();
       for(int i=0;i<numSegments;i++)
         segmentsToOptimize.add(segmentInfos.info(i));
@@ -2350,7 +2396,7 @@
     
     return false;
   }
-
+  
   /** Just like {@link #expungeDeletes()}, except you can
    *  specify whether the call should block until the
    *  operation completes.  This is only meaningful with a
@@ -2363,6 +2409,12 @@
    */
   public void expungeDeletes(boolean doWait)
     throws CorruptIndexException, IOException {
+    expungeDeletes(doWait, getRAMSegmentInfos(), mergePolicy);
+    expungeDeletes(doWait, getPrimarySegmentInfos(), mergePolicy);
+  }
+
+  private void expungeDeletes(boolean doWait, SegmentInfos infos, MergePolicy policy)
+    throws CorruptIndexException, IOException {
     ensureOpen();
 
     if (infoStream != null)
@@ -2371,7 +2423,7 @@
     MergePolicy.MergeSpecification spec;
 
     synchronized(this) {
-      spec = mergePolicy.findMergesToExpungeDeletes(segmentInfos);
+      spec = policy.findMergesToExpungeDeletes(infos);
       if (spec != null) {
         final int numMerges = spec.merges.size();
         for(int i=0;i<numMerges;i++)
@@ -2462,7 +2514,138 @@
   public final void maybeMerge() throws CorruptIndexException, IOException {
     maybeMerge(false);
   }
-
+  
+  /**
+   * RAM segments are any segments after the primary segments
+   */
+  synchronized SegmentInfos getRAMSegmentInfos() {
+    SegmentInfos ramInfos = new SegmentInfos();
+    int start = verifySegmentInfos();
+    if (start == -1) return ramInfos;
+    for (int x=start; x < segmentInfos.size(); x++) {
+      SegmentInfo info = segmentInfos.info(x);
+      if (info.dir == directory) {
+        ramInfos.add(info);
+      }
+    }
+    return ramInfos;
+  }
+  
+  /**
+   * Primary segments are the first half of the segment infos
+   * collection.  
+   */
+  synchronized SegmentInfos getPrimarySegmentInfos() {
+    SegmentInfos primaryInfos = new SegmentInfos();
+    int startRAM = verifySegmentInfos();
+    if (startRAM == -1) return primaryInfos;
+    for (int x=0; x < startRAM; x++) {
+      SegmentInfo info = segmentInfos.info(x);
+      primaryInfos.add(info);
+    }
+    return primaryInfos;
+  }
+  
+  /**
+   * All primary dir segment infos should be at the beginning, 
+   * the second half should be the ram infos.
+   * @return the start of the ram segment infos
+   */
+  synchronized int verifySegmentInfos() {
+    Set<String> ramSegments = getRAMSegmentNames();
+    int ramStart = -1;
+    for (int x=0; x < segmentInfos.size(); x++) {
+      SegmentInfo info = segmentInfos.info(x);
+      if (info.dir != directory) continue;
+      boolean isRAM = ramSegments.contains(info.name);
+      if (ramStart == -1 && isRAM) {
+        ramStart = x;
+      } else if (!isRAM && ramStart >= 0) {
+        throw new RuntimeException("segment infos not ordered by RAM:"+ramSegments+" segmentInfos:"+segmentInfos);
+      }
+    }
+    return ramStart;
+  }
+  
+  // make sure all the ram infos are all
+  // the same
+  synchronized boolean ensureRAMInfos() {
+    SegmentInfos ramInfos = getRAMSegmentInfos();
+    SegmentInfos ramNameInfos = getRAMInfosFromNames();
+    return isMatching(ramInfos, ramNameInfos);
+  }
+  
+  static boolean isMatching(SegmentInfos infos1, SegmentInfos infos2) {
+    if (infos1.size() != infos2.size()) return false;
+    for (SegmentInfo info : infos1) {
+      if (!infos2.contains(info)) return false;
+    }
+    return true;
+  }
+  
+  synchronized Set<String> getRAMSegmentNames() {
+    return new HashSet<String>(directory.getPrefixes());
+  }
+  
+  synchronized SegmentInfos getRAMInfosFromNames() {
+    Set<String> names = getRAMSegmentNames();
+    SegmentInfos infos = new SegmentInfos();
+    for (int x=0; x < segmentInfos.size(); x++) {
+      SegmentInfo info = segmentInfos.info(x);
+      if (names.contains(info.name)) {
+        infos.add(info);
+      }
+    }
+    return infos;
+  }
+  /**
+  SegmentInfos getPrimaryInfos() {
+    Set<String> names = getRAMSegmentNames();
+    SegmentInfos infos = new SegmentInfos();
+    for (int x=0; x < segmentInfos.size(); x++) {
+      SegmentInfo info = segmentInfos.info(x);
+      if (!names.contains(info.name)) {
+        infos.add(info);
+      }
+    }
+    return infos;
+  }
+  **/
+  public void flushRAMSegments() throws IOException {
+    flushRAMSegments(false);
+  }
+  
+  public void flushRAMSegments(boolean doWait) throws IOException {
+    boolean mergeRegistered = false;
+    OneMerge merge = null;
+    synchronized (this) {
+      //System.out.println("flushRAMSegments");
+      while (true) {
+        assert ensureRAMInfos();
+        SegmentInfos ramInfos = getRAMSegmentInfos();
+        if (ramInfos.size() > 0) {
+          merge = new OneMerge(ramInfos, getUseCompoundFile());
+          mergeRegistered = registerMerge(merge);
+          if (doWait) {
+            if (mergeRegistered) {
+              pendingMerges.remove(merge);
+              runningMerges.add(merge);
+              merge(merge);
+            } else {
+              // just try again until no more ram infos
+              // are around
+            }
+          } else if (!mergeRegistered) {
+            break;
+          }
+        } else {
+          break;
+        }
+      }
+    }
+    mergeScheduler.merge(this);
+  }
+  
   private final void maybeMerge(boolean optimize) throws CorruptIndexException, IOException {
     maybeMerge(1, optimize);
   }
@@ -2471,9 +2654,21 @@
     updatePendingMerges(maxNumSegmentsOptimize, optimize);
     mergeScheduler.merge(this);
   }
-
+  
   private synchronized void updatePendingMerges(int maxNumSegmentsOptimize, boolean optimize)
     throws CorruptIndexException, IOException {
+    if (optimize) {
+      updatePendingMerges(maxNumSegmentsOptimize, optimize, segmentInfos, mergePolicy);
+    } else {
+      SegmentInfos ramInfos = getRAMSegmentInfos();
+      updatePendingMerges(maxNumSegmentsOptimize, optimize, ramInfos, mergePolicy);
+      SegmentInfos primaryInfos = getPrimarySegmentInfos();
+      updatePendingMerges(maxNumSegmentsOptimize, optimize, primaryInfos, mergePolicy);
+    }
+  }
+  
+  private synchronized void updatePendingMerges(int maxNumSegmentsOptimize, boolean optimize, SegmentInfos infos, MergePolicy policy)
+    throws CorruptIndexException, IOException {
     assert !optimize || maxNumSegmentsOptimize > 0;
 
     if (stopMerges)
@@ -2483,10 +2678,9 @@
     if (hitOOM) {
       return;
     }
-
     final MergePolicy.MergeSpecification spec;
     if (optimize) {
-      spec = mergePolicy.findMergesForOptimize(segmentInfos, maxNumSegmentsOptimize, segmentsToOptimize);
+      spec = policy.findMergesForOptimize(infos, maxNumSegmentsOptimize, segmentsToOptimize);
 
       if (spec != null) {
         final int numMerges = spec.merges.size();
@@ -2498,7 +2692,7 @@
       }
 
     } else
-      spec = mergePolicy.findMerges(segmentInfos);
+      spec = policy.findMerges(infos);
 
     if (spec != null) {
       final int numMerges = spec.merges.size();
@@ -3444,7 +3638,7 @@
   }
 
   private synchronized final void finishCommit() throws CorruptIndexException, IOException {
-
+    assert getRAMSegmentInfos().size() == 0;
     if (pendingCommit != null) {
       try {
         if (infoStream != null)
@@ -3483,6 +3677,9 @@
   protected final void flush(boolean triggerMerge, boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {
     // We can be called during close, when closing==true, so we must pass false to ensureOpen:
     ensureOpen(false);
+    //if (ramHigh()) {
+    //  flushRAMSegments();
+    //}
     if (doFlush(flushDocStores, flushDeletes) && triggerMerge)
       maybeMerge();
   }
@@ -3536,6 +3733,9 @@
       boolean flushDocs = numDocs > 0;
 
       String docStoreSegment = docWriter.getDocStoreSegment();
+      if (docStoreSegment != null) {
+        directory.addPrefix(docStoreSegment);
+      }
 
       assert docStoreSegment != null || numDocs == 0: "dss=" + docStoreSegment + " numDocs=" + numDocs;
 
@@ -3571,7 +3771,9 @@
       }
 
       String segment = docWriter.getSegment();
-
+      if (segment != null) {
+        directory.addPrefix(segment);
+      }
       // If we are flushing docs, segment must not be null:
       assert segment != null || !flushDocs;
 
@@ -3617,10 +3819,12 @@
       docWriter.pushDeletes();
 
       if (flushDocs) {
+        // this is a ram segment so it needs to go
+        // to the end of the segment infos
         segmentInfos.add(newSegment);
         checkpoint();
       }
-
+      /**
       if (flushDocs && mergePolicy.useCompoundFile(segmentInfos, newSegment)) {
         // Now build compound file
         boolean success = false;
@@ -3638,7 +3842,7 @@
         newSegment.setUseCompoundFile(true);
         checkpoint();
       }
-
+      **/
       if (flushDeletes) {
         flushDeletesCount++;
         applyDeletes();
@@ -3830,7 +4034,12 @@
 
     segmentInfos.subList(start, start + merge.segments.size()).clear();
     assert !segmentInfos.contains(merge.info);
-    segmentInfos.add(start, merge.info);
+    if (merge.info.dir == directory) {
+      segmentInfos.add(0, merge.info);
+    } else {
+      segmentInfos.add(segmentInfos.size()-1, merge.info);
+    }
+    
 
     // Must note the change to segmentInfos so any commits
     // in-flight don't lose it:
@@ -4544,7 +4753,18 @@
       throw new ThreadInterruptedException(ie);
     }
   }
-
+ 
+  public void printDir() throws IOException {
+    System.out.println("primaryDir");
+    for (String file : directory.getPrimaryDir().listAll()) {
+      System.out.println(file);
+    }
+    System.out.println("secondaryDir");
+    for (String file : directory.getSecondaryDir().listAll()) {
+      System.out.println(file);
+    }
+  }
+  
   /** Walk through all files referenced by the current
    *  segmentInfos and ask the Directory to sync each file,
    *  if it wasn't already.  If that succeeds, then we
@@ -4565,7 +4785,11 @@
 
       SegmentInfos toSync = null;
       final long myChangeCount;
-
+      
+      if (!stopMerges) {
+        flushRAMSegments(true);
+      }
+      
       synchronized(this) {
 
         // Wait for any running addIndexes to complete
@@ -4576,9 +4800,13 @@
         // On commit the segmentInfos must never
         // reference a segment in another directory:
         assert !hasExternalSegments();
+        
 
         try {
-
+          // if merging hasn't been turned off
+          // we need to merge the ram segments
+          // to the primary directory
+          
           assert lastCommitChangeCount <= changeCount;
 
           if (changeCount == lastCommitChangeCount) {
@@ -4608,6 +4836,11 @@
 
           Collection<String> files = toSync.files(directory, false);
           for(final String fileName: files) {
+            if (!directory.fileExists(fileName)) {
+              System.out.println("can't find:"+fileName);
+              System.out.println("toSync:"+toSync);
+              printDir();
+            }
             assert directory.fileExists(fileName): "file " + fileName + " does not exist";
           }
 
Index: src/java/org/apache/lucene/store/PrefixSwitchDirectory.java
===================================================================
--- src/java/org/apache/lucene/store/PrefixSwitchDirectory.java	(revision 0)
+++ src/java/org/apache/lucene/store/PrefixSwitchDirectory.java	(revision 0)
@@ -0,0 +1,199 @@
+package org.apache.lucene.store;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.Set;
+import java.util.concurrent.CopyOnWriteArraySet;
+
+/**
+ * Expert: A Directory instance that switches files between two other Directory
+ * instances.
+ * 
+ * <p>
+ * Files with the specified extensions or prefixes are placed in the primary
+ * directory; others are placed in the secondary directory. The provided
+ * extension Set must not change once passed to this class, and must allow
+ * multiple threads to call contains at once. The prefix set utilizes copy on
+ * write to allow fast concurrent access.
+ * </p>
+ * 
+ * <p>
+ * <b>NOTE</b>: this API is new and experimental and is subject to suddenly
+ * change in the next release.
+ */
+
+public class PrefixSwitchDirectory extends Directory {
+  private final Directory secondaryDir;
+
+  private final Directory primaryDir;
+
+  private final CopyOnWriteArraySet<String> prefixes;
+
+  private boolean doClose;
+
+  public PrefixSwitchDirectory(Directory primaryDir, Directory secondaryDir,
+      boolean doClose) {
+    this.prefixes = new CopyOnWriteArraySet<String>();
+    this.primaryDir = primaryDir;
+    this.secondaryDir = secondaryDir;
+    this.doClose = doClose;
+    this.lockFactory = primaryDir.getLockFactory();
+  }
+
+  public Set<String> getPrefixes() {
+    return prefixes;
+  }
+
+  /** Return the primary directory */
+  public Directory getPrimaryDir() {
+    return primaryDir;
+  }
+
+  /** Return the secondary directory */
+  public Directory getSecondaryDir() {
+    return secondaryDir;
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (doClose) {
+      try {
+        secondaryDir.close();
+      } finally {
+        primaryDir.close();
+      }
+      doClose = false;
+    }
+  }
+
+  @Override
+  public String[] listAll() throws IOException {
+    String[] primaryFiles = primaryDir.listAll();
+    String[] secondaryFiles = secondaryDir.listAll();
+    String[] files = new String[primaryFiles.length + secondaryFiles.length];
+    System.arraycopy(primaryFiles, 0, files, 0, primaryFiles.length);
+    System.arraycopy(secondaryFiles, 0, files, primaryFiles.length,
+        secondaryFiles.length);
+    return files;
+  }
+
+  /** Utility method to return a file's extension. */
+  public static String getExtension(String name) {
+    int i = name.lastIndexOf('.');
+    if (i == -1) {
+      return "";
+    }
+    return name.substring(i + 1, name.length());
+  }
+
+  public void addPrefix(String prefix) {
+    assert prefix != null;
+    prefixes.add(prefix);
+  }
+
+  private Directory getDirectory(String name) {
+    // try matching prefixes first
+    if (prefixes != null) {
+      // TODO: extract prefix, and find in set
+      Iterator it = prefixes.iterator();
+      while (it.hasNext()) {
+        String prefix = (String) it.next();
+        if (name.startsWith(prefix)) {
+          return secondaryDir;
+        }
+      }
+    }
+    return primaryDir;
+  }
+
+  @Override
+  public boolean fileExists(String name) throws IOException {
+    return getDirectory(name).fileExists(name);
+  }
+
+  @Override
+  public long fileModified(String name) throws IOException {
+    return getDirectory(name).fileModified(name);
+  }
+
+  @Override
+  public void touchFile(String name) throws IOException {
+    getDirectory(name).touchFile(name);
+  }
+
+  private boolean containsPrefix(String prefix) throws IOException {
+    String[] files = listAll();
+    for (String name : files) {
+      if (name.startsWith(prefix))
+        return true;
+    }
+    return false;
+  }
+  
+  public String getDirName(String name) {
+    if (getDirectory(name) == primaryDir) return "primary";
+    else return "ram";
+  }
+  
+  /**
+   * If the deleted file is the last of a prefix, remove the prefix from the
+   * set.
+   */
+  @Override
+  public void deleteFile(String name) throws IOException {
+    //System.out.println("deleteFile:"+name+" "+getDirName(name));
+    getDirectory(name).deleteFile(name);
+    int idx = name.indexOf('.');
+    if (idx != -1) {
+      String prefix = name.substring(0, idx);
+      if (!containsPrefix(prefix)) {
+        prefixes.remove(prefix);
+      }
+    }
+  }
+
+  @Override
+  public long fileLength(String name) throws IOException {
+    Directory dir = getDirectory(name);
+    //if (dir == primaryDir) {
+    //  System.out.println("primaryDir fileLength:"+name);
+    //} else if (dir == secondaryDir) {
+    //  System.out.println("secondaryDir fileLength:"+name);
+    //}
+    return dir.fileLength(name);
+  }
+
+  @Override
+  public IndexOutput createOutput(String name) throws IOException {
+    //System.out.println("createOutput:"+name+" "+getDirName(name));
+    return getDirectory(name).createOutput(name);
+  }
+
+  @Override
+  public void sync(String name) throws IOException {
+    getDirectory(name).sync(name);
+  }
+
+  @Override
+  public IndexInput openInput(String name) throws IOException {
+    return getDirectory(name).openInput(name);
+  }
+}
Index: contrib/benchmark/conf/basicNRT.alg
===================================================================
--- contrib/benchmark/conf/basicNRT.alg	(revision 884977)
+++ contrib/benchmark/conf/basicNRT.alg	(working copy)
@@ -25,7 +25,7 @@
 analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
 directory=FSDirectory
 
-work.dir = /x/lucene/wiki.5M
+# work.dir = /x/lucene/wiki.5M
 
 doc.stored=true
 doc.body.stored=false
@@ -36,13 +36,20 @@
 log.step.Search = 10000
 compound = false
 
-content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
+# content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
+
 content.source.forever = false
 file.query.maker.file = queries.txt
 
-query.maker=org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker
-docs.file = /x/lucene/enwiki-20090306-lines-1k-fixed.txt
+# query.maker=org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker
+# docs.file = /x/lucene/enwiki-20090306-lines-1k-fixed.txt
+# doc.maker=org.apache.lucene.benchmark.byTask.feeds.EnwikiDocMaker
+docs.file = /home/jason/src/LUCENE-1577/contrib/benchmark/temp/enwiki-20070527-pages-articles.xml
 
+content.source=org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
+
 # task at this depth or less would print when they start
 task.max.depth.log=2
 
@@ -66,7 +73,7 @@
   [ "Searching" { Search > : * ] : 4 &
 
   # Wait 60 sec, then wrap up
-  Wait(5.0)
+  Wait(60.0)
 }
 CloseReader
 
Index: contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenIndexTask.java
===================================================================
--- contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenIndexTask.java	(revision 884977)
+++ contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenIndexTask.java	(working copy)
@@ -19,6 +19,7 @@
 
 import org.apache.lucene.benchmark.byTask.PerfRunData;
 import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexCommit;
 import org.apache.lucene.index.LogMergePolicy;
@@ -54,17 +55,22 @@
     PerfRunData runData = getRunData();
     Config config = runData.getConfig();
     final IndexCommit ic;
+    IndexWriter writer;
     if (commitUserData != null) {
       ic = OpenReaderTask.findIndexCommit(runData.getDirectory(), commitUserData);
+      writer = new IndexWriter(runData.getDirectory(),
+          runData.getAnalyzer(),
+          CreateIndexTask.getIndexDeletionPolicy(config),
+          IndexWriter.MaxFieldLength.UNLIMITED,
+          ic);
     } else {
-      ic = null;
+      writer = new IndexWriter(runData.getDirectory(),
+          runData.getAnalyzer(),
+          true,
+          CreateIndexTask.getIndexDeletionPolicy(config),
+          IndexWriter.MaxFieldLength.UNLIMITED);
     }
-    
-    IndexWriter writer = new IndexWriter(runData.getDirectory(),
-                                         runData.getAnalyzer(),
-                                         CreateIndexTask.getIndexDeletionPolicy(config),
-                                         IndexWriter.MaxFieldLength.UNLIMITED,
-                                         ic);
+    //boolean create = !IndexReader.indexExists(directory); 
     CreateIndexTask.setIndexWriterConfig(writer, config);
     runData.setIndexWriter(writer);
     return 1;
Index: contrib/benchmark/build.xml
===================================================================
--- contrib/benchmark/build.xml	(revision 884977)
+++ contrib/benchmark/build.xml	(working copy)
@@ -120,7 +120,9 @@
     <property name="task.alg" location="conf/micro-standard.alg"/>
     <property name="task.mem" value="140M"/>
 
-    <target name="run-task" depends="compile,check-files,get-files" 
+    <!-- <target name="run-task" depends="compile,check-files,get-files" 
+     description="Run compound penalty perf test (optional: -Dtask.alg=your-algorithm-file -Dtask.mem=java-max-mem)">-->
+    <target name="run-task" depends="compile" 
      description="Run compound penalty perf test (optional: -Dtask.alg=your-algorithm-file -Dtask.mem=java-max-mem)">
         <echo>Working Directory: ${working.dir}</echo>
         <java classname="org.apache.lucene.benchmark.byTask.Benchmark" maxmemory="${task.mem}" fork="true">
