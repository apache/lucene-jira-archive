

diff -ruN -x .svn -x build trunk/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FacetSource.java simplefacets/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FacetSource.java
--- trunk/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FacetSource.java	2013-01-21 11:50:43.986710157 -0500
+++ simplefacets/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FacetSource.java	2013-11-26 11:45:24.658940013 -0500
@@ -20,7 +20,8 @@
 import java.io.IOException;
 import java.util.List;
 
-import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.FacetsConfig;
 
 /**
  * Source items for facets.
@@ -34,7 +35,9 @@
    * account for multi-threading, as multiple threads can call this method
    * simultaneously.
    */
-  public abstract void getNextFacets(List<CategoryPath> facets) throws NoMoreDataException, IOException;
+  public abstract void getNextFacets(List<FacetField> facets) throws NoMoreDataException, IOException;
+
+  public abstract void configure(FacetsConfig config);
 
   @Override
   public void resetInputs() throws IOException {


diff -ruN -x .svn -x build trunk/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/RandomFacetSource.java simplefacets/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/RandomFacetSource.java
--- trunk/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/RandomFacetSource.java	2013-01-21 11:50:43.986710157 -0500
+++ simplefacets/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/RandomFacetSource.java	2013-12-18 19:12:47.777634108 -0500
@@ -22,7 +22,8 @@
 import java.util.Random;
 
 import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.FacetsConfig;
 
 /**
  * Simple implementation of a random facet source
@@ -31,6 +32,9 @@
  * <ul>
  * <li><b>rand.seed</b> - defines the seed to initialize {@link Random} with
  * (default: <b>13</b>).
+ * <li><b>max.doc.facet.dims</b> - Max number of random dimensions to
+ * create (default: <b>5</b>); actual number of dimensions
+ * would be anything between 1 and that number.
  * <li><b>max.doc.facets</b> - maximal #facets per doc (default: <b>10</b>).
  * Actual number of facets in a certain doc would be anything between 1 and that
  * number.
@@ -44,22 +48,38 @@
   private Random random;
   private int maxDocFacets;
   private int maxFacetDepth;
+  private int maxDims;
   private int maxValue = maxDocFacets * maxFacetDepth;
   
   @Override
-  public void getNextFacets(List<CategoryPath> facets) throws NoMoreDataException, IOException {
+  public void getNextFacets(List<FacetField> facets) throws NoMoreDataException, IOException {
     facets.clear();
     int numFacets = 1 + random.nextInt(maxDocFacets); // at least one facet to each doc
     for (int i = 0; i < numFacets; i++) {
-      int depth = 1 + random.nextInt(maxFacetDepth); // depth 0 is not useful
-      String[] components = new String[depth];
-      for (int k = 0; k < depth; k++) {
+      int depth;
+      if (maxFacetDepth == 2) {
+        depth = 2;
+      } else {
+        depth = 2 + random.nextInt(maxFacetDepth-2); // depth < 2 is not useful
+      }
+
+      String dim = Integer.toString(random.nextInt(maxDims));
+      String[] components = new String[depth-1];
+      for (int k = 0; k < depth-1; k++) {
         components[k] = Integer.toString(random.nextInt(maxValue));
         addItem();
       }
-      CategoryPath cp = new CategoryPath(components);
-      facets.add(cp);
-      addBytes(cp.toString().length()); // very rough approximation
+      FacetField ff = new FacetField(dim, components);
+      facets.add(ff);
+      addBytes(ff.toString().length()); // very rough approximation
+    }
+  }
+
+  @Override
+  public void configure(FacetsConfig config) {
+    for(int i=0;i<maxDims;i++) {
+      config.setHierarchical(Integer.toString(i), true);
+      config.setMultiValued(Integer.toString(i), true);
     }
   }
 
@@ -73,7 +93,11 @@
     super.setConfig(config);
     random = new Random(config.get("rand.seed", 13));
     maxDocFacets = config.get("max.doc.facets", 10);
+    maxDims = config.get("max.doc.facets.dims", 5);
     maxFacetDepth = config.get("max.facet.depth", 3);
+    if (maxFacetDepth < 2) {
+      throw new IllegalArgumentException("max.facet.depth must be at least 2; got: " + maxFacetDepth);
+    }
     maxValue = maxDocFacets * maxFacetDepth;
   }
 }


diff -ruN -x .svn -x build trunk/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddDocTask.java simplefacets/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddDocTask.java
--- trunk/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddDocTask.java	2012-07-21 08:52:57.797098744 -0400
+++ simplefacets/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddDocTask.java	2013-12-18 19:10:17.353638312 -0500
@@ -23,6 +23,7 @@
 import org.apache.lucene.benchmark.byTask.PerfRunData;
 import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
 import org.apache.lucene.document.Document;
+import org.apache.lucene.index.IndexDocument;
 
 /**
  * Add a document, optionally of a certain size.
@@ -41,7 +42,7 @@
    * volatile data passed between setup(), doLogic(), tearDown().
    * the doc is created at setup() and added at doLogic(). 
    */
-  protected Document doc = null;
+  protected IndexDocument doc = null;
 
   @Override
   public void setup() throws Exception {


diff -ruN -x .svn -x build trunk/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddFacetedDocTask.java simplefacets/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddFacetedDocTask.java
--- trunk/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddFacetedDocTask.java	2013-01-21 11:50:43.986710157 -0500
+++ simplefacets/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddFacetedDocTask.java	2013-12-18 19:13:33.865633121 -0500
@@ -22,8 +22,14 @@
 
 import org.apache.lucene.benchmark.byTask.PerfRunData;
 import org.apache.lucene.benchmark.byTask.feeds.FacetSource;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.index.IndexDocument;
+import org.apache.lucene.index.IndexableField;
+import org.apache.lucene.index.StorableField;
 
 /**
  * Add a faceted document.
@@ -44,8 +50,7 @@
  */
 public class AddFacetedDocTask extends AddDocTask {
 
-  private final List<CategoryPath> facets = new ArrayList<CategoryPath>();
-  private FacetFields facetFields;
+  private FacetsConfig config;
   
   public AddFacetedDocTask(PerfRunData runData) {
     super(runData);
@@ -54,19 +59,19 @@
   @Override
   public void setup() throws Exception {
     super.setup();
-    if (facetFields == null) {
+    if (config == null) {
       boolean withFacets = getRunData().getConfig().get("with.facets", true);
       if (withFacets) {
         FacetSource facetsSource = getRunData().getFacetSource();
-        facetFields = withFacets ? new FacetFields(getRunData().getTaxonomyWriter()) : null;
-        facetsSource.getNextFacets(facets);
+        config = new FacetsConfig();
+        facetsSource.configure(config);
       }
     }
   }
 
   @Override
   protected String getLogMessage(int recsCount) {
-    if (facetFields == null) {
+    if (config == null) {
       return super.getLogMessage(recsCount);
     }
     return super.getLogMessage(recsCount)+ " with facets";
@@ -74,10 +79,14 @@
   
   @Override
   public int doLogic() throws Exception {
-    if (facetFields != null) {
-      facetFields.addFields(doc, facets);
+    if (config != null) {
+      List<FacetField> facets = new ArrayList<FacetField>();
+      getRunData().getFacetSource().getNextFacets(facets);
+      for(FacetField ff : facets) {
+        ((Document) doc).add(ff);
+      }
+      doc = config.build(getRunData().getTaxonomyWriter(), doc);
     }
     return super.doLogic();
   }
-
 }


diff -ruN -x .svn -x build trunk/lucene/build.xml simplefacets/lucene/build.xml
--- trunk/lucene/build.xml	2013-11-20 09:27:41.221029017 -0500
+++ simplefacets/lucene/build.xml	2013-11-26 18:17:09.698311501 -0500
@@ -240,7 +240,7 @@
     <!-- core: problems -->
     <check-missing-javadocs dir="build/docs/demo" level="method"/>
     <check-missing-javadocs dir="build/docs/expressions" level="method"/>
-    <!-- facet: problems -->
+    <check-missing-javadocs dir="build/docs/facet" level="method"/>
     <!-- grouping: problems -->
     <!-- highlighter: problems -->
     <check-missing-javadocs dir="build/docs/join" level="method"/>


diff -ruN -x .svn -x build trunk/lucene/CHANGES.txt simplefacets/lucene/CHANGES.txt
--- trunk/lucene/CHANGES.txt	2013-12-17 10:38:41.684770713 -0500
+++ simplefacets/lucene/CHANGES.txt	2013-12-19 11:36:58.108054607 -0500
@@ -76,6 +76,11 @@
 * SOLR-1871: The RangeMapFloatFunction accepts an arbitrary ValueSource
   as target and default values. (Chris Harris, shalin)
 
+* LUCENE-5371: Speed up Lucene range faceting from O(N) per hit to
+  O(log(N)) per hit using segment trees; this only really starts to
+  matter in practice if the number of ranges is over 10 or so.  (Mike
+  McCandless)
+
 Build
 
 * LUCENE-5217: Maven config: get dependencies from Ant+Ivy config; disable


diff -ruN -x .svn -x build trunk/lucene/core/src/java/org/apache/lucene/document/Field.java simplefacets/lucene/core/src/java/org/apache/lucene/document/Field.java
--- trunk/lucene/core/src/java/org/apache/lucene/document/Field.java	2013-07-15 15:52:16.389877421 -0400
+++ simplefacets/lucene/core/src/java/org/apache/lucene/document/Field.java	2013-11-18 09:39:28.369633020 -0500
@@ -554,7 +554,7 @@
       return analyzer.tokenStream(name(), stringValue());
     }
 
-    throw new IllegalArgumentException("Field must have either TokenStream, String, Reader or Number value");
+    throw new IllegalArgumentException("Field must have either TokenStream, String, Reader or Number value; this=" + this);
   }
   
   static final class StringTokenStream extends TokenStream {


diff -ruN -x .svn -x build trunk/lucene/core/src/java/org/apache/lucene/search/ConjunctionScorer.java simplefacets/lucene/core/src/java/org/apache/lucene/search/ConjunctionScorer.java
--- trunk/lucene/core/src/java/org/apache/lucene/search/ConjunctionScorer.java	2013-12-05 17:09:06.975879906 -0500
+++ simplefacets/lucene/core/src/java/org/apache/lucene/search/ConjunctionScorer.java	2013-12-05 17:08:15.255880876 -0500
@@ -55,15 +55,12 @@
   }
 
   private int doNext(int doc) throws IOException {
-    System.out.println("doNext doc=" + doc);
     for(;;) {
       // doc may already be NO_MORE_DOCS here, but we don't check explicitly
       // since all scorers should advance to NO_MORE_DOCS, match, then
       // return that value.
       advanceHead: for(;;) {
-        System.out.println("cycle top");
         for (int i = 1; i < docsAndFreqs.length; i++) {
-          System.out.println("  cycle i=" + i + " doc=" + docsAndFreqs[i].doc);
           // invariant: docsAndFreqs[i].doc <= doc at this point.
 
           // docsAndFreqs[i].doc may already be equal to doc if we "broke advanceHead"


diff -ruN -x .svn -x build trunk/lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java simplefacets/lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java
--- trunk/lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java	2013-09-23 08:26:38.496802509 -0400
+++ simplefacets/lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java	2013-11-27 11:49:36.408622099 -0500
@@ -431,7 +431,7 @@
       limit = 1;
     }
     if (after != null && after.doc >= limit) {
-      throw new IllegalArgumentException("after.doc exceeds the number of documents in that reader: after.doc="
+      throw new IllegalArgumentException("after.doc exceeds the number of documents in the reader: after.doc="
           + after.doc + " limit=" + limit);
     }
     nDocs = Math.min(nDocs, limit);


diff -ruN -x .svn -x build trunk/lucene/core/src/test/org/apache/lucene/search/TestConjunctions.java simplefacets/lucene/core/src/test/org/apache/lucene/search/TestConjunctions.java
--- trunk/lucene/core/src/test/org/apache/lucene/search/TestConjunctions.java	2013-12-05 17:07:57.495881623 -0500
+++ simplefacets/lucene/core/src/test/org/apache/lucene/search/TestConjunctions.java	2013-07-15 15:52:16.677877416 -0400
@@ -22,22 +22,18 @@
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.similarities.Similarity;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 
 public class TestConjunctions extends LuceneTestCase {
@@ -132,39 +128,4 @@
       };
     }
   }
-
-  public void testLowHighFreq() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT,
-                                                 new MockAnalyzer(random()));
-    iwc.setMergePolicy(newLogMergePolicy());
-    IndexWriter writer = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(newStringField("field", "a", Field.Store.NO));
-    writer.addDocument(doc);
-
-    doc = new Document();
-    doc.add(newStringField("field", "b", Field.Store.NO));
-    writer.addDocument(doc);
-
-    doc = new Document();
-    doc.add(newStringField("field", "b", Field.Store.NO));
-    writer.addDocument(doc);
-
-    doc = new Document();
-    doc.add(newStringField("field", "b", Field.Store.NO));
-    writer.addDocument(doc);
-
-    doc = new Document();
-    doc.add(newStringField("field", "a", Field.Store.NO));
-    doc.add(newStringField("field", "b", Field.Store.NO));
-    writer.addDocument(doc);
-
-    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
-    BooleanQuery q = new BooleanQuery();
-    q.add(new TermQuery(new Term("field", "a")), BooleanClause.Occur.MUST);
-    q.add(new TermQuery(new Term("field", "b")), BooleanClause.Occur.MUST);
-    assertEquals(1, searcher.search(q, 10).totalHits);
-    IOUtils.close(writer, searcher.getIndexReader(), dir);
-  }
 }


diff -ruN -x .svn -x build trunk/lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java simplefacets/lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java
--- trunk/lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java	2013-07-29 13:55:02.725707536 -0400
+++ simplefacets/lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java	2013-11-27 19:36:19.019873146 -0500
@@ -1,33 +1,5 @@
 package org.apache.lucene.demo.facet;
 
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.associations.AssociationsFacetFields;
-import org.apache.lucene.facet.associations.CategoryAssociation;
-import org.apache.lucene.facet.associations.CategoryAssociationsContainer;
-import org.apache.lucene.facet.associations.CategoryFloatAssociation;
-import org.apache.lucene.facet.associations.CategoryIntAssociation;
-import org.apache.lucene.facet.associations.SumFloatAssociationFacetRequest;
-import org.apache.lucene.facet.associations.SumIntAssociationFacetRequest;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.RAMDirectory;
-
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -45,77 +17,76 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.FloatAssociationFacetField;
+import org.apache.lucene.facet.IntAssociationFacetField;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.TaxonomyFacetSumFloatAssociations;
+import org.apache.lucene.facet.TaxonomyFacetSumIntAssociations;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMDirectory;
+
 /** Shows example usage of category associations. */
 public class AssociationsFacetsExample {
 
-  /**
-   * Categories per document, {@link #ASSOCIATIONS} hold the association value
-   * for each category.
-   */
-  public static CategoryPath[][] CATEGORIES = {
-    // Doc #1
-    { new CategoryPath("tags", "lucene") , 
-      new CategoryPath("genre", "computing")
-    },
-        
-    // Doc #2
-    { new CategoryPath("tags", "lucene"),  
-      new CategoryPath("tags", "solr"),
-      new CategoryPath("genre", "computing"),
-      new CategoryPath("genre", "software")
-    }
-  };
-
-  /** Association values for each category. */
-  public static CategoryAssociation[][] ASSOCIATIONS = {
-    // Doc #1 associations
-    {
-      /* 3 occurrences for tag 'lucene' */
-      new CategoryIntAssociation(3), 
-      /* 87% confidence level of genre 'computing' */
-      new CategoryFloatAssociation(0.87f)
-    },
-    
-    // Doc #2 associations
-    {
-      /* 1 occurrence for tag 'lucene' */
-      new CategoryIntAssociation(1),
-      /* 2 occurrences for tag 'solr' */
-      new CategoryIntAssociation(2),
-      /* 75% confidence level of genre 'computing' */
-      new CategoryFloatAssociation(0.75f),
-      /* 34% confidence level of genre 'software' */
-      new CategoryFloatAssociation(0.34f),
-    }
-  };
-
   private final Directory indexDir = new RAMDirectory();
   private final Directory taxoDir = new RAMDirectory();
+  private final FacetsConfig config;
 
   /** Empty constructor */
-  public AssociationsFacetsExample() {}
+  public AssociationsFacetsExample() {
+    config = new FacetsConfig();
+    config.setMultiValued("tags", true);
+    config.setIndexFieldName("tags", "$tags");
+    config.setMultiValued("genre", true);
+    config.setIndexFieldName("genre", "$genre");
+  }
   
   /** Build the example index. */
   private void index() throws IOException {
-    IndexWriter indexWriter = new IndexWriter(indexDir, new IndexWriterConfig(FacetExamples.EXAMPLES_VER, 
-        new WhitespaceAnalyzer(FacetExamples.EXAMPLES_VER)));
+    IndexWriterConfig iwc = new IndexWriterConfig(FacetExamples.EXAMPLES_VER, 
+                                                  new WhitespaceAnalyzer(FacetExamples.EXAMPLES_VER));
+    IndexWriter indexWriter = new IndexWriter(indexDir, iwc);
 
     // Writes facet ords to a separate directory from the main index
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
 
-    // Reused across documents, to add the necessary facet fields
-    FacetFields facetFields = new AssociationsFacetFields(taxoWriter);
-    
-    for (int i = 0; i < CATEGORIES.length; i++) {
-      Document doc = new Document();
-      CategoryAssociationsContainer associations = new CategoryAssociationsContainer();
-      for (int j = 0; j < CATEGORIES[i].length; j++) {
-        associations.setAssociation(CATEGORIES[i][j], ASSOCIATIONS[i][j]);
-      }
-      facetFields.addFields(doc, associations);
-      indexWriter.addDocument(doc);
-    }
-    
+    Document doc = new Document();
+    // 3 occurrences for tag 'lucene'
+    doc.add(new IntAssociationFacetField(3, "tags", "lucene"));
+    // 87% confidence level of genre 'computing'
+    doc.add(new FloatAssociationFacetField(0.87f, "genre", "computing"));
+    indexWriter.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    // 1 occurrence for tag 'lucene'
+    doc.add(new IntAssociationFacetField(1, "tags", "lucene"));
+    // 2 occurrence for tag 'solr'
+    doc.add(new IntAssociationFacetField(2, "tags", "solr"));
+    // 75% confidence level of genre 'computing'
+    doc.add(new FloatAssociationFacetField(0.75f, "genre", "computing"));
+    // 34% confidence level of genre 'software'
+    doc.add(new FloatAssociationFacetField(0.34f, "genre", "software"));
+    indexWriter.addDocument(config.build(taxoWriter, doc));
+
     indexWriter.close();
     taxoWriter.close();
   }
@@ -126,25 +97,25 @@
     IndexSearcher searcher = new IndexSearcher(indexReader);
     TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
     
-    CategoryPath tags = new CategoryPath("tags");
-    CategoryPath genre = new CategoryPath("genre");
-    FacetSearchParams fsp = new FacetSearchParams(new SumIntAssociationFacetRequest(tags, 10), 
-        new SumFloatAssociationFacetRequest(genre, 10));
-    FacetsCollector fc = FacetsCollector.create(fsp, indexReader, taxoReader);
+    FacetsCollector fc = new FacetsCollector();
     
     // MatchAllDocsQuery is for "browsing" (counts facets
     // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), fc);
+    // you'd use a "normal" query:
+    FacetsCollector.search(searcher, new MatchAllDocsQuery(), 10, fc);
     
+    Facets tags = new TaxonomyFacetSumIntAssociations("$tags", taxoReader, config, fc);
+    Facets genre = new TaxonomyFacetSumFloatAssociations("$genre", taxoReader, config, fc);
+
     // Retrieve results
-    List<FacetResult> facetResults = fc.getFacetResults();
-    
+    List<FacetResult> results = new ArrayList<FacetResult>();
+    results.add(tags.getTopChildren(10, "tags"));
+    results.add(genre.getTopChildren(10, "genre"));
+
     indexReader.close();
     taxoReader.close();
     
-    return facetResults;
+    return results;
   }
   
   /** Runs summing association example. */
@@ -158,9 +129,7 @@
     System.out.println("Sum associations example:");
     System.out.println("-------------------------");
     List<FacetResult> results = new AssociationsFacetsExample().runSumAssociations();
-    for (FacetResult res : results) {
-      System.out.println(res);
-    }
+    System.out.println("tags: " + results.get(0));
+    System.out.println("genre: " + results.get(1));
   }
-  
 }


diff -ruN -x .svn -x build trunk/lucene/demo/src/java/org/apache/lucene/demo/facet/DistanceFacetsExample.java simplefacets/lucene/demo/src/java/org/apache/lucene/demo/facet/DistanceFacetsExample.java
--- trunk/lucene/demo/src/java/org/apache/lucene/demo/facet/DistanceFacetsExample.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/demo/src/java/org/apache/lucene/demo/facet/DistanceFacetsExample.java	2013-12-02 15:44:00.754949992 -0500
@@ -0,0 +1,160 @@
+package org.apache.lucene.demo.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.text.ParseException;
+
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.DoubleField;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.expressions.Expression;
+import org.apache.lucene.expressions.SimpleBindings;
+import org.apache.lucene.expressions.js.JavascriptCompiler;
+import org.apache.lucene.facet.DoubleRange;
+import org.apache.lucene.facet.DoubleRangeFacetCounts;
+import org.apache.lucene.facet.DrillDownQuery;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.SortField;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMDirectory;
+
+
+
+/** Shows simple usage of dynamic range faceting, using the
+ *  expressions module to calculate distance. */
+public class DistanceFacetsExample implements Closeable {
+
+  final DoubleRange ONE_KM = new DoubleRange("< 1 km", 0.0, true, 1.0, false);
+  final DoubleRange TWO_KM = new DoubleRange("< 2 km", 0.0, true, 2.0, false);
+  final DoubleRange FIVE_KM = new DoubleRange("< 5 km", 0.0, true, 5.0, false);
+  final DoubleRange TEN_KM = new DoubleRange("< 10 km", 0.0, true, 10.0, false);
+
+  private final Directory indexDir = new RAMDirectory();
+  private IndexSearcher searcher;
+
+  /** Empty constructor */
+  public DistanceFacetsExample() {}
+  
+  /** Build the example index. */
+  public void index() throws IOException {
+    IndexWriter writer = new IndexWriter(indexDir, new IndexWriterConfig(FacetExamples.EXAMPLES_VER, 
+        new WhitespaceAnalyzer(FacetExamples.EXAMPLES_VER)));
+
+    // Add documents with latitude/longitude location:
+    Document doc = new Document();
+    doc.add(new DoubleField("latitude", 40.759011, Field.Store.NO));
+    doc.add(new DoubleField("longitude", -73.9844722, Field.Store.NO));
+    writer.addDocument(doc);
+    
+    doc = new Document();
+    doc.add(new DoubleField("latitude", 40.718266, Field.Store.NO));
+    doc.add(new DoubleField("longitude", -74.007819, Field.Store.NO));
+    writer.addDocument(doc);
+    
+    doc = new Document();
+    doc.add(new DoubleField("latitude", 40.7051157, Field.Store.NO));
+    doc.add(new DoubleField("longitude", -74.0088305, Field.Store.NO));
+    writer.addDocument(doc);
+
+    // Open near-real-time searcher
+    searcher = new IndexSearcher(DirectoryReader.open(writer, true));
+    writer.close();
+  }
+
+  private ValueSource getDistanceValueSource() {
+    Expression distance;
+    try {
+      distance = JavascriptCompiler.compile("haversin(40.7143528,-74.0059731,latitude,longitude)");
+    } catch (ParseException pe) {
+      // Should not happen
+      throw new RuntimeException(pe);
+    }
+    SimpleBindings bindings = new SimpleBindings();
+    bindings.add(new SortField("latitude", SortField.Type.DOUBLE));
+    bindings.add(new SortField("longitude", SortField.Type.DOUBLE));
+
+    return distance.getValueSource(bindings);
+  }
+
+  /** User runs a query and counts facets. */
+  public FacetResult search() throws IOException {
+
+
+    FacetsCollector fc = new FacetsCollector();
+
+    searcher.search(new MatchAllDocsQuery(), fc);
+
+    Facets facets = new DoubleRangeFacetCounts("field", getDistanceValueSource(), fc,
+                                               ONE_KM,
+                                               TWO_KM,
+                                               FIVE_KM,
+                                               TEN_KM);
+
+    return facets.getTopChildren(10, "field");
+  }
+
+  /** User drills down on the specified range. */
+  public TopDocs drillDown(DoubleRange range) throws IOException {
+
+    // Passing no baseQuery means we drill down on all
+    // documents ("browse only"):
+    DrillDownQuery q = new DrillDownQuery(null);
+
+    q.add("field", new ConstantScoreQuery(range.getFilter(getDistanceValueSource())));
+
+    return searcher.search(q, 10);
+  }
+
+  @Override
+  public void close() throws IOException {
+    searcher.getIndexReader().close();
+    indexDir.close();
+  }
+
+  /** Runs the search and drill-down examples and prints the results. */
+  @SuppressWarnings("unchecked")
+  public static void main(String[] args) throws Exception {
+    DistanceFacetsExample example = new DistanceFacetsExample();
+    example.index();
+
+    System.out.println("Distance facet counting example:");
+    System.out.println("-----------------------");
+    System.out.println(example.search());
+
+    System.out.println("\n");
+    System.out.println("Distance facet drill-down example (field/< 2 km):");
+    System.out.println("---------------------------------------------");
+    TopDocs hits = example.drillDown(example.TWO_KM);
+    System.out.println(hits.totalHits + " totalHits");
+
+    example.close();
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/demo/src/java/org/apache/lucene/demo/facet/ExpressionAggregationFacetsExample.java simplefacets/lucene/demo/src/java/org/apache/lucene/demo/facet/ExpressionAggregationFacetsExample.java
--- trunk/lucene/demo/src/java/org/apache/lucene/demo/facet/ExpressionAggregationFacetsExample.java	2013-11-06 07:02:49.325619588 -0500
+++ simplefacets/lucene/demo/src/java/org/apache/lucene/demo/facet/ExpressionAggregationFacetsExample.java	2013-11-27 19:36:22.955872792 -0500
@@ -13,12 +13,13 @@
 import org.apache.lucene.expressions.Expression;
 import org.apache.lucene.expressions.SimpleBindings;
 import org.apache.lucene.expressions.js.JavascriptCompiler;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.search.SumValueSourceFacetRequest;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.TaxonomyFacetSumValueSource;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
@@ -53,18 +54,11 @@
 
   private final Directory indexDir = new RAMDirectory();
   private final Directory taxoDir = new RAMDirectory();
+  private final FacetsConfig config = new FacetsConfig();
 
   /** Empty constructor */
   public ExpressionAggregationFacetsExample() {}
   
-  private void add(IndexWriter indexWriter, FacetFields facetFields, String text, String category, long popularity) throws IOException {
-    Document doc = new Document();
-    doc.add(new TextField("c", text, Store.NO));
-    doc.add(new NumericDocValuesField("popularity", popularity));
-    facetFields.addFields(doc, Collections.singletonList(new CategoryPath(category, '/')));
-    indexWriter.addDocument(doc);
-  }
-
   /** Build the example index. */
   private void index() throws IOException {
     IndexWriter indexWriter = new IndexWriter(indexDir, new IndexWriterConfig(FacetExamples.EXAMPLES_VER, 
@@ -73,18 +67,24 @@
     // Writes facet ords to a separate directory from the main index
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
 
-    // Reused across documents, to add the necessary facet fields
-    FacetFields facetFields = new FacetFields(taxoWriter);
-
-    add(indexWriter, facetFields, "foo bar", "A/B", 5L);
-    add(indexWriter, facetFields, "foo foo bar", "A/C", 3L);
+    Document doc = new Document();
+    doc.add(new TextField("c", "foo bar", Store.NO));
+    doc.add(new NumericDocValuesField("popularity", 5L));
+    doc.add(new FacetField("A", "B"));
+    indexWriter.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new TextField("c", "foo foo bar", Store.NO));
+    doc.add(new NumericDocValuesField("popularity", 3L));
+    doc.add(new FacetField("A", "C"));
+    indexWriter.addDocument(config.build(taxoWriter, doc));
     
     indexWriter.close();
     taxoWriter.close();
   }
 
   /** User runs a query and aggregates facets. */
-  private List<FacetResult> search() throws IOException, ParseException {
+  private FacetResult search() throws IOException, ParseException {
     DirectoryReader indexReader = DirectoryReader.open(indexDir);
     IndexSearcher searcher = new IndexSearcher(indexReader);
     TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
@@ -96,29 +96,26 @@
     bindings.add(new SortField("_score", SortField.Type.SCORE)); // the score of the document
     bindings.add(new SortField("popularity", SortField.Type.LONG)); // the value of the 'popularity' field
 
-    FacetSearchParams fsp = new FacetSearchParams(
-        new SumValueSourceFacetRequest(new CategoryPath("A"), 10, expr.getValueSource(bindings), true));
-
     // Aggregates the facet values
-    FacetsCollector fc = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);
+    FacetsCollector fc = new FacetsCollector(true);
 
     // MatchAllDocsQuery is for "browsing" (counts facets
     // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), fc);
+    // you'd use a "normal" query:
+    FacetsCollector.search(searcher, new MatchAllDocsQuery(), 10, fc);
 
     // Retrieve results
-    List<FacetResult> facetResults = fc.getFacetResults();
+    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, fc, expr.getValueSource(bindings));
+    FacetResult result = facets.getTopChildren(10, "A");
     
     indexReader.close();
     taxoReader.close();
     
-    return facetResults;
+    return result;
   }
   
   /** Runs the search example. */
-  public List<FacetResult> runSearch() throws IOException, ParseException {
+  public FacetResult runSearch() throws IOException, ParseException {
     index();
     return search();
   }
@@ -127,10 +124,7 @@
   public static void main(String[] args) throws Exception {
     System.out.println("Facet counting example:");
     System.out.println("-----------------------");
-    List<FacetResult> results = new ExpressionAggregationFacetsExample().runSearch();
-    for (FacetResult res : results) {
-      System.out.println(res);
-    }
+    FacetResult result = new ExpressionAggregationFacetsExample().runSearch();
+    System.out.println(result);
   }
-  
 }


diff -ruN -x .svn -x build trunk/lucene/demo/src/java/org/apache/lucene/demo/facet/MultiCategoryListsFacetsExample.java simplefacets/lucene/demo/src/java/org/apache/lucene/demo/facet/MultiCategoryListsFacetsExample.java
--- trunk/lucene/demo/src/java/org/apache/lucene/demo/facet/MultiCategoryListsFacetsExample.java	2013-03-20 06:26:07.107245397 -0400
+++ simplefacets/lucene/demo/src/java/org/apache/lucene/demo/facet/MultiCategoryListsFacetsExample.java	2013-11-27 19:36:26.711873206 -0500
@@ -1,5 +1,22 @@
 package org.apache.lucene.demo.facet;
 
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.HashMap;
@@ -8,16 +25,15 @@
 
 import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.params.PerDimensionIndexingParams;
-import org.apache.lucene.facet.search.CountFacetRequest;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.FastTaxonomyFacetCounts;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.index.DirectoryReader;
@@ -28,48 +44,18 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
 
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Demonstrates indexing categories into different category lists. */
+/** Demonstrates indexing categories into different indexed fields. */
 public class MultiCategoryListsFacetsExample {
 
-  private final FacetIndexingParams indexingParams;
   private final Directory indexDir = new RAMDirectory();
   private final Directory taxoDir = new RAMDirectory();
+  private final FacetsConfig config = new FacetsConfig();
 
   /** Creates a new instance and populates the catetory list params mapping. */
   public MultiCategoryListsFacetsExample() {
-    // index all Author facets in one category list and all Publish Date in another.
-    Map<CategoryPath,CategoryListParams> categoryListParams = new HashMap<CategoryPath,CategoryListParams>();
-    categoryListParams.put(new CategoryPath("Author"), new CategoryListParams("author"));
-    categoryListParams.put(new CategoryPath("Publish Date"), new CategoryListParams("pubdate"));
-    indexingParams = new PerDimensionIndexingParams(categoryListParams);
-  }
-  
-  private void add(IndexWriter indexWriter, FacetFields facetFields, String ... categoryPaths) throws IOException {
-    Document doc = new Document();
-    
-    List<CategoryPath> paths = new ArrayList<CategoryPath>();
-    for (String categoryPath : categoryPaths) {
-      paths.add(new CategoryPath(categoryPath, '/'));
-    }
-    facetFields.addFields(doc, paths);
-    indexWriter.addDocument(doc);
+    config.setIndexFieldName("Author", "author");
+    config.setIndexFieldName("Publish Date", "pubdate");
+    config.setHierarchical("Publish Date", true);
   }
 
   /** Build the example index. */
@@ -80,14 +66,30 @@
     // Writes facet ords to a separate directory from the main index
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
 
-    // Reused across documents, to add the necessary facet fields
-    FacetFields facetFields = new FacetFields(taxoWriter, indexingParams);
-
-    add(indexWriter, facetFields, "Author/Bob", "Publish Date/2010/10/15");
-    add(indexWriter, facetFields, "Author/Lisa", "Publish Date/2010/10/20");
-    add(indexWriter, facetFields, "Author/Lisa", "Publish Date/2012/1/1");
-    add(indexWriter, facetFields, "Author/Susan", "Publish Date/2012/1/7");
-    add(indexWriter, facetFields, "Author/Frank", "Publish Date/1999/5/5");
+    Document doc = new Document();
+    doc.add(new FacetField("Author", "Bob"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "15"));
+    indexWriter.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "20"));
+    indexWriter.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "1"));
+    indexWriter.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Susan"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "7"));
+    indexWriter.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Frank"));
+    doc.add(new FacetField("Publish Date", "1999", "5", "5"));
+    indexWriter.addDocument(config.build(taxoWriter, doc));
     
     indexWriter.close();
     taxoWriter.close();
@@ -99,27 +101,27 @@
     IndexSearcher searcher = new IndexSearcher(indexReader);
     TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
 
-    // Count both "Publish Date" and "Author" dimensions
-    FacetSearchParams fsp = new FacetSearchParams(indexingParams,
-        new CountFacetRequest(new CategoryPath("Publish Date"), 10), 
-        new CountFacetRequest(new CategoryPath("Author"), 10));
-
-    // Aggregatses the facet counts
-    FacetsCollector fc = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);
+    FacetsCollector fc = new FacetsCollector();
 
     // MatchAllDocsQuery is for "browsing" (counts facets
     // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), fc);
+    // you'd use a "normal" query:
+    FacetsCollector.search(searcher, new MatchAllDocsQuery(), 10, fc);
 
     // Retrieve results
-    List<FacetResult> facetResults = fc.getFacetResults();
+    List<FacetResult> results = new ArrayList<FacetResult>();
+
+    // Count both "Publish Date" and "Author" dimensions
+    Facets author = new FastTaxonomyFacetCounts("author", taxoReader, config, fc);
+    results.add(author.getTopChildren(10, "Author"));
+
+    Facets pubDate = new FastTaxonomyFacetCounts("pubdate", taxoReader, config, fc);
+    results.add(pubDate.getTopChildren(10, "Publish Date"));
     
     indexReader.close();
     taxoReader.close();
     
-    return facetResults;
+    return results;
   }
 
   /** Runs the search example. */
@@ -133,9 +135,7 @@
     System.out.println("Facet counting over multiple category lists example:");
     System.out.println("-----------------------");
     List<FacetResult> results = new MultiCategoryListsFacetsExample().runSearch();
-    for (FacetResult res : results) {
-      System.out.println(res);
-    }
+    System.out.println("Author: " + results.get(0));
+    System.out.println("Publish Date: " + results.get(1));
   }
-  
 }


diff -ruN -x .svn -x build trunk/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java simplefacets/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java
--- trunk/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java	2013-07-29 13:55:02.729707538 -0400
+++ simplefacets/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java	2013-12-02 15:42:13.850952850 -0500
@@ -19,20 +19,19 @@
 
 import java.io.Closeable;
 import java.io.IOException;
-import java.util.List;
 
 import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.LongField;
 import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.range.LongRange;
-import org.apache.lucene.facet.range.RangeAccumulator;
-import org.apache.lucene.facet.range.RangeFacetRequest;
-import org.apache.lucene.facet.search.DrillDownQuery;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetsCollector;
+import org.apache.lucene.facet.DrillDownQuery;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.LongRange;
+import org.apache.lucene.facet.LongRangeFacetCounts;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -44,6 +43,7 @@
 import org.apache.lucene.store.RAMDirectory;
 
 
+
 /** Shows simple usage of dynamic range faceting. */
 public class RangeFacetsExample implements Closeable {
 
@@ -51,6 +51,10 @@
   private IndexSearcher searcher;
   private final long nowSec = System.currentTimeMillis();
 
+  final LongRange PAST_HOUR = new LongRange("Past hour", nowSec-3600, true, nowSec, true);
+  final LongRange PAST_SIX_HOURS = new LongRange("Past six hours", nowSec-6*3600, true, nowSec, true);
+  final LongRange PAST_DAY = new LongRange("Past day", nowSec-24*3600, true, nowSec, true);
+
   /** Empty constructor */
   public RangeFacetsExample() {}
   
@@ -76,24 +80,26 @@
     indexWriter.close();
   }
 
+  private FacetsConfig getConfig() {
+    return new FacetsConfig();
+  }
+
   /** User runs a query and counts facets. */
-  public List<FacetResult> search() throws IOException {
+  public FacetResult search() throws IOException {
 
-    RangeFacetRequest<LongRange> rangeFacetRequest = new RangeFacetRequest<LongRange>("timestamp",
-                                     new LongRange("Past hour", nowSec-3600, true, nowSec, true),
-                                     new LongRange("Past six hours", nowSec-6*3600, true, nowSec, true),
-                                     new LongRange("Past day", nowSec-24*3600, true, nowSec, true));
-    // Aggregatses the facet counts
-    FacetsCollector fc = FacetsCollector.create(new RangeAccumulator(rangeFacetRequest));
+    // Aggregates the facet counts
+    FacetsCollector fc = new FacetsCollector();
 
     // MatchAllDocsQuery is for "browsing" (counts facets
     // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), fc);
+    // you'd use a "normal" query:
+    FacetsCollector.search(searcher, new MatchAllDocsQuery(), 10, fc);
 
-    // Retrieve results
-    return fc.getFacetResults();
+    Facets facets = new LongRangeFacetCounts("timestamp", fc,
+                                             PAST_HOUR,
+                                             PAST_SIX_HOURS,
+                                             PAST_DAY);
+    return facets.getTopChildren(10, "timestamp");
   }
   
   /** User drills down on the specified range. */
@@ -101,10 +107,8 @@
 
     // Passing no baseQuery means we drill down on all
     // documents ("browse only"):
-    DrillDownQuery q = new DrillDownQuery(FacetIndexingParams.DEFAULT);
+    DrillDownQuery q = new DrillDownQuery(getConfig());
 
-    // Use FieldCacheRangeFilter; this will use
-    // NumericDocValues:
     q.add("timestamp", NumericRangeQuery.newLongRange("timestamp", range.min, range.max, range.minInclusive, range.maxInclusive));
 
     return searcher.search(q, 10);
@@ -124,15 +128,12 @@
 
     System.out.println("Facet counting example:");
     System.out.println("-----------------------");
-    List<FacetResult> results = example.search();
-    for (FacetResult res : results) {
-      System.out.println(res);
-    }
+    System.out.println(example.search());
 
     System.out.println("\n");
     System.out.println("Facet drill-down example (timestamp/Past six hours):");
     System.out.println("---------------------------------------------");
-    TopDocs hits = example.drillDown((LongRange) ((RangeFacetRequest<LongRange>) results.get(0).getFacetRequest()).ranges[1]);
+    TopDocs hits = example.drillDown(example.PAST_SIX_HOURS);
     System.out.println(hits.totalHits + " totalHits");
 
     example.close();


diff -ruN -x .svn -x build trunk/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleFacetsExample.java simplefacets/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleFacetsExample.java
--- trunk/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleFacetsExample.java	2013-11-03 19:05:04.731394187 -0500
+++ simplefacets/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleFacetsExample.java	2013-11-27 19:36:36.175872604 -0500
@@ -1,29 +1,5 @@
 package org.apache.lucene.demo.facet;
 
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.CountFacetRequest;
-import org.apache.lucene.facet.search.DrillDownQuery;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.RAMDirectory;
-
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -41,26 +17,44 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.FastTaxonomyFacetCounts;
+import org.apache.lucene.facet.DrillDownQuery;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMDirectory;
+
 /** Shows simple usage of faceted indexing and search. */
 public class SimpleFacetsExample {
 
   private final Directory indexDir = new RAMDirectory();
   private final Directory taxoDir = new RAMDirectory();
+  private final FacetsConfig config = new FacetsConfig();
 
   /** Empty constructor */
-  public SimpleFacetsExample() {}
-  
-  private void add(IndexWriter indexWriter, FacetFields facetFields, String ... categoryPaths) throws IOException {
-    Document doc = new Document();
-    
-    List<CategoryPath> paths = new ArrayList<CategoryPath>();
-    for (String categoryPath : categoryPaths) {
-      paths.add(new CategoryPath(categoryPath, '/'));
-    }
-    facetFields.addFields(doc, paths);
-    indexWriter.addDocument(doc);
+  public SimpleFacetsExample() {
+    config.setHierarchical("Publish Date", true);
   }
-
+  
   /** Build the example index. */
   private void index() throws IOException {
     IndexWriter indexWriter = new IndexWriter(indexDir, new IndexWriterConfig(FacetExamples.EXAMPLES_VER, 
@@ -69,14 +63,30 @@
     // Writes facet ords to a separate directory from the main index
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
 
-    // Reused across documents, to add the necessary facet fields
-    FacetFields facetFields = new FacetFields(taxoWriter);
-
-    add(indexWriter, facetFields, "Author/Bob", "Publish Date/2010/10/15");
-    add(indexWriter, facetFields, "Author/Lisa", "Publish Date/2010/10/20");
-    add(indexWriter, facetFields, "Author/Lisa", "Publish Date/2012/1/1");
-    add(indexWriter, facetFields, "Author/Susan", "Publish Date/2012/1/7");
-    add(indexWriter, facetFields, "Author/Frank", "Publish Date/1999/5/5");
+    Document doc = new Document();
+    doc.add(new FacetField("Author", "Bob"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "15"));
+    indexWriter.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "20"));
+    indexWriter.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "1"));
+    indexWriter.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Susan"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "7"));
+    indexWriter.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Frank"));
+    doc.add(new FacetField("Publish Date", "1999", "5", "5"));
+    indexWriter.addDocument(config.build(taxoWriter, doc));
     
     indexWriter.close();
     taxoWriter.close();
@@ -88,52 +98,50 @@
     IndexSearcher searcher = new IndexSearcher(indexReader);
     TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
 
-    // Count both "Publish Date" and "Author" dimensions
-    FacetSearchParams fsp = new FacetSearchParams(
-        new CountFacetRequest(new CategoryPath("Publish Date"), 10), 
-        new CountFacetRequest(new CategoryPath("Author"), 10));
-
-    // Aggregates the facet counts
-    FacetsCollector fc = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);
+    FacetsCollector fc = new FacetsCollector();
 
     // MatchAllDocsQuery is for "browsing" (counts facets
     // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), fc);
+    // you'd use a "normal" query:
+    FacetsCollector.search(searcher, new MatchAllDocsQuery(), 10, fc);
 
     // Retrieve results
-    List<FacetResult> facetResults = fc.getFacetResults();
+    List<FacetResult> results = new ArrayList<FacetResult>();
+
+    // Count both "Publish Date" and "Author" dimensions
+    Facets facets = new FastTaxonomyFacetCounts(taxoReader, config, fc);
+    results.add(facets.getTopChildren(10, "Author"));
+    results.add(facets.getTopChildren(10, "Publish Date"));
     
     indexReader.close();
     taxoReader.close();
     
-    return facetResults;
+    return results;
   }
   
   /** User drills down on 'Publish Date/2010'. */
-  private List<FacetResult> drillDown() throws IOException {
+  private FacetResult drillDown() throws IOException {
     DirectoryReader indexReader = DirectoryReader.open(indexDir);
     IndexSearcher searcher = new IndexSearcher(indexReader);
     TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
 
-    // Now user drills down on Publish Date/2010:
-    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath("Author"), 10));
-
     // Passing no baseQuery means we drill down on all
     // documents ("browse only"):
-    DrillDownQuery q = new DrillDownQuery(fsp.indexingParams);
-    q.add(new CategoryPath("Publish Date/2010", '/'));
-    FacetsCollector fc = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);
-    searcher.search(q, fc);
+    DrillDownQuery q = new DrillDownQuery(config);
+
+    // Now user drills down on Publish Date/2010:
+    q.add("Publish Date", "2010");
+    FacetsCollector fc = new FacetsCollector();
+    FacetsCollector.search(searcher, q, 10, fc);
 
     // Retrieve results
-    List<FacetResult> facetResults = fc.getFacetResults();
-    
+    Facets facets = new FastTaxonomyFacetCounts(taxoReader, config, fc);
+    FacetResult result = facets.getTopChildren(10, "Author");
+
     indexReader.close();
     taxoReader.close();
     
-    return facetResults;
+    return result;
   }
 
   /** Runs the search example. */
@@ -143,7 +151,7 @@
   }
   
   /** Runs the drill-down example. */
-  public List<FacetResult> runDrillDown() throws IOException {
+  public FacetResult runDrillDown() throws IOException {
     index();
     return drillDown();
   }
@@ -152,18 +160,15 @@
   public static void main(String[] args) throws Exception {
     System.out.println("Facet counting example:");
     System.out.println("-----------------------");
-    List<FacetResult> results = new SimpleFacetsExample().runSearch();
-    for (FacetResult res : results) {
-      System.out.println(res);
-    }
+    SimpleFacetsExample example = new SimpleFacetsExample();
+    List<FacetResult> results = example.runSearch();
+    System.out.println("Author: " + results.get(0));
+    System.out.println("Publish Date: " + results.get(1));
 
     System.out.println("\n");
     System.out.println("Facet drill-down example (Publish Date/2010):");
     System.out.println("---------------------------------------------");
-    results = new SimpleFacetsExample().runDrillDown();
-    for (FacetResult res : results) {
-      System.out.println(res);
-    }
+    System.out.println("Author: " + example.runDrillDown());
   }
   
 }


diff -ruN -x .svn -x build trunk/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java simplefacets/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java
--- trunk/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java	2013-07-29 13:55:02.729707538 -0400
+++ simplefacets/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java	2013-11-27 19:36:42.531872630 -0500
@@ -23,15 +23,15 @@
 
 import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.CountFacetRequest;
-import org.apache.lucene.facet.search.DrillDownQuery;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.sortedset.SortedSetDocValuesAccumulator;
-import org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetFields;
-import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.DrillDownQuery;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.SortedSetDocValuesFacetCounts;
+import org.apache.lucene.facet.SortedSetDocValuesFacetField;
+import org.apache.lucene.facet.SortedSetDocValuesReaderState;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -47,34 +47,40 @@
 public class SimpleSortedSetFacetsExample {
 
   private final Directory indexDir = new RAMDirectory();
+  private final FacetsConfig config = new FacetsConfig();
 
   /** Empty constructor */
-  public SimpleSortedSetFacetsExample() {}
-  
-  private void add(IndexWriter indexWriter, SortedSetDocValuesFacetFields facetFields, String ... categoryPaths) throws IOException {
-    Document doc = new Document();
-    
-    List<CategoryPath> paths = new ArrayList<CategoryPath>();
-    for (String categoryPath : categoryPaths) {
-      paths.add(new CategoryPath(categoryPath, '/'));
-    }
-    facetFields.addFields(doc, paths);
-    indexWriter.addDocument(doc);
+  public SimpleSortedSetFacetsExample() {
   }
 
   /** Build the example index. */
   private void index() throws IOException {
     IndexWriter indexWriter = new IndexWriter(indexDir, new IndexWriterConfig(FacetExamples.EXAMPLES_VER, 
         new WhitespaceAnalyzer(FacetExamples.EXAMPLES_VER)));
-
-    // Reused across documents, to add the necessary facet fields
-    SortedSetDocValuesFacetFields facetFields = new SortedSetDocValuesFacetFields();
-
-    add(indexWriter, facetFields, "Author/Bob", "Publish Year/2010");
-    add(indexWriter, facetFields, "Author/Lisa", "Publish Year/2010");
-    add(indexWriter, facetFields, "Author/Lisa", "Publish Year/2012");
-    add(indexWriter, facetFields, "Author/Susan", "Publish Year/2012");
-    add(indexWriter, facetFields, "Author/Frank", "Publish Year/1999");
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("Author", "Bob"));
+    doc.add(new SortedSetDocValuesFacetField("Publish Year", "2010"));
+    indexWriter.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("Author", "Lisa"));
+    doc.add(new SortedSetDocValuesFacetField("Publish Year", "2010"));
+    indexWriter.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("Author", "Lisa"));
+    doc.add(new SortedSetDocValuesFacetField("Publish Year", "2012"));
+    indexWriter.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("Author", "Susan"));
+    doc.add(new SortedSetDocValuesFacetField("Publish Year", "2012"));
+    indexWriter.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("Author", "Frank"));
+    doc.add(new SortedSetDocValuesFacetField("Publish Year", "1999"));
+    indexWriter.addDocument(config.build(doc));
     
     indexWriter.close();
   }
@@ -85,47 +91,43 @@
     IndexSearcher searcher = new IndexSearcher(indexReader);
     SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(indexReader);
 
-    // Count both "Publish Year" and "Author" dimensions
-    FacetSearchParams fsp = new FacetSearchParams(
-        new CountFacetRequest(new CategoryPath("Publish Year"), 10), 
-        new CountFacetRequest(new CategoryPath("Author"), 10));
-
     // Aggregatses the facet counts
-    FacetsCollector fc = FacetsCollector.create(new SortedSetDocValuesAccumulator(state, fsp));
+    FacetsCollector fc = new FacetsCollector();
 
     // MatchAllDocsQuery is for "browsing" (counts facets
     // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), fc);
+    // you'd use a "normal" query:
+    FacetsCollector.search(searcher, new MatchAllDocsQuery(), 10, fc);
 
     // Retrieve results
-    List<FacetResult> facetResults = fc.getFacetResults();
-    
+    Facets facets = new SortedSetDocValuesFacetCounts(state, fc);
+
+    List<FacetResult> results = new ArrayList<FacetResult>();
+    results.add(facets.getTopChildren(10, "Author"));
+    results.add(facets.getTopChildren(10, "Publish Year"));
     indexReader.close();
     
-    return facetResults;
+    return results;
   }
   
   /** User drills down on 'Publish Year/2010'. */
-  private List<FacetResult> drillDown() throws IOException {
+  private FacetResult drillDown() throws IOException {
     DirectoryReader indexReader = DirectoryReader.open(indexDir);
     IndexSearcher searcher = new IndexSearcher(indexReader);
     SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(indexReader);
 
     // Now user drills down on Publish Year/2010:
-    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath("Author"), 10));
-    DrillDownQuery q = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
-    q.add(new CategoryPath("Publish Year/2010", '/'));
-    FacetsCollector fc = FacetsCollector.create(new SortedSetDocValuesAccumulator(state, fsp));
-    searcher.search(q, fc);
+    DrillDownQuery q = new DrillDownQuery(config);
+    q.add("Publish Year", "2010");
+    FacetsCollector fc = new FacetsCollector();
+    FacetsCollector.search(searcher, q, 10, fc);
 
     // Retrieve results
-    List<FacetResult> facetResults = fc.getFacetResults();
-    
+    Facets facets = new SortedSetDocValuesFacetCounts(state, fc);
+    FacetResult result = facets.getTopChildren(10, "Author");
     indexReader.close();
     
-    return facetResults;
+    return result;
   }
 
   /** Runs the search example. */
@@ -135,7 +137,7 @@
   }
   
   /** Runs the drill-down example. */
-  public List<FacetResult> runDrillDown() throws IOException {
+  public FacetResult runDrillDown() throws IOException {
     index();
     return drillDown();
   }
@@ -144,18 +146,14 @@
   public static void main(String[] args) throws Exception {
     System.out.println("Facet counting example:");
     System.out.println("-----------------------");
-    List<FacetResult> results = new SimpleSortedSetFacetsExample().runSearch();
-    for (FacetResult res : results) {
-      System.out.println(res);
-    }
+    SimpleSortedSetFacetsExample example = new SimpleSortedSetFacetsExample();
+    List<FacetResult> results = example.runSearch();
+    System.out.println("Author: " + results.get(0));
+    System.out.println("Publish Year: " + results.get(0));
 
     System.out.println("\n");
     System.out.println("Facet drill-down example (Publish Year/2010):");
     System.out.println("---------------------------------------------");
-    results = new SimpleSortedSetFacetsExample().runDrillDown();
-    for (FacetResult res : results) {
-      System.out.println(res);
-    }
+    System.out.println("Author: " + example.runDrillDown());
   }
-  
 }


diff -ruN -x .svn -x build trunk/lucene/demo/src/test/org/apache/lucene/demo/facet/TestAssociationsFacetsExample.java simplefacets/lucene/demo/src/test/org/apache/lucene/demo/facet/TestAssociationsFacetsExample.java
--- trunk/lucene/demo/src/test/org/apache/lucene/demo/facet/TestAssociationsFacetsExample.java	2013-02-20 13:38:17.808711922 -0500
+++ simplefacets/lucene/demo/src/test/org/apache/lucene/demo/facet/TestAssociationsFacetsExample.java	2013-12-02 15:31:57.902969325 -0500
@@ -1,12 +1,5 @@
 package org.apache.lucene.demo.facet;
 
-import java.util.List;
-
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.util.LuceneTestCase;
-import org.junit.Test;
-
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -24,32 +17,19 @@
  * limitations under the License.
  */
 
+import java.util.List;
+
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+
 public class TestAssociationsFacetsExample extends LuceneTestCase {
   
-  private static final double[] EXPECTED_INT_SUM_RESULTS = { 4, 2};
-  private static final double[] EXPECTED_FLOAT_SUM_RESULTS = { 1.62, 0.34};
-
   @Test
   public void testExamples() throws Exception {
     List<FacetResult> res = new AssociationsFacetsExample().runSumAssociations();
-   
     assertEquals("Wrong number of results", 2, res.size());
-    
-    for (FacetResult fres : res) {
-      assertEquals("Wrong number of facets", 2, fres.getNumValidDescendants());
-    }
-    
-    Iterable<? extends FacetResultNode> it = res.get(0).getFacetResultNode().subResults;
-    int i = 0;
-    for (FacetResultNode fResNode : it) {
-      assertEquals("Wrong result for facet " + fResNode.label, EXPECTED_INT_SUM_RESULTS[i++], fResNode.value, 1E-5);
-    }
-    
-    it = res.get(1).getFacetResultNode().subResults;
-    i = 0;
-    for (FacetResultNode fResNode : it) {
-      assertEquals("Wrong result for facet " + fResNode.label, EXPECTED_FLOAT_SUM_RESULTS[i++], fResNode.value, 1E-5);
-    }
-  }
-  
+    assertEquals("dim=tags path=[] value=-1 childCount=2\n  lucene (4)\n  solr (2)\n", res.get(0).toString());
+    assertEquals("dim=genre path=[] value=-1.0 childCount=2\n  computing (1.62)\n  software (0.34)\n", res.get(1).toString());
+  }  
 }


diff -ruN -x .svn -x build trunk/lucene/demo/src/test/org/apache/lucene/demo/facet/TestDistanceFacetsExample.java simplefacets/lucene/demo/src/test/org/apache/lucene/demo/facet/TestDistanceFacetsExample.java
--- trunk/lucene/demo/src/test/org/apache/lucene/demo/facet/TestDistanceFacetsExample.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/demo/src/test/org/apache/lucene/demo/facet/TestDistanceFacetsExample.java	2013-12-02 15:30:16.802972033 -0500
@@ -0,0 +1,44 @@
+package org.apache.lucene.demo.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
+import org.apache.lucene.util.LuceneTestCase;
+
+@SuppressCodecs("Lucene3x")
+public class TestDistanceFacetsExample extends LuceneTestCase {
+
+  public void testSimple() throws Exception {
+    DistanceFacetsExample example = new DistanceFacetsExample();
+    example.index();
+    FacetResult result = example.search();
+    assertEquals("dim=field path=[] value=3 childCount=4\n  < 1 km (1)\n  < 2 km (2)\n  < 5 km (2)\n  < 10 km (3)\n", result.toString());
+    example.close();
+  }
+
+  public void testDrillDown() throws Exception {
+    DistanceFacetsExample example = new DistanceFacetsExample();
+    example.index();
+    TopDocs hits = example.drillDown(example.FIVE_KM);
+    assertEquals(2, hits.totalHits);
+    example.close();
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/demo/src/test/org/apache/lucene/demo/facet/TestExpressionAggregationFacetsExample.java simplefacets/lucene/demo/src/test/org/apache/lucene/demo/facet/TestExpressionAggregationFacetsExample.java
--- trunk/lucene/demo/src/test/org/apache/lucene/demo/facet/TestExpressionAggregationFacetsExample.java	2013-11-06 07:02:49.321619618 -0500
+++ simplefacets/lucene/demo/src/test/org/apache/lucene/demo/facet/TestExpressionAggregationFacetsExample.java	2013-12-02 15:30:34.506971557 -0500
@@ -1,13 +1,5 @@
 package org.apache.lucene.demo.facet;
 
-import java.util.List;
-import java.util.Locale;
-
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.util.LuceneTestCase;
-import org.junit.Test;
-
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -25,25 +17,18 @@
  * limitations under the License.
  */
 
-public class TestExpressionAggregationFacetsExample extends LuceneTestCase {
+import java.util.List;
+import java.util.Locale;
 
-  private static String toSimpleString(FacetResult fr) {
-    StringBuilder sb = new StringBuilder();
-    toSimpleString(fr.getFacetRequest().categoryPath.length, 0, sb, fr.getFacetResultNode(), "");
-    return sb.toString();
-  }
-  
-  private static void toSimpleString(int startLength, int depth, StringBuilder sb, FacetResultNode node, String indent) {
-    sb.append(String.format(Locale.ROOT, "%s%s (%.3f)\n", indent, node.label.components[startLength + depth - 1], node.value));
-    for (FacetResultNode childNode : node.subResults) {
-      toSimpleString(startLength, depth + 1, sb, childNode, indent + "  ");
-    }
-  }
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+
+public class TestExpressionAggregationFacetsExample extends LuceneTestCase {
 
   @Test
   public void testSimple() throws Exception {
-    List<FacetResult> facetResults = new ExpressionAggregationFacetsExample().runSearch();
-    assertEquals("A (0.000)\n  B (2.236)\n  C (1.732)\n", toSimpleString(facetResults.get(0)));
+    FacetResult result = new ExpressionAggregationFacetsExample().runSearch();
+    assertEquals("dim=A path=[] value=3.9681187 childCount=2\n  B (2.236068)\n  C (1.7320508)\n", result.toString());
   }
-  
 }


diff -ruN -x .svn -x build trunk/lucene/demo/src/test/org/apache/lucene/demo/facet/TestMultiCategoryListsFacetsExample.java simplefacets/lucene/demo/src/test/org/apache/lucene/demo/facet/TestMultiCategoryListsFacetsExample.java
--- trunk/lucene/demo/src/test/org/apache/lucene/demo/facet/TestMultiCategoryListsFacetsExample.java	2013-02-20 13:38:17.808711922 -0500
+++ simplefacets/lucene/demo/src/test/org/apache/lucene/demo/facet/TestMultiCategoryListsFacetsExample.java	2013-12-02 15:31:43.242969721 -0500
@@ -1,14 +1,5 @@
 package org.apache.lucene.demo.facet;
 
-import java.util.List;
-
-import org.apache.lucene.facet.collections.ObjectToIntMap;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.util.LuceneTestCase;
-import org.junit.Test;
-
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -26,33 +17,19 @@
  * limitations under the License.
  */
 
-public class TestMultiCategoryListsFacetsExample extends LuceneTestCase {
+import java.util.List;
 
-  private static final ObjectToIntMap<CategoryPath> expectedCounts = new ObjectToIntMap<CategoryPath>();
-  static {
-    expectedCounts.put(new CategoryPath("Publish Date", "2012"), 2);
-    expectedCounts.put(new CategoryPath("Publish Date", "2010"), 2);
-    expectedCounts.put(new CategoryPath("Publish Date", "1999"), 1);
-    expectedCounts.put(new CategoryPath("Author", "Lisa"), 2);
-    expectedCounts.put(new CategoryPath("Author", "Frank"), 1);
-    expectedCounts.put(new CategoryPath("Author", "Susan"), 1);
-    expectedCounts.put(new CategoryPath("Author", "Bob"), 1);
-  }
-  
-  private void assertExpectedCounts(List<FacetResult> facetResults, ObjectToIntMap<CategoryPath> expCounts) {
-    for (FacetResult res : facetResults) {
-      FacetResultNode root = res.getFacetResultNode();
-      for (FacetResultNode node : root.subResults) {
-        assertEquals("incorrect count for " + node.label, expCounts.get(node.label), (int) node.value);
-      }
-    }
-  }
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+
+public class TestMultiCategoryListsFacetsExample extends LuceneTestCase {
 
   @Test
   public void testExample() throws Exception {
-    List<FacetResult> facetResults = new MultiCategoryListsFacetsExample().runSearch();
-    assertEquals(2, facetResults.size());
-    assertExpectedCounts(facetResults, expectedCounts);
+    List<FacetResult> results = new MultiCategoryListsFacetsExample().runSearch();
+    assertEquals(2, results.size());
+    assertEquals("dim=Author path=[] value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", results.get(0).toString());
+    assertEquals("dim=Publish Date path=[] value=5 childCount=3\n  2010 (2)\n  2012 (2)\n  1999 (1)\n", results.get(1).toString());
   }
-  
 }


diff -ruN -x .svn -x build trunk/lucene/demo/src/test/org/apache/lucene/demo/facet/TestRangeFacetsExample.java simplefacets/lucene/demo/src/test/org/apache/lucene/demo/facet/TestRangeFacetsExample.java
--- trunk/lucene/demo/src/test/org/apache/lucene/demo/facet/TestRangeFacetsExample.java	2013-07-15 15:52:17.705877389 -0400
+++ simplefacets/lucene/demo/src/test/org/apache/lucene/demo/facet/TestRangeFacetsExample.java	2013-12-18 19:37:04.753595267 -0500
@@ -19,12 +19,7 @@
 
 import java.util.List;
 
-import org.apache.lucene.facet.collections.ObjectToIntMap;
-import org.apache.lucene.facet.range.LongRange;
-import org.apache.lucene.facet.range.RangeFacetRequest;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.FacetResult;
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 import org.apache.lucene.util.LuceneTestCase;
@@ -33,27 +28,12 @@
 @SuppressCodecs("Lucene3x")
 public class TestRangeFacetsExample extends LuceneTestCase {
 
-  private static final ObjectToIntMap<CategoryPath> expectedCounts = new ObjectToIntMap<CategoryPath>();
-  static {
-    expectedCounts.put(new CategoryPath("timestamp", "Past hour"), 4);
-    expectedCounts.put(new CategoryPath("timestamp", "Past six hours"), 22);
-    expectedCounts.put(new CategoryPath("timestamp", "Past day"), 87);
-  }
-  
-  private void assertExpectedCounts(FacetResult res, ObjectToIntMap<CategoryPath> expCounts) {
-    FacetResultNode root = res.getFacetResultNode();
-    for (FacetResultNode node : root.subResults) {
-      assertEquals("incorrect count for " + node.label, expCounts.get(node.label), (int) node.value);
-    }
-  }
-  
   @Test
   public void testSimple() throws Exception {
     RangeFacetsExample example = new RangeFacetsExample();
     example.index();
-    List<FacetResult> facetResults = example.search();
-    assertEquals(1, facetResults.size());
-    assertExpectedCounts(facetResults.get(0), expectedCounts);
+    FacetResult result = example.search();
+    assertEquals("dim=timestamp path=[] value=87 childCount=3\n  Past hour (4)\n  Past six hours (22)\n  Past day (87)\n", result.toString());
     example.close();
   }
 
@@ -62,8 +42,7 @@
   public void testDrillDown() throws Exception {
     RangeFacetsExample example = new RangeFacetsExample();
     example.index();
-    List<FacetResult> facetResults = example.search();
-    TopDocs hits = example.drillDown((LongRange) ((RangeFacetRequest<LongRange>) facetResults.get(0).getFacetRequest()).ranges[1]);
+    TopDocs hits = example.drillDown(example.PAST_SIX_HOURS);
     assertEquals(22, hits.totalHits);
     example.close();
   }


diff -ruN -x .svn -x build trunk/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleFacetsExample.java simplefacets/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleFacetsExample.java
--- trunk/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleFacetsExample.java	2013-02-20 13:38:17.808711922 -0500
+++ simplefacets/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleFacetsExample.java	2013-12-02 15:31:01.762970829 -0500
@@ -1,14 +1,5 @@
 package org.apache.lucene.demo.facet;
 
-import java.util.List;
-
-import org.apache.lucene.facet.collections.ObjectToIntMap;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.util.LuceneTestCase;
-import org.junit.Test;
-
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -26,46 +17,26 @@
  * limitations under the License.
  */
 
+import java.util.List;
+
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+
 public class TestSimpleFacetsExample extends LuceneTestCase {
 
-  private static final ObjectToIntMap<CategoryPath> expectedCounts = new ObjectToIntMap<CategoryPath>();
-  static {
-    expectedCounts.put(new CategoryPath("Publish Date", "2012"), 2);
-    expectedCounts.put(new CategoryPath("Publish Date", "2010"), 2);
-    expectedCounts.put(new CategoryPath("Publish Date", "1999"), 1);
-    expectedCounts.put(new CategoryPath("Author", "Lisa"), 2);
-    expectedCounts.put(new CategoryPath("Author", "Frank"), 1);
-    expectedCounts.put(new CategoryPath("Author", "Susan"), 1);
-    expectedCounts.put(new CategoryPath("Author", "Bob"), 1);
-  }
-  
-  private static final ObjectToIntMap<CategoryPath> expectedCountsDrillDown = new ObjectToIntMap<CategoryPath>();
-  static {
-    expectedCountsDrillDown.put(new CategoryPath("Author", "Lisa"), 1);
-    expectedCountsDrillDown.put(new CategoryPath("Author", "Bob"), 1);
-  }
-  
-  private void assertExpectedCounts(List<FacetResult> facetResults, ObjectToIntMap<CategoryPath> expCounts) {
-    for (FacetResult res : facetResults) {
-      FacetResultNode root = res.getFacetResultNode();
-      for (FacetResultNode node : root.subResults) {
-        assertEquals("incorrect count for " + node.label, expCounts.get(node.label), (int) node.value);
-      }
-    }
-  }
-  
   @Test
   public void testSimple() throws Exception {
-    List<FacetResult> facetResults = new SimpleFacetsExample().runSearch();
-    assertEquals(2, facetResults.size());
-    assertExpectedCounts(facetResults, expectedCounts);
+    List<FacetResult> results = new SimpleFacetsExample().runSearch();
+    assertEquals(2, results.size());
+    assertEquals("dim=Author path=[] value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", results.get(0).toString());
+    assertEquals("dim=Publish Date path=[] value=5 childCount=3\n  2010 (2)\n  2012 (2)\n  1999 (1)\n", results.get(1).toString());
   }
 
   @Test
   public void testDrillDown() throws Exception {
-    List<FacetResult> facetResults = new SimpleFacetsExample().runDrillDown();
-    assertEquals(1, facetResults.size());
-    assertExpectedCounts(facetResults, expectedCountsDrillDown);
+    FacetResult result = new SimpleFacetsExample().runDrillDown();
+    assertEquals("dim=Author path=[] value=2 childCount=2\n  Bob (1)\n  Lisa (1)\n", result.toString());
   }
-  
 }


diff -ruN -x .svn -x build trunk/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleSortedSetFacetsExample.java simplefacets/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleSortedSetFacetsExample.java
--- trunk/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleSortedSetFacetsExample.java	2013-11-04 06:14:43.234319371 -0500
+++ simplefacets/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleSortedSetFacetsExample.java	2013-12-02 15:31:26.150970176 -0500
@@ -1,15 +1,5 @@
 package org.apache.lucene.demo.facet;
 
-import java.util.List;
-
-import org.apache.lucene.facet.collections.ObjectToIntMap;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
-import org.apache.lucene.util.LuceneTestCase;
-import org.junit.Test;
-
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -27,48 +17,30 @@
  * limitations under the License.
  */
 
+import java.util.List;
+
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+
+
 // We require sorted set DVs:
 @SuppressCodecs({"Lucene40", "Lucene41"})
 public class TestSimpleSortedSetFacetsExample extends LuceneTestCase {
 
-  private static final ObjectToIntMap<CategoryPath> expectedCounts = new ObjectToIntMap<CategoryPath>();
-  static {
-    expectedCounts.put(new CategoryPath("Publish Year", "2012"), 2);
-    expectedCounts.put(new CategoryPath("Publish Year", "2010"), 2);
-    expectedCounts.put(new CategoryPath("Publish Year", "1999"), 1);
-    expectedCounts.put(new CategoryPath("Author", "Lisa"), 2);
-    expectedCounts.put(new CategoryPath("Author", "Frank"), 1);
-    expectedCounts.put(new CategoryPath("Author", "Susan"), 1);
-    expectedCounts.put(new CategoryPath("Author", "Bob"), 1);
-  }
-  
-  private static final ObjectToIntMap<CategoryPath> expectedCountsDrillDown = new ObjectToIntMap<CategoryPath>();
-  static {
-    expectedCountsDrillDown.put(new CategoryPath("Author", "Lisa"), 1);
-    expectedCountsDrillDown.put(new CategoryPath("Author", "Bob"), 1);
-  }
-  
-  private void assertExpectedCounts(List<FacetResult> facetResults, ObjectToIntMap<CategoryPath> expCounts) {
-    for (FacetResult res : facetResults) {
-      FacetResultNode root = res.getFacetResultNode();
-      for (FacetResultNode node : root.subResults) {
-        assertEquals("incorrect count for " + node.label, expCounts.get(node.label), (int) node.value);
-      }
-    }
-  }
-  
   @Test
   public void testSimple() throws Exception {
-    List<FacetResult> facetResults = new SimpleSortedSetFacetsExample().runSearch();
-    assertEquals(2, facetResults.size());
-    assertExpectedCounts(facetResults, expectedCounts);
+    List<FacetResult> results = new SimpleSortedSetFacetsExample().runSearch();
+    assertEquals(2, results.size());
+    assertEquals("dim=Author path=[] value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Frank (1)\n  Susan (1)\n", results.get(0).toString());
+    assertEquals("dim=Publish Year path=[] value=5 childCount=3\n  2010 (2)\n  2012 (2)\n  1999 (1)\n", results.get(1).toString());
   }
 
   @Test
   public void testDrillDown() throws Exception {
-    List<FacetResult> facetResults = new SimpleSortedSetFacetsExample().runDrillDown();
-    assertEquals(1, facetResults.size());
-    assertExpectedCounts(facetResults, expectedCountsDrillDown);
+    FacetResult result = new SimpleSortedSetFacetsExample().runDrillDown();
+    assertEquals("dim=Author path=[] value=2 childCount=2\n  Bob (1)\n  Lisa (1)\n", result.toString());
   }
-  
 }


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/AssociationFacetField.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/AssociationFacetField.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/AssociationFacetField.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/AssociationFacetField.java	2013-11-26 18:29:33.006291509 -0500
@@ -0,0 +1,72 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.document.Document; // javadocs
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.util.BytesRef;
+
+/** Add an instance of this to your {@link Document} to add
+ *  a facet label associated with an arbitrary byte[].
+ *  This will require a custom {@link Facets}
+ *  implementation at search time; see {@link
+ *  IntAssociationFacetField} and {@link
+ *  FloatAssociationFacetField} to use existing {@link
+ *  Facets} implementations.
+ * 
+ *  @lucene.experimental */
+public class AssociationFacetField extends Field {
+  static final FieldType TYPE = new FieldType();
+  static {
+    TYPE.setIndexed(true);
+    TYPE.freeze();
+  }
+  protected final String dim;
+  protected final String[] path;
+  protected final BytesRef assoc;
+
+  /** Creates this from {@code dim} and {@code path} and an
+   *  association */
+  public AssociationFacetField(BytesRef assoc, String dim, String... path) {
+    super("dummy", TYPE);
+    this.dim = dim;
+    this.assoc = assoc;
+    if (path.length == 0) {
+      throw new IllegalArgumentException("path must have at least one element");
+    }
+    this.path = path;
+  }
+
+  private static BytesRef intToBytesRef(int v) {
+    byte[] bytes = new byte[4];
+    // big-endian:
+    bytes[0] = (byte) (v >> 24);
+    bytes[1] = (byte) (v >> 16);
+    bytes[2] = (byte) (v >> 8);
+    bytes[3] = (byte) v;
+    return new BytesRef(bytes);
+  }
+
+  @Override
+  public String toString() {
+    return "AssociationFacetField(dim=" + dim + " path=" + Arrays.toString(path) + " bytes=" + assoc + ")";
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsDrillDownStream.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsDrillDownStream.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsDrillDownStream.java	2013-02-20 13:38:17.572711926 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsDrillDownStream.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,74 +0,0 @@
-package org.apache.lucene.facet.associations;
-
-import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
-import org.apache.lucene.facet.index.DrillDownStream;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.store.ByteArrayDataOutput;
-import org.apache.lucene.util.BytesRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link DrillDownStream} which adds to each drill-down token a
- * payload according to the {@link CategoryAssociation} defined in the
- * {@link CategoryAssociationsContainer}.
- * 
- * @lucene.experimental
- */
-public class AssociationsDrillDownStream extends DrillDownStream {
-
-  private final PayloadAttribute payloadAttribute;
-  private final BytesRef payload;
-  private final ByteArrayDataOutput output = new ByteArrayDataOutput();
-  private final CategoryAssociationsContainer associations;
-
-  public AssociationsDrillDownStream(CategoryAssociationsContainer associations, FacetIndexingParams indexingParams) {
-    super(associations, indexingParams);
-    this.associations = associations;
-    payloadAttribute = addAttribute(PayloadAttribute.class);
-    BytesRef bytes = payloadAttribute.getPayload();
-    if (bytes == null) {
-      bytes = new BytesRef(new byte[4]);
-      payloadAttribute.setPayload(bytes);
-    }
-    bytes.offset = 0;
-    this.payload = bytes;
-  }
-  
-  @Override
-  protected void addAdditionalAttributes(CategoryPath cp, boolean isParent) {
-    if (isParent) {
-      return; // associations are not added to parent categories
-    }
-    
-    CategoryAssociation association = associations.getAssociation(cp);
-    if (association == null) {
-      // it is ok to set a null association for a category - it's treated as a
-      // regular category in that case.
-      return;
-    }
-    if (payload.bytes.length < association.maxBytesNeeded()) {
-      payload.grow(association.maxBytesNeeded());
-    }
-    output.reset(payload.bytes);
-    association.serialize(output);
-    payload.length = output.getPosition();
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsFacetFields.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsFacetFields.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsFacetFields.java	2013-02-20 13:38:17.572711926 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsFacetFields.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,124 +0,0 @@
-package org.apache.lucene.facet.associations;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.facet.index.DrillDownStream;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A utility class for adding facet fields to a document. Usually one field will
- * be added for all facets, however per the
- * {@link FacetIndexingParams#getCategoryListParams(CategoryPath)}, one field
- * may be added for every group of facets.
- * 
- * @lucene.experimental
- */
-public class AssociationsFacetFields extends FacetFields {
-
-  // The drill-down field is added with a TokenStream, hence why it's based on
-  // TextField type. However for associations, we store a payload with the
-  // association value, therefore we set IndexOptions to include positions.
-  private static final FieldType DRILL_DOWN_TYPE = new FieldType(TextField.TYPE_NOT_STORED);
-  static {
-    DRILL_DOWN_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
-    DRILL_DOWN_TYPE.freeze();
-  }
-  
-  /**
-   * Constructs a new instance with the {@link FacetIndexingParams#DEFAULT
-   * default} facet indexing params.
-   * 
-   * @param taxonomyWriter
-   *          used to resolve given categories to ordinals
-   */
-  public AssociationsFacetFields(TaxonomyWriter taxonomyWriter) {
-    super(taxonomyWriter);
-  }
-
-  /**
-   * Constructs a new instance with the given facet indexing params.
-   * 
-   * @param taxonomyWriter
-   *          used to resolve given categories to ordinals
-   * @param params
-   *          determines under which fields the categories should be indexed
-   */
-  public AssociationsFacetFields(TaxonomyWriter taxonomyWriter, FacetIndexingParams params) {
-    super(taxonomyWriter, params);
-  }
-
-  @Override
-  protected Map<CategoryListParams,Iterable<CategoryPath>> createCategoryListMapping(
-      Iterable<CategoryPath> categories) {
-    CategoryAssociationsContainer categoryAssociations = (CategoryAssociationsContainer) categories;
-    HashMap<CategoryListParams,Iterable<CategoryPath>> categoryLists = 
-        new HashMap<CategoryListParams,Iterable<CategoryPath>>();
-    for (CategoryPath cp : categories) {
-      // each category may be indexed under a different field, so add it to the right list.
-      CategoryListParams clp = indexingParams.getCategoryListParams(cp);
-      CategoryAssociationsContainer clpContainer = (CategoryAssociationsContainer) categoryLists.get(clp);
-      if (clpContainer == null) {
-        clpContainer = new CategoryAssociationsContainer();
-        categoryLists.put(clp, clpContainer);
-      }
-      clpContainer.setAssociation(cp, categoryAssociations.getAssociation(cp));
-    }
-    return categoryLists;
-  }
-  
-  @Override
-  protected Map<String,BytesRef> getCategoryListData(CategoryListParams categoryListParams, IntsRef ordinals,
-      Iterable<CategoryPath> categories) throws IOException {
-    AssociationsListBuilder associations = new AssociationsListBuilder((CategoryAssociationsContainer) categories);
-    return associations.build(ordinals, categories);
-  }
-  
-  @Override
-  protected DrillDownStream getDrillDownStream(Iterable<CategoryPath> categories) {
-    return new AssociationsDrillDownStream((CategoryAssociationsContainer) categories, indexingParams);
-  }
-  
-  @Override
-  protected FieldType drillDownFieldType() {
-    return DRILL_DOWN_TYPE;
-  }
-
-  @Override
-  public void addFields(Document doc, Iterable<CategoryPath> categories) throws IOException {
-    if (!(categories instanceof CategoryAssociationsContainer)) {
-      throw new IllegalArgumentException("categories must be of type " + 
-          CategoryAssociationsContainer.class.getSimpleName());
-    }
-    super.addFields(doc, categories);
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsListBuilder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsListBuilder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsListBuilder.java	2013-07-24 09:03:17.397732274 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsListBuilder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,82 +0,0 @@
-package org.apache.lucene.facet.associations;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.facet.index.CategoryListBuilder;
-import org.apache.lucene.facet.index.CountingListBuilder;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.store.ByteArrayDataOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link AssociationsListBuilder} which encodes category-association value pairs.
- * Every category-association pair is written under the respective association's
- * {@link CategoryAssociation#getCategoryListID()}.
- * <p>
- * <b>NOTE:</b> associations list do not encode the counting list data. You
- * should use {@link CountingListBuilder} to build that information and then
- * merge the results of both {@link #build(IntsRef, Iterable)}.
- */
-public class AssociationsListBuilder implements CategoryListBuilder {
-  
-  private final CategoryAssociationsContainer associations;
-  private final ByteArrayDataOutput output = new ByteArrayDataOutput();
-  
-  public AssociationsListBuilder(CategoryAssociationsContainer associations) {
-    this.associations = associations;
-  }
-  
-  @Override
-  public Map<String,BytesRef> build(IntsRef ordinals, Iterable<CategoryPath> categories) throws IOException {
-    final HashMap<String,BytesRef> res = new HashMap<String,BytesRef>();
-    int idx = 0;
-    for (CategoryPath cp : categories) {
-      // build per-association key BytesRef
-      CategoryAssociation association = associations.getAssociation(cp);
-      
-      BytesRef bytes = res.get(association.getCategoryListID());
-      if (bytes == null) {
-        bytes = new BytesRef(32);
-        res.put(association.getCategoryListID(), bytes);
-      }
-      
-      int maxBytesNeeded = 4 /* int */ + association.maxBytesNeeded() + bytes.length;
-      if (bytes.bytes.length < maxBytesNeeded) {
-        bytes.grow(maxBytesNeeded);
-      }
-      
-      // reset the output to write from bytes.length (current position) until the end
-      output.reset(bytes.bytes, bytes.length, bytes.bytes.length - bytes.length);
-      output.writeInt(ordinals.ints[idx++]);
-      
-      // encode the association bytes
-      association.serialize(output);
-      
-      // update BytesRef
-      bytes.length = output.getPosition();
-    }
-
-    return res;
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryAssociation.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryAssociation.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryAssociation.java	2013-01-03 10:38:26.911307612 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryAssociation.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,48 +0,0 @@
-package org.apache.lucene.facet.associations;
-
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.ByteArrayDataOutput;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.DataOutput;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Allows associating an arbitrary value with a {@link CategoryPath}.
- * 
- * @lucene.experimental
- */
-public interface CategoryAssociation {
-  
-  /** Serializes the associated value into the given {@link DataOutput}. */
-  public void serialize(ByteArrayDataOutput output);
-
-  /** Deserializes the association value from the given {@link DataInput}. */
-  public void deserialize(ByteArrayDataInput input);
-  
-  /** Returns the maximum bytes needed to encode the association value. */
-  public int maxBytesNeeded();
-  
-  /**
-   * Returns the ID of the category association. The ID is used as e.g. the
-   * term's text under which to encode the association values.
-   */
-  public String getCategoryListID();
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryAssociationsContainer.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryAssociationsContainer.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryAssociationsContainer.java	2013-07-24 09:03:17.397732274 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryAssociationsContainer.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,65 +0,0 @@
-package org.apache.lucene.facet.associations;
-
-import java.util.HashMap;
-import java.util.Iterator;
-
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Holds {@link CategoryAssociation} per {@link CategoryPath}. */
-public class CategoryAssociationsContainer implements Iterable<CategoryPath> {
-
-  private final HashMap<CategoryPath,CategoryAssociation> categoryAssociations = 
-      new HashMap<CategoryPath,CategoryAssociation>();
-  
-  /**
-   * Adds the {@link CategoryAssociation} for the given {@link CategoryPath
-   * category}. Overrides any assocation that was previously set.
-   */
-  public void setAssociation(CategoryPath category, CategoryAssociation association) {
-    if (association == null) {
-      throw new IllegalArgumentException("cannot set a null association to a category");
-    }
-    categoryAssociations.put(category, association);
-  }
-  
-  /**
-   * Returns the {@link CategoryAssociation} that was set for the
-   * {@link CategoryPath category}, or {@code null} if none was defined.
-   */
-  public CategoryAssociation getAssociation(CategoryPath category) {
-    return categoryAssociations.get(category);
-  }
-
-  @Override
-  public Iterator<CategoryPath> iterator() {
-    return categoryAssociations.keySet().iterator();
-  }
-  
-  /** Clears all category associations. */
-  public void clear() {
-    categoryAssociations.clear();
-  }
-
-  @Override
-  public String toString() {
-    return categoryAssociations.toString();
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryFloatAssociation.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryFloatAssociation.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryFloatAssociation.java	2013-01-11 08:39:05.208858239 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryFloatAssociation.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,80 +0,0 @@
-package org.apache.lucene.facet.associations;
-
-import java.io.IOException;
-
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.ByteArrayDataOutput;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** A {@link CategoryAssociation} that associates a float with a category. */
-public class CategoryFloatAssociation implements CategoryAssociation {
-  
-  public static final String ASSOCIATION_LIST_ID = "$assoc_float$";
-  
-  private float value;
-  
-  public CategoryFloatAssociation() {
-    // used for deserialization
-  }
-  
-  public CategoryFloatAssociation(float value) {
-    this.value = value;
-  }
-  
-  @Override
-  public void serialize(ByteArrayDataOutput output) {
-    try {
-      output.writeInt(Float.floatToIntBits(value));
-    } catch (IOException e) {
-      throw new RuntimeException("unexpected exception writing to a byte[]", e);
-    }
-  }
-  
-  @Override
-  public void deserialize(ByteArrayDataInput input) {
-    value = Float.intBitsToFloat(input.readInt());
-  }
-  
-  @Override
-  public int maxBytesNeeded() {
-    // plain integer
-    return 4;
-  }
-  
-  @Override
-  public String getCategoryListID() {
-    return ASSOCIATION_LIST_ID;
-  }
-  
-  /**
-   * Returns the value associated with a category. If you used
-   * {@link #CategoryFloatAssociation()}, you should call
-   * {@link #deserialize(ByteArrayDataInput)} before calling this method, or
-   * otherwise the value returned is undefined.
-   */
-  public float getValue() {
-    return value;
-  }
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(" + value + ")";
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryIntAssociation.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryIntAssociation.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryIntAssociation.java	2013-01-11 08:39:05.208858239 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryIntAssociation.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,80 +0,0 @@
-package org.apache.lucene.facet.associations;
-
-import java.io.IOException;
-
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.ByteArrayDataOutput;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** A {@link CategoryAssociation} that associates an integer with a category. */
-public class CategoryIntAssociation implements CategoryAssociation {
-  
-  public static final String ASSOCIATION_LIST_ID = "$assoc_int$";
-  
-  private int value;
-  
-  public CategoryIntAssociation() {
-    // used for deserialization
-  }
-  
-  public CategoryIntAssociation(int value) {
-    this.value = value;
-  }
-  
-  @Override
-  public void serialize(ByteArrayDataOutput output) {
-    try {
-      output.writeInt(value);
-    } catch (IOException e) {
-      throw new RuntimeException("unexpected exception writing to a byte[]", e);
-    }
-  }
-  
-  @Override
-  public void deserialize(ByteArrayDataInput input) {
-    value = input.readInt();
-  }
-  
-  @Override
-  public int maxBytesNeeded() {
-    // plain integer
-    return 4;
-  }
-  
-  @Override
-  public String getCategoryListID() {
-    return ASSOCIATION_LIST_ID;
-  }
-  
-  /**
-   * Returns the value associated with a category. If you used
-   * {@link #CategoryIntAssociation()}, you should call
-   * {@link #deserialize(ByteArrayDataInput)} before calling this method, or
-   * otherwise the value returned is undefined.
-   */
-  public int getValue() {
-    return value;
-  }
-  
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(" + value + ")";
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/package.html simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/package.html
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/package.html	2013-02-20 13:38:17.572711926 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/package.html	1969-12-31 19:00:00.000000000 -0500
@@ -1,25 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Category Association</title>
-</head>
-<body>
-Allows associating arbitrary values with a category. The value can be used e.g. to compute
-the category's weight during faceted search.
-</body>
-</html>


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/SumFloatAssociationFacetRequest.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/SumFloatAssociationFacetRequest.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/SumFloatAssociationFacetRequest.java	2013-08-01 14:47:20.738689730 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/SumFloatAssociationFacetRequest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,46 +0,0 @@
-package org.apache.lucene.facet.associations;
-
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetsAggregator;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetRequest} for weighting facets according to their float
- * association by summing the association values.
- * 
- * @lucene.experimental
- */
-public class SumFloatAssociationFacetRequest extends FacetRequest {
-
-  /**
-   * Create a float association facet request for a given node in the
-   * taxonomy.
-   */
-  public SumFloatAssociationFacetRequest(CategoryPath path, int num) {
-    super(path, num);
-  }
-
-  @Override
-  public FacetsAggregator createFacetsAggregator(FacetIndexingParams fip) {
-    return new SumFloatAssociationFacetsAggregator();
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/SumFloatAssociationFacetsAggregator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/SumFloatAssociationFacetsAggregator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/SumFloatAssociationFacetsAggregator.java	2013-08-01 14:47:20.738689730 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/SumFloatAssociationFacetsAggregator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,92 +0,0 @@
-package org.apache.lucene.facet.associations;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetsAggregator;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.search.OrdinalValueResolver;
-import org.apache.lucene.facet.search.OrdinalValueResolver.FloatValueResolver;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.util.BytesRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetsAggregator} which computes the weight of a category as the sum
- * of the float values associated with it in the result documents. Assumes that
- * the association encoded for each ordinal is {@link CategoryFloatAssociation}.
- * <p>
- * <b>NOTE:</b> this aggregator does not support
- * {@link #rollupValues(FacetRequest, int, int[], int[], FacetArrays)}. It only
- * aggregates the categories for which you added a {@link CategoryAssociation}.
- * 
- * @lucene.experimental
- */
-public class SumFloatAssociationFacetsAggregator implements FacetsAggregator {
-
-  private final BytesRef bytes = new BytesRef(32);
-  
-  @Override
-  public void aggregate(MatchingDocs matchingDocs, CategoryListParams clp, FacetArrays facetArrays) throws IOException {
-    BinaryDocValues dv = matchingDocs.context.reader().getBinaryDocValues(clp.field + CategoryFloatAssociation.ASSOCIATION_LIST_ID);
-    if (dv == null) {
-      return; // no float associations in this reader
-    }
-    
-    final int length = matchingDocs.bits.length();
-    final float[] values = facetArrays.getFloatArray();
-    int doc = 0;
-    while (doc < length && (doc = matchingDocs.bits.nextSetBit(doc)) != -1) {
-      dv.get(doc, bytes);
-      if (bytes.length > 0) {
-        // aggreate float association values for ordinals
-        int bytesUpto = bytes.offset + bytes.length;
-        int pos = bytes.offset;
-        while (pos < bytesUpto) {
-          int ordinal = ((bytes.bytes[pos++] & 0xFF) << 24) | ((bytes.bytes[pos++] & 0xFF) << 16)
-              | ((bytes.bytes[pos++] & 0xFF) <<  8) | (bytes.bytes[pos++] & 0xFF);
-          
-          int value = ((bytes.bytes[pos++] & 0xFF) << 24) | ((bytes.bytes[pos++] & 0xFF) << 16)
-              | ((bytes.bytes[pos++] & 0xFF) <<  8) | (bytes.bytes[pos++] & 0xFF);
-          
-          values[ordinal] += Float.intBitsToFloat(value);
-        }
-      }
-      ++doc;
-    }
-  }
-
-  @Override
-  public boolean requiresDocScores() {
-    return false;
-  }
-
-  @Override
-  public void rollupValues(FacetRequest fr, int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
-    // NO-OP: this aggregator does no rollup values to the parents.
-  }
-
-  @Override
-  public OrdinalValueResolver createOrdinalValueResolver(FacetRequest facetRequest, FacetArrays arrays) {
-    return new FloatValueResolver(arrays);
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/SumIntAssociationFacetRequest.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/SumIntAssociationFacetRequest.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/SumIntAssociationFacetRequest.java	2013-08-01 14:47:20.738689730 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/SumIntAssociationFacetRequest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,46 +0,0 @@
-package org.apache.lucene.facet.associations;
-
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetsAggregator;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetRequest} for weighting facets according to their integer
- * association by summing the association values.
- * 
- * @lucene.experimental
- */
-public class SumIntAssociationFacetRequest extends FacetRequest {
-
-  /**
-   * Create an integer association facet request for a given node in the
-   * taxonomy.
-   */
-  public SumIntAssociationFacetRequest(CategoryPath path, int num) {
-    super(path, num);
-  }
-
-  @Override
-  public FacetsAggregator createFacetsAggregator(FacetIndexingParams fip) {
-    return new SumIntAssociationFacetsAggregator();
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/SumIntAssociationFacetsAggregator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/SumIntAssociationFacetsAggregator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/associations/SumIntAssociationFacetsAggregator.java	2013-08-01 14:47:20.738689730 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/associations/SumIntAssociationFacetsAggregator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,91 +0,0 @@
-package org.apache.lucene.facet.associations;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetsAggregator;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.search.OrdinalValueResolver;
-import org.apache.lucene.facet.search.OrdinalValueResolver.IntValueResolver;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.util.BytesRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetsAggregator} which computes the weight of a category as the sum
- * of the integer values associated with it in the result documents. Assumes
- * that the association encoded for each ordinal is
- * {@link CategoryIntAssociation}.
- * <p>
- * <b>NOTE:</b> this aggregator does not support
- * {@link #rollupValues(FacetRequest, int, int[], int[], FacetArrays)}. It only
- * aggregates the categories for which you added a {@link CategoryAssociation}.
- */
-public class SumIntAssociationFacetsAggregator implements FacetsAggregator {
-
-  private final BytesRef bytes = new BytesRef(32);
-  
-  @Override
-  public void aggregate(MatchingDocs matchingDocs, CategoryListParams clp, FacetArrays facetArrays) throws IOException {
-    BinaryDocValues dv = matchingDocs.context.reader().getBinaryDocValues(clp.field + CategoryIntAssociation.ASSOCIATION_LIST_ID);
-    if (dv == null) {
-      return; // no int associations in this reader
-    }
-    
-    final int length = matchingDocs.bits.length();
-    final int[] values = facetArrays.getIntArray();
-    int doc = 0;
-    while (doc < length && (doc = matchingDocs.bits.nextSetBit(doc)) != -1) {
-      dv.get(doc, bytes);
-      if (bytes.length > 0) {
-        // aggreate association values for ordinals
-        int bytesUpto = bytes.offset + bytes.length;
-        int pos = bytes.offset;
-        while (pos < bytesUpto) {
-          int ordinal = ((bytes.bytes[pos++] & 0xFF) << 24) | ((bytes.bytes[pos++] & 0xFF) << 16)
-              | ((bytes.bytes[pos++] & 0xFF) <<  8) | (bytes.bytes[pos++] & 0xFF);
-          
-          int value = ((bytes.bytes[pos++] & 0xFF) << 24) | ((bytes.bytes[pos++] & 0xFF) << 16)
-              | ((bytes.bytes[pos++] & 0xFF) <<  8) | (bytes.bytes[pos++] & 0xFF);
-          
-          values[ordinal] += value;
-        }
-      }
-      ++doc;
-    }
-  }
-
-  @Override
-  public boolean requiresDocScores() {
-    return false;
-  }
-
-  @Override
-  public void rollupValues(FacetRequest fr, int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
-    // NO-OP: this aggregator does no rollup values to the parents.
-  }
-
-  @Override
-  public OrdinalValueResolver createOrdinalValueResolver(FacetRequest facetRequest, FacetArrays arrays) {
-    return new IntValueResolver(arrays);
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/CachedOrdinalsReader.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/CachedOrdinalsReader.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/CachedOrdinalsReader.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/CachedOrdinalsReader.java	2013-11-26 18:29:51.086291075 -0500
@@ -0,0 +1,149 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.WeakHashMap;
+
+import org.apache.lucene.codecs.DocValuesFormat; // javadocs
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.BinaryDocValues; // javadocs
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/**
+ * A per-segment cache of documents' facet ordinals. Every
+ * {@link CachedOrds} holds the ordinals in a raw {@code
+ * int[]}, and therefore consumes as much RAM as the total
+ * number of ordinals found in the segment, but saves the
+ * CPU cost of decoding ordinals during facet counting.
+ * 
+ * <p>
+ * <b>NOTE:</b> every {@link CachedOrds} is limited to 2.1B
+ * total ordinals. If that is a limitation for you then
+ * consider limiting the segment size to fewer documents, or
+ * use an alternative cache which pages through the category
+ * ordinals.
+ * 
+ * <p>
+ * <b>NOTE:</b> when using this cache, it is advised to use
+ * a {@link DocValuesFormat} that does not cache the data in
+ * memory, at least for the category lists fields, or
+ * otherwise you'll be doing double-caching.
+ *
+ * <p>
+ * <b>NOTE:</b> create one instance of this and re-use it
+ * for all facet implementations (the cache is per-instance,
+ * not static).
+ */
+public class CachedOrdinalsReader extends OrdinalsReader {
+
+  private final OrdinalsReader source;
+
+  private final Map<Object,CachedOrds> ordsCache = new WeakHashMap<Object,CachedOrds>();
+
+  public CachedOrdinalsReader(OrdinalsReader source) {
+    this.source = source;
+  }
+
+  private synchronized CachedOrds getCachedOrds(AtomicReaderContext context) throws IOException {
+    Object cacheKey = context.reader().getCoreCacheKey();
+    CachedOrds ords = ordsCache.get(cacheKey);
+    if (ords == null) {
+      ords = new CachedOrds(source.getReader(context), context.reader().maxDoc());
+      ordsCache.put(cacheKey, ords);
+    }
+
+    return ords;
+  }
+
+  @Override
+  public String getIndexFieldName() {
+    return source.getIndexFieldName();
+  }
+
+  @Override
+  public OrdinalsSegmentReader getReader(AtomicReaderContext context) throws IOException {
+    final CachedOrds cachedOrds = getCachedOrds(context);
+    return new OrdinalsSegmentReader() {
+      @Override
+      public void get(int docID, IntsRef ordinals) {
+        ordinals.ints = cachedOrds.ordinals;
+        ordinals.offset = cachedOrds.offsets[docID];
+        ordinals.length = cachedOrds.offsets[docID+1] - ordinals.offset;
+      }
+    };
+  }
+
+  /** Holds the cached ordinals in two paralel {@code int[]} arrays. */
+  public static final class CachedOrds {
+    
+    public final int[] offsets;
+    public final int[] ordinals;
+
+    /**
+     * Creates a new {@link CachedOrds} from the {@link BinaryDocValues}.
+     * Assumes that the {@link BinaryDocValues} is not {@code null}.
+     */
+    public CachedOrds(OrdinalsSegmentReader source, int maxDoc) throws IOException {
+      final BytesRef buf = new BytesRef();
+
+      offsets = new int[maxDoc + 1];
+      int[] ords = new int[maxDoc]; // let's assume one ordinal per-document as an initial size
+
+      // this aggregator is limited to Integer.MAX_VALUE total ordinals.
+      long totOrds = 0;
+      final IntsRef values = new IntsRef(32);
+      for (int docID = 0; docID < maxDoc; docID++) {
+        offsets[docID] = (int) totOrds;
+        source.get(docID, values);
+        long nextLength = totOrds + values.length;
+        if (nextLength > ords.length) {
+          if (nextLength > ArrayUtil.MAX_ARRAY_LENGTH) {
+            throw new IllegalStateException("too many ordinals (>= " + nextLength + ") to cache");
+          }
+          ords = ArrayUtil.grow(ords, (int) nextLength);
+        }
+        System.arraycopy(values.ints, 0, ords, (int) totOrds, values.length);
+        totOrds = nextLength;
+      }
+      offsets[maxDoc] = (int) totOrds;
+      
+      // if ords array is bigger by more than 10% of what we really need, shrink it
+      if ((double) totOrds / ords.length < 0.9) { 
+        this.ordinals = new int[(int) totOrds];
+        System.arraycopy(ords, 0, this.ordinals, 0, (int) totOrds);
+      } else {
+        this.ordinals = ords;
+      }
+    }
+  }
+
+  /** How many bytes is this cache using? */
+  public synchronized long ramBytesUsed() {
+    long bytes = 0;
+    for(CachedOrds ords : ordsCache.values()) {
+      bytes += RamUsageEstimator.sizeOf(ords);
+    }
+
+    return bytes;
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/ArrayHashMap.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/ArrayHashMap.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/ArrayHashMap.java	2013-02-20 13:38:17.780711923 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/ArrayHashMap.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,554 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.Arrays;
-import java.util.Iterator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An Array-based hashtable which maps, similar to Java's HashMap, only
- * performance tests showed it performs better.
- * <p>
- * The hashtable is constructed with a given capacity, or 16 as a default. In
- * case there's not enough room for new pairs, the hashtable grows. Capacity is
- * adjusted to a power of 2, and there are 2 * capacity entries for the hash.
- * The pre allocated arrays (for keys, values) are at length of capacity + 1,
- * where index 0 is used as 'Ground' or 'NULL'.
- * <p>
- * The arrays are allocated ahead of hash operations, and form an 'empty space'
- * list, to which the &lt;key,value&gt; pair is allocated.
- * 
- * @lucene.experimental
- */
-public class ArrayHashMap<K,V> implements Iterable<V> {
-
-  /** Implements an IntIterator which iterates over all the allocated indexes. */
-  private final class IndexIterator implements IntIterator {
-    /**
-     * The last used baseHashIndex. Needed for "jumping" from one hash entry
-     * to another.
-     */
-    private int baseHashIndex = 0;
-
-    /** The next not-yet-visited index. */
-    private int index = 0;
-
-    /** Index of the last visited pair. Used in {@link #remove()}. */
-    private int lastIndex = 0;
-
-    /**
-     * Create the Iterator, make <code>index</code> point to the "first"
-     * index which is not empty. If such does not exist (eg. the map is
-     * empty) it would be zero.
-     */
-    public IndexIterator() {
-      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
-        index = baseHash[baseHashIndex];
-        if (index != 0) {
-          break;
-        }
-      }
-    }
-
-    @Override
-    public boolean hasNext() {
-      return index != 0;
-    }
-
-    @Override
-    public int next() {
-      // Save the last index visited
-      lastIndex = index;
-
-      // next the index
-      index = next[index];
-
-      // if the next index points to the 'Ground' it means we're done with
-      // the current hash entry and we need to jump to the next one. This
-      // is done until all the hash entries had been visited.
-      while (index == 0 && ++baseHashIndex < baseHash.length) {
-        index = baseHash[baseHashIndex];
-      }
-
-      return lastIndex;
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public void remove() {
-      ArrayHashMap.this.remove((K) keys[lastIndex]);
-    }
-
-  }
-
-  /** Implements an Iterator, used for iteration over the map's keys. */
-  private final class KeyIterator implements Iterator<K> {
-    private IntIterator iterator = new IndexIterator();
-
-    KeyIterator() { }
-
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public K next() {
-      return (K) keys[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /** Implements an Iterator, used for iteration over the map's values. */
-  private final class ValueIterator implements Iterator<V> {
-    private IntIterator iterator = new IndexIterator();
-
-    ValueIterator() { }
-
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public V next() {
-      return (V) values[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /** Default capacity - in case no capacity was specified in the constructor */
-  private static final int DEFAULT_CAPACITY = 16;
-
-  /**
-   * Holds the base hash entries. if the capacity is 2^N, than the base hash
-   * holds 2^(N+1).
-   */
-  int[] baseHash;
-
-  /**
-   * The current capacity of the map. Always 2^N and never less than 16. We
-   * never use the zero index. It is needed to improve performance and is also
-   * used as "ground".
-   */
-  private int capacity;
-
-  /**
-   * All objects are being allocated at map creation. Those objects are "free"
-   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
-   * taken from the free-linked list. as this is just a free list.
-   */
-  private int firstEmpty;
-
-  /** hashFactor is always (2^(N+1)) - 1. Used for faster hashing. */
-  private int hashFactor;
-
-  /** Holds the unique keys. */
-  Object[] keys;
-
-  /**
-   * In case of collisions, we implement a double linked list of the colliding
-   * hash's with the following next[] and prev[]. Those are also used to store
-   * the "empty" list.
-   */
-  int[] next;
-
-  private int prev;
-
-  /** Number of currently stored objects in the map. */
-  private int size;
-
-  /** Holds the values. */
-  Object[] values;
-
-  /** Constructs a map with default capacity. */
-  public ArrayHashMap() {
-    this(DEFAULT_CAPACITY);
-  }
-
-  /**
-   * Constructs a map with given capacity. Capacity is adjusted to a native
-   * power of 2, with minimum of 16.
-   * 
-   * @param capacity minimum capacity for the map.
-   */
-  public ArrayHashMap(int capacity) {
-    this.capacity = 16;
-    while (this.capacity < capacity) {
-      // Multiply by 2 as long as we're still under the requested capacity
-      this.capacity <<= 1;
-    }
-
-    // As mentioned, we use the first index (0) as 'Ground', so we need the
-    // length of the arrays to be one more than the capacity
-    int arrayLength = this.capacity + 1;
-
-    values = new Object[arrayLength];
-    keys = new Object[arrayLength];
-    next = new int[arrayLength];
-
-    // Hash entries are twice as big as the capacity.
-    int baseHashSize = this.capacity << 1;
-
-    baseHash = new int[baseHashSize];
-
-    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
-    // {@link #calcBaseHash()}
-    hashFactor = baseHashSize - 1;
-
-    size = 0;
-
-    clear();
-  }
-
-  /**
-   * Adds a pair to the map. Takes the first empty position from the
-   * empty-linked-list's head - {@link #firstEmpty}. New pairs are always
-   * inserted to baseHash, and are followed by the old colliding pair.
-   */
-  private void prvt_put(K key, V value) {
-    // Hash entry to which the new pair would be inserted
-    int hashIndex = calcBaseHashIndex(key);
-
-    // 'Allocating' a pair from the "Empty" list.
-    int objectIndex = firstEmpty;
-
-    // Setting data
-    firstEmpty = next[firstEmpty];
-    values[objectIndex] = value;
-    keys[objectIndex] = key;
-
-    // Inserting the new pair as the first node in the specific hash entry
-    next[objectIndex] = baseHash[hashIndex];
-    baseHash[hashIndex] = objectIndex;
-
-    // Announcing a new pair was added!
-    ++size;
-  }
-
-  /** Calculating the baseHash index using the internal internal <code>hashFactor</code>. */
-  protected int calcBaseHashIndex(K key) {
-    return key.hashCode() & hashFactor;
-  }
-
-  /** Empties the map. Generates the "Empty" space list for later allocation. */
-  public void clear() {
-    // Clears the hash entries
-    Arrays.fill(baseHash, 0);
-
-    // Set size to zero
-    size = 0;
-
-    // Mark all array entries as empty. This is done with
-    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
-    // used as 'Ground').
-    firstEmpty = 1;
-
-    // And setting all the <code>next[i]</code> to point at
-    // <code>i+1</code>.
-    for (int i = 1; i < capacity;) {
-      next[i] = ++i;
-    }
-
-    // Surly, the last one should point to the 'Ground'.
-    next[capacity] = 0;
-  }
-
-  /** Returns true iff the key exists in the map. */
-  public boolean containsKey(K key) {
-    return find(key) != 0;
-  }
-
-  /** Returns true iff the object exists in the map. */
-  public boolean containsValue(Object o) {
-    for (Iterator<V> iterator = iterator(); iterator.hasNext();) {
-      V object = iterator.next();
-      if (object.equals(o)) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /** Returns the index of the given key, or zero if the key wasn't found. */
-  protected int find(K key) {
-    // Calculate the hash entry.
-    int baseHashIndex = calcBaseHashIndex(key);
-
-    // Start from the hash entry.
-    int localIndex = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (localIndex != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[localIndex].equals(key)) {
-        return localIndex;
-      }
-
-      // next the local index
-      localIndex = next[localIndex];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return 0;
-  }
-
-  /**
-   * Finds the actual index of a given key with it's baseHashIndex. Some methods
-   * use the baseHashIndex. If those call {@link #find} there's no need to
-   * re-calculate that hash.
-   * 
-   * @return the index of the given key, or 0 if the key wasn't found.
-   */
-  private int findForRemove(K key, int baseHashIndex) {
-    // Start from the hash entry.
-    prev = 0;
-    int index = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (index != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[index].equals(key)) {
-        return index;
-      }
-
-      // next the local index
-      prev = index;
-      index = next[index];
-    }
-
-    // If we got thus far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return prev = 0;
-  }
-
-  /** Returns the object mapped with the given key, or null if the key wasn't found. */
-  @SuppressWarnings("unchecked")
-  public V get(K key) {
-    return (V) values[find(key)];
-  }
-
-  /**
-   * Allocates a new map of double the capacity, and fast-insert the old
-   * key-value pairs.
-   */
-  @SuppressWarnings("unchecked")
-  protected void grow() {
-    ArrayHashMap<K,V> newmap = new ArrayHashMap<K,V>(capacity * 2);
-
-    // Iterates fast over the collection. Any valid pair is put into the new
-    // map without checking for duplicates or if there's enough space for
-    // it.
-    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
-      int index = iterator.next();
-      newmap.prvt_put((K) keys[index], (V) values[index]);
-    }
-
-    // Copy that's data into this.
-    capacity = newmap.capacity;
-    size = newmap.size;
-    firstEmpty = newmap.firstEmpty;
-    values = newmap.values;
-    keys = newmap.keys;
-    next = newmap.next;
-    baseHash = newmap.baseHash;
-    hashFactor = newmap.hashFactor;
-  }
-
-  /** Returns true iff the map is empty. */
-  public boolean isEmpty() {
-    return size == 0;
-  }
-
-  /** Returns an iterator on the mapped objects. */
-  @Override
-  public Iterator<V> iterator() {
-    return new ValueIterator();
-  }
-
-  /** Returns an iterator on the map keys. */
-  public Iterator<K> keyIterator() {
-    return new KeyIterator();
-  }
-
-  /** Prints the baseHash array, used for debugging purposes. */
-  @SuppressWarnings("unused")
-  private String getBaseHashAsString() {
-    return Arrays.toString(this.baseHash);
-  }
-
-  /**
-   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
-   * this method updates the mapped value to the given one, returning the old
-   * mapped value.
-   * 
-   * @return the old mapped value, or null if the key didn't exist.
-   */
-  @SuppressWarnings("unchecked")
-  public V put(K key, V e) {
-    // Does key exists?
-    int index = find(key);
-
-    // Yes!
-    if (index != 0) {
-      // Set new data and exit.
-      V old = (V) values[index];
-      values[index] = e;
-      return old;
-    }
-
-    // Is there enough room for a new pair?
-    if (size == capacity) {
-      // No? Than grow up!
-      grow();
-    }
-
-    // Now that everything is set, the pair can be just put inside with no
-    // worries.
-    prvt_put(key, e);
-
-    return null;
-  }
-
-  /**
-   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
-   * or null if the none existed.
-   * 
-   * @param key used to find the value to remove
-   * @return the removed value or null if none existed.
-   */
-  @SuppressWarnings("unchecked")
-  public V remove(K key) {
-    int baseHashIndex = calcBaseHashIndex(key);
-    int index = findForRemove(key, baseHashIndex);
-    if (index != 0) {
-      // If it is the first in the collision list, we should promote its
-      // next colliding element.
-      if (prev == 0) {
-        baseHash[baseHashIndex] = next[index];
-      }
-
-      next[prev] = next[index];
-      next[index] = firstEmpty;
-      firstEmpty = index;
-      --size;
-      return (V) values[index];
-    }
-
-    return null;
-  }
-
-  /** Returns number of pairs currently in the map. */
-  public int size() {
-    return this.size;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of Objects
-   * 
-   * @return an object array of all the values currently in the map.
-   */
-  public Object[] toArray() {
-    int j = -1;
-    Object[] array = new Object[size];
-
-    // Iterates over the values, adding them to the array.
-    for (Iterator<V> iterator = iterator(); iterator.hasNext();) {
-      array[++j] = iterator.next();
-    }
-    return array;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of V
-   * 
-   * @param a the array into which the elements of the list are to be stored, if
-   *        it is big enough; otherwise, use as much space as it can.
-   * @return an array containing the elements of the list
-   */
-  public V[] toArray(V[] a) {
-    int j = 0;
-    // Iterates over the values, adding them to the array.
-    for (Iterator<V> iterator = iterator(); j < a.length
-    && iterator.hasNext(); ++j) {
-      a[j] = iterator.next();
-    }
-    if (j < a.length) {
-      a[j] = null;
-    }
-
-    return a;
-  }
-
-  @Override
-  public String toString() {
-    StringBuffer sb = new StringBuffer();
-    sb.append('{');
-    Iterator<K> keyIterator = keyIterator();
-    while (keyIterator.hasNext()) {
-      K key = keyIterator.next();
-      sb.append(key);
-      sb.append('=');
-      sb.append(get(key));
-      if (keyIterator.hasNext()) {
-        sb.append(',');
-        sb.append(' ');
-      }
-    }
-    sb.append('}');
-    return sb.toString();
-  }
-
-  @Override
-  public int hashCode() {
-    return getClass().hashCode() ^ size();
-  }
-
-  @SuppressWarnings("unchecked")
-  @Override
-  public boolean equals(Object o) {
-    ArrayHashMap<K, V> that = (ArrayHashMap<K,V>)o;
-    if (that.size() != this.size()) {
-      return false;
-    }
-
-    Iterator<K> it = keyIterator();
-    while (it.hasNext()) {
-      K key = it.next();
-      V v1 = this.get(key);
-      V v2 = that.get(key);
-      if ((v1 == null && v2 != null) ||
-          (v1 != null && v2 == null) ||
-          (!v1.equals(v2))) {
-        return false;
-      }
-    }
-    return true;
-  }
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/DoubleIterator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/DoubleIterator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/DoubleIterator.java	2013-02-20 13:38:17.784711922 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/DoubleIterator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,31 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Iterator interface for primitive double iteration. *
- * 
- * @lucene.experimental
- */
-public interface DoubleIterator {
-
-  boolean hasNext();
-  double next();
-  void remove();
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/FloatIterator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/FloatIterator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/FloatIterator.java	2013-02-20 13:38:17.780711923 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/FloatIterator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,31 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Iterator interface for primitive int iteration. *
- * 
- * @lucene.experimental
- */
-public interface FloatIterator {
-
-  boolean hasNext();
-  float next();
-  void remove();
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/FloatToObjectMap.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/FloatToObjectMap.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/FloatToObjectMap.java	2013-02-20 13:38:17.784711922 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/FloatToObjectMap.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,634 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.Arrays;
-import java.util.Iterator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
-
- * An Array-based hashtable which maps primitive float to Objects of generic type
- * T.<br>
- * The hashtable is constracted with a given capacity, or 16 as a default. In
- * case there's not enough room for new pairs, the hashtable grows. <br>
- * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
- * the hash.
- * 
- * The pre allocated arrays (for keys, values) are at length of capacity + 1,
- * when index 0 is used as 'Ground' or 'NULL'.<br>
- * 
- * The arrays are allocated ahead of hash operations, and form an 'empty space'
- * list, to which the key,value pair is allocated.
- * 
- * @lucene.experimental
- */
-public class FloatToObjectMap<T> implements Iterable<T> {
-
-  /**
-   * Implements an IntIterator which iterates over all the allocated indexes.
-   */
-  private final class IndexIterator implements IntIterator {
-    /**
-     * The last used baseHashIndex. Needed for "jumping" from one hash entry
-     * to another.
-     */
-    private int baseHashIndex = 0;
-
-    /**
-     * The next not-yet-visited index.
-     */
-    private int index = 0;
-
-    /**
-     * Index of the last visited pair. Used in {@link #remove()}.
-     */
-    private int lastIndex = 0;
-
-    /**
-     * Create the Iterator, make <code>index</code> point to the "first"
-     * index which is not empty. If such does not exist (eg. the map is
-     * empty) it would be zero.
-     */
-    public IndexIterator() {
-      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
-        index = baseHash[baseHashIndex];
-        if (index != 0) {
-          break;
-        }
-      }
-    }
-
-    @Override
-    public boolean hasNext() {
-      return (index != 0);
-    }
-
-    @Override
-    public int next() {
-      // Save the last index visited
-      lastIndex = index;
-
-      // next the index
-      index = next[index];
-
-      // if the next index points to the 'Ground' it means we're done with
-      // the current hash entry and we need to jump to the next one. This
-      // is done until all the hash entries had been visited.
-      while (index == 0 && ++baseHashIndex < baseHash.length) {
-        index = baseHash[baseHashIndex];
-      }
-
-      return lastIndex;
-    }
-
-    @Override
-    public void remove() {
-      FloatToObjectMap.this.remove(keys[lastIndex]);
-    }
-
-  }
-
-  /**
-   * Implements an IntIterator, used for iteration over the map's keys.
-   */
-  private final class KeyIterator implements FloatIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    KeyIterator() { }
-
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public float next() {
-      return keys[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Implements an Iterator of a generic type T used for iteration over the
-   * map's values.
-   */
-  private final class ValueIterator implements Iterator<T> {
-    private IntIterator iterator = new IndexIterator();
-
-    ValueIterator() { }
-
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public T next() {
-      return (T) values[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Default capacity - in case no capacity was specified in the constructor
-   */
-  private static int defaultCapacity = 16;
-
-  /**
-   * Holds the base hash entries. if the capacity is 2^N, than the base hash
-   * holds 2^(N+1). It can hold
-   */
-  int[] baseHash;
-
-  /**
-   * The current capacity of the map. Always 2^N and never less than 16. We
-   * never use the zero index. It is needed to improve performance and is also
-   * used as "ground".
-   */
-  private int capacity;
-  /**
-   * All objects are being allocated at map creation. Those objects are "free"
-   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
-   * taken from the free-linked list. as this is just a free list.
-   */
-  private int firstEmpty;
-
-  /**
-   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
-   */
-  private int hashFactor;
-
-  /**
-   * This array holds the unique keys
-   */
-  float[] keys;
-
-  /**
-   * In case of collisions, we implement a double linked list of the colliding
-   * hash's with the following next[] and prev[]. Those are also used to store
-   * the "empty" list.
-   */
-  int[] next;
-
-  private int prev;
-
-  /**
-   * Number of currently objects in the map.
-   */
-  private int size;
-
-  /**
-   * This array holds the values
-   */
-  Object[] values;
-
-  /**
-   * Constructs a map with default capacity.
-   */
-  public FloatToObjectMap() {
-    this(defaultCapacity);
-  }
-
-  /**
-   * Constructs a map with given capacity. Capacity is adjusted to a native
-   * power of 2, with minimum of 16.
-   * 
-   * @param capacity
-   *            minimum capacity for the map.
-   */
-  public FloatToObjectMap(int capacity) {
-    this.capacity = 16;
-    // Minimum capacity is 16..
-    while (this.capacity < capacity) {
-      // Multiply by 2 as long as we're still under the requested capacity
-      this.capacity <<= 1;
-    }
-
-    // As mentioned, we use the first index (0) as 'Ground', so we need the
-    // length of the arrays to be one more than the capacity
-    int arrayLength = this.capacity + 1;
-
-    this.values = new Object[arrayLength];
-    this.keys = new float[arrayLength];
-    this.next = new int[arrayLength];
-
-    // Hash entries are twice as big as the capacity.
-    int baseHashSize = this.capacity << 1;
-
-    this.baseHash = new int[baseHashSize];
-
-    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
-    // {@link #calcBaseHash()}
-    this.hashFactor = baseHashSize - 1;
-
-    this.size = 0;
-
-    clear();
-  }
-
-  /**
-   * Adds a pair to the map. Takes the first empty position from the
-   * empty-linked-list's head - {@link #firstEmpty}.
-   * 
-   * New pairs are always inserted to baseHash, and are followed by the old
-   * colliding pair.
-   * 
-   * @param key
-   *            integer which maps the given Object
-   * @param e
-   *            element which is being mapped using the given key
-   */
-  private void prvt_put(float key, T e) {
-    // Hash entry to which the new pair would be inserted
-    int hashIndex = calcBaseHashIndex(key);
-
-    // 'Allocating' a pair from the "Empty" list.
-    int objectIndex = firstEmpty;
-
-    // Setting data
-    firstEmpty = next[firstEmpty];
-    values[objectIndex] = e;
-    keys[objectIndex] = key;
-
-    // Inserting the new pair as the first node in the specific hash entry
-    next[objectIndex] = baseHash[hashIndex];
-    baseHash[hashIndex] = objectIndex;
-
-    // Announcing a new pair was added!
-    ++size;
-  }
-
-  /**
-   * Calculating the baseHash index using the internal <code>hashFactor</code>.
-   */
-  protected int calcBaseHashIndex(float key) {
-    return Float.floatToIntBits(key) & hashFactor;
-  }
-
-  /**
-   * Empties the map. Generates the "Empty" space list for later allocation.
-   */
-  public void clear() {
-    // Clears the hash entries
-    Arrays.fill(this.baseHash, 0);
-
-    // Set size to zero
-    size = 0;
-
-    // Mark all array entries as empty. This is done with
-    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
-    // used as 'Ground').
-    firstEmpty = 1;
-
-    // And setting all the <code>next[i]</code> to point at
-    // <code>i+1</code>.
-    for (int i = 1; i < this.capacity;) {
-      next[i] = ++i;
-    }
-
-    // Surly, the last one should point to the 'Ground'.
-    next[this.capacity] = 0;
-  }
-
-  /**
-   * Checks if a given key exists in the map.
-   * 
-   * @param key
-   *            that is checked against the map data.
-   * @return true if the key exists in the map. false otherwise.
-   */
-  public boolean containsKey(float key) {
-    return find(key) != 0;
-  }
-
-  /**
-   * Checks if the given object exists in the map.<br>
-   * This method iterates over the collection, trying to find an equal object.
-   * 
-   * @param o
-   *            object that is checked against the map data.
-   * @return true if the object exists in the map (in .equals() meaning).
-   *         false otherwise.
-   */
-  public boolean containsValue(Object o) {
-    for (Iterator<T> iterator = iterator(); iterator.hasNext();) {
-      T object = iterator.next();
-      if (object.equals(o)) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /**
-   * Find the actual index of a given key.
-   * 
-   * @return index of the key. zero if the key wasn't found.
-   */
-  protected int find(float key) {
-    // Calculate the hash entry.
-    int baseHashIndex = calcBaseHashIndex(key);
-
-    // Start from the hash entry.
-    int localIndex = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (localIndex != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[localIndex] == key) {
-        return localIndex;
-      }
-
-      // next the local index
-      localIndex = next[localIndex];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return 0;
-  }
-
-  /**
-   * Find the actual index of a given key with it's baseHashIndex.<br>
-   * Some methods use the baseHashIndex. If those call {@link #find} there's
-   * no need to re-calculate that hash.
-   * 
-   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
-   *         found.
-   */
-  private int findForRemove(float key, int baseHashIndex) {
-    // Start from the hash entry.
-    this.prev = 0;
-    int index = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (index != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[index] == key) {
-        return index;
-      }
-
-      // next the local index
-      prev = index;
-      index = next[index];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    this.prev = 0;
-    return 0;
-  }
-
-  /**
-   * Returns the object mapped with the given key.
-   * 
-   * @param key
-   *            int who's mapped object we're interested in.
-   * @return an object mapped by the given key. null if the key wasn't found.
-   */
-  @SuppressWarnings("unchecked")
-  public T get(float key) {
-    return (T) values[find(key)];
-  }
-
-  /**
-   * Grows the map. Allocates a new map of double the capacity, and
-   * fast-insert the old key-value pairs.
-   */
-  @SuppressWarnings("unchecked")
-  protected void grow() {
-    FloatToObjectMap<T> that = new FloatToObjectMap<T>(
-        this.capacity * 2);
-
-    // Iterates fast over the collection. Any valid pair is put into the new
-    // map without checking for duplicates or if there's enough space for
-    // it.
-    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
-      int index = iterator.next();
-      that.prvt_put(this.keys[index], (T) this.values[index]);
-    }
-
-    // Copy that's data into this.
-    this.capacity = that.capacity;
-    this.size = that.size;
-    this.firstEmpty = that.firstEmpty;
-    this.values = that.values;
-    this.keys = that.keys;
-    this.next = that.next;
-    this.baseHash = that.baseHash;
-    this.hashFactor = that.hashFactor;
-  }
-
-  /**
-   * 
-   * @return true if the map is empty. false otherwise.
-   */
-  public boolean isEmpty() {
-    return size == 0;
-  }
-
-  /**
-   * Returns a new iterator for the mapped objects.
-   */
-  @Override
-  public Iterator<T> iterator() {
-    return new ValueIterator();
-  }
-
-  /** Returns an iterator on the map keys. */
-  public FloatIterator keyIterator() {
-    return new KeyIterator();
-  }
-
-  /**
-   * Prints the baseHash array, used for DEBUG purposes.
-   */
-  @SuppressWarnings("unused")
-  private String getBaseHashAsString() {
-    return Arrays.toString(this.baseHash);
-  }
-
-  /**
-   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
-   * this method updates the mapped value to the given one, returning the old
-   * mapped value.
-   * 
-   * @return the old mapped value, or null if the key didn't exist.
-   */
-  @SuppressWarnings("unchecked")
-  public T put(float key, T e) {
-    // Does key exists?
-    int index = find(key);
-
-    // Yes!
-    if (index != 0) {
-      // Set new data and exit.
-      T old = (T) values[index];
-      values[index] = e;
-      return old;
-    }
-
-    // Is there enough room for a new pair?
-    if (size == capacity) {
-      // No? Than grow up!
-      grow();
-    }
-
-    // Now that everything is set, the pair can be just put inside with no
-    // worries.
-    prvt_put(key, e);
-
-    return null;
-  }
-
-  /**
-   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
-   * or null if the none existed.
-   * 
-   * @param key used to find the value to remove
-   * @return the removed value or null if none existed.
-   */
-  @SuppressWarnings("unchecked")
-  public T remove(float key) {
-    int baseHashIndex = calcBaseHashIndex(key);
-    int index = findForRemove(key, baseHashIndex);
-    if (index != 0) {
-      // If it is the first in the collision list, we should promote its
-      // next colliding element.
-      if (prev == 0) {
-        baseHash[baseHashIndex] = next[index];
-      }
-
-      next[prev] = next[index];
-      next[index] = firstEmpty;
-      firstEmpty = index;
-      --size;
-      return (T) values[index];
-    }
-
-    return null;
-  }
-
-  /**
-   * @return number of pairs currently in the map
-   */
-  public int size() {
-    return this.size;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of Objects
-   * 
-   * @return an object array of all the values currently in the map.
-   */
-  public Object[] toArray() {
-    int j = -1;
-    Object[] array = new Object[size];
-
-    // Iterates over the values, adding them to the array.
-    for (Iterator<T> iterator = iterator(); iterator.hasNext();) {
-      array[++j] = iterator.next();
-    }
-    return array;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of T
-   * 
-   * @param a
-   *            the array into which the elements of the list are to be
-   *            stored, if it is big enough; otherwise, use whatever space we
-   *            have, setting the one after the true data as null.
-   * 
-   * @return an array containing the elements of the list
-   * 
-   */
-  public T[] toArray(T[] a) {
-    int j = 0;
-    // Iterates over the values, adding them to the array.
-    for (Iterator<T> iterator = iterator(); j < a.length
-    && iterator.hasNext(); ++j) {
-      a[j] = iterator.next();
-    }
-
-    if (j < a.length) {
-      a[j] = null;
-    }
-
-    return a;
-  }
-
-  @Override
-  public String toString() {
-    StringBuffer sb = new StringBuffer();
-    sb.append('{');
-    FloatIterator keyIterator = keyIterator();
-    while (keyIterator.hasNext()) {
-      float key = keyIterator.next();
-      sb.append(key);
-      sb.append('=');
-      sb.append(get(key));
-      if (keyIterator.hasNext()) {
-        sb.append(',');
-        sb.append(' ');
-      }
-    }
-    sb.append('}');
-    return sb.toString();
-  }
-
-  @Override
-  public int hashCode() {
-    return getClass().hashCode() ^ size();
-  }
-
-  @SuppressWarnings("unchecked")
-  @Override
-  public boolean equals(Object o) {
-    FloatToObjectMap<T> that = (FloatToObjectMap<T>)o;
-    if (that.size() != this.size()) {
-      return false;
-    }
-
-    FloatIterator it = keyIterator();
-    while (it.hasNext()) {
-      float key = it.next();
-      if (!that.containsKey(key)) {
-        return false;
-      }
-
-      T v1 = this.get(key);
-      T v2 = that.get(key);
-      if ((v1 == null && v2 != null) ||
-          (v1 != null && v2 == null) ||
-          (!v1.equals(v2))) {
-        return false;
-      }
-    }
-    return true;
-  }
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/IntArray.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/IntArray.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/IntArray.java	2013-02-20 13:38:17.784711922 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/IntArray.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,252 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.Arrays;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A Class wrapper for a grow-able int[] which can be sorted and intersect with
- * other IntArrays.
- * 
- * @lucene.experimental
- */
-public class IntArray {
-
-  /**
-   * The int[] which holds the data
-   */
-  private int[] data;
-
-  /**
-   * Holds the number of items in the array.
-   */
-  private int size;
-
-  /**
-   * A flag which indicates whether a sort should occur of the array is
-   * already sorted.
-   */
-  private boolean shouldSort;
-
-  /**
-   * Construct a default IntArray, size 0 and surly a sort should not occur.
-   */
-  public IntArray() {
-    init(true);
-  }
-
-  private void init(boolean realloc) {
-    size = 0;
-    if (realloc) {
-      data = new int[0];
-    }
-    shouldSort = false;
-  }
-
-  /**
-   * Intersects the data with a given {@link IntHashSet}.
-   * 
-   * @param set
-   *            A given ArrayHashSetInt which holds the data to be intersected
-   *            against
-   */
-  public void intersect(IntHashSet set) {
-    int newSize = 0;
-    for (int i = 0; i < size; ++i) {
-      if (set.contains(data[i])) {
-        data[newSize] = data[i];
-        ++newSize;
-      }
-    }
-    this.size = newSize;
-  }
-
-  /**
-   * Intersects the data with a given IntArray
-   * 
-   * @param other
-   *            A given IntArray which holds the data to be intersected agains
-   */
-  public void intersect(IntArray other) {
-    sort();
-    other.sort();
-
-    int myIndex = 0;
-    int otherIndex = 0;
-    int newSize = 0;
-    if (this.size > other.size) {
-      while (otherIndex < other.size && myIndex < size) {
-        while (otherIndex < other.size
-            && other.data[otherIndex] < data[myIndex]) {
-          ++otherIndex;
-        }
-        if (otherIndex == other.size) {
-          break;
-        }
-        while (myIndex < size && other.data[otherIndex] > data[myIndex]) {
-          ++myIndex;
-        }
-        if (other.data[otherIndex] == data[myIndex]) {
-          data[newSize++] = data[myIndex];
-          ++otherIndex;
-          ++myIndex;
-        }
-      }
-    } else {
-      while (otherIndex < other.size && myIndex < size) {
-        while (myIndex < size && other.data[otherIndex] > data[myIndex]) {
-          ++myIndex;
-        }
-        if (myIndex == size) {
-          break;
-        }
-        while (otherIndex < other.size
-            && other.data[otherIndex] < data[myIndex]) {
-          ++otherIndex;
-        }
-        if (other.data[otherIndex] == data[myIndex]) {
-          data[newSize++] = data[myIndex];
-          ++otherIndex;
-          ++myIndex;
-        }
-      }
-    }
-    this.size = newSize;
-  }
-
-  /**
-   * Return the size of the Array. Not the allocated size, but the number of
-   * values actually set.
-   * 
-   * @return the (filled) size of the array
-   */
-  public int size() {
-    return size;
-  }
-
-  /**
-   * Adds a value to the array.
-   * 
-   * @param value
-   *            value to be added
-   */
-  public void addToArray(int value) {
-    if (size == data.length) {
-      int[] newArray = new int[2 * size + 1];
-      System.arraycopy(data, 0, newArray, 0, size);
-      data = newArray;
-    }
-    data[size] = value;
-    ++size;
-    shouldSort = true;
-  }
-
-  /**
-   * Equals method. Checking the sizes, than the values from the last index to
-   * the first (Statistically for random should be the same but for our
-   * specific use would find differences faster).
-   */
-  @Override
-  public boolean equals(Object o) {
-    if (!(o instanceof IntArray)) {
-      return false;
-    }
-
-    IntArray array = (IntArray) o;
-    if (array.size != size) {
-      return false;
-    }
-
-    sort();
-    array.sort();
-
-    boolean equal = true;
-
-    for (int i = size; i > 0 && equal;) {
-      --i;
-      equal = (array.data[i] == this.data[i]);
-    }
-
-    return equal;
-  }
-
-  /**
-   * Sorts the data. If it is needed.
-   */
-  public void sort() {
-    if (shouldSort) {
-      shouldSort = false;
-      Arrays.sort(data, 0, size);
-    }
-  }
-
-  /**
-   * Calculates a hash-code for HashTables
-   */
-  @Override
-  public int hashCode() {
-    int hash = 0;
-    for (int i = 0; i < size; ++i) {
-      hash = data[i] ^ (hash * 31);
-    }
-    return hash;
-  }
-
-  /**
-   * Get an element from a specific index.
-   * 
-   * @param i
-   *            index of which element should be retrieved.
-   */
-  public int get(int i) {
-    if (i >= size) {
-      throw new ArrayIndexOutOfBoundsException(i);
-    }
-    return this.data[i];
-  }
-
-  public void set(int idx, int value) {
-    if (idx >= size) {
-      throw new ArrayIndexOutOfBoundsException(idx);
-    }
-    this.data[idx] = value;
-  }
-
-  /**
-   * toString or not toString. That is the question!
-   */
-  @Override
-  public String toString() {
-    String s = "(" + size + ") ";
-    for (int i = 0; i < size; ++i) {
-      s += "" + data[i] + ", ";
-    }
-    return s;
-  }
-
-  /**
-   * Clear the IntArray (set all elements to zero).
-   * @param resize - if resize is true, then clear actually allocates
-   * a new array of size 0, essentially 'clearing' the array and freeing
-   * memory.
-   */
-  public void clear(boolean resize) {
-    init(resize);
-  }
-
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/IntHashSet.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/IntHashSet.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/IntHashSet.java	2013-02-20 13:38:17.784711922 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/IntHashSet.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,548 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.Arrays;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A Set or primitive int. Implemented as a HashMap of int->int. *
- * 
- * @lucene.experimental
- */
-public class IntHashSet {
-  
-  // TODO (Facet): This is wasteful as the "values" are actually the "keys" and
-  // we could spare this amount of space (capacity * sizeof(int)). Perhaps even
-  // though it is not OOP, we should re-implement the hash for just that cause.
-
-  /**
-   * Implements an IntIterator which iterates over all the allocated indexes.
-   */
-  private final class IndexIterator implements IntIterator {
-    /**
-     * The last used baseHashIndex. Needed for "jumping" from one hash entry
-     * to another.
-     */
-    private int baseHashIndex = 0;
-
-    /**
-     * The next not-yet-visited index.
-     */
-    private int index = 0;
-
-    /**
-     * Index of the last visited pair. Used in {@link #remove()}.
-     */
-    private int lastIndex = 0;
-
-    /**
-     * Create the Iterator, make <code>index</code> point to the "first"
-     * index which is not empty. If such does not exist (eg. the map is
-     * empty) it would be zero.
-     */
-    public IndexIterator() {
-      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
-        index = baseHash[baseHashIndex];
-        if (index != 0) {
-          break;
-        }
-      }
-    }
-
-    @Override
-    public boolean hasNext() {
-      return (index != 0);
-    }
-
-    @Override
-    public int next() {
-      // Save the last index visited
-      lastIndex = index;
-
-      // next the index
-      index = next[index];
-
-      // if the next index points to the 'Ground' it means we're done with
-      // the current hash entry and we need to jump to the next one. This
-      // is done until all the hash entries had been visited.
-      while (index == 0 && ++baseHashIndex < baseHash.length) {
-        index = baseHash[baseHashIndex];
-      }
-
-      return lastIndex;
-    }
-
-    @Override
-    public void remove() {
-      IntHashSet.this.remove(keys[lastIndex]);
-    }
-
-  }
-
-  /**
-   * Implements an IntIterator, used for iteration over the map's keys.
-   */
-  private final class KeyIterator implements IntIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    KeyIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public int next() {
-      return keys[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Default capacity - in case no capacity was specified in the constructor
-   */
-  private static int defaultCapacity = 16;
-
-  /**
-   * Holds the base hash entries. if the capacity is 2^N, than the base hash
-   * holds 2^(N+1). It can hold
-   */
-  int[] baseHash;
-
-  /**
-   * The current capacity of the map. Always 2^N and never less than 16. We
-   * never use the zero index. It is needed to improve performance and is also
-   * used as "ground".
-   */
-  private int capacity;
-  
-  /**
-   * All objects are being allocated at map creation. Those objects are "free"
-   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
-   * taken from the free-linked list. as this is just a free list.
-   */
-  private int firstEmpty;
-
-  /**
-   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
-   */
-  private int hashFactor;
-
-  /**
-   * This array holds the unique keys
-   */
-  int[] keys;
-
-  /**
-   * In case of collisions, we implement a double linked list of the colliding
-   * hash's with the following next[] and prev[]. Those are also used to store
-   * the "empty" list.
-   */
-  int[] next;
-
-  private int prev;
-
-  /**
-   * Number of currently objects in the map.
-   */
-  private int size;
-
-  /**
-   * Constructs a map with default capacity.
-   */
-  public IntHashSet() {
-    this(defaultCapacity);
-  }
-
-  /**
-   * Constructs a map with given capacity. Capacity is adjusted to a native
-   * power of 2, with minimum of 16.
-   * 
-   * @param capacity
-   *            minimum capacity for the map.
-   */
-  public IntHashSet(int capacity) {
-    this.capacity = 16;
-    // Minimum capacity is 16..
-    while (this.capacity < capacity) {
-      // Multiply by 2 as long as we're still under the requested capacity
-      this.capacity <<= 1;
-    }
-
-    // As mentioned, we use the first index (0) as 'Ground', so we need the
-    // length of the arrays to be one more than the capacity
-    int arrayLength = this.capacity + 1;
-
-    this.keys = new int[arrayLength];
-    this.next = new int[arrayLength];
-
-    // Hash entries are twice as big as the capacity.
-    int baseHashSize = this.capacity << 1;
-
-    this.baseHash = new int[baseHashSize];
-
-    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
-    // {@link #calcBaseHash()}
-    this.hashFactor = baseHashSize - 1;
-
-    this.size = 0;
-
-    clear();
-  }
-
-  /**
-   * Adds a pair to the map. Takes the first empty position from the
-   * empty-linked-list's head - {@link #firstEmpty}.
-   * 
-   * New pairs are always inserted to baseHash, and are followed by the old
-   * colliding pair.
-   * 
-   * @param key
-   *            integer which maps the given value
-   */
-  private void prvt_add(int key) {
-    // Hash entry to which the new pair would be inserted
-    int hashIndex = calcBaseHashIndex(key);
-
-    // 'Allocating' a pair from the "Empty" list.
-    int objectIndex = firstEmpty;
-
-    // Setting data
-    firstEmpty = next[firstEmpty];
-    keys[objectIndex] = key;
-
-    // Inserting the new pair as the first node in the specific hash entry
-    next[objectIndex] = baseHash[hashIndex];
-    baseHash[hashIndex] = objectIndex;
-
-    // Announcing a new pair was added!
-    ++size;
-  }
-
-  /**
-   * Calculating the baseHash index using the internal <code>hashFactor</code>
-   * .
-   */
-  protected int calcBaseHashIndex(int key) {
-    return key & hashFactor;
-  }
-
-  /**
-   * Empties the map. Generates the "Empty" space list for later allocation.
-   */
-  public void clear() {
-    // Clears the hash entries
-    Arrays.fill(this.baseHash, 0);
-
-    // Set size to zero
-    size = 0;
-
-    // Mark all array entries as empty. This is done with
-    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
-    // used as 'Ground').
-    firstEmpty = 1;
-
-    // And setting all the <code>next[i]</code> to point at
-    // <code>i+1</code>.
-    for (int i = 1; i < this.capacity;) {
-      next[i] = ++i;
-    }
-
-    // Surly, the last one should point to the 'Ground'.
-    next[this.capacity] = 0;
-  }
-
-  /**
-   * Checks if a given key exists in the map.
-   * 
-   * @param value
-   *            that is checked against the map data.
-   * @return true if the key exists in the map. false otherwise.
-   */
-  public boolean contains(int value) {
-    return find(value) != 0;
-  }
-
-  /**
-   * Find the actual index of a given key.
-   * 
-   * @return index of the key. zero if the key wasn't found.
-   */
-  protected int find(int key) {
-    // Calculate the hash entry.
-    int baseHashIndex = calcBaseHashIndex(key);
-
-    // Start from the hash entry.
-    int localIndex = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (localIndex != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[localIndex] == key) {
-        return localIndex;
-      }
-
-      // next the local index
-      localIndex = next[localIndex];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return 0;
-  }
-
-  /**
-   * Find the actual index of a given key with it's baseHashIndex.<br>
-   * Some methods use the baseHashIndex. If those call {@link #find} there's
-   * no need to re-calculate that hash.
-   * 
-   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
-   *         found.
-   */
-  private int findForRemove(int key, int baseHashIndex) {
-    // Start from the hash entry.
-    this.prev = 0;
-    int index = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (index != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[index] == key) {
-        return index;
-      }
-
-      // next the local index
-      prev = index;
-      index = next[index];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    this.prev = 0;
-    return 0;
-  }
-
-  /**
-   * Grows the map. Allocates a new map of double the capacity, and
-   * fast-insert the old key-value pairs.
-   */
-  protected void grow() {
-    IntHashSet that = new IntHashSet(this.capacity * 2);
-
-    // Iterates fast over the collection. Any valid pair is put into the new
-    // map without checking for duplicates or if there's enough space for
-    // it.
-    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
-      int index = iterator.next();
-      that.prvt_add(this.keys[index]);
-    }
-    // for (int i = capacity; i > 0; --i) {
-    //
-    // that._add(this.keys[i]);
-    //
-    // }
-
-    // Copy that's data into this.
-    this.capacity = that.capacity;
-    this.size = that.size;
-    this.firstEmpty = that.firstEmpty;
-    this.keys = that.keys;
-    this.next = that.next;
-    this.baseHash = that.baseHash;
-    this.hashFactor = that.hashFactor;
-  }
-
-  /**
-   * 
-   * @return true if the map is empty. false otherwise.
-   */
-  public boolean isEmpty() {
-    return size == 0;
-  }
-
-  /**
-   * Returns a new iterator for the mapped objects.
-   */
-  public IntIterator iterator() {
-    return new KeyIterator();
-  }
-
-  /**
-   * Prints the baseHash array, used for debug purposes.
-   */
-  @SuppressWarnings("unused")
-  private String getBaseHashAsString() {
-    return Arrays.toString(this.baseHash);
-  }
-
-  /**
-   * Add a mapping int key -> int value.
-   * <p>
-   * If the key was already inside just
-   * updating the value it refers to as the given object.
-   * <p>
-   * Otherwise if the map is full, first {@link #grow()} the map.
-   * 
-   * @param value
-   *            integer which maps the given value
-   * @return true always.
-   */
-  public boolean add(int value) {
-    // Does key exists?
-    int index = find(value);
-
-    // Yes!
-    if (index != 0) {
-      return true;
-    }
-
-    // Is there enough room for a new pair?
-    if (size == capacity) {
-      // No? Than grow up!
-      grow();
-    }
-
-    // Now that everything is set, the pair can be just put inside with no
-    // worries.
-    prvt_add(value);
-
-    return true;
-  }
-
-  /**
-   * Remove a pair from the map, specified by it's key.
-   * 
-   * @param value
-   *            specify the value to be removed
-   * 
-   * @return true if the map was changed (the key was found and removed).
-   *         false otherwise.
-   */
-  public boolean remove(int value) {
-    int baseHashIndex = calcBaseHashIndex(value);
-    int index = findForRemove(value, baseHashIndex);
-    if (index != 0) {
-      // If it is the first in the collision list, we should promote its
-      // next colliding element.
-      if (prev == 0) {
-        baseHash[baseHashIndex] = next[index];
-      }
-
-      next[prev] = next[index];
-      next[index] = firstEmpty;
-      firstEmpty = index;
-      --size;
-      return true;
-    }
-
-    return false;
-  }
-
-  /**
-   * @return number of pairs currently in the map
-   */
-  public int size() {
-    return this.size;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of Objects
-   * 
-   * @return an object array of all the values currently in the map.
-   */
-  public int[] toArray() {
-    int j = -1;
-    int[] array = new int[size];
-
-    // Iterates over the values, adding them to the array.
-    for (IntIterator iterator = iterator(); iterator.hasNext();) {
-      array[++j] = iterator.next();
-    }
-    return array;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of ints
-   * 
-   * @param a
-   *            the array into which the elements of the map are to be stored,
-   *            if it is big enough; otherwise, a new array of the same
-   *            runtime type is allocated for this purpose.
-   * 
-   * @return an array containing the values stored in the map
-   * 
-   */
-  public int[] toArray(int[] a) {
-    int j = 0;
-    if (a.length < size) {
-      a = new int[size];
-    }
-    // Iterates over the values, adding them to the array.
-    for (IntIterator iterator = iterator(); j < a.length
-        && iterator.hasNext(); ++j) {
-      a[j] = iterator.next();
-    }
-    return a;
-  }
-
-  /**
-   * I have no idea why would anyone call it - but for debug purposes.<br>
-   * Prints the entire map, including the index, key, object, next and prev.
-   */
-  @Override
-  public String toString() {
-    StringBuffer sb = new StringBuffer();
-    sb.append('{');
-    IntIterator iterator = iterator();
-    while (iterator.hasNext()) {
-      sb.append(iterator.next());
-      if (iterator.hasNext()) {
-        sb.append(',');
-        sb.append(' ');
-      }
-    }
-    sb.append('}');
-    return sb.toString();
-  }
-
-  public String toHashString() {
-    String string = "\n";
-    StringBuffer sb = new StringBuffer();
-
-    for (int i = 0; i < this.baseHash.length; i++) {
-      StringBuffer sb2 = new StringBuffer();
-      boolean shouldAppend = false;
-      sb2.append(i + ".\t");
-      for (int index = baseHash[i]; index != 0; index = next[index]) {
-        sb2.append(" -> " + keys[index] + "@" + index);
-        shouldAppend = true;
-      }
-      if (shouldAppend) {
-        sb.append(sb2);
-        sb.append(string);
-      }
-    }
-
-    return sb.toString();
-  }
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/IntIterator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/IntIterator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/IntIterator.java	2013-02-20 13:38:17.784711922 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/IntIterator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,31 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Iterator interface for primitive int iteration. *
- * 
- * @lucene.experimental
- */
-public interface IntIterator {
-
-  boolean hasNext();
-  int next();
-  void remove();
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToDoubleMap.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToDoubleMap.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToDoubleMap.java	2013-02-20 13:38:17.784711922 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToDoubleMap.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,631 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.Arrays;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An Array-based hashtable which maps primitive int to a primitive double.<br>
- * The hashtable is constracted with a given capacity, or 16 as a default. In
- * case there's not enough room for new pairs, the hashtable grows. <br>
- * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
- * the hash.
- * 
- * The pre allocated arrays (for keys, values) are at length of capacity + 1,
- * when index 0 is used as 'Ground' or 'NULL'.<br>
- * 
- * The arrays are allocated ahead of hash operations, and form an 'empty space'
- * list, to which the key,value pair is allocated.
- * 
- * @lucene.experimental
- */
-public class IntToDoubleMap {
-
-  public static final double GROUND = Double.NaN;
-
-  /**
-   * Implements an IntIterator which iterates over all the allocated indexes.
-   */
-  private final class IndexIterator implements IntIterator {
-    /**
-     * The last used baseHashIndex. Needed for "jumping" from one hash entry
-     * to another.
-     */
-    private int baseHashIndex = 0;
-
-    /**
-     * The next not-yet-visited index.
-     */
-    private int index = 0;
-
-    /**
-     * Index of the last visited pair. Used in {@link #remove()}.
-     */
-    private int lastIndex = 0;
-
-    /**
-     * Create the Iterator, make <code>index</code> point to the "first"
-     * index which is not empty. If such does not exist (eg. the map is
-     * empty) it would be zero.
-     */
-    public IndexIterator() {
-      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
-        index = baseHash[baseHashIndex];
-        if (index != 0) {
-          break;
-        }
-      }
-    }
-
-    @Override
-    public boolean hasNext() {
-      return (index != 0);
-    }
-
-    @Override
-    public int next() {
-      // Save the last index visited
-      lastIndex = index;
-
-      // next the index
-      index = next[index];
-
-      // if the next index points to the 'Ground' it means we're done with
-      // the current hash entry and we need to jump to the next one. This
-      // is done until all the hash entries had been visited.
-      while (index == 0 && ++baseHashIndex < baseHash.length) {
-        index = baseHash[baseHashIndex];
-      }
-
-      return lastIndex;
-    }
-
-    @Override
-    public void remove() {
-      IntToDoubleMap.this.remove(keys[lastIndex]);
-    }
-
-  }
-
-  /**
-   * Implements an IntIterator, used for iteration over the map's keys.
-   */
-  private final class KeyIterator implements IntIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    KeyIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public int next() {
-      return keys[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Implements an Iterator of a generic type T used for iteration over the
-   * map's values.
-   */
-  private final class ValueIterator implements DoubleIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    ValueIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public double next() {
-      return values[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Default capacity - in case no capacity was specified in the constructor
-   */
-  private static int defaultCapacity = 16;
-
-  /**
-   * Holds the base hash entries. if the capacity is 2^N, than the base hash
-   * holds 2^(N+1). It can hold
-   */
-  int[] baseHash;
-
-  /**
-   * The current capacity of the map. Always 2^N and never less than 16. We
-   * never use the zero index. It is needed to improve performance and is also
-   * used as "ground".
-   */
-  private int capacity;
-  /**
-   * All objects are being allocated at map creation. Those objects are "free"
-   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
-   * taken from the free-linked list. as this is just a free list.
-   */
-  private int firstEmpty;
-
-  /**
-   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
-   */
-  private int hashFactor;
-
-  /**
-   * This array holds the unique keys
-   */
-  int[] keys;
-
-  /**
-   * In case of collisions, we implement a double linked list of the colliding
-   * hash's with the following next[] and prev[]. Those are also used to store
-   * the "empty" list.
-   */
-  int[] next;
-
-  private int prev;
-
-  /**
-   * Number of currently objects in the map.
-   */
-  private int size;
-
-  /**
-   * This array holds the values
-   */
-  double[] values;
-
-  /**
-   * Constructs a map with default capacity.
-   */
-  public IntToDoubleMap() {
-    this(defaultCapacity);
-  }
-
-  /**
-   * Constructs a map with given capacity. Capacity is adjusted to a native
-   * power of 2, with minimum of 16.
-   * 
-   * @param capacity
-   *            minimum capacity for the map.
-   */
-  public IntToDoubleMap(int capacity) {
-    this.capacity = 16;
-    // Minimum capacity is 16..
-    while (this.capacity < capacity) {
-      // Multiply by 2 as long as we're still under the requested capacity
-      this.capacity <<= 1;
-    }
-
-    // As mentioned, we use the first index (0) as 'Ground', so we need the
-    // length of the arrays to be one more than the capacity
-    int arrayLength = this.capacity + 1;
-
-    this.values = new double[arrayLength];
-    this.keys = new int[arrayLength];
-    this.next = new int[arrayLength];
-
-    // Hash entries are twice as big as the capacity.
-    int baseHashSize = this.capacity << 1;
-
-    this.baseHash = new int[baseHashSize];
-
-    this.values[0] = GROUND;
-
-    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
-    // {@link #calcBaseHash()}
-    this.hashFactor = baseHashSize - 1;
-
-    this.size = 0;
-
-    clear();
-  }
-
-  /**
-   * Adds a pair to the map. Takes the first empty position from the
-   * empty-linked-list's head - {@link #firstEmpty}.
-   * 
-   * New pairs are always inserted to baseHash, and are followed by the old
-   * colliding pair.
-   * 
-   * @param key
-   *            integer which maps the given Object
-   * @param v
-   *            double value which is being mapped using the given key
-   */
-  private void prvt_put(int key, double v) {
-    // Hash entry to which the new pair would be inserted
-    int hashIndex = calcBaseHashIndex(key);
-
-    // 'Allocating' a pair from the "Empty" list.
-    int objectIndex = firstEmpty;
-
-    // Setting data
-    firstEmpty = next[firstEmpty];
-    values[objectIndex] = v;
-    keys[objectIndex] = key;
-
-    // Inserting the new pair as the first node in the specific hash entry
-    next[objectIndex] = baseHash[hashIndex];
-    baseHash[hashIndex] = objectIndex;
-
-    // Announcing a new pair was added!
-    ++size;
-  }
-
-  /**
-   * Calculating the baseHash index using the internal <code>hashFactor</code>
-   * .
-   */
-  protected int calcBaseHashIndex(int key) {
-    return key & hashFactor;
-  }
-
-  /**
-   * Empties the map. Generates the "Empty" space list for later allocation.
-   */
-  public void clear() {
-    // Clears the hash entries
-    Arrays.fill(this.baseHash, 0);
-
-    // Set size to zero
-    size = 0;
-
-    // Mark all array entries as empty. This is done with
-    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
-    // used as 'Ground').
-    firstEmpty = 1;
-
-    // And setting all the <code>next[i]</code> to point at
-    // <code>i+1</code>.
-    for (int i = 1; i < this.capacity;) {
-      next[i] = ++i;
-    }
-
-    // Surly, the last one should point to the 'Ground'.
-    next[this.capacity] = 0;
-  }
-
-  /**
-   * Checks if a given key exists in the map.
-   * 
-   * @param key
-   *            that is checked against the map data.
-   * @return true if the key exists in the map. false otherwise.
-   */
-  public boolean containsKey(int key) {
-    return find(key) != 0;
-  }
-
-  /**
-   * Checks if the given value exists in the map.<br>
-   * This method iterates over the collection, trying to find an equal object.
-   * 
-   * @param value
-   *            double value that is checked against the map data.
-   * @return true if the value exists in the map, false otherwise.
-   */
-  public boolean containsValue(double value) {
-    for (DoubleIterator iterator = iterator(); iterator.hasNext();) {
-      double d = iterator.next();
-      if (d == value) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /**
-   * Find the actual index of a given key.
-   * 
-   * @return index of the key. zero if the key wasn't found.
-   */
-  protected int find(int key) {
-    // Calculate the hash entry.
-    int baseHashIndex = calcBaseHashIndex(key);
-
-    // Start from the hash entry.
-    int localIndex = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (localIndex != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[localIndex] == key) {
-        return localIndex;
-      }
-        
-      // next the local index
-      localIndex = next[localIndex];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return 0;
-  }
-
-  /**
-   * Find the actual index of a given key with it's baseHashIndex.<br>
-   * Some methods use the baseHashIndex. If those call {@link #find} there's
-   * no need to re-calculate that hash.
-   * 
-   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
-   *         found.
-   */
-  private int findForRemove(int key, int baseHashIndex) {
-    // Start from the hash entry.
-    this.prev = 0;
-    int index = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (index != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[index] == key) {
-        return index;
-      }
-
-      // next the local index
-      prev = index;
-      index = next[index];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    this.prev = 0;
-    return 0;
-  }
-
-  /**
-   * Returns the value mapped with the given key.
-   * 
-   * @param key
-   *            int who's mapped object we're interested in.
-   * @return a double value mapped by the given key. Double.NaN if the key wasn't found.
-   */
-  public double get(int key) {
-    return values[find(key)];
-  }
-
-  /**
-   * Grows the map. Allocates a new map of double the capacity, and
-   * fast-insert the old key-value pairs.
-   */
-  protected void grow() {
-    IntToDoubleMap that = new IntToDoubleMap(
-        this.capacity * 2);
-
-    // Iterates fast over the collection. Any valid pair is put into the new
-    // map without checking for duplicates or if there's enough space for
-    // it.
-    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
-      int index = iterator.next();
-      that.prvt_put(this.keys[index], this.values[index]);
-    }
-
-    // Copy that's data into this.
-    this.capacity = that.capacity;
-    this.size = that.size;
-    this.firstEmpty = that.firstEmpty;
-    this.values = that.values;
-    this.keys = that.keys;
-    this.next = that.next;
-    this.baseHash = that.baseHash;
-    this.hashFactor = that.hashFactor;
-  }
-
-  /**
-   * 
-   * @return true if the map is empty. false otherwise.
-   */
-  public boolean isEmpty() {
-    return size == 0;
-  }
-
-  /**
-   * Returns a new iterator for the mapped double values.
-   */
-  public DoubleIterator iterator() {
-    return new ValueIterator();
-  }
-
-  /** Returns an iterator on the map keys. */
-  public IntIterator keyIterator() {
-    return new KeyIterator();
-  }
-
-  /**
-   * Prints the baseHash array, used for debug purposes.
-   */
-  @SuppressWarnings("unused")
-  private String getBaseHashAsString() {
-    return Arrays.toString(this.baseHash);
-  }
-
-  /**
-   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
-   * this method updates the mapped value to the given one, returning the old
-   * mapped value.
-   * 
-   * @return the old mapped value, or {@link Double#NaN} if the key didn't exist.
-   */
-  public double put(int key, double v) {
-    // Does key exists?
-    int index = find(key);
-
-    // Yes!
-    if (index != 0) {
-      // Set new data and exit.
-      double old = values[index];
-      values[index] = v;
-      return old;
-    }
-
-    // Is there enough room for a new pair?
-    if (size == capacity) {
-      // No? Than grow up!
-      grow();
-    }
-
-    // Now that everything is set, the pair can be just put inside with no
-    // worries.
-    prvt_put(key, v);
-
-    return Double.NaN;
-  }
-
-  /**
-   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
-   * or {@link Double#NaN} if the none existed.
-   * 
-   * @param key used to find the value to remove
-   * @return the removed value or {@link Double#NaN} if none existed.
-   */
-  public double remove(int key) {
-    int baseHashIndex = calcBaseHashIndex(key);
-    int index = findForRemove(key, baseHashIndex);
-    if (index != 0) {
-      // If it is the first in the collision list, we should promote its
-      // next colliding element.
-      if (prev == 0) {
-        baseHash[baseHashIndex] = next[index];
-      }
-
-      next[prev] = next[index];
-      next[index] = firstEmpty;
-      firstEmpty = index;
-      --size;
-      return values[index];
-    }
-
-    return Double.NaN;
-  }
-
-  /**
-   * @return number of pairs currently in the map
-   */
-  public int size() {
-    return this.size;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of Objects
-   * 
-   * @return a double array of all the values currently in the map.
-   */
-  public double[] toArray() {
-    int j = -1;
-    double[] array = new double[size];
-
-    // Iterates over the values, adding them to the array.
-    for (DoubleIterator iterator = iterator(); iterator.hasNext();) {
-      array[++j] = iterator.next();
-    }
-    return array;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of T
-   * 
-   * @param a
-   *            the array into which the elements of the list are to be
-   *            stored. If it is big enough use whatever space we need,
-   *            setting the one after the true data as {@link Double#NaN}.
-   * 
-   * @return an array containing the elements of the list, using the given
-   *         parameter if big enough, otherwise allocate an appropriate array
-   *         and return it.
-   * 
-   */
-  public double[] toArray(double[] a) {
-    int j = 0;
-    if (a.length < this.size()) {
-      a = new double[this.size()];
-    }
-
-    // Iterates over the values, adding them to the array.
-    for (DoubleIterator iterator = iterator(); iterator.hasNext(); ++j) {
-      a[j] = iterator.next();
-    }
-
-    if (j < a.length) {
-      a[j] = Double.NaN;
-    }
-
-    return a;
-  }
-
-  @Override
-  public String toString() {
-    StringBuffer sb = new StringBuffer();
-    sb.append('{');
-    IntIterator keyIterator = keyIterator();
-    while (keyIterator.hasNext()) {
-      int key = keyIterator.next();
-      sb.append(key);
-      sb.append('=');
-      sb.append(get(key));
-      if (keyIterator.hasNext()) {
-        sb.append(',');
-        sb.append(' ');
-      }
-    }
-    sb.append('}');
-    return sb.toString();
-  }
-  
-  @Override
-  public int hashCode() {
-    return getClass().hashCode() ^ size();
-  }
-  
-  @Override
-  public boolean equals(Object o) {
-    IntToDoubleMap that = (IntToDoubleMap)o;
-    if (that.size() != this.size()) {
-      return false;
-    }
-    
-    IntIterator it = keyIterator();
-    while (it.hasNext()) {
-      int key = it.next();
-      if (!that.containsKey(key)) {
-        return false;
-      }
-
-      double v1 = this.get(key);
-      double v2 = that.get(key);
-      if (Double.compare(v1, v2) != 0) {
-        return false;
-      }
-    }
-    return true;
-  }
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToFloatMap.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToFloatMap.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToFloatMap.java	2013-02-20 13:38:17.780711923 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToFloatMap.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,631 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.Arrays;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An Array-based hashtable which maps primitive int to a primitive float.<br>
- * The hashtable is constracted with a given capacity, or 16 as a default. In
- * case there's not enough room for new pairs, the hashtable grows. <br>
- * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
- * the hash.
- * 
- * The pre allocated arrays (for keys, values) are at length of capacity + 1,
- * when index 0 is used as 'Ground' or 'NULL'.<br>
- * 
- * The arrays are allocated ahead of hash operations, and form an 'empty space'
- * list, to which the key,value pair is allocated.
- * 
- * @lucene.experimental
- */
-public class IntToFloatMap {
-
-  public static final float GROUND = Float.NaN;
-
-  /**
-   * Implements an IntIterator which iterates over all the allocated indexes.
-   */
-  private final class IndexIterator implements IntIterator {
-    /**
-     * The last used baseHashIndex. Needed for "jumping" from one hash entry
-     * to another.
-     */
-    private int baseHashIndex = 0;
-
-    /**
-     * The next not-yet-visited index.
-     */
-    private int index = 0;
-
-    /**
-     * Index of the last visited pair. Used in {@link #remove()}.
-     */
-    private int lastIndex = 0;
-
-    /**
-     * Create the Iterator, make <code>index</code> point to the "first"
-     * index which is not empty. If such does not exist (eg. the map is
-     * empty) it would be zero.
-     */
-    public IndexIterator() {
-      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
-        index = baseHash[baseHashIndex];
-        if (index != 0) {
-          break;
-        }
-      }
-    }
-
-    @Override
-    public boolean hasNext() {
-      return (index != 0);
-    }
-
-    @Override
-    public int next() {
-      // Save the last index visited
-      lastIndex = index;
-
-      // next the index
-      index = next[index];
-
-      // if the next index points to the 'Ground' it means we're done with
-      // the current hash entry and we need to jump to the next one. This
-      // is done until all the hash entries had been visited.
-      while (index == 0 && ++baseHashIndex < baseHash.length) {
-        index = baseHash[baseHashIndex];
-      }
-
-      return lastIndex;
-    }
-
-    @Override
-    public void remove() {
-      IntToFloatMap.this.remove(keys[lastIndex]);
-    }
-
-  }
-
-  /**
-   * Implements an IntIterator, used for iteration over the map's keys.
-   */
-  private final class KeyIterator implements IntIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    KeyIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public int next() {
-      return keys[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Implements an Iterator of a generic type T used for iteration over the
-   * map's values.
-   */
-  private final class ValueIterator implements FloatIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    ValueIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public float next() {
-      return values[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Default capacity - in case no capacity was specified in the constructor
-   */
-  private static int defaultCapacity = 16;
-
-  /**
-   * Holds the base hash entries. if the capacity is 2^N, than the base hash
-   * holds 2^(N+1). It can hold
-   */
-  int[] baseHash;
-
-  /**
-   * The current capacity of the map. Always 2^N and never less than 16. We
-   * never use the zero index. It is needed to improve performance and is also
-   * used as "ground".
-   */
-  private int capacity;
-  /**
-   * All objects are being allocated at map creation. Those objects are "free"
-   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
-   * taken from the free-linked list. as this is just a free list.
-   */
-  private int firstEmpty;
-
-  /**
-   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
-   */
-  private int hashFactor;
-
-  /**
-   * This array holds the unique keys
-   */
-  int[] keys;
-
-  /**
-   * In case of collisions, we implement a float linked list of the colliding
-   * hash's with the following next[] and prev[]. Those are also used to store
-   * the "empty" list.
-   */
-  int[] next;
-
-  private int prev;
-
-  /**
-   * Number of currently objects in the map.
-   */
-  private int size;
-
-  /**
-   * This array holds the values
-   */
-  float[] values;
-
-  /**
-   * Constructs a map with default capacity.
-   */
-  public IntToFloatMap() {
-    this(defaultCapacity);
-  }
-
-  /**
-   * Constructs a map with given capacity. Capacity is adjusted to a native
-   * power of 2, with minimum of 16.
-   * 
-   * @param capacity
-   *            minimum capacity for the map.
-   */
-  public IntToFloatMap(int capacity) {
-    this.capacity = 16;
-    // Minimum capacity is 16..
-    while (this.capacity < capacity) {
-      // Multiply by 2 as long as we're still under the requested capacity
-      this.capacity <<= 1;
-    }
-
-    // As mentioned, we use the first index (0) as 'Ground', so we need the
-    // length of the arrays to be one more than the capacity
-    int arrayLength = this.capacity + 1;
-
-    this.values = new float[arrayLength];
-    this.keys = new int[arrayLength];
-    this.next = new int[arrayLength];
-
-    // Hash entries are twice as big as the capacity.
-    int baseHashSize = this.capacity << 1;
-
-    this.baseHash = new int[baseHashSize];
-
-    this.values[0] = GROUND;
-
-    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
-    // {@link #calcBaseHash()}
-    this.hashFactor = baseHashSize - 1;
-
-    this.size = 0;
-
-    clear();
-  }
-
-  /**
-   * Adds a pair to the map. Takes the first empty position from the
-   * empty-linked-list's head - {@link #firstEmpty}.
-   * 
-   * New pairs are always inserted to baseHash, and are followed by the old
-   * colliding pair.
-   * 
-   * @param key
-   *            integer which maps the given Object
-   * @param v
-   *            float value which is being mapped using the given key
-   */
-  private void prvt_put(int key, float v) {
-    // Hash entry to which the new pair would be inserted
-    int hashIndex = calcBaseHashIndex(key);
-
-    // 'Allocating' a pair from the "Empty" list.
-    int objectIndex = firstEmpty;
-
-    // Setting data
-    firstEmpty = next[firstEmpty];
-    values[objectIndex] = v;
-    keys[objectIndex] = key;
-
-    // Inserting the new pair as the first node in the specific hash entry
-    next[objectIndex] = baseHash[hashIndex];
-    baseHash[hashIndex] = objectIndex;
-
-    // Announcing a new pair was added!
-    ++size;
-  }
-
-  /**
-   * Calculating the baseHash index using the internal <code>hashFactor</code>
-   * .
-   */
-  protected int calcBaseHashIndex(int key) {
-    return key & hashFactor;
-  }
-
-  /**
-   * Empties the map. Generates the "Empty" space list for later allocation.
-   */
-  public void clear() {
-    // Clears the hash entries
-    Arrays.fill(this.baseHash, 0);
-
-    // Set size to zero
-    size = 0;
-
-    // Mark all array entries as empty. This is done with
-    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
-    // used as 'Ground').
-    firstEmpty = 1;
-
-    // And setting all the <code>next[i]</code> to point at
-    // <code>i+1</code>.
-    for (int i = 1; i < this.capacity;) {
-      next[i] = ++i;
-    }
-
-    // Surly, the last one should point to the 'Ground'.
-    next[this.capacity] = 0;
-  }
-
-  /**
-   * Checks if a given key exists in the map.
-   * 
-   * @param key
-   *            that is checked against the map data.
-   * @return true if the key exists in the map. false otherwise.
-   */
-  public boolean containsKey(int key) {
-    return find(key) != 0;
-  }
-
-  /**
-   * Checks if the given value exists in the map.<br>
-   * This method iterates over the collection, trying to find an equal object.
-   * 
-   * @param value
-   *            float value that is checked against the map data.
-   * @return true if the value exists in the map, false otherwise.
-   */
-  public boolean containsValue(float value) {
-    for (FloatIterator iterator = iterator(); iterator.hasNext();) {
-      float d = iterator.next();
-      if (d == value) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /**
-   * Find the actual index of a given key.
-   * 
-   * @return index of the key. zero if the key wasn't found.
-   */
-  protected int find(int key) {
-    // Calculate the hash entry.
-    int baseHashIndex = calcBaseHashIndex(key);
-
-    // Start from the hash entry.
-    int localIndex = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (localIndex != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[localIndex] == key) {
-        return localIndex;
-      }
-        
-      // next the local index
-      localIndex = next[localIndex];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return 0;
-  }
-
-  /**
-   * Find the actual index of a given key with it's baseHashIndex.<br>
-   * Some methods use the baseHashIndex. If those call {@link #find} there's
-   * no need to re-calculate that hash.
-   * 
-   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
-   *         found.
-   */
-  private int findForRemove(int key, int baseHashIndex) {
-    // Start from the hash entry.
-    this.prev = 0;
-    int index = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (index != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[index] == key) {
-        return index;
-      }
-
-      // next the local index
-      prev = index;
-      index = next[index];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    this.prev = 0;
-    return 0;
-  }
-
-  /**
-   * Returns the value mapped with the given key.
-   * 
-   * @param key
-   *            int who's mapped object we're interested in.
-   * @return a float value mapped by the given key. float.NaN if the key wasn't found.
-   */
-  public float get(int key) {
-    return values[find(key)];
-  }
-
-  /**
-   * Grows the map. Allocates a new map of float the capacity, and
-   * fast-insert the old key-value pairs.
-   */
-  protected void grow() {
-    IntToFloatMap that = new IntToFloatMap(
-        this.capacity * 2);
-
-    // Iterates fast over the collection. Any valid pair is put into the new
-    // map without checking for duplicates or if there's enough space for
-    // it.
-    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
-      int index = iterator.next();
-      that.prvt_put(this.keys[index], this.values[index]);
-    }
-
-    // Copy that's data into this.
-    this.capacity = that.capacity;
-    this.size = that.size;
-    this.firstEmpty = that.firstEmpty;
-    this.values = that.values;
-    this.keys = that.keys;
-    this.next = that.next;
-    this.baseHash = that.baseHash;
-    this.hashFactor = that.hashFactor;
-  }
-
-  /**
-   * 
-   * @return true if the map is empty. false otherwise.
-   */
-  public boolean isEmpty() {
-    return size == 0;
-  }
-
-  /**
-   * Returns a new iterator for the mapped float values.
-   */
-  public FloatIterator iterator() {
-    return new ValueIterator();
-  }
-
-  /** Returns an iterator on the map keys. */
-  public IntIterator keyIterator() {
-    return new KeyIterator();
-  }
-
-  /**
-   * Prints the baseHash array, used for debug purposes.
-   */
-  @SuppressWarnings("unused")
-  private String getBaseHashAsString() {
-    return Arrays.toString(this.baseHash);
-  }
-
-  /**
-   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
-   * this method updates the mapped value to the given one, returning the old
-   * mapped value.
-   * 
-   * @return the old mapped value, or {@link Float#NaN} if the key didn't exist.
-   */
-  public float put(int key, float v) {
-    // Does key exists?
-    int index = find(key);
-
-    // Yes!
-    if (index != 0) {
-      // Set new data and exit.
-      float old = values[index];
-      values[index] = v;
-      return old;
-    }
-
-    // Is there enough room for a new pair?
-    if (size == capacity) {
-      // No? Than grow up!
-      grow();
-    }
-
-    // Now that everything is set, the pair can be just put inside with no
-    // worries.
-    prvt_put(key, v);
-
-    return Float.NaN;
-  }
-
-  /**
-   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
-   * or {@link Float#NaN} if the none existed.
-   * 
-   * @param key used to find the value to remove
-   * @return the removed value or {@link Float#NaN} if none existed.
-   */
-  public float remove(int key) {
-    int baseHashIndex = calcBaseHashIndex(key);
-    int index = findForRemove(key, baseHashIndex);
-    if (index != 0) {
-      // If it is the first in the collision list, we should promote its
-      // next colliding element.
-      if (prev == 0) {
-        baseHash[baseHashIndex] = next[index];
-      }
-
-      next[prev] = next[index];
-      next[index] = firstEmpty;
-      firstEmpty = index;
-      --size;
-      return values[index];
-    }
-
-    return Float.NaN;
-  }
-
-  /**
-   * @return number of pairs currently in the map
-   */
-  public int size() {
-    return this.size;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of Objects
-   * 
-   * @return a float array of all the values currently in the map.
-   */
-  public float[] toArray() {
-    int j = -1;
-    float[] array = new float[size];
-
-    // Iterates over the values, adding them to the array.
-    for (FloatIterator iterator = iterator(); iterator.hasNext();) {
-      array[++j] = iterator.next();
-    }
-    return array;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of T
-   * 
-   * @param a
-   *            the array into which the elements of the list are to be
-   *            stored. If it is big enough use whatever space we need,
-   *            setting the one after the true data as {@link Float#NaN}.
-   * 
-   * @return an array containing the elements of the list, using the given
-   *         parameter if big enough, otherwise allocate an appropriate array
-   *         and return it.
-   * 
-   */
-  public float[] toArray(float[] a) {
-    int j = 0;
-    if (a.length < this.size()) {
-      a = new float[this.size()];
-    }
-
-    // Iterates over the values, adding them to the array.
-    for (FloatIterator iterator = iterator(); iterator.hasNext(); ++j) {
-      a[j] = iterator.next();
-    }
-
-    if (j < a.length) {
-      a[j] = Float.NaN;
-    }
-
-    return a;
-  }
-
-  @Override
-  public String toString() {
-    StringBuffer sb = new StringBuffer();
-    sb.append('{');
-    IntIterator keyIterator = keyIterator();
-    while (keyIterator.hasNext()) {
-      int key = keyIterator.next();
-      sb.append(key);
-      sb.append('=');
-      sb.append(get(key));
-      if (keyIterator.hasNext()) {
-        sb.append(',');
-        sb.append(' ');
-      }
-    }
-    sb.append('}');
-    return sb.toString();
-  }
-  
-  @Override
-  public int hashCode() {
-    return getClass().hashCode() ^ size();
-  }
-  
-  @Override
-  public boolean equals(Object o) {
-    IntToFloatMap that = (IntToFloatMap)o;
-    if (that.size() != this.size()) {
-      return false;
-    }
-    
-    IntIterator it = keyIterator();
-    while (it.hasNext()) {
-      int key = it.next();
-      if (!that.containsKey(key)) {
-        return false;
-      }
-
-      float v1 = this.get(key);
-      float v2 = that.get(key);
-      if (Float.compare(v1, v2) != 0) {
-        return false;
-      }
-    }
-    return true;
-  }
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToIntMap.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToIntMap.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToIntMap.java	2013-02-20 13:38:17.784711922 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToIntMap.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,622 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.Arrays;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An Array-based hashtable which maps primitive int to primitive int.<br>
- * The hashtable is constracted with a given capacity, or 16 as a default. In
- * case there's not enough room for new pairs, the hashtable grows. <br>
- * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
- * the hash.
- * 
- * The pre allocated arrays (for keys, values) are at length of capacity + 1,
- * when index 0 is used as 'Ground' or 'NULL'.<br>
- * 
- * The arrays are allocated ahead of hash operations, and form an 'empty space'
- * list, to which the key,value pair is allocated.
- * 
- * @lucene.experimental
- */
-public class IntToIntMap {
-
-  public static final int GROUD = -1;
-  /**
-   * Implements an IntIterator which iterates over all the allocated indexes.
-   */
-  private final class IndexIterator implements IntIterator {
-    /**
-     * The last used baseHashIndex. Needed for "jumping" from one hash entry
-     * to another.
-     */
-    private int baseHashIndex = 0;
-
-    /**
-     * The next not-yet-visited index.
-     */
-    private int index = 0;
-
-    /**
-     * Index of the last visited pair. Used in {@link #remove()}.
-     */
-    private int lastIndex = 0;
-
-    /**
-     * Create the Iterator, make <code>index</code> point to the "first"
-     * index which is not empty. If such does not exist (eg. the map is
-     * empty) it would be zero.
-     */
-    public IndexIterator() {
-      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
-        index = baseHash[baseHashIndex];
-        if (index != 0) {
-          break;
-        }
-      }
-    }
-
-    @Override
-    public boolean hasNext() {
-      return (index != 0);
-    }
-
-    @Override
-    public int next() {
-      // Save the last index visited
-      lastIndex = index;
-
-      // next the index
-      index = next[index];
-
-      // if the next index points to the 'Ground' it means we're done with
-      // the current hash entry and we need to jump to the next one. This
-      // is done until all the hash entries had been visited.
-      while (index == 0 && ++baseHashIndex < baseHash.length) {
-        index = baseHash[baseHashIndex];
-      }
-
-      return lastIndex;
-    }
-
-    @Override
-    public void remove() {
-      IntToIntMap.this.remove(keys[lastIndex]);
-    }
-
-  }
-
-  /**
-   * Implements an IntIterator, used for iteration over the map's keys.
-   */
-  private final class KeyIterator implements IntIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    KeyIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public int next() {
-      return keys[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Implements an IntIterator used for iteration over the map's values.
-   */
-  private final class ValueIterator implements IntIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    ValueIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public int next() {
-      return values[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Default capacity - in case no capacity was specified in the constructor
-   */
-  private static int defaultCapacity = 16;
-
-  /**
-   * Holds the base hash entries. if the capacity is 2^N, than the base hash
-   * holds 2^(N+1). It can hold
-   */
-  int[] baseHash;
-
-  /**
-   * The current capacity of the map. Always 2^N and never less than 16. We
-   * never use the zero index. It is needed to improve performance and is also
-   * used as "ground".
-   */
-  private int capacity;
-  /**
-   * All objects are being allocated at map creation. Those objects are "free"
-   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
-   * taken from the free-linked list. as this is just a free list.
-   */
-  private int firstEmpty;
-
-  /**
-   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
-   */
-  private int hashFactor;
-
-  /**
-   * This array holds the unique keys
-   */
-  int[] keys;
-
-  /**
-   * In case of collisions, we implement a double linked list of the colliding
-   * hash's with the following next[] and prev[]. Those are also used to store
-   * the "empty" list.
-   */
-  int[] next;
-
-  private int prev;
-
-  /**
-   * Number of currently objects in the map.
-   */
-  private int size;
-
-  /**
-   * This array holds the values
-   */
-  int[] values;
-
-  /**
-   * Constructs a map with default capacity.
-   */
-  public IntToIntMap() {
-    this(defaultCapacity);
-  }
-
-  /**
-   * Constructs a map with given capacity. Capacity is adjusted to a native
-   * power of 2, with minimum of 16.
-   * 
-   * @param capacity
-   *            minimum capacity for the map.
-   */
-  public IntToIntMap(int capacity) {
-    this.capacity = 16;
-    // Minimum capacity is 16..
-    while (this.capacity < capacity) {
-      // Multiply by 2 as long as we're still under the requested capacity
-      this.capacity <<= 1;
-    }
-
-    // As mentioned, we use the first index (0) as 'Ground', so we need the
-    // length of the arrays to be one more than the capacity
-    int arrayLength = this.capacity + 1;
-
-    this.values = new int[arrayLength];
-    this.keys = new int[arrayLength];
-    this.next = new int[arrayLength];
-
-    this.values[0] = GROUD;
-
-    // Hash entries are twice as big as the capacity.
-    int baseHashSize = this.capacity << 1;
-
-    this.baseHash = new int[baseHashSize];
-
-    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
-    // {@link #calcBaseHash()}
-    this.hashFactor = baseHashSize - 1;
-
-    this.size = 0;
-
-    clear();
-  }
-
-  /**
-   * Adds a pair to the map. Takes the first empty position from the
-   * empty-linked-list's head - {@link #firstEmpty}.
-   * 
-   * New pairs are always inserted to baseHash, and are followed by the old
-   * colliding pair.
-   * 
-   * @param key
-   *            integer which maps the given value
-   * @param e
-   *            value which is being mapped using the given key
-   */
-  private void prvt_put(int key, int e) {
-    // Hash entry to which the new pair would be inserted
-    int hashIndex = calcBaseHashIndex(key);
-
-    // 'Allocating' a pair from the "Empty" list.
-    int objectIndex = firstEmpty;
-
-    // Setting data
-    firstEmpty = next[firstEmpty];
-    values[objectIndex] = e;
-    keys[objectIndex] = key;
-
-    // Inserting the new pair as the first node in the specific hash entry
-    next[objectIndex] = baseHash[hashIndex];
-    baseHash[hashIndex] = objectIndex;
-
-    // Announcing a new pair was added!
-    ++size;
-  }
-
-  /**
-   * Calculating the baseHash index using the internal <code>hashFactor</code>.
-   */
-  protected int calcBaseHashIndex(int key) {
-    return key & hashFactor;
-  }
-
-  /**
-   * Empties the map. Generates the "Empty" space list for later allocation.
-   */
-  public void clear() {
-    // Clears the hash entries
-    Arrays.fill(this.baseHash, 0);
-
-    // Set size to zero
-    size = 0;
-
-    // Mark all array entries as empty. This is done with
-    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
-    // used as 'Ground').
-    firstEmpty = 1;
-
-    // And setting all the <code>next[i]</code> to point at
-    // <code>i+1</code>.
-    for (int i = 1; i < this.capacity;) {
-      next[i] = ++i;
-    }
-
-    // Surly, the last one should point to the 'Ground'.
-    next[this.capacity] = 0;
-  }
-
-  /**
-   * Checks if a given key exists in the map.
-   * 
-   * @param key
-   *            that is checked against the map data.
-   * @return true if the key exists in the map. false otherwise.
-   */
-  public boolean containsKey(int key) {
-    return find(key) != 0;
-  }
-
-  /**
-   * Checks if the given object exists in the map.<br>
-   * This method iterates over the collection, trying to find an equal object.
-   * 
-   * @param v
-   *            value that is checked against the map data.
-   * @return true if the value exists in the map (in .equals() meaning).
-   *         false otherwise.
-   */
-  public boolean containsValue(int v) {
-    for (IntIterator iterator = iterator(); iterator.hasNext();) {
-      if (v == iterator.next()) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /**
-   * Find the actual index of a given key.
-   * 
-   * @return index of the key. zero if the key wasn't found.
-   */
-  protected int find(int key) {
-    // Calculate the hash entry.
-    int baseHashIndex = calcBaseHashIndex(key);
-
-    // Start from the hash entry.
-    int localIndex = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (localIndex != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[localIndex] == key) {
-        return localIndex;
-      }
-      
-      // next the local index
-      localIndex = next[localIndex];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return 0;
-  }
-
-  /**
-   * Find the actual index of a given key with it's baseHashIndex.<br>
-   * Some methods use the baseHashIndex. If those call {@link #find} there's
-   * no need to re-calculate that hash.
-   * 
-   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
-   *         found.
-   */
-  private int findForRemove(int key, int baseHashIndex) {
-    // Start from the hash entry.
-    this.prev = 0;
-    int index = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (index != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[index] == key) {
-        return index;
-      }
-
-      // next the local index
-      prev = index;
-      index = next[index];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    this.prev = 0;
-    return 0;
-  }
-
-  /**
-   * Returns the object mapped with the given key.
-   * 
-   * @param key
-   *            int who's mapped object we're interested in.
-   * @return an object mapped by the given key. null if the key wasn't found.
-   */
-  public int get(int key) {
-    return values[find(key)];
-  }
-
-  /**
-   * Grows the map. Allocates a new map of double the capacity, and
-   * fast-insert the old key-value pairs.
-   */
-  protected void grow() {
-    IntToIntMap that = new IntToIntMap(
-        this.capacity * 2);
-
-    // Iterates fast over the collection. Any valid pair is put into the new
-    // map without checking for duplicates or if there's enough space for
-    // it.
-    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
-      int index = iterator.next();
-      that.prvt_put(this.keys[index], this.values[index]);
-    }
-
-    // Copy that's data into this.
-    this.capacity = that.capacity;
-    this.size = that.size;
-    this.firstEmpty = that.firstEmpty;
-    this.values = that.values;
-    this.keys = that.keys;
-    this.next = that.next;
-    this.baseHash = that.baseHash;
-    this.hashFactor = that.hashFactor;
-  }
-
-  /**
-   * 
-   * @return true if the map is empty. false otherwise.
-   */
-  public boolean isEmpty() {
-    return size == 0;
-  }
-
-  /**
-   * Returns a new iterator for the mapped objects.
-   */
-  public IntIterator iterator() {
-    return new ValueIterator();
-  }
-
-  /** Returns an iterator on the map keys. */
-  public IntIterator keyIterator() {
-    return new KeyIterator();
-  }
-
-  /**
-   * Prints the baseHash array, used for debug purposes.
-   */
-  @SuppressWarnings("unused")
-  private String getBaseHashAsString() {
-    return Arrays.toString(this.baseHash);
-  }
-
-  /**
-   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
-   * this method updates the mapped value to the given one, returning the old
-   * mapped value.
-   * 
-   * @return the old mapped value, or 0 if the key didn't exist.
-   */
-  public int put(int key, int e) {
-    // Does key exists?
-    int index = find(key);
-
-    // Yes!
-    if (index != 0) {
-      // Set new data and exit.
-      int old = values[index];
-      values[index] = e;
-      return old;
-    }
-
-    // Is there enough room for a new pair?
-    if (size == capacity) {
-      // No? Than grow up!
-      grow();
-    }
-
-    // Now that everything is set, the pair can be just put inside with no
-    // worries.
-    prvt_put(key, e);
-
-    return 0;
-  }
-
-  /**
-   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
-   * or 0 if the none existed.
-   * 
-   * @param key used to find the value to remove
-   * @return the removed value or 0 if none existed.
-   */
-  public int remove(int key) {
-    int baseHashIndex = calcBaseHashIndex(key);
-    int index = findForRemove(key, baseHashIndex);
-    if (index != 0) {
-      // If it is the first in the collision list, we should promote its
-      // next colliding element.
-      if (prev == 0) {
-        baseHash[baseHashIndex] = next[index];
-      }
-
-      next[prev] = next[index];
-      next[index] = firstEmpty;
-      firstEmpty = index;
-      --size;
-      return values[index];
-    }
-
-    return 0;
-  }
-
-  /**
-   * @return number of pairs currently in the map
-   */
-  public int size() {
-    return this.size;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of Objects
-   * 
-   * @return an object array of all the values currently in the map.
-   */
-  public int[] toArray() {
-    int j = -1;
-    int[] array = new int[size];
-
-    // Iterates over the values, adding them to the array.
-    for (IntIterator iterator = iterator(); iterator.hasNext();) {
-      array[++j] = iterator.next();
-    }
-    return array;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of ints
-   * 
-   * @param a
-   *            the array into which the elements of the map are to be
-   *            stored, if it is big enough; otherwise, a new array of the
-   *            same runtime type is allocated for this purpose.
-   * 
-   * @return an array containing the values stored in the map
-   * 
-   */
-  public int[] toArray(int[] a) {
-    int j = 0;
-    if (a.length < size) {
-      a = new int[size];
-    }
-    // Iterates over the values, adding them to the array.
-    for (IntIterator iterator = iterator(); j < a.length
-      && iterator.hasNext(); ++j) {
-      a[j] = iterator.next();
-    }
-    return a;
-  }
-
-  @Override
-  public String toString() {
-    StringBuffer sb = new StringBuffer();
-    sb.append('{');
-    IntIterator keyIterator = keyIterator();
-    while (keyIterator.hasNext()) {
-      int key = keyIterator.next();
-      sb.append(key);
-      sb.append('=');
-      sb.append(get(key));
-      if (keyIterator.hasNext()) {
-        sb.append(',');
-        sb.append(' ');
-      }
-    }
-    sb.append('}');
-    return sb.toString();
-  }
-  
-  @Override
-  public int hashCode() {
-    return getClass().hashCode() ^ size();
-  }
-  
-  @Override
-  public boolean equals(Object o) {
-    IntToIntMap that = (IntToIntMap)o;
-    if (that.size() != this.size()) {
-      return false;
-    }
-    
-    IntIterator it = keyIterator();
-    while (it.hasNext()) {
-      int key = it.next();
-      
-      if (!that.containsKey(key)) {
-        return false;
-      }
-      
-      int v1 = this.get(key);
-      int v2 = that.get(key);
-      if (v1 != v2) {
-        return false;
-      }
-    }
-    return true;
-  }
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToObjectMap.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToObjectMap.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToObjectMap.java	2013-02-20 13:38:17.780711923 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToObjectMap.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,634 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.Arrays;
-import java.util.Iterator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An Array-based hashtable which maps primitive int to Objects of generic type
- * T.<br>
- * The hashtable is constracted with a given capacity, or 16 as a default. In
- * case there's not enough room for new pairs, the hashtable grows. <br>
- * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
- * the hash.
- * 
- * The pre allocated arrays (for keys, values) are at length of capacity + 1,
- * when index 0 is used as 'Ground' or 'NULL'.<br>
- * 
- * The arrays are allocated ahead of hash operations, and form an 'empty space'
- * list, to which the key,value pair is allocated.
- * 
- * @lucene.experimental
- */
-public class IntToObjectMap<T> implements Iterable<T> {
-
-  /**
-   * Implements an IntIterator which iterates over all the allocated indexes.
-   */
-  private final class IndexIterator implements IntIterator {
-    /**
-     * The last used baseHashIndex. Needed for "jumping" from one hash entry
-     * to another.
-     */
-    private int baseHashIndex = 0;
-
-    /**
-     * The next not-yet-visited index.
-     */
-    private int index = 0;
-
-    /**
-     * Index of the last visited pair. Used in {@link #remove()}.
-     */
-    private int lastIndex = 0;
-
-    /**
-     * Create the Iterator, make <code>index</code> point to the "first"
-     * index which is not empty. If such does not exist (eg. the map is
-     * empty) it would be zero.
-     */
-    public IndexIterator() {
-      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
-        index = baseHash[baseHashIndex];
-        if (index != 0) {
-          break;
-        }
-      }
-    }
-
-    @Override
-    public boolean hasNext() {
-      return (index != 0);
-    }
-
-    @Override
-    public int next() {
-      // Save the last index visited
-      lastIndex = index;
-
-      // next the index
-      index = next[index];
-
-      // if the next index points to the 'Ground' it means we're done with
-      // the current hash entry and we need to jump to the next one. This
-      // is done until all the hash entries had been visited.
-      while (index == 0 && ++baseHashIndex < baseHash.length) {
-        index = baseHash[baseHashIndex];
-      }
-
-      return lastIndex;
-    }
-
-    @Override
-    public void remove() {
-      IntToObjectMap.this.remove(keys[lastIndex]);
-    }
-
-  }
-
-  /**
-   * Implements an IntIterator, used for iteration over the map's keys.
-   */
-  private final class KeyIterator implements IntIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    KeyIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public int next() {
-      return keys[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Implements an Iterator of a generic type T used for iteration over the
-   * map's values.
-   */
-  private final class ValueIterator implements Iterator<T> {
-    private IntIterator iterator = new IndexIterator();
-
-    ValueIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public T next() {
-      return (T) values[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Default capacity - in case no capacity was specified in the constructor
-   */
-  private static int defaultCapacity = 16;
-
-  /**
-   * Holds the base hash entries. if the capacity is 2^N, than the base hash
-   * holds 2^(N+1). It can hold
-   */
-  int[] baseHash;
-
-  /**
-   * The current capacity of the map. Always 2^N and never less than 16. We
-   * never use the zero index. It is needed to improve performance and is also
-   * used as "ground".
-   */
-  private int capacity;
-  /**
-   * All objects are being allocated at map creation. Those objects are "free"
-   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
-   * taken from the free-linked list. as this is just a free list.
-   */
-  private int firstEmpty;
-
-  /**
-   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
-   */
-  private int hashFactor;
-
-  /**
-   * This array holds the unique keys
-   */
-  int[] keys;
-
-  /**
-   * In case of collisions, we implement a double linked list of the colliding
-   * hash's with the following next[] and prev[]. Those are also used to store
-   * the "empty" list.
-   */
-  int[] next;
-
-  private int prev;
-
-  /**
-   * Number of currently objects in the map.
-   */
-  private int size;
-
-  /**
-   * This array holds the values
-   */
-  Object[] values;
-
-  /**
-   * Constructs a map with default capacity.
-   */
-  public IntToObjectMap() {
-    this(defaultCapacity);
-  }
-
-  /**
-   * Constructs a map with given capacity. Capacity is adjusted to a native
-   * power of 2, with minimum of 16.
-   * 
-   * @param capacity
-   *            minimum capacity for the map.
-   */
-  public IntToObjectMap(int capacity) {
-    this.capacity = 16;
-    // Minimum capacity is 16..
-    while (this.capacity < capacity) {
-      // Multiply by 2 as long as we're still under the requested capacity
-      this.capacity <<= 1;
-    }
-
-    // As mentioned, we use the first index (0) as 'Ground', so we need the
-    // length of the arrays to be one more than the capacity
-    int arrayLength = this.capacity + 1;
-
-    this.values = new Object[arrayLength];
-    this.keys = new int[arrayLength];
-    this.next = new int[arrayLength];
-
-    // Hash entries are twice as big as the capacity.
-    int baseHashSize = this.capacity << 1;
-
-    this.baseHash = new int[baseHashSize];
-
-    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
-    // {@link #calcBaseHash()}
-    this.hashFactor = baseHashSize - 1;
-
-    this.size = 0;
-
-    clear();
-  }
-
-  /**
-   * Adds a pair to the map. Takes the first empty position from the
-   * empty-linked-list's head - {@link #firstEmpty}.
-   * 
-   * New pairs are always inserted to baseHash, and are followed by the old
-   * colliding pair.
-   * 
-   * @param key
-   *            integer which maps the given Object
-   * @param e
-   *            element which is being mapped using the given key
-   */
-  private void prvt_put(int key, T e) {
-    // Hash entry to which the new pair would be inserted
-    int hashIndex = calcBaseHashIndex(key);
-
-    // 'Allocating' a pair from the "Empty" list.
-    int objectIndex = firstEmpty;
-
-    // Setting data
-    firstEmpty = next[firstEmpty];
-    values[objectIndex] = e;
-    keys[objectIndex] = key;
-
-    // Inserting the new pair as the first node in the specific hash entry
-    next[objectIndex] = baseHash[hashIndex];
-    baseHash[hashIndex] = objectIndex;
-
-    // Announcing a new pair was added!
-    ++size;
-  }
-
-  /**
-   * Calculating the baseHash index using the internal <code>hashFactor</code>.
-   * 
-   */
-  protected int calcBaseHashIndex(int key) {
-    return key & hashFactor;
-  }
-
-  /**
-   * Empties the map. Generates the "Empty" space list for later allocation.
-   */
-  public void clear() {
-    // Clears the hash entries
-    Arrays.fill(this.baseHash, 0);
-
-    // Set size to zero
-    size = 0;
-
-    // Mark all array entries as empty. This is done with
-    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
-    // used as 'Ground').
-    firstEmpty = 1;
-
-    // And setting all the <code>next[i]</code> to point at
-    // <code>i+1</code>.
-    for (int i = 1; i < this.capacity;) {
-      next[i] = ++i;
-    }
-
-    // Surly, the last one should point to the 'Ground'.
-    next[this.capacity] = 0;
-  }
-
-  /**
-   * Checks if a given key exists in the map.
-   * 
-   * @param key
-   *            that is checked against the map data.
-   * @return true if the key exists in the map. false otherwise.
-   */
-  public boolean containsKey(int key) {
-    return find(key) != 0;
-  }
-
-  /**
-   * Checks if the given object exists in the map.<br>
-   * This method iterates over the collection, trying to find an equal object.
-   * 
-   * @param o
-   *            object that is checked against the map data.
-   * @return true if the object exists in the map (in .equals() meaning).
-   *         false otherwise.
-   */
-  public boolean containsValue(Object o) {
-    for (Iterator<T> iterator = iterator(); iterator.hasNext();) {
-      T object = iterator.next();
-      if (object.equals(o)) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /**
-   * Find the actual index of a given key.
-   * 
-   * @return index of the key. zero if the key wasn't found.
-   */
-  protected int find(int key) {
-    // Calculate the hash entry.
-    int baseHashIndex = calcBaseHashIndex(key);
-
-    // Start from the hash entry.
-    int localIndex = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (localIndex != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[localIndex] == key) {
-        return localIndex;
-      }
-
-      // next the local index
-      localIndex = next[localIndex];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return 0;
-  }
-
-  /**
-   * Find the actual index of a given key with it's baseHashIndex.<br>
-   * Some methods use the baseHashIndex. If those call {@link #find} there's
-   * no need to re-calculate that hash.
-   * 
-   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
-   *         found.
-   */
-  private int findForRemove(int key, int baseHashIndex) {
-    // Start from the hash entry.
-    this.prev = 0;
-    int index = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (index != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[index] == key) {
-        return index;
-      }
-
-      // next the local index
-      prev = index;
-      index = next[index];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    this.prev = 0;
-    return 0;
-  }
-
-  /**
-   * Returns the object mapped with the given key.
-   * 
-   * @param key
-   *            int who's mapped object we're interested in.
-   * @return an object mapped by the given key. null if the key wasn't found.
-   */
-  @SuppressWarnings("unchecked")
-  public T get(int key) {
-    return (T) values[find(key)];
-  }
-
-  /**
-   * Grows the map. Allocates a new map of double the capacity, and
-   * fast-insert the old key-value pairs.
-   */
-  @SuppressWarnings("unchecked")
-  protected void grow() {
-    IntToObjectMap<T> that = new IntToObjectMap<T>(
-        this.capacity * 2);
-
-    // Iterates fast over the collection. Any valid pair is put into the new
-    // map without checking for duplicates or if there's enough space for
-    // it.
-    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
-      int index = iterator.next();
-      that.prvt_put(this.keys[index], (T) this.values[index]);
-    }
-
-    // Copy that's data into this.
-    this.capacity = that.capacity;
-    this.size = that.size;
-    this.firstEmpty = that.firstEmpty;
-    this.values = that.values;
-    this.keys = that.keys;
-    this.next = that.next;
-    this.baseHash = that.baseHash;
-    this.hashFactor = that.hashFactor;
-  }
-
-  /**
-   * 
-   * @return true if the map is empty. false otherwise.
-   */
-  public boolean isEmpty() {
-    return size == 0;
-  }
-
-  /**
-   * Returns a new iterator for the mapped objects.
-   */
-  @Override
-  public Iterator<T> iterator() {
-    return new ValueIterator();
-  }
-
-  /** Returns an iterator on the map keys. */
-  public IntIterator keyIterator() {
-    return new KeyIterator();
-  }
-
-  /**
-   * Prints the baseHash array, used for debug purposes.
-   */
-  @SuppressWarnings("unused")
-  private String getBaseHashAsString() {
-    return Arrays.toString(baseHash);
-  }
-
-  /**
-   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
-   * this method updates the mapped value to the given one, returning the old
-   * mapped value.
-   * 
-   * @return the old mapped value, or null if the key didn't exist.
-   */
-  @SuppressWarnings("unchecked")
-  public T put(int key, T e) {
-    // Does key exists?
-    int index = find(key);
-
-    // Yes!
-    if (index != 0) {
-      // Set new data and exit.
-      T old = (T) values[index];
-      values[index] = e;
-      return old;
-    }
-
-    // Is there enough room for a new pair?
-    if (size == capacity) {
-      // No? Than grow up!
-      grow();
-    }
-
-    // Now that everything is set, the pair can be just put inside with no
-    // worries.
-    prvt_put(key, e);
-
-    return null;
-  }
-
-  /**
-   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
-   * or null if the none existed.
-   * 
-   * @param key used to find the value to remove
-   * @return the removed value or null if none existed.
-   */
-  @SuppressWarnings("unchecked")
-  public T remove(int key) {
-    int baseHashIndex = calcBaseHashIndex(key);
-    int index = findForRemove(key, baseHashIndex);
-    if (index != 0) {
-      // If it is the first in the collision list, we should promote its
-      // next colliding element.
-      if (prev == 0) {
-        baseHash[baseHashIndex] = next[index];
-      }
-
-      next[prev] = next[index];
-      next[index] = firstEmpty;
-      firstEmpty = index;
-      --size;
-      return (T) values[index];
-    }
-
-    return null;
-  }
-
-  /**
-   * @return number of pairs currently in the map
-   */
-  public int size() {
-    return this.size;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of Objects
-   * 
-   * @return an object array of all the values currently in the map.
-   */
-  public Object[] toArray() {
-    int j = -1;
-    Object[] array = new Object[size];
-
-    // Iterates over the values, adding them to the array.
-    for (Iterator<T> iterator = iterator(); iterator.hasNext();) {
-      array[++j] = iterator.next();
-    }
-    return array;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of T
-   * 
-   * @param a
-   *            the array into which the elements of the list are to be
-   *            stored, if it is big enough; otherwise, use whatever space we
-   *            have, setting the one after the true data as null.
-   * 
-   * @return an array containing the elements of the list
-   * 
-   */
-  public T[] toArray(T[] a) {
-    int j = 0;
-    // Iterates over the values, adding them to the array.
-    for (Iterator<T> iterator = iterator(); j < a.length
-    && iterator.hasNext(); ++j) {
-      a[j] = iterator.next();
-    }
-
-    if (j < a.length) {
-      a[j] = null;
-    }
-
-    return a;
-  }
-
-  @Override
-  public String toString() {
-    StringBuffer sb = new StringBuffer();
-    sb.append('{');
-    IntIterator keyIterator = keyIterator();
-    while (keyIterator.hasNext()) {
-      int key = keyIterator.next();
-      sb.append(key);
-      sb.append('=');
-      sb.append(get(key));
-      if (keyIterator.hasNext()) {
-        sb.append(',');
-        sb.append(' ');
-      }
-    }
-    sb.append('}');
-    return sb.toString();
-  }
-  
-  @Override
-  public int hashCode() {
-    return getClass().hashCode() ^ size();
-  }
-  
-  @SuppressWarnings("unchecked")
-  @Override
-  public boolean equals(Object o) {
-    IntToObjectMap<T> that = (IntToObjectMap<T>)o;
-    if (that.size() != this.size()) {
-      return false;
-    }
-    
-    IntIterator it = keyIterator();
-    while (it.hasNext()) {
-      int key = it.next();
-      if (!that.containsKey(key)) {
-        return false;
-      }
-
-      T v1 = this.get(key);
-      T v2 = that.get(key);
-      if ((v1 == null && v2 != null) ||
-          (v1 != null && v2 == null) ||
-          (!v1.equals(v2))) {
-        return false;
-      }
-    }
-    return true;
-  }
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/LRUHashMap.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/LRUHashMap.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/LRUHashMap.java	2013-02-20 13:38:17.784711922 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/LRUHashMap.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,111 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.LinkedHashMap;
-import java.util.Map;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * LRUHashMap is an extension of Java's HashMap, which has a bounded size();
- * When it reaches that size, each time a new element is added, the least
- * recently used (LRU) entry is removed.
- * <p>
- * Java makes it very easy to implement LRUHashMap - all its functionality is
- * already available from {@link java.util.LinkedHashMap}, and we just need to
- * configure that properly.
- * <p>
- * Note that like HashMap, LRUHashMap is unsynchronized, and the user MUST
- * synchronize the access to it if used from several threads. Moreover, while
- * with HashMap this is only a concern if one of the threads is modifies the
- * map, with LURHashMap every read is a modification (because the LRU order
- * needs to be remembered) so proper synchronization is always necessary.
- * <p>
- * With the usual synchronization mechanisms available to the user, this
- * unfortunately means that LRUHashMap will probably perform sub-optimally under
- * heavy contention: while one thread uses the hash table (reads or writes), any
- * other thread will be blocked from using it - or even just starting to use it
- * (e.g., calculating the hash function). A more efficient approach would be not
- * to use LinkedHashMap at all, but rather to use a non-locking (as much as
- * possible) thread-safe solution, something along the lines of
- * java.util.concurrent.ConcurrentHashMap (though that particular class does not
- * support the additional LRU semantics, which will need to be added separately
- * using a concurrent linked list or additional storage of timestamps (in an
- * array or inside the entry objects), or whatever).
- * 
- * @lucene.experimental
- */
-public class LRUHashMap<K,V> extends LinkedHashMap<K,V> {
-
-  private int maxSize;
-
-  /**
-   * Create a new hash map with a bounded size and with least recently
-   * used entries removed.
-   * @param maxSize
-   *     the maximum size (in number of entries) to which the map can grow
-   *     before the least recently used entries start being removed.<BR>
-   *      Setting maxSize to a very large value, like
-   *      {@link Integer#MAX_VALUE} is allowed, but is less efficient than
-   *      using {@link java.util.HashMap} because our class needs
-   *      to keep track of the use order (via an additional doubly-linked
-   *      list) which is not used when the map's size is always below the
-   *      maximum size. 
-   */
-  public LRUHashMap(int maxSize) {
-    super(16, 0.75f, true);
-    this.maxSize = maxSize;
-  }
-
-  /**
-   * Return the max size
-   */
-  public int getMaxSize() {
-    return maxSize;
-  }
-
-  /**
-   * setMaxSize() allows changing the map's maximal number of elements
-   * which was defined at construction time.
-   * <P>
-   * Note that if the map is already larger than maxSize, the current 
-   * implementation does not shrink it (by removing the oldest elements);
-   * Rather, the map remains in its current size as new elements are
-   * added, and will only start shrinking (until settling again on the
-   * give maxSize) if existing elements are explicitly deleted.  
-   */
-  public void setMaxSize(int maxSize) {
-    this.maxSize = maxSize;
-  }
-
-  // We override LinkedHashMap's removeEldestEntry() method. This method
-  // is called every time a new entry is added, and if we return true
-  // here, the eldest element will be deleted automatically. In our case,
-  // we return true if the size of the map grew beyond our limit - ignoring
-  // what is that eldest element that we'll be deleting.
-  @Override
-  protected boolean removeEldestEntry(Map.Entry<K, V> eldest) {
-    return size() > maxSize;
-  }
-
-  @SuppressWarnings("unchecked")
-  @Override
-  public LRUHashMap<K,V> clone() {
-    return (LRUHashMap<K,V>) super.clone();
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/ObjectToFloatMap.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/ObjectToFloatMap.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/ObjectToFloatMap.java	2013-02-20 13:38:17.784711922 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/ObjectToFloatMap.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,623 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.Arrays;
-import java.util.Iterator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An Array-based hashtable which maps Objects of generic type
- * T to primitive float values.<br>
- * The hashtable is constructed with a given capacity, or 16 as a default. In
- * case there's not enough room for new pairs, the hashtable grows. <br>
- * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
- * the hash.
- * 
- * The pre allocated arrays (for keys, values) are at length of capacity + 1,
- * when index 0 is used as 'Ground' or 'NULL'.<br>
- * 
- * The arrays are allocated ahead of hash operations, and form an 'empty space'
- * list, to which the key,value pair is allocated.
- * 
- * @lucene.experimental
- */
-public class ObjectToFloatMap<K> {
-
-  /**
-   * Implements an IntIterator which iterates over all the allocated indexes.
-   */
-  private final class IndexIterator implements IntIterator {
-    /**
-     * The last used baseHashIndex. Needed for "jumping" from one hash entry
-     * to another.
-     */
-    private int baseHashIndex = 0;
-
-    /**
-     * The next not-yet-visited index.
-     */
-    private int index = 0;
-
-    /**
-     * Index of the last visited pair. Used in {@link #remove()}.
-     */
-    private int lastIndex = 0;
-
-    /**
-     * Create the Iterator, make <code>index</code> point to the "first"
-     * index which is not empty. If such does not exist (eg. the map is
-     * empty) it would be zero.
-     */
-    public IndexIterator() {
-      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
-        index = baseHash[baseHashIndex];
-        if (index != 0) {
-          break;
-        }
-      }
-    }
-
-    @Override
-    public boolean hasNext() {
-      return (index != 0);
-    }
-
-    @Override
-    public int next() {
-      // Save the last index visited
-      lastIndex = index;
-
-      // next the index
-      index = next[index];
-
-      // if the next index points to the 'Ground' it means we're done with
-      // the current hash entry and we need to jump to the next one. This
-      // is done until all the hash entries had been visited.
-      while (index == 0 && ++baseHashIndex < baseHash.length) {
-        index = baseHash[baseHashIndex];
-      }
-
-      return lastIndex;
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public void remove() {
-      ObjectToFloatMap.this.remove((K) keys[lastIndex]);
-    }
-
-  }
-
-  /**
-   * Implements an IntIterator, used for iteration over the map's keys.
-   */
-  private final class KeyIterator implements Iterator<K> {
-    private IntIterator iterator = new IndexIterator();
-
-    KeyIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public K next() {
-      return (K) keys[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Implements an Iterator of a generic type T used for iteration over the
-   * map's values.
-   */
-  private final class ValueIterator implements FloatIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    ValueIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public float next() {
-      return values[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Default capacity - in case no capacity was specified in the constructor
-   */
-  private static int defaultCapacity = 16;
-
-  /**
-   * Holds the base hash entries. if the capacity is 2^N, than the base hash
-   * holds 2^(N+1). It can hold
-   */
-  int[] baseHash;
-
-  /**
-   * The current capacity of the map. Always 2^N and never less than 16. We
-   * never use the zero index. It is needed to improve performance and is also
-   * used as "ground".
-   */
-  private int capacity;
-  /**
-   * All objects are being allocated at map creation. Those objects are "free"
-   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
-   * taken from the free-linked list. as this is just a free list.
-   */
-  private int firstEmpty;
-
-  /**
-   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
-   */
-  private int hashFactor;
-
-  /**
-   * This array holds the unique keys
-   */
-  Object[] keys;
-
-  /**
-   * In case of collisions, we implement a double linked list of the colliding
-   * hash's with the following next[] and prev[]. Those are also used to store
-   * the "empty" list.
-   */
-  int[] next;
-
-  private int prev;
-
-  /**
-   * Number of currently objects in the map.
-   */
-  private int size;
-
-  /**
-   * This array holds the values
-   */
-  float[] values;
-
-  /**
-   * Constructs a map with default capacity.
-   */
-  public ObjectToFloatMap() {
-    this(defaultCapacity);
-  }
-
-  /**
-   * Constructs a map with given capacity. Capacity is adjusted to a native
-   * power of 2, with minimum of 16.
-   * 
-   * @param capacity
-   *            minimum capacity for the map.
-   */
-  public ObjectToFloatMap(int capacity) {
-    this.capacity = 16;
-    // Minimum capacity is 16..
-    while (this.capacity < capacity) {
-      // Multiply by 2 as long as we're still under the requested capacity
-      this.capacity <<= 1;
-    }
-
-    // As mentioned, we use the first index (0) as 'Ground', so we need the
-    // length of the arrays to be one more than the capacity
-    int arrayLength = this.capacity + 1;
-
-    this.values = new float[arrayLength];
-    this.keys = new Object[arrayLength];
-    this.next = new int[arrayLength];
-
-    // Hash entries are twice as big as the capacity.
-    int baseHashSize = this.capacity << 1;
-
-    this.baseHash = new int[baseHashSize];
-
-    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
-    // {@link #calcBaseHash()}
-    this.hashFactor = baseHashSize - 1;
-
-    this.size = 0;
-
-    clear();
-  }
-
-  /**
-   * Adds a pair to the map. Takes the first empty position from the
-   * empty-linked-list's head - {@link #firstEmpty}.
-   * 
-   * New pairs are always inserted to baseHash, and are followed by the old
-   * colliding pair.
-   * 
-   * @param key
-   *            integer which maps the given Object
-   * @param e
-   *            element which is being mapped using the given key
-   */
-  private void prvt_put(K key, float e) {
-    // Hash entry to which the new pair would be inserted
-    int hashIndex = calcBaseHashIndex(key);
-
-    // 'Allocating' a pair from the "Empty" list.
-    int objectIndex = firstEmpty;
-
-    // Setting data
-    firstEmpty = next[firstEmpty];
-    values[objectIndex] = e;
-    keys[objectIndex] = key;
-
-    // Inserting the new pair as the first node in the specific hash entry
-    next[objectIndex] = baseHash[hashIndex];
-    baseHash[hashIndex] = objectIndex;
-
-    // Announcing a new pair was added!
-    ++size;
-  }
-
-  /**
-   * Calculating the baseHash index using the internal <code>hashFactor</code>.
-   */
-  protected int calcBaseHashIndex(K key) {
-    return key.hashCode() & hashFactor;
-  }
-
-  /**
-   * Empties the map. Generates the "Empty" space list for later allocation.
-   */
-  public void clear() {
-    // Clears the hash entries
-    Arrays.fill(this.baseHash, 0);
-
-    // Set size to zero
-    size = 0;
-
-    values[0] = Float.NaN;
-
-    // Mark all array entries as empty. This is done with
-    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
-    // used as 'Ground').
-    firstEmpty = 1;
-
-    // And setting all the <code>next[i]</code> to point at
-    // <code>i+1</code>.
-    for (int i = 1; i < this.capacity;) {
-      next[i] = ++i;
-    }
-
-    // Surly, the last one should point to the 'Ground'.
-    next[this.capacity] = 0;
-  }
-
-  /**
-   * Checks if a given key exists in the map.
-   * 
-   * @param key
-   *            that is checked against the map data.
-   * @return true if the key exists in the map. false otherwise.
-   */
-  public boolean containsKey(K key) {
-    return find(key) != 0;
-  }
-
-  /**
-   * Checks if the given object exists in the map.<br>
-   * This method iterates over the collection, trying to find an equal object.
-   * 
-   * @param o
-   *            object that is checked against the map data.
-   * @return true if the object exists in the map (in .equals() meaning).
-   *         false otherwise.
-   */
-  public boolean containsValue(float o) {
-    for (FloatIterator iterator = iterator(); iterator.hasNext();) {
-      if (o == iterator.next()) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /**
-   * Find the actual index of a given key.
-   * 
-   * @return index of the key. zero if the key wasn't found.
-   */
-  protected int find(K key) {
-    // Calculate the hash entry.
-    int baseHashIndex = calcBaseHashIndex(key);
-
-    // Start from the hash entry.
-    int localIndex = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (localIndex != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[localIndex].equals(key)) {
-        return localIndex;
-      }
-
-      // next the local index
-      localIndex = next[localIndex];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return 0;
-  }
-
-  /**
-   * Find the actual index of a given key with it's baseHashIndex.<br>
-   * Some methods use the baseHashIndex. If those call {@link #find} there's
-   * no need to re-calculate that hash.
-   * 
-   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
-   *         found.
-   */
-  private int findForRemove(K key, int baseHashIndex) {
-    // Start from the hash entry.
-    this.prev = 0;
-    int index = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (index != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[index].equals(key)) {
-        return index;
-      }
-
-      // next the local index
-      prev = index;
-      index = next[index];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    this.prev = 0;
-    return 0;
-  }
-
-  /**
-   * Returns the float mapped with the given key.
-   * 
-   * @param key
-   *            object who's mapped float we're interested in.
-   * @return a float mapped by the given key. Float.NaN if the key wasn't found.
-   */
-  public float get(K key) {
-    return values[find(key)];
-  }
-
-  /**
-   * Grows the map. Allocates a new map of double the capacity, and
-   * fast-insert the old key-value pairs.
-   */
-  @SuppressWarnings("unchecked")
-  protected void grow() {
-    ObjectToFloatMap<K> that = new ObjectToFloatMap<K>(
-        this.capacity * 2);
-
-    // Iterates fast over the collection. Any valid pair is put into the new
-    // map without checking for duplicates or if there's enough space for
-    // it.
-    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
-      int index = iterator.next();
-      that.prvt_put((K) this.keys[index], this.values[index]);
-    }
-
-    // Copy that's data into this.
-    this.capacity = that.capacity;
-    this.size = that.size;
-    this.firstEmpty = that.firstEmpty;
-    this.values = that.values;
-    this.keys = that.keys;
-    this.next = that.next;
-    this.baseHash = that.baseHash;
-    this.hashFactor = that.hashFactor;
-  }
-
-  /**
-   * 
-   * @return true if the map is empty. false otherwise.
-   */
-  public boolean isEmpty() {
-    return size == 0;
-  }
-
-  /**
-   * Returns a new iterator for the mapped floats.
-   */
-  public FloatIterator iterator() {
-    return new ValueIterator();
-  }
-
-  /** Returns an iterator on the map keys. */
-  public Iterator<K> keyIterator() {
-    return new KeyIterator();
-  }
-
-  /**
-   * Prints the baseHash array, used for debug purposes.
-   */
-  @SuppressWarnings("unused")
-  private String getBaseHashAsString() {
-    return Arrays.toString(baseHash);
-  }
-
-  /**
-   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
-   * this method updates the mapped value to the given one, returning the old
-   * mapped value.
-   * 
-   * @return the old mapped value, or {@link Float#NaN} if the key didn't exist.
-   */
-  public float put(K key, float e) {
-    // Does key exists?
-    int index = find(key);
-
-    // Yes!
-    if (index != 0) {
-      // Set new data and exit.
-      float old = values[index];
-      values[index] = e;
-      return old;
-    }
-
-    // Is there enough room for a new pair?
-    if (size == capacity) {
-      // No? Than grow up!
-      grow();
-    }
-    
-    // Now that everything is set, the pair can be just put inside with no
-    // worries.
-    prvt_put(key, e);
-
-    return Float.NaN;
-  }
-
-  /**
-   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
-   * or {@link Float#NaN} if the none existed.
-   * 
-   * @param key used to find the value to remove
-   * @return the removed value or {@link Float#NaN} if none existed.
-   */
-  public float remove(K key) {
-    int baseHashIndex = calcBaseHashIndex(key);
-    int index = findForRemove(key, baseHashIndex);
-    if (index != 0) {
-      // If it is the first in the collision list, we should promote its
-      // next colliding element.
-      if (prev == 0) {
-        baseHash[baseHashIndex] = next[index];
-      }
-
-      next[prev] = next[index];
-      next[index] = firstEmpty;
-      firstEmpty = index;
-      --size;
-      return values[index];
-    }
-
-    return Float.NaN;
-  }
-
-  /**
-   * @return number of pairs currently in the map
-   */
-  public int size() {
-    return this.size;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of Objects
-   * 
-   * @return an object array of all the values currently in the map.
-   */
-  public float[] toArray() {
-    int j = -1;
-    float[] array = new float[size];
-
-    // Iterates over the values, adding them to the array.
-    for (FloatIterator iterator = iterator(); iterator.hasNext();) {
-      array[++j] = iterator.next();
-    }
-    return array;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of T
-   * 
-   * @param a
-   *            the array into which the elements of the list are to be
-   *            stored, if it is big enough; otherwise, use as much space as it can.
-   * 
-   * @return an array containing the elements of the list
-   * 
-   */
-  public float[] toArray(float[] a) {
-    int j = 0;
-    // Iterates over the values, adding them to the array.
-    for (FloatIterator iterator = iterator(); j < a.length
-    && iterator.hasNext(); ++j) {
-      a[j] = iterator.next();
-    }
-    if (j < a.length) {
-      a[j] = Float.NaN;
-    }
-
-    return a;
-  }
-
-  @Override
-  public String toString() {
-    StringBuffer sb = new StringBuffer();
-    sb.append('{');
-    Iterator<K> keyIterator = keyIterator();
-    while (keyIterator.hasNext()) {
-      K key = keyIterator.next();
-      sb.append(key);
-      sb.append('=');
-      sb.append(get(key));
-      if (keyIterator.hasNext()) {
-        sb.append(',');
-        sb.append(' ');
-      }
-    }
-    sb.append('}');
-    return sb.toString();
-  }
-  
-  @Override
-  public int hashCode() {
-    return getClass().hashCode() ^ size();
-  }
-  
-  @SuppressWarnings("unchecked")
-  @Override
-  public boolean equals(Object o) {
-    ObjectToFloatMap<K> that = (ObjectToFloatMap<K>)o;
-    if (that.size() != this.size()) {
-      return false;
-    }
-    
-    Iterator<K> it = keyIterator();
-    while (it.hasNext()) {
-      K key = it.next();
-      float v1 = this.get(key);
-      float v2 = that.get(key);
-      if (Float.compare(v1, v2) != 0) {
-        return false;
-      }
-    }
-    return true;
-  }
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/ObjectToIntMap.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/ObjectToIntMap.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/ObjectToIntMap.java	2013-02-20 13:38:17.784711922 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/ObjectToIntMap.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,622 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.Arrays;
-import java.util.Iterator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An Array-based hashtable which maps Objects of generic type
- * T to primitive int values.<br>
- * The hashtable is constructed with a given capacity, or 16 as a default. In
- * case there's not enough room for new pairs, the hashtable grows. <br>
- * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
- * the hash.
- * 
- * The pre allocated arrays (for keys, values) are at length of capacity + 1,
- * when index 0 is used as 'Ground' or 'NULL'.<br>
- * 
- * The arrays are allocated ahead of hash operations, and form an 'empty space'
- * list, to which the key,value pair is allocated.
- * 
- * @lucene.experimental
- */
-public class ObjectToIntMap<K> {
-
-  /**
-   * Implements an IntIterator which iterates over all the allocated indexes.
-   */
-  private final class IndexIterator implements IntIterator {
-    /**
-     * The last used baseHashIndex. Needed for "jumping" from one hash entry
-     * to another.
-     */
-    private int baseHashIndex = 0;
-
-    /**
-     * The next not-yet-visited index.
-     */
-    private int index = 0;
-
-    /**
-     * Index of the last visited pair. Used in {@link #remove()}.
-     */
-    private int lastIndex = 0;
-
-    /**
-     * Create the Iterator, make <code>index</code> point to the "first"
-     * index which is not empty. If such does not exist (eg. the map is
-     * empty) it would be zero.
-     */
-    public IndexIterator() {
-      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
-        index = baseHash[baseHashIndex];
-        if (index != 0) {
-          break;
-        }
-      }
-    }
-
-    @Override
-    public boolean hasNext() {
-      return (index != 0);
-    }
-
-    @Override
-    public int next() {
-      // Save the last index visited
-      lastIndex = index;
-
-      // next the index
-      index = next[index];
-
-      // if the next index points to the 'Ground' it means we're done with
-      // the current hash entry and we need to jump to the next one. This
-      // is done until all the hash entries had been visited.
-      while (index == 0 && ++baseHashIndex < baseHash.length) {
-        index = baseHash[baseHashIndex];
-      }
-
-      return lastIndex;
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public void remove() {
-      ObjectToIntMap.this.remove((K) keys[lastIndex]);
-    }
-
-  }
-
-  /**
-   * Implements an IntIterator, used for iteration over the map's keys.
-   */
-  private final class KeyIterator implements Iterator<K> {
-    private IntIterator iterator = new IndexIterator();
-
-    KeyIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public K next() {
-      return (K) keys[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Implements an Iterator of a generic type T used for iteration over the
-   * map's values.
-   */
-  private final class ValueIterator implements IntIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    ValueIterator() {}
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public int next() {
-      return values[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Default capacity - in case no capacity was specified in the constructor
-   */
-  private static int defaultCapacity = 16;
-
-  /**
-   * Holds the base hash entries. if the capacity is 2^N, than the base hash
-   * holds 2^(N+1). It can hold
-   */
-  int[] baseHash;
-
-  /**
-   * The current capacity of the map. Always 2^N and never less than 16. We
-   * never use the zero index. It is needed to improve performance and is also
-   * used as "ground".
-   */
-  private int capacity;
-  /**
-   * All objects are being allocated at map creation. Those objects are "free"
-   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
-   * taken from the free-linked list. as this is just a free list.
-   */
-  private int firstEmpty;
-
-  /**
-   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
-   */
-  private int hashFactor;
-
-  /**
-   * This array holds the unique keys
-   */
-  Object[] keys;
-
-  /**
-   * In case of collisions, we implement a double linked list of the colliding
-   * hash's with the following next[] and prev[]. Those are also used to store
-   * the "empty" list.
-   */
-  int[] next;
-
-  private int prev;
-
-  /**
-   * Number of currently objects in the map.
-   */
-  private int size;
-
-  /**
-   * This array holds the values
-   */
-  int[] values;
-
-  /**
-   * Constructs a map with default capacity.
-   */
-  public ObjectToIntMap() {
-    this(defaultCapacity);
-  }
-
-  /**
-   * Constructs a map with given capacity. Capacity is adjusted to a native
-   * power of 2, with minimum of 16.
-   * 
-   * @param capacity
-   *            minimum capacity for the map.
-   */
-  public ObjectToIntMap(int capacity) {
-    this.capacity = 16;
-    // Minimum capacity is 16..
-    while (this.capacity < capacity) {
-      // Multiply by 2 as long as we're still under the requested capacity
-      this.capacity <<= 1;
-    }
-
-    // As mentioned, we use the first index (0) as 'Ground', so we need the
-    // length of the arrays to be one more than the capacity
-    int arrayLength = this.capacity + 1;
-
-    this.values = new int[arrayLength];
-    this.keys = new Object[arrayLength];
-    this.next = new int[arrayLength];
-
-    // Hash entries are twice as big as the capacity.
-    int baseHashSize = this.capacity << 1;
-
-    this.baseHash = new int[baseHashSize];
-
-    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
-    // {@link #calcBaseHash()}
-    this.hashFactor = baseHashSize - 1;
-
-    this.size = 0;
-
-    clear();
-  }
-
-  /**
-   * Adds a pair to the map. Takes the first empty position from the
-   * empty-linked-list's head - {@link #firstEmpty}.
-   * 
-   * New pairs are always inserted to baseHash, and are followed by the old
-   * colliding pair.
-   * 
-   * @param key
-   *            integer which maps the given Object
-   * @param e
-   *            element which is being mapped using the given key
-   */
-  private void prvt_put(K key, int e) {
-    // Hash entry to which the new pair would be inserted
-    int hashIndex = calcBaseHashIndex(key);
-
-    // 'Allocating' a pair from the "Empty" list.
-    int objectIndex = firstEmpty;
-
-    // Setting data
-    firstEmpty = next[firstEmpty];
-    values[objectIndex] = e;
-    keys[objectIndex] = key;
-
-    // Inserting the new pair as the first node in the specific hash entry
-    next[objectIndex] = baseHash[hashIndex];
-    baseHash[hashIndex] = objectIndex;
-
-    // Announcing a new pair was added!
-    ++size;
-  }
-
-  /**
-   * Calculating the baseHash index using the internal <code>hashFactor</code>.
-   */
-  protected int calcBaseHashIndex(K key) {
-    return key.hashCode() & hashFactor;
-  }
-
-  /**
-   * Empties the map. Generates the "Empty" space list for later allocation.
-   */
-  public void clear() {
-    // Clears the hash entries
-    Arrays.fill(this.baseHash, 0);
-
-    // Set size to zero
-    size = 0;
-
-    values[0] = Integer.MAX_VALUE;
-
-    // Mark all array entries as empty. This is done with
-    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
-    // used as 'Ground').
-    firstEmpty = 1;
-
-    // And setting all the <code>next[i]</code> to point at
-    // <code>i+1</code>.
-    for (int i = 1; i < this.capacity;) {
-      next[i] = ++i;
-    }
-
-    // Surly, the last one should point to the 'Ground'.
-    next[this.capacity] = 0;
-  }
-
-  /**
-   * Checks if a given key exists in the map.
-   * 
-   * @param key
-   *            that is checked against the map data.
-   * @return true if the key exists in the map. false otherwise.
-   */
-  public boolean containsKey(K key) {
-    return find(key) != 0;
-  }
-
-  /**
-   * Checks if the given object exists in the map.<br>
-   * This method iterates over the collection, trying to find an equal object.
-   * 
-   * @param o
-   *            object that is checked against the map data.
-   * @return true if the object exists in the map (in .equals() meaning).
-   *         false otherwise.
-   */
-  public boolean containsValue(int o) {
-    for (IntIterator iterator = iterator(); iterator.hasNext();) {
-      if (o == iterator.next()) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /**
-   * Find the actual index of a given key.
-   * 
-   * @return index of the key. zero if the key wasn't found.
-   */
-  protected int find(K key) {
-    // Calculate the hash entry.
-    int baseHashIndex = calcBaseHashIndex(key);
-
-    // Start from the hash entry.
-    int localIndex = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (localIndex != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[localIndex].equals(key)) {
-        return localIndex;
-      }
-
-      // next the local index
-      localIndex = next[localIndex];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return 0;
-  }
-
-  /**
-   * Find the actual index of a given key with it's baseHashIndex.<br>
-   * Some methods use the baseHashIndex. If those call {@link #find} there's
-   * no need to re-calculate that hash.
-   * 
-   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
-   *         found.
-   */
-  private int findForRemove(K key, int baseHashIndex) {
-    // Start from the hash entry.
-    this.prev = 0;
-    int index = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (index != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[index].equals(key)) {
-        return index;
-      }
-
-      // next the local index
-      prev = index;
-      index = next[index];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    this.prev = 0;
-    return 0;
-  }
-
-  /**
-   * Returns the int mapped with the given key.
-   * 
-   * @param key
-   *            int who's mapped object we're interested in.
-   * @return an object mapped by the given key. null if the key wasn't found.
-   */
-  public int get(K key) {
-    return values[find(key)];
-  }
-
-  /**
-   * Grows the map. Allocates a new map of double the capacity, and
-   * fast-insert the old key-value pairs.
-   */
-  @SuppressWarnings("unchecked")
-  protected void grow() {
-    ObjectToIntMap<K> that = new ObjectToIntMap<K>(
-        this.capacity * 2);
-
-    // Iterates fast over the collection. Any valid pair is put into the new
-    // map without checking for duplicates or if there's enough space for
-    // it.
-    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
-      int index = iterator.next();
-      that.prvt_put((K) this.keys[index], this.values[index]);
-    }
-
-    // Copy that's data into this.
-    this.capacity = that.capacity;
-    this.size = that.size;
-    this.firstEmpty = that.firstEmpty;
-    this.values = that.values;
-    this.keys = that.keys;
-    this.next = that.next;
-    this.baseHash = that.baseHash;
-    this.hashFactor = that.hashFactor;
-  }
-
-  /**
-   * 
-   * @return true if the map is empty. false otherwise.
-   */
-  public boolean isEmpty() {
-    return size == 0;
-  }
-
-  /**
-   * Returns a new iterator for the mapped objects.
-   */
-  public IntIterator iterator() {
-    return new ValueIterator();
-  }
-
-  public Iterator<K> keyIterator() {
-    return new KeyIterator();
-  }
-
-  /**
-   * Prints the baseHash array, used for debug purposes.
-   */
-  @SuppressWarnings("unused")
-  private String getBaseHashAsString() {
-    return Arrays.toString(baseHash);
-  }
-
-  /**
-   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
-   * this method updates the mapped value to the given one, returning the old
-   * mapped value.
-   * 
-   * @return the old mapped value, or 0 if the key didn't exist.
-   */
-  public int put(K key, int e) {
-    // Does key exists?
-    int index = find(key);
-
-    // Yes!
-    if (index != 0) {
-      // Set new data and exit.
-      int old = values[index];
-      values[index] = e;
-      return old;
-    }
-
-    // Is there enough room for a new pair?
-    if (size == capacity) {
-      // No? Than grow up!
-      grow();
-    }
-
-    // Now that everything is set, the pair can be just put inside with no
-    // worries.
-    prvt_put(key, e);
-
-    return 0;
-  }
-
-  /**
-   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
-   * or 0 if the none existed.
-   * 
-   * @param key used to find the value to remove
-   * @return the removed value or 0 if none existed.
-   */
-  public int remove(K key) {
-    int baseHashIndex = calcBaseHashIndex(key);
-    int index = findForRemove(key, baseHashIndex);
-    if (index != 0) {
-      // If it is the first in the collision list, we should promote its
-      // next colliding element.
-      if (prev == 0) {
-        baseHash[baseHashIndex] = next[index];
-      }
-
-      next[prev] = next[index];
-      next[index] = firstEmpty;
-      firstEmpty = index;
-      --size;
-      return values[index];
-    }
-
-    return 0;
-  }
-
-  /**
-   * @return number of pairs currently in the map
-   */
-  public int size() {
-    return this.size;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of Objects
-   * 
-   * @return an object array of all the values currently in the map.
-   */
-  public int[] toArray() {
-    int j = -1;
-    int[] array = new int[size];
-
-    // Iterates over the values, adding them to the array.
-    for (IntIterator iterator = iterator(); iterator.hasNext();) {
-      array[++j] = iterator.next();
-    }
-    return array;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of T
-   * 
-   * @param a
-   *            the array into which the elements of the list are to be
-   *            stored, if it is big enough; otherwise, use as much space as it can.
-   * 
-   * @return an array containing the elements of the list
-   * 
-   */
-  public int[] toArray(int[] a) {
-    int j = 0;
-    // Iterates over the values, adding them to the array.
-    for (IntIterator iterator = iterator(); j < a.length
-    && iterator.hasNext(); ++j) {
-      a[j] = iterator.next();
-    }
-    if (j < a.length) {
-      a[j] = Integer.MAX_VALUE;
-    }
-
-    return a;
-  }
-
-  @Override
-  public String toString() {
-    StringBuffer sb = new StringBuffer();
-    sb.append('{');
-    Iterator<K> keyIterator = keyIterator();
-    while (keyIterator.hasNext()) {
-      K key = keyIterator.next();
-      sb.append(key);
-      sb.append('=');
-      sb.append(get(key));
-      if (keyIterator.hasNext()) {
-        sb.append(',');
-        sb.append(' ');
-      }
-    }
-    sb.append('}');
-    return sb.toString();
-  }
-  
-  @Override
-  public int hashCode() {
-    return getClass().hashCode() ^ size();
-  }
-  
-  @SuppressWarnings("unchecked")
-  @Override
-  public boolean equals(Object o) {
-    ObjectToIntMap<K> that = (ObjectToIntMap<K>)o;
-    if (that.size() != this.size()) {
-      return false;
-    }
-    
-    Iterator<K> it = keyIterator();
-    while (it.hasNext()) {
-      K key = it.next();
-      int v1 = this.get(key);
-      int v2 = that.get(key);
-      if (Float.compare(v1, v2) != 0) {
-        return false;
-      }
-    }
-    return true;
-  }
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/package.html simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/package.html
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/collections/package.html	2013-02-20 13:38:17.784711922 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/collections/package.html	1969-12-31 19:00:00.000000000 -0500
@@ -1,24 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Facets Collections</title>
-</head>
-<body>
-Various optimized Collections implementations.
-</body>
-</html>
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/complements/package.html simplefacets/lucene/facet/src/java/org/apache/lucene/facet/complements/package.html
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/complements/package.html	2013-02-20 13:38:17.668711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/complements/package.html	1969-12-31 19:00:00.000000000 -0500
@@ -1,27 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Facets Complements counting</title>
-</head>
-<body>
-Allows to cache the total counts of categories, so that during search which 
-returns a large number of results (>60% of segment size), the complement set 
-of matching documents is counted. Useful for queries that visit a large 
-number of documents, e.g. overview queries.
-</body>
-</html>
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/complements/TotalFacetCountsCache.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/complements/TotalFacetCountsCache.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/complements/TotalFacetCountsCache.java	2013-02-20 13:38:17.668711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/complements/TotalFacetCountsCache.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,299 +0,0 @@
-package org.apache.lucene.facet.complements;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.LinkedHashMap;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.ConcurrentLinkedQueue;
-
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.IndexReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Manage an LRU cache for {@link TotalFacetCounts} per index, taxonomy, and
- * facet indexing params.
- * 
- * @lucene.experimental
- */
-public final class TotalFacetCountsCache {
-  
-  /**
-   * Default size of in memory cache for computed total facet counts.
-   * Set to 2 for the case when an application reopened a reader and 
-   * the original one is still in use (Otherwise there will be 
-   * switching again and again between the two.) 
-   */
-  public static final int DEFAULT_CACHE_SIZE = 2; 
-
-  private static final TotalFacetCountsCache singleton = new TotalFacetCountsCache();
-  
-  /**
-   * Get the single instance of this cache
-   */
-  public static TotalFacetCountsCache getSingleton() {
-    return singleton;
-  }
-  
-  /**
-   * In-memory cache of TFCs.
-   * <ul>  
-   * <li>It's size is kept within limits through {@link #trimCache()}.
-   * <li>An LRU eviction policy is applied, by maintaining active keys in {@link #lruKeys}. 
-   * <li>After each addition to the cache, trimCache is called, to remove entries least recently used.
-   * </ul>  
-   * @see #markRecentlyUsed(TFCKey)
-   */
-  private ConcurrentHashMap<TFCKey,TotalFacetCounts> cache = new ConcurrentHashMap<TFCKey,TotalFacetCounts>();
-  
-  /**
-   * A queue of active keys for applying LRU policy on eviction from the {@link #cache}.
-   * @see #markRecentlyUsed(TFCKey)
-   */
-  private ConcurrentLinkedQueue<TFCKey> lruKeys = new ConcurrentLinkedQueue<TFCKey>();
-  
-  private int maxCacheSize = DEFAULT_CACHE_SIZE; 
-  
-  /** private constructor for singleton pattern */ 
-  private TotalFacetCountsCache() {
-  }
-  
-  /**
-   * Get the total facet counts for a reader/taxonomy pair and facet indexing
-   * parameters. If not in cache, computed here and added to the cache for later
-   * use.
-   * 
-   * @param indexReader
-   *          the documents index
-   * @param taxonomy
-   *          the taxonomy index
-   * @param facetIndexingParams
-   *          facet indexing parameters
-   * @return the total facet counts.
-   */
-  public TotalFacetCounts getTotalCounts(IndexReader indexReader, TaxonomyReader taxonomy,
-      FacetIndexingParams facetIndexingParams) throws IOException {
-    // create the key
-    TFCKey key = new TFCKey(indexReader, taxonomy, facetIndexingParams);
-    // it is important that this call is not synchronized, so that available TFC 
-    // would not wait for one that needs to be computed.  
-    TotalFacetCounts tfc = cache.get(key);
-    if (tfc != null) {
-      markRecentlyUsed(key); 
-      return tfc;
-    }
-    return computeAndCache(key);
-  }
-
-  /**
-   * Mark key as it as recently used.
-   * <p>
-   * <b>Implementation notes: Synchronization considerations and the interaction between lruKeys and cache:</b>
-   * <ol>
-   *  <li>A concurrent {@link LinkedHashMap} would have made this class much simpler.
-   *      But unfortunately, Java does not provide one.
-   *      Instead, we combine two concurrent objects:
-   *  <ul>
-   *   <li>{@link ConcurrentHashMap} for the cached TFCs.
-   *   <li>{@link ConcurrentLinkedQueue} for active keys
-   *  </ul>
-   *  <li>Both {@link #lruKeys} and {@link #cache} are concurrently safe.
-   *  <li>Checks for a cached item through getTotalCounts() are not synchronized.
-   *      Therefore, the case that a needed TFC is in the cache is very fast:
-   *      it does not wait for the computation of other TFCs.
-   *  <li>computeAndCache() is synchronized, and, has a (double) check of the required
-   *       TFC, to avoid computing the same TFC twice. 
-   *  <li>A race condition in this method (markRecentlyUsed) might result in two copies 
-   *      of the same 'key' in lruKeys, but this is handled by the loop in trimCache(), 
-   *      where an attempt to remove the same key twice is a no-op.
-   * </ol>
-   */
-  private void markRecentlyUsed(TFCKey key) {
-    lruKeys.remove(key);  
-    lruKeys.add(key);
-  }
-
-  private synchronized void trimCache() {
-    // loop until cache is of desired  size.
-    while (cache.size()>maxCacheSize ) { 
-      TFCKey key = lruKeys.poll();
-      if (key==null) { //defensive
-        // it is defensive since lruKeys presumably covers the cache keys 
-        key = cache.keys().nextElement(); 
-      }
-      // remove this element. Note that an attempt to remove with the same key again is a no-op,
-      // which gracefully handles the possible race in markRecentlyUsed(). 
-      cache.remove(key);
-    }
-  }
-  
-  /**
-   * compute TFC and cache it, after verifying it was not just added - for this
-   * matter this method is synchronized, which is not too bad, because there is
-   * lots of work done in the computations.
-   */
-  private synchronized TotalFacetCounts computeAndCache(TFCKey key) throws IOException {
-    TotalFacetCounts tfc = cache.get(key); 
-    if (tfc == null) {
-      tfc = TotalFacetCounts.compute(key.indexReader, key.taxonomy, key.facetIndexingParams);
-      lruKeys.add(key);
-      cache.put(key,tfc);
-      trimCache();
-    }
-    return tfc;
-  }
-
-  /**
-   * Load {@link TotalFacetCounts} matching input parameters from the provided
-   * outputFile and add them into the cache for the provided indexReader,
-   * taxonomy, and facetIndexingParams. If a {@link TotalFacetCounts} for these
-   * parameters already exists in the cache, it will be replaced by the loaded
-   * one.
-   * 
-   * @param inputFile
-   *          file from which to read the data
-   * @param indexReader
-   *          the documents index
-   * @param taxonomy
-   *          the taxonomy index
-   * @param facetIndexingParams
-   *          the facet indexing parameters
-   * @throws IOException
-   *           on error
-   */
-  public synchronized void load(File inputFile, IndexReader indexReader, TaxonomyReader taxonomy,
-      FacetIndexingParams facetIndexingParams) throws IOException {
-    if (!inputFile.isFile() || !inputFile.exists() || !inputFile.canRead()) {
-      throw new IllegalArgumentException("Exepecting an existing readable file: "+inputFile);
-    }
-    TFCKey key = new TFCKey(indexReader, taxonomy, facetIndexingParams);
-    TotalFacetCounts tfc = TotalFacetCounts.loadFromFile(inputFile, taxonomy, facetIndexingParams);
-    cache.put(key,tfc);
-    trimCache();
-    markRecentlyUsed(key);
-  }
-  
-  /**
-   * Store the {@link TotalFacetCounts} matching input parameters into the
-   * provided outputFile, making them available for a later call to
-   * {@link #load(File, IndexReader, TaxonomyReader, FacetIndexingParams)}. If
-   * these {@link TotalFacetCounts} are available in the cache, they are used.
-   * But if they are not in the cache, this call will first compute them (which
-   * will also add them to the cache).
-   * 
-   * @param outputFile
-   *          file to store in.
-   * @param indexReader
-   *          the documents index
-   * @param taxonomy
-   *          the taxonomy index
-   * @param facetIndexingParams
-   *          the facet indexing parameters
-   * @throws IOException
-   *           on error
-   * @see #load(File, IndexReader, TaxonomyReader, FacetIndexingParams)
-   */
-  public void store(File outputFile, IndexReader indexReader, TaxonomyReader taxonomy,
-      FacetIndexingParams facetIndexingParams) throws IOException {
-    File parentFile = outputFile.getParentFile();
-    if (
-        ( outputFile.exists() && (!outputFile.isFile()      || !outputFile.canWrite())) ||
-        (!outputFile.exists() && (!parentFile.isDirectory() || !parentFile.canWrite()))
-      ) {
-      throw new IllegalArgumentException("Exepecting a writable file: "+outputFile);
-    }
-    TotalFacetCounts tfc = getTotalCounts(indexReader, taxonomy, facetIndexingParams);
-    TotalFacetCounts.storeToFile(outputFile, tfc);  
-  }
-  
-  private static class TFCKey {
-    final IndexReader indexReader;
-    final TaxonomyReader taxonomy;
-    private final Iterable<CategoryListParams> clps;
-    private final int hashCode;
-    private final int nDels; // needed when a reader used for faceted search was just used for deletion. 
-    final FacetIndexingParams facetIndexingParams;
-    
-    public TFCKey(IndexReader indexReader, TaxonomyReader taxonomy,
-        FacetIndexingParams facetIndexingParams) {
-      this.indexReader = indexReader;
-      this.taxonomy = taxonomy;
-      this.facetIndexingParams = facetIndexingParams;
-      this.clps = facetIndexingParams.getAllCategoryListParams();
-      this.nDels = indexReader.numDeletedDocs();
-      hashCode = indexReader.hashCode() ^ taxonomy.hashCode();
-    }
-    
-    @Override
-    public int hashCode() {
-      return hashCode;
-    }
-    
-    @Override
-    public boolean equals(Object other) {
-      TFCKey o = (TFCKey) other; 
-      if (indexReader != o.indexReader || taxonomy != o.taxonomy || nDels != o.nDels) {
-        return false;
-      }
-      Iterator<CategoryListParams> it1 = clps.iterator();
-      Iterator<CategoryListParams> it2 = o.clps.iterator();
-      while (it1.hasNext() && it2.hasNext()) {
-        if (!it1.next().equals(it2.next())) {
-          return false;
-        }
-      }
-      return it1.hasNext() == it2.hasNext();
-    }
-  }
-
-  /**
-   * Clear the cache.
-   */
-  public synchronized void clear() {
-    cache.clear();
-    lruKeys.clear();
-  }
-  
-  /**
-   * @return the maximal cache size
-   */
-  public int getCacheSize() {
-    return maxCacheSize;
-  }
-
-  /**
-   * Set the number of TotalFacetCounts arrays that will remain in memory cache.
-   * <p>
-   * If new size is smaller than current size, the cache is appropriately trimmed.
-   * <p>
-   * Minimal size is 1, so passing zero or negative size would result in size of 1.
-   * @param size new size to set
-   */
-  public void setCacheSize(int size) {
-    if (size < 1) size = 1;
-    int origSize = maxCacheSize;
-    maxCacheSize = size;
-    if (maxCacheSize < origSize) { // need to trim only if the cache was reduced
-      trimCache();
-    }
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/complements/TotalFacetCounts.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/complements/TotalFacetCounts.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/complements/TotalFacetCounts.java	2013-07-29 13:55:02.713707540 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/complements/TotalFacetCounts.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,180 +0,0 @@
-package org.apache.lucene.facet.complements;
-
-import java.io.BufferedInputStream;
-import java.io.BufferedOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.concurrent.atomic.AtomicInteger;
-
-import org.apache.lucene.facet.old.Aggregator;
-import org.apache.lucene.facet.old.CountingAggregator;
-import org.apache.lucene.facet.old.OldFacetsAccumulator;
-import org.apache.lucene.facet.old.ScoredDocIdsUtils;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.CategoryListIterator;
-import org.apache.lucene.facet.search.CountFacetRequest;
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.util.PartitionsUtils;
-import org.apache.lucene.index.IndexReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Maintain Total Facet Counts per partition, for given parameters:
- * <ul> 
- *  <li>Index reader of an index</li>
- *  <li>Taxonomy index reader</li>
- *  <li>Facet indexing params (and particularly the category list params)</li>
- *  <li></li>
- * </ul>
- * The total facet counts are maintained as an array of arrays of integers, 
- * where a separate array is kept for each partition.
- * 
- * @lucene.experimental
- */
-public class TotalFacetCounts {
-  
-  /** total facet counts per partition: totalCounts[partition][ordinal%partitionLength] */
-  private int[][] totalCounts = null;
-  
-  private final TaxonomyReader taxonomy;
-  private final FacetIndexingParams facetIndexingParams;
-
-  private final static AtomicInteger atomicGen4Test = new AtomicInteger(1);
-  /** Creation type for test purposes */
-  enum CreationType { Computed, Loaded } // for testing
-  final int gen4test;
-  final CreationType createType4test;
-  
-  /** 
-   * Construct by key - from index Directory or by recomputing.
-   */
-  private TotalFacetCounts (TaxonomyReader taxonomy, FacetIndexingParams facetIndexingParams,
-      int[][] counts, CreationType createType4Test) {
-    this.taxonomy = taxonomy;
-    this.facetIndexingParams = facetIndexingParams;
-    this.totalCounts = counts;
-    this.createType4test = createType4Test;
-    this.gen4test = atomicGen4Test.incrementAndGet();
-  }
-
-  /**
-   * Fill a partition's array with the TotalCountsArray values.
-   * @param partitionArray array to fill
-   * @param partition number of required partition 
-   */
-  public void fillTotalCountsForPartition(int[] partitionArray, int partition) {
-    int partitionSize = partitionArray.length;
-    int[] countArray = totalCounts[partition];
-    if (countArray == null) {
-      countArray = new int[partitionSize];
-      totalCounts[partition] = countArray;
-    }
-    int length = Math.min(partitionSize, countArray.length);
-    System.arraycopy(countArray, 0, partitionArray, 0, length);
-  }
-  
-  /**
-   * Return the total count of an input category
-   * @param ordinal ordinal of category whose total count is required 
-   */
-  public int getTotalCount(int ordinal) {
-    int partition = PartitionsUtils.partitionNumber(facetIndexingParams,ordinal);
-    int offset = ordinal % PartitionsUtils.partitionSize(facetIndexingParams, taxonomy);
-    return totalCounts[partition][offset];
-  }
-  
-  static TotalFacetCounts loadFromFile(File inputFile, TaxonomyReader taxonomy, 
-      FacetIndexingParams facetIndexingParams) throws IOException {
-    DataInputStream dis = new DataInputStream(new BufferedInputStream(new FileInputStream(inputFile)));
-    try {
-      int[][] counts = new int[dis.readInt()][];
-      for (int i=0; i<counts.length; i++) {
-        int size = dis.readInt();
-        if (size<0) {
-          counts[i] = null;
-        } else {
-          counts[i] = new int[size];
-          for (int j=0; j<size; j++) {
-            counts[i][j] = dis.readInt();
-          }
-        }
-      }
-      return new TotalFacetCounts(taxonomy, facetIndexingParams, counts, CreationType.Loaded);
-    } finally {
-      dis.close();
-    }
-  }
-
-  static void storeToFile(File outputFile, TotalFacetCounts tfc) throws IOException {
-    DataOutputStream dos = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(outputFile)));
-    try {
-      dos.writeInt(tfc.totalCounts.length);
-      for (int[] counts : tfc.totalCounts) {
-        if (counts == null) {
-          dos.writeInt(-1);
-        } else {
-          dos.writeInt(counts.length);
-          for (int i : counts) {
-            dos.writeInt(i);
-          }
-        }
-      }
-    } finally {
-      dos.close();
-    }
-  }
-  
-  // needed because FacetSearchParams do not allow empty FacetRequests
-  private static final FacetRequest DUMMY_REQ = new CountFacetRequest(CategoryPath.EMPTY, 1);
-
-  static TotalFacetCounts compute(final IndexReader indexReader, final TaxonomyReader taxonomy, 
-      final FacetIndexingParams facetIndexingParams) throws IOException {
-    int partitionSize = PartitionsUtils.partitionSize(facetIndexingParams, taxonomy);
-    final int[][] counts = new int[(int) Math.ceil(taxonomy.getSize()  /(float) partitionSize)][partitionSize];
-    FacetSearchParams newSearchParams = new FacetSearchParams(facetIndexingParams, DUMMY_REQ); 
-      //createAllListsSearchParams(facetIndexingParams,  this.totalCounts);
-    OldFacetsAccumulator sfa = new OldFacetsAccumulator(newSearchParams, indexReader, taxonomy) {
-      @Override
-      protected HashMap<CategoryListIterator, Aggregator> getCategoryListMap(
-          FacetArrays facetArrays, int partition) throws IOException {
-        
-        Aggregator aggregator = new CountingAggregator(counts[partition]);
-        HashMap<CategoryListIterator, Aggregator> map = new HashMap<CategoryListIterator, Aggregator>();
-        for (CategoryListParams clp: facetIndexingParams.getAllCategoryListParams()) {
-          map.put(clp.createCategoryListIterator(partition), aggregator);
-        }
-        return map;
-      }
-    };
-    sfa.setComplementThreshold(OldFacetsAccumulator.DISABLE_COMPLEMENT);
-    sfa.accumulate(ScoredDocIdsUtils.createAllDocsScoredDocIDs(indexReader));
-    return new TotalFacetCounts(taxonomy, facetIndexingParams, counts, CreationType.Computed);
-  }
-  
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/DocValuesOrdinalsReader.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/DocValuesOrdinalsReader.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/DocValuesOrdinalsReader.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/DocValuesOrdinalsReader.java	2013-11-26 10:43:49.639039183 -0500
@@ -0,0 +1,97 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/** Decodes ordinals previously indexed into a BinaryDocValues field */
+
+public class DocValuesOrdinalsReader extends OrdinalsReader {
+  private final String field;
+
+  public DocValuesOrdinalsReader() {
+    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME);
+  }
+
+  public DocValuesOrdinalsReader(String field) {
+    this.field = field;
+  }
+
+  @Override
+  public OrdinalsSegmentReader getReader(AtomicReaderContext context) throws IOException {
+    BinaryDocValues values0 = context.reader().getBinaryDocValues(field);
+    if (values0 == null) {
+      values0 = BinaryDocValues.EMPTY;
+    }
+
+    final BinaryDocValues values = values0;
+
+    return new OrdinalsSegmentReader() {
+      private final BytesRef bytes = new BytesRef(32);
+
+      @Override
+      public void get(int docID, IntsRef ordinals) throws IOException {
+        values.get(docID, bytes);
+        decode(bytes, ordinals);
+      }
+    };
+  }
+
+  @Override
+  public String getIndexFieldName() {
+    return field;
+  }
+
+  /** Subclass & override if you change the encoding. */
+  protected void decode(BytesRef buf, IntsRef ordinals) {
+
+    // grow the buffer up front, even if by a large number of values (buf.length)
+    // that saves the need to check inside the loop for every decoded value if
+    // the buffer needs to grow.
+    if (ordinals.ints.length < buf.length) {
+      ordinals.ints = ArrayUtil.grow(ordinals.ints, buf.length);
+    }
+
+    ordinals.offset = 0;
+    ordinals.length = 0;
+
+    // it is better if the decoding is inlined like so, and not e.g.
+    // in a utility method
+    int upto = buf.offset + buf.length;
+    int value = 0;
+    int offset = buf.offset;
+    int prev = 0;
+    while (offset < upto) {
+      byte b = buf.bytes[offset++];
+      if (b >= 0) {
+        ordinals.ints[ordinals.length] = ((value << 7) | b) + prev;
+        value = 0;
+        prev = ordinals.ints[ordinals.length];
+        ordinals.length++;
+      } else {
+        value = (value << 7) | (b & 0x7F);
+      }
+    }
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/DoubleRangeFacetCounts.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/DoubleRangeFacetCounts.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/DoubleRangeFacetCounts.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/DoubleRangeFacetCounts.java	2013-12-18 15:37:52.749979215 -0500
@@ -0,0 +1,107 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.lucene.document.DoubleDocValuesField; // javadocs
+import org.apache.lucene.document.FloatDocValuesField; // javadocs
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.valuesource.DoubleFieldSource;
+import org.apache.lucene.queries.function.valuesource.FloatFieldSource; // javadocs
+import org.apache.lucene.util.NumericUtils;
+
+/** {@link Facets} implementation that computes counts for
+ *  dynamic double ranges from a provided {@link
+ *  ValueSource}, using {@link FunctionValues#doubleVal}.  Use
+ *  this for dimensions that change in real-time (e.g. a
+ *  relative time based dimension like "Past day", "Past 2
+ *  days", etc.) or that change for each request (e.g.
+ *  distance from the user's location, "< 1 km", "< 2 km",
+ *  etc.).
+ *
+ *  <p> If you had indexed your field using {@link
+ *  FloatDocValuesField} then pass {@link FloatFieldSource}
+ *  as the {@link ValueSource}; if you used {@link
+ *  DoubleDocValuesField} then pass {@link
+ *  DoubleFieldSource} (this is the default used when you
+ *  pass just a the field name).
+ *
+ *  @lucene.experimental */
+public class DoubleRangeFacetCounts extends RangeFacetCounts {
+
+  /** Create {@code RangeFacetCounts}, using {@link
+   *  DoubleFieldSource} from the specified field. */
+  public DoubleRangeFacetCounts(String field, FacetsCollector hits, DoubleRange... ranges) throws IOException {
+    this(field, new DoubleFieldSource(field), hits, ranges);
+  }
+
+  /** Create {@code RangeFacetCounts}, using the provided
+   *  {@link ValueSource}. */
+  public DoubleRangeFacetCounts(String field, ValueSource valueSource, FacetsCollector hits, DoubleRange... ranges) throws IOException {
+    super(field, ranges);
+    count(valueSource, hits.getMatchingDocs());
+  }
+
+  private void count(ValueSource valueSource, List<MatchingDocs> matchingDocs) throws IOException {
+
+    DoubleRange[] ranges = (DoubleRange[]) this.ranges;
+
+    LongRange[] longRanges = new LongRange[ranges.length];
+    for(int i=0;i<ranges.length;i++) {
+      DoubleRange range = ranges[i];
+      longRanges[i] =  new LongRange(range.label,
+                                     NumericUtils.doubleToSortableLong(range.minIncl), true,
+                                     NumericUtils.doubleToSortableLong(range.maxIncl), true);
+    }
+
+    LongRangeCounter counter = new LongRangeCounter(longRanges);
+
+    // Compute min & max over all ranges:
+    double minIncl = Double.POSITIVE_INFINITY;
+    double maxIncl = Double.NEGATIVE_INFINITY;
+    for(DoubleRange range : ranges) {
+      minIncl = Math.min(minIncl, range.minIncl);
+      maxIncl = Math.max(maxIncl, range.maxIncl);
+    }
+
+    int missingCount = 0;
+    for (MatchingDocs hits : matchingDocs) {
+      FunctionValues fv = valueSource.getValues(Collections.emptyMap(), hits.context);
+      final int length = hits.bits.length();
+      int doc = 0;
+      totCount += hits.totalHits;
+      while (doc < length && (doc = hits.bits.nextSetBit(doc)) != -1) {
+        // Skip missing docs:
+        if (fv.exists(doc)) {
+          counter.add(NumericUtils.doubleToSortableLong(fv.doubleVal(doc)));
+        } else {
+          missingCount++;
+        }
+        doc++;
+      }
+    }
+
+    missingCount += counter.fillCounts(counts);
+    totCount -= missingCount;
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/DoubleRange.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/DoubleRange.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/DoubleRange.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/DoubleRange.java	2013-12-19 13:25:50.219879863 -0500
@@ -0,0 +1,163 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+
+import org.apache.lucene.document.DoubleDocValuesField; // javadocs
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.NumericRangeFilter; // javadocs
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.NumericUtils;
+
+/** Represents a range over double values. */
+public final class DoubleRange extends Range {
+  final double minIncl;
+  final double maxIncl;
+
+  public final double min;
+  public final double max;
+  public final boolean minInclusive;
+  public final boolean maxInclusive;
+
+  /** Create a DoubleRange. */
+  public DoubleRange(String label, double minIn, boolean minInclusive, double maxIn, boolean maxInclusive) {
+    super(label);
+    this.min = minIn;
+    this.max = maxIn;
+    this.minInclusive = minInclusive;
+    this.maxInclusive = maxInclusive;
+
+    // TODO: if DoubleDocValuesField used
+    // NumericUtils.doubleToSortableLong format (instead of
+    // Double.doubleToRawLongBits) we could do comparisons
+    // in long space 
+
+    if (Double.isNaN(min)) {
+      throw new IllegalArgumentException("min cannot be NaN");
+    }
+    if (!minInclusive) {
+      minIn = Math.nextUp(minIn);
+    }
+
+    if (Double.isNaN(max)) {
+      throw new IllegalArgumentException("max cannot be NaN");
+    }
+    if (!maxInclusive) {
+      // Why no Math.nextDown?
+      maxIn = Math.nextAfter(maxIn, Double.NEGATIVE_INFINITY);
+    }
+
+    if (minIn > maxIn) {
+      failNoMatch();
+    }
+
+    this.minIncl = minIn;
+    this.maxIncl = maxIn;
+  }
+
+  public boolean accept(double value) {
+    return value >= minIncl && value <= maxIncl;
+  }
+
+  LongRange toLongRange() {
+    return new LongRange(label,
+                         NumericUtils.doubleToSortableLong(minIncl), true,
+                         NumericUtils.doubleToSortableLong(maxIncl), true);
+  }
+
+  @Override
+  public String toString() {
+    return "DoubleRange(" + minIncl + " to " + maxIncl + ")";
+  }
+
+  /** Returns a new {@link Filter} accepting only documents
+   *  in this range.  Note that this filter is not
+   *  efficient: it's a linear scan of all docs, testing
+   *  each value.  If the {@link ValueSource} is static,
+   *  e.g. an indexed numeric field, then it's more
+   *  efficient to use {@link NumericRangeFilter}. */
+  public Filter getFilter(final ValueSource valueSource) {
+    return new Filter() {
+      @Override
+      public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
+
+        // TODO: this is just like ValueSourceScorer,
+        // ValueSourceFilter (spatial),
+        // ValueSourceRangeFilter (solr); also,
+        // https://issues.apache.org/jira/browse/LUCENE-4251
+
+        final FunctionValues values = valueSource.getValues(Collections.emptyMap(), context);
+
+        final int maxDoc = context.reader().maxDoc();
+
+        return new DocIdSet() {
+
+          @Override
+          public DocIdSetIterator iterator() {
+            return new DocIdSetIterator() {
+              int doc = -1;
+
+              @Override
+              public int nextDoc() throws IOException {
+                while (true) {
+                  doc++;
+                  if (doc == maxDoc) {
+                    return doc = NO_MORE_DOCS;
+                  }
+                  if (acceptDocs != null && acceptDocs.get(doc) == false) {
+                    continue;
+                  }
+                  double v = values.doubleVal(doc);
+                  if (accept(v)) {
+                    return doc;
+                  }
+                }
+              }
+
+              @Override
+              public int advance(int target) throws IOException {
+                doc = target-1;
+                return nextDoc();
+              }
+
+              @Override
+              public int docID() {
+                return doc;
+              }
+
+              @Override
+              public long cost() {
+                // Since we do a linear scan over all
+                // documents, our cost is O(maxDoc):
+                return maxDoc;
+              }
+            };
+          }
+        };
+      }
+    };
+  }
+}
+


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/DrillDownQuery.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/DrillDownQuery.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/DrillDownQuery.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/DrillDownQuery.java	2013-11-30 20:05:59.303152079 -0500
@@ -0,0 +1,217 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.FilteredQuery;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.NumericRangeQuery; // javadocs
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+
+/**
+ * A {@link Query} for drill-down over facet categories. You
+ * should call {@link #add(String, String...)} for every group of categories you
+ * want to drill-down over.
+ * <p>
+ * <b>NOTE:</b> if you choose to create your own {@link Query} by calling
+ * {@link #term}, it is recommended to wrap it with {@link ConstantScoreQuery}
+ * and set the {@link ConstantScoreQuery#setBoost(float) boost} to {@code 0.0f},
+ * so that it does not affect the scores of the documents.
+ * 
+ * @lucene.experimental
+ */
+public final class DrillDownQuery extends Query {
+
+  /** Creates a drill-down term. */
+  public static Term term(String field, String dim, String... path) {
+    return new Term(field, FacetsConfig.pathToString(dim, path));
+  }
+
+  private final FacetsConfig config;
+  private final BooleanQuery query;
+  private final Map<String,Integer> drillDownDims = new LinkedHashMap<String,Integer>();
+
+  /** Used by clone() */
+  DrillDownQuery(FacetsConfig config, BooleanQuery query, Map<String,Integer> drillDownDims) {
+    this.query = query.clone();
+    this.drillDownDims.putAll(drillDownDims);
+    this.config = config;
+  }
+
+  /** Used by DrillSideways */
+  DrillDownQuery(FacetsConfig config, Filter filter, DrillDownQuery other) {
+    query = new BooleanQuery(true); // disable coord
+
+    BooleanClause[] clauses = other.query.getClauses();
+    if (clauses.length == other.drillDownDims.size()) {
+      throw new IllegalArgumentException("cannot apply filter unless baseQuery isn't null; pass ConstantScoreQuery instead");
+    }
+    assert clauses.length == 1+other.drillDownDims.size(): clauses.length + " vs " + (1+other.drillDownDims.size());
+    drillDownDims.putAll(other.drillDownDims);
+    query.add(new FilteredQuery(clauses[0].getQuery(), filter), Occur.MUST);
+    for(int i=1;i<clauses.length;i++) {
+      query.add(clauses[i].getQuery(), Occur.MUST);
+    }
+    this.config = config;
+  }
+
+  /** Used by DrillSideways */
+  DrillDownQuery(FacetsConfig config, Query baseQuery, List<Query> clauses, Map<String,Integer> drillDownDims) {
+    this.query = new BooleanQuery(true);
+    if (baseQuery != null) {
+      query.add(baseQuery, Occur.MUST);      
+    }
+    for(Query clause : clauses) {
+      query.add(clause, Occur.MUST);
+    }
+    this.drillDownDims.putAll(drillDownDims);
+    this.config = config;
+  }
+
+  /** Creates a new {@code DrillDownQuery} without a base query, 
+   *  to perform a pure browsing query (equivalent to using
+   *  {@link MatchAllDocsQuery} as base). */
+  public DrillDownQuery(FacetsConfig config) {
+    this(config, null);
+  }
+  
+  /** Creates a new {@code DrillDownQuery} over the given base query. Can be
+   *  {@code null}, in which case the result {@link Query} from
+   *  {@link #rewrite(IndexReader)} will be a pure browsing query, filtering on
+   *  the added categories only. */
+  public DrillDownQuery(FacetsConfig config, Query baseQuery) {
+    query = new BooleanQuery(true); // disable coord
+    if (baseQuery != null) {
+      query.add(baseQuery, Occur.MUST);
+    }
+    this.config = config;
+  }
+
+  /** Merges (ORs) a new path into an existing AND'd
+   *  clause. */ 
+  private void merge(String dim, String[] path) {
+    int index = drillDownDims.get(dim);
+    if (query.getClauses().length == drillDownDims.size()+1) {
+      index++;
+    }
+    ConstantScoreQuery q = (ConstantScoreQuery) query.clauses().get(index).getQuery();
+    if ((q.getQuery() instanceof BooleanQuery) == false) {
+      // App called .add(dim, customQuery) and then tried to
+      // merge a facet label in:
+      throw new RuntimeException("cannot merge with custom Query");
+    }
+    String indexedField = config.getDimConfig(dim).indexFieldName;
+
+    BooleanQuery bq = (BooleanQuery) q.getQuery();
+    bq.add(new TermQuery(term(indexedField, dim, path)), Occur.SHOULD);
+  }
+
+  /** Adds one dimension of drill downs; if you pass the same
+   *  dimension more than once it is OR'd with the previous
+   *  cofnstraints on that dimension, and all dimensions are
+   *  AND'd against each other and the base query. */
+  public void add(String dim, String... path) {
+
+    if (drillDownDims.containsKey(dim)) {
+      merge(dim, path);
+      return;
+    }
+    String indexedField = config.getDimConfig(dim).indexFieldName;
+
+    BooleanQuery bq = new BooleanQuery(true); // disable coord
+    bq.add(new TermQuery(term(indexedField, dim, path)), Occur.SHOULD);
+
+    add(dim, bq);
+  }
+
+  /** Expert: add a custom drill-down subQuery.  Use this
+   *  when you have a separate way to drill-down on the
+   *  dimension than the indexed facet ordinals (for
+   *  example, use a {@link NumericRangeQuery} to drill down
+   *  after {@link LongRangeFacetCounts} or {@link DoubleRangeFacetCounts}. */
+  public void add(String dim, Query subQuery) {
+
+    // TODO: we should use FilteredQuery?
+
+    // So scores of the drill-down query don't have an
+    // effect:
+    final ConstantScoreQuery drillDownQuery = new ConstantScoreQuery(subQuery);
+    drillDownQuery.setBoost(0.0f);
+
+    query.add(drillDownQuery, Occur.MUST);
+
+    drillDownDims.put(dim, drillDownDims.size());
+  }
+
+  @Override
+  public DrillDownQuery clone() {
+    return new DrillDownQuery(config, query, drillDownDims);
+  }
+  
+  @Override
+  public int hashCode() {
+    final int prime = 31;
+    int result = super.hashCode();
+    return prime * result + query.hashCode();
+  }
+  
+  @Override
+  public boolean equals(Object obj) {
+    if (!(obj instanceof DrillDownQuery)) {
+      return false;
+    }
+    
+    DrillDownQuery other = (DrillDownQuery) obj;
+    return query.equals(other.query) && super.equals(other);
+  }
+  
+  @Override
+  public Query rewrite(IndexReader r) throws IOException {
+    if (query.clauses().size() == 0) {
+      return new MatchAllDocsQuery();
+    }
+    return query;
+  }
+
+  @Override
+  public String toString(String field) {
+    return query.toString(field);
+  }
+
+  BooleanQuery getBooleanQuery() {
+    return query;
+  }
+
+  Map<String,Integer> getDims() {
+    return drillDownDims;
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysCollector.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysCollector.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysCollector.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysCollector.java	2013-11-26 10:44:48.887037519 -0500
@@ -0,0 +1,188 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.IdentityHashMap;
+import java.util.Map;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.Scorer.ChildScorer;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Weight;
+
+/** Collector that scrutinizes each hit to determine if it
+ *  passed all constraints (a true hit) or if it missed
+ *  exactly one dimension (a near-miss, to count for
+ *  drill-sideways counts on that dimension). */
+class DrillSidewaysCollector extends Collector {
+
+  private final Collector hitCollector;
+  private final Collector drillDownCollector;
+  private final Collector[] drillSidewaysCollectors;
+  private final Scorer[] subScorers;
+  private final int exactCount;
+
+  // Maps Weight to either -1 (mainQuery) or to integer
+  // index of the dims drillDown.  We needs this when
+  // visiting the child scorers to correlate back to the
+  // right scorers:
+  private final Map<Weight,Integer> weightToIndex = new IdentityHashMap<Weight,Integer>();
+
+  private Scorer mainScorer;
+
+  public DrillSidewaysCollector(Collector hitCollector, Collector drillDownCollector, Collector[] drillSidewaysCollectors,
+                                      Map<String,Integer> dims) {
+    this.hitCollector = hitCollector;
+    this.drillDownCollector = drillDownCollector;
+    this.drillSidewaysCollectors = drillSidewaysCollectors;
+    subScorers = new Scorer[dims.size()];
+
+    if (dims.size() == 1) {
+      // When we have only one dim, we insert the
+      // MatchAllDocsQuery, bringing the clause count to
+      // 2:
+      exactCount = 2;
+    } else {
+      exactCount = dims.size();
+    }
+  }
+
+  @Override
+  public void collect(int doc) throws IOException {
+    //System.out.println("collect doc=" + doc + " main.freq=" + mainScorer.freq() + " main.doc=" + mainScorer.docID() + " exactCount=" + exactCount);
+      
+    if (mainScorer == null) {
+      // This segment did not have any docs with any
+      // drill-down field & value:
+      return;
+    }
+
+    if (mainScorer.freq() == exactCount) {
+      // All sub-clauses from the drill-down filters
+      // matched, so this is a "real" hit, so we first
+      // collect in both the hitCollector and the
+      // drillDown collector:
+      //System.out.println("  hit " + drillDownCollector);
+      hitCollector.collect(doc);
+      if (drillDownCollector != null) {
+        drillDownCollector.collect(doc);
+      }
+
+      // Also collect across all drill-sideways counts so
+      // we "merge in" drill-down counts for this
+      // dimension.
+      for(int i=0;i<subScorers.length;i++) {
+        // This cannot be null, because it was a hit,
+        // meaning all drill-down dims matched, so all
+        // dims must have non-null scorers:
+        assert subScorers[i] != null;
+        int subDoc = subScorers[i].docID();
+        assert subDoc == doc;
+        drillSidewaysCollectors[i].collect(doc);
+      }
+
+    } else {
+      boolean found = false;
+      for(int i=0;i<subScorers.length;i++) {
+        if (subScorers[i] == null) {
+          // This segment did not have any docs with this
+          // drill-down field & value:
+          drillSidewaysCollectors[i].collect(doc);
+          assert allMatchesFrom(i+1, doc);
+          found = true;
+          break;
+        }
+        int subDoc = subScorers[i].docID();
+        //System.out.println("  i=" + i + " sub: " + subDoc);
+        if (subDoc != doc) {
+          //System.out.println("  +ds[" + i + "]");
+          assert subDoc > doc: "subDoc=" + subDoc + " doc=" + doc;
+          drillSidewaysCollectors[i].collect(doc);
+          assert allMatchesFrom(i+1, doc);
+          found = true;
+          break;
+        }
+      }
+      assert found;
+    }
+  }
+
+  // Only used by assert:
+  private boolean allMatchesFrom(int startFrom, int doc) {
+    for(int i=startFrom;i<subScorers.length;i++) {
+      assert subScorers[i].docID() == doc;
+    }
+    return true;
+  }
+
+  @Override
+  public boolean acceptsDocsOutOfOrder() {
+    // We actually could accept docs out of order, but, we
+    // need to force BooleanScorer2 so that the
+    // sub-scorers are "on" each docID we are collecting:
+    return false;
+  }
+
+  @Override
+  public void setNextReader(AtomicReaderContext leaf) throws IOException {
+    //System.out.println("DS.setNextReader reader=" + leaf.reader());
+    hitCollector.setNextReader(leaf);
+    if (drillDownCollector != null) {
+      drillDownCollector.setNextReader(leaf);
+    }
+    for(Collector dsc : drillSidewaysCollectors) {
+      dsc.setNextReader(leaf);
+    }
+  }
+
+  void setWeight(Weight weight, int index) {
+    assert !weightToIndex.containsKey(weight);
+    weightToIndex.put(weight, index);
+  }
+
+  private void findScorers(Scorer scorer) {
+    Integer index = weightToIndex.get(scorer.getWeight());
+    if (index != null) {
+      if (index.intValue() == -1) {
+        mainScorer = scorer;
+      } else {
+        subScorers[index] = scorer;
+      }
+    }
+    for(ChildScorer child : scorer.getChildren()) {
+      findScorers(child.child);
+    }
+  }
+
+  @Override
+  public void setScorer(Scorer scorer) throws IOException {
+    mainScorer = null;
+    Arrays.fill(subScorers, null);
+    findScorers(scorer);
+    hitCollector.setScorer(scorer);
+    if (drillDownCollector != null) {
+      drillDownCollector.setScorer(scorer);
+    }
+    for(Collector dsc : drillSidewaysCollectors) {
+      dsc.setScorer(scorer);
+    }
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/DrillSideways.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/DrillSideways.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/DrillSideways.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/DrillSideways.java	2013-11-27 18:51:15.311945288 -0500
@@ -0,0 +1,439 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.FieldDoc;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.MultiCollector;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.TopFieldCollector;
+import org.apache.lucene.search.TopScoreDocCollector;
+import org.apache.lucene.search.Weight;
+
+/**     
+ * Computes drill down and sideways counts for the provided
+ * {@link DrillDownQuery}.  Drill sideways counts include
+ * alternative values/aggregates for the drill-down
+ * dimensions so that a dimension does not disappear after
+ * the user drills down into it.
+ *
+ * <p> Use one of the static search
+ * methods to do the search, and then get the hits and facet
+ * results from the returned {@link DrillSidewaysResult}.
+ *
+ * <p><b>NOTE</b>: this allocates one {@link
+ * FacetsCollector} for each drill-down, plus one.  If your
+ * index has high number of facet labels then this will
+ * multiply your memory usage.
+ *
+ * @lucene.experimental
+ */
+public class DrillSideways {
+
+  protected final IndexSearcher searcher;
+  protected final TaxonomyReader taxoReader;
+  protected final SortedSetDocValuesReaderState state;
+  protected final FacetsConfig config;
+
+  /** Create a new {@code DrillSideways} instance. */
+  public DrillSideways(IndexSearcher searcher, FacetsConfig config, TaxonomyReader taxoReader) {
+    this(searcher, config, taxoReader, null);
+  }
+    
+  /** Create a new {@code DrillSideways} instance, assuming the categories were
+   *  indexed with {@link SortedSetDocValuesFacetField}. */
+  public DrillSideways(IndexSearcher searcher, FacetsConfig config, SortedSetDocValuesReaderState state) {
+    this(searcher, config, null, state);
+  }
+
+  /** Create a new {@code DrillSideways} instance, where some
+   *  dimensions were indexed with {@link
+   *  SortedSetDocValuesFacetField} and others were indexed
+   *  with {@link FacetField}. */
+  public DrillSideways(IndexSearcher searcher, FacetsConfig config, TaxonomyReader taxoReader, SortedSetDocValuesReaderState state) {
+    this.searcher = searcher;
+    this.config = config;
+    this.taxoReader = taxoReader;
+    this.state = state;
+  }
+
+  /** Subclass can override to customize per-dim Facets
+   *  impl. */
+  protected Facets buildFacetsResult(FacetsCollector drillDowns, FacetsCollector[] drillSideways, String[] drillSidewaysDims) throws IOException {
+
+    Facets drillDownFacets;
+    Map<String,Facets> drillSidewaysFacets = new HashMap<String,Facets>();
+
+    if (taxoReader != null) {
+      drillDownFacets = new FastTaxonomyFacetCounts(taxoReader, config, drillDowns);
+      if (drillSideways != null) {
+        for(int i=0;i<drillSideways.length;i++) {
+          drillSidewaysFacets.put(drillSidewaysDims[i],
+                                  new FastTaxonomyFacetCounts(taxoReader, config, drillSideways[i]));
+        }
+      }
+    } else {
+      drillDownFacets = new SortedSetDocValuesFacetCounts(state, drillDowns);
+      if (drillSideways != null) {
+        for(int i=0;i<drillSideways.length;i++) {
+          drillSidewaysFacets.put(drillSidewaysDims[i],
+                                  new SortedSetDocValuesFacetCounts(state, drillSideways[i]));
+        }
+      }
+    }
+
+    if (drillSidewaysFacets.isEmpty()) {
+      return drillDownFacets;
+    } else {
+      return new MultiFacets(drillSidewaysFacets, drillDownFacets);
+    }
+  }
+
+  /**
+   * Search, collecting hits with a {@link Collector}, and
+   * computing drill down and sideways counts.
+   */
+  @SuppressWarnings({"rawtypes","unchecked"})
+  public DrillSidewaysResult search(DrillDownQuery query, Collector hitCollector) throws IOException {
+
+    Map<String,Integer> drillDownDims = query.getDims();
+
+    FacetsCollector drillDownCollector = new FacetsCollector();
+    
+    if (drillDownDims.isEmpty()) {
+      // There are no drill-down dims, so there is no
+      // drill-sideways to compute:
+      searcher.search(query, MultiCollector.wrap(hitCollector, drillDownCollector));
+      return new DrillSidewaysResult(buildFacetsResult(drillDownCollector, null, null), null);
+    }
+
+    BooleanQuery ddq = query.getBooleanQuery();
+    BooleanClause[] clauses = ddq.getClauses();
+
+    Query baseQuery;
+    int startClause;
+    if (clauses.length == drillDownDims.size()) {
+      // TODO: we could optimize this pure-browse case by
+      // making a custom scorer instead:
+      baseQuery = new MatchAllDocsQuery();
+      startClause = 0;
+    } else {
+      assert clauses.length == 1+drillDownDims.size();
+      baseQuery = clauses[0].getQuery();
+      startClause = 1;
+    }
+
+    FacetsCollector[] drillSidewaysCollectors = new FacetsCollector[drillDownDims.size()];
+
+    int idx = 0;
+    for(String dim : drillDownDims.keySet()) {
+      drillSidewaysCollectors[idx++] = new FacetsCollector();
+    }
+
+    boolean useCollectorMethod = scoreSubDocsAtOnce();
+
+    Term[][] drillDownTerms = null;
+
+    if (!useCollectorMethod) {
+      // Optimistic: assume subQueries of the DDQ are either
+      // TermQuery or BQ OR of TermQuery; if this is wrong
+      // then we detect it and fallback to the mome general
+      // but slower DrillSidewaysCollector:
+      drillDownTerms = new Term[clauses.length-startClause][];
+      for(int i=startClause;i<clauses.length;i++) {
+        Query q = clauses[i].getQuery();
+
+        // DrillDownQuery always wraps each subQuery in
+        // ConstantScoreQuery:
+        assert q instanceof ConstantScoreQuery;
+
+        q = ((ConstantScoreQuery) q).getQuery();
+
+        if (q instanceof TermQuery) {
+          drillDownTerms[i-startClause] = new Term[] {((TermQuery) q).getTerm()};
+        } else if (q instanceof BooleanQuery) {
+          BooleanQuery q2 = (BooleanQuery) q;
+          BooleanClause[] clauses2 = q2.getClauses();
+          drillDownTerms[i-startClause] = new Term[clauses2.length];
+          for(int j=0;j<clauses2.length;j++) {
+            if (clauses2[j].getQuery() instanceof TermQuery) {
+              drillDownTerms[i-startClause][j] = ((TermQuery) clauses2[j].getQuery()).getTerm();
+            } else {
+              useCollectorMethod = true;
+              break;
+            }
+          }
+        } else {
+          useCollectorMethod = true;
+        }
+      }
+    }
+
+    if (useCollectorMethod) {
+      // TODO: maybe we could push the "collector method"
+      // down into the optimized scorer to have a tighter
+      // integration ... and so TermQuery clauses could
+      // continue to run "optimized"
+      collectorMethod(query, baseQuery, startClause, hitCollector, drillDownCollector, drillSidewaysCollectors);
+    } else {
+      DrillSidewaysQuery dsq = new DrillSidewaysQuery(baseQuery, drillDownCollector, drillSidewaysCollectors, drillDownTerms);
+      searcher.search(dsq, hitCollector);
+    }
+
+    return new DrillSidewaysResult(buildFacetsResult(drillDownCollector, drillSidewaysCollectors, drillDownDims.keySet().toArray(new String[drillDownDims.size()])), null);
+  }
+
+  /** Uses the more general but slower method of sideways
+   *  counting. This method allows an arbitrary subQuery to
+   *  implement the drill down for a given dimension. */
+  private void collectorMethod(DrillDownQuery ddq, Query baseQuery, int startClause, Collector hitCollector, Collector drillDownCollector, Collector[] drillSidewaysCollectors) throws IOException {
+
+    BooleanClause[] clauses = ddq.getBooleanQuery().getClauses();
+
+    Map<String,Integer> drillDownDims = ddq.getDims();
+
+    BooleanQuery topQuery = new BooleanQuery(true);
+    final DrillSidewaysCollector collector = new DrillSidewaysCollector(hitCollector, drillDownCollector, drillSidewaysCollectors,
+                                                                                    drillDownDims);
+
+    // TODO: if query is already a BQ we could copy that and
+    // add clauses to it, instead of doing BQ inside BQ
+    // (should be more efficient)?  Problem is this can
+    // affect scoring (coord) ... too bad we can't disable
+    // coord on a clause by clause basis:
+    topQuery.add(baseQuery, BooleanClause.Occur.MUST);
+
+    // NOTE: in theory we could just make a single BQ, with
+    // +query a b c minShouldMatch=2, but in this case,
+    // annoyingly, BS2 wraps a sub-scorer that always
+    // returns 2 as the .freq(), not how many of the
+    // SHOULD clauses matched:
+    BooleanQuery subQuery = new BooleanQuery(true);
+
+    Query wrappedSubQuery = new QueryWrapper(subQuery,
+                                             new SetWeight() {
+                                               @Override
+                                               public void set(Weight w) {
+                                                 collector.setWeight(w, -1);
+                                               }
+                                             });
+    Query constantScoreSubQuery = new ConstantScoreQuery(wrappedSubQuery);
+
+    // Don't impact score of original query:
+    constantScoreSubQuery.setBoost(0.0f);
+
+    topQuery.add(constantScoreSubQuery, BooleanClause.Occur.MUST);
+
+    // Unfortunately this sub-BooleanQuery
+    // will never get BS1 because today BS1 only works
+    // if topScorer=true... and actually we cannot use BS1
+    // anyways because we need subDocsScoredAtOnce:
+    int dimIndex = 0;
+    for(int i=startClause;i<clauses.length;i++) {
+      Query q = clauses[i].getQuery();
+      // DrillDownQuery always wraps each subQuery in
+      // ConstantScoreQuery:
+      assert q instanceof ConstantScoreQuery;
+      q = ((ConstantScoreQuery) q).getQuery();
+
+      final int finalDimIndex = dimIndex;
+      subQuery.add(new QueryWrapper(q,
+                                    new SetWeight() {
+                                      @Override
+                                      public void set(Weight w) {
+                                        collector.setWeight(w, finalDimIndex);
+                                      }
+                                    }),
+                   BooleanClause.Occur.SHOULD);
+      dimIndex++;
+    }
+
+    // TODO: we could better optimize the "just one drill
+    // down" case w/ a separate [specialized]
+    // collector...
+    int minShouldMatch = drillDownDims.size()-1;
+    if (minShouldMatch == 0) {
+      // Must add another "fake" clause so BQ doesn't erase
+      // itself by rewriting to the single clause:
+      Query end = new MatchAllDocsQuery();
+      end.setBoost(0.0f);
+      subQuery.add(end, BooleanClause.Occur.SHOULD);
+      minShouldMatch++;
+    }
+
+    subQuery.setMinimumNumberShouldMatch(minShouldMatch);
+
+    // System.out.println("EXE " + topQuery);
+
+    // Collects against the passed-in
+    // drillDown/SidewaysCollectors as a side effect:
+    searcher.search(topQuery, collector);
+  }
+
+  /**
+   * Search, sorting by {@link Sort}, and computing
+   * drill down and sideways counts.
+   */
+  public DrillSidewaysResult search(DrillDownQuery query,
+                                          Filter filter, FieldDoc after, int topN, Sort sort, boolean doDocScores,
+                                          boolean doMaxScore) throws IOException {
+    if (filter != null) {
+      query = new DrillDownQuery(config, filter, query);
+    }
+    if (sort != null) {
+      int limit = searcher.getIndexReader().maxDoc();
+      if (limit == 0) {
+        limit = 1; // the collector does not alow numHits = 0
+      }
+      topN = Math.min(topN, limit);
+      final TopFieldCollector hitCollector = TopFieldCollector.create(sort,
+                                                                      topN,
+                                                                      after,
+                                                                      true,
+                                                                      doDocScores,
+                                                                      doMaxScore,
+                                                                      true);
+      DrillSidewaysResult r = search(query, hitCollector);
+      return new DrillSidewaysResult(r.facets, hitCollector.topDocs());
+    } else {
+      return search(after, query, topN);
+    }
+  }
+
+  /**
+   * Search, sorting by score, and computing
+   * drill down and sideways counts.
+   */
+  public DrillSidewaysResult search(DrillDownQuery query, int topN) throws IOException {
+    return search(null, query, topN);
+  }
+
+  /**
+   * Search, sorting by score, and computing
+   * drill down and sideways counts.
+   */
+  public DrillSidewaysResult search(ScoreDoc after,
+                                          DrillDownQuery query, int topN) throws IOException {
+    int limit = searcher.getIndexReader().maxDoc();
+    if (limit == 0) {
+      limit = 1; // the collector does not alow numHits = 0
+    }
+    topN = Math.min(topN, limit);
+    TopScoreDocCollector hitCollector = TopScoreDocCollector.create(topN, after, true);
+    DrillSidewaysResult r = search(query, hitCollector);
+    return new DrillSidewaysResult(r.facets, hitCollector.topDocs());
+  }
+
+  /** Override this and return true if your collector
+   *  (e.g., ToParentBlockJoinCollector) expects all
+   *  sub-scorers to be positioned on the document being
+   *  collected.  This will cause some performance loss;
+   *  default is false.  Note that if you return true from
+   *  this method (in a subclass) be sure your collector
+   *  also returns false from {@link
+   *  Collector#acceptsDocsOutOfOrder}: this will trick
+   *  BooleanQuery into also scoring all subDocs at once. */
+  protected boolean scoreSubDocsAtOnce() {
+    return false;
+  }
+
+  public static class DrillSidewaysResult {
+    /** Combined drill down & sideways results. */
+    public final Facets facets;
+
+    /** Hits. */
+    public final TopDocs hits;
+
+    public DrillSidewaysResult(Facets facets, TopDocs hits) {
+      this.facets = facets;
+      this.hits = hits;
+    }
+  }
+  private interface SetWeight {
+    public void set(Weight w);
+  }
+
+  /** Just records which Weight was given out for the
+   *  (possibly rewritten) Query. */
+  private static class QueryWrapper extends Query {
+    private final Query originalQuery;
+    private final SetWeight setter;
+
+    public QueryWrapper(Query originalQuery, SetWeight setter) {
+      this.originalQuery = originalQuery;
+      this.setter = setter;
+    }
+
+    @Override
+    public Weight createWeight(final IndexSearcher searcher) throws IOException {
+      Weight w = originalQuery.createWeight(searcher);
+      setter.set(w);
+      return w;
+    }
+
+    @Override
+    public Query rewrite(IndexReader reader) throws IOException {
+      Query rewritten = originalQuery.rewrite(reader);
+      if (rewritten != originalQuery) {
+        return new QueryWrapper(rewritten, setter);
+      } else {
+        return this;
+      }
+    }
+
+    @Override
+    public String toString(String s) {
+      return originalQuery.toString(s);
+    }
+
+    @Override
+    public boolean equals(Object o) {
+      if (!(o instanceof QueryWrapper)) return false;
+      final QueryWrapper other = (QueryWrapper) o;
+      return super.equals(o) && originalQuery.equals(other.originalQuery);
+    }
+
+    @Override
+    public int hashCode() {
+      return super.hashCode() * 31 + originalQuery.hashCode();
+    }
+  }
+}
+


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysQuery.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysQuery.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysQuery.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysQuery.java	2013-11-26 10:44:41.399037706 -0500
@@ -0,0 +1,198 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.Explanation;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Weight;
+import org.apache.lucene.util.Bits;
+
+/** Only purpose is to punch through and return a
+ *  SimpleDrillSidewaysScorer */ 
+
+class DrillSidewaysQuery extends Query {
+  final Query baseQuery;
+  final Collector drillDownCollector;
+  final Collector[] drillSidewaysCollectors;
+  final Term[][] drillDownTerms;
+
+  DrillSidewaysQuery(Query baseQuery, Collector drillDownCollector, Collector[] drillSidewaysCollectors, Term[][] drillDownTerms) {
+    this.baseQuery = baseQuery;
+    this.drillDownCollector = drillDownCollector;
+    this.drillSidewaysCollectors = drillSidewaysCollectors;
+    this.drillDownTerms = drillDownTerms;
+  }
+
+  @Override
+  public String toString(String field) {
+    return "DrillSidewaysQuery";
+  }
+
+  @Override
+  public Query rewrite(IndexReader reader) throws IOException {
+    Query newQuery = baseQuery;
+    while(true) {
+      Query rewrittenQuery = newQuery.rewrite(reader);
+      if (rewrittenQuery == newQuery) {
+        break;
+      }
+      newQuery = rewrittenQuery;
+    }
+    if (newQuery == baseQuery) {
+      return this;
+    } else {
+      return new DrillSidewaysQuery(newQuery, drillDownCollector, drillSidewaysCollectors, drillDownTerms);
+    }
+  }
+  
+  @Override
+  public Weight createWeight(IndexSearcher searcher) throws IOException {
+    final Weight baseWeight = baseQuery.createWeight(searcher);
+
+    return new Weight() {
+      @Override
+      public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+        return baseWeight.explain(context, doc);
+      }
+
+      @Override
+      public Query getQuery() {
+        return baseQuery;
+      }
+
+      @Override
+      public float getValueForNormalization() throws IOException {
+        return baseWeight.getValueForNormalization();
+      }
+
+      @Override
+      public void normalize(float norm, float topLevelBoost) {
+        baseWeight.normalize(norm, topLevelBoost);
+      }
+
+      @Override
+      public boolean scoresDocsOutOfOrder() {
+        // TODO: would be nice if AssertingIndexSearcher
+        // confirmed this for us
+        return false;
+      }
+
+      @Override
+      public Scorer scorer(AtomicReaderContext context, boolean scoreDocsInOrder,
+                           boolean topScorer, Bits acceptDocs) throws IOException {
+
+        DrillSidewaysScorer.DocsEnumsAndFreq[] dims = new DrillSidewaysScorer.DocsEnumsAndFreq[drillDownTerms.length];
+        TermsEnum termsEnum = null;
+        String lastField = null;
+        int nullCount = 0;
+        for(int dim=0;dim<dims.length;dim++) {
+          dims[dim] = new DrillSidewaysScorer.DocsEnumsAndFreq();
+          dims[dim].sidewaysCollector = drillSidewaysCollectors[dim];
+          String field = drillDownTerms[dim][0].field();
+          dims[dim].dim = drillDownTerms[dim][0].text();
+          if (lastField == null || !lastField.equals(field)) {
+            AtomicReader reader = context.reader();
+            Terms terms = reader.terms(field);
+            if (terms != null) {
+              termsEnum = terms.iterator(null);
+            } else {
+              termsEnum = null;
+            }
+            lastField = field;
+          }
+          dims[dim].docsEnums = new DocsEnum[drillDownTerms[dim].length];
+          if (termsEnum == null) {
+            nullCount++;
+            continue;
+          }
+          for(int i=0;i<drillDownTerms[dim].length;i++) {
+            if (termsEnum.seekExact(drillDownTerms[dim][i].bytes())) {
+              DocsEnum docsEnum = termsEnum.docs(null, null, 0);
+              if (docsEnum != null) {
+                dims[dim].docsEnums[i] = docsEnum;
+                dims[dim].maxCost = Math.max(dims[dim].maxCost, docsEnum.cost());
+              }
+            }
+          }
+        }
+
+        if (nullCount > 1 || (nullCount == 1 && dims.length == 1)) {
+          return null;
+        }
+
+        // Sort drill-downs by most restrictive first:
+        Arrays.sort(dims);
+
+        // TODO: it could be better if we take acceptDocs
+        // into account instead of baseScorer?
+        Scorer baseScorer = baseWeight.scorer(context, scoreDocsInOrder, false, acceptDocs);
+
+        if (baseScorer == null) {
+          return null;
+        }
+
+        return new DrillSidewaysScorer(this, context,
+                                             baseScorer,
+                                             drillDownCollector, dims);
+      }
+    };
+  }
+
+  // TODO: these should do "deeper" equals/hash on the 2-D drillDownTerms array
+
+  @Override
+  public int hashCode() {
+    final int prime = 31;
+    int result = super.hashCode();
+    result = prime * result + ((baseQuery == null) ? 0 : baseQuery.hashCode());
+    result = prime * result
+        + ((drillDownCollector == null) ? 0 : drillDownCollector.hashCode());
+    result = prime * result + Arrays.hashCode(drillDownTerms);
+    result = prime * result + Arrays.hashCode(drillSidewaysCollectors);
+    return result;
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (this == obj) return true;
+    if (!super.equals(obj)) return false;
+    if (getClass() != obj.getClass()) return false;
+    DrillSidewaysQuery other = (DrillSidewaysQuery) obj;
+    if (baseQuery == null) {
+      if (other.baseQuery != null) return false;
+    } else if (!baseQuery.equals(other.baseQuery)) return false;
+    if (drillDownCollector == null) {
+      if (other.drillDownCollector != null) return false;
+    } else if (!drillDownCollector.equals(other.drillDownCollector)) return false;
+    if (!Arrays.equals(drillDownTerms, other.drillDownTerms)) return false;
+    if (!Arrays.equals(drillSidewaysCollectors, other.drillSidewaysCollectors)) return false;
+    return true;
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysScorer.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysScorer.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysScorer.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysScorer.java	2013-11-26 10:44:55.599037247 -0500
@@ -0,0 +1,654 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Collections;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Weight;
+import org.apache.lucene.util.FixedBitSet;
+
+class DrillSidewaysScorer extends Scorer {
+
+  //private static boolean DEBUG = false;
+
+  private final Collector drillDownCollector;
+
+  private final DocsEnumsAndFreq[] dims;
+
+  // DrillDown DocsEnums:
+  private final Scorer baseScorer;
+
+  private final AtomicReaderContext context;
+
+  private static final int CHUNK = 2048;
+  private static final int MASK = CHUNK-1;
+
+  private int collectDocID = -1;
+  private float collectScore;
+
+  DrillSidewaysScorer(Weight w, AtomicReaderContext context, Scorer baseScorer, Collector drillDownCollector,
+                            DocsEnumsAndFreq[] dims) {
+    super(w);
+    this.dims = dims;
+    this.context = context;
+    this.baseScorer = baseScorer;
+    this.drillDownCollector = drillDownCollector;
+  }
+
+  @Override
+  public void score(Collector collector) throws IOException {
+    //if (DEBUG) {
+    //  System.out.println("\nscore: reader=" + context.reader());
+    //}
+    //System.out.println("score r=" + context.reader());
+    collector.setScorer(this);
+    if (drillDownCollector != null) {
+      drillDownCollector.setScorer(this);
+      drillDownCollector.setNextReader(context);
+    }
+    for(DocsEnumsAndFreq dim : dims) {
+      dim.sidewaysCollector.setScorer(this);
+      dim.sidewaysCollector.setNextReader(context);
+    }
+
+    // TODO: if we ever allow null baseScorer ... it will
+    // mean we DO score docs out of order ... hmm, or if we
+    // change up the order of the conjuntions below
+    assert baseScorer != null;
+
+    // Position all scorers to their first matching doc:
+    baseScorer.nextDoc();
+    for(DocsEnumsAndFreq dim : dims) {
+      for (DocsEnum docsEnum : dim.docsEnums) {
+        if (docsEnum != null) {
+          docsEnum.nextDoc();
+        }
+      }
+    }
+
+    final int numDims = dims.length;
+
+    DocsEnum[][] docsEnums = new DocsEnum[numDims][];
+    Collector[] sidewaysCollectors = new Collector[numDims];
+    long drillDownCost = 0;
+    for(int dim=0;dim<numDims;dim++) {
+      docsEnums[dim] = dims[dim].docsEnums;
+      sidewaysCollectors[dim] = dims[dim].sidewaysCollector;
+      for (DocsEnum de : dims[dim].docsEnums) {
+        if (de != null) {
+          drillDownCost += de.cost();
+        }
+      }
+    }
+
+    long baseQueryCost = baseScorer.cost();
+
+    /*
+    System.out.println("\nbaseDocID=" + baseScorer.docID() + " est=" + estBaseHitCount);
+    System.out.println("  maxDoc=" + context.reader().maxDoc());
+    System.out.println("  maxCost=" + maxCost);
+    System.out.println("  dims[0].freq=" + dims[0].freq);
+    if (numDims > 1) {
+      System.out.println("  dims[1].freq=" + dims[1].freq);
+    }
+    */
+
+    if (baseQueryCost < drillDownCost/10) {
+      //System.out.println("baseAdvance");
+      doBaseAdvanceScoring(collector, docsEnums, sidewaysCollectors);
+    } else if (numDims > 1 && (dims[1].maxCost < baseQueryCost/10)) {
+      //System.out.println("drillDownAdvance");
+      doDrillDownAdvanceScoring(collector, docsEnums, sidewaysCollectors);
+    } else {
+      //System.out.println("union");
+      doUnionScoring(collector, docsEnums, sidewaysCollectors);
+    }
+  }
+
+  /** Used when drill downs are highly constraining vs
+   *  baseQuery. */
+  private void doDrillDownAdvanceScoring(Collector collector, DocsEnum[][] docsEnums, Collector[] sidewaysCollectors) throws IOException {
+    final int maxDoc = context.reader().maxDoc();
+    final int numDims = dims.length;
+
+    //if (DEBUG) {
+    //  System.out.println("  doDrillDownAdvanceScoring");
+    //}
+
+    // TODO: maybe a class like BS, instead of parallel arrays
+    int[] filledSlots = new int[CHUNK];
+    int[] docIDs = new int[CHUNK];
+    float[] scores = new float[CHUNK];
+    int[] missingDims = new int[CHUNK];
+    int[] counts = new int[CHUNK];
+
+    docIDs[0] = -1;
+    int nextChunkStart = CHUNK;
+
+    final FixedBitSet seen = new FixedBitSet(CHUNK);
+
+    while (true) {
+      //if (DEBUG) {
+      //  System.out.println("\ncycle nextChunkStart=" + nextChunkStart + " docIds[0]=" + docIDs[0]);
+      //}
+
+      // First dim:
+      //if (DEBUG) {
+      //  System.out.println("  dim0");
+      //}
+      for(DocsEnum docsEnum : docsEnums[0]) {
+        if (docsEnum == null) {
+          continue;
+        }
+        int docID = docsEnum.docID();
+        while (docID < nextChunkStart) {
+          int slot = docID & MASK;
+
+          if (docIDs[slot] != docID) {
+            seen.set(slot);
+            // Mark slot as valid:
+            //if (DEBUG) {
+            //  System.out.println("    set docID=" + docID + " id=" + context.reader().document(docID).get("id"));
+            //}
+            docIDs[slot] = docID;
+            missingDims[slot] = 1;
+            counts[slot] = 1;
+          }
+
+          docID = docsEnum.nextDoc();
+        }
+      }
+
+      // Second dim:
+      //if (DEBUG) {
+      //  System.out.println("  dim1");
+      //}
+      for(DocsEnum docsEnum : docsEnums[1]) {
+        if (docsEnum == null) {
+          continue;
+        }
+        int docID = docsEnum.docID();
+        while (docID < nextChunkStart) {
+          int slot = docID & MASK;
+
+          if (docIDs[slot] != docID) {
+            // Mark slot as valid:
+            seen.set(slot);
+            //if (DEBUG) {
+            //  System.out.println("    set docID=" + docID + " missingDim=0 id=" + context.reader().document(docID).get("id"));
+            //}
+            docIDs[slot] = docID;
+            missingDims[slot] = 0;
+            counts[slot] = 1;
+          } else {
+            // TODO: single-valued dims will always be true
+            // below; we could somehow specialize
+            if (missingDims[slot] >= 1) {
+              missingDims[slot] = 2;
+              counts[slot] = 2;
+              //if (DEBUG) {
+              //  System.out.println("    set docID=" + docID + " missingDim=2 id=" + context.reader().document(docID).get("id"));
+              //}
+            } else {
+              counts[slot] = 1;
+              //if (DEBUG) {
+              //  System.out.println("    set docID=" + docID + " missingDim=" + missingDims[slot] + " id=" + context.reader().document(docID).get("id"));
+              //}
+            }
+          }
+
+          docID = docsEnum.nextDoc();
+        }
+      }
+
+      // After this we can "upgrade" to conjunction, because
+      // any doc not seen by either dim 0 or dim 1 cannot be
+      // a hit or a near miss:
+
+      //if (DEBUG) {
+      //  System.out.println("  baseScorer");
+      //}
+
+      // Fold in baseScorer, using advance:
+      int filledCount = 0;
+      int slot0 = 0;
+      while (slot0 < CHUNK && (slot0 = seen.nextSetBit(slot0)) != -1) {
+        int ddDocID = docIDs[slot0];
+        assert ddDocID != -1;
+
+        int baseDocID = baseScorer.docID();
+        if (baseDocID < ddDocID) {
+          baseDocID = baseScorer.advance(ddDocID);
+        }
+        if (baseDocID == ddDocID) {
+          //if (DEBUG) {
+          //  System.out.println("    keep docID=" + ddDocID + " id=" + context.reader().document(ddDocID).get("id"));
+          //}
+          scores[slot0] = baseScorer.score();
+          filledSlots[filledCount++] = slot0;
+          counts[slot0]++;
+        } else {
+          //if (DEBUG) {
+          //  System.out.println("    no docID=" + ddDocID + " id=" + context.reader().document(ddDocID).get("id"));
+          //}
+          docIDs[slot0] = -1;
+
+          // TODO: we could jump slot0 forward to the
+          // baseDocID ... but we'd need to set docIDs for
+          // intervening slots to -1
+        }
+        slot0++;
+      }
+      seen.clear(0, CHUNK);
+
+      if (filledCount == 0) {
+        if (nextChunkStart >= maxDoc) {
+          break;
+        }
+        nextChunkStart += CHUNK;
+        continue;
+      }
+      
+      // TODO: factor this out & share w/ union scorer,
+      // except we start from dim=2 instead:
+      for(int dim=2;dim<numDims;dim++) {
+        //if (DEBUG) {
+        //  System.out.println("  dim=" + dim + " [" + dims[dim].dim + "]");
+        //}
+        for(DocsEnum docsEnum : docsEnums[dim]) {
+          if (docsEnum == null) {
+            continue;
+          }
+          int docID = docsEnum.docID();
+          while (docID < nextChunkStart) {
+            int slot = docID & MASK;
+            if (docIDs[slot] == docID && counts[slot] >= dim) {
+              // TODO: single-valued dims will always be true
+              // below; we could somehow specialize
+              if (missingDims[slot] >= dim) {
+                //if (DEBUG) {
+                //  System.out.println("    set docID=" + docID + " count=" + (dim+2));
+                //}
+                missingDims[slot] = dim+1;
+                counts[slot] = dim+2;
+              } else {
+                //if (DEBUG) {
+                //  System.out.println("    set docID=" + docID + " missing count=" + (dim+1));
+                //}
+                counts[slot] = dim+1;
+              }
+            }
+            // TODO: sometimes use advance?
+            docID = docsEnum.nextDoc();
+          }
+        }
+      }
+
+      // Collect:
+      //if (DEBUG) {
+      //  System.out.println("  now collect: " + filledCount + " hits");
+      //}
+      for(int i=0;i<filledCount;i++) {
+        int slot = filledSlots[i];
+        collectDocID = docIDs[slot];
+        collectScore = scores[slot];
+        //if (DEBUG) {
+        //  System.out.println("    docID=" + docIDs[slot] + " count=" + counts[slot]);
+        //}
+        if (counts[slot] == 1+numDims) {
+          collectHit(collector, sidewaysCollectors);
+        } else if (counts[slot] == numDims) {
+          collectNearMiss(sidewaysCollectors, missingDims[slot]);
+        }
+      }
+
+      if (nextChunkStart >= maxDoc) {
+        break;
+      }
+
+      nextChunkStart += CHUNK;
+    }
+  }
+
+  /** Used when base query is highly constraining vs the
+   *  drilldowns; in this case we just .next() on base and
+   *  .advance() on the dims. */
+  private void doBaseAdvanceScoring(Collector collector, DocsEnum[][] docsEnums, Collector[] sidewaysCollectors) throws IOException {
+    //if (DEBUG) {
+    //  System.out.println("  doBaseAdvanceScoring");
+    //}
+    int docID = baseScorer.docID();
+
+    final int numDims = dims.length;
+
+    nextDoc: while (docID != NO_MORE_DOCS) {
+      int failedDim = -1;
+      for(int dim=0;dim<numDims;dim++) {
+        // TODO: should we sort this 2nd dimension of
+        // docsEnums from most frequent to least?
+        boolean found = false;
+        for(DocsEnum docsEnum : docsEnums[dim]) {
+          if (docsEnum == null) {
+            continue;
+          }
+          if (docsEnum.docID() < docID) {
+            docsEnum.advance(docID);
+          }
+          if (docsEnum.docID() == docID) {
+            found = true;
+            break;
+          }
+        }
+        if (!found) {
+          if (failedDim != -1) {
+            // More than one dim fails on this document, so
+            // it's neither a hit nor a near-miss; move to
+            // next doc:
+            docID = baseScorer.nextDoc();
+            continue nextDoc;
+          } else {
+            failedDim = dim;
+          }
+        }
+      }
+
+      collectDocID = docID;
+
+      // TODO: we could score on demand instead since we are
+      // daat here:
+      collectScore = baseScorer.score();
+
+      if (failedDim == -1) {
+        collectHit(collector, sidewaysCollectors);
+      } else {
+        collectNearMiss(sidewaysCollectors, failedDim);
+      }
+
+      docID = baseScorer.nextDoc();
+    }
+  }
+
+  private void collectHit(Collector collector, Collector[] sidewaysCollectors) throws IOException {
+    //if (DEBUG) {
+    //  System.out.println("      hit");
+    //}
+
+    collector.collect(collectDocID);
+    if (drillDownCollector != null) {
+      drillDownCollector.collect(collectDocID);
+    }
+
+    // TODO: we could "fix" faceting of the sideways counts
+    // to do this "union" (of the drill down hits) in the
+    // end instead:
+
+    // Tally sideways counts:
+    for(int dim=0;dim<sidewaysCollectors.length;dim++) {
+      sidewaysCollectors[dim].collect(collectDocID);
+    }
+  }
+
+  private void collectNearMiss(Collector[] sidewaysCollectors, int dim) throws IOException {
+    //if (DEBUG) {
+    //  System.out.println("      missingDim=" + dim);
+    //}
+    sidewaysCollectors[dim].collect(collectDocID);
+  }
+
+  private void doUnionScoring(Collector collector, DocsEnum[][] docsEnums, Collector[] sidewaysCollectors) throws IOException {
+    //if (DEBUG) {
+    //  System.out.println("  doUnionScoring");
+    //}
+
+    final int maxDoc = context.reader().maxDoc();
+    final int numDims = dims.length;
+
+    // TODO: maybe a class like BS, instead of parallel arrays
+    int[] filledSlots = new int[CHUNK];
+    int[] docIDs = new int[CHUNK];
+    float[] scores = new float[CHUNK];
+    int[] missingDims = new int[CHUNK];
+    int[] counts = new int[CHUNK];
+
+    docIDs[0] = -1;
+
+    // NOTE: this is basically a specialized version of
+    // BooleanScorer, to the minShouldMatch=N-1 case, but
+    // carefully tracking which dimension failed to match
+
+    int nextChunkStart = CHUNK;
+
+    while (true) {
+      //if (DEBUG) {
+      //  System.out.println("\ncycle nextChunkStart=" + nextChunkStart + " docIds[0]=" + docIDs[0]);
+      //}
+      int filledCount = 0;
+      int docID = baseScorer.docID();
+      //if (DEBUG) {
+      //  System.out.println("  base docID=" + docID);
+      //}
+      while (docID < nextChunkStart) {
+        int slot = docID & MASK;
+        //if (DEBUG) {
+        //  System.out.println("    docIDs[slot=" + slot + "]=" + docID + " id=" + context.reader().document(docID).get("id"));
+        //}
+
+        // Mark slot as valid:
+        assert docIDs[slot] != docID: "slot=" + slot + " docID=" + docID;
+        docIDs[slot] = docID;
+        scores[slot] = baseScorer.score();
+        filledSlots[filledCount++] = slot;
+        missingDims[slot] = 0;
+        counts[slot] = 1;
+
+        docID = baseScorer.nextDoc();
+      }
+
+      if (filledCount == 0) {
+        if (nextChunkStart >= maxDoc) {
+          break;
+        }
+        nextChunkStart += CHUNK;
+        continue;
+      }
+
+      // First drill-down dim, basically adds SHOULD onto
+      // the baseQuery:
+      //if (DEBUG) {
+      //  System.out.println("  dim=0 [" + dims[0].dim + "]");
+      //}
+      for(DocsEnum docsEnum : docsEnums[0]) {
+        if (docsEnum == null) {
+          continue;
+        }
+        docID = docsEnum.docID();
+        //if (DEBUG) {
+        //  System.out.println("    start docID=" + docID);
+        //}
+        while (docID < nextChunkStart) {
+          int slot = docID & MASK;
+          if (docIDs[slot] == docID) {
+            //if (DEBUG) {
+            //  System.out.println("      set docID=" + docID + " count=2");
+            //}
+            missingDims[slot] = 1;
+            counts[slot] = 2;
+          }
+          docID = docsEnum.nextDoc();
+        }
+      }
+
+      for(int dim=1;dim<numDims;dim++) {
+        //if (DEBUG) {
+        //  System.out.println("  dim=" + dim + " [" + dims[dim].dim + "]");
+        //}
+        for(DocsEnum docsEnum : docsEnums[dim]) {
+          if (docsEnum == null) {
+            continue;
+          }
+          docID = docsEnum.docID();
+          //if (DEBUG) {
+          //  System.out.println("    start docID=" + docID);
+          //}
+          while (docID < nextChunkStart) {
+            int slot = docID & MASK;
+            if (docIDs[slot] == docID && counts[slot] >= dim) {
+              // This doc is still in the running...
+              // TODO: single-valued dims will always be true
+              // below; we could somehow specialize
+              if (missingDims[slot] >= dim) {
+                //if (DEBUG) {
+                //  System.out.println("      set docID=" + docID + " count=" + (dim+2));
+                //}
+                missingDims[slot] = dim+1;
+                counts[slot] = dim+2;
+              } else {
+                //if (DEBUG) {
+                //  System.out.println("      set docID=" + docID + " missing count=" + (dim+1));
+                //}
+                counts[slot] = dim+1;
+              }
+            }
+            docID = docsEnum.nextDoc();
+          }
+
+          // TODO: sometimes use advance?
+
+          /*
+            int docBase = nextChunkStart - CHUNK;
+            for(int i=0;i<filledCount;i++) {
+              int slot = filledSlots[i];
+              docID = docBase + filledSlots[i];
+              if (docIDs[slot] == docID && counts[slot] >= dim) {
+                // This doc is still in the running...
+                int ddDocID = docsEnum.docID();
+                if (ddDocID < docID) {
+                  ddDocID = docsEnum.advance(docID);
+                }
+                if (ddDocID == docID) {
+                  if (missingDims[slot] >= dim && counts[slot] == allMatchCount) {
+                  //if (DEBUG) {
+                  //    System.out.println("    set docID=" + docID + " count=" + (dim+2));
+                   // }
+                    missingDims[slot] = dim+1;
+                    counts[slot] = dim+2;
+                  } else {
+                  //if (DEBUG) {
+                  //    System.out.println("    set docID=" + docID + " missing count=" + (dim+1));
+                   // }
+                    counts[slot] = dim+1;
+                  }
+                }
+              }
+            }            
+          */
+        }
+      }
+
+      // Collect:
+      //if (DEBUG) {
+      //  System.out.println("  now collect: " + filledCount + " hits");
+      //}
+      for(int i=0;i<filledCount;i++) {
+        // NOTE: This is actually in-order collection,
+        // because we only accept docs originally returned by
+        // the baseScorer (ie that Scorer is AND'd)
+        int slot = filledSlots[i];
+        collectDocID = docIDs[slot];
+        collectScore = scores[slot];
+        //if (DEBUG) {
+        //  System.out.println("    docID=" + docIDs[slot] + " count=" + counts[slot]);
+        //}
+        //System.out.println("  collect doc=" + collectDocID + " main.freq=" + (counts[slot]-1) + " main.doc=" + collectDocID + " exactCount=" + numDims);
+        if (counts[slot] == 1+numDims) {
+          //System.out.println("    hit");
+          collectHit(collector, sidewaysCollectors);
+        } else if (counts[slot] == numDims) {
+          //System.out.println("    sw");
+          collectNearMiss(sidewaysCollectors, missingDims[slot]);
+        }
+      }
+
+      if (nextChunkStart >= maxDoc) {
+        break;
+      }
+
+      nextChunkStart += CHUNK;
+    }
+  }
+
+  @Override
+  public int docID() {
+    return collectDocID;
+  }
+
+  @Override
+  public float score() {
+    return collectScore;
+  }
+
+  @Override
+  public int freq() {
+    return 1+dims.length;
+  }
+
+  @Override
+  public int nextDoc() {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public int advance(int target) {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public long cost() {
+    return baseScorer.cost();
+  }
+
+  @Override
+  public Collection<ChildScorer> getChildren() {
+    return Collections.singletonList(new ChildScorer(baseScorer, "MUST"));
+  }
+
+  static class DocsEnumsAndFreq implements Comparable<DocsEnumsAndFreq> {
+    DocsEnum[] docsEnums;
+    // Max cost for all docsEnums for this dim:
+    long maxCost;
+    Collector sidewaysCollector;
+    String dim;
+
+    @Override
+    public int compareTo(DocsEnumsAndFreq other) {
+      if (maxCost < other.maxCost) {
+        return -1;
+      } else if (maxCost > other.maxCost) {
+        return 1;
+      } else {
+        return 0;
+      }
+    }
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/ChunksIntEncoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/ChunksIntEncoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/ChunksIntEncoder.java	2013-02-20 13:38:17.676711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/ChunksIntEncoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,115 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link IntEncoder} which encodes values in chunks. Implementations of this
- * class assume the data which needs encoding consists of small, consecutive
- * values, and therefore the encoder is able to compress them better. You can
- * read more on the two implementations {@link FourFlagsIntEncoder} and
- * {@link EightFlagsIntEncoder}.
- * <p>
- * Extensions of this class need to implement {@link #encode(IntsRef, BytesRef)}
- * in order to build the proper indicator (flags). When enough values were
- * accumulated (typically the batch size), extensions can call
- * {@link #encodeChunk(BytesRef)} to flush the indicator and the rest of the
- * values.
- * <p>
- * <b>NOTE:</b> flags encoders do not accept values &le; 0 (zero) in their
- * {@link #encode(IntsRef, BytesRef)}. For performance reasons they do not check
- * that condition, however if such value is passed the result stream may be
- * corrupt or an exception will be thrown. Also, these encoders perform the best
- * when there are many consecutive small values (depends on the encoder
- * implementation). If that is not the case, the encoder will occupy 1 more byte
- * for every <i>batch</i> number of integers, over whatever
- * {@link VInt8IntEncoder} would have occupied. Therefore make sure to check
- * whether your data fits into the conditions of the specific encoder.
- * <p>
- * For the reasons mentioned above, these encoders are usually chained with
- * {@link UniqueValuesIntEncoder} and {@link DGapIntEncoder}.
- * 
- * @lucene.experimental
- */
-public abstract class ChunksIntEncoder extends IntEncoder {
-  
-  /** Holds the values which must be encoded, outside the indicator. */
-  protected final IntsRef encodeQueue;
-  
-  /** Represents bits flag byte. */
-  protected int indicator = 0;
-  
-  /** Counts the current ordinal of the encoded value. */
-  protected byte ordinal = 0;
-  
-  protected ChunksIntEncoder(int chunkSize) {
-    encodeQueue = new IntsRef(chunkSize);
-  }
-  
-  /**
-   * Encodes the values of the current chunk. First it writes the indicator, and
-   * then it encodes the values outside the indicator.
-   */
-  protected void encodeChunk(BytesRef buf) {
-    // ensure there's enough room in the buffer
-    int maxBytesRequired = buf.length + 1 + encodeQueue.length * 4; /* indicator + at most 4 bytes per positive VInt */
-    if (buf.bytes.length < maxBytesRequired) {
-      buf.grow(maxBytesRequired);
-    }
-    
-    buf.bytes[buf.length++] = ((byte) indicator);
-    for (int i = 0; i < encodeQueue.length; i++) {
-      // it is better if the encoding is inlined like so, and not e.g.
-      // in a utility method
-      int value = encodeQueue.ints[i];
-      if ((value & ~0x7F) == 0) {
-        buf.bytes[buf.length] = (byte) value;
-        buf.length++;
-      } else if ((value & ~0x3FFF) == 0) {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 1] = (byte) (value & 0x7F);
-        buf.length += 2;
-      } else if ((value & ~0x1FFFFF) == 0) {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
-        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 2] = (byte) (value & 0x7F);
-        buf.length += 3;
-      } else if ((value & ~0xFFFFFFF) == 0) {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0xFE00000) >> 21));
-        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
-        buf.bytes[buf.length + 2] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 3] = (byte) (value & 0x7F);
-        buf.length += 4;
-      } else {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0xF0000000) >> 28));
-        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0xFE00000) >> 21));
-        buf.bytes[buf.length + 2] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
-        buf.bytes[buf.length + 3] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 4] = (byte) (value & 0x7F);
-        buf.length += 5;
-      }
-    }
-    
-    ordinal = 0;
-    indicator = 0;
-    encodeQueue.length = 0;
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapIntDecoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapIntDecoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapIntDecoder.java	2013-02-20 13:38:17.680711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapIntDecoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,52 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link IntDecoder} which wraps another decoder and reverts the d-gap that
- * was encoded by {@link DGapIntEncoder}.
- * 
- * @lucene.experimental
- */
-public final class DGapIntDecoder extends IntDecoder {
-
-  private final IntDecoder decoder;
-
-  public DGapIntDecoder(IntDecoder decoder) {
-    this.decoder = decoder;
-  }
-
-  @Override
-  public void decode(BytesRef buf, IntsRef values) {
-    decoder.decode(buf, values);
-    int prev = 0;
-    for (int i = 0; i < values.length; i++) {
-      values.ints[i] += prev;
-      prev = values.ints[i];
-    }
-  }
-
-  @Override
-  public String toString() {
-    return "DGap(" + decoder.toString() + ")";
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapIntEncoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapIntEncoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapIntEncoder.java	2013-02-20 13:38:17.680711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapIntEncoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,67 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link IntEncoderFilter} which encodes the gap between the given values,
- * rather than the values themselves. This encoder usually yields better
- * encoding performance space-wise (i.e., the final encoded values consume less
- * space) if the values are 'close' to each other.
- * <p>
- * <b>NOTE:</b> this encoder assumes the values are given to
- * {@link #encode(IntsRef, BytesRef)} in an ascending sorted manner, which ensures only
- * positive values are encoded and thus yields better performance. If you are
- * not sure whether the values are sorted or not, it is possible to chain this
- * encoder with {@link SortingIntEncoder} to ensure the values will be
- * sorted before encoding.
- * 
- * @lucene.experimental
- */
-public final class DGapIntEncoder extends IntEncoderFilter {
-
-  /** Initializes with the given encoder. */
-  public DGapIntEncoder(IntEncoder encoder) {
-    super(encoder);
-  }
-
-  @Override
-  public void encode(IntsRef values, BytesRef buf) {
-    int prev = 0;
-    int upto = values.offset + values.length;
-    for (int i = values.offset; i < upto; i++) {
-      int tmp = values.ints[i];
-      values.ints[i] -= prev;
-      prev = tmp;
-    }
-    encoder.encode(values, buf);
-  }
-
-  @Override
-  public IntDecoder createMatchingDecoder() {
-    return new DGapIntDecoder(encoder.createMatchingDecoder());
-  }
-  
-  @Override
-  public String toString() {
-    return "DGap(" + encoder.toString() + ")";
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapVInt8IntDecoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapVInt8IntDecoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapVInt8IntDecoder.java	2013-02-20 13:38:17.676711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapVInt8IntDecoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,67 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Decodes values encoded by {@link DGapVInt8IntDecoder}.
- * 
- * @lucene.experimental
- */
-public final class DGapVInt8IntDecoder extends IntDecoder {
-
-  @Override
-  public void decode(BytesRef buf, IntsRef values) {
-    values.offset = values.length = 0;
-
-    // grow the buffer up front, even if by a large number of values (buf.length)
-    // that saves the need to check inside the loop for every decoded value if
-    // the buffer needs to grow.
-    if (values.ints.length < buf.length) {
-      values.ints = new int[ArrayUtil.oversize(buf.length, RamUsageEstimator.NUM_BYTES_INT)];
-    }
-
-    // it is better if the decoding is inlined like so, and not e.g.
-    // in a utility method
-    int upto = buf.offset + buf.length;
-    int value = 0;
-    int offset = buf.offset;
-    int prev = 0;
-    while (offset < upto) {
-      byte b = buf.bytes[offset++];
-      if (b >= 0) {
-        values.ints[values.length] = ((value << 7) | b) + prev;
-        value = 0;
-        prev = values.ints[values.length];
-        values.length++;
-      } else {
-        value = (value << 7) | (b & 0x7F);
-      }
-    }
-  }
-
-  @Override
-  public String toString() {
-    return "DGapVInt8";
-  }
-
-} 


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapVInt8IntEncoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapVInt8IntEncoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapVInt8IntEncoder.java	2013-02-20 13:38:17.680711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapVInt8IntEncoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,89 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link IntEncoder} which implements variable length encoding for the gap
- * between values. It's a specialized form of the combination of
- * {@link DGapIntEncoder} and {@link VInt8IntEncoder}.
- * 
- * @see VInt8IntEncoder
- * @see DGapIntEncoder
- * 
- * @lucene.experimental
- */
-public final class DGapVInt8IntEncoder extends IntEncoder {
-
-  @Override
-  public void encode(IntsRef values, BytesRef buf) {
-    buf.offset = buf.length = 0;
-    int maxBytesNeeded = 5 * values.length; // at most 5 bytes per VInt
-    if (buf.bytes.length < maxBytesNeeded) {
-      buf.grow(maxBytesNeeded);
-    }
-    
-    int upto = values.offset + values.length;
-    int prev = 0;
-    for (int i = values.offset; i < upto; i++) {
-      // it is better if the encoding is inlined like so, and not e.g.
-      // in a utility method
-      int value = values.ints[i] - prev;
-      if ((value & ~0x7F) == 0) {
-        buf.bytes[buf.length] = (byte) value;
-        buf.length++;
-      } else if ((value & ~0x3FFF) == 0) {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 1] = (byte) (value & 0x7F);
-        buf.length += 2;
-      } else if ((value & ~0x1FFFFF) == 0) {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
-        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 2] = (byte) (value & 0x7F);
-        buf.length += 3;
-      } else if ((value & ~0xFFFFFFF) == 0) {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0xFE00000) >> 21));
-        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
-        buf.bytes[buf.length + 2] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 3] = (byte) (value & 0x7F);
-        buf.length += 4;
-      } else {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0xF0000000) >> 28));
-        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0xFE00000) >> 21));
-        buf.bytes[buf.length + 2] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
-        buf.bytes[buf.length + 3] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 4] = (byte) (value & 0x7F);
-        buf.length += 5;
-      }
-      prev = values.ints[i];
-    }
-  }
-
-  @Override
-  public IntDecoder createMatchingDecoder() {
-    return new DGapVInt8IntDecoder();
-  }
-
-  @Override
-  public String toString() {
-    return "DGapVInt8";
-  }
-
-} 


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/EightFlagsIntDecoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/EightFlagsIntDecoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/EightFlagsIntDecoder.java	2013-02-20 13:38:17.680711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/EightFlagsIntDecoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,92 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Decodes values encoded with {@link EightFlagsIntEncoder}.
- * 
- * @lucene.experimental
- */
-public class EightFlagsIntDecoder extends IntDecoder {
-
-  /*
-   * Holds all combinations of <i>indicator</i> for fast decoding (saves time
-   * on real-time bit manipulation)
-   */
-  private static final byte[][] DECODE_TABLE = new byte[256][8];
-
-  /** Generating all combinations of <i>indicator</i> into separate flags. */
-  static {
-    for (int i = 256; i != 0;) {
-      --i;
-      for (int j = 8; j != 0;) {
-        --j;
-        DECODE_TABLE[i][j] = (byte) ((i >>> j) & 0x1);
-      }
-    }
-  }
-
-  @Override
-  public void decode(BytesRef buf, IntsRef values) {
-    values.offset = values.length = 0;
-    int upto = buf.offset + buf.length;
-    int offset = buf.offset;
-    while (offset < upto) {
-      // read indicator
-      int indicator = buf.bytes[offset++] & 0xFF;
-      int ordinal = 0;
-
-      int capacityNeeded = values.length + 8;
-      if (values.ints.length < capacityNeeded) {
-        values.grow(capacityNeeded);
-      }
-
-      // process indicator, until we read 8 values, or end-of-buffer
-      while (ordinal != 8) {
-        if (DECODE_TABLE[indicator][ordinal++] == 0) {
-          if (offset == upto) { // end of buffer
-            return;
-          }
-          // it is better if the decoding is inlined like so, and not e.g.
-          // in a utility method
-          int value = 0;
-          while (true) {
-            byte b = buf.bytes[offset++];
-            if (b >= 0) {
-              values.ints[values.length++] = ((value << 7) | b) + 2;
-              break;
-            } else {
-              value = (value << 7) | (b & 0x7F);
-            }
-          }
-        } else {
-          values.ints[values.length++] = 1;
-        }
-      }
-    }
-  }
-
-  @Override
-  public String toString() {
-    return "EightFlags(VInt8)";
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/EightFlagsIntEncoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/EightFlagsIntEncoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/EightFlagsIntEncoder.java	2013-02-20 13:38:17.680711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/EightFlagsIntEncoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,96 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link ChunksIntEncoder} which encodes data in chunks of 8. Every group
- * starts with a single byte (called indicator) which represents 8 - 1 bit
- * flags, where the value:
- * <ul>
- * <li>1 means the encoded value is '1'
- * <li>0 means the value is encoded using {@link VInt8IntEncoder}, and the
- * encoded bytes follow the indicator.<br>
- * Since value 0 is illegal, and 1 is encoded in the indicator, the actual value
- * that is encoded is <code>value-2</code>, which saves some more bits.
- * </ul>
- * Encoding example:
- * <ul>
- * <li>Original values: 6, 16, 5, 9, 7, 1
- * <li>After sorting: 1, 5, 6, 7, 9, 16
- * <li>D-Gap computing: 1, 4, 1, 1, 2, 5 (so far - done by
- * {@link DGapIntEncoder})
- * <li>Encoding: 1,0,1,1,0,0,0,0 as the indicator, by 2 (4-2), 0 (2-2), 3 (5-2).
- * <li>Binary encode: <u>0 | 0 | 0 | 0 | 1 | 1 | 0 | 1</u> 00000010 00000000
- * 00000011 (indicator is <u>underlined</u>).<br>
- * <b>NOTE:</b> the order of the values in the indicator is lsb &rArr; msb,
- * which allows for more efficient decoding.
- * </ul>
- * 
- * @lucene.experimental
- */
-public class EightFlagsIntEncoder extends ChunksIntEncoder {
-
-  /*
-   * Holds all combinations of <i>indicator</i> flags for fast encoding (saves
-   * time on bit manipulation at encode time)
-   */
-  private static final byte[] ENCODE_TABLE = new byte[] { 0x1, 0x2, 0x4, 0x8, 0x10, 0x20, 0x40, (byte) 0x80 };
-
-  public EightFlagsIntEncoder() {
-    super(8);
-  }
-
-  @Override
-  public void encode(IntsRef values, BytesRef buf) {
-    buf.offset = buf.length = 0;
-    int upto = values.offset + values.length;
-    for (int i = values.offset; i < upto; i++) {
-      int value = values.ints[i];
-      if (value == 1) {
-        indicator |= ENCODE_TABLE[ordinal];
-      } else {
-        encodeQueue.ints[encodeQueue.length++] = value - 2;
-      }
-      ++ordinal;
-      
-      // encode the chunk and the indicator
-      if (ordinal == 8) {
-        encodeChunk(buf);
-      }
-    }
-    
-    // encode remaining values
-    if (ordinal != 0) {
-      encodeChunk(buf);
-    }
-  }
-
-  @Override
-  public IntDecoder createMatchingDecoder() {
-    return new EightFlagsIntDecoder();
-  }
-
-  @Override
-  public String toString() {
-    return "EightFlags(VInt)";
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/FourFlagsIntDecoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/FourFlagsIntDecoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/FourFlagsIntDecoder.java	2013-02-20 13:38:17.680711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/FourFlagsIntDecoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,92 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Decodes values encoded with {@link FourFlagsIntEncoder}.
- * 
- * @lucene.experimental
- */
-public class FourFlagsIntDecoder extends IntDecoder {
-
-  /**
-   * Holds all combinations of <i>indicator</i> for fast decoding (saves time
-   * on real-time bit manipulation)
-   */
-  private final static byte[][] DECODE_TABLE = new byte[256][4];
-
-  /** Generating all combinations of <i>indicator</i> into separate flags. */
-  static {
-    for (int i = 256; i != 0;) {
-      --i;
-      for (int j = 4; j != 0;) {
-        --j;
-        DECODE_TABLE[i][j] = (byte) ((i >>> (j << 1)) & 0x3);
-      }
-    }
-  }
-
-  @Override
-  public void decode(BytesRef buf, IntsRef values) {
-    values.offset = values.length = 0;
-    int upto = buf.offset + buf.length;
-    int offset = buf.offset;
-    while (offset < upto) {
-      // read indicator
-      int indicator = buf.bytes[offset++] & 0xFF;
-      int ordinal = 0;
-      
-      int capacityNeeded = values.length + 4;
-      if (values.ints.length < capacityNeeded) {
-        values.grow(capacityNeeded);
-      }
-      
-      while (ordinal != 4) {
-        byte decodeVal = DECODE_TABLE[indicator][ordinal++];
-        if (decodeVal == 0) {
-          if (offset == upto) { // end of buffer
-            return;
-          }
-          // it is better if the decoding is inlined like so, and not e.g.
-          // in a utility method
-          int value = 0;
-          while (true) {
-            byte b = buf.bytes[offset++];
-            if (b >= 0) {
-              values.ints[values.length++] = ((value << 7) | b) + 4;
-              break;
-            } else {
-              value = (value << 7) | (b & 0x7F);
-            }
-          }
-        } else {
-          values.ints[values.length++] = decodeVal;
-        }
-      }
-    }
-  }
-
-  @Override
-  public String toString() {
-    return "FourFlags(VInt)";
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/FourFlagsIntEncoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/FourFlagsIntEncoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/FourFlagsIntEncoder.java	2013-02-20 13:38:17.676711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/FourFlagsIntEncoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,102 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link ChunksIntEncoder} which encodes values in chunks of 4. Every group
- * starts with a single byte (called indicator) which represents 4 - 2 bit
- * flags, where the values:
- * <ul>
- * <li>1, 2 or 3 mean the encoded value is '1', '2' or '3' respectively.
- * <li>0 means the value is encoded using {@link VInt8IntEncoder}, and the
- * encoded bytes follow the indicator.<br>
- * Since value 0 is illegal, and 1-3 are encoded in the indicator, the actual
- * value that is encoded is <code>value-4</code>, which saves some more bits.
- * </ul>
- * Encoding example:
- * <ul>
- * <li>Original values: 6, 16, 5, 9, 7, 1, 11
- * <li>After sorting: 1, 5, 6, 7, 9, 11, 16
- * <li>D-Gap computing: 1, 4, 1, 1, 2, 5 (so far - done by
- * {@link DGapIntEncoder})
- * <li>Encoding: 1,0,1,1 as the first indicator, followed by 0 (4-4), than
- * 2,0,0,0 as the second indicator, followed by 1 (5-4) encoded with.
- * <li>Binary encode: <u>01 | 01 | 00 | 01</u> 00000000 <u>00 | 00 | 00 | 10</u>
- * 00000001 (indicators are <u>underlined</u>).<br>
- * <b>NOTE:</b> the order of the values in the indicator is lsb &rArr; msb,
- * which allows for more efficient decoding.
- * </ul>
- * 
- * @lucene.experimental
- */
-public class FourFlagsIntEncoder extends ChunksIntEncoder {
-
-  /*
-   * Holds all combinations of <i>indicator</i> flags for fast encoding (saves
-   * time on bit manipulation @ encode time)
-   */
-  private static final byte[][] ENCODE_TABLE = new byte[][] {
-    new byte[] { 0x00, 0x00, 0x00, 0x00 },
-    new byte[] { 0x01, 0x04, 0x10, 0x40 },
-    new byte[] { 0x02, 0x08, 0x20, (byte) 0x80 },
-    new byte[] { 0x03, 0x0C, 0x30, (byte) 0xC0 },
-  };
-
-  public FourFlagsIntEncoder() {
-    super(4);
-  }
-
-  @Override
-  public void encode(IntsRef values, BytesRef buf) {
-    buf.offset = buf.length = 0;
-    int upto = values.offset + values.length;
-    for (int i = values.offset; i < upto; i++) {
-      int value = values.ints[i];
-      if (value <= 3) {
-        indicator |= ENCODE_TABLE[value][ordinal];
-      } else {
-        encodeQueue.ints[encodeQueue.length++] = value - 4;
-      }
-      ++ordinal;
-      
-      // encode the chunk and the indicator
-      if (ordinal == 4) {
-        encodeChunk(buf);
-      }
-    }
-    
-    // encode remaining values
-    if (ordinal != 0) {
-      encodeChunk(buf);
-    }
-  }
-
-  @Override
-  public IntDecoder createMatchingDecoder() {
-    return new FourFlagsIntDecoder();
-  }
-
-  @Override
-  public String toString() {
-    return "FourFlags(VInt)";
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntDecoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntDecoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntDecoder.java	2013-02-20 13:38:17.680711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntDecoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,37 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Decodes integers from a set {@link BytesRef}.
- * 
- * @lucene.experimental
- */
-public abstract class IntDecoder {
-  
-  /**
-   * Decodes the values from the buffer into the given {@link IntsRef}. Note
-   * that {@code values.offset} is set to 0, and {@code values.length} is
-   * updated to denote the number of decoded values.
-   */
-  public abstract void decode(BytesRef buf, IntsRef values);
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntEncoderFilter.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntEncoderFilter.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntEncoderFilter.java	2013-02-20 13:38:17.676711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntEncoderFilter.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,34 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An abstract implementation of {@link IntEncoder} which wraps another encoder.
- * 
- * @lucene.experimental
- */
-public abstract class IntEncoderFilter extends IntEncoder {
-
-  protected final IntEncoder encoder;
-
-  protected IntEncoderFilter(IntEncoder encoder) {
-    this.encoder = encoder;
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntEncoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntEncoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntEncoder.java	2013-02-20 13:38:17.680711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntEncoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,46 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Encodes integers to a set {@link BytesRef}. For convenience, each encoder
- * implements {@link #createMatchingDecoder()} for easy access to the matching
- * decoder.
- * 
- * @lucene.experimental
- */
-public abstract class IntEncoder {
-
-  public IntEncoder() {}
-
-  /**
-   * Encodes the values to the given buffer. Note that the buffer's offset and
-   * length are set to 0.
-   */
-  public abstract void encode(IntsRef values, BytesRef buf);
-
-  /**
-   * Returns an {@link IntDecoder} which can decode the values that were encoded
-   * with this encoder.
-   */
-  public abstract IntDecoder createMatchingDecoder();
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/NOnesIntDecoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/NOnesIntDecoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/NOnesIntDecoder.java	2013-02-20 13:38:17.680711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/NOnesIntDecoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,86 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Decodes values encoded encoded with {@link NOnesIntEncoder}.
- * 
- * @lucene.experimental
- */
-public class NOnesIntDecoder extends FourFlagsIntDecoder {
-
-  // Number of consecutive '1's to generate upon decoding a '2'
-  private final int n;
-  private final IntsRef internalBuffer;
-  
-  /**
-   * Constructs a decoder with a given N (Number of consecutive '1's which are
-   * translated into a single target value '2'.
-   */
-  public NOnesIntDecoder(int n) {
-    this.n = n;
-    // initial size (room for 100 integers)
-    internalBuffer = new IntsRef(100);
-  }
-
-  @Override
-  public void decode(BytesRef buf, IntsRef values) {
-    values.offset = values.length = 0;
-    internalBuffer.length = 0;
-    super.decode(buf, internalBuffer);
-    if (values.ints.length < internalBuffer.length) {
-      // need space for internalBuffer.length to internalBuffer.length*N,
-      // grow mildly at first
-      values.grow(internalBuffer.length * n/2);
-    }
-    
-    for (int i = 0; i < internalBuffer.length; i++) {
-      int decode = internalBuffer.ints[i];
-      if (decode == 1) {
-        if (values.length == values.ints.length) {
-          values.grow(values.length + 10); // grow by few items, however not too many
-        }
-        // 1 is 1
-        values.ints[values.length++] = 1;
-      } else if (decode == 2) {
-        if (values.length + n >= values.ints.length) {
-          values.grow(values.length + n); // grow by few items, however not too many
-        }
-        // '2' means N 1's
-        for (int j = 0; j < n; j++) {
-          values.ints[values.length++] = 1;
-        }
-      } else {
-        if (values.length == values.ints.length) {
-          values.grow(values.length + 10); // grow by few items, however not too many
-        }
-        // any other value is val-1
-        values.ints[values.length++] = decode - 1;
-      }
-    }
-  }
-
-  @Override
-  public String toString() {
-    return "NOnes(" + n + ") (" + super.toString() + ")";
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/NOnesIntEncoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/NOnesIntEncoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/NOnesIntEncoder.java	2013-02-20 13:38:17.680711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/NOnesIntEncoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,114 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A variation of {@link FourFlagsIntEncoder} which translates the data as
- * follows:
- * <ul>
- * <li>Values &ge; 2 are trnalsated to <code>value+1</code> (2 &rArr; 3, 3
- * &rArr; 4 and so forth).
- * <li>Any <code>N</code> occurrences of 1 are encoded as a single 2.
- * <li>Otherwise, each 1 is encoded as 1.
- * </ul>
- * <p>
- * Encoding examples:
- * <ul>
- * <li>N = 4: the data 1,1,1,1,1 is translated to: 2, 1
- * <li>N = 3: the data 1,2,3,4,1,1,1,1,5 is translated to 1,3,4,5,2,1,6
- * </ul>
- * <b>NOTE:</b> this encoder does not support values &le; 0 and
- * {@link Integer#MAX_VALUE}. 0 is not supported because it's not supported by
- * {@link FourFlagsIntEncoder} and {@link Integer#MAX_VALUE} because this
- * encoder translates N to N+1, which will cause an overflow and
- * {@link Integer#MAX_VALUE} will become a negative number, which is not
- * supported as well.<br>
- * This does not mean you cannot encode {@link Integer#MAX_VALUE}. If it is not
- * the first value to encode, and you wrap this encoder with
- * {@link DGapIntEncoder}, then the value that will be sent to this encoder will
- * be <code>MAX_VAL - prev</code>.
- * 
- * @lucene.experimental
- */
-public class NOnesIntEncoder extends FourFlagsIntEncoder {
-
-  private final IntsRef internalBuffer;
-  
-  /** Number of consecutive '1's to be translated into single target value '2'. */
-  private final int n;
-
-  /**
-   * Constructs an encoder with a given value of N (N: Number of consecutive
-   * '1's to be translated into single target value '2').
-   */
-  public NOnesIntEncoder(int n) {
-    this.n = n;
-    internalBuffer = new IntsRef(n);
-  }
-
-  @Override
-  public void encode(IntsRef values, BytesRef buf) {
-    internalBuffer.length = 0;
-    // make sure the internal buffer is large enough
-    if (values.length > internalBuffer.ints.length) {
-      internalBuffer.grow(values.length);
-    }
-    
-    int onesCounter = 0;
-    int upto = values.offset + values.length;
-    for (int i = values.offset; i < upto; i++) {
-      int value = values.ints[i];
-      if (value == 1) {
-        // every N 1's should be encoded as '2'
-        if (++onesCounter == n) {
-          internalBuffer.ints[internalBuffer.length++] = 2;
-          onesCounter = 0;
-        }
-      } else {
-        // there might have been 1's that we need to encode
-        while (onesCounter > 0) {
-          --onesCounter;
-          internalBuffer.ints[internalBuffer.length++] = 1;
-        }
-        
-        // encode value as value+1
-        internalBuffer.ints[internalBuffer.length++] = value + 1;
-      }
-    }
-    // there might have been 1's that we need to encode
-    while (onesCounter > 0) {
-      --onesCounter;
-      internalBuffer.ints[internalBuffer.length++] = 1;
-    }
-    super.encode(internalBuffer, buf);
-  }
-
-  @Override
-  public IntDecoder createMatchingDecoder() {
-    return new NOnesIntDecoder(n);
-  }
-
-  @Override
-  public String toString() {
-    return "NOnes(" + n + ") (" + super.toString() + ")";
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/package.html simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/package.html
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/package.html	2013-02-20 13:38:17.680711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/package.html	1969-12-31 19:00:00.000000000 -0500
@@ -1,24 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Facets Encoding</title>
-</head>
-<body>
-Offers various encoders and decoders for category ordinals.
-</body>
-</html>
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/SimpleIntDecoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/SimpleIntDecoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/SimpleIntDecoder.java	2013-02-20 13:38:17.680711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/SimpleIntDecoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,56 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Decodes values encoded with {@link SimpleIntEncoder}.
- * 
- * @lucene.experimental
- */
-public final class SimpleIntDecoder extends IntDecoder {
-
-  @Override
-  public void decode(BytesRef buf, IntsRef values) {
-    values.offset = values.length = 0;
-    int numValues = buf.length / 4; // every value is 4 bytes
-    if (values.ints.length < numValues) { // offset and length are 0
-      values.ints = new int[ArrayUtil.oversize(numValues, RamUsageEstimator.NUM_BYTES_INT)];
-    }
-    
-    int offset = buf.offset;
-    int upto = buf.offset + buf.length;
-    while (offset < upto) {
-      values.ints[values.length++] = 
-          ((buf.bytes[offset++] & 0xFF) << 24) | 
-          ((buf.bytes[offset++] & 0xFF) << 16) | 
-          ((buf.bytes[offset++] & 0xFF) <<  8) | 
-          (buf.bytes[offset++] & 0xFF);
-    }
-  }
-
-  @Override
-  public String toString() {
-    return "Simple";
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/SimpleIntEncoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/SimpleIntEncoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/SimpleIntEncoder.java	2013-02-20 13:38:17.676711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/SimpleIntEncoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,59 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A simple {@link IntEncoder}, writing an integer as 4 raw bytes. *
- * 
- * @lucene.experimental
- */
-public final class SimpleIntEncoder extends IntEncoder {
-
-  @Override
-  public void encode(IntsRef values, BytesRef buf) {
-    buf.offset = buf.length = 0;
-    // ensure there's enough room in the buffer
-    int bytesNeeded = values.length * 4;
-    if (buf.bytes.length < bytesNeeded) {
-      buf.grow(bytesNeeded);
-    }
-    
-    int upto = values.offset + values.length;
-    for (int i = values.offset; i < upto; i++) {
-      int value = values.ints[i];
-      buf.bytes[buf.length++] = (byte) (value >>> 24);
-      buf.bytes[buf.length++] = (byte) ((value >> 16) & 0xFF);
-      buf.bytes[buf.length++] = (byte) ((value >> 8) & 0xFF);
-      buf.bytes[buf.length++] = (byte) (value & 0xFF);
-    }
-  }
-
-  @Override
-  public IntDecoder createMatchingDecoder() {
-    return new SimpleIntDecoder();
-  }
-
-  @Override
-  public String toString() {
-    return "Simple";
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/SortingIntEncoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/SortingIntEncoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/SortingIntEncoder.java	2013-02-20 13:38:17.676711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/SortingIntEncoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,54 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import java.util.Arrays;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link IntEncoderFilter} which sorts the values to encode in ascending
- * order before encoding them.
- * 
- * @lucene.experimental
- */
-public final class SortingIntEncoder extends IntEncoderFilter {
-
-  /** Initializes with the given encoder. */
-  public SortingIntEncoder(IntEncoder encoder) {
-    super(encoder);
-  }
-
-  @Override
-  public void encode(IntsRef values, BytesRef buf) {
-    Arrays.sort(values.ints, values.offset, values.offset + values.length);
-    encoder.encode(values, buf);
-  }
-
-  @Override
-  public IntDecoder createMatchingDecoder() {
-    return encoder.createMatchingDecoder();
-  }
-  
-  @Override
-  public String toString() {
-    return "Sorting(" + encoder.toString() + ")";
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/UniqueValuesIntEncoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/UniqueValuesIntEncoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/UniqueValuesIntEncoder.java	2013-02-20 13:38:17.676711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/UniqueValuesIntEncoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,63 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link IntEncoderFilter} which ensures only unique values are encoded. The
- * implementation assumes the values given to {@link #encode(IntsRef, BytesRef)} are sorted.
- * If this is not the case, you can chain this encoder with
- * {@link SortingIntEncoder}.
- * 
- * @lucene.experimental
- */
-public final class UniqueValuesIntEncoder extends IntEncoderFilter {
-
-  /** Constructs a new instance with the given encoder. */
-  public UniqueValuesIntEncoder(IntEncoder encoder) {
-    super(encoder);
-  }
-
-  @Override
-  public void encode(IntsRef values, BytesRef buf) {
-    int prev = values.ints[values.offset];
-    int idx = values.offset + 1;
-    int upto = values.offset + values.length;
-    for (int i = idx; i < upto; i++) {
-      if (values.ints[i] != prev) {
-        values.ints[idx++] = values.ints[i];
-        prev = values.ints[i];
-      }
-    }
-    values.length = idx - values.offset;
-    encoder.encode(values, buf);
-  }
-
-  @Override
-  public IntDecoder createMatchingDecoder() {
-    return encoder.createMatchingDecoder();
-  }
-  
-  @Override
-  public String toString() {
-    return "Unique(" + encoder.toString() + ")";
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/VInt8IntDecoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/VInt8IntDecoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/VInt8IntDecoder.java	2013-02-20 13:38:17.680711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/VInt8IntDecoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,64 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Decodes values encoded by {@link VInt8IntEncoder}.
- * 
- * @lucene.experimental
- */
-public final class VInt8IntDecoder extends IntDecoder {
-
-  @Override
-  public void decode(BytesRef buf, IntsRef values) {
-    values.offset = values.length = 0;
-
-    // grow the buffer up front, even if by a large number of values (buf.length)
-    // that saves the need to check inside the loop for every decoded value if
-    // the buffer needs to grow.
-    if (values.ints.length < buf.length) {
-      values.ints = new int[ArrayUtil.oversize(buf.length, RamUsageEstimator.NUM_BYTES_INT)];
-    }
-
-    // it is better if the decoding is inlined like so, and not e.g.
-    // in a utility method
-    int upto = buf.offset + buf.length;
-    int value = 0;
-    int offset = buf.offset;
-    while (offset < upto) {
-      byte b = buf.bytes[offset++];
-      if (b >= 0) {
-        values.ints[values.length++] = (value << 7) | b;
-        value = 0;
-      } else {
-        value = (value << 7) | (b & 0x7F);
-      }
-    }
-  }
-
-  @Override
-  public String toString() {
-    return "VInt8";
-  }
-
-} 


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/VInt8IntEncoder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/VInt8IntEncoder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/encoding/VInt8IntEncoder.java	2013-02-20 13:38:17.680711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/encoding/VInt8IntEncoder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,104 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link IntEncoder} which implements variable length encoding. A number is
- * encoded as follows:
- * <ul>
- * <li>If it is less than 127 and non-negative, i.e. uses only 7 bits, it is
- * encoded as a single byte: 0bbbbbbb.
- * <li>If it occupies more than 7 bits, it is represented as a series of bytes,
- * each byte carrying 7 bits. All but the last byte have the MSB set, the last
- * one has it unset.
- * </ul>
- * Example:
- * <ol>
- * <li>n = 117 = 01110101: This has less than 8 significant bits, therefore is
- * encoded as 01110101 = 0x75.
- * <li>n = 100000 = (binary) 11000011010100000. This has 17 significant bits,
- * thus needs three Vint8 bytes. Pad it to a multiple of 7 bits, then split it
- * into chunks of 7 and add an MSB, 0 for the last byte, 1 for the others:
- * 1|0000110 1|0001101 0|0100000 = 0x86 0x8D 0x20.
- * </ol>
- * <b>NOTE:</b> although this encoder is not limited to values &ge; 0, it is not
- * recommended for use with negative values, as their encoding will result in 5
- * bytes written to the output stream, rather than 4. For such values, either
- * use {@link SimpleIntEncoder} or write your own version of variable length
- * encoding, which can better handle negative values.
- * 
- * @lucene.experimental
- */
-public final class VInt8IntEncoder extends IntEncoder {
-
-  @Override
-  public void encode(IntsRef values, BytesRef buf) {
-    buf.offset = buf.length = 0;
-    int maxBytesNeeded = 5 * values.length; // at most 5 bytes per VInt
-    if (buf.bytes.length < maxBytesNeeded) {
-      buf.grow(maxBytesNeeded);
-    }
-    
-    int upto = values.offset + values.length;
-    for (int i = values.offset; i < upto; i++) {
-      // it is better if the encoding is inlined like so, and not e.g.
-      // in a utility method
-      int value = values.ints[i];
-      if ((value & ~0x7F) == 0) {
-        buf.bytes[buf.length] = (byte) value;
-        buf.length++;
-      } else if ((value & ~0x3FFF) == 0) {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 1] = (byte) (value & 0x7F);
-        buf.length += 2;
-      } else if ((value & ~0x1FFFFF) == 0) {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
-        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 2] = (byte) (value & 0x7F);
-        buf.length += 3;
-      } else if ((value & ~0xFFFFFFF) == 0) {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0xFE00000) >> 21));
-        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
-        buf.bytes[buf.length + 2] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 3] = (byte) (value & 0x7F);
-        buf.length += 4;
-      } else {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0xF0000000) >> 28));
-        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0xFE00000) >> 21));
-        buf.bytes[buf.length + 2] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
-        buf.bytes[buf.length + 3] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 4] = (byte) (value & 0x7F);
-        buf.length += 5;
-      }
-    }
-  }
-
-  @Override
-  public IntDecoder createMatchingDecoder() {
-    return new VInt8IntDecoder();
-  }
-
-  @Override
-  public String toString() {
-    return "VInt8";
-  }
-
-} 


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/FacetField.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/FacetField.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/FacetField.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/FacetField.java	2013-11-27 18:49:35.303948456 -0500
@@ -0,0 +1,56 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.document.Document; // javadoc
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+
+/** Add an instance of this to your {@link Document} for
+ *  every facet label. */
+public class FacetField extends Field {
+  static final FieldType TYPE = new FieldType();
+  static {
+    TYPE.setIndexed(true);
+    TYPE.freeze();
+  }
+
+  /** Dimension for this field. */
+  public final String dim;
+
+  /** Path for this field. */
+  public final String[] path;
+
+  /** Creates the this from {@code dim} and
+   *  {@code path}. */
+  public FacetField(String dim, String... path) {
+    super("dummy", TYPE);
+    this.dim = dim;
+    if (path.length == 0) {
+      throw new IllegalArgumentException("path must have at least one element");
+    }
+    this.path = path;
+  }
+
+  @Override
+  public String toString() {
+    return "FacetField(dim=" + dim + " path=" + Arrays.toString(path) + ")";
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/FacetResult.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/FacetResult.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/FacetResult.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/FacetResult.java	2013-12-02 10:39:54.507438095 -0500
@@ -0,0 +1,86 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+public final class FacetResult {
+
+  /** Dimension that was requested. */
+  public final String dim;
+
+  /** Path whose children were requested. */
+  public final String[] path;
+
+  /** Total value for this path (sum of all child counts, or
+   *  sum of all child values), even those not included in
+   *  the topN. */
+  public final Number value;
+
+  /** How many child labels were encountered. */
+  public final int childCount;
+
+  /** Child counts. */
+  public final LabelAndValue[] labelValues;
+
+  public FacetResult(String dim, String[] path, Number value, LabelAndValue[] labelValues, int childCount) {
+    this.dim = dim;
+    this.path = path;
+    this.value = value;
+    this.labelValues = labelValues;
+    this.childCount = childCount;
+  }
+
+  @Override
+  public String toString() {
+    StringBuilder sb = new StringBuilder();
+    sb.append("dim=");
+    sb.append(dim);
+    sb.append(" path=");
+    sb.append(Arrays.toString(path));
+    sb.append(" value=");
+    sb.append(value);
+    sb.append(" childCount=");
+    sb.append(childCount);
+    sb.append('\n');
+    for(LabelAndValue labelValue : labelValues) {
+      sb.append("  " + labelValue + "\n");
+    }
+    return sb.toString();
+  }
+
+  @Override
+  public boolean equals(Object _other) {
+    if ((_other instanceof FacetResult) == false) {
+      return false;
+    }
+    FacetResult other = (FacetResult) _other;
+    return value.equals(other.value) &&
+      childCount == other.childCount &&
+      Arrays.equals(labelValues, other.labelValues);
+  }
+
+  @Override
+  public int hashCode() {
+    int hashCode = value.hashCode() + 31 * childCount;
+    for(LabelAndValue labelValue : labelValues) {
+      hashCode = labelValue.hashCode() + 31 * hashCode;
+    }
+    return hashCode;
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/FacetsCollector.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/FacetsCollector.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/FacetsCollector.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/FacetsCollector.java	2013-11-27 18:58:50.003932283 -0500
@@ -0,0 +1,250 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.FieldDoc;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.FilteredQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MultiCollector;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.TopDocsCollector;
+import org.apache.lucene.search.TopFieldCollector;
+import org.apache.lucene.search.TopFieldDocs;
+import org.apache.lucene.search.TopScoreDocCollector;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.FixedBitSet;
+
+/** Collects hits for subsequent faceting.  Once you've run
+ *  a search and collect hits into this, instantiate one of
+ *  the {@link Facets} subclasses to do the facet
+ *  counting.  Use the {@code search} utility methods to
+ *  perform an "ordinary" search but also collect into a
+ *  {@link Collector}. */
+public final class FacetsCollector extends Collector {
+
+  private AtomicReaderContext context;
+  private Scorer scorer;
+  private FixedBitSet bits;
+  private int totalHits;
+  private float[] scores;
+  private final boolean keepScores;
+  private final List<MatchingDocs> matchingDocs = new ArrayList<MatchingDocs>();
+  
+  /**
+   * Holds the documents that were matched in the {@link AtomicReaderContext}.
+   * If scores were required, then {@code scores} is not null.
+   */
+  public final static class MatchingDocs {
+    
+    public final AtomicReaderContext context;
+    public final FixedBitSet bits;
+    public final float[] scores;
+    public final int totalHits;
+    
+    public MatchingDocs(AtomicReaderContext context, FixedBitSet bits, int totalHits, float[] scores) {
+      this.context = context;
+      this.bits = bits;
+      this.scores = scores;
+      this.totalHits = totalHits;
+    }
+  }
+
+  public FacetsCollector() {
+    this(false);
+  }
+
+  public FacetsCollector(boolean keepScores) {
+    this.keepScores = keepScores;
+  }
+
+  public boolean getKeepScores() {
+    return keepScores;
+  }
+  
+  /**
+   * Returns the documents matched by the query, one {@link MatchingDocs} per
+   * visited segment.
+   */
+  public List<MatchingDocs> getMatchingDocs() {
+    if (bits != null) {
+      matchingDocs.add(new MatchingDocs(this.context, bits, totalHits, scores));
+      bits = null;
+      scores = null;
+      context = null;
+    }
+
+    return matchingDocs;
+  }
+    
+  @Override
+  public final boolean acceptsDocsOutOfOrder() {
+    // If we are keeping scores then we require in-order
+    // because we append each score to the float[] and
+    // expect that they correlate in order to the hits:
+    return keepScores == false;
+  }
+
+  @Override
+  public final void collect(int doc) throws IOException {
+    bits.set(doc);
+    if (keepScores) {
+      if (totalHits >= scores.length) {
+        float[] newScores = new float[ArrayUtil.oversize(totalHits + 1, 4)];
+        System.arraycopy(scores, 0, newScores, 0, totalHits);
+        scores = newScores;
+      }
+      scores[totalHits] = scorer.score();
+    }
+    totalHits++;
+  }
+
+  @Override
+  public final void setScorer(Scorer scorer) throws IOException {
+    this.scorer = scorer;
+  }
+    
+  @Override
+  public final void setNextReader(AtomicReaderContext context) throws IOException {
+    if (bits != null) {
+      matchingDocs.add(new MatchingDocs(this.context, bits, totalHits, scores));
+    }
+    bits = new FixedBitSet(context.reader().maxDoc());
+    totalHits = 0;
+    if (keepScores) {
+      scores = new float[64]; // some initial size
+    }
+    this.context = context;
+  }
+
+  /** Utility method, to search and also collect all hits
+   *  into the provided {@link Collector}. */
+  public static TopDocs search(IndexSearcher searcher, Query q, int n, Collector fc) throws IOException {
+    return doSearch(searcher, null, q, null, n, null, false, false, fc);
+  }
+
+  /** Utility method, to search and also collect all hits
+   *  into the provided {@link Collector}. */
+  public static TopDocs search(IndexSearcher searcher, Query q, Filter filter, int n, Collector fc) throws IOException {
+    return doSearch(searcher, null, q, filter, n, null, false, false, fc);
+  }
+
+  /** Utility method, to search and also collect all hits
+   *  into the provided {@link Collector}. */
+  public static TopFieldDocs search(IndexSearcher searcher, Query q, Filter filter, int n, Sort sort, Collector fc) throws IOException {
+    if (sort == null) {
+      throw new IllegalArgumentException("sort must not be null");
+    }
+    return (TopFieldDocs) doSearch(searcher, null, q, filter, n, sort, false, false, fc);
+  }
+
+  /** Utility method, to search and also collect all hits
+   *  into the provided {@link Collector}. */
+  public static TopFieldDocs search(IndexSearcher searcher, Query q, Filter filter, int n, Sort sort, boolean doDocScores, boolean doMaxScore, Collector fc) throws IOException {
+    if (sort == null) {
+      throw new IllegalArgumentException("sort must not be null");
+    }
+    return (TopFieldDocs) doSearch(searcher, null, q, filter, n, sort, doDocScores, doMaxScore, fc);
+  }
+
+  /** Utility method, to search and also collect all hits
+   *  into the provided {@link Collector}. */
+  public TopDocs searchAfter(IndexSearcher searcher, ScoreDoc after, Query q, int n, Collector fc) throws IOException {
+    return doSearch(searcher, after, q, null, n, null, false, false, fc);
+  }
+
+  /** Utility method, to search and also collect all hits
+   *  into the provided {@link Collector}. */
+  public static TopDocs searchAfter(IndexSearcher searcher, ScoreDoc after, Query q, Filter filter, int n, Collector fc) throws IOException {
+    return doSearch(searcher, after, q, filter, n, null, false, false, fc);
+  }
+
+  /** Utility method, to search and also collect all hits
+   *  into the provided {@link Collector}. */
+  public static TopDocs searchAfter(IndexSearcher searcher, ScoreDoc after, Query q, Filter filter, int n, Sort sort, Collector fc) throws IOException {
+    if (sort == null) {
+      throw new IllegalArgumentException("sort must not be null");
+    }
+    return (TopFieldDocs) doSearch(searcher, after, q, filter, n, sort, false, false, fc);
+  }
+
+  /** Utility method, to search and also collect all hits
+   *  into the provided {@link Collector}. */
+  public static TopDocs searchAfter(IndexSearcher searcher, ScoreDoc after, Query q, Filter filter, int n, Sort sort, boolean doDocScores, boolean doMaxScore, Collector fc) throws IOException {
+    if (sort == null) {
+      throw new IllegalArgumentException("sort must not be null");
+    }
+    return (TopFieldDocs) doSearch(searcher, after, q, filter, n, sort, doDocScores, doMaxScore, fc);
+  }
+
+  private static TopDocs doSearch(IndexSearcher searcher, ScoreDoc after, Query q, Filter filter, int n, Sort sort,
+                                  boolean doDocScores, boolean doMaxScore, Collector fc) throws IOException {
+
+    if (filter != null) {
+      q = new FilteredQuery(q, filter);
+    }
+
+    int limit = searcher.getIndexReader().maxDoc();
+    if (limit == 0) {
+      limit = 1;
+    }
+    n = Math.min(n, limit);
+
+    if (after != null && after.doc >= limit) {
+      throw new IllegalArgumentException("after.doc exceeds the number of documents in the reader: after.doc="
+                                         + after.doc + " limit=" + limit);
+    }
+
+    TopDocsCollector<?> hitsCollector;
+    if (sort != null) {
+      if (after != null && !(after instanceof FieldDoc)) {
+        // TODO: if we fix type safety of TopFieldDocs we can
+        // remove this
+        throw new IllegalArgumentException("after must be a FieldDoc; got " + after);
+      }
+      boolean fillFields = true;
+      hitsCollector = TopFieldCollector.create(sort, n,
+                                               (FieldDoc) after,
+                                               fillFields,
+                                               doDocScores,
+                                               doMaxScore,
+                                               false);
+    } else {
+      // TODO: can we pass the right boolean for
+      // in-order instead of hardwired to false...?  we'd
+      // need access to the protected IS.search methods
+      // taking Weight... could use reflection...
+      hitsCollector = TopScoreDocCollector.create(n, after, false);
+    }
+    searcher.search(q, MultiCollector.wrap(hitsCollector, fc));
+    return hitsCollector.topDocs();
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/FacetsConfig.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/FacetsConfig.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/FacetsConfig.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/FacetsConfig.java	2013-12-18 20:03:31.241552844 -0500
@@ -0,0 +1,531 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.ConcurrentHashMap;
+
+import org.apache.lucene.document.BinaryDocValuesField;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.SortedSetDocValuesField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.index.IndexDocument;
+import org.apache.lucene.index.IndexableField;
+import org.apache.lucene.index.IndexableFieldType;
+import org.apache.lucene.index.StorableField;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/** Records per-dimension configuration.  By default a
+ *  dimension is flat, single valued and does
+ *  not require count for the dimension; use
+ *  the setters in this class to change these settings for
+ *  each dim.
+ *
+ *  <p><b>NOTE</b>: this configuration is not saved into the
+ *  index, but it's vital, and up to the application to
+ *  ensure, that at search time the provided {@code
+ *  FacetsConfig} matches what was used during indexing.
+ *
+ *  @lucene.experimental */
+public class FacetsConfig {
+
+  public static final String DEFAULT_INDEX_FIELD_NAME = "$facets";
+
+  private final Map<String,DimConfig> fieldTypes = new ConcurrentHashMap<String,DimConfig>();
+
+  // Used only for best-effort detection of app mixing
+  // int/float/bytes in a single indexed field:
+  private final Map<String,String> assocDimTypes = new ConcurrentHashMap<String,String>();
+
+  /** Holds the configuration for one dimension
+   *
+   * @lucene.experimental */
+  public static final class DimConfig {
+    /** True if this dimension is hierarchical. */
+    public boolean hierarchical;
+
+    /** True if this dimension is multi-valued. */
+    public boolean multiValued;
+
+    /** True if the count/aggregate for the entire dimension
+     *  is required, which is unusual (default is false). */
+    public boolean requireDimCount;
+
+    /** Actual field where this dimension's facet labels
+     *  should be indexed */
+    public String indexFieldName = DEFAULT_INDEX_FIELD_NAME;
+  }
+
+  /** Default per-dimension configuration. */
+  public final static DimConfig DEFAULT_DIM_CONFIG = new DimConfig();
+
+  /** Default constructor. */
+  public FacetsConfig() {
+  }
+
+  /** Get the current configuration for a dimension. */
+  public synchronized DimConfig getDimConfig(String dimName) {
+    DimConfig ft = fieldTypes.get(dimName);
+    if (ft == null) {
+      ft = DEFAULT_DIM_CONFIG;
+    }
+    return ft;
+  }
+
+  /** Pass {@code true} if this dimension is hierarchical
+   *  (has depth > 1 paths). */
+  public synchronized void setHierarchical(String dimName, boolean v) {
+    DimConfig ft = fieldTypes.get(dimName);
+    if (ft == null) {
+      ft = new DimConfig();
+      fieldTypes.put(dimName, ft);
+    }
+    ft.hierarchical = v;
+  }
+
+  /** Pass {@code true} if this dimension may have more than
+   *  one value per document. */
+  public synchronized void setMultiValued(String dimName, boolean v) {
+    DimConfig ft = fieldTypes.get(dimName);
+    if (ft == null) {
+      ft = new DimConfig();
+      fieldTypes.put(dimName, ft);
+    }
+    ft.multiValued = v;
+  }
+
+  /** Pass {@code true} if at search time you require
+   *  accurate counts of the dimension, i.e. how many
+   *  hits have this dimension. */
+  public synchronized void setRequireDimCount(String dimName, boolean v) {
+    DimConfig ft = fieldTypes.get(dimName);
+    if (ft == null) {
+      ft = new DimConfig();
+      fieldTypes.put(dimName, ft);
+    }
+    ft.requireDimCount = v;
+  }
+
+  /** Specify which index field name should hold the
+   *  ordinals for this dimension; this is only used by the
+   *  taxonomy based facet methods. */
+  public synchronized void setIndexFieldName(String dimName, String indexFieldName) {
+    DimConfig ft = fieldTypes.get(dimName);
+    if (ft == null) {
+      ft = new DimConfig();
+      fieldTypes.put(dimName, ft);
+    }
+    ft.indexFieldName = indexFieldName;
+  }
+
+  public Map<String,DimConfig> getDimConfigs() {
+    return fieldTypes;
+  }
+
+  private static void checkSeen(Set<String> seenDims, String dim) {
+    if (seenDims.contains(dim)) {
+      throw new IllegalArgumentException("dimension \"" + dim + "\" is not multiValued, but it appears more than once in this document");
+    }
+    seenDims.add(dim);
+  }
+
+  /** Translates any added {@link FacetField}s into normal
+   *  fields for indexing; only use this version if you
+   *  did not add any taxonomy-based fields ({@link
+   *  FacetField} or {@link AssociationFacetField}) */
+  public IndexDocument build(IndexDocument doc) throws IOException {
+    return build(null, doc);
+  }
+
+  /** Translates any added {@link FacetField}s into normal
+   *  fields for indexing. */
+  public IndexDocument build(TaxonomyWriter taxoWriter, IndexDocument doc) throws IOException {
+    // Find all FacetFields, collated by the actual field:
+    Map<String,List<FacetField>> byField = new HashMap<String,List<FacetField>>();
+
+    // ... and also all SortedSetDocValuesFacetFields:
+    Map<String,List<SortedSetDocValuesFacetField>> dvByField = new HashMap<String,List<SortedSetDocValuesFacetField>>();
+
+    // ... and also all AssociationFacetFields
+    Map<String,List<AssociationFacetField>> assocByField = new HashMap<String,List<AssociationFacetField>>();
+
+    Set<String> seenDims = new HashSet<String>();
+
+    for(IndexableField field : doc.indexableFields()) {
+      if (field.fieldType() == FacetField.TYPE) {
+        FacetField facetField = (FacetField) field;
+        FacetsConfig.DimConfig dimConfig = getDimConfig(facetField.dim);
+        if (dimConfig.multiValued == false) {
+          checkSeen(seenDims, facetField.dim);
+        }
+        String indexFieldName = dimConfig.indexFieldName;
+        List<FacetField> fields = byField.get(indexFieldName);
+        if (fields == null) {
+          fields = new ArrayList<FacetField>();
+          byField.put(indexFieldName, fields);
+        }
+        fields.add(facetField);
+      }
+
+      if (field.fieldType() == SortedSetDocValuesFacetField.TYPE) {
+        SortedSetDocValuesFacetField facetField = (SortedSetDocValuesFacetField) field;
+        FacetsConfig.DimConfig dimConfig = getDimConfig(facetField.dim);
+        if (dimConfig.multiValued == false) {
+          checkSeen(seenDims, facetField.dim);
+        }
+        String indexFieldName = dimConfig.indexFieldName;
+        List<SortedSetDocValuesFacetField> fields = dvByField.get(indexFieldName);
+        if (fields == null) {
+          fields = new ArrayList<SortedSetDocValuesFacetField>();
+          dvByField.put(indexFieldName, fields);
+        }
+        fields.add(facetField);
+      }
+
+      if (field.fieldType() == AssociationFacetField.TYPE) {
+        AssociationFacetField facetField = (AssociationFacetField) field;
+        FacetsConfig.DimConfig dimConfig = getDimConfig(facetField.dim);
+        if (dimConfig.multiValued == false) {
+          checkSeen(seenDims, facetField.dim);
+        }
+        if (dimConfig.hierarchical) {
+          throw new IllegalArgumentException("AssociationFacetField cannot be hierarchical (dim=\"" + facetField.dim + "\")");
+        }
+        if (dimConfig.requireDimCount) {
+          throw new IllegalArgumentException("AssociationFacetField cannot requireDimCount (dim=\"" + facetField.dim + "\")");
+        }
+
+        String indexFieldName = dimConfig.indexFieldName;
+        List<AssociationFacetField> fields = assocByField.get(indexFieldName);
+        if (fields == null) {
+          fields = new ArrayList<AssociationFacetField>();
+          assocByField.put(indexFieldName, fields);
+        }
+        fields.add(facetField);
+
+        // Best effort: detect mis-matched types in same
+        // indexed field:
+        String type;
+        if (facetField instanceof IntAssociationFacetField) {
+          type = "int";
+        } else if (facetField instanceof FloatAssociationFacetField) {
+          type = "float";
+        } else {
+          type = "bytes";
+        }
+        // NOTE: not thread safe, but this is just best effort:
+        String curType = assocDimTypes.get(indexFieldName);
+        if (curType == null) {
+          assocDimTypes.put(indexFieldName, type);
+        } else if (!curType.equals(type)) {
+          throw new IllegalArgumentException("mixing incompatible types of AssocationFacetField (" + curType + " and " + type + ") in indexed field \"" + indexFieldName + "\"; use FacetsConfig to change the indexFieldName for each dimension");
+        }
+      }
+    }
+
+    List<Field> addedIndexedFields = new ArrayList<Field>();
+    List<Field> addedStoredFields = new ArrayList<Field>();
+
+    processFacetFields(taxoWriter, byField, addedIndexedFields, addedStoredFields);
+    processSSDVFacetFields(dvByField, addedIndexedFields, addedStoredFields);
+    processAssocFacetFields(taxoWriter, assocByField, addedIndexedFields, addedStoredFields);
+
+    //System.out.println("add stored: " + addedStoredFields);
+
+    final List<IndexableField> allIndexedFields = new ArrayList<IndexableField>();
+    for(IndexableField field : doc.indexableFields()) {
+      IndexableFieldType ft = field.fieldType();
+      if (ft != FacetField.TYPE && ft != SortedSetDocValuesFacetField.TYPE && ft != AssociationFacetField.TYPE) {
+        allIndexedFields.add(field);
+      }
+    }
+    allIndexedFields.addAll(addedIndexedFields);
+
+    final List<StorableField> allStoredFields = new ArrayList<StorableField>();
+    for(StorableField field : doc.storableFields()) {
+      allStoredFields.add(field);
+    }
+    allStoredFields.addAll(addedStoredFields);
+
+    //System.out.println("all indexed: " + allIndexedFields);
+    //System.out.println("all stored: " + allStoredFields);
+
+    return new IndexDocument() {
+        @Override
+        public Iterable<IndexableField> indexableFields() {
+          return allIndexedFields;
+        }
+
+        @Override
+        public Iterable<StorableField> storableFields() {
+          return allStoredFields;
+        }
+      };
+  }
+
+  private void processFacetFields(TaxonomyWriter taxoWriter, Map<String,List<FacetField>> byField, List<Field> addedIndexedFields, List<Field> addedStoredFields) throws IOException {
+
+    for(Map.Entry<String,List<FacetField>> ent : byField.entrySet()) {
+
+      String indexFieldName = ent.getKey();
+      //System.out.println("  indexFieldName=" + indexFieldName + " fields=" + ent.getValue());
+
+      IntsRef ordinals = new IntsRef(32);
+      for(FacetField facetField : ent.getValue()) {
+
+        FacetsConfig.DimConfig ft = getDimConfig(facetField.dim);
+        if (facetField.path.length > 1 && ft.hierarchical == false) {
+          throw new IllegalArgumentException("dimension \"" + facetField.dim + "\" is not hierarchical yet has " + facetField.path.length + " components");
+        }
+      
+        FacetLabel cp = new FacetLabel(facetField.dim, facetField.path);
+
+        checkTaxoWriter(taxoWriter);
+        int ordinal = taxoWriter.addCategory(cp);
+        if (ordinals.length == ordinals.ints.length) {
+          ordinals.grow(ordinals.length+1);
+        }
+        ordinals.ints[ordinals.length++] = ordinal;
+        //System.out.println("ords[" + (ordinals.length-1) + "]=" + ordinal);
+        //System.out.println("  add cp=" + cp);
+
+        if (ft.multiValued && (ft.hierarchical || ft.requireDimCount)) {
+          //System.out.println("  add parents");
+          // Add all parents too:
+          int parent = taxoWriter.getParent(ordinal);
+          while (parent > 0) {
+            if (ordinals.ints.length == ordinals.length) {
+              ordinals.grow(ordinals.length+1);
+            }
+            ordinals.ints[ordinals.length++] = parent;
+            parent = taxoWriter.getParent(parent);
+          }
+
+          if (ft.requireDimCount == false) {
+            // Remove last (dimension) ord:
+            ordinals.length--;
+          }
+        }
+
+        // Drill down:
+        for(int i=1;i<=cp.length;i++) {
+          addedIndexedFields.add(new StringField(indexFieldName, pathToString(cp.components, i), Field.Store.NO));
+        }
+      }
+
+      // Facet counts:
+      // DocValues are considered stored fields:
+      addedStoredFields.add(new BinaryDocValuesField(indexFieldName, dedupAndEncode(ordinals)));
+    }
+  }
+
+  private void processSSDVFacetFields(Map<String,List<SortedSetDocValuesFacetField>> byField, List<Field> addedIndexedFields, List<Field> addedStoredFields) throws IOException {
+    //System.out.println("process SSDV: " + byField);
+    for(Map.Entry<String,List<SortedSetDocValuesFacetField>> ent : byField.entrySet()) {
+
+      String indexFieldName = ent.getKey();
+      //System.out.println("  field=" + indexFieldName);
+
+      for(SortedSetDocValuesFacetField facetField : ent.getValue()) {
+        FacetLabel cp = new FacetLabel(facetField.dim, facetField.label);
+        String fullPath = pathToString(cp.components, cp.length);
+        //System.out.println("add " + fullPath);
+
+        // For facet counts:
+        addedStoredFields.add(new SortedSetDocValuesField(indexFieldName, new BytesRef(fullPath)));
+
+        // For drill-down:
+        addedIndexedFields.add(new StringField(indexFieldName, fullPath, Field.Store.NO));
+        addedIndexedFields.add(new StringField(indexFieldName, facetField.dim, Field.Store.NO));
+      }
+    }
+  }
+
+  private void processAssocFacetFields(TaxonomyWriter taxoWriter, Map<String,List<AssociationFacetField>> byField,
+                                       List<Field> addedIndexedFields, List<Field> addedStoredFields) throws IOException {
+    for(Map.Entry<String,List<AssociationFacetField>> ent : byField.entrySet()) {
+      byte[] bytes = new byte[16];
+      int upto = 0;
+      String indexFieldName = ent.getKey();
+      for(AssociationFacetField field : ent.getValue()) {
+        // NOTE: we don't add parents for associations
+        checkTaxoWriter(taxoWriter);
+        int ordinal = taxoWriter.addCategory(new FacetLabel(field.dim, field.path));
+        if (upto + 4 > bytes.length) {
+          bytes = ArrayUtil.grow(bytes, upto+4);
+        }
+        // big-endian:
+        bytes[upto++] = (byte) (ordinal >> 24);
+        bytes[upto++] = (byte) (ordinal >> 16);
+        bytes[upto++] = (byte) (ordinal >> 8);
+        bytes[upto++] = (byte) ordinal;
+        if (upto + field.assoc.length > bytes.length) {
+          bytes = ArrayUtil.grow(bytes, upto+field.assoc.length);
+        }
+        System.arraycopy(field.assoc.bytes, field.assoc.offset, bytes, upto, field.assoc.length);
+        upto += field.assoc.length;
+      }
+      addedStoredFields.add(new BinaryDocValuesField(indexFieldName, new BytesRef(bytes, 0, upto)));
+    }
+  }
+
+  /** Encodes ordinals into a BytesRef; expert: subclass can
+   *  override this to change encoding. */
+  protected BytesRef dedupAndEncode(IntsRef ordinals) {
+    Arrays.sort(ordinals.ints, ordinals.offset, ordinals.length);
+    byte[] bytes = new byte[5*ordinals.length];
+    int lastOrd = -1;
+    int upto = 0;
+    for(int i=0;i<ordinals.length;i++) {
+      int ord = ordinals.ints[ordinals.offset+i];
+      // ord could be == lastOrd, so we must dedup:
+      if (ord > lastOrd) {
+        int delta;
+        if (lastOrd == -1) {
+          delta = ord;
+        } else {
+          delta = ord - lastOrd;
+        }
+        if ((delta & ~0x7F) == 0) {
+          bytes[upto] = (byte) delta;
+          upto++;
+        } else if ((delta & ~0x3FFF) == 0) {
+          bytes[upto] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
+          bytes[upto + 1] = (byte) (delta & 0x7F);
+          upto += 2;
+        } else if ((delta & ~0x1FFFFF) == 0) {
+          bytes[upto] = (byte) (0x80 | ((delta & 0x1FC000) >> 14));
+          bytes[upto + 1] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
+          bytes[upto + 2] = (byte) (delta & 0x7F);
+          upto += 3;
+        } else if ((delta & ~0xFFFFFFF) == 0) {
+          bytes[upto] = (byte) (0x80 | ((delta & 0xFE00000) >> 21));
+          bytes[upto + 1] = (byte) (0x80 | ((delta & 0x1FC000) >> 14));
+          bytes[upto + 2] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
+          bytes[upto + 3] = (byte) (delta & 0x7F);
+          upto += 4;
+        } else {
+          bytes[upto] = (byte) (0x80 | ((delta & 0xF0000000) >> 28));
+          bytes[upto + 1] = (byte) (0x80 | ((delta & 0xFE00000) >> 21));
+          bytes[upto + 2] = (byte) (0x80 | ((delta & 0x1FC000) >> 14));
+          bytes[upto + 3] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
+          bytes[upto + 4] = (byte) (delta & 0x7F);
+          upto += 5;
+        }
+        lastOrd = ord;
+      }
+    }
+    return new BytesRef(bytes, 0, upto);
+  }
+
+  private void checkTaxoWriter(TaxonomyWriter taxoWriter) {
+    if (taxoWriter == null) {
+      throw new IllegalStateException("a non-null TaxonomyWriter must be provided when indexing FacetField or AssociationFacetField");
+    }
+  }
+
+  // Joins the path components together:
+  private static final char DELIM_CHAR = '\u001F';
+
+  // Escapes any occurrence of the path component inside the label:
+  private static final char ESCAPE_CHAR = '\u001E';
+
+  /** Turns a dim + path into an encoded string. */
+  public static String pathToString(String dim, String[] path) {
+    String[] fullPath = new String[1+path.length];
+    fullPath[0] = dim;
+    System.arraycopy(path, 0, fullPath, 1, path.length);
+    return pathToString(fullPath, fullPath.length);
+  }
+
+  /** Turns a dim + path into an encoded string. */
+  public static String pathToString(String[] path) {
+    return pathToString(path, path.length);
+  }
+
+  /** Turns the first {@code} length elements of {@code
+   * path} into an encoded string. */
+  public static String pathToString(String[] path, int length) {
+    if (length == 0) {
+      return "";
+    }
+    StringBuilder sb = new StringBuilder();
+    for(int i=0;i<length;i++) {
+      String s = path[i];
+      if (s.length() == 0) {
+        throw new IllegalArgumentException("each path component must have length > 0 (got: \"\")");
+      }
+      int numChars = s.length();
+      for(int j=0;j<numChars;j++) {
+        char ch = s.charAt(j);
+        if (ch == DELIM_CHAR || ch == ESCAPE_CHAR) {
+          sb.append(ESCAPE_CHAR);
+        }
+        sb.append(ch);
+      }
+      sb.append(DELIM_CHAR);
+    }
+
+    // Trim off last DELIM_CHAR:
+    sb.setLength(sb.length()-1);
+    return sb.toString();
+  }
+
+  /** Turns an encoded string (from a previous call to {@link
+   *  #pathToString}) back into the original {@code
+   *  String[]}. */
+  public static String[] stringToPath(String s) {
+    List<String> parts = new ArrayList<String>();
+    int length = s.length();
+    if (length == 0) {
+      return new String[0];
+    }
+    char[] buffer = new char[length];
+
+    int upto = 0;
+    boolean lastEscape = false;
+    for(int i=0;i<length;i++) {
+      char ch = s.charAt(i);
+      if (lastEscape) {
+        buffer[upto++] = ch;
+        lastEscape = false;
+      } else if (ch == ESCAPE_CHAR) {
+        lastEscape = true;
+      } else if (ch == DELIM_CHAR) {
+        parts.add(new String(buffer, 0, upto));
+        upto = 0;
+      } else {
+        buffer[upto++] = ch;
+      }
+    }
+    parts.add(new String(buffer, 0, upto));
+    assert !lastEscape;
+    return parts.toArray(new String[parts.size()]);
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/Facets.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/Facets.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/Facets.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/Facets.java	2013-11-27 18:56:20.723937789 -0500
@@ -0,0 +1,57 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.lucene.search.FieldDoc;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.FilteredQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MultiCollector;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.TopDocsCollector;
+import org.apache.lucene.search.TopFieldCollector;
+import org.apache.lucene.search.TopFieldDocs;
+import org.apache.lucene.search.TopScoreDocCollector;
+
+/** Common base class for all facets implementations.
+ *
+ *  @lucene.experimental */
+public abstract class Facets {
+  /** Returns the topN child labels under the specified
+   *  path.  Returns null if the specified path doesn't
+   *  exist or if this dimension was never seen. */
+  public abstract FacetResult getTopChildren(int topN, String dim, String... path) throws IOException;
+
+  /** Return the count or value
+   *  for a specific path.  Returns -1 if
+   *  this path doesn't exist, else the count. */
+  public abstract Number getSpecificValue(String dim, String... path) throws IOException;
+
+  /** Returns topN labels for any dimension that had hits,
+   *  sorted by the number of hits that dimension matched;
+   *  this is used for "sparse" faceting, where many
+   *  different dimensions were indexed, for example
+   *  depending on the type of document. */
+  public abstract List<FacetResult> getAllDims(int topN) throws IOException;
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/FastTaxonomyFacetCounts.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/FastTaxonomyFacetCounts.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/FastTaxonomyFacetCounts.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/FastTaxonomyFacetCounts.java	2013-11-27 19:16:07.623905485 -0500
@@ -0,0 +1,85 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+
+/** Computes facets counts, assuming the default encoding
+ *  into DocValues was used.
+ *
+ * @lucene.experimental */
+public class FastTaxonomyFacetCounts extends IntTaxonomyFacets {
+
+  /** Create {@code FastTaxonomyFacetCounts}, which also
+   *  counts all facet labels. */
+  public FastTaxonomyFacetCounts(TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
+  }
+
+  /** Create {@code FastTaxonomyFacetCounts}, using the
+   *  specified {@code indexFieldName} for ordinals.  Use
+   *  this if you had set {@link
+   *  FacetsConfig#setIndexFieldName} to change the index
+   *  field name for certain dimensions. */
+  public FastTaxonomyFacetCounts(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    super(indexFieldName, taxoReader, config);
+    count(fc.getMatchingDocs());
+  }
+
+  private final void count(List<MatchingDocs> matchingDocs) throws IOException {
+    for(MatchingDocs hits : matchingDocs) {
+      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
+      if (dv == null) { // this reader does not have DocValues for the requested category list
+        continue;
+      }
+      FixedBitSet bits = hits.bits;
+    
+      final int length = hits.bits.length();
+      int doc = 0;
+      BytesRef scratch = new BytesRef();
+      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
+        dv.get(doc, scratch);
+        byte[] bytes = scratch.bytes;
+        int end = scratch.offset + scratch.length;
+        int ord = 0;
+        int offset = scratch.offset;
+        int prev = 0;
+        while (offset < end) {
+          byte b = bytes[offset++];
+          if (b >= 0) {
+            prev = ord = ((ord << 7) | b) + prev;
+            ++values[ord];
+            ord = 0;
+          } else {
+            ord = (ord << 7) | (b & 0x7F);
+          }
+        }
+        ++doc;
+      }
+    }
+
+    rollup();
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/FloatAssociationFacetField.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/FloatAssociationFacetField.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/FloatAssociationFacetField.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/FloatAssociationFacetField.java	2013-11-26 18:34:23.938283906 -0500
@@ -0,0 +1,55 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.document.Document; // javadocs
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.util.BytesRef;
+
+/** Add an instance of this to your {@link Document} to add
+ *  a facet label associated with a float.  Use {@link
+ *  TaxonomyFacetSumFloatAssociations} to aggregate float values
+ *  per facet label at search time.
+ * 
+ *  @lucene.experimental */
+public class FloatAssociationFacetField extends AssociationFacetField {
+
+  /** Creates this from {@code dim} and {@code path} and a
+   *  float association */
+  public FloatAssociationFacetField(float assoc, String dim, String... path) {
+    super(floatToBytesRef(assoc), dim, path);
+  }
+
+  /** Encodes a {@code float} as a 4-byte {@link BytesRef}. */
+  public static BytesRef floatToBytesRef(float v) {
+    return IntAssociationFacetField.intToBytesRef(Float.floatToIntBits(v));
+  }
+
+  /** Decodes a previously encoded {@code float}. */
+  public static float bytesRefToFloat(BytesRef b) {
+    return Float.intBitsToFloat(IntAssociationFacetField.bytesRefToInt(b));
+  }
+
+  @Override
+  public String toString() {
+    return "FloatAssociationFacetField(dim=" + dim + " path=" + Arrays.toString(path) + " value=" + bytesRefToFloat(assoc) + ")";
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/FloatTaxonomyFacets.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/FloatTaxonomyFacets.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/FloatTaxonomyFacets.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/FloatTaxonomyFacets.java	2013-12-18 20:01:51.253555353 -0500
@@ -0,0 +1,146 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Map;
+
+import org.apache.lucene.facet.FacetsConfig.DimConfig;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+
+/** Base class for all taxonomy-based facets that aggregate
+ *  to a per-ords float[]. */
+
+public abstract class FloatTaxonomyFacets extends TaxonomyFacets {
+
+  protected final float[] values;
+
+  protected FloatTaxonomyFacets(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config) throws IOException {
+    super(indexFieldName, taxoReader, config);
+    values = new float[taxoReader.getSize()];
+  }
+  
+  protected void rollup() throws IOException {
+    // Rollup any necessary dims:
+    for(Map.Entry<String,DimConfig> ent : config.getDimConfigs().entrySet()) {
+      String dim = ent.getKey();
+      DimConfig ft = ent.getValue();
+      if (ft.hierarchical && ft.multiValued == false) {
+        int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
+        assert dimRootOrd > 0;
+        values[dimRootOrd] += rollup(children[dimRootOrd]);
+      }
+    }
+  }
+
+  private float rollup(int ord) {
+    float sum = 0;
+    while (ord != TaxonomyReader.INVALID_ORDINAL) {
+      float childValue = values[ord] + rollup(children[ord]);
+      values[ord] = childValue;
+      sum += childValue;
+      ord = siblings[ord];
+    }
+    return sum;
+  }
+
+  @Override
+  public Number getSpecificValue(String dim, String... path) throws IOException {
+    DimConfig dimConfig = verifyDim(dim);
+    if (path.length == 0) {
+      if (dimConfig.hierarchical && dimConfig.multiValued == false) {
+        // ok: rolled up at search time
+      } else if (dimConfig.requireDimCount && dimConfig.multiValued) {
+        // ok: we indexed all ords at index time
+      } else {
+        throw new IllegalArgumentException("cannot return dimension-level value alone; use getTopChildren instead");
+      }
+    }
+    int ord = taxoReader.getOrdinal(new FacetLabel(dim, path));
+    if (ord < 0) {
+      return -1;
+    }
+    return values[ord];
+  }
+
+  @Override
+  public FacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    if (topN <= 0) {
+      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
+    }
+    DimConfig dimConfig = verifyDim(dim);
+    FacetLabel cp = new FacetLabel(dim, path);
+    int dimOrd = taxoReader.getOrdinal(cp);
+    if (dimOrd == -1) {
+      return null;
+    }
+
+    TopOrdAndFloatQueue q = new TopOrdAndFloatQueue(Math.min(taxoReader.getSize(), topN));
+    float bottomValue = 0;
+
+    int ord = children[dimOrd];
+    float sumValues = 0;
+    int childCount = 0;
+
+    TopOrdAndFloatQueue.OrdAndValue reuse = null;
+    while(ord != TaxonomyReader.INVALID_ORDINAL) {
+      if (values[ord] > 0) {
+        sumValues += values[ord];
+        childCount++;
+        if (values[ord] > bottomValue) {
+          if (reuse == null) {
+            reuse = new TopOrdAndFloatQueue.OrdAndValue();
+          }
+          reuse.ord = ord;
+          reuse.value = values[ord];
+          reuse = q.insertWithOverflow(reuse);
+          if (q.size() == topN) {
+            bottomValue = q.top().value;
+          }
+        }
+      }
+
+      ord = siblings[ord];
+    }
+
+    if (sumValues == 0) {
+      return null;
+    }
+
+    if (dimConfig.multiValued) {
+      if (dimConfig.requireDimCount) {
+        sumValues = values[dimOrd];
+      } else {
+        // Our sum'd count is not correct, in general:
+        sumValues = -1;
+      }
+    } else {
+      // Our sum'd dim count is accurate, so we keep it
+    }
+
+    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
+    for(int i=labelValues.length-1;i>=0;i--) {
+      TopOrdAndFloatQueue.OrdAndValue ordAndValue = q.pop();
+      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
+      labelValues[i] = new LabelAndValue(child.components[cp.length], ordAndValue.value);
+    }
+
+    return new FacetResult(dim, path, sumValues, labelValues, childCount);
+  }
+}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/index/CategoryListBuilder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/index/CategoryListBuilder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/index/CategoryListBuilder.java	2013-01-11 08:39:05.224858238 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/index/CategoryListBuilder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,38 +0,0 @@
-package org.apache.lucene.facet.index;
-
-import java.io.IOException;
-import java.util.Map;
-
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Builds a category list data by encoding the appropriate information for every
- * category and ordinal given to {@link #build(IntsRef, Iterable)}.
- * 
- * @lucene.experimental
- */
-public interface CategoryListBuilder {
-  
-  /** Returns the encoded ordinals data. */
-  public Map<String,BytesRef> build(IntsRef ordinals, Iterable<CategoryPath> categories) throws IOException;
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/index/CountingListBuilder.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/index/CountingListBuilder.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/index/CountingListBuilder.java	2013-04-22 16:59:20.119676455 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/index/CountingListBuilder.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,170 +0,0 @@
-package org.apache.lucene.facet.index;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.Map.Entry;
-
-import org.apache.lucene.facet.encoding.IntEncoder;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.util.PartitionsUtils;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link CategoryListBuilder} which builds a counting list data by encoding
- * the category ordinals into one or more {@link BytesRef}. Each
- * {@link BytesRef} corresponds to a set of ordinals that belong to the same
- * partition. When partitions are not enabled (i.e.
- * {@link FacetIndexingParams#getPartitionSize()} returns
- * {@link Integer#MAX_VALUE}), only one {@link BytesRef} is returned by this
- * class.
- * <p>
- * Counting lists are used usually for computing the weight of categories by
- * summing their number of occurrences (hence counting) in a result set.
- */
-public class CountingListBuilder implements CategoryListBuilder {
- 
-  /** Specializes encoding ordinals when partitions are enabled/disabled. */
-  private static abstract class OrdinalsEncoder {
-    OrdinalsEncoder() {}
-    public abstract Map<String,BytesRef> encode(IntsRef ordinals);
-  }
-  
-  private static final class NoPartitionsOrdinalsEncoder extends OrdinalsEncoder {
-    
-    private final IntEncoder encoder;
-    private final String name = "";
-    
-    NoPartitionsOrdinalsEncoder(CategoryListParams categoryListParams) {
-      encoder = categoryListParams.createEncoder();
-    }
-    
-    @Override
-    public Map<String,BytesRef> encode(IntsRef ordinals) {
-      final BytesRef bytes = new BytesRef(128); // should be enough for most common applications
-      encoder.encode(ordinals, bytes);
-      return Collections.singletonMap(name, bytes);
-    }
-    
-  }
-  
-  private static final class PerPartitionOrdinalsEncoder extends OrdinalsEncoder {
-
-    private final FacetIndexingParams indexingParams;
-    private final CategoryListParams categoryListParams;
-    private final int partitionSize;
-    private final HashMap<String,IntEncoder> partitionEncoder = new HashMap<String,IntEncoder>();
-
-    PerPartitionOrdinalsEncoder(FacetIndexingParams indexingParams, CategoryListParams categoryListParams) {
-      this.indexingParams = indexingParams;
-      this.categoryListParams = categoryListParams;
-      this.partitionSize = indexingParams.getPartitionSize();
-    }
-
-    @Override
-    public HashMap<String,BytesRef> encode(IntsRef ordinals) {
-      // build the partitionOrdinals map
-      final HashMap<String,IntsRef> partitionOrdinals = new HashMap<String,IntsRef>();
-      for (int i = 0; i < ordinals.length; i++) {
-        int ordinal = ordinals.ints[i];
-        final String name = PartitionsUtils.partitionNameByOrdinal(indexingParams, ordinal);
-        IntsRef partitionOrds = partitionOrdinals.get(name);
-        if (partitionOrds == null) {
-          partitionOrds = new IntsRef(32);
-          partitionOrdinals.put(name, partitionOrds);
-          partitionEncoder.put(name, categoryListParams.createEncoder());
-        }
-        partitionOrds.ints[partitionOrds.length++] = ordinal % partitionSize;
-      }
-      
-      HashMap<String,BytesRef> partitionBytes = new HashMap<String,BytesRef>();
-      for (Entry<String,IntsRef> e : partitionOrdinals.entrySet()) {
-        String name = e.getKey();
-        final IntEncoder encoder = partitionEncoder.get(name);
-        final BytesRef bytes = new BytesRef(128); // should be enough for most common applications        
-        encoder.encode(e.getValue(), bytes);
-        partitionBytes.put(name, bytes);
-      }
-      return partitionBytes;
-    }
-    
-  }
-  
-  private final OrdinalsEncoder ordinalsEncoder;
-  private final TaxonomyWriter taxoWriter;
-  private final CategoryListParams clp;
-  
-  public CountingListBuilder(CategoryListParams categoryListParams, FacetIndexingParams indexingParams, 
-      TaxonomyWriter taxoWriter) {
-    this.taxoWriter = taxoWriter;
-    this.clp = categoryListParams;
-    if (indexingParams.getPartitionSize() == Integer.MAX_VALUE) {
-      ordinalsEncoder = new NoPartitionsOrdinalsEncoder(categoryListParams);
-    } else {
-      ordinalsEncoder = new PerPartitionOrdinalsEncoder(indexingParams, categoryListParams);
-    }
-  }
-
-  /**
-   * Every returned {@link BytesRef} corresponds to a single partition (as
-   * defined by {@link FacetIndexingParams#getPartitionSize()}) and the key
-   * denotes the partition ID. When no partitions are defined, the returned map
-   * contains only one value.
-   * <p>
-   * <b>NOTE:</b> the {@code ordinals} array is modified by adding parent
-   * ordinals to it. Also, some encoders may sort the array and remove duplicate
-   * ordinals. Therefore you may want to invoke this method after you finished
-   * processing the array for other purposes.
-   */
-  @Override
-  public Map<String,BytesRef> build(IntsRef ordinals, Iterable<CategoryPath> categories) throws IOException {
-    int upto = ordinals.length; // since we may add ordinals to IntsRef, iterate upto original length
-
-    Iterator<CategoryPath> iter = categories.iterator();
-    for (int i = 0; i < upto; i++) {
-      int ordinal = ordinals.ints[i];
-      CategoryPath cp = iter.next();
-      OrdinalPolicy op = clp.getOrdinalPolicy(cp.components[0]);
-      if (op != OrdinalPolicy.NO_PARENTS) {
-        // need to add parents too
-        int parent = taxoWriter.getParent(ordinal);
-        if (parent > 0) {
-          // only do this if the category is not a dimension itself, otherwise, it was just discarded by the 'if' below
-          while (parent > 0) {
-            ordinals.ints[ordinals.length++] = parent;
-            parent = taxoWriter.getParent(parent);
-          }
-          if (op == OrdinalPolicy.ALL_BUT_DIMENSION) { // discard the last added parent, which is the dimension
-            ordinals.length--;
-          }
-        }
-      }
-    }
-    return ordinalsEncoder.encode(ordinals);
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/index/DrillDownStream.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/index/DrillDownStream.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/index/DrillDownStream.java	2013-02-20 13:38:17.688711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/index/DrillDownStream.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,83 +0,0 @@
-package org.apache.lucene.facet.index;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link TokenStream} which creates category drill-down terms.
- * 
- * @lucene.experimental
- */
-public class DrillDownStream extends TokenStream {
-
-  private final FacetIndexingParams indexingParams;
-  private final Iterator<CategoryPath> categories;
-  private final CharTermAttribute termAttribute;
-  
-  private CategoryPath current;
-  private boolean isParent;
-  
-  public DrillDownStream(Iterable<CategoryPath> categories, FacetIndexingParams indexingParams) {
-    termAttribute = addAttribute(CharTermAttribute.class);
-    this.categories = categories.iterator();
-    this.indexingParams = indexingParams;
-  }
-
-  protected void addAdditionalAttributes(CategoryPath category, boolean isParent) {
-    // a hook for AssociationsDrillDownStream to add the associations payload to
-    // the drill-down terms
-  }
-  
-  @Override
-  public final boolean incrementToken() throws IOException {
-    if (current.length == 0) {
-      if (!categories.hasNext()) {
-        return false; // no more categories
-      }
-      current = categories.next();
-      termAttribute.resizeBuffer(current.fullPathLength());
-      isParent = false;
-    }
-
-    // copy current as drill-down term (it's either the leaf node or PathPolicy
-    // accepted it.
-    int nChars = indexingParams.drillDownTermText(current, termAttribute.buffer());
-    termAttribute.setLength(nChars);
-    addAdditionalAttributes(current, isParent);
-    
-    // prepare current for next call by trimming the last component (parents)
-    current = current.subpath(current.length - 1);
-    isParent = true;
-    return true;
-  }
-
-  @Override
-  public void reset() throws IOException {
-    current = categories.next();
-    termAttribute.resizeBuffer(current.fullPathLength());
-    isParent = false;
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/index/FacetFields.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/index/FacetFields.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/index/FacetFields.java	2013-03-20 06:26:07.035245398 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/index/FacetFields.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,194 +0,0 @@
-package org.apache.lucene.facet.index;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map.Entry;
-import java.util.Map;
-
-import org.apache.lucene.document.BinaryDocValuesField;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A utility class for adding facet fields to a document. Usually one field will
- * be added for all facets, however per the
- * {@link FacetIndexingParams#getCategoryListParams(CategoryPath)}, one field
- * may be added for every group of facets.
- * 
- * @lucene.experimental
- */
-public class FacetFields {
-
-  // The drill-down field is added with a TokenStream, hence why it's based on
-  // TextField type. However in practice, it is added just like StringField.
-  // Therefore we set its IndexOptions to DOCS_ONLY.
-  private static final FieldType DRILL_DOWN_TYPE = new FieldType(TextField.TYPE_NOT_STORED);
-  static {
-    DRILL_DOWN_TYPE.setIndexOptions(IndexOptions.DOCS_ONLY);
-    DRILL_DOWN_TYPE.setOmitNorms(true);
-    DRILL_DOWN_TYPE.freeze();
-  }
-  
-  protected final TaxonomyWriter taxonomyWriter;
-
-  protected final FacetIndexingParams indexingParams;
-
-  /**
-   * Constructs a new instance with the {@link FacetIndexingParams#DEFAULT
-   * default} facet indexing params.
-   * 
-   * @param taxonomyWriter
-   *          used to resolve given categories to ordinals
-   */
-  public FacetFields(TaxonomyWriter taxonomyWriter) {
-    this(taxonomyWriter, FacetIndexingParams.DEFAULT);
-  }
-
-  /**
-   * Constructs a new instance with the given facet indexing params.
-   * 
-   * @param taxonomyWriter
-   *          used to resolve given categories to ordinals
-   * @param params
-   *          determines under which fields the categories should be indexed
-   */
-  public FacetFields(TaxonomyWriter taxonomyWriter, FacetIndexingParams params) {
-    this.taxonomyWriter = taxonomyWriter;
-    this.indexingParams = params;
-  }
-
-  /**
-   * Creates a mapping between a {@link CategoryListParams} and all
-   * {@link CategoryPath categories} that are associated with it.
-   */
-  protected Map<CategoryListParams,Iterable<CategoryPath>> createCategoryListMapping(
-      Iterable<CategoryPath> categories) {
-    if (indexingParams.getAllCategoryListParams().size() == 1) {
-      return Collections.singletonMap(indexingParams.getCategoryListParams(null), categories);
-    }
-    HashMap<CategoryListParams,Iterable<CategoryPath>> categoryLists = 
-        new HashMap<CategoryListParams,Iterable<CategoryPath>>();
-    for (CategoryPath cp : categories) {
-      // each category may be indexed under a different field, so add it to the right list.
-      CategoryListParams clp = indexingParams.getCategoryListParams(cp);
-      List<CategoryPath> list = (List<CategoryPath>) categoryLists.get(clp);
-      if (list == null) {
-        list = new ArrayList<CategoryPath>();
-        categoryLists.put(clp, list);
-      }
-      list.add(cp);
-    }
-    return categoryLists;
-  }
-  
-  /**
-   * Returns the category list data, as a mapping from key to {@link BytesRef}
-   * which includes the encoded data. Every ordinal in {@code ordinals}
-   * corrspond to a {@link CategoryPath} returned from {@code categories}.
-   */
-  protected Map<String,BytesRef> getCategoryListData(CategoryListParams categoryListParams, 
-      IntsRef ordinals, Iterable<CategoryPath> categories /* needed for AssociationsFacetFields */) 
-      throws IOException {
-    return new CountingListBuilder(categoryListParams, indexingParams, taxonomyWriter).build(ordinals, categories);
-  }
-  
-  /**
-   * Returns a {@link DrillDownStream} for writing the categories drill-down
-   * terms.
-   */
-  protected DrillDownStream getDrillDownStream(Iterable<CategoryPath> categories) {
-    return new DrillDownStream(categories, indexingParams);
-  }
-  
-  /**
-   * Returns the {@link FieldType} with which the drill-down terms should be
-   * indexed. The default is {@link IndexOptions#DOCS_ONLY}.
-   */
-  protected FieldType drillDownFieldType() {
-    return DRILL_DOWN_TYPE;
-  }
-
-  /**
-   * Add the counting list data to the document under the given field. Note that
-   * the field is determined by the {@link CategoryListParams}.
-   */
-  protected void addCountingListData(Document doc, Map<String,BytesRef> categoriesData, String field) {
-    for (Entry<String,BytesRef> entry : categoriesData.entrySet()) {
-      doc.add(new BinaryDocValuesField(field + entry.getKey(), entry.getValue()));
-    }
-  }
-  
-  /** Adds the needed facet fields to the document. */
-  public void addFields(Document doc, Iterable<CategoryPath> categories) throws IOException {
-    if (categories == null) {
-      throw new IllegalArgumentException("categories should not be null");
-    }
-
-    // TODO: add reuse capabilities to this class, per CLP objects:
-    // - drill-down field
-    // - counting list field
-    // - DrillDownStream
-    // - CountingListStream
-
-    final Map<CategoryListParams,Iterable<CategoryPath>> categoryLists = createCategoryListMapping(categories);
-
-    // for each CLP we add a different field for drill-down terms as well as for
-    // counting list data.
-    IntsRef ordinals = new IntsRef(32); // should be enough for most common applications
-    for (Entry<CategoryListParams, Iterable<CategoryPath>> e : categoryLists.entrySet()) {
-      final CategoryListParams clp = e.getKey();
-      final String field = clp.field;
-
-      // build category list data
-      ordinals.length = 0; // reset
-      int maxNumOrds = 0;
-      for (CategoryPath cp : e.getValue()) {
-        int ordinal = taxonomyWriter.addCategory(cp);
-        maxNumOrds += cp.length; // ordinal and potentially all parents
-        if (ordinals.ints.length < maxNumOrds) {
-          ordinals.grow(maxNumOrds);
-        }
-        ordinals.ints[ordinals.length++] = ordinal;
-      }
-      Map<String,BytesRef> categoriesData = getCategoryListData(clp, ordinals, e.getValue());
-      
-      // add the counting list data
-      addCountingListData(doc, categoriesData, field);
-      
-      // add the drill-down field
-      DrillDownStream drillDownStream = getDrillDownStream(e.getValue());
-      Field drillDown = new Field(field, drillDownStream, drillDownFieldType());
-      doc.add(drillDown);
-    }
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/index/package.html simplefacets/lucene/facet/src/java/org/apache/lucene/facet/index/package.html
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/index/package.html	2013-07-23 16:25:55.719333095 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/index/package.html	1969-12-31 19:00:00.000000000 -0500
@@ -1,24 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Facets indexing code</title>
-</head>
-<body>
-Facets indexing code.
-</body>
-</html>
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/IntAssociationFacetField.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/IntAssociationFacetField.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/IntAssociationFacetField.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/IntAssociationFacetField.java	2013-11-26 18:34:31.618283566 -0500
@@ -0,0 +1,65 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.document.Document; // javadocs
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.util.BytesRef;
+
+/** Add an instance of this to your {@link Document} to add
+ *  a facet label associated with an int.  Use {@link
+ *  TaxonomyFacetSumIntAssociations} to aggregate int values
+ *  per facet label at search time.
+ * 
+ *  @lucene.experimental */
+public class IntAssociationFacetField extends AssociationFacetField {
+
+  /** Creates this from {@code dim} and {@code path} and an
+   *  int association */
+  public IntAssociationFacetField(int assoc, String dim, String... path) {
+    super(intToBytesRef(assoc), dim, path);
+  }
+
+  /** Encodes an {@code int} as a 4-byte {@link BytesRef},
+   *  big-endian. */
+  public static BytesRef intToBytesRef(int v) {
+    byte[] bytes = new byte[4];
+    // big-endian:
+    bytes[0] = (byte) (v >> 24);
+    bytes[1] = (byte) (v >> 16);
+    bytes[2] = (byte) (v >> 8);
+    bytes[3] = (byte) v;
+    return new BytesRef(bytes);
+  }
+
+  /** Decodes a previously encoded {@code int}. */
+  public static int bytesRefToInt(BytesRef b) {
+    return ((b.bytes[b.offset]&0xFF) << 24) |
+      ((b.bytes[b.offset+1]&0xFF) << 16) |
+      ((b.bytes[b.offset+2]&0xFF) << 8) |
+      (b.bytes[b.offset+3]&0xFF);
+  }
+
+  @Override
+  public String toString() {
+    return "IntAssociationFacetField(dim=" + dim + " path=" + Arrays.toString(path) + " value=" + bytesRefToInt(assoc) + ")";
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/IntTaxonomyFacets.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/IntTaxonomyFacets.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/IntTaxonomyFacets.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/IntTaxonomyFacets.java	2013-12-18 20:01:45.309555504 -0500
@@ -0,0 +1,150 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Map;
+
+import org.apache.lucene.facet.FacetsConfig.DimConfig;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+
+/** Base class for all taxonomy-based facets that aggregate
+ *  to a per-ords int[]. */
+
+public abstract class IntTaxonomyFacets extends TaxonomyFacets {
+
+  protected final int[] values;
+
+  protected IntTaxonomyFacets(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config) throws IOException {
+    super(indexFieldName, taxoReader, config);
+    values = new int[taxoReader.getSize()];
+  }
+  
+  protected void rollup() throws IOException {
+    // Rollup any necessary dims:
+    for(Map.Entry<String,DimConfig> ent : config.getDimConfigs().entrySet()) {
+      String dim = ent.getKey();
+      DimConfig ft = ent.getValue();
+      if (ft.hierarchical && ft.multiValued == false) {
+        int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
+        // It can be -1 if this field was declared in the
+        // config but never indexed:
+        if (dimRootOrd > 0) {
+          values[dimRootOrd] += rollup(children[dimRootOrd]);
+        }
+      }
+    }
+  }
+
+  private int rollup(int ord) {
+    int sum = 0;
+    while (ord != TaxonomyReader.INVALID_ORDINAL) {
+      int childValue = values[ord] + rollup(children[ord]);
+      values[ord] = childValue;
+      sum += childValue;
+      ord = siblings[ord];
+    }
+    return sum;
+  }
+
+  @Override
+  public Number getSpecificValue(String dim, String... path) throws IOException {
+    DimConfig dimConfig = verifyDim(dim);
+    if (path.length == 0) {
+      if (dimConfig.hierarchical && dimConfig.multiValued == false) {
+        // ok: rolled up at search time
+      } else if (dimConfig.requireDimCount && dimConfig.multiValued) {
+        // ok: we indexed all ords at index time
+      } else {
+        throw new IllegalArgumentException("cannot return dimension-level value alone; use getTopChildren instead");
+      }
+    }
+    int ord = taxoReader.getOrdinal(new FacetLabel(dim, path));
+    if (ord < 0) {
+      return -1;
+    }
+    return values[ord];
+  }
+
+  @Override
+  public FacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    if (topN <= 0) {
+      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
+    }
+    DimConfig dimConfig = verifyDim(dim);
+    FacetLabel cp = new FacetLabel(dim, path);
+    int dimOrd = taxoReader.getOrdinal(cp);
+    if (dimOrd == -1) {
+      return null;
+    }
+
+    TopOrdAndIntQueue q = new TopOrdAndIntQueue(Math.min(taxoReader.getSize(), topN));
+    
+    int bottomValue = 0;
+
+    int ord = children[dimOrd];
+    int totValue = 0;
+    int childCount = 0;
+
+    TopOrdAndIntQueue.OrdAndValue reuse = null;
+    while(ord != TaxonomyReader.INVALID_ORDINAL) {
+      if (values[ord] > 0) {
+        totValue += values[ord];
+        childCount++;
+        if (values[ord] > bottomValue) {
+          if (reuse == null) {
+            reuse = new TopOrdAndIntQueue.OrdAndValue();
+          }
+          reuse.ord = ord;
+          reuse.value = values[ord];
+          reuse = q.insertWithOverflow(reuse);
+          if (q.size() == topN) {
+            bottomValue = q.top().value;
+          }
+        }
+      }
+
+      ord = siblings[ord];
+    }
+
+    if (totValue == 0) {
+      return null;
+    }
+
+    if (dimConfig.multiValued) {
+      if (dimConfig.requireDimCount) {
+        totValue = values[dimOrd];
+      } else {
+        // Our sum'd value is not correct, in general:
+        totValue = -1;
+      }
+    } else {
+      // Our sum'd dim value is accurate, so we keep it
+    }
+
+    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
+    for(int i=labelValues.length-1;i>=0;i--) {
+      TopOrdAndIntQueue.OrdAndValue ordAndValue = q.pop();
+      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
+      labelValues[i] = new LabelAndValue(child.components[cp.length], ordAndValue.value);
+    }
+
+    return new FacetResult(dim, path, totValue, labelValues, childCount);
+  }
+}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/LabelAndValue.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/LabelAndValue.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/LabelAndValue.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/LabelAndValue.java	2013-11-27 17:42:44.132055196 -0500
@@ -0,0 +1,50 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public final class LabelAndValue {
+  /** Facet's label. */
+  public final String label;
+
+  /** Value associated with this label. */
+  public final Number value;
+
+  public LabelAndValue(String label, Number value) {
+    this.label = label;
+    this.value = value;
+  }
+
+  @Override
+  public String toString() {
+    return label + " (" + value + ")";
+  }
+
+  @Override
+  public boolean equals(Object _other) {
+    if ((_other instanceof LabelAndValue) == false) {
+      return false;
+    }
+    LabelAndValue other = (LabelAndValue) _other;
+    return label.equals(other.label) && value.equals(other.value);
+  }
+
+  @Override
+  public int hashCode() {
+    return label.hashCode() + 1439 * value.hashCode();
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/LongRangeCounter.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/LongRangeCounter.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/LongRangeCounter.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/LongRangeCounter.java	2013-12-17 16:02:57.476250249 -0500
@@ -0,0 +1,318 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+/** Counts how many times each range was seen;
+ *  per-hit it's just a binary search ({@link #add})
+ *  against the elementary intervals, and in the end we
+ *  rollup back to the original ranges. */
+
+final class LongRangeCounter {
+
+  final LongRangeNode root;
+  final long[] boundaries;
+  final int[] leafCounts;
+
+  // Used during rollup
+  private int leafUpto;
+  private int missingCount;
+
+  public LongRangeCounter(LongRange[] ranges) {
+    // Maps all range inclusive endpoints to int flags; 1
+    // = start of interval, 2 = end of interval.  We need to
+    // track the start vs end case separately because if a
+    // given point is both, then it must be its own
+    // elementary interval:
+    Map<Long,Integer> endsMap = new HashMap<Long,Integer>();
+
+    endsMap.put(Long.MIN_VALUE, 1);
+    endsMap.put(Long.MAX_VALUE, 2);
+
+    for(LongRange range : ranges) {
+      Integer cur = endsMap.get(range.minIncl);
+      if (cur == null) {
+        endsMap.put(range.minIncl, 1);
+      } else {
+        endsMap.put(range.minIncl, cur.intValue() | 1);
+      }
+      cur = endsMap.get(range.maxIncl);
+      if (cur == null) {
+        endsMap.put(range.maxIncl, 2);
+      } else {
+        endsMap.put(range.maxIncl, cur.intValue() | 2);
+      }
+    }
+
+    List<Long> endsList = new ArrayList<Long>(endsMap.keySet());
+    Collections.sort(endsList);
+
+    // Build elementaryIntervals (a 1D Venn diagram):
+    List<InclusiveRange> elementaryIntervals = new ArrayList<InclusiveRange>();
+    int upto0 = 1;
+    long v = endsList.get(0);
+    long prev;
+    if (endsMap.get(v) == 3) {
+      elementaryIntervals.add(new InclusiveRange(v, v));
+      prev = v+1;
+    } else {
+      prev = v;
+    }
+
+    while (upto0 < endsList.size()) {
+      v = endsList.get(upto0);
+      int flags = endsMap.get(v);
+      //System.out.println("  v=" + v + " flags=" + flags);
+      if (flags == 3) {
+        // This point is both an end and a start; we need to
+        // separate it:
+        if (v > prev) {
+          elementaryIntervals.add(new InclusiveRange(prev, v-1));
+        }
+        elementaryIntervals.add(new InclusiveRange(v, v));
+        prev = v+1;
+      } else if (flags == 1) {
+        // This point is only the start of an interval;
+        // attach it to next interval:
+        if (v > prev) {
+          elementaryIntervals.add(new InclusiveRange(prev, v-1));
+        }
+        prev = v;
+      } else {
+        assert flags == 2;
+        // This point is only the end of an interval; attach
+        // it to last interval:
+        elementaryIntervals.add(new InclusiveRange(prev, v));
+        prev = v+1;
+      }
+      //System.out.println("    ints=" + elementaryIntervals);
+      upto0++;
+    }
+
+    // Build binary tree on top of intervals:
+    root = split(0, elementaryIntervals.size(), elementaryIntervals);
+
+    // Set outputs, so we know which range to output for
+    // each node in the tree:
+    for(int i=0;i<ranges.length;i++) {
+      root.addOutputs(i, ranges[i]);
+    }
+
+    // Set boundaries (ends of each elementary interval):
+    boundaries = new long[elementaryIntervals.size()];
+    for(int i=0;i<boundaries.length;i++) {
+      boundaries[i] = elementaryIntervals.get(i).end;
+    }
+
+    leafCounts = new int[boundaries.length];
+
+    //System.out.println("ranges: " + Arrays.toString(ranges));
+    //System.out.println("intervals: " + elementaryIntervals);
+    //System.out.println("boundaries: " + Arrays.toString(boundaries));
+    //System.out.println("root:\n" + root);
+  }
+
+  public void add(long v) {
+    //System.out.println("add v=" + v);
+
+    // NOTE: this works too, but it's ~6% slower on a simple
+    // test with a high-freq TermQuery w/ range faceting on
+    // wikimediumall:
+    /*
+    int index = Arrays.binarySearch(boundaries, v);
+    if (index < 0) {
+      index = -index-1;
+    }
+    leafCounts[index]++;
+    */
+
+    // Binary search to find matched elementary range; we
+    // are guaranteed to find a match because the last
+    // boundary is Long.MAX_VALUE:
+
+    int lo = 0;
+    int hi = boundaries.length - 1;
+    int count = 0;
+    while (true) {
+      int mid = (lo + hi) >>> 1;
+      //System.out.println("  cycle lo=" + lo + " hi=" + hi + " mid=" + mid + " boundary=" + boundaries[mid] + " to " + boundaries[mid+1]);
+      if (v <= boundaries[mid]) {
+        if (mid == 0) {
+          leafCounts[0]++;
+          return;
+        } else {
+          hi = mid - 1;
+        }
+      } else if (v > boundaries[mid+1]) {
+        lo = mid + 1;
+      } else {
+        leafCounts[mid+1]++;
+        //System.out.println("  incr @ " + (mid+1) + "; now " + leafCounts[mid+1]);
+        return;
+      }
+    }
+  }
+
+  /** Fills counts corresponding to the original input
+   *  ranges, returning the missing count (how many hits
+   *  didn't match any ranges). */
+  public int fillCounts(int[] counts) {
+    //System.out.println("  rollup");
+    missingCount = 0;
+    leafUpto = 0;
+    rollup(root, counts, false);
+    return missingCount;
+  }
+
+  private int rollup(LongRangeNode node, int[] counts, boolean sawOutputs) {
+    int count;
+    sawOutputs |= node.outputs != null;
+    if (node.left != null) {
+      count = rollup(node.left, counts, sawOutputs);
+      count += rollup(node.right, counts, sawOutputs);
+    } else {
+      // Leaf:
+      count = leafCounts[leafUpto];
+      leafUpto++;
+      if (!sawOutputs) {
+        // This is a missing count (no output ranges were
+        // seen "above" us):
+        missingCount += count;
+      }
+    }
+    if (node.outputs != null) {
+      for(int rangeIndex : node.outputs) {
+        counts[rangeIndex] += count;
+      }
+    }
+    //System.out.println("  rollup node=" + node.start + " to " + node.end + ": count=" + count);
+    return count;
+  }
+
+  private static LongRangeNode split(int start, int end, List<InclusiveRange> elementaryIntervals) {
+    if (start == end-1) {
+      // leaf
+      InclusiveRange range = elementaryIntervals.get(start);
+      return new LongRangeNode(range.start, range.end, null, null, start);
+    } else {
+      int mid = (start + end) >>> 1;
+      LongRangeNode left = split(start, mid, elementaryIntervals);
+      LongRangeNode right = split(mid, end, elementaryIntervals);
+      return new LongRangeNode(left.start, right.end, left, right, -1);
+    }
+  }
+
+  private static final class InclusiveRange {
+    public final long start;
+    public final long end;
+
+    public InclusiveRange(long start, long end) {
+      assert end >= start;
+      this.start = start;
+      this.end = end;
+    }
+
+    @Override
+    public String toString() {
+      return start + " to " + end;
+    }
+  }
+
+  /** Holds one node of the segment tree. */
+  public static final class LongRangeNode {
+    final LongRangeNode left;
+    final LongRangeNode right;
+
+    // Our range, inclusive:
+    final long start;
+    final long end;
+
+    // If we are a leaf, the index into elementary ranges that
+    // we point to:
+    final int leafIndex;
+
+    // Which range indices to output when a query goes
+    // through this node:
+    List<Integer> outputs;
+
+    public LongRangeNode(long start, long end, LongRangeNode left, LongRangeNode right, int leafIndex) {
+      this.start = start;
+      this.end = end;
+      this.left = left;
+      this.right = right;
+      this.leafIndex = leafIndex;
+    }
+
+    @Override
+    public String toString() {
+      StringBuilder sb = new StringBuilder();
+      toString(sb, 0);
+      return sb.toString();
+    }
+
+    static void indent(StringBuilder sb, int depth) {
+      for(int i=0;i<depth;i++) {
+        sb.append("  ");
+      }
+    }
+
+    /** Recursively assigns range outputs to each node. */
+    void addOutputs(int index, LongRange range) {
+      if (start >= range.minIncl && end <= range.maxIncl) {
+        // Our range is fully included in the incoming
+        // range; add to our output list:
+        if (outputs == null) {
+          outputs = new ArrayList<Integer>();
+        }
+        outputs.add(index);
+      } else if (left != null) {
+        assert right != null;
+        // Recurse:
+        left.addOutputs(index, range);
+        right.addOutputs(index, range);
+      }
+    }
+
+    void toString(StringBuilder sb, int depth) {
+      indent(sb, depth);
+      if (left == null) {
+        assert right == null;
+        sb.append("leaf: " + start + " to " + end);
+      } else {
+        sb.append("node: " + start + " to " + end);
+      }
+      if (outputs != null) {
+        sb.append(" outputs=");
+        sb.append(outputs);
+      }
+      sb.append('\n');
+
+      if (left != null) {
+        assert right != null;
+        left.toString(sb, depth+1);
+        right.toString(sb, depth+1);
+      }
+    }
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/LongRangeFacetCounts.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/LongRangeFacetCounts.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/LongRangeFacetCounts.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/LongRangeFacetCounts.java	2013-12-17 15:42:00.520284074 -0500
@@ -0,0 +1,93 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.valuesource.LongFieldSource;
+
+/** {@link Facets} implementation that computes counts for
+ *  dynamic long ranges from a provided {@link ValueSource},
+ *  using {@link FunctionValues#longVal}.  Use
+ *  this for dimensions that change in real-time (e.g. a
+ *  relative time based dimension like "Past day", "Past 2
+ *  days", etc.) or that change for each request (e.g. 
+ *  distance from the user's location, "< 1 km", "< 2 km",
+ *  etc.).
+ *
+ *  @lucene.experimental */
+public class LongRangeFacetCounts extends RangeFacetCounts {
+
+  /** Create {@code LongRangeFacetCounts}, using {@link
+   *  LongFieldSource} from the specified field. */
+  public LongRangeFacetCounts(String field, FacetsCollector hits, LongRange... ranges) throws IOException {
+    this(field, new LongFieldSource(field), hits, ranges);
+  }
+
+  /** Create {@code RangeFacetCounts}, using the provided
+   *  {@link ValueSource}. */
+  public LongRangeFacetCounts(String field, ValueSource valueSource, FacetsCollector hits, LongRange... ranges) throws IOException {
+    super(field, ranges);
+    count(valueSource, hits.getMatchingDocs());
+  }
+
+  private void count(ValueSource valueSource, List<MatchingDocs> matchingDocs) throws IOException {
+
+    LongRange[] ranges = (LongRange[]) this.ranges;
+
+    // Compute min & max over all ranges:
+    long minIncl = Long.MAX_VALUE;
+    long maxIncl = Long.MIN_VALUE;
+    for(LongRange range : ranges) {
+      minIncl = Math.min(minIncl, range.minIncl);
+      maxIncl = Math.max(maxIncl, range.maxIncl);
+    }
+
+    LongRangeCounter counter = new LongRangeCounter(ranges);
+
+    int missingCount = 0;
+    for (MatchingDocs hits : matchingDocs) {
+      FunctionValues fv = valueSource.getValues(Collections.emptyMap(), hits.context);
+      final int length = hits.bits.length();
+      int doc = 0;
+      totCount += hits.totalHits;
+      while (doc < length && (doc = hits.bits.nextSetBit(doc)) != -1) {
+        // Skip missing docs:
+        if (fv.exists(doc)) {
+          counter.add(fv.longVal(doc));
+        } else {
+          missingCount++;
+        }
+
+        doc++;
+      }
+    }
+    
+    int x = counter.fillCounts(counts);
+
+    missingCount += x;
+
+    //System.out.println("totCount " + totCount + " missingCount " + counter.missingCount);
+    totCount -= missingCount;
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/LongRange.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/LongRange.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/LongRange.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/LongRange.java	2013-12-19 13:26:07.019879411 -0500
@@ -0,0 +1,154 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+
+import org.apache.lucene.document.NumericDocValuesField; // javadocs
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.NumericRangeFilter; // javadocs
+import org.apache.lucene.util.Bits;
+
+/** Represents a range over long values. */
+public final class LongRange extends Range {
+  final long minIncl;
+  final long maxIncl;
+
+  public final long min;
+  public final long max;
+  public final boolean minInclusive;
+  public final boolean maxInclusive;
+
+  // TODO: can we require fewer args? (same for
+  // Double/FloatRange too)
+
+  /** Create a LongRange. */
+  public LongRange(String label, long minIn, boolean minInclusive, long maxIn, boolean maxInclusive) {
+    super(label);
+    this.min = minIn;
+    this.max = maxIn;
+    this.minInclusive = minInclusive;
+    this.maxInclusive = maxInclusive;
+
+    if (!minInclusive) {
+      if (minIn != Long.MAX_VALUE) {
+        minIn++;
+      } else {
+        failNoMatch();
+      }
+    }
+
+    if (!maxInclusive) {
+      if (maxIn != Long.MIN_VALUE) {
+        maxIn--;
+      } else {
+        failNoMatch();
+      }
+    }
+
+    if (minIn > maxIn) {
+      failNoMatch();
+    }
+
+    this.minIncl = minIn;
+    this.maxIncl = maxIn;
+  }
+
+  public boolean accept(long value) {
+    return value >= minIncl && value <= maxIncl;
+  }
+
+  @Override
+  public String toString() {
+    return "LongRange(" + minIncl + " to " + maxIncl + ")";
+  }
+
+  /** Returns a new {@link Filter} accepting only documents
+   *  in this range.  Note that this filter is not
+   *  efficient: it's a linear scan of all docs, testing
+   *  each value.  If the {@link ValueSource} is static,
+   *  e.g. an indexed numeric field, then it's more
+   *  efficient to use {@link NumericRangeFilter}. */
+  public Filter getFilter(final ValueSource valueSource) {
+    return new Filter() {
+      @Override
+      public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
+
+        // TODO: this is just like ValueSourceScorer,
+        // ValueSourceFilter (spatial),
+        // ValueSourceRangeFilter (solr); also,
+        // https://issues.apache.org/jira/browse/LUCENE-4251
+
+        final FunctionValues values = valueSource.getValues(Collections.emptyMap(), context);
+
+        final int maxDoc = context.reader().maxDoc();
+
+        return new DocIdSet() {
+
+          @Override
+          public DocIdSetIterator iterator() {
+            return new DocIdSetIterator() {
+              int doc = -1;
+
+              @Override
+              public int nextDoc() throws IOException {
+                while (true) {
+                  doc++;
+                  if (doc == maxDoc) {
+                    return doc = NO_MORE_DOCS;
+                  }
+                  if (acceptDocs != null && acceptDocs.get(doc) == false) {
+                    continue;
+                  }
+                  long v = values.longVal(doc);
+                  if (accept(v)) {
+                    return doc;
+                  }
+                }
+              }
+
+              @Override
+              public int advance(int target) throws IOException {
+                doc = target-1;
+                return nextDoc();
+              }
+
+              @Override
+              public int docID() {
+                return doc;
+              }
+
+              @Override
+              public long cost() {
+                // Since we do a linear scan over all
+                // documents, our cost is O(maxDoc):
+                return maxDoc;
+              }
+            };
+          }
+        };
+      }
+    };
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/MultiFacets.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/MultiFacets.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/MultiFacets.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/MultiFacets.java	2013-12-18 19:04:51.821646904 -0500
@@ -0,0 +1,65 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+
+/** Maps specified dims to provided Facets impls; else, uses
+ *  the default Facets impl. */
+public class MultiFacets extends Facets {
+  private final Map<String,Facets> dimToFacets;
+  private final Facets defaultFacets;
+
+  public MultiFacets(Map<String,Facets> dimToFacets) {
+    this(dimToFacets = dimToFacets, null);
+  }
+
+  public MultiFacets(Map<String,Facets> dimToFacets, Facets defaultFacets) {
+    this.dimToFacets = dimToFacets;
+    this.defaultFacets = defaultFacets;
+  }
+
+  public FacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    Facets facets = dimToFacets.get(dim);
+    if (facets == null) {
+      if (defaultFacets == null) {
+        throw new IllegalArgumentException("invalid dim \"" + dim + "\"");
+      }
+      facets = defaultFacets;
+    }
+    return facets.getTopChildren(topN, dim, path);
+  }
+
+  public Number getSpecificValue(String dim, String... path) throws IOException {
+    Facets facets = dimToFacets.get(dim);
+    if (facets == null) {
+      if (defaultFacets == null) {
+        throw new IllegalArgumentException("invalid dim \"" + dim + "\"");
+      }
+      facets = defaultFacets;
+    }
+    return facets.getSpecificValue(dim, path);
+  }
+
+  public List<FacetResult> getAllDims(int topN) throws IOException {
+    // TODO
+    throw new UnsupportedOperationException();
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/old/AdaptiveFacetsAccumulator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/AdaptiveFacetsAccumulator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/old/AdaptiveFacetsAccumulator.java	2013-07-29 13:55:02.641707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/AdaptiveFacetsAccumulator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,116 +0,0 @@
-package org.apache.lucene.facet.old;
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.sampling.RandomSampler;
-import org.apache.lucene.facet.sampling.Sampler;
-import org.apache.lucene.facet.sampling.SamplingAccumulator;
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetsAccumulator;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.IndexReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * {@link FacetsAccumulator} whose behavior regarding complements, sampling,
- * etc. is not set up front but rather is determined at accumulation time
- * according to the statistics of the accumulated set of documents and the
- * index.
- * <p>
- * Note: Sampling accumulation (Accumulation over a sampled-set of the results),
- * does not guarantee accurate values for
- * {@link FacetResult#getNumValidDescendants()}.
- * 
- * @lucene.experimental
- */
-public final class AdaptiveFacetsAccumulator extends OldFacetsAccumulator {
-  
-  private Sampler sampler = new RandomSampler();
-
-  /**
-   * Create an {@link AdaptiveFacetsAccumulator} 
-   * @see OldFacetsAccumulator#OldFacetsAccumulator(FacetSearchParams, IndexReader, TaxonomyReader)
-   */
-  public AdaptiveFacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader, 
-      TaxonomyReader taxonomyReader) {
-    super(searchParams, indexReader, taxonomyReader);
-  }
-
-  /**
-   * Create an {@link AdaptiveFacetsAccumulator}
-   * 
-   * @see OldFacetsAccumulator#OldFacetsAccumulator(FacetSearchParams,
-   *      IndexReader, TaxonomyReader, FacetArrays)
-   */
-  public AdaptiveFacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader,
-      TaxonomyReader taxonomyReader, FacetArrays facetArrays) {
-    super(searchParams, indexReader, taxonomyReader, facetArrays);
-  }
-
-  /**
-   * Set the sampler.
-   * @param sampler sampler to set
-   */
-  public void setSampler(Sampler sampler) {
-    this.sampler = sampler;
-  }
-  
-  @Override
-  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {
-    OldFacetsAccumulator delegee = appropriateFacetCountingAccumulator(docids);
-
-    if (delegee == this) {
-      return super.accumulate(docids);
-    }
-
-    return delegee.accumulate(docids);
-  }
-
-  /**
-   * Compute the appropriate facet accumulator to use.
-   * If no special/clever adaptation is possible/needed return this (self).
-   */
-  private OldFacetsAccumulator appropriateFacetCountingAccumulator(ScoredDocIDs docids) {
-    // Verify that searchPareams permit sampling/complement/etc... otherwise do default
-    if (!mayComplement()) {
-      return this;
-    }
-    
-    // Now we're sure we can use the sampling methods as we're in a counting only mode
-    
-    // Verify that sampling is enabled and required ... otherwise do default
-    if (sampler == null || !sampler.shouldSample(docids)) {
-      return this;
-    }
-    
-    SamplingAccumulator samplingAccumulator = new SamplingAccumulator(sampler, searchParams, indexReader, taxonomyReader);
-    samplingAccumulator.setComplementThreshold(getComplementThreshold());
-    return samplingAccumulator;
-  }
-
-  /**
-   * @return the sampler in effect
-   */
-  public final Sampler getSampler() {
-    return sampler;
-  }
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/old/Aggregator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/Aggregator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/old/Aggregator.java	2013-07-29 13:57:07.349704317 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/Aggregator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,48 +0,0 @@
-package org.apache.lucene.facet.old;
-
-import java.io.IOException;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Aggregates the categories of documents given to
- * {@link #aggregate(int, float, IntsRef)}. Note that the document IDs are local
- * to the reader given to {@link #setNextReader(AtomicReaderContext)}.
- * 
- * @lucene.experimental
- */
-public interface Aggregator {
-
-  /**
-   * Sets the {@link AtomicReaderContext} for which
-   * {@link #aggregate(int, float, IntsRef)} calls will be made. If this method
-   * returns false, {@link #aggregate(int, float, IntsRef)} should not be called
-   * for this reader.
-   */
-  public boolean setNextReader(AtomicReaderContext context) throws IOException;
-  
-  /**
-   * Aggregate the ordinals of the given document ID (and its score). The given
-   * ordinals offset is always zero.
-   */
-  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException;
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/old/ComplementCountingAggregator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/ComplementCountingAggregator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/old/ComplementCountingAggregator.java	2013-07-29 13:55:02.641707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/ComplementCountingAggregator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,44 +0,0 @@
-package org.apache.lucene.facet.old;
-
-import java.io.IOException;
-
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link CountingAggregator} used during complement counting.
- * 
- * @lucene.experimental
- */
-public class ComplementCountingAggregator extends CountingAggregator {
-
-  public ComplementCountingAggregator(int[] counterArray) {
-    super(counterArray);
-  }
-
-  @Override
-  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
-    for (int i = 0; i < ordinals.length; i++) {
-      int ord = ordinals.ints[i];
-      assert counterArray[ord] != 0 : "complement aggregation: count is about to become negative for ordinal " + ord;
-      --counterArray[ord];
-    }
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/old/CountingAggregator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/CountingAggregator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/old/CountingAggregator.java	2013-07-29 13:55:02.641707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/CountingAggregator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,66 +0,0 @@
-package org.apache.lucene.facet.old;
-
-import java.io.IOException;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link Aggregator} which updates a counter array with the size of the
- * whole taxonomy, counting the number of times each category appears in the
- * given set of documents.
- * 
- * @lucene.experimental
- */
-public class CountingAggregator implements Aggregator {
-
-  protected int[] counterArray;
-  
-  public CountingAggregator(int[] counterArray) {
-    this.counterArray = counterArray;
-  }
-  
-  @Override
-  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
-    for (int i = 0; i < ordinals.length; i++) {
-      counterArray[ordinals.ints[i]]++;
-    }
-  }
-
-  @Override
-  public boolean equals(Object obj) {
-    if (obj == null || obj.getClass() != this.getClass()) {
-      return false;
-    }
-    CountingAggregator that = (CountingAggregator) obj;
-    return that.counterArray == this.counterArray;
-  }
-
-  @Override
-  public int hashCode() {
-    return counterArray == null ? 0 : counterArray.hashCode();
-  }
-  
-  @Override
-  public boolean setNextReader(AtomicReaderContext context) throws IOException {
-    return true;
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/old/MatchingDocsAsScoredDocIDs.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/MatchingDocsAsScoredDocIDs.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/old/MatchingDocsAsScoredDocIDs.java	2013-07-29 13:55:02.641707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/MatchingDocsAsScoredDocIDs.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,174 +0,0 @@
-package org.apache.lucene.facet.old;
-
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.List;
-
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.search.DocIdSet;
-import org.apache.lucene.search.DocIdSetIterator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** 
- * Represents {@link MatchingDocs} as {@link ScoredDocIDs}.
- * 
- * @lucene.experimental
- */
-public class MatchingDocsAsScoredDocIDs implements ScoredDocIDs {
-
-  // TODO remove this class once we get rid of ScoredDocIDs 
-
-  final List<MatchingDocs> matchingDocs;
-  final int size;
-  
-  public MatchingDocsAsScoredDocIDs(List<MatchingDocs> matchingDocs) {
-    this.matchingDocs = matchingDocs;
-    int totalSize = 0;
-    for (MatchingDocs md : matchingDocs) {
-      totalSize += md.totalHits;
-    }
-    this.size = totalSize;
-  }
-  
-  @Override
-  public ScoredDocIDsIterator iterator() throws IOException {
-    return new ScoredDocIDsIterator() {
-      
-      final Iterator<MatchingDocs> mdIter = matchingDocs.iterator();
-      
-      int scoresIdx = 0;
-      int doc = 0;
-      MatchingDocs current;
-      int currentLength;
-      boolean done = false;
-      
-      @Override
-      public boolean next() {
-        if (done) {
-          return false;
-        }
-        
-        while (current == null) {
-          if (!mdIter.hasNext()) {
-            done = true;
-            return false;
-          }
-          current = mdIter.next();
-          currentLength = current.bits.length();
-          doc = 0;
-          scoresIdx = 0;
-          
-          if (doc >= currentLength || (doc = current.bits.nextSetBit(doc)) == -1) {
-            current = null;
-          } else {
-            doc = -1; // we're calling nextSetBit later on
-          }
-        }
-        
-        ++doc;
-        if (doc >= currentLength || (doc = current.bits.nextSetBit(doc)) == -1) {
-          current = null;
-          return next();
-        }
-        
-        return true;
-      }
-      
-      @Override
-      public float getScore() {
-        return current.scores == null ? ScoredDocIDsIterator.DEFAULT_SCORE : current.scores[scoresIdx++];
-      }
-      
-      @Override
-      public int getDocID() {
-        return done ? DocIdSetIterator.NO_MORE_DOCS : doc + current.context.docBase;
-      }
-    };
-  }
-
-  @Override
-  public DocIdSet getDocIDs() {
-    return new DocIdSet() {
-      
-      final Iterator<MatchingDocs> mdIter = matchingDocs.iterator();
-      int doc = 0;
-      MatchingDocs current;
-      int currentLength;
-      boolean done = false;
-      
-      @Override
-      public DocIdSetIterator iterator() throws IOException {
-        return new DocIdSetIterator() {
-          
-          @Override
-          public int nextDoc() throws IOException {
-            if (done) {
-              return DocIdSetIterator.NO_MORE_DOCS;
-            }
-            
-            while (current == null) {
-              if (!mdIter.hasNext()) {
-                done = true;
-                return DocIdSetIterator.NO_MORE_DOCS;
-              }
-              current = mdIter.next();
-              currentLength = current.bits.length();
-              doc = 0;
-              
-              if (doc >= currentLength || (doc = current.bits.nextSetBit(doc)) == -1) {
-                current = null;
-              } else {
-                doc = -1; // we're calling nextSetBit later on
-              }
-            }
-            
-            ++doc;
-            if (doc >= currentLength || (doc = current.bits.nextSetBit(doc)) == -1) {
-              current = null;
-              return nextDoc();
-            }
-            
-            return doc + current.context.docBase;
-          }
-          
-          @Override
-          public int docID() {
-            return doc + current.context.docBase;
-          }
-          
-          @Override
-          public long cost() {
-            return size;
-          }
-
-          @Override
-          public int advance(int target) throws IOException {
-            throw new UnsupportedOperationException("not supported");
-          }
-        };
-      }
-    };
-  }
-
-  @Override
-  public int size() {
-    return size;
-  }
-  
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/old/OldFacetsAccumulator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/OldFacetsAccumulator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/old/OldFacetsAccumulator.java	2013-08-01 14:47:20.754689726 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/OldFacetsAccumulator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,457 +0,0 @@
-package org.apache.lucene.facet.old;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map.Entry;
-
-import org.apache.lucene.facet.complements.TotalFacetCounts;
-import org.apache.lucene.facet.complements.TotalFacetCountsCache;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.partitions.IntermediateFacetResult;
-import org.apache.lucene.facet.partitions.PartitionsFacetResultsHandler;
-import org.apache.lucene.facet.sampling.Sampler.OverSampledFacetRequest;
-import org.apache.lucene.facet.search.CategoryListIterator;
-import org.apache.lucene.facet.search.CountFacetRequest;
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetRequest.ResultMode;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetsAccumulator;
-import org.apache.lucene.facet.search.FacetsAggregator;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.search.OrdinalValueResolver;
-import org.apache.lucene.facet.search.OrdinalValueResolver.FloatValueResolver;
-import org.apache.lucene.facet.search.OrdinalValueResolver.IntValueResolver;
-import org.apache.lucene.facet.search.SumScoreFacetRequest;
-import org.apache.lucene.facet.search.TaxonomyFacetsAccumulator;
-import org.apache.lucene.facet.search.TopKFacetResultsHandler;
-import org.apache.lucene.facet.search.TopKInEachNodeHandler;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.util.PartitionsUtils;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetsAccumulator} which supports partitions, sampling and
- * complement counting.
- * <p>
- * <b>NOTE:</b> this accumulator still uses the old API and will be removed
- * eventually in favor of dedicated accumulators which support the above
- * features ovee the new {@link FacetsAggregator} API. It provides
- * {@link Aggregator} implementations for {@link CountFacetRequest},
- * {@link SumScoreFacetRequest} and {@link OverSampledFacetRequest}. If you need
- * to use it in conjunction with other facet requests, you should override
- * {@link #createAggregator(FacetRequest, FacetArrays)}.
- * 
- * @lucene.experimental
- */
-public class OldFacetsAccumulator extends TaxonomyFacetsAccumulator {
-
-  /**
-   * Default threshold for using the complements optimization.
-   * If accumulating facets for a document set larger than this ratio of the index size than 
-   * perform the complement optimization.
-   * @see #setComplementThreshold(double) for more info on the complements optimization.  
-   */
-  public static final double DEFAULT_COMPLEMENT_THRESHOLD = 0.6;
-
-  /**
-   * Passing this to {@link #setComplementThreshold(double)} will disable using complement optimization.
-   */
-  public static final double DISABLE_COMPLEMENT = Double.POSITIVE_INFINITY; // > 1 actually
-
-  /**
-   * Passing this to {@link #setComplementThreshold(double)} will force using complement optimization.
-   */
-  public static final double FORCE_COMPLEMENT = 0; // <=0  
-
-  protected int partitionSize;
-  protected int maxPartitions;
-  protected boolean isUsingComplements;
-
-  private TotalFacetCounts totalFacetCounts;
-
-  private Object accumulateGuard;
-
-  private double complementThreshold = DEFAULT_COMPLEMENT_THRESHOLD;
-  
-  private static FacetArrays createFacetArrays(FacetSearchParams searchParams, TaxonomyReader taxoReader) {
-    return new FacetArrays(PartitionsUtils.partitionSize(searchParams.indexingParams, taxoReader)); 
-  }
-  
-  public OldFacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader,  
-      TaxonomyReader taxonomyReader) {
-    this(searchParams, indexReader, taxonomyReader, null);
-  }
-
-  public OldFacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader,
-      TaxonomyReader taxonomyReader, FacetArrays facetArrays) {
-    super(searchParams, indexReader, taxonomyReader, facetArrays == null ? createFacetArrays(searchParams, taxonomyReader) : facetArrays);
-    
-    // can only be computed later when docids size is known
-    isUsingComplements = false;
-    partitionSize = PartitionsUtils.partitionSize(searchParams.indexingParams, taxonomyReader);
-    maxPartitions = (int) Math.ceil(this.taxonomyReader.getSize() / (double) partitionSize);
-    accumulateGuard = new Object();
-  }
-
-  // TODO: this should be removed once we clean the API
-  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {
-
-    // synchronize to prevent calling two accumulate()'s at the same time.
-    // We decided not to synchronize the method because that might mislead
-    // users to feel encouraged to call this method simultaneously.
-    synchronized (accumulateGuard) {
-
-      // only now we can compute this
-      isUsingComplements = shouldComplement(docids);
-
-      if (isUsingComplements) {
-        try {
-          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, searchParams.indexingParams);
-          if (totalFacetCounts != null) {
-            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);
-          } else {
-            isUsingComplements = false;
-          }
-        } catch (UnsupportedOperationException e) {
-          // TODO (Facet): this exception is thrown from TotalCountsKey if the
-          // IndexReader used does not support getVersion(). We should re-think
-          // this: is this tiny detail worth disabling total counts completely
-          // for such readers? Currently, it's not supported by Parallel and
-          // MultiReader, which might be problematic for several applications.
-          // We could, for example, base our "isCurrent" logic on something else
-          // than the reader's version. Need to think more deeply about it.
-          isUsingComplements = false;
-        } catch (IOException e) {
-          // silently fail if for some reason failed to load/save from/to dir 
-          isUsingComplements = false;
-        } catch (Exception e) {
-          // give up: this should not happen!
-          throw new IOException("PANIC: Got unexpected exception while trying to get/calculate total counts", e);
-        }
-      }
-
-      docids = actualDocsToAccumulate(docids);
-
-      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();
-
-      try {
-        for (int part = 0; part < maxPartitions; part++) {
-
-          // fill arrays from category lists
-          fillArraysForPartition(docids, facetArrays, part);
-
-          int offset = part * partitionSize;
-
-          // for each partition we go over all requests and handle
-          // each, where the request maintains the merged result.
-          // In this implementation merges happen after each partition,
-          // but other impl could merge only at the end.
-          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();
-          for (FacetRequest fr : searchParams.facetRequests) {
-            // Handle and merge only facet requests which were not already handled.  
-            if (handledRequests.add(fr)) {
-              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr, createOrdinalValueResolver(fr));
-              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);
-              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);
-              if (oldRes != null) {
-                res4fr = frHndlr.mergeResults(oldRes, res4fr);
-              }
-              fr2tmpRes.put(fr, res4fr);
-            } 
-          }
-        }
-      } finally {
-        facetArrays.free();
-      }
-
-      // gather results from all requests into a list for returning them
-      List<FacetResult> res = new ArrayList<FacetResult>();
-      for (FacetRequest fr : searchParams.facetRequests) {
-        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr, createOrdinalValueResolver(fr));
-        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);
-        if (tmpResult == null) {
-          // Add empty FacetResult:
-          res.add(emptyResult(taxonomyReader.getOrdinal(fr.categoryPath), fr));
-          continue;
-        }
-        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);
-        // final labeling if allowed (because labeling is a costly operation)
-        frHndlr.labelResult(facetRes);
-        res.add(facetRes);
-      }
-
-      return res;
-    }
-  }
-
-  /** check if all requests are complementable */
-  protected boolean mayComplement() {
-    for (FacetRequest freq : searchParams.facetRequests) {
-      if (!(freq instanceof CountFacetRequest)) {
-        return false;
-      }
-    }
-    return true;
-  }
-
-  @Override
-  public PartitionsFacetResultsHandler createFacetResultsHandler(FacetRequest fr, OrdinalValueResolver resolver) {
-    if (fr.getResultMode() == ResultMode.PER_NODE_IN_TREE) {
-      return new TopKInEachNodeHandler(taxonomyReader, fr, resolver, facetArrays);
-    } else {
-      return new TopKFacetResultsHandler(taxonomyReader, fr, resolver, facetArrays);
-    }
-  }
-  
-  /**
-   * Set the actual set of documents over which accumulation should take place.
-   * <p>
-   * Allows to override the set of documents to accumulate for. Invoked just
-   * before actual accumulating starts. From this point that set of documents
-   * remains unmodified. Default implementation just returns the input
-   * unchanged.
-   * 
-   * @param docids
-   *          candidate documents to accumulate for
-   * @return actual documents to accumulate for
-   */
-  protected ScoredDocIDs actualDocsToAccumulate(ScoredDocIDs docids) throws IOException {
-    return docids;
-  }
-
-  /** Check if it is worth to use complements */
-  protected boolean shouldComplement(ScoredDocIDs docids) {
-    return mayComplement() && (docids.size() > indexReader.numDocs() * getComplementThreshold()) ;
-  }
-
-  /**
-   * Creates an {@link OrdinalValueResolver} for the given {@link FacetRequest}.
-   * By default this method supports {@link CountFacetRequest} and
-   * {@link SumScoreFacetRequest}. You should override if you are using other
-   * requests with this accumulator.
-   */
-  public OrdinalValueResolver createOrdinalValueResolver(FacetRequest fr) {
-    if (fr instanceof CountFacetRequest) {
-      return new IntValueResolver(facetArrays);
-    } else if (fr instanceof SumScoreFacetRequest) {
-      return new FloatValueResolver(facetArrays);
-    } else if (fr instanceof OverSampledFacetRequest) {
-      return createOrdinalValueResolver(((OverSampledFacetRequest) fr).orig);
-    } else {
-      throw new IllegalArgumentException("unrecognized FacetRequest " + fr.getClass());
-    }
-  }
-  
-  /**
-   * Iterate over the documents for this partition and fill the facet arrays with the correct
-   * count/complement count/value.
-   */
-  private final void fillArraysForPartition(ScoredDocIDs docids, FacetArrays facetArrays, int partition) 
-      throws IOException {
-    
-    if (isUsingComplements) {
-      initArraysByTotalCounts(facetArrays, partition, docids.size());
-    } else {
-      facetArrays.free(); // to get a cleared array for this partition
-    }
-
-    HashMap<CategoryListIterator, Aggregator> categoryLists = getCategoryListMap(facetArrays, partition);
-
-    IntsRef ordinals = new IntsRef(32); // a reasonable start capacity for most common apps
-    for (Entry<CategoryListIterator, Aggregator> entry : categoryLists.entrySet()) {
-      final ScoredDocIDsIterator iterator = docids.iterator();
-      final CategoryListIterator categoryListIter = entry.getKey();
-      final Aggregator aggregator = entry.getValue();
-      Iterator<AtomicReaderContext> contexts = indexReader.leaves().iterator();
-      AtomicReaderContext current = null;
-      int maxDoc = -1;
-      while (iterator.next()) {
-        int docID = iterator.getDocID();
-        if (docID >= maxDoc) {
-          boolean iteratorDone = false;
-          do { // find the segment which contains this document
-            if (!contexts.hasNext()) {
-              throw new RuntimeException("ScoredDocIDs contains documents outside this reader's segments !?");
-            }
-            current = contexts.next();
-            maxDoc = current.docBase + current.reader().maxDoc();
-            if (docID < maxDoc) { // segment has docs, check if it has categories
-              boolean validSegment = categoryListIter.setNextReader(current);
-              validSegment &= aggregator.setNextReader(current);
-              if (!validSegment) { // if categoryList or aggregtor say it's an invalid segment, skip all docs
-                while (docID < maxDoc && iterator.next()) {
-                  docID = iterator.getDocID();
-                }
-                if (docID < maxDoc) {
-                  iteratorDone = true;
-                }
-              }
-            }
-          } while (docID >= maxDoc);
-          if (iteratorDone) { // iterator finished, terminate the loop
-            break;
-          }
-        }
-        docID -= current.docBase;
-        categoryListIter.getOrdinals(docID, ordinals);
-        if (ordinals.length == 0) {
-          continue; // document does not have category ordinals
-        }
-        aggregator.aggregate(docID, iterator.getScore(), ordinals);
-      }
-    }
-  }
-
-  /** Init arrays for partition by total counts, optionally applying a factor */
-  private final void initArraysByTotalCounts(FacetArrays facetArrays, int partition, int nAccumulatedDocs) {
-    int[] intArray = facetArrays.getIntArray();
-    totalFacetCounts.fillTotalCountsForPartition(intArray, partition);
-    double totalCountsFactor = getTotalCountsFactor();
-    // fix total counts, but only if the effect of this would be meaningful. 
-    if (totalCountsFactor < 0.99999) {
-      int delta = nAccumulatedDocs + 1;
-      for (int i = 0; i < intArray.length; i++) {
-        intArray[i] *= totalCountsFactor;
-        // also translate to prevent loss of non-positive values
-        // due to complement sampling (ie if sampled docs all decremented a certain category). 
-        intArray[i] += delta; 
-      }
-    }
-  }
-
-  /**
-   * Expert: factor by which counts should be multiplied when initializing
-   * the count arrays from total counts.
-   * Default implementation for this returns 1, which is a no op.  
-   * @return a factor by which total counts should be multiplied
-   */
-  protected double getTotalCountsFactor() {
-    return 1;
-  }
-
-  protected Aggregator createAggregator(FacetRequest fr, FacetArrays facetArrays) {
-    if (fr instanceof CountFacetRequest) {
-      // we rely on that, if needed, result is cleared by arrays!
-      int[] a = facetArrays.getIntArray();
-      if (isUsingComplements) {
-        return new ComplementCountingAggregator(a);
-      } else {
-        return new CountingAggregator(a);
-      }
-    } else if (fr instanceof SumScoreFacetRequest) {
-      if (isUsingComplements) {
-        throw new IllegalArgumentException("complements are not supported by SumScoreFacetRequest");
-      } else {
-        return new ScoringAggregator(facetArrays.getFloatArray());
-      }
-    } else if (fr instanceof OverSampledFacetRequest) {
-      return createAggregator(((OverSampledFacetRequest) fr).orig, facetArrays);
-    } else {
-      throw new IllegalArgumentException("unknown Aggregator implementation for request " + fr.getClass());
-    }
-  }
-  
-  /**
-   * Create an {@link Aggregator} and a {@link CategoryListIterator} for each
-   * and every {@link FacetRequest}. Generating a map, matching each
-   * categoryListIterator to its matching aggregator.
-   * <p>
-   * If two CategoryListIterators are served by the same aggregator, a single
-   * aggregator is returned for both.
-   * 
-   * <b>NOTE: </b>If a given category list iterator is needed with two different
-   * aggregators (e.g counting and association) - an exception is thrown as this
-   * functionality is not supported at this time.
-   */
-  protected HashMap<CategoryListIterator, Aggregator> getCategoryListMap(FacetArrays facetArrays,
-      int partition) throws IOException {
-    
-    HashMap<CategoryListIterator, Aggregator> categoryLists = new HashMap<CategoryListIterator, Aggregator>();
-
-    FacetIndexingParams indexingParams = searchParams.indexingParams;
-    for (FacetRequest facetRequest : searchParams.facetRequests) {
-      Aggregator categoryAggregator = createAggregator(facetRequest, facetArrays);
-
-      CategoryListIterator cli = indexingParams.getCategoryListParams(facetRequest.categoryPath).createCategoryListIterator(partition);
-      
-      // get the aggregator
-      Aggregator old = categoryLists.put(cli, categoryAggregator);
-
-      if (old != null && !old.equals(categoryAggregator)) {
-        throw new RuntimeException("Overriding existing category list with different aggregator");
-      }
-      // if the aggregator is the same we're covered
-    }
-
-    return categoryLists;
-  }
-  
-  @Override
-  public List<FacetResult> accumulate(List<MatchingDocs> matchingDocs) throws IOException {
-    return accumulate(new MatchingDocsAsScoredDocIDs(matchingDocs));
-  }
-
-  /**
-   * Returns the complement threshold.
-   * @see #setComplementThreshold(double)
-   */
-  public double getComplementThreshold() {
-    return complementThreshold;
-  }
-
-  /**
-   * Set the complement threshold.
-   * This threshold will dictate whether the complements optimization is applied.
-   * The optimization is to count for less documents. It is useful when the same 
-   * FacetSearchParams are used for varying sets of documents. The first time 
-   * complements is used the "total counts" are computed - counting for all the 
-   * documents in the collection. Then, only the complementing set of documents
-   * is considered, and used to decrement from the overall counts, thereby 
-   * walking through less documents, which is faster.
-   * <p>
-   * For the default settings see {@link #DEFAULT_COMPLEMENT_THRESHOLD}.
-   * <p>
-   * To forcing complements in all cases pass {@link #FORCE_COMPLEMENT}.
-   * This is mostly useful for testing purposes, as forcing complements when only 
-   * tiny fraction of available documents match the query does not make sense and 
-   * would incur performance degradations.
-   * <p>
-   * To disable complements pass {@link #DISABLE_COMPLEMENT}.
-   * @param complementThreshold the complement threshold to set
-   * @see #getComplementThreshold()
-   */
-  public void setComplementThreshold(double complementThreshold) {
-    this.complementThreshold = complementThreshold;
-  }
-
-  /** Returns true if complements are enabled. */
-  public boolean isUsingComplements() {
-    return isUsingComplements;
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/old/package.html simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/package.html
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/old/package.html	2013-07-29 13:55:02.641707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/package.html	1969-12-31 19:00:00.000000000 -0500
@@ -1,24 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Old Faceted Search API</title>
-</head>
-<body>
-Old faceted search API, kept until complements, sampling and partitions are migrated to the new API.
-</body>
-</html>
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIDsIterator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIDsIterator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIDsIterator.java	2013-07-29 13:55:02.641707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIDsIterator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,43 +0,0 @@
-package org.apache.lucene.facet.old;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Iterator over document IDs and their scores. Each {@link #next()} retrieves
- * the next docID and its score which can be later be retrieved by
- * {@link #getDocID()} and {@link #getScore()}. <b>NOTE:</b> you must call
- * {@link #next()} before {@link #getDocID()} and/or {@link #getScore()}, or
- * otherwise the returned values are unexpected.
- * 
- * @lucene.experimental
- */
-public interface ScoredDocIDsIterator {
-
-  /** Default score used in case scoring is disabled. */
-  public static final float DEFAULT_SCORE = 1.0f;
-
-  /** Iterate to the next document/score pair. Returns true iff there is such a pair. */
-  public abstract boolean next();
-
-  /** Returns the ID of the current document. */
-  public abstract int getDocID();
-
-  /** Returns the score of the current document. */
-  public abstract float getScore();
-
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIDs.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIDs.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIDs.java	2013-07-29 13:55:02.641707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIDs.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,42 +0,0 @@
-package org.apache.lucene.facet.old;
-
-import java.io.IOException;
-
-import org.apache.lucene.search.DocIdSet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Document IDs with scores for each, driving facets accumulation. Document
- * scores are optionally used in the process of facets scoring.
- * 
- * @see OldFacetsAccumulator#accumulate(ScoredDocIDs)
- * @lucene.experimental
- */
-public interface ScoredDocIDs {
-
-  /** Returns an iterator over the document IDs and their scores. */
-  public ScoredDocIDsIterator iterator() throws IOException;
-
-  /** Returns the set of doc IDs. */
-  public DocIdSet getDocIDs();
-
-  /** Returns the number of scored documents. */
-  public int size();
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIdsUtils.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIdsUtils.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIdsUtils.java	2013-07-29 13:55:02.641707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIdsUtils.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,446 +0,0 @@
-package org.apache.lucene.facet.old;
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.facet.old.ScoredDocIDs;
-import org.apache.lucene.facet.old.ScoredDocIDsIterator;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.search.DocIdSet;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.FixedBitSet;
-import org.apache.lucene.util.OpenBitSetDISI;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Utility methods for Scored Doc IDs.
- * 
- * @lucene.experimental
- */
-public class ScoredDocIdsUtils {
-
-  /**
-   * Create a complement of the input set. The returned {@link ScoredDocIDs}
-   * does not contain any scores, which makes sense given that the complementing
-   * documents were not scored.
-   * 
-   * Note: the complement set does NOT contain doc ids which are noted as deleted by the given reader
-   * 
-   * @param docids to be complemented.
-   * @param reader holding the number of documents & information about deletions.
-   */
-  public final static ScoredDocIDs getComplementSet(final ScoredDocIDs docids, final IndexReader reader)
-      throws IOException {
-    final int maxDoc = reader.maxDoc();
-
-    DocIdSet docIdSet = docids.getDocIDs();
-    final FixedBitSet complement;
-    if (docIdSet instanceof FixedBitSet) {
-      // That is the most common case, if ScoredDocIdsCollector was used.
-      complement = ((FixedBitSet) docIdSet).clone();
-    } else {
-      complement = new FixedBitSet(maxDoc);
-      DocIdSetIterator iter = docIdSet.iterator();
-      int doc;
-      while ((doc = iter.nextDoc()) < maxDoc) {
-        complement.set(doc);
-      }
-    }
-    complement.flip(0, maxDoc);
-    clearDeleted(reader, complement);
-
-    return createScoredDocIds(complement, maxDoc);
-  }
-  
-  /** Clear all deleted documents from a given open-bit-set according to a given reader */
-  private static void clearDeleted(final IndexReader reader, final FixedBitSet set) throws IOException {
-    // TODO use BitsFilteredDocIdSet?
-    
-    // If there are no deleted docs
-    if (!reader.hasDeletions()) {
-      return; // return immediately
-    }
-    
-    DocIdSetIterator it = set.iterator();
-    int doc = it.nextDoc(); 
-    for (AtomicReaderContext context : reader.leaves()) {
-      AtomicReader r = context.reader();
-      final int maxDoc = r.maxDoc() + context.docBase;
-      if (doc >= maxDoc) { // skip this segment
-        continue;
-      }
-      if (!r.hasDeletions()) { // skip all docs that belong to this reader as it has no deletions
-        while ((doc = it.nextDoc()) < maxDoc) {}
-        continue;
-      }
-      Bits liveDocs = r.getLiveDocs();
-      do {
-        if (!liveDocs.get(doc - context.docBase)) {
-          set.clear(doc);
-        }
-      } while ((doc = it.nextDoc()) < maxDoc);
-    }
-  }
-  
-  /**
-   * Create a subset of an existing ScoredDocIDs object.
-   * 
-   * @param allDocIds orginal set
-   * @param sampleSet Doc Ids of the subset.
-   */
-  public static final ScoredDocIDs createScoredDocIDsSubset(final ScoredDocIDs allDocIds,
-      final int[] sampleSet) throws IOException {
-
-    // sort so that we can scan docs in order
-    final int[] docids = sampleSet;
-    Arrays.sort(docids);
-    final float[] scores = new float[docids.length];
-    // fetch scores and compute size
-    ScoredDocIDsIterator it = allDocIds.iterator();
-    int n = 0;
-    while (it.next() && n < docids.length) {
-      int doc = it.getDocID();
-      if (doc == docids[n]) {
-        scores[n] = it.getScore();
-        ++n;
-      }
-    }
-    final int size = n;
-
-    return new ScoredDocIDs() {
-
-      @Override
-      public DocIdSet getDocIDs() {
-        return new DocIdSet() {
-
-          @Override
-          public boolean isCacheable() { return true; }
-
-          @Override
-          public DocIdSetIterator iterator() {
-            return new DocIdSetIterator() {
-
-              private int next = -1;
-
-              @Override
-              public int advance(int target) {
-                while (next < size && docids[next++] < target) {
-                }
-                return next == size ? NO_MORE_DOCS : docids[next];
-              }
-
-              @Override
-              public int docID() {
-                return docids[next];
-              }
-
-              @Override
-              public int nextDoc() {
-                if (++next >= size) {
-                  return NO_MORE_DOCS;
-                }
-                return docids[next];
-              }
-
-              @Override
-              public long cost() {
-                return size;
-              }
-            };
-          }
-        };
-      }
-
-      @Override
-      public ScoredDocIDsIterator iterator() {
-        return new ScoredDocIDsIterator() {
-
-          int next = -1;
-
-          @Override
-          public boolean next() { return ++next < size; }
-
-          @Override
-          public float getScore() { return scores[next]; }
-
-          @Override
-          public int getDocID() { return docids[next]; }
-        };
-      }
-
-      @Override
-      public int size() { return size; }
-
-    };
-  }
-
-  /**
-   * Creates a {@link ScoredDocIDs} which returns document IDs all non-deleted doc ids 
-   * according to the given reader. 
-   * The returned set contains the range of [0 .. reader.maxDoc ) doc ids
-   */
-  public static final ScoredDocIDs createAllDocsScoredDocIDs (final IndexReader reader) {
-    if (reader.hasDeletions()) {
-      return new AllLiveDocsScoredDocIDs(reader);
-    }
-    return new AllDocsScoredDocIDs(reader);
-  }
-
-  /**
-   * Create a ScoredDocIDs out of a given docIdSet and the total number of documents in an index  
-   */
-  public static final ScoredDocIDs createScoredDocIds(final DocIdSet docIdSet, final int maxDoc) {
-    return new ScoredDocIDs() {
-      private int size = -1;
-      @Override
-      public DocIdSet getDocIDs() { return docIdSet; }
-
-      @Override
-      public ScoredDocIDsIterator iterator() throws IOException {
-        final DocIdSetIterator docIterator = docIdSet.iterator();
-        return new ScoredDocIDsIterator() {
-          @Override
-          public boolean next() {
-            try {
-              return docIterator.nextDoc() != DocIdSetIterator.NO_MORE_DOCS;
-            } catch (IOException e) {
-              throw new RuntimeException(e);
-            }
-          }
-
-          @Override
-          public float getScore() { return DEFAULT_SCORE; }
-
-          @Override
-          public int getDocID() { return docIterator.docID(); }
-        };
-      }
-
-      @Override
-      public int size() {
-        // lazy size computation
-        if (size < 0) {
-          OpenBitSetDISI openBitSetDISI;
-          try {
-            openBitSetDISI = new OpenBitSetDISI(docIdSet.iterator(), maxDoc);
-          } catch (IOException e) {
-            throw new RuntimeException(e);
-          }
-          size = (int) openBitSetDISI.cardinality();
-        }
-        return size;
-      }
-    };
-  }
-
-  /**
-   * All docs ScoredDocsIDs - this one is simply an 'all 1' bitset. Used when
-   * there are no deletions in the index and we wish to go through each and
-   * every document
-   */
-  private static class AllDocsScoredDocIDs implements ScoredDocIDs {
-    final int maxDoc;
-
-    public AllDocsScoredDocIDs(IndexReader reader) {
-      this.maxDoc = reader.maxDoc();
-    }
-
-    @Override
-    public int size() {  
-      return maxDoc;
-    }
-
-    @Override
-    public DocIdSet getDocIDs() {
-      return new DocIdSet() {
-
-        @Override
-        public boolean isCacheable() {
-          return true;
-        }
-
-        @Override
-        public DocIdSetIterator iterator() {
-          return new DocIdSetIterator() {
-            private int next = -1;
-
-            @Override
-            public int advance(int target) {
-              if (target <= next) {
-                target = next + 1;
-              }
-              return next = target >= maxDoc ? NO_MORE_DOCS : target;
-            }
-
-            @Override
-            public int docID() {
-              return next;
-            }
-
-            @Override
-            public int nextDoc() {
-              return ++next < maxDoc ? next : NO_MORE_DOCS;
-            }
-
-            @Override
-            public long cost() {
-              return maxDoc;
-            }
-          };
-        }
-      };
-    }
-
-    @Override
-    public ScoredDocIDsIterator iterator() {
-      try {
-        final DocIdSetIterator iter = getDocIDs().iterator();
-        return new ScoredDocIDsIterator() {
-          @Override
-          public boolean next() {
-            try {
-              return iter.nextDoc() != DocIdSetIterator.NO_MORE_DOCS;
-            } catch (IOException e) {
-              // cannot happen
-              return false;
-            }
-          }
-
-          @Override
-          public float getScore() {
-            return DEFAULT_SCORE;
-          }
-
-          @Override
-          public int getDocID() {
-            return iter.docID();
-          }
-        };
-      } catch (IOException e) {
-        // cannot happen
-        throw new RuntimeException(e);
-      }
-    }
-  }
-
-  /**
-   * An All-docs bitset which has '0' for deleted documents and '1' for the
-   * rest. Useful for iterating over all 'live' documents in a given index.
-   * <p>
-   * NOTE: this class would work for indexes with no deletions at all,
-   * although it is recommended to use {@link AllDocsScoredDocIDs} to ease
-   * the performance cost of validating isDeleted() on each and every docId
-   */
-  private static final class AllLiveDocsScoredDocIDs implements ScoredDocIDs {
-    final int maxDoc;
-    final IndexReader reader;
-
-    AllLiveDocsScoredDocIDs(IndexReader reader) {
-      this.maxDoc = reader.maxDoc();
-      this.reader = reader;
-    }
-
-    @Override
-    public int size() {
-      return reader.numDocs();
-    }
-
-    @Override
-    public DocIdSet getDocIDs() {
-      return new DocIdSet() {
-
-        @Override
-        public boolean isCacheable() {
-          return true;
-        }
-
-        @Override
-        public DocIdSetIterator iterator() {
-          return new DocIdSetIterator() {
-            final Bits liveDocs = MultiFields.getLiveDocs(reader);
-            private int next = -1;
-
-            @Override
-            public int advance(int target) {
-              if (target > next) {
-                next = target - 1;
-              }
-              return nextDoc();
-            }
-
-            @Override
-            public int docID() {
-              return next;
-            }
-
-            @Override
-            public int nextDoc() {
-              do {
-                ++next;
-              } while (next < maxDoc && liveDocs != null && !liveDocs.get(next));
-
-              return next < maxDoc ? next : NO_MORE_DOCS;
-            }
-
-            @Override
-            public long cost() {
-              return maxDoc;
-            }
-          };
-        }
-      };
-    }
-
-    @Override
-    public ScoredDocIDsIterator iterator() {
-      try {
-        final DocIdSetIterator iter = getDocIDs().iterator();
-        return new ScoredDocIDsIterator() {
-          @Override
-          public boolean next() {
-            try {
-              return iter.nextDoc() != DocIdSetIterator.NO_MORE_DOCS;
-            } catch (IOException e) {
-              // cannot happen
-              return false;
-            }
-          }
-
-          @Override
-          public float getScore() {
-            return DEFAULT_SCORE;
-          }
-
-          @Override
-          public int getDocID() {
-            return iter.docID();
-          }
-        };
-      } catch (IOException e) {
-        // cannot happen
-        throw new RuntimeException(e);
-      }
-    }
-  }
-  
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/old/ScoringAggregator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/ScoringAggregator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/old/ScoringAggregator.java	2013-07-29 13:55:02.641707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/old/ScoringAggregator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,67 +0,0 @@
-package org.apache.lucene.facet.old;
-
-import java.io.IOException;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link Aggregator} which updates the weight of a category according to the
- * scores of the documents it was found in.
- * 
- * @lucene.experimental
- */
-public class ScoringAggregator implements Aggregator {
-
-  private final float[] scoreArray;
-  private final int hashCode;
-  
-  public ScoringAggregator(float[] counterArray) {
-    this.scoreArray = counterArray;
-    this.hashCode = scoreArray == null ? 0 : scoreArray.hashCode();
-  }
-
-  @Override
-  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
-    for (int i = 0; i < ordinals.length; i++) {
-      scoreArray[ordinals.ints[i]] += score;
-    }
-  }
-  
-  @Override
-  public boolean equals(Object obj) {
-    if (obj == null || obj.getClass() != this.getClass()) {
-      return false;
-    }
-    ScoringAggregator that = (ScoringAggregator) obj;
-    return that.scoreArray == this.scoreArray;
-  }
-
-  @Override
-  public int hashCode() {
-    return hashCode;
-  }
-
-  @Override
-  public boolean setNextReader(AtomicReaderContext context) throws IOException {
-    return true;
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/OrdinalsReader.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/OrdinalsReader.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/OrdinalsReader.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/OrdinalsReader.java	2013-11-26 10:43:30.583039324 -0500
@@ -0,0 +1,39 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.util.IntsRef;
+
+/** Provides per-document ordinals. */
+
+public abstract class OrdinalsReader {
+
+  public static abstract class OrdinalsSegmentReader {
+    /** Get the ordinals for this document.  ordinals.offset
+     *  must always be 0! */
+    public abstract void get(int doc, IntsRef ordinals) throws IOException;
+  }
+
+  /** Set current atomic reader. */
+  public abstract OrdinalsSegmentReader getReader(AtomicReaderContext context) throws IOException;
+
+  public abstract String getIndexFieldName();
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/package.html simplefacets/lucene/facet/src/java/org/apache/lucene/facet/package.html
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/package.html	2013-02-20 13:38:17.792711922 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/package.html	2013-11-30 20:10:53.451144210 -0500
@@ -20,5 +20,46 @@
   </head>
   <body>
     <h1>faceted search</h1>
+    <p>
+      This module provides multiple methods for computing facet counts and
+      value aggregations:
+      <ul>
+	<li> Taxonomy-based methods rely on a separate taxonomy index to
+          map hierarchical facet paths to global int ordinals for fast
+          counting at search time; these methods can compute counts
+          (({@link org.apache.lucene.facet.FastTaxonomyFacetCounts}, {@link
+          org.apache.lucene.facet.TaxonomyFacetCounts}) aggregate long or double values {@link
+          org.apache.lucene.facet.TaxonomyFacetSumIntAssociations}, {@link
+          org.apache.lucene.facet.TaxonomyFacetSumFloatAssociations}, {@link
+          org.apache.lucene.facet.TaxonomyFacetSumValueSource}.  Add {@link org.apache.lucene.facet.FacetField} or
+          {@link org.apache.lucene.facet.AssociationFacetField} to your documents at index time
+          to use taxonomy-based methods.
+
+	<li> Sorted-set doc values method does not require a separate
+          taxonomy index, and computes counts based on sorted set doc
+          values fields ({@link org.apache.lucene.facet.SortedSetDocValuesFacetCounts}).  Add
+          {@link org.apache.lucene.facet.SortedSetDocValuesFacetField} to your documents at
+          index time to use sorted set facet counts.
+
+	<li> Range faceting {@link org.apache.lucene.facet.LongRangeFacetCounts}, {@link
+          org.apache.lucene.facet.DoubleRangeFacetCounts} compute counts for a dynamic numeric
+          range from a provided {@link org.apache.lucene.facet.ValueSource} (previously indexed
+          numeric field, or a dynamic expression such as distance).
+      </ul>
+    </p>
+    <p>
+      At search time you first run your search, but pass a {@link
+      org.apache.lucene.facet.FacetsCollector} to gather all hits (and optionally, scores for each
+      hit).  Then, instantiate whichever facet methods you'd like to use
+      to compute aggregates.  Finally, all methods implement a common
+      {@link org.apache.lucene.facet.Facets} base API that you use to obtain specific facet
+      counts.
+    </p>
+    <p>
+      The various {@link org.apache.lucene.facet.FacetsCollector#search} utility methods are
+      useful for doing an "ordinary" search (sorting by score, or by a
+      specified Sort) but also collecting into a {@link org.apache.lucene.facet.FacetsCollector} for
+      subsequent faceting.
+    </p>
   </body>
-</html>
\ No newline at end of file
+</html>


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/params/CategoryListParams.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/params/CategoryListParams.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/params/CategoryListParams.java	2013-02-20 13:38:17.620711927 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/params/CategoryListParams.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,187 +0,0 @@
-package org.apache.lucene.facet.params;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.encoding.DGapVInt8IntEncoder;
-import org.apache.lucene.facet.encoding.IntDecoder;
-import org.apache.lucene.facet.encoding.IntEncoder;
-import org.apache.lucene.facet.encoding.SortingIntEncoder;
-import org.apache.lucene.facet.encoding.UniqueValuesIntEncoder;
-import org.apache.lucene.facet.search.CategoryListIterator;
-import org.apache.lucene.facet.search.DocValuesCategoryListIterator;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.util.PartitionsUtils;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Contains parameters for a category list *
- * 
- * @lucene.experimental
- */
-public class CategoryListParams {
-
-  /**
-   * Defines which category ordinals are encoded for every document. This also
-   * affects how category ordinals are aggregated, check the different policies
-   * for more details.
-   */
-  public static enum OrdinalPolicy {
-    /**
-     * Encodes only the ordinals of leaf nodes. That is, for the category A/B/C,
-     * the ordinals of A and A/B will not be encoded. This policy is efficient
-     * for hierarchical dimensions, as it reduces the number of ordinals that
-     * are visited per document. During faceted search, this policy behaves
-     * exactly like {@link #ALL_PARENTS}, and the counts of all path components
-     * will be computed as well.
-     * 
-     * <p>
-     * <b>NOTE:</b> this {@link OrdinalPolicy} requires a special collector or
-     * accumulator, which will fix the parents' counts.
-     * 
-     * <p>
-     * <b>NOTE:</b> since only leaf nodes are encoded for the document, you
-     * should use this policy when the same document doesn't share two
-     * categories that have a mutual parent, or otherwise the counts will be
-     * wrong (the mutual parent will be over-counted). For example, if a
-     * document has the categories A/B/C and A/B/D, then with this policy the
-     * counts of "A" and "B" will be 2, which is wrong. If you intend to index
-     * hierarchical dimensions, with more than one category per document, you
-     * should use either {@link #ALL_PARENTS} or {@link #ALL_BUT_DIMENSION}.
-     */
-    NO_PARENTS,
-    
-    /**
-     * Encodes the ordinals of all path components. That is, the category A/B/C
-     * will encode the ordinals of A and A/B as well. If you don't require the
-     * dimension's count during search, consider using
-     * {@link #ALL_BUT_DIMENSION}.
-     */
-    ALL_PARENTS,
-    
-    /**
-     * Encodes the ordinals of all path components except the dimension. The
-     * dimension of a category is defined to be the first components in
-     * {@link CategoryPath#components}. For the category A/B/C, the ordinal of
-     * A/B will be encoded as well, however not the ordinal of A.
-     * 
-     * <p>
-     * <b>NOTE:</b> when facets are aggregated, this policy behaves exactly like
-     * {@link #ALL_PARENTS}, except that the dimension is never counted. I.e. if
-     * you ask to count the facet "A", then while in {@link #ALL_PARENTS} you
-     * will get counts for "A" <u>and its children</u>, with this policy you
-     * will get counts for <u>only its children</u>. This policy is the default
-     * one, and makes sense for using with flat dimensions, whenever your
-     * application does not require the dimension's count. Otherwise, use
-     * {@link #ALL_PARENTS}.
-     */
-    ALL_BUT_DIMENSION
-  }
-  
-  /** The default field used to store the facets information. */
-  public static final String DEFAULT_FIELD = "$facets";
-
-  /**
-   * The default {@link OrdinalPolicy} that's used when encoding a document's
-   * category ordinals.
-   */
-  public static final OrdinalPolicy DEFAULT_ORDINAL_POLICY = OrdinalPolicy.ALL_BUT_DIMENSION;
-  
-  public final String field;
-
-  private final int hashCode;
-
-  /** Constructs a default category list parameters object, using {@link #DEFAULT_FIELD}. */
-  public CategoryListParams() {
-    this(DEFAULT_FIELD);
-  }
-
-  /** Constructs a category list parameters object, using the given field. */
-  public CategoryListParams(String field) {
-    this.field = field;
-    // Pre-compute the hashCode because these objects are immutable.  Saves
-    // some time on the comparisons later.
-    this.hashCode = field.hashCode();
-  }
-  
-  /**
-   * Allows to override how categories are encoded and decoded. A matching
-   * {@link IntDecoder} is provided by the {@link IntEncoder}.
-   * <p>
-   * Default implementation creates a new Sorting(<b>Unique</b>(DGap)) encoder.
-   * Uniqueness in this regard means when the same category appears twice in a
-   * document, only one appearance would be encoded. This has effect on facet
-   * counting results.
-   * <p>
-   * Some possible considerations when overriding may be:
-   * <ul>
-   * <li>an application "knows" that all categories are unique. So no need to
-   * pass through the unique filter.</li>
-   * <li>Another application might wish to count multiple occurrences of the
-   * same category, or, use a faster encoding which will consume more space.</li>
-   * </ul>
-   * In any event when changing this value make sure you know what you are
-   * doing, and test the results - e.g. counts, if the application is about
-   * counting facets.
-   */
-  public IntEncoder createEncoder() {
-    return new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapVInt8IntEncoder()));
-  }
-
-  @Override
-  public boolean equals(Object o) {
-    if (o == this) {
-      return true;
-    }
-    if (!(o instanceof CategoryListParams)) {
-      return false;
-    }
-    CategoryListParams other = (CategoryListParams) o;
-    if (hashCode != other.hashCode) {
-      return false;
-    }
-    return field.equals(other.field);
-  }
-
-  @Override
-  public int hashCode() {
-    return hashCode;
-  }
-
-  /** Create the {@link CategoryListIterator} for the specified partition. */
-  public CategoryListIterator createCategoryListIterator(int partition) throws IOException {
-    String categoryListTermStr = PartitionsUtils.partitionName(partition);
-    String docValuesField = field + categoryListTermStr;
-    return new DocValuesCategoryListIterator(docValuesField, createEncoder().createMatchingDecoder());
-  }
-  
-  /**
-   * Returns the {@link OrdinalPolicy} to use for the given dimension. This
-   * {@link CategoryListParams} always returns {@link #DEFAULT_ORDINAL_POLICY}
-   * for all dimensions.
-   */
-  public OrdinalPolicy getOrdinalPolicy(String dimension) {
-    return DEFAULT_ORDINAL_POLICY;
-  }
-  
-  @Override
-  public String toString() {
-    return "field=" + field + " encoder=" + createEncoder() + " ordinalPolicy=" + getOrdinalPolicy(null);
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/params/FacetIndexingParams.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/params/FacetIndexingParams.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/params/FacetIndexingParams.java	2013-03-07 11:04:28.508872751 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/params/FacetIndexingParams.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,180 +0,0 @@
-package org.apache.lucene.facet.params;
-
-import java.util.Collections;
-import java.util.List;
-
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Defines parameters that are needed for facets indexing. Note that this class
- * does not have any setters. That's because overriding the default parameters
- * is considered expert. If you wish to override them, simply extend this class
- * and override the relevant getter.
- * 
- * <p>
- * <b>NOTE:</b> This class is also used during faceted search in order to e.g.
- * know which field holds the drill-down terms or the fulltree posting.
- * Therefore this class should be initialized once and you should refrain from
- * changing it. Also note that if you make any changes to it (e.g. suddenly
- * deciding that drill-down terms should be read from a different field) and use
- * it on an existing index, things may not work as expected.
- * 
- * @lucene.experimental
- */
-public class FacetIndexingParams {
-  
-  // the default CLP, can be a singleton
-  protected static final CategoryListParams DEFAULT_CATEGORY_LIST_PARAMS = new CategoryListParams();
-
-  /**
-   * A {@link FacetIndexingParams} which fixes a single
-   * {@link CategoryListParams} with
-   * {@link CategoryListParams#DEFAULT_ORDINAL_POLICY}.
-   */
-  public static final FacetIndexingParams DEFAULT = new FacetIndexingParams();
-  
-  /**
-   * The default delimiter with which {@link CategoryPath#components} are
-   * concatenated when written to the index, e.g. as drill-down terms. If you
-   * choose to override it by overiding {@link #getFacetDelimChar()}, you should
-   * make sure that you return a character that's not found in any path
-   * component.
-   */
-  public static final char DEFAULT_FACET_DELIM_CHAR = '\u001F';
-  
-  private final int partitionSize = Integer.MAX_VALUE;
-
-  protected final CategoryListParams clParams;
-
-  /**
-   * Initializes new default params. You should use this constructor only if you
-   * intend to override any of the getters, otherwise you can use
-   * {@link #DEFAULT} to save unnecessary object allocations.
-   */
-  public FacetIndexingParams() {
-    this(DEFAULT_CATEGORY_LIST_PARAMS);
-  }
-
-  /** Initializes new params with the given {@link CategoryListParams}. */
-  public FacetIndexingParams(CategoryListParams categoryListParams) {
-    clParams = categoryListParams;
-  }
-
-  /**
-   * Returns the {@link CategoryListParams} for this {@link CategoryPath}. The
-   * default implementation returns the same {@link CategoryListParams} for all
-   * categories (even if {@code category} is {@code null}).
-   * 
-   * @see PerDimensionIndexingParams
-   */
-  public CategoryListParams getCategoryListParams(CategoryPath category) {
-    return clParams;
-  }
-
-  /**
-   * Copies the text required to execute a drill-down query on the given
-   * category to the given {@code char[]}, and returns the number of characters
-   * that were written.
-   * <p>
-   * <b>NOTE:</b> You should make sure that the {@code char[]} is large enough,
-   * by e.g. calling {@link CategoryPath#fullPathLength()}.
-   */
-  public int drillDownTermText(CategoryPath path, char[] buffer) {
-    return path.copyFullPath(buffer, 0, getFacetDelimChar());
-  }
-  
-  /**
-   * Returns the size of a partition. <i>Partitions</i> allow you to divide
-   * (hence, partition) the categories space into small sets to e.g. improve RAM
-   * consumption during faceted search. For instance, {@code partitionSize=100K}
-   * would mean that if your taxonomy index contains 420K categories, they will
-   * be divided into 5 groups and at search time a {@link FacetArrays} will be
-   * allocated at the size of the partition.
-   * 
-   * <p>
-   * This is real advanced setting and should be changed with care. By default,
-   * all categories are put in one partition. You should modify this setting if
-   * you have really large taxonomies (e.g. 1M+ nodes).
-   */
-  public int getPartitionSize() {
-    return partitionSize;
-  }
-  
-  /**
-   * Returns a list of all {@link CategoryListParams categoryListParams} that
-   * are used for facets indexing.
-   */
-  public List<CategoryListParams> getAllCategoryListParams() {
-    return Collections.singletonList(clParams);
-  }
-
-  @Override
-  public int hashCode() {
-    final int prime = 31;
-    int result = 1;
-    result = prime * result + ((clParams == null) ? 0 : clParams.hashCode());
-    result = prime * result + partitionSize;
-    
-    for (CategoryListParams clp : getAllCategoryListParams()) {
-      result ^= clp.hashCode();
-    }
-    
-    return result;
-  }
-
-  @Override
-  public boolean equals(Object obj) {
-    if (this == obj) {
-      return true;
-    }
-    if (obj == null) {
-      return false;
-    }
-    if (!(obj instanceof FacetIndexingParams)) {
-      return false;
-    }
-    FacetIndexingParams other = (FacetIndexingParams) obj;
-    if (clParams == null) {
-      if (other.clParams != null) {
-        return false;
-      }
-    } else if (!clParams.equals(other.clParams)) {
-      return false;
-    }
-    if (partitionSize != other.partitionSize) {
-      return false;
-    }
-    
-    Iterable<CategoryListParams> cLs = getAllCategoryListParams();
-    Iterable<CategoryListParams> otherCLs = other.getAllCategoryListParams();
-    
-    return cLs.equals(otherCLs);
-  }
-
-  /**
-   * Returns the delimiter character used internally for concatenating category
-   * path components, e.g. for drill-down terms.
-   */
-  public char getFacetDelimChar() {
-    return DEFAULT_FACET_DELIM_CHAR;
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/params/FacetSearchParams.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/params/FacetSearchParams.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/params/FacetSearchParams.java	2013-07-29 13:55:02.641707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/params/FacetSearchParams.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,93 +0,0 @@
-package org.apache.lucene.facet.params;
-
-import java.util.Arrays;
-import java.util.List;
-
-import org.apache.lucene.facet.search.FacetRequest;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Defines parameters that are needed for faceted search: the list of facet
- * {@link FacetRequest facet requests} which should be aggregated as well as the
- * {@link FacetIndexingParams indexing params} that were used to index them.
- * 
- * @lucene.experimental
- */
-public class FacetSearchParams {
-
-  public final FacetIndexingParams indexingParams;
-  public final List<FacetRequest> facetRequests;
-  
-  /**
-   * Initializes with the given {@link FacetRequest requests} and default
-   * {@link FacetIndexingParams#DEFAULT}. If you used a different
-   * {@link FacetIndexingParams}, you should use
-   * {@link #FacetSearchParams(FacetIndexingParams, List)}.
-   */
-  public FacetSearchParams(FacetRequest... facetRequests) {
-    this(FacetIndexingParams.DEFAULT, Arrays.asList(facetRequests));
-  }
-  
-  /**
-   * Initializes with the given {@link FacetRequest requests} and default
-   * {@link FacetIndexingParams#DEFAULT}. If you used a different
-   * {@link FacetIndexingParams}, you should use
-   * {@link #FacetSearchParams(FacetIndexingParams, List)}.
-   */
-  public FacetSearchParams(List<FacetRequest> facetRequests) {
-    this(FacetIndexingParams.DEFAULT, facetRequests);
-  }
-  
-  /**
-   * Initializes with the given {@link FacetRequest requests} and
-   * {@link FacetIndexingParams}.
-   */
-  public FacetSearchParams(FacetIndexingParams indexingParams, FacetRequest... facetRequests) {
-    this(indexingParams, Arrays.asList(facetRequests));
-  }
-
-  /**
-   * Initializes with the given {@link FacetRequest requests} and
-   * {@link FacetIndexingParams}.
-   */
-  public FacetSearchParams(FacetIndexingParams indexingParams, List<FacetRequest> facetRequests) {
-    if (facetRequests == null || facetRequests.size() == 0) {
-      throw new IllegalArgumentException("at least one FacetRequest must be defined");
-    }
-    this.facetRequests = facetRequests;
-    this.indexingParams = indexingParams;
-  }
-  
-  @Override
-  public String toString() {
-    final String INDENT = "  ";
-    final char NEWLINE = '\n';
-
-    StringBuilder sb = new StringBuilder("IndexingParams: ");
-    sb.append(NEWLINE).append(INDENT).append(indexingParams);
-    
-    sb.append(NEWLINE).append("FacetRequests:");
-    for (FacetRequest facetRequest : facetRequests) {
-      sb.append(NEWLINE).append(INDENT).append(facetRequest);
-    }
-    
-    return sb.toString();
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/params/package.html simplefacets/lucene/facet/src/java/org/apache/lucene/facet/params/package.html
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/params/package.html	2013-02-20 13:38:17.620711927 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/params/package.html	1969-12-31 19:00:00.000000000 -0500
@@ -1,25 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Facets indexing and search parameters</title>
-</head>
-<body>
-Facets indexing and search parameters. Define how facets are indexed 
-as well as which categories need to be aggregated.
-</body>
-</html>
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/params/PerDimensionIndexingParams.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/params/PerDimensionIndexingParams.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/params/PerDimensionIndexingParams.java	2013-02-20 13:38:17.620711927 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/params/PerDimensionIndexingParams.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,96 +0,0 @@
-package org.apache.lucene.facet.params;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetIndexingParams} that utilizes different category lists, defined
- * by the dimension specified by a {@link CategoryPath category} (see
- * {@link #PerDimensionIndexingParams(Map, CategoryListParams)}.
- * <p>
- * A 'dimension' is defined as the first or "zero-th" component in a
- * {@link CategoryPath}. For example, if a category is defined as
- * "Author/American/Mark Twain", then the dimension would be "Author".
- * 
- * @lucene.experimental
- */
-public class PerDimensionIndexingParams extends FacetIndexingParams {
-
-  private final Map<String, CategoryListParams> clParamsMap;
-
-  /**
-   * Initializes a new instance with the given dimension-to-params mapping. The
-   * dimension is considered as what's returned by
-   * {@link CategoryPath#components cp.components[0]}.
-   * 
-   * <p>
-   * <b>NOTE:</b> for any dimension whose {@link CategoryListParams} is not
-   * defined in the mapping, a default {@link CategoryListParams} will be used.
-   * 
-   * @see #PerDimensionIndexingParams(Map, CategoryListParams)
-   */
-  public PerDimensionIndexingParams(Map<CategoryPath, CategoryListParams> paramsMap) {
-    this(paramsMap, DEFAULT_CATEGORY_LIST_PARAMS);
-  }
-
-  /**
-   * Same as {@link #PerDimensionIndexingParams(Map)}, only the given
-   * {@link CategoryListParams} will be used for any dimension that is not
-   * specified in the given mapping.
-   */
-  public PerDimensionIndexingParams(Map<CategoryPath, CategoryListParams> paramsMap, 
-      CategoryListParams categoryListParams) {
-    super(categoryListParams);
-    clParamsMap = new HashMap<String,CategoryListParams>();
-    for (Entry<CategoryPath, CategoryListParams> e : paramsMap.entrySet()) {
-      clParamsMap.put(e.getKey().components[0], e.getValue());
-    }
-  }
-
-  @Override
-  public List<CategoryListParams> getAllCategoryListParams() {
-    ArrayList<CategoryListParams> vals = new ArrayList<CategoryListParams>(clParamsMap.values());
-    vals.add(clParams); // add the default too
-    return vals;
-  }
-
-  /**
-   * Returns the {@link CategoryListParams} for the corresponding dimension
-   * which is returned by {@code category.getComponent(0)}. If {@code category}
-   * is {@code null}, or was not specified in the map given to the constructor,
-   * returns the default {@link CategoryListParams}.
-   */
-  @Override
-  public CategoryListParams getCategoryListParams(CategoryPath category) {
-    if (category != null) {
-      CategoryListParams clParams = clParamsMap.get(category.components[0]);
-      if (clParams != null) {
-        return clParams;
-      }
-    }
-    return clParams;
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/params/PerDimensionOrdinalPolicy.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/params/PerDimensionOrdinalPolicy.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/params/PerDimensionOrdinalPolicy.java	2013-02-20 13:38:17.616711927 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/params/PerDimensionOrdinalPolicy.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,55 +0,0 @@
-package org.apache.lucene.facet.params;
-
-import java.util.Map;
-
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link CategoryListParams} which allow controlling the
- * {@link CategoryListParams.OrdinalPolicy} used for each dimension. The
- * dimension is specified as the first component in
- * {@link CategoryPath#components}.
- */
-public class PerDimensionOrdinalPolicy extends CategoryListParams {
-
-  private final Map<String,OrdinalPolicy> policies;
-  private final OrdinalPolicy defaultOP;
-  
-  public PerDimensionOrdinalPolicy(Map<String,OrdinalPolicy> policies) {
-    this(policies, DEFAULT_ORDINAL_POLICY);
-  }
-  
-  public PerDimensionOrdinalPolicy(Map<String,OrdinalPolicy> policies, OrdinalPolicy defaultOP) {
-    this.defaultOP = defaultOP;
-    this.policies = policies;
-  }
-
-  @Override
-  public OrdinalPolicy getOrdinalPolicy(String dimension) {
-    OrdinalPolicy op = policies.get(dimension);
-    return op == null ? defaultOP : op;
-  }
-  
-  @Override
-  public String toString() {
-    return super.toString() + " policies=" + policies;
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/partitions/IntermediateFacetResult.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/partitions/IntermediateFacetResult.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/partitions/IntermediateFacetResult.java	2013-02-20 13:38:17.696711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/partitions/IntermediateFacetResult.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,42 +0,0 @@
-package org.apache.lucene.facet.partitions;
-
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultsHandler;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Intermediate {@link FacetResult} of faceted search.
- * <p>
- * This is an empty interface on purpose.
- * <p>
- * It allows {@link FacetResultsHandler} to return intermediate result objects 
- * that only it knows how to interpret, and so the handler has maximal freedom
- * in defining what an intermediate result is, depending on its specific logic.  
- * 
- * @lucene.experimental
- */
-public interface IntermediateFacetResult {
-
-  /**
-   * Facet request for which this temporary result was created.
-   */
-  FacetRequest getFacetRequest();
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/partitions/package.html simplefacets/lucene/facet/src/java/org/apache/lucene/facet/partitions/package.html
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/partitions/package.html	2013-02-20 13:38:17.696711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/partitions/package.html	1969-12-31 19:00:00.000000000 -0500
@@ -1,27 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Category Partitions</title>
-</head>
-<body>
-<h1>Category Partitions</h1>
-
-Allows partitioning the category ordinals space, so that less RAM is consumed during search.
-Only meaningful for very large taxonomies (tens of millions of categories).
-</body>
-</html>


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/partitions/PartitionsFacetResultsHandler.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/partitions/PartitionsFacetResultsHandler.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/partitions/PartitionsFacetResultsHandler.java	2013-08-01 14:47:20.734689731 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/partitions/PartitionsFacetResultsHandler.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,137 +0,0 @@
-package org.apache.lucene.facet.partitions;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.old.OldFacetsAccumulator;
-import org.apache.lucene.facet.old.ScoredDocIDs;
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.search.FacetResultsHandler;
-import org.apache.lucene.facet.search.OrdinalValueResolver;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetResultsHandler} designed to work with facet partitions.
- * 
- * @lucene.experimental
- */
-public abstract class PartitionsFacetResultsHandler extends FacetResultsHandler {
-  
-  public PartitionsFacetResultsHandler(TaxonomyReader taxonomyReader, FacetRequest facetRequest, 
-      OrdinalValueResolver resolver, FacetArrays facetArrays) {
-    super(taxonomyReader, facetRequest, resolver, facetArrays);
-  }
-
-  /**
-   * Fetch results of a single partition, given facet arrays for that partition,
-   * and based on the matching documents and faceted search parameters.
-   * @param offset
-   *          offset in input arrays where partition starts
-   * 
-   * @return temporary facet result, potentially, to be passed back to
-   *         <b>this</b> result handler for merging, or <b>null</b> in case that
-   *         constructor parameter, <code>facetRequest</code>, requests an
-   *         illegal FacetResult, like, e.g., a root node category path that
-   *         does not exist in constructor parameter <code>taxonomyReader</code>
-   *         .
-   * @throws IOException
-   *           on error
-   */
-  public abstract IntermediateFacetResult fetchPartitionResult(int offset) throws IOException;
-
-  /**
-   * Merge results of several facet partitions. Logic of the merge is undefined
-   * and open for interpretations. For example, a merge implementation could
-   * keep top K results. Passed {@link IntermediateFacetResult} must be ones
-   * that were created by this handler otherwise a {@link ClassCastException} is
-   * thrown. In addition, all passed {@link IntermediateFacetResult} must have
-   * the same {@link FacetRequest} otherwise an {@link IllegalArgumentException}
-   * is thrown.
-   * 
-   * @param tmpResults one or more temporary results created by <b>this</b>
-   *        handler.
-   * @return temporary facet result that represents to union, as specified by
-   *         <b>this</b> handler, of the input temporary facet results.
-   * @throws IOException on error.
-   * @throws ClassCastException if the temporary result passed was not created
-   *         by this handler
-   * @throws IllegalArgumentException if passed <code>facetResults</code> do not
-   *         have the same {@link FacetRequest}
-   * @see IntermediateFacetResult#getFacetRequest()
-   */
-  public abstract IntermediateFacetResult mergeResults(IntermediateFacetResult... tmpResults) throws IOException;
-
-  /**
-   * Create a facet result from the temporary result.
-   * @param tmpResult temporary result to be rendered as a {@link FacetResult}
-   * @throws IOException on error.
-   */
-  public abstract FacetResult renderFacetResult(IntermediateFacetResult tmpResult) throws IOException ;
-
-  /**
-   * Perform any rearrangement as required on a facet result that has changed after
-   * it was rendered.
-   * <P>
-   * Possible use case: a sampling facets accumulator invoked another 
-   * other facets accumulator on a sample set of documents, obtained
-   * rendered facet results, fixed their counts, and now it is needed 
-   * to sort the results differently according to the fixed counts. 
-   * @param facetResult result to be rearranged.
-   * @see FacetResultNode#value
-   */
-  public abstract FacetResult rearrangeFacetResult(FacetResult facetResult);
-
-  /**
-   * Label results according to settings in {@link FacetRequest}, such as
-   * {@link FacetRequest#getNumLabel()}. Usually invoked by
-   * {@link OldFacetsAccumulator#accumulate(ScoredDocIDs)}
-   * 
-   * @param facetResult
-   *          facet result to be labeled.
-   * @throws IOException
-   *           on error
-   */
-  public abstract void labelResult(FacetResult facetResult) throws IOException;
-
-  /**
-   * Check if an array contains the partition which contains ordinal
-   * 
-   * @param ordinal
-   *          checked facet
-   * @param facetArrays
-   *          facet arrays for the certain partition
-   * @param offset
-   *          offset in input arrays where partition starts
-   */
-  protected boolean isSelfPartition (int ordinal, FacetArrays facetArrays, int offset) {
-    int partitionSize = facetArrays.arrayLength;
-    return ordinal / partitionSize == offset / partitionSize;
-  }
-  
-  @Override
-  public final FacetResult compute() throws IOException {
-    FacetResult res = renderFacetResult(fetchPartitionResult(0));
-    labelResult(res);
-    return res;
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/range/DoubleRange.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/range/DoubleRange.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/range/DoubleRange.java	2013-11-20 10:58:32.152883561 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/range/DoubleRange.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,71 +0,0 @@
-package org.apache.lucene.facet.range;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.document.DoubleDocValuesField; // javadocs
-
-/** Represents a range over double values indexed as {@link
- *  DoubleDocValuesField}.  */
-public final class DoubleRange extends Range {
-  private final double minIncl;
-  private final double maxIncl;
-
-  public final double min;
-  public final double max;
-  public final boolean minInclusive;
-  public final boolean maxInclusive;
-
-  /** Create a DoubleRange. */
-  public DoubleRange(String label, double min, boolean minInclusive, double max, boolean maxInclusive) {
-    super(label);
-    this.min = min;
-    this.max = max;
-    this.minInclusive = minInclusive;
-    this.maxInclusive = maxInclusive;
-
-    // TODO: if DoubleDocValuesField used
-    // NumericUtils.doubleToSortableLong format (instead of
-    // Double.doubleToRawLongBits) we could do comparisons
-    // in long space 
-
-    if (Double.isNaN(min)) {
-      throw new IllegalArgumentException("min cannot be NaN");
-    }
-    if (!minInclusive) {
-      min = Math.nextUp(min);
-    }
-
-    if (Double.isNaN(max)) {
-      throw new IllegalArgumentException("max cannot be NaN");
-    }
-    if (!maxInclusive) {
-      // Why no Math.nextDown?
-      max = Math.nextAfter(max, Double.NEGATIVE_INFINITY);
-    }
-
-    this.minIncl = min;
-    this.maxIncl = max;
-  }
-
-  @Override
-  public boolean accept(long value) {
-    double doubleValue = Double.longBitsToDouble(value);
-    return doubleValue >= minIncl && doubleValue <= maxIncl;
-  }
-}
-


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/range/FloatRange.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/range/FloatRange.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/range/FloatRange.java	2013-07-15 15:52:17.417877394 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/range/FloatRange.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,70 +0,0 @@
-package org.apache.lucene.facet.range;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.document.FloatDocValuesField; // javadocs
-
-/** Represents a range over float values indexed as {@link
- *  FloatDocValuesField}.  */
-public final class FloatRange extends Range {
-  private final float minIncl;
-  private final float maxIncl;
-
-  public final float min;
-  public final float max;
-  public final boolean minInclusive;
-  public final boolean maxInclusive;
-
-  /** Create a FloatRange. */
-  public FloatRange(String label, float min, boolean minInclusive, float max, boolean maxInclusive) {
-    super(label);
-    this.min = min;
-    this.max = max;
-    this.minInclusive = minInclusive;
-    this.maxInclusive = maxInclusive;
-
-    // TODO: if FloatDocValuesField used
-    // NumericUtils.floatToSortableInt format (instead of
-    // Float.floatToRawIntBits) we could do comparisons
-    // in int space 
-
-    if (Float.isNaN(min)) {
-      throw new IllegalArgumentException("min cannot be NaN");
-    }
-    if (!minInclusive) {
-      min = Math.nextUp(min);
-    }
-
-    if (Float.isNaN(max)) {
-      throw new IllegalArgumentException("max cannot be NaN");
-    }
-    if (!maxInclusive) {
-      // Why no Math.nextDown?
-      max = Math.nextAfter(max, Float.NEGATIVE_INFINITY);
-    }
-
-    this.minIncl = min;
-    this.maxIncl = max;
-  }
-
-  @Override
-  public boolean accept(long value) {
-    float floatValue = Float.intBitsToFloat((int) value);
-    return floatValue >= minIncl && floatValue <= maxIncl;
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/range/LongRange.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/range/LongRange.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/range/LongRange.java	2013-07-15 15:52:17.421877394 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/range/LongRange.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,57 +0,0 @@
-package org.apache.lucene.facet.range;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.document.NumericDocValuesField; // javadocs
-
-/** Represents a range over long values indexed as {@link
- *  NumericDocValuesField}.  */
-public final class LongRange extends Range {
-  private final long minIncl;
-  private final long maxIncl;
-
-  public final long min;
-  public final long max;
-  public final boolean minInclusive;
-  public final boolean maxInclusive;
-
-  /** Create a LongRange. */
-  public LongRange(String label, long min, boolean minInclusive, long max, boolean maxInclusive) {
-    super(label);
-    this.min = min;
-    this.max = max;
-    this.minInclusive = minInclusive;
-    this.maxInclusive = maxInclusive;
-
-    if (!minInclusive && min != Long.MAX_VALUE) {
-      min++;
-    }
-
-    if (!maxInclusive && max != Long.MIN_VALUE) {
-      max--;
-    }
-
-    this.minIncl = min;
-    this.maxIncl = max;
-  }
-
-  @Override
-  public boolean accept(long value) {
-    return value >= minIncl && value <= maxIncl;
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/range/package.html simplefacets/lucene/facet/src/java/org/apache/lucene/facet/range/package.html
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/range/package.html	2013-07-15 15:52:17.421877394 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/range/package.html	1969-12-31 19:00:00.000000000 -0500
@@ -1,24 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Facets numeric range code</title>
-</head>
-<body>
-Code to compute facets for numeric ranges.
-</body>
-</html>


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/range/RangeAccumulator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/range/RangeAccumulator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/range/RangeAccumulator.java	2013-11-20 10:49:23.608898139 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/range/RangeAccumulator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,114 +0,0 @@
-package org.apache.lucene.facet.range;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.search.FacetsAccumulator;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.queries.function.FunctionValues;
-
-/**
- * Uses {@link RangeFacetRequest#getValues(AtomicReaderContext)} and accumulates
- * counts for provided ranges.
- */
-public class RangeAccumulator extends FacetsAccumulator {
-
-  public RangeAccumulator(FacetRequest... facetRequests) {
-    this(Arrays.asList(facetRequests));
-  }
-  
-  public RangeAccumulator(List<FacetRequest> facetRequests) {
-    super(new FacetSearchParams(facetRequests));
-    for (FacetRequest fr : facetRequests) {
-      if (!(fr instanceof RangeFacetRequest)) {
-        throw new IllegalArgumentException("this accumulator only supports RangeFacetRequest; got " + fr);
-      }
-
-      if (fr.categoryPath.length != 1) {
-        throw new IllegalArgumentException("only flat (dimension only) CategoryPath is allowed");
-      }
-    }
-  }
-
-  @Override
-  public List<FacetResult> accumulate(List<MatchingDocs> matchingDocs) throws IOException {
-
-    // TODO: test if this is faster (in the past it was
-    // faster to do MachingDocs on the inside) ... see
-    // patches on LUCENE-4965):
-    List<FacetResult> results = new ArrayList<FacetResult>();
-    for (FacetRequest req : searchParams.facetRequests) {
-      RangeFacetRequest<?> rangeFR = (RangeFacetRequest<?>) req;
-      int[] counts = new int[rangeFR.ranges.length];
-      for (MatchingDocs hits : matchingDocs) {
-        FunctionValues fv = rangeFR.getValues(hits.context);
-        final int length = hits.bits.length();
-        int doc = 0;
-        while (doc < length && (doc = hits.bits.nextSetBit(doc)) != -1) {
-          // Skip missing docs:
-          if (!fv.exists(doc)) {
-            ++doc;
-            continue;
-          }
-          
-          long v = fv.longVal(doc);
-
-          // TODO: if all ranges are non-overlapping, we
-          // should instead do a bin-search up front
-          // (really, a specialized case of the interval
-          // tree)
-          // TODO: use interval tree instead of linear search:
-          for (int j = 0; j < rangeFR.ranges.length; j++) {
-            if (rangeFR.ranges[j].accept(v)) {
-              counts[j]++;
-            }
-          }
-
-          doc++;
-        }
-      }
-      
-      List<FacetResultNode> nodes = new ArrayList<FacetResultNode>(rangeFR.ranges.length);
-      for (int j = 0; j < rangeFR.ranges.length; j++) {
-        nodes.add(new RangeFacetResultNode(rangeFR.label, rangeFR.ranges[j], counts[j]));
-      }
-      
-      FacetResultNode rootNode = new FacetResultNode(-1, 0);
-      rootNode.label = rangeFR.categoryPath;
-      rootNode.subResults = nodes;
-
-      results.add(new FacetResult(req, rootNode, nodes.size()));
-    }
-    
-    return results;
-  }
-
-  @Override
-  public boolean requiresDocScores() {
-    return false;
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/range/RangeFacetRequest.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/range/RangeFacetRequest.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/range/RangeFacetRequest.java	2013-11-06 07:02:49.341619591 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/range/RangeFacetRequest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,107 +0,0 @@
-package org.apache.lucene.facet.range;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.List;
-
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.search.FacetsAggregator;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.valuesource.LongFieldSource;
-
-/**
- * A {@link FacetRequest} for dynamic ranges based on a {@link NumericDocValues}
- * field or {@link ValueSource}. This does not use the taxonomy index nor any
- * indexed facet values.
- * 
- * @lucene.experimental
- */
-public class RangeFacetRequest<T extends Range> extends FacetRequest {
-
-  public final Range[] ranges;
-  public final String label;
-  
-  private final ValueSource valueSource;
-  
-  /**
-   * Create a request for the given ranges over the specified
-   * {@link NumericDocValues} field. The field will be used to as the root's
-   * {@link FacetResultNode} label.
-   */
-  @SuppressWarnings("unchecked")
-  public RangeFacetRequest(String field, T...ranges) {
-    this(field, new LongFieldSource(field), ranges);
-  }
-
-  /**
-   * Create a request for the given ranges over the specified
-   * {@link NumericDocValues} field. The field will be used to as the root's
-   * {@link FacetResultNode} label.
-   */
-  @SuppressWarnings("unchecked")
-  public RangeFacetRequest(String field, List<T> ranges) {
-    this(field, (T[]) ranges.toArray(new Range[ranges.size()]));
-  }
-  
-  /**
-   * Create a request for the given ranges over the specified
-   * {@link ValueSource}. The label will be used to as the root's
-   * {@link FacetResultNode} label.
-   */
-  @SuppressWarnings("unchecked")
-  public RangeFacetRequest(String label, ValueSource valueSource, T...ranges) {
-    super(new CategoryPath(label), 1);
-    this.ranges = ranges;
-    this.valueSource = valueSource;
-    this.label = label;
-  }
-  
-  /**
-   * Create a request for the given ranges over the specified
-   * {@link ValueSource}. The label will be used to as the root's
-   * {@link FacetResultNode} label.
-   */
-  @SuppressWarnings("unchecked")
-  public RangeFacetRequest(String label, ValueSource valueSource, List<T> ranges) {
-    this(label, valueSource, (T[]) ranges.toArray(new Range[ranges.size()]));
-  }
-
-  /**
-   * Returns the {@link FunctionValues} for the given
-   * {@link AtomicReaderContext}. If the request was created over a
-   * {@link NumericDocValues} field, the respective {@link NumericDocValues} is
-   * returned.
-   */
-  public FunctionValues getValues(AtomicReaderContext context) throws IOException {
-    return valueSource.getValues(Collections.emptyMap(), context);
-  }
-  
-  @Override
-  public FacetsAggregator createFacetsAggregator(FacetIndexingParams fip) {
-    throw new UnsupportedOperationException("this FacetRequest does not support categories aggregation and only works with RangeAccumulator");
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/range/RangeFacetResultNode.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/range/RangeFacetResultNode.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/range/RangeFacetResultNode.java	2013-07-15 15:52:17.421877394 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/range/RangeFacetResultNode.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,33 +0,0 @@
-package org.apache.lucene.facet.range;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/** Holds the facet results for a {@link
- *  RangeFacetRequest}. */
-public class RangeFacetResultNode extends FacetResultNode {
-  public final Range range;
-
-  RangeFacetResultNode(String field, Range range, int count) {
-    super(-1, count);
-    this.range = range;
-    this.label = new CategoryPath(field, range.label);
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/range/Range.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/range/Range.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/range/Range.java	2013-07-15 15:52:17.421877394 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/range/Range.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,33 +0,0 @@
-package org.apache.lucene.facet.range;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Represents a single labelled range, one facet label in
- *  the facets computed by {@link RangeAccumulator}.
- *
- *  @lucene.experimental */
-
-public abstract class Range {
-  public final String label;
-
-  protected Range(String label) {
-    this.label = label;
-  }
-
-  public abstract boolean accept(long value);
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/RangeFacetCounts.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/RangeFacetCounts.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/RangeFacetCounts.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/RangeFacetCounts.java	2013-12-17 10:12:15.068813659 -0500
@@ -0,0 +1,67 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.List;
+
+
+/** Base class for range faceting.
+ *
+ *  @lucene.experimental */
+abstract class RangeFacetCounts extends Facets {
+  protected final Range[] ranges;
+  protected final int[] counts;
+  protected final String field;
+  protected int totCount;
+
+  /** Create {@code RangeFacetCounts}, using {@link
+   *  LongFieldSource} from the specified field. */
+  protected RangeFacetCounts(String field, Range[] ranges) throws IOException {
+    this.field = field;
+    this.ranges = ranges;
+    counts = new int[ranges.length];
+  }
+
+  @Override
+  public FacetResult getTopChildren(int topN, String dim, String... path) {
+    if (dim.equals(field) == false) {
+      throw new IllegalArgumentException("invalid dim \"" + dim + "\"; should be \"" + field + "\"");
+    }
+    if (path.length != 0) {
+      throw new IllegalArgumentException("path.length should be 0");
+    }
+    LabelAndValue[] labelValues = new LabelAndValue[counts.length];
+    for(int i=0;i<counts.length;i++) {
+      labelValues[i] = new LabelAndValue(ranges[i].label, counts[i]);
+    }
+    return new FacetResult(dim, path, totCount, labelValues, labelValues.length);
+  }
+
+  @Override
+  public Number getSpecificValue(String dim, String... path) throws IOException {
+    // TODO: should we impl this?
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public List<FacetResult> getAllDims(int topN) throws IOException {
+    return Collections.singletonList(getTopChildren(topN, null));
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/Range.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/Range.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/Range.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/Range.java	2013-12-14 15:08:29.571271998 -0500
@@ -0,0 +1,36 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Base class for a single labeled range.
+ *
+ *  @lucene.experimental */
+public abstract class Range {
+  public final String label;
+
+  protected Range(String label) {
+    if (label == null) {
+      throw new NullPointerException("label cannot be null");
+    }
+    this.label = label;
+  }
+
+  protected void failNoMatch() {
+    throw new IllegalArgumentException("range \"" + label + "\" matches nothing");
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/sampling/package.html simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sampling/package.html
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/sampling/package.html	2013-02-20 13:38:17.692711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sampling/package.html	1969-12-31 19:00:00.000000000 -0500
@@ -1,24 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Facets sampling</title>
-</head>
-<body>
-Facets sampling.
-</body>
-</html>
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/sampling/RandomSampler.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sampling/RandomSampler.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/sampling/RandomSampler.java	2013-07-29 13:55:02.629707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sampling/RandomSampler.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,71 +0,0 @@
-package org.apache.lucene.facet.sampling;
-
-import java.io.IOException;
-import java.util.Random;
-
-import org.apache.lucene.facet.old.ScoredDocIDs;
-import org.apache.lucene.facet.old.ScoredDocIDsIterator;
-import org.apache.lucene.facet.old.ScoredDocIdsUtils;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Simple random sampler
- */
-public class RandomSampler extends Sampler {
-  
-  private final Random random;
-
-  public RandomSampler() {
-    super();
-    this.random = new Random();
-  }
-
-  public RandomSampler(SamplingParams params, Random random) throws IllegalArgumentException {
-    super(params);
-    this.random = random;
-  }
-
-  @Override
-  protected SampleResult createSample(ScoredDocIDs docids, int actualSize, int sampleSetSize) throws IOException {
-    final int[] sample = new int[sampleSetSize];
-    final int maxStep = (actualSize * 2 ) / sampleSetSize; //floor
-    int remaining = actualSize;
-    ScoredDocIDsIterator it = docids.iterator();
-    int i = 0;
-    // select sample docs with random skipStep, make sure to leave sufficient #docs for selection after last skip
-    while (i<sample.length && remaining>(sampleSetSize-maxStep-i)) {
-      int skipStep = 1 + random.nextInt(maxStep);
-      // Skip over 'skipStep' documents
-      for (int j=0; j<skipStep; j++) {
-        it.next();
-        -- remaining;
-      }
-      sample[i++] = it.getDocID();
-    }
-    // Add leftover documents to the sample set
-    while (i<sample.length) {
-      it.next();
-      sample[i++] = it.getDocID();
-    }
-    ScoredDocIDs sampleRes = ScoredDocIdsUtils.createScoredDocIDsSubset(docids, sample);
-    SampleResult res = new SampleResult(sampleRes, sampleSetSize/(double)actualSize);
-    return res;
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/sampling/RepeatableSampler.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sampling/RepeatableSampler.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/sampling/RepeatableSampler.java	2013-07-29 13:55:02.629707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sampling/RepeatableSampler.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,405 +0,0 @@
-package org.apache.lucene.facet.sampling;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.logging.Level;
-import java.util.logging.Logger;
-
-import org.apache.lucene.facet.old.ScoredDocIDs;
-import org.apache.lucene.facet.old.ScoredDocIDsIterator;
-import org.apache.lucene.facet.old.ScoredDocIdsUtils;
-import org.apache.lucene.util.PriorityQueue;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Take random samples of large collections.
- * @lucene.experimental
- */
-public class RepeatableSampler extends Sampler {
-
-  private static final Logger logger = Logger.getLogger(RepeatableSampler.class.getName());
-
-  public RepeatableSampler(SamplingParams params) {
-    super(params);
-  }
-  
-  @Override
-  protected SampleResult createSample(ScoredDocIDs docids, int actualSize,
-      int sampleSetSize) throws IOException {
-    int[] sampleSet = null;
-    try {
-      sampleSet = repeatableSample(docids, actualSize,
-          sampleSetSize);
-    } catch (IOException e) {
-      if (logger.isLoggable(Level.WARNING)) {
-        logger.log(Level.WARNING, "sampling failed: "+e.getMessage()+" - falling back to no sampling!", e);
-      }
-      return new SampleResult(docids, 1d);
-    }
-
-    ScoredDocIDs sampled = ScoredDocIdsUtils.createScoredDocIDsSubset(docids,
-        sampleSet);
-    if (logger.isLoggable(Level.FINEST)) {
-      logger.finest("******************** " + sampled.size());
-    }
-    return new SampleResult(sampled, sampled.size()/(double)docids.size());
-  }
-  
-  /**
-   * Returns <code>sampleSize</code> values from the first <code>collectionSize</code>
-   * locations of <code>collection</code>, chosen using
-   * the <code>TRAVERSAL</code> algorithm. The sample values are not sorted.
-   * @param collection The values from which a sample is wanted.
-   * @param collectionSize The number of values (from the first) from which to draw the sample.
-   * @param sampleSize The number of values to return.
-   * @return An array of values chosen from the collection.
-   * @see Algorithm#TRAVERSAL
-   */
-  private static int[] repeatableSample(ScoredDocIDs collection,
-      int collectionSize, int sampleSize)
-  throws IOException {
-    return repeatableSample(collection, collectionSize,
-        sampleSize, Algorithm.HASHING, Sorted.NO);
-  }
-
-  /**
-   * Returns <code>sampleSize</code> values from the first <code>collectionSize</code>
-   * locations of <code>collection</code>, chosen using <code>algorithm</code>.
-   * @param collection The values from which a sample is wanted.
-   * @param collectionSize The number of values (from the first) from which to draw the sample.
-   * @param sampleSize The number of values to return.
-   * @param algorithm Which algorithm to use.
-   * @param sorted Sorted.YES to sort the sample values in ascending order before returning;
-   * Sorted.NO to return them in essentially random order.
-   * @return An array of values chosen from the collection.
-   */
-  private static int[] repeatableSample(ScoredDocIDs collection,
-      int collectionSize, int sampleSize,
-      Algorithm algorithm, Sorted sorted)
-  throws IOException {
-    if (collection == null) {
-      throw new IOException("docIdSet is null");
-    }
-    if (sampleSize < 1) {
-      throw new IOException("sampleSize < 1 (" + sampleSize + ")");
-    }
-    if (collectionSize < sampleSize) {
-      throw new IOException("collectionSize (" + collectionSize + ") less than sampleSize (" + sampleSize + ")");
-    }
-    int[] sample = new int[sampleSize];
-    long[] times = new long[4];
-    if (algorithm == Algorithm.TRAVERSAL) {
-      sample1(collection, collectionSize, sample, times);
-    } else if (algorithm == Algorithm.HASHING) {
-      sample2(collection, collectionSize, sample, times);
-    } else {
-      throw new IllegalArgumentException("Invalid algorithm selection");
-    }
-    if (sorted == Sorted.YES) {
-      Arrays.sort(sample);
-    }
-    if (returnTimings) {
-      times[3] = System.currentTimeMillis();
-      if (logger.isLoggable(Level.FINEST)) {
-        logger.finest("Times: " + (times[1] - times[0]) + "ms, "
-            + (times[2] - times[1]) + "ms, " + (times[3] - times[2])+"ms");
-      }
-    }
-    return sample;
-  }
-
-  /**
-   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>
-   * locations of <code>collection</code>, using the TRAVERSAL algorithm. The sample is
-   * pseudorandom: no subset of the original collection
-   * is in principle more likely to occur than any other, but for a given collection
-   * and sample size, the same sample will always be returned. This algorithm walks the
-   * original collection in a methodical way that is guaranteed not to visit any location
-   * more than once, which makes sampling without replacement faster because removals don't
-   * have to be tracked, and the number of operations is proportional to the sample size,
-   * not the collection size.
-   * Times for performance measurement
-   * are returned in <code>times</code>, which must be an array of at least three longs, containing
-   * nanosecond event times. The first
-   * is set when the algorithm starts; the second, when the step size has been calculated;
-   * and the third when the sample has been taken.
-   * @param collection The set to be sampled.
-   * @param collectionSize The number of values to use (starting from first).
-   * @param sample The array in which to return the sample.
-   * @param times The times of three events, for measuring performance.
-   */
-  private static void sample1(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) 
-  throws IOException {
-    ScoredDocIDsIterator it = collection.iterator();
-    if (returnTimings) {
-      times[0] = System.currentTimeMillis();
-    }
-    int sampleSize = sample.length;
-    int prime = findGoodStepSize(collectionSize, sampleSize);
-    int mod = prime % collectionSize;
-    if (returnTimings) {
-      times[1] = System.currentTimeMillis();
-    }
-    int sampleCount = 0;
-    int index = 0;
-    for (; sampleCount < sampleSize;) {
-      if (index + mod < collectionSize) {
-        for (int i = 0; i < mod; i++, index++) {
-          it.next();
-        }
-      } else {
-        index = index + mod - collectionSize;
-        it = collection.iterator();
-        for (int i = 0; i < index; i++) {
-          it.next();
-        }
-      }
-      sample[sampleCount++] = it.getDocID();
-    }
-    if (returnTimings) {
-      times[2] = System.currentTimeMillis();
-    }
-  }
-
-  /**
-   * Returns a value which will allow the caller to walk
-   * a collection of <code>collectionSize</code> values, without repeating or missing
-   * any, and spanning the collection from beginning to end at least once with
-   * <code>sampleSize</code> visited locations. Choosing a value
-   * that is relatively prime to the collection size ensures that stepping by that size (modulo
-   * the collection size) will hit all locations without repeating, eliminating the need to
-   * track previously visited locations for a "without replacement" sample. Starting with the
-   * square root of the collection size ensures that either the first or second prime tried will
-   * work (they can't both divide the collection size). It also has the property that N steps of
-   * size N will span a collection of N**2 elements once. If the sample is bigger than N, it will
-   * wrap multiple times (without repeating). If the sample is smaller, a step size is chosen
-   * that will result in at least one spanning of the collection.
-   * 
-   * @param collectionSize The number of values in the collection to be sampled.
-   * @param sampleSize The number of values wanted in the sample.
-   * @return A good increment value for walking the collection.
-   */
-  private static int findGoodStepSize(int collectionSize, int sampleSize) {
-    int i = (int) Math.sqrt(collectionSize);
-    if (sampleSize < i) {
-      i = collectionSize / sampleSize;
-    }
-    do {
-      i = findNextPrimeAfter(i);
-    } while (collectionSize % i == 0);
-    return i;
-  }
-
-  /**
-   * Returns the first prime number that is larger than <code>n</code>.
-   * @param n A number less than the prime to be returned.
-   * @return The smallest prime larger than <code>n</code>.
-   */
-  private static int findNextPrimeAfter(int n) {
-    n += (n % 2 == 0) ? 1 : 2; // next odd
-    foundFactor: for (;; n += 2) { //TODO labels??!!
-      int sri = (int) (Math.sqrt(n));
-      for (int primeIndex = 0; primeIndex < N_PRIMES; primeIndex++) {
-        int p = primes[primeIndex];
-        if (p > sri) {
-          return n;
-        }
-        if (n % p == 0) {
-          continue foundFactor;
-        }
-      }
-      for (int p = primes[N_PRIMES - 1] + 2;; p += 2) {
-        if (p > sri) {
-          return n;
-        }
-        if (n % p == 0) {
-          continue foundFactor;
-        }
-      }
-    }
-  }
-
-  /**
-   * The first N_PRIMES primes, after 2.
-   */
-  private static final int N_PRIMES = 4000;
-  private static int[] primes = new int[N_PRIMES];
-  static {
-    primes[0] = 3;
-    for (int count = 1; count < N_PRIMES; count++) {
-      primes[count] = findNextPrimeAfter(primes[count - 1]);
-    }
-  }
-
-  /**
-   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>
-   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements
-   * are returned in <code>times</code>, which must be an array of at least three longs. The first
-   * will be set when the algorithm starts; the second, when a hash key has been calculated and
-   * inserted into the priority queue for every element in the collection; and the third when the
-   * original elements associated with the keys remaining in the PQ have been stored in the sample
-   * array for return.
-   * <P>
-   * This algorithm slows as the sample size becomes a significant fraction of the collection
-   * size, because the PQ is as large as the sample set, and will not do early rejection of values
-   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,
-   * resulting in less early rejection and more logN insertions.
-   * 
-   * @param collection The set to be sampled.
-   * @param collectionSize The number of values to use (starting from first).
-   * @param sample The array in which to return the sample.
-   * @param times The times of three events, for measuring performance.
-   */
-  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) 
-  throws IOException {
-    if (returnTimings) {
-      times[0] = System.currentTimeMillis();
-    }
-    int sampleSize = sample.length;
-    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);
-    /*
-     * Convert every value in the collection to a hashed "weight" value, and insert
-     * into a bounded PQ (retains only sampleSize highest weights).
-     */
-    ScoredDocIDsIterator it = collection.iterator();
-    MI mi = null;
-    while (it.next()) {
-      if (mi == null) {
-        mi = new MI();
-      }
-      mi.value = (int) (it.getDocID() * PHI_32) & 0x7FFFFFFF;
-      mi = pq.insertWithOverflow(mi);
-    }
-    if (returnTimings) {
-      times[1] = System.currentTimeMillis();
-    }
-    /*
-     * Extract heap, convert weights back to original values, and return as integers.
-     */
-    Object[] heap = pq.getHeap();
-    for (int si = 0; si < sampleSize; si++) {
-      sample[si] = (int)(((MI) heap[si+1]).value * PHI_32I) & 0x7FFFFFFF;
-    }
-    if (returnTimings) {
-      times[2] = System.currentTimeMillis();
-    }
-  }
-  
-  /**
-   * A mutable integer that lets queue objects be reused once they start overflowing.
-   */
-  private static class MI {
-    MI() { }
-    public int value;
-  }
-
-  /**
-   * A bounded priority queue for Integers, to retain a specified number of
-   * the highest-weighted values for return as a random sample.
-   */
-  private static class IntPriorityQueue extends PriorityQueue<MI> {
-
-    /**
-     * Creates a bounded PQ of size <code>size</code>.
-     * @param size The number of elements to retain.
-     */
-    public IntPriorityQueue(int size) {
-      super(size);
-    }
-
-    /**
-     * Returns the underlying data structure for faster access. Extracting elements
-     * one at a time would require N logN time, and since we want the elements sorted
-     * in ascending order by value (not weight), the array is useful as-is.
-     * @return The underlying heap array.
-     */
-    public Object[] getHeap() {
-      return getHeapArray();
-    }
-
-    /**
-     * Returns true if <code>o1<code>'s weight is less than that of <code>o2</code>, for
-     * ordering in the PQ.
-     * @return True if <code>o1</code> weighs less than <code>o2</code>.
-     */
-    @Override
-    public boolean lessThan(MI o1, MI o2) {
-      return o1.value < o2.value;
-    }
-
-  }
-
-  /**
-   * For specifying which sampling algorithm to use.
-   */
-  private enum Algorithm {
-
-    /**
-     * Specifies a methodical traversal algorithm, which is guaranteed to span the collection
-     * at least once, and never to return duplicates. Faster than the hashing algorithm and
-     * uses much less space, but the randomness of the sample may be affected by systematic
-     * variations in the collection. Requires only an array for the sample, and visits only
-     * the number of elements in the sample set, not the full set.
-     */
-    // TODO (Facet): This one produces a bimodal distribution (very flat around
-    // each peak!) for collection size 10M and sample sizes 10k and 10544.
-    // Figure out why.
-    TRAVERSAL,
-
-    /**
-     * Specifies a Fibonacci-style hash algorithm (see Knuth, S&S), which generates a less
-     * systematically distributed subset of the sampled collection than the traversal method,
-     * but requires a bounded priority queue the size of the sample, and creates an object
-     * containing a sampled value and its hash, for every element in the full set. 
-     */
-    HASHING
-  }
-
-  /**
-   * For specifying whether to sort the sample.
-   */
-  private enum Sorted {
-
-    /**
-     * Sort resulting sample before returning.
-     */
-    YES,
-
-    /**
-     *Do not sort the resulting sample. 
-     */
-    NO
-  }
-
-  /**
-   * Magic number 1: prime closest to phi, in 32 bits.
-   */
-  private static final long PHI_32 = 2654435769L;
-
-  /**
-   * Magic number 2: multiplicative inverse of PHI_32, modulo 2**32.
-   */
-  private static final long PHI_32I = 340573321L;
-
-  /**
-   * Switch to cause methods to return timings.
-   */
-  private static boolean returnTimings = false;
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/sampling/SampleFixer.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sampling/SampleFixer.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/sampling/SampleFixer.java	2013-07-29 13:55:02.629707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sampling/SampleFixer.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,73 +0,0 @@
-package org.apache.lucene.facet.sampling;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.old.ScoredDocIDs;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Fixer of sample facet accumulation results.
- * 
- * @lucene.experimental
- */
-public abstract class SampleFixer {
-  
-  /**
-   * Alter the input result, fixing it to account for the sampling. This
-   * implementation can compute accurate or estimated counts for the sampled
-   * facets. For example, a faster correction could just multiply by a
-   * compensating factor.
-   * 
-   * @param origDocIds
-   *          full set of matching documents.
-   * @param fres
-   *          sample result to be fixed.
-   * @throws IOException
-   *           If there is a low-level I/O error.
-   */
-  public void fixResult(ScoredDocIDs origDocIds, FacetResult fres, double samplingRatio) throws IOException {
-    FacetResultNode topRes = fres.getFacetResultNode();
-    fixResultNode(topRes, origDocIds, samplingRatio);
-  }
-  
-  /**
-   * Fix result node count, and, recursively, fix all its children
-   * 
-   * @param facetResNode
-   *          result node to be fixed
-   * @param docIds
-   *          docids in effect
-   * @throws IOException
-   *           If there is a low-level I/O error.
-   */
-  protected void fixResultNode(FacetResultNode facetResNode, ScoredDocIDs docIds, double samplingRatio) 
-      throws IOException {
-    singleNodeFix(facetResNode, docIds, samplingRatio);
-    for (FacetResultNode frn : facetResNode.subResults) {
-      fixResultNode(frn, docIds, samplingRatio);
-    }
-  }
-  
-  /** Fix the given node's value. */
-  protected abstract void singleNodeFix(FacetResultNode facetResNode, ScoredDocIDs docIds, double samplingRatio) 
-      throws IOException;
-  
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/sampling/Sampler.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sampling/Sampler.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/sampling/Sampler.java	2013-08-01 14:47:20.750689724 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sampling/Sampler.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,216 +0,0 @@
-package org.apache.lucene.facet.sampling;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.facet.old.ScoredDocIDs;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.search.FacetsAggregator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Sampling definition for facets accumulation
- * <p>
- * The Sampler uses TAKMI style counting to provide a 'best guess' top-K result
- * set of the facets accumulated.
- * <p>
- * Note: Sampling accumulation (Accumulation over a sampled-set of the results),
- * does not guarantee accurate values for
- * {@link FacetResult#getNumValidDescendants()}.
- * 
- * @lucene.experimental
- */
-public abstract class Sampler {
-
-  protected final SamplingParams samplingParams;
-  
-  /**
-   * Construct with {@link SamplingParams}
-   */
-  public Sampler() {
-    this(new SamplingParams()); 
-  }
-  
-  /**
-   * Construct with certain {@link SamplingParams}
-   * 
-   * @param params sampling params in effect
-   * @throws IllegalArgumentException if the provided SamplingParams are not valid 
-   */
-  public Sampler(SamplingParams params) throws IllegalArgumentException {
-    if (!params.validate()) {
-      throw new IllegalArgumentException("The provided SamplingParams are not valid!!");
-    }
-    this.samplingParams = params;
-  }
-
-  /**
-   * Check if this sampler would complement for the input docIds
-   */
-  public boolean shouldSample(ScoredDocIDs docIds) {
-    return docIds.size() > samplingParams.getSamplingThreshold();
-  }
-  
-  /**
-   * Compute a sample set out of the input set, based on the {@link SamplingParams#getSampleRatio()}
-   * in effect. Sub classes can override to alter how the sample set is
-   * computed.
-   * <p> 
-   * If the input set is of size smaller than {@link SamplingParams#getMinSampleSize()}, 
-   * the input set is returned (no sampling takes place).
-   * <p>
-   * Other than that, the returned set size will not be larger than {@link SamplingParams#getMaxSampleSize()} 
-   * nor smaller than {@link SamplingParams#getMinSampleSize()}.  
-   * @param docids
-   *          full set of matching documents out of which a sample is needed.
-   */
-  public SampleResult getSampleSet(ScoredDocIDs docids) throws IOException {
-    if (!shouldSample(docids)) {
-      return new SampleResult(docids, 1d);
-    }
-
-    int actualSize = docids.size();
-    int sampleSetSize = (int) (actualSize * samplingParams.getSampleRatio());
-    sampleSetSize = Math.max(sampleSetSize, samplingParams.getMinSampleSize());
-    sampleSetSize = Math.min(sampleSetSize, samplingParams.getMaxSampleSize());
-
-    return createSample(docids, actualSize, sampleSetSize);
-  }
-
-  /**
-   * Create and return a sample of the input set
-   * @param docids input set out of which a sample is to be created 
-   * @param actualSize original size of set, prior to sampling
-   * @param sampleSetSize required size of sample set
-   * @return sample of the input set in the required size
-   */
-  protected abstract SampleResult createSample(ScoredDocIDs docids, int actualSize, int sampleSetSize) 
-      throws IOException;
-
-  /**
-   * Result of sample computation
-   */
-  public final static class SampleResult {
-    public final ScoredDocIDs docids;
-    public final double actualSampleRatio;
-    protected SampleResult(ScoredDocIDs docids, double actualSampleRatio) {
-      this.docids = docids;
-      this.actualSampleRatio = actualSampleRatio;
-    }
-  }
-  
-  /**
-   * Return the sampling params in effect
-   */
-  public final SamplingParams getSamplingParams() {
-    return samplingParams;
-  }
-
-  /**
-   * Trim the input facet result.<br>
-   * Note: It is only valid to call this method with result obtained for a
-   * facet request created through {@link #overSampledSearchParams(FacetSearchParams)}.
-   * 
-   * @throws IllegalArgumentException
-   *             if called with results not obtained for requests created
-   *             through {@link #overSampledSearchParams(FacetSearchParams)}
-   */
-  public FacetResult trimResult(FacetResult facetResult) throws IllegalArgumentException {
-    double overSampleFactor = getSamplingParams().getOversampleFactor();
-    if (overSampleFactor <= 1) { // no factoring done?
-      return facetResult;
-    }
-    
-    OverSampledFacetRequest sampledFreq = null;
-    
-    try {
-      sampledFreq = (OverSampledFacetRequest) facetResult.getFacetRequest();
-    } catch (ClassCastException e) {
-      throw new IllegalArgumentException(
-          "It is only valid to call this method with result obtained for a " +
-          "facet request created through sampler.overSamlpingSearchParams()",
-          e);
-    }
-    
-    FacetRequest origFrq = sampledFreq.orig;
-
-    FacetResultNode trimmedRootNode = facetResult.getFacetResultNode();
-    trimSubResults(trimmedRootNode, origFrq.numResults);
-    
-    return new FacetResult(origFrq, trimmedRootNode, facetResult.getNumValidDescendants());
-  }
-  
-  /** Trim sub results to a given size. */
-  private void trimSubResults(FacetResultNode node, int size) {
-    if (node.subResults == FacetResultNode.EMPTY_SUB_RESULTS || node.subResults.size() == 0) {
-      return;
-    }
-
-    ArrayList<FacetResultNode> trimmed = new ArrayList<FacetResultNode>(size);
-    for (int i = 0; i < node.subResults.size() && i < size; i++) {
-      FacetResultNode trimmedNode = node.subResults.get(i);
-      trimSubResults(trimmedNode, size);
-      trimmed.add(trimmedNode);
-    }
-    
-    node.subResults = trimmed;
-  }
-
-  /**
-   * Over-sampled search params, wrapping each request with an over-sampled one.
-   */
-  public FacetSearchParams overSampledSearchParams(FacetSearchParams original) {
-    FacetSearchParams res = original;
-    // So now we can sample -> altering the searchParams to accommodate for the statistical error for the sampling
-    double overSampleFactor = getSamplingParams().getOversampleFactor();
-    if (overSampleFactor > 1) { // any factoring to do?
-      List<FacetRequest> facetRequests = new ArrayList<FacetRequest>();
-      for (FacetRequest frq : original.facetRequests) {
-        int overSampledNumResults = (int) Math.ceil(frq.numResults * overSampleFactor);
-        facetRequests.add(new OverSampledFacetRequest(frq, overSampledNumResults));
-      }
-      res = new FacetSearchParams(original.indexingParams, facetRequests);
-    }
-    return res;
-  }
-  
-  /** Wrapping a facet request for over sampling. */
-  public static class OverSampledFacetRequest extends FacetRequest {
-    public final FacetRequest orig;
-    public OverSampledFacetRequest(FacetRequest orig, int num) {
-      super(orig.categoryPath, num);
-      this.orig = orig;
-      setDepth(orig.getDepth());
-      setNumLabel(0); // don't label anything as we're over-sampling
-      setResultMode(orig.getResultMode());
-      setSortOrder(orig.getSortOrder());
-    }
-    
-    @Override
-    public FacetsAggregator createFacetsAggregator(FacetIndexingParams fip) {
-      return orig.createFacetsAggregator(fip);
-    }
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingAccumulator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingAccumulator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingAccumulator.java	2013-08-01 14:47:20.750689724 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingAccumulator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,137 +0,0 @@
-package org.apache.lucene.facet.sampling;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.facet.old.OldFacetsAccumulator;
-import org.apache.lucene.facet.old.ScoredDocIDs;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.partitions.PartitionsFacetResultsHandler;
-import org.apache.lucene.facet.sampling.Sampler.SampleResult;
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetsAccumulator;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.IndexReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Facets accumulation with sampling.<br>
- * <p>
- * Note two major differences between this class and {@link SamplingWrapper}:
- * <ol>
- * <li>Latter can wrap any other {@link FacetsAccumulator} while this class
- * directly extends {@link OldFacetsAccumulator}.</li>
- * <li>This class can effectively apply sampling on the complement set of
- * matching document, thereby working efficiently with the complement
- * optimization - see {@link OldFacetsAccumulator#getComplementThreshold()}
- * .</li>
- * </ol>
- * <p>
- * Note: Sampling accumulation (Accumulation over a sampled-set of the results),
- * does not guarantee accurate values for
- * {@link FacetResult#getNumValidDescendants()}.
- * 
- * @see Sampler
- * @lucene.experimental
- */
-public class SamplingAccumulator extends OldFacetsAccumulator {
-  
-  private double samplingRatio = -1d;
-  private final Sampler sampler;
-  
-  public SamplingAccumulator(Sampler sampler, FacetSearchParams searchParams,
-      IndexReader indexReader, TaxonomyReader taxonomyReader,
-      FacetArrays facetArrays) {
-    super(searchParams, indexReader, taxonomyReader, facetArrays);
-    this.sampler = sampler;
-  }
-
-  /**
-   * Constructor...
-   */
-  public SamplingAccumulator(
-      Sampler sampler,
-      FacetSearchParams searchParams,
-      IndexReader indexReader, TaxonomyReader taxonomyReader) {
-    super(searchParams, indexReader, taxonomyReader);
-    this.sampler = sampler;
-  }
-
-  @Override
-  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {
-    // Replacing the original searchParams with the over-sampled
-    FacetSearchParams original = searchParams;
-    SampleFixer samplerFixer = sampler.samplingParams.getSampleFixer();
-    final boolean shouldOversample = sampler.samplingParams.shouldOverSample();
-    if (shouldOversample) {
-      searchParams = sampler.overSampledSearchParams(original);
-    }
-    
-    List<FacetResult> sampleRes = super.accumulate(docids);
-    
-    List<FacetResult> results = new ArrayList<FacetResult>();
-    for (FacetResult fres : sampleRes) {
-      // for sure fres is not null because this is guaranteed by the delegee.
-      FacetRequest fr = fres.getFacetRequest();
-      PartitionsFacetResultsHandler frh = createFacetResultsHandler(fr, createOrdinalValueResolver(fr));
-      if (samplerFixer != null) {
-        // fix the result of current request
-        samplerFixer.fixResult(docids, fres, samplingRatio);
-        
-        fres = frh.rearrangeFacetResult(fres); // let delegee's handler do any arranging it needs to
-
-        if (shouldOversample) {
-          // Using the sampler to trim the extra (over-sampled) results
-          fres = sampler.trimResult(fres);
-        }
-      }
-      
-      // final labeling if allowed (because labeling is a costly operation)
-      if (fres.getFacetResultNode().ordinal == TaxonomyReader.INVALID_ORDINAL) {
-        // category does not exist, add an empty result
-        results.add(emptyResult(fres.getFacetResultNode().ordinal, fr));
-      } else {
-        frh.labelResult(fres);
-        results.add(fres);
-      }
-    }
-    
-    searchParams = original; // Back to original params
-    
-    return results; 
-  }
-
-  @Override
-  protected ScoredDocIDs actualDocsToAccumulate(ScoredDocIDs docids) throws IOException {
-    SampleResult sampleRes = sampler.getSampleSet(docids);
-    samplingRatio = sampleRes.actualSampleRatio;
-    return sampleRes.docids;
-  }
-  
-  @Override
-  protected double getTotalCountsFactor() {
-    if (samplingRatio<0) {
-      throw new IllegalStateException("Total counts ratio unavailable because actualDocsToAccumulate() was not invoked");
-    }
-    return samplingRatio;
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingParams.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingParams.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingParams.java	2013-07-29 13:55:02.629707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingParams.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,196 +0,0 @@
-package org.apache.lucene.facet.sampling;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Parameters for sampling, dictating whether sampling is to take place and how. 
- * 
- * @lucene.experimental
- */
-public class SamplingParams {
-
-  /**
-   * Default factor by which more results are requested over the sample set.
-   * @see SamplingParams#getOversampleFactor()
-   */
-  public static final double DEFAULT_OVERSAMPLE_FACTOR = 1d;
-  
-  /**
-   * Default ratio between size of sample to original size of document set.
-   * @see Sampler#getSampleSet(org.apache.lucene.facet.old.ScoredDocIDs)
-   */
-  public static final double DEFAULT_SAMPLE_RATIO = 0.01;
-  
-  /**
-   * Default maximum size of sample.
-   * @see Sampler#getSampleSet(org.apache.lucene.facet.old.ScoredDocIDs)
-   */
-  public static final int DEFAULT_MAX_SAMPLE_SIZE = 10000;
-  
-  /**
-   * Default minimum size of sample.
-   * @see Sampler#getSampleSet(org.apache.lucene.facet.old.ScoredDocIDs)
-   */
-  public static final int DEFAULT_MIN_SAMPLE_SIZE = 100;
-  
-  /**
-   * Default sampling threshold, if number of results is less than this number - no sampling will take place
-   * @see SamplingParams#getSampleRatio()
-   */
-  public static final int DEFAULT_SAMPLING_THRESHOLD = 75000;
-
-  private int maxSampleSize = DEFAULT_MAX_SAMPLE_SIZE;
-  private int minSampleSize = DEFAULT_MIN_SAMPLE_SIZE;
-  private double sampleRatio = DEFAULT_SAMPLE_RATIO;
-  private int samplingThreshold = DEFAULT_SAMPLING_THRESHOLD;
-  private double oversampleFactor = DEFAULT_OVERSAMPLE_FACTOR;
-
-  private SampleFixer sampleFixer = null;
-  
-  /**
-   * Return the maxSampleSize.
-   * In no case should the resulting sample size exceed this value.  
-   * @see Sampler#getSampleSet(org.apache.lucene.facet.old.ScoredDocIDs)
-   */
-  public final int getMaxSampleSize() {
-    return maxSampleSize;
-  }
-
-  /**
-   * Return the minSampleSize.
-   * In no case should the resulting sample size be smaller than this value.  
-   * @see Sampler#getSampleSet(org.apache.lucene.facet.old.ScoredDocIDs)
-   */
-  public final int getMinSampleSize() {
-    return minSampleSize;
-  }
-
-  /**
-   * @return the sampleRatio
-   * @see Sampler#getSampleSet(org.apache.lucene.facet.old.ScoredDocIDs)
-   */
-  public final double getSampleRatio() {
-    return sampleRatio;
-  }
-  
-  /**
-   * Return the samplingThreshold.
-   * Sampling would be performed only for document sets larger than this.  
-   */
-  public final int getSamplingThreshold() {
-    return samplingThreshold;
-  }
-
-  /**
-   * @param maxSampleSize
-   *          the maxSampleSize to set
-   * @see #getMaxSampleSize()
-   */
-  public void setMaxSampleSize(int maxSampleSize) {
-    this.maxSampleSize = maxSampleSize;
-  }
-
-  /**
-   * @param minSampleSize
-   *          the minSampleSize to set
-   * @see #getMinSampleSize()
-   */
-  public void setMinSampleSize(int minSampleSize) {
-    this.minSampleSize = minSampleSize;
-  }
-
-  /**
-   * @param sampleRatio
-   *          the sampleRatio to set
-   * @see #getSampleRatio()
-   */
-  public void setSampleRatio(double sampleRatio) {
-    this.sampleRatio = sampleRatio;
-  }
-
-  /**
-   * Set a sampling-threshold
-   * @see #getSamplingThreshold()
-   */
-  public void setSamplingThreshold(int samplingThreshold) {
-    this.samplingThreshold = samplingThreshold;
-  }
-
-  /**
-   * Check validity of sampling settings, making sure that
-   * <ul>
-   * <li> <code>minSampleSize <= maxSampleSize <= samplingThreshold </code></li>
-   * <li> <code>0 < samplingRatio <= 1 </code></li>
-   * </ul> 
-   * 
-   * @return true if valid, false otherwise
-   */
-  public boolean validate() {
-    return 
-      samplingThreshold >= maxSampleSize && 
-      maxSampleSize >= minSampleSize && 
-      sampleRatio > 0 &&
-      sampleRatio < 1;
-  }
-
-  /**
-   * Return the oversampleFactor. When sampling, we would collect that much more
-   * results, so that later, when selecting top out of these, chances are higher
-   * to get actual best results. Note that having this value larger than 1 only
-   * makes sense when using a SampleFixer which finds accurate results, such as
-   * <code>TakmiSampleFixer</code>. When this value is smaller than 1, it is
-   * ignored and no oversampling takes place.
-   */
-  public final double getOversampleFactor() {
-    return oversampleFactor;
-  }
-
-  /**
-   * @param oversampleFactor the oversampleFactor to set
-   * @see #getOversampleFactor()
-   */
-  public void setOversampleFactor(double oversampleFactor) {
-    this.oversampleFactor = oversampleFactor;
-  }
-
-  /**
-   * @return {@link SampleFixer} to be used while fixing the sampled results, if
-   *         <code>null</code> no fixing will be performed
-   */
-  public SampleFixer getSampleFixer() {
-    return sampleFixer;
-  }
-
-  /**
-   * Set a {@link SampleFixer} to be used while fixing the sampled results.
-   * {@code null} means no fixing will be performed
-   */
-  public void setSampleFixer(SampleFixer sampleFixer) {
-    this.sampleFixer = sampleFixer;
-  }
-
-  /**
-   * Returns whether over-sampling should be done. By default returns
-   * {@code true} when {@link #getSampleFixer()} is not {@code null} and
-   * {@link #getOversampleFactor()} &gt; 1, {@code false} otherwise.
-   */
-  public boolean shouldOverSample() {
-    return sampleFixer != null && oversampleFactor > 1d;
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingWrapper.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingWrapper.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingWrapper.java	2013-08-01 14:47:20.750689724 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingWrapper.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,112 +0,0 @@
-package org.apache.lucene.facet.sampling;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.facet.old.OldFacetsAccumulator;
-import org.apache.lucene.facet.old.ScoredDocIDs;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.partitions.PartitionsFacetResultsHandler;
-import org.apache.lucene.facet.sampling.Sampler.SampleResult;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Wrap any Facets Accumulator with sampling.
- * <p>
- * Note: Sampling accumulation (Accumulation over a sampled-set of the results),
- * does not guarantee accurate values for
- * {@link FacetResult#getNumValidDescendants()}.
- * 
- * @lucene.experimental
- */
-public class SamplingWrapper extends OldFacetsAccumulator {
-
-  private OldFacetsAccumulator delegee;
-  private Sampler sampler;
-
-  public SamplingWrapper(OldFacetsAccumulator delegee, Sampler sampler) {
-    super(delegee.searchParams, delegee.indexReader, delegee.taxonomyReader);
-    this.delegee = delegee;
-    this.sampler = sampler;
-  }
-
-  @Override
-  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {
-    // Replacing the original searchParams with the over-sampled (and without statistics-compute)
-    FacetSearchParams original = delegee.searchParams;
-    boolean shouldOversample = sampler.samplingParams.shouldOverSample();
-   
-    if (shouldOversample) {
-      delegee.searchParams = sampler.overSampledSearchParams(original);
-    }
-    
-    SampleResult sampleSet = sampler.getSampleSet(docids);
-
-    List<FacetResult> sampleRes = delegee.accumulate(sampleSet.docids);
-
-    List<FacetResult> results = new ArrayList<FacetResult>();
-    SampleFixer sampleFixer = sampler.samplingParams.getSampleFixer();
-    
-    for (FacetResult fres : sampleRes) {
-      // for sure fres is not null because this is guaranteed by the delegee.
-      FacetRequest fr = fres.getFacetRequest();
-      PartitionsFacetResultsHandler frh = createFacetResultsHandler(fr, createOrdinalValueResolver(fr));
-      if (sampleFixer != null) {
-        // fix the result of current request
-        sampleFixer.fixResult(docids, fres, sampleSet.actualSampleRatio); 
-        fres = frh.rearrangeFacetResult(fres); // let delegee's handler do any
-      }
-      
-      if (shouldOversample) {
-        // Using the sampler to trim the extra (over-sampled) results
-        fres = sampler.trimResult(fres);
-      }
-      
-      // final labeling if allowed (because labeling is a costly operation)
-      if (fres.getFacetResultNode().ordinal == TaxonomyReader.INVALID_ORDINAL) {
-        // category does not exist, add an empty result
-        results.add(emptyResult(fres.getFacetResultNode().ordinal, fr));
-      } else {
-        frh.labelResult(fres);
-        results.add(fres);
-      }
-    }
-
-    if (shouldOversample) {
-      delegee.searchParams = original; // Back to original params
-    }
-    
-    return results; 
-  }
-
-  @Override
-  public double getComplementThreshold() {
-    return delegee.getComplementThreshold();
-  }
-
-  @Override
-  public void setComplementThreshold(double complementThreshold) {
-    delegee.setComplementThreshold(complementThreshold);
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/sampling/TakmiSampleFixer.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sampling/TakmiSampleFixer.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/sampling/TakmiSampleFixer.java	2013-07-29 13:55:02.629707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sampling/TakmiSampleFixer.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,168 +0,0 @@
-package org.apache.lucene.facet.sampling;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.old.ScoredDocIDs;
-import org.apache.lucene.facet.old.ScoredDocIDsIterator;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.DrillDownQuery;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.util.Bits;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Fix sampling results by correct results, by counting the intersection between
- * two lists: a TermDocs (list of documents in a certain category) and a
- * DocIdSetIterator (list of documents matching the query).
- * <p>
- * This fixer is suitable for scenarios which prioritize accuracy over
- * performance. 
- * <p>
- * <b>Note:</b> for statistically more accurate top-k selection, set
- * {@link SamplingParams#setOversampleFactor(double) oversampleFactor} to at
- * least 2, so that the top-k categories would have better chance of showing up
- * in the sampled top-cK results (see {@link SamplingParams#getOversampleFactor}
- * 
- * @lucene.experimental
- */
-public class TakmiSampleFixer extends SampleFixer {
-  
-  private TaxonomyReader taxonomyReader;
-  private IndexReader indexReader;
-  private FacetSearchParams searchParams;
-  
-  public TakmiSampleFixer(IndexReader indexReader,
-      TaxonomyReader taxonomyReader, FacetSearchParams searchParams) {
-    this.indexReader = indexReader;
-    this.taxonomyReader = taxonomyReader;
-    this.searchParams = searchParams;
-  }
-
-  @Override
-  public void singleNodeFix(FacetResultNode facetResNode, ScoredDocIDs docIds, double samplingRatio) throws IOException {
-    recount(facetResNode, docIds);
-  }
-  
-  /**
-   * Internal utility: recount for a facet result node
-   * 
-   * @param fresNode
-   *          result node to be recounted
-   * @param docIds
-   *          full set of matching documents.
-   * @throws IOException If there is a low-level I/O error.
-   */
-  private void recount(FacetResultNode fresNode, ScoredDocIDs docIds) throws IOException {
-    // TODO (Facet): change from void to return the new, smaller docSet, and use
-    // that for the children, as this will make their intersection ops faster.
-    // can do this only when the new set is "sufficiently" smaller.
-    
-    /* We need the category's path name in order to do its recounting.
-     * If it is missing, because the option to label only part of the
-     * facet results was exercise, we need to calculate them anyway, so
-     * in essence sampling with recounting spends some extra cycles for
-     * labeling results for which labels are not required. */
-    if (fresNode.label == null) {
-      fresNode.label = taxonomyReader.getPath(fresNode.ordinal);
-    }
-    CategoryPath catPath = fresNode.label;
-
-    Term drillDownTerm = DrillDownQuery.term(searchParams.indexingParams, catPath);
-    // TODO (Facet): avoid Multi*?
-    Bits liveDocs = MultiFields.getLiveDocs(indexReader);
-    int updatedCount = countIntersection(MultiFields.getTermDocsEnum(indexReader, liveDocs,
-                                                                     drillDownTerm.field(), drillDownTerm.bytes(),
-                                                                     0), docIds.iterator());
-    fresNode.value = updatedCount;
-  }
-
-  /**
-   * Count the size of the intersection between two lists: a TermDocs (list of
-   * documents in a certain category) and a DocIdSetIterator (list of documents
-   * matching a query).
-   */
-  private static int countIntersection(DocsEnum p1, ScoredDocIDsIterator p2)
-      throws IOException {
-    // The documentation of of both TermDocs and DocIdSetIterator claim
-    // that we must do next() before doc(). So we do, and if one of the
-    // lists is empty, obviously return 0;
-    if (p1 == null || p1.nextDoc() == DocIdSetIterator.NO_MORE_DOCS) {
-      return 0;
-    }
-    if (!p2.next()) {
-      return 0;
-    }
-    
-    int d1 = p1.docID();
-    int d2 = p2.getDocID();
-
-    int count = 0;
-    for (;;) {
-      if (d1 == d2) {
-        ++count;
-        if (p1.nextDoc() == DocIdSetIterator.NO_MORE_DOCS) {
-          break; // end of list 1, nothing more in intersection
-        }
-        d1 = p1.docID();
-        if (!advance(p2, d1)) {
-          break; // end of list 2, nothing more in intersection
-        }
-        d2 = p2.getDocID();
-      } else if (d1 < d2) {
-        if (p1.advance(d2) == DocIdSetIterator.NO_MORE_DOCS) {
-          break; // end of list 1, nothing more in intersection
-        }
-        d1 = p1.docID();
-      } else /* d1>d2 */ {
-        if (!advance(p2, d1)) {
-          break; // end of list 2, nothing more in intersection
-        }
-        d2 = p2.getDocID();
-      }
-    }
-    return count;
-  }
-
-  /**
-   * utility: advance the iterator until finding (or exceeding) specific
-   * document
-   * 
-   * @param iterator
-   *          iterator being advanced
-   * @param targetDoc
-   *          target of advancing
-   * @return false if iterator exhausted, true otherwise.
-   */
-  private static boolean advance(ScoredDocIDsIterator iterator, int targetDoc) {
-    while (iterator.next()) {
-      if (iterator.getDocID() >= targetDoc) {
-        return true; // target reached
-      }
-    }
-    return false; // exhausted
-  }
-
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/ArraysPool.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/ArraysPool.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/ArraysPool.java	2012-12-11 10:38:55.341944739 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/ArraysPool.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,110 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.util.Arrays;
-import java.util.concurrent.ArrayBlockingQueue;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A thread-safe pool of {@code int[]} and {@code float[]} arrays. One specifies
- * the maximum number of arrays in the constructor. Calls to
- * {@link #allocateFloatArray()} or {@link #allocateIntArray()} take an array
- * from the pool, and if one is not available, allocate a new one. When you are
- * done using the array, you should {@link #free(int[]) free} it.
- * <p>
- * This class is used by {@link ReusingFacetArrays} for temporal facet
- * aggregation arrays, which can be reused across searches instead of being
- * allocated afresh on every search.
- * 
- * @lucene.experimental
- */
-public final class ArraysPool {
-
-  private final ArrayBlockingQueue<int[]> intsPool;
-  private final ArrayBlockingQueue<float[]> floatsPool;
-  
-  public final int arrayLength;
-  
-  /**
-   * Specifies the max number of arrays to pool, as well as the length of each
-   * array to allocate.
-   * 
-   * @param arrayLength the size of the arrays to allocate
-   * @param maxArrays the maximum number of arrays to pool, from each type
-   * 
-   * @throws IllegalArgumentException if maxArrays is set to 0.
-   */
-  public ArraysPool(int arrayLength, int maxArrays) {
-    if (maxArrays == 0) {
-      throw new IllegalArgumentException(
-          "maxArrays cannot be 0 - don't use this class if you don't intend to pool arrays");
-    }
-    this.arrayLength = arrayLength;
-    this.intsPool = new ArrayBlockingQueue<int[]>(maxArrays);
-    this.floatsPool = new ArrayBlockingQueue<float[]>(maxArrays);
-  }
-
-  /**
-   * Allocates a new {@code int[]}. If there's an available array in the pool,
-   * it is used, otherwise a new array is allocated.
-   */
-  public final int[] allocateIntArray() {
-    int[] arr = intsPool.poll();
-    if (arr == null) {
-      return new int[arrayLength];
-    }
-    Arrays.fill(arr, 0); // reset array
-    return arr;
-  }
-
-  /**
-   * Allocates a new {@code float[]}. If there's an available array in the pool,
-   * it is used, otherwise a new array is allocated.
-   */
-  public final float[] allocateFloatArray() {
-    float[] arr = floatsPool.poll();
-    if (arr == null) {
-      return new float[arrayLength];
-    }
-    Arrays.fill(arr, 0f); // reset array
-    return arr;
-  }
-
-  /**
-   * Frees a no-longer-needed array. If there's room in the pool, the array is
-   * added to it, otherwise discarded.
-   */
-  public final void free(int[] arr) {
-    if (arr != null) {
-      // use offer - if there isn't room, we don't want to wait
-      intsPool.offer(arr);
-    }
-  }
-
-  /**
-   * Frees a no-longer-needed array. If there's room in the pool, the array is
-   * added to it, otherwise discarded.
-   */
-  public final void free(float[] arr) {
-    if (arr != null) {
-      // use offer - if there isn't room, we don't want to wait
-      floatsPool.offer(arr);
-    }
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/CachedOrdsCountingFacetsAggregator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/CachedOrdsCountingFacetsAggregator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/CachedOrdsCountingFacetsAggregator.java	2013-02-20 13:38:17.660711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/CachedOrdsCountingFacetsAggregator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,54 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.search.OrdinalsCache.CachedOrds;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetsAggregator} which updates categories values by counting their
- * occurrences in matching documents. Uses {@link OrdinalsCache} to obtain the
- * category ordinals of each segment.
- * 
- * @lucene.experimental
- */
-public class CachedOrdsCountingFacetsAggregator extends IntRollupFacetsAggregator {
-  
-  @Override
-  public void aggregate(MatchingDocs matchingDocs, CategoryListParams clp, FacetArrays facetArrays) throws IOException {
-    final CachedOrds ords = OrdinalsCache.getCachedOrds(matchingDocs.context, clp);
-    if (ords == null) {
-      return; // this segment has no ordinals for the given category list
-    }
-    final int[] counts = facetArrays.getIntArray();
-    int doc = 0;
-    int length = matchingDocs.bits.length();
-    while (doc < length && (doc = matchingDocs.bits.nextSetBit(doc)) != -1) {
-      int start = ords.offsets[doc];
-      int end = ords.offsets[doc + 1];
-      for (int i = start; i < end; i++) {
-        ++counts[ords.ordinals[i]];
-      }
-      ++doc;
-    }
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/CategoryListIterator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/CategoryListIterator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/CategoryListIterator.java	2013-01-14 13:43:40.376580180 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/CategoryListIterator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,56 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An interface for obtaining the category ordinals of documents.
- * {@link #getOrdinals(int, IntsRef)} calls are done with document IDs that are
- * local to the reader given to {@link #setNextReader(AtomicReaderContext)}.
- * <p>
- * <b>NOTE:</b> this class operates as a key to a map, and therefore you should
- * implement {@code equals()} and {@code hashCode()} for proper behavior.
- * 
- * @lucene.experimental
- */
-public interface CategoryListIterator {
-
-  /**
-   * Sets the {@link AtomicReaderContext} for which
-   * {@link #getOrdinals(int, IntsRef)} calls will be made. Returns true iff any
-   * of the documents in this reader have category ordinals. This method must be
-   * called before any calls to {@link #getOrdinals(int, IntsRef)}.
-   */
-  public boolean setNextReader(AtomicReaderContext context) throws IOException;
-  
-  /**
-   * Stores the category ordinals of the given document ID in the given
-   * {@link IntsRef}, starting at position 0 upto {@link IntsRef#length}. Grows
-   * the {@link IntsRef} if it is not large enough.
-   * 
-   * <p>
-   * <b>NOTE:</b> if the requested document does not have category ordinals
-   * associated with it, {@link IntsRef#length} is set to zero.
-   */
-  public void getOrdinals(int docID, IntsRef ints) throws IOException;
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/CountFacetRequest.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/CountFacetRequest.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/CountFacetRequest.java	2013-08-01 14:47:20.750689724 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/CountFacetRequest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,39 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Facet request for counting facets.
- * 
- * @lucene.experimental
- */
-public class CountFacetRequest extends FacetRequest {
-
-  public CountFacetRequest(CategoryPath path, int num) {
-    super(path, num);
-  }
-
-  @Override
-  public FacetsAggregator createFacetsAggregator(FacetIndexingParams fip) {
-    return CountingFacetsAggregator.create(fip.getCategoryListParams(categoryPath));
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/CountingFacetsAggregator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/CountingFacetsAggregator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/CountingFacetsAggregator.java	2013-07-29 13:55:02.625707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/CountingFacetsAggregator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,72 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.encoding.DGapVInt8IntDecoder;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetsAggregator} which counts the number of times each category
- * appears in the given set of documents. This aggregator uses the
- * {@link CategoryListIterator} to read the encoded categories. If you used the
- * default settings while idnexing, you can use
- * {@link FastCountingFacetsAggregator} for better performance.
- * 
- * @lucene.experimental
- */
-public class CountingFacetsAggregator extends IntRollupFacetsAggregator {
-  
-  /**
-   * Returns a {@link FacetsAggregator} suitable for counting categories given
-   * the {@link CategoryListParams}.
-   */
-  public static FacetsAggregator create(CategoryListParams clp) {
-    if (clp.createEncoder().createMatchingDecoder().getClass() == DGapVInt8IntDecoder.class) {
-      return new FastCountingFacetsAggregator();
-    } else {
-      return new CountingFacetsAggregator();
-    }
-  }
-
-  private final IntsRef ordinals = new IntsRef(32);
-  
-  @Override
-  public void aggregate(MatchingDocs matchingDocs, CategoryListParams clp, FacetArrays facetArrays) throws IOException {
-    final CategoryListIterator cli = clp.createCategoryListIterator(0);
-    if (!cli.setNextReader(matchingDocs.context)) {
-      return;
-    }
-    
-    final int length = matchingDocs.bits.length();
-    final int[] counts = facetArrays.getIntArray();
-    int doc = 0;
-    while (doc < length && (doc = matchingDocs.bits.nextSetBit(doc)) != -1) {
-      cli.getOrdinals(doc, ordinals);
-      final int upto = ordinals.offset + ordinals.length;
-      for (int i = ordinals.offset; i < upto; i++) {
-        ++counts[ordinals.ints[i]];
-      }
-      ++doc;
-    }
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/DepthOneFacetResultsHandler.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/DepthOneFacetResultsHandler.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/DepthOneFacetResultsHandler.java	2013-08-01 14:47:20.746689725 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/DepthOneFacetResultsHandler.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,136 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.Comparator;
-
-import org.apache.lucene.facet.search.FacetRequest.SortOrder;
-import org.apache.lucene.facet.taxonomy.ParallelTaxonomyArrays;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.util.CollectionUtil;
-import org.apache.lucene.util.PriorityQueue;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetResultsHandler} which counts the top-K facets at depth 1 only
- * and always labels all result categories. The results are always sorted by
- * value, in descending order.
- * 
- * @lucene.experimental
- */
-public class DepthOneFacetResultsHandler extends FacetResultsHandler {
-  
-  private static class FacetResultNodeQueue extends PriorityQueue<FacetResultNode> {
-    
-    public FacetResultNodeQueue(int maxSize, boolean prepopulate) {
-      super(maxSize, prepopulate);
-    }
-    
-    @Override
-    protected FacetResultNode getSentinelObject() {
-      return new FacetResultNode(TaxonomyReader.INVALID_ORDINAL, 0);
-    }
-    
-    @Override
-    protected boolean lessThan(FacetResultNode a, FacetResultNode b) {
-      return a.compareTo(b)  < 0;
-    }
-    
-  }
-
-  public DepthOneFacetResultsHandler(TaxonomyReader taxonomyReader, FacetRequest facetRequest, FacetArrays facetArrays, 
-      OrdinalValueResolver resolver) {
-    super(taxonomyReader, facetRequest, resolver, facetArrays);
-    assert facetRequest.getDepth() == 1 : "this handler only computes the top-K facets at depth 1";
-    assert facetRequest.numResults == facetRequest.getNumLabel() : "this handler always labels all top-K results";
-    assert facetRequest.getSortOrder() == SortOrder.DESCENDING : "this handler always sorts results in descending order";
-  }
-
-  @Override
-  public final FacetResult compute() throws IOException {
-    ParallelTaxonomyArrays arrays = taxonomyReader.getParallelTaxonomyArrays();
-    final int[] children = arrays.children();
-    final int[] siblings = arrays.siblings();
-    
-    int rootOrd = taxonomyReader.getOrdinal(facetRequest.categoryPath);
-        
-    FacetResultNode root = new FacetResultNode(rootOrd, resolver.valueOf(rootOrd));
-    root.label = facetRequest.categoryPath;
-    if (facetRequest.numResults > taxonomyReader.getSize()) {
-      // specialize this case, user is interested in all available results
-      ArrayList<FacetResultNode> nodes = new ArrayList<FacetResultNode>();
-      int ordinal = children[rootOrd];
-      while (ordinal != TaxonomyReader.INVALID_ORDINAL) {
-        double value = resolver.valueOf(ordinal);
-        if (value > 0) {
-          FacetResultNode node = new FacetResultNode(ordinal, value);
-          node.label = taxonomyReader.getPath(ordinal);
-          nodes.add(node);
-        }
-        ordinal = siblings[ordinal];
-      }
-
-      CollectionUtil.introSort(nodes, Collections.reverseOrder(new Comparator<FacetResultNode>() {
-        @Override
-        public int compare(FacetResultNode o1, FacetResultNode o2) {
-          return o1.compareTo(o2);
-        }
-      }));
-      
-      root.subResults = nodes;
-      return new FacetResult(facetRequest, root, nodes.size());
-    }
-    
-    // since we use sentinel objects, we cannot reuse PQ. but that's ok because it's not big
-    PriorityQueue<FacetResultNode> pq = new FacetResultNodeQueue(facetRequest.numResults, true);
-    int ordinal = children[rootOrd];
-    FacetResultNode top = pq.top();
-    int numSiblings = 0;
-    while (ordinal != TaxonomyReader.INVALID_ORDINAL) {
-      double value = resolver.valueOf(ordinal);
-      if (value > 0) {
-        ++numSiblings;
-        if (value > top.value) {
-          top.value = value;
-          top.ordinal = ordinal;
-          top = pq.updateTop();
-        }
-      }
-      ordinal = siblings[ordinal];
-    }
-
-    // pop() the least (sentinel) elements
-    int pqsize = pq.size();
-    int size = numSiblings < pqsize ? numSiblings : pqsize;
-    for (int i = pqsize - size; i > 0; i--) { pq.pop(); }
-
-    // create the FacetResultNodes.
-    FacetResultNode[] subResults = new FacetResultNode[size];
-    for (int i = size - 1; i >= 0; i--) {
-      FacetResultNode node = pq.pop();
-      node.label = taxonomyReader.getPath(node.ordinal);
-      subResults[i] = node;
-    }
-    root.subResults = Arrays.asList(subResults);
-    return new FacetResult(facetRequest, root, numSiblings);
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/DocValuesCategoryListIterator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/DocValuesCategoryListIterator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/DocValuesCategoryListIterator.java	2013-02-20 13:38:17.660711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/DocValuesCategoryListIterator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,87 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.encoding.IntDecoder;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** A {@link CategoryListIterator} which reads the ordinals from a {@link BinaryDocValues}. */
-public class DocValuesCategoryListIterator implements CategoryListIterator {
-  
-  private final IntDecoder decoder;
-  private final String field;
-  private final int hashCode;
-  private final BytesRef bytes = new BytesRef(32);
-  
-  private BinaryDocValues current;
-  
-  /**
-   * Constructs a new {@link DocValuesCategoryListIterator}.
-   */
-  public DocValuesCategoryListIterator(String field, IntDecoder decoder) {
-    this.field = field;
-    this.decoder = decoder;
-    this.hashCode = field.hashCode();
-  }
-  
-  @Override
-  public int hashCode() {
-    return hashCode;
-  }
-  
-  @Override
-  public boolean equals(Object o) {
-    if (!(o instanceof DocValuesCategoryListIterator)) {
-      return false;
-    }
-    DocValuesCategoryListIterator other = (DocValuesCategoryListIterator) o;
-    if (hashCode != other.hashCode) {
-      return false;
-    }
-    
-    // Hash codes are the same, check equals() to avoid cases of hash-collisions.
-    return field.equals(other.field);
-  }
-  
-  @Override
-  public boolean setNextReader(AtomicReaderContext context) throws IOException {
-    current = context.reader().getBinaryDocValues(field);
-    return current != null;
-  }
-  
-  @Override
-  public void getOrdinals(int docID, IntsRef ints) throws IOException {
-    assert current != null : "don't call this if setNextReader returned false";
-    current.get(docID, bytes);
-    ints.length = 0;
-    if (bytes.length > 0) {
-      decoder.decode(bytes, ints);
-    }
-  }
-  
-  @Override
-  public String toString() {
-    return field;
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/DrillDownQuery.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/DrillDownQuery.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/DrillDownQuery.java	2013-07-15 15:52:17.669877389 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/DrillDownQuery.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,222 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.FilteredQuery;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-
-/**
- * A {@link Query} for drill-down over {@link CategoryPath categories}. You
- * should call {@link #add(CategoryPath...)} for every group of categories you
- * want to drill-down over. Each category in the group is {@code OR'ed} with
- * the others, and groups are {@code AND'ed}.
- * <p>
- * <b>NOTE:</b> if you choose to create your own {@link Query} by calling
- * {@link #term}, it is recommended to wrap it with {@link ConstantScoreQuery}
- * and set the {@link ConstantScoreQuery#setBoost(float) boost} to {@code 0.0f},
- * so that it does not affect the scores of the documents.
- * 
- * @lucene.experimental
- */
-public final class DrillDownQuery extends Query {
-
-  /** Return a drill-down {@link Term} for a category. */
-  public static Term term(FacetIndexingParams iParams, CategoryPath path) {
-    CategoryListParams clp = iParams.getCategoryListParams(path);
-    char[] buffer = new char[path.fullPathLength()];
-    iParams.drillDownTermText(path, buffer);
-    return new Term(clp.field, String.valueOf(buffer));
-  }
-  
-  private final BooleanQuery query;
-  private final Map<String,Integer> drillDownDims = new LinkedHashMap<String,Integer>();
-  final FacetIndexingParams fip;
-
-  /** Used by clone() */
-  DrillDownQuery(FacetIndexingParams fip, BooleanQuery query, Map<String,Integer> drillDownDims) {
-    this.fip = fip;
-    this.query = query.clone();
-    this.drillDownDims.putAll(drillDownDims);
-  }
-
-  /** Used by DrillSideways */
-  DrillDownQuery(Filter filter, DrillDownQuery other) {
-    query = new BooleanQuery(true); // disable coord
-
-    BooleanClause[] clauses = other.query.getClauses();
-    if (clauses.length == other.drillDownDims.size()) {
-      throw new IllegalArgumentException("cannot apply filter unless baseQuery isn't null; pass ConstantScoreQuery instead");
-    }
-    assert clauses.length == 1+other.drillDownDims.size(): clauses.length + " vs " + (1+other.drillDownDims.size());
-    drillDownDims.putAll(other.drillDownDims);
-    query.add(new FilteredQuery(clauses[0].getQuery(), filter), Occur.MUST);
-    for(int i=1;i<clauses.length;i++) {
-      query.add(clauses[i].getQuery(), Occur.MUST);
-    }
-    fip = other.fip;
-  }
-
-  /** Used by DrillSideways */
-  DrillDownQuery(FacetIndexingParams fip, Query baseQuery, List<Query> clauses, Map<String,Integer> drillDownDims) {
-    this.fip = fip;
-    this.query = new BooleanQuery(true);
-    if (baseQuery != null) {
-      query.add(baseQuery, Occur.MUST);      
-    }
-    for(Query clause : clauses) {
-      query.add(clause, Occur.MUST);
-    }
-    this.drillDownDims.putAll(drillDownDims);
-  }
-
-  /**
-   * Creates a new {@link DrillDownQuery} without a base query, 
-   * to perform a pure browsing query (equivalent to using
-   * {@link MatchAllDocsQuery} as base).
-   */
-  public DrillDownQuery(FacetIndexingParams fip) {
-    this(fip, null);
-  }
-  
-  /**
-   * Creates a new {@link DrillDownQuery} over the given base query. Can be
-   * {@code null}, in which case the result {@link Query} from
-   * {@link #rewrite(IndexReader)} will be a pure browsing query, filtering on
-   * the added categories only.
-   */
-  public DrillDownQuery(FacetIndexingParams fip, Query baseQuery) {
-    query = new BooleanQuery(true); // disable coord
-    if (baseQuery != null) {
-      query.add(baseQuery, Occur.MUST);
-    }
-    this.fip = fip;
-  }
-
-  /**
-   * Adds one dimension of drill downs; if you pass multiple values they are
-   * OR'd, and then the entire dimension is AND'd against the base query.
-   */
-  public void add(CategoryPath... paths) {
-    Query q;
-    if (paths[0].length == 0) {
-      throw new IllegalArgumentException("all CategoryPaths must have length > 0");
-    }
-    String dim = paths[0].components[0];
-    if (drillDownDims.containsKey(dim)) {
-      throw new IllegalArgumentException("dimension '" + dim + "' was already added");
-    }
-    if (paths.length == 1) {
-      q = new TermQuery(term(fip, paths[0]));
-    } else {
-      BooleanQuery bq = new BooleanQuery(true); // disable coord
-      for (CategoryPath cp : paths) {
-        if (cp.length == 0) {
-          throw new IllegalArgumentException("all CategoryPaths must have length > 0");
-        }
-        if (!cp.components[0].equals(dim)) {
-          throw new IllegalArgumentException("multiple (OR'd) drill-down paths must be under same dimension; got '" 
-              + dim + "' and '" + cp.components[0] + "'");
-        }
-        bq.add(new TermQuery(term(fip, cp)), Occur.SHOULD);
-      }
-      q = bq;
-    }
-
-    add(dim, q);
-  }
-
-  /** Expert: add a custom drill-down subQuery.  Use this
-   *  when you have a separate way to drill-down on the
-   *  dimension than the indexed facet ordinals. */
-  public void add(String dim, Query subQuery) {
-
-    // TODO: we should use FilteredQuery?
-
-    // So scores of the drill-down query don't have an
-    // effect:
-    final ConstantScoreQuery drillDownQuery = new ConstantScoreQuery(subQuery);
-    drillDownQuery.setBoost(0.0f);
-
-    query.add(drillDownQuery, Occur.MUST);
-
-    drillDownDims.put(dim, drillDownDims.size());
-  }
-
-  @Override
-  public DrillDownQuery clone() {
-    return new DrillDownQuery(fip, query, drillDownDims);
-  }
-  
-  @Override
-  public int hashCode() {
-    final int prime = 31;
-    int result = super.hashCode();
-    return prime * result + query.hashCode();
-  }
-  
-  @Override
-  public boolean equals(Object obj) {
-    if (!(obj instanceof DrillDownQuery)) {
-      return false;
-    }
-    
-    DrillDownQuery other = (DrillDownQuery) obj;
-    return query.equals(other.query) && super.equals(other);
-  }
-  
-  @Override
-  public Query rewrite(IndexReader r) throws IOException {
-    if (query.clauses().size() == 0) {
-      // baseQuery given to the ctor was null + no drill-downs were added
-      // note that if only baseQuery was given to the ctor, but no drill-down terms
-      // is fine, since the rewritten query will be the original base query.
-      throw new IllegalStateException("no base query or drill-down categories given");
-    }
-    return query;
-  }
-
-  @Override
-  public String toString(String field) {
-    return query.toString(field);
-  }
-
-  BooleanQuery getBooleanQuery() {
-    return query;
-  }
-
-  Map<String,Integer> getDims() {
-    return drillDownDims;
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSidewaysCollector.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSidewaysCollector.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSidewaysCollector.java	2013-07-15 15:52:17.669877389 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSidewaysCollector.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,188 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.IdentityHashMap;
-import java.util.Map;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.Scorer.ChildScorer;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.search.Weight;
-
-/** Collector that scrutinizes each hit to determine if it
- *  passed all constraints (a true hit) or if it missed
- *  exactly one dimension (a near-miss, to count for
- *  drill-sideways counts on that dimension). */
-class DrillSidewaysCollector extends Collector {
-
-  private final Collector hitCollector;
-  private final Collector drillDownCollector;
-  private final Collector[] drillSidewaysCollectors;
-  private final Scorer[] subScorers;
-  private final int exactCount;
-
-  // Maps Weight to either -1 (mainQuery) or to integer
-  // index of the dims drillDown.  We needs this when
-  // visiting the child scorers to correlate back to the
-  // right scorers:
-  private final Map<Weight,Integer> weightToIndex = new IdentityHashMap<Weight,Integer>();
-
-  private Scorer mainScorer;
-
-  public DrillSidewaysCollector(Collector hitCollector, Collector drillDownCollector, Collector[] drillSidewaysCollectors,
-                                Map<String,Integer> dims) {
-    this.hitCollector = hitCollector;
-    this.drillDownCollector = drillDownCollector;
-    this.drillSidewaysCollectors = drillSidewaysCollectors;
-    subScorers = new Scorer[dims.size()];
-
-    if (dims.size() == 1) {
-      // When we have only one dim, we insert the
-      // MatchAllDocsQuery, bringing the clause count to
-      // 2:
-      exactCount = 2;
-    } else {
-      exactCount = dims.size();
-    }
-  }
-
-  @Override
-  public void collect(int doc) throws IOException {
-    //System.out.println("collect doc=" + doc + " main.freq=" + mainScorer.freq() + " main.doc=" + mainScorer.docID() + " exactCount=" + exactCount);
-      
-    if (mainScorer == null) {
-      // This segment did not have any docs with any
-      // drill-down field & value:
-      return;
-    }
-
-    if (mainScorer.freq() == exactCount) {
-      // All sub-clauses from the drill-down filters
-      // matched, so this is a "real" hit, so we first
-      // collect in both the hitCollector and the
-      // drillDown collector:
-      //System.out.println("  hit " + drillDownCollector);
-      hitCollector.collect(doc);
-      if (drillDownCollector != null) {
-        drillDownCollector.collect(doc);
-      }
-
-      // Also collect across all drill-sideways counts so
-      // we "merge in" drill-down counts for this
-      // dimension.
-      for(int i=0;i<subScorers.length;i++) {
-        // This cannot be null, because it was a hit,
-        // meaning all drill-down dims matched, so all
-        // dims must have non-null scorers:
-        assert subScorers[i] != null;
-        int subDoc = subScorers[i].docID();
-        assert subDoc == doc;
-        drillSidewaysCollectors[i].collect(doc);
-      }
-
-    } else {
-      boolean found = false;
-      for(int i=0;i<subScorers.length;i++) {
-        if (subScorers[i] == null) {
-          // This segment did not have any docs with this
-          // drill-down field & value:
-          drillSidewaysCollectors[i].collect(doc);
-          assert allMatchesFrom(i+1, doc);
-          found = true;
-          break;
-        }
-        int subDoc = subScorers[i].docID();
-        //System.out.println("  i=" + i + " sub: " + subDoc);
-        if (subDoc != doc) {
-          //System.out.println("  +ds[" + i + "]");
-          assert subDoc > doc: "subDoc=" + subDoc + " doc=" + doc;
-          drillSidewaysCollectors[i].collect(doc);
-          assert allMatchesFrom(i+1, doc);
-          found = true;
-          break;
-        }
-      }
-      assert found;
-    }
-  }
-
-  // Only used by assert:
-  private boolean allMatchesFrom(int startFrom, int doc) {
-    for(int i=startFrom;i<subScorers.length;i++) {
-      assert subScorers[i].docID() == doc;
-    }
-    return true;
-  }
-
-  @Override
-  public boolean acceptsDocsOutOfOrder() {
-    // We actually could accept docs out of order, but, we
-    // need to force BooleanScorer2 so that the
-    // sub-scorers are "on" each docID we are collecting:
-    return false;
-  }
-
-  @Override
-  public void setNextReader(AtomicReaderContext leaf) throws IOException {
-    //System.out.println("DS.setNextReader reader=" + leaf.reader());
-    hitCollector.setNextReader(leaf);
-    if (drillDownCollector != null) {
-      drillDownCollector.setNextReader(leaf);
-    }
-    for(Collector dsc : drillSidewaysCollectors) {
-      dsc.setNextReader(leaf);
-    }
-  }
-
-  void setWeight(Weight weight, int index) {
-    assert !weightToIndex.containsKey(weight);
-    weightToIndex.put(weight, index);
-  }
-
-  private void findScorers(Scorer scorer) {
-    Integer index = weightToIndex.get(scorer.getWeight());
-    if (index != null) {
-      if (index.intValue() == -1) {
-        mainScorer = scorer;
-      } else {
-        subScorers[index] = scorer;
-      }
-    }
-    for(ChildScorer child : scorer.getChildren()) {
-      findScorers(child.child);
-    }
-  }
-
-  @Override
-  public void setScorer(Scorer scorer) throws IOException {
-    mainScorer = null;
-    Arrays.fill(subScorers, null);
-    findScorers(scorer);
-    hitCollector.setScorer(scorer);
-    if (drillDownCollector != null) {
-      drillDownCollector.setScorer(scorer);
-    }
-    for(Collector dsc : drillSidewaysCollectors) {
-      dsc.setScorer(scorer);
-    }
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSideways.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSideways.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSideways.java	2013-07-29 13:55:02.625707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSideways.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,560 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashSet;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetFields;
-import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.FieldDoc;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.MultiCollector;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.search.TopFieldCollector;
-import org.apache.lucene.search.TopScoreDocCollector;
-import org.apache.lucene.search.Weight;
-
-/**     
- * Computes drill down and sideways counts for the provided
- * {@link DrillDownQuery}.  Drill sideways counts include
- * alternative values/aggregates for the drill-down
- * dimensions so that a dimension does not disappear after
- * the user drills down into it.
- *
- * <p> Use one of the static search
- * methods to do the search, and then get the hits and facet
- * results from the returned {@link DrillSidewaysResult}.
- *
- * <p><b>NOTE</b>: this allocates one {@link
- * FacetsCollector} for each drill-down, plus one.  If your
- * index has high number of facet labels then this will
- * multiply your memory usage.
- *
- * @lucene.experimental
- */
-
-public class DrillSideways {
-
-  protected final IndexSearcher searcher;
-  protected final TaxonomyReader taxoReader;
-  protected final SortedSetDocValuesReaderState state;
-  
-  /**
-   * Create a new {@code DrillSideways} instance, assuming the categories were
-   * indexed with {@link FacetFields}.
-   */
-  public DrillSideways(IndexSearcher searcher, TaxonomyReader taxoReader) {
-    this.searcher = searcher;
-    this.taxoReader = taxoReader;
-    this.state = null;
-  }
-  
-  /**
-   * Create a new {@code DrillSideways} instance, assuming the categories were
-   * indexed with {@link SortedSetDocValuesFacetFields}.
-   */
-  public DrillSideways(IndexSearcher searcher, SortedSetDocValuesReaderState state) {
-    this.searcher = searcher;
-    this.taxoReader = null;
-    this.state = state;
-  }
-
-  /** Moves any drill-downs that don't have a corresponding
-   *  facet request into the baseQuery.  This is unusual,
-   *  yet allowed, because typically the added drill-downs are because
-   *  the user has clicked on previously presented facets,
-   *  and those same facets would be computed this time
-   *  around. */
-  private static DrillDownQuery moveDrillDownOnlyClauses(DrillDownQuery in, FacetSearchParams fsp) {
-    Set<String> facetDims = new HashSet<String>();
-    for(FacetRequest fr : fsp.facetRequests) {
-      if (fr.categoryPath.length == 0) {
-        throw new IllegalArgumentException("all FacetRequests must have CategoryPath with length > 0");
-      }
-      facetDims.add(fr.categoryPath.components[0]);
-    }
-
-    BooleanClause[] clauses = in.getBooleanQuery().getClauses();
-    Map<String,Integer> drillDownDims = in.getDims();
-
-    String[] dimsByIndex = new String[drillDownDims.size()];
-    for(Map.Entry<String,Integer> ent : drillDownDims.entrySet()) {
-      dimsByIndex[ent.getValue()] = ent.getKey();
-    }
-
-    int startClause;
-    if (clauses.length == drillDownDims.size()) {
-      startClause = 0;
-    } else {
-      assert clauses.length == 1+drillDownDims.size();
-      startClause = 1;
-    }
-
-    // Break out drill-down clauses that have no
-    // corresponding facet request and move them inside the
-    // baseQuery:
-    List<Query> nonFacetClauses = new ArrayList<Query>();
-    List<Query> facetClauses = new ArrayList<Query>();
-    Map<String,Integer> dimToIndex = new LinkedHashMap<String,Integer>();
-    for(int i=startClause;i<clauses.length;i++) {
-      Query q = clauses[i].getQuery();
-      String dim = dimsByIndex[i-startClause];
-      if (!facetDims.contains(dim)) {
-        nonFacetClauses.add(q);
-      } else {
-        facetClauses.add(q);
-        dimToIndex.put(dim, dimToIndex.size());
-      }
-    }
-
-    if (!nonFacetClauses.isEmpty()) {
-      BooleanQuery newBaseQuery = new BooleanQuery(true);
-      if (startClause == 1) {
-        // Add original basaeQuery:
-        newBaseQuery.add(clauses[0].getQuery(), BooleanClause.Occur.MUST);
-      }
-      for(Query q : nonFacetClauses) {
-        newBaseQuery.add(q, BooleanClause.Occur.MUST);
-      }
-
-      return new DrillDownQuery(fsp.indexingParams, newBaseQuery, facetClauses, dimToIndex);
-    } else {
-      // No change:
-      return in;
-    }
-  }
-
-  /**
-   * Search, collecting hits with a {@link Collector}, and
-   * computing drill down and sideways counts.
-   */
-  @SuppressWarnings({"rawtypes","unchecked"})
-  public DrillSidewaysResult search(DrillDownQuery query,
-                                    Collector hitCollector, FacetSearchParams fsp) throws IOException {
-
-    if (query.fip != fsp.indexingParams) {
-      throw new IllegalArgumentException("DrillDownQuery's FacetIndexingParams should match FacetSearchParams'");
-    }
-
-    query = moveDrillDownOnlyClauses(query, fsp);
-
-    Map<String,Integer> drillDownDims = query.getDims();
-
-    if (drillDownDims.isEmpty()) {
-      // Just do ordinary search when there are no drill-downs:
-      FacetsCollector c = FacetsCollector.create(getDrillDownAccumulator(fsp));
-      searcher.search(query, MultiCollector.wrap(hitCollector, c));
-      return new DrillSidewaysResult(c.getFacetResults(), null);      
-    }
-
-    List<FacetRequest> ddRequests = new ArrayList<FacetRequest>();
-    for(FacetRequest fr : fsp.facetRequests) {
-      assert fr.categoryPath.length > 0;
-      if (!drillDownDims.containsKey(fr.categoryPath.components[0])) {
-        ddRequests.add(fr);
-      }
-    }
-    FacetSearchParams fsp2;
-    if (!ddRequests.isEmpty()) {
-      fsp2 = new FacetSearchParams(fsp.indexingParams, ddRequests);
-    } else {
-      fsp2 = null;
-    }
-
-    BooleanQuery ddq = query.getBooleanQuery();
-    BooleanClause[] clauses = ddq.getClauses();
-
-    Query baseQuery;
-    int startClause;
-    if (clauses.length == drillDownDims.size()) {
-      // TODO: we could optimize this pure-browse case by
-      // making a custom scorer instead:
-      baseQuery = new MatchAllDocsQuery();
-      startClause = 0;
-    } else {
-      assert clauses.length == 1+drillDownDims.size();
-      baseQuery = clauses[0].getQuery();
-      startClause = 1;
-    }
-
-    FacetsCollector drillDownCollector = fsp2 == null ? null : FacetsCollector.create(getDrillDownAccumulator(fsp2));
-
-    FacetsCollector[] drillSidewaysCollectors = new FacetsCollector[drillDownDims.size()];
-
-    int idx = 0;
-    for(String dim : drillDownDims.keySet()) {
-      List<FacetRequest> requests = new ArrayList<FacetRequest>();
-      for(FacetRequest fr : fsp.facetRequests) {
-        assert fr.categoryPath.length > 0;
-        if (fr.categoryPath.components[0].equals(dim)) {
-          requests.add(fr);
-        }
-      }
-      // We already moved all drill-downs that didn't have a
-      // FacetRequest, in moveDrillDownOnlyClauses above:
-      assert !requests.isEmpty();
-      drillSidewaysCollectors[idx++] = FacetsCollector.create(getDrillSidewaysAccumulator(dim, new FacetSearchParams(fsp.indexingParams, requests)));
-    }
-
-    boolean useCollectorMethod = scoreSubDocsAtOnce();
-
-    Term[][] drillDownTerms = null;
-
-    if (!useCollectorMethod) {
-      // Optimistic: assume subQueries of the DDQ are either
-      // TermQuery or BQ OR of TermQuery; if this is wrong
-      // then we detect it and fallback to the mome general
-      // but slower DrillSidewaysCollector:
-      drillDownTerms = new Term[clauses.length-startClause][];
-      for(int i=startClause;i<clauses.length;i++) {
-        Query q = clauses[i].getQuery();
-
-        // DrillDownQuery always wraps each subQuery in
-        // ConstantScoreQuery:
-        assert q instanceof ConstantScoreQuery;
-
-        q = ((ConstantScoreQuery) q).getQuery();
-
-        if (q instanceof TermQuery) {
-          drillDownTerms[i-startClause] = new Term[] {((TermQuery) q).getTerm()};
-        } else if (q instanceof BooleanQuery) {
-          BooleanQuery q2 = (BooleanQuery) q;
-          BooleanClause[] clauses2 = q2.getClauses();
-          drillDownTerms[i-startClause] = new Term[clauses2.length];
-          for(int j=0;j<clauses2.length;j++) {
-            if (clauses2[j].getQuery() instanceof TermQuery) {
-              drillDownTerms[i-startClause][j] = ((TermQuery) clauses2[j].getQuery()).getTerm();
-            } else {
-              useCollectorMethod = true;
-              break;
-            }
-          }
-        } else {
-          useCollectorMethod = true;
-        }
-      }
-    }
-
-    if (useCollectorMethod) {
-      // TODO: maybe we could push the "collector method"
-      // down into the optimized scorer to have a tighter
-      // integration ... and so TermQuery clauses could
-      // continue to run "optimized"
-      collectorMethod(query, baseQuery, startClause, hitCollector, drillDownCollector, drillSidewaysCollectors);
-    } else {
-      DrillSidewaysQuery dsq = new DrillSidewaysQuery(baseQuery, drillDownCollector, drillSidewaysCollectors, drillDownTerms);
-      searcher.search(dsq, hitCollector);
-    }
-
-    int numDims = drillDownDims.size();
-    List<FacetResult>[] drillSidewaysResults = new List[numDims];
-    List<FacetResult> drillDownResults = null;
-
-    List<FacetResult> mergedResults = new ArrayList<FacetResult>();
-    int[] requestUpto = new int[drillDownDims.size()];
-    int ddUpto = 0;
-    for(int i=0;i<fsp.facetRequests.size();i++) {
-      FacetRequest fr = fsp.facetRequests.get(i);
-      assert fr.categoryPath.length > 0;
-      Integer dimIndex = drillDownDims.get(fr.categoryPath.components[0]);
-      if (dimIndex == null) {
-        // Pure drill down dim (the current query didn't
-        // drill down on this dim):
-        if (drillDownResults == null) {
-          // Lazy init, in case all requests were against
-          // drill-sideways dims:
-          drillDownResults = drillDownCollector.getFacetResults();
-          //System.out.println("get DD results");
-        }
-        //System.out.println("add dd results " + i);
-        mergedResults.add(drillDownResults.get(ddUpto++));
-      } else {
-        // Drill sideways dim:
-        int dim = dimIndex.intValue();
-        List<FacetResult> sidewaysResult = drillSidewaysResults[dim];
-        if (sidewaysResult == null) {
-          // Lazy init, in case no facet request is against
-          // a given drill down dim:
-          sidewaysResult = drillSidewaysCollectors[dim].getFacetResults();
-          drillSidewaysResults[dim] = sidewaysResult;
-        }
-        mergedResults.add(sidewaysResult.get(requestUpto[dim]));
-        requestUpto[dim]++;
-      }
-    }
-
-    return new DrillSidewaysResult(mergedResults, null);
-  }
-
-  /** Uses the more general but slower method of sideways
-   *  counting. This method allows an arbitrary subQuery to
-   *  implement the drill down for a given dimension. */
-  private void collectorMethod(DrillDownQuery ddq, Query baseQuery, int startClause, Collector hitCollector, Collector drillDownCollector, Collector[] drillSidewaysCollectors) throws IOException {
-
-    BooleanClause[] clauses = ddq.getBooleanQuery().getClauses();
-
-    Map<String,Integer> drillDownDims = ddq.getDims();
-
-    BooleanQuery topQuery = new BooleanQuery(true);
-    final DrillSidewaysCollector collector = new DrillSidewaysCollector(hitCollector, drillDownCollector, drillSidewaysCollectors,
-                                                                        drillDownDims);
-
-    // TODO: if query is already a BQ we could copy that and
-    // add clauses to it, instead of doing BQ inside BQ
-    // (should be more efficient)?  Problem is this can
-    // affect scoring (coord) ... too bad we can't disable
-    // coord on a clause by clause basis:
-    topQuery.add(baseQuery, BooleanClause.Occur.MUST);
-
-    // NOTE: in theory we could just make a single BQ, with
-    // +query a b c minShouldMatch=2, but in this case,
-    // annoyingly, BS2 wraps a sub-scorer that always
-    // returns 2 as the .freq(), not how many of the
-    // SHOULD clauses matched:
-    BooleanQuery subQuery = new BooleanQuery(true);
-
-    Query wrappedSubQuery = new QueryWrapper(subQuery,
-                                             new SetWeight() {
-                                               @Override
-                                               public void set(Weight w) {
-                                                 collector.setWeight(w, -1);
-                                               }
-                                             });
-    Query constantScoreSubQuery = new ConstantScoreQuery(wrappedSubQuery);
-
-    // Don't impact score of original query:
-    constantScoreSubQuery.setBoost(0.0f);
-
-    topQuery.add(constantScoreSubQuery, BooleanClause.Occur.MUST);
-
-    // Unfortunately this sub-BooleanQuery
-    // will never get BS1 because today BS1 only works
-    // if topScorer=true... and actually we cannot use BS1
-    // anyways because we need subDocsScoredAtOnce:
-    int dimIndex = 0;
-    for(int i=startClause;i<clauses.length;i++) {
-      Query q = clauses[i].getQuery();
-      // DrillDownQuery always wraps each subQuery in
-      // ConstantScoreQuery:
-      assert q instanceof ConstantScoreQuery;
-      q = ((ConstantScoreQuery) q).getQuery();
-
-      final int finalDimIndex = dimIndex;
-      subQuery.add(new QueryWrapper(q,
-                                    new SetWeight() {
-                                      @Override
-                                      public void set(Weight w) {
-                                        collector.setWeight(w, finalDimIndex);
-                                      }
-                                    }),
-                   BooleanClause.Occur.SHOULD);
-      dimIndex++;
-    }
-
-    // TODO: we could better optimize the "just one drill
-    // down" case w/ a separate [specialized]
-    // collector...
-    int minShouldMatch = drillDownDims.size()-1;
-    if (minShouldMatch == 0) {
-      // Must add another "fake" clause so BQ doesn't erase
-      // itself by rewriting to the single clause:
-      Query end = new MatchAllDocsQuery();
-      end.setBoost(0.0f);
-      subQuery.add(end, BooleanClause.Occur.SHOULD);
-      minShouldMatch++;
-    }
-
-    subQuery.setMinimumNumberShouldMatch(minShouldMatch);
-
-    // System.out.println("EXE " + topQuery);
-
-    // Collects against the passed-in
-    // drillDown/SidewaysCollectors as a side effect:
-    searcher.search(topQuery, collector);
-  }
-
-  /**
-   * Search, sorting by {@link Sort}, and computing
-   * drill down and sideways counts.
-   */
-  public DrillSidewaysResult search(DrillDownQuery query,
-                                    Filter filter, FieldDoc after, int topN, Sort sort, boolean doDocScores,
-                                    boolean doMaxScore, FacetSearchParams fsp) throws IOException {
-    if (filter != null) {
-      query = new DrillDownQuery(filter, query);
-    }
-    if (sort != null) {
-      int limit = searcher.getIndexReader().maxDoc();
-      if (limit == 0) {
-        limit = 1; // the collector does not alow numHits = 0
-      }
-      topN = Math.min(topN, limit);
-      final TopFieldCollector hitCollector = TopFieldCollector.create(sort,
-                                                                      topN,
-                                                                      after,
-                                                                      true,
-                                                                      doDocScores,
-                                                                      doMaxScore,
-                                                                      true);
-      DrillSidewaysResult r = search(query, hitCollector, fsp);
-      return new DrillSidewaysResult(r.facetResults, hitCollector.topDocs());
-    } else {
-      return search(after, query, topN, fsp);
-    }
-  }
-
-  /**
-   * Search, sorting by score, and computing
-   * drill down and sideways counts.
-   */
-  public DrillSidewaysResult search(ScoreDoc after,
-                                    DrillDownQuery query, int topN, FacetSearchParams fsp) throws IOException {
-    int limit = searcher.getIndexReader().maxDoc();
-    if (limit == 0) {
-      limit = 1; // the collector does not alow numHits = 0
-    }
-    topN = Math.min(topN, limit);
-    TopScoreDocCollector hitCollector = TopScoreDocCollector.create(topN, after, true);
-    DrillSidewaysResult r = search(query, hitCollector, fsp);
-    return new DrillSidewaysResult(r.facetResults, hitCollector.topDocs());
-  }
-
-  /** Override this to use a custom drill-down {@link
-   *  FacetsAccumulator}. */
-  protected FacetsAccumulator getDrillDownAccumulator(FacetSearchParams fsp) throws IOException {
-    if (taxoReader != null) {
-      return FacetsAccumulator.create(fsp, searcher.getIndexReader(), taxoReader, null);
-    } else {
-      return FacetsAccumulator.create(fsp, state, null);
-    }
-  }
-
-  /** Override this to use a custom drill-sideways {@link
-   *  FacetsAccumulator}. */
-  protected FacetsAccumulator getDrillSidewaysAccumulator(String dim, FacetSearchParams fsp) throws IOException {
-    if (taxoReader != null) {
-      return FacetsAccumulator.create(fsp, searcher.getIndexReader(), taxoReader, null);
-    } else {
-      return FacetsAccumulator.create(fsp, state, null);
-    }
-  }
-
-  /** Override this and return true if your collector
-   *  (e.g., ToParentBlockJoinCollector) expects all
-   *  sub-scorers to be positioned on the document being
-   *  collected.  This will cause some performance loss;
-   *  default is false.  Note that if you return true from
-   *  this method (in a subclass) be sure your collector
-   *  also returns false from {@link
-   *  Collector#acceptsDocsOutOfOrder}: this will trick
-   *  BooleanQuery into also scoring all subDocs at once. */
-  protected boolean scoreSubDocsAtOnce() {
-    return false;
-  }
-
-  /**
-   * Represents the returned result from a drill sideways search. Note that if
-   * you called
-   * {@link DrillSideways#search(DrillDownQuery, Collector, FacetSearchParams)},
-   * then {@link #hits} will be {@code null}.
-   */
-  public static class DrillSidewaysResult {
-    /** Combined drill down & sideways results. */
-    public final List<FacetResult> facetResults;
-
-    /** Hits. */
-    public final TopDocs hits;
-
-    public DrillSidewaysResult(List<FacetResult> facetResults, TopDocs hits) {
-      this.facetResults = facetResults;
-      this.hits = hits;
-    }
-  }
-
-  private interface SetWeight {
-    public void set(Weight w);
-  }
-
-  /** Just records which Weight was given out for the
-   *  (possibly rewritten) Query. */
-  private static class QueryWrapper extends Query {
-    private final Query originalQuery;
-    private final SetWeight setter;
-
-    public QueryWrapper(Query originalQuery, SetWeight setter) {
-      this.originalQuery = originalQuery;
-      this.setter = setter;
-    }
-
-    @Override
-    public Weight createWeight(final IndexSearcher searcher) throws IOException {
-      Weight w = originalQuery.createWeight(searcher);
-      setter.set(w);
-      return w;
-    }
-
-    @Override
-    public Query rewrite(IndexReader reader) throws IOException {
-      Query rewritten = originalQuery.rewrite(reader);
-      if (rewritten != originalQuery) {
-        return new QueryWrapper(rewritten, setter);
-      } else {
-        return this;
-      }
-    }
-
-    @Override
-    public String toString(String s) {
-      return originalQuery.toString(s);
-    }
-
-    @Override
-    public boolean equals(Object o) {
-      if (!(o instanceof QueryWrapper)) return false;
-      final QueryWrapper other = (QueryWrapper) o;
-      return super.equals(o) && originalQuery.equals(other.originalQuery);
-    }
-
-    @Override
-    public int hashCode() {
-      return super.hashCode() * 31 + originalQuery.hashCode();
-    }
-  }
-}
-


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSidewaysQuery.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSidewaysQuery.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSidewaysQuery.java	2013-07-23 16:25:55.719333095 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSidewaysQuery.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,195 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.Explanation;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.search.Weight;
-import org.apache.lucene.util.Bits;
-
-class DrillSidewaysQuery extends Query {
-  final Query baseQuery;
-  final Collector drillDownCollector;
-  final Collector[] drillSidewaysCollectors;
-  final Term[][] drillDownTerms;
-
-  DrillSidewaysQuery(Query baseQuery, Collector drillDownCollector, Collector[] drillSidewaysCollectors, Term[][] drillDownTerms) {
-    this.baseQuery = baseQuery;
-    this.drillDownCollector = drillDownCollector;
-    this.drillSidewaysCollectors = drillSidewaysCollectors;
-    this.drillDownTerms = drillDownTerms;
-  }
-
-  @Override
-  public String toString(String field) {
-    return "DrillSidewaysQuery";
-  }
-
-  @Override
-  public Query rewrite(IndexReader reader) throws IOException {
-    Query newQuery = baseQuery;
-    while(true) {
-      Query rewrittenQuery = newQuery.rewrite(reader);
-      if (rewrittenQuery == newQuery) {
-        break;
-      }
-      newQuery = rewrittenQuery;
-    }
-    if (newQuery == baseQuery) {
-      return this;
-    } else {
-      return new DrillSidewaysQuery(newQuery, drillDownCollector, drillSidewaysCollectors, drillDownTerms);
-    }
-  }
-  
-  @Override
-  public Weight createWeight(IndexSearcher searcher) throws IOException {
-    final Weight baseWeight = baseQuery.createWeight(searcher);
-
-    return new Weight() {
-      @Override
-      public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
-        return baseWeight.explain(context, doc);
-      }
-
-      @Override
-      public Query getQuery() {
-        return baseQuery;
-      }
-
-      @Override
-      public float getValueForNormalization() throws IOException {
-        return baseWeight.getValueForNormalization();
-      }
-
-      @Override
-      public void normalize(float norm, float topLevelBoost) {
-        baseWeight.normalize(norm, topLevelBoost);
-      }
-
-      @Override
-      public boolean scoresDocsOutOfOrder() {
-        // TODO: would be nice if AssertingIndexSearcher
-        // confirmed this for us
-        return false;
-      }
-
-      @Override
-      public Scorer scorer(AtomicReaderContext context, boolean scoreDocsInOrder,
-                           boolean topScorer, Bits acceptDocs) throws IOException {
-
-        DrillSidewaysScorer.DocsEnumsAndFreq[] dims = new DrillSidewaysScorer.DocsEnumsAndFreq[drillDownTerms.length];
-        TermsEnum termsEnum = null;
-        String lastField = null;
-        int nullCount = 0;
-        for(int dim=0;dim<dims.length;dim++) {
-          dims[dim] = new DrillSidewaysScorer.DocsEnumsAndFreq();
-          dims[dim].sidewaysCollector = drillSidewaysCollectors[dim];
-          String field = drillDownTerms[dim][0].field();
-          dims[dim].dim = drillDownTerms[dim][0].text();
-          if (lastField == null || !lastField.equals(field)) {
-            AtomicReader reader = context.reader();
-            Terms terms = reader.terms(field);
-            if (terms != null) {
-              termsEnum = terms.iterator(null);
-            } else {
-              termsEnum = null;
-            }
-            lastField = field;
-          }
-          dims[dim].docsEnums = new DocsEnum[drillDownTerms[dim].length];
-          if (termsEnum == null) {
-            nullCount++;
-            continue;
-          }
-          for(int i=0;i<drillDownTerms[dim].length;i++) {
-            if (termsEnum.seekExact(drillDownTerms[dim][i].bytes())) {
-              DocsEnum docsEnum = termsEnum.docs(null, null, 0);
-              if (docsEnum != null) {
-                dims[dim].docsEnums[i] = docsEnum;
-                dims[dim].maxCost = Math.max(dims[dim].maxCost, docsEnum.cost());
-              }
-            }
-          }
-        }
-
-        if (nullCount > 1 || (nullCount == 1 && dims.length == 1)) {
-          return null;
-        }
-
-        // Sort drill-downs by most restrictive first:
-        Arrays.sort(dims);
-
-        // TODO: it could be better if we take acceptDocs
-        // into account instead of baseScorer?
-        Scorer baseScorer = baseWeight.scorer(context, scoreDocsInOrder, false, acceptDocs);
-
-        if (baseScorer == null) {
-          return null;
-        }
-
-        return new DrillSidewaysScorer(this, context,
-                                       baseScorer,
-                                       drillDownCollector, dims);
-      }
-    };
-  }
-
-  // TODO: these should do "deeper" equals/hash on the 2-D drillDownTerms array
-
-  @Override
-  public int hashCode() {
-    final int prime = 31;
-    int result = super.hashCode();
-    result = prime * result + ((baseQuery == null) ? 0 : baseQuery.hashCode());
-    result = prime * result
-        + ((drillDownCollector == null) ? 0 : drillDownCollector.hashCode());
-    result = prime * result + Arrays.hashCode(drillDownTerms);
-    result = prime * result + Arrays.hashCode(drillSidewaysCollectors);
-    return result;
-  }
-
-  @Override
-  public boolean equals(Object obj) {
-    if (this == obj) return true;
-    if (!super.equals(obj)) return false;
-    if (getClass() != obj.getClass()) return false;
-    DrillSidewaysQuery other = (DrillSidewaysQuery) obj;
-    if (baseQuery == null) {
-      if (other.baseQuery != null) return false;
-    } else if (!baseQuery.equals(other.baseQuery)) return false;
-    if (drillDownCollector == null) {
-      if (other.drillDownCollector != null) return false;
-    } else if (!drillDownCollector.equals(other.drillDownCollector)) return false;
-    if (!Arrays.equals(drillDownTerms, other.drillDownTerms)) return false;
-    if (!Arrays.equals(drillSidewaysCollectors, other.drillSidewaysCollectors)) return false;
-    return true;
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSidewaysScorer.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSidewaysScorer.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSidewaysScorer.java	2013-07-15 15:52:17.669877389 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSidewaysScorer.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,654 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Collections;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.search.Weight;
-import org.apache.lucene.util.FixedBitSet;
-
-class DrillSidewaysScorer extends Scorer {
-
-  //private static boolean DEBUG = false;
-
-  private final Collector drillDownCollector;
-
-  private final DocsEnumsAndFreq[] dims;
-
-  // DrillDown DocsEnums:
-  private final Scorer baseScorer;
-
-  private final AtomicReaderContext context;
-
-  private static final int CHUNK = 2048;
-  private static final int MASK = CHUNK-1;
-
-  private int collectDocID = -1;
-  private float collectScore;
-
-  DrillSidewaysScorer(Weight w, AtomicReaderContext context, Scorer baseScorer, Collector drillDownCollector,
-                      DocsEnumsAndFreq[] dims) {
-    super(w);
-    this.dims = dims;
-    this.context = context;
-    this.baseScorer = baseScorer;
-    this.drillDownCollector = drillDownCollector;
-  }
-
-  @Override
-  public void score(Collector collector) throws IOException {
-    //if (DEBUG) {
-    //  System.out.println("\nscore: reader=" + context.reader());
-    //}
-    //System.out.println("score r=" + context.reader());
-    collector.setScorer(this);
-    if (drillDownCollector != null) {
-      drillDownCollector.setScorer(this);
-      drillDownCollector.setNextReader(context);
-    }
-    for(DocsEnumsAndFreq dim : dims) {
-      dim.sidewaysCollector.setScorer(this);
-      dim.sidewaysCollector.setNextReader(context);
-    }
-
-    // TODO: if we ever allow null baseScorer ... it will
-    // mean we DO score docs out of order ... hmm, or if we
-    // change up the order of the conjuntions below
-    assert baseScorer != null;
-
-    // Position all scorers to their first matching doc:
-    baseScorer.nextDoc();
-    for(DocsEnumsAndFreq dim : dims) {
-      for (DocsEnum docsEnum : dim.docsEnums) {
-        if (docsEnum != null) {
-          docsEnum.nextDoc();
-        }
-      }
-    }
-
-    final int numDims = dims.length;
-
-    DocsEnum[][] docsEnums = new DocsEnum[numDims][];
-    Collector[] sidewaysCollectors = new Collector[numDims];
-    long drillDownCost = 0;
-    for(int dim=0;dim<numDims;dim++) {
-      docsEnums[dim] = dims[dim].docsEnums;
-      sidewaysCollectors[dim] = dims[dim].sidewaysCollector;
-      for (DocsEnum de : dims[dim].docsEnums) {
-        if (de != null) {
-          drillDownCost += de.cost();
-        }
-      }
-    }
-
-    long baseQueryCost = baseScorer.cost();
-
-    /*
-    System.out.println("\nbaseDocID=" + baseScorer.docID() + " est=" + estBaseHitCount);
-    System.out.println("  maxDoc=" + context.reader().maxDoc());
-    System.out.println("  maxCost=" + maxCost);
-    System.out.println("  dims[0].freq=" + dims[0].freq);
-    if (numDims > 1) {
-      System.out.println("  dims[1].freq=" + dims[1].freq);
-    }
-    */
-
-    if (baseQueryCost < drillDownCost/10) {
-      //System.out.println("baseAdvance");
-      doBaseAdvanceScoring(collector, docsEnums, sidewaysCollectors);
-    } else if (numDims > 1 && (dims[1].maxCost < baseQueryCost/10)) {
-      //System.out.println("drillDownAdvance");
-      doDrillDownAdvanceScoring(collector, docsEnums, sidewaysCollectors);
-    } else {
-      //System.out.println("union");
-      doUnionScoring(collector, docsEnums, sidewaysCollectors);
-    }
-  }
-
-  /** Used when drill downs are highly constraining vs
-   *  baseQuery. */
-  private void doDrillDownAdvanceScoring(Collector collector, DocsEnum[][] docsEnums, Collector[] sidewaysCollectors) throws IOException {
-    final int maxDoc = context.reader().maxDoc();
-    final int numDims = dims.length;
-
-    //if (DEBUG) {
-    //  System.out.println("  doDrillDownAdvanceScoring");
-    //}
-
-    // TODO: maybe a class like BS, instead of parallel arrays
-    int[] filledSlots = new int[CHUNK];
-    int[] docIDs = new int[CHUNK];
-    float[] scores = new float[CHUNK];
-    int[] missingDims = new int[CHUNK];
-    int[] counts = new int[CHUNK];
-
-    docIDs[0] = -1;
-    int nextChunkStart = CHUNK;
-
-    final FixedBitSet seen = new FixedBitSet(CHUNK);
-
-    while (true) {
-      //if (DEBUG) {
-      //  System.out.println("\ncycle nextChunkStart=" + nextChunkStart + " docIds[0]=" + docIDs[0]);
-      //}
-
-      // First dim:
-      //if (DEBUG) {
-      //  System.out.println("  dim0");
-      //}
-      for(DocsEnum docsEnum : docsEnums[0]) {
-        if (docsEnum == null) {
-          continue;
-        }
-        int docID = docsEnum.docID();
-        while (docID < nextChunkStart) {
-          int slot = docID & MASK;
-
-          if (docIDs[slot] != docID) {
-            seen.set(slot);
-            // Mark slot as valid:
-            //if (DEBUG) {
-            //  System.out.println("    set docID=" + docID + " id=" + context.reader().document(docID).get("id"));
-            //}
-            docIDs[slot] = docID;
-            missingDims[slot] = 1;
-            counts[slot] = 1;
-          }
-
-          docID = docsEnum.nextDoc();
-        }
-      }
-
-      // Second dim:
-      //if (DEBUG) {
-      //  System.out.println("  dim1");
-      //}
-      for(DocsEnum docsEnum : docsEnums[1]) {
-        if (docsEnum == null) {
-          continue;
-        }
-        int docID = docsEnum.docID();
-        while (docID < nextChunkStart) {
-          int slot = docID & MASK;
-
-          if (docIDs[slot] != docID) {
-            // Mark slot as valid:
-            seen.set(slot);
-            //if (DEBUG) {
-            //  System.out.println("    set docID=" + docID + " missingDim=0 id=" + context.reader().document(docID).get("id"));
-            //}
-            docIDs[slot] = docID;
-            missingDims[slot] = 0;
-            counts[slot] = 1;
-          } else {
-            // TODO: single-valued dims will always be true
-            // below; we could somehow specialize
-            if (missingDims[slot] >= 1) {
-              missingDims[slot] = 2;
-              counts[slot] = 2;
-              //if (DEBUG) {
-              //  System.out.println("    set docID=" + docID + " missingDim=2 id=" + context.reader().document(docID).get("id"));
-              //}
-            } else {
-              counts[slot] = 1;
-              //if (DEBUG) {
-              //  System.out.println("    set docID=" + docID + " missingDim=" + missingDims[slot] + " id=" + context.reader().document(docID).get("id"));
-              //}
-            }
-          }
-
-          docID = docsEnum.nextDoc();
-        }
-      }
-
-      // After this we can "upgrade" to conjunction, because
-      // any doc not seen by either dim 0 or dim 1 cannot be
-      // a hit or a near miss:
-
-      //if (DEBUG) {
-      //  System.out.println("  baseScorer");
-      //}
-
-      // Fold in baseScorer, using advance:
-      int filledCount = 0;
-      int slot0 = 0;
-      while (slot0 < CHUNK && (slot0 = seen.nextSetBit(slot0)) != -1) {
-        int ddDocID = docIDs[slot0];
-        assert ddDocID != -1;
-
-        int baseDocID = baseScorer.docID();
-        if (baseDocID < ddDocID) {
-          baseDocID = baseScorer.advance(ddDocID);
-        }
-        if (baseDocID == ddDocID) {
-          //if (DEBUG) {
-          //  System.out.println("    keep docID=" + ddDocID + " id=" + context.reader().document(ddDocID).get("id"));
-          //}
-          scores[slot0] = baseScorer.score();
-          filledSlots[filledCount++] = slot0;
-          counts[slot0]++;
-        } else {
-          //if (DEBUG) {
-          //  System.out.println("    no docID=" + ddDocID + " id=" + context.reader().document(ddDocID).get("id"));
-          //}
-          docIDs[slot0] = -1;
-
-          // TODO: we could jump slot0 forward to the
-          // baseDocID ... but we'd need to set docIDs for
-          // intervening slots to -1
-        }
-        slot0++;
-      }
-      seen.clear(0, CHUNK);
-
-      if (filledCount == 0) {
-        if (nextChunkStart >= maxDoc) {
-          break;
-        }
-        nextChunkStart += CHUNK;
-        continue;
-      }
-      
-      // TODO: factor this out & share w/ union scorer,
-      // except we start from dim=2 instead:
-      for(int dim=2;dim<numDims;dim++) {
-        //if (DEBUG) {
-        //  System.out.println("  dim=" + dim + " [" + dims[dim].dim + "]");
-        //}
-        for(DocsEnum docsEnum : docsEnums[dim]) {
-          if (docsEnum == null) {
-            continue;
-          }
-          int docID = docsEnum.docID();
-          while (docID < nextChunkStart) {
-            int slot = docID & MASK;
-            if (docIDs[slot] == docID && counts[slot] >= dim) {
-              // TODO: single-valued dims will always be true
-              // below; we could somehow specialize
-              if (missingDims[slot] >= dim) {
-                //if (DEBUG) {
-                //  System.out.println("    set docID=" + docID + " count=" + (dim+2));
-                //}
-                missingDims[slot] = dim+1;
-                counts[slot] = dim+2;
-              } else {
-                //if (DEBUG) {
-                //  System.out.println("    set docID=" + docID + " missing count=" + (dim+1));
-                //}
-                counts[slot] = dim+1;
-              }
-            }
-            // TODO: sometimes use advance?
-            docID = docsEnum.nextDoc();
-          }
-        }
-      }
-
-      // Collect:
-      //if (DEBUG) {
-      //  System.out.println("  now collect: " + filledCount + " hits");
-      //}
-      for(int i=0;i<filledCount;i++) {
-        int slot = filledSlots[i];
-        collectDocID = docIDs[slot];
-        collectScore = scores[slot];
-        //if (DEBUG) {
-        //  System.out.println("    docID=" + docIDs[slot] + " count=" + counts[slot]);
-        //}
-        if (counts[slot] == 1+numDims) {
-          collectHit(collector, sidewaysCollectors);
-        } else if (counts[slot] == numDims) {
-          collectNearMiss(sidewaysCollectors, missingDims[slot]);
-        }
-      }
-
-      if (nextChunkStart >= maxDoc) {
-        break;
-      }
-
-      nextChunkStart += CHUNK;
-    }
-  }
-
-  /** Used when base query is highly constraining vs the
-   *  drilldowns; in this case we just .next() on base and
-   *  .advance() on the dims. */
-  private void doBaseAdvanceScoring(Collector collector, DocsEnum[][] docsEnums, Collector[] sidewaysCollectors) throws IOException {
-    //if (DEBUG) {
-    //  System.out.println("  doBaseAdvanceScoring");
-    //}
-    int docID = baseScorer.docID();
-
-    final int numDims = dims.length;
-
-    nextDoc: while (docID != NO_MORE_DOCS) {
-      int failedDim = -1;
-      for(int dim=0;dim<numDims;dim++) {
-        // TODO: should we sort this 2nd dimension of
-        // docsEnums from most frequent to least?
-        boolean found = false;
-        for(DocsEnum docsEnum : docsEnums[dim]) {
-          if (docsEnum == null) {
-            continue;
-          }
-          if (docsEnum.docID() < docID) {
-            docsEnum.advance(docID);
-          }
-          if (docsEnum.docID() == docID) {
-            found = true;
-            break;
-          }
-        }
-        if (!found) {
-          if (failedDim != -1) {
-            // More than one dim fails on this document, so
-            // it's neither a hit nor a near-miss; move to
-            // next doc:
-            docID = baseScorer.nextDoc();
-            continue nextDoc;
-          } else {
-            failedDim = dim;
-          }
-        }
-      }
-
-      collectDocID = docID;
-
-      // TODO: we could score on demand instead since we are
-      // daat here:
-      collectScore = baseScorer.score();
-
-      if (failedDim == -1) {
-        collectHit(collector, sidewaysCollectors);
-      } else {
-        collectNearMiss(sidewaysCollectors, failedDim);
-      }
-
-      docID = baseScorer.nextDoc();
-    }
-  }
-
-  private void collectHit(Collector collector, Collector[] sidewaysCollectors) throws IOException {
-    //if (DEBUG) {
-    //  System.out.println("      hit");
-    //}
-
-    collector.collect(collectDocID);
-    if (drillDownCollector != null) {
-      drillDownCollector.collect(collectDocID);
-    }
-
-    // TODO: we could "fix" faceting of the sideways counts
-    // to do this "union" (of the drill down hits) in the
-    // end instead:
-
-    // Tally sideways counts:
-    for(int dim=0;dim<sidewaysCollectors.length;dim++) {
-      sidewaysCollectors[dim].collect(collectDocID);
-    }
-  }
-
-  private void collectNearMiss(Collector[] sidewaysCollectors, int dim) throws IOException {
-    //if (DEBUG) {
-    //  System.out.println("      missingDim=" + dim);
-    //}
-    sidewaysCollectors[dim].collect(collectDocID);
-  }
-
-  private void doUnionScoring(Collector collector, DocsEnum[][] docsEnums, Collector[] sidewaysCollectors) throws IOException {
-    //if (DEBUG) {
-    //  System.out.println("  doUnionScoring");
-    //}
-
-    final int maxDoc = context.reader().maxDoc();
-    final int numDims = dims.length;
-
-    // TODO: maybe a class like BS, instead of parallel arrays
-    int[] filledSlots = new int[CHUNK];
-    int[] docIDs = new int[CHUNK];
-    float[] scores = new float[CHUNK];
-    int[] missingDims = new int[CHUNK];
-    int[] counts = new int[CHUNK];
-
-    docIDs[0] = -1;
-
-    // NOTE: this is basically a specialized version of
-    // BooleanScorer, to the minShouldMatch=N-1 case, but
-    // carefully tracking which dimension failed to match
-
-    int nextChunkStart = CHUNK;
-
-    while (true) {
-      //if (DEBUG) {
-      //  System.out.println("\ncycle nextChunkStart=" + nextChunkStart + " docIds[0]=" + docIDs[0]);
-      //}
-      int filledCount = 0;
-      int docID = baseScorer.docID();
-      //if (DEBUG) {
-      //  System.out.println("  base docID=" + docID);
-      //}
-      while (docID < nextChunkStart) {
-        int slot = docID & MASK;
-        //if (DEBUG) {
-        //  System.out.println("    docIDs[slot=" + slot + "]=" + docID + " id=" + context.reader().document(docID).get("id"));
-        //}
-
-        // Mark slot as valid:
-        assert docIDs[slot] != docID: "slot=" + slot + " docID=" + docID;
-        docIDs[slot] = docID;
-        scores[slot] = baseScorer.score();
-        filledSlots[filledCount++] = slot;
-        missingDims[slot] = 0;
-        counts[slot] = 1;
-
-        docID = baseScorer.nextDoc();
-      }
-
-      if (filledCount == 0) {
-        if (nextChunkStart >= maxDoc) {
-          break;
-        }
-        nextChunkStart += CHUNK;
-        continue;
-      }
-
-      // First drill-down dim, basically adds SHOULD onto
-      // the baseQuery:
-      //if (DEBUG) {
-      //  System.out.println("  dim=0 [" + dims[0].dim + "]");
-      //}
-      for(DocsEnum docsEnum : docsEnums[0]) {
-        if (docsEnum == null) {
-          continue;
-        }
-        docID = docsEnum.docID();
-        //if (DEBUG) {
-        //  System.out.println("    start docID=" + docID);
-        //}
-        while (docID < nextChunkStart) {
-          int slot = docID & MASK;
-          if (docIDs[slot] == docID) {
-            //if (DEBUG) {
-            //  System.out.println("      set docID=" + docID + " count=2");
-            //}
-            missingDims[slot] = 1;
-            counts[slot] = 2;
-          }
-          docID = docsEnum.nextDoc();
-        }
-      }
-
-      for(int dim=1;dim<numDims;dim++) {
-        //if (DEBUG) {
-        //  System.out.println("  dim=" + dim + " [" + dims[dim].dim + "]");
-        //}
-        for(DocsEnum docsEnum : docsEnums[dim]) {
-          if (docsEnum == null) {
-            continue;
-          }
-          docID = docsEnum.docID();
-          //if (DEBUG) {
-          //  System.out.println("    start docID=" + docID);
-          //}
-          while (docID < nextChunkStart) {
-            int slot = docID & MASK;
-            if (docIDs[slot] == docID && counts[slot] >= dim) {
-              // This doc is still in the running...
-              // TODO: single-valued dims will always be true
-              // below; we could somehow specialize
-              if (missingDims[slot] >= dim) {
-                //if (DEBUG) {
-                //  System.out.println("      set docID=" + docID + " count=" + (dim+2));
-                //}
-                missingDims[slot] = dim+1;
-                counts[slot] = dim+2;
-              } else {
-                //if (DEBUG) {
-                //  System.out.println("      set docID=" + docID + " missing count=" + (dim+1));
-                //}
-                counts[slot] = dim+1;
-              }
-            }
-            docID = docsEnum.nextDoc();
-          }
-
-          // TODO: sometimes use advance?
-
-          /*
-            int docBase = nextChunkStart - CHUNK;
-            for(int i=0;i<filledCount;i++) {
-              int slot = filledSlots[i];
-              docID = docBase + filledSlots[i];
-              if (docIDs[slot] == docID && counts[slot] >= dim) {
-                // This doc is still in the running...
-                int ddDocID = docsEnum.docID();
-                if (ddDocID < docID) {
-                  ddDocID = docsEnum.advance(docID);
-                }
-                if (ddDocID == docID) {
-                  if (missingDims[slot] >= dim && counts[slot] == allMatchCount) {
-                  //if (DEBUG) {
-                  //    System.out.println("    set docID=" + docID + " count=" + (dim+2));
-                   // }
-                    missingDims[slot] = dim+1;
-                    counts[slot] = dim+2;
-                  } else {
-                  //if (DEBUG) {
-                  //    System.out.println("    set docID=" + docID + " missing count=" + (dim+1));
-                   // }
-                    counts[slot] = dim+1;
-                  }
-                }
-              }
-            }            
-          */
-        }
-      }
-
-      // Collect:
-      //if (DEBUG) {
-      //  System.out.println("  now collect: " + filledCount + " hits");
-      //}
-      for(int i=0;i<filledCount;i++) {
-        // NOTE: This is actually in-order collection,
-        // because we only accept docs originally returned by
-        // the baseScorer (ie that Scorer is AND'd)
-        int slot = filledSlots[i];
-        collectDocID = docIDs[slot];
-        collectScore = scores[slot];
-        //if (DEBUG) {
-        //  System.out.println("    docID=" + docIDs[slot] + " count=" + counts[slot]);
-        //}
-        //System.out.println("  collect doc=" + collectDocID + " main.freq=" + (counts[slot]-1) + " main.doc=" + collectDocID + " exactCount=" + numDims);
-        if (counts[slot] == 1+numDims) {
-          //System.out.println("    hit");
-          collectHit(collector, sidewaysCollectors);
-        } else if (counts[slot] == numDims) {
-          //System.out.println("    sw");
-          collectNearMiss(sidewaysCollectors, missingDims[slot]);
-        }
-      }
-
-      if (nextChunkStart >= maxDoc) {
-        break;
-      }
-
-      nextChunkStart += CHUNK;
-    }
-  }
-
-  @Override
-  public int docID() {
-    return collectDocID;
-  }
-
-  @Override
-  public float score() {
-    return collectScore;
-  }
-
-  @Override
-  public int freq() {
-    return 1+dims.length;
-  }
-
-  @Override
-  public int nextDoc() {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public int advance(int target) {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public long cost() {
-    return baseScorer.cost();
-  }
-
-  @Override
-  public Collection<ChildScorer> getChildren() {
-    return Collections.singletonList(new ChildScorer(baseScorer, "MUST"));
-  }
-
-  static class DocsEnumsAndFreq implements Comparable<DocsEnumsAndFreq> {
-    DocsEnum[] docsEnums;
-    // Max cost for all docsEnums for this dim:
-    long maxCost;
-    Collector sidewaysCollector;
-    String dim;
-
-    @Override
-    public int compareTo(DocsEnumsAndFreq other) {
-      if (maxCost < other.maxCost) {
-        return -1;
-      } else if (maxCost > other.maxCost) {
-        return 1;
-      } else {
-        return 0;
-      }
-    }
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/FacetArrays.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/FacetArrays.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/FacetArrays.java	2012-12-11 10:38:55.333944738 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/FacetArrays.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,83 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Provider of arrays used for facets aggregation. Returns either an
- * {@code int[]} or {@code float[]} of the specified array length. When the
- * arrays are no longer needed, you should call {@link #free()}, so that e.g.
- * they will be reclaimed.
- * 
- * <p>
- * <b>NOTE:</b> if you need to reuse the allocated arrays between search
- * requests, use {@link ReusingFacetArrays}.
- * 
- * <p>
- * <b>NOTE:</b> this class is not thread safe. You typically allocate it per
- * search.
- * 
- * @lucene.experimental
- */
-public class FacetArrays {
-
-  private int[] ints;
-  private float[] floats;
-  
-  public final int arrayLength;
-
-  /** Arrays will be allocated at the specified length. */
-  public FacetArrays(int arrayLength) {
-    this.arrayLength = arrayLength;
-  }
-  
-  protected float[] newFloatArray() {
-    return new float[arrayLength];
-  }
-  
-  protected int[] newIntArray() {
-    return new int[arrayLength];
-  }
-  
-  protected void doFree(float[] floats, int[] ints) {
-  }
-  
-  /**
-   * Notifies that the arrays obtained from {@link #getIntArray()}
-   * or {@link #getFloatArray()} are no longer needed and can be freed.
-   */
-  public final void free() {
-    doFree(floats, ints);
-    ints = null;
-    floats = null;
-  }
-
-  public final int[] getIntArray() {
-    if (ints == null) {
-      ints = newIntArray();
-    }
-    return ints;
-  }
-
-  public final float[] getFloatArray() {
-    if (floats == null) {
-      floats = newFloatArray();
-    }
-    return floats;
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/FacetRequest.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/FacetRequest.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/FacetRequest.java	2013-08-01 14:47:20.750689724 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/FacetRequest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,213 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.range.RangeFacetRequest;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Defines an aggregation request for a category. Allows specifying the
- * {@link #numResults number of child categories} to return as well as
- * {@link #getSortOrder() which} categories to consider the "top" (highest or
- * lowest ranking ones).
- * <p>
- * If the category being aggregated is hierarchical, you can also specify the
- * {@link #setDepth(int) depth} up which to aggregate child categories as well
- * as how the result should be {@link #setResultMode(ResultMode) constructed}.
- * 
- * @lucene.experimental
- */
-public abstract class FacetRequest {
-  
-  /**
-   * When {@link FacetRequest#getDepth()} is greater than 1, defines the
-   * structure of the result as well as how constraints such as
-   * {@link FacetRequest#numResults} and {@link FacetRequest#getNumLabel()} are
-   * applied.
-   */
-  public enum ResultMode { 
-    /**
-     * Constraints are applied per node, and the result has a full tree
-     * structure. Default result mode.
-     */
-    PER_NODE_IN_TREE, 
-    
-    /**
-     * Constraints are applied globally, on total number of results, and the
-     * result has a flat structure.
-     */
-    GLOBAL_FLAT
-  }
-  
-  /**
-   * Defines which categories to return. If {@link #DESCENDING} (the default),
-   * the highest {@link FacetRequest#numResults} weighted categories will be
-   * returned, otherwise the lowest ones.
-   */
-  public enum SortOrder { ASCENDING, DESCENDING }
-
-  /** The category being aggregated in this facet request. */
-  public final CategoryPath categoryPath;
-  
-  /** The number of child categories to return for {@link #categoryPath}. */
-  public final int numResults;
-  
-  private int numLabel;
-  private int depth = 1;
-  private SortOrder sortOrder = SortOrder.DESCENDING;
-  private ResultMode resultMode = ResultMode.PER_NODE_IN_TREE;
-  
-  // Computed at construction; based on categoryPath and numResults.
-  private final int hashCode;
-  
-  /**
-   * Constructor with the given category to aggregate and the number of child
-   * categories to return.
-   * 
-   * @param path
-   *          the category to aggregate. Cannot be {@code null}.
-   * @param numResults
-   *          the number of child categories to return. If set to
-   *          {@code Integer.MAX_VALUE}, all immediate child categories will be
-   *          returned. Must be greater than 0.
-   */
-  public FacetRequest(CategoryPath path, int numResults) {
-    if (numResults <= 0) {
-      throw new IllegalArgumentException("num results must be a positive (>0) number: " + numResults);
-    }
-    if (path == null) {
-      throw new IllegalArgumentException("category path cannot be null!");
-    }
-    categoryPath = path;
-    this.numResults = numResults;
-    numLabel = numResults;
-    hashCode = categoryPath.hashCode() ^ this.numResults;
-  }
-  
-  /**
-   * Returns the {@link FacetsAggregator} which can aggregate the categories of
-   * this facet request. The aggregator is expected to aggregate category values
-   * into {@link FacetArrays}. If the facet request does not support that, e.g.
-   * {@link RangeFacetRequest}, it can return {@code null}. Note though that
-   * such requests require a dedicated {@link FacetsAccumulator}.
-   */
-  public abstract FacetsAggregator createFacetsAggregator(FacetIndexingParams fip);
-  
-  @Override
-  public boolean equals(Object o) {
-    if (o instanceof FacetRequest) {
-      FacetRequest that = (FacetRequest) o;
-     return that.hashCode == this.hashCode &&
-          that.categoryPath.equals(this.categoryPath) &&
-          that.numResults == this.numResults &&
-          that.depth == this.depth &&
-          that.resultMode == this.resultMode &&
-          that.numLabel == this.numLabel &&
-          that.sortOrder == this.sortOrder;
-    }
-    return false;
-  }
-  
-  /**
-   * How deeply to look under {@link #categoryPath}. By default, only its
-   * immediate children are aggregated (depth=1). If set to
-   * {@code Integer.MAX_VALUE}, the entire sub-tree of the category will be
-   * aggregated.
-   * <p>
-   * <b>NOTE:</b> setting depth to 0 means that only the category itself should
-   * be aggregated. In that case, make sure to index the category with
-   * {@link OrdinalPolicy#ALL_PARENTS}, unless it is not the root category (the
-   * dimension), in which case {@link OrdinalPolicy#ALL_BUT_DIMENSION} is fine
-   * too.
-   */
-  public final int getDepth() {
-    // TODO an AUTO_EXPAND option could be useful  
-    return depth;
-  }
-  
-  /**
-   * Allows to specify the number of categories to label. By default all
-   * returned categories are labeled.
-   * <p>
-   * This allows an app to request a large number of results to return, while
-   * labeling them on-demand (e.g. when the UI requests to show more
-   * categories).
-   */
-  public final int getNumLabel() {
-    return numLabel;
-  }
-  
-  /** Return the requested result mode (defaults to {@link ResultMode#PER_NODE_IN_TREE}. */
-  public final ResultMode getResultMode() {
-    return resultMode;
-  }
-  
-  /** Return the requested order of results (defaults to {@link SortOrder#DESCENDING}. */
-  public final SortOrder getSortOrder() {
-    return sortOrder;
-  }
-  
-  @Override
-  public int hashCode() {
-    return hashCode; 
-  }
-  
-  /**
-   * Sets the depth up to which to aggregate facets.
-   * 
-   * @see #getDepth()
-   */
-  public void setDepth(int depth) {
-    this.depth = depth;
-  }
-  
-  /**
-   * Sets the number of categories to label.
-   * 
-   * @see #getNumLabel()
-   */
-  public void setNumLabel(int numLabel) {
-    this.numLabel = numLabel;
-  }
-  
-  /**
-   * Sets the {@link ResultMode} for this request.
-   * 
-   * @see #getResultMode()
-   */
-  public void setResultMode(ResultMode resultMode) {
-    this.resultMode = resultMode;
-  }
-
-  /**
-   * Sets the {@link SortOrder} for this request.
-   * 
-   * @see #getSortOrder()
-   */
-  public void setSortOrder(SortOrder sortOrder) {
-    this.sortOrder = sortOrder;
-  }
-  
-  @Override
-  public String toString() {
-    return categoryPath.toString() + " nRes=" + numResults + " nLbl=" + numLabel;
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResult.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResult.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResult.java	2013-08-01 14:47:20.746689725 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResult.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,245 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.util.CollectionUtil;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Result of faceted search.
- * 
- * @lucene.experimental
- */
-public class FacetResult {
-  
-  private static FacetResultNode addIfNotExist(Map<CategoryPath, FacetResultNode> nodes, FacetResultNode node) {
-    FacetResultNode n = nodes.get(node.label);
-    if (n == null) {
-      nodes.put(node.label, node);
-      n = node;
-    }
-    return n;
-  }
-
-  /**
-   * A utility for merging multiple {@link FacetResult} of the same
-   * (hierarchical) dimension into a single {@link FacetResult}, to reconstruct
-   * the hierarchy. The results are merged according to the following rules:
-   * <ul>
-   * <li>If two results share the same dimension (first component in their
-   * {@link CategoryPath}), they are merged.
-   * <li>If a result is missing ancestors in the other results, e.g. A/B/C but
-   * no corresponding A or A/B, these nodes are 'filled' with their label,
-   * ordinal and value (obtained from the respective {@link FacetArrays}).
-   * <li>If a result does not share a dimension with other results, it is
-   * returned as is.
-   * </ul>
-   * <p>
-   * <b>NOTE:</b> the returned results are not guaranteed to be in the same
-   * order of the input ones.
-   * 
-   * @param results
-   *          the results to merge
-   * @param taxoReader
-   *          the {@link TaxonomyReader} to use when creating missing ancestor
-   *          nodes
-   * @param dimArrays
-   *          a mapping from a dimension to the respective {@link FacetArrays}
-   *          from which to pull the nodes values
-   */
-  public static List<FacetResult> mergeHierarchies(List<FacetResult> results, TaxonomyReader taxoReader,
-      Map<String, FacetArrays> dimArrays) throws IOException {
-    final Map<String, List<FacetResult>> dims = new HashMap<String,List<FacetResult>>();
-    for (FacetResult fr : results) {
-      String dim = fr.getFacetRequest().categoryPath.components[0];
-      List<FacetResult> frs = dims.get(dim);
-      if (frs == null) {
-        frs = new ArrayList<FacetResult>();
-        dims.put(dim, frs);
-      }
-      frs.add(fr);
-    }
-
-    final List<FacetResult> res = new ArrayList<FacetResult>();
-    for (List<FacetResult> frs : dims.values()) {
-      FacetResult mergedResult = frs.get(0);
-      if (frs.size() > 1) {
-        CollectionUtil.introSort(frs, new Comparator<FacetResult>() {
-          @Override
-          public int compare(FacetResult fr1, FacetResult fr2) {
-            return fr1.getFacetRequest().categoryPath.compareTo(fr2.getFacetRequest().categoryPath);
-          }
-        });
-        Map<CategoryPath, FacetResultNode> mergedNodes = new HashMap<CategoryPath,FacetResultNode>();
-        FacetArrays arrays = dimArrays != null ? dimArrays.get(frs.get(0).getFacetRequest().categoryPath.components[0]) : null;
-        for (FacetResult fr : frs) {
-          FacetRequest freq = fr.getFacetRequest();
-          OrdinalValueResolver resolver = null;
-          if (arrays != null) {
-            resolver = freq.createFacetsAggregator(FacetIndexingParams.DEFAULT).createOrdinalValueResolver(freq, arrays);
-          }
-          FacetResultNode frn = fr.getFacetResultNode();
-          FacetResultNode merged = mergedNodes.get(frn.label);
-          if (merged == null) {
-            CategoryPath parent = frn.label.subpath(frn.label.length - 1);
-            FacetResultNode childNode = frn;
-            FacetResultNode parentNode = null;
-            while (parent.length > 0 && (parentNode = mergedNodes.get(parent)) == null) {
-              int parentOrd = taxoReader.getOrdinal(parent);
-              double parentValue = -1;
-              if (arrays != null) {
-                parentValue = resolver.valueOf(parentOrd);
-              }
-              parentNode = new FacetResultNode(parentOrd, parentValue);
-              parentNode.label = parent;
-              parentNode.subResults = new ArrayList<FacetResultNode>();
-              parentNode.subResults.add(childNode);
-              mergedNodes.put(parent, parentNode);
-              childNode = parentNode;
-              parent = parent.subpath(parent.length - 1);
-            }
-
-            // at least one parent was added, so link the final (existing)
-            // parent with the child
-            if (parent.length > 0) {
-              if (!(parentNode.subResults instanceof ArrayList)) {
-                parentNode.subResults = new ArrayList<FacetResultNode>(parentNode.subResults);
-              }
-              parentNode.subResults.add(childNode);
-            }
-
-            // for missing FRNs, add new ones with label and value=-1
-            // first time encountered this label, add it and all its children to
-            // the map.
-            mergedNodes.put(frn.label, frn);
-            for (FacetResultNode child : frn.subResults) {
-              addIfNotExist(mergedNodes, child);
-            }
-          } else {
-            if (!(merged.subResults instanceof ArrayList)) {
-              merged.subResults = new ArrayList<FacetResultNode>(merged.subResults);
-            }
-            for (FacetResultNode sub : frn.subResults) {
-              // make sure sub wasn't already added
-              sub = addIfNotExist(mergedNodes, sub);
-              if (!merged.subResults.contains(sub)) {
-                merged.subResults.add(sub);
-              }
-            }
-          }
-        }
-        
-        // find the 'first' node to put on the FacetResult root
-        CategoryPath min = null;
-        for (CategoryPath cp : mergedNodes.keySet()) {
-          if (min == null || cp.compareTo(min) < 0) {
-            min = cp;
-          }
-        }
-        FacetRequest dummy = new FacetRequest(min, frs.get(0).getFacetRequest().numResults) {
-          @Override
-          public FacetsAggregator createFacetsAggregator(FacetIndexingParams fip) {
-            throw new UnsupportedOperationException("not supported by this request");
-          }
-        };
-        mergedResult = new FacetResult(dummy, mergedNodes.get(min), -1);
-      }
-      res.add(mergedResult);
-    }
-    return res;
-  }
-
-  private final FacetRequest facetRequest;
-  private final FacetResultNode rootNode;
-  private final int numValidDescendants;
-  
-  public FacetResult(FacetRequest facetRequest, FacetResultNode rootNode,  int numValidDescendants) {
-    this.facetRequest = facetRequest;
-    this.rootNode = rootNode;
-    this.numValidDescendants = numValidDescendants;
-  }
-  
-  /**
-   * Facet result node matching the root of the {@link #getFacetRequest() facet request}.
-   * @see #getFacetRequest()
-   * @see FacetRequest#categoryPath
-   */
-  public final FacetResultNode getFacetResultNode() {
-    return rootNode;
-  }
-  
-  /**
-   * Number of descendants of {@link #getFacetResultNode() root facet result
-   * node}, up till the requested depth.
-   */
-  public final int getNumValidDescendants() {
-    return numValidDescendants;
-  }
-  
-  /**
-   * Request for which this result was obtained.
-   */
-  public final FacetRequest getFacetRequest() {
-    return this.facetRequest;
-  }
-
-  /**
-   * String representation of this facet result.
-   * Use with caution: might return a very long string.
-   * @param prefix prefix for each result line
-   * @see #toString()
-   */
-  public String toString(String prefix) {
-    StringBuilder sb = new StringBuilder();
-    String nl = "";
-    
-    // request
-    if (this.facetRequest != null) {
-      sb.append(nl).append(prefix).append("Request: ").append(
-          this.facetRequest.toString());
-      nl = "\n";
-    }
-    
-    // total facets
-    sb.append(nl).append(prefix).append("Num valid Descendants (up to specified depth): ").append(
-        this.numValidDescendants);
-    nl = "\n";
-    
-    // result node
-    if (this.rootNode != null) {
-      sb.append(nl).append(this.rootNode.toString(prefix + "\t"));
-    }
-    
-    return sb.toString();
-  }
-  
-  @Override
-  public String toString() {
-    return toString("");
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultNode.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultNode.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultNode.java	2013-08-01 14:47:20.746689725 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultNode.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,107 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.util.Collections;
-import java.util.List;
-
-import org.apache.lucene.facet.search.FacetRequest.ResultMode;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Result of faceted search for a certain taxonomy node. This class serves as a
- * bin of different attributes of the result node, such as its {@link #ordinal}
- * as well as {@link #label}. You are not expected to modify those values.
- * <p>
- * This class implements {@link Comparable} for easy comparisons of result
- * nodes, e.g. when sorting or computing top-K nodes.
- * 
- * @lucene.experimental
- */
-public class FacetResultNode implements Comparable<FacetResultNode> {
-
-  public static final List<FacetResultNode> EMPTY_SUB_RESULTS = Collections.emptyList();
-  
-  /** The category ordinal of this node. */
-  public int ordinal;
-
-  /**
-   * The {@link CategoryPath label} of this result. May be {@code null} if not
-   * computed, in which case use {@link TaxonomyReader#getPath(int)} to label
-   * it.
-   * <p>
-   * <b>NOTE:</b> by default, all nodes are labeled. Only when
-   * {@link FacetRequest#getNumLabel()} &lt;
-   * {@link FacetRequest#numResults} there will be unlabeled nodes.
-   */
-  public CategoryPath label;
-  
-  /**
-   * The value of this result. Its actual type depends on the
-   * {@link FacetRequest} used (e.g. in case of {@link CountFacetRequest} it is
-   * {@code int}).
-   */
-  public double value;
-
-  /**
-   * The sub-results of this result. If {@link FacetRequest#getResultMode()} is
-   * {@link ResultMode#PER_NODE_IN_TREE}, every sub result denotes an immediate
-   * child of this node. Otherwise, it is a descendant of any level.
-   * <p>
-   * <b>NOTE:</b> this member should not be {@code null}. To denote that a
-   * result does not have sub results, set it to {@link #EMPTY_SUB_RESULTS} (or
-   * don't modify it).
-   */
-  public List<FacetResultNode> subResults = EMPTY_SUB_RESULTS;
-
-  public FacetResultNode(int ordinal, double value) {
-    this.ordinal = ordinal;
-    this.value = value;
-  }
-
-  @Override
-  public int compareTo(FacetResultNode o) {
-    int res = Double.compare(value, o.value);
-    if (res == 0) {
-      res = ordinal - o.ordinal;
-    }
-    return res;
-  }
-  
-  @Override
-  public String toString() {
-    return toString("");
-  }
-  
-  /** Returns a String representation of this facet result node. */
-  public String toString(String prefix) {
-    StringBuilder sb = new StringBuilder(prefix);
-    if (label == null) {
-      sb.append("not labeled (ordinal=").append(ordinal).append(")");
-    } else {
-      sb.append(label.toString());
-    }
-    sb.append(" (").append(Double.toString(value)).append(")");
-    for (FacetResultNode sub : subResults) {
-      sb.append("\n").append(prefix).append(sub.toString(prefix + "  "));
-    }
-    return sb.toString();
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultsHandler.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultsHandler.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultsHandler.java	2013-08-01 14:47:20.746689725 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultsHandler.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,48 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Computes the top categories for a given {@link FacetRequest}. 
- * 
- * @lucene.experimental
- */
-public abstract class FacetResultsHandler {
-
-  public final TaxonomyReader taxonomyReader;
-  public final FacetRequest facetRequest;
-  
-  protected final OrdinalValueResolver resolver;
-  protected final FacetArrays facetArrays;
-
-  public FacetResultsHandler(TaxonomyReader taxonomyReader, FacetRequest facetRequest, OrdinalValueResolver resolver, 
-      FacetArrays facetArrays) {
-    this.taxonomyReader = taxonomyReader;
-    this.facetRequest = facetRequest;
-    this.facetArrays = facetArrays;
-    this.resolver = resolver;
-  }
-
-  /** Computes the {@link FacetResult} for the given {@link FacetArrays}. */
-  public abstract FacetResult compute() throws IOException;
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAccumulator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAccumulator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAccumulator.java	2013-07-29 13:55:02.625707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAccumulator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,172 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.facet.old.OldFacetsAccumulator;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.range.RangeAccumulator;
-import org.apache.lucene.facet.range.RangeFacetRequest;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.sortedset.SortedSetDocValuesAccumulator;
-import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.IndexReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Accumulates the facets defined in the {@link FacetSearchParams}.
- * 
- * @lucene.experimental
- */
-public abstract class FacetsAccumulator {
-
-  // TODO this should be final, but currently SamplingAccumulator modifies the params.
-  // need to review the class and if it's resolved, make it final
-  public /*final*/ FacetSearchParams searchParams;
-
-  /** Constructor with the given search params. */
-  protected FacetsAccumulator(FacetSearchParams fsp) {
-    this.searchParams = fsp;
-  }
-
-  /**
-   * Creates a {@link FacetsAccumulator} for the given facet requests. This
-   * method supports {@link RangeAccumulator} and
-   * {@link TaxonomyFacetsAccumulator} by dividing the facet requests into
-   * {@link RangeFacetRequest} and the rest.
-   * <p>
-   * If both types of facet requests are used, it returns a
-   * {@link MultiFacetsAccumulator} and the facet results returned from
-   * {@link #accumulate(List)} may not be in the same order as the given facet
-   * requests.
-   * 
-   * @param fsp
-   *          the search params define the facet requests and the
-   *          {@link FacetIndexingParams}
-   * @param indexReader
-   *          the {@link IndexReader} used for search
-   * @param taxoReader
-   *          the {@link TaxonomyReader} used for search
-   * @param arrays
-   *          the {@link FacetArrays} which the accumulator should use to store
-   *          the categories weights in. Can be {@code null}.
-   */
-  public static FacetsAccumulator create(FacetSearchParams fsp, IndexReader indexReader, TaxonomyReader taxoReader, 
-      FacetArrays arrays) {
-    if (fsp.indexingParams.getPartitionSize() != Integer.MAX_VALUE) {
-      return new OldFacetsAccumulator(fsp, indexReader, taxoReader, arrays);
-    }
-    
-    List<FacetRequest> rangeRequests = new ArrayList<FacetRequest>();
-    List<FacetRequest> nonRangeRequests = new ArrayList<FacetRequest>();
-    for (FacetRequest fr : fsp.facetRequests) {
-      if (fr instanceof RangeFacetRequest) {
-        rangeRequests.add(fr);
-      } else {
-        nonRangeRequests.add(fr);
-      }
-    }
-
-    if (rangeRequests.isEmpty()) {
-      return new TaxonomyFacetsAccumulator(fsp, indexReader, taxoReader, arrays);
-    } else if (nonRangeRequests.isEmpty()) {
-      return new RangeAccumulator(rangeRequests);
-    } else {
-      FacetSearchParams searchParams = new FacetSearchParams(fsp.indexingParams, nonRangeRequests);
-      FacetsAccumulator accumulator = new TaxonomyFacetsAccumulator(searchParams, indexReader, taxoReader, arrays);
-      RangeAccumulator rangeAccumulator = new RangeAccumulator(rangeRequests);
-      return MultiFacetsAccumulator.wrap(accumulator, rangeAccumulator);
-    }
-  }
-  
-  /**
-   * Creates a {@link FacetsAccumulator} for the given facet requests. This
-   * method supports {@link RangeAccumulator} and
-   * {@link SortedSetDocValuesAccumulator} by dividing the facet requests into
-   * {@link RangeFacetRequest} and the rest.
-   * <p>
-   * If both types of facet requests are used, it returns a
-   * {@link MultiFacetsAccumulator} and the facet results returned from
-   * {@link #accumulate(List)} may not be in the same order as the given facet
-   * requests.
-   * 
-   * @param fsp
-   *          the search params define the facet requests and the
-   *          {@link FacetIndexingParams}
-   * @param state
-   *          the {@link SortedSetDocValuesReaderState} needed for accumulating
-   *          the categories
-   * @param arrays
-   *          the {@link FacetArrays} which the accumulator should use to
-   *          store the categories weights in. Can be {@code null}.
-   */
-  public static FacetsAccumulator create(FacetSearchParams fsp, SortedSetDocValuesReaderState state, FacetArrays arrays) throws IOException {
-    if (fsp.indexingParams.getPartitionSize() != Integer.MAX_VALUE) {
-      throw new IllegalArgumentException("only default partition size is supported by this method: " + fsp.indexingParams.getPartitionSize());
-    }
-    
-    List<FacetRequest> rangeRequests = new ArrayList<FacetRequest>();
-    List<FacetRequest> nonRangeRequests = new ArrayList<FacetRequest>();
-    for (FacetRequest fr : fsp.facetRequests) {
-      if (fr instanceof RangeFacetRequest) {
-        rangeRequests.add(fr);
-      } else {
-        nonRangeRequests.add(fr);
-      }
-    }
-    
-    if (rangeRequests.isEmpty()) {
-      return new SortedSetDocValuesAccumulator(state, fsp, arrays);
-    } else if (nonRangeRequests.isEmpty()) {
-      return new RangeAccumulator(rangeRequests);
-    } else {
-      FacetSearchParams searchParams = new FacetSearchParams(fsp.indexingParams, nonRangeRequests);
-      FacetsAccumulator accumulator = new SortedSetDocValuesAccumulator(state, searchParams, arrays);
-      RangeAccumulator rangeAccumulator = new RangeAccumulator(rangeRequests);
-      return MultiFacetsAccumulator.wrap(accumulator, rangeAccumulator);
-    }
-  }
-  
-  /** Returns an empty {@link FacetResult}. */
-  protected static FacetResult emptyResult(int ordinal, FacetRequest fr) {
-    FacetResultNode root = new FacetResultNode(ordinal, 0);
-    root.label = fr.categoryPath;
-    return new FacetResult(fr, root, 0);
-  }
-  
-  /**
-   * Used by {@link FacetsCollector} to build the list of {@link FacetResult
-   * facet results} that match the {@link FacetRequest facet requests} that were
-   * given in the constructor.
-   * 
-   * @param matchingDocs
-   *          the documents that matched the query, per-segment.
-   */
-  public abstract List<FacetResult> accumulate(List<MatchingDocs> matchingDocs) throws IOException;
-
-  /**
-   * Used by {@link FacetsCollector} to determine if document scores need to be
-   * collected in addition to matching documents.
-   */
-  public abstract boolean requiresDocScores();
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAggregator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAggregator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAggregator.java	2013-08-01 14:47:20.746689725 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAggregator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,56 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Aggregates categories that were found in result documents (specified by
- * {@link MatchingDocs}). If the aggregator requires document scores too, it
- * should return {@code true} from {@link #requiresDocScores()}.
- * 
- * @lucene.experimental
- */
-public interface FacetsAggregator {
-  
-  /** Aggregate the facets found in the given matching documents. */
-  public void aggregate(MatchingDocs matchingDocs, CategoryListParams clp, FacetArrays facetArrays) throws IOException;
-  
-  /**
-   * Rollup the values of the given ordinal. This method is called when a
-   * category was indexed with {@link OrdinalPolicy#NO_PARENTS}. The given
-   * ordinal is the requested category, and you should use the children and
-   * siblings arrays to traverse its sub-tree.
-   */
-  public void rollupValues(FacetRequest fr, int ordinal, int[] children, int[] siblings, FacetArrays facetArrays);
-  
-  /** Returns {@code true} if this aggregator requires document scores. */
-  public boolean requiresDocScores();
-  
-  /**
-   * Creates the appropriate {@link OrdinalValueResolver} for this aggregator
-   * and the given {@link FacetRequest}. The request is passed so that compound
-   * aggregators can return the correct {@link OrdinalValueResolver}.
-   */
-  public OrdinalValueResolver createOrdinalValueResolver(FacetRequest facetRequest, FacetArrays arrays);
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsCollector.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsCollector.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsCollector.java	2013-08-21 18:21:00.792120986 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsCollector.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,249 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.FixedBitSet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link Collector} which executes faceted search and computes the weight of
- * requested facets. To get the facet results you should call
- * {@link #getFacetResults()}.
- * {@link #create(FacetSearchParams, IndexReader, TaxonomyReader)} returns the
- * most optimized {@link FacetsCollector} for the given parameters.
- * 
- * @lucene.experimental
- */
-public abstract class FacetsCollector extends Collector {
-
-  private static final class DocsAndScoresCollector extends FacetsCollector {
-
-    private AtomicReaderContext context;
-    private Scorer scorer;
-    private FixedBitSet bits;
-    private int totalHits;
-    private float[] scores;
-    
-    public DocsAndScoresCollector(FacetsAccumulator accumulator) {
-      super(accumulator);
-    }
-    
-    @Override
-    protected final void finish() {
-      if (bits != null) {
-        matchingDocs.add(new MatchingDocs(this.context, bits, totalHits, scores));
-        bits = null;
-        scores = null;
-        context = null;
-      }
-    }
-    
-    @Override
-    public final boolean acceptsDocsOutOfOrder() {
-      return false;
-    }
-
-    @Override
-    public final void collect(int doc) throws IOException {
-      bits.set(doc);
-      if (totalHits >= scores.length) {
-        float[] newScores = new float[ArrayUtil.oversize(totalHits + 1, 4)];
-        System.arraycopy(scores, 0, newScores, 0, totalHits);
-        scores = newScores;
-      }
-      scores[totalHits] = scorer.score();
-      totalHits++;
-    }
-
-    @Override
-    public final void setScorer(Scorer scorer) throws IOException {
-      this.scorer = scorer;
-    }
-    
-    @Override
-    protected final void doSetNextReader(AtomicReaderContext context) throws IOException {
-      if (bits != null) {
-        matchingDocs.add(new MatchingDocs(this.context, bits, totalHits, scores));
-      }
-      bits = new FixedBitSet(context.reader().maxDoc());
-      totalHits = 0;
-      scores = new float[64]; // some initial size
-      this.context = context;
-    }
-
-  }
-
-  private final static class DocsOnlyCollector extends FacetsCollector {
-
-    private AtomicReaderContext context;
-    private FixedBitSet bits;
-    private int totalHits;
-
-    public DocsOnlyCollector(FacetsAccumulator accumulator) {
-      super(accumulator);
-    }
-    
-    @Override
-    protected final void finish() {
-      if (bits != null) {
-        matchingDocs.add(new MatchingDocs(this.context, bits, totalHits, null));
-        bits = null;
-        context = null;
-      }
-    }
-    
-    @Override
-    public final boolean acceptsDocsOutOfOrder() {
-      return true;
-    }
-
-    @Override
-    public final void collect(int doc) throws IOException {
-      totalHits++;
-      bits.set(doc);
-    }
-
-    @Override
-    public final void setScorer(Scorer scorer) throws IOException {}
-    
-    @Override
-    protected final void doSetNextReader(AtomicReaderContext context) throws IOException {
-      if (bits != null) {
-        matchingDocs.add(new MatchingDocs(this.context, bits, totalHits, null));
-      }
-      bits = new FixedBitSet(context.reader().maxDoc());
-      totalHits = 0;
-      this.context = context;
-    }
-  }
-  
-  /**
-   * Holds the documents that were matched in the {@link AtomicReaderContext}.
-   * If scores were required, then {@code scores} is not null.
-   */
-  public final static class MatchingDocs {
-    
-    public final AtomicReaderContext context;
-    public final FixedBitSet bits;
-    public final float[] scores;
-    public final int totalHits;
-    
-    public MatchingDocs(AtomicReaderContext context, FixedBitSet bits, int totalHits, float[] scores) {
-      this.context = context;
-      this.bits = bits;
-      this.scores = scores;
-      this.totalHits = totalHits;
-    }
-  }
-  
-  /**
-   * Creates a {@link FacetsCollector} using the {@link
-   * FacetsAccumulator} from {@link FacetsAccumulator#create}.
-   */
-  public static FacetsCollector create(FacetSearchParams fsp, IndexReader indexReader, TaxonomyReader taxoReader) {
-    return create(FacetsAccumulator.create(fsp, indexReader, taxoReader, null));
-  }
-
-  /**
-   * Creates a {@link FacetsCollector} that satisfies the requirements of the
-   * given {@link FacetsAccumulator}.
-   */
-  public static FacetsCollector create(FacetsAccumulator accumulator) {
-    if (accumulator.requiresDocScores()) {
-      return new DocsAndScoresCollector(accumulator);
-    } else {
-      return new DocsOnlyCollector(accumulator);
-    }
-  }
-
-  private final FacetsAccumulator accumulator;
-  private List<FacetResult> cachedResults;
-  
-  protected final List<MatchingDocs> matchingDocs = new ArrayList<MatchingDocs>();
-
-  protected FacetsCollector(FacetsAccumulator accumulator) {
-    this.accumulator = accumulator;
-  }
-  
-  /**
-   * Called when the Collector has finished, so that the last
-   * {@link MatchingDocs} can be added.
-   */
-  protected abstract void finish();
-  
-  /** Performs the actual work of {@link #setNextReader(AtomicReaderContext)}. */
-  protected abstract void doSetNextReader(AtomicReaderContext context) throws IOException;
-  
-  /**
-   * Returns a {@link FacetResult} per {@link FacetRequest} set in
-   * {@link FacetSearchParams}. Note that if a {@link FacetRequest} defines a
-   * {@link CategoryPath} which does not exist in the taxonomy, an empty
-   * {@link FacetResult} will be returned for it.
-   */
-  public final List<FacetResult> getFacetResults() throws IOException {
-    // LUCENE-4893: if results are not cached, counts are multiplied as many
-    // times as this method is called. 
-    if (cachedResults == null) {
-      finish();
-      cachedResults = accumulator.accumulate(matchingDocs);
-    }
-    
-    return cachedResults;
-  }
-  
-  /**
-   * Returns the documents matched by the query, one {@link MatchingDocs} per
-   * visited segment.
-   */
-  public final List<MatchingDocs> getMatchingDocs() {
-    finish();
-    return matchingDocs;
-  }
-  
-  /**
-   * Allows to reuse the collector between search requests. This method simply
-   * clears all collected documents (and scores) information (as well as cached
-   * results), and does not attempt to reuse allocated memory spaces.
-   */
-  public final void reset() {
-    finish();
-    matchingDocs.clear();
-    cachedResults = null;
-  }
-
-  @Override
-  public final void setNextReader(AtomicReaderContext context) throws IOException {
-    // clear cachedResults - needed in case someone called getFacetResults()
-    // before doing a search and didn't call reset(). Defensive code to prevent
-    // traps.
-    cachedResults = null;
-    doSetNextReader(context);
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/FastCountingFacetsAggregator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/FastCountingFacetsAggregator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/FastCountingFacetsAggregator.java	2013-11-08 07:03:49.800995538 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/FastCountingFacetsAggregator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,80 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.encoding.DGapVInt8IntDecoder;
-import org.apache.lucene.facet.encoding.DGapVInt8IntEncoder;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.util.BytesRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetsAggregator} which counts the number of times each category
- * appears in the given set of documents. This aggregator reads the categories
- * from the {@link BinaryDocValues} field defined by
- * {@link CategoryListParams#field}, and assumes that the category ordinals were
- * encoded with {@link DGapVInt8IntEncoder}.
- * 
- * @lucene.experimental
- */
-public final class FastCountingFacetsAggregator extends IntRollupFacetsAggregator {
-  
-  private final BytesRef buf = new BytesRef(32);
-  
-  @Override
-  public final void aggregate(MatchingDocs matchingDocs, CategoryListParams clp, FacetArrays facetArrays) 
-      throws IOException {
-    assert clp.createEncoder().createMatchingDecoder().getClass() == DGapVInt8IntDecoder.class 
-        : "this aggregator assumes ordinals were encoded as dgap+vint";
-    
-    final BinaryDocValues dv = matchingDocs.context.reader().getBinaryDocValues(clp.field);
-    if (dv == null) { // this reader does not have DocValues for the requested category list
-      return;
-    }
-    
-    final int length = matchingDocs.bits.length();
-    final int[] counts = facetArrays.getIntArray();
-    int doc = 0;
-    while (doc < length && (doc = matchingDocs.bits.nextSetBit(doc)) != -1) {
-      dv.get(doc, buf);
-      if (buf.length > 0) {
-        // this document has facets
-        final int upto = buf.offset + buf.length;
-        int ord = 0;
-        int offset = buf.offset;
-        int prev = 0;
-        while (offset < upto) {
-          byte b = buf.bytes[offset++];
-          if (b >= 0) {
-            prev = ord = ((ord << 7) | b) + prev;
-            assert ord < counts.length: "ord=" + ord + " vs maxOrd=" + counts.length;
-            ++counts[ord];
-            ord = 0;
-          } else {
-            ord = (ord << 7) | (b & 0x7F);
-          }
-        }
-      }
-      ++doc;
-    }
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/Heap.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/Heap.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/Heap.java	2012-07-21 08:52:55.701098776 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/Heap.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,56 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** 
- * Declares an interface for heap (and heap alike) structures, 
- * handling a given type T
- * 
- * @lucene.experimental
- */
-public interface Heap<T> {
-  /**
-   * Get and remove the top of the Heap <BR>
-   * NOTE: Once {@link #pop()} is called no other {@link #add(Object)} or
-   * {@link #insertWithOverflow(Object)} should be called.
-   */
-  public T pop();
-  
-  /** Get (But not remove) the top of the Heap */ 
-  public T top();
-  
-  /**
-   * Insert a new value, returning the overflowen object <br>
-   * NOTE: This method should not be called after invoking {@link #pop()}
-   */
-  public T insertWithOverflow(T value);
-  
-  /** 
-   * Add a new value to the heap, return the new top(). <br>
-   * Some implementations may choose to not implement this functionality. 
-   * In such a case <code>null</code> should be returned. <BR> 
-   * NOTE: This method should not be called after invoking {@link #pop()}
-   */
-  public T add(T frn);
-  
-  /** Clear the heap */ 
-  public void clear();
-  
-  /** Return the amount of objects currently in the heap */
-  public int size();
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/IntRollupFacetsAggregator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/IntRollupFacetsAggregator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/IntRollupFacetsAggregator.java	2013-08-01 14:47:20.750689724 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/IntRollupFacetsAggregator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,69 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.search.OrdinalValueResolver.IntValueResolver;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetsAggregator} which implements
- * {@link #rollupValues(FacetRequest, int, int[], int[], FacetArrays)} by
- * summing the values from {@link FacetArrays#getIntArray()}. Extending classes
- * should only implement {@link #aggregate}. Also, {@link #requiresDocScores()}
- * always returns false.
- * 
- * @lucene.experimental
- */
-public abstract class IntRollupFacetsAggregator implements FacetsAggregator {
-  
-  @Override
-  public abstract void aggregate(MatchingDocs matchingDocs, CategoryListParams clp, FacetArrays facetArrays) throws IOException;
-  
-  private int rollupValues(int ordinal, int[] children, int[] siblings, int[] values) {
-    int value = 0;
-    while (ordinal != TaxonomyReader.INVALID_ORDINAL) {
-      int childValue = values[ordinal];
-      childValue += rollupValues(children[ordinal], children, siblings, values);
-      values[ordinal] = childValue;
-      value += childValue;
-      ordinal = siblings[ordinal];
-    }
-    return value;
-  }
-
-  @Override
-  public final void rollupValues(FacetRequest fr, int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
-    final int[] values = facetArrays.getIntArray();
-    values[ordinal] += rollupValues(children[ordinal], children, siblings, values);
-  }
-  
-  @Override
-  public final boolean requiresDocScores() {
-    return false;
-  }
-  
-  @Override
-  public OrdinalValueResolver createOrdinalValueResolver(FacetRequest facetRequest, FacetArrays arrays) {
-    return new IntValueResolver(arrays);
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/MultiFacetsAccumulator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/MultiFacetsAccumulator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/MultiFacetsAccumulator.java	2013-07-29 13:55:02.625707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/MultiFacetsAccumulator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,69 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetsAccumulator;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-
-/**
- * Wraps multiple {@link FacetsAccumulator} and returns a merged list of
- * {@link FacetResult}, in the order the accumulators were given.
- */
-public class MultiFacetsAccumulator extends FacetsAccumulator {
-  
-  private final FacetsAccumulator[] accumulators;
-  
-  /** Wraps the given {@link FacetsAccumulator accumulators}. */
-  public static FacetsAccumulator wrap(FacetsAccumulator... accumulators) {
-    if (accumulators.length == 0) {
-      return accumulators[0];
-    } else {
-      return new MultiFacetsAccumulator(accumulators);
-    }
-  }
-
-  private MultiFacetsAccumulator(FacetsAccumulator... accumulators) {
-    super((FacetSearchParams) null);
-    this.accumulators = accumulators;
-  }
-
-  @Override
-  public boolean requiresDocScores() {
-    for (FacetsAccumulator fa : accumulators) {
-      if (fa.requiresDocScores()) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  @Override
-  public List<FacetResult> accumulate(List<MatchingDocs> matchingDocs) throws IOException {
-    List<FacetResult> merged = new ArrayList<FacetResult>();
-    for (FacetsAccumulator fa : accumulators) {
-      merged.addAll(fa.accumulate(matchingDocs));
-    }
-    return merged;
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/MultiFacetsAggregator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/MultiFacetsAggregator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/MultiFacetsAggregator.java	2013-08-01 14:47:20.746689725 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/MultiFacetsAggregator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,96 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetsAggregator;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetsAggregator} which chains multiple aggregators for aggregating
- * the association values of categories that belong to the same category list.
- * While nothing prevents you from chaining general purpose aggregators, it is
- * only useful for aggregating association values, as each association type is
- * written in its own list.
- * 
- * @lucene.experimental
- */
-public class MultiFacetsAggregator implements FacetsAggregator {
-  
-  private final Map<CategoryPath,FacetsAggregator> categoryAggregators;
-  private final List<FacetsAggregator> aggregators;
-  
-  /**
-   * Constructor.
-   * <p>
-   * The mapping is used to rollup the values of the specific category by the
-   * corresponding {@link FacetsAggregator}. It is ok to pass differnet
-   * {@link FacetsAggregator} instances for each {@link CategoryPath} - the
-   * constructor ensures that each aggregator <u>type</u> (determined by its
-   * class) is invoked only once.
-   */
-  public MultiFacetsAggregator(Map<CategoryPath,FacetsAggregator> aggregators) {
-    this.categoryAggregators = aggregators;
-    
-    // make sure that each FacetsAggregator class is invoked only once, or
-    // otherwise categories may be aggregated multiple times.
-    Map<Class<? extends FacetsAggregator>, FacetsAggregator> aggsClasses = 
-        new HashMap<Class<? extends FacetsAggregator>,FacetsAggregator>();
-    for (FacetsAggregator fa : aggregators.values()) {
-      aggsClasses.put(fa.getClass(), fa);
-    }
-    this.aggregators = new ArrayList<FacetsAggregator>(aggsClasses.values());
-  }
-  
-  @Override
-  public void aggregate(MatchingDocs matchingDocs, CategoryListParams clp, FacetArrays facetArrays) throws IOException {
-    for (FacetsAggregator fa : aggregators) {
-      fa.aggregate(matchingDocs, clp, facetArrays);
-    }
-  }
-  
-  @Override
-  public void rollupValues(FacetRequest fr, int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
-    categoryAggregators.get(fr.categoryPath).rollupValues(fr, ordinal, children, siblings, facetArrays);
-  }
-  
-  @Override
-  public boolean requiresDocScores() {
-    for (FacetsAggregator fa : aggregators) {
-      if (fa.requiresDocScores()) {
-        return true;
-      }
-    }
-    return false;
-  }
-  
-  @Override
-  public OrdinalValueResolver createOrdinalValueResolver(FacetRequest facetRequest, FacetArrays arrays) {
-    return categoryAggregators.get(facetRequest.categoryPath).createOrdinalValueResolver(facetRequest, arrays);
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/OrdinalsCache.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/OrdinalsCache.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/OrdinalsCache.java	2013-10-23 20:08:27.454337388 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/OrdinalsCache.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,149 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.WeakHashMap;
-
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.facet.encoding.IntDecoder;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A per-segment cache of documents' category ordinals. Every {@link CachedOrds}
- * holds the ordinals in a raw {@code int[]}, and therefore consumes as much RAM
- * as the total number of ordinals found in the segment.
- * 
- * <p>
- * <b>NOTE:</b> every {@link CachedOrds} is limited to 2.1B total ordinals. If
- * that is a limitation for you then consider limiting the segment size to less
- * documents, or use an alternative cache which pages through the category
- * ordinals.
- * 
- * <p>
- * <b>NOTE:</b> when using this cache, it is advised to use a
- * {@link DocValuesFormat} that does not cache the data in memory, at least for
- * the category lists fields, or otherwise you'll be doing double-caching.
- */
-public class OrdinalsCache {
-  
-  /** Holds the cached ordinals in two paralel {@code int[]} arrays. */
-  public static final class CachedOrds {
-    
-    public final int[] offsets;
-    public final int[] ordinals;
-
-    /**
-     * Creates a new {@link CachedOrds} from the {@link BinaryDocValues}.
-     * Assumes that the {@link BinaryDocValues} is not {@code null}.
-     */
-    public CachedOrds(BinaryDocValues dv, int maxDoc, CategoryListParams clp) {
-      final BytesRef buf = new BytesRef();
-
-      offsets = new int[maxDoc + 1];
-      int[] ords = new int[maxDoc]; // let's assume one ordinal per-document as an initial size
-
-      // this aggregator is limited to Integer.MAX_VALUE total ordinals.
-      int totOrds = 0;
-      final IntDecoder decoder = clp.createEncoder().createMatchingDecoder();
-      final IntsRef values = new IntsRef(32);
-      for (int docID = 0; docID < maxDoc; docID++) {
-        offsets[docID] = totOrds;
-        dv.get(docID, buf);
-        if (buf.length > 0) {
-          // this document has facets
-          decoder.decode(buf, values);
-          if (totOrds + values.length >= ords.length) {
-            ords = ArrayUtil.grow(ords, totOrds + values.length + 1);
-          }
-          for (int i = 0; i < values.length; i++) {
-            ords[totOrds++] = values.ints[i];
-          }
-        }
-      }
-      offsets[maxDoc] = totOrds;
-      
-      // if ords array is bigger by more than 10% of what we really need, shrink it
-      if ((double) totOrds / ords.length < 0.9) { 
-        this.ordinals = new int[totOrds];
-        System.arraycopy(ords, 0, this.ordinals, 0, totOrds);
-      } else {
-        this.ordinals = ords;
-      }
-    }
-  }
-
-  // outer map is a WeakHashMap which uses reader.getCoreCacheKey() as the weak
-  // reference. When it's no longer referenced, the entire inner map can be
-  // evicted.
-  private static final Map<Object,Map<String,CachedOrds>> ordsCache = new WeakHashMap<Object,Map<String,CachedOrds>>();
-
-  /**
-   * Returns the {@link CachedOrds} relevant to the given
-   * {@link AtomicReaderContext}, or {@code null} if there is no
-   * {@link BinaryDocValues} in this reader for the requested
-   * {@link CategoryListParams#field}.
-   */
-  public static synchronized CachedOrds getCachedOrds(AtomicReaderContext context, CategoryListParams clp) throws IOException {
-    BinaryDocValues dv = context.reader().getBinaryDocValues(clp.field);
-    if (dv == null) {
-      return null;
-    }
-    Map<String,CachedOrds> fieldCache = ordsCache.get(context.reader().getCoreCacheKey());
-    if (fieldCache == null) {
-      fieldCache = new HashMap<String,OrdinalsCache.CachedOrds>();
-      ordsCache.put(context.reader().getCoreCacheKey(), fieldCache);
-    }
-    CachedOrds co = fieldCache.get(clp.field);
-    if (co == null) {
-      co = new CachedOrds(dv, context.reader().maxDoc(), clp);
-      fieldCache.put(clp.field, co);
-    }
-    return co;
-  }
-
-  /** Returns how many bytes the static ords cache is
-   *  consuming. */
-  public synchronized static long ramBytesUsed() {
-    long size = 0;
-    for (Map<String,CachedOrds> e : ordsCache.values()) {
-      for (CachedOrds co : e.values()) {
-        size += RamUsageEstimator.NUM_BYTES_OBJECT_REF              // CachedOrds reference in the map
-            + RamUsageEstimator.NUM_BYTES_OBJECT_HEADER             // CachedOrds object header
-            + RamUsageEstimator.NUM_BYTES_ARRAY_HEADER * 2          // 2 int[] (header)
-            + RamUsageEstimator.NUM_BYTES_OBJECT_REF * 2            // 2 int[] (ref)
-            + RamUsageEstimator.NUM_BYTES_INT * co.offsets.length   // sizeOf(offsets)
-            + RamUsageEstimator.NUM_BYTES_INT * co.ordinals.length; // sizeOf(ordinals)
-      }
-    }
-    return size;
-  }
-
-  /** Clears all entries from the cache. */
-  public synchronized static void clear() {
-    ordsCache.clear();
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/OrdinalValueResolver.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/OrdinalValueResolver.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/OrdinalValueResolver.java	2013-08-01 14:47:20.746689725 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/OrdinalValueResolver.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,76 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Resolves an ordinal's value to given the {@link FacetArrays}.
- * Implementations of this class are encouraged to initialize the needed array
- * from {@link FacetArrays} in the constructor.
- */
-public abstract class OrdinalValueResolver {
-
-  /**
-   * An {@link OrdinalValueResolver} which resolves ordinals value from
-   * {@link FacetArrays#getIntArray()}, by returning the value in the array.
-   */
-  public static final class IntValueResolver extends OrdinalValueResolver {
-
-    private final int[] values;
-    
-    public IntValueResolver(FacetArrays arrays) {
-      super(arrays);
-      this.values = arrays.getIntArray();
-    }
-
-    @Override
-    public final double valueOf(int ordinal) {
-      return values[ordinal];
-    }
-    
-  }
-  
-  /**
-   * An {@link OrdinalValueResolver} which resolves ordinals value from
-   * {@link FacetArrays#getFloatArray()}, by returning the value in the array.
-   */
-  public static final class FloatValueResolver extends OrdinalValueResolver {
-    
-    private final float[] values;
-    
-    public FloatValueResolver(FacetArrays arrays) {
-      super(arrays);
-      this.values = arrays.getFloatArray();
-    }
-    
-    @Override
-    public final double valueOf(int ordinal) {
-      return values[ordinal];
-    }
-    
-  }
-  
-  protected final FacetArrays arrays;
-  
-  protected OrdinalValueResolver(FacetArrays arrays) {
-    this.arrays = arrays;
-  }
-
-  /** Returns the value of the given ordinal. */
-  public abstract double valueOf(int ordinal);
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/package.html simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/package.html
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/package.html	2013-02-20 13:38:17.664711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/package.html	1969-12-31 19:00:00.000000000 -0500
@@ -1,24 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Facets search code</title>
-</head>
-<body>
-Facets search code.
-</body>
-</html>
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/PerCategoryListAggregator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/PerCategoryListAggregator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/PerCategoryListAggregator.java	2013-08-01 14:47:20.750689724 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/PerCategoryListAggregator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,71 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.Map;
-
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetsAggregator} which invokes the proper aggregator per
- * {@link CategoryListParams}.
- * {@link #rollupValues(FacetRequest, int, int[], int[], FacetArrays)} is
- * delegated to the proper aggregator which handles the
- * {@link CategoryListParams} the given {@link FacetRequest} belongs to.
- */
-public class PerCategoryListAggregator implements FacetsAggregator {
-  
-  private final Map<CategoryListParams,FacetsAggregator> aggregators;
-  private final FacetIndexingParams fip;
-  
-  public PerCategoryListAggregator(Map<CategoryListParams,FacetsAggregator> aggregators, FacetIndexingParams fip) {
-    this.aggregators = aggregators;
-    this.fip = fip;
-  }
-  
-  @Override
-  public void aggregate(MatchingDocs matchingDocs, CategoryListParams clp, FacetArrays facetArrays) throws IOException {
-    aggregators.get(clp).aggregate(matchingDocs, clp, facetArrays);
-  }
-  
-  @Override
-  public void rollupValues(FacetRequest fr, int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
-    CategoryListParams clp = fip.getCategoryListParams(fr.categoryPath);
-    aggregators.get(clp).rollupValues(fr, ordinal, children, siblings, facetArrays);
-  }
-  
-  @Override
-  public boolean requiresDocScores() {
-    for (FacetsAggregator aggregator : aggregators.values()) {
-      if (aggregator.requiresDocScores()) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  @Override
-  public OrdinalValueResolver createOrdinalValueResolver(FacetRequest facetRequest, FacetArrays arrays) {
-    CategoryListParams clp = fip.getCategoryListParams(facetRequest.categoryPath);
-    return aggregators.get(clp).createOrdinalValueResolver(facetRequest, arrays);
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/ReusingFacetArrays.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/ReusingFacetArrays.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/ReusingFacetArrays.java	2012-12-11 10:38:55.329944736 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/ReusingFacetArrays.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,51 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetArrays} which uses the {@link ArraysPool} to allocate new
- * arrays and pool them on {@link #free()}.
- * 
- * @lucene.experimental
- */
-public class ReusingFacetArrays extends FacetArrays {
-
-  private final ArraysPool arraysPool;
-
-  public ReusingFacetArrays(ArraysPool arraysPool) {
-    super(arraysPool.arrayLength);
-    this.arraysPool = arraysPool;
-  }
-
-  @Override
-  protected int[] newIntArray() {
-    return arraysPool.allocateIntArray();
-  }
-  
-  @Override
-  protected float[] newFloatArray() {
-    return arraysPool.allocateFloatArray();
-  }
-  
-  @Override
-  protected void doFree(float[] floats, int[] ints) {
-    arraysPool.free(floats);
-    arraysPool.free(ints);
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/SearcherTaxonomyManager.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/SearcherTaxonomyManager.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/SearcherTaxonomyManager.java	2013-07-15 15:52:17.669877389 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/SearcherTaxonomyManager.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,123 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.ReferenceManager;
-import org.apache.lucene.search.SearcherFactory;
-import org.apache.lucene.search.SearcherManager;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Manages near-real-time reopen of both an IndexSearcher
- * and a TaxonomyReader.
- *
- * <p><b>NOTE</b>: If you call {@link
- * DirectoryTaxonomyWriter#replaceTaxonomy} then you must
- * open a new {@code SearcherTaxonomyManager} afterwards.
- */
-public class SearcherTaxonomyManager extends ReferenceManager<SearcherTaxonomyManager.SearcherAndTaxonomy> {
-
-  /** Holds a matched pair of {@link IndexSearcher} and
-   *  {@link TaxonomyReader} */
-  public static class SearcherAndTaxonomy {
-    public final IndexSearcher searcher;
-    public final DirectoryTaxonomyReader taxonomyReader;
-
-    /** Create a SearcherAndTaxonomy */
-    public SearcherAndTaxonomy(IndexSearcher searcher, DirectoryTaxonomyReader taxonomyReader) {
-      this.searcher = searcher;
-      this.taxonomyReader = taxonomyReader;
-    }
-  }
-
-  private final SearcherFactory searcherFactory;
-  private final long taxoEpoch;
-  private final DirectoryTaxonomyWriter taxoWriter;
-
-  /** Creates near-real-time searcher and taxonomy reader
-   *  from the corresponding writers. */
-  public SearcherTaxonomyManager(IndexWriter writer, boolean applyAllDeletes, SearcherFactory searcherFactory, DirectoryTaxonomyWriter taxoWriter) throws IOException {
-    if (searcherFactory == null) {
-      searcherFactory = new SearcherFactory();
-    }
-    this.searcherFactory = searcherFactory;
-    this.taxoWriter = taxoWriter;
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    current = new SearcherAndTaxonomy(SearcherManager.getSearcher(searcherFactory, DirectoryReader.open(writer, applyAllDeletes)),
-                                      taxoReader);
-    taxoEpoch = taxoWriter.getTaxonomyEpoch();
-  }
-
-  @Override
-  protected void decRef(SearcherAndTaxonomy ref) throws IOException {
-    ref.searcher.getIndexReader().decRef();
-
-    // This decRef can fail, and then in theory we should
-    // tryIncRef the searcher to put back the ref count
-    // ... but 1) the below decRef should only fail because
-    // it decRef'd to 0 and closed and hit some IOException
-    // during close, in which case 2) very likely the
-    // searcher was also just closed by the above decRef and
-    // a tryIncRef would fail:
-    ref.taxonomyReader.decRef();
-  }
-
-  @Override
-  protected boolean tryIncRef(SearcherAndTaxonomy ref) throws IOException {
-    if (ref.searcher.getIndexReader().tryIncRef()) {
-      if (ref.taxonomyReader.tryIncRef()) {
-        return true;
-      } else {
-        ref.searcher.getIndexReader().decRef();
-      }
-    }
-    return false;
-  }
-
-  @Override
-  protected SearcherAndTaxonomy refreshIfNeeded(SearcherAndTaxonomy ref) throws IOException {
-    // Must re-open searcher first, otherwise we may get a
-    // new reader that references ords not yet known to the
-    // taxonomy reader:
-    final IndexReader r = ref.searcher.getIndexReader();
-    final IndexReader newReader = DirectoryReader.openIfChanged((DirectoryReader) r);
-    if (newReader == null) {
-      return null;
-    } else {
-      DirectoryTaxonomyReader tr = TaxonomyReader.openIfChanged(ref.taxonomyReader);
-      if (tr == null) {
-        ref.taxonomyReader.incRef();
-        tr = ref.taxonomyReader;
-      } else if (taxoWriter.getTaxonomyEpoch() != taxoEpoch) {
-        IOUtils.close(newReader, tr);
-        throw new IllegalStateException("DirectoryTaxonomyWriter.replaceTaxonomy was called, which is not allowed when using SearcherTaxonomyManager");
-      }
-
-      return new SearcherAndTaxonomy(SearcherManager.getSearcher(searcherFactory, newReader), tr);
-    }
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetRequest.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetRequest.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetRequest.java	2013-08-01 14:47:20.746689725 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetRequest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,41 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetRequest} for weighting facets by summing the scores of matching
- * documents.
- * 
- * @lucene.experimental
- */
-public class SumScoreFacetRequest extends FacetRequest {
-
-  /** Create a score facet request for a given node in the taxonomy. */
-  public SumScoreFacetRequest(CategoryPath path, int num) {
-    super(path, num);
-  }
-
-  @Override
-  public FacetsAggregator createFacetsAggregator(FacetIndexingParams fip) {
-    return new SumScoreFacetsAggregator();
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetsAggregator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetsAggregator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetsAggregator.java	2013-08-01 14:47:20.750689724 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetsAggregator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,86 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.search.OrdinalValueResolver.FloatValueResolver;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetsAggregator} which updates the weight of a category by summing the
- * scores of documents it was found in.
- */
-public class SumScoreFacetsAggregator implements FacetsAggregator {
-  
-  private final IntsRef ordinals = new IntsRef(32);
-  
-  @Override
-  public void aggregate(MatchingDocs matchingDocs, CategoryListParams clp, FacetArrays facetArrays) throws IOException {
-    CategoryListIterator cli = clp.createCategoryListIterator(0);
-    if (!cli.setNextReader(matchingDocs.context)) {
-      return;
-    }
-    
-    int doc = 0;
-    int length = matchingDocs.bits.length();
-    float[] scores = facetArrays.getFloatArray();
-    int scoresIdx = 0;
-    while (doc < length && (doc = matchingDocs.bits.nextSetBit(doc)) != -1) {
-      cli.getOrdinals(doc, ordinals);
-      int upto = ordinals.offset + ordinals.length;
-      final float score = matchingDocs.scores[scoresIdx++];
-      for (int i = ordinals.offset; i < upto; i++) {
-        scores[ordinals.ints[i]] += score;
-      }
-      ++doc;
-    }
-  }
-  
-  private float rollupScores(int ordinal, int[] children, int[] siblings, float[] scores) {
-    float score = 0f;
-    while (ordinal != TaxonomyReader.INVALID_ORDINAL) {
-      float childScore = scores[ordinal];
-      childScore += rollupScores(children[ordinal], children, siblings, scores);
-      scores[ordinal] = childScore;
-      score += childScore;
-      ordinal = siblings[ordinal];
-    }
-    return score;
-  }
-
-  @Override
-  public void rollupValues(FacetRequest fr, int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
-    float[] scores = facetArrays.getFloatArray();
-    scores[ordinal] += rollupScores(children[ordinal], children, siblings, scores);
-  }
-  
-  @Override
-  public boolean requiresDocScores() {
-    return true;
-  }
-
-  @Override
-  public OrdinalValueResolver createOrdinalValueResolver(FacetRequest facetRequest, FacetArrays arrays) {
-    return new FloatValueResolver(arrays);
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/SumValueSourceFacetRequest.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/SumValueSourceFacetRequest.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/SumValueSourceFacetRequest.java	2013-11-06 07:02:49.345619592 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/SumValueSourceFacetRequest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,194 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.search.OrdinalValueResolver.FloatValueResolver;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetRequest} which aggregates categories by the sum of the values,
- * returned by a {@link ValueSource}, in the documents they are associated with.
- * This allows aggregating the value of a category by e.g. summing the value of
- * a {@link NumericDocValuesField} indexed for the document, or a more complex
- * expression (from multiple fields) using the expressions module.
- * 
- * @lucene.experimental
- */
-public class SumValueSourceFacetRequest extends FacetRequest {
-
-  private static abstract class SumValueSourceFacetsAggregator implements FacetsAggregator {
-    
-    protected final ValueSource valueSource;
-    protected final IntsRef ordinals = new IntsRef(32);
-    
-    protected SumValueSourceFacetsAggregator(ValueSource valueSource) {
-      this.valueSource = valueSource;
-    }
-
-    private float doRollup(int ordinal, int[] children, int[] siblings, float[] values) {
-      float value = 0f;
-      while (ordinal != TaxonomyReader.INVALID_ORDINAL) {
-        float childValue = values[ordinal];
-        childValue += doRollup(children[ordinal], children, siblings, values);
-        values[ordinal] = childValue;
-        value += childValue;
-        ordinal = siblings[ordinal];
-      }
-      return value;
-    }
-
-    @Override
-    public void rollupValues(FacetRequest fr, int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
-      float[] values = facetArrays.getFloatArray();
-      values[ordinal] += doRollup(children[ordinal], children, siblings, values);
-    }
-
-    @Override
-    public OrdinalValueResolver createOrdinalValueResolver(FacetRequest facetRequest, FacetArrays arrays) {
-      return new FloatValueResolver(arrays);
-    }
-    
-  }
-  
-  private static class ScoreValueSourceFacetsAggregator extends SumValueSourceFacetsAggregator {
-
-    private static final class FakeScorer extends Scorer {
-      float score;
-      int docID;
-      FakeScorer() { super(null); }
-      @Override public float score() throws IOException { return score; }
-      @Override public int freq() throws IOException { throw new UnsupportedOperationException(); }
-      @Override public int docID() { return docID; }
-      @Override public int nextDoc() throws IOException { throw new UnsupportedOperationException(); }
-      @Override public int advance(int target) throws IOException { throw new UnsupportedOperationException(); }
-      @Override public long cost() { return 0; }
-    }
-
-    ScoreValueSourceFacetsAggregator(ValueSource valueSource) {
-      super(valueSource);
-    }
-
-    @Override
-    public void aggregate(MatchingDocs matchingDocs, CategoryListParams clp, FacetArrays facetArrays) throws IOException {
-      final CategoryListIterator cli = clp.createCategoryListIterator(0);
-      if (!cli.setNextReader(matchingDocs.context)) {
-        return;
-      }
-
-      assert matchingDocs.scores != null;
-
-      final FakeScorer scorer = new FakeScorer();
-      Map<String, Scorer> context = new HashMap<String, Scorer>();
-      context.put("scorer", scorer);
-
-      final FunctionValues fvalues = valueSource.getValues(context, matchingDocs.context);
-      final int length = matchingDocs.bits.length();
-      final float[] aggValues = facetArrays.getFloatArray();
-      int doc = 0;
-      int scoresIdx = 0;
-      while (doc < length && (doc = matchingDocs.bits.nextSetBit(doc)) != -1) {
-        scorer.docID = doc;
-        scorer.score = matchingDocs.scores[scoresIdx++];
-        cli.getOrdinals(doc, ordinals);
-        final int upto = ordinals.offset + ordinals.length;
-        float val = (float) fvalues.doubleVal(doc);
-        for (int i = ordinals.offset; i < upto; i++) {
-          aggValues[ordinals.ints[i]] += val;
-        }
-        ++doc;
-      }
-    }
-
-    @Override
-    public boolean requiresDocScores() {
-      return true;
-    }
-  }
-
-  private static class NoScoreValueSourceFacetsAggregator extends SumValueSourceFacetsAggregator {
-
-    NoScoreValueSourceFacetsAggregator(ValueSource valueSource) {
-      super(valueSource);
-    }
-
-    @Override
-    public void aggregate(MatchingDocs matchingDocs, CategoryListParams clp, FacetArrays facetArrays) throws IOException {
-      final CategoryListIterator cli = clp.createCategoryListIterator(0);
-      if (!cli.setNextReader(matchingDocs.context)) {
-        return;
-      }
-
-      final FunctionValues fvalues = valueSource.getValues(Collections.emptyMap(), matchingDocs.context);
-      final int length = matchingDocs.bits.length();
-      final float[] aggValues = facetArrays.getFloatArray();
-      int doc = 0;
-      while (doc < length && (doc = matchingDocs.bits.nextSetBit(doc)) != -1) {
-        cli.getOrdinals(doc, ordinals);
-        final int upto = ordinals.offset + ordinals.length;
-        float val = (float) fvalues.doubleVal(doc);
-        for (int i = ordinals.offset; i < upto; i++) {
-          aggValues[ordinals.ints[i]] += val;
-        }
-        ++doc;
-      }
-    }
-
-    @Override
-    public boolean requiresDocScores() {
-      return false;
-    }
-  }
-
-  private final ValueSource valueSource;
-  private final boolean requiresDocScores;
-
-  /**
-   * Constructor which takes the {@link ValueSource} from which to read the
-   * documents' values. You can also specify if the value source requires
-   * document scores or not.
-   */
-  public SumValueSourceFacetRequest(CategoryPath path, int num, ValueSource valueSource, boolean requiresDocScores) {
-    super(path, num);
-    this.valueSource = valueSource;
-    this.requiresDocScores = requiresDocScores;
-  }
-
-  @Override
-  public FacetsAggregator createFacetsAggregator(FacetIndexingParams fip) {
-    if (requiresDocScores) {
-      return new ScoreValueSourceFacetsAggregator(valueSource);
-    } else {
-      return new NoScoreValueSourceFacetsAggregator(valueSource);
-    }
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/TaxonomyFacetsAccumulator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/TaxonomyFacetsAccumulator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/TaxonomyFacetsAccumulator.java	2013-10-29 13:21:54.727597738 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/TaxonomyFacetsAccumulator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,218 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.FacetRequest.ResultMode;
-import org.apache.lucene.facet.search.FacetRequest.SortOrder;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.ParallelTaxonomyArrays;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.IndexReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetsAccumulator} suitable for accumulating categories that were
- * indexed into a taxonomy index.
- * 
- * @lucene.experimental
- */
-public class TaxonomyFacetsAccumulator extends FacetsAccumulator {
-
-  public final TaxonomyReader taxonomyReader;
-  public final IndexReader indexReader;
-  public final FacetArrays facetArrays;
-  
-  /**
-   * Initializes the accumulator with the given search params, index reader and
-   * taxonomy reader. This constructor creates the default {@link FacetArrays},
-   * which do not support reuse. If you want to use {@link ReusingFacetArrays},
-   * you should use the
-   * {@link #TaxonomyFacetsAccumulator(FacetSearchParams, IndexReader, TaxonomyReader)}
-   * constructor.
-   */
-  public TaxonomyFacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader, 
-      TaxonomyReader taxonomyReader) {
-    this(searchParams, indexReader, taxonomyReader, null);
-  }
-
-  /**
-   * Initializes the accumulator with the given parameters as well as
-   * {@link FacetArrays}. Note that the accumulator doesn't call
-   * {@link FacetArrays#free()}. If you require that (only makes sense if you
-   * use {@link ReusingFacetArrays}, you should do it after you've finished with
-   * the accumulator.
-   */
-  public TaxonomyFacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader, 
-      TaxonomyReader taxonomyReader, FacetArrays facetArrays) {
-    super(searchParams);
-    this.facetArrays = facetArrays == null ? new FacetArrays(taxonomyReader.getSize()) : facetArrays;
-    this.indexReader = indexReader;
-    this.taxonomyReader = taxonomyReader;
-  }
-
-  /** Group all requests that belong to the same {@link CategoryListParams}. */
-  protected Map<CategoryListParams,List<FacetRequest>> groupRequests() {
-    if (searchParams.indexingParams.getAllCategoryListParams().size() == 1) {
-      return Collections.singletonMap(searchParams.indexingParams.getCategoryListParams(null), searchParams.facetRequests);
-    }
-    
-    HashMap<CategoryListParams,List<FacetRequest>> requestsPerCLP = new HashMap<CategoryListParams,List<FacetRequest>>();
-    for (FacetRequest fr : searchParams.facetRequests) {
-      CategoryListParams clp = searchParams.indexingParams.getCategoryListParams(fr.categoryPath);
-      List<FacetRequest> requests = requestsPerCLP.get(clp);
-      if (requests == null) {
-        requests = new ArrayList<FacetRequest>();
-        requestsPerCLP.put(clp, requests);
-      }
-      requests.add(fr);
-    }
-    return requestsPerCLP;
-  }
-
-  /**
-   * Returns the {@link FacetsAggregator} to use for aggregating the categories
-   * found in the result documents.
-   */
-  public FacetsAggregator getAggregator() {
-    Map<CategoryListParams,List<FacetRequest>> requestsPerCLP = groupRequests();
-
-    // optimize for all-CountFacetRequest and single category list (common case)
-    if (requestsPerCLP.size() == 1) {
-      boolean allCount = true;
-      for (FacetRequest fr : searchParams.facetRequests) {
-        if (!(fr instanceof CountFacetRequest)) {
-          allCount = false;
-          break;
-        }
-      }
-      if (allCount) {
-        return requestsPerCLP.values().iterator().next().get(0).createFacetsAggregator(searchParams.indexingParams);
-      }
-    }
-    
-    // If we're here it means the facet requests are spread across multiple
-    // category lists, or there are multiple types of facet requests, or both.
-    // Therefore create a per-CategoryList mapping of FacetsAggregators.
-    Map<CategoryListParams,FacetsAggregator> perCLPAggregator = new HashMap<CategoryListParams,FacetsAggregator>();
-    for (Entry<CategoryListParams,List<FacetRequest>> e : requestsPerCLP.entrySet()) {
-      CategoryListParams clp = e.getKey();
-      List<FacetRequest> requests = e.getValue();
-      Map<Class<? extends FacetsAggregator>,FacetsAggregator> aggClasses = new HashMap<Class<? extends FacetsAggregator>,FacetsAggregator>();
-      Map<CategoryPath,FacetsAggregator> perCategoryAggregator = new HashMap<CategoryPath,FacetsAggregator>();
-      for (FacetRequest fr : requests) {
-        FacetsAggregator fa = fr.createFacetsAggregator(searchParams.indexingParams);
-        if (fa == null) {
-          throw new IllegalArgumentException("this accumulator only supports requests that create a FacetsAggregator: " + fr);
-        }
-        Class<? extends FacetsAggregator> faClass = fa.getClass();
-        if (!aggClasses.containsKey(faClass)) {
-          aggClasses.put(faClass, fa);
-        } else {
-          fa = aggClasses.get(faClass);
-        }
-        perCategoryAggregator.put(fr.categoryPath, fa);
-      }
-      
-      if (aggClasses.size() == 1) { // only one type of facet request
-        perCLPAggregator.put(clp, aggClasses.values().iterator().next());
-      } else {
-        perCLPAggregator.put(clp, new MultiFacetsAggregator(perCategoryAggregator));
-      }
-    }
-
-    return new PerCategoryListAggregator(perCLPAggregator, searchParams.indexingParams);
-  }
-  
-  /**
-   * Creates a {@link FacetResultsHandler} that matches the given
-   * {@link FacetRequest}, using the {@link OrdinalValueResolver}.
-   */
-  protected FacetResultsHandler createFacetResultsHandler(FacetRequest fr, OrdinalValueResolver resolver) {
-    if (fr.getDepth() == 1 && fr.getSortOrder() == SortOrder.DESCENDING) {
-      return new DepthOneFacetResultsHandler(taxonomyReader, fr, facetArrays, resolver);
-    }
-
-    if (fr.getResultMode() == ResultMode.PER_NODE_IN_TREE) {
-      return new TopKInEachNodeHandler(taxonomyReader, fr, resolver, facetArrays);
-    } else {
-      return new TopKFacetResultsHandler(taxonomyReader, fr, resolver, facetArrays);
-    }
-  }
-
-  /**
-   * Used by {@link FacetsCollector} to build the list of {@link FacetResult
-   * facet results} that match the {@link FacetRequest facet requests} that were
-   * given in the constructor.
-   * 
-   * @param matchingDocs
-   *          the documents that matched the query, per-segment.
-   */
-  @Override
-  public List<FacetResult> accumulate(List<MatchingDocs> matchingDocs) throws IOException {
-    // aggregate facets per category list (usually onle one category list)
-    FacetsAggregator aggregator = getAggregator();
-    for (CategoryListParams clp : groupRequests().keySet()) {
-      for (MatchingDocs md : matchingDocs) {
-        aggregator.aggregate(md, clp, facetArrays);
-      }
-    }
-    
-    ParallelTaxonomyArrays arrays = taxonomyReader.getParallelTaxonomyArrays();
-    
-    // compute top-K
-    final int[] children = arrays.children();
-    final int[] siblings = arrays.siblings();
-    List<FacetResult> res = new ArrayList<FacetResult>();
-    for (FacetRequest fr : searchParams.facetRequests) {
-      int rootOrd = taxonomyReader.getOrdinal(fr.categoryPath);
-      if (rootOrd == TaxonomyReader.INVALID_ORDINAL) { // category does not exist
-        // Add empty FacetResult
-        res.add(emptyResult(rootOrd, fr));
-        continue;
-      }
-      CategoryListParams clp = searchParams.indexingParams.getCategoryListParams(fr.categoryPath);
-      if (fr.categoryPath.length > 0) { // someone might ask to aggregate the ROOT category
-        OrdinalPolicy ordinalPolicy = clp.getOrdinalPolicy(fr.categoryPath.components[0]);
-        if (ordinalPolicy == OrdinalPolicy.NO_PARENTS) {
-          // rollup values
-          aggregator.rollupValues(fr, rootOrd, children, siblings, facetArrays);
-        }
-      }
-      
-      FacetResultsHandler frh = createFacetResultsHandler(fr, aggregator.createOrdinalValueResolver(fr, facetArrays));
-      res.add(frh.compute());
-    }
-    return res;
-  }
-
-  @Override
-  public boolean requiresDocScores() {
-    return getAggregator().requiresDocScores();
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/TopKFacetResultsHandler.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/TopKFacetResultsHandler.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/TopKFacetResultsHandler.java	2013-08-01 14:47:20.746689725 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/TopKFacetResultsHandler.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,276 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.ArrayList;
-
-import org.apache.lucene.facet.partitions.IntermediateFacetResult;
-import org.apache.lucene.facet.partitions.PartitionsFacetResultsHandler;
-import org.apache.lucene.facet.taxonomy.ParallelTaxonomyArrays;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.util.ResultSortUtils;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Generate Top-K results for a particular {@link FacetRequest}. K is global
- * (among all results) and is defined by {@link FacetRequest#numResults}.
- * 
- * @lucene.experimental
- */
-public class TopKFacetResultsHandler extends PartitionsFacetResultsHandler {
-  
-  /** Construct top-K results handler. */
-  public TopKFacetResultsHandler(TaxonomyReader taxonomyReader, FacetRequest facetRequest, 
-      OrdinalValueResolver resolver, FacetArrays facetArrays) {
-    super(taxonomyReader, facetRequest, resolver, facetArrays);
-  }
-  
-  // fetch top K for specific partition. 
-  @Override
-  public IntermediateFacetResult fetchPartitionResult(int offset)
-  throws IOException {
-    TopKFacetResult res = null;
-    int ordinal = taxonomyReader.getOrdinal(facetRequest.categoryPath);
-    if (ordinal != TaxonomyReader.INVALID_ORDINAL) {
-      double value = 0;  
-      if (isSelfPartition(ordinal, facetArrays, offset)) {
-        int partitionSize = facetArrays.arrayLength;
-        value = resolver.valueOf(ordinal % partitionSize);
-      }
-      
-      FacetResultNode parentResultNode = new FacetResultNode(ordinal, value);
-      
-      Heap<FacetResultNode> heap = ResultSortUtils.createSuitableHeap(facetRequest);
-      int totalFacets = heapDescendants(ordinal, heap, parentResultNode, offset);
-      res = new TopKFacetResult(facetRequest, parentResultNode, totalFacets);
-      res.setHeap(heap);
-    }
-    return res;
-  }
-  
-  // merge given top K results into current 
-  @Override
-  public IntermediateFacetResult mergeResults(IntermediateFacetResult... tmpResults) throws IOException {
-    
-    int ordinal = taxonomyReader.getOrdinal(facetRequest.categoryPath);
-    FacetResultNode resNode = new FacetResultNode(ordinal, 0);
-    
-    int totalFacets = 0;
-    Heap<FacetResultNode> heap = null;
-    
-    // merge other results in queue
-    for (IntermediateFacetResult tmpFres : tmpResults) {
-      // cast should succeed
-      TopKFacetResult fres = (TopKFacetResult) tmpFres;
-      totalFacets += fres.getNumValidDescendants();
-      // set the value for the result node representing the facet request
-      resNode.value += fres.getFacetResultNode().value;
-      Heap<FacetResultNode> tmpHeap = fres.getHeap();
-      if (heap == null) {
-        heap = tmpHeap;
-        continue;
-      }
-      // bring sub results from heap of tmp res into result heap
-      for (int i = tmpHeap.size(); i > 0; i--) {
-        heap.insertWithOverflow(tmpHeap.pop());
-      }
-    }
-    
-    TopKFacetResult res = new TopKFacetResult(facetRequest, resNode, totalFacets);
-    res.setHeap(heap);
-    return res;
-  }
-  
-  /**
-   * Finds the top K descendants of ordinal, which are at most facetRequest.getDepth()
-   * deeper than facetRequest.getCategoryPath (whose ordinal is input parameter ordinal). 
-   * Candidates are restricted to current "counting list" and current "partition",
-   * they join the overall priority queue pq of size K.  
-   * @return total number of descendants considered here by pq, excluding ordinal itself.
-   */
-  private int heapDescendants(int ordinal, Heap<FacetResultNode> pq, FacetResultNode parentResultNode, 
-      int offset) throws IOException {
-    int partitionSize = facetArrays.arrayLength;
-    int endOffset = offset + partitionSize;
-    ParallelTaxonomyArrays childrenArray = taxonomyReader.getParallelTaxonomyArrays();
-    int[] children = childrenArray.children();
-    int[] siblings = childrenArray.siblings();
-    FacetResultNode reusable = null;
-    int localDepth = 0;
-    int depth = facetRequest.getDepth();
-    int[] ordinalStack = new int[2+Math.min(Short.MAX_VALUE, depth)];
-    int childrenCounter = 0;
-    
-    int tosOrdinal; // top of stack element
-    
-    int yc = children[ordinal];
-    while (yc >= endOffset) {
-      yc = siblings[yc];
-    }
-    // make use of the fact that TaxonomyReader.INVALID_ORDINAL == -1, < endOffset
-    // and it, too, can stop the loop.
-    ordinalStack[++localDepth] = yc;
-    
-    /*
-     * stack holds input parameter ordinal in position 0.
-     * Other elements are < endoffset.
-     * Only top of stack can be TaxonomyReader.INVALID_ORDINAL, and this if and only if
-     * the element below it exhausted all its children: has them all processed.
-     * 
-     * stack elements are processed (counted and accumulated) only if they 
-     * belong to current partition (between offset and endoffset) and first time
-     * they are on top of stack 
-     * 
-     * loop as long as stack is not empty of elements other than input ordinal, or for a little while -- it sibling
-     */
-    while (localDepth > 0) {
-      tosOrdinal = ordinalStack[localDepth];
-      if (tosOrdinal == TaxonomyReader.INVALID_ORDINAL) {
-        // element below tos has all its children, and itself, all processed
-        // need to proceed to its sibling
-        localDepth--;
-        // change element now on top of stack to its sibling.
-        ordinalStack[localDepth] = siblings[ordinalStack[localDepth]];
-        continue;
-      }
-      // top of stack is not invalid, this is the first time we see it on top of stack.
-      // collect it, if belongs to current partition, and then push its kids on itself, if applicable
-      if (tosOrdinal >= offset) { // tosOrdinal resides in current partition
-        int relativeOrdinal = tosOrdinal % partitionSize;
-        double value = resolver.valueOf(relativeOrdinal);
-        if (value != 0 && !Double.isNaN(value)) {
-          // Count current ordinal -- the TOS
-          if (reusable == null) {
-            reusable = new FacetResultNode(tosOrdinal, value);
-          } else {
-            // it is safe to cast since reusable was created here.
-            reusable.ordinal = tosOrdinal;
-            reusable.value = value;
-            reusable.subResults.clear();
-            reusable.label = null;
-          }
-          ++childrenCounter;
-          reusable = pq.insertWithOverflow(reusable);
-        }
-      }
-      if (localDepth < depth) {
-        // push kid of current tos
-        yc = children[tosOrdinal];
-        while (yc >= endOffset) {
-          yc = siblings[yc];
-        }
-        ordinalStack[++localDepth] = yc;
-      } else { // localDepth == depth; current tos exhausted its possible children, mark this by pushing INVALID_ORDINAL
-        ordinalStack[++localDepth] = TaxonomyReader.INVALID_ORDINAL;
-      }
-    } // endof while stack is not empty
-    
-    return childrenCounter; // we're done
-  }
-  
-  @Override
-  public FacetResult renderFacetResult(IntermediateFacetResult tmpResult) {
-    TopKFacetResult res = (TopKFacetResult) tmpResult; // cast is safe by contract of this class
-    if (res != null) {
-      Heap<FacetResultNode> heap = res.getHeap();
-      FacetResultNode resNode = res.getFacetResultNode();
-      if (resNode.subResults == FacetResultNode.EMPTY_SUB_RESULTS) {
-        resNode.subResults = new ArrayList<FacetResultNode>();
-      }
-      for (int i = heap.size(); i > 0; i--) {
-        resNode.subResults.add(0, heap.pop());
-      }
-    }
-    return res;
-  }
-  
-  @Override
-  public FacetResult rearrangeFacetResult(FacetResult facetResult) {
-    TopKFacetResult res = (TopKFacetResult) facetResult; // cast is safe by contract of this class
-    Heap<FacetResultNode> heap = res.getHeap();
-    heap.clear(); // just to be safe
-    FacetResultNode topFrn = res.getFacetResultNode();
-    for (FacetResultNode frn : topFrn.subResults) {
-      heap.add(frn);
-    }
-    int size = heap.size();
-    ArrayList<FacetResultNode> subResults = new ArrayList<FacetResultNode>(size);
-    for (int i = heap.size(); i > 0; i--) {
-      subResults.add(0,heap.pop());
-    }
-    topFrn.subResults = subResults;
-    return res;
-  }
-  
-  @Override
-  public void labelResult(FacetResult facetResult) throws IOException {
-    if (facetResult != null) { // any result to label?
-      FacetResultNode facetResultNode = facetResult.getFacetResultNode();
-      if (facetResultNode != null) { // any result to label?
-        facetResultNode.label = taxonomyReader.getPath(facetResultNode.ordinal);
-        int num2label = facetRequest.getNumLabel();
-        for (FacetResultNode frn : facetResultNode.subResults) {
-          if (--num2label < 0) {
-            break;
-          }
-          frn.label = taxonomyReader.getPath(frn.ordinal);
-        }
-      }
-    }
-  }
-  
-  ////////////////////////////////////////////////////////////////////////////////////
-  ////////////////////////////////////////////////////////////////////////////////////
-  
-  /**
-   * Private Mutable implementation of result of faceted search.
-   */
-  private static class TopKFacetResult extends FacetResult implements IntermediateFacetResult {
-    
-    // TODO (Facet): is it worth to override PriorityQueue.getSentinelObject()
-    // for any of our PQs?
-    private Heap<FacetResultNode> heap; 
-    
-    /**
-     * Create a Facet Result.
-     * @param facetRequest Request for which this result was obtained.
-     * @param facetResultNode top result node for this facet result.
-     */
-    TopKFacetResult(FacetRequest facetRequest, FacetResultNode facetResultNode, int totalFacets) {
-      super(facetRequest, facetResultNode, totalFacets);
-    }
-    
-    /**
-     * @return the heap
-     */
-    public Heap<FacetResultNode> getHeap() {
-      return heap;
-    }
-    
-    /**
-     * Set the heap for this result.
-     * @param heap heap top be set.
-     */
-    public void setHeap(Heap<FacetResultNode> heap) {
-      this.heap = heap;
-    }
-    
-  }
-  
-  //////////////////////////////////////////////////////
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/search/TopKInEachNodeHandler.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/TopKInEachNodeHandler.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/search/TopKInEachNodeHandler.java	2013-08-01 14:47:20.746689725 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/search/TopKInEachNodeHandler.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,727 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.facet.collections.IntIterator;
-import org.apache.lucene.facet.collections.IntToObjectMap;
-import org.apache.lucene.facet.partitions.IntermediateFacetResult;
-import org.apache.lucene.facet.partitions.PartitionsFacetResultsHandler;
-import org.apache.lucene.facet.search.FacetRequest.SortOrder;
-import org.apache.lucene.facet.taxonomy.ParallelTaxonomyArrays;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.util.PriorityQueue;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Generates {@link FacetResult} from the {@link FacetArrays} aggregated for a
- * particular {@link FacetRequest}. The generated {@link FacetResult} is a
- * subtree of the taxonomy tree. Its root node,
- * {@link FacetResult#getFacetResultNode()}, is the facet specified by
- * {@link FacetRequest#categoryPath}, and the enumerated children,
- * {@link FacetResultNode#subResults}, of each node in that {@link FacetResult}
- * are the top K ( = {@link FacetRequest#numResults}) among its children in the
- * taxonomy. The depth (number of levels excluding the root) of the
- * {@link FacetResult} tree is specified by {@link FacetRequest#getDepth()}.
- * <p>
- * Because the number of selected children of each node is restricted, and not
- * the overall number of nodes in the {@link FacetResult}, facets not selected
- * into {@link FacetResult} might have better values, or ordinals, (typically,
- * higher counts), than facets that are selected into the {@link FacetResult}.
- * <p>
- * The generated {@link FacetResult} also provides with
- * {@link FacetResult#getNumValidDescendants()}, which returns the total number
- * of facets that are descendants of the root node, no deeper than
- * {@link FacetRequest#getDepth()}, and which have valid value. The rootnode
- * itself is not counted here. Valid value is determined by the
- * {@link FacetResultsHandler}. {@link TopKInEachNodeHandler} defines valid as
- * != 0.
- * <p>
- * <b>NOTE:</b> this code relies on the assumption that
- * {@link TaxonomyReader#INVALID_ORDINAL} == -1, a smaller value than any valid
- * ordinal.
- * 
- * @lucene.experimental
- */
-public class TopKInEachNodeHandler extends PartitionsFacetResultsHandler {
-
-  public TopKInEachNodeHandler(TaxonomyReader taxonomyReader, FacetRequest facetRequest, OrdinalValueResolver resolver, 
-      FacetArrays facetArrays) {
-    super(taxonomyReader, facetRequest, resolver, facetArrays);
-  }
-
-  /**
-   * Recursively explore all facets that can be potentially included in the
-   * {@link FacetResult} to be generated, and that belong to the given
-   * partition, so that values can be examined and collected. For each such
-   * node, gather its top K ({@link FacetRequest#numResults}) children among its
-   * children that are encountered in the given particular partition (aka
-   * current counting list).
-   * @param offset
-   *          to <code>offset</code> + the length of the count arrays within
-   *          <code>arrays</code> (exclusive)
-   * 
-   * @return {@link IntermediateFacetResult} consisting of
-   *         {@link IntToObjectMap} that maps potential {@link FacetResult}
-   *         nodes to their top K children encountered in the current partition.
-   *         Note that the mapped potential tree nodes need not belong to the
-   *         given partition, only the top K children mapped to. The aim is to
-   *         identify nodes that are certainly excluded from the
-   *         {@link FacetResult} to be eventually (after going through all the
-   *         partitions) returned by this handler, because they have K better
-   *         siblings, already identified in this partition. For the identified
-   *         excluded nodes, we only count number of their descendants in the
-   *         subtree (to be included in
-   *         {@link FacetResult#getNumValidDescendants()}), but not bother with
-   *         selecting top K in these generations, which, by definition, are,
-   *         too, excluded from the FacetResult tree.
-   * @throws IOException
-   *           in case
-   *           {@link TaxonomyReader#getOrdinal(org.apache.lucene.facet.taxonomy.CategoryPath)}
-   *           does.
-   * @see #fetchPartitionResult(int)
-   */
-  @Override
-  public IntermediateFacetResult fetchPartitionResult(int offset) throws IOException {
-
-    // get the root of the result tree to be returned, and the depth of that result tree
-    // (depth means number of node levels excluding the root). 
-    int rootNode = this.taxonomyReader.getOrdinal(facetRequest.categoryPath);
-    if (rootNode == TaxonomyReader.INVALID_ORDINAL) {
-      return null;
-    }
-
-    int K = Math.min(facetRequest.numResults,taxonomyReader.getSize()); // number of best results in each node
-
-    // this will grow into the returned IntermediateFacetResult
-    IntToObjectMap<AACO> AACOsOfOnePartition = new IntToObjectMap<AACO>();
-
-    // all partitions, except, possibly, the last, have the same length. Hence modulo is OK.
-    int partitionSize = facetArrays.arrayLength;
-
-    int depth = facetRequest.getDepth();
-
-    if (depth == 0) {
-      // Need to only have root node.
-      IntermediateFacetResultWithHash tempFRWH = new IntermediateFacetResultWithHash(
-          facetRequest, AACOsOfOnePartition);
-      if (isSelfPartition(rootNode, facetArrays, offset)) {
-        tempFRWH.isRootNodeIncluded = true;
-        tempFRWH.rootNodeValue = resolver.valueOf(rootNode % partitionSize);
-      }
-      return tempFRWH;
-    }
-
-    if (depth > Short.MAX_VALUE - 3) {
-      depth = Short.MAX_VALUE -3;
-    }
-
-    int endOffset = offset + partitionSize; // one past the largest ordinal in the partition
-    ParallelTaxonomyArrays childrenArray = taxonomyReader.getParallelTaxonomyArrays();
-    int[] children = childrenArray.children();
-    int[] siblings = childrenArray.siblings();
-    int totalNumOfDescendantsConsidered = 0; // total number of facets with value != 0, 
-    // in the tree. These include those selected as top K in each node, and all the others that
-    // were not. Not including rootNode
-
-    // the following priority queue will be used again and again for each node recursed into
-    // to select its best K children among its children encountered in the given partition
-    PriorityQueue<AggregatedCategory> pq = 
-      new AggregatedCategoryHeap(K, this.getSuitableACComparator());
-
-    // reusables will feed the priority queue in each use 
-    AggregatedCategory [] reusables = new AggregatedCategory[2+K];
-    for (int i = 0; i < reusables.length; i++) {
-      reusables[i] = new AggregatedCategory(1,0);
-    }
-
-    /*
-     * The returned map is built by a recursive visit of potential tree nodes. Nodes 
-     * determined to be excluded from the FacetResult are not recursively explored as others,
-     * they are only recursed in order to count the number of their descendants.
-     * Also, nodes that they and any of their descendants can not be mapped into facets encountered 
-     * in this partition, are, too, explored no further. These are facets whose ordinal 
-     * numbers are greater than the ordinals of the given partition. (recall that the Taxonomy
-     * maintains that a parent ordinal is smaller than any of its descendants' ordinals).  
-     * So, when scanning over all children of a potential tree node n: (1) all children with ordinal number
-     * greater than those in the given partition are skipped over, (2) among the children of n residing
-     * in this partition, the best K children are selected (using pq) for usual further recursion 
-     * and the rest (those rejected out from the pq) are only recursed for counting total number
-     * of descendants, and (3) all the children of ordinal numbers smaller than the given partition 
-     * are further explored in the usual way, since these may lead to descendants residing in this partition.
-     * 
-     * ordinalStack drives the recursive descent. 
-     * Top of stack holds the current node which we recurse from.
-     * ordinalStack[0] holds the root of the facetRequest, and
-     * it is always maintained that parent(ordianlStack[i]) = ordinalStack[i-1]. 
-     * localDepth points to the current top of ordinalStack.
-     * Only top of ordinalStack can be TaxonomyReader.INVALID_ORDINAL, and this if and only if
-     * the element below it explored all its relevant children.
-     */
-    int[] ordinalStack = new int[depth+2]; // for 0 and for invalid on top
-    ordinalStack[0] = rootNode;
-    int localDepth = 0;
-
-    /* 
-     * bestSignlingsStack[i] maintains the best K children of ordinalStack[i-1], namely,
-     * the best K siblings of ordinalStack[i], best K among those residing in the given partition.
-     * Note that the residents of ordinalStack need not belong
-     * to the current partition, only the residents of bestSignlingsStack.
-     * When exploring the children of ordianlStack[i-1] that reside in the current partition
-     * (after the top K of them have been determined and stored into bestSignlingsStack[i]),
-     * siblingExplored[i] points into bestSignlingsStack[i], to the child now explored, hence
-     * residing in ordinalStack[i], and firstToTheLeftOfPartition[i] holds the largest ordinal of
-     * a sibling smaller than the ordinals in the partition.  
-     * When siblingExplored[i] == max int, the top K siblings of ordinalStack[i] among those siblings
-     * that reside in this partition have not been determined yet. 
-     * if siblingExplored[i] < 0, the node in ordinalStack[i] is to the left of partition 
-     * (i.e. of a smaller ordinal than the current partition) 
-     * (step (3) above is executed for the children of ordianlStack[i-1])   
-     */
-    int[][] bestSignlingsStack = new int[depth+2][];
-    int[] siblingExplored = new int[depth+2];
-    int[] firstToTheLeftOfPartition = new int [depth+2];
-
-    int tosOrdinal; // top of stack element, the ordinal at the top of stack
-
-    /*
-     * to start the loop, complete the datastructures for root node: 
-     * push its youngest child to ordinalStack; make a note in siblingExplored[] that the children
-     * of rootNode, which reside in the current partition have not been read yet to select the top
-     * K of them.  Also, make rootNode as if, related to its parent, rootNode belongs to the children
-     * of ordinal numbers smaller than those of the current partition (this will ease on end condition -- 
-     * we can continue to the older sibling of rootNode once the localDepth goes down, before we verify that 
-     * it went that down)
-     */
-    ordinalStack[++localDepth] = children[rootNode];
-    siblingExplored[localDepth] = Integer.MAX_VALUE;  // we have not verified position wrt current partition
-    siblingExplored[0] = -1; // as if rootNode resides to the left of current position
-
-    /*
-     * now the whole recursion: loop as long as stack is not empty of elements descendants of 
-     * facetRequest's root.
-     */
-
-    while (localDepth > 0) {
-      tosOrdinal = ordinalStack[localDepth];
-      if (tosOrdinal == TaxonomyReader.INVALID_ORDINAL) {
-        // the brotherhood that has been occupying the top of stack is all exhausted.  
-        // Hence, element below tos, namely, father of tos, has all its children, 
-        // and itself, all explored. 
-        localDepth--;
-        // replace this father, now on top of stack, by this father's sibling:
-        // this parent's ordinal can not be greater than current partition, as otherwise
-        // its child, now just removed, would not have been pushed on it.
-        // so the father is either inside the partition, or smaller ordinal
-        if (siblingExplored[localDepth] < 0 ) {
-          ordinalStack[localDepth] = siblings[ordinalStack[localDepth]];
-          continue;
-        } 
-        // in this point, siblingExplored[localDepth] between 0 and number of bestSiblings
-        // it can not be max int
-        siblingExplored[localDepth]--;
-        if (siblingExplored[localDepth] == -1 ) {
-          //siblings residing in the partition have been all processed, we now move
-          // to those of ordinal numbers smaller than the partition
-          ordinalStack[localDepth] = firstToTheLeftOfPartition[localDepth];
-        } else {
-          // still explore siblings residing in the partition
-          // just move to the next one
-          ordinalStack[localDepth] = bestSignlingsStack[localDepth][siblingExplored[localDepth]];
-        }
-        continue;
-      } // endof tosOrdinal is invalid, and hence removed, and its parent was replaced by this 
-      // parent's sibling
-
-      // now try to push a kid, but first look at tos whether it 'deserves' its kids explored:
-      // it is not to the right of current partition, and we know whether to only count or to 
-      // select best K siblings.
-      if (siblingExplored[localDepth] == Integer.MAX_VALUE) {
-        //tosOrdinal was not examined yet for its position relative to current partition
-        // and the best K of current partition, among its siblings, have not been determined yet
-        while (tosOrdinal >= endOffset) {
-          tosOrdinal = siblings[tosOrdinal];
-        }
-        // now it is inside. Run it and all its siblings inside the partition through a heap
-        // and in doing so, count them, find best K
-        pq.clear();
-
-        //reusables are consumed as from a stack. The stack starts full and returns full.
-        int tosReuslables = reusables.length -1;  
-
-        while (tosOrdinal >= offset) { // while tosOrdinal belongs to the given partition; here, too, we use the fact
-          // that TaxonomyReader.INVALID_ORDINAL == -1 < offset
-          double value = resolver.valueOf(tosOrdinal % partitionSize);
-          if (value != 0) { // the value of yc is not 0, it is to be considered.  
-            totalNumOfDescendantsConsidered++;
-
-            // consume one reusable, and push to the priority queue
-            AggregatedCategory ac = reusables[tosReuslables--];  
-            ac.ordinal = tosOrdinal;
-            ac.value = value; 
-            ac = pq.insertWithOverflow(ac);
-            if (null != ac) {
-              /* when a facet is excluded from top K, because already in this partition it has
-               * K better siblings, it is only recursed for count only.
-               */ 
-              // update totalNumOfDescendants by the now excluded node and all its descendants
-              totalNumOfDescendantsConsidered--; // reduce the 1 earned when the excluded node entered the heap
-              // and now return it and all its descendants. These will never make it to FacetResult
-              totalNumOfDescendantsConsidered += countOnly (ac.ordinal, children, 
-                  siblings, partitionSize, offset, endOffset, localDepth, depth);
-              reusables[++tosReuslables] = ac;
-            }
-          }
-          tosOrdinal = siblings[tosOrdinal];  
-        }
-        // now pq has best K children of ordinals that belong to the given partition.   
-        // Populate a new AACO with them.
-        // tosOrdinal is now first sibling smaller than partition, make a note of that
-        firstToTheLeftOfPartition[localDepth] = tosOrdinal;
-        int aaci = pq.size();
-        int[] ords = new int[aaci];
-        double [] vals = new double [aaci];
-        while (aaci > 0) {
-          AggregatedCategory ac = pq.pop();
-          ords[--aaci] = ac.ordinal;
-          vals[aaci] = ac.value;
-          reusables[++tosReuslables] = ac;
-        }
-        // if more than 0 ordinals, add this AACO to the map to be returned, 
-        // and add ords to sibling stack, and make a note in siblingExplored that these are to 
-        // be visited now
-        if (ords.length > 0) {
-          AACOsOfOnePartition.put(ordinalStack[localDepth-1], new AACO(ords,vals));
-          bestSignlingsStack[localDepth] = ords;
-          siblingExplored[localDepth] = ords.length-1;
-          ordinalStack[localDepth] = ords[ords.length-1];
-        } else {
-          // no ordinals siblings of tosOrdinal in current partition, move to the left of it
-          // tosOrdinal is already there (to the left of partition).
-          // make a note of it in siblingExplored
-          ordinalStack[localDepth] = tosOrdinal;
-          siblingExplored[localDepth] = -1;
-        }
-        continue;
-      } // endof we did not check the position of a valid ordinal wrt partition
-
-      // now tosOrdinal is a valid ordinal, inside partition or to the left of it, we need 
-      // to push its kids on top of it, if not too deep. 
-      // Make a note that we did not check them yet
-      if (localDepth >= depth) { 
-        // localDepth == depth; current tos exhausted its possible children, mark this by pushing INVALID_ORDINAL
-        ordinalStack[++localDepth] = TaxonomyReader.INVALID_ORDINAL;
-        continue;
-      }
-      ordinalStack[++localDepth] = children[tosOrdinal];
-      siblingExplored[localDepth] = Integer.MAX_VALUE;
-    } // endof loop while stack is not empty
-
-    // now generate a TempFacetResult from AACOsOfOnePartition, and consider self.
-    IntermediateFacetResultWithHash tempFRWH = new IntermediateFacetResultWithHash(
-        facetRequest, AACOsOfOnePartition);
-    if (isSelfPartition(rootNode, facetArrays, offset)) {
-      tempFRWH.isRootNodeIncluded = true;
-      tempFRWH.rootNodeValue = resolver.valueOf(rootNode % partitionSize);
-    }
-    tempFRWH.totalNumOfFacetsConsidered = totalNumOfDescendantsConsidered;
-    return tempFRWH;
-
-  }
-
-  /**
-   * Recursively count <code>ordinal</code>, whose depth is <code>currentDepth</code>, 
-   * and all its descendants down to <code>maxDepth</code> (including), 
-   * descendants whose value in the count arrays, <code>arrays</code>, is != 0. 
-   * The count arrays only includes the current partition, from <code>offset</code>, to (exclusive) 
-   * <code>endOffset</code>.
-   * It is assumed that <code>ordinal</code> < <code>endOffset</code>, 
-   * otherwise, not <code>ordinal</code>, and none of its descendants, reside in
-   * the current partition. <code>ordinal</code> < <code>offset</code> is allowed, 
-   * as ordinal's descendants might be >= <code>offeset</code>.
-   * 
-   * @param ordinal a facet ordinal. 
-   * @param youngestChild mapping a given ordinal to its youngest child in the taxonomy (of largest ordinal number),
-   * or to -1 if has no children.  
-   * @param olderSibling  mapping a given ordinal to its older sibling, or to -1
-   * @param partitionSize  number of ordinals in the given partition
-   * @param offset  the first (smallest) ordinal in the given partition
-   * @param endOffset one larger than the largest ordinal that belong to this partition
-   * @param currentDepth the depth or ordinal in the TaxonomyTree (relative to rootnode of the facetRequest)
-   * @param maxDepth maximal depth of descendants to be considered here (measured relative to rootnode of the 
-   * facetRequest).
-   * @return the number of nodes, from ordinal down its descendants, of depth <= maxDepth,
-   * which reside in the current partition, and whose value != 0
-   */
-  private int countOnly(int ordinal, int[] youngestChild, int[] olderSibling, int partitionSize, int offset, 
-      int endOffset, int currentDepth, int maxDepth) {
-    int ret = 0;
-    if (offset <= ordinal) {
-      // ordinal belongs to the current partition
-      if (0 != resolver.valueOf(ordinal % partitionSize)) {
-        ret++;
-      }
-    }
-    // now consider children of ordinal, if not too deep
-    if (currentDepth >= maxDepth) {
-      return ret;
-    }
-
-    int yc = youngestChild[ordinal];
-    while (yc >= endOffset) {
-      yc = olderSibling[yc];
-    }
-    while (yc > TaxonomyReader.INVALID_ORDINAL) { // assuming this is -1, smaller than any legal ordinal
-      ret += countOnly (yc, youngestChild, olderSibling, partitionSize, 
-          offset, endOffset, currentDepth+1, maxDepth);
-      yc = olderSibling[yc];
-    }
-    return ret;
-  }
-
-  /**
-   * Merge several partitions' {@link IntermediateFacetResult}-s into one of the
-   * same format
-   * 
-   * @see #mergeResults(IntermediateFacetResult...)
-   */
-  @Override
-  public IntermediateFacetResult mergeResults(IntermediateFacetResult... tmpResults) {
-
-    if (tmpResults.length == 0) {
-      return null;
-    }
-
-    int i=0;
-    // skip over null tmpResults
-    for (; (i < tmpResults.length)&&(tmpResults[i] == null); i++) {}
-    if (i == tmpResults.length) {
-      // all inputs are null
-      return null;
-    }
-
-    // i points to the first non-null input 
-    int K = this.facetRequest.numResults; // number of best result in each node
-    IntermediateFacetResultWithHash tmpToReturn = (IntermediateFacetResultWithHash)tmpResults[i++];
-
-    // now loop over the rest of tmpResults and merge each into tmpToReturn
-    for ( ; i < tmpResults.length; i++) {
-      IntermediateFacetResultWithHash tfr = (IntermediateFacetResultWithHash)tmpResults[i];
-      tmpToReturn.totalNumOfFacetsConsidered += tfr.totalNumOfFacetsConsidered;
-      if (tfr.isRootNodeIncluded) {
-        tmpToReturn.isRootNodeIncluded = true;
-        tmpToReturn.rootNodeValue = tfr.rootNodeValue;
-      }
-      // now merge the HashMap of tfr into this of tmpToReturn
-      IntToObjectMap<AACO> tmpToReturnMapToACCOs = tmpToReturn.mapToAACOs;
-      IntToObjectMap<AACO> tfrMapToACCOs = tfr.mapToAACOs;
-      IntIterator tfrIntIterator = tfrMapToACCOs.keyIterator();
-      //iterate over all ordinals in tfr that are maps to their children
-      while (tfrIntIterator.hasNext()) {
-        int tfrkey = tfrIntIterator.next();
-        AACO tmpToReturnAACO = null;
-        if (null == (tmpToReturnAACO = tmpToReturnMapToACCOs.get(tfrkey))) {
-          // if tmpToReturn does not have any kids of tfrkey, map all the kids
-          // from tfr to it as one package, along with their redisude
-          tmpToReturnMapToACCOs.put(tfrkey, tfrMapToACCOs.get(tfrkey));
-        } else {
-          // merge the best K children of tfrkey as appear in tmpToReturn and in tfr
-          AACO tfrAACO = tfrMapToACCOs.get(tfrkey);
-          int resLength = tfrAACO.ordinals.length + tmpToReturnAACO.ordinals.length;
-          if (K < resLength) {
-            resLength = K;
-          }
-          int[] resOrds = new int [resLength];
-          double[] resVals = new double [resLength];
-          int indexIntoTmpToReturn = 0;
-          int indexIntoTFR = 0;
-          ACComparator merger = getSuitableACComparator(); // by facet Request
-          for (int indexIntoRes = 0; indexIntoRes < resLength; indexIntoRes++) {
-            if (indexIntoTmpToReturn >= tmpToReturnAACO.ordinals.length) {
-              //tmpToReturnAACO (former result to return) ran out of indices
-              // it is all merged into resOrds and resVal
-              resOrds[indexIntoRes] = tfrAACO.ordinals[indexIntoTFR];
-              resVals[indexIntoRes] = tfrAACO.values[indexIntoTFR];
-              indexIntoTFR++;
-              continue;
-            }
-            if (indexIntoTFR >= tfrAACO.ordinals.length) {
-              // tfr ran out of indices
-              resOrds[indexIntoRes] = tmpToReturnAACO.ordinals[indexIntoTmpToReturn];
-              resVals[indexIntoRes] = tmpToReturnAACO.values[indexIntoTmpToReturn];
-              indexIntoTmpToReturn++;
-              continue;
-            }
-            // select which goes now to res: next (ord, value) from tmpToReturn or from tfr:
-            if (merger.leftGoesNow(  tmpToReturnAACO.ordinals[indexIntoTmpToReturn], 
-                tmpToReturnAACO.values[indexIntoTmpToReturn], 
-                tfrAACO.ordinals[indexIntoTFR], 
-                tfrAACO.values[indexIntoTFR])) {
-              resOrds[indexIntoRes] = tmpToReturnAACO.ordinals[indexIntoTmpToReturn];
-              resVals[indexIntoRes] = tmpToReturnAACO.values[indexIntoTmpToReturn];
-              indexIntoTmpToReturn++;
-            } else {
-              resOrds[indexIntoRes] = tfrAACO.ordinals[indexIntoTFR];
-              resVals[indexIntoRes] = tfrAACO.values[indexIntoTFR];
-              indexIntoTFR++;
-            }
-          } // end of merge of best kids of tfrkey that appear in tmpToReturn and its kids that appear in tfr
-          // altogether yielding no more that best K kids for tfrkey, not to appear in the new shape of 
-          // tmpToReturn
-
-          //update the list of best kids of tfrkey as appear in tmpToReturn
-          tmpToReturnMapToACCOs.put(tfrkey, new AACO(resOrds, resVals));
-        } // endof need to merge both AACO -- children for same ordinal
-
-      } // endof loop over all ordinals in tfr 
-    } // endof loop over all temporary facet results to merge
-
-    return tmpToReturn;
-  }
-
-  private static class AggregatedCategoryHeap extends PriorityQueue<AggregatedCategory> {
-    
-    private ACComparator merger;
-    public AggregatedCategoryHeap(int size, ACComparator merger) {
-      super(size);
-      this.merger = merger;
-    }
-
-    @Override
-    protected boolean lessThan(AggregatedCategory arg1, AggregatedCategory arg2) {
-      return merger.leftGoesNow(arg2.ordinal, arg2.value, arg1.ordinal, arg1.value);
-    }
-
-  }
-
-  private static class ResultNodeHeap extends PriorityQueue<FacetResultNode> {
-    private ACComparator merger;
-    public ResultNodeHeap(int size, ACComparator merger) {
-      super(size);
-      this.merger = merger;
-    }
-
-    @Override
-    protected boolean lessThan(FacetResultNode arg1, FacetResultNode arg2) {
-      return merger.leftGoesNow(arg2.ordinal, arg2.value, arg1.ordinal, arg1.value);
-    }
-
-  }
-
-  /**
-   * @return the {@link ACComparator} that reflects the order,
-   * expressed in the {@link FacetRequest}, of 
-   * facets in the {@link FacetResult}. 
-   */
-
-  private ACComparator getSuitableACComparator() {
-    if (facetRequest.getSortOrder() == SortOrder.ASCENDING) {
-      return new AscValueACComparator();
-    } else {
-      return new DescValueACComparator();
-    }
-  }
-
-  /**
-   * A comparator of two Aggregated Categories according to the order
-   * (ascending / descending) and item (ordinal or value) specified in the 
-   * FacetRequest for the FacetResult to be generated
-   */
-
-  private static abstract class ACComparator {
-    ACComparator() { }
-    protected abstract boolean leftGoesNow (int ord1, double val1, int ord2, double val2); 
-  }
-
-  private static final class AscValueACComparator extends ACComparator {
-    
-    AscValueACComparator() { }
-    
-    @Override
-    protected boolean leftGoesNow (int ord1, double val1, int ord2, double val2) {
-      return (val1 == val2) ? (ord1 < ord2) : (val1 < val2);
-    }
-  }
-
-  private static final class DescValueACComparator extends ACComparator {
-    
-    DescValueACComparator() { }
-    
-    @Override
-    protected boolean leftGoesNow (int ord1, double val1, int ord2, double val2) {
-      return (val1 == val2) ? (ord1 > ord2) : (val1 > val2);
-    }
-  }
-
-  /**
-   * Intermediate result to hold counts from one or more partitions processed
-   * thus far. Its main field, constructor parameter <i>mapToAACOs</i>, is a map
-   * from ordinals to AACOs. The AACOs mapped to contain ordinals and values
-   * encountered in the count arrays of the partitions processed thus far. The
-   * ordinals mapped from are their parents, and they may be not contained in
-   * the partitions processed thus far. All nodes belong to the taxonomy subtree
-   * defined at the facet request, constructor parameter <i>facetReq</i>, by its
-   * root and depth.
-   */
-  public static class IntermediateFacetResultWithHash implements IntermediateFacetResult {
-    protected IntToObjectMap<AACO> mapToAACOs;
-    FacetRequest facetRequest;
-    boolean isRootNodeIncluded; // among the ordinals in the partitions 
-    // processed thus far
-    double rootNodeValue; // the value of it, in case encountered.
-    int totalNumOfFacetsConsidered; // total number of facets 
-    // which belong to facetRequest subtree and have value != 0,
-    // and have been encountered thus far in the partitions processed. 
-    // root node of result tree is not included in this count.
-
-    public IntermediateFacetResultWithHash(FacetRequest facetReq,
-                                    IntToObjectMap<AACO> mapToAACOs) {
-      this.mapToAACOs = mapToAACOs;
-      this.facetRequest = facetReq;
-      this.isRootNodeIncluded = false;
-      this.rootNodeValue = 0.0;
-      this.totalNumOfFacetsConsidered = 0;
-    }
-
-    @Override
-    public FacetRequest getFacetRequest() {
-      return this.facetRequest;
-    }
-  } // endof FacetResultWithHash
-
-  /**
-   * Maintains info of one entry in the filled up count array:
-   * an ordinal number of a category and the value aggregated for it 
-   * (typically, that value is the count for that ordinal).
-   */
-  private static final class AggregatedCategory {
-    int ordinal;
-    double value;
-    AggregatedCategory(int ord, double val) {
-      this.ordinal = ord;
-      this.value = val;
-    }
-  }
-
-  /**
-   * Maintains an array of <code>AggregatedCategory</code>. For space consideration, this is implemented as 
-   * a pair of arrays, <i>ordinals</i> and <i>values</i>, rather than one array of pairs.
-   * Enumerated in <i>ordinals</i> are siblings,  
-   * potential nodes of the {@link FacetResult} tree  
-   * (i.e., the descendants of the root node, no deeper than the specified depth).
-   * No more than K ( = {@link FacetRequest#numResults}) 
-   * siblings are enumerated.
-   * @lucene.internal
-   */
-  protected static final class AACO {
-    int [] ordinals; // ordinals of the best K children, sorted from best to least
-    double [] values; // the respective values for these children
-    AACO (int[] ords, double[] vals) {
-      this.ordinals = ords;
-      this.values = vals;
-    }
-  }
-
-  @Override
-  public void labelResult(FacetResult facetResult) throws IOException {
-    if (facetResult == null) {
-      return; // any result to label?
-    }
-    FacetResultNode rootNode = facetResult.getFacetResultNode();
-    recursivelyLabel(rootNode, facetRequest.getNumLabel());
-  }
-
-  private void recursivelyLabel(FacetResultNode node, int numToLabel) throws IOException {
-    if (node == null) {
-      return;
-    }
-    node.label = taxonomyReader.getPath(node.ordinal);
-
-    // recursively label the first numToLabel children of every node
-    int numLabeled = 0;
-    for (FacetResultNode frn : node.subResults) { 
-      recursivelyLabel(frn, numToLabel);
-      if (++numLabeled >= numToLabel) {
-        return;
-      }
-    }
-  }
-
-  @Override
-  // verifies that the children of each node are sorted by the order
-  // specified by the facetRequest.
-  // the values in these nodes may have changed due to a re-count, for example
-  // following the accumulation by Sampling.
-  // so now we test and re-order if necessary.
-  public FacetResult rearrangeFacetResult(FacetResult facetResult) {
-    PriorityQueue<FacetResultNode> nodesHeap = 
-      new ResultNodeHeap(this.facetRequest.numResults, this.getSuitableACComparator());
-    FacetResultNode topFrn = facetResult.getFacetResultNode();
-    rearrangeChilrenOfNode(topFrn, nodesHeap);
-    return facetResult;
-  }
-
-  private void rearrangeChilrenOfNode(FacetResultNode node, PriorityQueue<FacetResultNode> nodesHeap) {
-    nodesHeap.clear(); // just to be safe
-    for (FacetResultNode frn : node.subResults) {
-      nodesHeap.add(frn);
-    }
-    int size = nodesHeap.size();
-    ArrayList<FacetResultNode> subResults = new ArrayList<FacetResultNode>(size);
-    while (nodesHeap.size() > 0) {
-      subResults.add(0, nodesHeap.pop());
-    }
-    node.subResults = subResults;
-    for (FacetResultNode frn : node.subResults) {
-      rearrangeChilrenOfNode(frn, nodesHeap);
-    }
-
-  }
-
-  @Override
-  public FacetResult renderFacetResult(IntermediateFacetResult tmpResult) throws IOException {
-    IntermediateFacetResultWithHash tmp = (IntermediateFacetResultWithHash) tmpResult;
-    int ordinal = this.taxonomyReader.getOrdinal(this.facetRequest.categoryPath);
-    if ((tmp == null) || (ordinal == TaxonomyReader.INVALID_ORDINAL)) {
-      return null;
-    }
-    double value = Double.NaN;
-    if (tmp.isRootNodeIncluded) {
-      value = tmp.rootNodeValue;
-    }
-    FacetResultNode root = generateNode(ordinal, value, tmp.mapToAACOs);
-    return new FacetResult(tmp.facetRequest, root, tmp.totalNumOfFacetsConsidered);
-  }
-
-  private FacetResultNode generateNode(int ordinal, double val,  IntToObjectMap<AACO> mapToAACOs) {
-    FacetResultNode node = new FacetResultNode(ordinal, val);
-    AACO aaco = mapToAACOs.get(ordinal);
-    if (null == aaco) {
-      return node;
-    }
-    List<FacetResultNode> list = new ArrayList<FacetResultNode>();
-    for (int i = 0; i < aaco.ordinals.length; i++) {
-      list.add(generateNode(aaco.ordinals[i], aaco.values[i], mapToAACOs));
-    }
-    node.subResults = list;
-    return node;  
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/SearcherTaxonomyManager.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/SearcherTaxonomyManager.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/SearcherTaxonomyManager.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/SearcherTaxonomyManager.java	2013-11-26 10:50:16.779028832 -0500
@@ -0,0 +1,123 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.ReferenceManager;
+import org.apache.lucene.search.SearcherFactory;
+import org.apache.lucene.search.SearcherManager;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Manages near-real-time reopen of both an IndexSearcher
+ * and a TaxonomyReader.
+ *
+ * <p><b>NOTE</b>: If you call {@link
+ * DirectoryTaxonomyWriter#replaceTaxonomy} then you must
+ * open a new {@code SearcherTaxonomyManager} afterwards.
+ */
+public class SearcherTaxonomyManager extends ReferenceManager<SearcherTaxonomyManager.SearcherAndTaxonomy> {
+
+  /** Holds a matched pair of {@link IndexSearcher} and
+   *  {@link TaxonomyReader} */
+  public static class SearcherAndTaxonomy {
+    public final IndexSearcher searcher;
+    public final DirectoryTaxonomyReader taxonomyReader;
+
+    /** Create a SearcherAndTaxonomy */
+    public SearcherAndTaxonomy(IndexSearcher searcher, DirectoryTaxonomyReader taxonomyReader) {
+      this.searcher = searcher;
+      this.taxonomyReader = taxonomyReader;
+    }
+  }
+
+  private final SearcherFactory searcherFactory;
+  private final long taxoEpoch;
+  private final DirectoryTaxonomyWriter taxoWriter;
+
+  /** Creates near-real-time searcher and taxonomy reader
+   *  from the corresponding writers. */
+  public SearcherTaxonomyManager(IndexWriter writer, boolean applyAllDeletes, SearcherFactory searcherFactory, DirectoryTaxonomyWriter taxoWriter) throws IOException {
+    if (searcherFactory == null) {
+      searcherFactory = new SearcherFactory();
+    }
+    this.searcherFactory = searcherFactory;
+    this.taxoWriter = taxoWriter;
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    current = new SearcherAndTaxonomy(SearcherManager.getSearcher(searcherFactory, DirectoryReader.open(writer, applyAllDeletes)),
+                                      taxoReader);
+    taxoEpoch = taxoWriter.getTaxonomyEpoch();
+  }
+
+  @Override
+  protected void decRef(SearcherAndTaxonomy ref) throws IOException {
+    ref.searcher.getIndexReader().decRef();
+
+    // This decRef can fail, and then in theory we should
+    // tryIncRef the searcher to put back the ref count
+    // ... but 1) the below decRef should only fail because
+    // it decRef'd to 0 and closed and hit some IOException
+    // during close, in which case 2) very likely the
+    // searcher was also just closed by the above decRef and
+    // a tryIncRef would fail:
+    ref.taxonomyReader.decRef();
+  }
+
+  @Override
+  protected boolean tryIncRef(SearcherAndTaxonomy ref) throws IOException {
+    if (ref.searcher.getIndexReader().tryIncRef()) {
+      if (ref.taxonomyReader.tryIncRef()) {
+        return true;
+      } else {
+        ref.searcher.getIndexReader().decRef();
+      }
+    }
+    return false;
+  }
+
+  @Override
+  protected SearcherAndTaxonomy refreshIfNeeded(SearcherAndTaxonomy ref) throws IOException {
+    // Must re-open searcher first, otherwise we may get a
+    // new reader that references ords not yet known to the
+    // taxonomy reader:
+    final IndexReader r = ref.searcher.getIndexReader();
+    final IndexReader newReader = DirectoryReader.openIfChanged((DirectoryReader) r);
+    if (newReader == null) {
+      return null;
+    } else {
+      DirectoryTaxonomyReader tr = TaxonomyReader.openIfChanged(ref.taxonomyReader);
+      if (tr == null) {
+        ref.taxonomyReader.incRef();
+        tr = ref.taxonomyReader;
+      } else if (taxoWriter.getTaxonomyEpoch() != taxoEpoch) {
+        IOUtils.close(newReader, tr);
+        throw new IllegalStateException("DirectoryTaxonomyWriter.replaceTaxonomy was called, which is not allowed when using SearcherTaxonomyManager");
+      }
+
+      return new SearcherAndTaxonomy(SearcherManager.getSearcher(searcherFactory, newReader), tr);
+    }
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/sortedset/package.html simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sortedset/package.html
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/sortedset/package.html	2013-03-20 06:26:07.051245399 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sortedset/package.html	1969-12-31 19:00:00.000000000 -0500
@@ -1,24 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Classes to perform faceting without a separate taxonomy index, using on SortedSetDocValuesField</title>
-</head>
-<body>
-Classes to perform faceting without a separate taxonomy index, using on SortedSetDocValuesField.
-</body>
-</html>


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesAccumulator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesAccumulator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesAccumulator.java	2013-10-17 13:16:00.928867121 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesAccumulator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,324 +0,0 @@
-package org.apache.lucene.facet.sortedset;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.List;
-
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.CountFacetRequest;
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.search.FacetsAccumulator;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiDocValues;
-import org.apache.lucene.index.MultiDocValues.MultiSortedSetDocValues;
-import org.apache.lucene.index.ReaderUtil;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.PriorityQueue;
-
-/** A {@link FacetsAccumulator} that uses previously
- *  indexed {@link SortedSetDocValuesFacetFields} to perform faceting,
- *  without require a separate taxonomy index.  Faceting is
- *  a bit slower (~25%), and there is added cost on every
- *  {@link IndexReader} open to create a new {@link
- *  SortedSetDocValuesReaderState}.  Furthermore, this does
- *  not support hierarchical facets; only flat (dimension +
- *  label) facets, but it uses quite a bit less RAM to do so. */
-public class SortedSetDocValuesAccumulator extends FacetsAccumulator {
-
-  final SortedSetDocValuesReaderState state;
-  final SortedSetDocValues dv;
-  final String field;
-  final FacetArrays facetArrays;
-  
-  /** Constructor with the given facet search params. */
-  public SortedSetDocValuesAccumulator(SortedSetDocValuesReaderState state, FacetSearchParams fsp) 
-      throws IOException {
-    this(state, fsp, null);
-  }
-  
-  public SortedSetDocValuesAccumulator(SortedSetDocValuesReaderState state, FacetSearchParams fsp, FacetArrays arrays) 
-      throws IOException {
-    super(fsp);
-    this.state = state;
-    this.field = state.getField();
-    this.facetArrays = arrays == null ? new FacetArrays(state.getSize()) : arrays;
-    dv = state.getDocValues();
-
-    // Check params:
-    for (FacetRequest fr : fsp.facetRequests) {
-      if (!(fr instanceof CountFacetRequest)) {
-        throw new IllegalArgumentException("this accumulator only supports CountFacetRequest; got " + fr);
-      }
-      if (fr.categoryPath.length != 1) {
-        throw new IllegalArgumentException("this accumulator only supports 1-level CategoryPath; got " + fr.categoryPath);
-      }
-      if (fr.getDepth() != 1) {
-        throw new IllegalArgumentException("this accumulator only supports depth=1; got " + fr.getDepth());
-      }
-      String dim = fr.categoryPath.components[0];
-
-      SortedSetDocValuesReaderState.OrdRange ordRange = state.getOrdRange(dim);
-      if (ordRange == null) {
-        throw new IllegalArgumentException("dim \"" + dim + "\" does not exist");
-      }
-    }
-  }
-
-  /** Keeps highest count results. */
-  static class TopCountPQ extends PriorityQueue<FacetResultNode> {
-    public TopCountPQ(int topN) {
-      super(topN, false);
-    }
-
-    @Override
-    protected boolean lessThan(FacetResultNode a, FacetResultNode b) {
-      if (a.value < b.value) {
-        return true;
-      } else if (a.value > b.value) {
-        return false;
-      } else {
-        return a.ordinal > b.ordinal;
-      }
-    }
-  }
-
-  static class SortedSetAggregator {
-
-    private final SortedSetDocValuesReaderState state;
-    private final String field;
-    private final SortedSetDocValues dv;
-    
-    public SortedSetAggregator(String field, SortedSetDocValuesReaderState state, SortedSetDocValues dv) {
-      this.field = field;
-      this.state = state;
-      this.dv = dv;
-    }
-    
-    public void aggregate(MatchingDocs matchingDocs, FacetArrays facetArrays) throws IOException {
-
-      AtomicReader reader = matchingDocs.context.reader();
-
-      // LUCENE-5090: make sure the provided reader context "matches"
-      // the top-level reader passed to the
-      // SortedSetDocValuesReaderState, else cryptic
-      // AIOOBE can happen:
-      if (ReaderUtil.getTopLevelContext(matchingDocs.context).reader() != state.origReader) {
-        throw new IllegalStateException("the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader");
-      }
-      
-      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);
-      if (segValues == null) {
-        return;
-      }
-
-      final int[] counts = facetArrays.getIntArray();
-      final int maxDoc = reader.maxDoc();
-      assert maxDoc == matchingDocs.bits.length();
-
-      if (dv instanceof MultiSortedSetDocValues) {
-        MultiDocValues.OrdinalMap ordinalMap = ((MultiSortedSetDocValues) dv).mapping;
-        int segOrd = matchingDocs.context.ord;
-
-        int numSegOrds = (int) segValues.getValueCount();
-
-        if (matchingDocs.totalHits < numSegOrds/10) {
-          // Remap every ord to global ord as we iterate:
-          int doc = 0;
-          while (doc < maxDoc && (doc = matchingDocs.bits.nextSetBit(doc)) != -1) {
-            segValues.setDocument(doc);
-            int term = (int) segValues.nextOrd();
-            while (term != SortedSetDocValues.NO_MORE_ORDS) {
-              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;
-              term = (int) segValues.nextOrd();
-            }
-            ++doc;
-          }
-        } else {
-
-          // First count in seg-ord space:
-          final int[] segCounts = new int[numSegOrds];
-          int doc = 0;
-          while (doc < maxDoc && (doc = matchingDocs.bits.nextSetBit(doc)) != -1) {
-            segValues.setDocument(doc);
-            int term = (int) segValues.nextOrd();
-            while (term != SortedSetDocValues.NO_MORE_ORDS) {
-              segCounts[term]++;
-              term = (int) segValues.nextOrd();
-            }
-            ++doc;
-          }
-
-          // Then, migrate to global ords:
-          for(int ord=0;ord<numSegOrds;ord++) {
-            int count = segCounts[ord];
-            if (count != 0) {
-              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;
-            }
-          }
-        }
-      } else {
-        // No ord mapping (e.g., single segment index):
-        // just aggregate directly into counts:
-
-        int doc = 0;
-        while (doc < maxDoc && (doc = matchingDocs.bits.nextSetBit(doc)) != -1) {
-          segValues.setDocument(doc);
-          int term = (int) segValues.nextOrd();
-          while (term != SortedSetDocValues.NO_MORE_ORDS) {
-            counts[term]++;
-            term = (int) segValues.nextOrd();
-          }
-          ++doc;
-        }
-      }
-    }
-
-  }
-  
-  @Override
-  public List<FacetResult> accumulate(List<MatchingDocs> matchingDocs) throws IOException {
-
-    SortedSetAggregator aggregator = new SortedSetAggregator(field, state, dv);
-    for (MatchingDocs md : matchingDocs) {
-      aggregator.aggregate(md, facetArrays);
-    }
-
-    // compute top-K
-    List<FacetResult> results = new ArrayList<FacetResult>();
-
-    int[] counts = facetArrays.getIntArray();
-
-    BytesRef scratch = new BytesRef();
-
-    for (FacetRequest request : searchParams.facetRequests) {
-      String dim = request.categoryPath.components[0];
-      SortedSetDocValuesReaderState.OrdRange ordRange = state.getOrdRange(dim);
-      // checked in ctor:
-      assert ordRange != null;
-
-      if (request.numResults >= ordRange.end - ordRange.start + 1) {
-        // specialize this case, user is interested in all available results
-        ArrayList<FacetResultNode> nodes = new ArrayList<FacetResultNode>();
-        int dimCount = 0;
-        for(int ord=ordRange.start; ord<=ordRange.end; ord++) {
-          //System.out.println("  ord=" + ord + " count= "+ counts[ord] + " bottomCount=" + bottomCount);
-          if (counts[ord] != 0) {
-            dimCount += counts[ord];
-            FacetResultNode node = new FacetResultNode(ord, counts[ord]);
-            dv.lookupOrd(ord, scratch);
-            node.label = new CategoryPath(scratch.utf8ToString().split(state.separatorRegex, 2));
-            nodes.add(node);
-          }
-        }
-
-        Collections.sort(nodes, new Comparator<FacetResultNode>() {
-            @Override
-            public int compare(FacetResultNode o1, FacetResultNode o2) {
-              // First by highest count
-              int value = (int) (o2.value - o1.value);
-              if (value == 0) {
-                // ... then by lowest ord:
-                value = o1.ordinal - o2.ordinal;
-              }
-              return value;
-            }
-          });
-      
-        CategoryListParams.OrdinalPolicy op = searchParams.indexingParams.getCategoryListParams(request.categoryPath).getOrdinalPolicy(dim);
-        if (op == CategoryListParams.OrdinalPolicy.ALL_BUT_DIMENSION) {
-          dimCount = 0;
-        }
-
-        FacetResultNode rootNode = new FacetResultNode(-1, dimCount);
-        rootNode.label = new CategoryPath(new String[] {dim});
-        rootNode.subResults = nodes;
-        results.add(new FacetResult(request, rootNode, nodes.size()));
-        continue;
-      }
-
-      TopCountPQ q = new TopCountPQ(request.numResults);
-
-      int bottomCount = 0;
-
-      //System.out.println("collect");
-      int dimCount = 0;
-      int childCount = 0;
-      FacetResultNode reuse = null;
-      for(int ord=ordRange.start; ord<=ordRange.end; ord++) {
-        //System.out.println("  ord=" + ord + " count= "+ counts[ord] + " bottomCount=" + bottomCount);
-        if (counts[ord] > 0) {
-          childCount++;
-          if (counts[ord] > bottomCount) {
-            dimCount += counts[ord];
-            //System.out.println("    keep");
-            if (reuse == null) {
-              reuse = new FacetResultNode(ord, counts[ord]);
-            } else {
-              reuse.ordinal = ord;
-              reuse.value = counts[ord];
-            }
-            reuse = q.insertWithOverflow(reuse);
-            if (q.size() == request.numResults) {
-              bottomCount = (int) q.top().value;
-              //System.out.println("    new bottom=" + bottomCount);
-            }
-          }
-        }
-      }
-
-      CategoryListParams.OrdinalPolicy op = searchParams.indexingParams.getCategoryListParams(request.categoryPath).getOrdinalPolicy(dim);
-      if (op == CategoryListParams.OrdinalPolicy.ALL_BUT_DIMENSION) {
-        dimCount = 0;
-      }
-
-      FacetResultNode rootNode = new FacetResultNode(-1, dimCount);
-      rootNode.label = new CategoryPath(new String[] {dim});
-
-      FacetResultNode[] childNodes = new FacetResultNode[q.size()];
-      for(int i=childNodes.length-1;i>=0;i--) {
-        childNodes[i] = q.pop();
-        dv.lookupOrd(childNodes[i].ordinal, scratch);
-        childNodes[i].label = new CategoryPath(scratch.utf8ToString().split(state.separatorRegex, 2));
-      }
-      rootNode.subResults = Arrays.asList(childNodes);
-      
-      results.add(new FacetResult(request, rootNode, childCount));
-    }
-
-    return results;
-  }
-  
-  @Override
-  public boolean requiresDocScores() {
-    return false;
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetFields.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetFields.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetFields.java	2013-10-22 12:23:24.141395122 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetFields.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,86 +0,0 @@
-package org.apache.lucene.facet.sortedset;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Map.Entry;
-import java.util.Map;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.SortedSetDocValuesField;
-import org.apache.lucene.facet.index.DrillDownStream;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.util.BytesRef;
-
-/** Use this to index facets if you intend to
- *  use {@link SortedSetDocValuesAccumulator} to count facets
- *  at search time.  Note that this only supports flat
- *  facets (dimension + label).  Instantiate this class
- *  once, and then call {@link #addFields} to add the
- *  necessary fields to each {@link Document}. */
-
-public class SortedSetDocValuesFacetFields extends FacetFields {
-
-  /** Create a {@code SortedSetDocValuesFacetField} with the
-   *  provided {@link CategoryPath}. */
-  public SortedSetDocValuesFacetFields()  {
-    this(FacetIndexingParams.DEFAULT);
-  }
-
-  /** Create a {@code SortedSetDocValuesFacetField} with the
-   *  provided {@link CategoryPath}, and custom {@link
-   *  FacetIndexingParams}. */
-  public SortedSetDocValuesFacetFields(FacetIndexingParams fip)  {
-    super(null, fip);
-    if (fip.getPartitionSize() != Integer.MAX_VALUE) {
-      throw new IllegalArgumentException("partitions are not supported");
-    }
-  }
-
-  @Override
-  public void addFields(Document doc, Iterable<CategoryPath> categories) throws IOException {
-    if (categories == null) {
-      throw new IllegalArgumentException("categories should not be null");
-    }
-
-    final Map<CategoryListParams,Iterable<CategoryPath>> categoryLists = createCategoryListMapping(categories);
-    for (Entry<CategoryListParams, Iterable<CategoryPath>> e : categoryLists.entrySet()) {
-
-      CategoryListParams clp = e.getKey();
-      String dvField = clp.field + SortedSetDocValuesReaderState.FACET_FIELD_EXTENSION;
-
-      // Add sorted-set DV fields, one per value:
-      for(CategoryPath cp : e.getValue()) {
-        if (cp.length != 2) {
-          throw new IllegalArgumentException("only flat facets (dimension + label) are currently supported; got " + cp);
-        }
-        doc.add(new SortedSetDocValuesField(dvField, new BytesRef(cp.toString(indexingParams.getFacetDelimChar()))));
-      }
-
-      // add the drill-down field
-      DrillDownStream drillDownStream = getDrillDownStream(e.getValue());
-      Field drillDown = new Field(clp.field, drillDownStream, drillDownFieldType());
-      doc.add(drillDown);
-    }
-  }
-}
-


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesReaderState.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesReaderState.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesReaderState.java	2013-08-25 13:52:48.331306361 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesReaderState.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,155 +0,0 @@
-package org.apache.lucene.facet.sortedset;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.regex.Pattern;
-
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.CompositeReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.SlowCompositeReaderWrapper;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.util.BytesRef;
-
-/** Wraps a {@link IndexReader} and resolves ords
- *  using existing {@link SortedSetDocValues} APIs without a
- *  separate taxonomy index.  This only supports flat facets
- *  (dimension + label), and it makes faceting a bit
- *  slower, adds some cost at reopen time, but avoids
- *  managing the separate taxonomy index.  It also requires
- *  less RAM than the taxonomy index, as it manages the flat
- *  (2-level) hierarchy more efficiently.  In addition, the
- *  tie-break during faceting is now meaningful (in label
- *  sorted order).
- *
- *  <p><b>NOTE</b>: creating an instance of this class is
- *  somewhat costly, as it computes per-segment ordinal maps,
- *  so you should create it once and re-use that one instance
- *  for a given {@link IndexReader}. */
-
-public final class SortedSetDocValuesReaderState {
-
-  private final String field;
-  private final AtomicReader topReader;
-  private final int valueCount;
-  final IndexReader origReader;
-  final char separator;
-  final String separatorRegex;
-
-  /** Extension added to {@link CategoryListParams#field}
-   *  to determin which field to read/write facet ordinals from/to. */
-  public static final String FACET_FIELD_EXTENSION = "_sorted_doc_values";
-
-  /** Holds start/end range of ords, which maps to one
-   *  dimension (someday we may generalize it to map to
-   *  hierarchies within one dimension). */
-  static final class OrdRange {
-    /** Start of range, inclusive: */
-    public final int start;
-    /** End of range, inclusive: */
-    public final int end;
-
-    /** Start and end are inclusive. */
-    public OrdRange(int start, int end) {
-      this.start = start;
-      this.end = end;
-    }
-  }
-
-  private final Map<String,OrdRange> prefixToOrdRange = new HashMap<String,OrdRange>();
-
-  /** Create an instance, scanning the {@link
-   *  SortedSetDocValues} from the provided reader, with
-   *  default {@link FacetIndexingParams}. */
-  public SortedSetDocValuesReaderState(IndexReader reader) throws IOException {
-    this(FacetIndexingParams.DEFAULT, reader);
-  }
-
-  /** Create an instance, scanning the {@link
-   *  SortedSetDocValues} from the provided reader and
-   *  {@link FacetIndexingParams}. */
-  public SortedSetDocValuesReaderState(FacetIndexingParams fip, IndexReader reader) throws IOException {
-
-    this.field = fip.getCategoryListParams(null).field + FACET_FIELD_EXTENSION;
-    this.separator = fip.getFacetDelimChar();
-    this.separatorRegex = Pattern.quote(Character.toString(separator));
-    this.origReader = reader;
-
-    // We need this to create thread-safe MultiSortedSetDV
-    // per collector:
-    topReader = SlowCompositeReaderWrapper.wrap(reader);
-    SortedSetDocValues dv = topReader.getSortedSetDocValues(field);
-    if (dv == null) {
-      throw new IllegalArgumentException("field \"" + field + "\" was not indexed with SortedSetDocValues");
-    }
-    if (dv.getValueCount() > Integer.MAX_VALUE) {
-      throw new IllegalArgumentException("can only handle valueCount < Integer.MAX_VALUE; got " + dv.getValueCount());
-    }
-    valueCount = (int) dv.getValueCount();
-
-    // TODO: we can make this more efficient if eg we can be
-    // "involved" when OrdinalMap is being created?  Ie see
-    // each term/ord it's assigning as it goes...
-    String lastDim = null;
-    int startOrd = -1;
-    BytesRef spare = new BytesRef();
-
-    // TODO: this approach can work for full hierarchy?;
-    // TaxoReader can't do this since ords are not in
-    // "sorted order" ... but we should generalize this to
-    // support arbitrary hierarchy:
-    for(int ord=0;ord<valueCount;ord++) {
-      dv.lookupOrd(ord, spare);
-      String[] components = spare.utf8ToString().split(separatorRegex, 2);
-      if (components.length != 2) {
-        throw new IllegalArgumentException("this class can only handle 2 level hierarchy (dim/value); got: " + spare.utf8ToString());
-      }
-      if (!components[0].equals(lastDim)) {
-        if (lastDim != null) {
-          prefixToOrdRange.put(lastDim, new OrdRange(startOrd, ord-1));
-        }
-        startOrd = ord;
-        lastDim = components[0];
-      }
-    }
-
-    if (lastDim != null) {
-      prefixToOrdRange.put(lastDim, new OrdRange(startOrd, valueCount-1));
-    }
-  }
-
-  SortedSetDocValues getDocValues() throws IOException {
-    return topReader.getSortedSetDocValues(field);
-  }
-
-  OrdRange getOrdRange(String dim) {
-    return prefixToOrdRange.get(dim);
-  }
-
-  String getField() {
-    return field;
-  }
-
-  int getSize() {
-    return valueCount;
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetCounts.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetCounts.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetCounts.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetCounts.java	2013-12-18 20:05:46.921549224 -0500
@@ -0,0 +1,291 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.SortedSetDocValuesReaderState.OrdRange;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiDocValues.MultiSortedSetDocValues;
+import org.apache.lucene.index.MultiDocValues;
+import org.apache.lucene.index.ReaderUtil;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.util.BytesRef;
+
+/** Compute facets counts from previously
+ *  indexed {@link SortedSetDocValuesFacetField},
+ *  without require a separate taxonomy index.  Faceting is
+ *  a bit slower (~25%), and there is added cost on every
+ *  {@link IndexReader} open to create a new {@link
+ *  SortedSetDocValuesReaderState}.  Furthermore, this does
+ *  not support hierarchical facets; only flat (dimension +
+ *  label) facets, but it uses quite a bit less RAM to do
+ *  so.
+ *
+ *  <p><b>NOTE</b>: this class should be instantiated and
+ *  then used from a single thread, because it holds a
+ *  thread-private instance of {@link SortedSetDocValues}.
+ * 
+ * <p><b>NOTE:<b/>: tie-break is by unicode sort order
+ *
+ * @lucene.experimental */
+public class SortedSetDocValuesFacetCounts extends Facets {
+
+  final SortedSetDocValuesReaderState state;
+  final SortedSetDocValues dv;
+  final String field;
+  final int[] counts;
+
+  /** Sparse faceting: returns any dimension that had any
+   *  hits, topCount labels per dimension. */
+  public SortedSetDocValuesFacetCounts(SortedSetDocValuesReaderState state, FacetsCollector hits)
+      throws IOException {
+    this.state = state;
+    this.field = state.getField();
+    counts = new int[state.getSize()];
+    dv = state.getDocValues();
+    //System.out.println("field=" + field);
+    count(hits.getMatchingDocs());
+  }
+
+  @Override
+  public FacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    if (topN <= 0) {
+      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
+    }
+    if (path.length > 0) {
+      throw new IllegalArgumentException("path should be 0 length");
+    }
+    OrdRange ordRange = state.getOrdRange(dim);
+    if (ordRange == null) {
+      throw new IllegalArgumentException("dimension \"" + dim + "\" was not indexed");
+    }
+    return getDim(dim, ordRange, topN);
+  }
+
+  private final FacetResult getDim(String dim, OrdRange ordRange, int topN) {
+
+    TopOrdAndIntQueue q = null;
+
+    int bottomCount = 0;
+
+    int dimCount = 0;
+    int childCount = 0;
+
+    TopOrdAndIntQueue.OrdAndValue reuse = null;
+    //System.out.println("getDim : " + ordRange.start + " - " + ordRange.end);
+    for(int ord=ordRange.start; ord<=ordRange.end; ord++) {
+      //System.out.println("  ord=" + ord + " count=" + counts[ord]);
+      if (counts[ord] > 0) {
+        dimCount += counts[ord];
+        childCount++;
+        if (counts[ord] > bottomCount) {
+          if (reuse == null) {
+            reuse = new TopOrdAndIntQueue.OrdAndValue();
+          }
+          reuse.ord = ord;
+          reuse.value = counts[ord];
+          if (q == null) {
+            // Lazy init, so we don't create this for the
+            // sparse case unnecessarily
+            q = new TopOrdAndIntQueue(topN);
+          }
+          reuse = q.insertWithOverflow(reuse);
+          if (q.size() == topN) {
+            bottomCount = q.top().value;
+          }
+        }
+      }
+    }
+
+    if (q == null) {
+      return null;
+    }
+
+    BytesRef scratch = new BytesRef();
+
+    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
+    for(int i=labelValues.length-1;i>=0;i--) {
+      TopOrdAndIntQueue.OrdAndValue ordAndValue = q.pop();
+      dv.lookupOrd(ordAndValue.ord, scratch);
+      String[] parts = FacetsConfig.stringToPath(scratch.utf8ToString());
+      labelValues[i] = new LabelAndValue(parts[1], ordAndValue.value);
+    }
+
+    return new FacetResult(dim, new String[0], dimCount, labelValues, childCount);
+  }
+
+  /** Does all the "real work" of tallying up the counts. */
+  private final void count(List<MatchingDocs> matchingDocs) throws IOException {
+    //System.out.println("ssdv count");
+
+    MultiDocValues.OrdinalMap ordinalMap;
+
+    // TODO: is this right?  really, we need a way to
+    // verify that this ordinalMap "matches" the leaves in
+    // matchingDocs...
+    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {
+      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;
+    } else {
+      ordinalMap = null;
+    }
+
+    for(MatchingDocs hits : matchingDocs) {
+
+      AtomicReader reader = hits.context.reader();
+      //System.out.println("  reader=" + reader);
+      // LUCENE-5090: make sure the provided reader context "matches"
+      // the top-level reader passed to the
+      // SortedSetDocValuesReaderState, else cryptic
+      // AIOOBE can happen:
+      if (ReaderUtil.getTopLevelContext(hits.context).reader() != state.origReader) {
+        throw new IllegalStateException("the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader");
+      }
+      
+      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);
+      if (segValues == null) {
+        continue;
+      }
+
+      final int maxDoc = reader.maxDoc();
+      assert maxDoc == hits.bits.length();
+      //System.out.println("  dv=" + dv);
+
+      // TODO: yet another option is to count all segs
+      // first, only in seg-ord space, and then do a
+      // merge-sort-PQ in the end to only "resolve to
+      // global" those seg ords that can compete, if we know
+      // we just want top K?  ie, this is the same algo
+      // that'd be used for merging facets across shards
+      // (distributed faceting).  but this has much higher
+      // temp ram req'ts (sum of number of ords across all
+      // segs)
+      if (ordinalMap != null) {
+        int segOrd = hits.context.ord;
+
+        int numSegOrds = (int) segValues.getValueCount();
+
+        if (hits.totalHits < numSegOrds/10) {
+          //System.out.println("    remap as-we-go");
+          // Remap every ord to global ord as we iterate:
+          int doc = 0;
+          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {
+            //System.out.println("    doc=" + doc);
+            segValues.setDocument(doc);
+            int term = (int) segValues.nextOrd();
+            while (term != SortedSetDocValues.NO_MORE_ORDS) {
+              //System.out.println("      segOrd=" + segOrd + " ord=" + term + " globalOrd=" + ordinalMap.getGlobalOrd(segOrd, term));
+              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;
+              term = (int) segValues.nextOrd();
+            }
+            ++doc;
+          }
+        } else {
+          //System.out.println("    count in seg ord first");
+
+          // First count in seg-ord space:
+          final int[] segCounts = new int[numSegOrds];
+          int doc = 0;
+          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {
+            //System.out.println("    doc=" + doc);
+            segValues.setDocument(doc);
+            int term = (int) segValues.nextOrd();
+            while (term != SortedSetDocValues.NO_MORE_ORDS) {
+              //System.out.println("      ord=" + term);
+              segCounts[term]++;
+              term = (int) segValues.nextOrd();
+            }
+            ++doc;
+          }
+
+          // Then, migrate to global ords:
+          for(int ord=0;ord<numSegOrds;ord++) {
+            int count = segCounts[ord];
+            if (count != 0) {
+              //System.out.println("    migrate segOrd=" + segOrd + " ord=" + ord + " globalOrd=" + ordinalMap.getGlobalOrd(segOrd, ord));
+              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;
+            }
+          }
+        }
+      } else {
+        // No ord mapping (e.g., single segment index):
+        // just aggregate directly into counts:
+
+        int doc = 0;
+        while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {
+          segValues.setDocument(doc);
+          int term = (int) segValues.nextOrd();
+          while (term != SortedSetDocValues.NO_MORE_ORDS) {
+            counts[term]++;
+            term = (int) segValues.nextOrd();
+          }
+          ++doc;
+        }
+      }
+    }
+  }
+
+  @Override
+  public Number getSpecificValue(String dim, String... path) {
+    if (path.length != 1) {
+      throw new IllegalArgumentException("path must be length=1");
+    }
+    int ord = (int) dv.lookupTerm(new BytesRef(FacetsConfig.pathToString(dim, path)));
+    if (ord < 0) {
+      return -1;
+    }
+
+    return counts[ord];
+  }
+
+  @Override
+  public List<FacetResult> getAllDims(int topN) throws IOException {
+
+    List<FacetResult> results = new ArrayList<FacetResult>();
+    for(Map.Entry<String,OrdRange> ent : state.getPrefixToOrdRange().entrySet()) {
+      FacetResult fr = getDim(ent.getKey(), ent.getValue(), topN);
+      if (fr != null) {
+        results.add(fr);
+      }
+    }
+
+    // Sort by highest count:
+    Collections.sort(results,
+                     new Comparator<FacetResult>() {
+                       @Override
+                       public int compare(FacetResult a, FacetResult b) {
+                         if (a.value.intValue() > b.value.intValue()) {
+                           return -1;
+                         } else if (b.value.intValue() > a.value.intValue()) {
+                           return 1;
+                         } else {
+                           return a.dim.compareTo(b.dim);
+                         }
+                       }
+                     });
+
+    return results;
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetField.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetField.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetField.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetField.java	2013-11-26 07:58:40.391304230 -0500
@@ -0,0 +1,46 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+
+/** Add an instance of this to your Document for every facet
+ *  label to be indexed via SortedSetDocValues. */
+public class SortedSetDocValuesFacetField extends Field {
+  static final FieldType TYPE = new FieldType();
+  static {
+    TYPE.setIndexed(true);
+    TYPE.freeze();
+  }
+  final String dim;
+  final String label;
+
+  public SortedSetDocValuesFacetField(String dim, String label) {
+    super("dummy", TYPE);
+    this.dim = dim;
+    this.label = label;
+  }
+
+  @Override
+  public String toString() {
+    return "SortedSetDocValuesFacetField(dim=" + dim + " label=" + label + ")";
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesReaderState.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesReaderState.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesReaderState.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesReaderState.java	2013-11-27 07:01:06.985085314 -0500
@@ -0,0 +1,149 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.regex.Pattern;
+
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.CompositeReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.util.BytesRef;
+
+/** Wraps a {@link IndexReader} and resolves ords
+ *  using existing {@link SortedSetDocValues} APIs without a
+ *  separate taxonomy index.  This only supports flat facets
+ *  (dimension + label), and it makes faceting a bit
+ *  slower, adds some cost at reopen time, but avoids
+ *  managing the separate taxonomy index.  It also requires
+ *  less RAM than the taxonomy index, as it manages the flat
+ *  (2-level) hierarchy more efficiently.  In addition, the
+ *  tie-break during faceting is now meaningful (in label
+ *  sorted order).
+ *
+ *  <p><b>NOTE</b>: creating an instance of this class is
+ *  somewhat costly, as it computes per-segment ordinal maps,
+ *  so you should create it once and re-use that one instance
+ *  for a given {@link IndexReader}. */
+
+public final class SortedSetDocValuesReaderState {
+
+  private final String field;
+  private final AtomicReader topReader;
+  private final int valueCount;
+  public final IndexReader origReader;
+
+  /** Holds start/end range of ords, which maps to one
+   *  dimension (someday we may generalize it to map to
+   *  hierarchies within one dimension). */
+  public static final class OrdRange {
+    /** Start of range, inclusive: */
+    public final int start;
+    /** End of range, inclusive: */
+    public final int end;
+
+    /** Start and end are inclusive. */
+    public OrdRange(int start, int end) {
+      this.start = start;
+      this.end = end;
+    }
+  }
+
+  private final Map<String,OrdRange> prefixToOrdRange = new HashMap<String,OrdRange>();
+
+  /** Creates this, pulling doc values from the default {@link
+   *  FacetsConfig#DEFAULT_INDEX_FIELD_NAME}. */ 
+  public SortedSetDocValuesReaderState(IndexReader reader) throws IOException {
+    this(reader, FacetsConfig.DEFAULT_INDEX_FIELD_NAME);
+  }
+
+  /** Creates this, pulling doc values from the specified
+   *  field. */
+  public SortedSetDocValuesReaderState(IndexReader reader, String field) throws IOException {
+
+    this.field = field;
+    this.origReader = reader;
+
+    // We need this to create thread-safe MultiSortedSetDV
+    // per collector:
+    topReader = SlowCompositeReaderWrapper.wrap(reader);
+    SortedSetDocValues dv = topReader.getSortedSetDocValues(field);
+    if (dv == null) {
+      throw new IllegalArgumentException("field \"" + field + "\" was not indexed with SortedSetDocValues");
+    }
+    if (dv.getValueCount() > Integer.MAX_VALUE) {
+      throw new IllegalArgumentException("can only handle valueCount < Integer.MAX_VALUE; got " + dv.getValueCount());
+    }
+    valueCount = (int) dv.getValueCount();
+
+    // TODO: we can make this more efficient if eg we can be
+    // "involved" when OrdinalMap is being created?  Ie see
+    // each term/ord it's assigning as it goes...
+    String lastDim = null;
+    int startOrd = -1;
+    BytesRef spare = new BytesRef();
+
+    // TODO: this approach can work for full hierarchy?;
+    // TaxoReader can't do this since ords are not in
+    // "sorted order" ... but we should generalize this to
+    // support arbitrary hierarchy:
+    for(int ord=0;ord<valueCount;ord++) {
+      dv.lookupOrd(ord, spare);
+      String[] components = FacetsConfig.stringToPath(spare.utf8ToString());
+      if (components.length != 2) {
+        throw new IllegalArgumentException("this class can only handle 2 level hierarchy (dim/value); got: " + Arrays.toString(components) + " " + spare.utf8ToString());
+      }
+      if (!components[0].equals(lastDim)) {
+        if (lastDim != null) {
+          prefixToOrdRange.put(lastDim, new OrdRange(startOrd, ord-1));
+        }
+        startOrd = ord;
+        lastDim = components[0];
+      }
+    }
+
+    if (lastDim != null) {
+      prefixToOrdRange.put(lastDim, new OrdRange(startOrd, valueCount-1));
+    }
+  }
+
+  public SortedSetDocValues getDocValues() throws IOException {
+    return topReader.getSortedSetDocValues(field);
+  }
+
+  public Map<String,OrdRange> getPrefixToOrdRange() {
+    return prefixToOrdRange;
+  }
+
+  public OrdRange getOrdRange(String dim) {
+    return prefixToOrdRange.get(dim);
+  }
+
+  public String getField() {
+    return field;
+  }
+
+  public int getSize() {
+    return valueCount;
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/CategoryPath.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/CategoryPath.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/CategoryPath.java	2013-07-15 15:52:17.697877386 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/CategoryPath.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,282 +0,0 @@
-package org.apache.lucene.facet.taxonomy;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_SIZE;
-
-import java.util.Arrays;
-import java.util.regex.Pattern;
-
-/**
- * Holds a sequence of string components, specifying the hierarchical name of a
- * category.
- * 
- * @lucene.experimental
- */
-public class CategoryPath implements Comparable<CategoryPath> {
-
-  /*
-   * copied from DocumentWriterPerThread -- if a CategoryPath is resolved to a
-   * drill-down term which is encoded to a larger term than that length, it is
-   * silently dropped! Therefore we limit the number of characters to MAX/4 to
-   * be on the safe side.
-   */
-  /**
-   * The maximum number of characters a {@link CategoryPath} can have. That is
-   * {@link CategoryPath#toString(char)} length must not exceed that limit.
-   */
-  public final static int MAX_CATEGORY_PATH_LENGTH = (BYTE_BLOCK_SIZE - 2) / 4;
-
-  /** An empty {@link CategoryPath} */
-  public static final CategoryPath EMPTY = new CategoryPath();
-
-  /**
-   * The components of this {@link CategoryPath}. Note that this array may be
-   * shared with other {@link CategoryPath} instances, e.g. as a result of
-   * {@link #subpath(int)}, therefore you should traverse the array up to
-   * {@link #length} for this path's components.
-   */
-  public final String[] components;
-
-  /** The number of components of this {@link CategoryPath}. */
-  public final int length;
-
-  // Used by singleton EMPTY
-  private CategoryPath() {
-    components = null;
-    length = 0;
-  }
-
-  // Used by subpath
-  private CategoryPath(final CategoryPath copyFrom, final int prefixLen) {
-    // while the code which calls this method is safe, at some point a test
-    // tripped on AIOOBE in toString, but we failed to reproduce. adding the
-    // assert as a safety check.
-    assert prefixLen > 0 && prefixLen <= copyFrom.components.length : 
-      "prefixLen cannot be negative nor larger than the given components' length: prefixLen=" + prefixLen
-        + " components.length=" + copyFrom.components.length;
-    this.components = copyFrom.components;
-    length = prefixLen;
-  }
-  
-  /** Construct from the given path components. */
-  public CategoryPath(final String... components) {
-    assert components.length > 0 : "use CategoryPath.EMPTY to create an empty path";
-    long len = 0;
-    for (String comp : components) {
-      if (comp == null || comp.isEmpty()) {
-        throw new IllegalArgumentException("empty or null components not allowed: " + Arrays.toString(components));
-      }
-      len += comp.length();
-    }
-    len += components.length - 1; // add separators
-    if (len > MAX_CATEGORY_PATH_LENGTH) {
-      throw new IllegalArgumentException("category path exceeds maximum allowed path length: max="
-          + MAX_CATEGORY_PATH_LENGTH + " len=" + len
-          + " path=" + Arrays.toString(components).substring(0, 30) + "...");
-    }
-    this.components = components;
-    length = components.length;
-  }
-
-  /** Construct from a given path, separating path components with {@code delimiter}. */
-  public CategoryPath(final String pathString, final char delimiter) {
-    if (pathString.length() > MAX_CATEGORY_PATH_LENGTH) {
-      throw new IllegalArgumentException("category path exceeds maximum allowed path length: max="
-              + MAX_CATEGORY_PATH_LENGTH + " len=" + pathString.length()
-              + " path=" + pathString.substring(0, 30) + "...");
-    }
-
-    String[] comps = pathString.split(Pattern.quote(Character.toString(delimiter)));
-    if (comps.length == 1 && comps[0].isEmpty()) {
-      components = null;
-      length = 0;
-    } else {
-      for (String comp : comps) {
-        if (comp == null || comp.isEmpty()) {
-          throw new IllegalArgumentException("empty or null components not allowed: " + Arrays.toString(comps));
-        }
-      }
-      components = comps;
-      length = components.length;
-    }
-  }
-
-  /**
-   * Returns the number of characters needed to represent the path, including
-   * delimiter characters, for using with
-   * {@link #copyFullPath(char[], int, char)}.
-   */
-  public int fullPathLength() {
-    if (length == 0) return 0;
-    
-    int charsNeeded = 0;
-    for (int i = 0; i < length; i++) {
-      charsNeeded += components[i].length();
-    }
-    charsNeeded += length - 1; // num delimter chars
-    return charsNeeded;
-  }
-
-  /**
-   * Compares this path with another {@link CategoryPath} for lexicographic
-   * order.
-   */
-  @Override
-  public int compareTo(CategoryPath other) {
-    final int len = length < other.length ? length : other.length;
-    for (int i = 0, j = 0; i < len; i++, j++) {
-      int cmp = components[i].compareTo(other.components[j]);
-      if (cmp < 0) return -1; // this is 'before'
-      if (cmp > 0) return 1; // this is 'after'
-    }
-    
-    // one is a prefix of the other
-    return length - other.length;
-  }
-
-  private void hasDelimiter(String offender, char delimiter) {
-    throw new IllegalArgumentException("delimiter character '" + delimiter + "' (U+" + Integer.toHexString(delimiter) + ") appears in path component \"" + offender + "\"");
-  }
-
-  private void noDelimiter(char[] buf, int offset, int len, char delimiter) {
-    for(int idx=0;idx<len;idx++) {
-      if (buf[offset+idx] == delimiter) {
-        hasDelimiter(new String(buf, offset, len), delimiter);
-      }
-    }
-  }
-
-  /**
-   * Copies the path components to the given {@code char[]}, starting at index
-   * {@code start}. {@code delimiter} is copied between the path components.
-   * Returns the number of chars copied.
-   * 
-   * <p>
-   * <b>NOTE:</b> this method relies on the array being large enough to hold the
-   * components and separators - the amount of needed space can be calculated
-   * with {@link #fullPathLength()}.
-   */
-  public int copyFullPath(char[] buf, int start, char delimiter) {
-    if (length == 0) {
-      return 0;
-    }
-
-    int idx = start;
-    int upto = length - 1;
-    for (int i = 0; i < upto; i++) {
-      int len = components[i].length();
-      components[i].getChars(0, len, buf, idx);
-      noDelimiter(buf, idx, len, delimiter);
-      idx += len;
-      buf[idx++] = delimiter;
-    }
-    components[upto].getChars(0, components[upto].length(), buf, idx);
-    noDelimiter(buf, idx, components[upto].length(), delimiter);
-    
-    return idx + components[upto].length() - start;
-  }
-
-  @Override
-  public boolean equals(Object obj) {
-    if (!(obj instanceof CategoryPath)) {
-      return false;
-    }
-    
-    CategoryPath other = (CategoryPath) obj;
-    if (length != other.length) {
-      return false; // not same length, cannot be equal
-    }
-    
-    // CategoryPaths are more likely to differ at the last components, so start
-    // from last-first
-    for (int i = length - 1; i >= 0; i--) {
-      if (!components[i].equals(other.components[i])) {
-        return false;
-      }
-    }
-    return true;
-  }
-
-  @Override
-  public int hashCode() {
-    if (length == 0) {
-      return 0;
-    }
-    
-    int hash = length;
-    for (int i = 0; i < length; i++) {
-      hash = hash * 31 + components[i].hashCode();
-    }
-    return hash;
-  }
-
-  /** Calculate a 64-bit hash function for this path. */
-  public long longHashCode() {
-    if (length == 0) {
-      return 0;
-    }
-    
-    long hash = length;
-    for (int i = 0; i < length; i++) {
-      hash = hash * 65599 + components[i].hashCode();
-    }
-    return hash;
-  }
-
-  /** Returns a sub-path of this path up to {@code length} components. */
-  public CategoryPath subpath(final int length) {
-    if (length >= this.length || length < 0) {
-      return this;
-    } else if (length == 0) {
-      return EMPTY;
-    } else {
-      return new CategoryPath(this, length);
-    }
-  }
-
-  /**
-   * Returns a string representation of the path, separating components with
-   * '/'.
-   * 
-   * @see #toString(char)
-   */
-  @Override
-  public String toString() {
-    return toString('/');
-  }
-
-  /**
-   * Returns a string representation of the path, separating components with the
-   * given delimiter.
-   */
-  public String toString(char delimiter) {
-    if (length == 0) return "";
-    
-    StringBuilder sb = new StringBuilder();
-    for (int i = 0; i < length; i++) {
-      if (components[i].indexOf(delimiter) != -1) {
-        hasDelimiter(components[i], delimiter);
-      }
-      sb.append(components[i]).append(delimiter);
-    }
-    sb.setLength(sb.length() - 1); // remove last delimiter
-    return sb.toString();
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/Consts.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/Consts.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/Consts.java	2013-03-07 11:04:28.512872751 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/Consts.java	2013-11-27 13:49:59.564429043 -0500
@@ -23,23 +23,8 @@
  * @lucene.experimental
  */
 abstract class Consts {
-
   static final String FULL = "$full_path$";
   static final String FIELD_PAYLOADS = "$payloads$";
   static final String PAYLOAD_PARENT = "p";
   static final BytesRef PAYLOAD_PARENT_BYTES_REF = new BytesRef(PAYLOAD_PARENT);
-
-  /**
-   * Delimiter used for creating the full path of a category from the list of
-   * its labels from root. It is forbidden for labels to contain this
-   * character.
-   * <P>
-   * Originally, we used \uFFFE, officially a "unicode noncharacter" (invalid
-   * unicode character) for this purpose. Recently, we switched to the
-   * "private-use" character \uF749.  Even more recently, we
-   * switched to \U001F (INFORMATION_SEPARATOR).
-   */
-  //static final char DEFAULT_DELIMITER = '\uFFFE';
-  //static final char DEFAULT_DELIMITER = '\uF749';
-  static final char DEFAULT_DELIMITER = '\u001F';
 }


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java	2013-04-22 16:59:20.127676455 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java	2013-11-27 14:34:26.444357648 -0500
@@ -5,8 +5,9 @@
 import java.util.logging.Level;
 import java.util.logging.Logger;
 
-import org.apache.lucene.facet.collections.LRUHashMap;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.LRUHashMap;
 import org.apache.lucene.facet.taxonomy.ParallelTaxonomyArrays;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.index.CorruptIndexException; // javadocs
@@ -60,28 +61,26 @@
   private final DirectoryReader indexReader;
 
   // TODO: test DoubleBarrelLRUCache and consider using it instead
-  private LRUHashMap<CategoryPath, Integer> ordinalCache;
-  private LRUHashMap<Integer, CategoryPath> categoryCache;
+  private LRUHashMap<FacetLabel, Integer> ordinalCache;
+  private LRUHashMap<Integer, FacetLabel> categoryCache;
 
   private volatile TaxonomyIndexArrays taxoArrays;
 
-  private char delimiter = Consts.DEFAULT_DELIMITER;
-
   /**
    * Called only from {@link #doOpenIfChanged()}. If the taxonomy has been
    * recreated, you should pass {@code null} as the caches and parent/children
    * arrays.
    */
   DirectoryTaxonomyReader(DirectoryReader indexReader, DirectoryTaxonomyWriter taxoWriter,
-      LRUHashMap<CategoryPath,Integer> ordinalCache, LRUHashMap<Integer,CategoryPath> categoryCache,
+      LRUHashMap<FacetLabel,Integer> ordinalCache, LRUHashMap<Integer,FacetLabel> categoryCache,
       TaxonomyIndexArrays taxoArrays) throws IOException {
     this.indexReader = indexReader;
     this.taxoWriter = taxoWriter;
     this.taxoEpoch = taxoWriter == null ? -1 : taxoWriter.getTaxonomyEpoch();
     
     // use the same instance of the cache, note the protective code in getOrdinal and getPath
-    this.ordinalCache = ordinalCache == null ? new LRUHashMap<CategoryPath,Integer>(DEFAULT_CACHE_VALUE) : ordinalCache;
-    this.categoryCache = categoryCache == null ? new LRUHashMap<Integer,CategoryPath>(DEFAULT_CACHE_VALUE) : categoryCache;
+    this.ordinalCache = ordinalCache == null ? new LRUHashMap<FacetLabel,Integer>(DEFAULT_CACHE_VALUE) : ordinalCache;
+    this.categoryCache = categoryCache == null ? new LRUHashMap<Integer,FacetLabel>(DEFAULT_CACHE_VALUE) : categoryCache;
     
     this.taxoArrays = taxoArrays != null ? new TaxonomyIndexArrays(indexReader, taxoArrays) : null;
   }
@@ -103,8 +102,8 @@
 
     // These are the default cache sizes; they can be configured after
     // construction with the cache's setMaxSize() method
-    ordinalCache = new LRUHashMap<CategoryPath, Integer>(DEFAULT_CACHE_VALUE);
-    categoryCache = new LRUHashMap<Integer, CategoryPath>(DEFAULT_CACHE_VALUE);
+    ordinalCache = new LRUHashMap<FacetLabel, Integer>(DEFAULT_CACHE_VALUE);
+    categoryCache = new LRUHashMap<Integer, FacetLabel>(DEFAULT_CACHE_VALUE);
   }
   
   /**
@@ -122,8 +121,8 @@
     
     // These are the default cache sizes; they can be configured after
     // construction with the cache's setMaxSize() method
-    ordinalCache = new LRUHashMap<CategoryPath, Integer>(DEFAULT_CACHE_VALUE);
-    categoryCache = new LRUHashMap<Integer, CategoryPath>(DEFAULT_CACHE_VALUE);
+    ordinalCache = new LRUHashMap<FacetLabel, Integer>(DEFAULT_CACHE_VALUE);
+    categoryCache = new LRUHashMap<Integer, FacetLabel>(DEFAULT_CACHE_VALUE);
   }
   
   private synchronized void initTaxoArrays() throws IOException {
@@ -242,7 +241,7 @@
   }
 
   @Override
-  public int getOrdinal(CategoryPath cp) throws IOException {
+  public int getOrdinal(FacetLabel cp) throws IOException {
     ensureOpen();
     if (cp.length == 0) {
       return ROOT_ORDINAL;
@@ -270,7 +269,7 @@
     // If we're still here, we have a cache miss. We need to fetch the
     // value from disk, and then also put it in the cache:
     int ret = TaxonomyReader.INVALID_ORDINAL;
-    DocsEnum docs = MultiFields.getTermDocsEnum(indexReader, null, Consts.FULL, new BytesRef(cp.toString(delimiter)), 0);
+    DocsEnum docs = MultiFields.getTermDocsEnum(indexReader, null, Consts.FULL, new BytesRef(FacetsConfig.pathToString(cp.components, cp.length)), 0);
     if (docs != null && docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
       ret = docs.docID();
       
@@ -288,7 +287,7 @@
   }
 
   @Override
-  public CategoryPath getPath(int ordinal) throws IOException {
+  public FacetLabel getPath(int ordinal) throws IOException {
     ensureOpen();
     
     // Since the cache is shared with DTR instances allocated from
@@ -303,14 +302,14 @@
     // wrapped as LRU?
     Integer catIDInteger = Integer.valueOf(ordinal);
     synchronized (categoryCache) {
-      CategoryPath res = categoryCache.get(catIDInteger);
+      FacetLabel res = categoryCache.get(catIDInteger);
       if (res != null) {
         return res;
       }
     }
     
     StoredDocument doc = indexReader.document(ordinal);
-    CategoryPath ret = new CategoryPath(doc.get(Consts.FULL), delimiter);
+    FacetLabel ret = new FacetLabel(FacetsConfig.stringToPath(doc.get(Consts.FULL)));
     synchronized (categoryCache) {
       categoryCache.put(catIDInteger, ret);
     }
@@ -326,7 +325,7 @@
   
   /**
    * setCacheSize controls the maximum allowed size of each of the caches
-   * used by {@link #getPath(int)} and {@link #getOrdinal(CategoryPath)}.
+   * used by {@link #getPath(int)} and {@link #getOrdinal(FacetLabel)}.
    * <P>
    * Currently, if the given size is smaller than the current size of
    * a cache, it will not shrink, and rather we be limited to its current
@@ -343,28 +342,13 @@
     }
   }
 
-  /**
-   * setDelimiter changes the character that the taxonomy uses in its
-   * internal storage as a delimiter between category components. Do not
-   * use this method unless you really know what you are doing.
-   * <P>
-   * If you do use this method, make sure you call it before any other
-   * methods that actually queries the taxonomy. Moreover, make sure you
-   * always pass the same delimiter for all LuceneTaxonomyWriter and
-   * LuceneTaxonomyReader objects you create.
-   */
-  public void setDelimiter(char delimiter) {
-    ensureOpen();
-    this.delimiter = delimiter;
-  }
-  
   public String toString(int max) {
     ensureOpen();
     StringBuilder sb = new StringBuilder();
     int upperl = Math.min(max, indexReader.maxDoc());
     for (int i = 0; i < upperl; i++) {
       try {
-        CategoryPath category = this.getPath(i);
+        FacetLabel category = this.getPath(i);
         if (category == null) {
           sb.append(i + ": NULL!! \n");
           continue;


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java	2013-12-17 10:38:41.592770716 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java	2013-12-19 11:23:33.832076117 -0500
@@ -21,12 +21,13 @@
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
-import org.apache.lucene.facet.taxonomy.writercache.cl2o.Cl2oTaxonomyWriterCache;
-import org.apache.lucene.facet.taxonomy.writercache.lru.LruTaxonomyWriterCache;
+import org.apache.lucene.facet.taxonomy.writercache.Cl2oTaxonomyWriterCache;
+import org.apache.lucene.facet.taxonomy.writercache.LruTaxonomyWriterCache;
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.CorruptIndexException; // javadocs
@@ -34,8 +35,8 @@
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.LogByteSizeMergePolicy;
 import org.apache.lucene.index.ReaderManager;
 import org.apache.lucene.index.SegmentInfos;
@@ -103,8 +104,7 @@
   
   // Records the taxonomy index epoch, updated on replaceTaxonomy as well.
   private long indexEpoch;
-  
-  private char delimiter = Consts.DEFAULT_DELIMITER;
+
   private SinglePositionTokenStream parentStream = new SinglePositionTokenStream(Consts.PAYLOAD_PARENT);
   private Field parentStreamField;
   private Field fullPathField;
@@ -140,23 +140,6 @@
   }
   
   /**
-   * Changes the character that the taxonomy uses in its internal storage as a
-   * delimiter between category components. Do not use this method unless you
-   * really know what you are doing. It has nothing to do with whatever
-   * character the application may be using to represent categories for its own
-   * use.
-   * <p>
-   * If you do use this method, make sure you call it before any other methods
-   * that actually queries the taxonomy. Moreover, make sure you always pass the
-   * same delimiter for all taxonomy writer and reader instances you create for
-   * the same directory.
-   */
-  public void setDelimiter(char delimiter) {
-    ensureOpen();
-    this.delimiter = delimiter;
-  }
-
-  /**
    * Forcibly unlocks the taxonomy in the named directory.
    * <P>
    * Caution: this should only be used by failure recovery code, when it is
@@ -248,7 +231,7 @@
       cacheIsComplete = true;
       // Make sure that the taxonomy always contain the root category
       // with category id 0.
-      addCategory(CategoryPath.EMPTY);
+      addCategory(new FacetLabel());
     } else {
       // There are some categories on the disk, which we have not yet
       // read into the cache, and therefore the cache is incomplete.
@@ -389,7 +372,7 @@
    * returning the category's ordinal, or a negative number in case the
    * category does not yet exist in the taxonomy.
    */
-  protected synchronized int findCategory(CategoryPath categoryPath) throws IOException {
+  protected synchronized int findCategory(FacetLabel categoryPath) throws IOException {
     // If we can find the category in the cache, or we know the cache is
     // complete, we can return the response directly from it
     int res = cache.get(categoryPath);
@@ -422,7 +405,7 @@
     int doc = -1;
     DirectoryReader reader = readerManager.acquire();
     try {
-      final BytesRef catTerm = new BytesRef(categoryPath.toString(delimiter));
+      final BytesRef catTerm = new BytesRef(FacetsConfig.pathToString(categoryPath.components, categoryPath.length));
       TermsEnum termsEnum = null; // reuse
       DocsEnum docs = null; // reuse
       for (AtomicReaderContext ctx : reader.leaves()) {
@@ -448,7 +431,7 @@
   }
 
   @Override
-  public int addCategory(CategoryPath categoryPath) throws IOException {
+  public int addCategory(FacetLabel categoryPath) throws IOException {
     ensureOpen();
     // check the cache outside the synchronized block. this results in better
     // concurrency when categories are there.
@@ -480,14 +463,14 @@
    * parent is always added to the taxonomy before its child). We do this by
    * recursion.
    */
-  private int internalAddCategory(CategoryPath cp) throws IOException {
+  private int internalAddCategory(FacetLabel cp) throws IOException {
     // Find our parent's ordinal (recursively adding the parent category
     // to the taxonomy if it's not already there). Then add the parent
     // ordinal as payloads (rather than a stored field; payloads can be
     // more efficiently read into memory in bulk by LuceneTaxonomyReader)
     int parent;
     if (cp.length > 1) {
-      CategoryPath parentPath = cp.subpath(cp.length - 1);
+      FacetLabel parentPath = cp.subpath(cp.length - 1);
       parent = findCategory(parentPath);
       if (parent < 0) {
         parent = internalAddCategory(parentPath);
@@ -516,7 +499,7 @@
    * Note that the methods calling addCategoryDocument() are synchornized, so
    * this method is effectively synchronized as well.
    */
-  private int addCategoryDocument(CategoryPath categoryPath, int parent) throws IOException {
+  private int addCategoryDocument(FacetLabel categoryPath, int parent) throws IOException {
     // Before Lucene 2.9, position increments >=0 were supported, so we
     // added 1 to parent to allow the parent -1 (the parent of the root).
     // Unfortunately, starting with Lucene 2.9, after LUCENE-1542, this is
@@ -530,7 +513,7 @@
     Document d = new Document();
     d.add(parentStreamField);
 
-    fullPathField.setStringValue(categoryPath.toString(delimiter));
+    fullPathField.setStringValue(FacetsConfig.pathToString(categoryPath.components, categoryPath.length));
     d.add(fullPathField);
 
     // Note that we do no pass an Analyzer here because the fields that are
@@ -597,7 +580,7 @@
     }
   }
 
-  private void addToCache(CategoryPath categoryPath, int id) throws IOException {
+  private void addToCache(FacetLabel categoryPath, int id) throws IOException {
     if (cache.put(categoryPath, id)) {
       // If cache.put() returned true, it means the cache was limited in
       // size, became full, and parts of it had to be evicted. It is
@@ -730,7 +713,7 @@
               // hence documents), there are no deletions in the index. Therefore, it
               // is sufficient to call next(), and then doc(), exactly once with no
               // 'validation' checks.
-              CategoryPath cp = new CategoryPath(t.utf8ToString(), delimiter);
+              FacetLabel cp = new FacetLabel(FacetsConfig.stringToPath(t.utf8ToString()));
               docsEnum = termsEnum.docs(null, docsEnum, DocsEnum.FLAG_NONE);
               boolean res = cache.put(cp, docsEnum.nextDoc() + ctx.docBase);
               assert !res : "entries should not have been evicted from the cache";
@@ -819,8 +802,7 @@
         final Terms terms = ar.terms(Consts.FULL);
         te = terms.iterator(te);
         while (te.next() != null) {
-          String value = te.term().utf8ToString();
-          CategoryPath cp = new CategoryPath(value, delimiter);
+          FacetLabel cp = new FacetLabel(FacetsConfig.stringToPath(te.term().utf8ToString()));
           final int ordinal = addCategory(cp);
           docs = te.docs(null, docs, DocsEnum.FLAG_NONE);
           ordinalMap.addMapping(docs.nextDoc() + base, ordinal);


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/FacetLabel.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/FacetLabel.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/FacetLabel.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/FacetLabel.java	2013-12-19 13:26:29.559878808 -0500
@@ -0,0 +1,200 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.facet.taxonomy.writercache.LruTaxonomyWriterCache; // javadocs
+import org.apache.lucene.facet.taxonomy.writercache.NameHashIntCacheLRU; // javadocs
+
+import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_SIZE;
+
+/**
+ * Holds a sequence of string components, specifying the hierarchical name of a
+ * category.
+ * 
+ * @lucene.internal
+ */
+public class FacetLabel implements Comparable<FacetLabel> {
+
+  /*
+   * copied from DocumentWriterPerThread -- if a CategoryPath is resolved to a
+   * drill-down term which is encoded to a larger term than that length, it is
+   * silently dropped! Therefore we limit the number of characters to MAX/4 to
+   * be on the safe side.
+   */
+  /**
+   * The maximum number of characters a {@link FacetLabel} can have.
+   */
+  public final static int MAX_CATEGORY_PATH_LENGTH = (BYTE_BLOCK_SIZE - 2) / 4;
+
+  /**
+   * The components of this {@link FacetLabel}. Note that this array may be
+   * shared with other {@link FacetLabel} instances, e.g. as a result of
+   * {@link #subpath(int)}, therefore you should traverse the array up to
+   * {@link #length} for this path's components.
+   */
+  public final String[] components;
+
+  /** The number of components of this {@link FacetLabel}. */
+  public final int length;
+
+  // Used by singleton EMPTY
+  private FacetLabel() {
+    components = null;
+    length = 0;
+  }
+
+  // Used by subpath
+  private FacetLabel(final FacetLabel copyFrom, final int prefixLen) {
+    // while the code which calls this method is safe, at some point a test
+    // tripped on AIOOBE in toString, but we failed to reproduce. adding the
+    // assert as a safety check.
+    assert prefixLen >= 0 && prefixLen <= copyFrom.components.length : 
+      "prefixLen cannot be negative nor larger than the given components' length: prefixLen=" + prefixLen
+        + " components.length=" + copyFrom.components.length;
+    this.components = copyFrom.components;
+    length = prefixLen;
+  }
+  
+  /** Construct from the given path components. */
+  public FacetLabel(final String... components) {
+    this.components = components;
+    length = components.length;
+    checkComponents();
+  }
+
+  /** Construct from the dimension plus the given path components. */
+  public FacetLabel(String dim, String[] path) {
+    components = new String[1+path.length];
+    components[0] = dim;
+    System.arraycopy(path, 0, components, 1, path.length);
+    length = components.length;
+    checkComponents();
+  }
+
+  private void checkComponents() {
+    long len = 0;
+    for (String comp : components) {
+      if (comp == null || comp.isEmpty()) {
+        throw new IllegalArgumentException("empty or null components not allowed: " + Arrays.toString(components));
+      }
+      len += comp.length();
+    }
+    len += components.length - 1; // add separators
+    if (len > MAX_CATEGORY_PATH_LENGTH) {
+      throw new IllegalArgumentException("category path exceeds maximum allowed path length: max="
+          + MAX_CATEGORY_PATH_LENGTH + " len=" + len
+          + " path=" + Arrays.toString(components).substring(0, 30) + "...");
+    }
+  }
+
+  /**
+   * Compares this path with another {@link FacetLabel} for lexicographic
+   * order.
+   */
+  @Override
+  public int compareTo(FacetLabel other) {
+    final int len = length < other.length ? length : other.length;
+    for (int i = 0, j = 0; i < len; i++, j++) {
+      int cmp = components[i].compareTo(other.components[j]);
+      if (cmp < 0) {
+        return -1; // this is 'before'
+      }
+      if (cmp > 0) {
+        return 1; // this is 'after'
+      }
+    }
+    
+    // one is a prefix of the other
+    return length - other.length;
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (!(obj instanceof FacetLabel)) {
+      return false;
+    }
+    
+    FacetLabel other = (FacetLabel) obj;
+    if (length != other.length) {
+      return false; // not same length, cannot be equal
+    }
+    
+    // CategoryPaths are more likely to differ at the last components, so start
+    // from last-first
+    for (int i = length - 1; i >= 0; i--) {
+      if (!components[i].equals(other.components[i])) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public int hashCode() {
+    if (length == 0) {
+      return 0;
+    }
+    
+    int hash = length;
+    for (int i = 0; i < length; i++) {
+      hash = hash * 31 + components[i].hashCode();
+    }
+    return hash;
+  }
+
+  /** Calculate a 64-bit hash function for this path.  This
+   *  is necessary for {@link NameHashIntCacheLRU} (the
+   *  default cache impl for {@link
+   *  LruTaxonomyWriterCache}) to reduce the chance of
+   *  "silent but deadly" collisions. */
+  public long longHashCode() {
+    if (length == 0) {
+      return 0;
+    }
+    
+    long hash = length;
+    for (int i = 0; i < length; i++) {
+      hash = hash * 65599 + components[i].hashCode();
+    }
+    return hash;
+  }
+
+  /** Returns a sub-path of this path up to {@code length} components. */
+  public FacetLabel subpath(final int length) {
+    if (length >= this.length || length < 0) {
+      return this;
+    } else {
+      return new FacetLabel(this, length);
+    }
+  }
+
+  /**
+   * Returns a string representation of the path.
+   */
+  @Override
+  public String toString() {
+    if (length == 0) {
+      return "FacetLabel: []";
+    }
+    String[] parts = new String[length];
+    System.arraycopy(components, 0, parts, 0, length);
+    return "FacetLabel: " + Arrays.toString(parts);
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/LRUHashMap.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/LRUHashMap.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/LRUHashMap.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/LRUHashMap.java	2013-11-25 18:43:18.596580771 -0500
@@ -0,0 +1,111 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.LinkedHashMap;
+import java.util.Map;
+
+/**
+ * LRUHashMap is an extension of Java's HashMap, which has a bounded size();
+ * When it reaches that size, each time a new element is added, the least
+ * recently used (LRU) entry is removed.
+ * <p>
+ * Java makes it very easy to implement LRUHashMap - all its functionality is
+ * already available from {@link java.util.LinkedHashMap}, and we just need to
+ * configure that properly.
+ * <p>
+ * Note that like HashMap, LRUHashMap is unsynchronized, and the user MUST
+ * synchronize the access to it if used from several threads. Moreover, while
+ * with HashMap this is only a concern if one of the threads is modifies the
+ * map, with LURHashMap every read is a modification (because the LRU order
+ * needs to be remembered) so proper synchronization is always necessary.
+ * <p>
+ * With the usual synchronization mechanisms available to the user, this
+ * unfortunately means that LRUHashMap will probably perform sub-optimally under
+ * heavy contention: while one thread uses the hash table (reads or writes), any
+ * other thread will be blocked from using it - or even just starting to use it
+ * (e.g., calculating the hash function). A more efficient approach would be not
+ * to use LinkedHashMap at all, but rather to use a non-locking (as much as
+ * possible) thread-safe solution, something along the lines of
+ * java.util.concurrent.ConcurrentHashMap (though that particular class does not
+ * support the additional LRU semantics, which will need to be added separately
+ * using a concurrent linked list or additional storage of timestamps (in an
+ * array or inside the entry objects), or whatever).
+ * 
+ * @lucene.experimental
+ */
+public class LRUHashMap<K,V> extends LinkedHashMap<K,V> {
+
+  private int maxSize;
+
+  /**
+   * Create a new hash map with a bounded size and with least recently
+   * used entries removed.
+   * @param maxSize
+   *     the maximum size (in number of entries) to which the map can grow
+   *     before the least recently used entries start being removed.<BR>
+   *      Setting maxSize to a very large value, like
+   *      {@link Integer#MAX_VALUE} is allowed, but is less efficient than
+   *      using {@link java.util.HashMap} because our class needs
+   *      to keep track of the use order (via an additional doubly-linked
+   *      list) which is not used when the map's size is always below the
+   *      maximum size. 
+   */
+  public LRUHashMap(int maxSize) {
+    super(16, 0.75f, true);
+    this.maxSize = maxSize;
+  }
+
+  /**
+   * Return the max size
+   */
+  public int getMaxSize() {
+    return maxSize;
+  }
+
+  /**
+   * setMaxSize() allows changing the map's maximal number of elements
+   * which was defined at construction time.
+   * <P>
+   * Note that if the map is already larger than maxSize, the current 
+   * implementation does not shrink it (by removing the oldest elements);
+   * Rather, the map remains in its current size as new elements are
+   * added, and will only start shrinking (until settling again on the
+   * give maxSize) if existing elements are explicitly deleted.  
+   */
+  public void setMaxSize(int maxSize) {
+    this.maxSize = maxSize;
+  }
+
+  // We override LinkedHashMap's removeEldestEntry() method. This method
+  // is called every time a new entry is added, and if we return true
+  // here, the eldest element will be deleted automatically. In our case,
+  // we return true if the size of the map grew beyond our limit - ignoring
+  // what is that eldest element that we'll be deleting.
+  @Override
+  protected boolean removeEldestEntry(Map.Entry<K, V> eldest) {
+    return size() > maxSize;
+  }
+
+  @SuppressWarnings("unchecked")
+  @Override
+  public LRUHashMap<K,V> clone() {
+    return (LRUHashMap<K,V>) super.clone();
+  }
+  
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/PrintTaxonomyStats.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/PrintTaxonomyStats.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/PrintTaxonomyStats.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/PrintTaxonomyStats.java	2013-11-27 13:26:04.144467121 -0500
@@ -0,0 +1,92 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.io.IOException;
+import java.io.PrintStream;
+
+import org.apache.lucene.facet.taxonomy.TaxonomyReader.ChildrenIterator;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FSDirectory;
+
+/** Prints how many ords are under each dimension. */
+
+// java -cp ../build/core/classes/java:../build/facet/classes/java org.apache.lucene.facet.util.PrintTaxonomyStats -printTree /s2/scratch/indices/wikibig.trunk.noparents.facets.Lucene41.nd1M/facets
+public class PrintTaxonomyStats {
+
+  public static void main(String[] args) throws IOException {
+    boolean printTree = false;
+    String path = null;
+    for(int i=0;i<args.length;i++) {
+      if (args[i].equals("-printTree")) {
+        printTree = true;
+      } else {
+        path = args[i];
+      }
+    }
+    if (args.length != (printTree ? 2 : 1)) {
+      System.out.println("\nUsage: java -classpath ... org.apache.lucene.facet.util.PrintTaxonomyStats [-printTree] /path/to/taxononmy/index\n");
+      System.exit(1);
+    }
+    Directory dir = FSDirectory.open(new File(path));
+    TaxonomyReader r = new DirectoryTaxonomyReader(dir);
+    printStats(r, System.out, printTree);
+    r.close();
+    dir.close();
+  }
+
+  public static void printStats(TaxonomyReader r, PrintStream out, boolean printTree) throws IOException {
+    out.println(r.getSize() + " total categories.");
+
+    ChildrenIterator it = r.getChildren(TaxonomyReader.ROOT_ORDINAL);
+    int child;
+    while ((child = it.next()) != TaxonomyReader.INVALID_ORDINAL) {
+      ChildrenIterator chilrenIt = r.getChildren(child);
+      int numImmediateChildren = 0;
+      while (chilrenIt.next() != TaxonomyReader.INVALID_ORDINAL) {
+        numImmediateChildren++;
+      }
+      FacetLabel cp = r.getPath(child);
+      out.println("/" + cp.components[0] + ": " + numImmediateChildren + " immediate children; " + (1+countAllChildren(r, child)) + " total categories");
+      if (printTree) {
+        printAllChildren(out, r, child, "  ", 1);
+      }
+    }
+  }
+
+  private static int countAllChildren(TaxonomyReader r, int ord) throws IOException {
+    int count = 0;
+    ChildrenIterator it = r.getChildren(ord);
+    int child;
+    while ((child = it.next()) != TaxonomyReader.INVALID_ORDINAL) {
+      count += 1 + countAllChildren(r, child);
+    }
+    return count;
+  }
+
+  private static void printAllChildren(PrintStream out, TaxonomyReader r, int ord, String indent, int depth) throws IOException {
+    ChildrenIterator it = r.getChildren(ord);
+    int child;
+    while ((child = it.next()) != TaxonomyReader.INVALID_ORDINAL) {
+      out.println(indent + "/" + r.getPath(child).components[depth]);
+      printAllChildren(out, r, child, indent + "  ", depth+1);
+    }
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyReader.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyReader.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyReader.java	2013-04-22 16:59:20.127676455 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyReader.java	2013-11-27 19:04:12.647924733 -0500
@@ -92,7 +92,7 @@
   
   /**
    * The root category (the category with the empty path) always has the ordinal
-   * 0, to which we give a name ROOT_ORDINAL. {@link #getOrdinal(CategoryPath)}
+   * 0, to which we give a name ROOT_ORDINAL. {@link #getOrdinal(FacetLabel)}
    * of an empty path will always return {@code ROOT_ORDINAL}, and
    * {@link #getPath(int)} with {@code ROOT_ORDINAL} will return the empty path.
    */
@@ -215,10 +215,17 @@
    * @return the category's ordinal or {@link #INVALID_ORDINAL} if the category
    *         wasn't foun.
    */
-  public abstract int getOrdinal(CategoryPath categoryPath) throws IOException;
+  public abstract int getOrdinal(FacetLabel categoryPath) throws IOException;
+
+  public int getOrdinal(String dim, String[] path) throws IOException {
+    String[] fullPath = new String[path.length+1];
+    fullPath[0] = dim;
+    System.arraycopy(path, 0, fullPath, 1, path.length);
+    return getOrdinal(new FacetLabel(fullPath));
+  }
   
   /** Returns the path name of the category with the given ordinal. */
-  public abstract CategoryPath getPath(int ordinal) throws IOException;
+  public abstract FacetLabel getPath(int ordinal) throws IOException;
   
   /** Returns the current refCount for this taxonomy reader. */
   public final int getRefCount() {


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyWriter.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyWriter.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyWriter.java	2013-01-21 09:43:49.990827548 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyWriter.java	2013-11-14 15:40:50.022297838 -0500
@@ -64,7 +64,7 @@
    * ordinal of a category is guaranteed to be smaller then the ordinal of
    * any of its descendants. 
    */ 
-  public int addCategory(CategoryPath categoryPath) throws IOException;
+  public int addCategory(FacetLabel categoryPath) throws IOException;
   
   /**
    * getParent() returns the ordinal of the parent category of the category


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CategoryPathUtils.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CategoryPathUtils.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CategoryPathUtils.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CategoryPathUtils.java	2013-11-26 15:06:06.934618000 -0500
@@ -0,0 +1,82 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Utilities for use of {@link FacetLabel} by {@link CompactLabelToOrdinal}. */
+class CategoryPathUtils {
+  
+  /** Serializes the given {@link FacetLabel} to the {@link CharBlockArray}. */
+  public static void serialize(FacetLabel cp, CharBlockArray charBlockArray) {
+    charBlockArray.append((char) cp.length);
+    if (cp.length == 0) {
+      return;
+    }
+    for (int i = 0; i < cp.length; i++) {
+      charBlockArray.append((char) cp.components[i].length());
+      charBlockArray.append(cp.components[i]);
+    }
+  }
+
+  /**
+   * Calculates a hash function of a path that was serialized with
+   * {@link #serialize(FacetLabel, CharBlockArray)}.
+   */
+  public static int hashCodeOfSerialized(CharBlockArray charBlockArray, int offset) {
+    int length = charBlockArray.charAt(offset++);
+    if (length == 0) {
+      return 0;
+    }
+    
+    int hash = length;
+    for (int i = 0; i < length; i++) {
+      int len = charBlockArray.charAt(offset++);
+      hash = hash * 31 + charBlockArray.subSequence(offset, offset + len).hashCode();
+      offset += len;
+    }
+    return hash;
+  }
+
+  /**
+   * Check whether the {@link FacetLabel} is equal to the one serialized in
+   * {@link CharBlockArray}.
+   */
+  public static boolean equalsToSerialized(FacetLabel cp, CharBlockArray charBlockArray, int offset) {
+    int n = charBlockArray.charAt(offset++);
+    if (cp.length != n) {
+      return false;
+    }
+    if (cp.length == 0) {
+      return true;
+    }
+    
+    for (int i = 0; i < cp.length; i++) {
+      int len = charBlockArray.charAt(offset++);
+      if (len != cp.components[i].length()) {
+        return false;
+      }
+      if (!cp.components[i].equals(charBlockArray.subSequence(offset, offset + len))) {
+        return false;
+      }
+      offset += len;
+    }
+    return true;
+  }
+
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CharBlockArray.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CharBlockArray.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CharBlockArray.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CharBlockArray.java	2013-11-26 15:06:17.086617841 -0500
@@ -0,0 +1,212 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.io.OutputStream;
+import java.io.Serializable;
+import java.util.ArrayList;
+import java.util.List;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Similar to {@link StringBuilder}, but with a more efficient growing strategy.
+ * This class uses char array blocks to grow.
+ * 
+ * @lucene.experimental
+ */
+class CharBlockArray implements Appendable, Serializable, CharSequence {
+
+  private static final long serialVersionUID = 1L;
+
+  private final static int DefaultBlockSize = 32 * 1024;  // 32 KB default size
+
+  final static class Block implements Serializable, Cloneable {
+    private static final long serialVersionUID = 1L;
+
+    final char[] chars;
+    int length;
+
+    Block(int size) {
+      this.chars = new char[size];
+      this.length = 0;
+    }
+  }
+
+  List<Block> blocks;
+  Block current;
+  int blockSize;
+  int length;
+
+  CharBlockArray() {
+    this(DefaultBlockSize);
+  }
+
+  CharBlockArray(int blockSize) {
+    this.blocks = new ArrayList<Block>();
+    this.blockSize = blockSize;
+    addBlock();
+  }
+
+  private void addBlock() {
+    this.current = new Block(this.blockSize);
+    this.blocks.add(this.current);
+  }
+
+  int blockIndex(int index) {
+    return index / blockSize;
+  }
+
+  int indexInBlock(int index) {
+    return index % blockSize;
+  }
+
+  @Override
+  public CharBlockArray append(CharSequence chars) {
+    return append(chars, 0, chars.length());
+  }
+
+  @Override
+  public CharBlockArray append(char c) {
+    if (this.current.length == this.blockSize) {
+      addBlock();
+    }
+    this.current.chars[this.current.length++] = c;
+    this.length++;
+
+    return this;
+  }
+
+  @Override
+  public CharBlockArray append(CharSequence chars, int start, int length) {
+    int end = start + length;
+    for (int i = start; i < end; i++) {
+      append(chars.charAt(i));
+    }
+    return this;
+  }
+
+  public CharBlockArray append(char[] chars, int start, int length) {
+    int offset = start;
+    int remain = length;
+    while (remain > 0) {
+      if (this.current.length == this.blockSize) {
+        addBlock();
+      }
+      int toCopy = remain;
+      int remainingInBlock = this.blockSize - this.current.length;
+      if (remainingInBlock < toCopy) {
+        toCopy = remainingInBlock;
+      }
+      System.arraycopy(chars, offset, this.current.chars, this.current.length, toCopy);
+      offset += toCopy;
+      remain -= toCopy;
+      this.current.length += toCopy;
+    }
+
+    this.length += length;
+    return this;
+  }
+
+  public CharBlockArray append(String s) {
+    int remain = s.length();
+    int offset = 0;
+    while (remain > 0) {
+      if (this.current.length == this.blockSize) {
+        addBlock();
+      }
+      int toCopy = remain;
+      int remainingInBlock = this.blockSize - this.current.length;
+      if (remainingInBlock < toCopy) {
+        toCopy = remainingInBlock;
+      }
+      s.getChars(offset, offset + toCopy, this.current.chars, this.current.length);
+      offset += toCopy;
+      remain -= toCopy;
+      this.current.length += toCopy;
+    }
+
+    this.length += s.length();
+    return this;
+  }
+
+  @Override
+  public char charAt(int index) {
+    Block b = blocks.get(blockIndex(index));
+    return b.chars[indexInBlock(index)];
+  }
+
+  @Override
+  public int length() {
+    return this.length;
+  }
+
+  @Override
+  public CharSequence subSequence(int start, int end) {
+    int remaining = end - start;
+    StringBuilder sb = new StringBuilder(remaining);
+    int blockIdx = blockIndex(start);
+    int indexInBlock = indexInBlock(start);
+    while (remaining > 0) {
+      Block b = blocks.get(blockIdx++);
+      int numToAppend = Math.min(remaining, b.length - indexInBlock);
+      sb.append(b.chars, indexInBlock, numToAppend);
+      remaining -= numToAppend;
+      indexInBlock = 0; // 2nd+ iterations read from start of the block 
+    }
+    return sb.toString();
+  }
+
+  @Override
+  public String toString() {
+    StringBuilder sb = new StringBuilder();
+    for (Block b : blocks) {
+      sb.append(b.chars, 0, b.length);
+    }
+    return sb.toString();
+  }
+
+  void flush(OutputStream out) throws IOException {
+    ObjectOutputStream oos = null;
+    try {
+      oos = new ObjectOutputStream(out);
+      oos.writeObject(this);
+      oos.flush();
+    } finally {
+      if (oos != null) {
+        oos.close();
+      }
+    }
+  }
+
+  public static CharBlockArray open(InputStream in) throws IOException, ClassNotFoundException {
+    ObjectInputStream ois = null;
+    try {
+      ois = new ObjectInputStream(in);
+      CharBlockArray a = (CharBlockArray) ois.readObject();
+      return a;
+    } finally {
+      if (ois != null) {
+        ois.close();
+      }
+    }
+  }
+
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CategoryPathUtils.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CategoryPathUtils.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CategoryPathUtils.java	2013-07-15 15:52:17.693877386 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CategoryPathUtils.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,82 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.cl2o;
-
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Utilities for use of {@link CategoryPath} by {@link CompactLabelToOrdinal}. */
-class CategoryPathUtils {
-  
-  /** Serializes the given {@link CategoryPath} to the {@link CharBlockArray}. */
-  public static void serialize(CategoryPath cp, CharBlockArray charBlockArray) {
-    charBlockArray.append((char) cp.length);
-    if (cp.length == 0) {
-      return;
-    }
-    for (int i = 0; i < cp.length; i++) {
-      charBlockArray.append((char) cp.components[i].length());
-      charBlockArray.append(cp.components[i]);
-    }
-  }
-
-  /**
-   * Calculates a hash function of a path that was serialized with
-   * {@link #serialize(CategoryPath, CharBlockArray)}.
-   */
-  public static int hashCodeOfSerialized(CharBlockArray charBlockArray, int offset) {
-    int length = charBlockArray.charAt(offset++);
-    if (length == 0) {
-      return 0;
-    }
-    
-    int hash = length;
-    for (int i = 0; i < length; i++) {
-      int len = charBlockArray.charAt(offset++);
-      hash = hash * 31 + charBlockArray.subSequence(offset, offset + len).hashCode();
-      offset += len;
-    }
-    return hash;
-  }
-
-  /**
-   * Check whether the {@link CategoryPath} is equal to the one serialized in
-   * {@link CharBlockArray}.
-   */
-  public static boolean equalsToSerialized(CategoryPath cp, CharBlockArray charBlockArray, int offset) {
-    int n = charBlockArray.charAt(offset++);
-    if (cp.length != n) {
-      return false;
-    }
-    if (cp.length == 0) {
-      return true;
-    }
-    
-    for (int i = 0; i < cp.length; i++) {
-      int len = charBlockArray.charAt(offset++);
-      if (len != cp.components[i].length()) {
-        return false;
-      }
-      if (!cp.components[i].equals(charBlockArray.subSequence(offset, offset + len))) {
-        return false;
-      }
-      offset += len;
-    }
-    return true;
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CharBlockArray.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CharBlockArray.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CharBlockArray.java	2013-01-06 18:41:12.306961728 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CharBlockArray.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,212 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.cl2o;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.ObjectInputStream;
-import java.io.ObjectOutputStream;
-import java.io.OutputStream;
-import java.io.Serializable;
-import java.util.ArrayList;
-import java.util.List;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Similar to {@link StringBuilder}, but with a more efficient growing strategy.
- * This class uses char array blocks to grow.
- * 
- * @lucene.experimental
- */
-class CharBlockArray implements Appendable, Serializable, CharSequence {
-
-  private static final long serialVersionUID = 1L;
-
-  private final static int DefaultBlockSize = 32 * 1024;  // 32 KB default size
-
-  final static class Block implements Serializable, Cloneable {
-    private static final long serialVersionUID = 1L;
-
-    final char[] chars;
-    int length;
-
-    Block(int size) {
-      this.chars = new char[size];
-      this.length = 0;
-    }
-  }
-
-  List<Block> blocks;
-  Block current;
-  int blockSize;
-  int length;
-
-  CharBlockArray() {
-    this(DefaultBlockSize);
-  }
-
-  CharBlockArray(int blockSize) {
-    this.blocks = new ArrayList<Block>();
-    this.blockSize = blockSize;
-    addBlock();
-  }
-
-  private void addBlock() {
-    this.current = new Block(this.blockSize);
-    this.blocks.add(this.current);
-  }
-
-  int blockIndex(int index) {
-    return index / blockSize;
-  }
-
-  int indexInBlock(int index) {
-    return index % blockSize;
-  }
-
-  @Override
-  public CharBlockArray append(CharSequence chars) {
-    return append(chars, 0, chars.length());
-  }
-
-  @Override
-  public CharBlockArray append(char c) {
-    if (this.current.length == this.blockSize) {
-      addBlock();
-    }
-    this.current.chars[this.current.length++] = c;
-    this.length++;
-
-    return this;
-  }
-
-  @Override
-  public CharBlockArray append(CharSequence chars, int start, int length) {
-    int end = start + length;
-    for (int i = start; i < end; i++) {
-      append(chars.charAt(i));
-    }
-    return this;
-  }
-
-  public CharBlockArray append(char[] chars, int start, int length) {
-    int offset = start;
-    int remain = length;
-    while (remain > 0) {
-      if (this.current.length == this.blockSize) {
-        addBlock();
-      }
-      int toCopy = remain;
-      int remainingInBlock = this.blockSize - this.current.length;
-      if (remainingInBlock < toCopy) {
-        toCopy = remainingInBlock;
-      }
-      System.arraycopy(chars, offset, this.current.chars, this.current.length, toCopy);
-      offset += toCopy;
-      remain -= toCopy;
-      this.current.length += toCopy;
-    }
-
-    this.length += length;
-    return this;
-  }
-
-  public CharBlockArray append(String s) {
-    int remain = s.length();
-    int offset = 0;
-    while (remain > 0) {
-      if (this.current.length == this.blockSize) {
-        addBlock();
-      }
-      int toCopy = remain;
-      int remainingInBlock = this.blockSize - this.current.length;
-      if (remainingInBlock < toCopy) {
-        toCopy = remainingInBlock;
-      }
-      s.getChars(offset, offset + toCopy, this.current.chars, this.current.length);
-      offset += toCopy;
-      remain -= toCopy;
-      this.current.length += toCopy;
-    }
-
-    this.length += s.length();
-    return this;
-  }
-
-  @Override
-  public char charAt(int index) {
-    Block b = blocks.get(blockIndex(index));
-    return b.chars[indexInBlock(index)];
-  }
-
-  @Override
-  public int length() {
-    return this.length;
-  }
-
-  @Override
-  public CharSequence subSequence(int start, int end) {
-    int remaining = end - start;
-    StringBuilder sb = new StringBuilder(remaining);
-    int blockIdx = blockIndex(start);
-    int indexInBlock = indexInBlock(start);
-    while (remaining > 0) {
-      Block b = blocks.get(blockIdx++);
-      int numToAppend = Math.min(remaining, b.length - indexInBlock);
-      sb.append(b.chars, indexInBlock, numToAppend);
-      remaining -= numToAppend;
-      indexInBlock = 0; // 2nd+ iterations read from start of the block 
-    }
-    return sb.toString();
-  }
-
-  @Override
-  public String toString() {
-    StringBuilder sb = new StringBuilder();
-    for (Block b : blocks) {
-      sb.append(b.chars, 0, b.length);
-    }
-    return sb.toString();
-  }
-
-  void flush(OutputStream out) throws IOException {
-    ObjectOutputStream oos = null;
-    try {
-      oos = new ObjectOutputStream(out);
-      oos.writeObject(this);
-      oos.flush();
-    } finally {
-      if (oos != null) {
-        oos.close();
-      }
-    }
-  }
-
-  public static CharBlockArray open(InputStream in) throws IOException, ClassNotFoundException {
-    ObjectInputStream ois = null;
-    try {
-      ois = new ObjectInputStream(in);
-      CharBlockArray a = (CharBlockArray) ois.readObject();
-      return a;
-    } finally {
-      if (ois != null) {
-        ois.close();
-      }
-    }
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/Cl2oTaxonomyWriterCache.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/Cl2oTaxonomyWriterCache.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/Cl2oTaxonomyWriterCache.java	2013-01-06 18:41:12.306961728 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/Cl2oTaxonomyWriterCache.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,98 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.cl2o;
-
-import java.util.concurrent.locks.ReadWriteLock;
-import java.util.concurrent.locks.ReentrantReadWriteLock;
-
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * {@link TaxonomyWriterCache} using {@link CompactLabelToOrdinal}. Although
- * called cache, it maintains in memory all the mappings from category to
- * ordinal, relying on that {@link CompactLabelToOrdinal} is an efficient
- * mapping for this purpose.
- * 
- * @lucene.experimental
- */
-public class Cl2oTaxonomyWriterCache implements TaxonomyWriterCache {  
-
-  private final ReadWriteLock lock = new ReentrantReadWriteLock();
-  private final int initialCapcity, numHashArrays;
-  private final float loadFactor;
-  
-  private volatile CompactLabelToOrdinal cache;
-
-  public Cl2oTaxonomyWriterCache(int initialCapcity, float loadFactor, int numHashArrays) {
-    this.cache = new CompactLabelToOrdinal(initialCapcity, loadFactor, numHashArrays);
-    this.initialCapcity = initialCapcity;
-    this.numHashArrays = numHashArrays;
-    this.loadFactor = loadFactor;
-  }
-
-  @Override
-  public void clear() {
-    lock.writeLock().lock();
-    try {
-      cache = new CompactLabelToOrdinal(initialCapcity, loadFactor, numHashArrays);
-    } finally {
-      lock.writeLock().unlock();
-    }
-  }
-  
-  @Override
-  public synchronized void close() {
-    cache = null;
-  }
-
-  @Override
-  public boolean isFull() {
-    // This cache is never full
-    return false;
-  }
-
-  @Override
-  public int get(CategoryPath categoryPath) {
-    lock.readLock().lock();
-    try {
-      return cache.getOrdinal(categoryPath);
-    } finally {
-      lock.readLock().unlock();
-    }
-  }
-
-  @Override
-  public boolean put(CategoryPath categoryPath, int ordinal) {
-    lock.writeLock().lock();
-    try {
-      cache.addLabel(categoryPath, ordinal);
-      // Tell the caller we didn't clear part of the cache, so it doesn't
-      // have to flush its on-disk index now
-      return false;
-    } finally {
-      lock.writeLock().unlock();
-    }
-  }
-
-  /** Returns the number of bytes in memory used by this object. */
-  public int getMemoryUsage() {
-    return cache == null ? 0 : cache.getMemoryUsage();
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CollisionMap.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CollisionMap.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CollisionMap.java	2013-01-06 18:41:12.306961728 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CollisionMap.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,230 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.cl2o;
-
-import java.util.Iterator;
-import java.util.NoSuchElementException;
-
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * HashMap to store colliding labels. See {@link CompactLabelToOrdinal} for
- * details.
- * 
- * @lucene.experimental
- */
-public class CollisionMap {
-
-  private int capacity;
-  private float loadFactor;
-  private int size;
-  private int threshold;
-
-  static class Entry {
-    int offset;
-    int cid;
-    Entry next;
-    int hash;
-
-    Entry(int offset, int cid, int h, Entry e) {
-      this.offset = offset;
-      this.cid = cid;
-      this.next = e;
-      this.hash = h;
-    }
-  }
-
-  private CharBlockArray labelRepository;
-
-  private Entry[] entries;
-
-  CollisionMap(CharBlockArray labelRepository) {
-    this(16 * 1024, 0.75f, labelRepository);
-  }
-
-  CollisionMap(int initialCapacity, CharBlockArray labelRepository) {
-    this(initialCapacity, 0.75f, labelRepository);
-  }
-
-  private CollisionMap(int initialCapacity, float loadFactor, CharBlockArray labelRepository) {
-    this.labelRepository = labelRepository;
-    this.loadFactor = loadFactor;
-    this.capacity = CompactLabelToOrdinal.determineCapacity(2, initialCapacity);
-
-    this.entries = new Entry[this.capacity];
-    this.threshold = (int) (this.capacity * this.loadFactor);
-  }
-
-  public int size() {
-    return this.size;
-  }
-
-  public int capacity() {
-    return this.capacity;
-  }
-
-  private void grow() {
-    int newCapacity = this.capacity * 2;
-    Entry[] newEntries = new Entry[newCapacity];
-    Entry[] src = this.entries;
-
-    for (int j = 0; j < src.length; j++) {
-      Entry e = src[j];
-      if (e != null) {
-        src[j] = null;
-        do {
-          Entry next = e.next;
-          int hash = e.hash;
-          int i = indexFor(hash, newCapacity);  
-          e.next = newEntries[i];
-          newEntries[i] = e;
-          e = next;
-        } while (e != null);
-      }
-    }
-
-    this.capacity = newCapacity;
-    this.entries = newEntries;
-    this.threshold = (int) (this.capacity * this.loadFactor);
-  }
-
-  public int get(CategoryPath label, int hash) {
-    int bucketIndex = indexFor(hash, this.capacity);
-    Entry e = this.entries[bucketIndex];
-
-    while (e != null && !(hash == e.hash && CategoryPathUtils.equalsToSerialized(label, labelRepository, e.offset))) { 
-      e = e.next;
-    }
-    if (e == null) {
-      return LabelToOrdinal.INVALID_ORDINAL;
-    }
-
-    return e.cid;
-  }
-
-  public int addLabel(CategoryPath label, int hash, int cid) {
-    int bucketIndex = indexFor(hash, this.capacity);
-    for (Entry e = this.entries[bucketIndex]; e != null; e = e.next) {
-      if (e.hash == hash && CategoryPathUtils.equalsToSerialized(label, labelRepository, e.offset)) {
-        return e.cid;
-      }
-    }
-
-    // new string; add to label repository
-    int offset = labelRepository.length();
-    CategoryPathUtils.serialize(label, labelRepository);
-    addEntry(offset, cid, hash, bucketIndex);
-    return cid;
-  }
-
-  /**
-   * This method does not check if the same value is already in the map because
-   * we pass in an char-array offset, so so we now that we're in resize-mode
-   * here.
-   */
-  public void addLabelOffset(int hash, int offset, int cid) {
-    int bucketIndex = indexFor(hash, this.capacity);
-    addEntry(offset, cid, hash, bucketIndex);
-  }
-
-  private void addEntry(int offset, int cid, int hash, int bucketIndex) {
-    Entry e = this.entries[bucketIndex];
-    this.entries[bucketIndex] = new Entry(offset, cid, hash, e);
-    if (this.size++ >= this.threshold) {
-      grow();
-    }
-  }
-
-  Iterator<CollisionMap.Entry> entryIterator() {
-    return new EntryIterator(entries, size);
-  }
-
-  /**
-   * Returns index for hash code h. 
-   */
-  static int indexFor(int h, int length) {
-    return h & (length - 1);
-  }
-
-  /**
-   * Returns an estimate of the memory usage of this CollisionMap.
-   * @return The approximate number of bytes used by this structure.
-   */
-  int getMemoryUsage() {
-    int memoryUsage = 0;
-    if (this.entries != null) {
-      for (Entry e : this.entries) {
-        if (e != null) {
-          memoryUsage += (4 * 4);
-          for (Entry ee = e.next; ee != null; ee = ee.next) {
-            memoryUsage += (4 * 4);
-          }
-        }
-      }
-    }
-    return memoryUsage;
-  }
-
-  private class EntryIterator implements Iterator<Entry> {
-    Entry next;    // next entry to return
-    int index;        // current slot 
-    Entry[] ents;
-    
-    EntryIterator(Entry[] entries, int size) {
-      this.ents = entries;
-      Entry[] t = entries;
-      int i = t.length;
-      Entry n = null;
-      if (size != 0) { // advance to first entry
-        while (i > 0 && (n = t[--i]) == null) {
-          // advance
-        }
-      }
-      this.next = n;
-      this.index = i;
-    }
-
-    @Override
-    public boolean hasNext() {
-      return this.next != null;
-    }
-
-    @Override
-    public Entry next() { 
-      Entry e = this.next;
-      if (e == null) throw new NoSuchElementException();
-
-      Entry n = e.next;
-      Entry[] t = ents;
-      int i = this.index;
-      while (n == null && i > 0) {
-        n = t[--i];
-      }
-      this.index = i;
-      this.next = n;
-      return  e;
-    }
-
-    @Override
-    public void remove() {
-      throw new UnsupportedOperationException();
-    }
-
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CompactLabelToOrdinal.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CompactLabelToOrdinal.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CompactLabelToOrdinal.java	2013-01-06 18:41:12.306961728 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CompactLabelToOrdinal.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,465 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.cl2o;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.BufferedInputStream;
-import java.io.BufferedOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.util.Iterator;
-
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/**
- * This is a very efficient LabelToOrdinal implementation that uses a
- * CharBlockArray to store all labels and a configurable number of HashArrays to
- * reference the labels.
- * <p>
- * Since the HashArrays don't handle collisions, a {@link CollisionMap} is used
- * to store the colliding labels.
- * <p>
- * This data structure grows by adding a new HashArray whenever the number of
- * collisions in the {@link CollisionMap} exceeds {@code loadFactor} * 
- * {@link #getMaxOrdinal()}. Growing also includes reinserting all colliding
- * labels into the HashArrays to possibly reduce the number of collisions.
- * 
- * For setting the {@code loadFactor} see 
- * {@link #CompactLabelToOrdinal(int, float, int)}. 
- * 
- * <p>
- * This data structure has a much lower memory footprint (~30%) compared to a
- * Java HashMap&lt;String, Integer&gt;. It also only uses a small fraction of objects
- * a HashMap would use, thus limiting the GC overhead. Ingestion speed was also
- * ~50% faster compared to a HashMap for 3M unique labels.
- * 
- * @lucene.experimental
- */
-public class CompactLabelToOrdinal extends LabelToOrdinal {
-
-  public static final float DefaultLoadFactor = 0.15f;
-
-  static final char TERMINATOR_CHAR = 0xffff;
-  private static final int COLLISION = -5;
-
-  private HashArray[] hashArrays;
-  private CollisionMap collisionMap;
-  private CharBlockArray labelRepository;
-
-  private int capacity;
-  private int threshold;
-  private float loadFactor;
-
-  public int sizeOfMap() {
-    return this.collisionMap.size();
-  }
-
-  private CompactLabelToOrdinal() {
-  }
-
-  public CompactLabelToOrdinal(int initialCapacity, float loadFactor,
-                                int numHashArrays) {
-
-    this.hashArrays = new HashArray[numHashArrays];
-
-    this.capacity = determineCapacity((int) Math.pow(2, numHashArrays),
-        initialCapacity);
-    init();
-    this.collisionMap = new CollisionMap(this.labelRepository);
-
-    this.counter = 0;
-    this.loadFactor = loadFactor;
-
-    this.threshold = (int) (this.loadFactor * this.capacity);
-  }
-
-  static int determineCapacity(int minCapacity, int initialCapacity) {
-    int capacity = minCapacity;
-    while (capacity < initialCapacity) {
-      capacity <<= 1;
-    }
-    return capacity;
-  }
-
-  private void init() {
-    labelRepository = new CharBlockArray();
-    CategoryPathUtils.serialize(CategoryPath.EMPTY, labelRepository);
-
-    int c = this.capacity;
-    for (int i = 0; i < this.hashArrays.length; i++) {
-      this.hashArrays[i] = new HashArray(c);
-      c /= 2;
-    }
-  }
-
-  @Override
-  public void addLabel(CategoryPath label, int ordinal) {
-    if (collisionMap.size() > threshold) {
-      grow();
-    }
-
-    int hash = CompactLabelToOrdinal.stringHashCode(label);
-    for (int i = 0; i < this.hashArrays.length; i++) {
-      if (addLabel(this.hashArrays[i], label, hash, ordinal)) {
-        return;
-      }
-    }
-
-    int prevVal = collisionMap.addLabel(label, hash, ordinal);
-    if (prevVal != ordinal) {
-      throw new IllegalArgumentException("Label already exists: " + label.toString('/') + " prev ordinal " + prevVal);
-    }
-  }
-
-  @Override
-  public int getOrdinal(CategoryPath label) {
-    if (label == null) {
-      return LabelToOrdinal.INVALID_ORDINAL;
-    }
-
-    int hash = CompactLabelToOrdinal.stringHashCode(label);
-    for (int i = 0; i < this.hashArrays.length; i++) {
-      int ord = getOrdinal(this.hashArrays[i], label, hash);
-      if (ord != COLLISION) {
-        return ord;
-      }
-    }
-
-    return this.collisionMap.get(label, hash);
-  }
-
-  private void grow() {
-    HashArray temp = this.hashArrays[this.hashArrays.length - 1];
-
-    for (int i = this.hashArrays.length - 1; i > 0; i--) {
-      this.hashArrays[i] = this.hashArrays[i - 1];
-    }
-
-    this.capacity *= 2;
-    this.hashArrays[0] = new HashArray(this.capacity);
-
-    for (int i = 1; i < this.hashArrays.length; i++) {
-      int[] sourceOffsetArray = this.hashArrays[i].offsets;
-      int[] sourceCidsArray = this.hashArrays[i].cids;
-
-      for (int k = 0; k < sourceOffsetArray.length; k++) {
-
-        for (int j = 0; j < i && sourceOffsetArray[k] != 0; j++) {
-          int[] targetOffsetArray = this.hashArrays[j].offsets;
-          int[] targetCidsArray = this.hashArrays[j].cids;
-
-          int newIndex = indexFor(stringHashCode(
-              this.labelRepository, sourceOffsetArray[k]),
-              targetOffsetArray.length);
-          if (targetOffsetArray[newIndex] == 0) {
-            targetOffsetArray[newIndex] = sourceOffsetArray[k];
-            targetCidsArray[newIndex] = sourceCidsArray[k];
-            sourceOffsetArray[k] = 0;
-          }
-        }
-      }
-    }
-
-    for (int i = 0; i < temp.offsets.length; i++) {
-      int offset = temp.offsets[i];
-      if (offset > 0) {
-        int hash = stringHashCode(this.labelRepository, offset);
-        addLabelOffset(hash, temp.cids[i], offset);
-      }
-    }
-
-    CollisionMap oldCollisionMap = this.collisionMap;
-    this.collisionMap = new CollisionMap(oldCollisionMap.capacity(),
-        this.labelRepository);
-    this.threshold = (int) (this.capacity * this.loadFactor);
-
-    Iterator<CollisionMap.Entry> it = oldCollisionMap.entryIterator();
-    while (it.hasNext()) {
-      CollisionMap.Entry e = it.next();
-      addLabelOffset(stringHashCode(this.labelRepository, e.offset),
-          e.cid, e.offset);
-    }
-  }
-
-  private boolean addLabel(HashArray a, CategoryPath label, int hash, int ordinal) {
-    int index = CompactLabelToOrdinal.indexFor(hash, a.offsets.length);
-    int offset = a.offsets[index];
-
-    if (offset == 0) {
-      a.offsets[index] = this.labelRepository.length();
-      CategoryPathUtils.serialize(label, labelRepository);
-      a.cids[index] = ordinal;
-      return true;
-    }
-
-    return false;
-  }
-
-  private void addLabelOffset(int hash, int cid, int knownOffset) {
-    for (int i = 0; i < this.hashArrays.length; i++) {
-      if (addLabelOffsetToHashArray(this.hashArrays[i], hash, cid,
-          knownOffset)) {
-        return;
-      }
-    }
-
-    this.collisionMap.addLabelOffset(hash, knownOffset, cid);
-
-    if (this.collisionMap.size() > this.threshold) {
-      grow();
-    }
-  }
-
-  private boolean addLabelOffsetToHashArray(HashArray a, int hash, int ordinal,
-                                            int knownOffset) {
-
-    int index = CompactLabelToOrdinal.indexFor(hash, a.offsets.length);
-    int offset = a.offsets[index];
-
-    if (offset == 0) {
-      a.offsets[index] = knownOffset;
-      a.cids[index] = ordinal;
-      return true;
-    }
-
-    return false;
-  }
-
-  private int getOrdinal(HashArray a, CategoryPath label, int hash) {
-    if (label == null) {
-      return LabelToOrdinal.INVALID_ORDINAL;
-    }
-
-    int index = indexFor(hash, a.offsets.length);
-    int offset = a.offsets[index];
-    if (offset == 0) {
-      return LabelToOrdinal.INVALID_ORDINAL;
-    }
-
-    if (CategoryPathUtils.equalsToSerialized(label, labelRepository, offset)) {
-      return a.cids[index];
-    }
-
-    return COLLISION;
-  }
-
-  /** Returns index for hash code h. */
-  static int indexFor(int h, int length) {
-    return h & (length - 1);
-  }
-
-  // static int stringHashCode(String label) {
-  // int len = label.length();
-  // int hash = 0;
-  // int i;
-  // for (i = 0; i < len; ++i)
-  // hash = 33 * hash + label.charAt(i);
-  //
-  // hash = hash ^ ((hash >>> 20) ^ (hash >>> 12));
-  // hash = hash ^ (hash >>> 7) ^ (hash >>> 4);
-  //
-  // return hash;
-  //
-  // }
-
-  static int stringHashCode(CategoryPath label) {
-    int hash = label.hashCode();
-
-    hash = hash ^ ((hash >>> 20) ^ (hash >>> 12));
-    hash = hash ^ (hash >>> 7) ^ (hash >>> 4);
-
-    return hash;
-
-  }
-
-  static int stringHashCode(CharBlockArray labelRepository, int offset) {
-    int hash = CategoryPathUtils.hashCodeOfSerialized(labelRepository, offset);
-    hash = hash ^ ((hash >>> 20) ^ (hash >>> 12));
-    hash = hash ^ (hash >>> 7) ^ (hash >>> 4);
-    return hash;
-  }
-
-  // public static boolean equals(CharSequence label, CharBlockArray array,
-  // int offset) {
-  // // CONTINUE HERE
-  // int len = label.length();
-  // int bi = array.blockIndex(offset);
-  // CharBlockArray.Block b = array.blocks.get(bi);
-  // int index = array.indexInBlock(offset);
-  //
-  // for (int i = 0; i < len; i++) {
-  // if (label.charAt(i) != b.chars[index]) {
-  // return false;
-  // }
-  // index++;
-  // if (index == b.length) {
-  // b = array.blocks.get(++bi);
-  // index = 0;
-  // }
-  // }
-  //
-  // return b.chars[index] == TerminatorChar;
-  // }
-
-  /**
-   * Returns an estimate of the amount of memory used by this table. Called only in
-   * this package. Memory is consumed mainly by three structures: the hash arrays,
-   * label repository and collision map.
-   */
-  int getMemoryUsage() {
-    int memoryUsage = 0;
-    if (this.hashArrays != null) {
-      // HashArray capacity is instance-specific.
-      for (HashArray ha : this.hashArrays) {
-        // Each has 2 capacity-length arrays of ints.
-        memoryUsage += ( ha.capacity * 2 * 4 ) + 4;
-      }
-    }
-    if (this.labelRepository != null) {
-      // All blocks are the same size.
-      int blockSize = this.labelRepository.blockSize;
-      // Each block has room for blockSize UTF-16 chars.
-      int actualBlockSize = ( blockSize * 2 ) + 4;
-      memoryUsage += this.labelRepository.blocks.size() * actualBlockSize; 
-      memoryUsage += 8;   // Two int values for array as a whole.
-    }
-    if (this.collisionMap != null) {
-      memoryUsage += this.collisionMap.getMemoryUsage();
-    }
-    return memoryUsage;
-  }
-
-  /**
-   * Opens the file and reloads the CompactLabelToOrdinal. The file it expects
-   * is generated from the {@link #flush(File)} command.
-   */
-  static CompactLabelToOrdinal open(File file, float loadFactor,
-                                    int numHashArrays) throws IOException {
-    /**
-     * Part of the file is the labelRepository, which needs to be rehashed
-     * and label offsets re-added to the object. I am unsure as to why we
-     * can't just store these off in the file as well, but in keeping with
-     * the spirit of the original code, I did it this way. (ssuppe)
-     */
-    CompactLabelToOrdinal l2o = new CompactLabelToOrdinal();
-    l2o.loadFactor = loadFactor;
-    l2o.hashArrays = new HashArray[numHashArrays];
-
-    DataInputStream dis = null;
-    try {
-      dis = new DataInputStream(new BufferedInputStream(
-          new FileInputStream(file)));
-
-      // TaxiReader needs to load the "counter" or occupancy (L2O) to know
-      // the next unique facet. we used to load the delimiter too, but
-      // never used it.
-      l2o.counter = dis.readInt();
-
-      l2o.capacity = determineCapacity((int) Math.pow(2,
-          l2o.hashArrays.length), l2o.counter);
-      l2o.init();
-
-      // now read the chars
-      l2o.labelRepository = CharBlockArray.open(dis);
-
-      l2o.collisionMap = new CollisionMap(l2o.labelRepository);
-
-      // Calculate hash on the fly based on how CategoryPath hashes
-      // itself. Maybe in the future we can call some static based methods
-      // in CategoryPath so that this doesn't break again? I don't like
-      // having code in two different places...
-      int cid = 0;
-      // Skip the initial offset, it's the CategoryPath(0,0), which isn't
-      // a hashed value.
-      int offset = 1;
-      int lastStartOffset = offset;
-      // This loop really relies on a well-formed input (assumes pretty blindly
-      // that array offsets will work).  Since the initial file is machine 
-      // generated, I think this should be OK.
-      while (offset < l2o.labelRepository.length()) {
-        // identical code to CategoryPath.hashFromSerialized. since we need to
-        // advance offset, we cannot call the method directly. perhaps if we
-        // could pass a mutable Integer or something...
-        int length = (short) l2o.labelRepository.charAt(offset++);
-        int hash = length;
-        if (length != 0) {
-          for (int i = 0; i < length; i++) {
-            int len = (short) l2o.labelRepository.charAt(offset++);
-            hash = hash * 31 + l2o.labelRepository.subSequence(offset, offset + len).hashCode();
-            offset += len;
-          }
-        }
-        // Now that we've hashed the components of the label, do the
-        // final part of the hash algorithm.
-        hash = hash ^ ((hash >>> 20) ^ (hash >>> 12));
-        hash = hash ^ (hash >>> 7) ^ (hash >>> 4);
-        // Add the label, and let's keep going
-        l2o.addLabelOffset(hash, cid, lastStartOffset);
-        cid++;
-        lastStartOffset = offset;
-      }
-
-    } catch (ClassNotFoundException cnfe) {
-      throw new IOException("Invalid file format. Cannot deserialize.");
-    } finally {
-      if (dis != null) {
-        dis.close();
-      }
-    }
-
-    l2o.threshold = (int) (l2o.loadFactor * l2o.capacity);
-    return l2o;
-
-  }
-
-  void flush(File file) throws IOException {
-    FileOutputStream fos = new FileOutputStream(file);
-
-    try {
-      BufferedOutputStream os = new BufferedOutputStream(fos);
-
-      DataOutputStream dos = new DataOutputStream(os);
-      dos.writeInt(this.counter);
-
-      // write the labelRepository
-      this.labelRepository.flush(dos);
-
-      // Closes the data output stream
-      dos.close();
-
-    } finally {
-      fos.close();
-    }
-  }
-
-  private static final class HashArray {
-    int[] offsets;
-    int[] cids;
-
-    int capacity;
-
-    HashArray(int c) {
-      this.capacity = c;
-      this.offsets = new int[this.capacity];
-      this.cids = new int[this.capacity];
-    }
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/LabelToOrdinal.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/LabelToOrdinal.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/LabelToOrdinal.java	2013-01-06 18:41:12.306961728 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/LabelToOrdinal.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,60 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.cl2o;
-
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Abstract class for storing Label->Ordinal mappings in a taxonomy. 
- * 
- * @lucene.experimental
- */
-public abstract class LabelToOrdinal {
-
-  protected int counter;
-  public static final int INVALID_ORDINAL = -2;
-
-  /**
-   * return the maximal Ordinal assigned so far
-   */
-  public int getMaxOrdinal() {
-    return this.counter;
-  }
-
-  /**
-   * Returns the next unassigned ordinal. The default behavior of this method
-   * is to simply increment a counter.
-   */
-  public int getNextOrdinal() {
-    return this.counter++;
-  }
-
-  /**
-   * Adds a new label if its not yet in the table.
-   * Throws an {@link IllegalArgumentException} if the same label with
-   * a different ordinal was previoulsy added to this table.
-   */
-  public abstract void addLabel(CategoryPath label, int ordinal);
-
-  /**
-   * @return the ordinal assigned to the given label, 
-   * or {@link #INVALID_ORDINAL} if the label cannot be found in this table.
-   */
-  public abstract int getOrdinal(CategoryPath label);
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/package.html simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/package.html
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/package.html	2012-05-02 06:41:08.679778393 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/package.html	1969-12-31 19:00:00.000000000 -0500
@@ -1,27 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Category->Ordinal caching implementation using an optimized data-structures</title>
-</head>
-<body>
-	<h1>Category->Ordinal caching implementation using an optimized data-structures</h1>
-	
-	The internal map data structure consumes less memory (~30%) and is faster (~50%) compared to a
- 	Java HashMap&lt;String, Integer&gt;.
-</body>
-</html>
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/Cl2oTaxonomyWriterCache.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/Cl2oTaxonomyWriterCache.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/Cl2oTaxonomyWriterCache.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/Cl2oTaxonomyWriterCache.java	2013-11-26 15:06:15.822617859 -0500
@@ -0,0 +1,98 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+import java.util.concurrent.locks.ReadWriteLock;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * {@link TaxonomyWriterCache} using {@link CompactLabelToOrdinal}. Although
+ * called cache, it maintains in memory all the mappings from category to
+ * ordinal, relying on that {@link CompactLabelToOrdinal} is an efficient
+ * mapping for this purpose.
+ * 
+ * @lucene.experimental
+ */
+public class Cl2oTaxonomyWriterCache implements TaxonomyWriterCache {  
+
+  private final ReadWriteLock lock = new ReentrantReadWriteLock();
+  private final int initialCapcity, numHashArrays;
+  private final float loadFactor;
+  
+  private volatile CompactLabelToOrdinal cache;
+
+  public Cl2oTaxonomyWriterCache(int initialCapcity, float loadFactor, int numHashArrays) {
+    this.cache = new CompactLabelToOrdinal(initialCapcity, loadFactor, numHashArrays);
+    this.initialCapcity = initialCapcity;
+    this.numHashArrays = numHashArrays;
+    this.loadFactor = loadFactor;
+  }
+
+  @Override
+  public void clear() {
+    lock.writeLock().lock();
+    try {
+      cache = new CompactLabelToOrdinal(initialCapcity, loadFactor, numHashArrays);
+    } finally {
+      lock.writeLock().unlock();
+    }
+  }
+  
+  @Override
+  public synchronized void close() {
+    cache = null;
+  }
+
+  @Override
+  public boolean isFull() {
+    // This cache is never full
+    return false;
+  }
+
+  @Override
+  public int get(FacetLabel categoryPath) {
+    lock.readLock().lock();
+    try {
+      return cache.getOrdinal(categoryPath);
+    } finally {
+      lock.readLock().unlock();
+    }
+  }
+
+  @Override
+  public boolean put(FacetLabel categoryPath, int ordinal) {
+    lock.writeLock().lock();
+    try {
+      cache.addLabel(categoryPath, ordinal);
+      // Tell the caller we didn't clear part of the cache, so it doesn't
+      // have to flush its on-disk index now
+      return false;
+    } finally {
+      lock.writeLock().unlock();
+    }
+  }
+
+  /** Returns the number of bytes in memory used by this object. */
+  public int getMemoryUsage() {
+    return cache == null ? 0 : cache.getMemoryUsage();
+  }
+
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CollisionMap.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CollisionMap.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CollisionMap.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CollisionMap.java	2013-11-26 15:06:14.342617864 -0500
@@ -0,0 +1,230 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+import java.util.Iterator;
+import java.util.NoSuchElementException;
+
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * HashMap to store colliding labels. See {@link CompactLabelToOrdinal} for
+ * details.
+ * 
+ * @lucene.experimental
+ */
+public class CollisionMap {
+
+  private int capacity;
+  private float loadFactor;
+  private int size;
+  private int threshold;
+
+  static class Entry {
+    int offset;
+    int cid;
+    Entry next;
+    int hash;
+
+    Entry(int offset, int cid, int h, Entry e) {
+      this.offset = offset;
+      this.cid = cid;
+      this.next = e;
+      this.hash = h;
+    }
+  }
+
+  private CharBlockArray labelRepository;
+
+  private Entry[] entries;
+
+  CollisionMap(CharBlockArray labelRepository) {
+    this(16 * 1024, 0.75f, labelRepository);
+  }
+
+  CollisionMap(int initialCapacity, CharBlockArray labelRepository) {
+    this(initialCapacity, 0.75f, labelRepository);
+  }
+
+  private CollisionMap(int initialCapacity, float loadFactor, CharBlockArray labelRepository) {
+    this.labelRepository = labelRepository;
+    this.loadFactor = loadFactor;
+    this.capacity = CompactLabelToOrdinal.determineCapacity(2, initialCapacity);
+
+    this.entries = new Entry[this.capacity];
+    this.threshold = (int) (this.capacity * this.loadFactor);
+  }
+
+  public int size() {
+    return this.size;
+  }
+
+  public int capacity() {
+    return this.capacity;
+  }
+
+  private void grow() {
+    int newCapacity = this.capacity * 2;
+    Entry[] newEntries = new Entry[newCapacity];
+    Entry[] src = this.entries;
+
+    for (int j = 0; j < src.length; j++) {
+      Entry e = src[j];
+      if (e != null) {
+        src[j] = null;
+        do {
+          Entry next = e.next;
+          int hash = e.hash;
+          int i = indexFor(hash, newCapacity);  
+          e.next = newEntries[i];
+          newEntries[i] = e;
+          e = next;
+        } while (e != null);
+      }
+    }
+
+    this.capacity = newCapacity;
+    this.entries = newEntries;
+    this.threshold = (int) (this.capacity * this.loadFactor);
+  }
+
+  public int get(FacetLabel label, int hash) {
+    int bucketIndex = indexFor(hash, this.capacity);
+    Entry e = this.entries[bucketIndex];
+
+    while (e != null && !(hash == e.hash && CategoryPathUtils.equalsToSerialized(label, labelRepository, e.offset))) { 
+      e = e.next;
+    }
+    if (e == null) {
+      return LabelToOrdinal.INVALID_ORDINAL;
+    }
+
+    return e.cid;
+  }
+
+  public int addLabel(FacetLabel label, int hash, int cid) {
+    int bucketIndex = indexFor(hash, this.capacity);
+    for (Entry e = this.entries[bucketIndex]; e != null; e = e.next) {
+      if (e.hash == hash && CategoryPathUtils.equalsToSerialized(label, labelRepository, e.offset)) {
+        return e.cid;
+      }
+    }
+
+    // new string; add to label repository
+    int offset = labelRepository.length();
+    CategoryPathUtils.serialize(label, labelRepository);
+    addEntry(offset, cid, hash, bucketIndex);
+    return cid;
+  }
+
+  /**
+   * This method does not check if the same value is already in the map because
+   * we pass in an char-array offset, so so we now that we're in resize-mode
+   * here.
+   */
+  public void addLabelOffset(int hash, int offset, int cid) {
+    int bucketIndex = indexFor(hash, this.capacity);
+    addEntry(offset, cid, hash, bucketIndex);
+  }
+
+  private void addEntry(int offset, int cid, int hash, int bucketIndex) {
+    Entry e = this.entries[bucketIndex];
+    this.entries[bucketIndex] = new Entry(offset, cid, hash, e);
+    if (this.size++ >= this.threshold) {
+      grow();
+    }
+  }
+
+  Iterator<CollisionMap.Entry> entryIterator() {
+    return new EntryIterator(entries, size);
+  }
+
+  /**
+   * Returns index for hash code h. 
+   */
+  static int indexFor(int h, int length) {
+    return h & (length - 1);
+  }
+
+  /**
+   * Returns an estimate of the memory usage of this CollisionMap.
+   * @return The approximate number of bytes used by this structure.
+   */
+  int getMemoryUsage() {
+    int memoryUsage = 0;
+    if (this.entries != null) {
+      for (Entry e : this.entries) {
+        if (e != null) {
+          memoryUsage += (4 * 4);
+          for (Entry ee = e.next; ee != null; ee = ee.next) {
+            memoryUsage += (4 * 4);
+          }
+        }
+      }
+    }
+    return memoryUsage;
+  }
+
+  private class EntryIterator implements Iterator<Entry> {
+    Entry next;    // next entry to return
+    int index;        // current slot 
+    Entry[] ents;
+    
+    EntryIterator(Entry[] entries, int size) {
+      this.ents = entries;
+      Entry[] t = entries;
+      int i = t.length;
+      Entry n = null;
+      if (size != 0) { // advance to first entry
+        while (i > 0 && (n = t[--i]) == null) {
+          // advance
+        }
+      }
+      this.next = n;
+      this.index = i;
+    }
+
+    @Override
+    public boolean hasNext() {
+      return this.next != null;
+    }
+
+    @Override
+    public Entry next() { 
+      Entry e = this.next;
+      if (e == null) throw new NoSuchElementException();
+
+      Entry n = e.next;
+      Entry[] t = ents;
+      int i = this.index;
+      while (n == null && i > 0) {
+        n = t[--i];
+      }
+      this.index = i;
+      this.next = n;
+      return  e;
+    }
+
+    @Override
+    public void remove() {
+      throw new UnsupportedOperationException();
+    }
+
+  }
+
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CompactLabelToOrdinal.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CompactLabelToOrdinal.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CompactLabelToOrdinal.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CompactLabelToOrdinal.java	2013-12-01 16:02:26.737231693 -0500
@@ -0,0 +1,465 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedInputStream;
+import java.io.BufferedOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.util.Iterator;
+
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+
+/**
+ * This is a very efficient LabelToOrdinal implementation that uses a
+ * CharBlockArray to store all labels and a configurable number of HashArrays to
+ * reference the labels.
+ * <p>
+ * Since the HashArrays don't handle collisions, a {@link CollisionMap} is used
+ * to store the colliding labels.
+ * <p>
+ * This data structure grows by adding a new HashArray whenever the number of
+ * collisions in the {@link CollisionMap} exceeds {@code loadFactor} * 
+ * {@link #getMaxOrdinal()}. Growing also includes reinserting all colliding
+ * labels into the HashArrays to possibly reduce the number of collisions.
+ * 
+ * For setting the {@code loadFactor} see 
+ * {@link #CompactLabelToOrdinal(int, float, int)}. 
+ * 
+ * <p>
+ * This data structure has a much lower memory footprint (~30%) compared to a
+ * Java HashMap&lt;String, Integer&gt;. It also only uses a small fraction of objects
+ * a HashMap would use, thus limiting the GC overhead. Ingestion speed was also
+ * ~50% faster compared to a HashMap for 3M unique labels.
+ * 
+ * @lucene.experimental
+ */
+public class CompactLabelToOrdinal extends LabelToOrdinal {
+
+  public static final float DefaultLoadFactor = 0.15f;
+
+  static final char TERMINATOR_CHAR = 0xffff;
+  private static final int COLLISION = -5;
+
+  private HashArray[] hashArrays;
+  private CollisionMap collisionMap;
+  private CharBlockArray labelRepository;
+
+  private int capacity;
+  private int threshold;
+  private float loadFactor;
+
+  public int sizeOfMap() {
+    return this.collisionMap.size();
+  }
+
+  private CompactLabelToOrdinal() {
+  }
+
+  public CompactLabelToOrdinal(int initialCapacity, float loadFactor,
+                                int numHashArrays) {
+
+    this.hashArrays = new HashArray[numHashArrays];
+
+    this.capacity = determineCapacity((int) Math.pow(2, numHashArrays),
+        initialCapacity);
+    init();
+    this.collisionMap = new CollisionMap(this.labelRepository);
+
+    this.counter = 0;
+    this.loadFactor = loadFactor;
+
+    this.threshold = (int) (this.loadFactor * this.capacity);
+  }
+
+  static int determineCapacity(int minCapacity, int initialCapacity) {
+    int capacity = minCapacity;
+    while (capacity < initialCapacity) {
+      capacity <<= 1;
+    }
+    return capacity;
+  }
+
+  private void init() {
+    labelRepository = new CharBlockArray();
+    CategoryPathUtils.serialize(new FacetLabel(), labelRepository);
+
+    int c = this.capacity;
+    for (int i = 0; i < this.hashArrays.length; i++) {
+      this.hashArrays[i] = new HashArray(c);
+      c /= 2;
+    }
+  }
+
+  @Override
+  public void addLabel(FacetLabel label, int ordinal) {
+    if (collisionMap.size() > threshold) {
+      grow();
+    }
+
+    int hash = CompactLabelToOrdinal.stringHashCode(label);
+    for (int i = 0; i < this.hashArrays.length; i++) {
+      if (addLabel(this.hashArrays[i], label, hash, ordinal)) {
+        return;
+      }
+    }
+
+    int prevVal = collisionMap.addLabel(label, hash, ordinal);
+    if (prevVal != ordinal) {
+      throw new IllegalArgumentException("Label already exists: " + label + " prev ordinal " + prevVal);
+    }
+  }
+
+  @Override
+  public int getOrdinal(FacetLabel label) {
+    if (label == null) {
+      return LabelToOrdinal.INVALID_ORDINAL;
+    }
+
+    int hash = CompactLabelToOrdinal.stringHashCode(label);
+    for (int i = 0; i < this.hashArrays.length; i++) {
+      int ord = getOrdinal(this.hashArrays[i], label, hash);
+      if (ord != COLLISION) {
+        return ord;
+      }
+    }
+
+    return this.collisionMap.get(label, hash);
+  }
+
+  private void grow() {
+    HashArray temp = this.hashArrays[this.hashArrays.length - 1];
+
+    for (int i = this.hashArrays.length - 1; i > 0; i--) {
+      this.hashArrays[i] = this.hashArrays[i - 1];
+    }
+
+    this.capacity *= 2;
+    this.hashArrays[0] = new HashArray(this.capacity);
+
+    for (int i = 1; i < this.hashArrays.length; i++) {
+      int[] sourceOffsetArray = this.hashArrays[i].offsets;
+      int[] sourceCidsArray = this.hashArrays[i].cids;
+
+      for (int k = 0; k < sourceOffsetArray.length; k++) {
+
+        for (int j = 0; j < i && sourceOffsetArray[k] != 0; j++) {
+          int[] targetOffsetArray = this.hashArrays[j].offsets;
+          int[] targetCidsArray = this.hashArrays[j].cids;
+
+          int newIndex = indexFor(stringHashCode(
+              this.labelRepository, sourceOffsetArray[k]),
+              targetOffsetArray.length);
+          if (targetOffsetArray[newIndex] == 0) {
+            targetOffsetArray[newIndex] = sourceOffsetArray[k];
+            targetCidsArray[newIndex] = sourceCidsArray[k];
+            sourceOffsetArray[k] = 0;
+          }
+        }
+      }
+    }
+
+    for (int i = 0; i < temp.offsets.length; i++) {
+      int offset = temp.offsets[i];
+      if (offset > 0) {
+        int hash = stringHashCode(this.labelRepository, offset);
+        addLabelOffset(hash, temp.cids[i], offset);
+      }
+    }
+
+    CollisionMap oldCollisionMap = this.collisionMap;
+    this.collisionMap = new CollisionMap(oldCollisionMap.capacity(),
+        this.labelRepository);
+    this.threshold = (int) (this.capacity * this.loadFactor);
+
+    Iterator<CollisionMap.Entry> it = oldCollisionMap.entryIterator();
+    while (it.hasNext()) {
+      CollisionMap.Entry e = it.next();
+      addLabelOffset(stringHashCode(this.labelRepository, e.offset),
+          e.cid, e.offset);
+    }
+  }
+
+  private boolean addLabel(HashArray a, FacetLabel label, int hash, int ordinal) {
+    int index = CompactLabelToOrdinal.indexFor(hash, a.offsets.length);
+    int offset = a.offsets[index];
+
+    if (offset == 0) {
+      a.offsets[index] = this.labelRepository.length();
+      CategoryPathUtils.serialize(label, labelRepository);
+      a.cids[index] = ordinal;
+      return true;
+    }
+
+    return false;
+  }
+
+  private void addLabelOffset(int hash, int cid, int knownOffset) {
+    for (int i = 0; i < this.hashArrays.length; i++) {
+      if (addLabelOffsetToHashArray(this.hashArrays[i], hash, cid,
+          knownOffset)) {
+        return;
+      }
+    }
+
+    this.collisionMap.addLabelOffset(hash, knownOffset, cid);
+
+    if (this.collisionMap.size() > this.threshold) {
+      grow();
+    }
+  }
+
+  private boolean addLabelOffsetToHashArray(HashArray a, int hash, int ordinal,
+                                            int knownOffset) {
+
+    int index = CompactLabelToOrdinal.indexFor(hash, a.offsets.length);
+    int offset = a.offsets[index];
+
+    if (offset == 0) {
+      a.offsets[index] = knownOffset;
+      a.cids[index] = ordinal;
+      return true;
+    }
+
+    return false;
+  }
+
+  private int getOrdinal(HashArray a, FacetLabel label, int hash) {
+    if (label == null) {
+      return LabelToOrdinal.INVALID_ORDINAL;
+    }
+
+    int index = indexFor(hash, a.offsets.length);
+    int offset = a.offsets[index];
+    if (offset == 0) {
+      return LabelToOrdinal.INVALID_ORDINAL;
+    }
+
+    if (CategoryPathUtils.equalsToSerialized(label, labelRepository, offset)) {
+      return a.cids[index];
+    }
+
+    return COLLISION;
+  }
+
+  /** Returns index for hash code h. */
+  static int indexFor(int h, int length) {
+    return h & (length - 1);
+  }
+
+  // static int stringHashCode(String label) {
+  // int len = label.length();
+  // int hash = 0;
+  // int i;
+  // for (i = 0; i < len; ++i)
+  // hash = 33 * hash + label.charAt(i);
+  //
+  // hash = hash ^ ((hash >>> 20) ^ (hash >>> 12));
+  // hash = hash ^ (hash >>> 7) ^ (hash >>> 4);
+  //
+  // return hash;
+  //
+  // }
+
+  static int stringHashCode(FacetLabel label) {
+    int hash = label.hashCode();
+
+    hash = hash ^ ((hash >>> 20) ^ (hash >>> 12));
+    hash = hash ^ (hash >>> 7) ^ (hash >>> 4);
+
+    return hash;
+
+  }
+
+  static int stringHashCode(CharBlockArray labelRepository, int offset) {
+    int hash = CategoryPathUtils.hashCodeOfSerialized(labelRepository, offset);
+    hash = hash ^ ((hash >>> 20) ^ (hash >>> 12));
+    hash = hash ^ (hash >>> 7) ^ (hash >>> 4);
+    return hash;
+  }
+
+  // public static boolean equals(CharSequence label, CharBlockArray array,
+  // int offset) {
+  // // CONTINUE HERE
+  // int len = label.length();
+  // int bi = array.blockIndex(offset);
+  // CharBlockArray.Block b = array.blocks.get(bi);
+  // int index = array.indexInBlock(offset);
+  //
+  // for (int i = 0; i < len; i++) {
+  // if (label.charAt(i) != b.chars[index]) {
+  // return false;
+  // }
+  // index++;
+  // if (index == b.length) {
+  // b = array.blocks.get(++bi);
+  // index = 0;
+  // }
+  // }
+  //
+  // return b.chars[index] == TerminatorChar;
+  // }
+
+  /**
+   * Returns an estimate of the amount of memory used by this table. Called only in
+   * this package. Memory is consumed mainly by three structures: the hash arrays,
+   * label repository and collision map.
+   */
+  int getMemoryUsage() {
+    int memoryUsage = 0;
+    if (this.hashArrays != null) {
+      // HashArray capacity is instance-specific.
+      for (HashArray ha : this.hashArrays) {
+        // Each has 2 capacity-length arrays of ints.
+        memoryUsage += ( ha.capacity * 2 * 4 ) + 4;
+      }
+    }
+    if (this.labelRepository != null) {
+      // All blocks are the same size.
+      int blockSize = this.labelRepository.blockSize;
+      // Each block has room for blockSize UTF-16 chars.
+      int actualBlockSize = ( blockSize * 2 ) + 4;
+      memoryUsage += this.labelRepository.blocks.size() * actualBlockSize; 
+      memoryUsage += 8;   // Two int values for array as a whole.
+    }
+    if (this.collisionMap != null) {
+      memoryUsage += this.collisionMap.getMemoryUsage();
+    }
+    return memoryUsage;
+  }
+
+  /**
+   * Opens the file and reloads the CompactLabelToOrdinal. The file it expects
+   * is generated from the {@link #flush(File)} command.
+   */
+  static CompactLabelToOrdinal open(File file, float loadFactor,
+                                    int numHashArrays) throws IOException {
+    /**
+     * Part of the file is the labelRepository, which needs to be rehashed
+     * and label offsets re-added to the object. I am unsure as to why we
+     * can't just store these off in the file as well, but in keeping with
+     * the spirit of the original code, I did it this way. (ssuppe)
+     */
+    CompactLabelToOrdinal l2o = new CompactLabelToOrdinal();
+    l2o.loadFactor = loadFactor;
+    l2o.hashArrays = new HashArray[numHashArrays];
+
+    DataInputStream dis = null;
+    try {
+      dis = new DataInputStream(new BufferedInputStream(
+          new FileInputStream(file)));
+
+      // TaxiReader needs to load the "counter" or occupancy (L2O) to know
+      // the next unique facet. we used to load the delimiter too, but
+      // never used it.
+      l2o.counter = dis.readInt();
+
+      l2o.capacity = determineCapacity((int) Math.pow(2,
+          l2o.hashArrays.length), l2o.counter);
+      l2o.init();
+
+      // now read the chars
+      l2o.labelRepository = CharBlockArray.open(dis);
+
+      l2o.collisionMap = new CollisionMap(l2o.labelRepository);
+
+      // Calculate hash on the fly based on how CategoryPath hashes
+      // itself. Maybe in the future we can call some static based methods
+      // in CategoryPath so that this doesn't break again? I don't like
+      // having code in two different places...
+      int cid = 0;
+      // Skip the initial offset, it's the CategoryPath(0,0), which isn't
+      // a hashed value.
+      int offset = 1;
+      int lastStartOffset = offset;
+      // This loop really relies on a well-formed input (assumes pretty blindly
+      // that array offsets will work).  Since the initial file is machine 
+      // generated, I think this should be OK.
+      while (offset < l2o.labelRepository.length()) {
+        // identical code to CategoryPath.hashFromSerialized. since we need to
+        // advance offset, we cannot call the method directly. perhaps if we
+        // could pass a mutable Integer or something...
+        int length = (short) l2o.labelRepository.charAt(offset++);
+        int hash = length;
+        if (length != 0) {
+          for (int i = 0; i < length; i++) {
+            int len = (short) l2o.labelRepository.charAt(offset++);
+            hash = hash * 31 + l2o.labelRepository.subSequence(offset, offset + len).hashCode();
+            offset += len;
+          }
+        }
+        // Now that we've hashed the components of the label, do the
+        // final part of the hash algorithm.
+        hash = hash ^ ((hash >>> 20) ^ (hash >>> 12));
+        hash = hash ^ (hash >>> 7) ^ (hash >>> 4);
+        // Add the label, and let's keep going
+        l2o.addLabelOffset(hash, cid, lastStartOffset);
+        cid++;
+        lastStartOffset = offset;
+      }
+
+    } catch (ClassNotFoundException cnfe) {
+      throw new IOException("Invalid file format. Cannot deserialize.");
+    } finally {
+      if (dis != null) {
+        dis.close();
+      }
+    }
+
+    l2o.threshold = (int) (l2o.loadFactor * l2o.capacity);
+    return l2o;
+
+  }
+
+  void flush(File file) throws IOException {
+    FileOutputStream fos = new FileOutputStream(file);
+
+    try {
+      BufferedOutputStream os = new BufferedOutputStream(fos);
+
+      DataOutputStream dos = new DataOutputStream(os);
+      dos.writeInt(this.counter);
+
+      // write the labelRepository
+      this.labelRepository.flush(dos);
+
+      // Closes the data output stream
+      dos.close();
+
+    } finally {
+      fos.close();
+    }
+  }
+
+  private static final class HashArray {
+    int[] offsets;
+    int[] cids;
+
+    int capacity;
+
+    HashArray(int c) {
+      this.capacity = c;
+      this.offsets = new int[this.capacity];
+      this.cids = new int[this.capacity];
+    }
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/LabelToOrdinal.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/LabelToOrdinal.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/LabelToOrdinal.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/LabelToOrdinal.java	2013-11-26 15:06:10.878617877 -0500
@@ -0,0 +1,60 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Abstract class for storing Label->Ordinal mappings in a taxonomy. 
+ * 
+ * @lucene.experimental
+ */
+public abstract class LabelToOrdinal {
+
+  protected int counter;
+  public static final int INVALID_ORDINAL = -2;
+
+  /**
+   * return the maximal Ordinal assigned so far
+   */
+  public int getMaxOrdinal() {
+    return this.counter;
+  }
+
+  /**
+   * Returns the next unassigned ordinal. The default behavior of this method
+   * is to simply increment a counter.
+   */
+  public int getNextOrdinal() {
+    return this.counter++;
+  }
+
+  /**
+   * Adds a new label if its not yet in the table.
+   * Throws an {@link IllegalArgumentException} if the same label with
+   * a different ordinal was previoulsy added to this table.
+   */
+  public abstract void addLabel(FacetLabel label, int ordinal);
+
+  /**
+   * @return the ordinal assigned to the given label, 
+   * or {@link #INVALID_ORDINAL} if the label cannot be found in this table.
+   */
+  public abstract int getOrdinal(FacetLabel label);
+
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/LruTaxonomyWriterCache.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/LruTaxonomyWriterCache.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/LruTaxonomyWriterCache.java	2013-01-06 18:41:12.310961727 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/LruTaxonomyWriterCache.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,105 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.lru;
-
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * LRU {@link TaxonomyWriterCache} - good choice for huge taxonomies.
- * 
- * @lucene.experimental
- */
-public class LruTaxonomyWriterCache implements TaxonomyWriterCache {
-
-  /**
-   * Determines cache type.
-   * For guaranteed correctness - not relying on no-collisions in the hash
-   * function, LRU_STRING should be used.
-   */
-  public enum LRUType { LRU_HASHED, LRU_STRING }
-
-  private NameIntCacheLRU cache;
-
-  public LruTaxonomyWriterCache(int cacheSize) {
-    // TODO (Facet): choose between NameHashIntCacheLRU and NameIntCacheLRU.
-    // For guaranteed correctness - not relying on no-collisions in the hash
-    // function, NameIntCacheLRU should be used:
-    // On the other hand, NameHashIntCacheLRU takes less RAM but if there
-    // are collisions (which we never found) two different paths would be
-    // mapped to the same ordinal...
-    this(cacheSize, LRUType.LRU_HASHED);
-  }
-
-  public LruTaxonomyWriterCache(int cacheSize, LRUType lruType) {
-    // TODO (Facet): choose between NameHashIntCacheLRU and NameIntCacheLRU.
-    // For guaranteed correctness - not relying on no-collisions in the hash
-    // function, NameIntCacheLRU should be used:
-    // On the other hand, NameHashIntCacheLRU takes less RAM but if there
-    // are collisions (which we never found) two different paths would be
-    // mapped to the same ordinal...
-    if (lruType == LRUType.LRU_HASHED) {
-      this.cache = new NameHashIntCacheLRU(cacheSize);
-    } else {
-      this.cache = new NameIntCacheLRU(cacheSize);
-    }
-  }
-
-  @Override
-  public synchronized boolean isFull() {
-    return cache.getSize() == cache.getMaxSize();
-  }
-
-  @Override
-  public synchronized void clear() {
-    cache.clear();
-  }
-  
-  @Override
-  public synchronized void close() {
-    cache.clear();
-    cache = null;
-  }
-
-  @Override
-  public synchronized int get(CategoryPath categoryPath) {
-    Integer res = cache.get(categoryPath);
-    if (res == null) {
-      return -1;
-    }
-
-    return res.intValue();
-  }
-
-  @Override
-  public synchronized boolean put(CategoryPath categoryPath, int ordinal) {
-    boolean ret = cache.put(categoryPath, new Integer(ordinal));
-    // If the cache is full, we need to clear one or more old entries
-    // from the cache. However, if we delete from the cache a recent
-    // addition that isn't yet in our reader, for this entry to be
-    // visible to us we need to make sure that the changes have been
-    // committed and we reopen the reader. Because this is a slow
-    // operation, we don't delete entries one-by-one but rather in bulk
-    // (put() removes the 2/3rd oldest entries).
-    if (ret) {
-      cache.makeRoomLRU();
-    }
-    return ret;
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameHashIntCacheLRU.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameHashIntCacheLRU.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameHashIntCacheLRU.java	2013-02-28 09:00:26.706312171 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameHashIntCacheLRU.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,47 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.lru;
-
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An an LRU cache of mapping from name to int.
- * Used to cache Ordinals of category paths.
- * It uses as key, hash of the path instead of the path.
- * This way the cache takes less RAM, but correctness depends on
- * assuming no collisions. 
- * 
- * @lucene.experimental
- */
-public class NameHashIntCacheLRU extends NameIntCacheLRU {
-
-  NameHashIntCacheLRU(int maxCacheSize) {
-    super(maxCacheSize);
-  }
-
-  @Override
-  Object key(CategoryPath name) {
-    return new Long(name.longHashCode());
-  }
-
-  @Override
-  Object key(CategoryPath name, int prefixLen) {
-    return new Long(name.subpath(prefixLen).longHashCode());
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameIntCacheLRU.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameIntCacheLRU.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameIntCacheLRU.java	2013-01-06 18:41:12.310961727 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameIntCacheLRU.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,131 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.lru;
-
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.LinkedHashMap;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An an LRU cache of mapping from name to int.
- * Used to cache Ordinals of category paths.
- * 
- * @lucene.experimental
- */
-// Note: Nothing in this class is synchronized. The caller is assumed to be
-// synchronized so that no two methods of this class are called concurrently.
-class NameIntCacheLRU {
-
-  private HashMap<Object, Integer> cache;
-  long nMisses = 0; // for debug
-  long nHits = 0;  // for debug
-  private int maxCacheSize;
-
-  NameIntCacheLRU(int maxCacheSize) {
-    this.maxCacheSize = maxCacheSize;
-    createCache(maxCacheSize);
-  }
-
-  public int getMaxSize() {
-    return maxCacheSize;
-  }
-  
-  public int getSize() {
-    return cache.size();
-  }
-
-  private void createCache (int maxSize) {
-    if (maxSize<Integer.MAX_VALUE) {
-      cache = new LinkedHashMap<Object, Integer>(1000,(float)0.7,true); //for LRU
-    } else {
-      cache = new HashMap<Object, Integer>(1000,(float)0.7); //no need for LRU
-    }
-  }
-
-  Integer get (CategoryPath name) {
-    Integer res = cache.get(key(name));
-    if (res==null) {
-      nMisses ++;
-    } else {
-      nHits ++;
-    }
-    return res;
-  }
-
-  /** Subclasses can override this to provide caching by e.g. hash of the string. */
-  Object key(CategoryPath name) {
-    return name;
-  }
-
-  Object key(CategoryPath name, int prefixLen) {
-    return name.subpath(prefixLen);
-  }
-
-  /**
-   * Add a new value to cache.
-   * Return true if cache became full and some room need to be made. 
-   */
-  boolean put (CategoryPath name, Integer val) {
-    cache.put(key(name), val);
-    return isCacheFull();
-  }
-
-  boolean put (CategoryPath name, int prefixLen, Integer val) {
-    cache.put(key(name, prefixLen), val);
-    return isCacheFull();
-  }
-
-  private boolean isCacheFull() {
-    return cache.size() > maxCacheSize;
-  }
-
-  void clear() {
-    cache.clear();
-  }
-
-  String stats() {
-    return "#miss="+nMisses+" #hit="+nHits;
-  }
-  
-  /**
-   * If cache is full remove least recently used entries from cache. Return true
-   * if anything was removed, false otherwise.
-   * 
-   * See comment in DirectoryTaxonomyWriter.addToCache(CategoryPath, int) for an
-   * explanation why we clean 2/3rds of the cache, and not just one entry.
-   */ 
-  boolean makeRoomLRU() {
-    if (!isCacheFull()) {
-      return false;
-    }
-    int n = cache.size() - (2*maxCacheSize)/3;
-    if (n<=0) {
-      return false;
-    }
-    Iterator<Object> it = cache.keySet().iterator();
-    int i = 0;
-    while (i<n && it.hasNext()) {
-      it.next();
-      it.remove();
-      i++;
-    }
-    return true;
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/package.html simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/package.html
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/package.html	2012-05-02 06:41:08.699778396 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/package.html	1969-12-31 19:00:00.000000000 -0500
@@ -1,25 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>An LRU cache implementation for the CategoryPath to Ordinal map</title>
-</head>
-<body>
-	<h1>An LRU cache implementation for the CategoryPath to Ordinal map</h1>
-	
-</body>
-</html>
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/LruTaxonomyWriterCache.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/LruTaxonomyWriterCache.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/LruTaxonomyWriterCache.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/LruTaxonomyWriterCache.java	2013-12-03 13:09:53.700886071 -0500
@@ -0,0 +1,105 @@
+ package org.apache.lucene.facet.taxonomy.writercache;
+
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * LRU {@link TaxonomyWriterCache} - good choice for huge taxonomies.
+ * 
+ * @lucene.experimental
+ */
+public class LruTaxonomyWriterCache implements TaxonomyWriterCache {
+
+  /**
+   * Determines cache type.
+   * For guaranteed correctness - not relying on no-collisions in the hash
+   * function, LRU_STRING should be used.
+   */
+  public enum LRUType { LRU_HASHED, LRU_STRING }
+
+  private NameIntCacheLRU cache;
+
+  public LruTaxonomyWriterCache(int cacheSize) {
+    // TODO (Facet): choose between NameHashIntCacheLRU and NameIntCacheLRU.
+    // For guaranteed correctness - not relying on no-collisions in the hash
+    // function, NameIntCacheLRU should be used:
+    // On the other hand, NameHashIntCacheLRU takes less RAM but if there
+    // are collisions (which we never found) two different paths would be
+    // mapped to the same ordinal...
+    this(cacheSize, LRUType.LRU_HASHED);
+  }
+
+  public LruTaxonomyWriterCache(int cacheSize, LRUType lruType) {
+    // TODO (Facet): choose between NameHashIntCacheLRU and NameIntCacheLRU.
+    // For guaranteed correctness - not relying on no-collisions in the hash
+    // function, NameIntCacheLRU should be used:
+    // On the other hand, NameHashIntCacheLRU takes less RAM but if there
+    // are collisions (which we never found) two different paths would be
+    // mapped to the same ordinal...
+    if (lruType == LRUType.LRU_HASHED) {
+      this.cache = new NameHashIntCacheLRU(cacheSize);
+    } else {
+      this.cache = new NameIntCacheLRU(cacheSize);
+    }
+  }
+
+  @Override
+  public synchronized boolean isFull() {
+    return cache.getSize() == cache.getMaxSize();
+  }
+
+  @Override
+  public synchronized void clear() {
+    cache.clear();
+  }
+  
+  @Override
+  public synchronized void close() {
+    cache.clear();
+    cache = null;
+  }
+
+  @Override
+  public synchronized int get(FacetLabel categoryPath) {
+    Integer res = cache.get(categoryPath);
+    if (res == null) {
+      return -1;
+    }
+
+    return res.intValue();
+  }
+
+  @Override
+  public synchronized boolean put(FacetLabel categoryPath, int ordinal) {
+    boolean ret = cache.put(categoryPath, new Integer(ordinal));
+    // If the cache is full, we need to clear one or more old entries
+    // from the cache. However, if we delete from the cache a recent
+    // addition that isn't yet in our reader, for this entry to be
+    // visible to us we need to make sure that the changes have been
+    // committed and we reopen the reader. Because this is a slow
+    // operation, we don't delete entries one-by-one but rather in bulk
+    // (put() removes the 2/3rd oldest entries).
+    if (ret) {
+      cache.makeRoomLRU();
+    }
+    return ret;
+  }
+
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/NameHashIntCacheLRU.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/NameHashIntCacheLRU.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/NameHashIntCacheLRU.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/NameHashIntCacheLRU.java	2013-12-03 13:08:35.416888147 -0500
@@ -0,0 +1,47 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An an LRU cache of mapping from name to int.
+ * Used to cache Ordinals of category paths.
+ * It uses as key, hash of the path instead of the path.
+ * This way the cache takes less RAM, but correctness depends on
+ * assuming no collisions. 
+ * 
+ * @lucene.experimental
+ */
+public class NameHashIntCacheLRU extends NameIntCacheLRU {
+
+  NameHashIntCacheLRU(int maxCacheSize) {
+    super(maxCacheSize);
+  }
+
+  @Override
+  Object key(FacetLabel name) {
+    return new Long(name.longHashCode());
+  }
+
+  @Override
+  Object key(FacetLabel name, int prefixLen) {
+    return new Long(name.subpath(prefixLen).longHashCode());
+  }
+  
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/NameIntCacheLRU.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/NameIntCacheLRU.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/NameIntCacheLRU.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/NameIntCacheLRU.java	2013-11-26 15:07:17.150616283 -0500
@@ -0,0 +1,131 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.LinkedHashMap;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An an LRU cache of mapping from name to int.
+ * Used to cache Ordinals of category paths.
+ * 
+ * @lucene.experimental
+ */
+// Note: Nothing in this class is synchronized. The caller is assumed to be
+// synchronized so that no two methods of this class are called concurrently.
+class NameIntCacheLRU {
+
+  private HashMap<Object, Integer> cache;
+  long nMisses = 0; // for debug
+  long nHits = 0;  // for debug
+  private int maxCacheSize;
+
+  NameIntCacheLRU(int maxCacheSize) {
+    this.maxCacheSize = maxCacheSize;
+    createCache(maxCacheSize);
+  }
+
+  public int getMaxSize() {
+    return maxCacheSize;
+  }
+  
+  public int getSize() {
+    return cache.size();
+  }
+
+  private void createCache (int maxSize) {
+    if (maxSize<Integer.MAX_VALUE) {
+      cache = new LinkedHashMap<Object, Integer>(1000,(float)0.7,true); //for LRU
+    } else {
+      cache = new HashMap<Object, Integer>(1000,(float)0.7); //no need for LRU
+    }
+  }
+
+  Integer get (FacetLabel name) {
+    Integer res = cache.get(key(name));
+    if (res==null) {
+      nMisses ++;
+    } else {
+      nHits ++;
+    }
+    return res;
+  }
+
+  /** Subclasses can override this to provide caching by e.g. hash of the string. */
+  Object key(FacetLabel name) {
+    return name;
+  }
+
+  Object key(FacetLabel name, int prefixLen) {
+    return name.subpath(prefixLen);
+  }
+
+  /**
+   * Add a new value to cache.
+   * Return true if cache became full and some room need to be made. 
+   */
+  boolean put (FacetLabel name, Integer val) {
+    cache.put(key(name), val);
+    return isCacheFull();
+  }
+
+  boolean put (FacetLabel name, int prefixLen, Integer val) {
+    cache.put(key(name, prefixLen), val);
+    return isCacheFull();
+  }
+
+  private boolean isCacheFull() {
+    return cache.size() > maxCacheSize;
+  }
+
+  void clear() {
+    cache.clear();
+  }
+
+  String stats() {
+    return "#miss="+nMisses+" #hit="+nHits;
+  }
+  
+  /**
+   * If cache is full remove least recently used entries from cache. Return true
+   * if anything was removed, false otherwise.
+   * 
+   * See comment in DirectoryTaxonomyWriter.addToCache(CategoryPath, int) for an
+   * explanation why we clean 2/3rds of the cache, and not just one entry.
+   */ 
+  boolean makeRoomLRU() {
+    if (!isCacheFull()) {
+      return false;
+    }
+    int n = cache.size() - (2*maxCacheSize)/3;
+    if (n<=0) {
+      return false;
+    }
+    Iterator<Object> it = cache.keySet().iterator();
+    int i = 0;
+    while (i<n && it.hasNext()) {
+      it.next();
+      it.remove();
+      i++;
+    }
+    return true;
+  }
+
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/TaxonomyWriterCache.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/TaxonomyWriterCache.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/TaxonomyWriterCache.java	2013-01-06 18:41:12.306961728 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/TaxonomyWriterCache.java	2013-11-14 15:40:50.014297837 -0500
@@ -1,6 +1,6 @@
 package org.apache.lucene.facet.taxonomy.writercache;
 
-import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
 /*
@@ -37,7 +37,7 @@
  * <p>
  * However, if it does so, it should clear out large parts of the cache at once,
  * because the user will typically need to work hard to recover from every cache
- * cleanup (see {@link #put(CategoryPath, int)}'s return value).
+ * cleanup (see {@link #put(FacetLabel, int)}'s return value).
  * <p>
  * <b>NOTE:</b> the cache may be accessed concurrently by multiple threads,
  * therefore cache implementations should take this into consideration.
@@ -62,7 +62,7 @@
    * it means the category does not exist. Otherwise, the category might
    * still exist, but just be missing from the cache.
    */
-  public int get(CategoryPath categoryPath);
+  public int get(FacetLabel categoryPath);
 
   /**
    * Add a category to the cache, with the given ordinal as the value.
@@ -82,7 +82,7 @@
    * It doesn't really matter, because normally the next thing we do after
    * finding that a category does not exist is to add it.
    */
-  public boolean put(CategoryPath categoryPath, int ordinal);
+  public boolean put(FacetLabel categoryPath, int ordinal);
 
   /**
    * Returns true if the cache is full, such that the next {@link #put} will


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetCounts.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetCounts.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetCounts.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetCounts.java	2013-11-27 19:16:18.167905008 -0500
@@ -0,0 +1,66 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.IntsRef;
+
+/** Reads from any {@link OrdinalsReader}; use {@link
+ *  FastTaxonomyFacetCounts} if you are using the
+ *  default encoding from {@link BinaryDocValues}.
+ * 
+ * @lucene.experimental */
+public class TaxonomyFacetCounts extends IntTaxonomyFacets {
+  private final OrdinalsReader ordinalsReader;
+
+  /** Create {@code TaxonomyFacetCounts}, which also
+   *  counts all facet labels.  Use this for a non-default
+   *  {@link OrdinalsReader}; otherwise use {@link
+   *  FastTaxonomyFacetCounts}. */
+  public TaxonomyFacetCounts(OrdinalsReader ordinalsReader, TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    super(ordinalsReader.getIndexFieldName(), taxoReader, config);
+    this.ordinalsReader = ordinalsReader;
+    count(fc.getMatchingDocs());
+  }
+
+  private final void count(List<MatchingDocs> matchingDocs) throws IOException {
+    IntsRef scratch  = new IntsRef();
+    for(MatchingDocs hits : matchingDocs) {
+      OrdinalsReader.OrdinalsSegmentReader ords = ordinalsReader.getReader(hits.context);
+      FixedBitSet bits = hits.bits;
+    
+      final int length = hits.bits.length();
+      int doc = 0;
+      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
+        ords.get(doc, scratch);
+        for(int i=0;i<scratch.length;i++) {
+          values[scratch.ints[scratch.offset+i]]++;
+        }
+        ++doc;
+      }
+    }
+
+    rollup();
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacets.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacets.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacets.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacets.java	2013-12-02 16:58:25.626830552 -0500
@@ -0,0 +1,86 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.List;
+
+import org.apache.lucene.facet.taxonomy.ParallelTaxonomyArrays;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+
+/** Base class for all taxonomy-based facets impls. */
+public abstract class TaxonomyFacets extends Facets {
+  protected final String indexFieldName;
+  protected final TaxonomyReader taxoReader;
+  protected final FacetsConfig config;
+  protected final int[] children;
+  protected final int[] siblings;
+
+  protected TaxonomyFacets(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config) throws IOException {
+    this.indexFieldName = indexFieldName;
+    this.taxoReader = taxoReader;
+    this.config = config;
+    ParallelTaxonomyArrays pta = taxoReader.getParallelTaxonomyArrays();
+    children = pta.children();
+    siblings = pta.siblings();
+  }
+
+  protected FacetsConfig.DimConfig verifyDim(String dim) {
+    FacetsConfig.DimConfig dimConfig = config.getDimConfig(dim);
+    if (!dimConfig.indexFieldName.equals(indexFieldName)) {
+      throw new IllegalArgumentException("dimension \"" + dim + "\" was not indexed into field \"" + indexFieldName);
+    }
+    return dimConfig;
+  }
+
+  @Override
+  public List<FacetResult> getAllDims(int topN) throws IOException {
+    int ord = children[TaxonomyReader.ROOT_ORDINAL];
+    List<FacetResult> results = new ArrayList<FacetResult>();
+    while (ord != TaxonomyReader.INVALID_ORDINAL) {
+      String dim = taxoReader.getPath(ord).components[0];
+      FacetsConfig.DimConfig dimConfig = config.getDimConfig(dim);
+      if (dimConfig.indexFieldName.equals(indexFieldName)) {
+        FacetResult result = getTopChildren(topN, dim);
+        if (result != null) {
+          results.add(result);
+        }
+      }
+      ord = siblings[ord];
+    }
+
+    // Sort by highest value, tie break by value:
+    Collections.sort(results,
+                     new Comparator<FacetResult>() {
+                       @Override
+                       public int compare(FacetResult a, FacetResult b) {
+                         if (a.value.doubleValue() > b.value.doubleValue()) {
+                           return -1;
+                         } else if (b.value.doubleValue() > a.value.doubleValue()) {
+                           return 1;
+                         } else {
+                           return a.dim.compareTo(b.dim);
+                         }
+                       }
+                     });
+    return results;
+  }
+}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumFloatAssociations.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumFloatAssociations.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumFloatAssociations.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumFloatAssociations.java	2013-12-18 19:05:31.973645947 -0500
@@ -0,0 +1,89 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+
+/** Aggregates sum of int values previously indexed with
+ *  {@link FloatAssociationFacetField}, assuming the default
+ *  encoding.
+ *
+ *  @lucene.experimental */
+public class TaxonomyFacetSumFloatAssociations extends FloatTaxonomyFacets {
+
+  /** Create {@code TaxonomyFacetSumFloatAssociations} against
+   *  the default index field. */
+  public TaxonomyFacetSumFloatAssociations(TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
+  }
+
+  /** Create {@code TaxonomyFacetSumFloatAssociations} against
+   *  the specified index field. */
+  public TaxonomyFacetSumFloatAssociations(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    super(indexFieldName, taxoReader, config);
+    sumValues(fc.getMatchingDocs());
+  }
+
+  private final void sumValues(List<MatchingDocs> matchingDocs) throws IOException {
+    //System.out.println("count matchingDocs=" + matchingDocs + " facetsField=" + facetsFieldName);
+    for(MatchingDocs hits : matchingDocs) {
+      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
+      if (dv == null) { // this reader does not have DocValues for the requested category list
+        continue;
+      }
+      FixedBitSet bits = hits.bits;
+    
+      final int length = hits.bits.length();
+      int doc = 0;
+      BytesRef scratch = new BytesRef();
+      //System.out.println("count seg=" + hits.context.reader());
+      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
+        //System.out.println("  doc=" + doc);
+        // TODO: use OrdinalsReader?  we'd need to add a
+        // BytesRef getAssociation()?
+        dv.get(doc, scratch);
+        byte[] bytes = scratch.bytes;
+        int end = scratch.offset + scratch.length;
+        int offset = scratch.offset;
+        while (offset < end) {
+          int ord = ((bytes[offset]&0xFF) << 24) |
+            ((bytes[offset+1]&0xFF) << 16) |
+            ((bytes[offset+2]&0xFF) << 8) |
+            (bytes[offset+3]&0xFF);
+          offset += 4;
+          int value = ((bytes[offset]&0xFF) << 24) |
+            ((bytes[offset+1]&0xFF) << 16) |
+            ((bytes[offset+2]&0xFF) << 8) |
+            (bytes[offset+3]&0xFF);
+          offset += 4;
+          values[ord] += Float.intBitsToFloat(value);
+        }
+        ++doc;
+      }
+    }
+
+    rollup();
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumIntAssociations.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumIntAssociations.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumIntAssociations.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumIntAssociations.java	2013-12-18 19:05:34.309645859 -0500
@@ -0,0 +1,89 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+
+/** Aggregates sum of int values previously indexed with
+ *  {@link IntAssociationFacetField}, assuming the default
+ *  encoding.
+ *
+ *  @lucene.experimental */
+public class TaxonomyFacetSumIntAssociations extends IntTaxonomyFacets {
+
+  /** Create {@code TaxonomyFacetSumIntAssociations} against
+   *  the default index field. */
+  public TaxonomyFacetSumIntAssociations(TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
+  }
+
+  /** Create {@code TaxonomyFacetSumIntAssociations} against
+   *  the specified index field. */
+  public TaxonomyFacetSumIntAssociations(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    super(indexFieldName, taxoReader, config);
+    sumValues(fc.getMatchingDocs());
+  }
+
+  private final void sumValues(List<MatchingDocs> matchingDocs) throws IOException {
+    //System.out.println("count matchingDocs=" + matchingDocs + " facetsField=" + facetsFieldName);
+    for(MatchingDocs hits : matchingDocs) {
+      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
+      if (dv == null) { // this reader does not have DocValues for the requested category list
+        continue;
+      }
+      FixedBitSet bits = hits.bits;
+    
+      final int length = hits.bits.length();
+      int doc = 0;
+      BytesRef scratch = new BytesRef();
+      //System.out.println("count seg=" + hits.context.reader());
+      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
+        //System.out.println("  doc=" + doc);
+        // TODO: use OrdinalsReader?  we'd need to add a
+        // BytesRef getAssociation()?
+        dv.get(doc, scratch);
+        byte[] bytes = scratch.bytes;
+        int end = scratch.offset + scratch.length;
+        int offset = scratch.offset;
+        while (offset < end) {
+          int ord = ((bytes[offset]&0xFF) << 24) |
+            ((bytes[offset+1]&0xFF) << 16) |
+            ((bytes[offset+2]&0xFF) << 8) |
+            (bytes[offset+3]&0xFF);
+          offset += 4;
+          int value = ((bytes[offset]&0xFF) << 24) |
+            ((bytes[offset+1]&0xFF) << 16) |
+            ((bytes[offset+2]&0xFF) << 8) |
+            (bytes[offset+3]&0xFF);
+          offset += 4;
+          values[ord] += value;
+        }
+        ++doc;
+      }
+    }
+
+    rollup();
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumValueSource.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumValueSource.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumValueSource.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumValueSource.java	2013-12-02 16:58:00.294831227 -0500
@@ -0,0 +1,134 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.IntsRef;
+
+/** Aggregates sum of values from {@link
+ *  FunctionValues#doubleVal}, for each facet label.
+ *
+ *  @lucene.experimental */
+public class TaxonomyFacetSumValueSource extends FloatTaxonomyFacets {
+  private final OrdinalsReader ordinalsReader;
+
+  /** Aggreggates float facet values from the provided
+   *  {@link ValueSource}, pulling ordinals using {@link
+   *  DocValuesOrdinalsReader} against the default indexed
+   *  facet field {@link
+   *  FacetsConfig#DEFAULT_INDEX_FIELD_NAME}. */
+  public TaxonomyFacetSumValueSource(TaxonomyReader taxoReader, FacetsConfig config,
+                                     FacetsCollector fc, ValueSource valueSource) throws IOException {
+    this(new DocValuesOrdinalsReader(FacetsConfig.DEFAULT_INDEX_FIELD_NAME), taxoReader, config, fc, valueSource);
+  }
+
+  /** Aggreggates float facet values from the provided
+   *  {@link ValueSource}, and pulls ordinals from the
+   *  provided {@link OrdinalsReader}. */
+  public TaxonomyFacetSumValueSource(OrdinalsReader ordinalsReader, TaxonomyReader taxoReader,
+                                     FacetsConfig config, FacetsCollector fc, ValueSource valueSource) throws IOException {
+    super(ordinalsReader.getIndexFieldName(), taxoReader, config);
+    this.ordinalsReader = ordinalsReader;
+    sumValues(fc.getMatchingDocs(), fc.getKeepScores(), valueSource);
+  }
+
+  private static final class FakeScorer extends Scorer {
+    float score;
+    int docID;
+    FakeScorer() { super(null); }
+    @Override public float score() throws IOException { return score; }
+    @Override public int freq() throws IOException { throw new UnsupportedOperationException(); }
+    @Override public int docID() { return docID; }
+    @Override public int nextDoc() throws IOException { throw new UnsupportedOperationException(); }
+    @Override public int advance(int target) throws IOException { throw new UnsupportedOperationException(); }
+    @Override public long cost() { return 0; }
+  }
+
+  private final void sumValues(List<MatchingDocs> matchingDocs, boolean keepScores, ValueSource valueSource) throws IOException {
+    final FakeScorer scorer = new FakeScorer();
+    Map<String, Scorer> context = new HashMap<String, Scorer>();
+    if (keepScores) {
+      context.put("scorer", scorer);
+    }
+    IntsRef scratch = new IntsRef();
+    for(MatchingDocs hits : matchingDocs) {
+      OrdinalsReader.OrdinalsSegmentReader ords = ordinalsReader.getReader(hits.context);
+      FixedBitSet bits = hits.bits;
+    
+      final int length = hits.bits.length();
+      int doc = 0;
+      int scoresIdx = 0;
+      float[] scores = hits.scores;
+
+      FunctionValues functionValues = valueSource.getValues(context, hits.context);
+      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
+        ords.get(doc, scratch);
+        if (keepScores) {
+          scorer.docID = doc;
+          scorer.score = scores[scoresIdx++];
+        }
+        float value = (float) functionValues.doubleVal(doc);
+        for(int i=0;i<scratch.length;i++) {
+          values[scratch.ints[i]] += value;
+        }
+        ++doc;
+      }
+    }
+
+    rollup();
+  }
+
+  /** {@link ValueSource} that returns the score for each
+   *  hit; use this to aggregate the sum of all hit scores
+   *  for each facet label.  */
+  public static class ScoreValueSource extends ValueSource {
+    @Override
+    public FunctionValues getValues(@SuppressWarnings("rawtypes") Map context, AtomicReaderContext readerContext) throws IOException {
+      final Scorer scorer = (Scorer) context.get("scorer");
+      if (scorer == null) {
+        throw new IllegalStateException("scores are missing; be sure to pass keepScores=true to FacetsCollector");
+      }
+      return new DoubleDocValues(this) {
+        @Override
+        public double doubleVal(int document) {
+          try {
+            return scorer.score();
+          } catch (IOException exception) {
+            throw new RuntimeException(exception);
+          }
+        }
+      };
+    }
+
+    @Override public boolean equals(Object o) { return o == this; }
+    @Override public int hashCode() { return System.identityHashCode(this); }
+    @Override public String description() { return "score()"; }
+    };
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndFloatQueue.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndFloatQueue.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndFloatQueue.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndFloatQueue.java	2013-11-27 19:15:10.199907540 -0500
@@ -0,0 +1,45 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.PriorityQueue;
+
+/** Keeps highest results, first by largest float value,
+ *  then tie break by smallest ord. */
+class TopOrdAndFloatQueue extends PriorityQueue<TopOrdAndFloatQueue.OrdAndValue> {
+
+  public static final class OrdAndValue {
+    int ord;
+    float value;
+  }
+
+  public TopOrdAndFloatQueue(int topN) {
+    super(topN, false);
+  }
+
+  @Override
+  protected boolean lessThan(OrdAndValue a, OrdAndValue b) {
+    if (a.value < b.value) {
+      return true;
+    } else if (a.value > b.value) {
+      return false;
+    } else {
+      return a.ord > b.ord;
+    }
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndIntQueue.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndIntQueue.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndIntQueue.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndIntQueue.java	2013-11-26 10:45:01.839037236 -0500
@@ -0,0 +1,45 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.PriorityQueue;
+
+/** Keeps highest results, first by largest int value,
+ *  then tie break by smallest ord. */
+class TopOrdAndIntQueue extends PriorityQueue<TopOrdAndIntQueue.OrdAndValue> {
+
+  public static final class OrdAndValue {
+    int ord;
+    int value;
+  }
+
+  public TopOrdAndIntQueue(int topN) {
+    super(topN, false);
+  }
+
+  @Override
+  protected boolean lessThan(OrdAndValue a, OrdAndValue b) {
+    if (a.value < b.value) {
+      return true;
+    } else if (a.value > b.value) {
+      return false;
+    } else {
+      return a.ord > b.ord;
+    }
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/util/FacetsPayloadMigrationReader.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/util/FacetsPayloadMigrationReader.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/util/FacetsPayloadMigrationReader.java	2013-08-20 18:30:40.134416804 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/util/FacetsPayloadMigrationReader.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,265 +0,0 @@
-package org.apache.lucene.facet.util;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Map;
-
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.FilterAtomicReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * A {@link FilterAtomicReader} for migrating a facets index which encodes
- * category ordinals in a payload to {@link BinaryDocValues}. To migrate the index,
- * you should build a mapping from a field (String) to term ({@link Term}),
- * which denotes under which BinaryDocValues field to put the data encoded in the
- * matching term's payload. You can follow the code example below to migrate an
- * existing index:
- * 
- * <pre class="prettyprint">
- * // Add the index and migrate payload to DocValues on the go
- * DirectoryReader reader = DirectoryReader.open(oldDir);
- * IndexWriterConfig conf = new IndexWriterConfig(VER, ANALYZER);
- * IndexWriter writer = new IndexWriter(newDir, conf);
- * List&lt;AtomicReaderContext&gt; leaves = reader.leaves();
- * AtomicReader wrappedLeaves[] = new AtomicReader[leaves.size()];
- * for (int i = 0; i &lt; leaves.size(); i++) {
- *   wrappedLeaves[i] = new FacetPayloadMigrationReader(leaves.get(i).reader(),
- *       fieldTerms);
- * }
- * writer.addIndexes(new MultiReader(wrappedLeaves));
- * writer.commit();
- * </pre>
- * 
- * <p>
- * <b>NOTE:</b> to build the field-to-term map you can use
- * {@link #buildFieldTermsMap(Directory, FacetIndexingParams)}, as long as the
- * index to migrate contains the ordinals payload under
- * {@link #PAYLOAD_TERM_TEXT}.
- * 
- * @lucene.experimental
- */
-public class FacetsPayloadMigrationReader extends FilterAtomicReader {  
-
-  private class PayloadMigratingBinaryDocValues extends BinaryDocValues {
-
-    private Fields fields;
-    private Term term;
-    private DocsAndPositionsEnum dpe;
-    private int curDocID = -1;
-    private int lastRequestedDocID;
-
-    private DocsAndPositionsEnum getDPE() {
-      try {
-        DocsAndPositionsEnum dpe = null;
-        if (fields != null) {
-          Terms terms = fields.terms(term.field());
-          if (terms != null) {
-            TermsEnum te = terms.iterator(null); // no use for reusing
-            if (te.seekExact(term.bytes())) {
-              // we're not expected to be called for deleted documents
-              dpe = te.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_PAYLOADS);
-            }
-          }
-        }
-        return dpe;
-      } catch (IOException ioe) {
-        throw new RuntimeException(ioe);
-      }
-    }
-    
-    protected PayloadMigratingBinaryDocValues(Fields fields, Term term) {
-      this.fields = fields;
-      this.term = term;
-      this.dpe = getDPE();
-      if (dpe == null) {
-        curDocID = DocIdSetIterator.NO_MORE_DOCS;
-      } else {
-        try {
-          curDocID = dpe.nextDoc();
-        } catch (IOException e) {
-          throw new RuntimeException(e);
-        }
-      }
-    }
-    
-    @Override
-    public void get(int docID, BytesRef result) {
-      try {
-        // If caller is moving backwards (eg, during merge,
-        // the consuming DV format is free to iterate over
-        // our values as many times as it wants), we must
-        // re-init the dpe:
-        if (docID <= lastRequestedDocID) {
-          dpe = getDPE();
-          if (dpe == null) {
-            curDocID = DocIdSetIterator.NO_MORE_DOCS;
-          } else{
-            curDocID = dpe.nextDoc();
-          }
-        }
-        lastRequestedDocID = docID;
-        if (curDocID > docID) {
-          // document does not exist
-          result.length = 0;
-          return;
-        }
-      
-        if (curDocID < docID) {
-          curDocID = dpe.advance(docID);
-          if (curDocID != docID) { // requested document does not have a payload
-            result.length = 0;
-            return;
-          }
-        }
-        
-        dpe.nextPosition();
-        result.copyBytes(dpe.getPayload());
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-  }
-  
-  /** The {@link Term} text of the ordinals payload. */
-  public static final String PAYLOAD_TERM_TEXT = "$fulltree$";
-
-  /**
-   * A utility method for building the field-to-Term map, given the
-   * {@link FacetIndexingParams} and the directory of the index to migrate. The
-   * map that will be built will correspond to partitions as well as multiple
-   * {@link CategoryListParams}.
-   * <p>
-   * <b>NOTE:</b> since {@link CategoryListParams} no longer define a
-   * {@link Term}, this method assumes that the term used by the different
-   * {@link CategoryListParams} is {@link #PAYLOAD_TERM_TEXT}. If this is not
-   * the case, then you should build the map yourself, using the terms in your
-   * index.
-   */
-  public static Map<String,Term> buildFieldTermsMap(Directory dir, FacetIndexingParams fip) throws IOException {
-    // only add field-Term mapping that will actually have DocValues in the end.
-    // therefore traverse the index terms and add what exists. this pertains to
-    // multiple CLPs, as well as partitions
-    DirectoryReader reader = DirectoryReader.open(dir);
-    final Map<String,Term> fieldTerms = new HashMap<String,Term>();
-    for (AtomicReaderContext context : reader.leaves()) {
-      for (CategoryListParams clp : fip.getAllCategoryListParams()) {
-        Terms terms = context.reader().terms(clp.field);
-        if (terms != null) {
-          TermsEnum te = terms.iterator(null);
-          BytesRef termBytes = null;
-          while ((termBytes = te.next()) != null) {
-            String term = termBytes.utf8ToString();
-            if (term.startsWith(PAYLOAD_TERM_TEXT )) {
-              if (term.equals(PAYLOAD_TERM_TEXT)) {
-                fieldTerms.put(clp.field, new Term(clp.field, term));
-              } else {
-                fieldTerms.put(clp.field + term.substring(PAYLOAD_TERM_TEXT.length()), new Term(clp.field, term));
-              }
-            }
-          }
-        }        
-      }
-    }
-    reader.close();
-    return fieldTerms;
-  }
-  
-  private final Map<String,Term> fieldTerms;
-  
-  /**
-   * Wraps an {@link AtomicReader} and migrates the payload to {@link BinaryDocValues}
-   * fields by using the given mapping.
-   */
-  public FacetsPayloadMigrationReader(AtomicReader in, Map<String,Term> fieldTerms) {
-    super(in);
-    this.fieldTerms = fieldTerms;
-  }
-  
-  @Override
-  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
-    Term term = fieldTerms.get(field);
-    if (term == null) {
-      return super.getBinaryDocValues(field);
-    } else {
-      // we shouldn't return null, even if the term does not exist or has no
-      // payloads, since we already marked the field as having DocValues.
-      return new PayloadMigratingBinaryDocValues(fields(), term);
-    }
-  }
-
-  @Override
-  public Bits getDocsWithField(String field) throws IOException {
-    Term term = fieldTerms.get(field);
-    if (term == null) {
-      return super.getDocsWithField(field);
-    } else {
-      // we shouldn't return null, even if the term does not exist or has no
-      // payloads, since we already marked the field as having DocValues.
-      return new Bits.MatchAllBits(maxDoc());
-    }
-  }
-
-  @Override
-  public FieldInfos getFieldInfos() {
-    FieldInfos innerInfos = super.getFieldInfos();
-    ArrayList<FieldInfo> infos = new ArrayList<FieldInfo>(innerInfos.size());
-    // if there are partitions, then the source index contains one field for all their terms
-    // while with DocValues, we simulate that by multiple fields.
-    HashSet<String> leftoverFields = new HashSet<String>(fieldTerms.keySet());
-    int number = -1;
-    for (FieldInfo info : innerInfos) {
-      if (fieldTerms.containsKey(info.name)) {
-        // mark this field as having a DocValues
-        infos.add(new FieldInfo(info.name, true, info.number,
-            info.hasVectors(), info.omitsNorms(), info.hasPayloads(),
-            info.getIndexOptions(), DocValuesType.BINARY,
-            info.getNormType(), info.attributes()));
-        leftoverFields.remove(info.name);
-      } else {
-        infos.add(info);
-      }
-      number = Math.max(number, info.number);
-    }
-    for (String field : leftoverFields) {
-      infos.add(new FieldInfo(field, false, ++number, false, false, false,
-          null, DocValuesType.BINARY, null, null));
-    }
-    return new FieldInfos(infos.toArray(new FieldInfo[infos.size()]));
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/util/MultiCategoryListIterator.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/util/MultiCategoryListIterator.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/util/MultiCategoryListIterator.java	2013-01-14 13:43:40.388580180 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/util/MultiCategoryListIterator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,68 +0,0 @@
-package org.apache.lucene.facet.util;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.facet.search.CategoryListIterator;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Iterates over multiple {@link CategoryListIterator}s, consuming the provided
- * iterators in order.
- * 
- * @lucene.experimental
- */
-public class MultiCategoryListIterator implements CategoryListIterator {
-
-  private final CategoryListIterator[] iterators;
-  private final List<CategoryListIterator> validIterators;
-
-  /** Receives the iterators to iterate on */
-  public MultiCategoryListIterator(CategoryListIterator... iterators) {
-    this.iterators = iterators;
-    this.validIterators = new ArrayList<CategoryListIterator>();
-  }
-
-  @Override
-  public boolean setNextReader(AtomicReaderContext context) throws IOException {
-    validIterators.clear();
-    for (CategoryListIterator cli : iterators) {
-      if (cli.setNextReader(context)) {
-        validIterators.add(cli);
-      }
-    }
-    return !validIterators.isEmpty();
-  }
-  
-  @Override
-  public void getOrdinals(int docID, IntsRef ints) throws IOException {
-    IntsRef tmp = new IntsRef(ints.length);
-    for (CategoryListIterator cli : validIterators) {
-      cli.getOrdinals(docID, tmp);
-      if (ints.ints.length < ints.length + tmp.length) {
-        ints.grow(ints.length + tmp.length);
-      }
-      ints.length += tmp.length;
-    }
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/util/OrdinalMappingAtomicReader.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/util/OrdinalMappingAtomicReader.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/util/OrdinalMappingAtomicReader.java	2013-02-28 09:00:26.706312171 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/util/OrdinalMappingAtomicReader.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,146 +0,0 @@
-package org.apache.lucene.facet.util;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.facet.encoding.IntDecoder;
-import org.apache.lucene.facet.encoding.IntEncoder;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.FilterAtomicReader;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/**
- * A {@link FilterAtomicReader} for updating facets ordinal references,
- * based on an ordinal map. You should use this code in conjunction with merging
- * taxonomies - after you merge taxonomies, you receive an {@link OrdinalMap}
- * which maps the 'old' ordinals to the 'new' ones. You can use that map to
- * re-map the doc values which contain the facets information (ordinals) either
- * before or while merging the indexes.
- * <p>
- * For re-mapping the ordinals during index merge, do the following:
- * 
- * <pre class="prettyprint">
- * // merge the old taxonomy with the new one.
- * OrdinalMap map = new MemoryOrdinalMap();
- * DirectoryTaxonomyWriter.addTaxonomy(srcTaxoDir, map);
- * int[] ordmap = map.getMap();
- * 
- * // Add the index and re-map ordinals on the go
- * DirectoryReader reader = DirectoryReader.open(oldDir);
- * IndexWriterConfig conf = new IndexWriterConfig(VER, ANALYZER);
- * IndexWriter writer = new IndexWriter(newDir, conf);
- * List&lt;AtomicReaderContext&gt; leaves = reader.leaves();
- *   AtomicReader wrappedLeaves[] = new AtomicReader[leaves.size()];
- *   for (int i = 0; i < leaves.size(); i++) {
- *     wrappedLeaves[i] = new OrdinalMappingAtomicReader(leaves.get(i).reader(), ordmap);
- *   }
- * writer.addIndexes(new MultiReader(wrappedLeaves));
- * writer.commit();
- * </pre>
- * 
- * @lucene.experimental
- */
-public class OrdinalMappingAtomicReader extends FilterAtomicReader {
-  
-  private final int[] ordinalMap;
-  
-  private final Map<String,CategoryListParams> dvFieldMap = new HashMap<String,CategoryListParams>();
-  
-  /**
-   * Wraps an AtomicReader, mapping ordinals according to the ordinalMap.
-   * Calls {@link #OrdinalMappingAtomicReader(AtomicReader, int[], FacetIndexingParams)
-   * OrdinalMappingAtomicReader(in, ordinalMap, new DefaultFacetIndexingParams())}
-   */
-  public OrdinalMappingAtomicReader(AtomicReader in, int[] ordinalMap) {
-    this(in, ordinalMap, FacetIndexingParams.DEFAULT);
-  }
-  
-  /**
-   * Wraps an AtomicReader, mapping ordinals according to the ordinalMap,
-   * using the provided indexingParams.
-   */
-  public OrdinalMappingAtomicReader(AtomicReader in, int[] ordinalMap, FacetIndexingParams indexingParams) {
-    super(in);
-    this.ordinalMap = ordinalMap;
-    for (CategoryListParams params: indexingParams.getAllCategoryListParams()) {
-      dvFieldMap.put(params.field, params);
-    }
-  }
-
-  @Override
-  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
-    BinaryDocValues inner = super.getBinaryDocValues(field);
-    if (inner == null) {
-      return inner;
-    }
-    
-    CategoryListParams clp = dvFieldMap.get(field);
-    if (clp == null) {
-      return inner;
-    } else {
-      return new OrdinalMappingBinaryDocValues(clp, inner);
-    }
-  }
-  
-  private class OrdinalMappingBinaryDocValues extends BinaryDocValues {
-
-    private final IntEncoder encoder;
-    private final IntDecoder decoder;
-    private final IntsRef ordinals = new IntsRef(32);
-    private final BinaryDocValues delegate;
-    private final BytesRef scratch = new BytesRef();
-    
-    protected OrdinalMappingBinaryDocValues(CategoryListParams clp, BinaryDocValues delegate) {
-      this.delegate = delegate;
-      encoder = clp.createEncoder();
-      decoder = encoder.createMatchingDecoder();
-    }
-    
-    @SuppressWarnings("synthetic-access")
-    @Override
-    public void get(int docID, BytesRef result) {
-      // NOTE: this isn't quite koscher, because in general
-      // multiple threads can call BinaryDV.get which would
-      // then conflict on the single scratch instance, but
-      // because this impl is only used for merging, we know
-      // only 1 thread calls us:
-      delegate.get(docID, scratch);
-      if (scratch.length > 0) {
-        // We must use scratch (and not re-use result) here,
-        // else encoder may overwrite the DV provider's
-        // private byte[]:
-        decoder.decode(scratch, ordinals);
-        
-        // map the ordinals
-        for (int i = 0; i < ordinals.length; i++) {
-          ordinals.ints[i] = ordinalMap[ordinals.ints[i]];
-        }
-        
-        encoder.encode(ordinals, result);
-      }
-    }
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/util/package.html simplefacets/lucene/facet/src/java/org/apache/lucene/facet/util/package.html
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/util/package.html	2012-05-02 06:41:08.627778400 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/util/package.html	1969-12-31 19:00:00.000000000 -0500
@@ -1,24 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-  <head>
-    <title>Various utilities for faceted search</title>
-  </head>
-  <body>
-    <h1>Various utilities for faceted search</h1>
-  </body>
-</html>
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/util/PartitionsUtils.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/util/PartitionsUtils.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/util/PartitionsUtils.java	2013-02-20 13:38:17.704711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/util/PartitionsUtils.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,71 +0,0 @@
-package org.apache.lucene.facet.util;
-
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Utilities for partitions - sizes and such
- * 
- * @lucene.experimental
- */
-public final class PartitionsUtils {
-
-  /** The prefix that is added to the name of the partition. */
-  public static final String PART_NAME_PREFIX = "$part";
-  
-  /**
-   * Get the partition size in this parameter, or return the size of the taxonomy, which
-   * is smaller.  (Guarantees usage of as little memory as possible at search time).
-   */
-  public final static int partitionSize(FacetIndexingParams indexingParams, final TaxonomyReader taxonomyReader) {
-    return Math.min(indexingParams.getPartitionSize(), taxonomyReader.getSize());
-  }
-
-  /**
-   * Partition number of an ordinal.
-   * <p>
-   * This allows to locate the partition containing a certain (facet) ordinal.
-   * @see FacetIndexingParams#getPartitionSize()      
-   */
-  public final static int partitionNumber(FacetIndexingParams iParams, int ordinal) {
-    return ordinal / iParams.getPartitionSize();
-  }
-
-  /**
-   * Partition name by category ordinal
-   */
-  public final static String partitionNameByOrdinal(FacetIndexingParams iParams, int ordinal) {
-    int partition = partitionNumber(iParams, ordinal);
-    return partitionName(partition);
-  }
-
-  /** Partition name by its number */
-  public final static String partitionName(int partition) {
-    // TODO would be good if this method isn't called when partitions are not enabled.
-    // perhaps through some specialization code.
-    if (partition == 0) {
-      // since regular faceted search code goes through this method too,
-      // return the same value for partition 0 and when there are no partitions
-      return "";
-    }
-    return PART_NAME_PREFIX + Integer.toString(partition);
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/util/PrintTaxonomyStats.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/util/PrintTaxonomyStats.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/util/PrintTaxonomyStats.java	2013-04-22 16:59:20.123676455 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/util/PrintTaxonomyStats.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,94 +0,0 @@
-package org.apache.lucene.facet.util;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.io.IOException;
-import java.io.PrintStream;
-
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader.ChildrenIterator;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-
-/** Prints how many ords are under each dimension. */
-
-// java -cp ../build/core/classes/java:../build/facet/classes/java org.apache.lucene.facet.util.PrintTaxonomyStats -printTree /s2/scratch/indices/wikibig.trunk.noparents.facets.Lucene41.nd1M/facets
-public class PrintTaxonomyStats {
-
-  public static void main(String[] args) throws IOException {
-    boolean printTree = false;
-    String path = null;
-    for(int i=0;i<args.length;i++) {
-      if (args[i].equals("-printTree")) {
-        printTree = true;
-      } else {
-        path = args[i];
-      }
-    }
-    if (args.length != (printTree ? 2 : 1)) {
-      System.out.println("\nUsage: java -classpath ... org.apache.lucene.facet.util.PrintTaxonomyStats [-printTree] /path/to/taxononmy/index\n");
-      System.exit(1);
-    }
-    Directory dir = FSDirectory.open(new File(path));
-    TaxonomyReader r = new DirectoryTaxonomyReader(dir);
-    printStats(r, System.out, printTree);
-    r.close();
-    dir.close();
-  }
-
-  public static void printStats(TaxonomyReader r, PrintStream out, boolean printTree) throws IOException {
-    out.println(r.getSize() + " total categories.");
-
-    ChildrenIterator it = r.getChildren(TaxonomyReader.ROOT_ORDINAL);
-    int child;
-    while ((child = it.next()) != TaxonomyReader.INVALID_ORDINAL) {
-      ChildrenIterator chilrenIt = r.getChildren(child);
-      int numImmediateChildren = 0;
-      while (chilrenIt.next() != TaxonomyReader.INVALID_ORDINAL) {
-        numImmediateChildren++;
-      }
-      CategoryPath cp = r.getPath(child);
-      out.println("/" + cp + ": " + numImmediateChildren + " immediate children; " + (1+countAllChildren(r, child)) + " total categories");
-      if (printTree) {
-        printAllChildren(out, r, child, "  ", 1);
-      }
-    }
-  }
-
-  private static int countAllChildren(TaxonomyReader r, int ord) throws IOException {
-    int count = 0;
-    ChildrenIterator it = r.getChildren(ord);
-    int child;
-    while ((child = it.next()) != TaxonomyReader.INVALID_ORDINAL) {
-      count += 1 + countAllChildren(r, child);
-    }
-    return count;
-  }
-
-  private static void printAllChildren(PrintStream out, TaxonomyReader r, int ord, String indent, int depth) throws IOException {
-    ChildrenIterator it = r.getChildren(ord);
-    int child;
-    while ((child = it.next()) != TaxonomyReader.INVALID_ORDINAL) {
-      out.println(indent + "/" + r.getPath(child).components[depth]);
-      printAllChildren(out, r, child, indent + "  ", depth+1);
-    }
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/util/ResultSortUtils.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/util/ResultSortUtils.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/util/ResultSortUtils.java	2013-02-20 13:38:17.704711924 -0500
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/util/ResultSortUtils.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,164 +0,0 @@
-package org.apache.lucene.facet.util;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-
-import org.apache.lucene.util.PriorityQueue;
-
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.search.Heap;
-import org.apache.lucene.facet.search.FacetRequest.SortOrder;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Utilities for generating facet results sorted as required
- * 
- * @lucene.experimental
- */
-public class ResultSortUtils {
-
-  /**
-   * Create a suitable heap according to facet request being served. 
-   * @return heap for maintaining results for specified request.
-   * @throws IllegalArgumentException is provided facet request is not supported 
-   */
-  public static Heap<FacetResultNode> createSuitableHeap(FacetRequest facetRequest) {
-    int nresults = facetRequest.numResults;
-    boolean accending = (facetRequest.getSortOrder() == SortOrder.ASCENDING);
-
-    if (nresults == Integer.MAX_VALUE) {
-      return new AllValueHeap(accending);
-    }
-
-    if (accending) {
-      return new MaxValueHeap(nresults);
-    } else {
-      return new MinValueHeap(nresults);
-    }
-  }
-
-  private static class MinValueHeap extends PriorityQueue<FacetResultNode> implements Heap<FacetResultNode> {
-    public MinValueHeap(int size) {
-      super(size);
-    }
-
-    @Override
-    protected boolean lessThan(FacetResultNode arg0, FacetResultNode arg1) {
-      double value0 = arg0.value;
-      double value1 = arg1.value;
-
-      int valueCompare = Double.compare(value0, value1);
-      if (valueCompare == 0) { 
-        return arg0.ordinal < arg1.ordinal;
-      }
-
-      return valueCompare < 0;
-    }
-
-  }
-
-  private static class MaxValueHeap extends PriorityQueue<FacetResultNode> implements Heap<FacetResultNode> {
-    public MaxValueHeap(int size) {
-      super(size);
-    }
-
-    @Override
-    protected boolean lessThan(FacetResultNode arg0, FacetResultNode arg1) {
-      double value0 = arg0.value;
-      double value1 = arg1.value;
-
-      int valueCompare = Double.compare(value0, value1);
-      if (valueCompare == 0) { 
-        return arg0.ordinal > arg1.ordinal;
-      }
-
-      return valueCompare > 0;
-    }
-  }
-
-  /**
-   * Create a Heap-Look-Alike, which implements {@link Heap}, but uses a
-   * regular <code>ArrayList</code> for holding <b>ALL</b> the objects given,
-   * only sorting upon the first call to {@link #pop()}.
-   */
-  private static class AllValueHeap implements Heap<FacetResultNode> {
-    private ArrayList<FacetResultNode> resultNodes = new ArrayList<FacetResultNode>();
-    final boolean accending;
-    private boolean isReady = false;
-    public AllValueHeap(boolean accending) {
-      this.accending = accending;
-    }
-
-    @Override
-    public FacetResultNode insertWithOverflow(FacetResultNode node) {
-      resultNodes.add(node);
-      return null;
-    }
-
-    @Override
-    public FacetResultNode pop() {
-      if (!isReady) {
-        Collections.sort(resultNodes, new Comparator<FacetResultNode>() {
-          @Override
-          public int compare(FacetResultNode o1, FacetResultNode o2) {
-            int value = Double.compare(o1.value, o2.value);
-            if (value == 0) {
-              value = o1.ordinal - o2.ordinal;
-            }
-            if (accending) {
-              value = -value;
-            }
-            return value;
-          }
-        });
-        isReady = true;
-      }
-
-      return resultNodes.remove(0);
-    }
-
-    @Override
-    public int size() {
-      return resultNodes.size();
-    }
-
-    @Override
-    public FacetResultNode top() {
-      if (resultNodes.size() > 0) {
-        return resultNodes.get(0);
-      }
-
-      return null;
-    }
-
-    @Override
-    public FacetResultNode add(FacetResultNode frn) {
-      resultNodes.add(frn);
-      return null;
-    }
-
-    @Override
-    public void clear() {
-      resultNodes.clear();
-    }
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/java/org/apache/lucene/facet/util/TaxonomyMergeUtils.java simplefacets/lucene/facet/src/java/org/apache/lucene/facet/util/TaxonomyMergeUtils.java
--- trunk/lucene/facet/src/java/org/apache/lucene/facet/util/TaxonomyMergeUtils.java	2013-07-29 13:55:02.633707541 -0400
+++ simplefacets/lucene/facet/src/java/org/apache/lucene/facet/util/TaxonomyMergeUtils.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,66 +0,0 @@
-package org.apache.lucene.facet.util;
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.MultiReader;
-import org.apache.lucene.store.Directory;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Utility methods for merging index and taxonomy directories.
- * @lucene.experimental
- */
-public class TaxonomyMergeUtils {
-
-  /**
-   * Merges the given taxonomy and index directories and commits the changes to
-   * the given writers.
-   */
-  public static void merge(Directory srcIndexDir, Directory srcTaxDir, OrdinalMap map, IndexWriter destIndexWriter, 
-      DirectoryTaxonomyWriter destTaxWriter, FacetIndexingParams params) throws IOException {
-    // merge the taxonomies
-    destTaxWriter.addTaxonomy(srcTaxDir, map);
-    int ordinalMap[] = map.getMap();
-    DirectoryReader reader = DirectoryReader.open(srcIndexDir);
-    List<AtomicReaderContext> leaves = reader.leaves();
-    int numReaders = leaves.size();
-    AtomicReader wrappedLeaves[] = new AtomicReader[numReaders];
-    for (int i = 0; i < numReaders; i++) {
-      wrappedLeaves[i] = new OrdinalMappingAtomicReader(leaves.get(i).reader(), ordinalMap, params);
-    }
-    try {
-      destIndexWriter.addIndexes(new MultiReader(wrappedLeaves));
-      
-      // commit changes to taxonomy and index respectively.
-      destTaxWriter.commit();
-      destIndexWriter.commit();
-    } finally {
-      reader.close();
-    }
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/AssertingSubDocsAtOnceCollector.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/AssertingSubDocsAtOnceCollector.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/AssertingSubDocsAtOnceCollector.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/AssertingSubDocsAtOnceCollector.java	2013-11-26 10:49:13.531030402 -0500
@@ -0,0 +1,67 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.Scorer.ChildScorer;
+import org.apache.lucene.search.Scorer;
+
+/** Verifies in collect() that all child subScorers are on
+ *  the collected doc. */
+class AssertingSubDocsAtOnceCollector extends Collector {
+
+  // TODO: allow wrapping another Collector
+
+  List<Scorer> allScorers;
+
+  @Override
+  public void setScorer(Scorer s) {
+    // Gathers all scorers, including s and "under":
+    allScorers = new ArrayList<Scorer>();
+    allScorers.add(s);
+    int upto = 0;
+    while(upto < allScorers.size()) {
+      s = allScorers.get(upto++);
+      for (ChildScorer sub : s.getChildren()) {
+        allScorers.add(sub.child);
+      }
+    }
+  }
+
+  @Override
+  public void collect(int docID) {
+    for(Scorer s : allScorers) {
+      if (docID != s.docID()) {
+        throw new IllegalStateException("subScorer=" + s + " has docID=" + s.docID() + " != collected docID=" + docID);
+      }
+    }
+  }
+
+  @Override
+  public void setNextReader(AtomicReaderContext context) {
+  }
+
+  @Override
+  public boolean acceptsDocsOutOfOrder() {
+    return false;
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/associations/AssociationsFacetRequestTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/associations/AssociationsFacetRequestTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/associations/AssociationsFacetRequestTest.java	2013-10-17 13:16:00.924867115 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/associations/AssociationsFacetRequestTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,178 +0,0 @@
-package org.apache.lucene.facet.associations;
-
-import java.util.List;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.store.Directory;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Test for associations */
-public class AssociationsFacetRequestTest extends FacetTestCase {
-  
-  private static Directory dir;
-  private static IndexReader reader;
-  private static Directory taxoDir;
-  
-  private static final CategoryPath aint = new CategoryPath("int", "a");
-  private static final CategoryPath bint = new CategoryPath("int", "b");
-  private static final CategoryPath afloat = new CategoryPath("float", "a");
-  private static final CategoryPath bfloat = new CategoryPath("float", "b");
-  
-  @BeforeClass
-  public static void beforeClassAssociationsFacetRequestTest() throws Exception {
-    dir = newDirectory();
-    taxoDir = newDirectory();
-    // preparations - index, taxonomy, content
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, 
-        new MockAnalyzer(random(), MockTokenizer.KEYWORD, false)));
-    
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    
-    AssociationsFacetFields assocFacetFields = new AssociationsFacetFields(taxoWriter);
-    
-    // index documents, 50% have only 'b' and all have 'a'
-    for (int i = 0; i < 110; i++) {
-      Document doc = new Document();
-      CategoryAssociationsContainer associations = new CategoryAssociationsContainer();
-      // every 11th document is added empty, this used to cause the association
-      // aggregators to go into an infinite loop
-      if (i % 11 != 0) {
-        associations.setAssociation(aint, new CategoryIntAssociation(2));
-        associations.setAssociation(afloat, new CategoryFloatAssociation(0.5f));
-        if (i % 2 == 0) { // 50
-          associations.setAssociation(bint, new CategoryIntAssociation(3));
-          associations.setAssociation(bfloat, new CategoryFloatAssociation(0.2f));
-        }
-      }
-      assocFacetFields.addFields(doc, associations);
-      writer.addDocument(doc);
-    }
-    
-    taxoWriter.close();
-    reader = writer.getReader();
-    writer.close();
-  }
-  
-  @AfterClass
-  public static void afterClassAssociationsFacetRequestTest() throws Exception {
-    reader.close();
-    reader = null;
-    dir.close();
-    dir = null;
-    taxoDir.close();
-    taxoDir = null;
-  }
-  
-  @Test
-  public void testIntSumAssociation() throws Exception {
-    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-    
-    // facet requests for two facets
-    FacetSearchParams fsp = new FacetSearchParams(
-        new SumIntAssociationFacetRequest(aint, 10),
-        new SumIntAssociationFacetRequest(bint, 10));
-    
-    Query q = new MatchAllDocsQuery();
-    
-    FacetsCollector fc = FacetsCollector.create(fsp, reader, taxo);
-    
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(q, fc);
-    List<FacetResult> res = fc.getFacetResults();
-    
-    assertNotNull("No results!",res);
-    assertEquals("Wrong number of results!",2, res.size());
-    assertEquals("Wrong count for category 'a'!", 200, (int) res.get(0).getFacetResultNode().value);
-    assertEquals("Wrong count for category 'b'!", 150, (int) res.get(1).getFacetResultNode().value);
-    
-    taxo.close();
-  }
-  
-  @Test
-  public void testFloatSumAssociation() throws Exception {
-    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-    
-    // facet requests for two facets
-    FacetSearchParams fsp = new FacetSearchParams(
-        new SumFloatAssociationFacetRequest(afloat, 10),
-        new SumFloatAssociationFacetRequest(bfloat, 10));
-    
-    Query q = new MatchAllDocsQuery();
-    
-    FacetsCollector fc = FacetsCollector.create(fsp, reader, taxo);
-    
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(q, fc);
-    List<FacetResult> res = fc.getFacetResults();
-    
-    assertNotNull("No results!",res);
-    assertEquals("Wrong number of results!", 2, res.size());
-    assertEquals("Wrong count for category 'a'!",50f, (float) res.get(0).getFacetResultNode().value, 0.00001);
-    assertEquals("Wrong count for category 'b'!",10f, (float) res.get(1).getFacetResultNode().value, 0.00001);
-    
-    taxo.close();
-  }  
-  
-  @Test
-  public void testDifferentAggregatorsSameCategoryList() throws Exception {
-    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-    
-    // facet requests for two facets
-    FacetSearchParams fsp = new FacetSearchParams(
-        new SumIntAssociationFacetRequest(aint, 10),
-        new SumIntAssociationFacetRequest(bint, 10),
-        new SumFloatAssociationFacetRequest(afloat, 10),
-        new SumFloatAssociationFacetRequest(bfloat, 10));
-    
-    Query q = new MatchAllDocsQuery();
-    
-    FacetsCollector fc = FacetsCollector.create(fsp, reader, taxo);
-    
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(q, fc);
-    List<FacetResult> res = fc.getFacetResults();
-    
-    assertEquals("Wrong number of results!", 4, res.size());
-    assertEquals("Wrong count for category 'a'!", 200, (int) res.get(0).getFacetResultNode().value);
-    assertEquals("Wrong count for category 'b'!", 150, (int) res.get(1).getFacetResultNode().value);
-    assertEquals("Wrong count for category 'a'!",50f, (float) res.get(2).getFacetResultNode().value, 0.00001);
-    assertEquals("Wrong count for category 'b'!",10f, (float) res.get(3).getFacetResultNode().value, 0.00001);
-    
-    taxo.close();
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/ArrayHashMapTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/ArrayHashMapTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/ArrayHashMapTest.java	2013-02-20 13:38:17.532711926 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/ArrayHashMapTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,268 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.Random;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.collections.ArrayHashMap;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class ArrayHashMapTest extends FacetTestCase {
-
-  public static final int RANDOM_TEST_NUM_ITERATIONS = 100; // set to 100,000 for deeper test
-  
-  @Test
-  public void test0() {
-    ArrayHashMap<Integer,Integer> map = new ArrayHashMap<Integer,Integer>();
-
-    assertNull(map.get(0));
-
-    for (int i = 0; i < 100; ++i) {
-      int value = 100 + i;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-    }
-
-    assertEquals(100, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(100 + i, map.get(i).intValue());
-
-    }
-
-    for (int i = 10; i < 90; ++i) {
-      map.remove(i);
-      assertNull(map.get(i));
-    }
-
-    assertEquals(20, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
-    }
-
-    for (int i = 5; i < 85; ++i) {
-      map.put(i, Integer.valueOf(5 + i));
-    }
-    assertEquals(95, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
-    }
-    for (int i = 0; i < 5; ++i) {
-      assertEquals(map.get(i).intValue(), (100 + i));
-    }
-    for (int i = 5; i < 85; ++i) {
-      assertEquals(map.get(i).intValue(), (5 + i));
-    }
-    for (int i = 90; i < 100; ++i) {
-      assertEquals(map.get(i).intValue(), (100 + i));
-    }
-  }
-
-  @Test
-  public void test1() {
-    ArrayHashMap<Integer,Integer> map = new ArrayHashMap<Integer,Integer>();
-
-    for (int i = 0; i < 100; ++i) {
-      map.put(i, Integer.valueOf(100 + i));
-    }
-
-    HashSet<Integer> set = new HashSet<Integer>();
-
-    for (Iterator<Integer> iterator = map.iterator(); iterator.hasNext();) {
-      set.add(iterator.next());
-    }
-
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(set.contains(Integer.valueOf(100 + i)));
-    }
-
-    set.clear();
-    for (Iterator<Integer> iterator = map.iterator(); iterator.hasNext();) {
-      Integer integer = iterator.next();
-      if (integer % 2 == 1) {
-        iterator.remove();
-        continue;
-      }
-      set.add(integer);
-    }
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; i += 2) {
-      assertTrue(set.contains(Integer.valueOf(100 + i)));
-    }
-  }
-
-  @Test
-  public void test2() {
-    ArrayHashMap<Integer,Integer> map = new ArrayHashMap<Integer,Integer>();
-
-    assertTrue(map.isEmpty());
-    assertNull(map.get(0));
-    for (int i = 0; i < 128; ++i) {
-      int value = i * 4096;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-      assertFalse(map.isEmpty());
-    }
-
-    assertEquals(128, map.size());
-    for (int i = 0; i < 128; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i).intValue());
-    }
-
-    for (int i = 0; i < 200; i += 2) {
-      map.remove(i);
-    }
-    assertEquals(64, map.size());
-    for (int i = 1; i < 128; i += 2) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i).intValue());
-      map.remove(i);
-    }
-    assertTrue(map.isEmpty());
-  }
-
-  @Test
-  public void test3() {
-    ArrayHashMap<Integer,Integer> map = new ArrayHashMap<Integer,Integer>();
-    int length = 100;
-    for (int i = 0; i < length; ++i) {
-      map.put(i * 64, 100 + i);
-    }
-    HashSet<Integer> keySet = new HashSet<Integer>();
-    for (Iterator<Integer> iit = map.keyIterator(); iit.hasNext();) {
-      keySet.add(iit.next());
-    }
-    assertEquals(length, keySet.size());
-    for (int i = 0; i < length; ++i) {
-      assertTrue(keySet.contains(i * 64));
-    }
-
-    HashSet<Integer> valueSet = new HashSet<Integer>();
-    for (Iterator<Integer> iit = map.iterator(); iit.hasNext();) {
-      valueSet.add(iit.next());
-    }
-    assertEquals(length, valueSet.size());
-    Object[] array = map.toArray();
-    assertEquals(length, array.length);
-    for (Object value : array) {
-      assertTrue(valueSet.contains(value));
-    }
-
-    Integer[] array2 = new Integer[80];
-    array2 = map.toArray(array2);
-    for (int value : array2) {
-      assertTrue(valueSet.contains(value));
-    }
-    Integer[] array3 = new Integer[120];
-    array3 = map.toArray(array3);
-    for (int i = 0; i < length; ++i) {
-      assertTrue(valueSet.contains(array3[i]));
-    }
-    assertNull(array3[length]);
-
-    for (int i = 0; i < length; ++i) {
-      assertTrue(map.containsValue(i + 100));
-      assertTrue(map.containsKey(i * 64));
-    }
-
-    for (Iterator<Integer> iit = map.keyIterator(); iit.hasNext();) {
-      iit.next();
-      iit.remove();
-    }
-    assertTrue(map.isEmpty());
-    assertEquals(0, map.size());
-
-  }
-
-  // now with random data.. and lots of it
-  @Test
-  public void test4() {
-    ArrayHashMap<Integer,Integer> map = new ArrayHashMap<Integer,Integer>();
-    int length = RANDOM_TEST_NUM_ITERATIONS;
-    
-    // for a repeatable random sequence
-    long seed = random().nextLong();
-    Random random = new Random(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      map.put(i * 128, value);
-    }
-
-    assertEquals(length, map.size());
-    
-    // now repeat
-    random.setSeed(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      assertTrue(map.containsValue(value));
-      assertTrue(map.containsKey(i * 128));
-      assertEquals(Integer.valueOf(value), map.remove(i * 128));
-    }
-    assertEquals(0, map.size());
-    assertTrue(map.isEmpty());
-  }
-
-  @Test
-  public void testEquals() {
-    ArrayHashMap<Integer,Float> map1 = new ArrayHashMap<Integer,Float>(100);
-    ArrayHashMap<Integer,Float> map2 = new ArrayHashMap<Integer,Float>(100);
-    assertEquals("Empty maps should be equal", map1, map2);
-    assertEquals("hashCode() for empty maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    for (int i = 0; i < 100; ++i) {
-      map1.put(i, Float.valueOf(1f/i));
-      map2.put(i, Float.valueOf(1f/i));
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-
-    for (int i = 10; i < 20; i++) {
-      map1.remove(i);
-    }
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    for (int i = 19; i >=10; --i) {
-      map2.remove(i);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    map1.put(-1,-1f);
-    map2.put(-1,-1.1f);
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    map2.put(-1,-1f);
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/FloatToObjectMapTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/FloatToObjectMapTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/FloatToObjectMapTest.java	2013-02-20 13:38:17.532711926 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/FloatToObjectMapTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,267 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.Random;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.collections.FloatIterator;
-import org.apache.lucene.facet.collections.FloatToObjectMap;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class FloatToObjectMapTest extends FacetTestCase {
-
-  @Test
-  public void test0() {
-    FloatToObjectMap<Integer> map = new FloatToObjectMap<Integer>();
-
-    assertNull(map.get(0));
-
-    for (int i = 0; i < 100; ++i) {
-      int value = 100 + i;
-      assertFalse(map.containsValue(value));
-      map.put(i * 1.1f, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i * 1.1f));
-    }
-
-    assertEquals(100, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(map.containsKey(i*1.1f));
-      assertEquals(100 + i, map.get(i*1.1f).intValue());
-
-    }
-
-    for (int i = 10; i < 90; ++i) {
-      map.remove(i*1.1f);
-      assertNull(map.get(i*1.1f));
-    }
-
-    assertEquals(20, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i*1.1f), !(i >= 10 && i < 90));
-    }
-
-    for (int i = 5; i < 85; ++i) {
-      map.put(i*1.1f, Integer.valueOf(5 + i));
-    }
-    assertEquals(95, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i*1.1f), !(i >= 85 && i < 90));
-    }
-    for (int i = 0; i < 5; ++i) {
-      assertEquals(map.get(i*1.1f).intValue(), (100 + i));
-    }
-    for (int i = 5; i < 85; ++i) {
-      assertEquals(map.get(i*1.1f).intValue(), (5 + i));
-    }
-    for (int i = 90; i < 100; ++i) {
-      assertEquals(map.get(i*1.1f).intValue(), (100 + i));
-    }
-  }
-
-  @Test
-  public void test1() {
-    FloatToObjectMap<Integer> map = new FloatToObjectMap<Integer>();
-
-    for (int i = 0; i < 100; ++i) {
-      map.put(i*1.1f, Integer.valueOf(100 + i));
-    }
-
-    HashSet<Integer> set = new HashSet<Integer>();
-
-    for (Iterator<Integer> iterator = map.iterator(); iterator.hasNext();) {
-      set.add(iterator.next());
-    }
-
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(set.contains(Integer.valueOf(100 + i)));
-    }
-
-    set.clear();
-    for (Iterator<Integer> iterator = map.iterator(); iterator.hasNext();) {
-      Integer integer = iterator.next();
-      if (integer % 2 == 1) {
-        iterator.remove();
-        continue;
-      }
-      set.add(integer);
-    }
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; i += 2) {
-      assertTrue(set.contains(Integer.valueOf(100 + i)));
-    }
-  }
-
-  @Test
-  public void test2() {
-    FloatToObjectMap<Integer> map = new FloatToObjectMap<Integer>();
-
-    assertTrue(map.isEmpty());
-    assertNull(map.get(0));
-    for (int i = 0; i < 128; ++i) {
-      int value = i * 4096;
-      assertFalse(map.containsValue(value));
-      map.put(i*1.1f, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i*1.1f));
-      assertFalse(map.isEmpty());
-    }
-
-    assertEquals(128, map.size());
-    for (int i = 0; i < 128; ++i) {
-      assertTrue(map.containsKey(i*1.1f));
-      assertEquals(i * 4096, map.get(i*1.1f).intValue());
-    }
-
-    for (int i = 0; i < 200; i += 2) {
-      map.remove(i*1.1f);
-    }
-    assertEquals(64, map.size());
-    for (int i = 1; i < 128; i += 2) {
-      assertTrue(map.containsKey(i*1.1f));
-      assertEquals(i * 4096, map.get(i*1.1f).intValue());
-      map.remove(i*1.1f);
-    }
-    assertTrue(map.isEmpty());
-  }
-
-  @Test
-  public void test3() {
-    FloatToObjectMap<Integer> map = new FloatToObjectMap<Integer>();
-    int length = 100;
-    for (int i = 0; i < length; ++i) {
-      map.put(i * 64*1.1f, 100 + i);
-    }
-    HashSet<Float> keySet = new HashSet<Float>();
-    for (FloatIterator iit = map.keyIterator(); iit.hasNext();) {
-      keySet.add(iit.next());
-    }
-    assertEquals(length, keySet.size());
-    for (int i = 0; i < length; ++i) {
-      assertTrue(keySet.contains(i * 64*1.1f));
-    }
-
-    HashSet<Integer> valueSet = new HashSet<Integer>();
-    for (Iterator<Integer> iit = map.iterator(); iit.hasNext();) {
-      valueSet.add(iit.next());
-    }
-    assertEquals(length, valueSet.size());
-    Object[] array = map.toArray();
-    assertEquals(length, array.length);
-    for (Object value : array) {
-      assertTrue(valueSet.contains(value));
-    }
-
-    Integer[] array2 = new Integer[80];
-    array2 = map.toArray(array2);
-    for (int value : array2) {
-      assertTrue(valueSet.contains(value));
-    }
-    Integer[] array3 = new Integer[120];
-    array3 = map.toArray(array3);
-    for (int i = 0; i < length; ++i) {
-      assertTrue(valueSet.contains(array3[i]));
-    }
-    assertNull(array3[length]);
-
-    for (int i = 0; i < length; ++i) {
-      assertTrue(map.containsValue(i + 100));
-      assertTrue(map.containsKey(i * 64*1.1f));
-    }
-
-    for (FloatIterator iit = map.keyIterator(); iit.hasNext();) {
-      iit.next();
-      iit.remove();
-    }
-    assertTrue(map.isEmpty());
-    assertEquals(0, map.size());
-
-  }
-
-  // now with random data.. and lots of it
-  @Test
-  public void test4() {
-    FloatToObjectMap<Integer> map = new FloatToObjectMap<Integer>();
-    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
-    
-    // for a repeatable random sequence
-    long seed = random().nextLong();
-    Random random = new Random(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      map.put(i * 128*1.1f, value);
-    }
-
-    assertEquals(length, map.size());
-
-    // now repeat
-    random.setSeed(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      assertTrue(map.containsValue(value));
-      assertTrue(map.containsKey(i * 128*1.1f));
-      assertEquals(Integer.valueOf(value), map.remove(i * 128*1.1f));
-    }
-    assertEquals(0, map.size());
-    assertTrue(map.isEmpty());
-  }
-
-  @Test
-  public void testEquals() {
-    FloatToObjectMap<Integer> map1 = new FloatToObjectMap<Integer>(100);
-    FloatToObjectMap<Integer> map2 = new FloatToObjectMap<Integer>(100);
-    assertEquals("Empty maps should be equal", map1, map2);
-    assertEquals("hashCode() for empty maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    for (int i = 0; i < 100; ++i) {
-      map1.put(i * 1.1f, 100 + i);
-      map2.put(i * 1.1f, 100 + i);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-
-    for (int i = 10; i < 20; i++) {
-      map1.remove(i * 1.1f);
-    }
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    for (int i = 19; i >=10; --i) {
-      map2.remove(i * 1.1f);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    map1.put(-1.1f,-1);
-    map2.put(-1.1f,-2);
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    map2.put(-1.1f,-1);
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/IntArrayTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/IntArrayTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/IntArrayTest.java	2013-02-20 13:38:17.532711926 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/IntArrayTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,125 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.collections.IntArray;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class IntArrayTest extends FacetTestCase {
-  
-  @Test
-  public void test0() {
-    IntArray array = new IntArray();
-    
-    assertEquals(0, array.size());
-    
-    for (int i = 0; i < 100; ++i) {
-      array.addToArray(i);
-    }
-    
-    assertEquals(100, array.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(i, array.get(i));
-    }
-    
-    assertTrue(array.equals(array));
-  }
-  
-  @Test
-  public void test1() {
-    IntArray array = new IntArray();
-    IntArray array2 = new IntArray();
-    
-    assertEquals(0, array.size());
-    
-    for (int i = 0; i < 100; ++i) {
-      array.addToArray(99-i);
-      array2.addToArray(99-i);
-    }
-    
-    assertEquals(100, array.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(i, array.get(99-i));
-    }
-    
-    array.sort();
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(i, array.get(i));
-    }
-
-    assertTrue(array.equals(array2));
-  }
-  
-  @Test
-  public void test2() {
-    IntArray array = new IntArray();
-    IntArray array2 = new IntArray();
-    IntArray array3 = new IntArray();
-    
-    for (int i = 0; i < 100; ++i) {
-      array.addToArray(i);
-    }
-
-    for (int i = 0; i < 100; ++i) {
-      array2.addToArray(i*2);
-    }
-
-    for (int i = 0; i < 50; ++i) {
-      array3.addToArray(i*2);
-    }
-
-    assertFalse(array.equals(array2));
-    
-    array.intersect(array2);
-    assertTrue(array.equals(array3));
-    assertFalse(array.equals(array2));
-  }
-  
-  @Test
-  public void testSet() {
-    int[] original = new int[] { 2,4,6,8,10,12,14 };
-    int[] toSet = new int[] { 1,3,5,7,9,11};
-    
-    IntArray arr = new IntArray();
-    for (int val : original) {
-      arr.addToArray(val);
-    }
-    
-    for (int i = 0; i < toSet.length; i++ ) {
-      int val = toSet[i];
-      arr.set(i, val);
-    }
-    
-    // Test to see if the set worked correctly
-    for (int i = 0; i < toSet.length; i++ ) {
-      assertEquals(toSet[i], arr.get(i));
-    }
-    
-    // Now attempt to set something outside of the array
-    try {
-      arr.set(100, 99);
-      fail("IntArray.set should have thrown an exception for attempting to set outside the array");
-    } catch (ArrayIndexOutOfBoundsException e) {
-      // We expected this to happen so let it fall through
-      // silently
-    }
-    
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/IntHashSetTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/IntHashSetTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/IntHashSetTest.java	2013-02-20 13:38:17.532711926 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/IntHashSetTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,223 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.HashSet;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.collections.IntHashSet;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class IntHashSetTest extends FacetTestCase {
-
-  @Test
-  public void test0() {
-    IntHashSet set0 = new IntHashSet();
-
-    assertEquals(0, set0.size());
-    assertTrue(set0.isEmpty());
-    set0.add(0);
-    assertEquals(1, set0.size());
-    assertFalse(set0.isEmpty());
-    set0.remove(0);
-    assertEquals(0, set0.size());
-    assertTrue(set0.isEmpty());
-  }
-
-  @Test
-  public void test1() {
-    IntHashSet set0 = new IntHashSet();
-
-    assertEquals(0, set0.size());
-    assertTrue(set0.isEmpty());
-    for (int i = 0; i < 1000; ++i) {
-      set0.add(i);
-    }
-    assertEquals(1000, set0.size());
-    assertFalse(set0.isEmpty());
-    for (int i = 0; i < 1000; ++i) {
-      assertTrue(set0.contains(i));
-    }
-
-    set0.clear();
-    assertEquals(0, set0.size());
-    assertTrue(set0.isEmpty());
-
-  }
-
-  @Test
-  public void test2() {
-    IntHashSet set0 = new IntHashSet();
-
-    assertEquals(0, set0.size());
-    assertTrue(set0.isEmpty());
-    for (int i = 0; i < 1000; ++i) {
-      set0.add(1);
-      set0.add(-382);
-    }
-    assertEquals(2, set0.size());
-    assertFalse(set0.isEmpty());
-    set0.remove(-382);
-    set0.remove(1);
-    assertEquals(0, set0.size());
-    assertTrue(set0.isEmpty());
-
-  }
-
-  @Test
-  public void test3() {
-    IntHashSet set0 = new IntHashSet();
-
-    assertEquals(0, set0.size());
-    assertTrue(set0.isEmpty());
-    for (int i = 0; i < 1000; ++i) {
-      set0.add(i);
-    }
-
-    for (int i = 0; i < 1000; i += 2) {
-      set0.remove(i);
-    }
-
-    assertEquals(500, set0.size());
-    for (int i = 0; i < 1000; ++i) {
-      if (i % 2 == 0) {
-        assertFalse(set0.contains(i));
-      } else {
-        assertTrue(set0.contains(i));
-      }
-    }
-
-  }
-
-  @Test
-  public void test4() {
-    IntHashSet set1 = new IntHashSet();
-    HashSet<Integer> set2 = new HashSet<Integer>();
-    for (int i = 0; i < ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS; ++i) {
-      int value = random().nextInt() % 500;
-      boolean shouldAdd = random().nextBoolean();
-      if (shouldAdd) {
-        set1.add(value);
-        set2.add(value);
-      } else {
-        set1.remove(value);
-        set2.remove(value);
-      }
-    }
-    assertEquals(set2.size(), set1.size());
-    for (int value : set2) {
-      assertTrue(set1.contains(value));
-    }
-  }
-
-  @Test
-  public void testRegularJavaSet() {
-    HashSet<Integer> set = new HashSet<Integer>();
-    for (int j = 0; j < 100; ++j) {
-      for (int i = 0; i < ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS; ++i) {
-        int value = random().nextInt() % 5000;
-        boolean shouldAdd = random().nextBoolean();
-        if (shouldAdd) {
-          set.add(value);
-        } else {
-          set.remove(value);
-        }
-      }
-      set.clear();
-    }
-  }
-
-  @Test
-  public void testMySet() {
-    IntHashSet set = new IntHashSet();
-    for (int j = 0; j < 100; ++j) {
-      for (int i = 0; i < ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS; ++i) {
-        int value = random().nextInt() % 5000;
-        boolean shouldAdd = random().nextBoolean();
-        if (shouldAdd) {
-          set.add(value);
-        } else {
-          set.remove(value);
-        }
-      }
-      set.clear();
-    }
-  }
-
-  @Test
-  public void testToArray() {
-    IntHashSet set = new IntHashSet();
-    for (int j = 0; j < 100; ++j) {
-      for (int i = 0; i < ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS; ++i) {
-        int value = random().nextInt() % 5000;
-        boolean shouldAdd = random().nextBoolean();
-        if (shouldAdd) {
-          set.add(value);
-        } else {
-          set.remove(value);
-        }
-      }
-      int[] vals = set.toArray();
-      assertEquals(set.size(), vals.length);
-
-      int[] realValues = new int[set.size()];
-      int[] unrealValues = set.toArray(realValues);
-      assertEquals(realValues, unrealValues);
-      for (int value : vals) {
-        assertTrue(set.remove(value));
-      }
-      for (int i = 0; i < vals.length; ++i) {
-        assertEquals(vals[i], realValues[i]);
-      }
-    }
-  }
-
-  @Test
-  public void testZZRegularJavaSet() {
-    HashSet<Integer> set = new HashSet<Integer>();
-    for (int j = 0; j < 100; ++j) {
-      for (int i = 0; i < ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS; ++i) {
-        int value = random().nextInt() % 5000;
-        boolean shouldAdd = random().nextBoolean();
-        if (shouldAdd) {
-          set.add(value);
-        } else {
-          set.remove(value);
-        }
-      }
-      set.clear();
-    }
-  }
-
-  @Test
-  public void testZZMySet() {
-    IntHashSet set = new IntHashSet();
-    for (int j = 0; j < 100; ++j) {
-      for (int i = 0; i < ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS; ++i) {
-        int value = random().nextInt() % 5000;
-        boolean shouldAdd = random().nextBoolean();
-        if (shouldAdd) {
-          set.add(value);
-        } else {
-          set.remove(value);
-        }
-      }
-      set.clear();
-    }
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToDoubleMapTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToDoubleMapTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToDoubleMapTest.java	2013-02-20 13:38:17.532711926 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToDoubleMapTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,272 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.HashSet;
-import java.util.Random;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.collections.DoubleIterator;
-import org.apache.lucene.facet.collections.IntIterator;
-import org.apache.lucene.facet.collections.IntToDoubleMap;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class IntToDoubleMapTest extends FacetTestCase {
-  
-  private static void assertGround(double value) {
-    assertEquals(IntToDoubleMap.GROUND, value, Double.MAX_VALUE);
-  }
-  
-  @Test
-  public void test0() {
-    IntToDoubleMap map = new IntToDoubleMap();
-
-    assertGround(map.get(0));
-    
-    for (int i = 0; i < 100; ++i) {
-      int value = 100 + i;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-    }
-
-    assertEquals(100, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(100 + i, map.get(i), Double.MAX_VALUE);
-
-    }
-
-    for (int i = 10; i < 90; ++i) {
-      map.remove(i);
-      assertGround(map.get(i));
-    }
-
-    assertEquals(20, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
-    }
-
-    for (int i = 5; i < 85; ++i) {
-      map.put(i, Integer.valueOf(5 + i));
-    }
-    assertEquals(95, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
-    }
-    for (int i = 0; i < 5; ++i) {
-      assertEquals(map.get(i), (100 + i), Double.MAX_VALUE);
-    }
-    for (int i = 5; i < 85; ++i) {
-      assertEquals(map.get(i), (5 + i), Double.MAX_VALUE);
-    }
-    for (int i = 90; i < 100; ++i) {
-      assertEquals(map.get(i), (100 + i), Double.MAX_VALUE);
-    }
-  }
-
-  @Test
-  public void test1() {
-    IntToDoubleMap map = new IntToDoubleMap();
-
-    for (int i = 0; i < 100; ++i) {
-      map.put(i, Integer.valueOf(100 + i));
-    }
-    
-    HashSet<Double> set = new HashSet<Double>();
-    
-    for (DoubleIterator iterator = map.iterator(); iterator.hasNext();) {
-      set.add(iterator.next());
-    }
-
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(set.contains(Double.valueOf(100+i)));
-    }
-
-    set.clear();
-    for (DoubleIterator iterator = map.iterator(); iterator.hasNext();) {
-      double d = iterator.next();
-      if (d % 2 == 1) {
-        iterator.remove();
-        continue;
-      }
-      set.add(d);
-    }
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; i+=2) {
-      assertTrue(set.contains(Double.valueOf(100+i)));
-    }
-  }
-  
-  @Test
-  public void test2() {
-    IntToDoubleMap map = new IntToDoubleMap();
-
-    assertTrue(map.isEmpty());
-    assertGround(map.get(0));
-    for (int i = 0; i < 128; ++i) {
-      int value = i * 4096;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-      assertFalse(map.isEmpty());
-    }
-
-    assertEquals(128, map.size());
-    for (int i = 0; i < 128; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i), Double.MAX_VALUE);
-    }
-    
-    for (int i = 0 ; i < 200; i+=2) {
-      map.remove(i);
-    }
-    assertEquals(64, map.size());
-    for (int i = 1; i < 128; i+=2) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i), Double.MAX_VALUE);
-      map.remove(i);
-    }
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void test3() {
-    IntToDoubleMap map = new IntToDoubleMap();
-    int length = 100;
-    for (int i = 0; i < length; ++i) {
-      map.put(i*64, 100 + i);
-    }
-    HashSet<Integer> keySet = new HashSet<Integer>();
-    for (IntIterator iit = map.keyIterator(); iit.hasNext(); ) {
-      keySet.add(iit.next());
-    }
-    assertEquals(length, keySet.size());
-    for (int i = 0; i < length; ++i) {
-      assertTrue(keySet.contains(i * 64));
-    }
-    
-    HashSet<Double> valueSet = new HashSet<Double>();
-    for (DoubleIterator iit = map.iterator(); iit.hasNext(); ) {
-      valueSet.add(iit.next());
-    }
-    assertEquals(length, valueSet.size());
-    double[] array = map.toArray();
-    assertEquals(length, array.length);
-    for (double value: array) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    double[] array2 = new double[80];
-    array2 = map.toArray(array2);
-    assertEquals(length, array2.length);
-    for (double value: array2) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    double[] array3 = new double[120];
-    array3 = map.toArray(array3);
-    for (int i = 0 ;i < length; ++i) {
-      assertTrue(valueSet.contains(array3[i]));
-    }
-    
-    for (int i = 0; i < length; ++i) {
-      assertTrue(map.containsValue(i + 100));
-      assertTrue(map.containsKey(i*64));
-    }
-    
-    for (IntIterator iit = map.keyIterator(); iit.hasNext(); ) {
-      iit.next();
-      iit.remove();
-    }
-    assertTrue(map.isEmpty());
-    assertEquals(0, map.size());
-    
-  }
-
-  // now with random data.. and lots of it
-  @Test
-  public void test4() {
-    IntToDoubleMap map = new IntToDoubleMap();
-    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
-    // for a repeatable random sequence
-    long seed = random().nextLong();
-    Random random = new Random(seed);
-    
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      map.put(i*128, value);
-    }
-
-    assertEquals(length, map.size());
-
-    // now repeat
-    random.setSeed(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      assertTrue(map.containsValue(value));
-      assertTrue(map.containsKey(i*128));
-      assertEquals(0, Double.compare(value, map.remove(i*128)));
-    }
-    assertEquals(0, map.size());
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void testEquals() {
-    IntToDoubleMap map1 = new IntToDoubleMap(100);
-    IntToDoubleMap map2 = new IntToDoubleMap(100);
-    assertEquals("Empty maps should be equal", map1, map2);
-    assertEquals("hashCode() for empty maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    for (int i = 0; i < 100; ++i) {
-      map1.put(i, Float.valueOf(1f/i));
-      map2.put(i, Float.valueOf(1f/i));
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-
-    for (int i = 10; i < 20; i++) {
-      map1.remove(i);
-    }
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    for (int i = 19; i >=10; --i) {
-      map2.remove(i);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    map1.put(-1,-1f);
-    map2.put(-1,-1.1f);
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    map2.put(-1,-1f);
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToFloatMapTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToFloatMapTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToFloatMapTest.java	2013-02-20 13:38:17.532711926 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToFloatMapTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,272 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.HashSet;
-import java.util.Random;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.collections.FloatIterator;
-import org.apache.lucene.facet.collections.IntIterator;
-import org.apache.lucene.facet.collections.IntToFloatMap;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class IntToFloatMapTest extends FacetTestCase {
-  
-  private static void assertGround(float value) {
-    assertEquals(IntToFloatMap.GROUND, value, Float.MAX_VALUE);
-  }
-  
-  @Test
-  public void test0() {
-    IntToFloatMap map = new IntToFloatMap();
-
-    assertGround(map.get(0));
-    
-    for (int i = 0; i < 100; ++i) {
-      int value = 100 + i;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-    }
-
-    assertEquals(100, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(100 + i, map.get(i), Float.MAX_VALUE);
-
-    }
-
-    for (int i = 10; i < 90; ++i) {
-      map.remove(i);
-      assertGround(map.get(i));
-    }
-
-    assertEquals(20, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
-    }
-
-    for (int i = 5; i < 85; ++i) {
-      map.put(i, Integer.valueOf(5 + i));
-    }
-    assertEquals(95, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
-    }
-    for (int i = 0; i < 5; ++i) {
-      assertEquals(map.get(i), (100 + i), Float.MAX_VALUE);
-    }
-    for (int i = 5; i < 85; ++i) {
-      assertEquals(map.get(i), (5 + i), Float.MAX_VALUE);
-    }
-    for (int i = 90; i < 100; ++i) {
-      assertEquals(map.get(i), (100 + i), Float.MAX_VALUE);
-    }
-  }
-
-  @Test
-  public void test1() {
-    IntToFloatMap map = new IntToFloatMap();
-
-    for (int i = 0; i < 100; ++i) {
-      map.put(i, Integer.valueOf(100 + i));
-    }
-    
-    HashSet<Float> set = new HashSet<Float>();
-    
-    for (FloatIterator iterator = map.iterator(); iterator.hasNext();) {
-      set.add(iterator.next());
-    }
-
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(set.contains(Float.valueOf(100+i)));
-    }
-
-    set.clear();
-    for (FloatIterator iterator = map.iterator(); iterator.hasNext();) {
-      float d = iterator.next();
-      if (d % 2 == 1) {
-        iterator.remove();
-        continue;
-      }
-      set.add(d);
-    }
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; i+=2) {
-      assertTrue(set.contains(Float.valueOf(100+i)));
-    }
-  }
-  
-  @Test
-  public void test2() {
-    IntToFloatMap map = new IntToFloatMap();
-
-    assertTrue(map.isEmpty());
-    assertGround(map.get(0));
-    for (int i = 0; i < 128; ++i) {
-      int value = i * 4096;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-      assertFalse(map.isEmpty());
-    }
-
-    assertEquals(128, map.size());
-    for (int i = 0; i < 128; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i), Float.MAX_VALUE);
-    }
-    
-    for (int i = 0 ; i < 200; i+=2) {
-      map.remove(i);
-    }
-    assertEquals(64, map.size());
-    for (int i = 1; i < 128; i+=2) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i), Float.MAX_VALUE);
-      map.remove(i);
-    }
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void test3() {
-    IntToFloatMap map = new IntToFloatMap();
-    int length = 100;
-    for (int i = 0; i < length; ++i) {
-      map.put(i*64, 100 + i);
-    }
-    HashSet<Integer> keySet = new HashSet<Integer>();
-    for (IntIterator iit = map.keyIterator(); iit.hasNext(); ) {
-      keySet.add(iit.next());
-    }
-    assertEquals(length, keySet.size());
-    for (int i = 0; i < length; ++i) {
-      assertTrue(keySet.contains(i * 64));
-    }
-    
-    HashSet<Float> valueSet = new HashSet<Float>();
-    for (FloatIterator iit = map.iterator(); iit.hasNext(); ) {
-      valueSet.add(iit.next());
-    }
-    assertEquals(length, valueSet.size());
-    float[] array = map.toArray();
-    assertEquals(length, array.length);
-    for (float value: array) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    float[] array2 = new float[80];
-    array2 = map.toArray(array2);
-    assertEquals(length, array2.length);
-    for (float value: array2) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    float[] array3 = new float[120];
-    array3 = map.toArray(array3);
-    for (int i = 0 ;i < length; ++i) {
-      assertTrue(valueSet.contains(array3[i]));
-    }
-    
-    for (int i = 0; i < length; ++i) {
-      assertTrue(map.containsValue(i + 100));
-      assertTrue(map.containsKey(i*64));
-    }
-    
-    for (IntIterator iit = map.keyIterator(); iit.hasNext(); ) {
-      iit.next();
-      iit.remove();
-    }
-    assertTrue(map.isEmpty());
-    assertEquals(0, map.size());
-    
-  }
-
-  // now with random data.. and lots of it
-  @Test
-  public void test4() {
-    IntToFloatMap map = new IntToFloatMap();
-    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
-    // for a repeatable random sequence
-    long seed = random().nextLong();
-    Random random = new Random(seed);
-    
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      map.put(i*128, value);
-    }
-
-    assertEquals(length, map.size());
-
-    // now repeat
-    random.setSeed(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      assertTrue(map.containsValue(value));
-      assertTrue(map.containsKey(i*128));
-      assertEquals(0, Float.compare(value, map.remove(i*128)));
-    }
-    assertEquals(0, map.size());
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void testEquals() {
-    IntToFloatMap map1 = new IntToFloatMap(100);
-    IntToFloatMap map2 = new IntToFloatMap(100);
-    assertEquals("Empty maps should be equal", map1, map2);
-    assertEquals("hashCode() for empty maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    for (int i = 0; i < 100; ++i) {
-      map1.put(i, Float.valueOf(1f/i));
-      map2.put(i, Float.valueOf(1f/i));
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-
-    for (int i = 10; i < 20; i++) {
-      map1.remove(i);
-    }
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    for (int i = 19; i >=10; --i) {
-      map2.remove(i);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    map1.put(-1,-1f);
-    map2.put(-1,-1.1f);
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    map2.put(-1,-1f);
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToIntMapTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToIntMapTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToIntMapTest.java	2013-02-20 13:38:17.532711926 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToIntMapTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,272 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.HashSet;
-import java.util.Random;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.collections.IntIterator;
-import org.apache.lucene.facet.collections.IntToIntMap;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class IntToIntMapTest extends FacetTestCase {
-  
-  private static void assertGround(int value) {
-    assertEquals(IntToIntMap.GROUD, value);
-  }
-  
-  @Test
-  public void test0() {
-    IntToIntMap map = new IntToIntMap();
-
-    assertGround(map.get(0));
-    
-    for (int i = 0; i < 100; ++i) {
-      int value = 100 + i;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-    }
-
-    assertEquals(100, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(100 + i, map.get(i));
-
-    }
-
-    for (int i = 10; i < 90; ++i) {
-      map.remove(i);
-      assertGround(map.get(i));
-    }
-
-    assertEquals(20, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
-    }
-
-    for (int i = 5; i < 85; ++i) {
-      map.put(i, Integer.valueOf(5 + i));
-    }
-    assertEquals(95, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
-    }
-    for (int i = 0; i < 5; ++i) {
-      assertEquals(map.get(i), (100 + i));
-    }
-    for (int i = 5; i < 85; ++i) {
-      assertEquals(map.get(i), (5 + i));
-    }
-    for (int i = 90; i < 100; ++i) {
-      assertEquals(map.get(i), (100 + i));
-    }
-  }
-
-  @Test
-  public void test1() {
-    IntToIntMap map = new IntToIntMap();
-
-    for (int i = 0; i < 100; ++i) {
-      map.put(i, Integer.valueOf(100 + i));
-    }
-    
-    HashSet<Integer> set = new HashSet<Integer>();
-    
-    for (IntIterator iterator = map.iterator(); iterator.hasNext();) {
-      set.add(iterator.next());
-    }
-
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(set.contains(Integer.valueOf(100+i)));
-    }
-
-    set.clear();
-    for (IntIterator iterator = map.iterator(); iterator.hasNext();) {
-      Integer integer = iterator.next();
-      if (integer % 2 == 1) {
-        iterator.remove();
-        continue;
-      }
-      set.add(integer);
-    }
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; i+=2) {
-      assertTrue(set.contains(Integer.valueOf(100+i)));
-    }
-  }
-  
-  @Test
-  public void test2() {
-    IntToIntMap map = new IntToIntMap();
-
-    assertTrue(map.isEmpty());
-    assertGround(map.get(0));
-    for (int i = 0; i < 128; ++i) {
-      int value = i * 4096;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-      assertFalse(map.isEmpty());
-    }
-
-    assertEquals(128, map.size());
-    for (int i = 0; i < 128; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i));
-    }
-    
-    for (int i = 0 ; i < 200; i+=2) {
-      map.remove(i);
-    }
-    assertEquals(64, map.size());
-    for (int i = 1; i < 128; i+=2) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i));
-      map.remove(i);
-    }
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void test3() {
-    IntToIntMap map = new IntToIntMap();
-    int length = 100;
-    for (int i = 0; i < length; ++i) {
-      map.put(i*64, 100 + i);
-    }
-    HashSet<Integer> keySet = new HashSet<Integer>();
-    for (IntIterator iit = map.keyIterator(); iit.hasNext(); ) {
-      keySet.add(iit.next());
-    }
-    assertEquals(length, keySet.size());
-    for (int i = 0; i < length; ++i) {
-      assertTrue(keySet.contains(i * 64));
-    }
-    
-    HashSet<Integer> valueSet = new HashSet<Integer>();
-    for (IntIterator iit = map.iterator(); iit.hasNext(); ) {
-      valueSet.add(iit.next());
-    }
-    assertEquals(length, valueSet.size());
-    int[] array = map.toArray();
-    assertEquals(length, array.length);
-    for (int value: array) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    int[] array2 = new int[80];
-    array2 = map.toArray(array2);
-    assertEquals(length, array2.length);
-    for (int value: array2) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    int[] array3 = new int[120];
-    array3 = map.toArray(array3);
-    for (int i = 0 ;i < length; ++i) {
-      assertTrue(valueSet.contains(array3[i]));
-    }
-    
-    for (int i = 0; i < length; ++i) {
-      assertTrue(map.containsValue(i + 100));
-      assertTrue(map.containsKey(i*64));
-    }
-    
-    for (IntIterator iit = map.keyIterator(); iit.hasNext(); ) {
-      iit.next();
-      iit.remove();
-    }
-    assertTrue(map.isEmpty());
-    assertEquals(0, map.size());
-    
-  }
-
-  // now with random data.. and lots of it
-  @Test
-  public void test4() {
-    IntToIntMap map = new IntToIntMap();
-    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
-    
-    // for a repeatable random sequence
-    long seed = random().nextLong();
-    Random random = new Random(seed);
-    
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      map.put(i*128, value);
-    }
-
-    assertEquals(length, map.size());
-
-    // now repeat
-    random.setSeed(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      
-      assertTrue(map.containsValue(value));
-      assertTrue(map.containsKey(i*128));
-      assertEquals(value, map.remove(i*128));
-    }
-    assertEquals(0, map.size());
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void testEquals() {
-    IntToIntMap map1 = new IntToIntMap(100);
-    IntToIntMap map2 = new IntToIntMap(100);
-    assertEquals("Empty maps should be equal", map1, map2);
-    assertEquals("hashCode() for empty maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    for (int i = 0; i < 100; ++i) {
-      map1.put(i, 100 + i);
-      map2.put(i, 100 + i);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-
-    for (int i = 10; i < 20; i++) {
-      map1.remove(i);
-    }
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    for (int i = 19; i >=10; --i) {
-      map2.remove(i);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    map1.put(-1,-1);
-    map2.put(-1,-2);
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    map2.put(-1,-1);
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToObjectMapTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToObjectMapTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToObjectMapTest.java	2013-02-20 13:38:17.532711926 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToObjectMapTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,267 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.Random;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.collections.IntIterator;
-import org.apache.lucene.facet.collections.IntToObjectMap;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class IntToObjectMapTest extends FacetTestCase {
-
-  @Test
-  public void test0() {
-    IntToObjectMap<Integer> map = new IntToObjectMap<Integer>();
-
-    assertNull(map.get(0));
-
-    for (int i = 0; i < 100; ++i) {
-      int value = 100 + i;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-    }
-
-    assertEquals(100, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(100 + i, map.get(i).intValue());
-
-    }
-
-    for (int i = 10; i < 90; ++i) {
-      map.remove(i);
-      assertNull(map.get(i));
-    }
-
-    assertEquals(20, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
-    }
-
-    for (int i = 5; i < 85; ++i) {
-      map.put(i, Integer.valueOf(5 + i));
-    }
-    assertEquals(95, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
-    }
-    for (int i = 0; i < 5; ++i) {
-      assertEquals(map.get(i).intValue(), (100 + i));
-    }
-    for (int i = 5; i < 85; ++i) {
-      assertEquals(map.get(i).intValue(), (5 + i));
-    }
-    for (int i = 90; i < 100; ++i) {
-      assertEquals(map.get(i).intValue(), (100 + i));
-    }
-  }
-
-  @Test
-  public void test1() {
-    IntToObjectMap<Integer> map = new IntToObjectMap<Integer>();
-
-    for (int i = 0; i < 100; ++i) {
-      map.put(i, Integer.valueOf(100 + i));
-    }
-
-    HashSet<Integer> set = new HashSet<Integer>();
-
-    for (Iterator<Integer> iterator = map.iterator(); iterator.hasNext();) {
-      set.add(iterator.next());
-    }
-
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(set.contains(Integer.valueOf(100 + i)));
-    }
-
-    set.clear();
-    for (Iterator<Integer> iterator = map.iterator(); iterator.hasNext();) {
-      Integer integer = iterator.next();
-      if (integer % 2 == 1) {
-        iterator.remove();
-        continue;
-      }
-      set.add(integer);
-    }
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; i += 2) {
-      assertTrue(set.contains(Integer.valueOf(100 + i)));
-    }
-  }
-
-  @Test
-  public void test2() {
-    IntToObjectMap<Integer> map = new IntToObjectMap<Integer>();
-
-    assertTrue(map.isEmpty());
-    assertNull(map.get(0));
-    for (int i = 0; i < 128; ++i) {
-      int value = i * 4096;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-      assertFalse(map.isEmpty());
-    }
-
-    assertEquals(128, map.size());
-    for (int i = 0; i < 128; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i).intValue());
-    }
-
-    for (int i = 0; i < 200; i += 2) {
-      map.remove(i);
-    }
-    assertEquals(64, map.size());
-    for (int i = 1; i < 128; i += 2) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i).intValue());
-      map.remove(i);
-    }
-    assertTrue(map.isEmpty());
-  }
-
-  @Test
-  public void test3() {
-    IntToObjectMap<Integer> map = new IntToObjectMap<Integer>();
-    int length = 100;
-    for (int i = 0; i < length; ++i) {
-      map.put(i * 64, 100 + i);
-    }
-    HashSet<Integer> keySet = new HashSet<Integer>();
-    for (IntIterator iit = map.keyIterator(); iit.hasNext();) {
-      keySet.add(iit.next());
-    }
-    assertEquals(length, keySet.size());
-    for (int i = 0; i < length; ++i) {
-      assertTrue(keySet.contains(i * 64));
-    }
-
-    HashSet<Integer> valueSet = new HashSet<Integer>();
-    for (Iterator<Integer> iit = map.iterator(); iit.hasNext();) {
-      valueSet.add(iit.next());
-    }
-    assertEquals(length, valueSet.size());
-    Object[] array = map.toArray();
-    assertEquals(length, array.length);
-    for (Object value : array) {
-      assertTrue(valueSet.contains(value));
-    }
-
-    Integer[] array2 = new Integer[80];
-    array2 = map.toArray(array2);
-    for (int value : array2) {
-      assertTrue(valueSet.contains(value));
-    }
-    Integer[] array3 = new Integer[120];
-    array3 = map.toArray(array3);
-    for (int i = 0; i < length; ++i) {
-      assertTrue(valueSet.contains(array3[i]));
-    }
-    assertNull(array3[length]);
-
-    for (int i = 0; i < length; ++i) {
-      assertTrue(map.containsValue(i + 100));
-      assertTrue(map.containsKey(i * 64));
-    }
-
-    for (IntIterator iit = map.keyIterator(); iit.hasNext();) {
-      iit.next();
-      iit.remove();
-    }
-    assertTrue(map.isEmpty());
-    assertEquals(0, map.size());
-
-  }
-
-  // now with random data.. and lots of it
-  @Test
-  public void test4() {
-    IntToObjectMap<Integer> map = new IntToObjectMap<Integer>();
-    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
-    
-    // for a repeatable random sequence
-    long seed = random().nextLong();
-    Random random = new Random(seed);
-    
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      map.put(i * 128, value);
-    }
-
-    assertEquals(length, map.size());
-
-    // now repeat
-    random.setSeed(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      assertTrue(map.containsValue(value));
-      assertTrue(map.containsKey(i * 128));
-      assertEquals(Integer.valueOf(value), map.remove(i * 128));
-    }
-    assertEquals(0, map.size());
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void testEquals() {
-    IntToObjectMap<Double> map1 = new IntToObjectMap<Double>(100);
-    IntToObjectMap<Double> map2 = new IntToObjectMap<Double>(100);
-    assertEquals("Empty maps should be equal", map1, map2);
-    assertEquals("hashCode() for empty maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    for (int i = 0; i < 100; ++i) {
-      map1.put(i, Double.valueOf(1f/i));
-      map2.put(i, Double.valueOf(1f/i));
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-  
-    for (int i = 10; i < 20; i++) {
-      map1.remove(i);
-    }
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    for (int i = 19; i >=10; --i) {
-      map2.remove(i);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    map1.put(-1,-1d);
-    map2.put(-1,-1.1d);
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    map2.put(-1,-1d);
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/ObjectToFloatMapTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/ObjectToFloatMapTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/ObjectToFloatMapTest.java	2013-02-20 13:38:17.532711926 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/ObjectToFloatMapTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,279 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.Random;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.collections.FloatIterator;
-import org.apache.lucene.facet.collections.ObjectToFloatMap;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class ObjectToFloatMapTest extends FacetTestCase {
-
-  @Test
-  public void test0() {
-    ObjectToFloatMap<Integer> map = new ObjectToFloatMap<Integer>();
-
-    assertNaN(map.get(0));
-    
-    for (int i = 0; i < 100; ++i) {
-      int value = 100 + i;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNaN(map.get(i));
-    }
-
-    assertEquals(100, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(100 + i, map.get(i), 1E-5);
-
-    }
-
-    for (int i = 10; i < 90; ++i) {
-      map.remove(i);
-      assertNaN(map.get(i));
-    }
-
-    assertEquals(20, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
-    }
-
-    for (int i = 5; i < 85; ++i) {
-      map.put(i, Integer.valueOf(5 + i));
-    }
-    assertEquals(95, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
-    }
-    for (int i = 0; i < 5; ++i) {
-      assertEquals(map.get(i), (100 + i), 1E-5);
-    }
-    for (int i = 5; i < 85; ++i) {
-      assertEquals(map.get(i), (5 + i), 1E-5);
-    }
-    for (int i = 90; i < 100; ++i) {
-      assertEquals(map.get(i), (100 + i), 1E-5);
-    }
-  }
-
-  private static void assertNaN(float f) {
-    assertTrue(Float.isNaN(f));
-  }
-  
-  private static void assertNotNaN(float f) {
-    assertFalse(Float.isNaN(f));
-  }
-
-  @Test
-  public void test1() {
-    ObjectToFloatMap<Integer> map = new ObjectToFloatMap<Integer>();
-
-    for (int i = 0; i < 100; ++i) {
-      map.put(i, Integer.valueOf(100 + i));
-    }
-    
-    HashSet<Float> set = new HashSet<Float>();
-    
-    for (FloatIterator iterator = map.iterator(); iterator.hasNext();) {
-      set.add(iterator.next());
-    }
-
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(set.contains(Float.valueOf(100+i)));
-    }
-
-    set.clear();
-    for (FloatIterator iterator = map.iterator(); iterator.hasNext();) {
-      Float value = iterator.next();
-      if (value % 2 == 1) {
-        iterator.remove();
-        continue;
-      }
-      set.add(value);
-    }
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; i+=2) {
-      assertTrue(set.contains(Float.valueOf(100+i)));
-    }
-  }
-  
-  @Test
-  public void test2() {
-    ObjectToFloatMap<Integer> map = new ObjectToFloatMap<Integer>();
-
-    assertTrue(map.isEmpty());
-    assertNaN(map.get(0));
-    for (int i = 0; i < 128; ++i) {
-      int value = i * 4096;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNaN(map.get(i));
-      assertFalse(map.isEmpty());
-    }
-
-    assertEquals(128, map.size());
-    for (int i = 0; i < 128; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i), 1E-5);
-    }
-    
-    for (int i = 0 ; i < 200; i+=2) {
-      map.remove(i);
-    }
-    assertEquals(64, map.size());
-    for (int i = 1; i < 128; i+=2) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i), 1E-5);
-      map.remove(i);
-    }
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void test3() {
-    ObjectToFloatMap<Integer> map = new ObjectToFloatMap<Integer>();
-    int length = 100;
-    for (int i = 0; i < length; ++i) {
-      map.put(i*64, 100 + i);
-    }
-    HashSet<Integer> keySet = new HashSet<Integer>();
-    for (Iterator<Integer> iit = map.keyIterator(); iit.hasNext(); ) {
-      keySet.add(iit.next());
-    }
-    assertEquals(length, keySet.size());
-    for (int i = 0; i < length; ++i) {
-      assertTrue(keySet.contains(i * 64));
-    }
-    
-    HashSet<Float> valueSet = new HashSet<Float>();
-    for (FloatIterator iit = map.iterator(); iit.hasNext(); ) {
-      valueSet.add(iit.next());
-    }
-    assertEquals(length, valueSet.size());
-    float[] array = map.toArray();
-    assertEquals(length, array.length);
-    for (float value: array) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    float[] array2 = new float[80];
-    array2 = map.toArray(array2);
-    assertEquals(80, array2.length);
-    for (float value: array2) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    float[] array3 = new float[120];
-    array3 = map.toArray(array3);
-    for (int i = 0 ;i < length; ++i) {
-      assertTrue(valueSet.contains(array3[i]));
-    }
-    assertNaN(array3[length]);
-    
-    for (int i = 0; i < length; ++i) {
-      assertTrue(map.containsValue(i + 100));
-      assertTrue(map.containsKey(i*64));
-    }
-    
-    for (Iterator<Integer> iit = map.keyIterator(); iit.hasNext(); ) {
-      iit.next();
-      iit.remove();
-    }
-    assertTrue(map.isEmpty());
-    assertEquals(0, map.size());
-    
-  }
-
-  // now with random data.. and lots of it
-  @Test
-  public void test4() {
-    ObjectToFloatMap<Integer> map = new ObjectToFloatMap<Integer>();
-    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
-    
-    // for a repeatable random sequence
-    long seed = random().nextLong();
-    Random random = new Random(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      map.put(i*128, value);
-    }
-
-    assertEquals(length, map.size());
-
-    // now repeat
-    random.setSeed(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      
-      assertTrue(map.containsValue(value));
-      assertTrue(map.containsKey(i*128));
-      assertEquals(0, Float.compare(value, map.remove(i*128)));
-    }
-    assertEquals(0, map.size());
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void testEquals() {
-    ObjectToFloatMap<Integer> map1 = new ObjectToFloatMap<Integer>(100);
-    ObjectToFloatMap<Integer> map2 = new ObjectToFloatMap<Integer>(100);
-    assertEquals("Empty maps should be equal", map1, map2);
-    assertEquals("hashCode() for empty maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    for (int i = 0; i < 100; ++i) {
-      map1.put(i, Float.valueOf(1f/i));
-      map2.put(i, Float.valueOf(1f/i));
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-
-    for (int i = 10; i < 20; i++) {
-      map1.remove(i);
-    }
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    for (int i = 19; i >=10; --i) {
-      map2.remove(i);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    map1.put(-1,-1f);
-    map2.put(-1,-1.1f);
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    map2.put(-1,-1f);
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/ObjectToIntMapTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/ObjectToIntMapTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/ObjectToIntMapTest.java	2013-02-20 13:38:17.532711926 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/ObjectToIntMapTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,277 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.Random;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.collections.IntIterator;
-import org.apache.lucene.facet.collections.ObjectToIntMap;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class ObjectToIntMapTest extends FacetTestCase {
-
-  @Test
-  public void test0() {
-    ObjectToIntMap<Integer> map = new ObjectToIntMap<Integer>();
-
-    assertIntegerMaxValue(map.get(0));
-    
-    for (int i = 0; i < 100; ++i) {
-      int value = 100 + i;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotIntegerMaxValue(map.get(i));
-    }
-
-    assertEquals(100, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(100 + i, map.get(i), 1E-5);
-
-    }
-
-    for (int i = 10; i < 90; ++i) {
-      map.remove(i);
-      assertIntegerMaxValue(map.get(i));
-    }
-
-    assertEquals(20, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
-    }
-
-    for (int i = 5; i < 85; ++i) {
-      map.put(i, Integer.valueOf(5 + i));
-    }
-    assertEquals(95, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
-    }
-    for (int i = 0; i < 5; ++i) {
-      assertEquals(map.get(i), (100 + i), 1E-5);
-    }
-    for (int i = 5; i < 85; ++i) {
-      assertEquals(map.get(i), (5 + i), 1E-5);
-    }
-    for (int i = 90; i < 100; ++i) {
-      assertEquals(map.get(i), (100 + i), 1E-5);
-    }
-  }
-
-  private static void assertIntegerMaxValue(int i) {
-    assertTrue(i == Integer.MAX_VALUE);
-  }
-  
-  private static void assertNotIntegerMaxValue(int i) {
-    assertFalse(i == Integer.MAX_VALUE);
-  }
-
-  @Test
-  public void test1() {
-    ObjectToIntMap<Integer> map = new ObjectToIntMap<Integer>();
-
-    for (int i = 0; i < 100; ++i) {
-      map.put(i, Integer.valueOf(100 + i));
-    }
-    
-    HashSet<Integer> set = new HashSet<Integer>();
-    
-    for (IntIterator iterator = map.iterator(); iterator.hasNext();) {
-      set.add(iterator.next());
-    }
-
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(set.contains(Integer.valueOf(100+i)));
-    }
-
-    set.clear();
-    for (IntIterator iterator = map.iterator(); iterator.hasNext();) {
-      Integer value = iterator.next();
-      if (value % 2 == 1) {
-        iterator.remove();
-        continue;
-      }
-      set.add(value);
-    }
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; i+=2) {
-      assertTrue(set.contains(Integer.valueOf(100+i)));
-    }
-  }
-  
-  @Test
-  public void test2() {
-    ObjectToIntMap<Integer> map = new ObjectToIntMap<Integer>();
-
-    assertTrue(map.isEmpty());
-    assertIntegerMaxValue(map.get(0));
-    for (int i = 0; i < 128; ++i) {
-      int value = i * 4096;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotIntegerMaxValue(map.get(i));
-      assertFalse(map.isEmpty());
-    }
-
-    assertEquals(128, map.size());
-    for (int i = 0; i < 128; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i), 1E-5);
-    }
-    
-    for (int i = 0 ; i < 200; i+=2) {
-      map.remove(i);
-    }
-    assertEquals(64, map.size());
-    for (int i = 1; i < 128; i+=2) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i), 1E-5);
-      map.remove(i);
-    }
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void test3() {
-    ObjectToIntMap<Integer> map = new ObjectToIntMap<Integer>();
-    int length = 100;
-    for (int i = 0; i < length; ++i) {
-      map.put(i*64, 100 + i);
-    }
-    HashSet<Integer> keySet = new HashSet<Integer>();
-    for (Iterator<Integer> iit = map.keyIterator(); iit.hasNext(); ) {
-      keySet.add(iit.next());
-    }
-    assertEquals(length, keySet.size());
-    for (int i = 0; i < length; ++i) {
-      assertTrue(keySet.contains(i * 64));
-    }
-    
-    HashSet<Integer> valueSet = new HashSet<Integer>();
-    for (IntIterator iit = map.iterator(); iit.hasNext(); ) {
-      valueSet.add(iit.next());
-    }
-    assertEquals(length, valueSet.size());
-    int[] array = map.toArray();
-    assertEquals(length, array.length);
-    for (int value: array) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    int[] array2 = new int[80];
-    array2 = map.toArray(array2);
-    assertEquals(80, array2.length);
-    for (int value: array2) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    int[] array3 = new int[120];
-    array3 = map.toArray(array3);
-    for (int i = 0 ;i < length; ++i) {
-      assertTrue(valueSet.contains(array3[i]));
-    }
-    assertIntegerMaxValue(array3[length]);
-    
-    for (int i = 0; i < length; ++i) {
-      assertTrue(map.containsValue(i + 100));
-      assertTrue(map.containsKey(i*64));
-    }
-    
-    for (Iterator<Integer> iit = map.keyIterator(); iit.hasNext(); ) {
-      iit.next();
-      iit.remove();
-    }
-    assertTrue(map.isEmpty());
-    assertEquals(0, map.size());
-    
-  }
-
-  // now with random data.. and lots of it
-  @Test
-  public void test4() {
-    ObjectToIntMap<Integer> map = new ObjectToIntMap<Integer>();
-    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
-    
-    // for a repeatable random sequence
-    long seed = random().nextLong();
-    Random random = new Random(seed);
-    
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      map.put(i*128, value);
-    }
-
-    assertEquals(length, map.size());
-    
-    // now repeat
-    random.setSeed(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      assertTrue(map.containsValue(value));
-      assertTrue(map.containsKey(i*128));
-      assertEquals(value, map.remove(i*128));
-    }
-    assertEquals(0, map.size());
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void testEquals() {
-    ObjectToIntMap<Float> map1 = new ObjectToIntMap<Float>(100);
-    ObjectToIntMap<Float> map2 = new ObjectToIntMap<Float>(100);
-    assertEquals("Empty maps should be equal", map1, map2);
-    assertEquals("hashCode() for empty maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    for (int i = 0; i < 100; ++i) {
-      map1.put(i * 1.1f, 100 + i);
-      map2.put(i * 1.1f, 100 + i);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-
-    for (int i = 10; i < 20; i++) {
-      map1.remove(i * 1.1f);
-    }
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    for (int i = 19; i >=10; --i) {
-      map2.remove(i * 1.1f);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    map1.put(-1.1f,-1);
-    map2.put(-1.1f,-2);
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    map2.put(-1.1f,-1);
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/TestLRUHashMap.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/TestLRUHashMap.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/collections/TestLRUHashMap.java	2013-02-20 13:38:17.532711926 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/collections/TestLRUHashMap.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,60 +0,0 @@
-package org.apache.lucene.facet.collections;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.collections.LRUHashMap;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestLRUHashMap extends FacetTestCase {
-  // testLRU() tests that the specified size limit is indeed honored, and
-  // the remaining objects in the map are indeed those that have been most
-  // recently used
-  @Test
-  public void testLRU() throws Exception {
-    LRUHashMap<String, String> lru = new LRUHashMap<String, String>(3);
-    assertEquals(0, lru.size());
-    lru.put("one", "Hello world");
-    assertEquals(1, lru.size());
-    lru.put("two", "Hi man");
-    assertEquals(2, lru.size());
-    lru.put("three", "Bonjour");
-    assertEquals(3, lru.size());
-    lru.put("four", "Shalom");
-    assertEquals(3, lru.size());
-    assertNotNull(lru.get("three"));
-    assertNotNull(lru.get("two"));
-    assertNotNull(lru.get("four"));
-    assertNull(lru.get("one"));
-    lru.put("five", "Yo!");
-    assertEquals(3, lru.size());
-    assertNull(lru.get("three")); // three was last used, so it got removed
-    assertNotNull(lru.get("five"));
-    lru.get("four");
-    lru.put("six", "hi");
-    lru.put("seven", "hey dude");
-    assertEquals(3, lru.size());
-    assertNull(lru.get("one"));
-    assertNull(lru.get("two"));
-    assertNull(lru.get("three"));
-    assertNotNull(lru.get("four"));
-    assertNull(lru.get("five"));
-    assertNotNull(lru.get("six"));
-    assertNotNull(lru.get("seven"));
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/complements/TestFacetsAccumulatorWithComplement.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/complements/TestFacetsAccumulatorWithComplement.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/complements/TestFacetsAccumulatorWithComplement.java	2013-07-29 13:55:02.605707540 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/complements/TestFacetsAccumulatorWithComplement.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,135 +0,0 @@
-package org.apache.lucene.facet.complements;
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.lucene.facet.FacetTestBase;
-import org.apache.lucene.facet.old.OldFacetsAccumulator;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.CountFacetRequest;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiReader;
-import org.apache.lucene.index.ParallelAtomicReader;
-import org.apache.lucene.index.SlowCompositeReaderWrapper;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestFacetsAccumulatorWithComplement extends FacetTestBase {
-  
-  private FacetIndexingParams fip;
-  
-  @Override
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-    fip = getFacetIndexingParams(Integer.MAX_VALUE);
-    initIndex(fip);
-  }
-  
-  @Override
-  @After
-  public void tearDown() throws Exception {
-    closeAll();
-    super.tearDown();
-  }
-  
-  /**
-   * Test that complements does not cause a failure when using a parallel reader
-   */
-  @Test
-  public void testComplementsWithParallerReader() throws Exception {
-    IndexReader origReader = indexReader; 
-    ParallelAtomicReader pr = new ParallelAtomicReader(SlowCompositeReaderWrapper.wrap(origReader));
-    indexReader = pr;
-    try {
-      doTestComplements();
-    } finally {
-      indexReader = origReader;
-    }
-  }
-
-  /**
-   * Test that complements works with MultiReader
-   */
-  @Test
-  public void testComplementsWithMultiReader() throws Exception {
-    final IndexReader origReader = indexReader; 
-    indexReader = new MultiReader(origReader);
-    try {
-      doTestComplements();
-    } finally {
-      indexReader = origReader;
-    }
-  }
-  
-  /**
-   * Test that score is indeed constant when using a constant score
-   */
-  @Test
-  public void testComplements() throws Exception {
-    doTestComplements();
-  }
-  
-  private void doTestComplements() throws Exception {
-    // verify by facet values
-    List<FacetResult> countResWithComplement = findFacets(true);
-    List<FacetResult> countResNoComplement = findFacets(false);
-    
-    assertEquals("Wrong number of facet count results with complement!",1,countResWithComplement.size());
-    assertEquals("Wrong number of facet count results no complement!",1,countResNoComplement.size());
-    
-    FacetResultNode parentResWithComp = countResWithComplement.get(0).getFacetResultNode();
-    FacetResultNode parentResNoComp = countResWithComplement.get(0).getFacetResultNode();
-    
-    assertEquals("Wrong number of top count aggregated categories with complement!",3,parentResWithComp.subResults.size());
-    assertEquals("Wrong number of top count aggregated categories no complement!",3,parentResNoComp.subResults.size());
-  }
-  
-  /** compute facets with certain facet requests and docs */
-  private List<FacetResult> findFacets(boolean withComplement) throws IOException {
-    FacetSearchParams fsp = new FacetSearchParams(fip, new CountFacetRequest(new CategoryPath("root","a"), 10));
-    OldFacetsAccumulator sfa = new OldFacetsAccumulator(fsp, indexReader, taxoReader);
-    sfa.setComplementThreshold(withComplement ? OldFacetsAccumulator.FORCE_COMPLEMENT : OldFacetsAccumulator.DISABLE_COMPLEMENT);
-    FacetsCollector fc = FacetsCollector.create(sfa);
-    searcher.search(new MatchAllDocsQuery(), fc);
-    
-    List<FacetResult> res = fc.getFacetResults();
-    
-    // Results are ready, printing them...
-    int i = 0;
-    if (VERBOSE) {
-      for (FacetResult facetResult : res) {
-        System.out.println("Res "+(i++)+": "+facetResult);
-      }
-    }
-    
-    assertEquals(withComplement, sfa.isUsingComplements());
-    
-    return res;
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/complements/TestTotalFacetCountsCache.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/complements/TestTotalFacetCountsCache.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/complements/TestTotalFacetCountsCache.java	2013-02-20 13:38:17.432711928 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/complements/TestTotalFacetCountsCache.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,503 +0,0 @@
-package org.apache.lucene.facet.complements;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.Collections;
-import java.util.List;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.SlowRAMDirectory;
-import org.apache.lucene.facet.complements.TotalFacetCounts;
-import org.apache.lucene.facet.complements.TotalFacetCountsCache;
-import org.apache.lucene.facet.complements.TotalFacetCounts.CreationType;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.CountFacetRequest;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-import org.junit.Before;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestTotalFacetCountsCache extends FacetTestCase {
-
-  static final TotalFacetCountsCache TFC = TotalFacetCountsCache.getSingleton();
-
-  /**
-   * Thread class to be used in tests for this method. This thread gets a TFC
-   * and records times.
-   */
-  private static class TFCThread extends Thread {
-    private final IndexReader r;
-    private final DirectoryTaxonomyReader tr;
-    private final FacetIndexingParams iParams;
-    
-    TotalFacetCounts tfc;
-
-    public TFCThread(IndexReader r, DirectoryTaxonomyReader tr, FacetIndexingParams iParams) {
-      this.r = r;
-      this.tr = tr;
-      this.iParams = iParams;
-    }
-    @Override
-    public void run() {
-      try {
-        tfc = TFC.getTotalCounts(r, tr, iParams);
-      } catch (Exception e) {
-        throw new RuntimeException(e);
-      }
-    }
-  }
-
-  /** Utility method to add a document and facets to an index/taxonomy. */
-  private static void addFacets(FacetIndexingParams iParams, IndexWriter iw,
-      TaxonomyWriter tw, String... strings) throws IOException {
-    Document doc = new Document();
-    FacetFields facetFields = new FacetFields(tw, iParams);
-    facetFields.addFields(doc, Collections.singletonList(new CategoryPath(strings)));
-    iw.addDocument(doc);
-  }
-
-  /** Clears the cache and sets its size to one. */
-  private static void initCache() {
-    TFC.clear();
-    TFC.setCacheSize(1); // Set to keep one in memory
-  }
-
-  @Override
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-    initCache();
-  }
-
-  /** runs few searches in parallel */
-  public void testGeneralSynchronization() throws Exception {
-    int numIters = atLeast(4);
-    Random random = random();
-    for (int i = 0; i < numIters; i++) {
-      int numThreads = random.nextInt(3) + 2; // 2-4
-      int sleepMillis = random.nextBoolean() ? -1 : random.nextInt(10) + 1 /*1-10*/;
-      int cacheSize = random.nextInt(4); // 0-3
-      doTestGeneralSynchronization(numThreads, sleepMillis, cacheSize);
-    }
-  }
-
-  private static final String[] CATEGORIES = new String[] { "a/b", "c/d", "a/e", "a/d", "c/g", "c/z", "b/a", "1/2", "b/c" };
-
-  private void index(Directory indexDir, Directory taxoDir) throws IOException {
-    IndexWriter indexWriter = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetFields facetFields = new FacetFields(taxoWriter);
-    
-    for (String cat : CATEGORIES) {
-      Document doc = new Document();
-      facetFields.addFields(doc, Collections.singletonList(new CategoryPath(cat, '/')));
-      indexWriter.addDocument(doc);
-    }
-    
-    IOUtils.close(indexWriter, taxoWriter);
-  }
-  
-  private void doTestGeneralSynchronization(int numThreads, int sleepMillis, int cacheSize) throws Exception {
-    TFC.setCacheSize(cacheSize);
-    SlowRAMDirectory slowIndexDir = new SlowRAMDirectory(-1, random());
-    MockDirectoryWrapper indexDir = new MockDirectoryWrapper(random(), slowIndexDir);
-    SlowRAMDirectory slowTaxoDir = new SlowRAMDirectory(-1, random());
-    MockDirectoryWrapper taxoDir = new MockDirectoryWrapper(random(), slowTaxoDir);
-
-    // Index documents without the "slowness"
-    index(indexDir, taxoDir);
-
-    slowIndexDir.setSleepMillis(sleepMillis);
-    slowTaxoDir.setSleepMillis(sleepMillis);
-    
-    // Open the slow readers
-    IndexReader slowIndexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader slowTaxoReader = new DirectoryTaxonomyReader(taxoDir);
-
-    // Class to perform search and return results as threads
-    class Multi extends Thread {
-      private List<FacetResult> results;
-      private FacetIndexingParams iParams;
-      private IndexReader indexReader;
-      private TaxonomyReader taxoReader;
-
-      public Multi(IndexReader indexReader, TaxonomyReader taxoReader, FacetIndexingParams iParams) {
-        this.indexReader = indexReader;
-        this.taxoReader = taxoReader;
-        this.iParams = iParams;
-      }
-
-      public List<FacetResult> getResults() {
-        return results;
-      }
-
-      @Override
-      public void run() {
-        try {
-          FacetSearchParams fsp = new FacetSearchParams(iParams, new CountFacetRequest(new CategoryPath("a"), 10),
-              new CountFacetRequest(new CategoryPath("b"), 10));
-          IndexSearcher searcher = new IndexSearcher(indexReader);
-          FacetsCollector fc = FacetsCollector.create(fsp, indexReader, taxoReader);
-          searcher.search(new MatchAllDocsQuery(), fc);
-          results = fc.getFacetResults();
-        } catch (Exception e) {
-          throw new RuntimeException(e);
-        }
-      }
-    }
-
-    Multi[] multis = new Multi[numThreads];
-    for (int i = 0; i < numThreads; i++) {
-      multis[i] = new Multi(slowIndexReader, slowTaxoReader, FacetIndexingParams.DEFAULT);
-    }
-
-    for (Multi m : multis) {
-      m.start();
-    }
-
-    // Wait for threads and get results
-    String[] expLabelsA = new String[] { "a/d", "a/e", "a/b" };
-    String[] expLabelsB = new String[] { "b/c", "b/a" };
-    for (Multi m : multis) {
-      m.join();
-      List<FacetResult> facetResults = m.getResults();
-      assertEquals("expected two results", 2, facetResults.size());
-      
-      FacetResultNode nodeA = facetResults.get(0).getFacetResultNode();
-      int i = 0;
-      for (FacetResultNode node : nodeA.subResults) {
-        assertEquals("wrong count", 1, (int) node.value);
-        assertEquals(expLabelsA[i++], node.label.toString('/'));
-      }
-      
-      FacetResultNode nodeB = facetResults.get(1).getFacetResultNode();
-      i = 0;
-      for (FacetResultNode node : nodeB.subResults) {
-        assertEquals("wrong count", 1, (int) node.value);
-        assertEquals(expLabelsB[i++], node.label.toString('/'));
-      }
-    }
-    
-    IOUtils.close(slowIndexReader, slowTaxoReader, indexDir, taxoDir);
-  }
-
-  /**
-   * Simple test to make sure the TotalFacetCountsManager updates the
-   * TotalFacetCounts array only when it is supposed to, and whether it
-   * is recomputed or read from disk.
-   */
-  @Test
-  public void testGenerationalConsistency() throws Exception {
-    // Create temporary RAMDirectories
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Create our index/taxonomy writers
-    IndexWriter indexWriter = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetIndexingParams iParams = FacetIndexingParams.DEFAULT;
-
-    // Add a facet to the index
-    addFacets(iParams, indexWriter, taxoWriter, "a", "b");
-
-    // Commit Changes
-    indexWriter.commit();
-    taxoWriter.commit();
-
-    // Open readers
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-
-    // As this is the first time we have invoked the TotalFacetCountsManager, 
-    // we should expect to compute and not read from disk.
-    TotalFacetCounts totalCounts = TFC.getTotalCounts(indexReader, taxoReader, iParams);
-    int prevGen = assertRecomputed(totalCounts, 0, "after first attempt to get it!");
-
-    // Repeating same operation should pull from the cache - not recomputed. 
-    assertTrue("Should be obtained from cache at 2nd attempt",totalCounts == 
-      TFC.getTotalCounts(indexReader, taxoReader, iParams));
-
-    // Repeat the same operation as above. but clear first - now should recompute again
-    initCache();
-    totalCounts = TFC.getTotalCounts(indexReader, taxoReader, iParams);
-    prevGen = assertRecomputed(totalCounts, prevGen, "after cache clear, 3rd attempt to get it!");
-    
-    //store to file
-    File outputFile = _TestUtil.createTempFile("test", "tmp", TEMP_DIR);
-    initCache();
-    TFC.store(outputFile, indexReader, taxoReader, iParams);
-    totalCounts = TFC.getTotalCounts(indexReader, taxoReader, iParams);
-    prevGen = assertRecomputed(totalCounts, prevGen, "after cache clear, 4th attempt to get it!");
-
-    //clear and load
-    initCache();
-    TFC.load(outputFile, indexReader, taxoReader, iParams);
-    totalCounts = TFC.getTotalCounts(indexReader, taxoReader, iParams);
-    prevGen = assertReadFromDisc(totalCounts, prevGen, "after 5th attempt to get it!");
-
-    // Add a new facet to the index, commit and refresh readers
-    addFacets(iParams, indexWriter, taxoWriter, "c", "d");
-    IOUtils.close(indexWriter, taxoWriter);
-
-    TaxonomyReader newTaxoReader = TaxonomyReader.openIfChanged(taxoReader);
-    assertNotNull(newTaxoReader);
-    assertTrue("should have received more cagtegories in updated taxonomy", newTaxoReader.getSize() > taxoReader.getSize());
-    taxoReader.close();
-    taxoReader = newTaxoReader;
-    
-    DirectoryReader r2 = DirectoryReader.openIfChanged(indexReader);
-    assertNotNull(r2);
-    indexReader.close();
-    indexReader = r2;
-
-    // now use the new reader - should recompute
-    totalCounts = TFC.getTotalCounts(indexReader, taxoReader, iParams);
-    prevGen = assertRecomputed(totalCounts, prevGen, "after updating the index - 7th attempt!");
-
-    // try again - should not recompute
-    assertTrue("Should be obtained from cache at 8th attempt",totalCounts == 
-      TFC.getTotalCounts(indexReader, taxoReader, iParams));
-    
-    IOUtils.close(indexReader, taxoReader);
-    outputFile.delete();
-    IOUtils.close(indexDir, taxoDir);
-  }
-
-  private int assertReadFromDisc(TotalFacetCounts totalCounts, int prevGen, String errMsg) {
-    assertEquals("should read from disk "+errMsg, CreationType.Loaded, totalCounts.createType4test);
-    int gen4test = totalCounts.gen4test;
-    assertTrue("should read from disk "+errMsg, gen4test > prevGen);
-    return gen4test;
-  }
-  
-  private int assertRecomputed(TotalFacetCounts totalCounts, int prevGen, String errMsg) {
-    assertEquals("should recompute "+errMsg, CreationType.Computed, totalCounts.createType4test);
-    int gen4test = totalCounts.gen4test;
-    assertTrue("should recompute "+errMsg, gen4test > prevGen);
-    return gen4test;
-  }
-
-  /**
-   * This test is to address a bug in a previous version.  If a TFC cache is
-   * written to disk, and then the taxonomy grows (but the index does not change),
-   * and then the TFC cache is re-read from disk, there will be an exception
-   * thrown, as the integers are read off of the disk according to taxonomy
-   * size, which has changed.
-   */
-  @Test
-  public void testGrowingTaxonomy() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    // Create our index/taxonomy writers
-    IndexWriter indexWriter = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetIndexingParams iParams = new FacetIndexingParams() {
-      @Override
-      public int getPartitionSize() {
-        return 2;
-      }
-    };
-    // Add a facet to the index
-    addFacets(iParams, indexWriter, taxoWriter, "a", "b");
-    // Commit Changes
-    indexWriter.commit();
-    taxoWriter.commit();
-
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-
-    // Create TFC and write cache to disk
-    File outputFile = _TestUtil.createTempFile("test", "tmp", TEMP_DIR);
-    TFC.store(outputFile, indexReader, taxoReader, iParams);
-    
-    // Make the taxonomy grow without touching the index
-    for (int i = 0; i < 10; i++) {
-      taxoWriter.addCategory(new CategoryPath("foo", Integer.toString(i)));
-    }
-    taxoWriter.commit();
-    TaxonomyReader newTaxoReader = TaxonomyReader.openIfChanged(taxoReader);
-    assertNotNull(newTaxoReader);
-    taxoReader.close();
-    taxoReader = newTaxoReader;
-
-    initCache();
-
-    // With the bug, this next call should result in an exception
-    TFC.load(outputFile, indexReader, taxoReader, iParams);
-    TotalFacetCounts totalCounts = TFC.getTotalCounts(indexReader, taxoReader, iParams);
-    assertReadFromDisc(totalCounts, 0, "after reading from disk.");
-    
-    outputFile.delete();
-    IOUtils.close(indexWriter, taxoWriter, indexReader, taxoReader);
-    IOUtils.close(indexDir, taxoDir);
-  }
-
-  /**
-   * Test that a new TFC is only calculated and placed in memory (by two
-   * threads who want it at the same time) only once.
-   */
-  @Test
-  public void testMemoryCacheSynchronization() throws Exception {
-    SlowRAMDirectory indexDir = new SlowRAMDirectory(-1, null);
-    SlowRAMDirectory taxoDir = new SlowRAMDirectory(-1, null);
-
-    // Write index using 'normal' directories
-    IndexWriter w = new IndexWriter(indexDir, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
-    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
-    FacetIndexingParams iParams = FacetIndexingParams.DEFAULT;
-    // Add documents and facets
-    for (int i = 0; i < 1000; i++) {
-      addFacets(iParams, w, tw, "facet", Integer.toString(i));
-    }
-    w.close();
-    tw.close();
-
-    indexDir.setSleepMillis(1);
-    taxoDir.setSleepMillis(1);
-
-    IndexReader r = DirectoryReader.open(indexDir);
-    DirectoryTaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
-
-    // Create and start threads. Thread1 should lock the cache and calculate
-    // the TFC array. The second thread should block until the first is
-    // done, then successfully retrieve from the cache without recalculating
-    // or reading from disk.
-    TFCThread tfcCalc1 = new TFCThread(r, tr, iParams);
-    TFCThread tfcCalc2 = new TFCThread(r, tr, iParams);
-    tfcCalc1.start();
-    // Give thread 1 a head start to ensure correct sequencing for testing
-    Thread.sleep(5);
-    tfcCalc2.start();
-
-    tfcCalc1.join();
-    tfcCalc2.join();
-
-    // Since this test ends up with references to the same TFC object, we
-    // can only test the times to make sure that they are the same.
-    assertRecomputed(tfcCalc1.tfc, 0, "thread 1 should recompute");
-    assertRecomputed(tfcCalc2.tfc, 0, "thread 2 should recompute");
-    assertTrue("Both results should be the same (as their inputs are the same objects)",
-        tfcCalc1.tfc == tfcCalc2.tfc);
-
-    r.close();
-    tr.close();
-  }
-
-  /**
-   * Simple test to make sure the TotalFacetCountsManager updates the
-   * TotalFacetCounts array only when it is supposed to, and whether it
-   * is recomputed or read from disk, but this time with TWO different
-   * TotalFacetCounts
-   */
-  @Test
-  public void testMultipleIndices() throws IOException {
-    Directory indexDir1 = newDirectory(), indexDir2 = newDirectory();
-    Directory taxoDir1 = newDirectory(), taxoDir2 = newDirectory();
-    
-    // Create our index/taxonomy writers
-    IndexWriter indexWriter1 = new IndexWriter(indexDir1, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
-    IndexWriter indexWriter2 = new IndexWriter(indexDir2, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
-    TaxonomyWriter taxoWriter1 = new DirectoryTaxonomyWriter(taxoDir1);
-    TaxonomyWriter taxoWriter2 = new DirectoryTaxonomyWriter(taxoDir2);
-    FacetIndexingParams iParams = FacetIndexingParams.DEFAULT;
-
-    // Add a facet to the index
-    addFacets(iParams, indexWriter1, taxoWriter1, "a", "b");
-    addFacets(iParams, indexWriter1, taxoWriter1, "d", "e");
-    // Commit Changes
-    indexWriter1.commit();
-    indexWriter2.commit();
-    taxoWriter1.commit();
-    taxoWriter2.commit();
-
-    // Open two readers
-    DirectoryReader indexReader1 = DirectoryReader.open(indexDir1);
-    DirectoryReader indexReader2 = DirectoryReader.open(indexDir2);
-    TaxonomyReader taxoReader1 = new DirectoryTaxonomyReader(taxoDir1);
-    TaxonomyReader taxoReader2 = new DirectoryTaxonomyReader(taxoDir2);
-
-    // As this is the first time we have invoked the TotalFacetCountsManager, we
-    // should expect to compute.
-    TotalFacetCounts totalCounts0 = TFC.getTotalCounts(indexReader1, taxoReader1, iParams);
-    int prevGen = -1;
-    prevGen = assertRecomputed(totalCounts0, prevGen, "after attempt 1");
-    assertTrue("attempt 1b for same input [0] shout find it in cache",
-        totalCounts0 == TFC.getTotalCounts(indexReader1, taxoReader1, iParams));
-    
-    // 2nd Reader - As this is the first time we have invoked the
-    // TotalFacetCountsManager, we should expect a state of NEW to be returned.
-    TotalFacetCounts totalCounts1 = TFC.getTotalCounts(indexReader2, taxoReader2, iParams);
-    prevGen = assertRecomputed(totalCounts1, prevGen, "after attempt 2");
-    assertTrue("attempt 2b for same input [1] shout find it in cache",
-        totalCounts1 == TFC.getTotalCounts(indexReader2, taxoReader2, iParams));
-
-    // Right now cache size is one, so first TFC is gone and should be recomputed  
-    totalCounts0 = TFC.getTotalCounts(indexReader1, taxoReader1, iParams);
-    prevGen = assertRecomputed(totalCounts0, prevGen, "after attempt 3");
-    
-    // Similarly will recompute the second result  
-    totalCounts1 = TFC.getTotalCounts(indexReader2, taxoReader2, iParams);
-    prevGen = assertRecomputed(totalCounts1, prevGen, "after attempt 4");
-
-    // Now we set the cache size to two, meaning both should exist in the
-    // cache simultaneously
-    TFC.setCacheSize(2);
-
-    // Re-compute totalCounts0 (was evicted from the cache when the cache was smaller)
-    totalCounts0 = TFC.getTotalCounts(indexReader1, taxoReader1, iParams);
-    prevGen = assertRecomputed(totalCounts0, prevGen, "after attempt 5");
-
-    // now both are in the larger cache and should not be recomputed 
-    totalCounts1 = TFC.getTotalCounts(indexReader2, taxoReader2, iParams);
-    assertTrue("with cache of size 2 res no. 0 should come from cache",
-        totalCounts0 == TFC.getTotalCounts(indexReader1, taxoReader1, iParams));
-    assertTrue("with cache of size 2 res no. 1 should come from cache",
-        totalCounts1 == TFC.getTotalCounts(indexReader2, taxoReader2, iParams));
-    
-    IOUtils.close(indexWriter1, indexWriter2, taxoWriter1, taxoWriter2);
-    IOUtils.close(indexReader1, indexReader2, taxoReader1, taxoReader2);
-    IOUtils.close(indexDir1, indexDir2, taxoDir1, taxoDir2);
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/complements/TestTotalFacetCounts.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/complements/TestTotalFacetCounts.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/complements/TestTotalFacetCounts.java	2013-02-20 13:38:17.428711928 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/complements/TestTotalFacetCounts.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,130 +0,0 @@
-package org.apache.lucene.facet.complements;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collections;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.complements.TotalFacetCounts;
-import org.apache.lucene.facet.complements.TotalFacetCountsCache;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestTotalFacetCounts extends FacetTestCase {
-
-  private static void initCache() {
-    TotalFacetCountsCache.getSingleton().clear();
-    TotalFacetCountsCache.getSingleton().setCacheSize(1); // Set to keep one in mem
-  }
-
-  @Test
-  public void testWriteRead() throws IOException {
-    doTestWriteRead(14);
-    doTestWriteRead(100);
-    doTestWriteRead(7);
-    doTestWriteRead(3);
-    doTestWriteRead(1);
-  }
-
-  private void doTestWriteRead(final int partitionSize) throws IOException {
-    initCache();
-
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriter indexWriter = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    
-    FacetIndexingParams iParams = new FacetIndexingParams() {
-      @Override
-      public int getPartitionSize() {
-        return partitionSize;
-      }
-      
-      @Override
-      public CategoryListParams getCategoryListParams(CategoryPath category) {
-        return new CategoryListParams() {
-          @Override
-          public OrdinalPolicy getOrdinalPolicy(String dimension) {
-            return OrdinalPolicy.ALL_PARENTS;
-          }
-        };
-      }
-    };
-    // The counts that the TotalFacetCountsArray should have after adding
-    // the below facets to the index.
-    int[] expectedCounts = new int[] { 0, 3, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1 };
-    String[] categories = new String[] { "a/b", "c/d", "a/e", "a/d", "c/g", "c/z", "b/a", "1/2", "b/c" };
-
-    FacetFields facetFields = new FacetFields(taxoWriter, iParams);
-    for (String cat : categories) {
-      Document doc = new Document();
-      facetFields.addFields(doc, Collections.singletonList(new CategoryPath(cat, '/')));
-      indexWriter.addDocument(doc);
-    }
-
-    // Commit Changes
-    IOUtils.close(indexWriter, taxoWriter);
-
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    
-    int[] intArray = new int[iParams.getPartitionSize()];
-
-    TotalFacetCountsCache tfcc = TotalFacetCountsCache.getSingleton();
-    File tmpFile = _TestUtil.createTempFile("test", "tmp", TEMP_DIR);
-    tfcc.store(tmpFile, indexReader, taxoReader, iParams);
-    tfcc.clear(); // not really required because TFCC overrides on load(), but in the test we need not rely on this.
-    tfcc.load(tmpFile, indexReader, taxoReader, iParams);
-    
-    // now retrieve the one just loaded
-    TotalFacetCounts totalCounts = tfcc.getTotalCounts(indexReader, taxoReader, iParams);
-
-    int partition = 0;
-    for (int i = 0; i < expectedCounts.length; i += partitionSize) {
-      totalCounts.fillTotalCountsForPartition(intArray, partition);
-      int[] partitionExpectedCounts = new int[partitionSize];
-      int nToCopy = Math.min(partitionSize,expectedCounts.length-i);
-      System.arraycopy(expectedCounts, i, partitionExpectedCounts, 0, nToCopy);
-      assertTrue("Wrong counts! for partition "+partition+
-          "\nExpected:\n" + Arrays.toString(partitionExpectedCounts)+
-          "\nActual:\n" + Arrays.toString(intArray),
-          Arrays.equals(partitionExpectedCounts, intArray));
-      ++partition;
-    }
-    IOUtils.close(indexReader, taxoReader);
-    IOUtils.close(indexDir, taxoDir);
-    tmpFile.delete();
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/encoding/EncodingSpeed.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/encoding/EncodingSpeed.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/encoding/EncodingSpeed.java	2013-02-20 13:38:17.436711928 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/encoding/EncodingSpeed.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,649 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import java.io.IOException;
-import java.text.NumberFormat;
-import java.util.Arrays;
-import java.util.Locale;
-
-import org.apache.lucene.facet.encoding.DGapIntEncoder;
-import org.apache.lucene.facet.encoding.DGapVInt8IntEncoder;
-import org.apache.lucene.facet.encoding.EightFlagsIntEncoder;
-import org.apache.lucene.facet.encoding.FourFlagsIntEncoder;
-import org.apache.lucene.facet.encoding.IntDecoder;
-import org.apache.lucene.facet.encoding.IntEncoder;
-import org.apache.lucene.facet.encoding.NOnesIntEncoder;
-import org.apache.lucene.facet.encoding.SortingIntEncoder;
-import org.apache.lucene.facet.encoding.UniqueValuesIntEncoder;
-import org.apache.lucene.facet.encoding.VInt8IntEncoder;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class EncodingSpeed {
-
-  private static int[] data3630 = null;
-  private static int[] data9910 = null;
-  private static int[] data501871 = null;
-  private static int[] data10k = null;
-  private static String resultsFormat = "%-60s %10s %20d %26s %20d %26s";
-  private static String headerFormat = "%-60s %10s %20s %26s %20s %26s";
-  private static int integers = 100000000;
-
-  private static NumberFormat nf;
-
-  public static void main(String[] args) throws IOException {
-    testFacetIDs(data3630, 3630);
-    testFacetIDs(data9910, 9910);
-    testFacetIDs(data10k, 10000);
-    testFacetIDs(data501871, 501871);
-  }
-
-  private static IntsRef newIntsRef(int[] data) {
-    IntsRef res = new IntsRef(data.length);
-    System.arraycopy(data, 0, res.ints, 0, data.length);
-    res.length = data.length;
-    return res;
-  }
-  
-  private static void testFacetIDs(int[] facetIDs, int docID) throws IOException {
-    int loopFactor = integers / facetIDs.length;
-    System.out
-        .println("\nEstimating ~"
-            + integers
-            + " Integers compression time by\nEncoding/decoding facets' ID payload of docID = "
-            + docID + " (unsorted, length of: " + facetIDs.length
-            + ") " + loopFactor + " times.");
-
-    System.out.println();
-    String header = String.format(Locale.ROOT, headerFormat, "Encoder", "Bits/Int",
-        "Encode Time", "Encode Time", "Decode Time", "Decode Time");
-
-    System.out.println(header);
-    String header2 = String.format(Locale.ROOT, headerFormat, "", "", "[milliseconds]",
-        "[microsecond / int]", "[milliseconds]", "[microsecond / int]");
-
-    System.out.println(header2);
-
-    char[] separator = header.toCharArray();
-    Arrays.fill(separator, '-');
-    System.out.println(separator);
-
-    encoderTest(new VInt8IntEncoder(), facetIDs, loopFactor);
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new VInt8IntEncoder())), facetIDs, loopFactor);
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new VInt8IntEncoder()))), facetIDs, loopFactor);
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapVInt8IntEncoder())), facetIDs, loopFactor);
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new EightFlagsIntEncoder()))), facetIDs, loopFactor);
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new FourFlagsIntEncoder()))), facetIDs, loopFactor);
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new NOnesIntEncoder(3)))), facetIDs, loopFactor);
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new NOnesIntEncoder(4)))), facetIDs, loopFactor);
-
-    System.out.println();
-  }
-
-  private static void encoderTest(IntEncoder encoder, int[] values, int loopFactor) throws IOException {
-
-    BytesRef bytes = new BytesRef(values.length); // at least one byte per value
-
-    // -- Looping 100 times as a warm up --------------------------
-    for (int i = 100; i != 0; --i) {
-      IntsRef data = newIntsRef(values);
-      encoder.encode(data, bytes);
-    }
-    // -----------------------------------------------------------
-
-    long encodeTime = 0;
-    for (int factor = loopFactor; factor > 0; --factor) {
-      IntsRef data = newIntsRef(values);
-      long start = System.currentTimeMillis();
-      encoder.encode(data, bytes);
-      encodeTime += System.currentTimeMillis() - start;
-    }
-
-    IntsRef decoded = new IntsRef(values.length);
-    int encodedSize = bytes.length;
-    IntDecoder decoder = encoder.createMatchingDecoder();
-    
-    // -- Looping 100 times as a warm up --------------------------
-    for (int i = 100; i != 0; --i) {
-      decoder.decode(bytes, decoded);
-    }
-    // -----------------------------------------------------------
-
-    long decodeTime = 0;
-    for (int i = loopFactor; i > 0; --i) {
-      long start = System.currentTimeMillis();
-      decoder.decode(bytes, decoded);
-      decodeTime += System.currentTimeMillis() - start;
-    }
-    
-    if (decoded.length != values.length) {
-      throw new RuntimeException("wrong num values. expected=" + values.length + " actual=" + decoded.length + 
-          " decoder=" + decoder);
-    }
-
-    System.out.println(String.format(Locale.ROOT, resultsFormat, encoder, 
-        nf.format(encodedSize * 8.0 / values.length), 
-        encodeTime, 
-        nf.format(encodeTime * 1000000.0 / (loopFactor * values.length)), 
-        decodeTime, 
-        nf.format(decodeTime * 1000000.0 / (loopFactor * values.length))));
-  }
-
-  static {
-    nf = NumberFormat.getInstance(Locale.ROOT);
-    nf.setMaximumFractionDigits(4);
-    nf.setMinimumFractionDigits(4);
-
-    data9910 = new int[] { 2, 4, 149085, 11, 12292, 69060, 69061, 149309,
-        99090, 568, 5395, 149310, 3911, 149311, 149312, 148752, 1408,
-        1410, 1411, 1412, 4807, 1413, 1414, 1417, 1415, 1416, 1418,
-        1420, 470, 4808, 1422, 1423, 1424, 4809, 4810, 1427, 1429,
-        1430, 4811, 1432, 1433, 3752, 1435, 3753, 1437, 1439, 1440,
-        4812, 1442, 1443, 4813, 1445, 1446, 1447, 4814, 4815, 1450,
-        4816, 353, 1452, 89004, 1624, 1625, 2052, 1626, 1627, 63991,
-        725, 726, 727, 728, 35543, 729, 730, 731, 1633, 733, 734, 735,
-        37954, 737, 738, 76315, 23068, 76316, 1634, 740, 741, 742, 744,
-        745, 76317, 15645, 748, 17488, 2904, 89005, 752, 753, 89006,
-        754, 755, 756, 757, 41, 261, 758, 89007, 760, 762, 763, 89008,
-        764, 765, 766, 85930, 165, 768, 149313, 33593, 149314, 149315,
-        81589, 39456, 15467, 1296, 149316, 39457, 2235, 144, 2236,
-        2309, 3050, 2237, 2311, 89692, 2240, 2241, 2243, 2244, 2245,
-        2246, 2314, 12856, 2248, 2250, 2251, 2253, 2254, 12857, 7677,
-        12858, 39149, 2257, 23147, 3303, 2258, 7422, 2322, 2262, 2317,
-        2263, 7423, 24264, 2232, 89693, 12862, 89694, 12863, 12864,
-        23201, 2329, 33019, 2255, 12865, 3517, 2492, 2277, 2280, 2267,
-        2260, 25368, 12866, 2281, 2282, 2283, 12867, 2284, 9055, 2287,
-        125133, 2337, 2286, 2288, 2338, 125134, 2290, 125135, 12869,
-        965, 966, 1298, 17945, 1300, 970, 971, 972, 973, 974, 296,
-        17946, 1303, 1391, 902, 1304, 1395, 1308, 1309, 1310, 1312,
-        967, 9414, 1315, 1317, 1318, 9415, 1321, 23592, 1322, 22433,
-        1323, 1324, 1326, 109241, 31225, 1330, 1331, 2540, 27196, 1332,
-        1334, 1335, 11999, 414, 340, 3651, 44040, 31995, 1344, 1343,
-        4618, 116770, 116771, 1474, 1349, 42122, 14199, 149317, 451,
-        149318, 29, 14200, 14198, 14201, 1979, 1980, 1981, 3132, 3147,
-        34090, 1987, 12770, 1329, 80818, 80819, 1988, 23522, 1986,
-        15880, 1985, 32975, 1992, 1993, 7165, 3141, 3143, 86346, 1982,
-        1984, 3145, 86347, 78064, 23456, 29578, 3136, 17752, 4710,
-        4711, 4712, 149319, 424, 4713, 95735, 4715, 149320, 4717, 4718,
-        149321, 192, 149322, 108126, 29976, 5404, 38059, 5406, 2030,
-        289, 1804, 1557, 1558, 94080, 29651, 94317, 1561, 1562, 1563,
-        1565, 24632, 1927, 1928, 1566, 1570, 1571, 1572, 1573, 1574,
-        1575, 94318, 1576, 2674, 9351, 94319, 94320, 2677, 2678, 29654,
-        2946, 2945, 2682, 2683, 2947, 3102, 3402, 3104, 4780, 3106,
-        3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3116, 3117,
-        3118, 19610, 44805, 3119, 3407, 3121, 3122, 3124, 3126, 3127,
-        41745, 41746, 3130, 459, 460, 461, 462, 463, 464, 466, 467,
-        40306, 468, 471, 472, 40307, 4467, 475, 476, 477, 478, 479,
-        40308, 481, 482, 20129, 483, 484, 485, 486, 4473, 488, 489,
-        458, 491, 40309, 494, 495, 496, 497, 499, 500, 501, 502, 355,
-        356, 1549, 358, 359, 360, 37971, 362, 2579, 2581, 24578, 2583,
-        24579, 2586, 2587, 2588, 2590, 2591, 24580, 24581, 3666, 24582,
-        2594, 24583, 2596, 2597, 24584, 2599, 18013, 24585, 2601,
-        49361, 280, 3969, 11651, 11652, 3926, 5103, 11653, 11654,
-        11655, 6896, 417, 168, 149323, 11268, 11657, 38089, 59517,
-        149324, 38092, 149325, 5110, 38094, 59520, 38096, 38097, 28916,
-        59703, 4992, 149326, 32383, 2478, 3985, 2479, 2480, 2481, 2482,
-        2483, 2484, 2485, 2486, 24146, 22184, 2488, 2489, 2490, 2494,
-        2493, 18043, 2495, 2542, 2497, 5062, 2499, 2501, 24147, 24148,
-        2504, 2505, 2506, 2507, 2508, 394, 2660, 2509, 2511, 24149,
-        2512, 2513, 2514, 3988, 4410, 3989, 2518, 2522, 2521, 24150,
-        12082, 2524, 3990, 24151, 387, 24152, 2529, 2530, 2528, 3991,
-        24153, 2534, 24154, 2536, 24155, 2538, 22510, 6332, 3554, 5309,
-        7700, 6333, 6334, 6335, 6336, 6337, 5693, 117020, 6339, 149327,
-        149328, 149329, 6340, 6343, 117022, 4324, 283, 284, 285, 286,
-        2688, 287, 2689, 288, 8880, 290, 291, 2690, 292, 295, 294,
-        24543, 13899, 297, 298, 299, 300, 303, 301, 59178, 302, 8881,
-        34403, 13900, 17811, 305, 307, 306, 308, 2727, 368, 364,
-        110416, 1587, 366, 367, 2692, 26624, 7233, 9082, 35684, 7250,
-        13902, 304, 13903, 991, 110417, 273, 274, 275, 276, 277, 278,
-        41095, 281, 282, 4419, 2768, 229, 230, 231, 232, 233, 234, 235,
-        236, 237, 1065, 239, 2745, 2746, 240, 9250, 241, 242, 244, 245,
-        9251, 246, 247, 248, 249, 250, 251, 253, 254, 255, 9252, 257,
-        258, 259, 9253, 9254, 2751, 265, 266, 267, 268, 9255, 9256,
-        270, 271, 9257, 238, 1024, 829, 1025, 1026, 1028, 1029, 1030,
-        9258, 1032, 1033, 1034, 1027, 1035, 1036, 9259, 1037, 1038,
-        1039, 4594, 4429, 1041, 1042, 1043, 70332, 1045, 1046, 1047,
-        1048, 21128, 1050, 122787, 72433, 1052, 2762, 1054, 1055, 1056,
-        9548, 1057, 71311, 1058, 1059, 1060, 61413, 2765, 4436, 1064,
-        1066, 11610, 3485, 22357, 104580, 149330, 149331, 15471, 5679,
-        5680, 687, 5683, 5684, 953, 8849, 102120, 149332, 5688, 5689,
-        149333, 6920, 60202, 33855, 33856, 33857, 19163, 33858, 3491,
-        149334, 914, 2202, 916, 917, 919, 920, 921, 922, 3568, 924,
-        925, 926, 927, 928, 929, 8752, 931, 932, 933, 934, 3570, 1876,
-        9138, 1877, 1878, 2210, 1880, 1881, 3571, 1883, 1884, 2212,
-        1886, 2214, 1888, 1889, 1890, 8753, 1891, 1892, 1893, 1894,
-        1895, 1896, 1898, 2217, 3572, 1901, 1902, 688, 2219, 107, 1904,
-        1905, 3573, 1907, 3323, 1909, 1910, 1911, 8754, 1912, 55911,
-        1913, 1914, 3574, 1741, 3575, 1916, 2226, 3576, 1919, 2227,
-        1920, 3577, 3578, 2229, 1923, 85396, 174, 175, 114875, 178,
-        180, 181, 182, 1477, 185, 186, 172, 187, 188, 85397, 85398,
-        190, 191, 891, 893, 19778, 18068, 895, 897, 896, 25985, 894,
-        900, 361, 1206, 193, 194, 195, 196, 197, 198, 199, 200, 55009,
-        201, 33266, 29064, 204, 205, 40129, 206, 207, 208, 2842, 209,
-        210, 211, 212, 149335, 870, 871, 18005, 872, 18006, 874, 875,
-        876, 1479, 1480, 1481, 879, 881, 57212, 2779, 57213, 886, 887,
-        57214, 57215, 889, 890, 806, 69305, 808, 809, 86327, 812, 813,
-        814, 815, 26724, 816, 69307, 43484, 818, 819, 63904, 820, 821,
-        822, 86328, 13498, 824, 825, 12218, 149336, 49042, 4464, 4466,
-        35536, 73245, 73246, 474, 73247, 480, 46247, 29624, 21086,
-        73248, 490, 493, 73249, 73250, 401, 403, 405, 2860, 15483,
-        74826, 408, 409, 74827, 410, 411, 413, 74828, 415, 2863, 68707,
-        33284, 2865, 2866, 2867, 2868, 2869, 2870, 17976, 3032, 38498,
-        7350, 2876, 2874, 24506, 918, 923, 64562, 64563, 32648, 930,
-        1875, 32649, 1879, 32650, 1882, 1887, 32651, 64564, 32652,
-        1897, 32653, 18170, 1900, 32654, 1906, 1915, 64565, 1921, 1922,
-        90662, 2234, 37449, 8886, 37450, 7418, 37451, 37452, 37453,
-        37454, 1609, 1610, 1611, 1612, 113456, 1212, 1616, 1617,
-        113457, 1615, 1619, 113458, 1620, 8747, 113459, 8748, 42233,
-        78065, 42235, 2149, 42236, 78066, 42237, 42238, 4335, 42239,
-        78067, 42241, 78068, 42243, 78069, 42244, 78070, 54587, 12993,
-        2040, 1130, 1131, 51172, 1133, 1134, 1135, 1136, 1137, 1138,
-        1139, 1140, 1141, 149337, 1115, 5178, 149338, 452, 7784, 21522,
-        1361, 103718, 149339, 15990, 79432, 149340, 4232, 149341,
-        15998, 53917, 15996, 53918, 149342, 149343, 97544, 53920,
-        97546, 841, 1954, 842, 41926, 844, 2589, 845, 846, 27370, 848,
-        849, 41927, 25165, 852, 1956, 854, 856, 1957, 855, 1959, 35170,
-        23055, 75673, 116783, 857, 116784, 851, 116785, 858, 859, 860,
-        861, 57422, 1964, 864, 866, 867, 1965, 1966, 1968, 1969, 2989,
-        116786, 1972, 1973, 116787, 1975, 1976, 1977, 2580, 39540,
-        2585, 39541, 21755, 39542, 2592, 34859, 2593, 39543, 38540,
-        2595, 39544, 149344, 35433, 81849, 35434, 40257, 873, 877,
-        2778, 32040, 882, 883, 884, 885, 888, 3358, 1559, 1560, 1438,
-        25387, 1569, 38135, 66925, 2673, 3095, 2679, 59053, 25443,
-        34369, 1983, 17749, 9343, 1989, 13565, 31525, 61690, 18165,
-        17751, 78234, 26506, 9348, 20307, 18154, 3133, 2572, 3134,
-        12131, 19770, 48724, 25759, 13549, 65465, 19936, 13545, 25645,
-        4786, 15756, 19547, 1581, 92226, 1362, 21524, 13059, 23717,
-        149345, 20198, 27123, 149346, 149347, 26030, 27126, 27652,
-        10538, 1667, 40282, 14134, 40284, 16368, 149348, 40287, 8870,
-        40288, 149349, 40289, 149350, 149351, 40295, 10424, 7012,
-        13178, 45608, 10423, 13181, 4201, 672, 13182, 10174, 10607,
-        13183, 580, 149352, 149353, 96298, 53691, 3721, 66048, 21584,
-        149354, 48206, 48207, 149355, 1405, 1406, 1407, 11162, 577,
-        149356, 6941, 6942, 16583, 1284, 10511, 16584, 16585, 422, 423,
-        1249, 1244, 1245, 1247, 2544, 1248, 1250, 2545, 1252, 2547,
-        1253, 2549, 1259, 1257, 1258, 1260, 1261, 2551, 1262, 1263,
-        1264, 1265, 2553, 1266, 17795, 2554, 17796, 1270, 1271, 1273,
-        17797, 2556, 1275, 1276, 2557, 1277, 1278, 1279, 1280, 1282,
-        68, 69, 5080, 5256, 6869, 10148, 6960, 10150, 149357, 10152,
-        14413, 149358, 14414, 56037, 651, 56038, 131797, 555, 14415,
-        14416, 149359, 149360, 56042, 14418, 149361, 149, 56043, 97512,
-        34512, 797, 7396, 9395, 9396, 9397, 63417, 805, 23984, 13665,
-        10452, 55147, 5656, 53, 4348, 4349, 4350, 148488, 13669, 6527,
-        149362, 11374, 11376, 11377, 8092, 11378, 11380, 152, 5013,
-        8093, 561, 11381, 5623, 4176, 26840, 3564, 3565, 3708, 3567,
-        18783, 18784, 4039, 10540, 18786, 30100, 30101, 1528, 149363,
-        19561, 19562, 19563, 19564, 1110, 134146, 10600, 149364, 10602,
-        149365, 149366, 10603, 10604, 4981, 57075, 37508, 149367,
-        34589, 1209, 149368, 19592, 19593, 7620, 9674, 3481, 10240,
-        81835, 8001, 33872, 8907, 55155, 1585, 31731, 49694, 25760,
-        31733, 903, 904, 2539, 49695, 1194, 1195, 1196, 31734, 1197,
-        1198, 1199, 1593, 899, 1200, 1201, 9276, 1202, 40181, 40482,
-        55718, 80833, 24596, 3669, 15699, 55720, 55721, 40481, 3672,
-        39826, 80363, 2602, 2603, 2604, 62126, 2605, 2606, 2607, 8714,
-        2608, 2609, 2610, 2612, 149369, 2894, 15241, 15242, 15262,
-        5384, 20290, 20291, 7792, 20295, 64413, 39236, 18011, 71494,
-        898, 51015, 19782, 105107, 149370, 7634, 149371, 149372,
-        115458, 22821, 19894, 2213, 66926 };
-
-    data3630 = new int[] { 2, 4, 86133, 11, 16505, 86134, 86135, 86136,
-        1290, 86137, 86138, 32473, 19346, 32474, 4922, 32475, 86139,
-        16914, 86140, 86141, 86142, 86143, 32478, 86144, 86145, 32480,
-        4884, 4887, 32481, 86146, 16572, 86147, 16295, 165, 86148,
-        3183, 21920, 21921, 21922, 555, 4006, 32484, 21925, 21926,
-        13775, 86149, 13777, 85833, 85834, 13779, 13773, 13780, 75266,
-        17674, 13784, 13785, 13786, 13787, 13788, 6258, 86150, 13790,
-        75267, 13793, 13794, 13795, 312, 4914, 4915, 6222, 86151, 4845,
-        4883, 4918, 4894, 4919, 86152, 4921, 6223, 6224, 6225, 6226,
-        67909, 6229, 18170, 6230, 5198, 25625, 6231, 6232, 6233, 1808,
-        6234, 6235, 6236, 41376, 6238, 6239, 67911, 6240, 86153, 6243,
-        6244, 83549, 6246, 6247, 6248, 6249, 782, 444, 6251, 6250,
-        19863, 28963, 310, 2234, 144, 2236, 2309, 69437, 2311, 2325,
-        2241, 69438, 69439, 2244, 2245, 2246, 23504, 2314, 69440,
-        36603, 2250, 2268, 2271, 2251, 2254, 2255, 2257, 2240, 36604,
-        84726, 36605, 84727, 2262, 2263, 18431, 38853, 2317, 2149,
-        2326, 2327, 2329, 3980, 2275, 2277, 2258, 84728, 2260, 84729,
-        84730, 13766, 36607, 2282, 2283, 84731, 2284, 2286, 2287, 2337,
-        7424, 2288, 2338, 3522, 2290, 84733, 32902, 371, 37708, 2096,
-        3065, 3066, 375, 377, 374, 378, 2100, 86154, 381, 382, 58795,
-        379, 383, 384, 385, 4449, 387, 388, 389, 390, 9052, 391, 18358,
-        2107, 394, 2111, 2108, 393, 2109, 395, 86155, 86156, 397, 2113,
-        398, 399, 400, 273, 274, 275, 40980, 276, 277, 31716, 279, 280,
-        31717, 281, 282, 1628, 1623, 1624, 1625, 2052, 1626, 725, 727,
-        728, 729, 730, 731, 1633, 733, 734, 735, 86157, 737, 738, 739,
-        1634, 3563, 3564, 3565, 1667, 12461, 76276, 3567, 5413, 77622,
-        5415, 5416, 5417, 5418, 107, 86158, 7784, 15363, 153, 3723,
-        2713, 7786, 3835, 7787, 86159, 7789, 7791, 7792, 7794, 86160,
-        7796, 86161, 6708, 7798, 7799, 7800, 7801, 7802, 7803, 1665,
-        43150, 15365, 1581, 5656, 43152, 80258, 7450, 39922, 86162,
-        51587, 9059, 4606, 396, 86163, 86164, 7250, 401, 403, 2860,
-        33281, 2964, 408, 9119, 409, 86165, 7669, 2861, 410, 413,
-        86166, 414, 415, 33282, 405, 33283, 7498, 2865, 7230, 33284,
-        2866, 86167, 2867, 47518, 2868, 86168, 2869, 2870, 4712, 7096,
-        28484, 6913, 6914, 6915, 6916, 37169, 37170, 7103, 28269, 6919,
-        86169, 45431, 6922, 7104, 6923, 7108, 6924, 6925, 6926, 6927,
-        6928, 86170, 86171, 86172, 6930, 6931, 6932, 6934, 6935, 6936,
-        451, 6937, 6938, 4756, 3554, 5309, 8145, 3586, 16417, 9767,
-        14126, 25854, 6580, 10174, 86173, 5519, 21309, 8561, 20938,
-        10386, 86174, 781, 2030, 16419, 30323, 16420, 16421, 16424,
-        86175, 86176, 86177, 28871, 86178, 28872, 63980, 6329, 49561,
-        4271, 38778, 86179, 86180, 20126, 16245, 193, 195, 196, 197,
-        56973, 199, 200, 201, 202, 203, 204, 56974, 56975, 205, 206,
-        4662, 207, 208, 209, 210, 211, 212, 47901, 641, 642, 643, 1380,
-        1079, 47902, 1381, 1081, 1082, 1083, 47903, 1382, 47904, 1087,
-        47905, 965, 966, 1298, 968, 1387, 1300, 50288, 971, 972, 973,
-        974, 23974, 22183, 1390, 23313, 1389, 1391, 902, 23029, 296,
-        1304, 1395, 1303, 1309, 1308, 50289, 1312, 50290, 50291, 1315,
-        1317, 9270, 19796, 3605, 1320, 1321, 44946, 1322, 1323, 50292,
-        967, 1587, 1326, 1331, 17482, 633, 29115, 53858, 29118, 29119,
-        62624, 44494, 6965, 6966, 6959, 6967, 71562, 6969, 23459,
-        23460, 17464, 4225, 23461, 23462, 23463, 5893, 23464, 17467,
-        17468, 23465, 12562, 1405, 1406, 1407, 960, 961, 962, 687, 963,
-        86181, 86182, 5997, 10812, 11976, 11977, 1850, 577, 13393,
-        10810, 13394, 65040, 86183, 3935, 3936, 3937, 710, 86184, 5785,
-        5786, 29949, 5787, 5788, 283, 284, 2687, 285, 286, 287, 2689,
-        288, 289, 8880, 290, 2690, 13899, 991, 292, 295, 42007, 35616,
-        63103, 298, 299, 3520, 297, 9024, 303, 301, 302, 300, 31345,
-        3719, 304, 305, 306, 307, 308, 368, 364, 85002, 9026, 63105,
-        367, 39596, 25835, 19746, 293, 294, 26505, 85003, 18377, 56785,
-        10122, 10123, 10124, 86185, 39863, 86186, 10125, 39865, 4066,
-        4067, 24257, 4068, 4070, 86187, 4073, 4074, 86188, 4076, 7538,
-        4077, 86189, 4078, 4079, 7540, 7541, 4084, 4085, 7542, 86190,
-        4086, 86191, 4087, 4088, 86192, 7545, 44874, 7821, 44875,
-        86193, 4286, 86194, 51470, 17609, 1408, 47486, 1411, 1412,
-        47487, 1413, 1414, 1417, 1415, 47488, 1416, 1418, 1420, 470,
-        1422, 1423, 1424, 5001, 5002, 47489, 1427, 1429, 1430, 31811,
-        1432, 1433, 47490, 1435, 3753, 1437, 1439, 1440, 47491, 1443,
-        47492, 1446, 5004, 5005, 1450, 47493, 353, 1452, 42145, 3103,
-        3402, 3104, 3105, 4780, 3106, 3107, 3108, 12157, 3111, 42146,
-        42147, 3114, 4782, 42148, 3116, 3117, 42149, 42150, 3407, 3121,
-        3122, 18154, 3126, 3127, 3128, 3410, 3130, 3411, 3412, 3415,
-        24241, 3417, 3418, 3449, 42151, 3421, 3422, 7587, 42152, 3424,
-        3427, 3428, 3448, 3430, 3432, 42153, 42154, 41648, 1991, 407,
-        57234, 411, 2862, 57235, 2863, 18368, 57236, 2874, 7350, 4115,
-        2876, 2877, 17975, 86195, 4116, 2881, 2882, 2883, 2886, 463,
-        870, 872, 873, 874, 875, 8783, 8784, 877, 1480, 1481, 459,
-        2778, 881, 8785, 2779, 8786, 8787, 8788, 886, 887, 8789, 889,
-        8790, 86196, 6920, 86197, 5080, 5081, 7395, 7396, 9395, 9396,
-        1528, 42737, 805, 86198, 1209, 13595, 4126, 9680, 34368, 9682,
-        86199, 86200, 174, 175, 176, 177, 178, 179, 180, 182, 183,
-        1477, 31138, 186, 172, 187, 188, 189, 190, 191, 458, 871,
-        31294, 31295, 27604, 31296, 31297, 882, 883, 884, 31298, 890,
-        1089, 1488, 1489, 1092, 1093, 1094, 1095, 1096, 1097, 1490,
-        1098, 1495, 1502, 1099, 1100, 1101, 1493, 2997, 12223, 1103,
-        2654, 1498, 1499, 1500, 80615, 80616, 80617, 33359, 86201,
-        9294, 1501, 86202, 1506, 1507, 23454, 38802, 38803, 1014,
-        86203, 5583, 5584, 651, 74717, 5586, 5587, 5588, 5589, 74720,
-        5590, 38808, 33527, 78330, 10930, 5119, 10931, 1000, 10928,
-        10932, 10933, 10934, 10935, 5863, 10936, 86204, 10938, 10939,
-        86205, 192, 194, 38754, 38755, 198, 38756, 38757, 38758, 2842,
-        640, 22780, 22781, 1080, 86206, 86207, 1084, 1086, 1088, 63916,
-        9412, 970, 9413, 9414, 9415, 9416, 9417, 1310, 7168, 7169,
-        1318, 9418, 1324, 39159, 1804, 1557, 24850, 41499, 1560, 41500,
-        1562, 1563, 1565, 1927, 1928, 1566, 1569, 1570, 1571, 1572,
-        1573, 1574, 1575, 1576, 2674, 2677, 2678, 2679, 2946, 2682,
-        2676, 2683, 2947, 1156, 1157, 1158, 1467, 1160, 1468, 1469,
-        1161, 1162, 1163, 4369, 1165, 1166, 1167, 12923, 2917, 1169,
-        1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 18153, 8359,
-        1178, 1164, 1191, 1180, 12924, 86208, 86209, 54817, 66962,
-        2476, 86210, 86211, 41820, 41821, 41822, 41824, 1130, 1131,
-        1132, 32692, 1134, 34848, 1136, 1133, 1137, 1138, 1139, 1140,
-        1141, 1143, 1144, 1145, 34849, 2639, 34850, 1146, 1147, 1148,
-        34851, 1150, 1151, 1152, 1153, 1154, 1155, 1678, 1679, 1680,
-        1681, 40870, 2059, 1685, 1686, 32686, 14970, 1688, 1689, 86212,
-        1692, 1682, 1693, 1695, 1696, 1698, 12955, 8909, 41690, 1700,
-        41691, 86213, 30949, 41692, 1703, 1704, 1705, 41693, 14976,
-        1708, 2071, 1709, 1710, 1711, 1712, 1727, 86214, 86215, 86216,
-        1715, 86217, 1714, 1717, 1690, 41697, 86218, 1720, 86219, 2073,
-        41699, 1724, 2075, 1726, 1729, 1730, 1732, 2078, 2223, 1735,
-        1713, 41700, 1737, 14977, 1739, 1740, 1741, 2080, 1743, 1744,
-        1745, 1746, 1747, 1748, 1749, 1750, 1751, 41701, 1752, 1753,
-        1909, 86220, 2085, 1754, 19548, 86221, 19551, 5733, 3856, 5190,
-        4581, 25145, 86222, 86223, 4846, 86224, 4861, 86225, 86226,
-        86227, 25150, 86228, 86229, 13820, 2027, 4898, 4899, 4901,
-        2135, 4902, 4868, 4904, 86230, 4905, 25155, 4907, 86231, 4909,
-        4910, 4911, 4912, 86232, 6220, 81357, 86233, 2589, 73877,
-        29706, 6227, 6228, 86234, 6237, 86235, 6241, 6242, 1812, 13808,
-        13809, 70908, 2293, 2294, 86236, 2295, 2296, 2297, 22947,
-        16511, 2299, 2300, 2301, 13097, 73079, 86237, 13099, 50121,
-        86238, 86239, 13101, 86240, 2424, 4725, 4726, 4727, 4728, 4729,
-        4730, 86241, 26881, 10944, 4734, 4735, 4736, 26239, 26240,
-        71408, 86242, 57401, 71410, 26244, 5344, 26245, 86243, 4102,
-        71414, 11091, 6736, 86244, 6737, 6738, 38152, 6740, 6741, 6742,
-        6298, 6743, 6745, 6746, 20867, 6749, 20616, 86245, 9801, 65297,
-        20617, 65298, 20619, 5629, 65299, 20621, 20622, 8385, 20623,
-        20624, 5191, 20625, 20626, 442, 443, 445, 27837, 77681, 86246,
-        27839, 86247, 86248, 41435, 66511, 2478, 2479, 2480, 2481,
-        2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2494,
-        2493, 33025, 12084, 2542, 2497, 2499, 2501, 2503, 2504, 2505,
-        33026, 2506, 2507, 2508, 2509, 2511, 1787, 12080, 2513, 2514,
-        3988, 3176, 3989, 2518, 2521, 9285, 2522, 2524, 2525, 3990,
-        2527, 2528, 27499, 2529, 2530, 3991, 2532, 2534, 2535, 18038,
-        2536, 2538, 2495, 46077, 61493, 61494, 1006, 713, 4971, 4972,
-        4973, 4975, 4976, 650, 170, 7549, 7550, 7551, 7552, 7553,
-        86249, 7936, 956, 11169, 11170, 1249, 1244, 1245, 1247, 2544,
-        1250, 2545, 1252, 2547, 1253, 1254, 2549, 39636, 1259, 1257,
-        1258, 39637, 1260, 1261, 2551, 1262, 1263, 848, 86250, 86251,
-        854, 74596, 856, 1957, 86252, 855, 1959, 1961, 857, 86253, 851,
-        859, 860, 862, 1964, 864, 865, 866, 867, 1965, 1966, 1967,
-        1968, 1969, 86254, 1971, 1972, 1973, 1974, 1975, 1976, 1977,
-        841, 1954, 842, 2978, 846, 847, 849, 850, 852, 1956, 17452,
-        71941, 86255, 86256, 73665, 1471, 13690, 185, 503, 504, 2342,
-        505, 506, 4378, 508, 4379, 17313, 510, 511, 512, 520, 513,
-        4384, 17314, 514, 515, 46158, 17317, 518, 34269, 519, 4386,
-        523, 524, 525, 46159, 528, 529, 17319, 531, 532, 533, 534, 535,
-        7482, 537, 538, 5267, 536, 539, 541, 540, 19858, 17320, 17321,
-        906, 907, 908, 17322, 910, 17323, 912, 15850, 913, 4398, 17324,
-        86257, 278, 2948, 2949, 2950, 3007, 2951, 2952, 2953, 2954,
-        2955, 3013, 35352, 3014, 3015, 2962, 3016, 33505, 39118, 3017,
-        3018, 20492, 4000, 3021, 3022, 35353, 39293, 3024, 18443, 3029,
-        9467, 20529, 39119, 8380, 2965, 3030, 3043, 22714, 39120, 2956,
-        3035, 39121, 3037, 3038, 2688, 86258, 36675, 30894, 24505,
-        8888, 13541, 49728, 27660, 9082, 27661, 365, 366, 2232, 76098,
-        7233, 1494, 17391, 606, 607, 611, 610, 612, 614, 615, 613, 616,
-        9117, 617, 618, 21155, 1789, 619, 620, 7636, 12019, 621, 622,
-        1793, 623, 625, 624, 631, 626, 627, 21578, 21103, 628, 21579,
-        629, 9122, 9123, 12189, 9289, 3168, 3169, 630, 632, 634, 21580,
-        9121, 635, 636, 637, 21581, 12781, 1801, 638, 639, 1559, 24343,
-        9419, 9420, 795, 796, 1611, 86259, 1612, 21551, 21552, 3741,
-        1617, 3742, 1615, 1619, 1620, 6301, 3744, 1622, 67685, 8521,
-        55937, 9025, 27663, 8881, 13581, 86260, 11592, 44720, 86261,
-        63231, 50873, 42925, 52332, 86262, 72706, 17705, 17707, 17708,
-        3401, 40217, 1248, 40218, 86263, 7098, 86264, 86265, 1264,
-        86266, 1266, 1267, 1268, 1269, 86267, 1271, 1272, 1273, 1274,
-        2556, 1275, 1276, 1277, 1278, 1279, 1280, 1282, 1283, 22680,
-        11889, 86268, 45662, 7038, 86269, 19315, 45663, 45664, 86270,
-        5855, 34002, 49245, 10447, 5663, 86271, 15429, 53877, 49249,
-        86272, 86273, 86274, 60128, 60453, 60129, 5552, 31923, 43407,
-        4287, 17980, 64977, 86275, 86276, 8234, 86277, 3649, 8240,
-        1330, 11999, 1332, 27618, 1334, 1335, 340, 3651, 25640, 18165,
-        1343, 4618, 1474, 3653, 75921, 1349, 53519, 1779, 45454, 22778,
-        40153, 67677, 63826, 45455, 15128, 67678, 67679, 1792, 67680,
-        3171, 47816, 45457, 9288, 59891, 67681, 25703, 35731, 35732,
-        369, 35713, 35714, 35715, 34652, 35716, 31681, 35717, 12779,
-        35718, 35719, 11992, 806, 807, 808, 43499, 43500, 810, 776,
-        812, 813, 814, 241, 43501, 43502, 816, 755, 43503, 818, 819,
-        820, 43504, 821, 822, 823, 824, 825, 826, 43505, 43506, 43507,
-        828, 829, 20083, 43508, 43509, 832, 833, 834, 835, 86278,
-        19984, 19985, 86279, 24125, 19986, 86280, 19988, 86281, 5414,
-        86282, 85808, 5479, 5420, 5421, 5422, 5423, 63800, 86283,
-        86284, 30965, 86285, 416, 1510, 5740, 5741, 81991, 86286,
-        28938, 50149, 1003, 55512, 14306, 6960, 688, 86287, 14307,
-        5399, 5400, 17783, 24118, 720, 86288, 44913, 24557, 667, 24876,
-        6529, 24877, 24878, 24879, 24880, 31847, 20671, 4011, 171, 580,
-        86289, 3863, 914, 2202, 916, 917, 918, 919, 921, 922, 923,
-        7585, 925, 7586, 926, 927, 928, 7588, 929, 930, 931, 932, 933,
-        934, 1875, 1876, 7589, 7590, 1878, 1879, 7591, 7592, 1882,
-        1883, 1884, 2212, 7593, 1887, 1888, 1889, 1890, 1891, 1892,
-        1893, 1894, 1895, 1896, 1897, 1898, 2217, 1900, 7594, 1902,
-        2219, 7595, 1905, 1906, 1907, 3323, 7596, 1911, 1912, 7597,
-        1914, 1915, 1916, 2226, 1919, 7598, 2227, 1920, 1921, 7599,
-        7600, 4708, 1923, 355, 356, 1549, 358, 32077, 360, 32078,
-        21117, 362, 19043, 71677, 5716, 86290, 49790, 86291, 86292,
-        86293, 49792, 86294, 86295, 49794, 86296, 86297, 86298, 86299,
-        11882, 86300, 49798, 86301, 49800, 49801, 49802, 49803, 453,
-        49804, 8591, 6794, 49806, 18989, 49807, 49808, 16308, 49809,
-        86302, 86303, 10105, 86304, 5285, 10106, 10107, 6557, 86305,
-        23571, 10109, 38883, 10110, 5401, 86306, 67557, 16430, 67558,
-        40171, 16433, 25878, 86307, 21762, 23, 86308, 86309, 21766,
-        86310, 86311, 5149, 3926, 21768, 21769, 47826, 942, 46985,
-        6588, 58867, 6589, 6590, 86312, 6592, 6006, 53855, 9565, 359,
-        86313, 2845, 876, 879, 27556, 27557, 885, 27558, 888, 2847,
-        27559, 2115, 2116, 2117, 53962, 57839, 315, 316, 317, 318, 319,
-        86314, 321, 322, 2122, 323, 2123, 324, 325, 328, 326, 327,
-        40542, 329, 330, 18079, 18080, 331, 1790, 7382, 332, 7380,
-        7236, 23413, 23414, 18924, 18925, 333, 335, 336, 39750, 337,
-        86315, 339, 341, 342, 343, 16264, 16265, 6615, 86316, 86317,
-        86318, 86319, 16269, 10538, 33226, 86320, 16272, 5824, 16273,
-        16274, 16276, 16277, 16278, 16279, 16280, 14517, 1547, 6463,
-        3394, 49677, 659, 10380, 30013, 10382, 10378, 10379, 10383,
-        10384, 10385, 86321, 4139, 13370, 13371, 86322, 86323, 11878,
-        64509, 15141, 15142, 15143, 32737, 14183, 15144, 39101, 42768,
-        5645, 32738, 801, 803, 804, 86324, 14707, 86325, 6601, 12402,
-        712, 12403, 2936, 1447, 15477, 1410, 44872, 1550, 8614, 15478,
-        15479, 15480, 15481, 4811, 3752, 1442, 15482, 8818, 1445, 5006,
-        16304, 32277, 16305, 16306, 86326, 16307, 53691, 69305, 809,
-        86327, 815, 26724, 69307, 43484, 63904, 86328, 13498, 827,
-        86329, 831, 2857, 836, 86330, 86331, 837, 838, 839, 840, 228,
-        229, 43722, 230, 231, 43723, 234, 235, 236, 237, 238, 239,
-        2745, 2746, 240, 242, 243, 244, 43724, 19788, 246, 247, 21134,
-        248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 43725, 43726,
-        41, 43727, 262, 43728, 2751, 264, 265, 266, 267, 268, 269, 270,
-        271, 272, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032,
-        1033, 1034, 43729, 1035, 43730, 1037, 21821, 2926, 14388,
-        10432, 14389, 14390, 14391, 14392, 86332, 14394, 14395, 2035,
-        2169, 86333, 14397, 14398, 14399, 14400, 52, 14401, 14402,
-        7077, 21822, 14405, 14406, 14396, 86334, 17356, 17357, 84679,
-        84680, 76383, 17360, 17361, 86335, 38801, 2060, 30850, 12963,
-        1684, 1687, 2061, 14978, 1694, 43387, 1697, 1699, 2067, 1701,
-        1702, 1706, 43388, 43389, 76325, 1716, 1718, 26832, 1719, 1723,
-        2081, 2063, 1728, 39059, 76326, 1731, 86336, 1736, 76327, 1738,
-        19657, 6579, 6581, 6582, 6583, 6584, 6585, 29979, 1818, 28239,
-        68, 69, 3391, 86337, 10266, 63528, 86338, 10269, 10270, 10271,
-        10272, 86339, 86340, 63530, 63531, 63532, 63533, 10273, 63534,
-        86341, 10681, 10682, 86342, 9673, 86343, 10683, 460, 461, 462,
-        467, 4464, 4466, 3729, 471, 472, 468, 81634, 474, 81635, 475,
-        476, 477, 479, 480, 81636, 81637, 482, 17442, 81638, 81639,
-        484, 485, 486, 4473, 488, 489, 490, 493, 466, 494, 495, 496,
-        497, 499, 500, 501, 502, 34376, 86344, 63836, 56281, 1707,
-        20416, 61452, 56282, 1755, 56283, 56284, 18508, 53650, 63444,
-        86345, 3579, 63445, 3677, 1979, 1980, 1981, 3132, 3147, 34090,
-        1987, 12770, 1329, 80818, 80819, 1988, 23522, 1986, 15880,
-        1985, 32975, 1992, 1993, 7165, 3141, 3143, 86346, 1982, 1984,
-        3145, 86347, 78064, 55453, 2656, 2657, 35634, 35635, 2167,
-        43479 };
-
-    data10k = new int[] { 2, 4, 149900, 11, 70236, 149901, 149902, 6721,
-        149929, 29212, 34600, 149930, 149931, 149932, 141696, 149908,
-        149909, 149910 };
-
-    data501871 = new int[] { 1368366, 1368367, 1817408, 11, 2513, 1817409,
-        1817410, 1817411, 1382349, 126700, 1817412, 5539, 21862, 21863,
-        21864, 1233, 1127, 121, 15254, 15255, 357, 449, 15256, 8817,
-        15257, 15258, 1406, 1096, 281, 4826, 4827, 223, 166, 2372, 168,
-        169, 2219, 170, 171, 1176, 172, 173, 2222, 3035, 177, 178, 179,
-        180, 181, 183, 3036, 2378, 1157, 1158, 2380, 1160, 1161, 1162,
-        2384, 1164, 1165, 1166, 1167, 1168, 2385, 3037, 1171, 1172,
-        1173, 2238, 1175, 1177, 1178, 1179, 1180, 1181, 2243, 3038,
-        1182, 2244, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190,
-        59766, 471, 7349, 3599, 2847, 59767, 59768, 59769, 59770,
-        59771, 59772, 59773, 59774, 59775, 2625, 852, 853, 2632, 854,
-        855, 856, 2284, 857, 862, 1031, 859, 860, 861, 866, 1033, 867,
-        1035, 868, 870, 2294, 871, 2295, 873, 874, 875, 876, 877, 878,
-        879, 66632, 66633, 66634, 66635, 14823, 66636, 66637, 3763,
-        77345, 1370, 3764, 3765, 3766, 5666, 3768, 3770, 16892, 3771,
-        3772, 3773, 3244, 3246, 3247, 1504, 266, 29250, 24764, 29251,
-        689, 12844, 8068, 29252, 38918, 750, 751, 770, 3704, 753, 754,
-        765, 755, 3708, 757, 758, 759, 760, 3710, 761, 762, 763, 3712,
-        766, 767, 768, 769, 771, 3719, 4380, 3722, 3723, 3725, 4381,
-        3727, 3728, 3731, 3732, 764, 4382, 2316, 334, 1637, 4383, 4384,
-        4385, 4386, 4387, 184, 185, 1134, 186, 1135, 187, 188, 1138,
-        197, 191, 3517, 193, 194, 195, 196, 208, 3519, 198, 9210, 937,
-        9211, 9212, 916, 917, 117, 118, 919, 122, 921, 123, 124, 125,
-        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 924, 137,
-        138, 139, 140, 141, 588, 928, 142, 143, 144, 929, 146, 147,
-        148, 149, 150, 151, 3775, 3776, 3777, 3778, 3780, 3781, 3783,
-        3784, 3785, 3796, 4169, 3788, 4170, 3790, 3791, 3793, 3803,
-        3794, 3797, 4171, 3799, 3800, 3801, 3802, 3804, 4172, 3806,
-        4173, 4174, 3811, 4175, 3813, 3829, 3815, 3816, 3817, 4176,
-        4177, 3820, 3821, 3822, 2168, 3039, 2460, 2170, 2459, 2174,
-        2175, 2176, 2461, 2462, 2463, 3040, 2466, 2467, 2469, 2468,
-        2470, 3041, 2472, 3042, 3043, 3044, 3045, 231, 881, 882, 1219,
-        884, 2038, 886, 887, 888, 891, 892, 1221, 894, 895, 1222, 2039,
-        899, 1225, 900, 901, 902, 2492, 2494, 2495, 2496, 4052, 2498,
-        2502, 2500, 2501, 2503, 2504, 4653, 5514, 18671, 10350, 1122,
-        44317, 44318, 44319, 44320, 44321, 44322, 44323, 44324, 7923,
-        1422, 10284, 10285, 6146, 9803, 10286, 466, 5998, 696, 3257,
-        6043, 6195, 6196, 6197, 6198, 6199, 6200, 6201, 7029, 4405,
-        4864, 450, 349, 11214, 3548, 1092, 5728, 7395, 6533, 1123,
-        5736, 1115, 6535, 6536, 2739, 2832, 2833, 2834, 2835, 2836,
-        23972, 2837, 23973, 2839, 2840, 2691, 1339, 20116, 3219, 8210,
-        3170, 3171, 3172, 3173, 2094, 2095, 2096, 2097, 2099, 2100,
-        2102, 3174, 2104, 1372, 2105, 2107, 2108, 2109, 2110, 2113,
-        2114, 2115, 2117, 2118, 3221, 3222, 2122, 2123, 2124, 4611,
-        2125, 2126, 2127, 2128, 2129, 2130, 2131, 575, 576, 2132, 4612,
-        2134, 2135, 2136, 4368, 5931, 5932, 5933, 5934, 5935, 5936,
-        5937, 5938, 5939, 2902, 4057, 4058, 4059, 4060, 4062, 4063,
-        4064, 4654, 4655, 4067, 4068, 4069, 4656, 4657, 4073, 4658,
-        4074, 4075, 4659, 4660, 4661, 4076, 4662, 4663, 4664, 4078,
-        4079, 4080, 4665, 4082, 4083, 4084, 4666, 4086, 4087, 4088,
-        544, 545, 546, 547, 548, 549, 550, 559, 1227, 552, 553, 5035,
-        555, 554, 1228, 556, 1229, 557, 558, 560, 561, 562, 563, 564,
-        565, 1230, 566, 567, 568, 569, 570, 572, 573, 222, 7461, 2059,
-        2060, 2061, 5664, 2062, 7463, 16997, 2065, 2066, 2067, 2068,
-        2069, 2070, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 7464,
-        2079, 2080, 2081, 7465, 2082, 2083, 2084, 2085, 2086, 2087,
-        199, 206, 200, 203, 205, 211, 1140, 3699, 209, 214, 215, 216,
-        217, 218, 777, 778, 779, 780, 2298, 781, 782, 783, 784, 785,
-        787, 788, 384, 789, 790, 791, 2677, 793, 794, 795, 796, 797,
-        2307, 798, 799, 801, 802, 3645, 803, 4337, 805, 3648, 3649,
-        807, 808, 3651, 810, 812, 813, 814, 815, 816, 3654, 818, 819,
-        13780, 930, 932, 4221, 935, 936, 938, 2197, 939, 940, 941,
-        2200, 943, 1591, 1952, 2630, 1592, 2631, 1602, 1607, 1595,
-        1596, 1597, 1598, 1599, 1955, 1601, 1603, 1956, 1605, 1606,
-        1608, 1610, 1638, 20608, 968, 969, 970, 971, 972, 973, 974,
-        975, 2729, 2730, 977, 2731, 979, 980, 981, 982, 983, 984, 3506,
-        987, 989, 990, 991, 2732, 2733, 6051, 6053, 6055, 910, 6056,
-        4339, 4340, 577, 4341, 579, 580, 581, 616, 584, 585, 586, 4342,
-        4343, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 5046,
-        599, 600, 5047, 601, 602, 603, 604, 605, 5053, 608, 609, 610,
-        5055, 612, 613, 5056, 615, 617, 618, 619, 620, 621, 622, 623,
-        624, 6882, 627, 628, 629, 630, 631, 5330, 633, 634, 635, 636,
-        637, 639, 640, 7870, 632, 34480, 13118, 903, 904, 905, 907,
-        2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2643,
-        1685, 1686, 1687, 1688, 1690, 1691, 2644, 2645, 1695, 2646,
-        1699, 2647, 2648, 1702, 2649, 2650, 1706, 22082, 5516, 4307,
-        2203, 1995, 1996, 1998, 1999, 2206, 2002, 2003, 4407, 2005,
-        4408, 2007, 2008, 2009, 2010, 2011, 4409, 2013, 2014, 2015,
-        2017, 3227, 3149, 6025, 22913, 22914, 3228, 7925, 10123, 10124,
-        10125, 10127, 16978, 14094, 1593, 4869, 4870, 3477, 3844, 3845,
-        9923, 3846, 3847, 39767, 39768, 39769, 3541, 39770, 39771,
-        14179, 39772, 39773, 39774, 42558, 1043, 4203, 42559, 42560,
-        42561, 42562, 42563, 42564, 11018, 42565, 42566, 4589, 4590,
-        4591, 4312, 18283, 4317, 4318, 4319, 12659, 11706, 11707,
-        53395, 53396, 29410, 8040, 8041, 915, 20105, 22952, 22953,
-        20596, 4161, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054,
-        3055, 1474, 3056, 3057, 3058, 3059, 3060, 3061, 2549, 2551,
-        3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 515, 3070,
-        3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080,
-        3081, 3082, 506, 3083, 3084, 3085, 3086, 3087, 3088, 3089,
-        3090, 3091, 527, 528, 2995, 530, 531, 533, 534, 535, 537, 538 };
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/encoding/EncodingTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/encoding/EncodingTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/encoding/EncodingTest.java	2013-02-20 13:38:17.432711928 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/encoding/EncodingTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,170 +0,0 @@
-package org.apache.lucene.facet.encoding;
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.encoding.DGapIntEncoder;
-import org.apache.lucene.facet.encoding.DGapVInt8IntEncoder;
-import org.apache.lucene.facet.encoding.EightFlagsIntEncoder;
-import org.apache.lucene.facet.encoding.FourFlagsIntEncoder;
-import org.apache.lucene.facet.encoding.IntDecoder;
-import org.apache.lucene.facet.encoding.IntEncoder;
-import org.apache.lucene.facet.encoding.NOnesIntEncoder;
-import org.apache.lucene.facet.encoding.SimpleIntEncoder;
-import org.apache.lucene.facet.encoding.SortingIntEncoder;
-import org.apache.lucene.facet.encoding.UniqueValuesIntEncoder;
-import org.apache.lucene.facet.encoding.VInt8IntEncoder;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class EncodingTest extends FacetTestCase {
-
-  private static IntsRef uniqueSortedData, data;
-  
-  @BeforeClass
-  public static void beforeClassEncodingTest() throws Exception {
-    int capacity = atLeast(10000);
-    data = new IntsRef(capacity);
-    for (int i = 0; i < 10; i++) {
-      data.ints[i] = i + 1; // small values
-    }
-    for (int i = 10; i < data.ints.length; i++) {
-      data.ints[i] = random().nextInt(Integer.MAX_VALUE - 1) + 1; // some encoders don't allow 0
-    }
-    data.length = data.ints.length;
-    
-    uniqueSortedData = IntsRef.deepCopyOf(data);
-    Arrays.sort(uniqueSortedData.ints);
-    uniqueSortedData.length = 0;
-    int prev = -1;
-    for (int i = 0; i < uniqueSortedData.ints.length; i++) {
-      if (uniqueSortedData.ints[i] != prev) {
-        uniqueSortedData.ints[uniqueSortedData.length++] = uniqueSortedData.ints[i];
-        prev = uniqueSortedData.ints[i];
-      }
-    }
-  }
-  
-  private static void encoderTest(IntEncoder encoder, IntsRef data, IntsRef expected) throws IOException {
-    // ensure toString is implemented
-    String toString = encoder.toString();
-    assertFalse(toString.startsWith(encoder.getClass().getName() + "@"));
-    IntDecoder decoder = encoder.createMatchingDecoder();
-    toString = decoder.toString();
-    assertFalse(toString.startsWith(decoder.getClass().getName() + "@"));
-    
-    BytesRef bytes = new BytesRef(100); // some initial capacity - encoders should grow the byte[]
-    IntsRef values = new IntsRef(100); // some initial capacity - decoders should grow the int[]
-    for (int i = 0; i < 2; i++) {
-      // run 2 iterations to catch encoders/decoders which don't reset properly
-      encoding(encoder, data, bytes);
-      decoding(bytes, values, encoder.createMatchingDecoder());
-      assertTrue(expected.intsEquals(values));
-    }
-  }
-
-  private static void encoding(IntEncoder encoder, IntsRef data, BytesRef bytes) throws IOException {
-    final IntsRef values;
-    if (random().nextBoolean()) { // randomly set the offset
-      values = new IntsRef(data.length + 1);
-      System.arraycopy(data.ints, 0, values.ints, 1, data.length);
-      values.offset = 1; // ints start at index 1
-      values.length = data.length;
-    } else {
-      // need to copy the array because it may be modified by encoders (e.g. sorting)
-      values = IntsRef.deepCopyOf(data);
-    }
-    encoder.encode(values, bytes);
-  }
-
-  private static void decoding(BytesRef bytes, IntsRef values, IntDecoder decoder) throws IOException {
-    int offset = 0;
-    if (random().nextBoolean()) { // randomly set the offset and length to other than 0,0
-      bytes.grow(bytes.length + 1); // ensure that we have enough capacity to shift values by 1
-      bytes.offset = 1; // bytes start at index 1 (must do that after grow)
-      System.arraycopy(bytes.bytes, 0, bytes.bytes, 1, bytes.length);
-      offset = 1;
-    }
-    decoder.decode(bytes, values);
-    assertEquals(offset, bytes.offset); // decoders should not mess with offsets
-  }
-
-  @Test
-  public void testVInt8() throws Exception {
-    encoderTest(new VInt8IntEncoder(), data, data);
-    
-    // cover negative numbers;
-    BytesRef bytes = new BytesRef(5);
-    IntEncoder enc = new VInt8IntEncoder();
-    IntsRef values = new IntsRef(1);
-    values.ints[values.length++] = -1;
-    enc.encode(values, bytes);
-    
-    IntDecoder dec = enc.createMatchingDecoder();
-    values.length = 0;
-    dec.decode(bytes, values);
-    assertEquals(1, values.length);
-    assertEquals(-1, values.ints[0]);
-  }
-  
-  @Test
-  public void testSimpleInt() throws Exception {
-    encoderTest(new SimpleIntEncoder(), data, data);
-  }
-  
-  @Test
-  public void testSortingUniqueValues() throws Exception {
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new VInt8IntEncoder())), data, uniqueSortedData);
-  }
-
-  @Test
-  public void testSortingUniqueDGap() throws Exception {
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new VInt8IntEncoder()))), data, uniqueSortedData);
-  }
-
-  @Test
-  public void testSortingUniqueDGapEightFlags() throws Exception {
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new EightFlagsIntEncoder()))), data, uniqueSortedData);
-  }
-
-  @Test
-  public void testSortingUniqueDGapFourFlags() throws Exception {
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new FourFlagsIntEncoder()))), data, uniqueSortedData);
-  }
-
-  @Test
-  public void testSortingUniqueDGapNOnes4() throws Exception {
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new NOnesIntEncoder(4)))), data, uniqueSortedData);
-  }
-  
-  @Test
-  public void testSortingUniqueDGapNOnes3() throws Exception {
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new NOnesIntEncoder(3)))), data, uniqueSortedData);
-  }
-  
-  @Test
-  public void testSortingUniqueDGapVInt() throws Exception {
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapVInt8IntEncoder())), data, uniqueSortedData);
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/FacetTestBase.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/FacetTestBase.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/FacetTestBase.java	2013-02-20 13:38:17.460711929 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/FacetTestBase.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,362 +0,0 @@
-package org.apache.lucene.facet;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.facet.collections.IntToObjectMap;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
-import org.apache.lucene.util._TestUtil;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-@SuppressCodecs({"SimpleText"})
-public abstract class FacetTestBase extends FacetTestCase {
-  
-  /** Holds a search and taxonomy Directories pair. */
-  private static final class SearchTaxoDirPair {
-    Directory searchDir, taxoDir;
-    SearchTaxoDirPair() {}
-  }
-  
-  private static IntToObjectMap<SearchTaxoDirPair> dirsPerPartitionSize;
-  private static IntToObjectMap<FacetIndexingParams> fipPerPartitionSize;
-  private static File TEST_DIR;
-  
-  /** Documents text field. */
-  protected static final String CONTENT_FIELD = "content";
-  
-  /** taxonomy Reader for the test. */
-  protected TaxonomyReader taxoReader;
-  
-  /** Index Reader for the test. */
-  protected IndexReader indexReader;
-  
-  /** Searcher for the test. */
-  protected IndexSearcher searcher;
-  
-  @BeforeClass
-  public static void beforeClassFacetTestBase() {
-    TEST_DIR = _TestUtil.getTempDir("facets");
-    dirsPerPartitionSize = new IntToObjectMap<FacetTestBase.SearchTaxoDirPair>();
-    fipPerPartitionSize = new IntToObjectMap<FacetIndexingParams>();
-  }
-  
-  @AfterClass
-  public static void afterClassFacetTestBase() throws Exception {
-    Iterator<SearchTaxoDirPair> iter = dirsPerPartitionSize.iterator();
-    while (iter.hasNext()) {
-      SearchTaxoDirPair pair = iter.next();
-      IOUtils.close(pair.searchDir, pair.taxoDir);
-    }
-  }
-  
-  /** documents text (for the text field). */
-  private static final String[] DEFAULT_CONTENT = {
-      "the white car is the one I want.",
-      "the white dog does not belong to anyone.",
-  };
-  
-  /** Facets: facets[D][F] == category-path no. F for document no. D. */
-  private static final CategoryPath[][] DEFAULT_CATEGORIES = {
-      { new CategoryPath("root","a","f1"), new CategoryPath("root","a","f2") },
-      { new CategoryPath("root","a","f1"), new CategoryPath("root","a","f3") },
-  };
-  
-  /** categories to be added to specified doc */
-  protected List<CategoryPath> getCategories(int doc) {
-    return Arrays.asList(DEFAULT_CATEGORIES[doc]);
-  }
-  
-  /** Number of documents to index */
-  protected int numDocsToIndex() {
-    return DEFAULT_CONTENT.length;
-  }
-  
-  /** content to be added to specified doc */
-  protected String getContent(int doc) {
-    return DEFAULT_CONTENT[doc];
-  }
-  
-  /** Prepare index (in RAM) with some documents and some facets. */
-  protected final void initIndex(FacetIndexingParams fip) throws Exception {
-    initIndex(false, fip);
-  }
-
-  /** Prepare index (in RAM/Disk) with some documents and some facets. */
-  protected final void initIndex(boolean forceDisk, FacetIndexingParams fip) throws Exception {
-    int partitionSize = fip.getPartitionSize();
-    if (VERBOSE) {
-      System.out.println("Partition Size: " + partitionSize + "  forceDisk: "+forceDisk);
-    }
-
-    SearchTaxoDirPair pair = dirsPerPartitionSize.get(Integer.valueOf(partitionSize));
-    if (pair == null) {
-      pair = new SearchTaxoDirPair();
-      if (forceDisk) {
-        pair.searchDir = newFSDirectory(new File(TEST_DIR, "index"));
-        pair.taxoDir = newFSDirectory(new File(TEST_DIR, "taxo"));
-      } else {
-        pair.searchDir = newDirectory();
-        pair.taxoDir = newDirectory();
-      }
-      
-      RandomIndexWriter iw = new RandomIndexWriter(random(), pair.searchDir, getIndexWriterConfig(getAnalyzer()));
-      TaxonomyWriter taxo = new DirectoryTaxonomyWriter(pair.taxoDir, OpenMode.CREATE);
-      
-      populateIndex(iw, taxo, fip);
-      
-      // commit changes (taxonomy prior to search index for consistency)
-      taxo.commit();
-      iw.commit();
-      taxo.close();
-      iw.close();
-      
-      dirsPerPartitionSize.put(Integer.valueOf(partitionSize), pair);
-    }
-    
-    // prepare for searching
-    taxoReader = new DirectoryTaxonomyReader(pair.taxoDir);
-    indexReader = DirectoryReader.open(pair.searchDir);
-    searcher = newSearcher(indexReader);
-  }
-  
-  /** Returns indexing params for the main index */
-  protected IndexWriterConfig getIndexWriterConfig(Analyzer analyzer) {
-    return newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
-  }
-
-  /** Returns a {@link FacetIndexingParams} per the given partition size. */
-  protected FacetIndexingParams getFacetIndexingParams(final int partSize) {
-    return getFacetIndexingParams(partSize, false);
-  }
-  
-  /**
-   * Returns a {@link FacetIndexingParams} per the given partition size. If
-   * requested, then {@link OrdinalPolicy} will be set to
-   * {@link OrdinalPolicy#ALL_PARENTS}, otherwise it will randomize.
-   */
-  protected FacetIndexingParams getFacetIndexingParams(final int partSize, final boolean forceAllParents) {
-    FacetIndexingParams fip = fipPerPartitionSize.get(partSize);
-    if (fip == null) {
-      // randomize OrdinalPolicy. Since not all Collectors / Accumulators
-      // support NO_PARENTS, don't include it.
-      // TODO: once all code paths support NO_PARENTS, randomize it too.
-      CategoryListParams randomOP = new CategoryListParams() {
-        final OrdinalPolicy op = random().nextBoolean() ? OrdinalPolicy.ALL_BUT_DIMENSION : OrdinalPolicy.ALL_PARENTS;
-        @Override
-        public OrdinalPolicy getOrdinalPolicy(String dimension) {
-          return forceAllParents ? OrdinalPolicy.ALL_PARENTS : op;
-        }
-      };
-      
-      // several of our encoders don't support the value 0, 
-      // which is one of the values encoded when dealing w/ partitions,
-      // therefore don't randomize the encoder.
-      fip = new FacetIndexingParams(randomOP) {
-        @Override
-        public int getPartitionSize() {
-          return partSize;
-        }
-      };
-      fipPerPartitionSize.put(partSize, fip);
-    }
-    return fip;
-  }
-  
-  /**
-   * Faceted Search Params for the test. Sub classes should override in order to
-   * test with different faceted search params.
-   */
-  protected FacetSearchParams getFacetSearchParams(FacetIndexingParams iParams, FacetRequest... facetRequests) {
-    return new FacetSearchParams(iParams, facetRequests);
-  }
-
-  /**
-   * Faceted Search Params for the test. Sub classes should override in order to
-   * test with different faceted search params.
-   */
-  protected FacetSearchParams getFacetSearchParams(List<FacetRequest> facetRequests, FacetIndexingParams iParams) {
-    return new FacetSearchParams(iParams, facetRequests);
-  }
-
-  /**
-   * Populate the test index+taxonomy for this test.
-   * <p>Subclasses can override this to test different scenarios
-   */
-  protected void populateIndex(RandomIndexWriter iw, TaxonomyWriter taxo, FacetIndexingParams iParams)
-      throws IOException {
-    // add test documents 
-    int numDocsToIndex = numDocsToIndex();
-    for (int doc=0; doc<numDocsToIndex; doc++) {
-      indexDoc(iParams, iw, taxo, getContent(doc), getCategories(doc));
-    }
-    
-    // also add a document that would be deleted, so that all tests are also working against deletions in the index
-    String content4del = "ContentOfDocToDelete";
-    indexDoc(iParams, iw, taxo, content4del, getCategories(0));
-    iw.commit(); // commit it
-    iw.deleteDocuments(new Term(CONTENT_FIELD,content4del)); // now delete the committed doc 
-  }
-  
-  /** Close all indexes */
-  protected void closeAll() throws Exception {
-    // close and nullify everything
-    IOUtils.close(taxoReader, indexReader);
-    taxoReader = null;
-    indexReader = null;
-    searcher = null;
-  }
-  
-  /**
-   * Analyzer to use for the test.
-   * Sub classes should override in order to test with different analyzer.
-   */
-  protected Analyzer getAnalyzer() {
-    return new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false);
-  }
-  
-  /** convenience method: convert sub results to an array */  
-  protected static FacetResultNode[] resultNodesAsArray(FacetResultNode parentRes) {
-    ArrayList<FacetResultNode> a = new ArrayList<FacetResultNode>();
-    for (FacetResultNode frn : parentRes.subResults) {
-      a.add(frn);
-    }
-    return a.toArray(new FacetResultNode[0]);
-  }
-  
-  /** utility Create a dummy document with specified categories and content */
-  protected final void indexDoc(FacetIndexingParams iParams, RandomIndexWriter iw,
-      TaxonomyWriter tw, String content, List<CategoryPath> categories) throws IOException {
-    Document d = new Document();
-    FacetFields facetFields = new FacetFields(tw, iParams);
-    facetFields.addFields(d, categories);
-    d.add(new TextField("content", content, Field.Store.YES));
-    iw.addDocument(d);
-  }
-  
-  /** Build the "truth" with ALL the facets enumerating indexes content. */
-  protected Map<CategoryPath, Integer> facetCountsTruth() throws IOException {
-    FacetIndexingParams iParams = getFacetIndexingParams(Integer.MAX_VALUE);
-    String delim = String.valueOf(iParams.getFacetDelimChar());
-    Map<CategoryPath, Integer> res = new HashMap<CategoryPath, Integer>();
-    HashSet<String> handledTerms = new HashSet<String>();
-    for (CategoryListParams clp : iParams.getAllCategoryListParams()) {
-      if (!handledTerms.add(clp.field)) {
-        continue; // already handled this term (for another list) 
-      }
-      Terms terms = MultiFields.getTerms(indexReader, clp.field);
-      if (terms == null) {
-        continue;
-      }
-      Bits liveDocs = MultiFields.getLiveDocs(indexReader);
-      TermsEnum te = terms.iterator(null);
-      DocsEnum de = null;
-      while (te.next() != null) {
-        de = _TestUtil.docs(random(), te, liveDocs, de, DocsEnum.FLAG_NONE);
-        int cnt = 0;
-        while (de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
-          cnt++;
-        }
-        res.put(new CategoryPath(te.term().utf8ToString().split(delim)), cnt);
-      }
-    }
-    return res;
-  }
-  
-  /** Validate counts for returned facets, and that there are not too many results */
-  protected static void assertCountsAndCardinality(Map<CategoryPath, Integer> facetCountsTruth, List<FacetResult> facetResults) throws Exception {
-    for (FacetResult fr : facetResults) {
-      FacetResultNode topResNode = fr.getFacetResultNode();
-      FacetRequest freq = fr.getFacetRequest();
-      if (VERBOSE) {
-        System.out.println(freq.categoryPath.toString()+ "\t\t" + topResNode);
-      }
-      assertCountsAndCardinality(facetCountsTruth, topResNode, freq.numResults);
-    }
-  }
-    
-  /** Validate counts for returned facets, and that there are not too many results */
-  private static void assertCountsAndCardinality(Map<CategoryPath,Integer> facetCountsTruth,  FacetResultNode resNode, int reqNumResults) throws Exception {
-    int actualNumResults = resNode.subResults.size();
-    if (VERBOSE) {
-      System.out.println("NumResults: " + actualNumResults);
-    }
-    assertTrue("Too many results!", actualNumResults <= reqNumResults);
-    for (FacetResultNode subRes : resNode.subResults) {
-      assertEquals("wrong count for: "+subRes, facetCountsTruth.get(subRes.label).intValue(), (int)subRes.value);
-      assertCountsAndCardinality(facetCountsTruth, subRes, reqNumResults); // recurse into child results
-    }
-  }
-
-  /** Validate results equality */
-  protected static void assertSameResults(List<FacetResult> expected, List<FacetResult> actual) {
-    assertEquals("wrong number of facet results", expected.size(), actual.size());
-    int size = expected.size();
-    for (int i = 0; i < size; i++) {
-      FacetResult expectedResult = expected.get(i);
-      FacetResult actualResult = actual.get(i);
-      String expectedStr = FacetTestUtils.toSimpleString(expectedResult);
-      String actualStr = FacetTestUtils.toSimpleString(actualResult);
-      assertEquals("Results not the same!\nExpected:" + expectedStr + "\nActual:\n" + actualStr, expectedStr, actualStr);
-    }
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java	2013-11-03 19:05:04.703394188 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java	2013-12-02 17:57:26.194735835 -0500
@@ -1,19 +1,5 @@
 package org.apache.lucene.facet;
 
-import java.util.Random;
-
-import org.apache.lucene.facet.encoding.DGapIntEncoder;
-import org.apache.lucene.facet.encoding.DGapVInt8IntEncoder;
-import org.apache.lucene.facet.encoding.EightFlagsIntEncoder;
-import org.apache.lucene.facet.encoding.FourFlagsIntEncoder;
-import org.apache.lucene.facet.encoding.IntEncoder;
-import org.apache.lucene.facet.encoding.NOnesIntEncoder;
-import org.apache.lucene.facet.encoding.SortingIntEncoder;
-import org.apache.lucene.facet.encoding.UniqueValuesIntEncoder;
-import org.apache.lucene.facet.encoding.VInt8IntEncoder;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.util.LuceneTestCase;
-
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -31,34 +17,189 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
 public abstract class FacetTestCase extends LuceneTestCase {
   
-  private static final IntEncoder[] ENCODERS = new IntEncoder[] {
-    new SortingIntEncoder(new UniqueValuesIntEncoder(new VInt8IntEncoder())),
-    new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new VInt8IntEncoder()))),
-    new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapVInt8IntEncoder())),
-    new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new EightFlagsIntEncoder()))),
-    new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new FourFlagsIntEncoder()))),
-    new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new NOnesIntEncoder(3)))),
-    new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new NOnesIntEncoder(4)))), 
-  };
-  
-  /** Returns a {@link CategoryListParams} with random {@link IntEncoder} and field. */
-  public static CategoryListParams randomCategoryListParams() {
-    final String field = CategoryListParams.DEFAULT_FIELD + "$" + random().nextInt();
-    return randomCategoryListParams(field);
+  public Facets getTaxonomyFacetCounts(TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector c) throws IOException {
+    return getTaxonomyFacetCounts(taxoReader, config, c, FacetsConfig.DEFAULT_INDEX_FIELD_NAME);
   }
-  
-  /** Returns a {@link CategoryListParams} with random {@link IntEncoder}. */
-  public static CategoryListParams randomCategoryListParams(String field) {
-    Random random = random();
-    final IntEncoder encoder = ENCODERS[random.nextInt(ENCODERS.length)];
-    return new CategoryListParams(field) {
-      @Override
-      public IntEncoder createEncoder() {
-        return encoder;
+
+  public Facets getTaxonomyFacetCounts(TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector c, String indexFieldName) throws IOException {
+    Facets facets;
+    if (random().nextBoolean()) {
+      facets = new FastTaxonomyFacetCounts(indexFieldName, taxoReader, config, c);
+    } else {
+      OrdinalsReader ordsReader = new DocValuesOrdinalsReader(indexFieldName);
+      if (random().nextBoolean()) {
+        ordsReader = new CachedOrdinalsReader(ordsReader);
       }
-    };
+      facets = new TaxonomyFacetCounts(ordsReader, taxoReader, config, c);
+    }
+
+    return facets;
+  }
+
+  protected String[] getRandomTokens(int count) {
+    String[] tokens = new String[count];
+    for(int i=0;i<tokens.length;i++) {
+      tokens[i] = _TestUtil.randomRealisticUnicodeString(random(), 1, 10);
+      //tokens[i] = _TestUtil.randomSimpleString(random(), 1, 10);
+    }
+    return tokens;
+  }
+
+  protected String pickToken(String[] tokens) {
+    for(int i=0;i<tokens.length;i++) {
+      if (random().nextBoolean()) {
+        return tokens[i];
+      }
+    }
+
+    // Move long tail onto first token:
+    return tokens[0];
+  }
+
+  protected static class TestDoc {
+    public String content;
+    public String[] dims;
+    public float value;
+  }
+
+  protected List<TestDoc> getRandomDocs(String[] tokens, int count, int numDims) {
+    List<TestDoc> docs = new ArrayList<>();
+    for(int i=0;i<count;i++) {
+      TestDoc doc = new TestDoc();
+      docs.add(doc);
+      doc.content = pickToken(tokens);
+      doc.dims = new String[numDims];
+      for(int j=0;j<numDims;j++) {
+        doc.dims[j] = pickToken(tokens);
+        if (random().nextInt(10) < 3) {
+          break;
+        }
+      }
+      if (VERBOSE) {
+        System.out.println("  doc " + i + ": content=" + doc.content);
+        for(int j=0;j<numDims;j++) {
+          if (doc.dims[j] != null) {
+            System.out.println("    dim[" + j + "]=" + doc.dims[j]);
+          }
+        }
+      }
+    }
+
+    return docs;
   }
   
+  protected void sortTies(List<FacetResult> results) {
+    for(FacetResult result : results) {
+      sortTies(result.labelValues);
+    }
+  }
+
+  protected void sortTies(LabelAndValue[] labelValues) {
+    double lastValue = -1;
+    int numInRow = 0;
+    int i = 0;
+    while(i <= labelValues.length) {
+      if (i < labelValues.length && labelValues[i].value.doubleValue() == lastValue) {
+        numInRow++;
+      } else {
+        if (numInRow > 1) {
+          Arrays.sort(labelValues, i-numInRow, i,
+                      new Comparator<LabelAndValue>() {
+                        @Override
+                        public int compare(LabelAndValue a, LabelAndValue b) {
+                          assert a.value.doubleValue() == b.value.doubleValue();
+                          return new BytesRef(a.label).compareTo(new BytesRef(b.label));
+                        }
+                      });
+        }
+        numInRow = 1;
+        if (i < labelValues.length) {
+          lastValue = labelValues[i].value.doubleValue();
+        }
+      }
+      i++;
+    }
+  }
+
+  protected void sortLabelValues(List<LabelAndValue> labelValues) {
+    Collections.sort(labelValues,
+                     new Comparator<LabelAndValue>() {
+                       @Override
+                       public int compare(LabelAndValue a, LabelAndValue b) {
+                         if (a.value.doubleValue() > b.value.doubleValue()) {
+                           return -1;
+                         } else if (a.value.doubleValue() < b.value.doubleValue()) {
+                           return 1;
+                         } else {
+                           return new BytesRef(a.label).compareTo(new BytesRef(b.label));
+                         }
+                       }
+                     });
+  }
+
+  protected void sortFacetResults(List<FacetResult> results) {
+      Collections.sort(results,
+                       new Comparator<FacetResult>() {
+                         @Override
+                         public int compare(FacetResult a, FacetResult b) {
+                           if (a.value.doubleValue() > b.value.doubleValue()) {
+                             return -1;
+                           } else if (b.value.doubleValue() > a.value.doubleValue()) {
+                             return 1;
+                           } else {
+                             return 0;
+                           }
+                         }
+                       });
+  }
+
+  protected void assertFloatValuesEquals(List<FacetResult> a, List<FacetResult> b) {
+    assertEquals(a.size(), b.size());
+    float lastValue = Float.POSITIVE_INFINITY;
+    Map<String,FacetResult> aByDim = new HashMap<String,FacetResult>();
+    for(int i=0;i<a.size();i++) {
+      assertTrue(a.get(i).value.floatValue() <= lastValue);
+      lastValue = a.get(i).value.floatValue();
+      aByDim.put(a.get(i).dim, a.get(i));
+    }
+    lastValue = Float.POSITIVE_INFINITY;
+    Map<String,FacetResult> bByDim = new HashMap<String,FacetResult>();
+    for(int i=0;i<b.size();i++) {
+      bByDim.put(b.get(i).dim, b.get(i));
+      assertTrue(b.get(i).value.floatValue() <= lastValue);
+      lastValue = b.get(i).value.floatValue();
+    }
+    for(String dim : aByDim.keySet()) {
+      assertFloatValuesEquals(aByDim.get(dim), bByDim.get(dim));
+    }
+  }
+
+  protected void assertFloatValuesEquals(FacetResult a, FacetResult b) {
+    assertEquals(a.dim, b.dim);
+    assertTrue(Arrays.equals(a.path, b.path));
+    assertEquals(a.childCount, b.childCount);
+    assertEquals(a.value.floatValue(), b.value.floatValue(), a.value.floatValue()/1e5);
+    assertEquals(a.labelValues.length, b.labelValues.length);
+    for(int i=0;i<a.labelValues.length;i++) {
+      assertEquals(a.labelValues[i].label, b.labelValues[i].label);
+      assertEquals(a.labelValues[i].value.floatValue(), b.labelValues[i].value.floatValue(), a.labelValues[i].value.floatValue()/1e5);
+    }
+  }
 }


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java	2013-03-20 06:26:06.983245399 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,38 +0,0 @@
-package org.apache.lucene.facet;
-
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class FacetTestUtils {
-
-  public static String toSimpleString(FacetResult fr) {
-    StringBuilder sb = new StringBuilder();
-    toSimpleString(fr.getFacetRequest().categoryPath.length, 0, sb, fr.getFacetResultNode(), "");
-    return sb.toString();
-  }
-  
-  private static void toSimpleString(int startLength, int depth, StringBuilder sb, FacetResultNode node, String indent) {
-    sb.append(indent + node.label.components[startLength+depth-1] + " (" + (int) node.value + ")\n");
-    for (FacetResultNode childNode : node.subResults) {
-      toSimpleString(startLength, depth + 1, sb, childNode, indent + "  ");
-    }
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/old/AdaptiveAccumulatorTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/old/AdaptiveAccumulatorTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/old/AdaptiveAccumulatorTest.java	2013-07-29 13:55:02.605707540 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/old/AdaptiveAccumulatorTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,38 +0,0 @@
-package org.apache.lucene.facet.old;
-
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.sampling.BaseSampleTestTopK;
-import org.apache.lucene.facet.sampling.Sampler;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.util.LuceneTestCase.Slow;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-@Slow
-public class AdaptiveAccumulatorTest extends BaseSampleTestTopK {
-
-  @Override
-  protected OldFacetsAccumulator getSamplingAccumulator(Sampler sampler, TaxonomyReader taxoReader, 
-      IndexReader indexReader, FacetSearchParams searchParams) {
-    AdaptiveFacetsAccumulator res = new AdaptiveFacetsAccumulator(searchParams, indexReader, taxoReader);
-    res.setSampler(sampler);
-    return res;
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/old/TestScoredDocIDsUtils.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/old/TestScoredDocIDsUtils.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/old/TestScoredDocIDsUtils.java	2013-10-11 20:18:48.350056241 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/old/TestScoredDocIDsUtils.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,154 +0,0 @@
-package org.apache.lucene.facet.old;
-
-import java.io.IOException;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.DocIdSet;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.FixedBitSet;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestScoredDocIDsUtils extends FacetTestCase {
-
-  @Test
-  public void testComplementIterator() throws Exception {
-    final int n = atLeast(10000);
-    final FixedBitSet bits = new FixedBitSet(n);
-    Random random = random();
-    for (int i = 0; i < n; i++) {
-      int idx = random.nextInt(n);
-      bits.flip(idx, idx + 1);
-    }
-    
-    FixedBitSet verify = bits.clone();
-
-    ScoredDocIDs scoredDocIDs = ScoredDocIdsUtils.createScoredDocIds(bits, n); 
-
-    Directory dir = newDirectory();
-    IndexReader reader = createReaderWithNDocs(random, n, dir);
-    try { 
-      assertEquals(n - verify.cardinality(), ScoredDocIdsUtils.getComplementSet(scoredDocIDs, reader).size());
-    } finally {
-      reader.close();
-      dir.close();
-    }
-  }
-
-  @Test
-  public void testAllDocs() throws Exception {
-    int maxDoc = 3;
-    Directory dir = newDirectory();
-    IndexReader reader = createReaderWithNDocs(random(), maxDoc, dir);
-    try {
-      ScoredDocIDs all = ScoredDocIdsUtils.createAllDocsScoredDocIDs(reader);
-      assertEquals("invalid size", maxDoc, all.size());
-      ScoredDocIDsIterator iter = all.iterator();
-      int doc = 0;
-      while (iter.next()) {
-        assertEquals("invalid doc ID: " + iter.getDocID(), doc++, iter.getDocID());
-        assertEquals("invalid score: " + iter.getScore(), ScoredDocIDsIterator.DEFAULT_SCORE, iter.getScore(), 0.0f);
-      }
-      assertEquals("invalid maxDoc: " + doc, maxDoc, doc);
-      
-      DocIdSet docIDs = all.getDocIDs();
-      assertTrue("should be cacheable", docIDs.isCacheable());
-      DocIdSetIterator docIDsIter = docIDs.iterator();
-      assertEquals("nextDoc() hasn't been called yet", -1, docIDsIter.docID());
-      assertEquals(0, docIDsIter.nextDoc());
-      assertEquals(1, docIDsIter.advance(1));
-      // if advance is smaller than current doc, advance to cur+1.
-      assertEquals(2, docIDsIter.advance(0));
-    } finally {
-      reader.close();
-      dir.close();
-    }
-  }
-  
-  /**
-   * Creates an index with n documents, this method is meant for testing purposes ONLY
-   */
-  static IndexReader createReaderWithNDocs(Random random, int nDocs, Directory directory) throws IOException {
-    return createReaderWithNDocs(random, nDocs, new DocumentFactory(nDocs), directory);
-  }
-
-  private static class DocumentFactory {
-    protected final static String field = "content";
-    protected final static String delTxt = "delete";
-    protected final static String alphaTxt = "alpha";
-    
-    private final static Field deletionMark = new StringField(field, delTxt, Field.Store.NO);
-    private final static Field alphaContent = new StringField(field, alphaTxt, Field.Store.NO);
-    
-    public DocumentFactory(int totalNumDocs) {
-    }
-    
-    public boolean markedDeleted(int docNum) {
-      return false;
-    }
-
-    public Document getDoc(int docNum) {
-      Document doc = new Document();
-      if (markedDeleted(docNum)) {
-        doc.add(deletionMark);
-        // Add a special field for docs that are marked for deletion. Later we
-        // assert that those docs are not returned by all-scored-doc-IDs.
-        FieldType ft = new FieldType();
-        ft.setStored(true);
-        doc.add(new Field("del", Integer.toString(docNum), ft));
-      }
-
-      if (haveAlpha(docNum)) {
-        doc.add(alphaContent);
-      }
-      return doc;
-    }
-
-    public boolean haveAlpha(int docNum) {
-      return false;
-    }
-  }
-
-  static IndexReader createReaderWithNDocs(Random random, int nDocs, DocumentFactory docFactory, Directory dir) throws IOException {
-    RandomIndexWriter writer = new RandomIndexWriter(random, dir,
-        newIndexWriterConfig(random, TEST_VERSION_CURRENT,
-            new MockAnalyzer(random, MockTokenizer.KEYWORD, false)));
-    for (int docNum = 0; docNum < nDocs; docNum++) {
-      writer.addDocument(docFactory.getDoc(docNum));
-    }
-    // Delete documents marked for deletion
-    writer.deleteDocuments(new Term(DocumentFactory.field, DocumentFactory.delTxt));
-    writer.close();
-
-    // Open a fresh read-only reader with the deletions in place
-    return DirectoryReader.open(dir);
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/params/CategoryListParamsTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/params/CategoryListParamsTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/params/CategoryListParamsTest.java	2013-02-20 13:38:17.396711928 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/params/CategoryListParamsTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,91 +0,0 @@
-package org.apache.lucene.facet.params;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.encoding.DGapVInt8IntEncoder;
-import org.apache.lucene.facet.encoding.IntDecoder;
-import org.apache.lucene.facet.encoding.IntEncoder;
-import org.apache.lucene.facet.encoding.SortingIntEncoder;
-import org.apache.lucene.facet.encoding.UniqueValuesIntEncoder;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class CategoryListParamsTest extends FacetTestCase {
-
-  @Test
-  public void testDefaultSettings() {
-    CategoryListParams clp = new CategoryListParams();
-    assertEquals("wrong default field", "$facets", clp.field);
-    IntEncoder encoder = new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapVInt8IntEncoder()));
-    IntDecoder decoder = encoder.createMatchingDecoder();
-    assertEquals("unexpected default encoder", encoder.toString(), clp.createEncoder().toString());
-    assertEquals("unexpected default decoder", decoder.toString(), clp.createEncoder().createMatchingDecoder().toString());
-  }
-  
-  /**
-   * Test that the {@link CategoryListParams#hashCode()} and
-   * {@link CategoryListParams#equals(Object)} are consistent.
-   */
-  @Test
-  public void testIdentity() {
-    CategoryListParams clParams1 = new CategoryListParams();
-    // Assert identity is correct - a CategoryListParams equals itself.
-    assertEquals("A CategoryListParams object does not equal itself.",
-        clParams1, clParams1);
-    // For completeness, the object's hashcode equals itself
-    assertEquals("A CategoryListParams object's hashCode does not equal itself.",
-        clParams1.hashCode(), clParams1.hashCode());
-  }
-
-  /**
-   * Test that CategoryListParams behave correctly when compared against each
-   * other.
-   */
-  @Test
-  public void testIdentityConsistency() {
-    // Test 2 CategoryListParams with the default parameter
-    CategoryListParams clParams1 = new CategoryListParams();
-    CategoryListParams clParams2 = new CategoryListParams();
-    assertEquals(
-        "2 CategoryListParams with the same default term should equal each other.",
-        clParams1, clParams2);
-    assertEquals("2 CategoryListParams with the same default term should have the same hashcode",
-        clParams1.hashCode(), clParams2.hashCode());
-
-    // Test 2 CategoryListParams with the same specified Term
-    clParams1 = new CategoryListParams("test");
-    clParams2 = new CategoryListParams("test");
-    assertEquals(
-        "2 CategoryListParams with the same term should equal each other.",
-        clParams1, clParams2);
-    assertEquals("2 CategoryListParams with the same term should have the same hashcode",
-        clParams1.hashCode(), clParams2.hashCode());
-    
-    // Test 2 CategoryListParams with DIFFERENT terms
-    clParams1 = new CategoryListParams("test1");
-    clParams2 = new CategoryListParams("test2");
-    assertFalse(
-        "2 CategoryListParams with the different terms should NOT equal each other.",
-        clParams1.equals(clParams2));
-    assertFalse(
-        "2 CategoryListParams with the different terms should NOT have the same hashcode.",
-        clParams1.hashCode() == clParams2.hashCode());
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/params/FacetIndexingParamsTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/params/FacetIndexingParamsTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/params/FacetIndexingParamsTest.java	2013-02-20 13:38:17.396711928 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/params/FacetIndexingParamsTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,68 +0,0 @@
-package org.apache.lucene.facet.params;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.DrillDownQuery;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.util.PartitionsUtils;
-import org.apache.lucene.index.Term;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class FacetIndexingParamsTest extends FacetTestCase {
-
-  @Test
-  public void testDefaultSettings() {
-    FacetIndexingParams dfip = FacetIndexingParams.DEFAULT;
-    assertNotNull("Missing default category list", dfip.getAllCategoryListParams());
-    assertEquals("all categories have the same CategoryListParams by default",
-        dfip.getCategoryListParams(null), dfip.getCategoryListParams(new CategoryPath("a")));
-    assertEquals("Expected default category list field is $facets", "$facets", dfip.getCategoryListParams(null).field);
-    String expectedDDText = "a"
-        + dfip.getFacetDelimChar() + "b";
-    CategoryPath cp = new CategoryPath("a", "b");
-    assertEquals("wrong drill-down term", new Term("$facets",
-        expectedDDText), DrillDownQuery.term(dfip,cp));
-    char[] buf = new char[20];
-    int numchars = dfip.drillDownTermText(cp, buf);
-    assertEquals("3 characters should be written", 3, numchars);
-    assertEquals("wrong drill-down term text", expectedDDText, new String(
-        buf, 0, numchars));
-    assertEquals("partition for all ordinals is the first", "", 
-        PartitionsUtils.partitionNameByOrdinal(dfip, 250));
-    assertEquals("for partition 0, the same name should be returned",
-        "", PartitionsUtils.partitionName(0));
-    assertEquals(
-        "for any other, it's the concatenation of name + partition",
-        PartitionsUtils.PART_NAME_PREFIX + "1", PartitionsUtils.partitionName(1));
-    assertEquals("default partition number is always 0", 0, 
-        PartitionsUtils.partitionNumber(dfip,100));
-    assertEquals("default partition size is unbounded", Integer.MAX_VALUE,
-        dfip.getPartitionSize());
-  }
-
-  @Test
-  public void testCategoryListParamsWithDefaultIndexingParams() {
-    CategoryListParams clp = new CategoryListParams("clp");
-    FacetIndexingParams dfip = new FacetIndexingParams(clp);
-    assertEquals("Expected default category list field is " + clp.field, clp.field, dfip.getCategoryListParams(null).field);
-  }
-
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/params/FacetSearchParamsTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/params/FacetSearchParamsTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/params/FacetSearchParamsTest.java	2013-02-20 13:38:17.396711928 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/params/FacetSearchParamsTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,34 +0,0 @@
-package org.apache.lucene.facet.params;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class FacetSearchParamsTest extends FacetTestCase {
-
-  @Test
-  public void testSearchParamsWithNullRequest() throws Exception {
-    try {
-      assertNull(new FacetSearchParams());
-      fail("FacetSearchParams should throw IllegalArgumentException when not adding requests");
-    } catch (IllegalArgumentException e) {
-    }
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/params/PerDimensionIndexingParamsTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/params/PerDimensionIndexingParamsTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/params/PerDimensionIndexingParamsTest.java	2013-02-20 13:38:17.396711928 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/params/PerDimensionIndexingParamsTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,64 +0,0 @@
-package org.apache.lucene.facet.params;
-
-import java.util.Collections;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.PerDimensionIndexingParams;
-import org.apache.lucene.facet.search.DrillDownQuery;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.util.PartitionsUtils;
-import org.apache.lucene.index.Term;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class PerDimensionIndexingParamsTest extends FacetTestCase {
-
-  @Test
-  public void testTopLevelSettings() {
-    FacetIndexingParams ifip = new PerDimensionIndexingParams(Collections.<CategoryPath, CategoryListParams>emptyMap());
-    assertNotNull("Missing default category list", ifip.getAllCategoryListParams());
-    assertEquals("Expected default category list field is $facets", "$facets", ifip.getCategoryListParams(null).field);
-    String expectedDDText = "a" + ifip.getFacetDelimChar() + "b";
-    CategoryPath cp = new CategoryPath("a", "b");
-    assertEquals("wrong drill-down term", new Term("$facets", expectedDDText), DrillDownQuery.term(ifip,cp));
-    char[] buf = new char[20];
-    int numchars = ifip.drillDownTermText(cp, buf);
-    assertEquals("3 characters should be written", 3, numchars);
-    assertEquals("wrong drill-down term text", expectedDDText, new String(buf, 0, numchars));
-    
-    assertEquals("partition for all ordinals is the first", "", PartitionsUtils.partitionNameByOrdinal(ifip, 250));
-    assertEquals("for partition 0, the same name should be returned", "", PartitionsUtils.partitionName(0));
-    assertEquals("for any other, it's the concatenation of name + partition", PartitionsUtils.PART_NAME_PREFIX + "1", PartitionsUtils.partitionName(1));
-    assertEquals("default partition number is always 0", 0, PartitionsUtils.partitionNumber(ifip,100));
-    assertEquals("default partition size is unbounded", Integer.MAX_VALUE, ifip.getPartitionSize());
-  }
-
-  @Test
-  public void testCategoryListParamsAddition() {
-    CategoryListParams clp = new CategoryListParams("clp");
-    PerDimensionIndexingParams tlfip = new PerDimensionIndexingParams(
-        Collections.<CategoryPath,CategoryListParams> singletonMap(new CategoryPath("a"), clp));
-    assertEquals("Expected category list field is " + clp.field, 
-        clp.field, tlfip.getCategoryListParams(new CategoryPath("a")).field);
-    assertNotSame("Unexpected default category list " + clp.field, clp, tlfip.getCategoryListParams(null));
-  }
-
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeAccumulator.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeAccumulator.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeAccumulator.java	2013-11-21 06:16:28.531024645 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeAccumulator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,677 +0,0 @@
-package org.apache.lucene.facet.range;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.DoubleDocValuesField;
-import org.apache.lucene.document.DoubleField;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FloatDocValuesField;
-import org.apache.lucene.document.FloatField;
-import org.apache.lucene.document.LongField;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.FacetTestUtils;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.CountFacetRequest;
-import org.apache.lucene.facet.search.DrillDownQuery;
-import org.apache.lucene.facet.search.DrillSideways;
-import org.apache.lucene.facet.search.DrillSideways.DrillSidewaysResult;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.search.FacetsAccumulator;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetFields;
-import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.NumericRangeQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-
-public class TestRangeAccumulator extends FacetTestCase {
-
-  public void testBasicLong() throws Exception {
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), d);
-    Document doc = new Document();
-    NumericDocValuesField field = new NumericDocValuesField("field", 0L);
-    doc.add(field);
-    for(long l=0;l<100;l++) {
-      field.setLongValue(l);
-      w.addDocument(doc);
-    }
-    field.setLongValue(Long.MAX_VALUE);
-    w.addDocument(doc);
-
-    IndexReader r = w.getReader();
-    w.close();
-
-    RangeAccumulator a = new RangeAccumulator(new RangeFacetRequest<LongRange>("field",
-        new LongRange("less than 10", 0L, true, 10L, false),
-        new LongRange("less than or equal to 10", 0L, true, 10L, true),
-        new LongRange("over 90", 90L, false, 100L, false),
-        new LongRange("90 or above", 90L, true, 100L, false),
-        new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, true)));
-    
-    FacetsCollector fc = FacetsCollector.create(a);
-
-    IndexSearcher s = newSearcher(r);
-    s.search(new MatchAllDocsQuery(), fc);
-    List<FacetResult> result = fc.getFacetResults();
-    assertEquals(1, result.size());
-    assertEquals("field (0)\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (1)\n", FacetTestUtils.toSimpleString(result.get(0)));
-    
-    r.close();
-    d.close();
-  }
-
-  /** Tests single request that mixes Range and non-Range
-   *  faceting, with DrillSideways and taxonomy. */
-  public void testMixedRangeAndNonRangeTaxonomy() throws Exception {
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), d);
-    Directory td = newDirectory();
-    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(td, IndexWriterConfig.OpenMode.CREATE);
-    FacetFields ff = new FacetFields(tw);
-
-    for (long l = 0; l < 100; l++) {
-      Document doc = new Document();
-      // For computing range facet counts:
-      doc.add(new NumericDocValuesField("field", l));
-      // For drill down by numeric range:
-      doc.add(new LongField("field", l, Field.Store.NO));
-
-      CategoryPath cp;
-      if ((l&3) == 0) {
-        cp = new CategoryPath("dim", "a");
-      } else {
-        cp = new CategoryPath("dim", "b");
-      }
-      ff.addFields(doc, Collections.singletonList(cp));
-      w.addDocument(doc);
-    }
-
-    final IndexReader r = w.getReader();
-    w.close();
-
-    final TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
-    tw.close();
-
-    IndexSearcher s = newSearcher(r);
-
-    final CountFacetRequest countRequest = new CountFacetRequest(new CategoryPath("dim"), 2);
-    final RangeFacetRequest<LongRange> rangeRequest = new RangeFacetRequest<LongRange>("field",
-                          new LongRange("less than 10", 0L, true, 10L, false),
-                          new LongRange("less than or equal to 10", 0L, true, 10L, true),
-                          new LongRange("over 90", 90L, false, 100L, false),
-                          new LongRange("90 or above", 90L, true, 100L, false),
-                          new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, false));
-    FacetSearchParams fsp = new FacetSearchParams(countRequest, rangeRequest);
-    
-    final Set<String> dimSeen = new HashSet<String>();
-
-    DrillSideways ds = new DrillSideways(s, tr) {
-        @Override
-        protected FacetsAccumulator getDrillDownAccumulator(FacetSearchParams fsp) {
-          checkSeen(fsp);
-          return FacetsAccumulator.create(fsp, r, tr, null);
-        }
-
-        @Override
-        protected FacetsAccumulator getDrillSidewaysAccumulator(String dim, FacetSearchParams fsp) {
-          checkSeen(fsp);
-          return FacetsAccumulator.create(fsp, r, tr, null);
-        }
-
-        private void checkSeen(FacetSearchParams fsp) {
-          // Each dim should up only once, across
-          // both drillDown and drillSideways requests:
-          for(FacetRequest fr : fsp.facetRequests) {
-            String dim = fr.categoryPath.components[0];
-            assertFalse("dim " + dim + " already seen", dimSeen.contains(dim));
-            dimSeen.add(dim);
-          }
-        }
-
-        @Override
-        protected boolean scoreSubDocsAtOnce() {
-          return random().nextBoolean();
-        }
-      };
-
-    // First search, no drill downs:
-    DrillDownQuery ddq = new DrillDownQuery(FacetIndexingParams.DEFAULT, new MatchAllDocsQuery());
-    DrillSidewaysResult dsr = ds.search(null, ddq, 10, fsp);
-
-    assertEquals(100, dsr.hits.totalHits);
-    assertEquals(2, dsr.facetResults.size());
-    assertEquals("dim (0)\n  b (75)\n  a (25)\n", FacetTestUtils.toSimpleString(dsr.facetResults.get(0)));
-    assertEquals("field (0)\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n", FacetTestUtils.toSimpleString(dsr.facetResults.get(1)));
-
-    // Second search, drill down on dim=b:
-    ddq = new DrillDownQuery(FacetIndexingParams.DEFAULT, new MatchAllDocsQuery());
-    ddq.add(new CategoryPath("dim", "b"));
-    dimSeen.clear();
-    dsr = ds.search(null, ddq, 10, fsp);
-
-    assertEquals(75, dsr.hits.totalHits);
-    assertEquals(2, dsr.facetResults.size());
-    assertEquals("dim (0)\n  b (75)\n  a (25)\n", FacetTestUtils.toSimpleString(dsr.facetResults.get(0)));
-    assertEquals("field (0)\n  less than 10 (7)\n  less than or equal to 10 (8)\n  over 90 (7)\n  90 or above (8)\n  over 1000 (0)\n", FacetTestUtils.toSimpleString(dsr.facetResults.get(1)));
-
-    // Third search, drill down on "less than or equal to 10":
-    ddq = new DrillDownQuery(FacetIndexingParams.DEFAULT, new MatchAllDocsQuery());
-    ddq.add("field", NumericRangeQuery.newLongRange("field", 0L, 10L, true, true));
-    dimSeen.clear();
-    dsr = ds.search(null, ddq, 10, fsp);
-
-    assertEquals(11, dsr.hits.totalHits);
-    assertEquals(2, dsr.facetResults.size());
-    assertEquals("dim (0)\n  b (8)\n  a (3)\n", FacetTestUtils.toSimpleString(dsr.facetResults.get(0)));
-    assertEquals("field (0)\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n", FacetTestUtils.toSimpleString(dsr.facetResults.get(1)));
-
-    IOUtils.close(tr, td, r, d);
-  }
-
-  /** Tests single request that mixes Range and non-Range
-   *  faceting, with DrillSideways and SortedSet. */
-  public void testMixedRangeAndNonRangeSortedSet() throws Exception {
-    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), d);
-    SortedSetDocValuesFacetFields ff = new SortedSetDocValuesFacetFields();
-
-    for (long l = 0; l < 100; l++) {
-      Document doc = new Document();
-      // For computing range facet counts:
-      doc.add(new NumericDocValuesField("field", l));
-      // For drill down by numeric range:
-      doc.add(new LongField("field", l, Field.Store.NO));
-
-      CategoryPath cp;
-      if ((l&3) == 0) {
-        cp = new CategoryPath("dim", "a");
-      } else {
-        cp = new CategoryPath("dim", "b");
-      }
-      ff.addFields(doc, Collections.singletonList(cp));
-      w.addDocument(doc);
-    }
-
-    final IndexReader r = w.getReader();
-    w.close();
-
-    IndexSearcher s = newSearcher(r);
-    final SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(s.getIndexReader());
-
-    final CountFacetRequest countRequest = new CountFacetRequest(new CategoryPath("dim"), 2);
-    final RangeFacetRequest<LongRange> rangeRequest = new RangeFacetRequest<LongRange>("field",
-                          new LongRange("less than 10", 0L, true, 10L, false),
-                          new LongRange("less than or equal to 10", 0L, true, 10L, true),
-                          new LongRange("over 90", 90L, false, 100L, false),
-                          new LongRange("90 or above", 90L, true, 100L, false),
-                          new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, false));
-    FacetSearchParams fsp = new FacetSearchParams(countRequest, rangeRequest);
-    
-    final Set<String> dimSeen = new HashSet<String>();
-
-    DrillSideways ds = new DrillSideways(s, state) {
-        @Override
-        protected FacetsAccumulator getDrillDownAccumulator(FacetSearchParams fsp) throws IOException {
-          checkSeen(fsp);
-          return FacetsAccumulator.create(fsp, state, null);
-        }
-
-        @Override
-        protected FacetsAccumulator getDrillSidewaysAccumulator(String dim, FacetSearchParams fsp) throws IOException {
-          checkSeen(fsp);
-          return FacetsAccumulator.create(fsp, state, null);
-        }
-
-        private void checkSeen(FacetSearchParams fsp) {
-          // Each dim should up only once, across
-          // both drillDown and drillSideways requests:
-          for(FacetRequest fr : fsp.facetRequests) {
-            String dim = fr.categoryPath.components[0];
-            assertFalse("dim " + dim + " already seen", dimSeen.contains(dim));
-            dimSeen.add(dim);
-          }
-        }
-
-        @Override
-        protected boolean scoreSubDocsAtOnce() {
-          return random().nextBoolean();
-        }
-      };
-
-    // First search, no drill downs:
-    DrillDownQuery ddq = new DrillDownQuery(FacetIndexingParams.DEFAULT, new MatchAllDocsQuery());
-    DrillSidewaysResult dsr = ds.search(null, ddq, 10, fsp);
-
-    assertEquals(100, dsr.hits.totalHits);
-    assertEquals(2, dsr.facetResults.size());
-    assertEquals("dim (0)\n  b (75)\n  a (25)\n", FacetTestUtils.toSimpleString(dsr.facetResults.get(0)));
-    assertEquals("field (0)\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n", FacetTestUtils.toSimpleString(dsr.facetResults.get(1)));
-
-    // Second search, drill down on dim=b:
-    ddq = new DrillDownQuery(FacetIndexingParams.DEFAULT, new MatchAllDocsQuery());
-    ddq.add(new CategoryPath("dim", "b"));
-    dimSeen.clear();
-    dsr = ds.search(null, ddq, 10, fsp);
-
-    assertEquals(75, dsr.hits.totalHits);
-    assertEquals(2, dsr.facetResults.size());
-    assertEquals("dim (0)\n  b (75)\n  a (25)\n", FacetTestUtils.toSimpleString(dsr.facetResults.get(0)));
-    assertEquals("field (0)\n  less than 10 (7)\n  less than or equal to 10 (8)\n  over 90 (7)\n  90 or above (8)\n  over 1000 (0)\n", FacetTestUtils.toSimpleString(dsr.facetResults.get(1)));
-
-    // Third search, drill down on "less than or equal to 10":
-    ddq = new DrillDownQuery(FacetIndexingParams.DEFAULT, new MatchAllDocsQuery());
-    ddq.add("field", NumericRangeQuery.newLongRange("field", 0L, 10L, true, true));
-    dimSeen.clear();
-    dsr = ds.search(null, ddq, 10, fsp);
-
-    assertEquals(11, dsr.hits.totalHits);
-    assertEquals(2, dsr.facetResults.size());
-    assertEquals("dim (0)\n  b (8)\n  a (3)\n", FacetTestUtils.toSimpleString(dsr.facetResults.get(0)));
-    assertEquals("field (0)\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n", FacetTestUtils.toSimpleString(dsr.facetResults.get(1)));
-
-    IOUtils.close(r, d);
-  }
-
-  public void testBasicDouble() throws Exception {
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), d);
-    Document doc = new Document();
-    DoubleDocValuesField field = new DoubleDocValuesField("field", 0.0);
-    doc.add(field);
-    for(long l=0;l<100;l++) {
-      field.setDoubleValue(l);
-      w.addDocument(doc);
-    }
-
-    IndexReader r = w.getReader();
-    w.close();
-
-    RangeAccumulator a = new RangeAccumulator(new RangeFacetRequest<DoubleRange>("field",
-        new DoubleRange("less than 10", 0.0, true, 10.0, false),
-        new DoubleRange("less than or equal to 10", 0.0, true, 10.0, true),
-        new DoubleRange("over 90", 90.0, false, 100.0, false),
-        new DoubleRange("90 or above", 90.0, true, 100.0, false),
-        new DoubleRange("over 1000", 1000.0, false, Double.POSITIVE_INFINITY, false)));
-    
-    FacetsCollector fc = FacetsCollector.create(a);
-
-    IndexSearcher s = newSearcher(r);
-    s.search(new MatchAllDocsQuery(), fc);
-    List<FacetResult> result = fc.getFacetResults();
-    assertEquals(1, result.size());
-    assertEquals("field (0)\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n", FacetTestUtils.toSimpleString(result.get(0)));
-    
-    r.close();
-    d.close();
-  }
-
-  public void testBasicFloat() throws Exception {
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), d);
-    Document doc = new Document();
-    FloatDocValuesField field = new FloatDocValuesField("field", 0.0f);
-    doc.add(field);
-    for(long l=0;l<100;l++) {
-      field.setFloatValue(l);
-      w.addDocument(doc);
-    }
-
-    IndexReader r = w.getReader();
-    w.close();
-
-    RangeAccumulator a = new RangeAccumulator(new RangeFacetRequest<FloatRange>("field",
-        new FloatRange("less than 10", 0.0f, true, 10.0f, false),
-        new FloatRange("less than or equal to 10", 0.0f, true, 10.0f, true),
-        new FloatRange("over 90", 90.0f, false, 100.0f, false),
-        new FloatRange("90 or above", 90.0f, true, 100.0f, false),
-        new FloatRange("over 1000", 1000.0f, false, Float.POSITIVE_INFINITY, false)));
-    
-    FacetsCollector fc = FacetsCollector.create(a);
-
-    IndexSearcher s = newSearcher(r);
-    s.search(new MatchAllDocsQuery(), fc);
-    List<FacetResult> result = fc.getFacetResults();
-    assertEquals(1, result.size());
-    assertEquals("field (0)\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n", FacetTestUtils.toSimpleString(result.get(0)));
-    
-    r.close();
-    d.close();
-  }
-
-  public void testRandomLongs() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
-
-    int numDocs = atLeast(1000);
-    long[] values = new long[numDocs];
-    for(int i=0;i<numDocs;i++) {
-      Document doc = new Document();
-      long v = random().nextLong();
-      values[i] = v;
-      doc.add(new NumericDocValuesField("field", v));
-      doc.add(new LongField("field", v, Field.Store.NO));
-      w.addDocument(doc);
-    }
-    IndexReader r = w.getReader();
-    w.close();
-
-    IndexSearcher s = newSearcher(r);
-    
-    int numIters = atLeast(10);
-    for(int iter=0;iter<numIters;iter++) {
-      if (VERBOSE) {
-        System.out.println("TEST: iter=" + iter);
-      }
-      int numRange = _TestUtil.nextInt(random(), 1, 5);
-      LongRange[] ranges = new LongRange[numRange];
-      int[] expectedCounts = new int[numRange];
-      for(int rangeID=0;rangeID<numRange;rangeID++) {
-        long min = random().nextLong();
-        long max = random().nextLong();
-        if (min > max) {
-          long x = min;
-          min = max;
-          max = x;
-        }
-        boolean minIncl = random().nextBoolean();
-        boolean maxIncl = random().nextBoolean();
-        ranges[rangeID] = new LongRange("r" + rangeID, min, minIncl, max, maxIncl);
-
-        // Do "slow but hopefully correct" computation of
-        // expected count:
-        for(int i=0;i<numDocs;i++) {
-          boolean accept = true;
-          if (minIncl) {
-            accept &= values[i] >= min;
-          } else {
-            accept &= values[i] > min;
-          }
-          if (maxIncl) {
-            accept &= values[i] <= max;
-          } else {
-            accept &= values[i] < max;
-          }
-          if (accept) {
-            expectedCounts[rangeID]++;
-          }
-        }
-      }
-
-      FacetsCollector fc = FacetsCollector.create(new RangeAccumulator(new RangeFacetRequest<LongRange>("field", ranges)));
-      s.search(new MatchAllDocsQuery(), fc);
-      List<FacetResult> results = fc.getFacetResults();
-      assertEquals(1, results.size());
-      List<FacetResultNode> nodes = results.get(0).getFacetResultNode().subResults;
-      assertEquals(numRange, nodes.size());
-      for(int rangeID=0;rangeID<numRange;rangeID++) {
-        if (VERBOSE) {
-          System.out.println("  range " + rangeID + " expectedCount=" + expectedCounts[rangeID]);
-        }
-        FacetResultNode subNode = nodes.get(rangeID);
-        assertEquals("field/r" + rangeID, subNode.label.toString('/'));
-        assertEquals(expectedCounts[rangeID], (int) subNode.value);
-
-        LongRange range = (LongRange) ((RangeFacetRequest<?>) results.get(0).getFacetRequest()).ranges[rangeID];
-
-        // Test drill-down:
-        DrillDownQuery ddq = new DrillDownQuery(FacetIndexingParams.DEFAULT);
-        ddq.add("field", NumericRangeQuery.newLongRange("field", range.min, range.max, range.minInclusive, range.maxInclusive));
-        assertEquals(expectedCounts[rangeID], s.search(ddq, 10).totalHits);
-      }
-    }
-
-    r.close();
-    dir.close();
-  }
-
-  public void testRandomFloats() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
-
-    int numDocs = atLeast(1000);
-    float[] values = new float[numDocs];
-    for(int i=0;i<numDocs;i++) {
-      Document doc = new Document();
-      float v = random().nextFloat();
-      values[i] = v;
-      doc.add(new FloatDocValuesField("field", v));
-      doc.add(new FloatField("field", v, Field.Store.NO));
-      w.addDocument(doc);
-    }
-    IndexReader r = w.getReader();
-    w.close();
-
-    IndexSearcher s = newSearcher(r);
-    
-    int numIters = atLeast(10);
-    for(int iter=0;iter<numIters;iter++) {
-      if (VERBOSE) {
-        System.out.println("TEST: iter=" + iter);
-      }
-      int numRange = _TestUtil.nextInt(random(), 1, 5);
-      FloatRange[] ranges = new FloatRange[numRange];
-      int[] expectedCounts = new int[numRange];
-      for(int rangeID=0;rangeID<numRange;rangeID++) {
-        float min = random().nextFloat();
-        float max = random().nextFloat();
-        if (min > max) {
-          float x = min;
-          min = max;
-          max = x;
-        }
-        boolean minIncl = random().nextBoolean();
-        boolean maxIncl = random().nextBoolean();
-        ranges[rangeID] = new FloatRange("r" + rangeID, min, minIncl, max, maxIncl);
-
-        // Do "slow but hopefully correct" computation of
-        // expected count:
-        for(int i=0;i<numDocs;i++) {
-          boolean accept = true;
-          if (minIncl) {
-            accept &= values[i] >= min;
-          } else {
-            accept &= values[i] > min;
-          }
-          if (maxIncl) {
-            accept &= values[i] <= max;
-          } else {
-            accept &= values[i] < max;
-          }
-          if (accept) {
-            expectedCounts[rangeID]++;
-          }
-        }
-      }
-
-      FacetsCollector fc = FacetsCollector.create(new RangeAccumulator(new RangeFacetRequest<FloatRange>("field", ranges)));
-      s.search(new MatchAllDocsQuery(), fc);
-      List<FacetResult> results = fc.getFacetResults();
-      assertEquals(1, results.size());
-      List<FacetResultNode> nodes = results.get(0).getFacetResultNode().subResults;
-      assertEquals(numRange, nodes.size());
-      for(int rangeID=0;rangeID<numRange;rangeID++) {
-        if (VERBOSE) {
-          System.out.println("  range " + rangeID + " expectedCount=" + expectedCounts[rangeID]);
-        }
-        FacetResultNode subNode = nodes.get(rangeID);
-        assertEquals("field/r" + rangeID, subNode.label.toString('/'));
-        assertEquals(expectedCounts[rangeID], (int) subNode.value);
-
-        FloatRange range = (FloatRange) ((RangeFacetRequest<?>) results.get(0).getFacetRequest()).ranges[rangeID];
-
-        // Test drill-down:
-        DrillDownQuery ddq = new DrillDownQuery(FacetIndexingParams.DEFAULT);
-        ddq.add("field", NumericRangeQuery.newFloatRange("field", range.min, range.max, range.minInclusive, range.maxInclusive));
-        assertEquals(expectedCounts[rangeID], s.search(ddq, 10).totalHits);
-      }
-    }
-
-    r.close();
-    dir.close();
-  }
-
-  public void testRandomDoubles() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
-
-    int numDocs = atLeast(1000);
-    double[] values = new double[numDocs];
-    for(int i=0;i<numDocs;i++) {
-      Document doc = new Document();
-      double v = random().nextDouble();
-      values[i] = v;
-      doc.add(new DoubleDocValuesField("field", v));
-      doc.add(new DoubleField("field", v, Field.Store.NO));
-      w.addDocument(doc);
-    }
-    IndexReader r = w.getReader();
-    w.close();
-
-    IndexSearcher s = newSearcher(r);
-    
-    int numIters = atLeast(10);
-    for(int iter=0;iter<numIters;iter++) {
-      if (VERBOSE) {
-        System.out.println("TEST: iter=" + iter);
-      }
-      int numRange = _TestUtil.nextInt(random(), 1, 5);
-      DoubleRange[] ranges = new DoubleRange[numRange];
-      int[] expectedCounts = new int[numRange];
-      for(int rangeID=0;rangeID<numRange;rangeID++) {
-        double min = random().nextDouble();
-        double max = random().nextDouble();
-        if (min > max) {
-          double x = min;
-          min = max;
-          max = x;
-        }
-        boolean minIncl = random().nextBoolean();
-        boolean maxIncl = random().nextBoolean();
-        ranges[rangeID] = new DoubleRange("r" + rangeID, min, minIncl, max, maxIncl);
-
-        // Do "slow but hopefully correct" computation of
-        // expected count:
-        for(int i=0;i<numDocs;i++) {
-          boolean accept = true;
-          if (minIncl) {
-            accept &= values[i] >= min;
-          } else {
-            accept &= values[i] > min;
-          }
-          if (maxIncl) {
-            accept &= values[i] <= max;
-          } else {
-            accept &= values[i] < max;
-          }
-          if (accept) {
-            expectedCounts[rangeID]++;
-          }
-        }
-      }
-
-      FacetsCollector fc = FacetsCollector.create(new RangeAccumulator(new RangeFacetRequest<DoubleRange>("field", ranges)));
-      s.search(new MatchAllDocsQuery(), fc);
-      List<FacetResult> results = fc.getFacetResults();
-      assertEquals(1, results.size());
-      List<FacetResultNode> nodes = results.get(0).getFacetResultNode().subResults;
-      assertEquals(numRange, nodes.size());
-      for(int rangeID=0;rangeID<numRange;rangeID++) {
-        if (VERBOSE) {
-          System.out.println("  range " + rangeID + " expectedCount=" + expectedCounts[rangeID]);
-        }
-        FacetResultNode subNode = nodes.get(rangeID);
-        assertEquals("field/r" + rangeID, subNode.label.toString('/'));
-        assertEquals(expectedCounts[rangeID], (int) subNode.value);
-
-        DoubleRange range = (DoubleRange) ((RangeFacetRequest<?>) results.get(0).getFacetRequest()).ranges[rangeID];
-
-        // Test drill-down:
-        DrillDownQuery ddq = new DrillDownQuery(FacetIndexingParams.DEFAULT);
-        ddq.add("field", NumericRangeQuery.newDoubleRange("field", range.min, range.max, range.minInclusive, range.maxInclusive));
-        assertEquals(expectedCounts[rangeID], s.search(ddq, 10).totalHits);
-      }
-    }
-
-    r.close();
-    dir.close();
-  }
-
-  // LUCENE-5178
-  public void testMissingValues() throws Exception {
-    assumeTrue("codec does not support docsWithField", defaultCodecSupportsDocsWithField());
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), d);
-    Document doc = new Document();
-    NumericDocValuesField field = new NumericDocValuesField("field", 0L);
-    doc.add(field);
-    for(long l=0;l<100;l++) {
-      if (l % 5 == 0) {
-        // Every 5th doc is missing the value:
-        w.addDocument(new Document());
-        continue;
-      }
-      field.setLongValue(l);
-      w.addDocument(doc);
-    }
-
-    IndexReader r = w.getReader();
-    w.close();
-
-    RangeAccumulator a = new RangeAccumulator(new RangeFacetRequest<LongRange>("field",
-        new LongRange("less than 10", 0L, true, 10L, false),
-        new LongRange("less than or equal to 10", 0L, true, 10L, true),
-        new LongRange("over 90", 90L, false, 100L, false),
-        new LongRange("90 or above", 90L, true, 100L, false),
-        new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, false)));
-    
-    FacetsCollector fc = FacetsCollector.create(a);
-
-    IndexSearcher s = newSearcher(r);
-    s.search(new MatchAllDocsQuery(), fc);
-    List<FacetResult> result = fc.getFacetResults();
-    assertEquals(1, result.size());
-    assertEquals("field (0)\n  less than 10 (8)\n  less than or equal to 10 (8)\n  over 90 (8)\n  90 or above (8)\n  over 1000 (0)\n", FacetTestUtils.toSimpleString(result.get(0)));
-    
-    r.close();
-    d.close();
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/sampling/BaseSampleTestTopK.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/sampling/BaseSampleTestTopK.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/sampling/BaseSampleTestTopK.java	2013-07-29 13:55:02.601707542 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/sampling/BaseSampleTestTopK.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,149 +0,0 @@
-package org.apache.lucene.facet.sampling;
-
-import java.util.List;
-import java.util.Random;
-
-import org.apache.lucene.facet.old.OldFacetsAccumulator;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.BaseTestTopK;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetRequest.ResultMode;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public abstract class BaseSampleTestTopK extends BaseTestTopK {
-  
-  /** Number of top results */
-  protected static final int K = 2; 
-  
-  /** since there is a chance that this test would fail even if the code is correct, retry the sampling */
-  protected static final int RETRIES = 10;
-  
-  @Override
-  protected FacetSearchParams searchParamsWithRequests(int numResults, FacetIndexingParams fip) {
-    FacetSearchParams res = super.searchParamsWithRequests(numResults, fip);
-    for (FacetRequest req : res.facetRequests) {
-      // randomize the way we aggregate results
-      if (random().nextBoolean()) {
-        req.setResultMode(ResultMode.GLOBAL_FLAT);
-      } else {
-        req.setResultMode(ResultMode.PER_NODE_IN_TREE);
-      }
-    }
-    return res;
-  }
-  
-  protected abstract OldFacetsAccumulator getSamplingAccumulator(Sampler sampler, TaxonomyReader taxoReader, 
-      IndexReader indexReader, FacetSearchParams searchParams);
-  
-  /**
-   * Try out faceted search with sampling enabled and complements either disabled or enforced
-   * Lots of randomly generated data is being indexed, and later on a "90% docs" faceted search
-   * is performed. The results are compared to non-sampled ones.
-   */
-  public void testCountUsingSampling() throws Exception {
-    boolean useRandomSampler = random().nextBoolean();
-    for (int partitionSize : partitionSizes) {
-      try {
-        // complements return counts for all ordinals, so force ALL_PARENTS indexing
-        // so that it's easier to compare
-        FacetIndexingParams fip = getFacetIndexingParams(partitionSize, true);
-        initIndex(fip);
-        // Get all of the documents and run the query, then do different
-        // facet counts and compare to control
-        Query q = new TermQuery(new Term(CONTENT_FIELD, BETA)); // 90% of the docs
-        
-        FacetSearchParams expectedSearchParams = searchParamsWithRequests(K, fip); 
-        FacetsCollector fc = FacetsCollector.create(expectedSearchParams, indexReader, taxoReader);
-        
-        searcher.search(q, fc);
-        
-        List<FacetResult> expectedResults = fc.getFacetResults();
-        
-        FacetSearchParams samplingSearchParams = searchParamsWithRequests(K, fip); 
-        
-        // try several times in case of failure, because the test has a chance to fail 
-        // if the top K facets are not sufficiently common with the sample set
-        for (int nTrial = 0; nTrial < RETRIES; nTrial++) {
-          try {
-            // complement with sampling!
-            final Sampler sampler = createSampler(nTrial, useRandomSampler, samplingSearchParams);
-            
-            assertSampling(expectedResults, q, sampler, samplingSearchParams, false);
-            assertSampling(expectedResults, q, sampler, samplingSearchParams, true);
-            
-            break; // succeeded
-          } catch (AssertionError e) {
-            if (nTrial >= RETRIES - 1) {
-              throw e; // no more retries allowed, must fail
-            }
-          }
-        }
-      } finally { 
-        closeAll();
-      }
-    }
-  }
-  
-  private void assertSampling(List<FacetResult> expected, Query q, Sampler sampler, FacetSearchParams params, boolean complement) throws Exception {
-    FacetsCollector samplingFC = samplingCollector(complement, sampler, params);
-    
-    searcher.search(q, samplingFC);
-    List<FacetResult> sampledResults = samplingFC.getFacetResults();
-    
-    assertSameResults(expected, sampledResults);
-  }
-  
-  private FacetsCollector samplingCollector(final boolean complement, final Sampler sampler,
-      FacetSearchParams samplingSearchParams) {
-    OldFacetsAccumulator sfa = getSamplingAccumulator(sampler, taxoReader, indexReader, samplingSearchParams);
-    sfa.setComplementThreshold(complement ? OldFacetsAccumulator.FORCE_COMPLEMENT : OldFacetsAccumulator.DISABLE_COMPLEMENT);
-    return FacetsCollector.create(sfa);
-  }
-  
-  private Sampler createSampler(int nTrial, boolean useRandomSampler, FacetSearchParams sParams) {
-    SamplingParams samplingParams = new SamplingParams();
-    
-    /*
-     * Set sampling to Exact fixing with TakmiSampleFixer as it is not easy to
-     * validate results with amortized results. 
-     */
-    samplingParams.setSampleFixer(new TakmiSampleFixer(indexReader, taxoReader, sParams));
-        
-    final double retryFactor = Math.pow(1.01, nTrial);
-    samplingParams.setOversampleFactor(5.0 * retryFactor); // Oversampling 
-    samplingParams.setSampleRatio(0.8 * retryFactor);
-    samplingParams.setMinSampleSize((int) (100 * retryFactor));
-    samplingParams.setMaxSampleSize((int) (10000 * retryFactor));
-    samplingParams.setSamplingThreshold(11000); //force sampling
-
-    Sampler sampler = useRandomSampler ? 
-        new RandomSampler(samplingParams, new Random(random().nextLong())) :
-          new RepeatableSampler(samplingParams);
-    return sampler;
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/sampling/OversampleWithDepthTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/sampling/OversampleWithDepthTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/sampling/OversampleWithDepthTest.java	2013-07-29 13:55:02.601707542 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/sampling/OversampleWithDepthTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,124 +0,0 @@
-package org.apache.lucene.facet.sampling;
-
-import java.io.IOException;
-import java.util.Collections;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.old.OldFacetsAccumulator;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.CountFacetRequest;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetRequest.ResultMode;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class OversampleWithDepthTest extends FacetTestCase {
-  
-  @Test
-  public void testCountWithdepthUsingSampling() throws Exception, IOException {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    FacetIndexingParams fip = new FacetIndexingParams(randomCategoryListParams());
-    
-    // index 100 docs, each with one category: ["root", docnum/10, docnum]
-    // e.g. root/8/87
-    index100Docs(indexDir, taxoDir, fip);
-    
-    DirectoryReader r = DirectoryReader.open(indexDir);
-    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
-    
-    CountFacetRequest facetRequest = new CountFacetRequest(new CategoryPath("root"), 10);
-    // Setting the depth to '2', should potentially get all categories
-    facetRequest.setDepth(2);
-    facetRequest.setResultMode(ResultMode.PER_NODE_IN_TREE);
-
-    FacetSearchParams fsp = new FacetSearchParams(fip, facetRequest);
-    
-    // Craft sampling params to enforce sampling
-    final SamplingParams params = new SamplingParams();
-    params.setMinSampleSize(2);
-    params.setMaxSampleSize(50);
-    params.setOversampleFactor(5);
-    params.setSamplingThreshold(60);
-    params.setSampleRatio(0.1);
-    
-    FacetResult res = searchWithFacets(r, tr, fsp, params);
-    FacetRequest req = res.getFacetRequest();
-    assertEquals(facetRequest, req);
-    
-    FacetResultNode rootNode = res.getFacetResultNode();
-    
-    // Each node below root should also have sub-results as the requested depth was '2'
-    for (FacetResultNode node : rootNode.subResults) {
-      assertTrue("node " + node.label + " should have had children as the requested depth was '2'", node.subResults.size() > 0);
-    }
-    
-    IOUtils.close(r, tr, indexDir, taxoDir);
-  }
-
-  private void index100Docs(Directory indexDir, Directory taxoDir, FacetIndexingParams fip) throws IOException {
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
-    IndexWriter w = new IndexWriter(indexDir, iwc);
-    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
-    
-    FacetFields facetFields = new FacetFields(tw, fip);
-    for (int i = 0; i < 100; i++) {
-      Document doc = new Document();
-      CategoryPath cp = new CategoryPath("root",Integer.toString(i / 10), Integer.toString(i));
-      facetFields.addFields(doc, Collections.singletonList(cp));
-      w.addDocument(doc);
-    }
-    IOUtils.close(tw, w);
-  }
-
-  /** search reader <code>r</code>*/
-  private FacetResult searchWithFacets(IndexReader r, TaxonomyReader tr, FacetSearchParams fsp, 
-      final SamplingParams params) throws IOException {
-    // a FacetsCollector with a sampling accumulator
-    Sampler sampler = new RandomSampler(params, random());
-    OldFacetsAccumulator sfa = new SamplingAccumulator(sampler, fsp, r, tr);
-    FacetsCollector fcWithSampling = FacetsCollector.create(sfa);
-    
-    IndexSearcher s = newSearcher(r);
-    s.search(new MatchAllDocsQuery(), fcWithSampling);
-    
-    // there's only one expected result, return just it.
-    return fcWithSampling.getFacetResults().get(0);
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplerTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplerTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplerTest.java	2013-07-29 13:55:02.601707542 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplerTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,111 +0,0 @@
-package org.apache.lucene.facet.sampling;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.facet.FacetTestBase;
-import org.apache.lucene.facet.old.OldFacetsAccumulator;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.CountFacetRequest;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.junit.After;
-import org.junit.Before;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class SamplerTest extends FacetTestBase {
-  
-  private FacetIndexingParams fip;
-  
-  @Override
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-    fip = getFacetIndexingParams(Integer.MAX_VALUE);
-    initIndex(fip);
-  }
-  
-  @Override
-  protected int numDocsToIndex() {
-    return 100;
-  }
-  
-  @Override
-  protected List<CategoryPath> getCategories(final int doc) {
-    return new ArrayList<CategoryPath>() {
-      {
-        add(new CategoryPath("root", "a", Integer.toString(doc % 10)));
-      }
-    };
-  }
-  
-  @Override
-  protected String getContent(int doc) {
-    return "";
-  }
-  
-  @Override
-  @After
-  public void tearDown() throws Exception {
-    closeAll();
-    super.tearDown();
-  }
-  
-  public void testDefaultFixer() throws Exception {
-    RandomSampler randomSampler = new RandomSampler();
-    SampleFixer fixer = randomSampler.samplingParams.getSampleFixer();
-    assertEquals(null, fixer);
-  }
-  
-  public void testCustomFixer() throws Exception {
-    SamplingParams sp = new SamplingParams();
-    sp.setSampleFixer(new TakmiSampleFixer(null, null, null));
-    assertEquals(TakmiSampleFixer.class, sp.getSampleFixer().getClass());
-  }
-  
-  public void testNoFixing() throws Exception {
-    SamplingParams sp = new SamplingParams();
-    sp.setMaxSampleSize(10);
-    sp.setMinSampleSize(5);
-    sp.setSampleRatio(0.01d);
-    sp.setSamplingThreshold(50);
-    sp.setOversampleFactor(5d);
-    
-    assertNull("Fixer should be null as the test is for no-fixing",
-        sp.getSampleFixer());
-    FacetSearchParams fsp = new FacetSearchParams(fip, new CountFacetRequest(
-        new CategoryPath("root", "a"), 1));
-    SamplingAccumulator accumulator = new SamplingAccumulator(
-        new RandomSampler(sp, random()), fsp, indexReader, taxoReader);
-    
-    // Make sure no complements are in action
-    accumulator
-        .setComplementThreshold(OldFacetsAccumulator.DISABLE_COMPLEMENT);
-    
-    FacetsCollector fc = FacetsCollector.create(accumulator);
-    
-    searcher.search(new MatchAllDocsQuery(), fc);
-    FacetResultNode node = fc.getFacetResults().get(0).getFacetResultNode();
-    
-    assertTrue(node.value < numDocsToIndex());
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingAccumulatorTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingAccumulatorTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingAccumulatorTest.java	2013-07-29 13:55:02.601707542 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingAccumulatorTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,34 +0,0 @@
-package org.apache.lucene.facet.sampling;
-
-import org.apache.lucene.facet.old.OldFacetsAccumulator;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.util.LuceneTestCase.Slow;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-@Slow
-public class SamplingAccumulatorTest extends BaseSampleTestTopK {
-
-  @Override
-  protected OldFacetsAccumulator getSamplingAccumulator(Sampler sampler, TaxonomyReader taxoReader, 
-      IndexReader indexReader, FacetSearchParams searchParams) {
-    return new SamplingAccumulator(sampler, searchParams, indexReader, taxoReader);
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingWrapperTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingWrapperTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingWrapperTest.java	2013-07-29 13:55:02.601707542 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingWrapperTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,35 +0,0 @@
-package org.apache.lucene.facet.sampling;
-
-import org.apache.lucene.facet.old.OldFacetsAccumulator;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.util.LuceneTestCase.Slow;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-@Slow
-public class SamplingWrapperTest extends BaseSampleTestTopK {
-
-  @Override
-  protected OldFacetsAccumulator getSamplingAccumulator(Sampler sampler, TaxonomyReader taxoReader, 
-      IndexReader indexReader, FacetSearchParams searchParams) {
-    return new SamplingWrapper(new OldFacetsAccumulator(searchParams, indexReader, taxoReader), sampler);
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/AssertingSubDocsAtOnceCollector.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/AssertingSubDocsAtOnceCollector.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/AssertingSubDocsAtOnceCollector.java	2013-07-15 15:52:17.213877399 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/AssertingSubDocsAtOnceCollector.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,67 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.Scorer.ChildScorer;
-import org.apache.lucene.search.Scorer;
-
-/** Verifies in collect() that all child subScorers are on
- *  the collected doc. */
-class AssertingSubDocsAtOnceCollector extends Collector {
-
-  // TODO: allow wrapping another Collector
-
-  List<Scorer> allScorers;
-
-  @Override
-  public void setScorer(Scorer s) {
-    // Gathers all scorers, including s and "under":
-    allScorers = new ArrayList<Scorer>();
-    allScorers.add(s);
-    int upto = 0;
-    while(upto < allScorers.size()) {
-      s = allScorers.get(upto++);
-      for (ChildScorer sub : s.getChildren()) {
-        allScorers.add(sub.child);
-      }
-    }
-  }
-
-  @Override
-  public void collect(int docID) {
-    for(Scorer s : allScorers) {
-      if (docID != s.docID()) {
-        throw new IllegalStateException("subScorer=" + s + " has docID=" + s.docID() + " != collected docID=" + docID);
-      }
-    }
-  }
-
-  @Override
-  public void setNextReader(AtomicReaderContext context) {
-  }
-
-  @Override
-  public boolean acceptsDocsOutOfOrder() {
-    return false;
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/BaseTestTopK.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/BaseTestTopK.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/BaseTestTopK.java	2013-02-20 13:38:17.424711928 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/BaseTestTopK.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,112 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.util._TestUtil;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.facet.FacetTestBase;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public abstract class BaseTestTopK extends FacetTestBase {
-
-  protected static final String ALPHA = "alpha";
-  protected static final String BETA  = "beta";
-
-  /** partition sizes on which the tests are run */
-  protected static int[] partitionSizes = new int[] { 2, 3, 100, Integer.MAX_VALUE };
-
-  /** Categories are generated from range [0,maxCategory) */
-  protected static int maxCategory = 5000;
-  private static final int categoriesPow2 = maxCategory * maxCategory;
-
-  private int currDoc;
-  private int nextInt;
-
-  @Override
-  protected void populateIndex(RandomIndexWriter iw, TaxonomyWriter taxo, FacetIndexingParams fip) throws IOException {
-    currDoc = -1;
-    super.populateIndex(iw, taxo, fip);
-  }
-  
-  /** prepare the next random int */
-  private void nextInt(int doc) {
-    if (currDoc == doc ) {
-      return;
-    }
-    currDoc = doc;
-    // the code below tries to achieve non-uniform distribution of
-    // categories. Perhaps we can use random.nextGaussian() instead,
-    // something like nextGaussian() * stdev + maxCategory/2. Or
-    // try to generate a Zipf distribution.
-    nextInt = random().nextInt(categoriesPow2);
-    nextInt = (int)Math.sqrt(nextInt);
-  }
-  
-  @Override
-  protected String getContent(int doc) {
-    nextInt(doc);
-    if (random().nextDouble() > 0.1) {
-      return ALPHA + ' ' + BETA;
-    }
-    return ALPHA;
-  }
-  
-  @Override
-  protected List<CategoryPath> getCategories(int doc) {
-    nextInt(doc);
-    CategoryPath cp = new CategoryPath(
-        "a", 
-        Integer.toString(nextInt / 1000), 
-        Integer.toString(nextInt / 100), 
-        Integer.toString(nextInt / 10));
-    if (VERBOSE) {
-      System.out.println("Adding CP: " + cp.toString());
-    }
-    return Arrays.asList(cp);
-  }
-
-  protected FacetSearchParams searchParamsWithRequests(int numResults, FacetIndexingParams fip) {
-    List<FacetRequest> facetRequests = new ArrayList<FacetRequest>();
-    facetRequests.add(new CountFacetRequest(new CategoryPath("a"), numResults));
-    facetRequests.add(new CountFacetRequest(new CategoryPath("a", "1"), numResults));
-    facetRequests.add(new CountFacetRequest(new CategoryPath("a", "1", "10"), numResults));
-    facetRequests.add(new CountFacetRequest(new CategoryPath("a", "2",  "26", "267"), numResults));
-    return getFacetSearchParams(facetRequests, fip);
-  }
-
-  @Override
-  protected int numDocsToIndex() {
-    return 20000;
-  }
-
-  @Override
-  protected IndexWriterConfig getIndexWriterConfig(Analyzer analyzer) {
-    return super.getIndexWriterConfig(analyzer).setMaxBufferedDocs(_TestUtil.nextInt(random(), 500, 10000));
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/CategoryListIteratorTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/CategoryListIteratorTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/CategoryListIteratorTest.java	2013-02-20 13:38:17.424711928 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/CategoryListIteratorTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,144 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.util.HashSet;
-import java.util.Set;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.BinaryDocValuesField;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.encoding.DGapIntEncoder;
-import org.apache.lucene.facet.encoding.IntEncoder;
-import org.apache.lucene.facet.encoding.SortingIntEncoder;
-import org.apache.lucene.facet.encoding.UniqueValuesIntEncoder;
-import org.apache.lucene.facet.encoding.VInt8IntEncoder;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class CategoryListIteratorTest extends FacetTestCase {
-
-  static final IntsRef[] data = new IntsRef[] {
-    new IntsRef(new int[] { 1, 2 }, 0, 2), 
-    new IntsRef(new int[] { 3, 4 }, 0, 2),
-    new IntsRef(new int[] { 1, 3 }, 0, 2),
-    new IntsRef(new int[] { 1, 2, 3, 4 }, 0, 4)
-  };
-
-  @Test
-  public void test() throws Exception {
-    Directory dir = newDirectory();
-    final IntEncoder encoder = randomCategoryListParams().createEncoder();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, 
-        new MockAnalyzer(random(), MockTokenizer.KEYWORD, false)).setMergePolicy(newLogMergePolicy()));
-    BytesRef buf = new BytesRef();
-    for (int i = 0; i < data.length; i++) {
-      Document doc = new Document();
-      encoder.encode(IntsRef.deepCopyOf(data[i]), buf);
-      doc.add(new BinaryDocValuesField("f", buf));
-      writer.addDocument(doc);
-    }
-    IndexReader reader = writer.getReader();
-    writer.close();
-
-    int totalCategories = 0;
-    IntsRef ordinals = new IntsRef();
-    CategoryListIterator cli = new DocValuesCategoryListIterator("f", encoder.createMatchingDecoder());
-    for (AtomicReaderContext context : reader.leaves()) {
-      assertTrue("failed to initalize iterator", cli.setNextReader(context));
-      int maxDoc = context.reader().maxDoc();
-      int dataIdx = context.docBase;
-      for (int doc = 0; doc < maxDoc; doc++, dataIdx++) {
-        Set<Integer> values = new HashSet<Integer>();
-        for (int j = 0; j < data[dataIdx].length; j++) {
-          values.add(data[dataIdx].ints[j]);
-        }
-        cli.getOrdinals(doc, ordinals);
-        assertTrue("no ordinals for document " + doc, ordinals.length > 0);
-        for (int j = 0; j < ordinals.length; j++) {
-          assertTrue("expected category not found: " + ordinals.ints[j], values.contains(ordinals.ints[j]));
-        }
-        totalCategories += ordinals.length;
-      }
-    }
-    assertEquals("Missing categories!", 10, totalCategories);
-    reader.close();
-    dir.close();
-  }
-
-  @Test
-  public void testEmptyDocuments() throws Exception {
-    Directory dir = newDirectory();
-    final IntEncoder encoder = new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new VInt8IntEncoder())));
-    // NOTE: test is wired to LogMP... because test relies on certain docids having payloads
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, 
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
-    for (int i = 0; i < data.length; i++) {
-      Document doc = new Document();
-      if (i == 0) {
-        BytesRef buf = new BytesRef();
-        encoder.encode(IntsRef.deepCopyOf(data[i]), buf );
-        doc.add(new BinaryDocValuesField("f", buf));
-      } else {
-        doc.add(new BinaryDocValuesField("f", new BytesRef()));
-      }
-      writer.addDocument(doc);
-      writer.commit();
-    }
-
-    IndexReader reader = writer.getReader();
-    writer.close();
-
-    int totalCategories = 0;
-    IntsRef ordinals = new IntsRef();
-    CategoryListIterator cli = new DocValuesCategoryListIterator("f", encoder.createMatchingDecoder());
-    for (AtomicReaderContext context : reader.leaves()) {
-      assertTrue("failed to initalize iterator", cli.setNextReader(context));
-      int maxDoc = context.reader().maxDoc();
-      int dataIdx = context.docBase;
-      for (int doc = 0; doc < maxDoc; doc++, dataIdx++) {
-        Set<Integer> values = new HashSet<Integer>();
-        for (int j = 0; j < data[dataIdx].length; j++) {
-          values.add(data[dataIdx].ints[j]);
-        }
-        cli.getOrdinals(doc, ordinals);
-        if (dataIdx == 0) {
-          assertTrue("document 0 must have ordinals", ordinals.length > 0);
-          for (int j = 0; j < ordinals.length; j++) {
-            assertTrue("expected category not found: " + ordinals.ints[j], values.contains(ordinals.ints[j]));
-          }
-          totalCategories += ordinals.length;
-        } else {
-          assertTrue("only document 0 should have ordinals", ordinals.length == 0);
-        }
-      }
-    }
-    assertEquals("Wrong number of total categories!", 2, totalCategories);
-
-    reader.close();
-    dir.close();
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/CountingFacetsAggregatorTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/CountingFacetsAggregatorTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/CountingFacetsAggregatorTest.java	2013-07-29 13:55:02.597707543 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/CountingFacetsAggregatorTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,395 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.collections.ObjectToIntMap;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.params.PerDimensionOrdinalPolicy;
-import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.NoMergePolicy;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class CountingFacetsAggregatorTest extends FacetTestCase {
-  
-  private static final Term A = new Term("f", "a");
-  private static final CategoryPath CP_A = new CategoryPath("A"), CP_B = new CategoryPath("B");
-  private static final CategoryPath CP_C = new CategoryPath("C"), CP_D = new CategoryPath("D"); // indexed w/ NO_PARENTS
-  private static final int NUM_CHILDREN_CP_A = 5, NUM_CHILDREN_CP_B = 3;
-  private static final int NUM_CHILDREN_CP_C = 5, NUM_CHILDREN_CP_D = 5;
-  private static final CategoryPath[] CATEGORIES_A, CATEGORIES_B;
-  private static final CategoryPath[] CATEGORIES_C, CATEGORIES_D;
-  static {
-    CATEGORIES_A = new CategoryPath[NUM_CHILDREN_CP_A];
-    for (int i = 0; i < NUM_CHILDREN_CP_A; i++) {
-      CATEGORIES_A[i] = new CategoryPath(CP_A.components[0], Integer.toString(i));
-    }
-    CATEGORIES_B = new CategoryPath[NUM_CHILDREN_CP_B];
-    for (int i = 0; i < NUM_CHILDREN_CP_B; i++) {
-      CATEGORIES_B[i] = new CategoryPath(CP_B.components[0], Integer.toString(i));
-    }
-    
-    // NO_PARENTS categories
-    CATEGORIES_C = new CategoryPath[NUM_CHILDREN_CP_C];
-    for (int i = 0; i < NUM_CHILDREN_CP_C; i++) {
-      CATEGORIES_C[i] = new CategoryPath(CP_C.components[0], Integer.toString(i));
-    }
-    
-    // Multi-level categories
-    CATEGORIES_D = new CategoryPath[NUM_CHILDREN_CP_D];
-    for (int i = 0; i < NUM_CHILDREN_CP_D; i++) {
-      String val = Integer.toString(i);
-      CATEGORIES_D[i] = new CategoryPath(CP_D.components[0], val, val + val); // e.g. D/1/11, D/2/22...
-    }
-  }
-  
-  private static Directory indexDir, taxoDir;
-  private static ObjectToIntMap<CategoryPath> allExpectedCounts, termExpectedCounts;
-  private static FacetIndexingParams fip;
-
-  @AfterClass
-  public static void afterClassCountingFacetsAggregatorTest() throws Exception {
-    IOUtils.close(indexDir, taxoDir); 
-  }
-  
-  private static List<CategoryPath> randomCategories(Random random) {
-    // add random categories from the two dimensions, ensuring that the same
-    // category is not added twice.
-    int numFacetsA = random.nextInt(3) + 1; // 1-3
-    int numFacetsB = random.nextInt(2) + 1; // 1-2
-    ArrayList<CategoryPath> categories_a = new ArrayList<CategoryPath>();
-    categories_a.addAll(Arrays.asList(CATEGORIES_A));
-    ArrayList<CategoryPath> categories_b = new ArrayList<CategoryPath>();
-    categories_b.addAll(Arrays.asList(CATEGORIES_B));
-    Collections.shuffle(categories_a, random);
-    Collections.shuffle(categories_b, random);
-
-    ArrayList<CategoryPath> categories = new ArrayList<CategoryPath>();
-    categories.addAll(categories_a.subList(0, numFacetsA));
-    categories.addAll(categories_b.subList(0, numFacetsB));
-    
-    // add the NO_PARENT categories
-    categories.add(CATEGORIES_C[random().nextInt(NUM_CHILDREN_CP_C)]);
-    categories.add(CATEGORIES_D[random().nextInt(NUM_CHILDREN_CP_D)]);
-
-    return categories;
-  }
-
-  private static void addField(Document doc) {
-    doc.add(new StringField(A.field(), A.text(), Store.NO));
-  }
-  
-  private static void addFacets(Document doc, FacetFields facetFields, boolean updateTermExpectedCounts) 
-      throws IOException {
-    List<CategoryPath> docCategories = randomCategories(random());
-    for (CategoryPath cp : docCategories) {
-      if (cp.components[0].equals(CP_D.components[0])) {
-        cp = cp.subpath(2); // we'll get counts for the 2nd level only
-      }
-      allExpectedCounts.put(cp, allExpectedCounts.get(cp) + 1);
-      if (updateTermExpectedCounts) {
-        termExpectedCounts.put(cp, termExpectedCounts.get(cp) + 1);
-      }
-    }
-    // add 1 to each NO_PARENTS dimension
-    allExpectedCounts.put(CP_B, allExpectedCounts.get(CP_B) + 1);
-    allExpectedCounts.put(CP_C, allExpectedCounts.get(CP_C) + 1);
-    allExpectedCounts.put(CP_D, allExpectedCounts.get(CP_D) + 1);
-    if (updateTermExpectedCounts) {
-      termExpectedCounts.put(CP_B, termExpectedCounts.get(CP_B) + 1);
-      termExpectedCounts.put(CP_C, termExpectedCounts.get(CP_C) + 1);
-      termExpectedCounts.put(CP_D, termExpectedCounts.get(CP_D) + 1);
-    }
-    
-    facetFields.addFields(doc, docCategories);
-  }
-  
-  private static void indexDocsNoFacets(IndexWriter indexWriter) throws IOException {
-    int numDocs = atLeast(2);
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      addField(doc);
-      indexWriter.addDocument(doc);
-    }
-    indexWriter.commit(); // flush a segment
-  }
-  
-  private static void indexDocsWithFacetsNoTerms(IndexWriter indexWriter, TaxonomyWriter taxoWriter, 
-      ObjectToIntMap<CategoryPath> expectedCounts) throws IOException {
-    Random random = random();
-    int numDocs = atLeast(random, 2);
-    FacetFields facetFields = new FacetFields(taxoWriter, fip);
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      addFacets(doc, facetFields, false);
-      indexWriter.addDocument(doc);
-    }
-    indexWriter.commit(); // flush a segment
-  }
-  
-  private static void indexDocsWithFacetsAndTerms(IndexWriter indexWriter, TaxonomyWriter taxoWriter, 
-      ObjectToIntMap<CategoryPath> expectedCounts) throws IOException {
-    Random random = random();
-    int numDocs = atLeast(random, 2);
-    FacetFields facetFields = new FacetFields(taxoWriter, fip);
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      addFacets(doc, facetFields, true);
-      addField(doc);
-      indexWriter.addDocument(doc);
-    }
-    indexWriter.commit(); // flush a segment
-  }
-  
-  private static void indexDocsWithFacetsAndSomeTerms(IndexWriter indexWriter, TaxonomyWriter taxoWriter, 
-      ObjectToIntMap<CategoryPath> expectedCounts) throws IOException {
-    Random random = random();
-    int numDocs = atLeast(random, 2);
-    FacetFields facetFields = new FacetFields(taxoWriter, fip);
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      boolean hasContent = random.nextBoolean();
-      if (hasContent) {
-        addField(doc);
-      }
-      addFacets(doc, facetFields, hasContent);
-      indexWriter.addDocument(doc);
-    }
-    indexWriter.commit(); // flush a segment
-  }
-  
-  // initialize expectedCounts w/ 0 for all categories
-  private static ObjectToIntMap<CategoryPath> newCounts() {
-    ObjectToIntMap<CategoryPath> counts = new ObjectToIntMap<CategoryPath>();
-    counts.put(CP_A, 0);
-    counts.put(CP_B, 0);
-    counts.put(CP_C, 0);
-    counts.put(CP_D, 0);
-    for (CategoryPath cp : CATEGORIES_A) {
-      counts.put(cp, 0);
-    }
-    for (CategoryPath cp : CATEGORIES_B) {
-      counts.put(cp, 0);
-    }
-    for (CategoryPath cp : CATEGORIES_C) {
-      counts.put(cp, 0);
-    }
-    for (CategoryPath cp : CATEGORIES_D) {
-      counts.put(cp.subpath(2), 0);
-    }
-    return counts;
-  }
-  
-  @BeforeClass
-  public static void beforeClassCountingFacetsAggregatorTest() throws Exception {
-    indexDir = newDirectory();
-    taxoDir = newDirectory();
-    
-    // create an index which has:
-    // 1. Segment with no categories, but matching results
-    // 2. Segment w/ categories, but no results
-    // 3. Segment w/ categories and results
-    // 4. Segment w/ categories, but only some results
-    
-    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // prevent merges, so we can control the index segments
-    IndexWriter indexWriter = new IndexWriter(indexDir, conf);
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-
-    Map<String,OrdinalPolicy> policies = new HashMap<String,CategoryListParams.OrdinalPolicy>();
-    policies.put(CP_B.components[0], OrdinalPolicy.ALL_PARENTS);
-    policies.put(CP_C.components[0], OrdinalPolicy.NO_PARENTS);
-    policies.put(CP_D.components[0], OrdinalPolicy.NO_PARENTS);
-    CategoryListParams clp = new PerDimensionOrdinalPolicy(policies);
-    fip = new FacetIndexingParams(clp);
-    
-    allExpectedCounts = newCounts();
-    termExpectedCounts = newCounts();
-    
-    // segment w/ no categories
-    indexDocsNoFacets(indexWriter);
-
-    // segment w/ categories, no content
-    indexDocsWithFacetsNoTerms(indexWriter, taxoWriter, allExpectedCounts);
-
-    // segment w/ categories and content
-    indexDocsWithFacetsAndTerms(indexWriter, taxoWriter, allExpectedCounts);
-    
-    // segment w/ categories and some content
-    indexDocsWithFacetsAndSomeTerms(indexWriter, taxoWriter, allExpectedCounts);
-    
-    IOUtils.close(indexWriter, taxoWriter);
-  }
-  
-  private TaxonomyFacetsAccumulator randomAccumulator(FacetSearchParams fsp, IndexReader indexReader, TaxonomyReader taxoReader) {
-    final FacetsAggregator aggregator;
-    double val = random().nextDouble();
-    if (val < 0.6) {
-      aggregator = new FastCountingFacetsAggregator(); // it's the default, so give it the highest chance
-    } else if (val < 0.8) {
-      aggregator = new CountingFacetsAggregator();
-    } else {
-      aggregator = new CachedOrdsCountingFacetsAggregator();
-    }
-    return new TaxonomyFacetsAccumulator(fsp, indexReader, taxoReader) {
-      @Override
-      public FacetsAggregator getAggregator() {
-        return aggregator;
-      }
-    };
-  }
-  
-  @Test
-  public void testDifferentNumResults() throws Exception {
-    // test the collector w/ FacetRequests and different numResults
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher searcher = newSearcher(indexReader);
-    
-    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(CP_A, NUM_CHILDREN_CP_A), 
-        new CountFacetRequest(CP_B, NUM_CHILDREN_CP_B));
-    FacetsCollector fc = FacetsCollector.create(randomAccumulator(fsp, indexReader, taxoReader));
-    TermQuery q = new TermQuery(A);
-    searcher.search(q, fc);
-    
-    List<FacetResult> facetResults = fc.getFacetResults();
-    assertEquals("invalid number of facet results", 2, facetResults.size());
-    for (FacetResult res : facetResults) {
-      FacetResultNode root = res.getFacetResultNode();
-      assertEquals("wrong count for " + root.label, termExpectedCounts.get(root.label), (int) root.value);
-      for (FacetResultNode child : root.subResults) {
-        assertEquals("wrong count for " + child.label, termExpectedCounts.get(child.label), (int) child.value);
-      }
-    }
-    
-    IOUtils.close(indexReader, taxoReader);
-  }
-  
-  @Test
-  public void testAllCounts() throws Exception {
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher searcher = newSearcher(indexReader);
-    
-    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(CP_A, NUM_CHILDREN_CP_A), 
-        new CountFacetRequest(CP_B, NUM_CHILDREN_CP_B));
-    FacetsCollector fc = FacetsCollector.create(randomAccumulator(fsp, indexReader, taxoReader));
-    searcher.search(new MatchAllDocsQuery(), fc);
-    
-    List<FacetResult> facetResults = fc.getFacetResults();
-    assertEquals("invalid number of facet results", 2, facetResults.size());
-    for (FacetResult res : facetResults) {
-      FacetResultNode root = res.getFacetResultNode();
-      assertEquals("wrong count for " + root.label, allExpectedCounts.get(root.label), (int) root.value);
-      int prevValue = Integer.MAX_VALUE;
-      int prevOrdinal = Integer.MAX_VALUE;
-      for (FacetResultNode child : root.subResults) {
-        assertEquals("wrong count for " + child.label, allExpectedCounts.get(child.label), (int) child.value);
-        assertTrue("wrong sort order of sub results: child.value=" + child.value + " prevValue=" + prevValue, child.value <= prevValue);
-        if (child.value == prevValue) {
-          assertTrue("wrong sort order of sub results", child.ordinal < prevOrdinal);
-        }
-        prevValue = (int) child.value;
-        prevOrdinal = child.ordinal;
-      }
-    }
-    
-    IOUtils.close(indexReader, taxoReader);
-  }
-  
-  @Test
-  public void testBigNumResults() throws Exception {
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher searcher = newSearcher(indexReader);
-    
-    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(CP_A, Integer.MAX_VALUE), 
-        new CountFacetRequest(CP_B, Integer.MAX_VALUE));
-    FacetsCollector fc = FacetsCollector.create(randomAccumulator(fsp, indexReader, taxoReader));
-    searcher.search(new MatchAllDocsQuery(), fc);
-    
-    List<FacetResult> facetResults = fc.getFacetResults();
-    assertEquals("invalid number of facet results", 2, facetResults.size());
-    for (FacetResult res : facetResults) {
-      FacetResultNode root = res.getFacetResultNode();
-      assertEquals("wrong count for " + root.label, allExpectedCounts.get(root.label), (int) root.value);
-      for (FacetResultNode child : root.subResults) {
-        assertEquals("wrong count for " + child.label, allExpectedCounts.get(child.label), (int) child.value);
-      }
-    }
-    
-    IOUtils.close(indexReader, taxoReader);
-  }
-  
-  @Test
-  public void testNoParents() throws Exception {
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher searcher = newSearcher(indexReader);
-    FacetSearchParams fsp = new FacetSearchParams(fip, new CountFacetRequest(CP_C, NUM_CHILDREN_CP_C), 
-        new CountFacetRequest(CP_D, NUM_CHILDREN_CP_D));
-    FacetsCollector fc = FacetsCollector.create(randomAccumulator(fsp, indexReader, taxoReader));
-    searcher.search(new MatchAllDocsQuery(), fc);
-    
-    List<FacetResult> facetResults = fc.getFacetResults();
-    assertEquals("invalid number of facet results", fsp.facetRequests.size(), facetResults.size());
-    for (FacetResult res : facetResults) {
-      FacetResultNode root = res.getFacetResultNode();
-      assertEquals("wrong count for " + root.label, allExpectedCounts.get(root.label), (int) root.value);
-      for (FacetResultNode child : root.subResults) {
-        assertEquals("wrong count for " + child.label, allExpectedCounts.get(child.label), (int) child.value);
-      }
-    }
-    
-    IOUtils.close(indexReader, taxoReader);
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/DrillDownQueryTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/DrillDownQueryTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/DrillDownQueryTest.java	2013-11-24 07:03:15.484015628 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/DrillDownQueryTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,275 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.PerDimensionIndexingParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.QueryUtils;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class DrillDownQueryTest extends FacetTestCase {
-  
-  private static IndexReader reader;
-  private static DirectoryTaxonomyReader taxo;
-  private static Directory dir;
-  private static Directory taxoDir;
-  
-  private FacetIndexingParams defaultParams;
-  private PerDimensionIndexingParams nonDefaultParams;
-
-  @AfterClass
-  public static void afterClassDrillDownQueryTest() throws Exception {
-    IOUtils.close(reader, taxo, dir, taxoDir);
-    reader = null;
-    taxo = null;
-    dir = null;
-    taxoDir = null;
-  }
-
-  @BeforeClass
-  public static void beforeClassDrillDownQueryTest() throws Exception {
-    dir = newDirectory();
-    Random r = random();
-    RandomIndexWriter writer = new RandomIndexWriter(r, dir, 
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(r, MockTokenizer.KEYWORD, false)));
-    
-    taxoDir = newDirectory();
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    
-    for (int i = 0; i < 100; i++) {
-      ArrayList<CategoryPath> paths = new ArrayList<CategoryPath>();
-      Document doc = new Document();
-      if (i % 2 == 0) { // 50
-        doc.add(new TextField("content", "foo", Field.Store.NO));
-      }
-      if (i % 3 == 0) { // 33
-        doc.add(new TextField("content", "bar", Field.Store.NO));
-      }
-      if (i % 4 == 0) { // 25
-        if (r.nextBoolean()) {
-          paths.add(new CategoryPath("a/1", '/'));
-        } else {
-          paths.add(new CategoryPath("a/2", '/'));
-        }
-      }
-      if (i % 5 == 0) { // 20
-        paths.add(new CategoryPath("b"));
-      }
-      FacetFields facetFields = new FacetFields(taxoWriter);
-      if (paths.size() > 0) {
-        facetFields.addFields(doc, paths);
-      }
-      writer.addDocument(doc);
-    }
-    
-    taxoWriter.close();
-    reader = writer.getReader();
-    writer.close();
-    
-    taxo = new DirectoryTaxonomyReader(taxoDir);
-  }
-  
-  public DrillDownQueryTest() {
-    Map<CategoryPath,CategoryListParams> paramsMap = new HashMap<CategoryPath,CategoryListParams>();
-    paramsMap.put(new CategoryPath("a"), randomCategoryListParams("testing_facets_a"));
-    paramsMap.put(new CategoryPath("b"), randomCategoryListParams("testing_facets_b"));
-    nonDefaultParams = new PerDimensionIndexingParams(paramsMap);
-    defaultParams = new FacetIndexingParams(randomCategoryListParams(CategoryListParams.DEFAULT_FIELD));
-  }
-  
-  @Test
-  public void testDefaultField() {
-    String defaultField = CategoryListParams.DEFAULT_FIELD;
-    
-    Term termA = DrillDownQuery.term(defaultParams, new CategoryPath("a"));
-    assertEquals(new Term(defaultField, "a"), termA);
-    
-    Term termB = DrillDownQuery.term(defaultParams, new CategoryPath("b"));
-    assertEquals(new Term(defaultField, "b"), termB);
-  }
-  
-  @Test
-  public void testAndOrs() throws Exception {
-    IndexSearcher searcher = newSearcher(reader);
-
-    // test (a/1 OR a/2) AND b
-    DrillDownQuery q = new DrillDownQuery(defaultParams);
-    q.add(new CategoryPath("a/1", '/'), new CategoryPath("a/2", '/'));
-    q.add(new CategoryPath("b"));
-    TopDocs docs = searcher.search(q, 100);
-    assertEquals(5, docs.totalHits);
-  }
-  
-  @Test
-  public void testQuery() throws IOException {
-    IndexSearcher searcher = newSearcher(reader);
-
-    // Making sure the query yields 25 documents with the facet "a"
-    DrillDownQuery q = new DrillDownQuery(defaultParams);
-    q.add(new CategoryPath("a"));
-    QueryUtils.check(q);
-    TopDocs docs = searcher.search(q, 100);
-    assertEquals(25, docs.totalHits);
-    
-    // Making sure the query yields 5 documents with the facet "b" and the
-    // previous (facet "a") query as a base query
-    DrillDownQuery q2 = new DrillDownQuery(defaultParams, q);
-    q2.add(new CategoryPath("b"));
-    docs = searcher.search(q2, 100);
-    assertEquals(5, docs.totalHits);
-
-    // Making sure that a query of both facet "a" and facet "b" yields 5 results
-    DrillDownQuery q3 = new DrillDownQuery(defaultParams);
-    q3.add(new CategoryPath("a"));
-    q3.add(new CategoryPath("b"));
-    docs = searcher.search(q3, 100);
-    
-    assertEquals(5, docs.totalHits);
-    // Check that content:foo (which yields 50% results) and facet/b (which yields 20%)
-    // would gather together 10 results (10%..) 
-    Query fooQuery = new TermQuery(new Term("content", "foo"));
-    DrillDownQuery q4 = new DrillDownQuery(defaultParams, fooQuery);
-    q4.add(new CategoryPath("b"));
-    docs = searcher.search(q4, 100);
-    assertEquals(10, docs.totalHits);
-  }
-  
-  @Test
-  public void testQueryImplicitDefaultParams() throws IOException {
-    IndexSearcher searcher = newSearcher(reader);
-
-    // Create the base query to start with
-    DrillDownQuery q = new DrillDownQuery(defaultParams);
-    q.add(new CategoryPath("a"));
-    
-    // Making sure the query yields 5 documents with the facet "b" and the
-    // previous (facet "a") query as a base query
-    DrillDownQuery q2 = new DrillDownQuery(defaultParams, q);
-    q2.add(new CategoryPath("b"));
-    TopDocs docs = searcher.search(q2, 100);
-    assertEquals(5, docs.totalHits);
-
-    // Check that content:foo (which yields 50% results) and facet/b (which yields 20%)
-    // would gather together 10 results (10%..) 
-    Query fooQuery = new TermQuery(new Term("content", "foo"));
-    DrillDownQuery q4 = new DrillDownQuery(defaultParams, fooQuery);
-    q4.add(new CategoryPath("b"));
-    docs = searcher.search(q4, 100);
-    assertEquals(10, docs.totalHits);
-  }
-  
-  @Test
-  public void testScoring() throws IOException {
-    // verify that drill-down queries do not modify scores
-    IndexSearcher searcher = newSearcher(reader);
-
-    float[] scores = new float[reader.maxDoc()];
-    
-    Query q = new TermQuery(new Term("content", "foo"));
-    TopDocs docs = searcher.search(q, reader.maxDoc()); // fetch all available docs to this query
-    for (ScoreDoc sd : docs.scoreDocs) {
-      scores[sd.doc] = sd.score;
-    }
-    
-    // create a drill-down query with category "a", scores should not change
-    DrillDownQuery q2 = new DrillDownQuery(defaultParams, q);
-    q2.add(new CategoryPath("a"));
-    docs = searcher.search(q2, reader.maxDoc()); // fetch all available docs to this query
-    for (ScoreDoc sd : docs.scoreDocs) {
-      assertEquals("score of doc=" + sd.doc + " modified", scores[sd.doc], sd.score, 0f);
-    }
-  }
-  
-  @Test
-  public void testScoringNoBaseQuery() throws IOException {
-    // verify that drill-down queries (with no base query) returns 0.0 score
-    IndexSearcher searcher = newSearcher(reader);
-    
-    DrillDownQuery q = new DrillDownQuery(defaultParams);
-    q.add(new CategoryPath("a"));
-    TopDocs docs = searcher.search(q, reader.maxDoc()); // fetch all available docs to this query
-    for (ScoreDoc sd : docs.scoreDocs) {
-      assertEquals(0f, sd.score, 0f);
-    }
-  }
-  
-  @Test
-  public void testTermNonDefault() {
-    Term termA = DrillDownQuery.term(nonDefaultParams, new CategoryPath("a"));
-    assertEquals(new Term("testing_facets_a", "a"), termA);
-    
-    Term termB = DrillDownQuery.term(nonDefaultParams, new CategoryPath("b"));
-    assertEquals(new Term("testing_facets_b", "b"), termB);
-  }
-
-  @Test
-  public void testClone() throws Exception {
-    DrillDownQuery q = new DrillDownQuery(defaultParams, new MatchAllDocsQuery());
-    q.add(new CategoryPath("a"));
-    
-    DrillDownQuery clone = q.clone();
-    clone.add(new CategoryPath("b"));
-    
-    assertFalse("query wasn't cloned: source=" + q + " clone=" + clone, q.toString().equals(clone.toString()));
-  }
-  
-  @Test(expected=IllegalStateException.class)
-  public void testNoBaseNorDrillDown() throws Exception {
-    DrillDownQuery q = new DrillDownQuery(defaultParams);
-    q.rewrite(reader);
-  }
-  
-  public void testNoDrillDown() throws Exception {
-    Query base = new MatchAllDocsQuery();
-    DrillDownQuery q = new DrillDownQuery(defaultParams, base);
-    Query rewrite = q.rewrite(reader).rewrite(reader);
-    assertSame(base, rewrite);
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/FacetRequestTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/FacetRequestTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/FacetRequestTest.java	2013-07-15 15:52:17.217877401 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/FacetRequestTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,47 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.search.CountFacetRequest;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class FacetRequestTest extends FacetTestCase {
-  
-  @Test(expected=IllegalArgumentException.class)
-  public void testIllegalNumResults() throws Exception {
-    assertNotNull(new CountFacetRequest(new CategoryPath("a", "b"), 0));
-  }
-  
-  @Test(expected=IllegalArgumentException.class)
-  public void testIllegalCategoryPath() throws Exception {
-    assertNotNull(new CountFacetRequest(null, 1));
-  }
-  
-  @Test
-  public void testHashAndEquals() {
-    CountFacetRequest fr1 = new CountFacetRequest(new CategoryPath("a"), 8);
-    CountFacetRequest fr2 = new CountFacetRequest(new CategoryPath("a"), 8);
-    assertEquals("hashCode() should agree on both objects", fr1.hashCode(), fr2.hashCode());
-    assertTrue("equals() should return true", fr1.equals(fr2));
-    fr1.setDepth(10);
-    assertFalse("equals() should return false as fr1.depth != fr2.depth", fr1.equals(fr2));
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/FacetResultTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/FacetResultTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/FacetResultTest.java	2013-07-29 13:55:02.597707543 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/FacetResultTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,204 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.FacetTestUtils;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.DrillSideways.DrillSidewaysResult;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.CollectionUtil;
-import org.apache.lucene.util.IOUtils;
-import org.junit.Test;
-
-public class FacetResultTest extends FacetTestCase {
-  
-  private Document newDocument(FacetFields facetFields, String... categories) throws IOException {
-    Document doc = new Document();
-    List<CategoryPath> cats = new ArrayList<CategoryPath>();
-    for (String cat : categories) {
-      cats.add(new CategoryPath(cat, '/'));
-    }
-    facetFields.addFields(doc, cats);
-    return doc;
-  }
-  
-  private void initIndex(Directory indexDir, Directory taxoDir) throws IOException {
-    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    IndexWriter indexWriter = new IndexWriter(indexDir, conf);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetFields facetFields = new FacetFields(taxoWriter);
-    indexWriter.addDocument(newDocument(facetFields, "Date/2010/March/12", "A/1"));
-    indexWriter.addDocument(newDocument(facetFields, "Date/2010/March/23", "A/2"));
-    indexWriter.addDocument(newDocument(facetFields, "Date/2010/April/17", "A/3"));
-    indexWriter.addDocument(newDocument(facetFields, "Date/2010/May/18", "A/1"));
-    indexWriter.addDocument(newDocument(facetFields, "Date/2011/January/1", "A/3"));
-    indexWriter.addDocument(newDocument(facetFields, "Date/2011/February/12", "A/1"));
-    indexWriter.addDocument(newDocument(facetFields, "Date/2011/February/18", "A/4"));
-    indexWriter.addDocument(newDocument(facetFields, "Date/2012/August/15", "A/1"));
-    indexWriter.addDocument(newDocument(facetFields, "Date/2012/July/5", "A/2"));
-    indexWriter.addDocument(newDocument(facetFields, "Date/2013/September/13", "A/1"));
-    indexWriter.addDocument(newDocument(facetFields, "Date/2013/September/25", "A/4"));
-    IOUtils.close(indexWriter, taxoWriter);
-  }
-  
-  private void searchIndex(TaxonomyReader taxoReader, IndexSearcher searcher, boolean fillMissingCounts, String[] exp,
-      String[][] drillDowns, int[] numResults) throws IOException {
-    CategoryPath[][] cps = new CategoryPath[drillDowns.length][];
-    for (int i = 0; i < cps.length; i++) {
-      cps[i] = new CategoryPath[drillDowns[i].length];
-      for (int j = 0; j < cps[i].length; j++) {
-        cps[i][j] = new CategoryPath(drillDowns[i][j], '/');
-      }
-    }
-    DrillDownQuery ddq = new DrillDownQuery(FacetIndexingParams.DEFAULT, new MatchAllDocsQuery());
-    for (CategoryPath[] cats : cps) {
-      ddq.add(cats);
-    }
-    
-    List<FacetRequest> facetRequests = new ArrayList<FacetRequest>();
-    for (CategoryPath[] cats : cps) {
-      for (int i = 0; i < cats.length; i++) {
-        CategoryPath cp = cats[i];
-        int numres = numResults == null ? 2 : numResults[i];
-        // for each drill-down, add itself as well as its parent as requests, so
-        // we get the drill-sideways
-        facetRequests.add(new CountFacetRequest(cp, numres));
-        CountFacetRequest parent = new CountFacetRequest(cp.subpath(cp.length - 1), numres);
-        if (!facetRequests.contains(parent) && parent.categoryPath.length > 0) {
-          facetRequests.add(parent);
-        }
-      }
-    }
-    
-    FacetSearchParams fsp = new FacetSearchParams(facetRequests);
-    final DrillSideways ds;
-    final Map<String,FacetArrays> dimArrays;
-    if (fillMissingCounts) {
-      dimArrays = new HashMap<String,FacetArrays>();
-      ds = new DrillSideways(searcher, taxoReader) {
-        @Override
-        protected FacetsAccumulator getDrillSidewaysAccumulator(String dim, FacetSearchParams fsp) throws IOException {
-          FacetsAccumulator fa = super.getDrillSidewaysAccumulator(dim, fsp);
-          dimArrays.put(dim, ((TaxonomyFacetsAccumulator) fa).facetArrays);
-          return fa;
-        }
-      };
-    } else {
-      ds = new DrillSideways(searcher, taxoReader);
-      dimArrays = null;
-    }
-    
-    final DrillSidewaysResult sidewaysRes = ds.search(null, ddq, 5, fsp);
-    List<FacetResult> facetResults = FacetResult.mergeHierarchies(sidewaysRes.facetResults, taxoReader, dimArrays);
-    CollectionUtil.introSort(facetResults, new Comparator<FacetResult>() {
-      @Override
-      public int compare(FacetResult o1, FacetResult o2) {
-        return o1.getFacetRequest().categoryPath.compareTo(o2.getFacetRequest().categoryPath);
-      }
-    });
-    assertEquals(exp.length, facetResults.size()); // A + single one for date
-    for (int i = 0; i < facetResults.size(); i++) {
-      assertEquals(exp[i], FacetTestUtils.toSimpleString(facetResults.get(i)));
-    }
-  }
-  
-  @Test
-  public void testMergeHierarchies() throws Exception {
-    Directory indexDir = new RAMDirectory(), taxoDir = new RAMDirectory();
-    initIndex(indexDir, taxoDir);
-    
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher searcher = new IndexSearcher(indexReader);
-    
-    String[] exp = new String[] { "Date (0)\n  2010 (4)\n  2011 (3)\n" };
-    searchIndex(taxoReader, searcher, false, exp, new String[][] { new String[] { "Date" } }, null);
-    
-    // two dimensions
-    exp = new String[] { "A (0)\n  1 (5)\n  4 (2)\n", "Date (0)\n  2010 (4)\n  2011 (3)\n" };
-    searchIndex(taxoReader, searcher, false, exp, new String[][] { new String[] { "Date" }, new String[] { "A" } }, null);
-    
-    // both parent and child are OR'd
-    exp = new String[] { "Date (-1)\n  2010 (4)\n    March (2)\n      23 (1)\n      12 (1)\n    May (1)\n" };
-    searchIndex(taxoReader, searcher, false, exp, new String[][] { new String[] { "Date/2010/March", "Date/2010/March/23" }}, null);
-    
-    // both parent and child are OR'd (fill counts)
-    exp = new String[] { "Date (0)\n  2010 (4)\n    March (2)\n      23 (1)\n      12 (1)\n    May (1)\n" };
-    searchIndex(taxoReader, searcher, true, exp, new String[][] { new String[] { "Date/2010/March", "Date/2010/March/23" }}, null);
-    
-    // same DD twice
-    exp = new String[] { "Date (0)\n  2010 (4)\n    March (2)\n    May (1)\n  2011 (3)\n" };
-    searchIndex(taxoReader, searcher, false, exp, new String[][] { new String[] { "Date/2010", "Date/2010" }}, null);
-    
-    exp = new String[] { "Date (0)\n  2010 (4)\n    March (2)\n    May (1)\n  2011 (3)\n" };
-    searchIndex(taxoReader, searcher, false, exp, new String[][] { new String[] { "Date/2010" }}, null);
-    
-    exp = new String[] { "Date (0)\n  2010 (4)\n    March (2)\n    May (1)\n  2011 (3)\n    February (2)\n    January (1)\n" };
-    searchIndex(taxoReader, searcher, false, exp, new String[][] { new String[] { "Date/2010", "Date/2011" }}, null);
-    
-    exp = new String[] { "Date (0)\n  2010 (4)\n    March (2)\n      23 (1)\n      12 (1)\n    May (1)\n  2011 (3)\n    February (2)\n    January (1)\n" };
-    searchIndex(taxoReader, searcher, false, exp, new String[][] { new String[] { "Date/2010/March", "Date/2011" }}, null);
-    
-    // Date/2010/April not in top-2 of Date/2010
-    exp = new String[] { "Date (0)\n  2010 (4)\n    March (2)\n      23 (1)\n      12 (1)\n    May (1)\n    April (1)\n      17 (1)\n  2011 (3)\n    February (2)\n    January (1)\n" };
-    searchIndex(taxoReader, searcher, false, exp, new String[][] { new String[] { "Date/2010/March", "Date/2010/April", "Date/2011" }}, null);
-    
-    // missing ancestors
-    exp = new String[] { "Date (-1)\n  2010 (4)\n    March (2)\n    May (1)\n    April (1)\n      17 (1)\n  2011 (-1)\n    January (1)\n      1 (1)\n" };
-    searchIndex(taxoReader, searcher, false, exp, new String[][] { new String[] { "Date/2011/January/1", "Date/2010/April" }}, null);
-    
-    // missing ancestors (fill counts)
-    exp = new String[] { "Date (0)\n  2010 (4)\n    March (2)\n    May (1)\n    April (1)\n      17 (1)\n  2011 (3)\n    January (1)\n      1 (1)\n" };
-    searchIndex(taxoReader, searcher, true, exp, new String[][] { new String[] { "Date/2011/January/1", "Date/2010/April" }}, null);
-    
-    // non-hierarchical dimension with both parent and child
-    exp = new String[] { "A (0)\n  1 (5)\n  4 (2)\n  3 (2)\n" };
-    searchIndex(taxoReader, searcher, INFOSTREAM, exp, new String[][] { new String[] { "A", "A/3" }}, null);
-    
-    // non-hierarchical dimension with same request but different numResults
-    exp = new String[] { "A (0)\n  1 (5)\n  4 (2)\n  3 (2)\n  2 (2)\n" };
-    searchIndex(taxoReader, searcher, INFOSTREAM, exp, new String[][] { new String[] { "A", "A" }}, new int[] { 2, 4 });
-    
-    IOUtils.close(indexReader, taxoReader);
-    
-    IOUtils.close(indexDir, taxoDir);
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/MultiCategoryListIteratorTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/MultiCategoryListIteratorTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/MultiCategoryListIteratorTest.java	2013-02-20 13:38:17.428711928 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/MultiCategoryListIteratorTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,118 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.Random;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.encoding.IntDecoder;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.PerDimensionIndexingParams;
-import org.apache.lucene.facet.search.CategoryListIterator;
-import org.apache.lucene.facet.search.DocValuesCategoryListIterator;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.facet.util.MultiCategoryListIterator;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class MultiCategoryListIteratorTest extends FacetTestCase {
-
-  @Test
-  public void testMultipleCategoryLists() throws Exception {
-    Random random = random();
-    int numDimensions = atLeast(random, 2); // at least 2 dimensions
-    String[] dimensions = new String[numDimensions];
-    for (int i = 0; i < numDimensions; i++) {
-      dimensions[i] = "dim" + i;
-    }
-    
-    // build the PerDimensionIndexingParams
-    HashMap<CategoryPath,CategoryListParams> clps = new HashMap<CategoryPath,CategoryListParams>();
-    for (String dim : dimensions) {
-      CategoryPath cp = new CategoryPath(dim);
-      CategoryListParams clp = randomCategoryListParams("$" + dim);
-      clps.put(cp, clp);
-    }
-    PerDimensionIndexingParams indexingParams = new PerDimensionIndexingParams(clps);
-    
-    // index some documents
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriter indexWriter = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null).setMaxBufferedDocs(2));
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetFields facetFields = new FacetFields(taxoWriter, indexingParams);
-    int ndocs = atLeast(random, 10);
-    for (int i = 0; i < ndocs; i++) {
-      Document doc = new Document();
-      int numCategories = random.nextInt(numDimensions) + 1;
-      ArrayList<CategoryPath> categories = new ArrayList<CategoryPath>();
-      for (int j = 0; j < numCategories; j++) {
-        String dimension = dimensions[random.nextInt(dimensions.length)];
-        categories.add(new CategoryPath(dimension, Integer.toString(i)));
-      }
-      facetFields.addFields(doc, categories);
-      indexWriter.addDocument(doc);
-    }
-    IOUtils.close(indexWriter, taxoWriter);
-    
-    // test the multi iterator
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    CategoryListIterator[] iterators = new CategoryListIterator[numDimensions];
-    for (int i = 0; i < iterators.length; i++) {
-      CategoryListParams clp = indexingParams.getCategoryListParams(new CategoryPath(dimensions[i]));
-      IntDecoder decoder = clp.createEncoder().createMatchingDecoder();
-      iterators[i] = new DocValuesCategoryListIterator(clp.field, decoder);
-    }
-    MultiCategoryListIterator cli = new MultiCategoryListIterator(iterators);
-    for (AtomicReaderContext context : indexReader.leaves()) {
-      assertTrue("failed to init multi-iterator", cli.setNextReader(context));
-      IntsRef ordinals = new IntsRef();
-      final int maxDoc = context.reader().maxDoc();
-      for (int i = 0; i < maxDoc; i++) {
-        cli.getOrdinals(i, ordinals);
-        assertTrue("document " + i + " does not have categories", ordinals.length > 0);
-        for (int j = 0; j < ordinals.length; j++) {
-          CategoryPath cp = taxoReader.getPath(ordinals.ints[j]);
-          assertNotNull("ordinal " + ordinals.ints[j] + " not found in taxonomy", cp);
-          if (cp.length == 2) {
-            int globalDoc = i + context.docBase;
-            assertEquals("invalid category for document " + globalDoc, globalDoc, Integer.parseInt(cp.components[1]));
-          }
-        }
-      }
-    }
-    
-    IOUtils.close(indexReader, taxoReader);
-    IOUtils.close(indexDir, taxoDir);
-  }
-  
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/OrdinalsCacheTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/OrdinalsCacheTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/OrdinalsCacheTest.java	2013-10-23 20:10:12.178334583 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/OrdinalsCacheTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,94 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class OrdinalsCacheTest extends FacetTestCase {
-
-  @Test
-  public void testOrdinalsCacheWithThreads() throws Exception {
-    // LUCENE-5303: OrdinalsCache used the ThreadLocal BinaryDV instead of reader.getCoreCacheKey().
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    IndexWriter writer = new IndexWriter(indexDir, conf);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetFields facetFields = new FacetFields(taxoWriter);
-    
-    Document doc = new Document();
-    facetFields.addFields(doc, Arrays.asList(new CategoryPath("A", "1")));
-    writer.addDocument(doc);
-    doc = new Document();
-    facetFields.addFields(doc, Arrays.asList(new CategoryPath("A", "2")));
-    writer.addDocument(doc);
-    writer.close();
-    taxoWriter.close();
-    
-    final DirectoryReader reader = DirectoryReader.open(indexDir);
-    Thread[] threads = new Thread[3];
-    for (int i = 0; i < threads.length; i++) {
-      threads[i] = new Thread("CachedOrdsThread-" + i) {
-        @Override
-        public void run() {
-          for (AtomicReaderContext context : reader.leaves()) {
-            try {
-              OrdinalsCache.getCachedOrds(context, FacetIndexingParams.DEFAULT.getCategoryListParams(new CategoryPath("A")));
-            } catch (IOException e) {
-              throw new RuntimeException(e);
-            }
-          }
-        }
-      };
-    }
-
-    OrdinalsCache.clear();
-
-    long ramBytesUsed = 0;
-    for (Thread t : threads) {
-      t.start();
-      t.join();
-      if (ramBytesUsed == 0) {
-        ramBytesUsed = OrdinalsCache.ramBytesUsed();
-      } else {
-        assertEquals(ramBytesUsed, OrdinalsCache.ramBytesUsed());
-      }
-    }
-    
-    reader.close();
-    
-    IOUtils.close(indexDir, taxoDir);
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets.java	2013-11-07 19:48:13.926079767 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,311 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintStream;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.FacetTestUtils;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.facet.util.PrintTaxonomyStats;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.similarities.DefaultSimilarity;
-import org.apache.lucene.search.similarities.PerFieldSimilarityWrapper;
-import org.apache.lucene.search.similarities.Similarity;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-
-public class TestDemoFacets extends FacetTestCase {
-
-  private DirectoryTaxonomyWriter taxoWriter;
-  private RandomIndexWriter writer;
-  private FacetFields facetFields;
-
-  private void add(String ... categoryPaths) throws IOException {
-    Document doc = new Document();
-    
-    List<CategoryPath> paths = new ArrayList<CategoryPath>();
-    for(String categoryPath : categoryPaths) {
-      paths.add(new CategoryPath(categoryPath, '/'));
-    }
-    facetFields.addFields(doc, paths);
-    writer.addDocument(doc);
-  }
-
-  public void test() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    writer = new RandomIndexWriter(random(), dir);
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    // Reused across documents, to add the necessary facet
-    // fields:
-    facetFields = new FacetFields(taxoWriter);
-
-    add("Author/Bob", "Publish Date/2010/10/15");
-    add("Author/Lisa", "Publish Date/2010/10/20");
-    add("Author/Lisa", "Publish Date/2012/1/1");
-    add("Author/Susan", "Publish Date/2012/1/7");
-    add("Author/Frank", "Publish Date/1999/5/5");
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    // Count both "Publish Date" and "Author" dimensions:
-    FacetSearchParams fsp = new FacetSearchParams(
-        new CountFacetRequest(new CategoryPath("Publish Date"), 10), 
-        new CountFacetRequest(new CategoryPath("Author"), 10));
-
-    // Aggregate the facet counts:
-    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);
-
-    // MatchAllDocsQuery is for "browsing" (counts facets
-    // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), c);
-
-    // Retrieve & verify results:
-    List<FacetResult> results = c.getFacetResults();
-    assertEquals(2, results.size());
-    assertEquals("Publish Date (0)\n  2012 (2)\n  2010 (2)\n  1999 (1)\n",
-        FacetTestUtils.toSimpleString(results.get(0)));
-    assertEquals("Author (0)\n  Lisa (2)\n  Frank (1)\n  Susan (1)\n  Bob (1)\n",
-        FacetTestUtils.toSimpleString(results.get(1)));
-
-    
-    // Now user drills down on Publish Date/2010:
-    fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath("Author"), 10));
-    DrillDownQuery q2 = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
-    q2.add(new CategoryPath("Publish Date/2010", '/'));
-    c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);
-    searcher.search(q2, c);
-    results = c.getFacetResults();
-    assertEquals(1, results.size());
-    assertEquals("Author (0)\n  Lisa (1)\n  Bob (1)\n",
-        FacetTestUtils.toSimpleString(results.get(0)));
-
-    // Smoke test PrintTaxonomyStats:
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintTaxonomyStats.printStats(taxoReader, new PrintStream(bos, false, "UTF-8"), true);
-    String result = bos.toString("UTF-8");
-    assertTrue(result.indexOf("/Author: 4 immediate children; 5 total categories") != -1);
-    assertTrue(result.indexOf("/Publish Date: 3 immediate children; 12 total categories") != -1);
-    // Make sure at least a few nodes of the tree came out:
-    assertTrue(result.indexOf("  /1999") != -1);
-    assertTrue(result.indexOf("  /2012") != -1);
-    assertTrue(result.indexOf("      /20") != -1);
-
-    taxoReader.close();
-    searcher.getIndexReader().close();
-    dir.close();
-    taxoDir.close();
-  }
-
-  public void testReallyNoNormsForDrillDown() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setSimilarity(new PerFieldSimilarityWrapper() {
-        final Similarity sim = new DefaultSimilarity();
-
-        @Override
-        public Similarity get(String name) {
-          assertEquals("field", name);
-          return sim;
-        }
-      });
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-    FacetFields facetFields = new FacetFields(taxoWriter);      
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    facetFields.addFields(doc, Collections.singletonList(new CategoryPath("a/path", '/')));
-    writer.addDocument(doc);
-    writer.close();
-    taxoWriter.close();
-    dir.close();
-    taxoDir.close();
-  }
-
-  public void testAllParents() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    CategoryListParams clp = new CategoryListParams("$facets") {
-        @Override
-        public OrdinalPolicy getOrdinalPolicy(String fieldName) {
-          return OrdinalPolicy.ALL_PARENTS;
-        }
-      };
-    FacetIndexingParams fip = new FacetIndexingParams(clp);
-
-    FacetFields facetFields = new FacetFields(taxoWriter, fip);
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    facetFields.addFields(doc, Collections.singletonList(new CategoryPath("a/path", '/')));
-    writer.addDocument(doc);
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-    
-    FacetSearchParams fsp = new FacetSearchParams(fip,
-                                                  new CountFacetRequest(new CategoryPath("a", '/'), 10));
-
-    // Aggregate the facet counts:
-    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);
-
-    // MatchAllDocsQuery is for "browsing" (counts facets
-    // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), c);
-    List<FacetResult> results = c.getFacetResults();
-    assertEquals(1, results.size());
-    assertEquals(1, (int) results.get(0).getFacetResultNode().value);
-
-    // LUCENE-4913:
-    for(FacetResultNode childNode : results.get(0).getFacetResultNode().subResults) {
-      assertTrue(childNode.ordinal != 0);
-    }
-
-    searcher.getIndexReader().close();
-    taxoReader.close();
-    dir.close();
-    taxoDir.close();
-  }
-
-  public void testLabelWithDelimiter() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetFields facetFields = new FacetFields(taxoWriter);
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    BytesRef br = new BytesRef(new byte[] {(byte) 0xee, (byte) 0x92, (byte) 0xaa, (byte) 0xef, (byte) 0x9d, (byte) 0x89});
-    facetFields.addFields(doc, Collections.singletonList(new CategoryPath("dim/" + br.utf8ToString(), '/')));
-    try {
-      writer.addDocument(doc);
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    writer.close();
-    taxoWriter.close();
-    dir.close();
-    taxoDir.close();
-  }
-  
-  // LUCENE-4583: make sure if we require > 32 KB for one
-  // document, we don't hit exc when using Facet42DocValuesFormat
-  public void testManyFacetsInOneDocument() throws Exception {
-    assumeTrue("default Codec doesn't support huge BinaryDocValues", _TestUtil.fieldSupportsHugeBinaryDocValues(CategoryListParams.DEFAULT_FIELD));
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-    
-    FacetFields facetFields = new FacetFields(taxoWriter);
-    
-    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);
-    
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    List<CategoryPath> paths = new ArrayList<CategoryPath>();
-    for (int i = 0; i < numLabels; i++) {
-      paths.add(new CategoryPath("dim", "" + i));
-    }
-    facetFields.addFields(doc, paths);
-    writer.addDocument(doc);
-    
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-    
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-    
-    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath("dim"), Integer.MAX_VALUE));
-    
-    // Aggregate the facet counts:
-    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);
-    
-    // MatchAllDocsQuery is for "browsing" (counts facets
-    // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), c);
-    List<FacetResult> results = c.getFacetResults();
-    assertEquals(1, results.size());
-    FacetResultNode root = results.get(0).getFacetResultNode();
-    assertEquals(numLabels, root.subResults.size());
-    Set<String> allLabels = new HashSet<String>();
-    for (FacetResultNode childNode : root.subResults) {
-      assertEquals(2, childNode.label.length);
-      allLabels.add(childNode.label.components[1]);
-      assertEquals(1, (int) childNode.value);
-    }
-    assertEquals(numLabels, allLabels.size());
-    
-    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestDrillSideways.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestDrillSideways.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestDrillSideways.java	2013-11-03 19:05:04.703394188 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestDrillSideways.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,1170 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.FacetTestUtils;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.DrillSideways.DrillSidewaysResult;
-import org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetFields;
-import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.DocIdSet;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.SortField;
-import org.apache.lucene.search.SortField.Type;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FixedBitSet;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.InPlaceMergeSorter;
-import org.apache.lucene.util.InfoStream;
-import org.apache.lucene.util._TestUtil;
-import org.junit.Test;
-
-public class TestDrillSideways extends FacetTestCase {
-
-  private DirectoryTaxonomyWriter taxoWriter;
-  private RandomIndexWriter writer;
-  private FacetFields facetFields;
-
-  private void add(String ... categoryPaths) throws IOException {
-    Document doc = new Document();
-    List<CategoryPath> paths = new ArrayList<CategoryPath>();
-    for(String categoryPath : categoryPaths) {
-      paths.add(new CategoryPath(categoryPath, '/'));
-    }
-    facetFields.addFields(doc, paths);
-    writer.addDocument(doc);
-  }
-
-  public void testBasic() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    writer = new RandomIndexWriter(random(), dir);
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    // Reused across documents, to add the necessary facet
-    // fields:
-    facetFields = new FacetFields(taxoWriter);
-
-    add("Author/Bob", "Publish Date/2010/10/15");
-    add("Author/Lisa", "Publish Date/2010/10/20");
-    add("Author/Lisa", "Publish Date/2012/1/1");
-    add("Author/Susan", "Publish Date/2012/1/7");
-    add("Author/Frank", "Publish Date/1999/5/5");
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    //System.out.println("searcher=" + searcher);
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    // Count both "Publish Date" and "Author" dimensions, in
-    // drill-down:
-    FacetSearchParams fsp = new FacetSearchParams(
-        new CountFacetRequest(new CategoryPath("Publish Date"), 10), 
-        new CountFacetRequest(new CategoryPath("Author"), 10));
-
-    DrillSideways ds = new DrillSideways(searcher, taxoReader);
-
-    // Simple case: drill-down on a single field; in this
-    // case the drill-sideways + drill-down counts ==
-    // drill-down of just the query: 
-    DrillDownQuery ddq = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
-    ddq.add(new CategoryPath("Author", "Lisa"));
-    DrillSidewaysResult r = ds.search(null, ddq, 10, fsp);
-
-    assertEquals(2, r.hits.totalHits);
-    assertEquals(2, r.facetResults.size());
-    // Publish Date is only drill-down, and Lisa published
-    // one in 2012 and one in 2010:
-    assertEquals("Publish Date: 2012=1 2010=1", toString(r.facetResults.get(0)));
-    // Author is drill-sideways + drill-down: Lisa
-    // (drill-down) published twice, and Frank/Susan/Bob
-    // published once:
-    assertEquals("Author: Lisa=2 Frank=1 Susan=1 Bob=1", toString(r.facetResults.get(1)));
-
-    // Same simple case, but no baseQuery (pure browse):
-    // drill-down on a single field; in this case the
-    // drill-sideways + drill-down counts == drill-down of
-    // just the query:
-    ddq = new DrillDownQuery(fsp.indexingParams);
-    ddq.add(new CategoryPath("Author", "Lisa"));
-    r = ds.search(null, ddq, 10, fsp);
-
-    assertEquals(2, r.hits.totalHits);
-    assertEquals(2, r.facetResults.size());
-    // Publish Date is only drill-down, and Lisa published
-    // one in 2012 and one in 2010:
-    assertEquals("Publish Date: 2012=1 2010=1", toString(r.facetResults.get(0)));
-    assertEquals(2, r.facetResults.get(0).getNumValidDescendants());
-
-    // Author is drill-sideways + drill-down: Lisa
-    // (drill-down) published twice, and Frank/Susan/Bob
-    // published once:
-    assertEquals("Author: Lisa=2 Frank=1 Susan=1 Bob=1", toString(r.facetResults.get(1)));
-    assertEquals(4, r.facetResults.get(1).getNumValidDescendants());
-
-    // Another simple case: drill-down on on single fields
-    // but OR of two values
-    ddq = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
-    ddq.add(new CategoryPath("Author", "Lisa"), new CategoryPath("Author", "Bob"));
-    r = ds.search(null, ddq, 10, fsp);
-    assertEquals(3, r.hits.totalHits);
-    assertEquals(2, r.facetResults.size());
-    // Publish Date is only drill-down: Lisa and Bob
-    // (drill-down) published twice in 2010 and once in 2012:
-    assertEquals("Publish Date: 2010=2 2012=1", toString(r.facetResults.get(0)));
-    // Author is drill-sideways + drill-down: Lisa
-    // (drill-down) published twice, and Frank/Susan/Bob
-    // published once:
-    assertEquals("Author: Lisa=2 Frank=1 Susan=1 Bob=1", toString(r.facetResults.get(1)));
-
-    // More interesting case: drill-down on two fields
-    ddq = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
-    ddq.add(new CategoryPath("Author", "Lisa"));
-    ddq.add(new CategoryPath("Publish Date", "2010"));
-    r = ds.search(null, ddq, 10, fsp);
-    assertEquals(1, r.hits.totalHits);
-    assertEquals(2, r.facetResults.size());
-    // Publish Date is drill-sideways + drill-down: Lisa
-    // (drill-down) published once in 2010 and once in 2012:
-    assertEquals("Publish Date: 2012=1 2010=1", toString(r.facetResults.get(0)));
-    // Author is drill-sideways + drill-down:
-    // only Lisa & Bob published (once each) in 2010:
-    assertEquals("Author: Lisa=1 Bob=1", toString(r.facetResults.get(1)));
-
-    // Even more interesting case: drill down on two fields,
-    // but one of them is OR
-    ddq = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
-
-    // Drill down on Lisa or Bob:
-    ddq.add(new CategoryPath("Author", "Lisa"),
-            new CategoryPath("Author", "Bob"));
-    ddq.add(new CategoryPath("Publish Date", "2010"));
-    r = ds.search(null, ddq, 10, fsp);
-    assertEquals(2, r.hits.totalHits);
-    assertEquals(2, r.facetResults.size());
-    // Publish Date is both drill-sideways + drill-down:
-    // Lisa or Bob published twice in 2010 and once in 2012:
-    assertEquals("Publish Date: 2010=2 2012=1", toString(r.facetResults.get(0)));
-    // Author is drill-sideways + drill-down:
-    // only Lisa & Bob published (once each) in 2010:
-    assertEquals("Author: Lisa=1 Bob=1", toString(r.facetResults.get(1)));
-
-    // Test drilling down on invalid field:
-    ddq = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
-    ddq.add(new CategoryPath("Foobar", "Baz"));
-    fsp = new FacetSearchParams(
-        new CountFacetRequest(new CategoryPath("Publish Date"), 10), 
-        new CountFacetRequest(new CategoryPath("Foobar"), 10));
-    r = ds.search(null, ddq, 10, fsp);
-    assertEquals(0, r.hits.totalHits);
-    assertEquals(2, r.facetResults.size());
-    assertEquals("Publish Date:", toString(r.facetResults.get(0)));
-    assertEquals("Foobar:", toString(r.facetResults.get(1)));
-
-    // Test drilling down on valid term or'd with invalid term:
-    ddq = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
-    ddq.add(new CategoryPath("Author", "Lisa"),
-            new CategoryPath("Author", "Tom"));
-    fsp = new FacetSearchParams(
-        new CountFacetRequest(new CategoryPath("Publish Date"), 10), 
-        new CountFacetRequest(new CategoryPath("Author"), 10));
-    r = ds.search(null, ddq, 10, fsp);
-    assertEquals(2, r.hits.totalHits);
-    assertEquals(2, r.facetResults.size());
-    // Publish Date is only drill-down, and Lisa published
-    // one in 2012 and one in 2010:
-    assertEquals("Publish Date: 2012=1 2010=1", toString(r.facetResults.get(0)));
-    // Author is drill-sideways + drill-down: Lisa
-    // (drill-down) published twice, and Frank/Susan/Bob
-    // published once:
-    assertEquals("Author: Lisa=2 Frank=1 Susan=1 Bob=1", toString(r.facetResults.get(1)));
-
-    // LUCENE-4915: test drilling down on a dimension but
-    // NOT facet counting it:
-    ddq = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
-    ddq.add(new CategoryPath("Author", "Lisa"),
-            new CategoryPath("Author", "Tom"));
-    fsp = new FacetSearchParams(
-              new CountFacetRequest(new CategoryPath("Publish Date"), 10));
-    r = ds.search(null, ddq, 10, fsp);
-    assertEquals(2, r.hits.totalHits);
-    assertEquals(1, r.facetResults.size());
-    // Publish Date is only drill-down, and Lisa published
-    // one in 2012 and one in 2010:
-    assertEquals("Publish Date: 2012=1 2010=1", toString(r.facetResults.get(0)));
-
-    // Test main query gets null scorer:
-    fsp = new FacetSearchParams(
-        new CountFacetRequest(new CategoryPath("Publish Date"), 10), 
-        new CountFacetRequest(new CategoryPath("Author"), 10));
-    ddq = new DrillDownQuery(fsp.indexingParams, new TermQuery(new Term("foobar", "baz")));
-    ddq.add(new CategoryPath("Author", "Lisa"));
-    r = ds.search(null, ddq, 10, fsp);
-
-    assertEquals(0, r.hits.totalHits);
-    assertEquals(2, r.facetResults.size());
-    assertEquals("Publish Date:", toString(r.facetResults.get(0)));
-    assertEquals("Author:", toString(r.facetResults.get(1)));
-
-    searcher.getIndexReader().close();
-    taxoReader.close();
-    dir.close();
-    taxoDir.close();
-  }
-
-  public void testSometimesInvalidDrillDown() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    writer = new RandomIndexWriter(random(), dir);
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    // Reused across documents, to add the necessary facet
-    // fields:
-    facetFields = new FacetFields(taxoWriter);
-
-    add("Author/Bob", "Publish Date/2010/10/15");
-    add("Author/Lisa", "Publish Date/2010/10/20");
-    writer.commit();
-    // 2nd segment has no Author:
-    add("Foobar/Lisa", "Publish Date/2012/1/1");
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    //System.out.println("searcher=" + searcher);
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    // Count both "Publish Date" and "Author" dimensions, in
-    // drill-down:
-    FacetSearchParams fsp = new FacetSearchParams(
-        new CountFacetRequest(new CategoryPath("Publish Date"), 10), 
-        new CountFacetRequest(new CategoryPath("Author"), 10));
-
-    DrillDownQuery ddq = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
-    ddq.add(new CategoryPath("Author", "Lisa"));
-    DrillSidewaysResult r = new DrillSideways(searcher, taxoReader).search(null, ddq, 10, fsp);
-
-    assertEquals(1, r.hits.totalHits);
-    assertEquals(2, r.facetResults.size());
-    // Publish Date is only drill-down, and Lisa published
-    // one in 2012 and one in 2010:
-    assertEquals("Publish Date: 2010=1", toString(r.facetResults.get(0)));
-    // Author is drill-sideways + drill-down: Lisa
-    // (drill-down) published once, and Bob
-    // published once:
-    assertEquals("Author: Lisa=1 Bob=1", toString(r.facetResults.get(1)));
-
-    searcher.getIndexReader().close();
-    taxoReader.close();
-    dir.close();
-    taxoDir.close();
-  }
-
-  private static class Doc implements Comparable<Doc> {
-    String id;
-    String contentToken;
-
-    public Doc() {}
-    
-    // -1 if the doc is missing this dim, else the index
-    // -into the values for this dim:
-    int[] dims;
-
-    // 2nd value per dim for the doc (so we test
-    // multi-valued fields):
-    int[] dims2;
-    boolean deleted;
-
-    @Override
-    public int compareTo(Doc other) {
-      return id.compareTo(other.id);
-    }
-  }
-
-  private double aChance, bChance, cChance;
-
-  private String randomContentToken(boolean isQuery) {
-    double d = random().nextDouble();
-    if (isQuery) {
-      if (d < 0.33) {
-        return "a";
-      } else if (d < 0.66) {
-        return "b";
-      } else {
-        return "c";
-      }
-    } else {
-      if (d <= aChance) {
-        return "a";
-      } else if (d < aChance + bChance) {
-        return "b";
-      } else {
-        return "c";
-      }
-    }
-  }
-
-  public void testMultipleRequestsPerDim() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    writer = new RandomIndexWriter(random(), dir);
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    // Reused across documents, to add the necessary facet
-    // fields:
-    facetFields = new FacetFields(taxoWriter);
-
-    add("dim/a/x");
-    add("dim/a/y");
-    add("dim/a/z");
-    add("dim/b");
-    add("dim/c");
-    add("dim/d");
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    //System.out.println("searcher=" + searcher);
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    // Two requests against the same dim:
-    FacetSearchParams fsp = new FacetSearchParams(
-        new CountFacetRequest(new CategoryPath("dim"), 10), 
-        new CountFacetRequest(new CategoryPath("dim", "a"), 10));
-
-    DrillDownQuery ddq = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
-    ddq.add(new CategoryPath("dim", "a"));
-    DrillSidewaysResult r = new DrillSideways(searcher, taxoReader).search(null, ddq, 10, fsp);
-
-    assertEquals(3, r.hits.totalHits);
-    assertEquals(2, r.facetResults.size());
-    // Publish Date is only drill-down, and Lisa published
-    // one in 2012 and one in 2010:
-    assertEquals("dim: a=3 d=1 c=1 b=1", toString(r.facetResults.get(0)));
-    // Author is drill-sideways + drill-down: Lisa
-    // (drill-down) published twice, and Frank/Susan/Bob
-    // published once:
-    assertEquals("a (3)\n  z (1)\n  y (1)\n  x (1)\n", FacetTestUtils.toSimpleString(r.facetResults.get(1)));
-
-    searcher.getIndexReader().close();
-    taxoReader.close();
-    dir.close();
-    taxoDir.close();
-  }
-
-  public void testRandom() throws Exception {
-
-    boolean canUseDV = defaultCodecSupportsSortedSet();
-
-    while (aChance == 0.0) {
-      aChance = random().nextDouble();
-    }
-    while (bChance == 0.0) {
-      bChance = random().nextDouble();
-    }
-    while (cChance == 0.0) {
-      cChance = random().nextDouble();
-    }
-    /*
-    aChance = .01;
-    bChance = 0.5;
-    cChance = 1.0;
-    */
-    double sum = aChance + bChance + cChance;
-    aChance /= sum;
-    bChance /= sum;
-    cChance /= sum;
-
-    int numDims = _TestUtil.nextInt(random(), 2, 5);
-    //int numDims = 3;
-    int numDocs = atLeast(3000);
-    //int numDocs = 20;
-    if (VERBOSE) {
-      System.out.println("numDims=" + numDims + " numDocs=" + numDocs + " aChance=" + aChance + " bChance=" + bChance + " cChance=" + cChance);
-    }
-    String[][] dimValues = new String[numDims][];
-    int valueCount = 2;
-
-    for(int dim=0;dim<numDims;dim++) {
-      Set<String> values = new HashSet<String>();
-      while (values.size() < valueCount) {
-        String s;
-        while (true) {
-          s = _TestUtil.randomRealisticUnicodeString(random());
-          //s = _TestUtil.randomSimpleString(random());
-          // We cannot include this character else we hit
-          // IllegalArgExc: 
-          if (s.indexOf(FacetIndexingParams.DEFAULT_FACET_DELIM_CHAR) == -1 &&
-              (!canUseDV || s.indexOf('/') == -1)) {
-            break;
-          }
-        }
-        if (s.length() > 0) {
-          values.add(s);
-        }
-      } 
-      dimValues[dim] = values.toArray(new String[values.size()]);
-      valueCount *= 2;
-    }
-
-    List<Doc> docs = new ArrayList<Doc>();
-    for(int i=0;i<numDocs;i++) {
-      Doc doc = new Doc();
-      doc.id = ""+i;
-      doc.contentToken = randomContentToken(false);
-      doc.dims = new int[numDims];
-      doc.dims2 = new int[numDims];
-      for(int dim=0;dim<numDims;dim++) {
-        if (random().nextInt(5) == 3) {
-          // This doc is missing this dim:
-          doc.dims[dim] = -1;
-        } else if (dimValues[dim].length <= 4) {
-          int dimUpto = 0;
-          doc.dims[dim] = dimValues[dim].length-1;
-          while (dimUpto < dimValues[dim].length) {
-            if (random().nextBoolean()) {
-              doc.dims[dim] = dimUpto;
-              break;
-            }
-            dimUpto++;
-          }
-        } else {
-          doc.dims[dim] = random().nextInt(dimValues[dim].length);
-        }
-
-        if (random().nextInt(5) == 3) {
-          // 2nd value:
-          doc.dims2[dim] = random().nextInt(dimValues[dim].length);
-        } else {
-          doc.dims2[dim] = -1;
-        }
-      }
-      docs.add(doc);
-    }
-
-    Directory d = newDirectory();
-    Directory td = newDirectory();
-
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setInfoStream(InfoStream.NO_OUTPUT);
-    RandomIndexWriter w = new RandomIndexWriter(random(), d, iwc);
-    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(td, IndexWriterConfig.OpenMode.CREATE);
-    facetFields = new FacetFields(tw);
-    SortedSetDocValuesFacetFields dvFacetFields = new SortedSetDocValuesFacetFields();
-
-    boolean doUseDV = canUseDV && random().nextBoolean();
-
-    for(Doc rawDoc : docs) {
-      Document doc = new Document();
-      doc.add(newStringField("id", rawDoc.id, Field.Store.YES));
-      doc.add(newStringField("content", rawDoc.contentToken, Field.Store.NO));
-      List<CategoryPath> paths = new ArrayList<CategoryPath>();
-
-      if (VERBOSE) {
-        System.out.println("  doc id=" + rawDoc.id + " token=" + rawDoc.contentToken);
-      }
-      for(int dim=0;dim<numDims;dim++) {
-        int dimValue = rawDoc.dims[dim];
-        if (dimValue != -1) {
-          CategoryPath cp = new CategoryPath("dim" + dim, dimValues[dim][dimValue]);
-          paths.add(cp);
-          doc.add(new StringField("dim" + dim, dimValues[dim][dimValue], Field.Store.YES));
-          if (VERBOSE) {
-            System.out.println("    dim" + dim + "=" + new BytesRef(dimValues[dim][dimValue]));
-          }
-        }
-        int dimValue2 = rawDoc.dims2[dim];
-        if (dimValue2 != -1) {
-          CategoryPath cp = new CategoryPath("dim" + dim, dimValues[dim][dimValue2]);
-          paths.add(cp);
-          doc.add(new StringField("dim" + dim, dimValues[dim][dimValue2], Field.Store.YES));
-          if (VERBOSE) {
-            System.out.println("      dim" + dim + "=" + new BytesRef(dimValues[dim][dimValue2]));
-          }
-        }
-      }
-      if (!paths.isEmpty()) {
-        if (doUseDV) {
-          dvFacetFields.addFields(doc, paths);
-        } else {
-          facetFields.addFields(doc, paths);
-        }
-      }
-
-      w.addDocument(doc);
-    }
-
-    if (random().nextBoolean()) {
-      // Randomly delete a few docs:
-      int numDel = _TestUtil.nextInt(random(), 1, (int) (numDocs*0.05));
-      if (VERBOSE) {
-        System.out.println("delete " + numDel);
-      }
-      int delCount = 0;
-      while (delCount < numDel) {
-        Doc doc = docs.get(random().nextInt(docs.size()));
-        if (!doc.deleted) {
-          if (VERBOSE) {
-            System.out.println("  delete id=" + doc.id);
-          }
-          doc.deleted = true;
-          w.deleteDocuments(new Term("id", doc.id));
-          delCount++;
-        }
-      }
-    }
-
-    if (random().nextBoolean()) {
-      if (VERBOSE) {
-        System.out.println("TEST: forceMerge(1)...");
-      }
-      w.forceMerge(1);
-    }
-    IndexReader r = w.getReader();
-    w.close();
-
-    final SortedSetDocValuesReaderState sortedSetDVState;
-    IndexSearcher s = newSearcher(r);
-    if (doUseDV) {
-      sortedSetDVState = new SortedSetDocValuesReaderState(s.getIndexReader());
-    } else {
-      sortedSetDVState = null;
-    }
-
-    if (VERBOSE) {
-      System.out.println("r.numDocs() = " + r.numDocs());
-    }
-
-    // NRT open
-    TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
-    tw.close();
-
-    int numIters = atLeast(10);
-
-    for(int iter=0;iter<numIters;iter++) {
-
-      String contentToken = random().nextInt(30) == 17 ? null : randomContentToken(true);
-      int numDrillDown = _TestUtil.nextInt(random(), 1, Math.min(4, numDims));
-      if (VERBOSE) {
-        System.out.println("\nTEST: iter=" + iter + " baseQuery=" + contentToken + " numDrillDown=" + numDrillDown + " useSortedSetDV=" + doUseDV);
-      }
-
-      List<FacetRequest> requests = new ArrayList<FacetRequest>();
-      while(true) {
-        for(int i=0;i<numDims;i++) {
-          // LUCENE-4915: sometimes don't request facet
-          // counts on the dim(s) we drill down on
-          if (random().nextDouble() <= 0.9) {
-            if (VERBOSE) {
-              System.out.println("  do facet request on dim=" + i);
-            }
-            requests.add(new CountFacetRequest(new CategoryPath("dim" + i), dimValues[numDims-1].length));
-          } else {
-            if (VERBOSE) {
-              System.out.println("  skip facet request on dim=" + i);
-            }
-          }
-        }
-        if (!requests.isEmpty()) {
-          break;
-        }
-      }
-      FacetSearchParams fsp = new FacetSearchParams(requests);
-      String[][] drillDowns = new String[numDims][];
-
-      int count = 0;
-      boolean anyMultiValuedDrillDowns = false;
-      while (count < numDrillDown) {
-        int dim = random().nextInt(numDims);
-        if (drillDowns[dim] == null) {
-          if (random().nextBoolean()) {
-            // Drill down on one value:
-            drillDowns[dim] = new String[] {dimValues[dim][random().nextInt(dimValues[dim].length)]};
-          } else {
-            int orCount = _TestUtil.nextInt(random(), 1, Math.min(5, dimValues[dim].length));
-            drillDowns[dim] = new String[orCount];
-            anyMultiValuedDrillDowns |= orCount > 1;
-            for(int i=0;i<orCount;i++) {
-              while (true) {
-                String value = dimValues[dim][random().nextInt(dimValues[dim].length)];
-                for(int j=0;j<i;j++) {
-                  if (value.equals(drillDowns[dim][j])) {
-                    value = null;
-                    break;
-                  }
-                }
-                if (value != null) {
-                  drillDowns[dim][i] = value;
-                  break;
-                }
-              }
-            }
-          }
-          if (VERBOSE) {
-            BytesRef[] values = new BytesRef[drillDowns[dim].length];
-            for(int i=0;i<values.length;i++) {
-              values[i] = new BytesRef(drillDowns[dim][i]);
-            }
-            System.out.println("  dim" + dim + "=" + Arrays.toString(values));
-          }
-          count++;
-        }
-      }
-
-      Query baseQuery;
-      if (contentToken == null) {
-        baseQuery = new MatchAllDocsQuery();
-      } else {
-        baseQuery = new TermQuery(new Term("content", contentToken));
-      }
-
-      DrillDownQuery ddq = new DrillDownQuery(fsp.indexingParams, baseQuery);
-
-      for(int dim=0;dim<numDims;dim++) {
-        if (drillDowns[dim] != null) {
-          CategoryPath[] paths = new CategoryPath[drillDowns[dim].length];
-          int upto = 0;
-          for(String value : drillDowns[dim]) {
-            paths[upto++] = new CategoryPath("dim" + dim, value);
-          }
-          ddq.add(paths);
-        }
-      }
-
-      Filter filter;
-      if (random().nextInt(7) == 6) {
-        if (VERBOSE) {
-          System.out.println("  only-even filter");
-        }
-        filter = new Filter() {
-            @Override
-            public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-              int maxDoc = context.reader().maxDoc();
-              final FixedBitSet bits = new FixedBitSet(maxDoc);
-              for(int docID=0;docID < maxDoc;docID++) {
-                // Keeps only the even ids:
-                if ((acceptDocs == null || acceptDocs.get(docID)) && ((Integer.parseInt(context.reader().document(docID).get("id")) & 1) == 0)) {
-                  bits.set(docID);
-                }
-              }
-              return bits;
-            }
-          };
-      } else {
-        filter = null;
-      }
-
-      // Verify docs are always collected in order.  If we
-      // had an AssertingScorer it could catch it when
-      // Weight.scoresDocsOutOfOrder lies!:
-      new DrillSideways(s, tr).search(ddq,
-                           new Collector() {
-                             int lastDocID;
-
-                             @Override
-                             public void setScorer(Scorer s) {
-                             }
-
-                             @Override
-                             public void collect(int doc) {
-                               assert doc > lastDocID;
-                               lastDocID = doc;
-                             }
-
-                             @Override
-                             public void setNextReader(AtomicReaderContext context) {
-                               lastDocID = -1;
-                             }
-
-                             @Override
-                             public boolean acceptsDocsOutOfOrder() {
-                               return false;
-                             }
-                           }, fsp);
-
-      // Also separately verify that DS respects the
-      // scoreSubDocsAtOnce method, to ensure that all
-      // subScorers are on the same docID:
-      if (!anyMultiValuedDrillDowns) {
-        // Can only do this test when there are no OR'd
-        // drill-down values, beacuse in that case it's
-        // easily possible for one of the DD terms to be on
-        // a future docID:
-        new DrillSideways(s, tr) {
-          @Override
-          protected boolean scoreSubDocsAtOnce() {
-            return true;
-          }
-        }.search(ddq, new AssertingSubDocsAtOnceCollector(), fsp);
-      }
-
-      SimpleFacetResult expected = slowDrillSidewaysSearch(s, requests, docs, contentToken, drillDowns, dimValues, filter);
-
-      Sort sort = new Sort(new SortField("id", SortField.Type.STRING));
-      DrillSideways ds;
-      if (doUseDV) {
-        ds = new DrillSideways(s, sortedSetDVState);
-      } else {
-        ds = new DrillSideways(s, tr);
-      }
-
-      // Retrieve all facets:
-      DrillSidewaysResult actual = ds.search(ddq, filter, null, numDocs, sort, true, true, fsp);
-
-      TopDocs hits = s.search(baseQuery, numDocs);
-      Map<String,Float> scores = new HashMap<String,Float>();
-      for(ScoreDoc sd : hits.scoreDocs) {
-        scores.put(s.doc(sd.doc).get("id"), sd.score);
-      }
-      if (VERBOSE) {
-        System.out.println("  verify all facets");
-      }
-      verifyEquals(requests, dimValues, s, expected, actual, scores, -1, doUseDV);
-
-      // Retrieve topN facets:
-      int topN = _TestUtil.nextInt(random(), 1, 20);
-
-      List<FacetRequest> newRequests = new ArrayList<FacetRequest>();
-      for(FacetRequest oldRequest : requests) {
-        newRequests.add(new CountFacetRequest(oldRequest.categoryPath, topN));
-      }
-      fsp = new FacetSearchParams(newRequests);
-      actual = ds.search(ddq, filter, null, numDocs, sort, true, true, fsp);
-      if (VERBOSE) {
-        System.out.println("  verify topN=" + topN);
-      }
-      verifyEquals(newRequests, dimValues, s, expected, actual, scores, topN, doUseDV);
-
-      // Make sure drill down doesn't change score:
-      TopDocs ddqHits = s.search(ddq, filter, numDocs);
-      assertEquals(expected.hits.size(), ddqHits.totalHits);
-      for(int i=0;i<expected.hits.size();i++) {
-        // Score should be IDENTICAL:
-        assertEquals(scores.get(expected.hits.get(i).id), ddqHits.scoreDocs[i].score, 0.0f);
-      }
-    }
-
-    tr.close();
-    r.close();
-    td.close();
-    d.close();
-  }
-
-  private static class Counters {
-    int[][] counts;
-
-    public Counters(String[][] dimValues) {
-      counts = new int[dimValues.length][];
-      for(int dim=0;dim<dimValues.length;dim++) {
-        counts[dim] = new int[dimValues[dim].length];
-      }
-    }
-
-    public void inc(int[] dims, int[] dims2) {
-      inc(dims, dims2, -1);
-    }
-
-    public void inc(int[] dims, int[] dims2, int onlyDim) {
-      assert dims.length == counts.length;
-      assert dims2.length == counts.length;
-      for(int dim=0;dim<dims.length;dim++) {
-        if (onlyDim == -1 || dim == onlyDim) {
-          if (dims[dim] != -1) {
-            counts[dim][dims[dim]]++;
-          }
-          if (dims2[dim] != -1 && dims2[dim] != dims[dim]) {
-            counts[dim][dims2[dim]]++;
-          }
-        }
-      }
-    }
-  }
-
-  private static class SimpleFacetResult {
-    List<Doc> hits;
-    int[][] counts;
-    int[] uniqueCounts;
-    public SimpleFacetResult() {}
-  }
-  
-  private int[] getTopNOrds(final int[] counts, final String[] values, int topN) {
-    final int[] ids = new int[counts.length];
-    for(int i=0;i<ids.length;i++) {
-      ids[i] = i;
-    }
-
-    // Naive (on purpose, to reduce bug in tester/gold):
-    // sort all ids, then return top N slice:
-    new InPlaceMergeSorter() {
-
-      @Override
-      protected void swap(int i, int j) {
-        int id = ids[i];
-        ids[i] = ids[j];
-        ids[j] = id;
-      }
-
-      @Override
-      protected int compare(int i, int j) {
-        int counti = counts[ids[i]];
-        int countj = counts[ids[j]];
-        // Sort by count descending...
-        if (counti > countj) {
-          return -1;
-        } else if (counti < countj) {
-          return 1;
-        } else {
-          // ... then by label ascending:
-          return new BytesRef(values[ids[i]]).compareTo(new BytesRef(values[ids[j]]));
-        }
-      }
-
-    }.sort(0, ids.length);
-
-    if (topN > ids.length) {
-      topN = ids.length;
-    }
-
-    int numSet = topN;
-    for(int i=0;i<topN;i++) {
-      if (counts[ids[i]] == 0) {
-        numSet = i;
-        break;
-      }
-    }
-
-    int[] topNIDs = new int[numSet];
-    System.arraycopy(ids, 0, topNIDs, 0, topNIDs.length);
-    return topNIDs;
-  }
-
-  private SimpleFacetResult slowDrillSidewaysSearch(IndexSearcher s, List<FacetRequest> requests, List<Doc> docs,
-                                                    String contentToken, String[][] drillDowns,
-                                                    String[][] dimValues, Filter onlyEven) throws Exception {
-    int numDims = dimValues.length;
-
-    List<Doc> hits = new ArrayList<Doc>();
-    Counters drillDownCounts = new Counters(dimValues);
-    Counters[] drillSidewaysCounts = new Counters[dimValues.length];
-    for(int dim=0;dim<numDims;dim++) {
-      drillSidewaysCounts[dim] = new Counters(dimValues);
-    }
-
-    if (VERBOSE) {
-      System.out.println("  compute expected");
-    }
-
-    nextDoc: for(Doc doc : docs) {
-      if (doc.deleted) {
-        continue;
-      }
-      if (onlyEven != null & (Integer.parseInt(doc.id) & 1) != 0) {
-        continue;
-      }
-      if (contentToken == null || doc.contentToken.equals(contentToken)) {
-        int failDim = -1;
-        for(int dim=0;dim<numDims;dim++) {
-          if (drillDowns[dim] != null) {
-            String docValue = doc.dims[dim] == -1 ? null : dimValues[dim][doc.dims[dim]];
-            String docValue2 = doc.dims2[dim] == -1 ? null : dimValues[dim][doc.dims2[dim]];
-            boolean matches = false;
-            for(String value : drillDowns[dim]) {
-              if (value.equals(docValue) || value.equals(docValue2)) {
-                matches = true;
-                break;
-              }
-            }
-            if (!matches) {
-              if (failDim == -1) {
-                // Doc could be a near-miss, if no other dim fails
-                failDim = dim;
-              } else {
-                // Doc isn't a hit nor a near-miss
-                continue nextDoc;
-              }
-            }
-          }
-        }
-
-        if (failDim == -1) {
-          if (VERBOSE) {
-            System.out.println("    exp: id=" + doc.id + " is a hit");
-          }
-          // Hit:
-          hits.add(doc);
-          drillDownCounts.inc(doc.dims, doc.dims2);
-          for(int dim=0;dim<dimValues.length;dim++) {
-            drillSidewaysCounts[dim].inc(doc.dims, doc.dims2);
-          }
-        } else {
-          if (VERBOSE) {
-            System.out.println("    exp: id=" + doc.id + " is a near-miss on dim=" + failDim);
-          }
-          drillSidewaysCounts[failDim].inc(doc.dims, doc.dims2, failDim);
-        }
-      }
-    }
-
-    Map<String,Integer> idToDocID = new HashMap<String,Integer>();
-    for(int i=0;i<s.getIndexReader().maxDoc();i++) {
-      idToDocID.put(s.doc(i).get("id"), i);
-    }
-
-    Collections.sort(hits);
-
-    SimpleFacetResult res = new SimpleFacetResult();
-    res.hits = hits;
-    res.counts = new int[numDims][];
-    res.uniqueCounts = new int[numDims];
-    for (int i = 0; i < requests.size(); i++) {
-      int dim = Integer.parseInt(requests.get(i).categoryPath.components[0].substring(3));
-      if (drillDowns[dim] != null) {
-        res.counts[dim] = drillSidewaysCounts[dim].counts[dim];
-      } else {
-        res.counts[dim] = drillDownCounts.counts[dim];
-      }
-      int uniqueCount = 0;
-      for (int j = 0; j < res.counts[dim].length; j++) {
-        if (res.counts[dim][j] != 0) {
-          uniqueCount++;
-        }
-      }
-      res.uniqueCounts[dim] = uniqueCount;
-    }
-
-    return res;
-  }
-
-  void verifyEquals(List<FacetRequest> requests, String[][] dimValues, IndexSearcher s, SimpleFacetResult expected,
-                    DrillSidewaysResult actual, Map<String,Float> scores, int topN, boolean isSortedSetDV) throws Exception {
-    if (VERBOSE) {
-      System.out.println("  verify totHits=" + expected.hits.size());
-    }
-    assertEquals(expected.hits.size(), actual.hits.totalHits);
-    assertEquals(expected.hits.size(), actual.hits.scoreDocs.length);
-    for(int i=0;i<expected.hits.size();i++) {
-      if (VERBOSE) {
-        System.out.println("    hit " + i + " expected=" + expected.hits.get(i).id);
-      }
-      assertEquals(expected.hits.get(i).id,
-                   s.doc(actual.hits.scoreDocs[i].doc).get("id"));
-      // Score should be IDENTICAL:
-      assertEquals(scores.get(expected.hits.get(i).id), actual.hits.scoreDocs[i].score, 0.0f);
-    }
-
-    int numExpected = 0;
-    for(int dim=0;dim<expected.counts.length;dim++) {
-      if (expected.counts[dim] != null) {
-        numExpected++;
-      }
-    }
-
-    assertEquals(numExpected, actual.facetResults.size());
-
-    for(int dim=0;dim<expected.counts.length;dim++) {
-      if (expected.counts[dim] == null) {
-        continue;
-      }
-      int idx = -1;
-      for(int i=0;i<requests.size();i++) {
-        if (Integer.parseInt(requests.get(i).categoryPath.components[0].substring(3)) == dim) {
-          idx = i;
-          break;
-        }
-      }
-      assert idx != -1;
-      FacetResult fr = actual.facetResults.get(idx);
-      List<FacetResultNode> subResults = fr.getFacetResultNode().subResults;
-      if (VERBOSE) {
-        System.out.println("    dim" + dim);
-        System.out.println("      actual");
-      }
-
-      Map<String,Integer> actualValues = new HashMap<String,Integer>();
-      idx = 0;
-      for(FacetResultNode childNode : subResults) {
-        actualValues.put(childNode.label.components[1], (int) childNode.value);
-        if (VERBOSE) {
-          System.out.println("        " + idx + ": " + new BytesRef(childNode.label.components[1]) + ": " + (int) childNode.value);
-          idx++;
-        }
-      }
-
-      if (topN != -1) {
-        int[] topNIDs = getTopNOrds(expected.counts[dim], dimValues[dim], topN);
-        if (VERBOSE) {
-          idx = 0;
-          System.out.println("      expected (sorted)");
-          for(int i=0;i<topNIDs.length;i++) {
-            int expectedOrd = topNIDs[i];
-            String value = dimValues[dim][expectedOrd];
-            System.out.println("        " + idx + ": " + new BytesRef(value) + ": " + expected.counts[dim][expectedOrd]);
-            idx++;
-          }
-        }
-        if (VERBOSE) {
-          System.out.println("      topN=" + topN + " expectedTopN=" + topNIDs.length);
-        }
-
-        assertEquals(topNIDs.length, subResults.size());
-        for(int i=0;i<topNIDs.length;i++) {
-          FacetResultNode node = subResults.get(i);
-          int expectedOrd = topNIDs[i];
-          assertEquals(expected.counts[dim][expectedOrd], (int) node.value);
-          assertEquals(2, node.label.length);
-          if (isSortedSetDV) {
-            // Tie-break facet labels are only in unicode
-            // order with SortedSetDVFacets:
-            assertEquals("value @ idx=" + i, dimValues[dim][expectedOrd], node.label.components[1]);
-          }
-        }
-      } else {
-
-        if (VERBOSE) {
-          idx = 0;
-          System.out.println("      expected (unsorted)");
-          for(int i=0;i<dimValues[dim].length;i++) {
-            String value = dimValues[dim][i];
-            if (expected.counts[dim][i] != 0) {
-              System.out.println("        " + idx + ": " + new BytesRef(value) + ": " + expected.counts[dim][i]);
-              idx++;
-            } 
-          }
-        }
-
-        int setCount = 0;
-        for(int i=0;i<dimValues[dim].length;i++) {
-          String value = dimValues[dim][i];
-          if (expected.counts[dim][i] != 0) {
-            assertTrue(actualValues.containsKey(value));
-            assertEquals(expected.counts[dim][i], actualValues.get(value).intValue());
-            setCount++;
-          } else {
-            assertFalse(actualValues.containsKey(value));
-          }
-        }
-        assertEquals(setCount, actualValues.size());
-      }
-
-      assertEquals("dim=" + dim, expected.uniqueCounts[dim], fr.getNumValidDescendants());
-    }
-  }
-
-  /** Just gathers counts of values under the dim. */
-  private String toString(FacetResult fr) {
-    StringBuilder b = new StringBuilder();
-    FacetResultNode node = fr.getFacetResultNode();
-    b.append(node.label);
-    b.append(":");
-    for(FacetResultNode childNode : node.subResults) {
-      b.append(' ');
-      b.append(childNode.label.components[1]);
-      b.append('=');
-      b.append((int) childNode.value);
-    }
-    return b.toString();
-  }
-  
-  @Test
-  public void testEmptyIndex() throws Exception {
-    // LUCENE-5045: make sure DrillSideways works with an empty index
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    writer = new RandomIndexWriter(random(), dir);
-    taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    // Count "Author"
-    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath("Author"), 10));
-
-    DrillSideways ds = new DrillSideways(searcher, taxoReader);
-    DrillDownQuery ddq = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
-    ddq.add(new CategoryPath("Author", "Lisa"));
-    
-    DrillSidewaysResult r = ds.search(null, ddq, 10, fsp); // this used to fail on IllegalArgEx
-    assertEquals(0, r.hits.totalHits);
-
-    r = ds.search(ddq, null, null, 10, new Sort(new SortField("foo", Type.INT)), false, false, fsp); // this used to fail on IllegalArgEx
-    assertEquals(0, r.hits.totalHits);
-    
-    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);
-  }
-}
-


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetArrays.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetArrays.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetArrays.java	2013-02-20 13:38:17.424711928 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetArrays.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,61 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestFacetArrays extends FacetTestCase {
-
-  @Test
-  public void testFacetArrays() {
-    for (boolean reusing : new boolean[] { false, true }) {
-      final FacetArrays arrays;
-      if (reusing) {
-        arrays = new ReusingFacetArrays(new ArraysPool(1, 1));
-      } else {
-        arrays = new FacetArrays(1);
-      }
-      
-      int[] intArray = arrays.getIntArray();
-      // Set the element, then free
-      intArray[0] = 1;
-      arrays.free();
-      
-      // We should expect a cleared array back
-      int[] newIntArray = arrays.getIntArray();
-      assertEquals("Expected a cleared array back, but the array is still filled", 0, newIntArray[0]);
-      
-      float[] floatArray = arrays.getFloatArray();
-      // Set the element, then free
-      floatArray[0] = 1.0f;
-      arrays.free();
-      
-      // We should expect a cleared array back
-      float[] newFloatArray = arrays.getFloatArray();
-      assertEquals("Expected a cleared array back, but the array is still filled", 0.0f, newFloatArray[0], 0.0);
-      
-      if (reusing) {
-        // same instance should be returned after free()
-        assertSame("ReusingFacetArrays did not reuse the array!", intArray, newIntArray);
-        assertSame("ReusingFacetArrays did not reuse the array!", floatArray, newFloatArray);
-      }
-    }
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector.java	2013-10-17 13:16:00.928867121 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,444 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.List;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.old.AdaptiveFacetsAccumulator;
-import org.apache.lucene.facet.old.OldFacetsAccumulator;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.params.PerDimensionIndexingParams;
-import org.apache.lucene.facet.range.LongRange;
-import org.apache.lucene.facet.range.RangeAccumulator;
-import org.apache.lucene.facet.range.RangeFacetRequest;
-import org.apache.lucene.facet.sampling.RandomSampler;
-import org.apache.lucene.facet.sampling.Sampler;
-import org.apache.lucene.facet.sampling.SamplingAccumulator;
-import org.apache.lucene.facet.sampling.SamplingParams;
-import org.apache.lucene.facet.sampling.SamplingWrapper;
-import org.apache.lucene.facet.sampling.TakmiSampleFixer;
-import org.apache.lucene.facet.search.FacetRequest.ResultMode;
-import org.apache.lucene.facet.sortedset.SortedSetDocValuesAccumulator;
-import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.MultiCollector;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.search.TopScoreDocCollector;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestFacetsCollector extends FacetTestCase {
-
-  @Test
-  public void testSumScoreAggregator() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-
-    FacetFields facetFields = new FacetFields(taxonomyWriter);
-    for(int i = atLeast(30); i > 0; --i) {
-      Document doc = new Document();
-      if (random().nextBoolean()) { // don't match all documents
-        doc.add(new StringField("f", "v", Store.NO));
-      }
-      facetFields.addFields(doc, Collections.singletonList(new CategoryPath("a")));
-      iw.addDocument(doc);
-    }
-    
-    taxonomyWriter.close();
-    iw.close();
-    
-    DirectoryReader r = DirectoryReader.open(indexDir);
-    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-    
-    FacetSearchParams fsp = new FacetSearchParams(new SumScoreFacetRequest(new CategoryPath("a"), 10));
-    FacetsCollector fc = FacetsCollector.create(fsp, r, taxo);
-    TopScoreDocCollector topDocs = TopScoreDocCollector.create(10, false);
-    ConstantScoreQuery csq = new ConstantScoreQuery(new MatchAllDocsQuery());
-    csq.setBoost(2.0f);
-    
-    newSearcher(r).search(csq, MultiCollector.wrap(fc, topDocs));
-    
-    List<FacetResult> res = fc.getFacetResults();
-    float value = (float) res.get(0).getFacetResultNode().value;
-    TopDocs td = topDocs.topDocs();
-    int expected = (int) (td.getMaxScore() * td.totalHits);
-    assertEquals(expected, (int) value);
-    
-    IOUtils.close(taxo, taxoDir, r, indexDir);
-  }
-  
-  @Test
-  public void testMultiCountingLists() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    FacetIndexingParams fip = new PerDimensionIndexingParams(Collections.singletonMap(new CategoryPath("b"), new CategoryListParams("$b")));
-    
-    FacetFields facetFields = new FacetFields(taxonomyWriter, fip);
-    for(int i = atLeast(30); i > 0; --i) {
-      Document doc = new Document();
-      doc.add(new StringField("f", "v", Store.NO));
-      List<CategoryPath> cats = new ArrayList<CategoryPath>();
-      cats.add(new CategoryPath("a"));
-      cats.add(new CategoryPath("b"));
-      facetFields.addFields(doc, cats);
-      iw.addDocument(doc);
-    }
-    
-    taxonomyWriter.close();
-    iw.close();
-    
-    DirectoryReader r = DirectoryReader.open(indexDir);
-    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-    
-    FacetSearchParams sParams = new FacetSearchParams(fip,
-        new CountFacetRequest(new CategoryPath("a"), 10), 
-        new CountFacetRequest(new CategoryPath("b"), 10));
-    FacetsCollector fc = FacetsCollector.create(sParams, r, taxo);
-    newSearcher(r).search(new MatchAllDocsQuery(), fc);
-    
-    for (FacetResult res : fc.getFacetResults()) {
-      assertEquals("unexpected count for " + res, r.maxDoc(), (int) res.getFacetResultNode().value);
-    }
-    
-    IOUtils.close(taxo, taxoDir, r, indexDir);
-  }
-  
-  @Test
-  public void testCountAndSumScore() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    FacetIndexingParams fip = new PerDimensionIndexingParams(Collections.singletonMap(new CategoryPath("b"), new CategoryListParams("$b")));
-    
-    FacetFields facetFields = new FacetFields(taxonomyWriter, fip);
-    for(int i = atLeast(30); i > 0; --i) {
-      Document doc = new Document();
-      doc.add(new StringField("f", "v", Store.NO));
-      List<CategoryPath> cats = new ArrayList<CategoryPath>();
-      cats.add(new CategoryPath("a"));
-      cats.add(new CategoryPath("b"));
-      facetFields.addFields(doc, cats);
-      iw.addDocument(doc);
-    }
-    
-    taxonomyWriter.close();
-    iw.close();
-    
-    DirectoryReader r = DirectoryReader.open(indexDir);
-    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-    
-    FacetSearchParams sParams = new FacetSearchParams(fip,
-        new CountFacetRequest(new CategoryPath("a"), 10), 
-        new SumScoreFacetRequest(new CategoryPath("b"), 10));
-    
-    FacetsCollector fc = FacetsCollector.create(sParams, r, taxo);
-    TopScoreDocCollector topDocs = TopScoreDocCollector.create(10, false);
-    newSearcher(r).search(new MatchAllDocsQuery(), MultiCollector.wrap(fc, topDocs));
-    
-    List<FacetResult> facetResults = fc.getFacetResults();
-    FacetResult fresA = facetResults.get(0);
-    assertEquals("unexpected count for " + fresA, r.maxDoc(), (int) fresA.getFacetResultNode().value);
-    
-    FacetResult fresB = facetResults.get(1);
-    double expected = topDocs.topDocs().getMaxScore() * r.numDocs();
-    assertEquals("unexpected value for " + fresB, expected, fresB.getFacetResultNode().value, 1E-10);
-    
-    IOUtils.close(taxo, taxoDir, r, indexDir);
-  }
-  
-  @Test
-  public void testCountRoot() throws Exception {
-    // LUCENE-4882: FacetsAccumulator threw NPE if a FacetRequest was defined on CP.EMPTY
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    
-    FacetFields facetFields = new FacetFields(taxonomyWriter);
-    for(int i = atLeast(30); i > 0; --i) {
-      Document doc = new Document();
-      facetFields.addFields(doc, Arrays.asList(new CategoryPath("a"), new CategoryPath("b")));
-      iw.addDocument(doc);
-    }
-    
-    taxonomyWriter.close();
-    iw.close();
-    
-    DirectoryReader r = DirectoryReader.open(indexDir);
-    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-    
-    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(CategoryPath.EMPTY, 10));
-    
-    final TaxonomyFacetsAccumulator fa = random().nextBoolean() ? new TaxonomyFacetsAccumulator(fsp, r, taxo) : new OldFacetsAccumulator(fsp, r, taxo);
-    FacetsCollector fc = FacetsCollector.create(fa);
-    newSearcher(r).search(new MatchAllDocsQuery(), fc);
-    
-    FacetResult res = fc.getFacetResults().get(0);
-    for (FacetResultNode node : res.getFacetResultNode().subResults) {
-      assertEquals(r.numDocs(), (int) node.value);
-    }
-    
-    IOUtils.close(taxo, taxoDir, r, indexDir);
-  }
-
-  @Test
-  public void testGetFacetResultsTwice() throws Exception {
-    // LUCENE-4893: counts were multiplied as many times as getFacetResults was called.
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    
-    FacetFields facetFields = new FacetFields(taxonomyWriter);
-    Document doc = new Document();
-    facetFields.addFields(doc, Arrays.asList(new CategoryPath("a/1", '/'), new CategoryPath("b/1", '/')));
-    iw.addDocument(doc);
-    taxonomyWriter.close();
-    iw.close();
-    
-    DirectoryReader r = DirectoryReader.open(indexDir);
-    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-    
-    FacetSearchParams fsp = new FacetSearchParams(
-        new CountFacetRequest(new CategoryPath("a"), 10), 
-        new CountFacetRequest(new CategoryPath("b"), 10));
-    final TaxonomyFacetsAccumulator fa = random().nextBoolean() ? new TaxonomyFacetsAccumulator(fsp, r, taxo) : new OldFacetsAccumulator(fsp, r, taxo);
-    final FacetsCollector fc = FacetsCollector.create(fa);
-    newSearcher(r).search(new MatchAllDocsQuery(), fc);
-    
-    List<FacetResult> res1 = fc.getFacetResults();
-    List<FacetResult> res2 = fc.getFacetResults();
-    assertSame("calling getFacetResults twice should return the exact same result", res1, res2);
-    
-    IOUtils.close(taxo, taxoDir, r, indexDir);
-  }
-  
-  @Test
-  public void testReset() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    
-    FacetFields facetFields = new FacetFields(taxonomyWriter);
-    Document doc = new Document();
-    facetFields.addFields(doc, Arrays.asList(new CategoryPath("a/1", '/'), new CategoryPath("b/1", '/')));
-    iw.addDocument(doc);
-    taxonomyWriter.close();
-    iw.close();
-    
-    DirectoryReader r = DirectoryReader.open(indexDir);
-    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-    
-    FacetSearchParams fsp = new FacetSearchParams(
-        new CountFacetRequest(new CategoryPath("a"), 10), 
-        new CountFacetRequest(new CategoryPath("b"), 10));
-    final TaxonomyFacetsAccumulator fa = random().nextBoolean() ? new TaxonomyFacetsAccumulator(fsp, r, taxo) : new OldFacetsAccumulator(fsp, r, taxo);
-    final FacetsCollector fc = FacetsCollector.create(fa);
-    // this should populate the cached results, but doing search should clear the cache
-    fc.getFacetResults();
-    newSearcher(r).search(new MatchAllDocsQuery(), fc);
-    
-    List<FacetResult> res1 = fc.getFacetResults();
-    // verify that we didn't get the cached result
-    assertEquals(2, res1.size());
-    for (FacetResult res : res1) {
-      assertEquals(1, res.getFacetResultNode().subResults.size());
-      assertEquals(1, (int) res.getFacetResultNode().subResults.get(0).value);
-    }
-    fc.reset();
-    List<FacetResult> res2 = fc.getFacetResults();
-    assertNotSame("reset() should clear the cached results", res1, res2);
-    
-    IOUtils.close(taxo, taxoDir, r, indexDir);
-  }
-  
-  @Test
-  public void testParentOrdinal() throws Exception {
-    // LUCENE-4913: root ordinal was always 0 when all children were requested
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    
-    FacetFields facetFields = new FacetFields(taxonomyWriter);
-    Document doc = new Document();
-    facetFields.addFields(doc, Arrays.asList(new CategoryPath("a/1", '/')));
-    iw.addDocument(doc);
-    taxonomyWriter.close();
-    iw.close();
-    
-    DirectoryReader r = DirectoryReader.open(indexDir);
-    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-
-    // assert IntFacetResultHandler
-    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath("a"), 10));
-    TaxonomyFacetsAccumulator fa = random().nextBoolean() ? new TaxonomyFacetsAccumulator(fsp, r, taxo) : new OldFacetsAccumulator(fsp, r, taxo);
-    FacetsCollector fc = FacetsCollector.create(fa);
-    newSearcher(r).search(new MatchAllDocsQuery(), fc);
-    assertTrue("invalid ordinal for child node: 0", 0 != fc.getFacetResults().get(0).getFacetResultNode().subResults.get(0).ordinal);
-    
-    // assert IntFacetResultHandler
-    fsp = new FacetSearchParams(new SumScoreFacetRequest(new CategoryPath("a"), 10));
-    if (random().nextBoolean()) {
-      fa = new TaxonomyFacetsAccumulator(fsp, r, taxo);
-    } else {
-      fa = new OldFacetsAccumulator(fsp, r, taxo);
-    }
-    fc = FacetsCollector.create(fa);
-    newSearcher(r).search(new MatchAllDocsQuery(), fc);
-    assertTrue("invalid ordinal for child node: 0", 0 != fc.getFacetResults().get(0).getFacetResultNode().subResults.get(0).ordinal);
-    
-    IOUtils.close(taxo, taxoDir, r, indexDir);
-  }
-  
-  @Test
-  public void testNumValidDescendants() throws Exception {
-    // LUCENE-4885: FacetResult.numValidDescendants was not set properly by FacetsAccumulator
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    
-    FacetFields facetFields = new FacetFields(taxonomyWriter);
-    for (int i = 0; i < 10; i++) {
-      Document doc = new Document();
-      facetFields.addFields(doc, Arrays.asList(new CategoryPath("a", Integer.toString(i))));
-      iw.addDocument(doc);
-    }
-    
-    taxonomyWriter.close();
-    iw.close();
-    
-    DirectoryReader r = DirectoryReader.open(indexDir);
-    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-    
-    CountFacetRequest cfr = new CountFacetRequest(new CategoryPath("a"), 2);
-    cfr.setResultMode(random().nextBoolean() ? ResultMode.GLOBAL_FLAT : ResultMode.PER_NODE_IN_TREE);
-    FacetSearchParams fsp = new FacetSearchParams(cfr);
-    final TaxonomyFacetsAccumulator fa = random().nextBoolean() ? new TaxonomyFacetsAccumulator(fsp, r, taxo) : new OldFacetsAccumulator(fsp, r, taxo);
-    FacetsCollector fc = FacetsCollector.create(fa);
-    newSearcher(r).search(new MatchAllDocsQuery(), fc);
-    
-    FacetResult res = fc.getFacetResults().get(0);
-    assertEquals(10, res.getNumValidDescendants());
-    
-    IOUtils.close(taxo, taxoDir, r, indexDir);
-  }
-
-  @Test
-  public void testLabeling() throws Exception {
-    Directory indexDir = newDirectory(), taxoDir = newDirectory();
-
-    // create the index
-    IndexWriter indexWriter = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetFields facetFields = new FacetFields(taxoWriter);
-    Document doc = new Document();
-    facetFields.addFields(doc, Arrays.asList(new CategoryPath("A/1", '/')));
-    indexWriter.addDocument(doc);
-    IOUtils.close(indexWriter, taxoWriter);
-    
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher searcher = new IndexSearcher(indexReader);
-    // ask to count a non-existing category to test labeling
-    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath("B"), 5));
-    
-    final SamplingParams sampleParams = new SamplingParams();
-    sampleParams.setMaxSampleSize(100);
-    sampleParams.setMinSampleSize(100);
-    sampleParams.setSamplingThreshold(100);
-    sampleParams.setOversampleFactor(1.0d);
-    if (random().nextBoolean()) {
-      sampleParams.setSampleFixer(new TakmiSampleFixer(indexReader, taxoReader, fsp));
-    }
-    final Sampler sampler = new RandomSampler(sampleParams, random());
-    
-    TaxonomyFacetsAccumulator[] accumulators = new TaxonomyFacetsAccumulator[] {
-      new TaxonomyFacetsAccumulator(fsp, indexReader, taxoReader),
-      new OldFacetsAccumulator(fsp, indexReader, taxoReader),
-      new SamplingAccumulator(sampler, fsp, indexReader, taxoReader),
-      new AdaptiveFacetsAccumulator(fsp, indexReader, taxoReader),
-      new SamplingWrapper(new OldFacetsAccumulator(fsp, indexReader, taxoReader), sampler)
-    };
-    
-    for (TaxonomyFacetsAccumulator fa : accumulators) {
-      FacetsCollector fc = FacetsCollector.create(fa);
-      searcher.search(new MatchAllDocsQuery(), fc);
-      List<FacetResult> facetResults = fc.getFacetResults();
-      assertNotNull(facetResults);
-      assertEquals("incorrect label returned for " + fa, fsp.facetRequests.get(0).categoryPath, facetResults.get(0).getFacetResultNode().label);
-    }
-    
-    try {
-      // SortedSetDocValuesAccumulator cannot even be created in such state
-      assertNull(new SortedSetDocValuesAccumulator(new SortedSetDocValuesReaderState(indexReader), fsp));
-      // if this ever changes, make sure FacetResultNode is labeled correctly 
-      fail("should not have succeeded to execute a request over a category which wasn't indexed as SortedSetDVField");
-    } catch (IllegalArgumentException e) {
-      // expected
-    }
-
-    RangeAccumulator ra = new RangeAccumulator(new RangeFacetRequest<LongRange>("f", new LongRange("grr", 0, true, 1, true)));
-    FacetsCollector fc = FacetsCollector.create(ra);
-    searcher.search(new MatchAllDocsQuery(), fc);
-    List<FacetResult> facetResults = fc.getFacetResults();
-    assertNotNull(facetResults);
-    assertEquals("incorrect label returned for RangeAccumulator", new CategoryPath("f"), facetResults.get(0).getFacetResultNode().label);
-
-    IOUtils.close(indexReader, taxoReader);
-
-    IOUtils.close(indexDir, taxoDir);
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java	2013-11-24 10:34:27.063676664 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,354 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.params.PerDimensionIndexingParams;
-import org.apache.lucene.facet.search.FacetRequest.ResultMode;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.MultiCollector;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TopScoreDocCollector;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestMultipleCategoryLists extends FacetTestCase {
-
-  private static final CategoryPath[] CATEGORIES = new CategoryPath[] {
-    new CategoryPath("Author", "Mark Twain"),
-    new CategoryPath("Author", "Stephen King"),
-    new CategoryPath("Author", "Kurt Vonnegut"),
-    new CategoryPath("Band", "Rock & Pop", "The Beatles"),
-    new CategoryPath("Band", "Punk", "The Ramones"),
-    new CategoryPath("Band", "Rock & Pop", "U2"),
-    new CategoryPath("Band", "Rock & Pop", "REM"),
-    new CategoryPath("Band", "Rock & Pop", "Dave Matthews Band"),
-    new CategoryPath("Composer", "Bach"),
-  };
-  
-  @Test
-  public void testDefault() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    // create and open an index writer
-    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
-    // create and open a taxonomy writer
-    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
-
-    PerDimensionIndexingParams iParams = new PerDimensionIndexingParams(Collections.<CategoryPath, CategoryListParams>emptyMap());
-
-    seedIndex(iw, tw, iParams);
-
-    IndexReader ir = iw.getReader();
-    tw.commit();
-
-    // prepare index reader and taxonomy.
-    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
-
-    // prepare searcher to search against
-    IndexSearcher searcher = newSearcher(ir);
-
-    FacetsCollector facetsCollector = performSearch(iParams, tr, ir, searcher);
-
-    // Obtain facets results and hand-test them
-    assertCorrectResults(facetsCollector);
-
-    assertOrdinalsExist("$facets", ir);
-
-    IOUtils.close(tr, ir, iw, tw);
-    IOUtils.close(indexDir, taxoDir);
-  }
-
-  @Test
-  public void testCustom() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    // create and open an index writer
-    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
-    // create and open a taxonomy writer
-    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
-
-    PerDimensionIndexingParams iParams = new PerDimensionIndexingParams(
-        Collections.singletonMap(new CategoryPath("Author"), new CategoryListParams("$author")));
-    seedIndex(iw, tw, iParams);
-
-    IndexReader ir = iw.getReader();
-    tw.commit();
-
-    // prepare index reader and taxonomy.
-    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
-
-    // prepare searcher to search against
-    IndexSearcher searcher = newSearcher(ir);
-
-    FacetsCollector facetsCollector = performSearch(iParams, tr, ir, searcher);
-
-    // Obtain facets results and hand-test them
-    assertCorrectResults(facetsCollector);
-
-    assertOrdinalsExist("$facets", ir);
-    assertOrdinalsExist("$author", ir);
-
-    IOUtils.close(tr, ir, iw, tw);
-    IOUtils.close(indexDir, taxoDir);
-  }
-
-  @Test
-  public void testTwoCustomsSameField() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    // create and open an index writer
-    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
-    // create and open a taxonomy writer
-    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
-
-    Map<CategoryPath,CategoryListParams> paramsMap = new HashMap<CategoryPath,CategoryListParams>();
-    paramsMap.put(new CategoryPath("Band"), new CategoryListParams("$music"));
-    paramsMap.put(new CategoryPath("Composer"), new CategoryListParams("$music"));
-    PerDimensionIndexingParams iParams = new PerDimensionIndexingParams(paramsMap);
-    seedIndex(iw, tw, iParams);
-
-    IndexReader ir = iw.getReader();
-    tw.commit();
-
-    // prepare index reader and taxonomy.
-    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
-
-    // prepare searcher to search against
-    IndexSearcher searcher = newSearcher(ir);
-
-    FacetsCollector facetsCollector = performSearch(iParams, tr, ir, searcher);
-
-    // Obtain facets results and hand-test them
-    assertCorrectResults(facetsCollector);
-
-    assertOrdinalsExist("$facets", ir);
-    assertOrdinalsExist("$music", ir);
-    assertOrdinalsExist("$music", ir);
-
-    IOUtils.close(tr, ir, iw, tw);
-    IOUtils.close(indexDir, taxoDir);
-  }
-
-  private void assertOrdinalsExist(String field, IndexReader ir) throws IOException {
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
-      if (r.getBinaryDocValues(field) != null) {
-        return; // not all segments must have this DocValues
-      }
-    }
-    fail("no ordinals found for " + field);
-  }
-
-  @Test
-  public void testDifferentFieldsAndText() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // create and open an index writer
-    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
-    // create and open a taxonomy writer
-    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
-
-    Map<CategoryPath,CategoryListParams> paramsMap = new HashMap<CategoryPath,CategoryListParams>();
-    paramsMap.put(new CategoryPath("Band"), new CategoryListParams("$bands"));
-    paramsMap.put(new CategoryPath("Composer"), new CategoryListParams("$composers"));
-    PerDimensionIndexingParams iParams = new PerDimensionIndexingParams(paramsMap);
-    seedIndex(iw, tw, iParams);
-
-    IndexReader ir = iw.getReader();
-    tw.commit();
-
-    // prepare index reader and taxonomy.
-    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
-
-    // prepare searcher to search against
-    IndexSearcher searcher = newSearcher(ir);
-
-    FacetsCollector facetsCollector = performSearch(iParams, tr, ir, searcher);
-
-    // Obtain facets results and hand-test them
-    assertCorrectResults(facetsCollector);
-    assertOrdinalsExist("$facets", ir);
-    assertOrdinalsExist("$bands", ir);
-    assertOrdinalsExist("$composers", ir);
-
-    IOUtils.close(tr, ir, iw, tw);
-    IOUtils.close(indexDir, taxoDir);
-  }
-
-  @Test
-  public void testSomeSameSomeDifferent() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    // create and open an index writer
-    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
-    // create and open a taxonomy writer
-    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
-
-    Map<CategoryPath,CategoryListParams> paramsMap = new HashMap<CategoryPath,CategoryListParams>();
-    paramsMap.put(new CategoryPath("Band"), new CategoryListParams("$music"));
-    paramsMap.put(new CategoryPath("Composer"), new CategoryListParams("$music"));
-    paramsMap.put(new CategoryPath("Author"), new CategoryListParams("$literature"));
-    PerDimensionIndexingParams iParams = new PerDimensionIndexingParams(paramsMap);
-
-    seedIndex(iw, tw, iParams);
-
-    IndexReader ir = iw.getReader();
-    tw.commit();
-
-    // prepare index reader and taxonomy.
-    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
-
-    // prepare searcher to search against
-    IndexSearcher searcher = newSearcher(ir);
-
-    FacetsCollector facetsCollector = performSearch(iParams, tr, ir, searcher);
-
-    // Obtain facets results and hand-test them
-    assertCorrectResults(facetsCollector);
-    assertOrdinalsExist("$music", ir);
-    assertOrdinalsExist("$literature", ir);
-
-    IOUtils.close(tr, ir, iw, tw);
-    IOUtils.close(indexDir, taxoDir);
-  }
-
-  private void assertCorrectResults(FacetsCollector facetsCollector) throws IOException {
-    List<FacetResult> res = facetsCollector.getFacetResults();
-
-    FacetResult results = res.get(0);
-    FacetResultNode resNode = results.getFacetResultNode();
-    Iterable<? extends FacetResultNode> subResults = resNode.subResults;
-    Iterator<? extends FacetResultNode> subIter = subResults.iterator();
-
-    checkResult(subIter.next(), "Band/Rock & Pop", 4.0);
-    checkResult(subIter.next(), "Band/Punk", 1.0);
-
-    results = res.get(1);
-    resNode = results.getFacetResultNode();
-    subResults = resNode.subResults;
-    subIter = subResults.iterator();
-
-    checkResult(subIter.next(), "Band/Rock & Pop", 4.0);
-    checkResult(subIter.next(), "Band/Rock & Pop/Dave Matthews Band", 1.0);
-    checkResult(subIter.next(), "Band/Rock & Pop/REM", 1.0);
-    checkResult(subIter.next(), "Band/Rock & Pop/U2", 1.0);
-    checkResult(subIter.next(), "Band/Punk/The Ramones", 1.0);
-    checkResult(subIter.next(), "Band/Punk", 1.0);
-    checkResult(subIter.next(), "Band/Rock & Pop/The Beatles", 1.0);
-
-    results = res.get(2);
-    resNode = results.getFacetResultNode();
-    subResults = resNode.subResults;
-    subIter = subResults.iterator();
-
-    checkResult(subIter.next(), "Author/Kurt Vonnegut", 1.0);
-    checkResult(subIter.next(), "Author/Stephen King", 1.0);
-    checkResult(subIter.next(), "Author/Mark Twain", 1.0);
-
-    results = res.get(3);
-    resNode = results.getFacetResultNode();
-    subResults = resNode.subResults;
-    subIter = subResults.iterator();
-
-    checkResult(subIter.next(), "Band/Rock & Pop/Dave Matthews Band", 1.0);
-    checkResult(subIter.next(), "Band/Rock & Pop/REM", 1.0);
-    checkResult(subIter.next(), "Band/Rock & Pop/U2", 1.0);
-    checkResult(subIter.next(), "Band/Rock & Pop/The Beatles", 1.0);
-  }
-
-  private FacetsCollector performSearch(FacetIndexingParams iParams, TaxonomyReader tr, IndexReader ir, 
-      IndexSearcher searcher) throws IOException {
-    // step 1: collect matching documents into a collector
-    Query q = new MatchAllDocsQuery();
-    TopScoreDocCollector topDocsCollector = TopScoreDocCollector.create(10, true);
-
-    List<FacetRequest> facetRequests = new ArrayList<FacetRequest>();
-    facetRequests.add(new CountFacetRequest(new CategoryPath("Band"), 10));
-    CountFacetRequest bandDepth = new CountFacetRequest(new CategoryPath("Band"), 10);
-    bandDepth.setDepth(2);
-    // makes it easier to check the results in the test.
-    bandDepth.setResultMode(ResultMode.GLOBAL_FLAT);
-    facetRequests.add(bandDepth);
-    facetRequests.add(new CountFacetRequest(new CategoryPath("Author"), 10));
-    facetRequests.add(new CountFacetRequest(new CategoryPath("Band", "Rock & Pop"), 10));
-
-    // Faceted search parameters indicate which facets are we interested in
-    FacetSearchParams facetSearchParams = new FacetSearchParams(iParams, facetRequests);
-
-    // perform documents search and facets accumulation
-    FacetsCollector facetsCollector = FacetsCollector.create(facetSearchParams, ir, tr);
-    searcher.search(q, MultiCollector.wrap(topDocsCollector, facetsCollector));
-    return facetsCollector;
-  }
-
-  private void seedIndex(RandomIndexWriter iw, TaxonomyWriter tw, FacetIndexingParams iParams) throws IOException {
-    FacetFields facetFields = new FacetFields(tw, iParams);
-    for (CategoryPath cp : CATEGORIES) {
-      Document doc = new Document();
-      facetFields.addFields(doc, Collections.singletonList(cp));
-      doc.add(new TextField("content", "alpha", Field.Store.YES));
-      iw.addDocument(doc);
-    }
-  }
-
-  private static void checkResult(FacetResultNode sub, String label, double value) {
-    assertEquals("Label of subresult " + sub.label + " was incorrect", label, sub.label.toString());
-    assertEquals("Value for " + sub.label + " subresult was incorrect", value, sub.value, 0.0);
-  }
-
-}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestSameRequestAccumulation.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestSameRequestAccumulation.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestSameRequestAccumulation.java	2013-02-20 13:38:17.424711928 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestSameRequestAccumulation.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,76 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.util.List;
-
-import org.apache.lucene.facet.FacetTestBase;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.junit.After;
-import org.junit.Before;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestSameRequestAccumulation extends FacetTestBase {
-  
-  private FacetIndexingParams fip;
-  
-  @Override
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-    fip = getFacetIndexingParams(Integer.MAX_VALUE);
-    initIndex(fip);
-  }
-  
-  // Following LUCENE-4461 - ensure requesting the (exact) same request more
-  // than once does not alter the results
-  public void testTwoSameRequests() throws Exception {
-    final CountFacetRequest facetRequest = new CountFacetRequest(new CategoryPath("root"), 10);
-    FacetSearchParams fsp = new FacetSearchParams(fip, facetRequest);
-    
-    FacetsCollector fc = FacetsCollector.create(fsp, indexReader, taxoReader);
-    searcher.search(new MatchAllDocsQuery(), fc);
-    
-    final String expected = fc.getFacetResults().get(0).toString();
-
-    // now add the same facet request with duplicates (same instance and same one)
-    fsp = new FacetSearchParams(fip, facetRequest, facetRequest, new CountFacetRequest(new CategoryPath("root"), 10));
-
-    // make sure the search params holds 3 requests now
-    assertEquals(3, fsp.facetRequests.size());
-    
-    fc = FacetsCollector.create(fsp, indexReader, taxoReader);
-    searcher.search(new MatchAllDocsQuery(), fc);
-    List<FacetResult> actual = fc.getFacetResults();
-
-    // all 3 results should have the same toString()
-    assertEquals("same FacetRequest but different result?", expected, actual.get(0).toString());
-    assertEquals("same FacetRequest but different result?", expected, actual.get(1).toString());
-    assertEquals("same FacetRequest but different result?", expected, actual.get(2).toString());
-  }
-  
-  @Override
-  @After
-  public void tearDown() throws Exception {
-    closeAll();
-    super.tearDown();
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestSearcherTaxonomyManager.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestSearcherTaxonomyManager.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestSearcherTaxonomyManager.java	2013-04-22 16:59:20.055676456 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestSearcherTaxonomyManager.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,193 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.SearcherTaxonomyManager.SearcherAndTaxonomy;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-
-public class TestSearcherTaxonomyManager extends FacetTestCase {
-  public void test() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    final IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    final DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
-    final FacetFields facetFields = new FacetFields(tw);
-    final AtomicBoolean stop = new AtomicBoolean();
-
-    // How many unique facets to index before stopping:
-    final int ordLimit = TEST_NIGHTLY ? 100000 : 6000;
-
-    Thread indexer = new Thread() {
-        @Override
-        public void run() {
-          try {
-            Set<String> seen = new HashSet<String>();
-            List<String> paths = new ArrayList<String>();
-            while (true) {
-              Document doc = new Document();
-              List<CategoryPath> docPaths = new ArrayList<CategoryPath>();
-              int numPaths = _TestUtil.nextInt(random(), 1, 5);
-              for(int i=0;i<numPaths;i++) {
-                String path;
-                if (!paths.isEmpty() && random().nextInt(5) != 4) {
-                  // Use previous path
-                  path = paths.get(random().nextInt(paths.size()));
-                } else {
-                  // Create new path
-                  path = null;
-                  while (true) {
-                    path = _TestUtil.randomRealisticUnicodeString(random());
-                    if (path.length() != 0 && !seen.contains(path) && path.indexOf(FacetIndexingParams.DEFAULT_FACET_DELIM_CHAR) == -1) {
-                      seen.add(path);
-                      paths.add(path);
-                      break;
-                    }
-                  }
-                }
-                docPaths.add(new CategoryPath("field", path));
-              }
-              try {
-                facetFields.addFields(doc, docPaths);
-                w.addDocument(doc);
-              } catch (IOException ioe) {
-                throw new RuntimeException(ioe);
-              }
-
-              if (tw.getSize() >= ordLimit) {
-                break;
-              }
-            }
-          } finally {
-            stop.set(true);
-          }
-        }
-      };
-
-    final SearcherTaxonomyManager mgr = new SearcherTaxonomyManager(w, true, null, tw);
-
-    Thread reopener = new Thread() {
-        @Override
-        public void run() {
-          while(!stop.get()) {
-            try {
-              // Sleep for up to 20 msec:
-              Thread.sleep(random().nextInt(20));
-
-              if (VERBOSE) {
-                System.out.println("TEST: reopen");
-              }
-
-              mgr.maybeRefresh();
-
-              if (VERBOSE) {
-                System.out.println("TEST: reopen done");
-              }
-            } catch (Exception ioe) {
-              throw new RuntimeException(ioe);
-            }
-          }
-        }
-      };
-    reopener.start();
-
-    indexer.start();
-
-    try {
-      while (!stop.get()) {
-        SearcherAndTaxonomy pair = mgr.acquire();
-        try {
-          //System.out.println("search maxOrd=" + pair.taxonomyReader.getSize());
-          int topN = _TestUtil.nextInt(random(), 1, 20);
-          CountFacetRequest cfr = new CountFacetRequest(new CategoryPath("field"), topN);
-          FacetSearchParams fsp = new FacetSearchParams(cfr);
-          FacetsCollector fc = FacetsCollector.create(fsp, pair.searcher.getIndexReader(), pair.taxonomyReader);
-          pair.searcher.search(new MatchAllDocsQuery(), fc);
-          List<FacetResult> results = fc.getFacetResults();
-          FacetResult fr = results.get(0);
-          FacetResultNode root = results.get(0).getFacetResultNode();
-          assertTrue(root.ordinal != 0);
-
-          if (pair.searcher.getIndexReader().numDocs() > 0) { 
-            //System.out.println(pair.taxonomyReader.getSize());
-            assertTrue(fr.getNumValidDescendants() > 0);
-            assertFalse(root.subResults.isEmpty());
-          }
-
-          //if (VERBOSE) {
-          //System.out.println("TEST: facets=" + FacetTestUtils.toSimpleString(results.get(0)));
-          //}
-        } finally {
-          mgr.release(pair);
-        }
-      }
-    } finally {
-      indexer.join();
-      reopener.join();
-    }
-
-    if (VERBOSE) {
-      System.out.println("TEST: now stop");
-    }
-
-    IOUtils.close(mgr, tw, w, taxoDir, dir);
-  }
-
-  public void testReplaceTaxonomy() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
-
-    Directory taxoDir2 = newDirectory();
-    DirectoryTaxonomyWriter tw2 = new DirectoryTaxonomyWriter(taxoDir2);
-    tw2.close();
-
-    SearcherTaxonomyManager mgr = new SearcherTaxonomyManager(w, true, null, tw);
-    w.addDocument(new Document());
-    tw.replaceTaxonomy(taxoDir2);
-    taxoDir2.close();
-
-    try {
-      mgr.maybeRefresh();
-      fail("should have hit exception");
-    } catch (IllegalStateException ise) {
-      // expected
-    }
-
-    IOUtils.close(mgr, tw, w, taxoDir, dir);
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestStandardFacetsAccumulator.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestStandardFacetsAccumulator.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestStandardFacetsAccumulator.java	2013-04-22 16:59:20.055676456 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestStandardFacetsAccumulator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,116 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.List;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.facet.util.AssertingCategoryListIterator;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.NoMergePolicy;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestStandardFacetsAccumulator extends FacetTestCase {
-  
-  private void indexTwoDocs(IndexWriter indexWriter, FacetFields facetFields, boolean withContent) throws Exception {
-    for (int i = 0; i < 2; i++) {
-      Document doc = new Document();
-      if (withContent) {
-        doc.add(new StringField("f", "a", Store.NO));
-      }
-      if (facetFields != null) {
-        facetFields.addFields(doc, Collections.singletonList(new CategoryPath("A", Integer.toString(i))));
-      }
-      indexWriter.addDocument(doc);
-    }
-    
-    indexWriter.commit();
-  }
-  
-  @Test
-  public void testSegmentsWithoutCategoriesOrResults() throws Exception {
-    // tests the accumulator when there are segments with no results
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // prevent merges
-    IndexWriter indexWriter = new IndexWriter(indexDir, iwc);
-    FacetIndexingParams fip = new FacetIndexingParams(new CategoryListParams() {
-      @Override
-      public CategoryListIterator createCategoryListIterator(int partition) throws IOException {
-        return new AssertingCategoryListIterator(super.createCategoryListIterator(partition));
-      }
-    });
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetFields facetFields = new FacetFields(taxoWriter, fip);
-    indexTwoDocs(indexWriter, facetFields, false); // 1st segment, no content, with categories
-    indexTwoDocs(indexWriter, null, true);         // 2nd segment, with content, no categories
-    indexTwoDocs(indexWriter, facetFields, true);  // 3rd segment ok
-    indexTwoDocs(indexWriter, null, false);        // 4th segment, no content, or categories
-    indexTwoDocs(indexWriter, null, true);         // 5th segment, with content, no categories
-    indexTwoDocs(indexWriter, facetFields, true);  // 6th segment, with content, with categories
-    indexTwoDocs(indexWriter, null, true);         // 7th segment, with content, no categories
-    IOUtils.close(indexWriter, taxoWriter);
-
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher indexSearcher = newSearcher(indexReader);
-    
-    // search for "f:a", only segments 1 and 3 should match results
-    Query q = new TermQuery(new Term("f", "a"));
-    FacetRequest countNoComplements = new CountFacetRequest(new CategoryPath("A"), 10);
-    FacetSearchParams fsp = new FacetSearchParams(fip, countNoComplements);
-    FacetsCollector fc = FacetsCollector.create(fsp , indexReader, taxoReader);
-    indexSearcher.search(q, fc);
-    List<FacetResult> results = fc.getFacetResults();
-    assertEquals("received too many facet results", 1, results.size());
-    FacetResultNode frn = results.get(0).getFacetResultNode();
-    assertEquals("wrong number of children", 2, frn.subResults.size());
-    for (FacetResultNode node : frn.subResults) {
-      assertEquals("wrong weight for child " + node.label, 2, (int) node.value);
-    }
-    IOUtils.close(indexReader, taxoReader);
-    
-    IOUtils.close(indexDir, taxoDir);
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestSumValueSourceFacetRequest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestSumValueSourceFacetRequest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestSumValueSourceFacetRequest.java	2013-11-06 07:02:49.341619591 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestSumValueSourceFacetRequest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,185 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.FacetTestUtils;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.queries.function.FunctionQuery;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
-import org.apache.lucene.queries.function.valuesource.LongFieldSource;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.MultiCollector;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.search.TopScoreDocCollector;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestSumValueSourceFacetRequest extends FacetTestCase {
-
-  @Test
-  public void testNoScore() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-
-    FacetFields facetFields = new FacetFields(taxonomyWriter);
-    for (int i = 0; i < 4; i++) {
-      Document doc = new Document();
-      doc.add(new NumericDocValuesField("price", (i+1)));
-      facetFields.addFields(doc, Collections.singletonList(new CategoryPath("a", Integer.toString(i % 2))));
-      iw.addDocument(doc);
-    }
-    
-    taxonomyWriter.close();
-    iw.close();
-    
-    DirectoryReader r = DirectoryReader.open(indexDir);
-    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-
-    ValueSource valueSource = new LongFieldSource("price");
-    FacetSearchParams fsp = new FacetSearchParams(new SumValueSourceFacetRequest(new CategoryPath("a"), 10, valueSource, false));
-    FacetsCollector fc = FacetsCollector.create(fsp, r, taxo);
-    newSearcher(r).search(new MatchAllDocsQuery(), fc);
-    
-    List<FacetResult> res = fc.getFacetResults();
-    assertEquals("a (0)\n  1 (6)\n  0 (4)\n", FacetTestUtils.toSimpleString(res.get(0)));
-    
-    IOUtils.close(taxo, taxoDir, r, indexDir);
-  }
-
-  @Test
-  public void testWithScore() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-
-    FacetFields facetFields = new FacetFields(taxonomyWriter);
-    for (int i = 0; i < 4; i++) {
-      Document doc = new Document();
-      doc.add(new NumericDocValuesField("price", (i+1)));
-      facetFields.addFields(doc, Collections.singletonList(new CategoryPath("a", Integer.toString(i % 2))));
-      iw.addDocument(doc);
-    }
-    
-    taxonomyWriter.close();
-    iw.close();
-    
-    DirectoryReader r = DirectoryReader.open(indexDir);
-    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-
-    ValueSource valueSource = new ValueSource() {
-      @Override
-      public FunctionValues getValues(@SuppressWarnings("rawtypes") Map context, AtomicReaderContext readerContext) throws IOException {
-        final Scorer scorer = (Scorer) context.get("scorer");
-        assert scorer != null;
-        return new DoubleDocValues(this) {
-          @Override
-          public double doubleVal(int document) {
-            try {
-              return scorer.score();
-            } catch (IOException exception) {
-              throw new RuntimeException(exception);
-            }
-          }
-        };
-      }
-
-      @Override public boolean equals(Object o) { return o == this; }
-      @Override public int hashCode() { return System.identityHashCode(this); }
-      @Override public String description() { return "score()"; }
-    };
-    
-    FacetSearchParams fsp = new FacetSearchParams(new SumValueSourceFacetRequest(new CategoryPath("a"), 10, valueSource, true));
-    FacetsCollector fc = FacetsCollector.create(fsp, r, taxo);
-    TopScoreDocCollector tsdc = TopScoreDocCollector.create(10, true);
-    // score documents by their 'price' field - makes asserting the correct counts for the categories easier
-    Query q = new FunctionQuery(new LongFieldSource("price"));
-    newSearcher(r).search(q, MultiCollector.wrap(tsdc, fc));
-    
-    List<FacetResult> res = fc.getFacetResults();
-    assertEquals("a (0)\n  1 (6)\n  0 (4)\n", FacetTestUtils.toSimpleString(res.get(0)));
-    
-    IOUtils.close(taxo, taxoDir, r, indexDir);
-  }
-
-  @Test
-  public void testRollupValues() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    FacetIndexingParams fip = new FacetIndexingParams(new CategoryListParams() {
-      @Override
-      public OrdinalPolicy getOrdinalPolicy(String dimension) {
-        return OrdinalPolicy.NO_PARENTS;
-      }
-    });
-    FacetFields facetFields = new FacetFields(taxonomyWriter, fip);
-    for (int i = 0; i < 4; i++) {
-      Document doc = new Document();
-      doc.add(new NumericDocValuesField("price", (i+1)));
-      facetFields.addFields(doc, Collections.singletonList(new CategoryPath("a", Integer.toString(i % 2), "1")));
-      iw.addDocument(doc);
-    }
-    
-    taxonomyWriter.close();
-    iw.close();
-    
-    DirectoryReader r = DirectoryReader.open(indexDir);
-    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-
-    ValueSource valueSource = new LongFieldSource("price");
-    FacetSearchParams fsp = new FacetSearchParams(fip, new SumValueSourceFacetRequest(new CategoryPath("a"), 10, valueSource, false));
-    FacetsCollector fc = FacetsCollector.create(fsp, r, taxo);
-    newSearcher(r).search(new MatchAllDocsQuery(), fc);
-    
-    List<FacetResult> res = fc.getFacetResults();
-    assertEquals("a (10)\n  1 (6)\n  0 (4)\n", FacetTestUtils.toSimpleString(res.get(0)));
-    
-    IOUtils.close(taxo, taxoDir, r, indexDir);
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java	2013-08-01 14:47:20.730689730 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,304 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.old.OldFacetsAccumulator;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.FacetRequest.ResultMode;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.facet.util.PartitionsUtils;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.store.Directory;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestTopKInEachNodeResultHandler extends FacetTestCase {
-
-  //TODO (Facet): Move to extend BaseTestTopK and separate to several smaller test cases (methods) - see TestTopKResultsHandler
-  
-  @Test
-  public void testSimple() throws Exception {
-
-    int[] partitionSizes = new int[] { 
-        2,3,4, 5, 6, 7, 10, 1000,
-        Integer.MAX_VALUE };
-
-    for (int partitionSize : partitionSizes) {
-      Directory iDir = newDirectory();
-      Directory tDir = newDirectory();
-
-      if (VERBOSE) {
-        System.out.println("Partition Size: " + partitionSize);
-      }
-      
-      final int pSize = partitionSize;
-      FacetIndexingParams iParams = new FacetIndexingParams() {
-        @Override
-        public int getPartitionSize() {
-          return pSize;
-        }
-      };
-
-      RandomIndexWriter iw = new RandomIndexWriter(random(), iDir,
-          newIndexWriterConfig(TEST_VERSION_CURRENT,
-              new MockAnalyzer(random())).setOpenMode(OpenMode.CREATE));
-      TaxonomyWriter tw = new DirectoryTaxonomyWriter(tDir);
-      prvt_add(iParams, iw, tw, "a", "b");
-      prvt_add(iParams, iw, tw, "a", "b", "1");
-      prvt_add(iParams, iw, tw, "a", "b", "1");
-      prvt_add(iParams, iw, tw, "a", "b", "2");
-      prvt_add(iParams, iw, tw, "a", "b", "2");
-      prvt_add(iParams, iw, tw, "a", "b", "2");
-      prvt_add(iParams, iw, tw, "a", "b", "3");
-      prvt_add(iParams, iw, tw, "a", "b", "4");
-      prvt_add(iParams, iw, tw, "a", "c");
-      prvt_add(iParams, iw, tw, "a", "c");
-      prvt_add(iParams, iw, tw, "a", "c");
-      prvt_add(iParams, iw, tw, "a", "c");
-      prvt_add(iParams, iw, tw, "a", "c");
-      prvt_add(iParams, iw, tw, "a", "c", "1");
-      prvt_add(iParams, iw, tw, "a", "d");
-      prvt_add(iParams, iw, tw, "a", "e");
-
-      IndexReader ir = iw.getReader();
-      iw.close();
-      tw.commit();
-      tw.close();
-
-      IndexSearcher is = newSearcher(ir);
-      DirectoryTaxonomyReader tr = new DirectoryTaxonomyReader(tDir);
-
-      // Get all of the documents and run the query, then do different
-      // facet counts and compare to control
-      Query q = new TermQuery(new Term("content", "alpha"));
-
-      CountFacetRequest cfra23 = new CountFacetRequest(new CategoryPath("a"), 2);
-      cfra23.setDepth(3);
-      cfra23.setResultMode(ResultMode.PER_NODE_IN_TREE);
-
-      CountFacetRequest cfra22 = new CountFacetRequest(new CategoryPath("a"), 2);
-      cfra22.setDepth(2);
-      cfra22.setResultMode(ResultMode.PER_NODE_IN_TREE);
-
-      CountFacetRequest cfra21 = new CountFacetRequest(new CategoryPath("a"), 2);
-      cfra21.setDepth(1);
-      cfra21.setResultMode(ResultMode.PER_NODE_IN_TREE);
-
-      CountFacetRequest cfrb22 = new CountFacetRequest(new CategoryPath("a", "b"), 2);
-      cfrb22.setDepth(2);
-      cfrb22.setResultMode(ResultMode.PER_NODE_IN_TREE);
-
-      CountFacetRequest cfrb23 = new CountFacetRequest(new CategoryPath("a", "b"), 2);
-      cfrb23.setDepth(3);
-      cfrb23.setResultMode(ResultMode.PER_NODE_IN_TREE);
-
-      CountFacetRequest cfrb21 = new CountFacetRequest(new CategoryPath("a", "b"), 2);
-      cfrb21.setDepth(1);
-      cfrb21.setResultMode(ResultMode.PER_NODE_IN_TREE);
-
-      CountFacetRequest doctor = new CountFacetRequest(new CategoryPath("Doctor"), 2);
-      doctor.setDepth(1);
-      doctor.setResultMode(ResultMode.PER_NODE_IN_TREE);
-
-      CountFacetRequest cfrb20 = new CountFacetRequest(new CategoryPath("a", "b"), 2);
-      cfrb20.setDepth(0);
-      cfrb20.setResultMode(ResultMode.PER_NODE_IN_TREE);
-
-      List<FacetRequest> facetRequests = new ArrayList<FacetRequest>();
-      facetRequests.add(cfra23);
-      facetRequests.add(cfra22);
-      facetRequests.add(cfra21);
-      facetRequests.add(cfrb23);
-      facetRequests.add(cfrb22);
-      facetRequests.add(cfrb21);
-      facetRequests.add(doctor);
-      facetRequests.add(cfrb20);
-      FacetSearchParams facetSearchParams = new FacetSearchParams(iParams, facetRequests);
-      
-      FacetArrays facetArrays = new FacetArrays(PartitionsUtils.partitionSize(facetSearchParams.indexingParams, tr));
-      OldFacetsAccumulator sfa = new OldFacetsAccumulator(facetSearchParams, is.getIndexReader(), tr, facetArrays);
-      sfa.setComplementThreshold(OldFacetsAccumulator.DISABLE_COMPLEMENT);
-      FacetsCollector fc = FacetsCollector.create(sfa);
-      
-      is.search(q, fc);
-      List<FacetResult> facetResults = fc.getFacetResults();
-
-      FacetResult fr = facetResults.get(0); // a, depth=3, K=2
-      boolean hasDoctor = "Doctor".equals(fr.getFacetRequest().categoryPath.components[0]);
-      assertEquals(9, fr.getNumValidDescendants());
-      FacetResultNode parentRes = fr.getFacetResultNode();
-      assertEquals(2, parentRes.subResults.size());
-      // two nodes sorted by descending values: a/b with 8  and a/c with 6
-      // a/b has two children a/b/2 with value 3, and a/b/1 with value 2. 
-      // a/c has one child a/c/1 with value 1.
-      double [] expectedValues0 = { 8.0, 3.0, 2.0, 6.0, 1.0 };
-      int i = 0;
-      for (FacetResultNode node : parentRes.subResults) {
-        assertEquals(expectedValues0[i++], node.value, Double.MIN_VALUE);
-        for (FacetResultNode node2 : node.subResults) {
-          assertEquals(expectedValues0[i++], node2.value, Double.MIN_VALUE);
-        }
-      }
-
-      // now just change the value of the first child of the root to 5, and then rearrange
-      // expected are: first a/c of value 6, and one child a/c/1 with value 1
-      // then a/b with value 5, and both children: a/b/2 with value 3, and a/b/1 with value 2.
-      for (FacetResultNode node : parentRes.subResults) {
-        node.value = 5.0;
-        break;
-      }
-      // now rearrange
-      double [] expectedValues00 = { 6.0, 1.0, 5.0, 3.0, 2.0 };
-      fr = sfa.createFacetResultsHandler(cfra23, sfa.createOrdinalValueResolver(cfra23)).rearrangeFacetResult(fr);
-      i = 0;
-      for (FacetResultNode node : parentRes.subResults) {
-        assertEquals(expectedValues00[i++], node.value, Double.MIN_VALUE);
-        for (FacetResultNode node2 : node.subResults) {
-          assertEquals(expectedValues00[i++], node2.value, Double.MIN_VALUE);
-        }
-      }
-
-      fr = facetResults.get(1); // a, depth=2, K=2. same result as before
-      hasDoctor |= "Doctor".equals(fr.getFacetRequest().categoryPath.components[0]);
-      assertEquals(9, fr.getNumValidDescendants());
-      parentRes = fr.getFacetResultNode();
-      assertEquals(2, parentRes.subResults.size());
-      // two nodes sorted by descending values: a/b with 8  and a/c with 6
-      // a/b has two children a/b/2 with value 3, and a/b/1 with value 2. 
-      // a/c has one child a/c/1 with value 1.
-      i = 0;
-      for (FacetResultNode node : parentRes.subResults) {
-        assertEquals(expectedValues0[i++], node.value, Double.MIN_VALUE);
-        for (FacetResultNode node2 : node.subResults) {
-          assertEquals(expectedValues0[i++], node2.value, Double.MIN_VALUE);
-        }
-      }
-
-      fr = facetResults.get(2); // a, depth=1, K=2
-      hasDoctor |= "Doctor".equals(fr.getFacetRequest().categoryPath.components[0]);
-      assertEquals(4, fr.getNumValidDescendants(), 4);
-      parentRes = fr.getFacetResultNode();
-      assertEquals(2, parentRes.subResults.size());
-      // two nodes sorted by descending values: 
-      // a/b with value 8 and a/c with value 6
-      double [] expectedValues2 = { 8.0, 6.0, 0.0};
-      i = 0;
-      for (FacetResultNode node : parentRes.subResults) {
-        assertEquals(expectedValues2[i++], node.value, Double.MIN_VALUE);
-        assertEquals(node.subResults.size(), 0);
-      }
-      
-      fr = facetResults.get(3); // a/b, depth=3, K=2
-      hasDoctor |= "Doctor".equals(fr.getFacetRequest().categoryPath.components[0]);
-      assertEquals(4, fr.getNumValidDescendants());
-      parentRes = fr.getFacetResultNode();
-      assertEquals(8.0, parentRes.value, Double.MIN_VALUE);
-      assertEquals(2, parentRes.subResults.size());
-      double [] expectedValues3 = { 3.0, 2.0 };
-      i = 0;
-      for (FacetResultNode node : parentRes.subResults) {
-        assertEquals(expectedValues3[i++], node.value, Double.MIN_VALUE);
-        assertEquals(0, node.subResults.size());
-      }
-
-      fr = facetResults.get(4); // a/b, depth=2, K=2
-      hasDoctor |= "Doctor".equals(fr.getFacetRequest().categoryPath.components[0]);
-      assertEquals(4, fr.getNumValidDescendants());
-      parentRes = fr.getFacetResultNode();
-      assertEquals(8.0, parentRes.value, Double.MIN_VALUE);
-      assertEquals(2, parentRes.subResults.size());
-      i = 0;
-      for (FacetResultNode node : parentRes.subResults) {
-        assertEquals(expectedValues3[i++], node.value, Double.MIN_VALUE);
-        assertEquals(0, node.subResults.size());
-      }
-
-      fr = facetResults.get(5); // a/b, depth=1, K=2
-      hasDoctor |= "Doctor".equals(fr.getFacetRequest().categoryPath.components[0]);
-      assertEquals(4, fr.getNumValidDescendants());
-      parentRes = fr.getFacetResultNode();
-      assertEquals(8.0, parentRes.value, Double.MIN_VALUE);
-      assertEquals(2, parentRes.subResults.size());
-      i = 0;
-      for (FacetResultNode node : parentRes.subResults) {
-        assertEquals(expectedValues3[i++], node.value, Double.MIN_VALUE);
-        assertEquals(0, node.subResults.size());
-      }
-      
-      fr = facetResults.get(6); // Doctor, depth=0, K=2
-      hasDoctor |= "Doctor".equals(fr.getFacetRequest().categoryPath.components[0]);
-      assertEquals(0, fr.getNumValidDescendants()); // 0 descendants but rootnode
-      parentRes = fr.getFacetResultNode();
-      assertEquals(0.0, parentRes.value, Double.MIN_VALUE);
-      assertEquals(0, parentRes.subResults.size());
-      hasDoctor |= "Doctor".equals(fr.getFacetRequest().categoryPath.components[0]);
-
-      // doctor, depth=1, K=2
-      assertTrue("Should have found an empty FacetResult " +
-          "for a facet that doesn't exist in the index.", hasDoctor);
-      assertEquals("Shouldn't have found more than 8 request.", 8, facetResults.size());
-
-      fr = facetResults.get(7); // a/b, depth=0, K=2
-      assertEquals(0, fr.getNumValidDescendants());
-      parentRes = fr.getFacetResultNode();
-      assertEquals(8.0, parentRes.value, Double.MIN_VALUE);
-      assertEquals(0, parentRes.subResults.size());
-      i = 0;
-      for (FacetResultNode node : parentRes.subResults) {
-        assertEquals(expectedValues3[i++], node.value, Double.MIN_VALUE);
-        assertEquals(0, node.subResults.size());
-      }
-
-      ir.close();
-      tr.close();
-      iDir.close();
-      tDir.close();
-    }
-
-  }
-
-  private void prvt_add(FacetIndexingParams iParams, RandomIndexWriter iw,
-      TaxonomyWriter tw, String... strings) throws IOException {
-    Document d = new Document();
-    FacetFields facetFields = new FacetFields(tw, iParams);
-    facetFields.addFields(d, Collections.singletonList(new CategoryPath(strings)));
-    d.add(new TextField("content", "alpha", Field.Store.YES));
-    iw.addDocument(d);
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandler.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandler.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandler.java	2013-02-28 09:00:26.690312171 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandler.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,211 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
-import org.apache.lucene.facet.search.FacetRequest.ResultMode;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestTopKResultsHandler extends BaseTestTopK {
-
-  private static final CategoryPath[] CATEGORIES = {
-    new CategoryPath( "a", "b"),
-    new CategoryPath( "a", "b", "1"),
-    new CategoryPath( "a", "b", "1"),
-    new CategoryPath( "a", "b", "2"),
-    new CategoryPath( "a", "b", "2"),
-    new CategoryPath( "a", "b", "3"),
-    new CategoryPath( "a", "b", "4"),
-    new CategoryPath( "a", "c"),
-    new CategoryPath( "a", "c"),
-    new CategoryPath( "a", "c"),
-    new CategoryPath( "a", "c"),
-    new CategoryPath( "a", "c"),
-    new CategoryPath( "a", "c", "1"),
-  };
-
-  @Override
-  protected String getContent(int doc) {
-    return ALPHA;
-  }
-  
-  @Override
-  protected int numDocsToIndex() {
-    return CATEGORIES.length;
-  }
-  
-  @Override
-  protected List<CategoryPath> getCategories(int doc) {
-    return Arrays.asList(CATEGORIES[doc]);
-  }
-  
-  /**
-   * Straightforward test: Adding specific documents with specific facets and
-   * counting them in the most basic form.
-   */
-  @Test
-  public void testSimple() throws Exception {
-    for (int partitionSize : partitionSizes) {
-      FacetIndexingParams fip = getFacetIndexingParams(partitionSize);
-      OrdinalPolicy op = fip.getCategoryListParams(null).getOrdinalPolicy(null);
-      initIndex(fip);
-
-      List<FacetRequest> facetRequests = new ArrayList<FacetRequest>();
-      facetRequests.add(new CountFacetRequest(new CategoryPath("a"), 100));
-      CountFacetRequest cfra = new CountFacetRequest(new CategoryPath("a"), 100);
-      cfra.setDepth(3);
-      // makes it easier to check the results in the test.
-      cfra.setResultMode(ResultMode.GLOBAL_FLAT);
-      facetRequests.add(cfra);
-      facetRequests.add(new CountFacetRequest(new CategoryPath("a", "b"), 100));
-      facetRequests.add(new CountFacetRequest(new CategoryPath("a", "b", "1"), 100));
-      facetRequests.add(new CountFacetRequest(new CategoryPath("a", "c"), 100));
-      
-      // do different facet counts and compare to control
-      FacetSearchParams sParams = getFacetSearchParams(facetRequests, fip);
-      FacetsCollector fc = FacetsCollector.create(sParams, indexReader, taxoReader);
-      
-      searcher.search(new MatchAllDocsQuery(), fc);
-      List<FacetResult> facetResults = fc.getFacetResults();
-      
-      FacetResult fr = facetResults.get(0);
-      FacetResultNode parentRes = fr.getFacetResultNode();
-      if (op == OrdinalPolicy.ALL_PARENTS) {
-        assertEquals(13.0, parentRes.value, Double.MIN_VALUE);
-      }
-      FacetResultNode[] frn = resultNodesAsArray(parentRes);
-      assertEquals(7.0, frn[0].value, Double.MIN_VALUE);
-      assertEquals(6.0, frn[1].value, Double.MIN_VALUE);
-
-      fr = facetResults.get(1);
-      parentRes = fr.getFacetResultNode();
-      if (op == OrdinalPolicy.ALL_PARENTS) {
-        assertEquals(13.0, parentRes.value, Double.MIN_VALUE);
-      }
-      frn = resultNodesAsArray(parentRes);
-      assertEquals(7.0, frn[0].value, Double.MIN_VALUE);
-      assertEquals(6.0, frn[1].value, Double.MIN_VALUE);
-      assertEquals(2.0, frn[2].value, Double.MIN_VALUE);
-      assertEquals(2.0, frn[3].value, Double.MIN_VALUE);
-      assertEquals(1.0, frn[4].value, Double.MIN_VALUE);
-      assertEquals(1.0, frn[5].value, Double.MIN_VALUE);
-
-      fr = facetResults.get(2);
-      parentRes = fr.getFacetResultNode();
-      if (op == OrdinalPolicy.ALL_PARENTS) {
-        assertEquals(7.0, parentRes.value, Double.MIN_VALUE);
-      }
-      frn = resultNodesAsArray(parentRes);
-      assertEquals(2.0, frn[0].value, Double.MIN_VALUE);
-      assertEquals(2.0, frn[1].value, Double.MIN_VALUE);
-      assertEquals(1.0, frn[2].value, Double.MIN_VALUE);
-      assertEquals(1.0, frn[3].value, Double.MIN_VALUE);
-
-      fr = facetResults.get(3);
-      parentRes = fr.getFacetResultNode();
-      if (op == OrdinalPolicy.ALL_PARENTS) {
-        assertEquals(2.0, parentRes.value, Double.MIN_VALUE);
-      }
-      frn = resultNodesAsArray(parentRes);
-      assertEquals(0, frn.length);
-
-      fr = facetResults.get(4);
-      parentRes = fr.getFacetResultNode();
-      if (op == OrdinalPolicy.ALL_PARENTS) {
-        assertEquals(6.0, parentRes.value, Double.MIN_VALUE);
-      }
-      frn = resultNodesAsArray(parentRes);
-      assertEquals(1.0, frn[0].value, Double.MIN_VALUE);
-      closeAll();
-    }
-  }
-  
-  /**
-   * Creating an index, matching the results of an top K = Integer.MAX_VALUE and top-1000 requests
-   */
-  @Test
-  public void testGetMaxIntFacets() throws Exception {
-    for (int partitionSize : partitionSizes) {
-      FacetIndexingParams fip = getFacetIndexingParams(partitionSize);
-      initIndex(fip);
-
-      // do different facet counts and compare to control
-      CategoryPath path = new CategoryPath("a", "b");
-      FacetSearchParams sParams = getFacetSearchParams(fip, new CountFacetRequest(path, Integer.MAX_VALUE));
-      FacetsCollector fc = FacetsCollector.create(sParams, indexReader, taxoReader);
-      
-      searcher.search(new MatchAllDocsQuery(), fc);
-      List<FacetResult> results = fc.getFacetResults();
-
-      assertEquals("Should only be one result as there's only one request", 1, results.size());
-      FacetResult res = results.get(0);
-      assertEquals(path + " should only have 4 desendants", 4, res.getNumValidDescendants());
-
-      // As a control base results, ask for top-1000 results
-      FacetSearchParams sParams2 = getFacetSearchParams(fip, new CountFacetRequest(path, Integer.MAX_VALUE));
-      FacetsCollector fc2 = FacetsCollector.create(sParams2, indexReader, taxoReader);
-      
-      searcher.search(new MatchAllDocsQuery(), fc2);
-      List<FacetResult> baseResults = fc2.getFacetResults();
-      FacetResult baseRes = baseResults.get(0);
-
-      // Removing the first line which holds the REQUEST and this is surly different between the two
-      String baseResultString = baseRes.toString();
-      baseResultString = baseResultString.substring(baseResultString.indexOf('\n'));
-      
-      // Removing the first line
-      String resultString = res.toString();
-      resultString = resultString.substring(resultString.indexOf('\n'));
-      
-      assertTrue("Results for k=MAX_VALUE do not match the regular results for k=1000!!",
-          baseResultString.equals(resultString));
-      
-      closeAll();
-    }
-  }
-
-  @Test
-  public void testSimpleSearchForNonexistentFacet() throws Exception {
-    for (int partitionSize : partitionSizes) {
-      FacetIndexingParams fip = getFacetIndexingParams(partitionSize);
-      initIndex(fip);
-
-      CategoryPath path = new CategoryPath("Miau Hattulla");
-      FacetSearchParams sParams = getFacetSearchParams(fip, new CountFacetRequest(path, 10));
-
-      FacetsCollector fc = FacetsCollector.create(sParams, indexReader, taxoReader);
-      
-      searcher.search(new MatchAllDocsQuery(), fc);
-      
-      List<FacetResult> facetResults = fc.getFacetResults();
-      
-      assertEquals("Shouldn't have found anything for a FacetRequest "
-          + "of a facet that doesn't exist in the index.", 1, facetResults.size());
-      assertEquals("Miau Hattulla", facetResults.get(0).getFacetResultNode().label.components[0]);
-      closeAll();
-    }
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandlerRandom.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandlerRandom.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandlerRandom.java	2013-07-29 13:55:02.597707543 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandlerRandom.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,130 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.List;
-
-import org.apache.lucene.facet.old.OldFacetsAccumulator;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestTopKResultsHandlerRandom extends BaseTestTopK {
-  
-  private List<FacetResult> countFacets(FacetIndexingParams fip, int numResults, final boolean doComplement)
-      throws IOException {
-    Query q = new MatchAllDocsQuery();
-    FacetSearchParams facetSearchParams = searchParamsWithRequests(numResults, fip);
-    OldFacetsAccumulator sfa = new OldFacetsAccumulator(facetSearchParams, indexReader, taxoReader);
-    sfa.setComplementThreshold(doComplement ? OldFacetsAccumulator.FORCE_COMPLEMENT : OldFacetsAccumulator.DISABLE_COMPLEMENT);
-    FacetsCollector fc = FacetsCollector.create(sfa);
-    searcher.search(q, fc);
-    List<FacetResult> facetResults = fc.getFacetResults();
-    return facetResults;
-  }
-
-  /**
-   * Test that indeed top results are returned, ordered same as all results 
-   * also when some facets have the same counts.
-   */
-  @Test
-  public void testTopCountsOrder() throws Exception {
-    for (int partitionSize : partitionSizes) {
-      FacetIndexingParams fip = getFacetIndexingParams(partitionSize);
-      initIndex(fip);
-      
-      /*
-       * Try out faceted search in it's most basic form (no sampling nor complement
-       * that is). In this test lots (and lots..) of randomly generated data is
-       * being indexed, and later on an "over-all" faceted search is performed. The
-       * results are checked against the DF of each facet by itself
-       */
-      List<FacetResult> facetResults = countFacets(fip, 100000, false);
-      assertCountsAndCardinality(facetCountsTruth(), facetResults);
-      
-      /*
-       * Try out faceted search with complements. In this test lots (and lots..) of
-       * randomly generated data is being indexed, and later on, a "beta" faceted
-       * search is performed - retrieving ~90% of the documents so complements takes
-       * place in here. The results are checked against the a regular (a.k.a
-       * no-complement, no-sampling) faceted search with the same parameters.
-       */
-      facetResults = countFacets(fip, 100000, true);
-      assertCountsAndCardinality(facetCountsTruth(), facetResults);
-      
-      List<FacetResult> allFacetResults = countFacets(fip, 100000, false);
-      
-      HashMap<String,Integer> all = new HashMap<String,Integer>();
-      int maxNumNodes = 0;
-      int k = 0;
-      for (FacetResult fr : allFacetResults) {
-        FacetResultNode topResNode = fr.getFacetResultNode();
-        maxNumNodes = Math.max(maxNumNodes, topResNode.subResults.size());
-        int prevCount = Integer.MAX_VALUE;
-        int pos = 0;
-        for (FacetResultNode frn: topResNode.subResults) {
-          assertTrue("wrong counts order: prev="+prevCount+" curr="+frn.value, prevCount>=frn.value);
-          prevCount = (int) frn.value;
-          String key = k+"--"+frn.label+"=="+frn.value;
-          if (VERBOSE) {
-            System.out.println(frn.label + " - " + frn.value + "  "+key+"  "+pos);
-          }
-          all.put(key, pos++); // will use this later to verify order of sub-results
-        }
-        k++;
-      }
-      
-      // verify that when asking for less results, they are always of highest counts
-      // also verify that the order is stable
-      for (int n=1; n<maxNumNodes; n++) {
-        if (VERBOSE) {
-          System.out.println("-------  verify for "+n+" top results");
-        }
-        List<FacetResult> someResults = countFacets(fip, n, false);
-        k = 0;
-        for (FacetResult fr : someResults) {
-          FacetResultNode topResNode = fr.getFacetResultNode();
-          assertTrue("too many results: n="+n+" but got "+topResNode.subResults.size(), n>=topResNode.subResults.size());
-          int pos = 0;
-          for (FacetResultNode frn: topResNode.subResults) {
-            String key = k+"--"+frn.label+"=="+frn.value;
-            if (VERBOSE) {
-              System.out.println(frn.label + " - " + frn.value + "  "+key+"  "+pos);
-            }
-            Integer origPos = all.get(key);
-            assertNotNull("missing in all results: "+frn,origPos);
-            assertEquals("wrong order of sub-results!",pos++, origPos.intValue()); // verify order of sub-results
-          }
-          k++;
-        }
-      }
-      
-      closeAll(); // done with this partition
-    }
-  }
-
-  @Override
-  protected int numDocsToIndex() {
-    return TEST_NIGHTLY ? 20000 : 1000;
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/sortedset/TestSortedSetDocValuesFacets.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/sortedset/TestSortedSetDocValuesFacets.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/sortedset/TestSortedSetDocValuesFacets.java	2013-07-29 13:55:02.601707542 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/sortedset/TestSortedSetDocValuesFacets.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,196 +0,0 @@
-package org.apache.lucene.facet.sortedset;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.FacetTestUtils;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.CountFacetRequest;
-import org.apache.lucene.facet.search.DrillDownQuery;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.store.Directory;
-
-public class TestSortedSetDocValuesFacets extends FacetTestCase {
-
-  // NOTE: TestDrillSideways.testRandom also sometimes
-  // randomly uses SortedSetDV
-
-  public void testSortedSetDocValuesAccumulator() throws Exception {
-    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    // Use a custom delim char to make sure the impls
-    // respect it:
-    final char delim = ':';
-    FacetIndexingParams fip = new FacetIndexingParams() {
-        @Override
-        public char getFacetDelimChar() {
-          return delim;
-        }
-      };
-
-    SortedSetDocValuesFacetFields dvFields = new SortedSetDocValuesFacetFields(fip);
-
-    Document doc = new Document();
-    // Mixup order we add these paths, to verify tie-break
-    // order is by label (unicode sort) and has nothing to
-    // do w/ order we added them:
-    List<CategoryPath> paths = new ArrayList<CategoryPath>();
-    paths.add(new CategoryPath("a", "foo"));
-    paths.add(new CategoryPath("a", "bar"));
-    paths.add(new CategoryPath("a", "zoo"));
-    Collections.shuffle(paths, random());
-
-    paths.add(new CategoryPath("b", "baz"));
-    paths.add(new CategoryPath("b" + FacetIndexingParams.DEFAULT_FACET_DELIM_CHAR, "bazfoo"));
-
-    dvFields.addFields(doc, paths);
-
-    writer.addDocument(doc);
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    dvFields.addFields(doc, Collections.singletonList(new CategoryPath("a", "foo")));
-    writer.addDocument(doc);
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    List<FacetRequest> requests = new ArrayList<FacetRequest>();
-    requests.add(new CountFacetRequest(new CategoryPath("a"), 10));
-    requests.add(new CountFacetRequest(new CategoryPath("b"), 10));
-    requests.add(new CountFacetRequest(new CategoryPath("b" + FacetIndexingParams.DEFAULT_FACET_DELIM_CHAR), 10));
-
-    final boolean doDimCount = random().nextBoolean();
-
-    CategoryListParams clp = new CategoryListParams() {
-        @Override
-        public OrdinalPolicy getOrdinalPolicy(String dimension) {
-          return doDimCount ? OrdinalPolicy.NO_PARENTS : OrdinalPolicy.ALL_BUT_DIMENSION;
-        }
-      };
-
-    FacetSearchParams fsp = new FacetSearchParams(new FacetIndexingParams(clp), requests);
-
-    // Per-top-reader state:
-    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(fip, searcher.getIndexReader());
-    
-    //SortedSetDocValuesCollector c = new SortedSetDocValuesCollector(state);
-    //SortedSetDocValuesCollectorMergeBySeg c = new SortedSetDocValuesCollectorMergeBySeg(state);
-
-    FacetsCollector c = FacetsCollector.create(new SortedSetDocValuesAccumulator(state, fsp));
-
-    searcher.search(new MatchAllDocsQuery(), c);
-
-    //List<FacetResult> results = c.getFacetResults(requests);
-    List<FacetResult> results = c.getFacetResults();
-
-    assertEquals(3, results.size());
-
-    int dimCount = doDimCount ? 4 : 0;
-    assertEquals("a (" + dimCount + ")\n  foo (2)\n  bar (1)\n  zoo (1)\n", FacetTestUtils.toSimpleString(results.get(0)));
-
-    dimCount = doDimCount ? 1 : 0;
-    assertEquals("b (" + dimCount + ")\n  baz (1)\n", FacetTestUtils.toSimpleString(results.get(1)));
-
-    dimCount = doDimCount ? 1 : 0;
-    assertEquals("b" + FacetIndexingParams.DEFAULT_FACET_DELIM_CHAR + " (" + dimCount + ")\n  bazfoo (1)\n", FacetTestUtils.toSimpleString(results.get(2)));
-
-    // DrillDown:
-
-    DrillDownQuery q = new DrillDownQuery(fip);
-    q.add(new CategoryPath("a", "foo"));
-    q.add(new CategoryPath("b", "baz"));
-    TopDocs hits = searcher.search(q, 1);
-    assertEquals(1, hits.totalHits);
-
-    q = new DrillDownQuery(fip);
-    q.add(new CategoryPath("a"));
-    hits = searcher.search(q, 1);
-    assertEquals(2, hits.totalHits);
-
-    searcher.getIndexReader().close();
-    dir.close();
-  }
-
-  // LUCENE-5090
-  public void testStaleState() throws Exception {
-    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    SortedSetDocValuesFacetFields dvFields = new SortedSetDocValuesFacetFields();
-
-    Document doc = new Document();
-    dvFields.addFields(doc, Collections.singletonList(new CategoryPath("a", "foo")));
-    writer.addDocument(doc);
-
-    IndexReader r = writer.getReader();
-    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(r);
-
-    doc = new Document();
-    dvFields.addFields(doc, Collections.singletonList(new CategoryPath("a", "bar")));
-    writer.addDocument(doc);
-
-    doc = new Document();
-    dvFields.addFields(doc, Collections.singletonList(new CategoryPath("a", "baz")));
-    writer.addDocument(doc);
-
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    List<FacetRequest> requests = new ArrayList<FacetRequest>();
-    requests.add(new CountFacetRequest(new CategoryPath("a"), 10));
-
-    FacetSearchParams fsp = new FacetSearchParams(requests);
-    
-    FacetsCollector c = FacetsCollector.create(new SortedSetDocValuesAccumulator(state, fsp));
-
-    searcher.search(new MatchAllDocsQuery(), c);
-
-    try {
-      c.getFacetResults();
-      fail("did not hit expected exception");
-    } catch (IllegalStateException ise) {
-      // expected
-    }
-
-    r.close();
-    writer.close();
-    searcher.getIndexReader().close();
-    dir.close();
-  }
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestAddTaxonomy.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestAddTaxonomy.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestAddTaxonomy.java	2013-02-20 13:38:17.548711926 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestAddTaxonomy.java	2013-11-14 15:40:50.002297836 -0500
@@ -6,7 +6,7 @@
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.DiskOrdinalMap;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.MemoryOrdinalMap;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
@@ -48,7 +48,7 @@
             while (numCats.decrementAndGet() > 0) {
               String cat = Integer.toString(random.nextInt(range));
               try {
-                tw.addCategory(new CategoryPath("a", cat));
+                tw.addCategory(new FacetLabel("a", cat));
               } catch (IOException e) {
                 throw new RuntimeException(e);
               }
@@ -97,7 +97,7 @@
         // validate that all source categories exist in destination, and their
         // ordinals are as expected.
         for (int j = 1; j < srcSize; j++) {
-          CategoryPath cp = srcTR.getPath(j);
+          FacetLabel cp = srcTR.getPath(j);
           int destOrdinal = destTR.getOrdinal(cp);
           assertTrue(cp + " not found in destination", destOrdinal > 0);
           assertEquals(destOrdinal, map[j]);
@@ -113,8 +113,8 @@
   public void testAddEmpty() throws Exception {
     Directory dest = newDirectory();
     DirectoryTaxonomyWriter destTW = new DirectoryTaxonomyWriter(dest);
-    destTW.addCategory(new CategoryPath("Author", "Rob Pike"));
-    destTW.addCategory(new CategoryPath("Aardvarks", "Bob"));
+    destTW.addCategory(new FacetLabel("Author", "Rob Pike"));
+    destTW.addCategory(new FacetLabel("Aardvarks", "Bob"));
     destTW.commit();
     
     Directory src = newDirectory();
@@ -134,8 +134,8 @@
     
     Directory src = newDirectory();
     DirectoryTaxonomyWriter srcTW = new DirectoryTaxonomyWriter(src);
-    srcTW.addCategory(new CategoryPath("Author", "Rob Pike"));
-    srcTW.addCategory(new CategoryPath("Aardvarks", "Bob"));
+    srcTW.addCategory(new FacetLabel("Author", "Rob Pike"));
+    srcTW.addCategory(new FacetLabel("Aardvarks", "Bob"));
     srcTW.close();
     
     DirectoryTaxonomyWriter destTW = new DirectoryTaxonomyWriter(dest);
@@ -168,14 +168,14 @@
   public void testSimple() throws Exception {
     Directory dest = newDirectory();
     DirectoryTaxonomyWriter tw1 = new DirectoryTaxonomyWriter(dest);
-    tw1.addCategory(new CategoryPath("Author", "Mark Twain"));
-    tw1.addCategory(new CategoryPath("Animals", "Dog"));
-    tw1.addCategory(new CategoryPath("Author", "Rob Pike"));
+    tw1.addCategory(new FacetLabel("Author", "Mark Twain"));
+    tw1.addCategory(new FacetLabel("Animals", "Dog"));
+    tw1.addCategory(new FacetLabel("Author", "Rob Pike"));
     
     Directory src = newDirectory();
     DirectoryTaxonomyWriter tw2 = new DirectoryTaxonomyWriter(src);
-    tw2.addCategory(new CategoryPath("Author", "Rob Pike"));
-    tw2.addCategory(new CategoryPath("Aardvarks", "Bob"));
+    tw2.addCategory(new FacetLabel("Author", "Rob Pike"));
+    tw2.addCategory(new FacetLabel("Aardvarks", "Bob"));
     tw2.close();
 
     OrdinalMap map = randomOrdinalMap();
@@ -196,7 +196,7 @@
     Directory src = newDirectory();
     DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(src);
     for (int i = 0; i < numCategories; i++) {
-      tw.addCategory(new CategoryPath("a", Integer.toString(i)));
+      tw.addCategory(new FacetLabel("a", Integer.toString(i)));
     }
     tw.close();
     
@@ -209,7 +209,7 @@
       public void run() {
         for (int i = 0; i < numCategories; i++) {
           try {
-            destTW.addCategory(new CategoryPath("a", Integer.toString(i)));
+            destTW.addCategory(new FacetLabel("a", Integer.toString(i)));
           } catch (IOException e) {
             // shouldn't happen - if it does, let the test fail on uncaught exception.
             throw new RuntimeException(e);
@@ -229,9 +229,9 @@
     DirectoryTaxonomyReader dtr = new DirectoryTaxonomyReader(dest);
     // +2 to account for the root category + "a"
     assertEquals(numCategories + 2, dtr.getSize());
-    HashSet<CategoryPath> categories = new HashSet<CategoryPath>();
+    HashSet<FacetLabel> categories = new HashSet<FacetLabel>();
     for (int i = 1; i < dtr.getSize(); i++) {
-      CategoryPath cat = dtr.getPath(i);
+      FacetLabel cat = dtr.getPath(i);
       assertTrue("category " + cat + " already existed", categories.add(cat));
     }
     dtr.close();


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestConcurrentFacetedIndexing.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestConcurrentFacetedIndexing.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestConcurrentFacetedIndexing.java	2013-02-20 13:38:17.548711926 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestConcurrentFacetedIndexing.java	2013-12-03 12:50:59.004916506 -0500
@@ -1,19 +1,18 @@
 package org.apache.lucene.facet.taxonomy.directory;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
 import java.util.Random;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetField;
 import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
-import org.apache.lucene.facet.taxonomy.writercache.cl2o.Cl2oTaxonomyWriterCache;
-import org.apache.lucene.facet.taxonomy.writercache.lru.LruTaxonomyWriterCache;
+import org.apache.lucene.facet.taxonomy.writercache.Cl2oTaxonomyWriterCache;
+import org.apache.lucene.facet.taxonomy.writercache.LruTaxonomyWriterCache;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.store.Directory;
@@ -46,9 +45,9 @@
     @Override
     public void close() {}
     @Override
-    public int get(CategoryPath categoryPath) { return -1; }
+    public int get(FacetLabel categoryPath) { return -1; }
     @Override
-    public boolean put(CategoryPath categoryPath, int ordinal) { return true; }
+    public boolean put(FacetLabel categoryPath, int ordinal) { return true; }
     @Override
     public boolean isFull() { return true; }
     @Override
@@ -56,12 +55,12 @@
     
   };
   
-  static CategoryPath newCategory() {
+  static FacetField newCategory() {
     Random r = random();
     String l1 = "l1." + r.nextInt(10); // l1.0-l1.9 (10 categories)
     String l2 = "l2." + r.nextInt(30); // l2.0-l2.29 (30 categories)
     String l3 = "l3." + r.nextInt(100); // l3.0-l3.99 (100 categories)
-    return new CategoryPath(l1, l2, l3);
+    return new FacetField(l1, l2, l3);
   }
   
   static TaxonomyWriterCache newTaxoWriterCache(int ndocs) {
@@ -87,10 +86,14 @@
     final IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
     final DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE, newTaxoWriterCache(numDocs.get()));
     final Thread[] indexThreads = new Thread[atLeast(4)];
+    final FacetsConfig config = new FacetsConfig();
+    for(int i=0;i<10;i++) {
+      config.setHierarchical("l1." + i, true);
+      config.setMultiValued("l1." + i, true);
+    }
 
     for (int i = 0; i < indexThreads.length; i++) {
       indexThreads[i] = new Thread() {
-        private final FacetFields facetFields = new FacetFields(tw);
         
         @Override
         public void run() {
@@ -99,20 +102,20 @@
             try {
               Document doc = new Document();
               int numCats = random.nextInt(3) + 1; // 1-3
-              List<CategoryPath> cats = new ArrayList<CategoryPath>(numCats);
               while (numCats-- > 0) {
-                CategoryPath cp = newCategory();
-                cats.add(cp);
+                FacetField ff = newCategory();
+                doc.add(ff);
+
+                FacetLabel label = new FacetLabel(ff.dim, ff.path);
                 // add all prefixes to values
-                int level = cp.length;
+                int level = label.length;
                 while (level > 0) {
-                  String s = cp.subpath(level).toString('/');
+                  String s = FacetsConfig.pathToString(label.components, level);
                   values.put(s, s);
                   --level;
                 }
               }
-              facetFields.addFields(doc, cats);
-              iw.addDocument(doc);
+              iw.addDocument(config.build(tw, doc));
             } catch (IOException e) {
               throw new RuntimeException(e);
             }
@@ -125,14 +128,23 @@
     for (Thread t : indexThreads) t.join();
     
     DirectoryTaxonomyReader tr = new DirectoryTaxonomyReader(tw);
-    assertEquals("mismatch number of categories", values.size() + 1, tr.getSize()); // +1 for root category
+    // +1 for root category
+    if (values.size() + 1 != tr.getSize()) {
+      for(String value : values.keySet()) {
+        FacetLabel label = new FacetLabel(FacetsConfig.stringToPath(value));
+        if (tr.getOrdinal(label) == -1) {
+          System.out.println("FAIL: path=" + label + " not recognized");
+        }
+      }
+      fail("mismatch number of categories");
+    }
     int[] parents = tr.getParallelTaxonomyArrays().parents();
     for (String cat : values.keySet()) {
-      CategoryPath cp = new CategoryPath(cat, '/');
+      FacetLabel cp = new FacetLabel(FacetsConfig.stringToPath(cat));
       assertTrue("category not found " + cp, tr.getOrdinal(cp) > 0);
       int level = cp.length;
       int parentOrd = 0; // for root, parent is always virtual ROOT (ord=0)
-      CategoryPath path = CategoryPath.EMPTY;
+      FacetLabel path = null;
       for (int i = 0; i < level; i++) {
         path = cp.subpath(i + 1);
         int ord = tr.getOrdinal(path);
@@ -140,9 +152,8 @@
         parentOrd = ord; // next level should have this parent
       }
     }
-    tr.close();
 
-    IOUtils.close(tw, iw, taxoDir, indexDir);
+    IOUtils.close(tw, iw, tr, taxoDir, indexDir);
   }
 
 }


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyReader.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyReader.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyReader.java	2013-10-13 15:35:17.913888714 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyReader.java	2013-11-14 15:40:50.002297836 -0500
@@ -8,7 +8,7 @@
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader.ChildrenIterator;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
@@ -46,7 +46,7 @@
   public void testCloseAfterIncRef() throws Exception {
     Directory dir = newDirectory();
     DirectoryTaxonomyWriter ltw = new DirectoryTaxonomyWriter(dir);
-    ltw.addCategory(new CategoryPath("a"));
+    ltw.addCategory(new FacetLabel("a"));
     ltw.close();
     
     DirectoryTaxonomyReader ltr = new DirectoryTaxonomyReader(dir);
@@ -64,7 +64,7 @@
   public void testCloseTwice() throws Exception {
     Directory dir = newDirectory();
     DirectoryTaxonomyWriter ltw = new DirectoryTaxonomyWriter(dir);
-    ltw.addCategory(new CategoryPath("a"));
+    ltw.addCategory(new FacetLabel("a"));
     ltw.close();
     
     DirectoryTaxonomyReader ltr = new DirectoryTaxonomyReader(dir);
@@ -84,13 +84,13 @@
       dir = newDirectory();
       ltw = new DirectoryTaxonomyWriter(dir);
       
-      ltw.addCategory(new CategoryPath("a"));
+      ltw.addCategory(new FacetLabel("a"));
       ltw.commit();
       
       ltr = new DirectoryTaxonomyReader(dir);
       assertNull("Nothing has changed", TaxonomyReader.openIfChanged(ltr));
       
-      ltw.addCategory(new CategoryPath("b"));
+      ltw.addCategory(new FacetLabel("b"));
       ltw.commit();
       
       DirectoryTaxonomyReader newtr = TaxonomyReader.openIfChanged(ltr);
@@ -106,7 +106,7 @@
   public void testAlreadyClosed() throws Exception {
     Directory dir = newDirectory();
     DirectoryTaxonomyWriter ltw = new DirectoryTaxonomyWriter(dir);
-    ltw.addCategory(new CategoryPath("a"));
+    ltw.addCategory(new FacetLabel("a"));
     ltw.close();
     
     DirectoryTaxonomyReader ltr = new DirectoryTaxonomyReader(dir);
@@ -140,16 +140,16 @@
     
     // prepare a few categories
     int  n = 10;
-    CategoryPath[] cp = new CategoryPath[n];
+    FacetLabel[] cp = new FacetLabel[n];
     for (int i=0; i<n; i++) {
-      cp[i] = new CategoryPath("a", Integer.toString(i));
+      cp[i] = new FacetLabel("a", Integer.toString(i));
     }
     
     try {
       dir = newDirectory();
       
       tw = new DirectoryTaxonomyWriter(dir);
-      tw.addCategory(new CategoryPath("a"));
+      tw.addCategory(new FacetLabel("a"));
       tw.close();
       
       tr = new DirectoryTaxonomyReader(dir);
@@ -183,7 +183,7 @@
     Directory dir = new RAMDirectory(); // no need for random directories here
 
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(dir);
-    taxoWriter.addCategory(new CategoryPath("a"));
+    taxoWriter.addCategory(new FacetLabel("a"));
     taxoWriter.commit();
 
     TaxonomyReader taxoReader = new DirectoryTaxonomyReader(dir);
@@ -192,7 +192,7 @@
     taxoReader.incRef();
     assertEquals("wrong refCount", 2, taxoReader.getRefCount());
 
-    taxoWriter.addCategory(new CategoryPath("a", "b"));
+    taxoWriter.addCategory(new FacetLabel("a", "b"));
     taxoWriter.commit();
     TaxonomyReader newtr = TaxonomyReader.openIfChanged(taxoReader);
     assertNotNull(newtr);
@@ -226,7 +226,7 @@
     for (int i = 0; i < numRounds; i++) {
       int numCats = random().nextInt(4) + 1;
       for (int j = 0; j < numCats; j++) {
-        writer.addCategory(new CategoryPath(Integer.toString(i), Integer.toString(j)));
+        writer.addCategory(new FacetLabel(Integer.toString(i), Integer.toString(j)));
       }
       numCategories += numCats + 1 /* one for round-parent */;
       TaxonomyReader newtr = TaxonomyReader.openIfChanged(reader);
@@ -236,11 +236,11 @@
       
       // assert categories
       assertEquals(numCategories, reader.getSize());
-      int roundOrdinal = reader.getOrdinal(new CategoryPath(Integer.toString(i)));
+      int roundOrdinal = reader.getOrdinal(new FacetLabel(Integer.toString(i)));
       int[] parents = reader.getParallelTaxonomyArrays().parents();
       assertEquals(0, parents[roundOrdinal]); // round's parent is root
       for (int j = 0; j < numCats; j++) {
-        int ord = reader.getOrdinal(new CategoryPath(Integer.toString(i), Integer.toString(j)));
+        int ord = reader.getOrdinal(new FacetLabel(Integer.toString(i), Integer.toString(j)));
         assertEquals(roundOrdinal, parents[ord]); // round's parent is root
       }
     }
@@ -276,7 +276,7 @@
 
     // add category and call forceMerge -- this should flush IW and merge segments down to 1
     // in ParentArray.initFromReader, this used to fail assuming there are no parents.
-    writer.addCategory(new CategoryPath("1"));
+    writer.addCategory(new FacetLabel("1"));
     iw.forceMerge(1);
     
     // now calling openIfChanged should trip on the bug
@@ -315,7 +315,7 @@
     
     // add a category so that the following DTR open will cause a flush and 
     // a new segment will be created
-    writer.addCategory(new CategoryPath("a"));
+    writer.addCategory(new FacetLabel("a"));
     
     TaxonomyReader reader = new DirectoryTaxonomyReader(writer);
     assertEquals(2, reader.getSize());
@@ -342,7 +342,7 @@
     // tests that if the taxonomy is recreated, no data is reused from the previous taxonomy
     Directory dir = newDirectory();
     DirectoryTaxonomyWriter writer = new DirectoryTaxonomyWriter(dir);
-    CategoryPath cp_a = new CategoryPath("a");
+    FacetLabel cp_a = new FacetLabel("a");
     writer.addCategory(cp_a);
     writer.close();
     
@@ -353,7 +353,7 @@
     
     // now recreate, add a different category
     writer = new DirectoryTaxonomyWriter(dir, OpenMode.CREATE);
-    CategoryPath cp_b = new CategoryPath("b");
+    FacetLabel cp_b = new FacetLabel("b");
     writer.addCategory(cp_b);
     writer.close();
     
@@ -384,7 +384,7 @@
       Directory dir = newDirectory();
       DirectoryTaxonomyWriter writer = new DirectoryTaxonomyWriter(dir);
       
-      CategoryPath cp_a = new CategoryPath("a");
+      FacetLabel cp_a = new FacetLabel("a");
       writer.addCategory(cp_a);
       if (!nrt) writer.commit();
       
@@ -393,7 +393,7 @@
       assertEquals(1, r1.getOrdinal(cp_a));
       assertEquals(cp_a, r1.getPath(1));
       
-      CategoryPath cp_b = new CategoryPath("b");
+      FacetLabel cp_b = new FacetLabel("b");
       writer.addCategory(cp_b);
       if (!nrt) writer.commit();
       
@@ -421,7 +421,7 @@
     // only can work with NRT as well
     Directory src = newDirectory();
     DirectoryTaxonomyWriter w = new DirectoryTaxonomyWriter(src);
-    CategoryPath cp_b = new CategoryPath("b");
+    FacetLabel cp_b = new FacetLabel("b");
     w.addCategory(cp_b);
     w.close();
     
@@ -429,7 +429,7 @@
       Directory dir = newDirectory();
       DirectoryTaxonomyWriter writer = new DirectoryTaxonomyWriter(dir);
       
-      CategoryPath cp_a = new CategoryPath("a");
+      FacetLabel cp_a = new FacetLabel("a");
       writer.addCategory(cp_a);
       if (!nrt) writer.commit();
       
@@ -474,29 +474,29 @@
     int numA = 0, numB = 0;
     Random random = random();
     // add the two categories for which we'll also add children (so asserts are simpler)
-    taxoWriter.addCategory(new CategoryPath("a"));
-    taxoWriter.addCategory(new CategoryPath("b"));
+    taxoWriter.addCategory(new FacetLabel("a"));
+    taxoWriter.addCategory(new FacetLabel("b"));
     for (int i = 0; i < numCategories; i++) {
       if (random.nextBoolean()) {
-        taxoWriter.addCategory(new CategoryPath("a", Integer.toString(i)));
+        taxoWriter.addCategory(new FacetLabel("a", Integer.toString(i)));
         ++numA;
       } else {
-        taxoWriter.addCategory(new CategoryPath("b", Integer.toString(i)));
+        taxoWriter.addCategory(new FacetLabel("b", Integer.toString(i)));
         ++numB;
       }
     }
     // add category with no children
-    taxoWriter.addCategory(new CategoryPath("c"));
+    taxoWriter.addCategory(new FacetLabel("c"));
     taxoWriter.close();
     
     DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(dir);
 
     // non existing category
-    ChildrenIterator it = taxoReader.getChildren(taxoReader.getOrdinal(new CategoryPath("invalid")));
+    ChildrenIterator it = taxoReader.getChildren(taxoReader.getOrdinal(new FacetLabel("invalid")));
     assertEquals(TaxonomyReader.INVALID_ORDINAL, it.next());
 
     // a category with no children
-    it = taxoReader.getChildren(taxoReader.getOrdinal(new CategoryPath("c")));
+    it = taxoReader.getChildren(taxoReader.getOrdinal(new FacetLabel("c")));
     assertEquals(TaxonomyReader.INVALID_ORDINAL, it.next());
 
     // arbitrary negative ordinal
@@ -507,20 +507,20 @@
     Set<String> roots = new HashSet<String>(Arrays.asList("a", "b", "c"));
     it = taxoReader.getChildren(TaxonomyReader.ROOT_ORDINAL);
     while (!roots.isEmpty()) {
-      CategoryPath root = taxoReader.getPath(it.next());
+      FacetLabel root = taxoReader.getPath(it.next());
       assertEquals(1, root.length);
       assertTrue(roots.remove(root.components[0]));
     }
     assertEquals(TaxonomyReader.INVALID_ORDINAL, it.next());
     
     for (int i = 0; i < 2; i++) {
-      CategoryPath cp = i == 0 ? new CategoryPath("a") : new CategoryPath("b");
+      FacetLabel cp = i == 0 ? new FacetLabel("a") : new FacetLabel("b");
       int ordinal = taxoReader.getOrdinal(cp);
       it = taxoReader.getChildren(ordinal);
       int numChildren = 0;
       int child;
       while ((child = it.next()) != TaxonomyReader.INVALID_ORDINAL) {
-        CategoryPath path = taxoReader.getPath(child);
+        FacetLabel path = taxoReader.getPath(child);
         assertEquals(2, path.length);
         assertEquals(path.components[0], i == 0 ? "a" : "b");
         ++numChildren;


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java	2013-09-25 14:34:00.235590297 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java	2013-12-03 13:11:35.048883332 -0500
@@ -1,7 +1,6 @@
 package org.apache.lucene.facet.taxonomy.directory;
 
 import java.io.IOException;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Random;
@@ -10,21 +9,21 @@
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetField;
 import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.DrillDownQuery;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.DrillDownQuery;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.MemoryOrdinalMap;
 import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
-import org.apache.lucene.facet.taxonomy.writercache.cl2o.Cl2oTaxonomyWriterCache;
-import org.apache.lucene.facet.taxonomy.writercache.lru.LruTaxonomyWriterCache;
+import org.apache.lucene.facet.taxonomy.writercache.Cl2oTaxonomyWriterCache;
+import org.apache.lucene.facet.taxonomy.writercache.LruTaxonomyWriterCache;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.SegmentInfos;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.store.AlreadyClosedException;
@@ -59,9 +58,9 @@
     @Override
     public void close() {}
     @Override
-    public int get(CategoryPath categoryPath) { return -1; }
+    public int get(FacetLabel categoryPath) { return -1; }
     @Override
-    public boolean put(CategoryPath categoryPath, int ordinal) { return true; }
+    public boolean put(FacetLabel categoryPath, int ordinal) { return true; }
     @Override
     public boolean isFull() { return true; }
     @Override
@@ -77,7 +76,7 @@
     DirectoryTaxonomyWriter ltw = new DirectoryTaxonomyWriter(dir, OpenMode.CREATE_OR_APPEND, NO_OP_CACHE);
     assertFalse(DirectoryReader.indexExists(dir));
     ltw.commit(); // first commit, so that an index will be created
-    ltw.addCategory(new CategoryPath("a"));
+    ltw.addCategory(new FacetLabel("a"));
     
     IndexReader r = DirectoryReader.open(dir);
     assertEquals("No categories should have been committed to the underlying directory", 1, r.numDocs());
@@ -91,8 +90,8 @@
     // Verifies taxonomy commit data
     Directory dir = newDirectory();
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(dir, OpenMode.CREATE_OR_APPEND, NO_OP_CACHE);
-    taxoWriter.addCategory(new CategoryPath("a"));
-    taxoWriter.addCategory(new CategoryPath("b"));
+    taxoWriter.addCategory(new FacetLabel("a"));
+    taxoWriter.addCategory(new FacetLabel("b"));
     Map<String, String> userCommitData = new HashMap<String, String>();
     userCommitData.put("testing", "1 2 3");
     taxoWriter.setCommitData(userCommitData);
@@ -109,7 +108,7 @@
     // in the commit data, otherwise DirTaxoReader.refresh() might not detect
     // that the taxonomy index has been recreated.
     taxoWriter = new DirectoryTaxonomyWriter(dir, OpenMode.CREATE_OR_APPEND, NO_OP_CACHE);
-    taxoWriter.addCategory(new CategoryPath("c")); // add a category so that commit will happen
+    taxoWriter.addCategory(new FacetLabel("c")); // add a category so that commit will happen
     taxoWriter.setCommitData(new HashMap<String, String>(){{
       put("just", "data");
     }});
@@ -133,10 +132,10 @@
     // Verifies that if rollback is called, DTW is closed.
     Directory dir = newDirectory();
     DirectoryTaxonomyWriter dtw = new DirectoryTaxonomyWriter(dir);
-    dtw.addCategory(new CategoryPath("a"));
+    dtw.addCategory(new FacetLabel("a"));
     dtw.rollback();
     try {
-      dtw.addCategory(new CategoryPath("a"));
+      dtw.addCategory(new FacetLabel("a"));
       fail("should not have succeeded to add a category following rollback.");
     } catch (AlreadyClosedException e) {
       // expected
@@ -164,7 +163,7 @@
     DirectoryTaxonomyWriter dtw = new DirectoryTaxonomyWriter(dir);
     dtw.close();
     try {
-      dtw.addCategory(new CategoryPath("a"));
+      dtw.addCategory(new FacetLabel("a"));
       fail("should not have succeeded to add a category following close.");
     } catch (AlreadyClosedException e) {
       // expected
@@ -172,7 +171,7 @@
     dir.close();
   }
 
-  private void touchTaxo(DirectoryTaxonomyWriter taxoWriter, CategoryPath cp) throws IOException {
+  private void touchTaxo(DirectoryTaxonomyWriter taxoWriter, FacetLabel cp) throws IOException {
     taxoWriter.addCategory(cp);
     taxoWriter.setCommitData(new HashMap<String, String>(){{
       put("just", "data");
@@ -188,11 +187,11 @@
     Directory dir = newDirectory();
     
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(dir, OpenMode.CREATE_OR_APPEND, NO_OP_CACHE);
-    touchTaxo(taxoWriter, new CategoryPath("a"));
+    touchTaxo(taxoWriter, new FacetLabel("a"));
     
     TaxonomyReader taxoReader = new DirectoryTaxonomyReader(dir);
 
-    touchTaxo(taxoWriter, new CategoryPath("b"));
+    touchTaxo(taxoWriter, new FacetLabel("b"));
     
     TaxonomyReader newtr = TaxonomyReader.openIfChanged(taxoReader);
     taxoReader.close();
@@ -202,11 +201,11 @@
     // now recreate the taxonomy, and check that the epoch is preserved after opening DirTW again.
     taxoWriter.close();
     taxoWriter = new DirectoryTaxonomyWriter(dir, OpenMode.CREATE, NO_OP_CACHE);
-    touchTaxo(taxoWriter, new CategoryPath("c"));
+    touchTaxo(taxoWriter, new FacetLabel("c"));
     taxoWriter.close();
     
     taxoWriter = new DirectoryTaxonomyWriter(dir, OpenMode.CREATE_OR_APPEND, NO_OP_CACHE);
-    touchTaxo(taxoWriter, new CategoryPath("d"));
+    touchTaxo(taxoWriter, new FacetLabel("d"));
     taxoWriter.close();
 
     newtr = TaxonomyReader.openIfChanged(taxoReader);
@@ -257,6 +256,9 @@
       // this is slower than CL2O, but less memory consuming, and exercises finding categories on disk too.
       cache = new LruTaxonomyWriterCache(ncats / 10);
     }
+    if (VERBOSE) {
+      System.out.println("TEST: use cache=" + cache);
+    }
     final DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(dir, OpenMode.CREATE, cache);
     Thread[] addThreads = new Thread[atLeast(4)];
     for (int z = 0; z < addThreads.length; z++) {
@@ -267,14 +269,14 @@
           while (numCats.decrementAndGet() > 0) {
             try {
               int value = random.nextInt(range);
-              CategoryPath cp = new CategoryPath(Integer.toString(value / 1000), Integer.toString(value / 10000),
+              FacetLabel cp = new FacetLabel(Integer.toString(value / 1000), Integer.toString(value / 10000),
                   Integer.toString(value / 100000), Integer.toString(value));
               int ord = tw.addCategory(cp);
               assertTrue("invalid parent for ordinal " + ord + ", category " + cp, tw.getParent(ord) != -1);
-              String l1 = cp.subpath(1).toString('/');
-              String l2 = cp.subpath(2).toString('/');
-              String l3 = cp.subpath(3).toString('/');
-              String l4 = cp.subpath(4).toString('/');
+              String l1 = FacetsConfig.pathToString(cp.components, 1);
+              String l2 = FacetsConfig.pathToString(cp.components, 2);
+              String l3 = FacetsConfig.pathToString(cp.components, 3);
+              String l4 = FacetsConfig.pathToString(cp.components, 4);
               values.put(l1, l1);
               values.put(l2, l2);
               values.put(l3, l3);
@@ -292,14 +294,24 @@
     tw.close();
     
     DirectoryTaxonomyReader dtr = new DirectoryTaxonomyReader(dir);
-    assertEquals("mismatch number of categories", values.size() + 1, dtr.getSize()); // +1 for root category
+    // +1 for root category
+    if (values.size() + 1 != dtr.getSize()) {
+      for(String value : values.keySet()) {
+        FacetLabel label = new FacetLabel(FacetsConfig.stringToPath(value));
+        if (dtr.getOrdinal(label) == -1) {
+          System.out.println("FAIL: path=" + label + " not recognized");
+        }
+      }
+      fail("mismatch number of categories");
+    }
+
     int[] parents = dtr.getParallelTaxonomyArrays().parents();
     for (String cat : values.keySet()) {
-      CategoryPath cp = new CategoryPath(cat, '/');
+      FacetLabel cp = new FacetLabel(FacetsConfig.stringToPath(cat));
       assertTrue("category not found " + cp, dtr.getOrdinal(cp) > 0);
       int level = cp.length;
       int parentOrd = 0; // for root, parent is always virtual ROOT (ord=0)
-      CategoryPath path = CategoryPath.EMPTY;
+      FacetLabel path = new FacetLabel();
       for (int i = 0; i < level; i++) {
         path = cp.subpath(i + 1);
         int ord = dtr.getOrdinal(path);
@@ -307,9 +319,8 @@
         parentOrd = ord; // next level should have this parent
       }
     }
-    dtr.close();
-    
-    dir.close();
+
+    IOUtils.close(dtr, dir);
   }
 
   private long getEpoch(Directory taxoDir) throws IOException {
@@ -322,13 +333,13 @@
   public void testReplaceTaxonomy() throws Exception {
     Directory input = newDirectory();
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(input);
-    int ordA = taxoWriter.addCategory(new CategoryPath("a"));
+    int ordA = taxoWriter.addCategory(new FacetLabel("a"));
     taxoWriter.close();
     
     Directory dir = newDirectory();
     taxoWriter = new DirectoryTaxonomyWriter(dir);
-    int ordB = taxoWriter.addCategory(new CategoryPath("b"));
-    taxoWriter.addCategory(new CategoryPath("c"));
+    int ordB = taxoWriter.addCategory(new FacetLabel("b"));
+    taxoWriter.addCategory(new FacetLabel("c"));
     taxoWriter.commit();
     
     long origEpoch = getEpoch(dir);
@@ -339,10 +350,10 @@
     // LUCENE-4633: make sure that category "a" is not added again in any case
     taxoWriter.addTaxonomy(input, new MemoryOrdinalMap());
     assertEquals("no categories should have been added", 2, taxoWriter.getSize()); // root + 'a'
-    assertEquals("category 'a' received new ordinal?", ordA, taxoWriter.addCategory(new CategoryPath("a")));
+    assertEquals("category 'a' received new ordinal?", ordA, taxoWriter.addCategory(new FacetLabel("a")));
 
     // add the same category again -- it should not receive the same ordinal !
-    int newOrdB = taxoWriter.addCategory(new CategoryPath("b"));
+    int newOrdB = taxoWriter.addCategory(new FacetLabel("b"));
     assertNotSame("new ordinal cannot be the original ordinal", ordB, newOrdB);
     assertEquals("ordinal should have been 2 since only one category was added by replaceTaxonomy", 2, newOrdB);
 
@@ -362,8 +373,8 @@
     // is being added.
     Directory dir = newDirectory();
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(dir, OpenMode.CREATE, NO_OP_CACHE);
-    int o1 = taxoWriter.addCategory(new CategoryPath("a"));
-    int o2 = taxoWriter.addCategory(new CategoryPath("a"));
+    int o1 = taxoWriter.addCategory(new FacetLabel("a"));
+    int o2 = taxoWriter.addCategory(new FacetLabel("a"));
     assertTrue("ordinal for same category that is added twice should be the same !", o1 == o2);
     taxoWriter.close();
     dir.close();
@@ -374,7 +385,7 @@
     // LUCENE-4972: DTW used to create empty commits even if no changes were made
     Directory dir = newDirectory();
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(dir);
-    taxoWriter.addCategory(new CategoryPath("a"));
+    taxoWriter.addCategory(new FacetLabel("a"));
     taxoWriter.commit();
     
     long gen1 = SegmentInfos.getLastCommitGeneration(dir);
@@ -391,7 +402,7 @@
     // LUCENE-4972: DTW used to create empty commits even if no changes were made
     Directory dir = newDirectory();
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(dir);
-    taxoWriter.addCategory(new CategoryPath("a"));
+    taxoWriter.addCategory(new FacetLabel("a"));
     taxoWriter.commit();
     
     long gen1 = SegmentInfos.getLastCommitGeneration(dir);
@@ -408,7 +419,7 @@
     // LUCENE-4972: DTW used to create empty commits even if no changes were made
     Directory dir = newDirectory();
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(dir);
-    taxoWriter.addCategory(new CategoryPath("a"));
+    taxoWriter.addCategory(new FacetLabel("a"));
     taxoWriter.prepareCommit();
     taxoWriter.commit();
     
@@ -427,30 +438,28 @@
     Directory indexDir = newDirectory(), taxoDir = newDirectory();
     IndexWriter indexWriter = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE, new Cl2oTaxonomyWriterCache(2, 1f, 1));
-    FacetFields facetFields = new FacetFields(taxoWriter);
+    FacetsConfig config = new FacetsConfig();
     
     // Add one huge label:
     String bigs = null;
     int ordinal = -1;
-    CategoryPath cp = null;
-    while (true) {
-      int len = CategoryPath.MAX_CATEGORY_PATH_LENGTH - 4; // for the dimension and separator
-      bigs = _TestUtil.randomSimpleString(random(), len, len);
-      cp = new CategoryPath("dim", bigs);
-      ordinal = taxoWriter.addCategory(cp);
-      Document doc = new Document();
-      facetFields.addFields(doc, Collections.singletonList(cp));
-      indexWriter.addDocument(doc);
-      break;
-    }
+
+    int len = FacetLabel.MAX_CATEGORY_PATH_LENGTH - 4; // for the dimension and separator
+    bigs = _TestUtil.randomSimpleString(random(), len, len);
+    FacetField ff = new FacetField("dim", bigs);
+    FacetLabel cp = new FacetLabel("dim", bigs);
+    ordinal = taxoWriter.addCategory(cp);
+    Document doc = new Document();
+    doc.add(ff);
+    indexWriter.addDocument(config.build(taxoWriter, doc));
 
     // Add tiny ones to cause a re-hash
     for (int i = 0; i < 3; i++) {
       String s = _TestUtil.randomSimpleString(random(), 1, 10);
-      taxoWriter.addCategory(new CategoryPath("dim", s));
-      Document doc = new Document();
-      facetFields.addFields(doc, Collections.singletonList(new CategoryPath("dim", s)));
-      indexWriter.addDocument(doc);
+      taxoWriter.addCategory(new FacetLabel("dim", s));
+      doc = new Document();
+      doc.add(new FacetField("dim", s));
+      indexWriter.addDocument(config.build(taxoWriter, doc));
     }
 
     // when too large components were allowed to be added, this resulted in a new added category
@@ -461,13 +470,11 @@
     DirectoryReader indexReader = DirectoryReader.open(indexDir);
     TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
     IndexSearcher searcher = new IndexSearcher(indexReader);
-    DrillDownQuery ddq = new DrillDownQuery(FacetIndexingParams.DEFAULT);
-    ddq.add(cp);
+    DrillDownQuery ddq = new DrillDownQuery(new FacetsConfig());
+    ddq.add("dim", bigs);
     assertEquals(1, searcher.search(ddq, 10).totalHits);
     
-    IOUtils.close(indexReader, taxoReader);
-    
-    IOUtils.close(indexDir, taxoDir);
+    IOUtils.close(indexReader, taxoReader, indexDir, taxoDir);
   }
   
   @Test
@@ -476,11 +483,11 @@
     
     // build source, large, taxonomy
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(srcTaxoDir);
-    int ord = taxoWriter.addCategory(new CategoryPath("A/1/1/1/1/1/1", '/'));
+    int ord = taxoWriter.addCategory(new FacetLabel("A", "1", "1", "1", "1", "1", "1"));
     taxoWriter.close();
     
     taxoWriter = new DirectoryTaxonomyWriter(targetTaxoDir);
-    int ordinal = taxoWriter.addCategory(new CategoryPath("B/1", '/'));
+    int ordinal = taxoWriter.addCategory(new FacetLabel("B", "1"));
     assertEquals(1, taxoWriter.getParent(ordinal)); // call getParent to initialize taxoArrays
     taxoWriter.commit();
     


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestCategoryPath.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestCategoryPath.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestCategoryPath.java	2013-07-15 15:52:17.317877397 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestCategoryPath.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,306 +0,0 @@
-package org.apache.lucene.facet.taxonomy;
-
-import java.util.Arrays;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.util._TestUtil;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestCategoryPath extends FacetTestCase {
-  
-  @Test 
-  public void testBasic() {
-    assertEquals(0, CategoryPath.EMPTY.length);
-    assertEquals(1, new CategoryPath("hello").length);
-    assertEquals(2, new CategoryPath("hello", "world").length);
-  }
-  
-  @Test 
-  public void testToString() {
-    // When the category is empty, we expect an empty string
-    assertEquals("", CategoryPath.EMPTY.toString('/'));
-    // one category (so no delimiter needed)
-    assertEquals("hello", new CategoryPath("hello").toString('/'));
-    // more than one category (so no delimiter needed)
-    assertEquals("hello/world", new CategoryPath("hello", "world").toString('/'));
-  }
-
-  @Test 
-  public void testGetComponent() {
-    String[] components = new String[atLeast(10)];
-    for (int i = 0; i < components.length; i++) {
-      components[i] = Integer.toString(i);
-    }
-    CategoryPath cp = new CategoryPath(components);
-    for (int i = 0; i < components.length; i++) {
-      assertEquals(i, Integer.parseInt(cp.components[i]));
-    }
-  }
-
-  @Test
-  public void testDelimiterConstructor() {
-    CategoryPath p = new CategoryPath("", '/');
-    assertEquals(0, p.length);
-    p = new CategoryPath("hello", '/');
-    assertEquals(p.length, 1);
-    assertEquals(p.toString('@'), "hello");
-    p = new CategoryPath("hi/there", '/');
-    assertEquals(p.length, 2);
-    assertEquals(p.toString('@'), "hi@there");
-    p = new CategoryPath("how/are/you/doing?", '/');
-    assertEquals(p.length, 4);
-    assertEquals(p.toString('@'), "how@are@you@doing?");
-  }
-  
-  @Test
-  public void testDefaultConstructor() {
-    // test that the default constructor (no parameters) currently
-    // defaults to creating an object with a 0 initial capacity.
-    // If we change this default later, we also need to change this
-    // test.
-    CategoryPath p = CategoryPath.EMPTY;
-    assertEquals(0, p.length);
-    assertEquals("", p.toString('/'));
-  }
-  
-  @Test 
-  public void testSubPath() {
-    final CategoryPath p = new CategoryPath("hi", "there", "man");
-    assertEquals(p.length, 3);
-    
-    CategoryPath p1 = p.subpath(2);
-    assertEquals(2, p1.length);
-    assertEquals("hi/there", p1.toString('/'));
-
-    p1 = p.subpath(1);
-    assertEquals(1, p1.length);
-    assertEquals("hi", p1.toString('/'));
-
-    p1 = p.subpath(0);
-    assertEquals(0, p1.length);
-    assertEquals("", p1.toString('/'));
-
-    // with all the following lengths, the prefix should be the whole path 
-    int[] lengths = { 3, -1, 4 };
-    for (int i = 0; i < lengths.length; i++) {
-      p1 = p.subpath(lengths[i]);
-      assertEquals(3, p1.length);
-      assertEquals("hi/there/man", p1.toString('/'));
-      assertEquals(p, p1);
-    }
-  }
-
-  @Test 
-  public void testEquals() {
-    assertEquals(CategoryPath.EMPTY, CategoryPath.EMPTY);
-    assertFalse(CategoryPath.EMPTY.equals(new CategoryPath("hi")));
-    assertFalse(CategoryPath.EMPTY.equals(Integer.valueOf(3)));
-    assertEquals(new CategoryPath("hello", "world"), new CategoryPath("hello", "world"));    
-  }
-  
-  @Test 
-  public void testHashCode() {
-    assertEquals(CategoryPath.EMPTY.hashCode(), CategoryPath.EMPTY.hashCode());
-    assertFalse(CategoryPath.EMPTY.hashCode() == new CategoryPath("hi").hashCode());
-    assertEquals(new CategoryPath("hello", "world").hashCode(), new CategoryPath("hello", "world").hashCode());
-  }
-  
-  @Test 
-  public void testLongHashCode() {
-    assertEquals(CategoryPath.EMPTY.longHashCode(), CategoryPath.EMPTY.longHashCode());
-    assertFalse(CategoryPath.EMPTY.longHashCode() == new CategoryPath("hi").longHashCode());
-    assertEquals(new CategoryPath("hello", "world").longHashCode(), new CategoryPath("hello", "world").longHashCode());
-  }
-  
-  @Test 
-  public void testArrayConstructor() {
-    CategoryPath p = new CategoryPath("hello", "world", "yo");
-    assertEquals(3, p.length);
-    assertEquals("hello/world/yo", p.toString('/'));
-  }
-  
-  @Test 
-  public void testCharsNeededForFullPath() {
-    assertEquals(0, CategoryPath.EMPTY.fullPathLength());
-    String[] components = { "hello", "world", "yo" };
-    CategoryPath cp = new CategoryPath(components);
-    int expectedCharsNeeded = 0;
-    for (String comp : components) {
-      expectedCharsNeeded += comp.length();
-    }
-    expectedCharsNeeded += cp.length - 1; // delimiter chars
-    assertEquals(expectedCharsNeeded, cp.fullPathLength());
-  }
-  
-  @Test 
-  public void testCopyToCharArray() {
-    CategoryPath p = new CategoryPath("hello", "world", "yo");
-    char[] charArray = new char[p.fullPathLength()];
-    int numCharsCopied = p.copyFullPath(charArray, 0, '.');
-    assertEquals(p.fullPathLength(), numCharsCopied);
-    assertEquals("hello.world.yo", new String(charArray, 0, numCharsCopied));
-  }
-  
-  @Test 
-  public void testCompareTo() {
-    CategoryPath p = new CategoryPath("a/b/c/d", '/');
-    CategoryPath pother = new CategoryPath("a/b/c/d", '/');
-    assertEquals(0, pother.compareTo(p));
-    assertEquals(0, p.compareTo(pother));
-    pother = new CategoryPath("", '/');
-    assertTrue(pother.compareTo(p) < 0);
-    assertTrue(p.compareTo(pother) > 0);
-    pother = new CategoryPath("a/b_/c/d", '/');
-    assertTrue(pother.compareTo(p) > 0);
-    assertTrue(p.compareTo(pother) < 0);
-    pother = new CategoryPath("a/b/c", '/');
-    assertTrue(pother.compareTo(p) < 0);
-    assertTrue(p.compareTo(pother) > 0);
-    pother = new CategoryPath("a/b/c/e", '/');
-    assertTrue(pother.compareTo(p) > 0);
-    assertTrue(p.compareTo(pother) < 0);
-  }
-
-  @Test
-  public void testEmptyNullComponents() throws Exception {
-    // LUCENE-4724: CategoryPath should not allow empty or null components
-    String[][] components_tests = new String[][] {
-      new String[] { "", "test" }, // empty in the beginning
-      new String[] { "test", "" }, // empty in the end
-      new String[] { "test", "", "foo" }, // empty in the middle
-      new String[] { null, "test" }, // null at the beginning
-      new String[] { "test", null }, // null in the end
-      new String[] { "test", null, "foo" }, // null in the middle
-    };
-
-    for (String[] components : components_tests) {
-      try {
-        assertNotNull(new CategoryPath(components));
-        fail("empty or null components should not be allowed: " + Arrays.toString(components));
-      } catch (IllegalArgumentException e) {
-        // ok
-      }
-    }
-    
-    String[] path_tests = new String[] {
-        "/test", // empty in the beginning
-        "test//foo", // empty in the middle
-    };
-    
-    for (String path : path_tests) {
-      try {
-        assertNotNull(new CategoryPath(path, '/'));
-        fail("empty or null components should not be allowed: " + path);
-      } catch (IllegalArgumentException e) {
-        // ok
-      }
-    }
-
-    // a trailing path separator is produces only one component
-    assertNotNull(new CategoryPath("test/", '/'));
-    
-  }
-
-  @Test
-  public void testInvalidDelimChar() throws Exception {
-    // Make sure CategoryPath doesn't silently corrupt:
-    char[] buf = new char[100];
-    CategoryPath cp = new CategoryPath("foo/bar");
-    try {
-      cp.toString();
-      fail("expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    try {
-      cp.copyFullPath(buf, 0, '/');
-      fail("expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    cp = new CategoryPath("abc", "foo/bar");
-    try {
-      cp.toString();
-      fail("expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    try {
-      cp.copyFullPath(buf, 0, '/');
-      fail("expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    cp = new CategoryPath("foo:bar");
-    try {
-      cp.toString(':');
-      fail("expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    try {
-      cp.copyFullPath(buf, 0, ':');
-      fail("expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    cp = new CategoryPath("abc", "foo:bar");
-    try {
-      cp.toString(':');
-      fail("expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    try {
-      cp.copyFullPath(buf, 0, ':');
-      fail("expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-  }
-
-  @Test
-  public void testLongPath() throws Exception {
-    String bigComp = null;
-    while (true) {
-      int len = CategoryPath.MAX_CATEGORY_PATH_LENGTH;
-      bigComp = _TestUtil.randomSimpleString(random(), len, len);
-      if (bigComp.indexOf('\u001f') != -1) {
-        continue;
-      }
-      break;
-    }
-
-    try {
-      assertNotNull(new CategoryPath("dim", bigComp));
-      fail("long paths should not be allowed; len=" + bigComp.length());
-    } catch (IllegalArgumentException e) {
-      // expected
-    }
-
-    try {
-      assertNotNull(new CategoryPath("dim\u001f" + bigComp, '\u001f'));
-      fail("long paths should not be allowed; len=" + bigComp.length());
-    } catch (IllegalArgumentException e) {
-      // expected
-    }
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestFacetLabel.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestFacetLabel.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestFacetLabel.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestFacetLabel.java	2013-12-03 13:09:15.496886922 -0500
@@ -0,0 +1,185 @@
+package org.apache.lucene.facet.taxonomy;
+
+import java.util.Arrays;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.util._TestUtil;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestFacetLabel extends FacetTestCase {
+  
+  @Test 
+  public void testBasic() {
+    assertEquals(0, new FacetLabel().length);
+    assertEquals(1, new FacetLabel("hello").length);
+    assertEquals(2, new FacetLabel("hello", "world").length);
+  }
+  
+  @Test 
+  public void testToString() {
+    // When the category is empty, we expect an empty string
+    assertEquals("FacetLabel: []", new FacetLabel().toString());
+    // one category
+    assertEquals("FacetLabel: [hello]", new FacetLabel("hello").toString());
+    // more than one category
+    assertEquals("FacetLabel: [hello, world]", new FacetLabel("hello", "world").toString());
+  }
+
+  @Test 
+  public void testGetComponent() {
+    String[] components = new String[atLeast(10)];
+    for (int i = 0; i < components.length; i++) {
+      components[i] = Integer.toString(i);
+    }
+    FacetLabel cp = new FacetLabel(components);
+    for (int i = 0; i < components.length; i++) {
+      assertEquals(i, Integer.parseInt(cp.components[i]));
+    }
+  }
+
+  @Test
+  public void testDefaultConstructor() {
+    // test that the default constructor (no parameters) currently
+    // defaults to creating an object with a 0 initial capacity.
+    // If we change this default later, we also need to change this
+    // test.
+    FacetLabel p = new FacetLabel();
+    assertEquals(0, p.length);
+    assertEquals("FacetLabel: []", p.toString());
+  }
+  
+  @Test 
+  public void testSubPath() {
+    final FacetLabel p = new FacetLabel("hi", "there", "man");
+    assertEquals(p.length, 3);
+    
+    FacetLabel p1 = p.subpath(2);
+    assertEquals(2, p1.length);
+    assertEquals("FacetLabel: [hi, there]", p1.toString());
+
+    p1 = p.subpath(1);
+    assertEquals(1, p1.length);
+    assertEquals("FacetLabel: [hi]", p1.toString());
+
+    p1 = p.subpath(0);
+    assertEquals(0, p1.length);
+    assertEquals("FacetLabel: []", p1.toString());
+
+    // with all the following lengths, the prefix should be the whole path 
+    int[] lengths = { 3, -1, 4 };
+    for (int i = 0; i < lengths.length; i++) {
+      p1 = p.subpath(lengths[i]);
+      assertEquals(3, p1.length);
+      assertEquals("FacetLabel: [hi, there, man]", p1.toString());
+      assertEquals(p, p1);
+    }
+  }
+
+  @Test 
+  public void testEquals() {
+    assertEquals(new FacetLabel(), new FacetLabel());
+    assertFalse(new FacetLabel().equals(new FacetLabel("hi")));
+    assertFalse(new FacetLabel().equals(Integer.valueOf(3)));
+    assertEquals(new FacetLabel("hello", "world"), new FacetLabel("hello", "world"));    
+  }
+  
+  @Test 
+  public void testHashCode() {
+    assertEquals(new FacetLabel().hashCode(), new FacetLabel().hashCode());
+    assertFalse(new FacetLabel().hashCode() == new FacetLabel("hi").hashCode());
+    assertEquals(new FacetLabel("hello", "world").hashCode(), new FacetLabel("hello", "world").hashCode());
+  }
+  
+  @Test 
+  public void testLongHashCode() {
+    assertEquals(new FacetLabel().longHashCode(), new FacetLabel().longHashCode());
+    assertFalse(new FacetLabel().longHashCode() == new FacetLabel("hi").longHashCode());
+    assertEquals(new FacetLabel("hello", "world").longHashCode(), new FacetLabel("hello", "world").longHashCode());
+  }
+  
+  @Test 
+  public void testArrayConstructor() {
+    FacetLabel p = new FacetLabel("hello", "world", "yo");
+    assertEquals(3, p.length);
+    assertEquals("FacetLabel: [hello, world, yo]", p.toString());
+  }
+  
+  @Test 
+  public void testCompareTo() {
+    FacetLabel p = new FacetLabel("a", "b", "c", "d");
+    FacetLabel pother = new FacetLabel("a", "b", "c", "d");
+    assertEquals(0, pother.compareTo(p));
+    assertEquals(0, p.compareTo(pother));
+    pother = new FacetLabel();
+    assertTrue(pother.compareTo(p) < 0);
+    assertTrue(p.compareTo(pother) > 0);
+    pother = new FacetLabel("a", "b_", "c", "d");
+    assertTrue(pother.compareTo(p) > 0);
+    assertTrue(p.compareTo(pother) < 0);
+    pother = new FacetLabel("a", "b", "c");
+    assertTrue(pother.compareTo(p) < 0);
+    assertTrue(p.compareTo(pother) > 0);
+    pother = new FacetLabel("a", "b", "c", "e");
+    assertTrue(pother.compareTo(p) > 0);
+    assertTrue(p.compareTo(pother) < 0);
+  }
+
+  @Test
+  public void testEmptyNullComponents() throws Exception {
+    // LUCENE-4724: CategoryPath should not allow empty or null components
+    String[][] components_tests = new String[][] {
+      new String[] { "", "test" }, // empty in the beginning
+      new String[] { "test", "" }, // empty in the end
+      new String[] { "test", "", "foo" }, // empty in the middle
+      new String[] { null, "test" }, // null at the beginning
+      new String[] { "test", null }, // null in the end
+      new String[] { "test", null, "foo" }, // null in the middle
+    };
+
+    for (String[] components : components_tests) {
+      try {
+        assertNotNull(new FacetLabel(components));
+        fail("empty or null components should not be allowed: " + Arrays.toString(components));
+      } catch (IllegalArgumentException e) {
+        // ok
+      }
+    }
+  }
+
+  @Test
+  public void testLongPath() throws Exception {
+    String bigComp = null;
+    while (true) {
+      int len = FacetLabel.MAX_CATEGORY_PATH_LENGTH;
+      bigComp = _TestUtil.randomSimpleString(random(), len, len);
+      if (bigComp.indexOf('\u001f') != -1) {
+        continue;
+      }
+      break;
+    }
+
+    try {
+      assertNotNull(new FacetLabel("dim", bigComp));
+      fail("long paths should not be allowed; len=" + bigComp.length());
+    } catch (IllegalArgumentException e) {
+      // expected
+    }
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestLRUHashMap.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestLRUHashMap.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestLRUHashMap.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestLRUHashMap.java	2013-11-25 18:44:52.492578506 -0500
@@ -0,0 +1,60 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.taxonomy.LRUHashMap;
+import org.junit.Test;
+
+public class TestLRUHashMap extends FacetTestCase {
+  // testLRU() tests that the specified size limit is indeed honored, and
+  // the remaining objects in the map are indeed those that have been most
+  // recently used
+  @Test
+  public void testLRU() throws Exception {
+    LRUHashMap<String, String> lru = new LRUHashMap<String, String>(3);
+    assertEquals(0, lru.size());
+    lru.put("one", "Hello world");
+    assertEquals(1, lru.size());
+    lru.put("two", "Hi man");
+    assertEquals(2, lru.size());
+    lru.put("three", "Bonjour");
+    assertEquals(3, lru.size());
+    lru.put("four", "Shalom");
+    assertEquals(3, lru.size());
+    assertNotNull(lru.get("three"));
+    assertNotNull(lru.get("two"));
+    assertNotNull(lru.get("four"));
+    assertNull(lru.get("one"));
+    lru.put("five", "Yo!");
+    assertEquals(3, lru.size());
+    assertNull(lru.get("three")); // three was last used, so it got removed
+    assertNotNull(lru.get("five"));
+    lru.get("four");
+    lru.put("six", "hi");
+    lru.put("seven", "hey dude");
+    assertEquals(3, lru.size());
+    assertNull(lru.get("one"));
+    assertNull(lru.get("two"));
+    assertNull(lru.get("three"));
+    assertNotNull(lru.get("four"));
+    assertNull(lru.get("five"));
+    assertNotNull(lru.get("six"));
+    assertNotNull(lru.get("seven"));
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java	2013-03-20 06:26:07.011245400 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java	2013-12-01 16:03:38.673229769 -0500
@@ -113,7 +113,7 @@
    */
   public static void fillTaxonomy(TaxonomyWriter tw) throws IOException {
     for (int i = 0; i < categories.length; i++) {
-      int ordinal = tw.addCategory(new CategoryPath(categories[i]));
+      int ordinal = tw.addCategory(new FacetLabel(categories[i]));
       int expectedOrdinal = expectedPaths[i][expectedPaths[i].length-1];
       if (ordinal!=expectedOrdinal) {
         fail("For category "+showcat(categories[i])+" expected ordinal "+
@@ -140,14 +140,14 @@
     return sb.toString();
   }
 
-  private String showcat(CategoryPath path) {
+  private String showcat(FacetLabel path) {
     if (path==null) {
       return "<null>";
     }
     if (path.length==0) {
       return "<empty>";
     }
-    return "<"+path.toString('/')+">";
+    return "<"+path.toString()+">";
   }
 
   /**  Basic tests for TaxonomyWriter. Basically, we test that
@@ -232,7 +232,7 @@
     tw = new DirectoryTaxonomyWriter(indexDir);
     fillTaxonomy(tw);
     // Add one new category, just to make commit() do something:
-    tw.addCategory(new CategoryPath("hi"));
+    tw.addCategory(new FacetLabel("hi"));
     // Do a commit(). Here was a bug - if tw had a reader open, it should
     // be reopened after the commit. However, in our case the reader should
     // not be open (as explained above) but because it was not set to null,
@@ -254,34 +254,34 @@
     TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
     assertEquals(1, tw.getSize()); // the root only
     // Test that adding a new top-level category works
-    assertEquals(1, tw.addCategory(new CategoryPath("a")));
+    assertEquals(1, tw.addCategory(new FacetLabel("a")));
     assertEquals(2, tw.getSize());
     // Test that adding the same category again is noticed, and the
     // same ordinal (and not a new one) is returned.
-    assertEquals(1, tw.addCategory(new CategoryPath("a")));
+    assertEquals(1, tw.addCategory(new FacetLabel("a")));
     assertEquals(2, tw.getSize());
     // Test that adding another top-level category returns a new ordinal,
     // not the same one
-    assertEquals(2, tw.addCategory(new CategoryPath("b")));
+    assertEquals(2, tw.addCategory(new FacetLabel("b")));
     assertEquals(3, tw.getSize());
     // Test that adding a category inside one of the above adds just one
     // new ordinal:
-    assertEquals(3, tw.addCategory(new CategoryPath("a","c")));
+    assertEquals(3, tw.addCategory(new FacetLabel("a","c")));
     assertEquals(4, tw.getSize());
     // Test that adding the same second-level category doesn't do anything:
-    assertEquals(3, tw.addCategory(new CategoryPath("a","c")));
+    assertEquals(3, tw.addCategory(new FacetLabel("a","c")));
     assertEquals(4, tw.getSize());
     // Test that adding a second-level category with two new components
     // indeed adds two categories
-    assertEquals(5, tw.addCategory(new CategoryPath("d","e")));
+    assertEquals(5, tw.addCategory(new FacetLabel("d","e")));
     assertEquals(6, tw.getSize());
     // Verify that the parents were added above in the order we expected
-    assertEquals(4, tw.addCategory(new CategoryPath("d")));
+    assertEquals(4, tw.addCategory(new FacetLabel("d")));
     // Similar, but inside a category that already exists:
-    assertEquals(7, tw.addCategory(new CategoryPath("b", "d","e")));
+    assertEquals(7, tw.addCategory(new FacetLabel("b", "d","e")));
     assertEquals(8, tw.getSize());
     // And now inside two levels of categories that already exist:
-    assertEquals(8, tw.addCategory(new CategoryPath("b", "d","f")));
+    assertEquals(8, tw.addCategory(new FacetLabel("b", "d","f")));
     assertEquals(9, tw.getSize());
     
     tw.close();
@@ -304,7 +304,7 @@
     assertEquals(1, tr.getSize());
     assertEquals(0, tr.getPath(0).length);
     assertEquals(TaxonomyReader.INVALID_ORDINAL, tr.getParallelTaxonomyArrays().parents()[0]);
-    assertEquals(0, tr.getOrdinal(CategoryPath.EMPTY));
+    assertEquals(0, tr.getOrdinal(new FacetLabel()));
     tr.close();
     indexDir.close();
   }
@@ -323,7 +323,7 @@
     assertEquals(1, tr.getSize());
     assertEquals(0, tr.getPath(0).length);
     assertEquals(TaxonomyReader.INVALID_ORDINAL, tr.getParallelTaxonomyArrays().parents()[0]);
-    assertEquals(0, tr.getOrdinal(CategoryPath.EMPTY));
+    assertEquals(0, tr.getOrdinal(new FacetLabel()));
     tw.close();
     tr.close();
     indexDir.close();
@@ -352,8 +352,8 @@
 
     // test TaxonomyReader.getCategory():
     for (int i = 1; i < tr.getSize(); i++) {
-      CategoryPath expectedCategory = new CategoryPath(expectedCategories[i]);
-      CategoryPath category = tr.getPath(i);
+      FacetLabel expectedCategory = new FacetLabel(expectedCategories[i]);
+      FacetLabel category = tr.getPath(i);
       if (!expectedCategory.equals(category)) {
         fail("For ordinal "+i+" expected category "+
             showcat(expectedCategory)+", but got "+showcat(category));
@@ -367,15 +367,15 @@
     // test TaxonomyReader.getOrdinal():
     for (int i = 1; i < expectedCategories.length; i++) {
       int expectedOrdinal = i;
-      int ordinal = tr.getOrdinal(new CategoryPath(expectedCategories[i]));
+      int ordinal = tr.getOrdinal(new FacetLabel(expectedCategories[i]));
       if (expectedOrdinal != ordinal) {
         fail("For category "+showcat(expectedCategories[i])+" expected ordinal "+
             expectedOrdinal+", but got "+ordinal);
       }
     }
     // (also test invalid categories:)
-    assertEquals(TaxonomyReader.INVALID_ORDINAL, tr.getOrdinal(new CategoryPath("non-existant")));
-    assertEquals(TaxonomyReader.INVALID_ORDINAL, tr.getOrdinal(new CategoryPath("Author", "Jules Verne")));
+    assertEquals(TaxonomyReader.INVALID_ORDINAL, tr.getOrdinal(new FacetLabel("non-existant")));
+    assertEquals(TaxonomyReader.INVALID_ORDINAL, tr.getOrdinal(new FacetLabel("Author", "Jules Verne")));
 
     tr.close();
     indexDir.close();
@@ -407,9 +407,9 @@
 
     // check parent of non-root ordinals:
     for (int ordinal=1; ordinal<tr.getSize(); ordinal++) {
-      CategoryPath me = tr.getPath(ordinal);
+      FacetLabel me = tr.getPath(ordinal);
       int parentOrdinal = parents[ordinal];
-      CategoryPath parent = tr.getPath(parentOrdinal);
+      FacetLabel parent = tr.getPath(parentOrdinal);
       if (parent==null) {
         fail("Parent of "+ordinal+" is "+parentOrdinal+
         ", but this is not a valid category.");
@@ -476,9 +476,9 @@
 
     // check parent of non-root ordinals:
     for (int ordinal = 1; ordinal < tr.getSize(); ordinal++) {
-      CategoryPath me = tr.getPath(ordinal);
+      FacetLabel me = tr.getPath(ordinal);
       int parentOrdinal = tw.getParent(ordinal);
-      CategoryPath parent = tr.getPath(parentOrdinal);
+      FacetLabel parent = tr.getPath(parentOrdinal);
       if (parent == null) {
         fail("Parent of " + ordinal + " is " + parentOrdinal
             + ", but this is not a valid category.");
@@ -668,7 +668,7 @@
   public void testChildrenArraysGrowth() throws Exception {
     Directory indexDir = newDirectory();
     TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
-    tw.addCategory(new CategoryPath("hi", "there"));
+    tw.addCategory(new FacetLabel("hi", "there"));
     tw.commit();
     TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     ParallelTaxonomyArrays ca = tr.getParallelTaxonomyArrays();
@@ -677,8 +677,8 @@
     assertEquals(3, ca.children().length);
     assertTrue(Arrays.equals(new int[] { 1, 2, -1 }, ca.children()));
     assertTrue(Arrays.equals(new int[] { -1, -1, -1 }, ca.siblings()));
-    tw.addCategory(new CategoryPath("hi", "ho"));
-    tw.addCategory(new CategoryPath("hello"));
+    tw.addCategory(new FacetLabel("hi", "ho"));
+    tw.addCategory(new FacetLabel("hello"));
     tw.commit();
     // Before refresh, nothing changed..
     ParallelTaxonomyArrays newca = tr.getParallelTaxonomyArrays();
@@ -708,8 +708,8 @@
     // compute base child arrays - after first chunk, and after the other
     Directory indexDirBase = newDirectory();
     TaxonomyWriter twBase = new DirectoryTaxonomyWriter(indexDirBase);
-    twBase.addCategory(new CategoryPath("a", "0"));
-    final CategoryPath abPath = new CategoryPath("a", "b");
+    twBase.addCategory(new FacetLabel("a", "0"));
+    final FacetLabel abPath = new FacetLabel("a", "b");
     twBase.addCategory(abPath);
     twBase.commit();
     TaxonomyReader trBase = new DirectoryTaxonomyReader(indexDirBase);
@@ -721,7 +721,7 @@
     
     final int numCategories = atLeast(800);
     for (int i = 0; i < numCategories; i++) {
-      twBase.addCategory(new CategoryPath("a", "b", Integer.toString(i)));
+      twBase.addCategory(new FacetLabel("a", "b", Integer.toString(i)));
     }
     twBase.close();
     
@@ -742,18 +742,18 @@
     indexDirBase.close();
   }
 
-  private void assertConsistentYoungestChild(final CategoryPath abPath,
+  private void assertConsistentYoungestChild(final FacetLabel abPath,
       final int abOrd, final int abYoungChildBase1, final int abYoungChildBase2, final int retry, int numCategories)
       throws Exception {
     SlowRAMDirectory indexDir = new SlowRAMDirectory(-1, null); // no slowness for intialization
     TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
-    tw.addCategory(new CategoryPath("a", "0"));
+    tw.addCategory(new FacetLabel("a", "0"));
     tw.addCategory(abPath);
     tw.commit();
     
     final DirectoryTaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
     for (int i = 0; i < numCategories; i++) {
-      final CategoryPath cp = new CategoryPath("a", "b", Integer.toString(i));
+      final FacetLabel cp = new FacetLabel("a", "b", Integer.toString(i));
       tw.addCategory(cp);
       assertEquals("Ordinal of "+cp+" must be invalid until Taxonomy Reader was refreshed", TaxonomyReader.INVALID_ORDINAL, tr.getOrdinal(cp));
     }
@@ -840,7 +840,7 @@
     TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
 
     assertEquals(1, tr.getSize()); // the empty taxonomy has size 1 (the root)
-    tw.addCategory(new CategoryPath("Author"));
+    tw.addCategory(new FacetLabel("Author"));
     assertEquals(1, tr.getSize()); // still root only...
     assertNull(TaxonomyReader.openIfChanged(tr)); // this is not enough, because tw.commit() hasn't been done yet
     assertEquals(1, tr.getSize()); // still root only...
@@ -864,7 +864,7 @@
     // the parent of this category is correct (this requires the reader
     // to correctly update its prefetched parent vector), and that the
     // old information also wasn't ruined:
-    tw.addCategory(new CategoryPath("Author", "Richard Dawkins"));
+    tw.addCategory(new FacetLabel("Author", "Richard Dawkins"));
     int dawkins = 2;
     tw.commit();
     newTaxoReader = TaxonomyReader.openIfChanged(tr);
@@ -889,7 +889,7 @@
     TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
 
     // Test getOrdinal():
-    CategoryPath author = new CategoryPath("Author");
+    FacetLabel author = new FacetLabel("Author");
 
     assertEquals(1, tr.getSize()); // the empty taxonomy has size 1 (the root)
     assertEquals(TaxonomyReader.INVALID_ORDINAL, tr.getOrdinal(author));
@@ -924,13 +924,13 @@
     // native fslock impl gets angry if we use it, so use RAMDirectory explicitly.
     Directory indexDir = new RAMDirectory();
     TaxonomyWriter tw = new DirectoryTaxonomyWriter(indexDir);
-    tw.addCategory(new CategoryPath("hi", "there"));
+    tw.addCategory(new FacetLabel("hi", "there"));
     tw.commit();
     // we deliberately not close the write now, and keep it open and
     // locked.
     // Verify that the writer worked:
     TaxonomyReader tr = new DirectoryTaxonomyReader(indexDir);
-    assertEquals(2, tr.getOrdinal(new CategoryPath("hi", "there")));
+    assertEquals(2, tr.getOrdinal(new FacetLabel("hi", "there")));
     // Try to open a second writer, with the first one locking the directory.
     // We expect to get a LockObtainFailedException.
     try {
@@ -943,14 +943,14 @@
     // write to the new writer.
     DirectoryTaxonomyWriter.unlock(indexDir);
     TaxonomyWriter tw2 = new DirectoryTaxonomyWriter(indexDir);
-    tw2.addCategory(new CategoryPath("hey"));
+    tw2.addCategory(new FacetLabel("hey"));
     tw2.close();
     // See that the writer indeed wrote:
     TaxonomyReader newtr = TaxonomyReader.openIfChanged(tr);
     assertNotNull(newtr);
     tr.close();
     tr = newtr;
-    assertEquals(3, tr.getOrdinal(new CategoryPath("hey")));
+    assertEquals(3, tr.getOrdinal(new FacetLabel("hey")));
     tr.close();
     tw.close();
     indexDir.close();
@@ -967,7 +967,7 @@
    */
   public static void fillTaxonomyCheckPaths(TaxonomyWriter tw) throws IOException {
     for (int i = 0; i < categories.length; i++) {
-      int ordinal = tw.addCategory(new CategoryPath(categories[i]));
+      int ordinal = tw.addCategory(new FacetLabel(categories[i]));
       int expectedOrdinal = expectedPaths[i][expectedPaths[i].length-1];
       if (ordinal!=expectedOrdinal) {
         fail("For category "+showcat(categories[i])+" expected ordinal "+
@@ -1052,7 +1052,7 @@
     DirectoryTaxonomyWriter writer = new DirectoryTaxonomyWriter(dir);
     TaxonomyReader reader = new DirectoryTaxonomyReader(writer);
     
-    CategoryPath cp = new CategoryPath("a");
+    FacetLabel cp = new FacetLabel("a");
     writer.addCategory(cp);
     TaxonomyReader newReader = TaxonomyReader.openIfChanged(reader);
     assertNotNull("expected a new instance", newReader);


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/cl2o/TestCharBlockArray.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/cl2o/TestCharBlockArray.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/cl2o/TestCharBlockArray.java	2013-08-01 14:47:20.730689730 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/cl2o/TestCharBlockArray.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,108 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.cl2o;
-
-import java.io.BufferedInputStream;
-import java.io.BufferedOutputStream;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.FileOutputStream;
-import java.nio.ByteBuffer;
-import java.nio.charset.CharsetDecoder;
-import java.nio.charset.CodingErrorAction;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestCharBlockArray extends FacetTestCase {
-
-  @Test public void testArray() throws Exception {
-    CharBlockArray array = new CharBlockArray();
-    StringBuilder builder = new StringBuilder();
-
-    final int n = 100 * 1000;
-
-    byte[] buffer = new byte[50];
-
-    for (int i = 0; i < n; i++) {
-      random().nextBytes(buffer);
-      int size = 1 + random().nextInt(50);
-      // This test is turning random bytes into a string,
-      // this is asking for trouble.
-      CharsetDecoder decoder = IOUtils.CHARSET_UTF_8.newDecoder()
-          .onUnmappableCharacter(CodingErrorAction.REPLACE)
-          .onMalformedInput(CodingErrorAction.REPLACE);
-      String s = decoder.decode(ByteBuffer.wrap(buffer, 0, size)).toString();
-      array.append(s);
-      builder.append(s);
-    }
-
-    for (int i = 0; i < n; i++) {
-      random().nextBytes(buffer);
-      int size = 1 + random().nextInt(50);
-      // This test is turning random bytes into a string,
-      // this is asking for trouble.
-      CharsetDecoder decoder = IOUtils.CHARSET_UTF_8.newDecoder()
-          .onUnmappableCharacter(CodingErrorAction.REPLACE)
-          .onMalformedInput(CodingErrorAction.REPLACE);
-      String s = decoder.decode(ByteBuffer.wrap(buffer, 0, size)).toString();
-      array.append((CharSequence)s);
-      builder.append(s);
-    }
-
-    for (int i = 0; i < n; i++) {
-      random().nextBytes(buffer);
-      int size = 1 + random().nextInt(50);
-      // This test is turning random bytes into a string,
-      // this is asking for trouble.
-      CharsetDecoder decoder = IOUtils.CHARSET_UTF_8.newDecoder()
-          .onUnmappableCharacter(CodingErrorAction.REPLACE)
-          .onMalformedInput(CodingErrorAction.REPLACE);
-      String s = decoder.decode(ByteBuffer.wrap(buffer, 0, size)).toString();
-      for (int j = 0; j < s.length(); j++) {
-        array.append(s.charAt(j));
-      }
-      builder.append(s);
-    }
-
-    assertEqualsInternal("GrowingCharArray<->StringBuilder mismatch.", builder, array);
-
-    File tempDir = _TestUtil.getTempDir("growingchararray");
-    File f = new File(tempDir, "GrowingCharArrayTest.tmp");
-    BufferedOutputStream out = new BufferedOutputStream(new FileOutputStream(f));
-    array.flush(out);
-    out.flush();
-    out.close();
-
-    BufferedInputStream in = new BufferedInputStream(new FileInputStream(f));
-    array = CharBlockArray.open(in);
-    assertEqualsInternal("GrowingCharArray<->StringBuilder mismatch after flush/load.", builder, array);
-    in.close();
-    f.delete();
-  }
-
-  private static void assertEqualsInternal(String msg, StringBuilder expected, CharBlockArray actual) {
-    assertEquals(msg, expected.length(), actual.length());
-    for (int i = 0; i < expected.length(); i++) {
-      assertEquals(msg, expected.charAt(i), actual.charAt(i));
-    }
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/cl2o/TestCompactLabelToOrdinal.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/cl2o/TestCompactLabelToOrdinal.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/cl2o/TestCompactLabelToOrdinal.java	2013-02-20 13:38:17.544711926 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/cl2o/TestCompactLabelToOrdinal.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,125 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.cl2o;
-
-import java.io.File;
-import java.nio.ByteBuffer;
-import java.nio.charset.CharsetDecoder;
-import java.nio.charset.CodingErrorAction;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Random;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestCompactLabelToOrdinal extends FacetTestCase {
-
-  @Test
-  public void testL2O() throws Exception {
-    LabelToOrdinal map = new LabelToOrdinalMap();
-
-    CompactLabelToOrdinal compact = new CompactLabelToOrdinal(2000000, 0.15f, 3);
-
-    final int n = atLeast(10 * 1000);
-    final int numUniqueValues = 50 * 1000;
-
-    String[] uniqueValues = new String[numUniqueValues];
-    byte[] buffer = new byte[50];
-
-    Random random = random();
-    for (int i = 0; i < numUniqueValues;) {
-      random.nextBytes(buffer);
-      int size = 1 + random.nextInt(buffer.length);
-
-      // This test is turning random bytes into a string,
-      // this is asking for trouble.
-      CharsetDecoder decoder = IOUtils.CHARSET_UTF_8.newDecoder()
-          .onUnmappableCharacter(CodingErrorAction.REPLACE)
-          .onMalformedInput(CodingErrorAction.REPLACE);
-      uniqueValues[i] = decoder.decode(ByteBuffer.wrap(buffer, 0, size)).toString();
-      // we cannot have empty path components, so eliminate all prefix as well
-      // as middle consecuive delimiter chars.
-      uniqueValues[i] = uniqueValues[i].replaceAll("/+", "/");
-      if (uniqueValues[i].startsWith("/")) {
-        uniqueValues[i] = uniqueValues[i].substring(1);
-      }
-      if (uniqueValues[i].indexOf(CompactLabelToOrdinal.TERMINATOR_CHAR) == -1) {
-        i++;
-      }
-    }
-
-    File tmpDir = _TestUtil.getTempDir("testLableToOrdinal");
-    File f = new File(tmpDir, "CompactLabelToOrdinalTest.tmp");
-    int flushInterval = 10;
-
-    for (int i = 0; i < n; i++) {
-      if (i > 0 && i % flushInterval == 0) {
-        compact.flush(f);    
-        compact = CompactLabelToOrdinal.open(f, 0.15f, 3);
-        assertTrue(f.delete());
-        if (flushInterval < (n / 10)) {
-          flushInterval *= 10;
-        }
-      }
-
-      int index = random.nextInt(numUniqueValues);
-      CategoryPath label = new CategoryPath(uniqueValues[index], '/');
-
-      int ord1 = map.getOrdinal(label);
-      int ord2 = compact.getOrdinal(label);
-
-      assertEquals(ord1, ord2);
-
-      if (ord1 == LabelToOrdinal.INVALID_ORDINAL) {
-        ord1 = compact.getNextOrdinal();
-        map.addLabel(label, ord1);
-        compact.addLabel(label, ord1);
-      }
-    }
-
-    for (int i = 0; i < numUniqueValues; i++) {
-      CategoryPath label = new CategoryPath(uniqueValues[i], '/');
-      int ord1 = map.getOrdinal(label);
-      int ord2 = compact.getOrdinal(label);
-      assertEquals(ord1, ord2);
-    }
-  }
-
-  private static class LabelToOrdinalMap extends LabelToOrdinal {
-    private Map<CategoryPath, Integer> map = new HashMap<CategoryPath, Integer>();
-
-    LabelToOrdinalMap() { }
-    
-    @Override
-    public void addLabel(CategoryPath label, int ordinal) {
-      map.put(label, ordinal);
-    }
-
-    @Override
-    public int getOrdinal(CategoryPath label) {
-      Integer value = map.get(label);
-      return (value != null) ? value.intValue() : LabelToOrdinal.INVALID_ORDINAL;
-    }
-
-  }
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/TestCharBlockArray.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/TestCharBlockArray.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/TestCharBlockArray.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/TestCharBlockArray.java	2013-11-26 15:06:03.430618152 -0500
@@ -0,0 +1,108 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+import java.io.BufferedInputStream;
+import java.io.BufferedOutputStream;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileOutputStream;
+import java.nio.ByteBuffer;
+import java.nio.charset.CharsetDecoder;
+import java.nio.charset.CodingErrorAction;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestCharBlockArray extends FacetTestCase {
+
+  @Test public void testArray() throws Exception {
+    CharBlockArray array = new CharBlockArray();
+    StringBuilder builder = new StringBuilder();
+
+    final int n = 100 * 1000;
+
+    byte[] buffer = new byte[50];
+
+    for (int i = 0; i < n; i++) {
+      random().nextBytes(buffer);
+      int size = 1 + random().nextInt(50);
+      // This test is turning random bytes into a string,
+      // this is asking for trouble.
+      CharsetDecoder decoder = IOUtils.CHARSET_UTF_8.newDecoder()
+          .onUnmappableCharacter(CodingErrorAction.REPLACE)
+          .onMalformedInput(CodingErrorAction.REPLACE);
+      String s = decoder.decode(ByteBuffer.wrap(buffer, 0, size)).toString();
+      array.append(s);
+      builder.append(s);
+    }
+
+    for (int i = 0; i < n; i++) {
+      random().nextBytes(buffer);
+      int size = 1 + random().nextInt(50);
+      // This test is turning random bytes into a string,
+      // this is asking for trouble.
+      CharsetDecoder decoder = IOUtils.CHARSET_UTF_8.newDecoder()
+          .onUnmappableCharacter(CodingErrorAction.REPLACE)
+          .onMalformedInput(CodingErrorAction.REPLACE);
+      String s = decoder.decode(ByteBuffer.wrap(buffer, 0, size)).toString();
+      array.append((CharSequence)s);
+      builder.append(s);
+    }
+
+    for (int i = 0; i < n; i++) {
+      random().nextBytes(buffer);
+      int size = 1 + random().nextInt(50);
+      // This test is turning random bytes into a string,
+      // this is asking for trouble.
+      CharsetDecoder decoder = IOUtils.CHARSET_UTF_8.newDecoder()
+          .onUnmappableCharacter(CodingErrorAction.REPLACE)
+          .onMalformedInput(CodingErrorAction.REPLACE);
+      String s = decoder.decode(ByteBuffer.wrap(buffer, 0, size)).toString();
+      for (int j = 0; j < s.length(); j++) {
+        array.append(s.charAt(j));
+      }
+      builder.append(s);
+    }
+
+    assertEqualsInternal("GrowingCharArray<->StringBuilder mismatch.", builder, array);
+
+    File tempDir = _TestUtil.getTempDir("growingchararray");
+    File f = new File(tempDir, "GrowingCharArrayTest.tmp");
+    BufferedOutputStream out = new BufferedOutputStream(new FileOutputStream(f));
+    array.flush(out);
+    out.flush();
+    out.close();
+
+    BufferedInputStream in = new BufferedInputStream(new FileInputStream(f));
+    array = CharBlockArray.open(in);
+    assertEqualsInternal("GrowingCharArray<->StringBuilder mismatch after flush/load.", builder, array);
+    in.close();
+    f.delete();
+  }
+
+  private static void assertEqualsInternal(String msg, StringBuilder expected, CharBlockArray actual) {
+    assertEquals(msg, expected.length(), actual.length());
+    for (int i = 0; i < expected.length(); i++) {
+      assertEquals(msg, expected.charAt(i), actual.charAt(i));
+    }
+  }
+
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/TestCompactLabelToOrdinal.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/TestCompactLabelToOrdinal.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/TestCompactLabelToOrdinal.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/TestCompactLabelToOrdinal.java	2013-12-01 16:04:00.497229185 -0500
@@ -0,0 +1,137 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+import java.io.File;
+import java.nio.ByteBuffer;
+import java.nio.charset.CharsetDecoder;
+import java.nio.charset.CodingErrorAction;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Random;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestCompactLabelToOrdinal extends FacetTestCase {
+
+  @Test
+  public void testL2O() throws Exception {
+    LabelToOrdinal map = new LabelToOrdinalMap();
+
+    CompactLabelToOrdinal compact = new CompactLabelToOrdinal(2000000, 0.15f, 3);
+
+    final int n = atLeast(10 * 1000);
+    final int numUniqueValues = 50 * 1000;
+
+    String[] uniqueValues = new String[numUniqueValues];
+    byte[] buffer = new byte[50];
+
+    Random random = random();
+    for (int i = 0; i < numUniqueValues;) {
+      random.nextBytes(buffer);
+      int size = 1 + random.nextInt(buffer.length);
+
+      // This test is turning random bytes into a string,
+      // this is asking for trouble.
+      CharsetDecoder decoder = IOUtils.CHARSET_UTF_8.newDecoder()
+          .onUnmappableCharacter(CodingErrorAction.REPLACE)
+          .onMalformedInput(CodingErrorAction.REPLACE);
+      uniqueValues[i] = decoder.decode(ByteBuffer.wrap(buffer, 0, size)).toString();
+      // we cannot have empty path components, so eliminate all prefix as well
+      // as middle consecutive delimiter chars.
+      uniqueValues[i] = uniqueValues[i].replaceAll("/+", "/");
+      if (uniqueValues[i].startsWith("/")) {
+        uniqueValues[i] = uniqueValues[i].substring(1);
+      }
+      if (uniqueValues[i].indexOf(CompactLabelToOrdinal.TERMINATOR_CHAR) == -1) {
+        i++;
+      }
+    }
+
+    File tmpDir = _TestUtil.getTempDir("testLableToOrdinal");
+    File f = new File(tmpDir, "CompactLabelToOrdinalTest.tmp");
+    int flushInterval = 10;
+
+    for (int i = 0; i < n; i++) {
+      if (i > 0 && i % flushInterval == 0) {
+        compact.flush(f);    
+        compact = CompactLabelToOrdinal.open(f, 0.15f, 3);
+        assertTrue(f.delete());
+        if (flushInterval < (n / 10)) {
+          flushInterval *= 10;
+        }
+      }
+
+      int index = random.nextInt(numUniqueValues);
+      FacetLabel label;
+      String s = uniqueValues[index];
+      if (s.length() == 0) {
+        label = new FacetLabel();
+      } else {
+        label = new FacetLabel(s.split("/"));
+      }
+
+      int ord1 = map.getOrdinal(label);
+      int ord2 = compact.getOrdinal(label);
+
+      assertEquals(ord1, ord2);
+
+      if (ord1 == LabelToOrdinal.INVALID_ORDINAL) {
+        ord1 = compact.getNextOrdinal();
+        map.addLabel(label, ord1);
+        compact.addLabel(label, ord1);
+      }
+    }
+
+    for (int i = 0; i < numUniqueValues; i++) {
+      FacetLabel label;
+      String s = uniqueValues[i];
+      if (s.length() == 0) {
+        label = new FacetLabel();
+      } else {
+        label = new FacetLabel(s.split("/"));
+      }
+      int ord1 = map.getOrdinal(label);
+      int ord2 = compact.getOrdinal(label);
+      assertEquals(ord1, ord2);
+    }
+  }
+
+  private static class LabelToOrdinalMap extends LabelToOrdinal {
+    private Map<FacetLabel, Integer> map = new HashMap<FacetLabel, Integer>();
+
+    LabelToOrdinalMap() { }
+    
+    @Override
+    public void addLabel(FacetLabel label, int ordinal) {
+      map.put(label, ordinal);
+    }
+
+    @Override
+    public int getOrdinal(FacetLabel label) {
+      Integer value = map.get(label);
+      return (value != null) ? value.intValue() : LabelToOrdinal.INVALID_ORDINAL;
+    }
+
+  }
+
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/TestCachedOrdinalsReader.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestCachedOrdinalsReader.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/TestCachedOrdinalsReader.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestCachedOrdinalsReader.java	2013-11-27 18:15:21.904003950 -0500
@@ -0,0 +1,83 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.junit.Test;
+
+public class TestCachedOrdinalsReader extends FacetTestCase {
+
+  @Test
+  public void testWithThreads() throws Exception {
+    // LUCENE-5303: OrdinalsCache used the ThreadLocal BinaryDV instead of reader.getCoreCacheKey().
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    IndexWriter writer = new IndexWriter(indexDir, conf);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    FacetsConfig config = new FacetsConfig();
+    
+    Document doc = new Document();
+    doc.add(new FacetField("A", "1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+    doc = new Document();
+    doc.add(new FacetField("A", "2"));
+    writer.addDocument(config.build(taxoWriter, doc));
+    
+    final DirectoryReader reader = DirectoryReader.open(writer, true);
+    final CachedOrdinalsReader ordsReader = new CachedOrdinalsReader(new DocValuesOrdinalsReader(FacetsConfig.DEFAULT_INDEX_FIELD_NAME));
+    Thread[] threads = new Thread[3];
+    for (int i = 0; i < threads.length; i++) {
+      threads[i] = new Thread("CachedOrdsThread-" + i) {
+        @Override
+        public void run() {
+          for (AtomicReaderContext context : reader.leaves()) {
+            try {
+              ordsReader.getReader(context);
+            } catch (IOException e) {
+              throw new RuntimeException(e);
+            }
+          }
+        }
+      };
+    }
+
+    long ramBytesUsed = 0;
+    for (Thread t : threads) {
+      t.start();
+      t.join();
+      if (ramBytesUsed == 0) {
+        ramBytesUsed = ordsReader.ramBytesUsed();
+      } else {
+        assertEquals(ramBytesUsed, ordsReader.ramBytesUsed());
+      }
+    }
+    
+    IOUtils.close(writer, taxoWriter, reader, indexDir, taxoDir);
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/TestDrillDownQuery.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestDrillDownQuery.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/TestDrillDownQuery.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestDrillDownQuery.java	2013-11-27 18:15:29.680002775 -0500
@@ -0,0 +1,246 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.QueryUtils;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+
+public class TestDrillDownQuery extends FacetTestCase {
+  
+  private static IndexReader reader;
+  private static DirectoryTaxonomyReader taxo;
+  private static Directory dir;
+  private static Directory taxoDir;
+  private static FacetsConfig config;
+
+  @AfterClass
+  public static void afterClassDrillDownQueryTest() throws Exception {
+    IOUtils.close(reader, taxo, dir, taxoDir);
+    reader = null;
+    taxo = null;
+    dir = null;
+    taxoDir = null;
+    config = null;
+  }
+
+  @BeforeClass
+  public static void beforeClassDrillDownQueryTest() throws Exception {
+    dir = newDirectory();
+    Random r = random();
+    RandomIndexWriter writer = new RandomIndexWriter(r, dir, 
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(r, MockTokenizer.KEYWORD, false)));
+    
+    taxoDir = newDirectory();
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    config = new FacetsConfig();
+
+    // Randomize the per-dim config:
+    config.setHierarchical("a", random().nextBoolean());
+    config.setMultiValued("a", random().nextBoolean());
+    if (random().nextBoolean()) {
+      config.setIndexFieldName("a", "$a");
+    }
+    config.setRequireDimCount("a", true);
+
+    config.setHierarchical("b", random().nextBoolean());
+    config.setMultiValued("b", random().nextBoolean());
+    if (random().nextBoolean()) {
+      config.setIndexFieldName("b", "$b");
+    }
+    config.setRequireDimCount("b", true);
+
+    for (int i = 0; i < 100; i++) {
+      Document doc = new Document();
+      if (i % 2 == 0) { // 50
+        doc.add(new TextField("content", "foo", Field.Store.NO));
+      }
+      if (i % 3 == 0) { // 33
+        doc.add(new TextField("content", "bar", Field.Store.NO));
+      }
+      if (i % 4 == 0) { // 25
+        if (r.nextBoolean()) {
+          doc.add(new FacetField("a", "1"));
+        } else {
+          doc.add(new FacetField("a", "2"));
+        }
+      }
+      if (i % 5 == 0) { // 20
+        doc.add(new FacetField("b", "1"));
+      }
+      writer.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    taxoWriter.close();
+    reader = writer.getReader();
+    writer.close();
+    
+    taxo = new DirectoryTaxonomyReader(taxoDir);
+  }
+  
+  public void testAndOrs() throws Exception {
+    IndexSearcher searcher = newSearcher(reader);
+
+    // test (a/1 OR a/2) AND b/1
+    DrillDownQuery q = new DrillDownQuery(config);
+    q.add("a", "1");
+    q.add("a", "2");
+    q.add("b", "1");
+    TopDocs docs = searcher.search(q, 100);
+    assertEquals(5, docs.totalHits);
+  }
+  
+  public void testQuery() throws IOException {
+    IndexSearcher searcher = newSearcher(reader);
+
+    // Making sure the query yields 25 documents with the facet "a"
+    DrillDownQuery q = new DrillDownQuery(config);
+    q.add("a");
+    System.out.println("q=" + q);
+    QueryUtils.check(q);
+    TopDocs docs = searcher.search(q, 100);
+    assertEquals(25, docs.totalHits);
+    
+    // Making sure the query yields 5 documents with the facet "b" and the
+    // previous (facet "a") query as a base query
+    DrillDownQuery q2 = new DrillDownQuery(config, q);
+    q2.add("b");
+    docs = searcher.search(q2, 100);
+    assertEquals(5, docs.totalHits);
+
+    // Making sure that a query of both facet "a" and facet "b" yields 5 results
+    DrillDownQuery q3 = new DrillDownQuery(config);
+    q3.add("a");
+    q3.add("b");
+    docs = searcher.search(q3, 100);
+    
+    assertEquals(5, docs.totalHits);
+    // Check that content:foo (which yields 50% results) and facet/b (which yields 20%)
+    // would gather together 10 results (10%..) 
+    Query fooQuery = new TermQuery(new Term("content", "foo"));
+    DrillDownQuery q4 = new DrillDownQuery(config, fooQuery);
+    q4.add("b");
+    docs = searcher.search(q4, 100);
+    assertEquals(10, docs.totalHits);
+  }
+  
+  public void testQueryImplicitDefaultParams() throws IOException {
+    IndexSearcher searcher = newSearcher(reader);
+
+    // Create the base query to start with
+    DrillDownQuery q = new DrillDownQuery(config);
+    q.add("a");
+    
+    // Making sure the query yields 5 documents with the facet "b" and the
+    // previous (facet "a") query as a base query
+    DrillDownQuery q2 = new DrillDownQuery(config, q);
+    q2.add("b");
+    TopDocs docs = searcher.search(q2, 100);
+    assertEquals(5, docs.totalHits);
+
+    // Check that content:foo (which yields 50% results) and facet/b (which yields 20%)
+    // would gather together 10 results (10%..) 
+    Query fooQuery = new TermQuery(new Term("content", "foo"));
+    DrillDownQuery q4 = new DrillDownQuery(config, fooQuery);
+    q4.add("b");
+    docs = searcher.search(q4, 100);
+    assertEquals(10, docs.totalHits);
+  }
+  
+  public void testScoring() throws IOException {
+    // verify that drill-down queries do not modify scores
+    IndexSearcher searcher = newSearcher(reader);
+
+    float[] scores = new float[reader.maxDoc()];
+    
+    Query q = new TermQuery(new Term("content", "foo"));
+    TopDocs docs = searcher.search(q, reader.maxDoc()); // fetch all available docs to this query
+    for (ScoreDoc sd : docs.scoreDocs) {
+      scores[sd.doc] = sd.score;
+    }
+    
+    // create a drill-down query with category "a", scores should not change
+    DrillDownQuery q2 = new DrillDownQuery(config, q);
+    q2.add("a");
+    docs = searcher.search(q2, reader.maxDoc()); // fetch all available docs to this query
+    for (ScoreDoc sd : docs.scoreDocs) {
+      assertEquals("score of doc=" + sd.doc + " modified", scores[sd.doc], sd.score, 0f);
+    }
+  }
+  
+  public void testScoringNoBaseQuery() throws IOException {
+    // verify that drill-down queries (with no base query) returns 0.0 score
+    IndexSearcher searcher = newSearcher(reader);
+    
+    DrillDownQuery q = new DrillDownQuery(config);
+    q.add("a");
+    TopDocs docs = searcher.search(q, reader.maxDoc()); // fetch all available docs to this query
+    for (ScoreDoc sd : docs.scoreDocs) {
+      assertEquals(0f, sd.score, 0f);
+    }
+  }
+  
+  public void testTermNonDefault() {
+    String aField = config.getDimConfig("a").indexFieldName;
+    Term termA = DrillDownQuery.term(aField, "a");
+    assertEquals(new Term(aField, "a"), termA);
+    
+    String bField = config.getDimConfig("b").indexFieldName;
+    Term termB = DrillDownQuery.term(bField, "b");
+    assertEquals(new Term(bField, "b"), termB);
+  }
+
+  public void testClone() throws Exception {
+    DrillDownQuery q = new DrillDownQuery(config, new MatchAllDocsQuery());
+    q.add("a");
+    
+    DrillDownQuery clone = q.clone();
+    clone.add("b");
+    
+    assertFalse("query wasn't cloned: source=" + q + " clone=" + clone, q.toString().equals(clone.toString()));
+  }
+  
+  public void testNoDrillDown() throws Exception {
+    Query base = new MatchAllDocsQuery();
+    DrillDownQuery q = new DrillDownQuery(config, base);
+    Query rewrite = q.rewrite(reader).rewrite(reader);
+    assertSame(base, rewrite);
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java	2013-12-02 10:39:52.083438159 -0500
@@ -0,0 +1,1061 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.facet.DrillSideways.DrillSidewaysResult;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.SortField;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.InPlaceMergeSorter;
+import org.apache.lucene.util.InfoStream;
+import org.apache.lucene.util._TestUtil;
+
+public class TestDrillSideways extends FacetTestCase {
+
+  public void testBasic() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setHierarchical("Publish Date", true);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new FacetField("Author", "Bob"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "15"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "20"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Susan"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "7"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Frank"));
+    doc.add(new FacetField("Publish Date", "1999", "5", "5"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    //System.out.println("searcher=" + searcher);
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    DrillSideways ds = new DrillSideways(searcher, config, taxoReader);
+
+    //  case: drill-down on a single field; in this
+    // case the drill-sideways + drill-down counts ==
+    // drill-down of just the query: 
+    DrillDownQuery ddq = new DrillDownQuery(config);
+    ddq.add("Author", "Lisa");
+    DrillSidewaysResult r = ds.search(null, ddq, 10);
+    assertEquals(2, r.hits.totalHits);
+    // Publish Date is only drill-down, and Lisa published
+    // one in 2012 and one in 2010:
+    assertEquals("dim=Publish Date path=[] value=2 childCount=2\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+
+    // Author is drill-sideways + drill-down: Lisa
+    // (drill-down) published twice, and Frank/Susan/Bob
+    // published once:
+    assertEquals("dim=Author path=[] value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    // Same simple case, but no baseQuery (pure browse):
+    // drill-down on a single field; in this case the
+    // drill-sideways + drill-down counts == drill-down of
+    // just the query:
+    ddq = new DrillDownQuery(config);
+    ddq.add("Author", "Lisa");
+    r = ds.search(null, ddq, 10);
+
+    assertEquals(2, r.hits.totalHits);
+    // Publish Date is only drill-down, and Lisa published
+    // one in 2012 and one in 2010:
+    assertEquals("dim=Publish Date path=[] value=2 childCount=2\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+
+    // Author is drill-sideways + drill-down: Lisa
+    // (drill-down) published twice, and Frank/Susan/Bob
+    // published once:
+    assertEquals("dim=Author path=[] value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    // Another simple case: drill-down on on single fields
+    // but OR of two values
+    ddq = new DrillDownQuery(config);
+    ddq.add("Author", "Lisa");
+    ddq.add("Author", "Bob");
+    r = ds.search(null, ddq, 10);
+    assertEquals(3, r.hits.totalHits);
+    // Publish Date is only drill-down: Lisa and Bob
+    // (drill-down) published twice in 2010 and once in 2012:
+    assertEquals("dim=Publish Date path=[] value=3 childCount=2\n  2010 (2)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+    // Author is drill-sideways + drill-down: Lisa
+    // (drill-down) published twice, and Frank/Susan/Bob
+    // published once:
+    assertEquals("dim=Author path=[] value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    // More interesting case: drill-down on two fields
+    ddq = new DrillDownQuery(config);
+    ddq.add("Author", "Lisa");
+    ddq.add("Publish Date", "2010");
+    r = ds.search(null, ddq, 10);
+    assertEquals(1, r.hits.totalHits);
+    // Publish Date is drill-sideways + drill-down: Lisa
+    // (drill-down) published once in 2010 and once in 2012:
+    assertEquals("dim=Publish Date path=[] value=2 childCount=2\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+    // Author is drill-sideways + drill-down:
+    // only Lisa & Bob published (once each) in 2010:
+    assertEquals("dim=Author path=[] value=2 childCount=2\n  Bob (1)\n  Lisa (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    // Even more interesting case: drill down on two fields,
+    // but one of them is OR
+    ddq = new DrillDownQuery(config);
+
+    // Drill down on Lisa or Bob:
+    ddq.add("Author", "Lisa");
+    ddq.add("Publish Date", "2010");
+    ddq.add("Author", "Bob");
+    r = ds.search(null, ddq, 10);
+    assertEquals(2, r.hits.totalHits);
+    // Publish Date is both drill-sideways + drill-down:
+    // Lisa or Bob published twice in 2010 and once in 2012:
+    assertEquals("dim=Publish Date path=[] value=3 childCount=2\n  2010 (2)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+    // Author is drill-sideways + drill-down:
+    // only Lisa & Bob published (once each) in 2010:
+    assertEquals("dim=Author path=[] value=2 childCount=2\n  Bob (1)\n  Lisa (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    // Test drilling down on invalid field:
+    ddq = new DrillDownQuery(config);
+    ddq.add("Foobar", "Baz");
+    r = ds.search(null, ddq, 10);
+    assertEquals(0, r.hits.totalHits);
+    assertNull(r.facets.getTopChildren(10, "Publish Date"));
+    assertNull(r.facets.getTopChildren(10, "Foobar"));
+
+    // Test drilling down on valid term or'd with invalid term:
+    ddq = new DrillDownQuery(config);
+    ddq.add("Author", "Lisa");
+    ddq.add("Author", "Tom");
+    r = ds.search(null, ddq, 10);
+    assertEquals(2, r.hits.totalHits);
+    // Publish Date is only drill-down, and Lisa published
+    // one in 2012 and one in 2010:
+    assertEquals("dim=Publish Date path=[] value=2 childCount=2\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+    // Author is drill-sideways + drill-down: Lisa
+    // (drill-down) published twice, and Frank/Susan/Bob
+    // published once:
+    assertEquals("dim=Author path=[] value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    // LUCENE-4915: test drilling down on a dimension but
+    // NOT facet counting it:
+    ddq = new DrillDownQuery(config);
+    ddq.add("Author", "Lisa");
+    ddq.add("Author", "Tom");
+    r = ds.search(null, ddq, 10);
+    assertEquals(2, r.hits.totalHits);
+    // Publish Date is only drill-down, and Lisa published
+    // one in 2012 and one in 2010:
+    assertEquals("dim=Publish Date path=[] value=2 childCount=2\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+
+    // Test main query gets null scorer:
+    ddq = new DrillDownQuery(config, new TermQuery(new Term("foobar", "baz")));
+    ddq.add("Author", "Lisa");
+    r = ds.search(null, ddq, 10);
+
+    assertEquals(0, r.hits.totalHits);
+    assertNull(r.facets.getTopChildren(10, "Publish Date"));
+    assertNull(r.facets.getTopChildren(10, "Author"));
+    IOUtils.close(searcher.getIndexReader(), taxoReader, writer, taxoWriter, dir, taxoDir);
+  }
+
+  public void testSometimesInvalidDrillDown() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setHierarchical("Publish Date", true);
+
+    Document doc = new Document();
+    doc.add(new FacetField("Author", "Bob"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "15"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "20"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    writer.commit();
+
+    // 2nd segment has no Author:
+    doc = new Document();
+    doc.add(new FacetField("Foobar", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    //System.out.println("searcher=" + searcher);
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    DrillDownQuery ddq = new DrillDownQuery(config);
+    ddq.add("Author", "Lisa");
+    DrillSidewaysResult r = new DrillSideways(searcher, config, taxoReader).search(null, ddq, 10);
+
+    assertEquals(1, r.hits.totalHits);
+    // Publish Date is only drill-down, and Lisa published
+    // one in 2012 and one in 2010:
+    assertEquals("dim=Publish Date path=[] value=1 childCount=1\n  2010 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+    // Author is drill-sideways + drill-down: Lisa
+    // (drill-down) published once, and Bob
+    // published once:
+    assertEquals("dim=Author path=[] value=2 childCount=2\n  Bob (1)\n  Lisa (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    IOUtils.close(searcher.getIndexReader(), taxoReader, writer, taxoWriter, dir, taxoDir);
+  }
+
+  public void testMultipleRequestsPerDim() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setHierarchical("dim", true);
+
+    Document doc = new Document();
+    doc.add(new FacetField("dim", "a", "x"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("dim", "a", "y"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("dim", "a", "z"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("dim", "b"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("dim", "c"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("dim", "d"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    //System.out.println("searcher=" + searcher);
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    DrillDownQuery ddq = new DrillDownQuery(config);
+    ddq.add("dim", "a");
+    DrillSidewaysResult r = new DrillSideways(searcher, config, taxoReader).search(null, ddq, 10);
+
+    assertEquals(3, r.hits.totalHits);
+    assertEquals("dim=dim path=[] value=6 childCount=4\n  a (3)\n  b (1)\n  c (1)\n  d (1)\n", r.facets.getTopChildren(10, "dim").toString());
+    assertEquals("dim=dim path=[a] value=3 childCount=3\n  x (1)\n  y (1)\n  z (1)\n", r.facets.getTopChildren(10, "dim", "a").toString());
+
+    IOUtils.close(searcher.getIndexReader(), taxoReader, writer, taxoWriter, dir, taxoDir);
+  }
+
+  private static class Doc implements Comparable<Doc> {
+    String id;
+    String contentToken;
+
+    public Doc() {}
+    
+    // -1 if the doc is missing this dim, else the index
+    // -into the values for this dim:
+    int[] dims;
+
+    // 2nd value per dim for the doc (so we test
+    // multi-valued fields):
+    int[] dims2;
+    boolean deleted;
+
+    @Override
+    public int compareTo(Doc other) {
+      return id.compareTo(other.id);
+    }
+  }
+
+  private double aChance, bChance, cChance;
+
+  private String randomContentToken(boolean isQuery) {
+    double d = random().nextDouble();
+    if (isQuery) {
+      if (d < 0.33) {
+        return "a";
+      } else if (d < 0.66) {
+        return "b";
+      } else {
+        return "c";
+      }
+    } else {
+      if (d <= aChance) {
+        return "a";
+      } else if (d < aChance + bChance) {
+        return "b";
+      } else {
+        return "c";
+      }
+    }
+  }
+
+  public void testRandom() throws Exception {
+
+    boolean canUseDV = defaultCodecSupportsSortedSet();
+
+    while (aChance == 0.0) {
+      aChance = random().nextDouble();
+    }
+    while (bChance == 0.0) {
+      bChance = random().nextDouble();
+    }
+    while (cChance == 0.0) {
+      cChance = random().nextDouble();
+    }
+    //aChance = .01;
+    //bChance = 0.5;
+    //cChance = 1.0;
+    double sum = aChance + bChance + cChance;
+    aChance /= sum;
+    bChance /= sum;
+    cChance /= sum;
+
+    int numDims = _TestUtil.nextInt(random(), 2, 5);
+    //int numDims = 3;
+    int numDocs = atLeast(3000);
+    //int numDocs = 20;
+    if (VERBOSE) {
+      System.out.println("numDims=" + numDims + " numDocs=" + numDocs + " aChance=" + aChance + " bChance=" + bChance + " cChance=" + cChance);
+    }
+    String[][] dimValues = new String[numDims][];
+    int valueCount = 2;
+
+    for(int dim=0;dim<numDims;dim++) {
+      Set<String> values = new HashSet<String>();
+      while (values.size() < valueCount) {
+        String s = _TestUtil.randomRealisticUnicodeString(random());
+        //String s = _TestUtil.randomString(random());
+        if (s.length() > 0) {
+          values.add(s);
+        }
+      } 
+      dimValues[dim] = values.toArray(new String[values.size()]);
+      valueCount *= 2;
+    }
+
+    List<Doc> docs = new ArrayList<Doc>();
+    for(int i=0;i<numDocs;i++) {
+      Doc doc = new Doc();
+      doc.id = ""+i;
+      doc.contentToken = randomContentToken(false);
+      doc.dims = new int[numDims];
+      doc.dims2 = new int[numDims];
+      for(int dim=0;dim<numDims;dim++) {
+        if (random().nextInt(5) == 3) {
+          // This doc is missing this dim:
+          doc.dims[dim] = -1;
+        } else if (dimValues[dim].length <= 4) {
+          int dimUpto = 0;
+          doc.dims[dim] = dimValues[dim].length-1;
+          while (dimUpto < dimValues[dim].length) {
+            if (random().nextBoolean()) {
+              doc.dims[dim] = dimUpto;
+              break;
+            }
+            dimUpto++;
+          }
+        } else {
+          doc.dims[dim] = random().nextInt(dimValues[dim].length);
+        }
+
+        if (random().nextInt(5) == 3) {
+          // 2nd value:
+          doc.dims2[dim] = random().nextInt(dimValues[dim].length);
+        } else {
+          doc.dims2[dim] = -1;
+        }
+      }
+      docs.add(doc);
+    }
+
+    Directory d = newDirectory();
+    Directory td = newDirectory();
+
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setInfoStream(InfoStream.NO_OUTPUT);
+    RandomIndexWriter w = new RandomIndexWriter(random(), d, iwc);
+    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(td, IndexWriterConfig.OpenMode.CREATE);
+    FacetsConfig config = new FacetsConfig();
+    for(int i=0;i<numDims;i++) {
+      config.setMultiValued("dim"+i, true);
+    }
+
+    boolean doUseDV = canUseDV && random().nextBoolean();
+
+    for(Doc rawDoc : docs) {
+      Document doc = new Document();
+      doc.add(newStringField("id", rawDoc.id, Field.Store.YES));
+      doc.add(newStringField("content", rawDoc.contentToken, Field.Store.NO));
+
+      if (VERBOSE) {
+        System.out.println("  doc id=" + rawDoc.id + " token=" + rawDoc.contentToken);
+      }
+      for(int dim=0;dim<numDims;dim++) {
+        int dimValue = rawDoc.dims[dim];
+        if (dimValue != -1) {
+          if (doUseDV) {
+            doc.add(new SortedSetDocValuesFacetField("dim" + dim, dimValues[dim][dimValue]));
+          } else {
+            doc.add(new FacetField("dim" + dim, dimValues[dim][dimValue]));
+          }
+          doc.add(new StringField("dim" + dim, dimValues[dim][dimValue], Field.Store.YES));
+          if (VERBOSE) {
+            System.out.println("    dim" + dim + "=" + new BytesRef(dimValues[dim][dimValue]));
+          }
+        }
+        int dimValue2 = rawDoc.dims2[dim];
+        if (dimValue2 != -1) {
+          if (doUseDV) {
+            doc.add(new SortedSetDocValuesFacetField("dim" + dim, dimValues[dim][dimValue2]));
+          } else {
+            doc.add(new FacetField("dim" + dim, dimValues[dim][dimValue2]));
+          }
+          doc.add(new StringField("dim" + dim, dimValues[dim][dimValue2], Field.Store.YES));
+          if (VERBOSE) {
+            System.out.println("      dim" + dim + "=" + new BytesRef(dimValues[dim][dimValue2]));
+          }
+        }
+      }
+
+      w.addDocument(config.build(tw, doc));
+    }
+
+    if (random().nextBoolean()) {
+      // Randomly delete a few docs:
+      int numDel = _TestUtil.nextInt(random(), 1, (int) (numDocs*0.05));
+      if (VERBOSE) {
+        System.out.println("delete " + numDel);
+      }
+      int delCount = 0;
+      while (delCount < numDel) {
+        Doc doc = docs.get(random().nextInt(docs.size()));
+        if (!doc.deleted) {
+          if (VERBOSE) {
+            System.out.println("  delete id=" + doc.id);
+          }
+          doc.deleted = true;
+          w.deleteDocuments(new Term("id", doc.id));
+          delCount++;
+        }
+      }
+    }
+
+    if (random().nextBoolean()) {
+      if (VERBOSE) {
+        System.out.println("TEST: forceMerge(1)...");
+      }
+      w.forceMerge(1);
+    }
+    IndexReader r = w.getReader();
+
+    final SortedSetDocValuesReaderState sortedSetDVState;
+    IndexSearcher s = newSearcher(r);
+    
+    if (doUseDV) {
+      sortedSetDVState = new SortedSetDocValuesReaderState(s.getIndexReader());
+    } else {
+      sortedSetDVState = null;
+    }
+
+    if (VERBOSE) {
+      System.out.println("r.numDocs() = " + r.numDocs());
+    }
+
+    // NRT open
+    TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
+
+    int numIters = atLeast(10);
+
+    for(int iter=0;iter<numIters;iter++) {
+
+      String contentToken = random().nextInt(30) == 17 ? null : randomContentToken(true);
+      int numDrillDown = _TestUtil.nextInt(random(), 1, Math.min(4, numDims));
+      if (VERBOSE) {
+        System.out.println("\nTEST: iter=" + iter + " baseQuery=" + contentToken + " numDrillDown=" + numDrillDown + " useSortedSetDV=" + doUseDV);
+      }
+
+      String[][] drillDowns = new String[numDims][];
+
+      int count = 0;
+      boolean anyMultiValuedDrillDowns = false;
+      while (count < numDrillDown) {
+        int dim = random().nextInt(numDims);
+        if (drillDowns[dim] == null) {
+          if (random().nextBoolean()) {
+            // Drill down on one value:
+            drillDowns[dim] = new String[] {dimValues[dim][random().nextInt(dimValues[dim].length)]};
+          } else {
+            int orCount = _TestUtil.nextInt(random(), 1, Math.min(5, dimValues[dim].length));
+            drillDowns[dim] = new String[orCount];
+            anyMultiValuedDrillDowns |= orCount > 1;
+            for(int i=0;i<orCount;i++) {
+              while (true) {
+                String value = dimValues[dim][random().nextInt(dimValues[dim].length)];
+                for(int j=0;j<i;j++) {
+                  if (value.equals(drillDowns[dim][j])) {
+                    value = null;
+                    break;
+                  }
+                }
+                if (value != null) {
+                  drillDowns[dim][i] = value;
+                  break;
+                }
+              }
+            }
+          }
+          if (VERBOSE) {
+            BytesRef[] values = new BytesRef[drillDowns[dim].length];
+            for(int i=0;i<values.length;i++) {
+              values[i] = new BytesRef(drillDowns[dim][i]);
+            }
+            System.out.println("  dim" + dim + "=" + Arrays.toString(values));
+          }
+          count++;
+        }
+      }
+
+      Query baseQuery;
+      if (contentToken == null) {
+        baseQuery = new MatchAllDocsQuery();
+      } else {
+        baseQuery = new TermQuery(new Term("content", contentToken));
+      }
+
+      DrillDownQuery ddq = new DrillDownQuery(config, baseQuery);
+
+      for(int dim=0;dim<numDims;dim++) {
+        if (drillDowns[dim] != null) {
+          int upto = 0;
+          for(String value : drillDowns[dim]) {
+            ddq.add("dim" + dim, value);
+          }
+        }
+      }
+
+      Filter filter;
+      if (random().nextInt(7) == 6) {
+        if (VERBOSE) {
+          System.out.println("  only-even filter");
+        }
+        filter = new Filter() {
+            @Override
+            public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+              int maxDoc = context.reader().maxDoc();
+              final FixedBitSet bits = new FixedBitSet(maxDoc);
+              for(int docID=0;docID < maxDoc;docID++) {
+                // Keeps only the even ids:
+                if ((acceptDocs == null || acceptDocs.get(docID)) && ((Integer.parseInt(context.reader().document(docID).get("id")) & 1) == 0)) {
+                  bits.set(docID);
+                }
+              }
+              return bits;
+            }
+          };
+      } else {
+        filter = null;
+      }
+
+      // Verify docs are always collected in order.  If we
+      // had an AssertingScorer it could catch it when
+      // Weight.scoresDocsOutOfOrder lies!:
+      new DrillSideways(s, config, tr).search(ddq,
+                           new Collector() {
+                             int lastDocID;
+
+                             @Override
+                             public void setScorer(Scorer s) {
+                             }
+
+                             @Override
+                             public void collect(int doc) {
+                               assert doc > lastDocID;
+                               lastDocID = doc;
+                             }
+
+                             @Override
+                             public void setNextReader(AtomicReaderContext context) {
+                               lastDocID = -1;
+                             }
+
+                             @Override
+                             public boolean acceptsDocsOutOfOrder() {
+                               return false;
+                             }
+                           });
+
+      // Also separately verify that DS respects the
+      // scoreSubDocsAtOnce method, to ensure that all
+      // subScorers are on the same docID:
+      if (!anyMultiValuedDrillDowns) {
+        // Can only do this test when there are no OR'd
+        // drill-down values, beacuse in that case it's
+        // easily possible for one of the DD terms to be on
+        // a future docID:
+        new DrillSideways(s, config, tr) {
+          @Override
+          protected boolean scoreSubDocsAtOnce() {
+            return true;
+          }
+        }.search(ddq, new AssertingSubDocsAtOnceCollector());
+      }
+
+      TestFacetResult expected = slowDrillSidewaysSearch(s, docs, contentToken, drillDowns, dimValues, filter);
+
+      Sort sort = new Sort(new SortField("id", SortField.Type.STRING));
+      DrillSideways ds;
+      if (doUseDV) {
+        ds = new DrillSideways(s, config, sortedSetDVState);
+      } else {
+        ds = new DrillSideways(s, config, tr) {
+            @Override
+            protected Facets buildFacetsResult(FacetsCollector drillDowns, FacetsCollector[] drillSideways, String[] drillSidewaysDims) throws IOException {
+              Map<String,Facets> drillSidewaysFacets = new HashMap<String,Facets>();
+              Facets drillDownFacets = getTaxonomyFacetCounts(taxoReader, config, drillDowns);
+              if (drillSideways != null) {
+                for(int i=0;i<drillSideways.length;i++) {
+                  drillSidewaysFacets.put(drillSidewaysDims[i],
+                                          getTaxonomyFacetCounts(taxoReader, config, drillSideways[i]));
+                }
+              }
+
+              if (drillSidewaysFacets.isEmpty()) {
+                return drillDownFacets;
+              } else {
+                return new MultiFacets(drillSidewaysFacets, drillDownFacets);
+              }
+
+            }
+          };
+      }
+
+      // Retrieve all facets:
+      DrillSidewaysResult actual = ds.search(ddq, filter, null, numDocs, sort, true, true);
+
+      TopDocs hits = s.search(baseQuery, numDocs);
+      Map<String,Float> scores = new HashMap<String,Float>();
+      for(ScoreDoc sd : hits.scoreDocs) {
+        scores.put(s.doc(sd.doc).get("id"), sd.score);
+      }
+      if (VERBOSE) {
+        System.out.println("  verify all facets");
+      }
+      verifyEquals(dimValues, s, expected, actual, scores, doUseDV);
+
+      // Make sure drill down doesn't change score:
+      TopDocs ddqHits = s.search(ddq, filter, numDocs);
+      assertEquals(expected.hits.size(), ddqHits.totalHits);
+      for(int i=0;i<expected.hits.size();i++) {
+        // Score should be IDENTICAL:
+        assertEquals(scores.get(expected.hits.get(i).id), ddqHits.scoreDocs[i].score, 0.0f);
+      }
+    }
+
+    IOUtils.close(r, tr, w, tw, d, td);
+  }
+
+  private static class Counters {
+    int[][] counts;
+
+    public Counters(String[][] dimValues) {
+      counts = new int[dimValues.length][];
+      for(int dim=0;dim<dimValues.length;dim++) {
+        counts[dim] = new int[dimValues[dim].length];
+      }
+    }
+
+    public void inc(int[] dims, int[] dims2) {
+      inc(dims, dims2, -1);
+    }
+
+    public void inc(int[] dims, int[] dims2, int onlyDim) {
+      assert dims.length == counts.length;
+      assert dims2.length == counts.length;
+      for(int dim=0;dim<dims.length;dim++) {
+        if (onlyDim == -1 || dim == onlyDim) {
+          if (dims[dim] != -1) {
+            counts[dim][dims[dim]]++;
+          }
+          if (dims2[dim] != -1 && dims2[dim] != dims[dim]) {
+            counts[dim][dims2[dim]]++;
+          }
+        }
+      }
+    }
+  }
+
+  private static class TestFacetResult {
+    List<Doc> hits;
+    int[][] counts;
+    int[] uniqueCounts;
+  }
+  
+  private int[] getTopNOrds(final int[] counts, final String[] values, int topN) {
+    final int[] ids = new int[counts.length];
+    for(int i=0;i<ids.length;i++) {
+      ids[i] = i;
+    }
+
+    // Naive (on purpose, to reduce bug in tester/gold):
+    // sort all ids, then return top N slice:
+    new InPlaceMergeSorter() {
+
+      @Override
+      protected void swap(int i, int j) {
+        int id = ids[i];
+        ids[i] = ids[j];
+        ids[j] = id;
+      }
+
+      @Override
+      protected int compare(int i, int j) {
+        int counti = counts[ids[i]];
+        int countj = counts[ids[j]];
+        // Sort by count descending...
+        if (counti > countj) {
+          return -1;
+        } else if (counti < countj) {
+          return 1;
+        } else {
+          // ... then by label ascending:
+          return new BytesRef(values[ids[i]]).compareTo(new BytesRef(values[ids[j]]));
+        }
+      }
+
+    }.sort(0, ids.length);
+
+    if (topN > ids.length) {
+      topN = ids.length;
+    }
+
+    int numSet = topN;
+    for(int i=0;i<topN;i++) {
+      if (counts[ids[i]] == 0) {
+        numSet = i;
+        break;
+      }
+    }
+
+    int[] topNIDs = new int[numSet];
+    System.arraycopy(ids, 0, topNIDs, 0, topNIDs.length);
+    return topNIDs;
+  }
+
+  private TestFacetResult slowDrillSidewaysSearch(IndexSearcher s, List<Doc> docs,
+                                                        String contentToken, String[][] drillDowns,
+                                                        String[][] dimValues, Filter onlyEven) throws Exception {
+    int numDims = dimValues.length;
+
+    List<Doc> hits = new ArrayList<Doc>();
+    Counters drillDownCounts = new Counters(dimValues);
+    Counters[] drillSidewaysCounts = new Counters[dimValues.length];
+    for(int dim=0;dim<numDims;dim++) {
+      drillSidewaysCounts[dim] = new Counters(dimValues);
+    }
+
+    if (VERBOSE) {
+      System.out.println("  compute expected");
+    }
+
+    nextDoc: for(Doc doc : docs) {
+      if (doc.deleted) {
+        continue;
+      }
+      if (onlyEven != null & (Integer.parseInt(doc.id) & 1) != 0) {
+        continue;
+      }
+      if (contentToken == null || doc.contentToken.equals(contentToken)) {
+        int failDim = -1;
+        for(int dim=0;dim<numDims;dim++) {
+          if (drillDowns[dim] != null) {
+            String docValue = doc.dims[dim] == -1 ? null : dimValues[dim][doc.dims[dim]];
+            String docValue2 = doc.dims2[dim] == -1 ? null : dimValues[dim][doc.dims2[dim]];
+            boolean matches = false;
+            for(String value : drillDowns[dim]) {
+              if (value.equals(docValue) || value.equals(docValue2)) {
+                matches = true;
+                break;
+              }
+            }
+            if (!matches) {
+              if (failDim == -1) {
+                // Doc could be a near-miss, if no other dim fails
+                failDim = dim;
+              } else {
+                // Doc isn't a hit nor a near-miss
+                continue nextDoc;
+              }
+            }
+          }
+        }
+
+        if (failDim == -1) {
+          if (VERBOSE) {
+            System.out.println("    exp: id=" + doc.id + " is a hit");
+          }
+          // Hit:
+          hits.add(doc);
+          drillDownCounts.inc(doc.dims, doc.dims2);
+          for(int dim=0;dim<dimValues.length;dim++) {
+            drillSidewaysCounts[dim].inc(doc.dims, doc.dims2);
+          }
+        } else {
+          if (VERBOSE) {
+            System.out.println("    exp: id=" + doc.id + " is a near-miss on dim=" + failDim);
+          }
+          drillSidewaysCounts[failDim].inc(doc.dims, doc.dims2, failDim);
+        }
+      }
+    }
+
+    Map<String,Integer> idToDocID = new HashMap<String,Integer>();
+    for(int i=0;i<s.getIndexReader().maxDoc();i++) {
+      idToDocID.put(s.doc(i).get("id"), i);
+    }
+
+    Collections.sort(hits);
+
+    TestFacetResult res = new TestFacetResult();
+    res.hits = hits;
+    res.counts = new int[numDims][];
+    res.uniqueCounts = new int[numDims];
+    for (int dim = 0; dim < numDims; dim++) {
+      if (drillDowns[dim] != null) {
+        res.counts[dim] = drillSidewaysCounts[dim].counts[dim];
+      } else {
+        res.counts[dim] = drillDownCounts.counts[dim];
+      }
+      int uniqueCount = 0;
+      for (int j = 0; j < res.counts[dim].length; j++) {
+        if (res.counts[dim][j] != 0) {
+          uniqueCount++;
+        }
+      }
+      res.uniqueCounts[dim] = uniqueCount;
+    }
+
+    return res;
+  }
+
+  void verifyEquals(String[][] dimValues, IndexSearcher s, TestFacetResult expected,
+                    DrillSidewaysResult actual, Map<String,Float> scores, boolean isSortedSetDV) throws Exception {
+    if (VERBOSE) {
+      System.out.println("  verify totHits=" + expected.hits.size());
+    }
+    assertEquals(expected.hits.size(), actual.hits.totalHits);
+    assertEquals(expected.hits.size(), actual.hits.scoreDocs.length);
+    for(int i=0;i<expected.hits.size();i++) {
+      if (VERBOSE) {
+        System.out.println("    hit " + i + " expected=" + expected.hits.get(i).id);
+      }
+      assertEquals(expected.hits.get(i).id,
+                   s.doc(actual.hits.scoreDocs[i].doc).get("id"));
+      // Score should be IDENTICAL:
+      assertEquals(scores.get(expected.hits.get(i).id), actual.hits.scoreDocs[i].score, 0.0f);
+    }
+
+    for(int dim=0;dim<expected.counts.length;dim++) {
+      int topN = random().nextBoolean() ? dimValues[dim].length : _TestUtil.nextInt(random(), 1, dimValues[dim].length);
+      FacetResult fr = actual.facets.getTopChildren(topN, "dim"+dim);
+      if (VERBOSE) {
+        System.out.println("    dim" + dim + " topN=" + topN + " (vs " + dimValues[dim].length + " unique values)");
+        System.out.println("      actual");
+      }
+
+      int idx = 0;
+      Map<String,Integer> actualValues = new HashMap<String,Integer>();
+
+      if (fr != null) {
+        for(LabelAndValue labelValue : fr.labelValues) {
+          actualValues.put(labelValue.label, labelValue.value.intValue());
+          if (VERBOSE) {
+            System.out.println("        " + idx + ": " + new BytesRef(labelValue.label) + ": " + labelValue.value);
+            idx++;
+          }
+        }
+        assertEquals("dim=" + dim, expected.uniqueCounts[dim], fr.childCount);
+      }
+
+      if (topN < dimValues[dim].length) {
+        int[] topNIDs = getTopNOrds(expected.counts[dim], dimValues[dim], topN);
+        if (VERBOSE) {
+          idx = 0;
+          System.out.println("      expected (sorted)");
+          for(int i=0;i<topNIDs.length;i++) {
+            int expectedOrd = topNIDs[i];
+            String value = dimValues[dim][expectedOrd];
+            System.out.println("        " + idx + ": " + new BytesRef(value) + ": " + expected.counts[dim][expectedOrd]);
+            idx++;
+          }
+        }
+        if (VERBOSE) {
+          System.out.println("      topN=" + topN + " expectedTopN=" + topNIDs.length);
+        }
+
+        if (fr != null) {
+          assertEquals(topNIDs.length, fr.labelValues.length);
+        } else {
+          assertEquals(0, topNIDs.length);
+        }
+        for(int i=0;i<topNIDs.length;i++) {
+          int expectedOrd = topNIDs[i];
+          assertEquals(expected.counts[dim][expectedOrd], fr.labelValues[i].value.intValue());
+          if (isSortedSetDV) {
+            // Tie-break facet labels are only in unicode
+            // order with SortedSetDVFacets:
+            assertEquals("value @ idx=" + i, dimValues[dim][expectedOrd], fr.labelValues[i].label);
+          }
+        }
+      } else {
+
+        if (VERBOSE) {
+          idx = 0;
+          System.out.println("      expected (unsorted)");
+          for(int i=0;i<dimValues[dim].length;i++) {
+            String value = dimValues[dim][i];
+            if (expected.counts[dim][i] != 0) {
+              System.out.println("        " + idx + ": " + new BytesRef(value) + ": " + expected.counts[dim][i]);
+              idx++;
+            } 
+          }
+        }
+
+        int setCount = 0;
+        for(int i=0;i<dimValues[dim].length;i++) {
+          String value = dimValues[dim][i];
+          if (expected.counts[dim][i] != 0) {
+            assertTrue(actualValues.containsKey(value));
+            assertEquals(expected.counts[dim][i], actualValues.get(value).intValue());
+            setCount++;
+          } else {
+            assertFalse(actualValues.containsKey(value));
+          }
+        }
+        assertEquals(setCount, actualValues.size());
+      }
+    }
+  }
+
+  public void testEmptyIndex() throws Exception {
+    // LUCENE-5045: make sure DrillSideways works with an empty index
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    // Count "Author"
+    FacetsConfig config = new FacetsConfig();
+    DrillSideways ds = new DrillSideways(searcher, config, taxoReader);
+    DrillDownQuery ddq = new DrillDownQuery(config);
+    ddq.add("Author", "Lisa");
+    
+    DrillSidewaysResult r = ds.search(ddq, 10); // this used to fail on IllegalArgEx
+    assertEquals(0, r.hits.totalHits);
+
+    r = ds.search(ddq, null, null, 10, new Sort(new SortField("foo", SortField.Type.INT)), false, false); // this used to fail on IllegalArgEx
+    assertEquals(0, r.hits.totalHits);
+    
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+}
+


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/TestFacetsConfig.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestFacetsConfig.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/TestFacetsConfig.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestFacetsConfig.java	2013-11-27 14:34:22.920357740 -0500
@@ -0,0 +1,46 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.util._TestUtil;
+
+public class TestFacetsConfig extends FacetTestCase {
+  public void testPathToStringAndBack() throws Exception {
+    int iters = atLeast(1000);
+    for(int i=0;i<iters;i++) {
+      int numParts = _TestUtil.nextInt(random(), 1, 6);
+      String[] parts = new String[numParts];
+      for(int j=0;j<numParts;j++) {
+        String s;
+        while (true) {
+          s = _TestUtil.randomUnicodeString(random());
+          if (s.length() > 0) {
+            break;
+          }
+        }
+        parts[j] = s;
+      }
+
+      String s = FacetsConfig.pathToString(parts);
+      String[] parts2 = FacetsConfig.stringToPath(s);
+      assertTrue(Arrays.equals(parts, parts2));
+    }
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/TestMultipleIndexFields.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestMultipleIndexFields.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/TestMultipleIndexFields.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestMultipleIndexFields.java	2013-12-02 10:39:48.035438268 -0500
@@ -0,0 +1,296 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.junit.Test;
+
+public class TestMultipleIndexFields extends FacetTestCase {
+
+  private static final FacetField[] CATEGORIES = new FacetField[] {
+    new FacetField("Author", "Mark Twain"),
+    new FacetField("Author", "Stephen King"),
+    new FacetField("Author", "Kurt Vonnegut"),
+    new FacetField("Band", "Rock & Pop", "The Beatles"),
+    new FacetField("Band", "Punk", "The Ramones"),
+    new FacetField("Band", "Rock & Pop", "U2"),
+    new FacetField("Band", "Rock & Pop", "REM"),
+    new FacetField("Band", "Rock & Pop", "Dave Matthews Band"),
+    new FacetField("Composer", "Bach"),
+  };
+
+  private FacetsConfig getConfig() {
+    FacetsConfig config = new FacetsConfig();
+    config.setHierarchical("Band", true);
+    return config;
+  }
+  
+  @Test
+  public void testDefault() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    // create and open an index writer
+    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
+        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
+    // create and open a taxonomy writer
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
+    FacetsConfig config = getConfig();
+
+    seedIndex(tw, iw, config);
+
+    IndexReader ir = iw.getReader();
+    tw.commit();
+
+    // prepare index reader and taxonomy.
+    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
+
+    // prepare searcher to search against
+    IndexSearcher searcher = newSearcher(ir);
+
+    FacetsCollector sfc = performSearch(tr, ir, searcher);
+
+    // Obtain facets results and hand-test them
+    assertCorrectResults(getTaxonomyFacetCounts(tr, config, sfc));
+
+    assertOrdinalsExist("$facets", ir);
+
+    IOUtils.close(tr, ir, iw, tw, indexDir, taxoDir);
+  }
+
+  @Test
+  public void testCustom() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    // create and open an index writer
+    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
+        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
+    // create and open a taxonomy writer
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
+
+    FacetsConfig config = getConfig();
+    config.setIndexFieldName("Author", "$author");
+    seedIndex(tw, iw, config);
+
+    IndexReader ir = iw.getReader();
+    tw.commit();
+
+    // prepare index reader and taxonomy.
+    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
+
+    // prepare searcher to search against
+    IndexSearcher searcher = newSearcher(ir);
+
+    FacetsCollector sfc = performSearch(tr, ir, searcher);
+
+    Map<String,Facets> facetsMap = new HashMap<String,Facets>();
+    facetsMap.put("Author", getTaxonomyFacetCounts(tr, config, sfc, "$author"));
+    Facets facets = new MultiFacets(facetsMap, getTaxonomyFacetCounts(tr, config, sfc));
+
+    // Obtain facets results and hand-test them
+    assertCorrectResults(facets);
+
+    assertOrdinalsExist("$facets", ir);
+    assertOrdinalsExist("$author", ir);
+
+    IOUtils.close(tr, ir, iw, tw, indexDir, taxoDir);
+  }
+
+  @Test
+  public void testTwoCustomsSameField() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    // create and open an index writer
+    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
+        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
+    // create and open a taxonomy writer
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
+
+    FacetsConfig config = getConfig();
+    config.setIndexFieldName("Band", "$music");
+    config.setIndexFieldName("Composer", "$music");
+    seedIndex(tw, iw, config);
+
+    IndexReader ir = iw.getReader();
+    tw.commit();
+
+    // prepare index reader and taxonomy.
+    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
+
+    // prepare searcher to search against
+    IndexSearcher searcher = newSearcher(ir);
+
+    FacetsCollector sfc = performSearch(tr, ir, searcher);
+
+    Map<String,Facets> facetsMap = new HashMap<String,Facets>();
+    Facets facets2 = getTaxonomyFacetCounts(tr, config, sfc, "$music");
+    facetsMap.put("Band", facets2);
+    facetsMap.put("Composer", facets2);
+    Facets facets = new MultiFacets(facetsMap, getTaxonomyFacetCounts(tr, config, sfc));
+
+    // Obtain facets results and hand-test them
+    assertCorrectResults(facets);
+
+    assertOrdinalsExist("$facets", ir);
+    assertOrdinalsExist("$music", ir);
+    assertOrdinalsExist("$music", ir);
+
+    IOUtils.close(tr, ir, iw, tw, indexDir, taxoDir);
+  }
+
+  private void assertOrdinalsExist(String field, IndexReader ir) throws IOException {
+    for (AtomicReaderContext context : ir.leaves()) {
+      AtomicReader r = context.reader();
+      if (r.getBinaryDocValues(field) != null) {
+        return; // not all segments must have this DocValues
+      }
+    }
+    fail("no ordinals found for " + field);
+  }
+
+  @Test
+  public void testDifferentFieldsAndText() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // create and open an index writer
+    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
+        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
+    // create and open a taxonomy writer
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
+
+    FacetsConfig config = getConfig();
+    config.setIndexFieldName("Band", "$bands");
+    config.setIndexFieldName("Composer", "$composers");
+    seedIndex(tw, iw, config);
+
+    IndexReader ir = iw.getReader();
+    tw.commit();
+
+    // prepare index reader and taxonomy.
+    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
+
+    // prepare searcher to search against
+    IndexSearcher searcher = newSearcher(ir);
+
+    FacetsCollector sfc = performSearch(tr, ir, searcher);
+
+    Map<String,Facets> facetsMap = new HashMap<String,Facets>();
+    facetsMap.put("Band", getTaxonomyFacetCounts(tr, config, sfc, "$bands"));
+    facetsMap.put("Composer", getTaxonomyFacetCounts(tr, config, sfc, "$composers"));
+    Facets facets = new MultiFacets(facetsMap, getTaxonomyFacetCounts(tr, config, sfc));
+
+    // Obtain facets results and hand-test them
+    assertCorrectResults(facets);
+    assertOrdinalsExist("$facets", ir);
+    assertOrdinalsExist("$bands", ir);
+    assertOrdinalsExist("$composers", ir);
+
+    IOUtils.close(tr, ir, iw, tw, indexDir, taxoDir);
+  }
+
+  @Test
+  public void testSomeSameSomeDifferent() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    // create and open an index writer
+    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
+        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
+    // create and open a taxonomy writer
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
+
+    FacetsConfig config = getConfig();
+    config.setIndexFieldName("Band", "$music");
+    config.setIndexFieldName("Composer", "$music");
+    config.setIndexFieldName("Author", "$literature");
+    seedIndex(tw, iw, config);
+
+    IndexReader ir = iw.getReader();
+    tw.commit();
+
+    // prepare index reader and taxonomy.
+    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
+
+    // prepare searcher to search against
+    IndexSearcher searcher = newSearcher(ir);
+
+    FacetsCollector sfc = performSearch(tr, ir, searcher);
+
+    Map<String,Facets> facetsMap = new HashMap<String,Facets>();
+    Facets facets2 = getTaxonomyFacetCounts(tr, config, sfc, "$music");
+    facetsMap.put("Band", facets2);
+    facetsMap.put("Composer", facets2);
+    facetsMap.put("Author", getTaxonomyFacetCounts(tr, config, sfc, "$literature"));
+    Facets facets = new MultiFacets(facetsMap, getTaxonomyFacetCounts(tr, config, sfc));
+
+    // Obtain facets results and hand-test them
+    assertCorrectResults(facets);
+    assertOrdinalsExist("$music", ir);
+    assertOrdinalsExist("$literature", ir);
+
+    IOUtils.close(tr, ir, iw, tw);
+    IOUtils.close(indexDir, taxoDir);
+  }
+
+  private void assertCorrectResults(Facets facets) throws IOException {
+    assertEquals(5, facets.getSpecificValue("Band"));
+    assertEquals("dim=Band path=[] value=5 childCount=2\n  Rock & Pop (4)\n  Punk (1)\n", facets.getTopChildren(10, "Band").toString());
+    assertEquals("dim=Band path=[Rock & Pop] value=4 childCount=4\n  The Beatles (1)\n  U2 (1)\n  REM (1)\n  Dave Matthews Band (1)\n", facets.getTopChildren(10, "Band", "Rock & Pop").toString());
+    assertEquals("dim=Author path=[] value=3 childCount=3\n  Mark Twain (1)\n  Stephen King (1)\n  Kurt Vonnegut (1)\n", facets.getTopChildren(10, "Author").toString());
+  }
+
+  private FacetsCollector performSearch(TaxonomyReader tr, IndexReader ir, 
+      IndexSearcher searcher) throws IOException {
+    FacetsCollector fc = new FacetsCollector();
+    FacetsCollector.search(searcher, new MatchAllDocsQuery(), 10, fc);
+    return fc;
+  }
+
+  private void seedIndex(TaxonomyWriter tw, RandomIndexWriter iw, FacetsConfig config) throws IOException {
+    for (FacetField ff : CATEGORIES) {
+      Document doc = new Document();
+      doc.add(ff);
+      doc.add(new TextField("content", "alpha", Field.Store.YES));
+      iw.addDocument(config.build(tw, doc));
+    }
+  }
+}
\ No newline at end of file


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/TestRangeFacetCounts.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestRangeFacetCounts.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/TestRangeFacetCounts.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestRangeFacetCounts.java	2013-12-18 19:03:52.701648532 -0500
@@ -0,0 +1,797 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.DoubleDocValuesField;
+import org.apache.lucene.document.DoubleField;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FloatDocValuesField;
+import org.apache.lucene.document.FloatField;
+import org.apache.lucene.document.LongField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.facet.DrillSideways.DrillSidewaysResult;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
+import org.apache.lucene.queries.function.valuesource.FloatFieldSource;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.NumericRangeQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+
+
+public class TestRangeFacetCounts extends FacetTestCase {
+
+  public void testBasicLong() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Document doc = new Document();
+    NumericDocValuesField field = new NumericDocValuesField("field", 0L);
+    doc.add(field);
+    for(long l=0;l<100;l++) {
+      field.setLongValue(l);
+      w.addDocument(doc);
+    }
+
+    // Also add Long.MAX_VALUE
+    field.setLongValue(Long.MAX_VALUE);
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+    w.close();
+
+    FacetsCollector fc = new FacetsCollector();
+    IndexSearcher s = newSearcher(r);
+    s.search(new MatchAllDocsQuery(), fc);
+
+    Facets facets = new LongRangeFacetCounts("field", fc,
+        new LongRange("less than 10", 0L, true, 10L, false),
+        new LongRange("less than or equal to 10", 0L, true, 10L, true),
+        new LongRange("over 90", 90L, false, 100L, false),
+        new LongRange("90 or above", 90L, true, 100L, false),
+        new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, true));
+
+    FacetResult result = facets.getTopChildren(10, "field");
+    assertEquals("dim=field path=[] value=22 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (1)\n",
+                 result.toString());
+    
+    r.close();
+    d.close();
+  }
+
+  public void testUselessRange() {
+    try {
+      new LongRange("useless", 7, true, 6, true);
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    try {
+      new LongRange("useless", 7, true, 7, false);
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    try {
+      new DoubleRange("useless", 7.0, true, 6.0, true);
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    try {
+      new DoubleRange("useless", 7.0, true, 7.0, false);
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+  }
+
+  public void testLongMinMax() throws Exception {
+
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Document doc = new Document();
+    NumericDocValuesField field = new NumericDocValuesField("field", 0L);
+    doc.add(field);
+    field.setLongValue(Long.MIN_VALUE);
+    w.addDocument(doc);
+    field.setLongValue(0);
+    w.addDocument(doc);
+    field.setLongValue(Long.MAX_VALUE);
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+    w.close();
+
+    FacetsCollector fc = new FacetsCollector();
+    IndexSearcher s = newSearcher(r);
+    s.search(new MatchAllDocsQuery(), fc);
+
+    Facets facets = new LongRangeFacetCounts("field", fc,
+        new LongRange("min", Long.MIN_VALUE, true, Long.MIN_VALUE, true),
+        new LongRange("max", Long.MAX_VALUE, true, Long.MAX_VALUE, true),
+        new LongRange("all0", Long.MIN_VALUE, true, Long.MAX_VALUE, true),
+        new LongRange("all1", Long.MIN_VALUE, false, Long.MAX_VALUE, true),
+        new LongRange("all2", Long.MIN_VALUE, true, Long.MAX_VALUE, false),
+        new LongRange("all3", Long.MIN_VALUE, false, Long.MAX_VALUE, false));
+
+    FacetResult result = facets.getTopChildren(10, "field");
+    assertEquals("dim=field path=[] value=3 childCount=6\n  min (1)\n  max (1)\n  all0 (3)\n  all1 (2)\n  all2 (2)\n  all3 (1)\n",
+                 result.toString());
+    
+    r.close();
+    d.close();
+  }
+
+  public void testOverlappedEndStart() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Document doc = new Document();
+    NumericDocValuesField field = new NumericDocValuesField("field", 0L);
+    doc.add(field);
+    for(long l=0;l<100;l++) {
+      field.setLongValue(l);
+      w.addDocument(doc);
+    }
+    field.setLongValue(Long.MAX_VALUE);
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+    w.close();
+
+    FacetsCollector fc = new FacetsCollector();
+    IndexSearcher s = newSearcher(r);
+    s.search(new MatchAllDocsQuery(), fc);
+
+    Facets facets = new LongRangeFacetCounts("field", fc,
+        new LongRange("0-10", 0L, true, 10L, true),
+        new LongRange("10-20", 10L, true, 20L, true),
+        new LongRange("20-30", 20L, true, 30L, true),
+        new LongRange("30-40", 30L, true, 40L, true));
+    
+    FacetResult result = facets.getTopChildren(10, "field");
+    assertEquals("dim=field path=[] value=41 childCount=4\n  0-10 (11)\n  10-20 (11)\n  20-30 (11)\n  30-40 (11)\n",
+                 result.toString());
+    
+    r.close();
+    d.close();
+  }
+
+  /** Tests single request that mixes Range and non-Range
+   *  faceting, with DrillSideways and taxonomy. */
+  public void testMixedRangeAndNonRangeTaxonomy() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Directory td = newDirectory();
+    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(td, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+
+    for (long l = 0; l < 100; l++) {
+      Document doc = new Document();
+      // For computing range facet counts:
+      doc.add(new NumericDocValuesField("field", l));
+      // For drill down by numeric range:
+      doc.add(new LongField("field", l, Field.Store.NO));
+
+      if ((l&3) == 0) {
+        doc.add(new FacetField("dim", "a"));
+      } else {
+        doc.add(new FacetField("dim", "b"));
+      }
+      w.addDocument(config.build(tw, doc));
+    }
+
+    final IndexReader r = w.getReader();
+
+    final TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
+
+    IndexSearcher s = newSearcher(r);
+
+    DrillSideways ds = new DrillSideways(s, config, tr) {
+
+        @Override
+        protected Facets buildFacetsResult(FacetsCollector drillDowns, FacetsCollector[] drillSideways, String[] drillSidewaysDims) throws IOException {        
+          FacetsCollector dimFC = drillDowns;
+          FacetsCollector fieldFC = drillDowns;
+          if (drillSideways != null) {
+            for(int i=0;i<drillSideways.length;i++) {
+              String dim = drillSidewaysDims[i];
+              if (dim.equals("field")) {
+                fieldFC = drillSideways[i];
+              } else {
+                dimFC = drillSideways[i];
+              }
+            }
+          }
+
+          Map<String,Facets> byDim = new HashMap<String,Facets>();
+          byDim.put("field",
+                    new LongRangeFacetCounts("field", fieldFC,
+                          new LongRange("less than 10", 0L, true, 10L, false),
+                          new LongRange("less than or equal to 10", 0L, true, 10L, true),
+                          new LongRange("over 90", 90L, false, 100L, false),
+                          new LongRange("90 or above", 90L, true, 100L, false),
+                          new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, false)));
+          byDim.put("dim", getTaxonomyFacetCounts(taxoReader, config, dimFC));
+          return new MultiFacets(byDim, null);
+        }
+
+        @Override
+        protected boolean scoreSubDocsAtOnce() {
+          return random().nextBoolean();
+        }
+      };
+
+    // First search, no drill downs:
+    DrillDownQuery ddq = new DrillDownQuery(config);
+    DrillSidewaysResult dsr = ds.search(null, ddq, 10);
+
+    assertEquals(100, dsr.hits.totalHits);
+    assertEquals("dim=dim path=[] value=100 childCount=2\n  b (75)\n  a (25)\n", dsr.facets.getTopChildren(10, "dim").toString());
+    assertEquals("dim=field path=[] value=21 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
+                 dsr.facets.getTopChildren(10, "field").toString());
+
+    // Second search, drill down on dim=b:
+    ddq = new DrillDownQuery(config);
+    ddq.add("dim", "b");
+    dsr = ds.search(null, ddq, 10);
+
+    assertEquals(75, dsr.hits.totalHits);
+    assertEquals("dim=dim path=[] value=100 childCount=2\n  b (75)\n  a (25)\n", dsr.facets.getTopChildren(10, "dim").toString());
+    assertEquals("dim=field path=[] value=16 childCount=5\n  less than 10 (7)\n  less than or equal to 10 (8)\n  over 90 (7)\n  90 or above (8)\n  over 1000 (0)\n",
+                 dsr.facets.getTopChildren(10, "field").toString());
+
+    // Third search, drill down on "less than or equal to 10":
+    ddq = new DrillDownQuery(config);
+    ddq.add("field", NumericRangeQuery.newLongRange("field", 0L, 10L, true, true));
+    dsr = ds.search(null, ddq, 10);
+
+    assertEquals(11, dsr.hits.totalHits);
+    assertEquals("dim=dim path=[] value=11 childCount=2\n  b (8)\n  a (3)\n", dsr.facets.getTopChildren(10, "dim").toString());
+    assertEquals("dim=field path=[] value=21 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
+                 dsr.facets.getTopChildren(10, "field").toString());
+    IOUtils.close(tw, tr, td, w, r, d);
+  }
+
+  public void testBasicDouble() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Document doc = new Document();
+    DoubleDocValuesField field = new DoubleDocValuesField("field", 0.0);
+    doc.add(field);
+    for(long l=0;l<100;l++) {
+      field.setDoubleValue(l);
+      w.addDocument(doc);
+    }
+
+    IndexReader r = w.getReader();
+
+    FacetsCollector fc = new FacetsCollector();
+
+    IndexSearcher s = newSearcher(r);
+    s.search(new MatchAllDocsQuery(), fc);
+    Facets facets = new DoubleRangeFacetCounts("field", fc,
+        new DoubleRange("less than 10", 0.0, true, 10.0, false),
+        new DoubleRange("less than or equal to 10", 0.0, true, 10.0, true),
+        new DoubleRange("over 90", 90.0, false, 100.0, false),
+        new DoubleRange("90 or above", 90.0, true, 100.0, false),
+        new DoubleRange("over 1000", 1000.0, false, Double.POSITIVE_INFINITY, false));
+                                         
+    assertEquals("dim=field path=[] value=21 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
+                 facets.getTopChildren(10, "field").toString());
+
+    IOUtils.close(w, r, d);
+  }
+
+  public void testBasicFloat() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Document doc = new Document();
+    FloatDocValuesField field = new FloatDocValuesField("field", 0.0f);
+    doc.add(field);
+    for(long l=0;l<100;l++) {
+      field.setFloatValue(l);
+      w.addDocument(doc);
+    }
+
+    IndexReader r = w.getReader();
+
+    FacetsCollector fc = new FacetsCollector();
+
+    IndexSearcher s = newSearcher(r);
+    s.search(new MatchAllDocsQuery(), fc);
+
+    Facets facets = new DoubleRangeFacetCounts("field", new FloatFieldSource("field"), fc,
+        new DoubleRange("less than 10", 0.0f, true, 10.0f, false),
+        new DoubleRange("less than or equal to 10", 0.0f, true, 10.0f, true),
+        new DoubleRange("over 90", 90.0f, false, 100.0f, false),
+        new DoubleRange("90 or above", 90.0f, true, 100.0f, false),
+        new DoubleRange("over 1000", 1000.0f, false, Double.POSITIVE_INFINITY, false));
+    
+    assertEquals("dim=field path=[] value=21 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
+                 facets.getTopChildren(10, "field").toString());
+    
+    IOUtils.close(w, r, d);
+  }
+
+  public void testRandomLongs() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
+
+    int numDocs = atLeast(1000);
+    if (VERBOSE) {
+      System.out.println("TEST: numDocs=" + numDocs);
+    }
+    long[] values = new long[numDocs];
+    for(int i=0;i<numDocs;i++) {
+      Document doc = new Document();
+      long v = random().nextLong();
+      values[i] = v;
+      doc.add(new NumericDocValuesField("field", v));
+      doc.add(new LongField("field", v, Field.Store.NO));
+      w.addDocument(doc);
+    }
+    IndexReader r = w.getReader();
+
+    IndexSearcher s = newSearcher(r);
+    FacetsConfig config = new FacetsConfig();
+    
+    int numIters = atLeast(10);
+    for(int iter=0;iter<numIters;iter++) {
+      if (VERBOSE) {
+        System.out.println("TEST: iter=" + iter);
+      }
+      int numRange = _TestUtil.nextInt(random(), 1, 100);
+      LongRange[] ranges = new LongRange[numRange];
+      int[] expectedCounts = new int[numRange];
+      for(int rangeID=0;rangeID<numRange;rangeID++) {
+        long min;
+        if (rangeID > 0 && random().nextInt(10) == 7) {
+          // Use an existing boundary:
+          LongRange prevRange = ranges[random().nextInt(rangeID)];
+          if (random().nextBoolean()) {
+            min = prevRange.min;
+          } else {
+            min = prevRange.max;
+          }
+        } else {
+          min = random().nextLong();
+        }
+        long max;
+        if (rangeID > 0 && random().nextInt(10) == 7) {
+          // Use an existing boundary:
+          LongRange prevRange = ranges[random().nextInt(rangeID)];
+          if (random().nextBoolean()) {
+            max = prevRange.min;
+          } else {
+            max = prevRange.max;
+          }
+        } else {
+          max = random().nextLong();
+        }
+
+        if (min > max) {
+          long x = min;
+          min = max;
+          max = x;
+        }
+        boolean minIncl;
+        boolean maxIncl;
+        if (min == max) {
+          minIncl = true;
+          maxIncl = true;
+        } else {
+          minIncl = random().nextBoolean();
+          maxIncl = random().nextBoolean();
+        }
+        ranges[rangeID] = new LongRange("r" + rangeID, min, minIncl, max, maxIncl);
+        if (VERBOSE) {
+          System.out.println("  range " + rangeID + ": " + ranges[rangeID]);      
+        }
+
+        // Do "slow but hopefully correct" computation of
+        // expected count:
+        for(int i=0;i<numDocs;i++) {
+          boolean accept = true;
+          if (minIncl) {
+            accept &= values[i] >= min;
+          } else {
+            accept &= values[i] > min;
+          }
+          if (maxIncl) {
+            accept &= values[i] <= max;
+          } else {
+            accept &= values[i] < max;
+          }
+          if (accept) {
+            expectedCounts[rangeID]++;
+          }
+        }
+      }
+
+      FacetsCollector sfc = new FacetsCollector();
+      s.search(new MatchAllDocsQuery(), sfc);
+      Facets facets = new LongRangeFacetCounts("field", sfc, ranges);
+      FacetResult result = facets.getTopChildren(10, "field");
+      assertEquals(numRange, result.labelValues.length);
+      for(int rangeID=0;rangeID<numRange;rangeID++) {
+        if (VERBOSE) {
+          System.out.println("  range " + rangeID + " expectedCount=" + expectedCounts[rangeID]);
+        }
+        LabelAndValue subNode = result.labelValues[rangeID];
+        assertEquals("r" + rangeID, subNode.label);
+        assertEquals(expectedCounts[rangeID], subNode.value.intValue());
+
+        LongRange range = ranges[rangeID];
+
+        // Test drill-down:
+        DrillDownQuery ddq = new DrillDownQuery(config);
+        ddq.add("field", NumericRangeQuery.newLongRange("field", range.min, range.max, range.minInclusive, range.maxInclusive));
+        assertEquals(expectedCounts[rangeID], s.search(ddq, 10).totalHits);
+      }
+    }
+
+    IOUtils.close(w, r, dir);
+  }
+
+  public void testRandomFloats() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
+
+    int numDocs = atLeast(1000);
+    float[] values = new float[numDocs];
+    for(int i=0;i<numDocs;i++) {
+      Document doc = new Document();
+      float v = random().nextFloat();
+      values[i] = v;
+      doc.add(new FloatDocValuesField("field", v));
+      doc.add(new FloatField("field", v, Field.Store.NO));
+      w.addDocument(doc);
+    }
+    IndexReader r = w.getReader();
+
+    IndexSearcher s = newSearcher(r);
+    FacetsConfig config = new FacetsConfig();
+    
+    int numIters = atLeast(10);
+    for(int iter=0;iter<numIters;iter++) {
+      if (VERBOSE) {
+        System.out.println("TEST: iter=" + iter);
+      }
+      int numRange = _TestUtil.nextInt(random(), 1, 5);
+      DoubleRange[] ranges = new DoubleRange[numRange];
+      int[] expectedCounts = new int[numRange];
+      for(int rangeID=0;rangeID<numRange;rangeID++) {
+        double min;
+        if (rangeID > 0 && random().nextInt(10) == 7) {
+          // Use an existing boundary:
+          DoubleRange prevRange = ranges[random().nextInt(rangeID)];
+          if (random().nextBoolean()) {
+            min = prevRange.min;
+          } else {
+            min = prevRange.max;
+          }
+        } else {
+          min = random().nextDouble();
+        }
+        double max;
+        if (rangeID > 0 && random().nextInt(10) == 7) {
+          // Use an existing boundary:
+          DoubleRange prevRange = ranges[random().nextInt(rangeID)];
+          if (random().nextBoolean()) {
+            max = prevRange.min;
+          } else {
+            max = prevRange.max;
+          }
+        } else {
+          max = random().nextDouble();
+        }
+
+        if (min > max) {
+          double x = min;
+          min = max;
+          max = x;
+        }
+
+        boolean minIncl;
+        boolean maxIncl;
+        if (min == max) {
+          minIncl = true;
+          maxIncl = true;
+        } else {
+          minIncl = random().nextBoolean();
+          maxIncl = random().nextBoolean();
+        }
+        ranges[rangeID] = new DoubleRange("r" + rangeID, min, minIncl, max, maxIncl);
+
+        // Do "slow but hopefully correct" computation of
+        // expected count:
+        for(int i=0;i<numDocs;i++) {
+          boolean accept = true;
+          if (minIncl) {
+            accept &= values[i] >= min;
+          } else {
+            accept &= values[i] > min;
+          }
+          if (maxIncl) {
+            accept &= values[i] <= max;
+          } else {
+            accept &= values[i] < max;
+          }
+          if (accept) {
+            expectedCounts[rangeID]++;
+          }
+        }
+      }
+
+      FacetsCollector sfc = new FacetsCollector();
+      s.search(new MatchAllDocsQuery(), sfc);
+      Facets facets = new DoubleRangeFacetCounts("field", new FloatFieldSource("field"), sfc, ranges);
+      FacetResult result = facets.getTopChildren(10, "field");
+      assertEquals(numRange, result.labelValues.length);
+      for(int rangeID=0;rangeID<numRange;rangeID++) {
+        if (VERBOSE) {
+          System.out.println("  range " + rangeID + " expectedCount=" + expectedCounts[rangeID]);
+        }
+        LabelAndValue subNode = result.labelValues[rangeID];
+        assertEquals("r" + rangeID, subNode.label);
+        assertEquals(expectedCounts[rangeID], subNode.value.intValue());
+
+        DoubleRange range = ranges[rangeID];
+
+        // Test drill-down:
+        DrillDownQuery ddq = new DrillDownQuery(config);
+        ddq.add("field", NumericRangeQuery.newFloatRange("field", (float) range.min, (float) range.max, range.minInclusive, range.maxInclusive));
+        assertEquals(expectedCounts[rangeID], s.search(ddq, 10).totalHits);
+      }
+    }
+
+    IOUtils.close(w, r, dir);
+  }
+
+  public void testRandomDoubles() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
+
+    int numDocs = atLeast(1000);
+    double[] values = new double[numDocs];
+    for(int i=0;i<numDocs;i++) {
+      Document doc = new Document();
+      double v = random().nextDouble();
+      values[i] = v;
+      doc.add(new DoubleDocValuesField("field", v));
+      doc.add(new DoubleField("field", v, Field.Store.NO));
+      w.addDocument(doc);
+    }
+    IndexReader r = w.getReader();
+
+    IndexSearcher s = newSearcher(r);
+    FacetsConfig config = new FacetsConfig();
+    
+    int numIters = atLeast(10);
+    for(int iter=0;iter<numIters;iter++) {
+      if (VERBOSE) {
+        System.out.println("TEST: iter=" + iter);
+      }
+      int numRange = _TestUtil.nextInt(random(), 1, 5);
+      DoubleRange[] ranges = new DoubleRange[numRange];
+      int[] expectedCounts = new int[numRange];
+      for(int rangeID=0;rangeID<numRange;rangeID++) {
+        double min;
+        if (rangeID > 0 && random().nextInt(10) == 7) {
+          // Use an existing boundary:
+          DoubleRange prevRange = ranges[random().nextInt(rangeID)];
+          if (random().nextBoolean()) {
+            min = prevRange.min;
+          } else {
+            min = prevRange.max;
+          }
+        } else {
+          min = random().nextDouble();
+        }
+        double max;
+        if (rangeID > 0 && random().nextInt(10) == 7) {
+          // Use an existing boundary:
+          DoubleRange prevRange = ranges[random().nextInt(rangeID)];
+          if (random().nextBoolean()) {
+            max = prevRange.min;
+          } else {
+            max = prevRange.max;
+          }
+        } else {
+          max = random().nextDouble();
+        }
+
+        if (min > max) {
+          double x = min;
+          min = max;
+          max = x;
+        }
+
+        boolean minIncl;
+        boolean maxIncl;
+        if (min == max) {
+          minIncl = true;
+          maxIncl = true;
+        } else {
+          minIncl = random().nextBoolean();
+          maxIncl = random().nextBoolean();
+        }
+        ranges[rangeID] = new DoubleRange("r" + rangeID, min, minIncl, max, maxIncl);
+
+        // Do "slow but hopefully correct" computation of
+        // expected count:
+        for(int i=0;i<numDocs;i++) {
+          boolean accept = true;
+          if (minIncl) {
+            accept &= values[i] >= min;
+          } else {
+            accept &= values[i] > min;
+          }
+          if (maxIncl) {
+            accept &= values[i] <= max;
+          } else {
+            accept &= values[i] < max;
+          }
+          if (accept) {
+            expectedCounts[rangeID]++;
+          }
+        }
+      }
+
+      FacetsCollector sfc = new FacetsCollector();
+      s.search(new MatchAllDocsQuery(), sfc);
+      Facets facets = new DoubleRangeFacetCounts("field", sfc, ranges);
+      FacetResult result = facets.getTopChildren(10, "field");
+      assertEquals(numRange, result.labelValues.length);
+      for(int rangeID=0;rangeID<numRange;rangeID++) {
+        if (VERBOSE) {
+          System.out.println("  range " + rangeID + " expectedCount=" + expectedCounts[rangeID]);
+        }
+        LabelAndValue subNode = result.labelValues[rangeID];
+        assertEquals("r" + rangeID, subNode.label);
+        assertEquals(expectedCounts[rangeID], subNode.value.intValue());
+
+        DoubleRange range = ranges[rangeID];
+
+        // Test drill-down:
+        DrillDownQuery ddq = new DrillDownQuery(config);
+        ddq.add("field", NumericRangeQuery.newDoubleRange("field", range.min, range.max, range.minInclusive, range.maxInclusive));
+        assertEquals(expectedCounts[rangeID], s.search(ddq, 10).totalHits);
+      }
+    }
+
+    IOUtils.close(w, r, dir);
+  }
+
+  // LUCENE-5178
+  public void testMissingValues() throws Exception {
+    assumeTrue("codec does not support docsWithField", defaultCodecSupportsDocsWithField());
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Document doc = new Document();
+    NumericDocValuesField field = new NumericDocValuesField("field", 0L);
+    doc.add(field);
+    for(long l=0;l<100;l++) {
+      if (l % 5 == 0) {
+        // Every 5th doc is missing the value:
+        w.addDocument(new Document());
+        continue;
+      }
+      field.setLongValue(l);
+      w.addDocument(doc);
+    }
+
+    IndexReader r = w.getReader();
+
+    FacetsCollector fc = new FacetsCollector();
+
+    IndexSearcher s = newSearcher(r);
+    s.search(new MatchAllDocsQuery(), fc);
+    Facets facets = new LongRangeFacetCounts("field", fc,
+        new LongRange("less than 10", 0L, true, 10L, false),
+        new LongRange("less than or equal to 10", 0L, true, 10L, true),
+        new LongRange("over 90", 90L, false, 100L, false),
+        new LongRange("90 or above", 90L, true, 100L, false),
+        new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, false));
+    
+    assertEquals("dim=field path=[] value=16 childCount=5\n  less than 10 (8)\n  less than or equal to 10 (8)\n  over 90 (8)\n  90 or above (8)\n  over 1000 (0)\n",
+                 facets.getTopChildren(10, "field").toString());
+
+    IOUtils.close(w, r, d);
+  }
+
+  public void testCustomDoublesValueSource() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    
+    Document doc = new Document();
+    writer.addDocument(doc);
+    
+    doc = new Document();
+    writer.addDocument(doc);
+    
+    doc = new Document();
+    writer.addDocument(doc);
+
+    writer.forceMerge(1);
+
+    ValueSource vs = new ValueSource() {
+        @Override
+        public FunctionValues getValues(Map ignored, AtomicReaderContext ignored2) {
+          return new DoubleDocValues(null) {
+            public double doubleVal(int doc) {
+              return doc+1;
+            }
+          };
+        }
+
+        @Override
+        public boolean equals(Object o) {
+          throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public int hashCode() {
+          throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public String description() {
+          throw new UnsupportedOperationException();
+        }
+      };
+    
+    FacetsCollector fc = new FacetsCollector();
+
+    IndexReader r = writer.getReader();
+    IndexSearcher s = newSearcher(r);
+    s.search(new MatchAllDocsQuery(), fc);
+
+    Facets facets = new DoubleRangeFacetCounts("field", vs, fc,
+        new DoubleRange("< 1", 0.0, true, 1.0, false),
+        new DoubleRange("< 2", 0.0, true, 2.0, false),
+        new DoubleRange("< 5", 0.0, true, 5.0, false),
+        new DoubleRange("< 10", 0.0, true, 10.0, false),
+        new DoubleRange("< 20", 0.0, true, 20.0, false),
+        new DoubleRange("< 50", 0.0, true, 50.0, false));
+
+    assertEquals("dim=field path=[] value=3 childCount=6\n  < 1 (0)\n  < 2 (1)\n  < 5 (3)\n  < 10 (3)\n  < 20 (3)\n  < 50 (3)\n", facets.getTopChildren(10, "field").toString());
+
+    // Test drill-down:
+    assertEquals(1, s.search(new ConstantScoreQuery(new DoubleRange("< 2", 0.0, true, 2.0, false).getFilter(vs)), 10).totalHits);
+
+    IOUtils.close(r, writer, dir);
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/TestSearcherTaxonomyManager.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestSearcherTaxonomyManager.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/TestSearcherTaxonomyManager.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestSearcherTaxonomyManager.java	2013-11-27 18:18:21.967998037 -0500
@@ -0,0 +1,183 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.SearcherTaxonomyManager.SearcherAndTaxonomy;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+
+public class TestSearcherTaxonomyManager extends FacetTestCase {
+  public void test() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    final IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    final DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
+    final FacetsConfig config = new FacetsConfig();
+    config.setMultiValued("field", true);
+    final AtomicBoolean stop = new AtomicBoolean();
+
+    // How many unique facets to index before stopping:
+    final int ordLimit = TEST_NIGHTLY ? 100000 : 6000;
+
+    Thread indexer = new Thread() {
+        @Override
+        public void run() {
+          try {
+            Set<String> seen = new HashSet<String>();
+            List<String> paths = new ArrayList<String>();
+            while (true) {
+              Document doc = new Document();
+              int numPaths = _TestUtil.nextInt(random(), 1, 5);
+              for(int i=0;i<numPaths;i++) {
+                String path;
+                if (!paths.isEmpty() && random().nextInt(5) != 4) {
+                  // Use previous path
+                  path = paths.get(random().nextInt(paths.size()));
+                } else {
+                  // Create new path
+                  path = null;
+                  while (true) {
+                    path = _TestUtil.randomRealisticUnicodeString(random());
+                    if (path.length() != 0 && !seen.contains(path)) {
+                      seen.add(path);
+                      paths.add(path);
+                      break;
+                    }
+                  }
+                }
+                doc.add(new FacetField("field", path));
+              }
+              try {
+                w.addDocument(config.build(tw, doc));
+              } catch (IOException ioe) {
+                throw new RuntimeException(ioe);
+              }
+
+              if (tw.getSize() >= ordLimit) {
+                break;
+              }
+            }
+          } finally {
+            stop.set(true);
+          }
+        }
+      };
+
+    final SearcherTaxonomyManager mgr = new SearcherTaxonomyManager(w, true, null, tw);
+
+    Thread reopener = new Thread() {
+        @Override
+        public void run() {
+          while(!stop.get()) {
+            try {
+              // Sleep for up to 20 msec:
+              Thread.sleep(random().nextInt(20));
+
+              if (VERBOSE) {
+                System.out.println("TEST: reopen");
+              }
+
+              mgr.maybeRefresh();
+
+              if (VERBOSE) {
+                System.out.println("TEST: reopen done");
+              }
+            } catch (Exception ioe) {
+              throw new RuntimeException(ioe);
+            }
+          }
+        }
+      };
+    reopener.start();
+
+    indexer.start();
+
+    try {
+      while (!stop.get()) {
+        SearcherAndTaxonomy pair = mgr.acquire();
+        try {
+          //System.out.println("search maxOrd=" + pair.taxonomyReader.getSize());
+          int topN = _TestUtil.nextInt(random(), 1, 20);
+          
+          FacetsCollector sfc = new FacetsCollector();
+          pair.searcher.search(new MatchAllDocsQuery(), sfc);
+          Facets facets = getTaxonomyFacetCounts(pair.taxonomyReader, config, sfc);
+          FacetResult result = facets.getTopChildren(10, "field");
+          if (pair.searcher.getIndexReader().numDocs() > 0) { 
+            //System.out.println(pair.taxonomyReader.getSize());
+            assertTrue(result.childCount > 0);
+            assertTrue(result.labelValues.length > 0);
+          }
+
+          //if (VERBOSE) {
+          //System.out.println("TEST: facets=" + FacetTestUtils.toString(results.get(0)));
+          //}
+        } finally {
+          mgr.release(pair);
+        }
+      }
+    } finally {
+      indexer.join();
+      reopener.join();
+    }
+
+    if (VERBOSE) {
+      System.out.println("TEST: now stop");
+    }
+
+    IOUtils.close(mgr, tw, w, taxoDir, dir);
+  }
+
+  public void testReplaceTaxonomy() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
+
+    Directory taxoDir2 = newDirectory();
+    DirectoryTaxonomyWriter tw2 = new DirectoryTaxonomyWriter(taxoDir2);
+    tw2.close();
+
+    SearcherTaxonomyManager mgr = new SearcherTaxonomyManager(w, true, null, tw);
+    w.addDocument(new Document());
+    tw.replaceTaxonomy(taxoDir2);
+    taxoDir2.close();
+
+    try {
+      mgr.maybeRefresh();
+      fail("should have hit exception");
+    } catch (IllegalStateException ise) {
+      // expected
+    }
+
+    IOUtils.close(mgr, tw, w, taxoDir, dir);
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/TestSortedSetDocValuesFacets.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestSortedSetDocValuesFacets.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/TestSortedSetDocValuesFacets.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestSortedSetDocValuesFacets.java	2013-12-02 15:49:44.662940789 -0500
@@ -0,0 +1,350 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+
+public class TestSortedSetDocValuesFacets extends FacetTestCase {
+
+  // NOTE: TestDrillSideways.testRandom also sometimes
+  // randomly uses SortedSetDV
+
+  public void testBasic() throws Exception {
+    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
+    Directory dir = newDirectory();
+
+    FacetsConfig config = new FacetsConfig();
+    config.setMultiValued("a", true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo"));
+    doc.add(new SortedSetDocValuesFacetField("a", "bar"));
+    doc.add(new SortedSetDocValuesFacetField("a", "zoo"));
+    doc.add(new SortedSetDocValuesFacetField("b", "baz"));
+    writer.addDocument(config.build(doc));
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // Per-top-reader state:
+    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
+    
+    FacetsCollector c = new FacetsCollector();
+
+    searcher.search(new MatchAllDocsQuery(), c);
+
+    SortedSetDocValuesFacetCounts facets = new SortedSetDocValuesFacetCounts(state, c);
+
+    assertEquals("dim=a path=[] value=4 childCount=3\n  foo (2)\n  bar (1)\n  zoo (1)\n", facets.getTopChildren(10, "a").toString());
+    assertEquals("dim=b path=[] value=1 childCount=1\n  baz (1)\n", facets.getTopChildren(10, "b").toString());
+
+    // DrillDown:
+    DrillDownQuery q = new DrillDownQuery(config);
+    q.add("a", "foo");
+    q.add("b", "baz");
+    TopDocs hits = searcher.search(q, 1);
+    assertEquals(1, hits.totalHits);
+
+    IOUtils.close(writer, searcher.getIndexReader(), dir);
+  }
+
+  // LUCENE-5090
+  public void testStaleState() throws Exception {
+    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
+    Directory dir = newDirectory();
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo"));
+    writer.addDocument(config.build(doc));
+
+    IndexReader r = writer.getReader();
+    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(r);
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "bar"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "baz"));
+    writer.addDocument(config.build(doc));
+
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    FacetsCollector c = new FacetsCollector();
+
+    searcher.search(new MatchAllDocsQuery(), c);
+
+    try {
+      new SortedSetDocValuesFacetCounts(state, c);
+      fail("did not hit expected exception");
+    } catch (IllegalStateException ise) {
+      // expected
+    }
+
+    r.close();
+    writer.close();
+    searcher.getIndexReader().close();
+    dir.close();
+  }
+
+  // LUCENE-5333
+  public void testSparseFacets() throws Exception {
+    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
+    Directory dir = newDirectory();
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo1"));
+    writer.addDocument(config.build(doc));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo2"));
+    doc.add(new SortedSetDocValuesFacetField("b", "bar1"));
+    writer.addDocument(config.build(doc));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo3"));
+    doc.add(new SortedSetDocValuesFacetField("b", "bar2"));
+    doc.add(new SortedSetDocValuesFacetField("c", "baz1"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+
+    // Per-top-reader state:
+    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+    SortedSetDocValuesFacetCounts facets = new SortedSetDocValuesFacetCounts(state, c);
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<FacetResult> results = facets.getAllDims(10);
+
+    assertEquals(3, results.size());
+    assertEquals("dim=a path=[] value=3 childCount=3\n  foo1 (1)\n  foo2 (1)\n  foo3 (1)\n", results.get(0).toString());
+    assertEquals("dim=b path=[] value=2 childCount=2\n  bar1 (1)\n  bar2 (1)\n", results.get(1).toString());
+    assertEquals("dim=c path=[] value=1 childCount=1\n  baz1 (1)\n", results.get(2).toString());
+
+    searcher.getIndexReader().close();
+    dir.close();
+  }
+
+  public void testSomeSegmentsMissing() throws Exception {
+    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
+    Directory dir = newDirectory();
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo1"));
+    writer.addDocument(config.build(doc));
+    writer.commit();
+
+    doc = new Document();
+    writer.addDocument(config.build(doc));
+    writer.commit();
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo2"));
+    writer.addDocument(config.build(doc));
+    writer.commit();
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+
+    // Per-top-reader state:
+    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+    SortedSetDocValuesFacetCounts facets = new SortedSetDocValuesFacetCounts(state, c);
+
+    // Ask for top 10 labels for any dims that have counts:
+    assertEquals("dim=a path=[] value=2 childCount=2\n  foo1 (1)\n  foo2 (1)\n", facets.getTopChildren(10, "a").toString());
+
+    searcher.getIndexReader().close();
+    dir.close();
+  }
+
+  public void testSlowCompositeReaderWrapper() throws Exception {
+    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
+    Directory dir = newDirectory();
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo1"));
+    writer.addDocument(config.build(doc));
+
+    writer.commit();
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo2"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = new IndexSearcher(SlowCompositeReaderWrapper.wrap(writer.getReader()));
+
+    // Per-top-reader state:
+    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+    Facets facets = new SortedSetDocValuesFacetCounts(state, c);
+
+    // Ask for top 10 labels for any dims that have counts:
+    assertEquals("dim=a path=[] value=2 childCount=2\n  foo1 (1)\n  foo2 (1)\n", facets.getTopChildren(10, "a").toString());
+
+    IOUtils.close(writer, searcher.getIndexReader(), dir);
+  }
+
+
+  public void testRandom() throws Exception {
+    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
+    String[] tokens = getRandomTokens(10);
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    RandomIndexWriter w = new RandomIndexWriter(random(), indexDir);
+    FacetsConfig config = new FacetsConfig();
+    int numDocs = atLeast(1000);
+    int numDims = _TestUtil.nextInt(random(), 1, 7);
+    List<TestDoc> testDocs = getRandomDocs(tokens, numDocs, numDims);
+    for(TestDoc testDoc : testDocs) {
+      Document doc = new Document();
+      doc.add(newStringField("content", testDoc.content, Field.Store.NO));
+      for(int j=0;j<numDims;j++) {
+        if (testDoc.dims[j] != null) {
+          doc.add(new SortedSetDocValuesFacetField("dim" + j, testDoc.dims[j]));
+        }
+      }
+      w.addDocument(config.build(doc));
+    }
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(w.getReader());
+    
+    // Per-top-reader state:
+    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
+
+    int iters = atLeast(100);
+    for(int iter=0;iter<iters;iter++) {
+      String searchToken = tokens[random().nextInt(tokens.length)];
+      if (VERBOSE) {
+        System.out.println("\nTEST: iter content=" + searchToken);
+      }
+      FacetsCollector fc = new FacetsCollector();
+      TopDocs hits = FacetsCollector.search(searcher, new TermQuery(new Term("content", searchToken)), 10, fc);
+      Facets facets = new SortedSetDocValuesFacetCounts(state, fc);
+
+      // Slow, yet hopefully bug-free, faceting:
+      @SuppressWarnings({"rawtypes","unchecked"}) Map<String,Integer>[] expectedCounts = new HashMap[numDims];
+      for(int i=0;i<numDims;i++) {
+        expectedCounts[i] = new HashMap<String,Integer>();
+      }
+
+      for(TestDoc doc : testDocs) {
+        if (doc.content.equals(searchToken)) {
+          for(int j=0;j<numDims;j++) {
+            if (doc.dims[j] != null) {
+              Integer v = expectedCounts[j].get(doc.dims[j]);
+              if (v == null) {
+                expectedCounts[j].put(doc.dims[j], 1);
+              } else {
+                expectedCounts[j].put(doc.dims[j], v.intValue() + 1);
+              }
+            }
+          }
+        }
+      }
+
+      List<FacetResult> expected = new ArrayList<FacetResult>();
+      for(int i=0;i<numDims;i++) {
+        List<LabelAndValue> labelValues = new ArrayList<LabelAndValue>();
+        int totCount = 0;
+        for(Map.Entry<String,Integer> ent : expectedCounts[i].entrySet()) {
+          labelValues.add(new LabelAndValue(ent.getKey(), ent.getValue()));
+          totCount += ent.getValue();
+        }
+        sortLabelValues(labelValues);
+        if (totCount > 0) {
+          expected.add(new FacetResult("dim" + i, new String[0], totCount, labelValues.toArray(new LabelAndValue[labelValues.size()]), labelValues.size()));
+        }
+      }
+
+      // Sort by highest value, tie break by value:
+      sortFacetResults(expected);
+
+      List<FacetResult> actual = facets.getAllDims(10);
+
+      // Messy: fixup ties
+      //sortTies(actual);
+
+      assertEquals(expected, actual);
+    }
+
+    IOUtils.close(w, searcher.getIndexReader(), indexDir, taxoDir);
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetAssociations.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetAssociations.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetAssociations.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetAssociations.java	2013-12-02 07:13:15.351769787 -0500
@@ -0,0 +1,218 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+
+/** Test for associations */
+public class TestTaxonomyFacetAssociations extends FacetTestCase {
+  
+  private static Directory dir;
+  private static IndexReader reader;
+  private static Directory taxoDir;
+  private static TaxonomyReader taxoReader;
+
+  private static FacetsConfig config;
+
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    dir = newDirectory();
+    taxoDir = newDirectory();
+    // preparations - index, taxonomy, content
+    
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+
+    // Cannot mix ints & floats in the same indexed field:
+    config = new FacetsConfig();
+    config.setIndexFieldName("int", "$facets.int");
+    config.setMultiValued("int", true);
+    config.setIndexFieldName("float", "$facets.float");
+    config.setMultiValued("float", true);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    // index documents, 50% have only 'b' and all have 'a'
+    for (int i = 0; i < 110; i++) {
+      Document doc = new Document();
+      // every 11th document is added empty, this used to cause the association
+      // aggregators to go into an infinite loop
+      if (i % 11 != 0) {
+        doc.add(new IntAssociationFacetField(2, "int", "a"));
+        doc.add(new FloatAssociationFacetField(0.5f, "float", "a"));
+        if (i % 2 == 0) { // 50
+          doc.add(new IntAssociationFacetField(3, "int", "b"));
+          doc.add(new FloatAssociationFacetField(0.2f, "float", "b"));
+        }
+      }
+      writer.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    taxoWriter.close();
+    reader = writer.getReader();
+    writer.close();
+    taxoReader = new DirectoryTaxonomyReader(taxoDir);
+  }
+  
+  @AfterClass
+  public static void afterClass() throws Exception {
+    reader.close();
+    reader = null;
+    dir.close();
+    dir = null;
+    taxoReader.close();
+    taxoReader = null;
+    taxoDir.close();
+    taxoDir = null;
+  }
+  
+  public void testIntSumAssociation() throws Exception {
+    
+    FacetsCollector fc = new FacetsCollector();
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(new MatchAllDocsQuery(), fc);
+
+    Facets facets = new TaxonomyFacetSumIntAssociations("$facets.int", taxoReader, config, fc);
+    assertEquals("dim=int path=[] value=-1 childCount=2\n  a (200)\n  b (150)\n", facets.getTopChildren(10, "int").toString());
+    assertEquals("Wrong count for category 'a'!", 200, facets.getSpecificValue("int", "a").intValue());
+    assertEquals("Wrong count for category 'b'!", 150, facets.getSpecificValue("int", "b").intValue());
+  }
+
+  public void testFloatSumAssociation() throws Exception {
+    FacetsCollector fc = new FacetsCollector();
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(new MatchAllDocsQuery(), fc);
+    
+    Facets facets = new TaxonomyFacetSumFloatAssociations("$facets.float", taxoReader, config, fc);
+    assertEquals("dim=float path=[] value=-1.0 childCount=2\n  a (50.0)\n  b (9.999995)\n", facets.getTopChildren(10, "float").toString());
+    assertEquals("Wrong count for category 'a'!", 50f, facets.getSpecificValue("float", "a").floatValue(), 0.00001);
+    assertEquals("Wrong count for category 'b'!", 10f, facets.getSpecificValue("float", "b").floatValue(), 0.00001);
+  }  
+
+  /** Make sure we can test both int and float assocs in one
+   *  index, as long as we send each to a different field. */
+  public void testIntAndFloatAssocation() throws Exception {
+    FacetsCollector fc = new FacetsCollector();
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(new MatchAllDocsQuery(), fc);
+    
+    Facets facets = new TaxonomyFacetSumFloatAssociations("$facets.float", taxoReader, config, fc);
+    assertEquals("Wrong count for category 'a'!", 50f, facets.getSpecificValue("float", "a").floatValue(), 0.00001);
+    assertEquals("Wrong count for category 'b'!", 10f, facets.getSpecificValue("float", "b").floatValue(), 0.00001);
+    
+    facets = new TaxonomyFacetSumIntAssociations("$facets.int", taxoReader, config, fc);
+    assertEquals("Wrong count for category 'a'!", 200, facets.getSpecificValue("int", "a").intValue());
+    assertEquals("Wrong count for category 'b'!", 150, facets.getSpecificValue("int", "b").intValue());
+  }
+
+  public void testWrongIndexFieldName() throws Exception {
+    FacetsCollector fc = new FacetsCollector();
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(new MatchAllDocsQuery(), fc);
+    Facets facets = new TaxonomyFacetSumFloatAssociations(taxoReader, config, fc);
+    try {
+      facets.getSpecificValue("float");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    try {
+      facets.getTopChildren(10, "float");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+  }
+
+  public void testMixedTypesInSameIndexField() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    FacetsConfig config = new FacetsConfig();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new IntAssociationFacetField(14, "a", "x"));
+    doc.add(new FloatAssociationFacetField(55.0f, "b", "y"));
+    try {
+      writer.addDocument(config.build(taxoWriter, doc));
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException exc) {
+      // expected
+    }
+    IOUtils.close(writer, taxoWriter, dir, taxoDir);
+  }
+
+  public void testNoHierarchy() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    FacetsConfig config = new FacetsConfig();
+    config.setHierarchical("a", true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new IntAssociationFacetField(14, "a", "x"));
+    try {
+      writer.addDocument(config.build(taxoWriter, doc));
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException exc) {
+      // expected
+    }
+    IOUtils.close(writer, taxoWriter, dir, taxoDir);
+  }
+
+  public void testRequireDimCount() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    FacetsConfig config = new FacetsConfig();
+    config.setRequireDimCount("a", true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new IntAssociationFacetField(14, "a", "x"));
+    try {
+      writer.addDocument(config.build(taxoWriter, doc));
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException exc) {
+      // expected
+    }
+    IOUtils.close(writer, taxoWriter, dir, taxoDir);
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts2.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts2.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts2.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts2.java	2013-11-27 18:23:58.243989061 -0500
@@ -0,0 +1,366 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.NoMergePolicy;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class TestTaxonomyFacetCounts2 extends FacetTestCase {
+  
+  private static final Term A = new Term("f", "a");
+  private static final String CP_A = "A", CP_B = "B";
+  private static final String CP_C = "C", CP_D = "D"; // indexed w/ NO_PARENTS
+  private static final int NUM_CHILDREN_CP_A = 5, NUM_CHILDREN_CP_B = 3;
+  private static final int NUM_CHILDREN_CP_C = 5, NUM_CHILDREN_CP_D = 5;
+  private static final FacetField[] CATEGORIES_A, CATEGORIES_B;
+  private static final FacetField[] CATEGORIES_C, CATEGORIES_D;
+  static {
+    CATEGORIES_A = new FacetField[NUM_CHILDREN_CP_A];
+    for (int i = 0; i < NUM_CHILDREN_CP_A; i++) {
+      CATEGORIES_A[i] = new FacetField(CP_A, Integer.toString(i));
+    }
+    CATEGORIES_B = new FacetField[NUM_CHILDREN_CP_B];
+    for (int i = 0; i < NUM_CHILDREN_CP_B; i++) {
+      CATEGORIES_B[i] = new FacetField(CP_B, Integer.toString(i));
+    }
+    
+    // NO_PARENTS categories
+    CATEGORIES_C = new FacetField[NUM_CHILDREN_CP_C];
+    for (int i = 0; i < NUM_CHILDREN_CP_C; i++) {
+      CATEGORIES_C[i] = new FacetField(CP_C, Integer.toString(i));
+    }
+    
+    // Multi-level categories
+    CATEGORIES_D = new FacetField[NUM_CHILDREN_CP_D];
+    for (int i = 0; i < NUM_CHILDREN_CP_D; i++) {
+      String val = Integer.toString(i);
+      CATEGORIES_D[i] = new FacetField(CP_D, val, val + val); // e.g. D/1/11, D/2/22...
+    }
+  }
+  
+  private static Directory indexDir, taxoDir;
+  private static Map<String,Integer> allExpectedCounts, termExpectedCounts;
+
+  @AfterClass
+  public static void afterClassCountingFacetsAggregatorTest() throws Exception {
+    IOUtils.close(indexDir, taxoDir); 
+  }
+  
+  private static List<FacetField> randomCategories(Random random) {
+    // add random categories from the two dimensions, ensuring that the same
+    // category is not added twice.
+    int numFacetsA = random.nextInt(3) + 1; // 1-3
+    int numFacetsB = random.nextInt(2) + 1; // 1-2
+    ArrayList<FacetField> categories_a = new ArrayList<FacetField>();
+    categories_a.addAll(Arrays.asList(CATEGORIES_A));
+    ArrayList<FacetField> categories_b = new ArrayList<FacetField>();
+    categories_b.addAll(Arrays.asList(CATEGORIES_B));
+    Collections.shuffle(categories_a, random);
+    Collections.shuffle(categories_b, random);
+
+    ArrayList<FacetField> categories = new ArrayList<FacetField>();
+    categories.addAll(categories_a.subList(0, numFacetsA));
+    categories.addAll(categories_b.subList(0, numFacetsB));
+    
+    // add the NO_PARENT categories
+    categories.add(CATEGORIES_C[random().nextInt(NUM_CHILDREN_CP_C)]);
+    categories.add(CATEGORIES_D[random().nextInt(NUM_CHILDREN_CP_D)]);
+
+    return categories;
+  }
+
+  private static void addField(Document doc) {
+    doc.add(new StringField(A.field(), A.text(), Store.NO));
+  }
+
+  private static void addFacets(Document doc, FacetsConfig config, boolean updateTermExpectedCounts) 
+      throws IOException {
+    List<FacetField> docCategories = randomCategories(random());
+    for (FacetField ff : docCategories) {
+      doc.add(ff);
+      String cp = ff.dim + "/" + ff.path[0];
+      allExpectedCounts.put(cp, allExpectedCounts.get(cp) + 1);
+      if (updateTermExpectedCounts) {
+        termExpectedCounts.put(cp, termExpectedCounts.get(cp) + 1);
+      }
+    }
+    // add 1 to each NO_PARENTS dimension
+    allExpectedCounts.put(CP_B, allExpectedCounts.get(CP_B) + 1);
+    allExpectedCounts.put(CP_C, allExpectedCounts.get(CP_C) + 1);
+    allExpectedCounts.put(CP_D, allExpectedCounts.get(CP_D) + 1);
+    if (updateTermExpectedCounts) {
+      termExpectedCounts.put(CP_B, termExpectedCounts.get(CP_B) + 1);
+      termExpectedCounts.put(CP_C, termExpectedCounts.get(CP_C) + 1);
+      termExpectedCounts.put(CP_D, termExpectedCounts.get(CP_D) + 1);
+    }
+  }
+
+  private static FacetsConfig getConfig() {
+    FacetsConfig config = new FacetsConfig();
+    config.setMultiValued("A", true);
+    config.setMultiValued("B", true);
+    config.setRequireDimCount("B", true);
+    config.setHierarchical("D", true);
+    return config;
+  }
+
+  private static void indexDocsNoFacets(IndexWriter indexWriter) throws IOException {
+    int numDocs = atLeast(2);
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      addField(doc);
+      indexWriter.addDocument(doc);
+    }
+    indexWriter.commit(); // flush a segment
+  }
+  
+  private static void indexDocsWithFacetsNoTerms(IndexWriter indexWriter, TaxonomyWriter taxoWriter, 
+                                                 Map<String,Integer> expectedCounts) throws IOException {
+    Random random = random();
+    int numDocs = atLeast(random, 2);
+    FacetsConfig config = getConfig();
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      addFacets(doc, config, false);
+      indexWriter.addDocument(config.build(taxoWriter, doc));
+    }
+    indexWriter.commit(); // flush a segment
+  }
+  
+  private static void indexDocsWithFacetsAndTerms(IndexWriter indexWriter, TaxonomyWriter taxoWriter, 
+                                                  Map<String,Integer> expectedCounts) throws IOException {
+    Random random = random();
+    int numDocs = atLeast(random, 2);
+    FacetsConfig config = getConfig();
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      addFacets(doc, config, true);
+      addField(doc);
+      indexWriter.addDocument(config.build(taxoWriter, doc));
+    }
+    indexWriter.commit(); // flush a segment
+  }
+  
+  private static void indexDocsWithFacetsAndSomeTerms(IndexWriter indexWriter, TaxonomyWriter taxoWriter, 
+                                                      Map<String,Integer> expectedCounts) throws IOException {
+    Random random = random();
+    int numDocs = atLeast(random, 2);
+    FacetsConfig config = getConfig();
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      boolean hasContent = random.nextBoolean();
+      if (hasContent) {
+        addField(doc);
+      }
+      addFacets(doc, config, hasContent);
+      indexWriter.addDocument(config.build(taxoWriter, doc));
+    }
+    indexWriter.commit(); // flush a segment
+  }
+  
+  // initialize expectedCounts w/ 0 for all categories
+  private static Map<String,Integer> newCounts() {
+    Map<String,Integer> counts = new HashMap<String,Integer>();
+    counts.put(CP_A, 0);
+    counts.put(CP_B, 0);
+    counts.put(CP_C, 0);
+    counts.put(CP_D, 0);
+    for (FacetField ff : CATEGORIES_A) {
+      counts.put(ff.dim + "/" + ff.path[0], 0);
+    }
+    for (FacetField ff : CATEGORIES_B) {
+      counts.put(ff.dim + "/" + ff.path[0], 0);
+    }
+    for (FacetField ff : CATEGORIES_C) {
+      counts.put(ff.dim + "/" + ff.path[0], 0);
+    }
+    for (FacetField ff : CATEGORIES_D) {
+      counts.put(ff.dim + "/" + ff.path[0], 0);
+    }
+    return counts;
+  }
+  
+  @BeforeClass
+  public static void beforeClassCountingFacetsAggregatorTest() throws Exception {
+    indexDir = newDirectory();
+    taxoDir = newDirectory();
+    
+    // create an index which has:
+    // 1. Segment with no categories, but matching results
+    // 2. Segment w/ categories, but no results
+    // 3. Segment w/ categories and results
+    // 4. Segment w/ categories, but only some results
+    
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // prevent merges, so we can control the index segments
+    IndexWriter indexWriter = new IndexWriter(indexDir, conf);
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+
+    allExpectedCounts = newCounts();
+    termExpectedCounts = newCounts();
+    
+    // segment w/ no categories
+    indexDocsNoFacets(indexWriter);
+
+    // segment w/ categories, no content
+    indexDocsWithFacetsNoTerms(indexWriter, taxoWriter, allExpectedCounts);
+
+    // segment w/ categories and content
+    indexDocsWithFacetsAndTerms(indexWriter, taxoWriter, allExpectedCounts);
+    
+    // segment w/ categories and some content
+    indexDocsWithFacetsAndSomeTerms(indexWriter, taxoWriter, allExpectedCounts);
+    
+    IOUtils.close(indexWriter, taxoWriter);
+  }
+  
+  @Test
+  public void testDifferentNumResults() throws Exception {
+    // test the collector w/ FacetRequests and different numResults
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    IndexSearcher searcher = newSearcher(indexReader);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    TermQuery q = new TermQuery(A);
+    searcher.search(q, sfc);
+    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
+    FacetResult result = facets.getTopChildren(NUM_CHILDREN_CP_A, CP_A);
+    assertEquals(-1, result.value.intValue());
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(termExpectedCounts.get(CP_A + "/" + labelValue.label), labelValue.value);
+    }
+    result = facets.getTopChildren(NUM_CHILDREN_CP_B, CP_B);
+    assertEquals(termExpectedCounts.get(CP_B), result.value);
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(termExpectedCounts.get(CP_B + "/" + labelValue.label), labelValue.value);
+    }
+    
+    IOUtils.close(indexReader, taxoReader);
+  }
+  
+  @Test
+  public void testAllCounts() throws Exception {
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    IndexSearcher searcher = newSearcher(indexReader);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), sfc);
+
+    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
+    
+    FacetResult result = facets.getTopChildren(NUM_CHILDREN_CP_A, CP_A);
+    assertEquals(-1, result.value.intValue());
+    int prevValue = Integer.MAX_VALUE;
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(allExpectedCounts.get(CP_A + "/" + labelValue.label), labelValue.value);
+      assertTrue("wrong sort order of sub results: labelValue.value=" + labelValue.value + " prevValue=" + prevValue, labelValue.value.intValue() <= prevValue);
+      prevValue = labelValue.value.intValue();
+    }
+
+    result = facets.getTopChildren(NUM_CHILDREN_CP_B, CP_B);
+    assertEquals(allExpectedCounts.get(CP_B), result.value);
+    prevValue = Integer.MAX_VALUE;
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(allExpectedCounts.get(CP_B + "/" + labelValue.label), labelValue.value);
+      assertTrue("wrong sort order of sub results: labelValue.value=" + labelValue.value + " prevValue=" + prevValue, labelValue.value.intValue() <= prevValue);
+      prevValue = labelValue.value.intValue();
+    }
+
+    IOUtils.close(indexReader, taxoReader);
+  }
+  
+  @Test
+  public void testBigNumResults() throws Exception {
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    IndexSearcher searcher = newSearcher(indexReader);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), sfc);
+
+    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
+
+    FacetResult result = facets.getTopChildren(Integer.MAX_VALUE, CP_A);
+    assertEquals(-1, result.value.intValue());
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(allExpectedCounts.get(CP_A + "/" + labelValue.label), labelValue.value);
+    }
+    result = facets.getTopChildren(Integer.MAX_VALUE, CP_B);
+    assertEquals(allExpectedCounts.get(CP_B), result.value);
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(allExpectedCounts.get(CP_B + "/" + labelValue.label), labelValue.value);
+    }
+    
+    IOUtils.close(indexReader, taxoReader);
+  }
+  
+  @Test
+  public void testNoParents() throws Exception {
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    IndexSearcher searcher = newSearcher(indexReader);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), sfc);
+
+    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
+
+    FacetResult result = facets.getTopChildren(NUM_CHILDREN_CP_C, CP_C);
+    assertEquals(allExpectedCounts.get(CP_C), result.value);
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(allExpectedCounts.get(CP_C + "/" + labelValue.label), labelValue.value);
+    }
+    result = facets.getTopChildren(NUM_CHILDREN_CP_D, CP_D);
+    assertEquals(allExpectedCounts.get(CP_C), result.value);
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(allExpectedCounts.get(CP_D + "/" + labelValue.label), labelValue.value);
+    }
+    
+    IOUtils.close(indexReader, taxoReader);
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts.java	2013-12-18 19:31:31.133604271 -0500
@@ -0,0 +1,754 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.ByteArrayOutputStream;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.facet.taxonomy.PrintTaxonomyStats;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.NoMergePolicy;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.similarities.DefaultSimilarity;
+import org.apache.lucene.search.similarities.PerFieldSimilarityWrapper;
+import org.apache.lucene.search.similarities.Similarity;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+
+public class TestTaxonomyFacetCounts extends FacetTestCase {
+
+  public void testBasic() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setHierarchical("Publish Date", true);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new FacetField("Author", "Bob"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "15"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "20"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Susan"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "7"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Frank"));
+    doc.add(new FacetField("Publish Date", "1999", "5", "5"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    // Aggregate the facet counts:
+    FacetsCollector c = new FacetsCollector();
+
+    // MatchAllDocsQuery is for "browsing" (counts facets
+    // for all non-deleted docs in the index); normally
+    // you'd use a "normal" query, and use MultiCollector to
+    // wrap collecting the "normal" hits and also facets:
+    searcher.search(new MatchAllDocsQuery(), c);
+
+    Facets facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
+
+    // Retrieve & verify results:
+    assertEquals("dim=Publish Date path=[] value=5 childCount=3\n  2010 (2)\n  2012 (2)\n  1999 (1)\n", facets.getTopChildren(10, "Publish Date").toString());
+    assertEquals("dim=Author path=[] value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", facets.getTopChildren(10, "Author").toString());
+
+    // Now user drills down on Publish Date/2010:
+    DrillDownQuery q2 = new DrillDownQuery(config);
+    q2.add("Publish Date", "2010");
+    c = new FacetsCollector();
+    searcher.search(q2, c);
+    facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
+    assertEquals("dim=Author path=[] value=2 childCount=2\n  Bob (1)\n  Lisa (1)\n", facets.getTopChildren(10, "Author").toString());
+
+    assertEquals(1, facets.getSpecificValue("Author", "Lisa"));
+
+    assertNull(facets.getTopChildren(10, "Non exitent dim"));
+
+    // Smoke test PrintTaxonomyStats:
+    ByteArrayOutputStream bos = new ByteArrayOutputStream();
+    PrintTaxonomyStats.printStats(taxoReader, new PrintStream(bos, false, "UTF-8"), true);
+    String result = bos.toString("UTF-8");
+    assertTrue(result.indexOf("/Author: 4 immediate children; 5 total categories") != -1);
+    assertTrue(result.indexOf("/Publish Date: 3 immediate children; 12 total categories") != -1);
+    // Make sure at least a few nodes of the tree came out:
+    assertTrue(result.indexOf("  /1999") != -1);
+    assertTrue(result.indexOf("  /2012") != -1);
+    assertTrue(result.indexOf("      /20") != -1);
+
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, taxoDir, dir);
+  }
+
+  // LUCENE-5333
+  public void testSparseFacets() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(new FacetField("a", "foo1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new FacetField("a", "foo2"));
+    doc.add(new FacetField("b", "bar1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new FacetField("a", "foo3"));
+    doc.add(new FacetField("b", "bar2"));
+    doc.add(new FacetField("c", "baz1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+
+    Facets facets = getTaxonomyFacetCounts(taxoReader, new FacetsConfig(), c);
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<FacetResult> results = facets.getAllDims(10);
+
+    assertEquals(3, results.size());
+    assertEquals("dim=a path=[] value=3 childCount=3\n  foo1 (1)\n  foo2 (1)\n  foo3 (1)\n", results.get(0).toString());
+    assertEquals("dim=b path=[] value=2 childCount=2\n  bar1 (1)\n  bar2 (1)\n", results.get(1).toString());
+    assertEquals("dim=c path=[] value=1 childCount=1\n  baz1 (1)\n", results.get(2).toString());
+
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, taxoDir, dir);
+  }
+
+  public void testWrongIndexFieldName() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setIndexFieldName("a", "$facets2");
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new FacetField("a", "foo1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+
+    // Uses default $facets field:
+    Facets facets;
+    if (random().nextBoolean()) {
+      facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
+    } else {
+      OrdinalsReader ordsReader = new DocValuesOrdinalsReader();
+      if (random().nextBoolean()) {
+        ordsReader = new CachedOrdinalsReader(ordsReader);
+      }
+      facets = new TaxonomyFacetCounts(ordsReader, taxoReader, config, c);
+    }
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<FacetResult> results = facets.getAllDims(10);
+    assertTrue(results.isEmpty());
+
+    try {
+      facets.getSpecificValue("a");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    try {
+      facets.getTopChildren(10, "a");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, taxoDir, dir);
+  }
+
+  public void testReallyNoNormsForDrillDown() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setSimilarity(new PerFieldSimilarityWrapper() {
+        final Similarity sim = new DefaultSimilarity();
+
+        @Override
+        public Similarity get(String name) {
+          assertEquals("field", name);
+          return sim;
+        }
+      });
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    doc.add(new FacetField("a", "path"));
+    writer.addDocument(config.build(taxoWriter, doc));
+    IOUtils.close(writer, taxoWriter, dir, taxoDir);
+  }
+
+  public void testMultiValuedHierarchy() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    FacetsConfig config = new FacetsConfig();
+    config.setHierarchical("a", true);
+    config.setMultiValued("a", true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    doc.add(new FacetField("a", "path", "x"));
+    doc.add(new FacetField("a", "path", "y"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    // Aggregate the facet counts:
+    FacetsCollector c = new FacetsCollector();
+
+    // MatchAllDocsQuery is for "browsing" (counts facets
+    // for all non-deleted docs in the index); normally
+    // you'd use a "normal" query, and use MultiCollector to
+    // wrap collecting the "normal" hits and also facets:
+    searcher.search(new MatchAllDocsQuery(), c);
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
+
+    try {
+      facets.getSpecificValue("a");
+      fail("didn't hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    FacetResult result = facets.getTopChildren(10, "a");
+    assertEquals(1, result.labelValues.length);
+    assertEquals(1, result.labelValues[0].value.intValue());
+
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+
+  public void testLabelWithDelimiter() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setMultiValued("dim", true);
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    doc.add(new FacetField("dim", "test\u001Fone"));
+    doc.add(new FacetField("dim", "test\u001Etwo"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);
+    
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
+    assertEquals(1, facets.getSpecificValue("dim", "test\u001Fone"));
+    assertEquals(1, facets.getSpecificValue("dim", "test\u001Etwo"));
+
+    FacetResult result = facets.getTopChildren(10, "dim");
+    assertEquals("dim=dim path=[] value=-1 childCount=2\n  test\u001Fone (1)\n  test\u001Etwo (1)\n", result.toString());
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+
+  public void testRequireDimCount() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setRequireDimCount("dim", true);
+
+    config.setMultiValued("dim2", true);
+    config.setRequireDimCount("dim2", true);
+
+    config.setMultiValued("dim3", true);
+    config.setHierarchical("dim3", true);
+    config.setRequireDimCount("dim3", true);
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    doc.add(new FacetField("dim", "a"));
+    doc.add(new FacetField("dim2", "a"));
+    doc.add(new FacetField("dim2", "b"));
+    doc.add(new FacetField("dim3", "a", "b"));
+    doc.add(new FacetField("dim3", "a", "c"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);
+    
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
+    assertEquals(1, facets.getTopChildren(10, "dim").value);
+    assertEquals(1, facets.getTopChildren(10, "dim2").value);
+    assertEquals(1, facets.getTopChildren(10, "dim3").value);
+    try {
+      assertEquals(1, facets.getSpecificValue("dim"));
+      fail("didn't hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    assertEquals(1, facets.getSpecificValue("dim2"));
+    assertEquals(1, facets.getSpecificValue("dim3"));
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+
+  // LUCENE-4583: make sure if we require > 32 KB for one
+  // document, we don't hit exc when using Facet42DocValuesFormat
+  public void testManyFacetsInOneDocument() throws Exception {
+    assumeTrue("default Codec doesn't support huge BinaryDocValues", _TestUtil.fieldSupportsHugeBinaryDocValues(FacetsConfig.DEFAULT_INDEX_FIELD_NAME));
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setMultiValued("dim", true);
+    
+    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);
+    
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    for (int i = 0; i < numLabels; i++) {
+      doc.add(new FacetField("dim", "" + i));
+    }
+    writer.addDocument(config.build(taxoWriter, doc));
+    
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    // Aggregate the facet counts:
+    FacetsCollector c = new FacetsCollector();
+    
+    // MatchAllDocsQuery is for "browsing" (counts facets
+    // for all non-deleted docs in the index); normally
+    // you'd use a "normal" query, and use MultiCollector to
+    // wrap collecting the "normal" hits and also facets:
+    searcher.search(new MatchAllDocsQuery(), c);
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
+
+    FacetResult result = facets.getTopChildren(Integer.MAX_VALUE, "dim");
+    assertEquals(numLabels, result.labelValues.length);
+    Set<String> allLabels = new HashSet<String>();
+    for (LabelAndValue labelValue : result.labelValues) {
+      allLabels.add(labelValue.label);
+      assertEquals(1, labelValue.value.intValue());
+    }
+    assertEquals(numLabels, allLabels.size());
+    
+    IOUtils.close(searcher.getIndexReader(), taxoWriter, writer, taxoReader, dir, taxoDir);
+  }
+
+  // Make sure we catch when app didn't declare field as
+  // hierarchical but it was:
+  public void testDetectHierarchicalField() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    doc.add(new FacetField("a", "path", "other"));
+    try {
+      config.build(taxoWriter, doc);
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    IOUtils.close(writer, taxoWriter, dir, taxoDir);
+  }
+
+  // Make sure we catch when app didn't declare field as
+  // multi-valued but it was:
+  public void testDetectMultiValuedField() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    doc.add(new FacetField("a", "path"));
+    doc.add(new FacetField("a", "path2"));
+    try {
+      config.build(taxoWriter, doc);
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    IOUtils.close(writer, taxoWriter, dir, taxoDir);
+  }
+
+  public void testSeparateIndexedFields() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig();
+    config.setIndexFieldName("b", "$b");
+    
+    for(int i = atLeast(30); i > 0; --i) {
+      Document doc = new Document();
+      doc.add(new StringField("f", "v", Field.Store.NO));
+      doc.add(new FacetField("a", "1"));
+      doc.add(new FacetField("b", "1"));
+      iw.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
+    Facets facets1 = getTaxonomyFacetCounts(taxoReader, config, sfc);
+    Facets facets2 = getTaxonomyFacetCounts(taxoReader, config, sfc, "$b");
+    assertEquals(r.maxDoc(), facets1.getTopChildren(10, "a").value.intValue());
+    assertEquals(r.maxDoc(), facets2.getTopChildren(10, "b").value.intValue());
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+  
+  public void testCountRoot() throws Exception {
+    // LUCENE-4882: FacetsAccumulator threw NPE if a FacetRequest was defined on CP.EMPTY
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig();
+    for(int i = atLeast(30); i > 0; --i) {
+      Document doc = new Document();
+      doc.add(new FacetField("a", "1"));
+      doc.add(new FacetField("b", "1"));
+      iw.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
+    for (FacetResult result : facets.getAllDims(10)) {
+      assertEquals(r.numDocs(), result.value.intValue());
+    }
+    
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+
+  public void testGetFacetResultsTwice() throws Exception {
+    // LUCENE-4893: counts were multiplied as many times as getFacetResults was called.
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(new FacetField("a", "1"));
+    doc.add(new FacetField("b", "1"));
+    iw.addDocument(config.build(taxoWriter, doc));
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    final FacetsCollector sfc = new FacetsCollector();
+    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
+
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
+    List<FacetResult> res1 = facets.getAllDims(10);
+    List<FacetResult> res2 = facets.getAllDims(10);
+    assertEquals("calling getFacetResults twice should return the .equals()=true result", res1, res2);
+    
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+  
+  public void testChildCount() throws Exception {
+    // LUCENE-4885: FacetResult.numValidDescendants was not set properly by FacetsAccumulator
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig();
+    for (int i = 0; i < 10; i++) {
+      Document doc = new Document();
+      doc.add(new FacetField("a", Integer.toString(i)));
+      iw.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
+    
+    assertEquals(10, facets.getTopChildren(2, "a").childCount);
+
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+
+  private void indexTwoDocs(TaxonomyWriter taxoWriter, IndexWriter indexWriter, FacetsConfig config, boolean withContent) throws Exception {
+    for (int i = 0; i < 2; i++) {
+      Document doc = new Document();
+      if (withContent) {
+        doc.add(new StringField("f", "a", Field.Store.NO));
+      }
+      if (config != null) {
+        doc.add(new FacetField("A", Integer.toString(i)));
+        indexWriter.addDocument(config.build(taxoWriter, doc));
+      } else {
+        indexWriter.addDocument(doc);
+      }
+    }
+    
+    indexWriter.commit();
+  }
+  
+  public void testSegmentsWithoutCategoriesOrResults() throws Exception {
+    // tests the accumulator when there are segments with no results
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // prevent merges
+    IndexWriter indexWriter = new IndexWriter(indexDir, iwc);
+
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    FacetsConfig config = new FacetsConfig();
+    indexTwoDocs(taxoWriter, indexWriter, config, false); // 1st segment, no content, with categories
+    indexTwoDocs(taxoWriter, indexWriter, null, true);         // 2nd segment, with content, no categories
+    indexTwoDocs(taxoWriter, indexWriter, config, true);  // 3rd segment ok
+    indexTwoDocs(taxoWriter, indexWriter, null, false);        // 4th segment, no content, or categories
+    indexTwoDocs(taxoWriter, indexWriter, null, true);         // 5th segment, with content, no categories
+    indexTwoDocs(taxoWriter, indexWriter, config, true);  // 6th segment, with content, with categories
+    indexTwoDocs(taxoWriter, indexWriter, null, true);         // 7th segment, with content, no categories
+    IOUtils.close(indexWriter, taxoWriter);
+
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    IndexSearcher indexSearcher = newSearcher(indexReader);
+    
+    // search for "f:a", only segments 1 and 3 should match results
+    Query q = new TermQuery(new Term("f", "a"));
+    FacetsCollector sfc = new FacetsCollector();
+    indexSearcher.search(q, sfc);
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
+    FacetResult result = facets.getTopChildren(10, "A");
+    assertEquals("wrong number of children", 2, result.labelValues.length);
+    for (LabelAndValue labelValue : result.labelValues) {
+      assertEquals("wrong weight for child " + labelValue.label, 2, labelValue.value.intValue());
+    }
+
+    IOUtils.close(indexReader, taxoReader, indexDir, taxoDir);
+  }
+
+  public void testRandom() throws Exception {
+    String[] tokens = getRandomTokens(10);
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    RandomIndexWriter w = new RandomIndexWriter(random(), indexDir);
+    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
+    FacetsConfig config = new FacetsConfig();
+    int numDocs = atLeast(1000);
+    int numDims = _TestUtil.nextInt(random(), 1, 7);
+    List<TestDoc> testDocs = getRandomDocs(tokens, numDocs, numDims);
+    for(TestDoc testDoc : testDocs) {
+      Document doc = new Document();
+      doc.add(newStringField("content", testDoc.content, Field.Store.NO));
+      for(int j=0;j<numDims;j++) {
+        if (testDoc.dims[j] != null) {
+          doc.add(new FacetField("dim" + j, testDoc.dims[j]));
+        }
+      }
+      w.addDocument(config.build(tw, doc));
+    }
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(w.getReader());
+    
+    // NRT open
+    TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
+
+    int iters = atLeast(100);
+    for(int iter=0;iter<iters;iter++) {
+      String searchToken = tokens[random().nextInt(tokens.length)];
+      if (VERBOSE) {
+        System.out.println("\nTEST: iter content=" + searchToken);
+      }
+      FacetsCollector fc = new FacetsCollector();
+      TopDocs hits = FacetsCollector.search(searcher, new TermQuery(new Term("content", searchToken)), 10, fc);
+      Facets facets = getTaxonomyFacetCounts(tr, config, fc);
+
+      // Slow, yet hopefully bug-free, faceting:
+      @SuppressWarnings({"rawtypes","unchecked"}) Map<String,Integer>[] expectedCounts = new HashMap[numDims];
+      for(int i=0;i<numDims;i++) {
+        expectedCounts[i] = new HashMap<String,Integer>();
+      }
+
+      for(TestDoc doc : testDocs) {
+        if (doc.content.equals(searchToken)) {
+          for(int j=0;j<numDims;j++) {
+            if (doc.dims[j] != null) {
+              Integer v = expectedCounts[j].get(doc.dims[j]);
+              if (v == null) {
+                expectedCounts[j].put(doc.dims[j], 1);
+              } else {
+                expectedCounts[j].put(doc.dims[j], v.intValue() + 1);
+              }
+            }
+          }
+        }
+      }
+
+      List<FacetResult> expected = new ArrayList<FacetResult>();
+      for(int i=0;i<numDims;i++) {
+        List<LabelAndValue> labelValues = new ArrayList<LabelAndValue>();
+        int totCount = 0;
+        for(Map.Entry<String,Integer> ent : expectedCounts[i].entrySet()) {
+          labelValues.add(new LabelAndValue(ent.getKey(), ent.getValue()));
+          totCount += ent.getValue();
+        }
+        sortLabelValues(labelValues);
+        if (totCount > 0) {
+          expected.add(new FacetResult("dim" + i, new String[0], totCount, labelValues.toArray(new LabelAndValue[labelValues.size()]), labelValues.size()));
+        }
+      }
+
+      // Sort by highest value, tie break by value:
+      sortFacetResults(expected);
+
+      List<FacetResult> actual = facets.getAllDims(10);
+
+      // Messy: fixup ties
+      sortTies(actual);
+
+      assertEquals(expected, actual);
+    }
+
+    IOUtils.close(w, tw, searcher.getIndexReader(), tr, indexDir, taxoDir);
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetSumValueSource.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetSumValueSource.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetSumValueSource.java	1969-12-31 19:00:00.000000000 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetSumValueSource.java	2013-12-02 17:02:55.146823340 -0500
@@ -0,0 +1,516 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.DoubleDocValuesField;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FloatDocValuesField;
+import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.queries.function.FunctionQuery;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
+import org.apache.lucene.queries.function.valuesource.DoubleFieldSource;
+import org.apache.lucene.queries.function.valuesource.FloatFieldSource;
+import org.apache.lucene.queries.function.valuesource.IntFieldSource;
+import org.apache.lucene.queries.function.valuesource.LongFieldSource;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.TopScoreDocCollector;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+
+public class TestTaxonomyFacetSumValueSource extends FacetTestCase {
+
+  public void testBasic() throws Exception {
+
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    FacetsConfig config = new FacetsConfig();
+
+    // Reused across documents, to add the necessary facet
+    // fields:
+    Document doc = new Document();
+    doc.add(new IntField("num", 10, Field.Store.NO));
+    doc.add(new FacetField("Author", "Bob"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new IntField("num", 20, Field.Store.NO));
+    doc.add(new FacetField("Author", "Lisa"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new IntField("num", 30, Field.Store.NO));
+    doc.add(new FacetField("Author", "Lisa"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new IntField("num", 40, Field.Store.NO));
+    doc.add(new FacetField("Author", "Susan"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new IntField("num", 45, Field.Store.NO));
+    doc.add(new FacetField("Author", "Frank"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    // Aggregate the facet counts:
+    FacetsCollector c = new FacetsCollector();
+
+    // MatchAllDocsQuery is for "browsing" (counts facets
+    // for all non-deleted docs in the index); normally
+    // you'd use a "normal" query and one of the
+    // Facets.search utility methods:
+    searcher.search(new MatchAllDocsQuery(), c);
+
+    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, new FacetsConfig(), c, new IntFieldSource("num"));
+
+    // Retrieve & verify results:
+    assertEquals("dim=Author path=[] value=145.0 childCount=4\n  Lisa (50.0)\n  Frank (45.0)\n  Susan (40.0)\n  Bob (10.0)\n", facets.getTopChildren(10, "Author").toString());
+
+    taxoReader.close();
+    searcher.getIndexReader().close();
+    dir.close();
+    taxoDir.close();
+  }
+
+  // LUCENE-5333
+  public void testSparseFacets() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(new IntField("num", 10, Field.Store.NO));
+    doc.add(new FacetField("a", "foo1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new IntField("num", 20, Field.Store.NO));
+    doc.add(new FacetField("a", "foo2"));
+    doc.add(new FacetField("b", "bar1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new IntField("num", 30, Field.Store.NO));
+    doc.add(new FacetField("a", "foo3"));
+    doc.add(new FacetField("b", "bar2"));
+    doc.add(new FacetField("c", "baz1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+
+    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, new FacetsConfig(), c, new IntFieldSource("num"));
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<FacetResult> results = facets.getAllDims(10);
+
+    assertEquals(3, results.size());
+    assertEquals("dim=a path=[] value=60.0 childCount=3\n  foo3 (30.0)\n  foo2 (20.0)\n  foo1 (10.0)\n", results.get(0).toString());
+    assertEquals("dim=b path=[] value=50.0 childCount=2\n  bar2 (30.0)\n  bar1 (20.0)\n", results.get(1).toString());
+    assertEquals("dim=c path=[] value=30.0 childCount=1\n  baz1 (30.0)\n", results.get(2).toString());
+
+    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+
+  public void testWrongIndexFieldName() throws Exception {
+
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setIndexFieldName("a", "$facets2");
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new IntField("num", 10, Field.Store.NO));
+    doc.add(new FacetField("a", "foo1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+
+    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, config, c, new IntFieldSource("num"));
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<FacetResult> results = facets.getAllDims(10);
+    assertTrue(results.isEmpty());
+
+    try {
+      facets.getSpecificValue("a");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    try {
+      facets.getTopChildren(10, "a");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+
+  public void testSumScoreAggregator() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+
+    FacetsConfig config = new FacetsConfig();
+
+    for(int i = atLeast(30); i > 0; --i) {
+      Document doc = new Document();
+      if (random().nextBoolean()) { // don't match all documents
+        doc.add(new StringField("f", "v", Field.Store.NO));
+      }
+      doc.add(new FacetField("dim", "a"));
+      iw.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    FacetsCollector fc = new FacetsCollector(true);
+    ConstantScoreQuery csq = new ConstantScoreQuery(new MatchAllDocsQuery());
+    csq.setBoost(2.0f);
+    
+    TopDocs td = FacetsCollector.search(newSearcher(r), csq, 10, fc);
+
+    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, fc, new TaxonomyFacetSumValueSource.ScoreValueSource());
+    
+    int expected = (int) (td.getMaxScore() * td.totalHits);
+    assertEquals(expected, facets.getSpecificValue("dim", "a").intValue());
+    
+    IOUtils.close(iw, taxoWriter, taxoReader, taxoDir, r, indexDir);
+  }
+  
+  public void testNoScore() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig();
+    for (int i = 0; i < 4; i++) {
+      Document doc = new Document();
+      doc.add(new NumericDocValuesField("price", (i+1)));
+      doc.add(new FacetField("a", Integer.toString(i % 2)));
+      iw.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    FacetsCollector sfc = new FacetsCollector();
+    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
+    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, sfc, new LongFieldSource("price"));
+    assertEquals("dim=a path=[] value=10.0 childCount=2\n  1 (6.0)\n  0 (4.0)\n", facets.getTopChildren(10, "a").toString());
+    
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+
+  public void testWithScore() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+
+    FacetsConfig config = new FacetsConfig();
+    for (int i = 0; i < 4; i++) {
+      Document doc = new Document();
+      doc.add(new NumericDocValuesField("price", (i+1)));
+      doc.add(new FacetField("a", Integer.toString(i % 2)));
+      iw.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    ValueSource valueSource = new ValueSource() {
+      @Override
+      public FunctionValues getValues(@SuppressWarnings("rawtypes") Map context, AtomicReaderContext readerContext) throws IOException {
+        final Scorer scorer = (Scorer) context.get("scorer");
+        assert scorer != null;
+        return new DoubleDocValues(this) {
+          @Override
+          public double doubleVal(int document) {
+            try {
+              return scorer.score();
+            } catch (IOException exception) {
+              throw new RuntimeException(exception);
+            }
+          }
+        };
+      }
+
+      @Override public boolean equals(Object o) { return o == this; }
+      @Override public int hashCode() { return System.identityHashCode(this); }
+      @Override public String description() { return "score()"; }
+    };
+    
+    FacetsCollector fc = new FacetsCollector(true);
+    TopScoreDocCollector tsdc = TopScoreDocCollector.create(10, true);
+    // score documents by their 'price' field - makes asserting the correct counts for the categories easier
+    Query q = new FunctionQuery(new LongFieldSource("price"));
+    FacetsCollector.search(newSearcher(r), q, 10, fc);
+    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, fc, valueSource);
+    
+    assertEquals("dim=a path=[] value=10.0 childCount=2\n  1 (6.0)\n  0 (4.0)\n", facets.getTopChildren(10, "a").toString());
+    
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+
+  public void testRollupValues() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig();
+    config.setHierarchical("a", true);
+    //config.setRequireDimCount("a", true);
+    
+    for (int i = 0; i < 4; i++) {
+      Document doc = new Document();
+      doc.add(new NumericDocValuesField("price", (i+1)));
+      doc.add(new FacetField("a", Integer.toString(i % 2), "1"));
+      iw.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    ValueSource valueSource = new LongFieldSource("price");
+    FacetsCollector sfc = new FacetsCollector();
+    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
+    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, sfc, valueSource);
+    
+    assertEquals("dim=a path=[] value=10.0 childCount=2\n  1 (6.0)\n  0 (4.0)\n", facets.getTopChildren(10, "a").toString());
+    
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+
+  public void testCountAndSumScore() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig();
+    config.setIndexFieldName("b", "$b");
+    
+    for(int i = atLeast(30); i > 0; --i) {
+      Document doc = new Document();
+      doc.add(new StringField("f", "v", Field.Store.NO));
+      doc.add(new FacetField("a", "1"));
+      doc.add(new FacetField("b", "1"));
+      iw.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    FacetsCollector fc = new FacetsCollector(true);
+    TopDocs hits = FacetsCollector.search(newSearcher(r), new MatchAllDocsQuery(), 10, fc);
+    
+    Facets facets1 = getTaxonomyFacetCounts(taxoReader, config, fc);
+    Facets facets2 = new TaxonomyFacetSumValueSource(new DocValuesOrdinalsReader("$b"), taxoReader, config, fc, new TaxonomyFacetSumValueSource.ScoreValueSource());
+
+    assertEquals(r.maxDoc(), facets1.getTopChildren(10, "a").value.intValue());
+    double expected = hits.getMaxScore() * r.numDocs();
+    assertEquals(r.maxDoc(), facets2.getTopChildren(10, "b").value.doubleValue(), 1E-10);
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+
+  public void testRandom() throws Exception {
+    String[] tokens = getRandomTokens(10);
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    RandomIndexWriter w = new RandomIndexWriter(random(), indexDir);
+    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
+    FacetsConfig config = new FacetsConfig();
+    int numDocs = atLeast(1000);
+    int numDims = _TestUtil.nextInt(random(), 1, 7);
+    List<TestDoc> testDocs = getRandomDocs(tokens, numDocs, numDims);
+    for(TestDoc testDoc : testDocs) {
+      Document doc = new Document();
+      doc.add(newStringField("content", testDoc.content, Field.Store.NO));
+      testDoc.value = random().nextFloat();
+      doc.add(new FloatDocValuesField("value", testDoc.value));
+      for(int j=0;j<numDims;j++) {
+        if (testDoc.dims[j] != null) {
+          doc.add(new FacetField("dim" + j, testDoc.dims[j]));
+        }
+      }
+      w.addDocument(config.build(tw, doc));
+    }
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(w.getReader());
+    
+    // NRT open
+    TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
+
+    ValueSource values = new FloatFieldSource("value");
+
+    int iters = atLeast(100);
+    for(int iter=0;iter<iters;iter++) {
+      String searchToken = tokens[random().nextInt(tokens.length)];
+      if (VERBOSE) {
+        System.out.println("\nTEST: iter content=" + searchToken);
+      }
+      FacetsCollector fc = new FacetsCollector();
+      TopDocs hits = FacetsCollector.search(searcher, new TermQuery(new Term("content", searchToken)), 10, fc);
+      Facets facets = new TaxonomyFacetSumValueSource(tr, config, fc, values);
+
+      // Slow, yet hopefully bug-free, faceting:
+      @SuppressWarnings({"rawtypes","unchecked"}) Map<String,Float>[] expectedValues = new HashMap[numDims];
+      for(int i=0;i<numDims;i++) {
+        expectedValues[i] = new HashMap<String,Float>();
+      }
+
+      for(TestDoc doc : testDocs) {
+        if (doc.content.equals(searchToken)) {
+          for(int j=0;j<numDims;j++) {
+            if (doc.dims[j] != null) {
+              Float v = expectedValues[j].get(doc.dims[j]);
+              if (v == null) {
+                expectedValues[j].put(doc.dims[j], doc.value);
+              } else {
+                expectedValues[j].put(doc.dims[j], v.floatValue() + doc.value);
+              }
+            }
+          }
+        }
+      }
+
+      List<FacetResult> expected = new ArrayList<FacetResult>();
+      for(int i=0;i<numDims;i++) {
+        List<LabelAndValue> labelValues = new ArrayList<LabelAndValue>();
+        double totValue = 0;
+        for(Map.Entry<String,Float> ent : expectedValues[i].entrySet()) {
+          labelValues.add(new LabelAndValue(ent.getKey(), ent.getValue()));
+          totValue += ent.getValue();
+        }
+        sortLabelValues(labelValues);
+        if (totValue > 0) {
+          expected.add(new FacetResult("dim" + i, new String[0], totValue, labelValues.toArray(new LabelAndValue[labelValues.size()]), labelValues.size()));
+        }
+      }
+
+      // Sort by highest value, tie break by value:
+      sortFacetResults(expected);
+
+      List<FacetResult> actual = facets.getAllDims(10);
+
+      // Messy: fixup ties
+      sortTies(actual);
+
+      if (VERBOSE) {
+        System.out.println("expected=\n" + expected.toString());
+        System.out.println("actual=\n" + actual.toString());
+      }
+
+      assertFloatValuesEquals(expected, actual);
+    }
+
+    IOUtils.close(w, tw, searcher.getIndexReader(), tr, indexDir, taxoDir);
+  }
+}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/util/AssertingCategoryListIterator.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/util/AssertingCategoryListIterator.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/util/AssertingCategoryListIterator.java	2013-01-14 13:43:40.360580181 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/util/AssertingCategoryListIterator.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,65 +0,0 @@
-package org.apache.lucene.facet.util;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.search.CategoryListIterator;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link CategoryListIterator} which asserts that
- * {@link #getOrdinals(int, IntsRef)} is not called before
- * {@link #setNextReader(AtomicReaderContext)} and that if
- * {@link #setNextReader(AtomicReaderContext)} returns false,
- * {@link #getOrdinals(int, IntsRef)} isn't called.
- */
-public class AssertingCategoryListIterator implements CategoryListIterator {
- 
-  private final CategoryListIterator delegate;
-  private boolean setNextReaderCalled = false;
-  private boolean validSegment = false;
-  private int maxDoc;
-  
-  public AssertingCategoryListIterator(CategoryListIterator delegate) {
-    this.delegate = delegate;
-  }
-  
-  @Override
-  public boolean setNextReader(AtomicReaderContext context) throws IOException {
-    setNextReaderCalled = true;
-    maxDoc = context.reader().maxDoc();
-    return validSegment = delegate.setNextReader(context);
-  }
-  
-  @Override
-  public void getOrdinals(int docID, IntsRef ints) throws IOException {
-    if (!setNextReaderCalled) {
-      throw new RuntimeException("should not call getOrdinals without setNextReader first");
-    }
-    if (!validSegment) {
-      throw new RuntimeException("should not call getOrdinals if setNextReader returned false");
-    }
-    if (docID >= maxDoc) {
-      throw new RuntimeException("docID is larger than current maxDoc; forgot to call setNextReader?");
-    }
-    delegate.getOrdinals(docID, ints);
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/util/OrdinalMappingReaderTest.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/util/OrdinalMappingReaderTest.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/util/OrdinalMappingReaderTest.java	2013-02-20 13:38:17.464711929 -0500
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/util/OrdinalMappingReaderTest.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,123 +0,0 @@
-package org.apache.lucene.facet.util;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.CountFacetRequest;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.MemoryOrdinalMap;
-import org.apache.lucene.facet.util.TaxonomyMergeUtils;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class OrdinalMappingReaderTest extends FacetTestCase {
-  
-  private static final int NUM_DOCS = 100;
-  
-  @Test
-  public void testTaxonomyMergeUtils() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxDir = newDirectory();
-    FacetIndexingParams fip = new FacetIndexingParams(randomCategoryListParams());
-    buildIndexWithFacets(dir, taxDir, true, fip);
-    
-    Directory dir1 = newDirectory();
-    Directory taxDir1 = newDirectory();
-    buildIndexWithFacets(dir1, taxDir1, false, fip);
-    
-    IndexWriter destIndexWriter = new IndexWriter(dir1, new IndexWriterConfig(TEST_VERSION_CURRENT, null));
-    DirectoryTaxonomyWriter destTaxWriter = new DirectoryTaxonomyWriter(taxDir1);
-    try {
-      TaxonomyMergeUtils.merge(dir, taxDir, new MemoryOrdinalMap(), destIndexWriter, destTaxWriter, fip);
-    } finally {
-      IOUtils.close(destIndexWriter, destTaxWriter);
-    }
-    
-    verifyResults(dir1, taxDir1, fip);
-    dir1.close();
-    taxDir1.close();
-    dir.close();
-    taxDir.close();
-  }
-
-  private void verifyResults(Directory dir, Directory taxDir, FacetIndexingParams fip) throws IOException {
-    DirectoryReader reader1 = DirectoryReader.open(dir);
-    DirectoryTaxonomyReader taxReader = new DirectoryTaxonomyReader(taxDir);
-    IndexSearcher searcher = newSearcher(reader1);
-    FacetSearchParams fsp = new FacetSearchParams(fip, new CountFacetRequest(new CategoryPath("tag"), NUM_DOCS));
-    FacetsCollector collector = FacetsCollector.create(fsp, reader1, taxReader);
-    searcher.search(new MatchAllDocsQuery(), collector);
-    FacetResult result = collector.getFacetResults().get(0);
-    FacetResultNode node = result.getFacetResultNode();
-    for (FacetResultNode facet: node.subResults) {
-      int weight = (int)facet.value;
-      int label = Integer.parseInt(facet.label.components[1]);
-      //System.out.println(label + ": " + weight);
-      if (VERBOSE) {
-        System.out.println(label + ": " + weight);
-      }
-      assertEquals(NUM_DOCS ,weight);
-    }
-    reader1.close();
-    taxReader.close();
-  }
-
-  private void buildIndexWithFacets(Directory dir, Directory taxDir, boolean asc, FacetIndexingParams fip) throws IOException {
-    IndexWriterConfig config = newIndexWriterConfig(TEST_VERSION_CURRENT, 
-        new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false));
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, config);
-    
-    DirectoryTaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxDir);
-    for (int i = 1; i <= NUM_DOCS; i++) {
-      Document doc = new Document();
-      List<CategoryPath> categoryPaths = new ArrayList<CategoryPath>(i + 1);
-      for (int j = i; j <= NUM_DOCS; j++) {
-        int facetValue = asc? j: NUM_DOCS - j;
-        categoryPaths.add(new CategoryPath("tag", Integer.toString(facetValue)));
-      }
-      FacetFields facetFields = new FacetFields(taxonomyWriter, fip);
-      facetFields.addFields(doc, categoryPaths);
-      writer.addDocument(doc);
-    }    
-    taxonomyWriter.close();
-    writer.close();
-  }  
-
-}


diff -ruN -x .svn -x build trunk/lucene/facet/src/test/org/apache/lucene/facet/util/TestFacetsPayloadMigrationReader.java simplefacets/lucene/facet/src/test/org/apache/lucene/facet/util/TestFacetsPayloadMigrationReader.java
--- trunk/lucene/facet/src/test/org/apache/lucene/facet/util/TestFacetsPayloadMigrationReader.java	2013-04-22 16:59:20.059676456 -0400
+++ simplefacets/lucene/facet/src/test/org/apache/lucene/facet/util/TestFacetsPayloadMigrationReader.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,412 +0,0 @@
-package org.apache.lucene.facet.util;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.params.PerDimensionIndexingParams;
-import org.apache.lucene.facet.params.PerDimensionOrdinalPolicy;
-import org.apache.lucene.facet.search.CategoryListIterator;
-import org.apache.lucene.facet.search.CountFacetRequest;
-import org.apache.lucene.facet.search.DrillDownQuery;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.MultiReader;
-import org.apache.lucene.index.NoMergePolicy;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.MultiCollector;
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.TotalHitCountCollector;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Tests facets index migration from payload to DocValues.*/
-public class TestFacetsPayloadMigrationReader extends FacetTestCase {
-  
-  private static class PayloadFacetFields extends FacetFields {
-
-    private static final class CountingListStream extends TokenStream {
-      private final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);
-      private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-      private final Iterator<Entry<String,BytesRef>> categoriesData;
-      
-      CountingListStream(Map<String,BytesRef> categoriesData) {
-        this.categoriesData = categoriesData.entrySet().iterator();
-      }
-      
-      @Override
-      public boolean incrementToken() throws IOException {
-        if (!categoriesData.hasNext()) {
-          return false;
-        }
-        
-        Entry<String,BytesRef> entry = categoriesData.next();
-        termAtt.setEmpty().append(FacetsPayloadMigrationReader.PAYLOAD_TERM_TEXT + entry.getKey());
-        payloadAtt.setPayload(entry.getValue());
-        return true;
-      }
-      
-    }
-
-    private static final FieldType COUNTING_LIST_PAYLOAD_TYPE = new FieldType();
-    static {
-      COUNTING_LIST_PAYLOAD_TYPE.setIndexed(true);
-      COUNTING_LIST_PAYLOAD_TYPE.setTokenized(true);
-      COUNTING_LIST_PAYLOAD_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
-      COUNTING_LIST_PAYLOAD_TYPE.setStored(false);
-      COUNTING_LIST_PAYLOAD_TYPE.setOmitNorms(true);
-      COUNTING_LIST_PAYLOAD_TYPE.freeze();
-    }
-    
-    public PayloadFacetFields(TaxonomyWriter taxonomyWriter, FacetIndexingParams params) {
-      super(taxonomyWriter, params);
-    }
-
-    @Override
-    protected FieldType drillDownFieldType() {
-      // Since the payload is indexed in the same field as the drill-down terms,
-      // we must set IndexOptions to DOCS_AND_FREQS_AND_POSITIONS
-      final FieldType type = new FieldType(TextField.TYPE_NOT_STORED);
-      type.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
-      type.freeze();
-      return type;
-    }
-
-    @Override
-    protected void addCountingListData(Document doc, Map<String,BytesRef> categoriesData, String field) {
-      CountingListStream ts = new CountingListStream(categoriesData);
-      doc.add(new Field(field, ts, COUNTING_LIST_PAYLOAD_TYPE));
-    }
-  }
-
-  private static final String[] DIMENSIONS = new String[] { "dim1", "dim2", "dim3.1", "dim3.2" };
-  
-  private HashMap<String,Integer> createIndex(Directory indexDir, Directory taxoDir, FacetIndexingParams fip) 
-      throws Exception {
-    Random random = random();
-    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));
-    conf.setMaxBufferedDocs(2); // force few segments
-    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // avoid merges so that we're left with few segments
-    IndexWriter indexWriter = new IndexWriter(indexDir, conf);
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    
-    FacetFields facetFields = new PayloadFacetFields(taxoWriter, fip);
-    
-    HashMap<String,Integer> expectedCounts = new HashMap<String,Integer>(DIMENSIONS.length);
-    int numDocs = atLeast(10);
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      int numCategories = random.nextInt(3) + 1;
-      ArrayList<CategoryPath> categories = new ArrayList<CategoryPath>(numCategories);
-      HashSet<String> docDimensions = new HashSet<String>();
-      while (numCategories-- > 0) {
-        String dim = DIMENSIONS[random.nextInt(DIMENSIONS.length)];
-        // we should only increment the expected count by 1 per document
-        docDimensions.add(dim);
-        categories.add(new CategoryPath(dim, Integer.toString(i), Integer.toString(numCategories)));
-      }
-      facetFields.addFields(doc, categories);
-      doc.add(new StringField("docid", Integer.toString(i), Store.YES));
-      doc.add(new TextField("foo", "content" + i, Store.YES));
-      indexWriter.addDocument(doc);
-
-      // update expected count per dimension
-      for (String dim : docDimensions) {
-        Integer val = expectedCounts.get(dim);
-        if (val == null) {
-          expectedCounts.put(dim, Integer.valueOf(1));
-        } else {
-          expectedCounts.put(dim, Integer.valueOf(val.intValue() + 1));
-        }
-      }
-      
-      if (random.nextDouble() < 0.2) { // add some documents that will be deleted
-        doc = new Document();
-        doc.add(new StringField("del", "key", Store.NO));
-        facetFields.addFields(doc, Collections.singletonList(new CategoryPath("dummy")));
-        indexWriter.addDocument(doc);
-      }
-    }
-    
-    indexWriter.commit();
-    taxoWriter.commit();
-
-    // delete the docs that were marked for deletion. note that the 'dummy'
-    // category is not removed from the taxonomy, so must account for it when we
-    // verify the migrated index.
-    indexWriter.deleteDocuments(new Term("del", "key"));
-    indexWriter.commit();
-    
-    IOUtils.close(indexWriter, taxoWriter);
-    
-    return expectedCounts;
-  }
-  
-  private void migrateIndex(Directory indexDir, FacetIndexingParams fip) throws Exception {
-    final Map<String,Term> fieldTerms = FacetsPayloadMigrationReader.buildFieldTermsMap(indexDir, fip);
-    DirectoryReader reader = DirectoryReader.open(indexDir);
-    List<AtomicReaderContext> leaves = reader.leaves();
-    int numReaders = leaves.size();
-    AtomicReader wrappedLeaves[] = new AtomicReader[numReaders];
-    for (int i = 0; i < numReaders; i++) {
-      wrappedLeaves[i] = new FacetsPayloadMigrationReader(leaves.get(i).reader(), fieldTerms);
-    }
-    
-    IndexWriter writer = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
-    writer.deleteAll();
-    try {
-      writer.addIndexes(new MultiReader(wrappedLeaves));
-      writer.commit();
-    } finally {
-      reader.close();
-      writer.close();
-    }
-  }
-  
-  private void verifyMigratedIndex(Directory indexDir, Directory taxoDir, HashMap<String,Integer> expectedCounts, 
-      FacetIndexingParams fip) throws Exception {
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher searcher = newSearcher(indexReader);
-
-    assertFalse("index should not have deletions", indexReader.hasDeletions());
-    
-    verifyNotFacetsData(indexReader, searcher);
-    verifyFacetedSearch(expectedCounts, fip, indexReader, taxoReader, searcher);
-    verifyDrillDown(expectedCounts, fip, indexReader, taxoReader, searcher);
-    verifyIndexOrdinals(indexReader, taxoReader, fip);
-    
-    IOUtils.close(indexReader, taxoReader);
-  }
-  
-  private void verifyNotFacetsData(DirectoryReader indexReader, IndexSearcher searcher) throws IOException {
-    // verify that non facets data was not damaged
-    TotalHitCountCollector total = new TotalHitCountCollector();
-    searcher.search(new PrefixQuery(new Term("foo", "content")), total);
-    assertEquals("invalid number of results for content query", total.getTotalHits(), indexReader.maxDoc());
-    
-    int numDocIDs = 0;
-    for (AtomicReaderContext context : indexReader.leaves()) {
-      Terms docIDs = context.reader().terms("docid");
-      assertNotNull(docIDs);
-      TermsEnum te = docIDs.iterator(null);
-      while (te.next() != null) {
-        ++numDocIDs;
-      }
-    }
-    assertEquals("invalid number of docid terms", indexReader.maxDoc(), numDocIDs);
-  }
-  
-  private void verifyFacetedSearch(Map<String,Integer> expectedCounts, FacetIndexingParams fip, 
-      DirectoryReader indexReader, TaxonomyReader taxoReader, IndexSearcher searcher) throws IOException {
-    // run faceted search and assert expected counts
-    ArrayList<FacetRequest> requests = new ArrayList<FacetRequest>(expectedCounts.size());
-    for (String dim : expectedCounts.keySet()) {
-      requests.add(new CountFacetRequest(new CategoryPath(dim), 5));
-    }
-    FacetSearchParams fsp = new FacetSearchParams(fip, requests);
-    FacetsCollector fc = FacetsCollector.create(fsp, indexReader, taxoReader);
-    MatchAllDocsQuery base = new MatchAllDocsQuery();
-    searcher.search(base, fc);
-    List<FacetResult> facetResults = fc.getFacetResults();
-    assertEquals(requests.size(), facetResults.size());
-    for (FacetResult res : facetResults) {
-      FacetResultNode node = res.getFacetResultNode();
-      String dim = node.label.components[0];
-      assertEquals("wrong count for " + dim, expectedCounts.get(dim).intValue(), (int) node.value);
-    }
-  }
-  
-  private void verifyDrillDown(Map<String,Integer> expectedCounts, FacetIndexingParams fip, DirectoryReader indexReader, 
-      TaxonomyReader taxoReader, IndexSearcher searcher) throws IOException {
-    // verify drill-down
-    for (String dim : expectedCounts.keySet()) {
-      CategoryPath drillDownCP = new CategoryPath(dim);
-      FacetSearchParams fsp = new FacetSearchParams(fip, new CountFacetRequest(drillDownCP, 10));
-      DrillDownQuery drillDown = new DrillDownQuery(fip, new MatchAllDocsQuery());
-      drillDown.add(drillDownCP);
-      TotalHitCountCollector total = new TotalHitCountCollector();
-      FacetsCollector fc = FacetsCollector.create(fsp, indexReader, taxoReader);
-      searcher.search(drillDown, MultiCollector.wrap(fc, total));
-      assertTrue("no results for drill-down query " + drillDown, total.getTotalHits() > 0);
-      List<FacetResult> facetResults = fc.getFacetResults();
-      assertEquals(1, facetResults.size());
-      FacetResultNode rootNode = facetResults.get(0).getFacetResultNode();
-      assertEquals("wrong count for " + dim, expectedCounts.get(dim).intValue(), (int) rootNode.value);
-    }
-  }
-  
-  private void verifyIndexOrdinals(DirectoryReader indexReader, TaxonomyReader taxoReader, FacetIndexingParams fip) 
-      throws IOException {
-    // verify that the ordinals in the index match the ones in the taxonomy, and vice versa
-    
-    // collect all fields which have DocValues, to assert later that all were
-    // visited i.e. that during migration we didn't add FieldInfos with no
-    // DocValues
-    HashSet<String> docValuesFields = new HashSet<String>();
-    for (AtomicReaderContext context : indexReader.leaves()) {
-      FieldInfos infos = context.reader().getFieldInfos();
-      for (FieldInfo info : infos) {
-        if (info.hasDocValues()) {
-          docValuesFields.add(info.name);
-        }
-      }
-    }
-    
-    // check that all visited ordinals are found in the taxonomy and vice versa
-    boolean[] foundOrdinals = new boolean[taxoReader.getSize()];
-    for (int i = 0; i < foundOrdinals.length; i++) {
-      foundOrdinals[i] = false; // init to be on the safe side
-    }
-    foundOrdinals[0] = true; // ROOT ordinals isn't indexed
-    // mark 'dummy' category ordinal as seen
-    int dummyOrdinal = taxoReader.getOrdinal(new CategoryPath("dummy"));
-    if (dummyOrdinal > 0) {
-      foundOrdinals[dummyOrdinal] = true;
-    }
-    
-    int partitionSize = fip.getPartitionSize();
-    int numPartitions = (int) Math.ceil(taxoReader.getSize() / (double) partitionSize);
-    final IntsRef ordinals = new IntsRef(32);
-    for (String dim : DIMENSIONS) {
-      CategoryListParams clp = fip.getCategoryListParams(new CategoryPath(dim));
-      int partitionOffset = 0;
-      for (int partition = 0; partition < numPartitions; partition++, partitionOffset += partitionSize) {
-        final CategoryListIterator cli = clp.createCategoryListIterator(partition);
-        for (AtomicReaderContext context : indexReader.leaves()) {
-          if (cli.setNextReader(context)) { // not all fields may exist in all segments
-            // remove that field from the list of DocValues fields
-            docValuesFields.remove(clp.field + PartitionsUtils.partitionName(partition));
-            int maxDoc = context.reader().maxDoc();
-            for (int doc = 0; doc < maxDoc; doc++) {
-              cli.getOrdinals(doc, ordinals);
-              for (int j = 0; j < ordinals.length; j++) {
-                // verify that the ordinal is recognized by the taxonomy
-                int ordinal = ordinals.ints[j] + partitionOffset;
-                assertTrue("should not have received dummy ordinal (" + dummyOrdinal + ")", dummyOrdinal != ordinal);
-                assertNotNull("missing category for ordinal " + ordinal, taxoReader.getPath(ordinal));
-                foundOrdinals[ordinal] = true;
-              }
-            }
-          }
-        }
-      }
-    }
-    
-    assertTrue("some fields which have docValues were not visited: " + docValuesFields, docValuesFields.isEmpty());
-    
-    for (int i = 0; i < foundOrdinals.length; i++) {
-      assertTrue("ordinal " + i + " not visited", foundOrdinals[i]);
-    }
-  }
-  
-  private void doTestMigration(final int partitionSize) throws Exception {
-    // create a facets index with PayloadFacetFields and check it after migration
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    // set custom CLP fields for two dimensions and use the default ($facets) for the other two
-    HashMap<CategoryPath,CategoryListParams> params = new HashMap<CategoryPath,CategoryListParams>();
-    params.put(new CategoryPath(DIMENSIONS[0]), new CategoryListParams(DIMENSIONS[0]) {
-      @Override
-      public OrdinalPolicy getOrdinalPolicy(String dimension) {
-        return OrdinalPolicy.ALL_PARENTS;
-      }
-    });
-    params.put(new CategoryPath(DIMENSIONS[1]), new CategoryListParams(DIMENSIONS[1]) {
-      @Override
-      public OrdinalPolicy getOrdinalPolicy(String dimension) {
-        return OrdinalPolicy.ALL_PARENTS;
-      }
-    });
-    
-    HashMap<String,OrdinalPolicy> policies = new HashMap<String,CategoryListParams.OrdinalPolicy>();
-    policies.put(DIMENSIONS[2], OrdinalPolicy.ALL_PARENTS);
-    policies.put(DIMENSIONS[3], OrdinalPolicy.ALL_PARENTS);
-    FacetIndexingParams fip = new PerDimensionIndexingParams(params, new PerDimensionOrdinalPolicy(policies)) {
-      @Override
-      public int getPartitionSize() {
-        return partitionSize;
-      }
-    };
-    
-    HashMap<String,Integer> expectedCounts = createIndex(indexDir, taxoDir, fip);
-    migrateIndex(indexDir, fip);
-    verifyMigratedIndex(indexDir, taxoDir, expectedCounts, fip);
-    
-    IOUtils.close(indexDir, taxoDir);
-  }
-  
-  @Test
-  public void testMigration() throws Exception {
-    doTestMigration(Integer.MAX_VALUE);
-  }
-  
-  @Test
-  public void testMigrationWithPartitions() throws Exception {
-    doTestMigration(2);
-  }
-  
-}


diff -ruN -x .svn -x build trunk/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyReplicationClientTest.java simplefacets/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyReplicationClientTest.java
--- trunk/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyReplicationClientTest.java	2013-07-29 13:55:02.497707545 -0400
+++ simplefacets/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyReplicationClientTest.java	2013-11-27 18:39:56.043963361 -0500
@@ -26,18 +26,19 @@
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.CountFacetRequest;
-import org.apache.lucene.facet.search.DrillDownQuery;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.DrillDownQuery;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.FastTaxonomyFacetCounts;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexDocument;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.SnapshotDeletionPolicy;
@@ -63,11 +64,14 @@
     private final Directory indexDir, taxoDir;
     private DirectoryReader indexReader;
     private DirectoryTaxonomyReader taxoReader;
+    private FacetsConfig config;
     private long lastIndexGeneration = -1;
     
     public IndexAndTaxonomyReadyCallback(Directory indexDir, Directory taxoDir) throws IOException {
       this.indexDir = indexDir;
       this.taxoDir = taxoDir;
+      config = new FacetsConfig();
+      config.setHierarchical("A", true);
       if (DirectoryReader.indexExists(indexDir)) {
         indexReader = DirectoryReader.open(indexDir);
         lastIndexGeneration = indexReader.getIndexCommit().getGeneration();
@@ -102,14 +106,14 @@
         
         // verify faceted search
         int id = Integer.parseInt(indexReader.getIndexCommit().getUserData().get(VERSION_ID), 16);
-        CategoryPath cp = new CategoryPath("A", Integer.toString(id, 16));
         IndexSearcher searcher = new IndexSearcher(indexReader);
-        FacetsCollector fc = FacetsCollector.create(new FacetSearchParams(new CountFacetRequest(cp, 10)), indexReader, taxoReader);
+        FacetsCollector fc = new FacetsCollector();
         searcher.search(new MatchAllDocsQuery(), fc);
-        assertEquals(1, (int) fc.getFacetResults().get(0).getFacetResultNode().value);
+        Facets facets = new FastTaxonomyFacetCounts(taxoReader, config, fc);
+        assertEquals(1, facets.getSpecificValue("A", Integer.toString(id, 16)).intValue());
         
-        DrillDownQuery drillDown = new DrillDownQuery(FacetIndexingParams.DEFAULT);
-        drillDown.add(cp);
+        DrillDownQuery drillDown = new DrillDownQuery(config);
+        drillDown.add("A", Integer.toString(id, 16));
         TopDocs docs = searcher.search(drillDown, 10);
         assertEquals(1, docs.totalHits);
       }
@@ -130,6 +134,7 @@
   private ReplicationHandler handler;
   private IndexWriter publishIndexWriter;
   private SnapshotDirectoryTaxonomyWriter publishTaxoWriter;
+  private FacetsConfig config;
   private IndexAndTaxonomyReadyCallback callback;
   private File clientWorkDir;
   
@@ -175,11 +180,10 @@
     return new IndexAndTaxonomyRevision(publishIndexWriter, publishTaxoWriter);
   }
   
-  private Document newDocument(TaxonomyWriter taxoWriter, int id) throws IOException {
+  private IndexDocument newDocument(TaxonomyWriter taxoWriter, int id) throws IOException {
     Document doc = new Document();
-    FacetFields facetFields = new FacetFields(taxoWriter);
-    facetFields.addFields(doc, Collections.singleton(new CategoryPath("A", Integer.toString(id, 16))));
-    return doc;
+    doc.add(new FacetField("A", Integer.toString(id, 16)));
+    return config.build(publishTaxoWriter, doc);
   }
   
   @Override
@@ -201,6 +205,8 @@
     conf.setIndexDeletionPolicy(new SnapshotDeletionPolicy(conf.getIndexDeletionPolicy()));
     publishIndexWriter = new IndexWriter(publishIndexDir, conf);
     publishTaxoWriter = new SnapshotDirectoryTaxonomyWriter(publishTaxoDir);
+    config = new FacetsConfig();
+    config.setHierarchical("A", true);
   }
   
   @After


diff -ruN -x .svn -x build trunk/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyRevisionTest.java simplefacets/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyRevisionTest.java
--- trunk/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyRevisionTest.java	2013-07-29 13:55:02.497707545 -0400
+++ simplefacets/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyRevisionTest.java	2013-11-27 18:40:08.499963259 -0500
@@ -21,13 +21,15 @@
 import java.io.InputStream;
 import java.util.Collections;
 import java.util.List;
-import java.util.Map;
 import java.util.Map.Entry;
+import java.util.Map;
 
 import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.index.IndexDocument;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -41,11 +43,11 @@
 
 public class IndexAndTaxonomyRevisionTest extends ReplicatorTestCase {
   
-  private Document newDocument(TaxonomyWriter taxoWriter) throws IOException {
+  private IndexDocument newDocument(TaxonomyWriter taxoWriter) throws IOException {
+    FacetsConfig config = new FacetsConfig();
     Document doc = new Document();
-    FacetFields ff = new FacetFields(taxoWriter);
-    ff.addFields(doc, Collections.singleton(new CategoryPath("A")));
-    return doc;
+    doc.add(new FacetField("A", "1"));
+    return config.build(taxoWriter, doc);
   }
   
   @Test


