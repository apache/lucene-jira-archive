diff --git a/lucene/core/src/java/org/apache/lucene/search/DocIdSet.java b/lucene/core/src/java/org/apache/lucene/search/DocIdSet.java
index 05870d9..242cdb6 100644
--- a/lucene/core/src/java/org/apache/lucene/search/DocIdSet.java
+++ b/lucene/core/src/java/org/apache/lucene/search/DocIdSet.java
@@ -21,6 +21,7 @@ import java.io.IOException;
 
 import org.apache.lucene.util.Accountable;
 import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.RamUsageEstimator;
 
 /**
  * A DocIdSet contains a set of doc ids. Implementing classes must
@@ -48,6 +49,34 @@ public abstract class DocIdSet implements Accountable {
     }
   };
 
+  /**
+   * Return a {@link DocIdSet} that matches all documents between {@code 0}
+   * and {@code maxDoc}.
+   */
+  public static DocIdSet all(int maxDoc) {
+    return new AllDocsSet(maxDoc);
+  }
+
+  private static class AllDocsSet extends DocIdSet {
+    private static final long RAM_BYTES_USED = RamUsageEstimator.shallowSizeOfInstance(AllDocsSet.class);
+
+    private final int maxDoc;
+
+    private AllDocsSet(int maxDoc) {
+      this.maxDoc = maxDoc;
+    }
+
+    @Override
+    public DocIdSetIterator iterator() throws IOException {
+      return DocIdSetIterator.all(maxDoc);
+    }
+
+    @Override
+    public long ramBytesUsed() {
+      return RAM_BYTES_USED;
+    }
+  }
+
   /** Provides a {@link DocIdSetIterator} to access the set.
    * This implementation can return <code>null</code> if there
    * are no docs that match. */
diff --git a/lucene/core/src/java/org/apache/lucene/search/LRUQueryCache.java b/lucene/core/src/java/org/apache/lucene/search/LRUQueryCache.java
index 8598645..af4073f 100644
--- a/lucene/core/src/java/org/apache/lucene/search/LRUQueryCache.java
+++ b/lucene/core/src/java/org/apache/lucene/search/LRUQueryCache.java
@@ -41,7 +41,6 @@ import org.apache.lucene.index.TieredMergePolicy;
 import org.apache.lucene.util.Accountable;
 import org.apache.lucene.util.Accountables;
 import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.RoaringDocIdSet;
 
 /**
  * A {@link QueryCache} that evicts queries using a LRU (least-recently-used)
@@ -502,25 +501,6 @@ public class LRUQueryCache implements QueryCache, Accountable {
   }
 
   /**
-   * Default cache implementation: uses {@link RoaringDocIdSet}.
-   */
-  protected DocIdSet cacheImpl(BulkScorer scorer, int maxDoc) throws IOException {
-    RoaringDocIdSet.Builder builder = new RoaringDocIdSet.Builder(maxDoc);
-    scorer.score(new LeafCollector() {
-
-      @Override
-      public void setScorer(Scorer scorer) throws IOException {}
-
-      @Override
-      public void collect(int doc) throws IOException {
-        builder.add(doc);
-      }
-
-    }, null);
-    return builder.build();
-  }
-
-  /**
    * Return the total number of times that a {@link Query} has been looked up
    * in this {@link QueryCache}. Note that this number is incremented once per
    * segment so running a cached query only once will increment this counter
@@ -679,15 +659,6 @@ public class LRUQueryCache implements QueryCache, Accountable {
       return worstCaseRamUsage * 5 < totalRamAvailable;
     }
 
-    private DocIdSet cache(LeafReaderContext context) throws IOException {
-      final BulkScorer scorer = in.bulkScorer(context);
-      if (scorer == null) {
-        return DocIdSet.EMPTY;
-      } else {
-        return cacheImpl(scorer, context.reader().maxDoc());
-      }
-    }
-
     /** Check whether this segment is eligible for caching, regardless of the query. */
     private boolean shouldCache(LeafReaderContext context) throws IOException {
       return cacheEntryHasReasonableWorstCaseSize(ReaderUtil.getTopLevelContext(context).reader().maxDoc())
@@ -719,7 +690,7 @@ public class LRUQueryCache implements QueryCache, Accountable {
 
       if (docIdSet == null) {
         if (policy.shouldCache(in.getQuery())) {
-          docIdSet = cache(context);
+          docIdSet = in.computeMatchingDocs(context);
           putIfAbsent(in.getQuery(), context, docIdSet);
         } else {
           return in.scorer(context);
@@ -763,7 +734,7 @@ public class LRUQueryCache implements QueryCache, Accountable {
 
       if (docIdSet == null) {
         if (policy.shouldCache(in.getQuery())) {
-          docIdSet = cache(context);
+          docIdSet = in.computeMatchingDocs(context);
           putIfAbsent(in.getQuery(), context, docIdSet);
         } else {
           return in.bulkScorer(context);
diff --git a/lucene/core/src/java/org/apache/lucene/search/PointRangeQuery.java b/lucene/core/src/java/org/apache/lucene/search/PointRangeQuery.java
index fb7051d..60ba45b 100644
--- a/lucene/core/src/java/org/apache/lucene/search/PointRangeQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/PointRangeQuery.java
@@ -169,17 +169,17 @@ public abstract class PointRangeQuery extends Query {
       }
 
       @Override
-      public Scorer scorer(LeafReaderContext context) throws IOException {
+      public DocIdSet computeMatchingDocs(LeafReaderContext context) throws IOException {
         LeafReader reader = context.reader();
         PointValues values = reader.getPointValues();
         if (values == null) {
           // No docs in this segment indexed any points
-          return null;
+          return DocIdSet.EMPTY;
         }
         FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);
         if (fieldInfo == null) {
           // No docs in this segment indexed this field at all
-          return null;
+          return DocIdSet.EMPTY;
         }
         if (fieldInfo.getPointDimensionCount() != numDims) {
           throw new IllegalArgumentException("field=\"" + field + "\" was indexed with numDims=" + fieldInfo.getPointDimensionCount() + " but this query has numDims=" + numDims);
@@ -205,15 +205,16 @@ public abstract class PointRangeQuery extends Query {
           allDocsMatch = false;
         }
 
-        DocIdSetIterator iterator;
         if (allDocsMatch) {
-          // all docs have a value and all points are within bounds, so everything matches
-          iterator = DocIdSetIterator.all(reader.maxDoc());
+          return DocIdSet.all(context.reader().maxDoc());
         } else {
-          iterator = buildMatchingDocIdSet(reader, values).iterator();
+          return buildMatchingDocIdSet(reader, values);
         }
+      }
 
-        return new ConstantScoreScorer(this, score(), iterator);
+      @Override
+      public Scorer scorer(LeafReaderContext context) throws IOException {
+        return new ConstantScoreScorer(this, score(), computeMatchingDocs(context).iterator());
       }
     };
   }
diff --git a/lucene/core/src/java/org/apache/lucene/search/Weight.java b/lucene/core/src/java/org/apache/lucene/search/Weight.java
index 40d2aaf..4fcd827 100644
--- a/lucene/core/src/java/org/apache/lucene/search/Weight.java
+++ b/lucene/core/src/java/org/apache/lucene/search/Weight.java
@@ -26,6 +26,7 @@ import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.similarities.Similarity;
 import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.RoaringDocIdSet;
 
 /**
  * Expert: Calculate query weights and build query scorers.
@@ -97,6 +98,32 @@ public abstract class Weight {
   /** Assigns the query normalization factor and boost to this. */
   public abstract void normalize(float norm, float boost);
 
+  /** Build a {@link DocIdSet} that contains all matching documents on the given
+   *  {@link LeafReaderContext}. This is typically used by {@link QueryCache}
+   *  implementations.
+   *  <p>The default implementation loads documents using the
+   *  {@link #bulkScorer(LeafReaderContext)} into a {@link RoaringDocIdSet}.
+   *  <p>This method never returns {@code null}. */
+  public DocIdSet computeMatchingDocs(LeafReaderContext context) throws IOException {
+    BulkScorer scorer = bulkScorer(context);
+    if (scorer == null) {
+      return DocIdSet.EMPTY;
+    }
+    RoaringDocIdSet.Builder builder = new RoaringDocIdSet.Builder(context.reader().maxDoc());
+    scorer.score(new LeafCollector() {
+
+      @Override
+      public void setScorer(Scorer scorer) throws IOException {}
+
+      @Override
+      public void collect(int doc) throws IOException {
+        builder.add(doc);
+      }
+
+    }, null);
+    return builder.build();
+  }
+
   /**
    * Returns a {@link Scorer} which can iterate in order over all matching
    * documents and assign them a score.
diff --git a/lucene/test-framework/src/java/org/apache/lucene/search/AssertingWeight.java b/lucene/test-framework/src/java/org/apache/lucene/search/AssertingWeight.java
index 29ca12e..0bc8eb0 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/search/AssertingWeight.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/search/AssertingWeight.java
@@ -72,4 +72,11 @@ class AssertingWeight extends Weight {
 
     return AssertingBulkScorer.wrap(new Random(random.nextLong()), inScorer, context.reader().maxDoc());
   }
+
+  @Override
+  public DocIdSet computeMatchingDocs(LeafReaderContext context) throws IOException {
+    DocIdSet set = in.computeMatchingDocs(context);
+    assert set != null;
+    return set;
+  }
 }
