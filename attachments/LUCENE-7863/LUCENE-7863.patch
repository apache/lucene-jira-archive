diff --git a/lucene/benchmark/conf/derivative-terms-only.alg b/lucene/benchmark/conf/derivative-terms-only.alg
new file mode 100644
index 0000000..82be2e7
--- /dev/null
+++ b/lucene/benchmark/conf/derivative-terms-only.alg
@@ -0,0 +1,91 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+#
+# based on wiki 
+#
+# modified to use wikipedia sources and index entire docs
+# currently just used to measure ingest rate
+
+#ram.flush.mb=flush:1000:50
+ram.flush.mb=flush:50
+
+max.field.length=2147483647
+
+compound=false
+
+analyzer=org.apache.lucene.benchmark.utils.NoUrlsButEdgesAnalyzer
+directory=FSDirectory
+#default.codec=cdc                   :org.apache.lucene.codecs.lucene70.Lucene70Codec:org.apache.lucene.benchmark.utils.DeriveBodyRevCodec
+#content.source=src                     :org.apache.lucene.benchmark.byTask.feeds.EnwikiEdgeContentSource:org.apache.lucene.benchmark.byTask.feeds.EnwikiEmptyEdgeContentSource
+#work.dir=work dir :edge:deriv
+default.codec=cdc                   :org.apache.lucene.benchmark.utils.DeriveBodyRevCodec
+content.source=src                     :org.apache.lucene.benchmark.byTask.feeds.EnwikiEmptyEdgeContentSource
+work.dir=work dir :deriv
+#writer.info.stream=SystemOut
+
+doc.stored=false
+doc.tokenized=true
+doc.term.vector=false
+log.step=5000
+
+docs.file=temp/enwiki-20070527-pages-articles.xml
+
+
+#to inject _rev field placeholder 
+doc.index.props=true
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.InfixesQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=false
+print.hits.field=docid
+# -------------------------------------------------------------------------------------
+
+{ "Rounds"
+
+    ResetSystemErase
+
+    { "Populate"
+        CreateIndex
+        { "index"
+            { "MAddDocs" AddDoc > : 500
+            #0000
+            CloseIndex
+        }
+    }
+    
+    OpenReader  
+   
+#    [ 
+    {"search" Search > : 50 
+   
+    CloseReader 
+
+    NewRound
+
+    RepSumByPrefRound index
+    RepSumByPrefRound search
+#2
+} : 1
+
+#RepSumByName
+RepSumByPrefRound index
+RepSumByPrefRound search
diff --git a/lucene/benchmark/conf/derivative-terms-wikipedia.alg b/lucene/benchmark/conf/derivative-terms-wikipedia.alg
new file mode 100644
index 0000000..c15f1c8
--- /dev/null
+++ b/lucene/benchmark/conf/derivative-terms-wikipedia.alg
@@ -0,0 +1,87 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+#
+# based on wiki 
+#
+# modified to use wikipedia sources and index entire docs
+# currently just used to measure ingest rate
+
+#merge.factor=mrg:10:100:10:100
+#max.buffered=buf:10:10:100:100
+ram.flush.mb=flush:1000:50
+
+max.field.length=2147483647
+
+compound=false
+
+analyzer=org.apache.lucene.benchmark.utils.NoUrlsButEdgesAnalyzer
+directory=FSDirectory
+default.codec=cdc                   :org.apache.lucene.codecs.lucene70.Lucene70Codec:org.apache.lucene.benchmark.utils.DeriveBodyRevCodec
+content.source=src                     :org.apache.lucene.benchmark.byTask.feeds.EnwikiEdgeContentSource:org.apache.lucene.benchmark.byTask.feeds.EnwikiEmptyEdgeContentSource
+work.dir=work dir :edge:deriv
+
+doc.stored=false
+doc.tokenized=true
+doc.term.vector=false
+log.step=5000
+
+docs.file=temp/enwiki-20070527-pages-articles.xml
+
+
+#to inject _rev field placeholder 
+doc.index.props=true
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.InfixesQueryMaker
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=false
+print.hits.field=docid
+# -------------------------------------------------------------------------------------
+
+{ "Rounds"
+
+    ResetSystemErase
+
+    { "Populate"
+        CreateIndex
+        { "index"
+            { "MAddDocs" AddDoc > : 5000000
+            CloseIndex
+        }
+    }
+    
+    OpenReader  
+   
+#    [ 
+    {"search" Search > : 50 
+   
+    CloseReader 
+
+    NewRound
+
+    RepSumByPrefRound index
+    RepSumByPrefRound search
+
+} : 2
+
+#RepSumByName
+RepSumByPrefRound index
+RepSumByPrefRound search
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiEdgeContentSource.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiEdgeContentSource.java
new file mode 100644
index 0000000..98d013a
--- /dev/null
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiEdgeContentSource.java
@@ -0,0 +1,33 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.benchmark.byTask.feeds;
+
+import java.io.IOException;
+import java.util.Properties;
+
+public class EnwikiEdgeContentSource extends EnwikiContentSource {
+
+  @Override
+  public synchronized DocData getNextDocData(DocData docData) throws NoMoreDataException, IOException {
+    final DocData data = super.getNextDocData(docData);
+    Properties props = new Properties();
+      //index edges
+      props.put(EnwikiEmptyEdgeContentSource.BODY_EDGE, data.getBody());
+    data.setProps(props);
+    return data;
+  }
+}
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiEmptyEdgeContentSource.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiEmptyEdgeContentSource.java
new file mode 100644
index 0000000..91af0d1
--- /dev/null
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/EnwikiEmptyEdgeContentSource.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.benchmark.byTask.feeds;
+
+import java.io.IOException;
+import java.util.Properties;
+
+public class EnwikiEmptyEdgeContentSource extends EnwikiContentSource {
+
+  public static final String BODY_EDGE = org.apache.lucene.benchmark.byTask.feeds.DocMaker.BODY_FIELD+"_edge";
+
+  @Override
+  public synchronized DocData getNextDocData(DocData docData) throws NoMoreDataException, IOException {
+    final DocData data = super.getNextDocData(docData);
+    Properties props = new Properties();
+      // we need a placeholder to trigger auto reverse  
+      props.put(BODY_EDGE, "f");
+    data.setProps(props);
+    return data;
+  }
+}
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/InfixesQueryMaker.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/InfixesQueryMaker.java
new file mode 100644
index 0000000..8c0e146
--- /dev/null
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/InfixesQueryMaker.java
@@ -0,0 +1,103 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.benchmark.byTask.feeds;
+
+import java.util.LinkedList;
+import java.util.List;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.LowerCaseFilter;
+import org.apache.lucene.analysis.StopFilter;
+import org.apache.lucene.analysis.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.miscellaneous.LengthFilter;
+import org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.analysis.standard.StandardFilter;
+import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+
+public class InfixesQueryMaker extends AbstractQueryMaker {
+
+  private static String[] STANDARD_QUERIES = { "Images catbox gif",
+      "Imunisasi haram", "Favicon ico", "Michael jackson", "Unknown artist",
+      "Lily Thai", "Neda", "The Last Song", "Metallica", "Nicola Tesla",
+      "Max B", "Skil Corporation", "\"The 100 Greatest Artists of All Time\"",
+      "\"Top 100 Global Universities\"", "Pink floyd", "Bolton Sullivan",
+      "Frank Lucas Jr", "Drake Woods", "Radiohead", "George Freeman",
+      "Oksana Grigorieva", "The Elder Scrolls V", "Deadpool", "Green day",
+      "\"Red hot chili peppers\"", "Jennifer Bini Taylor",
+      "The Paradiso Girls", "Queen", "3Me4Ph", "Paloma Jimenez", "AUDI A4",
+      "Edith Bouvier Beale: A Life In Pictures", "\"Skylar James Deleon\"",
+      "Simple Explanation", "Juxtaposition", "The Woody Show", "London WITHER",
+      "In A Dark Place", "George Freeman", "LuAnn de Lesseps", "Muhammad.",
+      "U2", "List of countries by GDP", "Dean Martin Discography", "Web 3.0",
+      "List of American actors", "The Expendables",
+      "\"100 Greatest Guitarists of All Time\"", "Vince Offer.",
+      "\"List of ZIP Codes in the United States\"", "Blood type diet",
+      "Jennifer Gimenez", "List of hobbies", "The beatles", "Acdc",
+      "Nightwish", "Iron maiden", "Murder Was the Case", "Pelvic hernia",
+      "Naruto Shippuuden", "campaign", "Enthesopathy of hip region",
+      "operating system", "mouse",
+      "List of Xbox 360 games without region encoding", "Shakepearian sonnet",
+      "\"The Monday Night Miracle\"", "India", "Dad's Army",
+      "Solanum melanocerasum", "\"List of PlayStation Portable Wi-Fi games\"",
+      "Little Pixie Geldof", "Planes, Trains & Automobiles", "Freddy Ingalls",
+      "The Return of Chef", "Nehalem", "Turtle", "Calculus", "Superman-Prime",
+      "\"The Losers\"", "pen-pal", "Audio stream input output", "lifehouse",
+      "50 greatest gunners", "Polyfecalia", "freeloader", "The Filthy Youth" ,
+      "night","trading", "ford","credit"  };
+  
+  @Override
+  protected Query[] prepareQueries() throws Exception {
+    List<Query> rez = new LinkedList<Query>();
+    try(Analyzer analyzer = getEdgeAnalyzer()){
+      for (String text:STANDARD_QUERIES) {
+        final TokenStream stream = analyzer.tokenStream("body", text);
+        final CharTermAttribute term = stream.addAttribute(CharTermAttribute.class);
+        stream.reset();
+        while(stream.incrementToken()) {
+          rez.add(new PrefixQuery(new Term(EnwikiEmptyEdgeContentSource.BODY_EDGE,term.toString())));
+        }
+        stream.close();
+      }
+    }
+    return rez.toArray(new Query[] {});
+  }
+
+  private Analyzer getEdgeAnalyzer() {
+    return new StopwordAnalyzerBase(StandardAnalyzer.ENGLISH_STOP_WORDS_SET) {
+
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        final Tokenizer src = new StandardTokenizer();
+
+        TokenStream tok = new StandardFilter(src);
+        tok = new LowerCaseFilter(tok);
+        tok = new StopFilter(tok, stopwords);
+        tok = new EdgeNGramTokenFilter(tok, 3, 10);
+        tok = new LengthFilter(tok,3, 20);
+        return new TokenStreamComponents(src, tok);
+      }
+    };
+  }
+
+}
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/TaskStats.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/TaskStats.java
index ab116a9..6915591 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/TaskStats.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/stats/TaskStats.java
@@ -17,7 +17,10 @@
 package org.apache.lucene.benchmark.byTask.stats;
 
 
+import java.io.IOException;
+
 import org.apache.lucene.benchmark.byTask.tasks.PerfTask;
+import org.apache.lucene.store.Directory;
 
 /**
  * Statistics for a task run. 
@@ -61,6 +64,8 @@ public class TaskStats implements Cloneable {
    * Used when summing up on few runs/instances of similar tasks.
    */
   private int numRuns = 1;
+
+  private long dirFilesLength;
   
   /**
    * Create a run data for a task that is starting now.
@@ -90,6 +95,16 @@ public class TaskStats implements Cloneable {
     }
     this.numParallelTasks = numParallelTasks;
     this.count = count;
+    try {
+      final Directory directory = task.getRunData().getDirectory();
+      long sum=0;
+      for (String f : directory.listAll()) {
+        sum += directory.fileLength(f);
+      }
+      dirFilesLength = sum;
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
   }
   
   private int[] countsByTime;
@@ -115,6 +130,11 @@ public class TaskStats implements Cloneable {
     return taskRunNum;
   }
 
+  /** directory size in bytes */
+  public long getDirFilesLength() {
+    return dirFilesLength;
+  }
+  
   /* (non-Javadoc)
    * @see java.lang.Object#toString()
    */
@@ -191,6 +211,8 @@ public class TaskStats implements Cloneable {
       round = -1; // no meaning if aggregating tasks of different round. 
     }
 
+    dirFilesLength += stat2.getDirFilesLength();
+    
     if (countsByTime != null && stat2.countsByTime != null) {
       if (countsByTimeStepMSec != stat2.countsByTimeStepMSec) {
         throw new IllegalStateException("different by-time msec step");
@@ -202,6 +224,7 @@ public class TaskStats implements Cloneable {
         countsByTime[i] += stat2.countsByTime[i];
       }
     }
+    dirFilesLength = Math.max(dirFilesLength, stat2.dirFilesLength);
   }
 
   /* (non-Javadoc)
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java
index 4ffad3d..72c1fb2 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java
@@ -128,6 +128,7 @@ public abstract class ReadTask extends PerfTask {
         if (hits != null) {
           final String printHitsField = getRunData().getConfig().get("print.hits.field", null);
           if (printHitsField != null && printHitsField.length() > 0) {
+            System.out.println(q);
             System.out.println("totalHits = " + hits.totalHits);
             System.out.println("maxDoc()  = " + reader.maxDoc());
             System.out.println("numDocs() = " + reader.numDocs());
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReportTask.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReportTask.java
index 844fbce..3a2ad01 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReportTask.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReportTask.java
@@ -67,15 +67,16 @@ public abstract class ReportTask extends PerfTask {
   protected static final String ELAPSED =     "  elapsedSec";
   protected static final String USEDMEM =     "    avgUsedMem";
   protected static final String TOTMEM =      "    avgTotalMem";
+  private static final String DIRSIZE = " directory size, Mb";
   protected static final String COLS[] = {
       RUNCNT,
       RECCNT,
       RECSEC,
       ELAPSED,
       USEDMEM,
-      TOTMEM
+      TOTMEM,
+      DIRSIZE
   };
-
   /**
    * Compute a title line for a report table
    * @param longestOp size of longest op name in the table
@@ -130,6 +131,7 @@ public abstract class ReportTask extends PerfTask {
     sb.append(Format.format(2, (float) stat.getElapsed() / 1000, ELAPSED));
     sb.append(Format.format(0, (float) stat.getMaxUsedMem() / stat.getNumRuns(), USEDMEM)); 
     sb.append(Format.format(0, (float) stat.getMaxTotMem() / stat.getNumRuns(), TOTMEM));
+    sb.append(Format.format(2, (float) (stat.getDirFilesLength() / (1024*1024)), DIRSIZE));
     return sb.toString();
   }
 
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Config.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Config.java
index b7afcf2..024e55b 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Config.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Config.java
@@ -423,7 +423,7 @@ public class Config {
         } else if (a instanceof String[]) {
           String ad[] = (String[]) a;
           int n = roundNum % ad.length;
-          sb.append(ad[n]);
+          sb.append(stripPackage(ad[n]));
         } else {
           boolean ab[] = (boolean[]) a;
           int n = roundNum % ab.length;
@@ -434,6 +434,14 @@ public class Config {
     return sb.toString();
   }
 
+  private String stripPackage(String val) {
+    int dot = val.lastIndexOf(".");
+    if (dot>0) {
+      return " "+val.substring(dot+1);
+    } else
+      return val;
+  }
+
   /**
    * @return the round number.
    */
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/utils/DeriveBodyRevCodec.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/utils/DeriveBodyRevCodec.java
new file mode 100644
index 0000000..08d1788
--- /dev/null
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/utils/DeriveBodyRevCodec.java
@@ -0,0 +1,29 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.benchmark.utils;
+
+import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
+import org.apache.lucene.codecs.derivativeterms.TermDerivatives;
+import org.apache.lucene.codecs.derivativeterms.TermsDerivingCodec;
+
+public class DeriveBodyRevCodec extends TermsDerivingCodec {
+
+  public DeriveBodyRevCodec() {
+    super(TermDerivatives.edge_original, "_edge", DocMaker.BODY_FIELD);
+  }
+
+}
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/utils/NoUrlsButEdgesAnalyzer.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/utils/NoUrlsButEdgesAnalyzer.java
new file mode 100644
index 0000000..3352231
--- /dev/null
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/utils/NoUrlsButEdgesAnalyzer.java
@@ -0,0 +1,59 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.benchmark.utils;
+
+import java.util.Arrays;
+import java.util.HashSet;
+
+import org.apache.lucene.analysis.LowerCaseFilter;
+import org.apache.lucene.analysis.StopFilter;
+import org.apache.lucene.analysis.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.TypeTokenFilter;
+import org.apache.lucene.analysis.miscellaneous.LengthFilter;
+import org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter;
+import org.apache.lucene.analysis.reverse.ReverseStringFilter;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.analysis.standard.StandardFilter;
+import org.apache.lucene.analysis.standard.UAX29URLEmailTokenizer;
+import org.apache.lucene.benchmark.byTask.feeds.EnwikiEmptyEdgeContentSource;
+
+public class NoUrlsButEdgesAnalyzer extends StopwordAnalyzerBase {
+
+  public NoUrlsButEdgesAnalyzer() {
+    super(PER_FIELD_REUSE_STRATEGY, StandardAnalyzer.STOP_WORDS_SET);
+  }
+  
+  @Override
+  protected TokenStreamComponents createComponents(String fieldName) {
+    final Tokenizer src = new UAX29URLEmailTokenizer();
+    
+    TokenStream tok = new StandardFilter(src);
+    tok = new TypeTokenFilter(tok, new HashSet<>(Arrays.asList("<URL>","<EMAIL>")));
+    tok = new LengthFilter(tok,1, StandardAnalyzer.DEFAULT_MAX_TOKEN_LENGTH);
+    tok = new LowerCaseFilter(tok);
+    tok = new StopFilter(tok, stopwords);
+    if (fieldName.equals(EnwikiEmptyEdgeContentSource.BODY_EDGE)) {
+      tok = new ReverseStringFilter(tok);
+      tok = new EdgeNGramTokenFilter(tok,1, StandardAnalyzer.DEFAULT_MAX_TOKEN_LENGTH);
+      tok = new ReverseStringFilter(tok);
+    }
+    return new TokenStreamComponents(src, tok);
+  }
+
+}
diff --git a/lucene/benchmark/src/java/org/apache/lucene/codecs/derivativeterms/TermDerivatives.java b/lucene/benchmark/src/java/org/apache/lucene/codecs/derivativeterms/TermDerivatives.java
new file mode 100644
index 0000000..44643a3
--- /dev/null
+++ b/lucene/benchmark/src/java/org/apache/lucene/codecs/derivativeterms/TermDerivatives.java
@@ -0,0 +1,91 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.derivativeterms;
+
+import java.io.IOException;
+import java.io.StringReader;
+import java.util.function.Consumer;
+import java.util.function.Supplier;
+
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
+import org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter;
+import org.apache.lucene.analysis.reverse.ReverseStringFilter;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.codecs.derivativeterms.TermsDerivingCodec.TermMapper;
+import org.apache.lucene.util.BytesRef;
+
+public class TermDerivatives {
+
+  /** yields reversed terms */
+  public static final Supplier<TermMapper> reverse = () -> {return new TermMapper() {
+    
+    final private Tokenizer in = new KeywordTokenizer();
+    final private TokenFilter out = new ReverseStringFilter(in);
+    final private CharTermAttribute charTerm = out.addAttribute(CharTermAttribute.class);
+    
+    @Override
+    public void map(BytesRef input, Consumer<BytesRef> output) throws IOException {
+      in.setReader(new StringReader(input.utf8ToString()));
+      out.reset();
+      while (out.incrementToken()) {
+        BytesRef copy = new BytesRef(charTerm);
+        output.accept(copy);
+      }
+      out.end();
+      out.close();
+    }
+    
+  };
+  };
+  /** yields edgengramms concatenated with original term*/
+  public static final Supplier<TermMapper> edge_original = () -> {return new TermMapper() {
+    
+    final private KeywordTokenizer in = new KeywordTokenizer();
+    final private TokenFilter out = new ReverseStringFilter(
+                                          new EdgeNGramTokenFilter(
+                                              new ReverseStringFilter(in), 1, 255));
+    final private CharTermAttribute charTerm = out.addAttribute(CharTermAttribute.class);
+    final private StringBuilder buffer = new StringBuilder();
+    
+    @Override
+    public void map(BytesRef input, Consumer<BytesRef> output) throws IOException {
+      final String inputString = input.utf8ToString();
+      in.setReader(new StringReader(inputString));
+      out.reset();
+      while (out.incrementToken()) {
+        buffer.setLength(0);
+        buffer.append(charTerm);
+       // buffer.append(inputString);
+        BytesRef copy = new BytesRef(buffer);
+        
+        //System.out.println(copy.utf8ToString());
+        
+        output.accept(copy);
+      }
+      out.end();
+      out.close();
+    }
+    
+  };
+  };
+
+  private TermDerivatives() {}
+  
+  
+}
diff --git a/lucene/benchmark/src/test/org/apache/lucene/benchmark/utils/NoUrlsAnalyzerTest.java b/lucene/benchmark/src/test/org/apache/lucene/benchmark/utils/NoUrlsAnalyzerTest.java
new file mode 100644
index 0000000..2444390
--- /dev/null
+++ b/lucene/benchmark/src/test/org/apache/lucene/benchmark/utils/NoUrlsAnalyzerTest.java
@@ -0,0 +1,86 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.benchmark.utils;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Iterator;
+import java.util.List;
+import java.util.stream.Collectors;
+import java.util.stream.IntStream;
+import java.util.stream.Stream;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.benchmark.byTask.feeds.DocMaker;
+import org.apache.lucene.benchmark.byTask.feeds.EnwikiEmptyEdgeContentSource;
+import org.apache.lucene.util.LuceneTestCase;
+
+public class NoUrlsAnalyzerTest extends LuceneTestCase{
+
+  public void testNoURLs() throws IOException {
+    try( Analyzer analyzer = new NoUrlsButEdgesAnalyzer()) {
+      final TokenStream tokens = analyzer.tokenStream("body", "Encyclopædia Britannica. 2006. Encyclopædia Britannica Premium Service. [[29 August]] [[2006]] &lt;http://www.britannica.com/eb/article-9117285&gt;. Anarchism is &quot;a cluster of doctrines and attitudes centred ");
+      CharTermAttribute term = tokens.addAttribute(CharTermAttribute.class);
+      tokens.reset();
+      while(tokens.incrementToken()) {
+        final String token = term.toString();
+        assertFalse(token, token.contains("http"));
+      }  
+    }
+  }
+  
+  public void testEdges() throws IOException {
+    try (Analyzer analyzer = new NoUrlsButEdgesAnalyzer()) {
+      final List<String> qbf = Arrays.asList("quick", "brown", "fox");
+      assertTokensOnFields(analyzer, "the quick brown fox",
+          new String[] {DocMaker.BODY_FIELD},
+          qbf);
+
+      final List<String> edgeTokens = qbf.stream().flatMap((t) -> {
+        return tailEdges(t);
+      }).collect(Collectors.toList());
+      Arrays.asList("quick", "brown", "fox");
+      assertTokensOnFields(analyzer, "the quick brown fox",
+          new String[] {EnwikiEmptyEdgeContentSource.BODY_EDGE},
+          edgeTokens);
+    }
+  }
+
+  Stream<String> tailEdges(String t) {
+    return IntStream.range(1, t.length()+1)
+        .<String>mapToObj((i) -> {
+          return t.substring(t.length() - i, t.length());
+        });
+  }
+
+  void assertTokensOnFields(Analyzer analyzer, final String text, final String[] fields,
+      final List<String> tokensExpected) throws IOException {
+    for (String field:fields) {
+      final TokenStream tokens = analyzer.tokenStream(field, text);
+      CharTermAttribute term = tokens.addAttribute(CharTermAttribute.class);
+      tokens.reset();
+      final Iterator<String> qbf = tokensExpected.iterator();
+      while(tokens.incrementToken()) {
+        final String token = term.toString();
+        assertEquals(qbf.next(), token);
+      }  
+      tokens.close();
+    }
+  }
+}
diff --git a/lucene/benchmark/src/test/org/apache/lucene/codecs/derivativeterms/TestDerivedNGramms.java b/lucene/benchmark/src/test/org/apache/lucene/codecs/derivativeterms/TestDerivedNGramms.java
new file mode 100644
index 0000000..289a0a9
--- /dev/null
+++ b/lucene/benchmark/src/test/org/apache/lucene/codecs/derivativeterms/TestDerivedNGramms.java
@@ -0,0 +1,162 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.derivativeterms;
+
+
+import java.io.IOException;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.MultiDocValues;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.SortField;
+import org.apache.lucene.search.SortField.Type;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.LuceneTestCase.SuppressSysoutChecks;
+
+
+@SuppressSysoutChecks(bugUrl = "foo.bar")
+public class TestDerivedNGramms extends LuceneTestCase {
+  
+  private Field field = newField("field", "", StringField.TYPE_NOT_STORED);
+  private NumericDocValuesField idField = new NumericDocValuesField("id", 0);
+  private int id;
+  private IndexReader ir;
+
+  public void testBasic() throws Exception {
+    
+    Directory dir = newDirectory();
+    final Random r = random();
+    final IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(r));
+    
+    iwc.setCodec(new TermsDerivingCodec(TermDerivatives.edge_original, "_edge","field"));
+    
+    RandomIndexWriter iw = new RandomIndexWriter(r, dir, iwc);
+    Document doc = new Document();
+    Field field_rev = newField("field_edge", "", StringField.TYPE_NOT_STORED);
+    doc.add(field);
+   // doc.add(idField);
+    doc.add(field_rev);
+    doc.add(field_rev);
+
+    doc.add(idField);
+   
+    field_rev.setStringValue("f");
+    final int dupes = atLeast(4);
+    String abcSuffix = rarely() ? "d" : "";
+    String ebcSuffix = rarely() ? "f" : "";
+    for (int i=0;i<dupes;i++) {
+      addDoc(iw, doc,  "abc"+abcSuffix);
+      addDoc(iw, doc,  "ijk");
+      addDoc(iw, doc,  "ebc"+ebcSuffix);
+      addDoc(iw, doc,  "klm");
+    }
+    
+    final int prefixes = atLeast(2000);
+    int radix = Math.min(atLeast(16),Character.MAX_RADIX);
+    for (int i=0; i<prefixes;i++) {
+      addDoc(iw, doc, Integer.toString(i, radix).toUpperCase()+"gh");
+      if (i%2==0) {
+        addDoc(iw, doc, Integer.toString(i, radix).toUpperCase()+"xy");
+      }
+    }
+    ir = iw.getReader();
+    iw.close();
+    
+    IndexSearcher is = newSearcher(ir);
+    
+    NumericDocValues ndv = MultiDocValues.getNumericValues(ir, "id");
+    { // two terms matches *bc*
+      TopDocs td = is.search(new PrefixQuery(new Term("field_edge", new BytesRef("bc"))), 5*dupes,
+          new Sort(new SortField("id", Type.LONG))
+          );
+      assertEquals(2*dupes, td.totalHits);
+      for (int i=0;i<dupes;i++) {
+        assertEquals(0+(4*i),id(ndv,td.scoreDocs[0+(i*2)].doc ))//ir.document(
+            ;
+        assertEquals(2+(4*i),id(ndv,td.scoreDocs[1+(i*2)].doc )
+            );
+      }
+    }
+    
+    { // there is only single match for *ebc* *bcf*
+      TopDocs td = is.search(new PrefixQuery(new Term("field_edge", 
+          new BytesRef(ebcSuffix.length()>0 && random().nextBoolean()? "bc"+ebcSuffix :"ebc"))), 5*dupes);
+      assertEquals(1*dupes, td.totalHits);
+      for (int i=0;i<dupes;i++) {
+        assertEquals(2+(4*i), id(ndv,td.scoreDocs[0+i].doc));
+      }
+    }
+    
+    { // no reverse
+      TopDocs td = is.search(new PrefixQuery(new Term("field_edge", 
+          new BytesRef(random().nextBoolean() ? "cb" : "fc"))), 5);
+      assertEquals(0, td.totalHits);
+    }
+    
+    { // two terms matches *k*
+      TopDocs td = is.search(new PrefixQuery(new Term("field_edge", new BytesRef("k"))), 5*dupes);
+      assertEquals(2*dupes, td.totalHits);
+      for (int i=0;i<dupes;i++) {
+        assertEquals(1+(4*i), id(ndv, td.scoreDocs[0+(i*2)].doc));
+        assertEquals(3+(4*i), id(ndv, td.scoreDocs[1+(i*2)].doc));
+      }
+    }
+    { 
+      TopDocs td = is.search(new PrefixQuery(new Term("field_edge", 
+          new BytesRef( "gh"))), prefixes);
+      assertEquals(prefixes, td.totalHits);
+    }
+    { 
+      TopDocs td = is.search(new PrefixQuery(new Term("field_edge", 
+          new BytesRef( "xy"))), prefixes);
+      assertEquals(prefixes/2, td.totalHits);
+    }
+    ir.close();
+    dir.close();
+  }
+
+  private int id(NumericDocValues ndv, int docNum) throws IOException {
+    if (ndv.docID()>docNum) {
+      ndv = MultiDocValues.getNumericValues(ir, "id");
+    }
+    if(ndv.docID()!=docNum) {
+      ndv.advance(docNum);
+    }
+    return (int) ndv.longValue();
+  }
+
+  protected void addDoc(RandomIndexWriter iw, Document doc, final String value) throws IOException {
+    field.setStringValue(value);
+    idField.setLongValue(id++);
+    iw.addDocument(doc);
+  }
+}
diff --git a/lucene/benchmark/src/test/org/apache/lucene/codecs/derivativeterms/TestDerivedTerms.java b/lucene/benchmark/src/test/org/apache/lucene/codecs/derivativeterms/TestDerivedTerms.java
new file mode 100644
index 0000000..2a97e32
--- /dev/null
+++ b/lucene/benchmark/src/test/org/apache/lucene/codecs/derivativeterms/TestDerivedTerms.java
@@ -0,0 +1,98 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.derivativeterms;
+
+
+import java.io.IOException;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+
+public class TestDerivedTerms extends LuceneTestCase {
+  
+  public void testBasic() throws Exception {
+    Directory dir = newDirectory();
+    final Random r = random();
+    final IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(r));
+    
+    iwc.setCodec(new TermsDerivingCodec(TermDerivatives.reverse, "_rev","field"));
+    
+    RandomIndexWriter iw = new RandomIndexWriter(r, dir, iwc);
+    Document doc = new Document();
+    Field field = newField("field", "", StringField.TYPE_NOT_STORED);
+    Field field_rev = newField("field_rev", "", StringField.TYPE_NOT_STORED);
+    doc.add(field);
+    doc.add(field_rev);
+    
+    field_rev.setStringValue("");
+    addDoc(iw, doc, field, "ABC");
+    addDoc(iw, doc, field, "abc");
+    addDoc(iw, doc, field, "abcdef");
+    addDoc(iw, doc, field, "ijk");
+    addDoc(iw, doc, field, "ebc");
+    
+    
+    IndexReader ir = iw.getReader();
+    iw.close();
+    
+    IndexSearcher is = newSearcher(ir);
+    
+    assert_rev(is, 1, "cba");
+    assert_rev(is, 0, "CBA");
+    assert_rev(is, 2, "fedcba");
+    assert_rev(is, 3, "kji");
+    assert_rev(is, "abc");
+    
+    TopDocs td = is.search(new PrefixQuery(new Term("field_rev", new BytesRef("cb"))), 5);
+    assertEquals(2, td.totalHits);
+    assertEquals(1, td.scoreDocs[0].doc);
+    assertEquals(4, td.scoreDocs[1].doc);
+    
+    ir.close();
+    dir.close();
+  }
+
+  protected void assert_rev(IndexSearcher is, final int i, final String text) throws IOException {
+    TopDocs td = is.search(new TermQuery( new Term("field_rev", new BytesRef(text))), 5);
+    assertEquals(i, td.scoreDocs[0].doc);
+    assertEquals(1, td.totalHits);
+  }
+  
+  protected void assert_rev(IndexSearcher is, final String text) throws IOException {
+    TopDocs td = is.search(new TermQuery( new Term("field_rev", new BytesRef(text))), 5);
+    assertEquals(0, td.totalHits);
+  }
+
+  protected void addDoc(RandomIndexWriter iw, Document doc, Field field, final String value) throws IOException {
+    field.setStringValue(value);
+    iw.addDocument(doc);
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/analysis/StopwordAnalyzerBase.java b/lucene/core/src/java/org/apache/lucene/analysis/StopwordAnalyzerBase.java
index c35e715..e4df6d5 100644
--- a/lucene/core/src/java/org/apache/lucene/analysis/StopwordAnalyzerBase.java
+++ b/lucene/core/src/java/org/apache/lucene/analysis/StopwordAnalyzerBase.java
@@ -36,6 +36,13 @@ public abstract class StopwordAnalyzerBase extends Analyzer {
    */
   protected final CharArraySet stopwords;
 
+  public StopwordAnalyzerBase(ReuseStrategy reuseStrategy, final CharArraySet stopwords) {
+    super(reuseStrategy);
+    // analyzers should use char array set for stopwords!
+    this.stopwords = stopwords == null ? CharArraySet.EMPTY_SET : CharArraySet
+        .unmodifiableSet(CharArraySet.copy(stopwords));
+  }
+
   /**
    * Returns the analyzer's stopword set or an empty set if the analyzer has no
    * stopwords
@@ -54,9 +61,7 @@ public abstract class StopwordAnalyzerBase extends Analyzer {
    *          the analyzer's stopword set
    */
   protected StopwordAnalyzerBase(final CharArraySet stopwords) {
-    // analyzers should use char array set for stopwords!
-    this.stopwords = stopwords == null ? CharArraySet.EMPTY_SET : CharArraySet
-        .unmodifiableSet(CharArraySet.copy(stopwords));
+      this(GLOBAL_REUSE_STRATEGY,stopwords);
   }
 
   /**
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/PushPostingsWriterBase.java b/lucene/core/src/java/org/apache/lucene/codecs/PushPostingsWriterBase.java
index 1fb83b9..333cbf6 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/PushPostingsWriterBase.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/PushPostingsWriterBase.java
@@ -116,6 +116,10 @@ public abstract class PushPostingsWriterBase extends PostingsWriterBase {
     return 0;
   }
 
+  public FieldInfo getField() {
+    return fieldInfo;
+  }
+  
   @Override
   public final BlockTermState writeTerm(BytesRef term, TermsEnum termsEnum, FixedBitSet docsSeen) throws IOException {
     startTerm();
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java
index bdacc22..aae3887 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java
@@ -188,7 +188,7 @@ import org.apache.lucene.util.fst.Util;
  * @see BlockTreeTermsReader
  * @lucene.experimental
  */
-public final class BlockTreeTermsWriter extends FieldsConsumer {
+public class BlockTreeTermsWriter extends FieldsConsumer {
 
   /** Suggested default value for the {@code
    *  minItemsInBlock} parameter to {@link
@@ -330,7 +330,8 @@ public final class BlockTreeTermsWriter extends FieldsConsumer {
       }
 
       TermsEnum termsEnum = terms.iterator();
-      TermsWriter termsWriter = new TermsWriter(fieldInfos.fieldInfo(field));
+      final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+      TermsWriter termsWriter = createTermsWriter(fieldInfo);
       while (true) {
         BytesRef term = termsEnum.next();
         //if (DEBUG) System.out.println("BTTW: next term " + term);
@@ -348,6 +349,10 @@ public final class BlockTreeTermsWriter extends FieldsConsumer {
       //if (DEBUG) System.out.println("\nBTTW.write done seg=" + segment + " field=" + field);
     }
   }
+
+  protected TermsWriter createTermsWriter(final FieldInfo fieldInfo) {
+    return new TermsWriter(fieldInfo);
+  }
   
   static long encodeOutput(long fp, boolean hasTerms, boolean isFloor) {
     assert fp < (1L << 62);
@@ -508,11 +513,11 @@ public final class BlockTreeTermsWriter extends FieldsConsumer {
 
   static final BytesRef EMPTY_BYTES_REF = new BytesRef();
 
-  class TermsWriter {
+  public class TermsWriter {
     private final FieldInfo fieldInfo;
     private final int longsSize;
     private long numTerms;
-    final FixedBitSet docsSeen;
+    protected final FixedBitSet docsSeen;
     long sumTotalTermFreq;
     long sumDocFreq;
     long indexStartFP;
@@ -723,8 +728,8 @@ public final class BlockTreeTermsWriter extends FieldsConsumer {
           // Write term meta data
           postingsWriter.encodeTerm(longs, bytesWriter, fieldInfo, state, absolute);
           for (int pos = 0; pos < longsSize; pos++) {
-            assert longs[pos] >= 0;
-            metaWriter.writeVLong(longs[pos]);
+            //assert longs[pos] >= 0;
+            metaWriter.writeZLong(longs[pos]);
           }
           bytesWriter.writeTo(metaWriter);
           bytesWriter.reset();
@@ -774,8 +779,8 @@ public final class BlockTreeTermsWriter extends FieldsConsumer {
             // Write term meta data
             postingsWriter.encodeTerm(longs, bytesWriter, fieldInfo, state, absolute);
             for (int pos = 0; pos < longsSize; pos++) {
-              assert longs[pos] >= 0;
-              metaWriter.writeVLong(longs[pos]);
+              //assert longs[pos] >= 0;
+              metaWriter.writeZLong(longs[pos]);
             }
             bytesWriter.writeTo(metaWriter);
             bytesWriter.reset();
@@ -842,7 +847,7 @@ public final class BlockTreeTermsWriter extends FieldsConsumer {
       return new PendingBlock(prefix, startFP, hasTerms, isFloor, floorLeadLabel, subIndices);
     }
 
-    TermsWriter(FieldInfo fieldInfo) {
+    public TermsWriter(FieldInfo fieldInfo) {
       this.fieldInfo = fieldInfo;
       assert fieldInfo.getIndexOptions() != IndexOptions.NONE;
       docsSeen = new FixedBitSet(maxDoc);
@@ -861,7 +866,7 @@ public final class BlockTreeTermsWriter extends FieldsConsumer {
       }
       */
 
-      BlockTermState state = postingsWriter.writeTerm(text, termsEnum, docsSeen);
+      BlockTermState state = writePosting(text, termsEnum);
       if (state != null) {
 
         assert state.docFreq != 0;
@@ -882,6 +887,10 @@ public final class BlockTreeTermsWriter extends FieldsConsumer {
       }
     }
 
+    protected BlockTermState writePosting(BytesRef text, TermsEnum termsEnum) throws IOException {
+      return postingsWriter.writeTerm(text, termsEnum, docsSeen);
+    }
+
     /** Pushes the new term to the top of the stack, and writes new blocks. */
     private void pushTerm(BytesRef text) throws IOException {
       int limit = Math.min(lastTerm.length(), text.length);
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/blocktree/IntersectTermsEnumFrame.java b/lucene/core/src/java/org/apache/lucene/codecs/blocktree/IntersectTermsEnumFrame.java
index 578e145..47d64b6 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/blocktree/IntersectTermsEnumFrame.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/blocktree/IntersectTermsEnumFrame.java
@@ -293,7 +293,7 @@ final class IntersectTermsEnumFrame {
       }
       // metadata 
       for (int i = 0; i < ite.fr.longsSize; i++) {
-        longs[i] = bytesReader.readVLong();
+        longs[i] = bytesReader.readZLong();
       }
       ite.fr.parent.postingsReader.decodeTerm(longs, bytesReader, ite.fr.fieldInfo, termState, absolute);
 
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/blocktree/SegmentTermsEnumFrame.java b/lucene/core/src/java/org/apache/lucene/codecs/blocktree/SegmentTermsEnumFrame.java
index 0860b30..906d7d7 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/blocktree/SegmentTermsEnumFrame.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/blocktree/SegmentTermsEnumFrame.java
@@ -423,7 +423,7 @@ final class SegmentTermsEnumFrame {
       }
       // metadata 
       for (int i = 0; i < ste.fr.longsSize; i++) {
-        longs[i] = bytesReader.readVLong();
+        longs[i] = bytesReader.readZLong();
       }
       ste.fr.parent.postingsReader.decodeTerm(longs, bytesReader, ste.fr.fieldInfo, state, absolute);
 
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/ByteArrayDerivativeWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/ByteArrayDerivativeWriter.java
new file mode 100644
index 0000000..b6a9015
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/ByteArrayDerivativeWriter.java
@@ -0,0 +1,271 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.derivativeterms;
+
+import java.io.IOException;
+import java.util.AbstractMap;
+import java.util.AbstractSet;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.NoSuchElementException;
+import java.util.Set;
+import java.util.function.Supplier;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.derivativeterms.TermsDerivingCodec.TermMapper;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefArray.BytesRefIdxIterator;
+import org.apache.lucene.util.BytesRefBuilder;
+import org.apache.lucene.util.BytesRefHash;
+import org.apache.lucene.util.IntBlockPool;
+import org.apache.lucene.util.IntBlockPool.SliceReader;
+
+public class ByteArrayDerivativeWriter extends TreeMapDerivativeWriter {
+
+  private static final BytesRef separator = new BytesRef(""+'\u0001');
+
+  public ByteArrayDerivativeWriter(SegmentWriteState state, PostingsWriterBase postingsWriter,
+      Map<String,FieldData> termsByField, String fieldSuffix, Supplier<TermMapper> mapperFactory) throws IOException {
+    super(state, postingsWriter, termsByField, fieldSuffix, mapperFactory);
+  }
+
+  @Override
+  protected Map<BytesRef,BlockTermState> deriveTerms(FieldData stateByTerms) throws IOException {
+    final TermMapper termMapper = mapperFactory.get();
+    
+    BytesRefHash uniqNGramms = new BytesRefHash();
+    final int [] startsPosByNGrammId;
+    final int [] blockPosns;
+    
+      final BytesRefIdxIterator inputTermDict = stateByTerms.terms.iterator();
+      
+      IntBlockPool ngrammIdPool = new IntBlockPool();
+      final IntBlockPool.SliceWriter ngrammIdColumn = new IntBlockPool.SliceWriter(ngrammIdPool);
+      int ngrammIdSlice = ngrammIdColumn.startNewSlice();
+      
+      IntBlockPool blockPosPool = new IntBlockPool();
+      final IntBlockPool.SliceWriter blockPosColumn = new IntBlockPool.SliceWriter(blockPosPool);
+      int blockPosSlice = blockPosColumn.startNewSlice();
+      
+      BytesRef inputTerm;
+      
+      final int[] totalNGramms = new int[] {0};
+      
+      while ((inputTerm=inputTermDict.next())!=null) {
+        final BytesRef in = inputTerm;
+        termMapper.map(inputTerm,
+                         (t)->{
+                             int ngrammId = uniqNGramms.add(t);
+                             if (ngrammId>=0) {// new
+                             } else {
+                               ngrammId=-1-ngrammId;
+                             }
+                             ngrammIdColumn.writeInt(ngrammId);
+                             int index = inputTermDict.index();
+                             blockPosColumn.writeInt(index);
+                             totalNGramms[0]++; // I'm not sure how to extract counter from that fancy writer
+                             
+                             /*System.out.println(totalNGramms[0]+"\t"+in.utf8ToString()+"\t"+
+                                 t.utf8ToString()+"\t->\t["+ngrammId+",\t"+index+"]");*/
+                           }
+            );
+      }
+      startsPosByNGrammId = startsInDenseArray(
+          ngrammIdPool, ngrammIdSlice, ngrammIdColumn.getCurrentOffset(),
+          uniqNGramms);
+      int [] posInGroup = startsPosByNGrammId.clone();
+      SliceReader blockPosReader;
+      {
+        blockPosns = new int [totalNGramms[0]];
+        IntBlockPool.SliceReader ngrammIdReader = new IntBlockPool.SliceReader(ngrammIdPool);    
+        ngrammIdReader.reset(ngrammIdSlice, ngrammIdColumn.getCurrentOffset());
+        
+        blockPosReader = new IntBlockPool.SliceReader(blockPosPool);
+        blockPosReader.reset(blockPosSlice, blockPosColumn.getCurrentOffset());
+        
+        for(int i=0;!blockPosReader.endOfSlice(); i++) {
+          int ngramm = ngrammIdReader.readInt();
+          assert blockPosns[posInGroup[ngramm]]==0;
+          blockPosns[posInGroup[ngramm]++] = blockPosReader.readInt();
+          
+          assert blockPosReader.endOfSlice()==ngrammIdReader.endOfSlice();
+        }
+        /*
+        for (int i=0;i<startsPosByNGrammId.length-1;i++) {
+          BytesRef ngramm = uniqNGramms.get(i, new BytesRef());
+          System.out.print("\nngrammId:"+i+"\t"+ngramm.utf8ToString()+
+              " ["+startsPosByNGrammId[i]+".."+startsPosByNGrammId[i+1]+")=\t");
+          for(int p=startsPosByNGrammId[i];p<startsPosByNGrammId[i+1];p++) {
+            BytesRefBuilder spare = new BytesRefBuilder();
+            stateByTerms.terms.get(spare, blockPosns[p]);
+            System.out.print(blockPosns[p]+":"+spare.get().utf8ToString()+",");
+          }
+        }
+        System.out.println();*/
+      }
+    
+    final int[] sorted = uniqNGramms.sort();
+    final IntBlockPool sortedPosPool;
+    final int[] slicePoses;
+    {
+      sortedPosPool = new IntBlockPool();
+      final IntBlockPool.SliceWriter sortedPosColumn = new IntBlockPool.SliceWriter(sortedPosPool);
+      slicePoses = new int[uniqNGramms.size()*2];
+      for(int i=0; i<uniqNGramms.size(); i++) {
+        slicePoses[i*2] = sortedPosColumn.startNewSlice();
+        // write ngrammId, first
+        int ngrammId = sorted[i];
+        sortedPosColumn.writeInt(ngrammId);
+        //// DON'T write group length
+        int from = startsPosByNGrammId[ngrammId];
+        int to = startsPosByNGrammId[ngrammId+1];
+        //sortedPosColumn.writeInt(to-from);
+        // write whole group
+        for (int p=from; p<to;p++) {
+          sortedPosColumn.writeInt(blockPosns[p]);
+        }
+        slicePoses[i*2+1] = sortedPosColumn.getCurrentOffset();
+      }
+    }
+    return new AbstractMap<BytesRef,BlockTermState>() {
+
+      @Override
+      public Set<Entry<BytesRef,BlockTermState>> entrySet() {
+        return new AbstractSet<Map.Entry<BytesRef,BlockTermState>>() {
+
+          @Override
+          public Iterator<Entry<BytesRef,BlockTermState>> iterator() {
+            
+            return new Iterator<Map.Entry<BytesRef,BlockTermState>>() {
+              Entry<BytesRef,BlockTermState> scratch;
+              BytesRefBuilder outputBytes = new BytesRefBuilder();
+              
+              int posInGroup = 0;
+              int cnt=0;
+              int sliceNum=0;
+              boolean needReset = true;
+              int groupSize = 0;
+              int ngrammLength;
+              
+              SliceReader sortedPosReader = new IntBlockPool.SliceReader(sortedPosPool);
+              
+              BytesRef buff = new BytesRef();
+              @Override
+              public Entry<BytesRef,BlockTermState> next() {
+                if (hasNext()) {
+                    if (scratch==null) {
+                      scratch = new SimpleEntry<BytesRef,BlockTermState>(outputBytes.get(),null);
+                    }
+                    
+                    if(needReset) {
+                      sortedPosReader.reset(slicePoses[sliceNum*2],slicePoses[sliceNum*2+1]);
+                      int ngrammId = sortedPosReader.readInt();//sorted[ngrammIndex];
+                      uniqNGramms.get(ngrammId, buff);
+                      groupSize = 0;
+                      while(!sortedPosReader.endOfSlice()) { // how to get size of the current slice? 
+                        sortedPosReader.readInt();
+                        groupSize++;
+                      }
+                      sortedPosReader.reset(slicePoses[sliceNum*2],slicePoses[sliceNum*2+1]);
+                      {
+                        int ngramm2 = sortedPosReader.readInt();
+                        assert ngramm2==ngrammId;
+                      }
+                      assert scratch.getKey()!=null;
+                      
+                      outputBytes.clear();
+                      outputBytes.append(buff);
+                      outputBytes.append(separator);
+                      ngrammLength = outputBytes.length();
+                      posInGroup=0;
+                      needReset=false;
+                      sliceNum++;// next time, next slice;
+                    }
+                    outputBytes.setLength(ngrammLength);
+                    // add padded block pos
+                    for(int shift=24;shift>=0;shift-=8) {
+                      if ((groupSize>>shift&0xff)>0) {
+                        outputBytes.append((byte) (posInGroup>>shift&0xff));
+                      }
+                    }
+                    scratch.setValue(stateByTerms.blockStates[sortedPosReader.readInt()]);
+                    posInGroup++;
+                    cnt++;
+                    if(sortedPosReader.endOfSlice()) {
+                      needReset=true; 
+                    }
+                    /*
+                    BytesRef key = scratch.getKey();
+                    String r;
+                    try {
+                      r = key.utf8ToString();
+                    }catch (Throwable e) {
+                      r = key.toString();
+                    }
+                    System.out.println(cnt+"\t"+r+"\t"+scratch.getKey()
+                    +"\t->\t"+scratch.getValue());
+                    */
+                  return scratch;
+                }else {
+                  throw new NoSuchElementException("yeilded "+cnt+" of " + blockPosns.length +" already");
+                }
+              }
+
+              @Override
+              public boolean hasNext() {
+                return cnt < blockPosns.length;//ngramms.size();
+              }
+            };
+          }
+
+          @Override
+          public int size() {
+            return blockPosns.length;
+          }
+          
+        };
+      }
+      
+    };
+  }
+
+  /** @returns subj plus one meaningless trailing elem, to simplify following logic*/
+  private int[] startsInDenseArray(IntBlockPool ngrammIdPool, int from, int to, BytesRefHash uniqNGramms) {
+    int ngrams = uniqNGramms.size();
+    
+    int[] ngramData = new int[ngrams+1];
+    IntBlockPool.SliceReader reader = new IntBlockPool.SliceReader(ngrammIdPool);    
+    reader.reset(from, to);
+    while (!reader.endOfSlice()) {
+      int ngramm = reader.readInt();
+      ngramData[ngramm]++;
+    }
+    // now we have collision list sizes in array. 
+    // let's convert to starts
+    int pos = 0;
+    for (int i = 0; i<=ngrams; i++) {
+      int prePos = pos;
+      pos += ngramData[i];
+      ngramData[i] = prePos;
+    }
+    
+    return ngramData;
+  }
+  
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/FieldData.java b/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/FieldData.java
new file mode 100644
index 0000000..d0d9c4e
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/FieldData.java
@@ -0,0 +1,111 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.derivativeterms;
+
+import java.io.IOException;
+import java.util.AbstractMap;
+import java.util.AbstractSet;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.NoSuchElementException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefArray;
+import org.apache.lucene.util.BytesRefIterator;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/** this is not a map but entryset iterable and put */
+public class FieldData extends AbstractMap<BytesRef,BlockTermState>{
+
+  /* TODO steal it from injecting termsWriter */
+  FixedBitSet docsSeen;
+  
+  public FieldData(FixedBitSet docsSeen) {
+    super();
+    this.docsSeen = docsSeen;
+  }
+  
+  final BytesRefArray terms = new BytesRefArray(Counter.newCounter());
+   BlockTermState [] blockStates = new BlockTermState[0];
+  
+  public BlockTermState put(BytesRef key, BlockTermState value) {
+    final int pos = terms.append(key);
+    if (blockStates.length<=pos) {
+      BlockTermState[] tmpTermState = new BlockTermState[ArrayUtil.oversize(pos+1, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
+      System.arraycopy(blockStates, 0, tmpTermState, 0, blockStates.length);
+      blockStates = tmpTermState;
+    }
+    blockStates[pos] = value;
+    //System.out.println("["+pos+"] "+key.utf8ToString()+"="+value);
+    return null;
+  }
+
+  @Override
+  public Set<Entry<BytesRef,BlockTermState>> entrySet() {
+    return new AbstractSet<Map.Entry<BytesRef,BlockTermState>>() {
+
+      @Override
+      public Iterator<Entry<BytesRef,BlockTermState>> iterator() {
+        final BytesRefIterator iter = terms.iterator();
+
+        return new Iterator<Map.Entry<BytesRef,BlockTermState>>() {
+          int pos = 0;
+          SimpleEntry<BytesRef,BlockTermState> entryScratch;
+
+          @Override
+          public boolean hasNext() {
+            return pos<terms.size();
+          }
+
+          @Override
+          public Entry<BytesRef,BlockTermState> next() {
+            final BytesRef next;
+            try {
+              next = iter.next();
+              if (next==null) {
+                assert pos == terms.size();
+                throw new NoSuchElementException("yielded "+pos+" elems already");
+              } 
+            } catch (IOException e) {
+              throw new RuntimeException(e);
+            }
+            if (entryScratch==null) {
+              entryScratch = new SimpleEntry<BytesRef,BlockTermState>(next, null);
+            } else {
+              assert entryScratch.getKey()==next;
+            }
+            entryScratch.setValue(blockStates[pos++]);
+            //System.out.println(entryScratch);
+            return entryScratch;
+          }
+          
+        };
+      }
+
+      @Override
+      public int size() {
+        return terms.size();
+      }
+    };
+  }
+  
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/InjectingBlockTreeTermsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/InjectingBlockTreeTermsWriter.java
new file mode 100644
index 0000000..e05bd7d
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/InjectingBlockTreeTermsWriter.java
@@ -0,0 +1,222 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.derivativeterms;
+
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.FilterLeafReader.FilterFields;
+import org.apache.lucene.index.PostingsEnum;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+
+abstract class InjectingBlockTreeTermsWriter extends BlockTreeTermsWriter {
+  
+  final private Map<String,FieldData> termsByField;
+  
+  InjectingBlockTreeTermsWriter(SegmentWriteState state, PostingsWriterBase postingsWriter,
+                      Map<String,FieldData> termsByField ) throws IOException {
+    super(state, postingsWriter, BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE,
+        BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
+    this.termsByField = termsByField;
+  }
+
+  Fields decorate(Fields input) {
+    return new FilterFields(input) {
+      @Override
+      public Terms terms(String field) throws IOException {
+        final String originalFieldName = originalFieldName(field);
+        if (originalFieldName!=null) {
+          final FieldData source = termsByField.get(originalFieldName);
+          final Map<BytesRef,BlockTermState> derivedTermsForCurrentField = deriveTerms(source);
+          return new Terms() {
+            
+            @Override
+            public long size() throws IOException {
+              return derivedTermsForCurrentField.size();
+            }
+            
+            @Override
+            public TermsEnum iterator() throws IOException {
+              return new BlockStateTermsEnum(derivedTermsForCurrentField, source.docsSeen);
+            }
+            
+            @Override
+            public boolean hasPositions() {
+              return false;
+            }
+            
+            @Override
+            public boolean hasPayloads() {
+              return false;
+            }
+            
+            @Override
+            public boolean hasOffsets() {
+              return false;
+            }
+            
+            @Override
+            public boolean hasFreqs() {
+              return false;
+            }
+            
+            @Override
+            public long getSumTotalTermFreq() throws IOException {
+              return 0;
+            }
+            
+            @Override
+            public long getSumDocFreq() throws IOException {
+              return 0;
+            }
+            
+            @Override
+            public int getDocCount() throws IOException {
+              return 0;
+            }
+          };
+        } else {
+          return super.terms(field);
+        }
+      }
+    };
+  }
+
+  protected abstract String originalFieldName(String field) ;
+
+  @Override
+  public void write(Fields fields) throws IOException {
+    super.write(decorate(fields));
+  }
+
+  @Override
+  protected TermsWriter createTermsWriter(final FieldInfo fieldInfo) {
+    final String originalFieldName = originalFieldName(fieldInfo.name);
+    if (originalFieldName != null) {
+      return new TermsWriter(fieldInfo) {
+        private FixedBitSet fieldDocs;
+
+        @Override
+        protected BlockTermState writePosting(BytesRef text, TermsEnum termsEnum) throws IOException {
+          // remember fields' docs which we need when terms is over
+          InjectingBlockTreeTermsWriter.BlockStateTermsEnum states = (InjectingBlockTreeTermsWriter.BlockStateTermsEnum) termsEnum;
+          fieldDocs = states.getFieldDocs();
+          // don't write anything, just retrieve FP
+          return states.termState();
+        };
+
+        @Override
+        public void finish() throws IOException {
+
+          if (fieldDocs != null) {
+            docsSeen.or(fieldDocs);
+          }
+          super.finish();
+        }
+      };
+    } else {
+      return super.createTermsWriter(fieldInfo);
+    }
+  }
+
+
+  protected abstract Map<BytesRef,BlockTermState> deriveTerms(FieldData stateByTerms) throws IOException;
+
+  private static final class BlockStateTermsEnum extends TermsEnum {
+  
+    private final Iterator<Entry<BytesRef,BlockTermState>> sourceIter;
+    private final FixedBitSet fieldDocs;
+  
+    private BlockTermState termState;
+    private BytesRef term;
+  
+    private BlockStateTermsEnum(Map<BytesRef,BlockTermState> derivedTermsForCurrentField, FixedBitSet fieldDocs) {
+      this.sourceIter = derivedTermsForCurrentField.entrySet().iterator();
+      this.fieldDocs = fieldDocs;
+    }
+  
+    @Override
+    public BytesRef next() throws IOException {
+  
+      if(!sourceIter.hasNext()) {
+        return null;
+      }
+      final Entry<BytesRef,BlockTermState> next = sourceIter.next();
+      termState = next.getValue();
+      term = next.getKey();
+     // System.out.println(term + " " + termState);
+      return term;
+    }
+  
+    /** @returns state associated with the last returned term */
+    public BlockTermState termState() {
+      return termState;
+    }
+  
+    /** @returns docs having any field values */
+    public FixedBitSet getFieldDocs() {
+      return fieldDocs;
+    }
+  
+    @Override
+    public long totalTermFreq() throws IOException {
+      return 0;
+    }
+  
+    @Override
+    public BytesRef term() throws IOException {
+      return term;
+    }
+  
+    @Override
+    public void seekExact(long ord) throws IOException {
+      throw new UnsupportedOperationException();
+    }
+  
+    @Override
+    public SeekStatus seekCeil(BytesRef text) throws IOException {
+      throw new UnsupportedOperationException();
+    }
+  
+    @Override
+    public PostingsEnum postings(PostingsEnum reuse, int flags) throws IOException {
+      throw new UnsupportedOperationException();
+    }
+  
+    @Override
+    public long ord() throws IOException {
+      throw new UnsupportedOperationException();
+    }
+  
+    @Override
+    public int docFreq() throws IOException {
+      return 0;
+    }
+  }
+
+}
\ No newline at end of file
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/PostingWriterDelegate.java b/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/PostingWriterDelegate.java
new file mode 100644
index 0000000..7d5ac40
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/PostingWriterDelegate.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.derivativeterms;
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.PushPostingsWriterBase;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+
+class PostingWriterDelegate extends PostingsWriterBase {
+  private final PostingsWriterBase postingsWriter;
+
+  PostingWriterDelegate(PostingsWriterBase postingsWriter) {
+    this.postingsWriter = postingsWriter;
+  }
+
+  @Override
+  public void init(IndexOutput termsOut, SegmentWriteState state) throws IOException {
+    postingsWriter.init(termsOut, state);
+  }
+
+  FieldInfo getFieldInfo(){
+    return ((PushPostingsWriterBase)postingsWriter).getField();
+  }
+  
+  @Override
+  public BlockTermState writeTerm(BytesRef term, TermsEnum termsEnum, FixedBitSet docsSeen)
+      throws IOException {
+    return postingsWriter.writeTerm(term, termsEnum, docsSeen);
+  }
+
+  @Override
+  public void encodeTerm(long[] longs, DataOutput out, FieldInfo fieldInfo, BlockTermState state,
+      boolean absolute) throws IOException {
+    postingsWriter.encodeTerm(longs, out, fieldInfo, state, absolute);
+  }
+
+  @Override
+  public int setField(FieldInfo fieldInfo) {
+    return postingsWriter.setField(fieldInfo);
+  }
+
+  @Override
+  public void close() throws IOException {
+    postingsWriter.close();
+  }
+}
\ No newline at end of file
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/TermsDerivingCodec.java b/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/TermsDerivingCodec.java
new file mode 100644
index 0000000..c57e0fd
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/TermsDerivingCodec.java
@@ -0,0 +1,71 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.derivativeterms;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.HashSet;
+import java.util.function.Consumer;
+import java.util.function.Supplier;
+
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.lucene70.Lucene70Codec;
+import org.apache.lucene.util.BytesRef;
+
+public class TermsDerivingCodec extends Lucene70Codec {
+
+  final private String derivativeFieldSuffix;
+  
+  final private TermsDerivingPostingsFormat hijackingPostingsFormat ;
+
+  private final HashSet<String> fieldsToCapture;
+  
+  public TermsDerivingCodec(Supplier<TermMapper> termMapper, String derivativeFieldSuffix, String ... fieldsToCapt) {
+   // super("TermsDerivingCodec");
+    this.derivativeFieldSuffix = derivativeFieldSuffix;
+    fieldsToCapture = new HashSet<String>(Arrays.asList(fieldsToCapt));
+    this.hijackingPostingsFormat = new TermsDerivingPostingsFormat( 
+        fieldsToCapture, derivativeFieldSuffix, termMapper);
+  }
+  
+  @Override
+  public PostingsFormat getPostingsFormatForField(String field) {
+
+    if (fieldsToCapture.contains(field)) {
+      return hijackingPostingsFormat;
+    } else {
+      if(field.endsWith(derivativeFieldSuffix)) {
+
+        final String originalField = field.substring(0, field.length()-derivativeFieldSuffix.length());
+        if (fieldsToCapture.contains(originalField)) {
+          return hijackingPostingsFormat;
+        } else {
+          throw new IllegalStateException("forgot to capture "+ originalField + 
+              " to inject into "+field);
+        }
+      }
+
+      return super.getPostingsFormatForField(field);
+    }
+  }
+
+  @FunctionalInterface
+  //TODO change to Function<BytesRef, Stream<BytesRef>> ? oh my 
+  interface TermMapper {
+    void map(BytesRef in, Consumer<BytesRef> out) throws IOException ;
+  }
+}
\ No newline at end of file
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/TermsDerivingPostingsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/TermsDerivingPostingsFormat.java
new file mode 100644
index 0000000..e38d234
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/TermsDerivingPostingsFormat.java
@@ -0,0 +1,113 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.derivativeterms;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.function.Supplier;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.derivativeterms.TermsDerivingCodec.TermMapper;
+import org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat;
+import org.apache.lucene.codecs.lucene50.Lucene50PostingsWriter;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.IOUtils;
+
+final public class TermsDerivingPostingsFormat extends PostingsFormat {
+  
+  private final PostingsFormat postingsFormat= new Lucene50PostingsFormat(); 
+
+  static final String NAME = "Lucene50Hijack";
+  final private String derivativeFieldSuffix;
+  final private Supplier<TermMapper> termMapperFactory;
+  final private Set<String> fieldsToCapture;
+  
+  public TermsDerivingPostingsFormat() {
+    this(null, null, null);
+  }
+  
+  protected TermsDerivingPostingsFormat( 
+      Set<String> fieldsToCapture, String derivativeFieldSuffix, Supplier<TermMapper> termMapperFactory) {
+    super(NAME);
+    this.derivativeFieldSuffix = derivativeFieldSuffix;
+    this.termMapperFactory = termMapperFactory;
+    this.fieldsToCapture = fieldsToCapture;
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    
+    final PostingsWriterBase postingsWriter = new Lucene50PostingsWriter(state);
+
+    final Map<String, FieldData> termsByField = new HashMap<String,FieldData>(fieldsToCapture.size());
+    
+    final int maxDoc = state.segmentInfo.maxDoc();
+    PostingsWriterBase hijack = new PostingWriterDelegate(postingsWriter) {
+      public BlockTermState writeTerm(BytesRef term, TermsEnum termsEnum, FixedBitSet docsSeen) throws IOException {
+        final BlockTermState state = super.writeTerm(term, termsEnum, docsSeen);
+        
+        final String fieldName = getFieldInfo().name;
+        if (fieldsToCapture.contains(fieldName)) {
+          FieldData fieldData = termsByField.get(fieldName);
+          
+          if (fieldData==null) {
+            termsByField.put(fieldName,
+                fieldData = new FieldData(new FixedBitSet(maxDoc)));
+          }
+          fieldData.put(BytesRef.deepCopyOf(term), state);
+          fieldData.docsSeen.or(docsSeen);
+          //System.out.println(termsByField);
+        }
+        return state;
+      }
+      
+    };
+    
+    return fieldsConsumer(state, hijack, termsByField);
+  }
+
+  FieldsConsumer fieldsConsumer(SegmentWriteState state, final PostingsWriterBase postingsWriter, Map<String,FieldData> termsByField)
+      throws IOException {
+    boolean success = false;
+    try {
+      FieldsConsumer ret = new ///*ByteArray**/ 
+          //TreeMap
+          ByteArrayDerivativeWriter(state, postingsWriter, 
+                          termsByField, derivativeFieldSuffix, termMapperFactory);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(postingsWriter);
+      }
+    }
+  }
+  
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    return postingsFormat.fieldsProducer(state);
+  }
+}
\ No newline at end of file
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/TreeMapDerivativeWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/TreeMapDerivativeWriter.java
new file mode 100644
index 0000000..789d958
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/derivativeterms/TreeMapDerivativeWriter.java
@@ -0,0 +1,92 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.derivativeterms;
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.TreeMap;
+import java.util.function.Supplier;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.derivativeterms.TermsDerivingCodec.TermMapper;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefBuilder;
+
+class TreeMapDerivativeWriter extends InjectingBlockTreeTermsWriter {
+  private final String fieldSuffix ;
+  protected final Supplier<TermMapper> mapperFactory ;
+  
+
+  public TreeMapDerivativeWriter(SegmentWriteState state, PostingsWriterBase postingsWriter,
+      Map<String,FieldData> termsByField, String fieldSuffix, Supplier<TermMapper> mapperFactory) throws IOException {
+    super(state, postingsWriter, termsByField);
+    this.fieldSuffix = fieldSuffix;
+    this.mapperFactory = mapperFactory;
+  }
+
+  @Override
+  protected String originalFieldName(String field) {
+    if (field.endsWith(fieldSuffix)) {
+      return field.substring(0, field.length()-fieldSuffix.length());
+    } else {
+      return null;
+    }
+  }
+
+  @Override
+  protected Map<BytesRef,BlockTermState> deriveTerms(FieldData stateByTerms) throws IOException {
+
+    final TermMapper termMapper = mapperFactory.get();
+    
+    Map<BytesRef,BlockTermState> output = new TreeMap<BytesRef,BlockTermState> ();
+    int termNum[]=new int[] {0};
+    for(Entry<BytesRef,BlockTermState> tuple : stateByTerms.entrySet()) {
+      BytesRef term = tuple.getKey();
+      final BlockTermState postingOffset = tuple.getValue();
+      System.out.println("mapping "+termNum[0]+"th "+term+" "+term.utf8ToString());
+      termMapper.map(term,
+                       (t)->{
+                         final BytesRefBuilder buff = new BytesRefBuilder();
+                         buff.clear();
+                         buff.append(t);
+                        // final int codePoint = termNum[0];
+                       //  final char[] chars = Character.toChars(codePoint);
+//                         if (false) {
+//                           buff.grow(buff.length()+10);
+//                           //buff.append(b, off, len);
+//                           final int copied = UnicodeUtil.UTF16toUTF8(new String(chars), 0, chars.length, buff.bytes(),buff.length());
+//                          buff.setLength(buff.length() +
+//                               copied
+//                                   );
+//                         } else {
+                           buff.append(new BytesRef(""+'\u0001'));
+                           buff.append(term);
+                        // }
+                         final BytesRef outBr = buff.toBytesRef();
+                      //   System.out.println(outBr+"~"+" term num "+codePoint+" "+Arrays.toString(chars));
+                         final BlockTermState old = output.put(outBr, postingOffset);
+                         assert old==null : outBr+"~"+outBr.utf8ToString();
+                         }
+          );
+      termNum[0]++;
+    }
+    return output;
+  }
+}
\ No newline at end of file
diff --git a/lucene/core/src/java/org/apache/lucene/util/BytesRefArray.java b/lucene/core/src/java/org/apache/lucene/util/BytesRefArray.java
index bab3d04..6ad0664 100644
--- a/lucene/core/src/java/org/apache/lucene/util/BytesRefArray.java
+++ b/lucene/core/src/java/org/apache/lucene/util/BytesRefArray.java
@@ -170,7 +170,7 @@ public final class BytesRefArray implements SortableBytesRefArray {
   /**
    * sugar for {@link #iterator(Comparator)} with a <code>null</code> comparator
    */
-  public BytesRefIterator iterator() {
+  public BytesRefIdxIterator iterator() {
     return iterator(null);
   }
   
@@ -189,22 +189,49 @@ public final class BytesRefArray implements SortableBytesRefArray {
    * </p>
    */
   @Override
-  public BytesRefIterator iterator(final Comparator<BytesRef> comp) {
+  public BytesRefIdxIterator iterator(final Comparator<BytesRef> comp) {
     final BytesRefBuilder spare = new BytesRefBuilder();
     final BytesRef result = new BytesRef();
     final int size = size();
     final int[] indices = comp == null ? null : sort(comp);
-    return new BytesRefIterator() {
-      int pos = 0;
-      
-      @Override
-      public BytesRef next() {
-        if (pos < size) {
-          setBytesRef(spare, result, indices == null ? pos++ : indices[pos++]);
-          return result;
-        }
-        return null;
+    return new BytesRefIdxIter(indices, result, spare, size);
+  }
+
+  private class BytesRefIdxIter implements BytesRefIdxIterator {
+    private final int[] indices;
+    private final BytesRef result;
+    private final BytesRefBuilder spare;
+    private final int size;
+    int pos = 0;
+    private int index = -1;
+
+    private BytesRefIdxIter(int[] indices, BytesRef result, BytesRefBuilder spare, int size) {
+      this.indices = indices;
+      this.result = result;
+      this.spare = spare;
+      this.size = size;
+    }
+
+    @Override
+    public BytesRef next() {
+      if (pos < size) {
+        index = indices == null ? pos++ : indices[pos++];
+        setBytesRef(spare, result, index);
+        return result;
       }
-    };
+      return null;
+    }
+
+    @Override
+    public int index() {
+      return index;
+    }
+
   }
+
+  public interface BytesRefIdxIterator extends BytesRefIterator {
+    /** @return index of the last next() result, undetermined before that */
+    int index();
+  }
+
 }
diff --git a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.Codec b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
index 773c168..4e0db95 100644
--- a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
+++ b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
@@ -14,3 +14,4 @@
 #  limitations under the License.
 
 org.apache.lucene.codecs.lucene70.Lucene70Codec
+org.apache.lucene.benchmark.utils.DeriveBodyRevCodec
diff --git a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
index f7390e2..c26c415 100644
--- a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
+++ b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
@@ -14,3 +14,4 @@
 #  limitations under the License.
 
 org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat
+org.apache.lucene.codecs.derivativeterms.TermsDerivingPostingsFormat
