Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestMultiWordSynonymFilter.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>windows-1252
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestMultiWordSynonymFilter.java	(revision )
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestMultiWordSynonymFilter.java	(revision )
@@ -0,0 +1,1133 @@
+package org.apache.lucene.analysis.synonym;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.StringReader;
+import java.text.ParseException;
+import java.util.Arrays;
+import java.util.HashSet;
+import java.util.Random;
+import java.util.Set;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockGraphTokenFilter;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.TokenStreamToAutomaton;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionLengthAttribute;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefBuilder;
+import org.apache.lucene.util.CharsRef;
+import org.apache.lucene.util.CharsRefBuilder;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.IntsRefBuilder;
+import org.apache.lucene.util.TestUtil;
+import org.apache.lucene.util.automaton.Automaton;
+import org.apache.lucene.util.automaton.Operations;
+import org.apache.lucene.util.fst.Util;
+
+/**
+ * @since solr 1.4
+ */
+public class TestMultiWordSynonymFilter extends BaseSynonymParserTestCase {
+
+  /**
+   * verify type of token and positionLengths on synonyms of different word counts.
+   */
+  public void testPositionLengthAndType() throws Exception {
+    String testFile =
+        "spider man, spiderman\n" +
+        "usa,united states,u s a,united states of america";
+    Analyzer analyzer = new MockAnalyzer(random());
+    SolrSynonymParser parser = new SolrSynonymParser(true, true, analyzer);
+    parser.parse(new StringReader(testFile));
+    final SynonymMap map = parser.build();
+    analyzer.close();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, true, true));
+      }
+    };
+
+    BytesRef value = Util.get(map.fst, Util.toUTF32(new CharsRef("usa"), new IntsRefBuilder()));
+    ByteArrayDataInput bytesReader = new ByteArrayDataInput(value.bytes, value.offset, value.length);
+    final int code = bytesReader.readVInt();
+    final int count = code >>> 1;
+
+    final int[] synonymsIdxs = new int[count];
+    for (int i = 0; i < count; i++) {
+      synonymsIdxs[i] = bytesReader.readVInt();
+    }
+
+    BytesRef scratchBytes = new BytesRef();
+    map.words.get(synonymsIdxs[2], scratchBytes);
+
+    int synonymLength = 1;
+    for (int i = scratchBytes.offset; i < scratchBytes.offset + scratchBytes.length; i++) {
+      if (scratchBytes.bytes[i] == SynonymMap.WORD_SEPARATOR) {
+        synonymLength++;
+      }
+    }
+
+    assertEquals(count, 3);
+    assertEquals(synonymLength, 4);
+
+    assertAnalyzesTo(analyzer, "spider man",
+        new String[]{"spider", "spiderman", "man"},
+        new int[]{0, 0, 7},
+        new int[]{6, 10, 10},
+        new String[]{"word", "SYNONYM", "word"},
+        new int[]{1, 0, 1},
+        new int[]{1, 2, 1});
+
+    assertAnalyzesToPositions(analyzer, "amazing spider man",
+        new String[]{"amazing", "spider", "spiderman", "man"},
+        new String[]{"word", "word", "SYNONYM", "word"},
+        new int[]{1, 1, 0, 1},
+        new int[]{1, 1, 2, 1});
+
+    assertAnalyzesTo(analyzer, "the united states of america is wealthy",
+        new String[]{"the", "united", "usa", "united", "u", "states", "states", "s", "of", "a", "america", "is", "wealthy"},
+        new int[]{0, 4, 4, 4, 4, 11, 11, 11, 18, 18, 21, 29, 32},
+        new int[]{3, 10, 28, 10, 10, 17, 28, 17, 20, 28, 28, 31, 39},
+        new String[]{"word", "word", "SYNONYM", "SYNONYM", "SYNONYM", "word", "SYNONYM", "SYNONYM", "word", "SYNONYM", "word", "word", "word"},
+        new int[]{1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1},
+        new int[]{1, 1, 4, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1});
+
+    assertAnalyzesToPositions(analyzer, "spiderman",
+        new String[]{"spider", "spiderman", "man"},
+        new String[]{"SYNONYM", "word", "SYNONYM"},
+        new int[]{1, 0, 1},
+        new int[]{1, 2, 1});
+
+    assertAnalyzesTo(analyzer, "spiderman enemies",
+        new String[]{"spider", "spiderman", "man", "enemies"},
+        new int[]{0, 0, 0, 10},
+        new int[]{9, 9, 9, 17},
+        new String[]{"SYNONYM", "word", "SYNONYM", "word"},
+        new int[]{1, 0, 1, 1},
+        new int[]{1, 2, 1, 1});
+
+    // TODO: Problems: In the substitution below, how to differentiate the phrases "united states" and "u s a"?
+    // So that they are not 'sausaged' and the phrase "united s a" would not match.
+    assertAnalyzesTo(analyzer, "the usa is wealthy",
+        new String[]{"the", "united", "usa", "united", "u", "states", "states", "s", "of", "a", "america", "is", "wealthy"},
+        new int[]{0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 11},
+        new int[]{3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 10, 18},
+        new String[]{"word", "SYNONYM", "word", "SYNONYM", "SYNONYM", "SYNONYM", "SYNONYM", "SYNONYM", "SYNONYM", "SYNONYM", "SYNONYM", "word", "word"},
+        new int[]{1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1},
+        new int[]{1, 1, 4, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1});
+    
+    assertAllStrings(analyzer, "the usa is wealthy", new String[] {
+        "the usa is wealthy",
+        "the united states is wealthy",
+        "the u s a is wealthy",
+        "the united states of america is wealthy",
+// Wrong. Here only due to "sausagization" of the multi word synonyms.
+        "the u states is wealthy",
+        "the u states a is wealthy",
+        "the u s of america is wealthy",
+        "the u states of america is wealthy",
+        "the united s a is wealthy",
+        "the united states a is wealthy",
+        "the united s of america is wealthy"});
+
+    assertAnalyzesTo(analyzer, "the united states is wealthy",
+        new String[]{"the", "united", "united", "usa", "u", "states", "states", "s", "of", "a", "america", "is", "wealthy"},
+        new int[]{0, 4, 4, 4, 4, 11, 11, 11, 11, 11, 11, 18, 21},
+        new int[]{3, 10, 10, 17, 10, 17, 17, 17, 17, 17, 17, 20, 28},
+        new String[]{"word", "SYNONYM", "word", "SYNONYM", "SYNONYM", "SYNONYM", "word", "SYNONYM", "SYNONYM", "SYNONYM", "SYNONYM", "word", "word"},
+        new int[]{1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1},
+        new int[]{1, 1, 1, 4, 1, 1, 3, 1, 1, 2, 1, 1, 1});
+
+    assertAnalyzesTo(analyzer, "the united states of balance",
+        new String[]{"the", "united", "united", "usa", "u", "states", "states", "s", "of", "a", "america", "of", "balance"},
+        new int[]{0, 4, 4, 4, 4, 11, 11, 11, 11, 11, 11, 18, 21},
+        new int[]{3, 10, 10, 17, 10, 17, 17, 17, 17, 17, 17, 20, 28},
+        new String[]{"word", "SYNONYM", "word", "SYNONYM", "SYNONYM", "SYNONYM", "word", "SYNONYM", "SYNONYM", "SYNONYM", "SYNONYM", "word", "word"},
+        new int[]{1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1},
+        new int[]{1, 1, 1, 4, 1, 1, 3, 1, 1, 2, 1, 1, 1});
+
+    analyzer.close();
+  }
+
+  /**
+   * verify type of token and positionLengths on synonyms of different word counts, with non preserving, explicit rules.
+   */
+  public void testNonPreservingMultiwordSynonyms() throws Exception {
+    String testFile =
+      "aaa => two words\n" +
+      "bbb => one two, very many multiple words\n" +
+      "ee ff, gg, h i j k, h i => one\n" +
+      "cc dd => usa,united states,u s a,united states of america";
+
+    Analyzer analyzer = new MockAnalyzer(random());
+    SolrSynonymParser parser = new SolrSynonymParser(true, true, analyzer);
+    parser.parse(new StringReader(testFile));
+    final SynonymMap map = parser.build();
+    analyzer.close();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, true, true));
+      }
+    };
+
+    assertAnalyzesTo(analyzer, "aaa",
+        new String[]{"two", "words"},
+        new int[]{0, 0},
+        new int[]{3, 3},
+        new String[]{"SYNONYM", "SYNONYM"},
+        new int[]{1, 1},
+        new int[]{1, 1});
+
+    assertAnalyzesToPositions(analyzer, "amazing aaa",
+        new String[]{"amazing", "two", "words"},
+        new String[]{"word", "SYNONYM", "SYNONYM"},
+        new int[]{1, 1, 1},
+        new int[]{1, 1, 1});
+
+    assertAnalyzesTo(analyzer, "p bbb s",
+        new String[]{"p", "very", "one", "many", "two", "multiple", "words", "s"},
+        new int[]{0, 2, 2, 2, 2, 2, 2, 6},
+        new int[]{1, 5, 5, 5, 5, 5, 5, 7},
+        new String[]{"word", "SYNONYM", "SYNONYM", "SYNONYM", "SYNONYM", "SYNONYM", "SYNONYM", "word"},
+        new int[]{1, 1, 0, 1, 0, 1, 1, 1},
+        new int[]{1, 1, 1, 1, 3, 1, 1, 1});
+
+    assertAnalyzesTo(analyzer, "p ee ff s",
+        new String[]{"p", "one", "s"},
+        new int[]{0, 2, 8},
+        new int[]{1, 7, 9},
+        new String[]{"word", "SYNONYM", "word"},
+        new int[]{1, 1, 1},
+        new int[]{1, 1, 1});
+
+    assertAnalyzesTo(analyzer, "p h i j s",
+        new String[]{"p", "one", "j", "s"},
+        new int[]{0, 2, 6, 8},
+        new int[]{1, 5, 7, 9},
+        new String[]{"word", "SYNONYM", "word", "word"},
+        new int[]{1, 1, 1, 1},
+        new int[]{1, 1, 1, 1});
+
+    analyzer.close();
+  }
+
+  public void testMoreThanOneLookAhead() throws Exception {
+    String testFile =
+        "c => 8 2\n" +
+            "f c e d f, 1\n" +
+            "c g a f d, 6 5 5\n" +
+            "e c => 4\n" +
+            "g => 5\n" +
+            "a g b f e => 5 0 7 7\n" +
+            "b => 1";
+
+    Analyzer analyzer = new MockAnalyzer(random());
+    SolrSynonymParser parser = new SolrSynonymParser(true, true, analyzer);
+    parser.parse(new StringReader(testFile));
+    final SynonymMap map = parser.build();
+    analyzer.close();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, true, true));
+      }
+    };
+
+    String doc = "b c g e d b d f d d f d a d f d d e e b b e f b b b d g f e e b g g g e b a e g d c b b f e c d a e e e e g g d f b g f b a e d f f d c c e d c f c b a b e e e b c g d b f f f a c c g f d g f c e a e d f g e b a g e d c a a d d c e a b e c f f g a a a f b e b c c b d g e b a g c b a g c a f a e a e d e f e e a c d a d e g a f c c c f c c g f d a b f g a b g g d c g d g c b g c g g g e b b b c f a e a a d d e d b d g a d g c g a d c d a f a a f d e c f f a g d d a b d a c c c b e b f g e c e b e e b g g e f b g c b b a f c f b e g f b b b d b c d f a b f a b f a f a e f a a c c b c c e d b e c g c b d e e b e a g a g e c a a g f g f c f f d d g d f f g g g c e c b g f b f g d e c c d b g c c d c g g f g b g a f c e f d f d g b a g b b f c b a c a c e b g b b e g a c d e g d d e c b d b f d d g d b a d f e a f c a f g f g g a a g d g d g b a e d a a d a b b f b b c e e f a d a b e a a f f d b f f e b e d d f g e c a b e a d d f a g a e f d e g e g f d c a c c e f c g f f c c b d a c e b g b g d a f c a f a f g c d f e g b f b f c g d f c f a e d d d f c b f g b a a e b e f g c c g a f d b b b g b a e b a b g a c e g a c e f f e c f e f g b g e b e d c d e g g a b a a g a d d a b c a f a g c d g b b f b b e a b g a c b b e e b g a b c f g g b f f d c c c g a e c e g f g a f g d f g c b e g e f d g c d a f g d d c g e b a c d a b g a c d a c b d e e b a g a d g b g b d a a e g a e e g g f d f g a d b g b c g b e d e f f a f d f a d d e d c c d a a g d b f c f f f c d f g e a e g c d d a f b g a a a e b g c b a a d g c f c a d e c c b a f e a b e d b f f b b a a c f b c f e b g a f d g g g f a g e e g d b g f g e e d f a b e f c c c b b d a e c a g f a e e g b a b c g c f a g b f b f f c f e f d b e d e c b e e g e f e a f f e c b f b d c b d c d e c d f a c a d a f a d b c e d a g g f c e e e b b d d f a d g c c c f e e f b c b d c g a g d b e a f b d g f c f a b b c d g a g e c b g a b g b b e c g d e f a c b e e a a c d e f f b a f a g c g g b e a a a b d f f g a a d e f c c g a d d g f a g g b e a a g e a g c a f f b g c g f c g c b b c c c d e d f e c g g d b d d d c e b e g a b f g g a f g b a e c b g e a b d f c b b f g c e a f d e c d a g c c c g c b g b g d b a b c a d c d b b c c c d f a b f g f b d c c a f a g a d c c b d d c c e f b d f g f a d d e c d c d a g e e g d b a e e e b d c d a f f b b d a c e a g d e e c e d c f c b c c f a a e g b f d b a g e e b c b b d e b d g f c b e f f c b f f g b c f a c f g d f f a a b e e e b f g a g e g d a b g d c g c f b g g a b f e g g g b c a d e d b a e f d b b d b a a c c d b f b c b d g d f g a g g b a f a a b c b e g f c a c d c c c b e g g b b f a e b a f c e c b b a b a b d f f d a d g b c f b g a b d g f f f a g b d g d b a a c e a e e a d a e g c d b d g c f f a f c d d c e b a b a d g d c g d b g g a d c e a e g c a b e c g b a c f f e g d e c g d e g c g f c b g a a e e c a b e f d c c a c c e a e e c d f f d d g d d e a g g e a d d a f c c e d g e c a d e f g b f c g f e g d d a d b d g d d a d f d b f c b b f c d d g g g b e a g e a f e d f b d c a g b d c c d b d b a a f e g b a d d g a c c f f d e c d d f e c g d b c a f b g a e a g f c e d c e f b c f e c d c e g b g c b b e g c a g e d d b c d c b f d c e c e b d d g a d a d e f g f g e d c a e f f b a f c b a b c c f d b a g g f e e f b b g f g b c f g f a g g g f d b c b b a f b b c d g e a c e e a f d b d f e c g c c c b c g e c f g e a d f g c d g a e f e c a f b g b g d e a a e g b c b g f c d f a a a b e g e c e d g a c a d f g e e g e e a f f d g c e e d g e b a a b c a e f b e g e c g b b a d a g g f b f c d c f d a d e c f f a e g c c f a a f b b c b b b g a a f c e c a d e g d e a e d c d c c a g g f b c f b e a f f b g f g g d c a d a c e d b d c f e e e a f c f g a a b e d a f a d g g g f f f c g e a g f g f d c c f d d c b a c e g c c b d b e c g e f b c g c c g a d e a f g d f g d e f f b a e f b f b d b f c e g b c c f a c d c b c f f f b e e c c f f c e f b g g c b b d e d e d g g e e a c g f f e e b a f e f f f a g f a g b d g f b a c a f b f a b d d e g b d d f c e d d b e c c e a f e g d d a f c g c c c c g a f g f e g f b g g b d g f a c f g f e b a c b d c a d d f e f a d d d c a a e g a g f c a e g e f a g d f g g d g b a d b f g e b a d g d b e d c b c b d d d d c b b a c f f c a f d g e a a f d c b c a b c b b c a b a e b d c a a e b a b d b a f e f d f a e d e f b b f g f f c b d c d d d e b b d a g b b g e c c b c b d c c g g d c g d b f f e c d c b d c a a e g b d g a a d e d b d e f d g f d d a c d g c e b b a e c a b f b f f f f b b g d g g f c g d a b c g a e e f e g c f c f g a e g e d b g d a e g d c a e f a d f f d e f a e d d b g b e e d a a e e c d g f g b c c g b d f c d g e d g e f g g a a d b e f a a c f b e d f e a c b e d f e e d b e e e e f g g e d b e g g b g d b b f b b d e d b a b g g c e a b d g b f b a c g a a f c g d f d c c b b e c f d d c e d g e d c a b e f a c e b b d b f e b a c d d g a f b b f g f g e b g c a f a b d a g b b e c a a f g g f d a d f f g e b e f b c e g b a c f a d e f c f b a b e f c f d e a e b a d g a g c e f f d g c c f e a b c g c d b d a c d a b d a c d c a b f c d d b g e b e g g f d c c e e g f a g a g g c g e d d f e c b e g a b a e g b c d b b a b b g c a d a c b f f g g a d e g g g b c c g f c c b f e a c b g c g f c a d b d d e a g e d f b c c e c d b d f f e c d a c c f g a e c d e d g d d e d a b d a a e a f g b a c d e e g e e d d a d f e e b e e f g b a c f g f d f e b b g f a f c f b c d a g b a g d a f d e g b d a d g a e e d b g c g c a e a b f g a f g a a d d g b e d g e a g d f a g a d c c a g c f f g e b g a d f a g c a g c f c c e b c e d f a c f c e f d e d g c b g d g a b c d c e b c d d b g d a g b a f g c c g c e a f d g g b f g c d f d a d b b c c g c b g f d a e c c d c a e c c e a f c f f b g f d d g b d a e b a e a b c b g a g b e b b b c f g c b d f g e f g a e c g e d a d b e a a b e a e b c b f a a e d f e f g b d g g c b d b e e g d g e f f d a f d g d c a e a g a d d g f e c b f c g c a b a g e g d d g e e e b g e c f f e b g a g c a e g e g f e b e a f b e f e d e f c d e d b d c a b b b c d d c c a d b b e e e g d a c c f b d a e e g f e b c e b b g g b f g c e c c a f e b g b a a c g a b a f e d a g g c c g a a f e d c a g b d g f e c d g a e c e e a c d b a a a b b a f d a d a a e f f c e b f e d f e e a c c a e d d g f e b c f b f e f f b f f g c a g g b d f c e f a b e e d g f e e b a f e b d c e g e g a d g c d d d b a g e e g g e c b c g f b c d a d e g c f d d e a f b c c b d b c f b d d a a g e f d d c c e b d a g a b d e g a f g b a g c d e g f g c a d c g c c c e d f f f e g c a d b c c e g g b g g a e b b f d c d g b c b a e d a d c a e a b a e e f f b a c g b c f d a g d a c d f c b e a e e e a d g a b f a e e c a f c a b g e g b b d g d e d d d f d c f b a f b e d a b c b b g c d c a f c e f g g b f d d f d e g f f c c e b g b a d f e a b c b f c g c c f a g b f d c a b f c f f e g a a b f c b e f b d a b f f g g f d d g g g e c b d e f b g e b b d g d d e g a e f f c d d f a c b f c g b c c b g d g e d e a d f b g d f d b c e e g a e c f c c f c b a a e c e g g f a e b g b d a b e b c e d b e g a c f f b e g c e e g e b g f d a a e e b c d g c b b f a d f c e c c a d d f a a e a b g f c b g a a g d g b a c d e a e b b d g a d d d c d f b g c b a f e d g f g b d b a d f b b e c e a e c a e e e f d a b e e e a e b b c b e a g f g b e c g d g b c c g e d b f a f c e c a d c b b f b c d a c f c e d e d f f c a e e a g e c f g d a c a g g e g g f b a a f a a d d a b a d d d d a c f a a f e a g b g a e f g d b f c f a c a g c b b c b d b f c a g b f a d d f g g d d d f b f d f a g d b c g d g d g f e a e b g c b f e g a f c f a g c f b f b c f a g f c c d b a c g a g b d e b a d c f f f b b c b d b f e e f g f g c a e b e c b g g b c c e g f f d a a g b f d e c e e e e b g e f a d e e g b f d f d g d d f b d b b b a b b e f g d b e b d g d g g d f d e d g d e g d d a e c f e f g c d a c e e g d f e g g a g f a a c e b a d b b a c f f b f d a b e g a g e a d c g c b e f g e a d g g c a g f c b g g d a c g b a f c e g g c c d b e d e e f e f e e f f g c b g f g c b e f a g e d g e c e a e e g d e e b f c c f f a d g e c b f d a a b f g a c a e e e d b d g b c g g a d d g c d d a g b f f g a g f f g d b c d a b c g e b g c c d g a a c c f c";
+    String[] expected = new String[]{"1", "8", "2", "5", "e", "d", "1", "d", "f", "d", "d", "f", "d", "a", "d", "f", "d", "d", "e", "e", "1", "1", "e", "f", "1", "1", "1", "d", "5", "f", "e", "e", "1", "5", "5", "5", "e", "1", "a", "e", "5", "d", "8", "2", "1", "1", "f", "4", "d", "a", "e", "e", "e", "e", "5", "5", "d", "f", "1", "5", "f", "1", "a", "e", "d", "f", "f", "d", "8", "2", "8", "2", "e", "d", "8", "2", "f", "8", "2", "1", "a", "1", "e", "e", "e", "1", "8", "2", "5", "d", "1", "f", "f", "f", "a", "8", "2", "8", "2", "5", "f", "d", "5", "f", "8", "2", "e", "a", "e", "d", "f", "5", "e", "1", "a", "5", "e", "d", "8", "2", "a", "a", "d", "d", "8", "2", "e", "a", "1", "4", "f", "f", "5", "a", "a", "a", "f", "1", "e", "1", "8", "2", "8", "2", "1", "d", "5", "e", "1", "a", "5", "8", "2", "1", "a", "5", "8", "2", "a", "f", "a", "e", "a", "e", "d", "e", "f", "e", "e", "a", "8", "2", "d", "a", "d", "e", "5", "a", "f", "8", "2", "8", "2", "8", "2", "f", "8", "2", "8", "2", "5", "f", "d", "a", "1", "f", "5", "a", "1", "5", "5", "d", "8", "2", "5", "d", "5", "8", "2", "1", "5", "8", "2", "5", "5", "5", "e", "1", "1", "1", "8", "2", "f", "a", "e", "a", "a", "d", "d", "e", "d", "1", "d", "5", "a", "d", "5", "8", "2", "5", "a", "d", "8", "2", "d", "a", "f", "a", "a", "f", "d", "4", "f", "f", "a", "5", "d", "d", "a", "1", "d", "a", "8", "2", "8", "2", "8", "2", "1", "e", "1", "f", "5", "4", "e", "1", "e", "e", "1", "5", "5", "e", "f", "1", "5", "8", "2", "1", "1", "a", "f", "8", "2", "f", "1", "e", "5", "f", "1", "1", "1", "d", "1", "8", "2", "d", "f", "a", "1", "f", "a", "1", "f", "a", "f", "a", "e", "f", "a", "a", "8", "2", "8", "2", "1", "8", "2", "8", "2", "e", "d", "1", "4", "5", "8", "2", "1", "d", "e", "e", "1", "e", "a", "5", "a", "5", "4", "a", "a", "5", "f", "5", "f", "8", "2", "f", "f", "d", "d", "5", "d", "f", "f", "5", "5", "5", "8", "2", "4", "1", "5", "f", "1", "f", "5", "d", "4", "8", "2", "d", "1", "5", "8", "2", "8", "2", "d", "8", "2", "5", "5", "f", "5", "1", "5", "a", "f", "8", "2", "e", "f", "d", "f", "d", "5", "1", "a", "5", "1", "1", "f", "8", "2", "1", "a", "8", "2", "a", "8", "2", "e", "1", "5", "1", "1", "e", "5", "a", "8", "2", "d", "e", "5", "d", "d", "4", "1", "d", "1", "f", "d", "d", "5", "d", "1", "a", "d", "f", "e", "a", "f", "8", "2", "a", "f", "5", "f", "5", "5", "a", "a", "5", "d", "5", "d", "5", "1", "a", "e", "d", "a", "a", "d", "a", "1", "1", "f", "1", "1", "8", "2", "e", "e", "f", "a", "d", "a", "1", "e", "a", "a", "f", "f", "d", "1", "f", "f", "e", "1", "e", "d", "d", "f", "5", "4", "a", "1", "e", "a", "d", "d", "f", "a", "5", "a", "e", "f", "d", "e", "5", "e", "5", "f", "d", "8", "2", "a", "8", "2", "8", "2", "e", "f", "8", "2", "5", "f", "f", "8", "2", "8", "2", "1", "d", "a", "8", "2", "e", "1", "5", "1", "5", "d", "a", "f", "8", "2", "a", "f", "a", "f", "5", "8", "2", "d", "f", "e", "5", "1", "f", "1", "f", "8", "2", "5", "d", "f", "8", "2", "f", "a", "e", "d", "d", "d", "f", "8", "2", "1", "f", "5", "1", "a", "a", "e", "1", "e", "f", "5", "8", "2", "c", "6", "g", "5", "a", "5", "f", "d", "1", "1", "1", "5", "1", "a", "e", "1", "a", "1", "5", "a", "8", "2", "e", "5", "a", "8", "2", "e", "f", "f", "4", "f", "e", "f", "5", "1", "5", "e", "1", "e", "d", "8", "2", "d", "e", "5", "5", "a", "1", "a", "a", "5", "a", "d", "d", "a", "1", "8", "2", "a", "f", "a", "5", "8", "2", "d", "5", "1", "1", "f", "1", "1", "e", "a", "1", "5", "a", "8", "2", "1", "1", "e", "e", "1", "5", "a", "1", "8", "2", "f", "5", "5", "1", "f", "f", "d", "8", "2", "8", "2", "8", "2", "5", "a", "4", "e", "5", "f", "5", "a", "f", "5", "d", "f", "5", "8", "2", "1", "e", "5", "e", "f", "d", "5", "8", "2", "d", "a", "f", "5", "d", "d", "8", "2", "5", "e", "1", "a", "8", "2", "d", "a", "1", "5", "a", "8", "2", "d", "a", "8", "2", "1", "d", "e", "e", "1", "a", "5", "a", "d", "5", "1", "5", "1", "d", "a", "a", "e", "5", "a", "e", "e", "5", "5", "f", "d", "f", "5", "a", "d", "1", "5", "1", "8", "2", "5", "1", "e", "d", "e", "f", "f", "a", "f", "d", "f", "a", "d", "d", "e", "d", "8", "2", "8", "2", "d", "a", "a", "5", "d", "1", "f", "8", "2", "f", "f", "f", "8", "2", "d", "f", "5", "e", "a", "e", "5", "8", "2", "d", "d", "a", "f", "1", "5", "a", "a", "a", "e", "1", "5", "8", "2", "1", "a", "a", "d", "5", "8", "2", "f", "8", "2", "a", "d", "4", "8", "2", "1", "a", "f", "e", "a", "1", "e", "d", "1", "f", "f", "1", "1", "a", "a", "8", "2", "f", "1", "8", "2", "f", "e", "1", "5", "a", "f", "d", "5", "5", "5", "f", "a", "5", "e", "e", "5", "d", "1", "5", "f", "5", "e", "e", "d", "f", "a", "1", "e", "f", "8", "2", "8", "2", "8", "2", "1", "1", "d", "a", "4", "a", "5", "f", "a", "e", "e", "5", "1", "a", "1", "8", "2", "5", "8", "2", "f", "a", "5", "1", "f", "1", "f", "f", "8", "2", "f", "e", "f", "d", "1", "e", "d", "4", "1", "e", "e", "5", "e", "f", "e", "a", "f", "f", "4", "1", "f", "1", "d", "8", "2", "1", "d", "8", "2", "d", "4", "d", "f", "a", "8", "2", "a", "d", "a", "f", "a", "d", "1", "8", "2", "e", "d", "a", "5", "5", "f", "8", "2", "e", "e", "e", "1", "1", "d", "d", "f", "a", "d", "5", "8", "2", "8", "2", "8", "2", "f", "e", "e", "f", "1", "8", "2", "1", "d", "8", "2", "5", "a", "5", "d", "1", "e", "a", "f", "1", "d", "5", "f", "8", "2", "f", "a", "1", "1", "8", "2", "d", "5", "a", "5", "4", "1", "5", "a", "1", "5", "1", "1", "4", "5", "d", "e", "f", "a", "8", "2", "1", "e", "e", "a", "a", "8", "2", "d", "e", "f", "f", "1", "a", "f", "a", "5", "8", "2", "5", "5", "1", "e", "a", "a", "a", "1", "d", "f", "f", "5", "a", "a", "d", "e", "f", "8", "2", "8", "2", "5", "a", "d", "d", "5", "f", "a", "5", "5", "1", "e", "a", "a", "5", "e", "a", "5", "8", "2", "a", "f", "f", "1", "5", "8", "2", "5", "f", "8", "2", "5", "8", "2", "1", "1", "8", "2", "8", "2", "8", "2", "d", "e", "d", "f", "4", "5", "5", "d", "1", "d", "d", "d", "8", "2", "e", "1", "e", "5", "a", "1", "f", "5", "5", "a", "f", "5", "1", "a", "4", "1", "5", "e", "a", "1", "d", "f", "8", "2", "1", "1", "f", "5", "8", "2", "e", "a", "f", "d", "4", "d", "a", "5", "8", "2", "8", "2", "8", "2", "5", "8", "2", "1", "5", "1", "5", "d", "1", "a", "1", "8", "2", "a", "d", "8", "2", "d", "1", "1", "8", "2", "8", "2", "8", "2", "d", "f", "a", "1", "f", "5", "f", "1", "d", "8", "2", "8", "2", "a", "f", "a", "5", "a", "d", "8", "2", "8", "2", "1", "d", "d", "8", "2", "8", "2", "e", "f", "1", "d", "f", "5", "f", "a", "d", "d", "4", "d", "8", "2", "d", "a", "5", "e", "e", "5", "d", "1", "a", "e", "e", "e", "1", "d", "8", "2", "d", "a", "f", "f", "1", "1", "d", "a", "8", "2", "e", "a", "5", "d", "e", "4", "e", "d", "8", "2", "f", "8", "2", "1", "8", "2", "8", "2", "f", "a", "a", "e", "5", "1", "f", "d", "1", "a", "5", "e", "e", "1", "8", "2", "1", "1", "d", "e", "1", "d", "5", "f", "8", "2", "1", "e", "f", "f", "8", "2", "1", "f", "f", "5", "1", "8", "2", "f", "a", "8", "2", "f", "5", "d", "f", "f", "a", "a", "1", "e", "e", "e", "1", "f", "5", "a", "5", "e", "5", "d", "a", "1", "5", "d", "8", "2", "5", "8", "2", "f", "1", "5", "5", "a", "1", "f", "e", "5", "5", "5", "1", "8", "2", "a", "d", "e", "d", "1", "a", "e", "f", "d", "1", "1", "d", "1", "a", "a", "8", "2", "8", "2", "d", "1", "f", "1", "8", "2", "1", "d", "5", "d", "f", "5", "a", "5", "5", "1", "a", "f", "a", "a", "1", "8", "2", "1", "e", "5", "f", "8", "2", "a", "8", "2", "d", "8", "2", "8", "2", "8", "2", "1", "e", "5", "5", "1", "1", "f", "a", "e", "1", "a", "f", "8", "2", "4", "1", "1", "a", "1", "a", "1", "d", "f", "f", "d", "a", "d", "5", "1", "8", "2", "f", "1", "5", "a", "1", "d", "5", "f", "f", "f", "a", "5", "1", "d", "5", "d", "1", "a", "a", "8", "2", "e", "a", "e", "e", "a", "d", "a", "e", "5", "8", "2", "d", "1", "d", "5", "8", "2", "f", "f", "a", "f", "8", "2", "d", "d", "8", "2", "e", "1", "a", "1", "a", "d", "5", "d", "8", "2", "5", "d", "1", "5", "5", "a", "d", "8", "2", "e", "a", "e", "5", "8", "2", "a", "1", "4", "5", "1", "a", "8", "2", "f", "f", "e", "5", "d", "4", "5", "d", "e", "5", "8", "2", "5", "f", "8", "2", "1", "5", "a", "a", "e", "4", "a", "1", "e", "f", "d", "8", "2", "8", "2", "a", "8", "2", "8", "2", "e", "a", "e", "4", "d", "f", "f", "d", "d", "5", "d", "d", "e", "a", "5", "5", "e", "a", "d", "d", "a", "f", "8", "2", "8", "2", "e", "d", "5", "4", "a", "d", "e", "f", "5", "1", "f", "8", "2", "5", "f", "e", "5", "d", "d", "a", "d", "1", "d", "5", "d", "d", "a", "d", "f", "d", "1", "f", "8", "2", "1", "1", "f", "8", "2", "d", "d", "5", "5", "5", "1", "e", "a", "5", "e", "a", "f", "e", "d", "f", "1", "d", "8", "2", "a", "5", "1", "d", "8", "2", "8", "2", "d", "1", "d", "1", "a", "a", "f", "e", "5", "1", "a", "d", "d", "5", "a", "8", "2", "8", "2", "f", "f", "d", "4", "d", "d", "f", "4", "5", "d", "1", "8", "2", "a", "f", "1", "5", "a", "e", "a", "5", "f", "8", "2", "e", "d", "8", "2", "e", "f", "1", "8", "2", "f", "4", "d", "8", "2", "e", "5", "1", "5", "8", "2", "1", "1", "e", "5", "8", "2", "a", "5", "e", "d", "d", "1", "8", "2", "d", "8", "2", "1", "f", "d", "8", "2", "4", "e", "1", "d", "d", "5", "a", "d", "a", "d", "e", "f", "5", "f", "5", "e", "d", "8", "2", "a", "e", "f", "f", "1", "a", "f", "8", "2", "1", "a", "1", "8", "2", "8", "2", "f", "d", "1", "a", "5", "5", "f", "e", "e", "f", "1", "1", "5", "f", "5", "1", "8", "2", "f", "5", "f", "a", "5", "5", "5", "f", "d", "1", "8", "2", "1", "1", "a", "f", "1", "1", "8", "2", "d", "5", "e", "a", "8", "2", "e", "e", "a", "f", "d", "1", "d", "f", "4", "5", "8", "2", "8", "2", "8", "2", "1", "8", "2", "5", "4", "f", "5", "e", "a", "d", "f", "5", "8", "2", "d", "5", "a", "e", "f", "4", "a", "f", "1", "5", "1", "5", "d", "e", "a", "a", "e", "5", "1", "8", "2", "1", "5", "f", "8", "2", "d", "f", "a", "a", "a", "1", "e", "5", "4", "e", "d", "5", "a", "8", "2", "a", "d", "f", "5", "e", "e", "5", "e", "e", "a", "f", "f", "d", "5", "8", "2", "e", "e", "d", "5", "e", "1", "a", "a", "1", "8", "2", "a", "e", "f", "1", "e", "5", "4", "5", "1", "1", "a", "d", "a", "5", "5", "f", "1", "f", "8", "2", "d", "8", "2", "f", "d", "a", "d", "4", "f", "f", "a", "e", "5", "8", "2", "8", "2", "f", "a", "a", "f", "1", "1", "8", "2", "1", "1", "1", "5", "a", "a", "f", "8", "2", "4", "a", "d", "e", "5", "d", "e", "a", "e", "d", "8", "2", "d", "8", "2", "8", "2", "a", "5", "5", "f", "1", "8", "2", "f", "1", "e", "a", "f", "f", "1", "5", "f", "5", "5", "d", "8", "2", "a", "d", "a", "8", "2", "e", "d", "1", "d", "8", "2", "f", "e", "e", "e", "a", "f", "8", "2", "f", "5", "a", "a", "1", "e", "d", "a", "f", "a", "d", "5", "5", "5", "f", "f", "f", "8", "2", "5", "e", "a", "5", "f", "5", "f", "d", "8", "2", "8", "2", "f", "d", "d", "8", "2", "1", "a", "8", "2", "e", "5", "8", "2", "8", "2", "1", "d", "1", "4", "5", "e", "f", "1", "8", "2", "5", "8", "2", "8", "2", "5", "a", "d", "e", "a", "f", "5", "d", "f", "5", "d", "e", "f", "f", "1", "a", "e", "f", "1", "f", "1", "d", "1", "f", "8", "2", "e", "5", "1", "8", "2", "8", "2", "f", "a", "8", "2", "d", "8", "2", "1", "8", "2", "f", "f", "f", "1", "e", "4", "8", "2", "f", "f", "8", "2", "e", "f", "1", "5", "5", "8", "2", "1", "1", "d", "e", "d", "e", "d", "5", "5", "e", "e", "a", "8", "2", "5", "f", "f", "e", "e", "1", "a", "f", "e", "f", "f", "f", "a", "5", "f", "a", "5", "1", "d", "5", "f", "1", "a", "8", "2", "a", "f", "1", "f", "a", "1", "d", "d", "e", "5", "1", "d", "d", "f", "8", "2", "e", "d", "d", "1", "4", "8", "2", "e", "a", "f", "e", "5", "d", "d", "a", "f", "8", "2", "5", "8", "2", "8", "2", "8", "2", "8", "2", "5", "a", "f", "5", "f", "e", "5", "f", "1", "5", "5", "1", "d", "5", "f", "a", "8", "2", "f", "5", "f", "e", "1", "a", "8", "2", "1", "d", "8", "2", "a", "d", "d", "f", "e", "f", "a", "d", "d", "d", "8", "2", "a", "a", "e", "5", "a", "5", "f", "8", "2", "a", "e", "5", "e", "f", "a", "5", "d", "f", "5", "5", "d", "5", "1", "a", "d", "1", "f", "5", "e", "1", "a", "d", "5", "d", "1", "e", "d", "8", "2", "1", "8", "2", "1", "d", "d", "d", "d", "8", "2", "1", "1", "a", "8", "2", "f", "f", "8", "2", "a", "f", "d", "5", "e", "a", "a", "f", "d", "8", "2", "1", "8", "2", "a", "1", "8", "2", "1", "1", "8", "2", "a", "1", "a", "e", "1", "d", "8", "2", "a", "a", "e", "1", "a", "1", "d", "1", "a", "f", "e", "f", "d", "f", "a", "e", "d", "e", "f", "1", "1", "f", "5", "f", "f", "8", "2", "1", "d", "8", "2", "d", "d", "d", "e", "1", "1", "d", "a", "5", "1", "1", "5", "4", "8", "2", "1", "8", "2", "1", "d", "8", "2", "8", "2", "5", "5", "d", "8", "2", "5", "d", "1", "f", "f", "4", "d", "8", "2", "1", "d", "8", "2", "a", "a", "e", "5", "1", "d", "5", "a", "a", "d", "e", "d", "1", "d", "e", "f", "d", "5", "f", "d", "d", "a", "8", "2", "d", "5", "8", "2", "e", "1", "1", "a", "4", "a", "1", "f", "1", "f", "f", "f", "f", "1", "1", "5", "d", "5", "5", "f", "8", "2", "5", "d", "a", "1", "8", "2", "5", "a", "e", "e", "f", "e", "5", "8", "2", "f", "8", "2", "f", "5", "a", "e", "5", "e", "d", "1", "5", "d", "a", "e", "5", "d", "8", "2", "a", "e", "f", "a", "d", "f", "f", "d", "e", "f", "a", "e", "d", "d", "1", "5", "1", "e", "e", "d", "a", "a", "e", "4", "d", "5", "f", "5", "1", "8", "2", "8", "2", "5", "1", "d", "f", "8", "2", "d", "5", "e", "d", "5", "e", "f", "5", "5", "a", "a", "d", "1", "e", "f", "a", "a", "8", "2", "f", "1", "e", "d", "f", "e", "a", "8", "2", "1", "e", "d", "f", "e", "e", "d", "1", "e", "e", "e", "e", "f", "5", "5", "e", "d", "1", "e", "5", "5", "1", "5", "d", "1", "1", "f", "1", "1", "d", "e", "d", "1", "a", "1", "5", "5", "8", "2", "e", "a", "1", "d", "5", "1", "f", "1", "a", "8", "2", "5", "a", "a", "f", "8", "2", "5", "d", "f", "d", "8", "2", "8", "2", "1", "1", "4", "f", "d", "d", "8", "2", "e", "d", "5", "e", "d", "8", "2", "a", "1", "e", "f", "a", "8", "2", "e", "1", "1", "d", "1", "f", "e", "1", "a", "8", "2", "d", "d", "5", "a", "f", "1", "1", "f", "5", "f", "5", "e", "1", "5", "8", "2", "a", "f", "a", "1", "d", "a", "5", "1", "1", "4", "a", "a", "f", "5", "5", "f", "d", "a", "d", "f", "f", "5", "e", "1", "e", "f", "1", "8", "2", "e", "5", "1", "a", "8", "2", "f", "a", "d", "e", "f", "8", "2", "f", "1", "a", "1", "e", "f", "8", "2", "f", "d", "e", "a", "e", "1", "a", "d", "5", "a", "5", "8", "2", "e", "f", "f", "d", "5", "8", "2", "8", "2", "f", "e", "a", "1", "8", "2", "5", "8", "2", "d", "1", "d", "a", "8", "2", "d", "a", "1", "d", "a", "8", "2", "d", "8", "2", "a", "1", "f", "8", "2", "d", "d", "1", "5", "e", "1", "e", "5", "5", "f", "d", "8", "2", "8", "2", "e", "e", "5", "f", "a", "5", "a", "5", "5", "8", "2", "5", "e", "d", "d", "f", "4", "1", "e", "5", "a", "1", "a", "e", "5", "1", "8", "2", "d", "1", "1", "a", "1", "1", "5", "8", "2", "a", "d", "a", "8", "2", "1", "f", "f", "5", "5", "a", "d", "e", "5", "5", "5", "1", "8", "2", "8", "2", "5", "f", "8", "2", "8", "2", "1", "f", "e", "a", "8", "2", "1", "5", "8", "2", "5", "f", "8", "2", "a", "d", "1", "d", "d", "e", "a", "5", "e", "d", "f", "1", "8", "2", "8", "2", "4", "d", "1", "d", "f", "f", "4", "d", "a", "8", "2", "8", "2", "f", "5", "a", "4", "d", "e", "d", "5", "d", "d", "e", "d", "a", "1", "d", "a", "a", "e", "a", "f", "5", "1", "a", "8", "2", "d", "e", "e", "5", "e", "e", "d", "d", "a", "d", "f", "e", "e", "1", "e", "e", "f", "5", "1", "a", "8", "2", "f", "5", "f", "d", "f", "e", "1", "1", "5", "f", "a", "f", "8", "2", "f", "1", "8", "2", "d", "a", "5", "1", "a", "5", "d", "a", "f", "d", "e", "5", "1", "d", "a", "d", "5", "a", "e", "e", "d", "1", "5", "8", "2", "5", "8", "2", "a", "e", "a", "1", "f", "5", "a", "f", "5", "a", "a", "d", "d", "5", "1", "e", "d", "5", "e", "a", "5", "d", "f", "a", "5", "a", "d", "8", "2", "8", "2", "a", "5", "8", "2", "f", "f", "5", "e", "1", "5", "a", "d", "f", "a", "5", "8", "2", "a", "5", "8", "2", "f", "8", "2", "8", "2", "e", "1", "8", "2", "e", "d", "f", "a", "8", "2", "f", "8", "2", "e", "f", "d", "e", "d", "5", "8", "2", "1", "5", "d", "5", "a", "1", "8", "2", "d", "8", "2", "e", "1", "8", "2", "d", "d", "1", "5", "d", "a", "5", "1", "a", "f", "5", "8", "2", "8", "2", "5", "8", "2", "e", "a", "f", "d", "5", "5", "1", "f", "5", "8", "2", "d", "f", "d", "a", "d", "1", "1", "8", "2", "8", "2", "5", "8", "2", "1", "5", "f", "d", "a", "4", "8", "2", "d", "8", "2", "a", "4", "8", "2", "e", "a", "f", "8", "2", "f", "f", "1", "5", "f", "d", "d", "5", "1", "d", "a", "e", "1", "a", "e", "a", "1", "8", "2", "1", "5", "a", "5", "1", "e", "1", "1", "1", "8", "2", "f", "5", "8", "2", "1", "d", "f", "5", "e", "f", "5", "a", "4", "5", "e", "d", "a", "d", "1", "e", "a", "a", "1", "e", "a", "e", "1", "8", "2", "1", "f", "a", "a", "e", "d", "f", "e", "f", "5", "1", "d", "5", "5", "8", "2", "1", "d", "1", "e", "e", "5", "d", "5", "e", "f", "f", "d", "a", "f", "d", "5", "d", "8", "2", "a", "e", "a", "5", "a", "d", "d", "5", "f", "4", "1", "f", "8", "2", "5", "8", "2", "a", "1", "a", "5", "e", "5", "d", "d", "5", "e", "e", "e", "1", "5", "4", "f", "f", "e", "1", "5", "a", "5", "8", "2", "a", "e", "5", "e", "5", "f", "e", "1", "e", "a", "f", "1", "e", "f", "e", "d", "e", "f", "8", "2", "d", "e", "d", "1", "d", "8", "2", "a", "1", "1", "1", "8", "2", "d", "d", "8", "2", "8", "2", "a", "d", "1", "1", "e", "e", "e", "5", "d", "a", "8", "2", "8", "2", "f", "1", "d", "a", "e", "e", "5", "f", "e", "1", "8", "2", "e", "1", "1", "5", "5", "1", "f", "5", "8", "2", "4", "8", "2", "a", "f", "e", "1", "5", "1", "a", "a", "8", "2", "5", "a", "1", "a", "f", "e", "d", "a", "5", "5", "8", "2", "8", "2", "5", "a", "a", "f", "e", "d", "8", "2", "a", "5", "1", "d", "5", "f", "4", "d", "5", "a", "4", "e", "e", "a", "8", "2", "d", "1", "a", "a", "a", "1", "1", "a", "f", "d", "a", "d", "a", "a", "e", "f", "f", "8", "2", "e", "1", "f", "e", "d", "f", "e", "e", "a", "8", "2", "8", "2", "a", "e", "d", "d", "5", "f", "e", "1", "8", "2", "f", "1", "f", "e", "f", "f", "1", "f", "f", "5", "8", "2", "a", "5", "5", "1", "d", "f", "8", "2", "e", "f", "a", "1", "e", "e", "d", "5", "f", "e", "e", "1", "a", "f", "e", "1", "d", "8", "2", "e", "5", "e", "5", "a", "d", "5", "8", "2", "d", "d", "d", "1", "a", "5", "e", "e", "5", "5", "4", "1", "8", "2", "5", "f", "1", "8", "2", "d", "a", "d", "e", "5", "8", "2", "f", "d", "d", "e", "a", "f", "1", "8", "2", "8", "2", "1", "d", "1", "8", "2", "f", "1", "d", "d", "a", "a", "5", "e", "f", "d", "d", "8", "2", "8", "2", "e", "1", "d", "a", "5", "a", "1", "d", "e", "5", "a", "f", "5", "1", "a", "5", "8", "2", "d", "e", "5", "f", "5", "8", "2", "a", "d", "8", "2", "5", "8", "2", "8", "2", "8", "2", "e", "d", "f", "f", "f", "e", "5", "8", "2", "a", "d", "1", "8", "2", "8", "2", "e", "5", "5", "1", "5", "5", "a", "e", "1", "1", "f", "d", "8", "2", "d", "5", "1", "8", "2", "1", "a", "e", "d", "a", "d", "8", "2", "a", "e", "a", "1", "a", "e", "e", "f", "f", "1", "a", "8", "2", "5", "1", "8", "2", "f", "d", "a", "5", "d", "a", "8", "2", "d", "f", "8", "2", "1", "e", "a", "e", "e", "e", "a", "d", "5", "a", "1", "f", "a", "e", "4", "a", "f", "8", "2", "a", "1", "5", "e", "5", "1", "1", "d", "5", "d", "e", "d", "d", "d", "f", "d", "8", "2", "f", "1", "a", "f", "1", "e", "d", "a", "1", "8", "2", "1", "1", "5", "8", "2", "d", "8", "2", "a", "f", "8", "2", "e", "f", "5", "5", "1", "f", "d", "d", "f", "d", "e", "5", "f", "f", "8", "2", "8", "2", "e", "1", "5", "1", "a", "d", "f", "e", "a", "1", "8", "2", "1", "f", "8", "2", "5", "8", "2", "8", "2", "f", "a", "5", "1", "f", "d", "8", "2", "a", "1", "f", "8", "2", "f", "f", "e", "5", "a", "a", "1", "f", "8", "2", "1", "e", "f", "1", "d", "a", "1", "f", "f", "5", "5", "f", "d", "d", "5", "5", "5", "4", "1", "d", "e", "f", "1", "5", "e", "1", "1", "d", "5", "d", "d", "e", "5", "a", "e", "f", "f", "8", "2", "d", "d", "f", "a", "8", "2", "1", "f", "8", "2", "5", "1", "8", "2", "8", "2", "1", "5", "d", "5", "e", "d", "e", "a", "d", "f", "1", "5", "d", "f", "d", "1", "8", "2", "e", "e", "5", "a", "4", "f", "8", "2", "8", "2", "f", "8", "2", "1", "a", "a", "4", "e", "5", "5", "f", "a", "e", "1", "5", "1", "d", "a", "1", "e", "1", "8", "2", "e", "d", "1", "e", "5", "a", "8", "2", "f", "f", "1", "e", "5", "8", "2", "e", "e", "5", "e", "1", "5", "f", "d", "a", "a", "e", "e", "1", "8", "2", "d", "5", "8", "2", "1", "1", "f", "a", "d", "f", "8", "2", "4", "8", "2", "a", "d", "d", "f", "a", "a", "e", "a", "1", "5", "f", "8", "2", "1", "5", "a", "a", "5", "d", "5", "1", "a", "8", "2", "d", "e", "a", "e", "1", "1", "d", "5", "a", "d", "d", "d", "8", "2", "d", "f", "1", "5", "8", "2", "1", "a", "f", "e", "d", "5", "f", "5", "1", "d", "1", "a", "d", "f", "1", "1", "4", "e", "a", "4", "a", "e", "e", "e", "f", "d", "a", "1", "e", "e", "e", "a", "e", "1", "1", "8", "2", "1", "e", "a", "5", "f", "5", "1", "4", "5", "d", "5", "1", "8", "2", "8", "2", "5", "e", "d", "1", "f", "a", "f", "8", "2", "4", "a", "d", "8", "2", "1", "1", "f", "1", "8", "2", "d", "a", "8", "2", "f", "8", "2", "e", "d", "e", "d", "f", "f", "8", "2", "a", "e", "e", "a", "5", "4", "f", "5", "d", "a", "8", "2", "a", "5", "5", "e", "5", "5", "f", "1", "a", "a", "f", "a", "a", "d", "d", "a", "1", "a", "d", "d", "d", "d", "a", "8", "2", "f", "a", "a", "f", "e", "a", "5", "1", "5", "a", "e", "f", "5", "d", "1", "f", "8", "2", "f", "a", "8", "2", "a", "5", "8", "2", "1", "1", "8", "2", "1", "d", "1", "f", "8", "2", "a", "5", "1", "f", "a", "d", "d", "f", "5", "5", "d", "d", "d", "f", "1", "f", "d", "f", "a", "5", "d", "1", "8", "2", "5", "d", "5", "d", "5", "f", "e", "a", "e", "1", "5", "8", "2", "1", "f", "e", "5", "a", "f", "8", "2", "f", "a", "5", "8", "2", "f", "1", "f", "1", "8", "2", "f", "a", "5", "f", "8", "2", "8", "2", "d", "1", "a", "8", "2", "5", "a", "5", "1", "d", "e", "1", "a", "d", "8", "2", "f", "f", "f", "1", "1", "8", "2", "1", "d", "1", "f", "e", "e", "f", "5", "f", "5", "8", "2", "a", "e", "1", "4", "1", "5", "5", "1", "8", "2", "8", "2", "e", "5", "f", "f", "d", "a", "a", "5", "1", "f", "d", "4", "e", "e", "e", "e", "1", "5", "e", "f", "a", "d", "e", "e", "5", "1", "f", "d", "f", "d", "5", "d", "d", "f", "1", "d", "1", "1", "1", "a", "1", "1", "e", "f", "5", "d", "1", "e", "1", "d", "5", "d", "5", "5", "d", "f", "d", "e", "d", "5", "d", "e", "5", "d", "d", "a", "4", "f", "e", "f", "5", "8", "2", "d", "a", "8", "2", "e", "e", "5", "d", "f", "e", "5", "5", "a", "5", "f", "a", "a", "8", "2", "e", "1", "a", "d", "1", "1", "a", "8", "2", "f", "f", "1", "f", "d", "a", "1", "e", "5", "a", "5", "e", "a", "d", "8", "2", "5", "8", "2", "1", "e", "f", "5", "e", "a", "d", "5", "5", "8", "2", "a", "5", "f", "8", "2", "1", "5", "5", "d", "a", "8", "2", "5", "1", "a", "f", "8", "2", "e", "5", "5", "8", "2", "8", "2", "d", "1", "e", "d", "e", "e", "f", "e", "f", "e", "e", "f", "f", "5", "8", "2", "1", "5", "f", "5", "8", "2", "1", "e", "f", "a", "5", "e", "d", "5", "4", "e", "a", "e", "e", "5", "d", "e", "e", "1", "f", "8", "2", "8", "2", "f", "f", "a", "d", "5", "4", "1", "f", "d", "a", "a", "1", "f", "5", "a", "8", "2", "a", "e", "e", "e", "d", "1", "d", "5", "1", "8", "2", "5", "5", "a", "d", "d", "5", "8", "2", "d", "d", "a", "5", "1", "f", "f", "5", "a", "5", "f", "f", "5", "d", "1", "8", "2", "d", "a", "1", "8", "2", "5", "e", "1", "5", "8", "2", "8", "2", "d", "5", "a", "a", "8", "2", "8", "2", "f", "8", "2"};
+    assertAnalyzesTo(analyzer, doc, expected);
+
+    doc = "b c g a f b d";
+    expected = new String[]{"1", "8", "2", "5", "a", "f", "1", "d"};
+    assertAnalyzesTo(analyzer, doc, expected);
+  }
+
+  public void testBufferLength() throws Exception {
+    String testFile =
+        "c => 8 2 5 6 7\n" +
+            "f c e d f, 1\n" +
+            "c g a f d, 6 5 5\n" +
+            "e c => 4\n" +
+            "g => 5\n" +
+            "a g b f e => 5 0 7 7\n" +
+            "b => 1";
+
+    Analyzer analyzer = new MockAnalyzer(random());
+    SolrSynonymParser parser = new SolrSynonymParser(true, true, analyzer);
+    parser.parse(new StringReader(testFile));
+    final SynonymMap map = parser.build();
+    analyzer.close();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, true, true));
+      }
+    };
+
+    String doc = "b c g a f b d";
+    String[] expected = new String[]{"1", "8", "2", "5", "6", "7", "5", "a", "f", "1", "d"};
+    assertAnalyzesTo(analyzer, doc, expected);
+  }
+
+  // TODO: test with the non expand all rule.
+
+
+  /**
+   * Tests some simple examples from the solr wiki
+   */
+  public void testSimple() throws Exception {
+    String testFile =
+        "i-pod, ipod, ipoooood\n" +
+            "foo => foo bar\n" +
+            "foo => baz\n" +
+            "this test, that testing";
+
+    Analyzer analyzer = new MockAnalyzer(random());
+    SolrSynonymParser parser = new SolrSynonymParser(true, true, analyzer);
+    parser.parse(new StringReader(testFile));
+    final SynonymMap map = parser.build();
+    analyzer.close();
+
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, true, true));
+      }
+    };
+
+    assertAnalyzesTo(analyzer, "ball",
+        new String[]{"ball"},
+        new int[]{1});
+
+    assertAnalyzesTo(analyzer, "i-pod",
+        new String[]{"i-pod", "ipod", "ipoooood"},
+        new int[]{1, 0, 0});
+
+    assertAnalyzesTo(analyzer, "foo",
+        new String[]{"foo", "baz", "bar"},
+        new int[]{1, 0, 1});
+
+    assertAnalyzesTo(analyzer, "this test",
+        new String[]{"this", "that", "test", "testing"},
+        new int[]{1, 0, 1, 0});
+    analyzer.close();
+  }
+
+  /**
+   * parse a syn file with bad syntax
+   */
+  public void testInvalidDoubleMap() throws Exception {
+    String testFile = "a => b => c";
+    Analyzer analyzer = new MockAnalyzer(random());
+    SolrSynonymParser parser = new SolrSynonymParser(true, true, analyzer);
+    try {
+      parser.parse(new StringReader(testFile));
+      fail("didn't get expected exception");
+    } catch (ParseException expected) {
+      // expected exc
+    }
+    analyzer.close();
+  }
+
+  /**
+   * parse a syn file with bad syntax
+   */
+  public void testInvalidAnalyzesToNothingOutput() throws Exception {
+    String testFile = "a => 1";
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, false);
+    SolrSynonymParser parser = new SolrSynonymParser(true, true, analyzer);
+    try {
+      parser.parse(new StringReader(testFile));
+      fail("didn't get expected exception");
+    } catch (ParseException expected) {
+      // expected exc
+    }
+    analyzer.close();
+  }
+
+  /**
+   * parse a syn file with some escaped syntax chars
+   */
+  public void testEscapedStuff() throws Exception {
+    String testFile =
+        "a\\=>a => b\\=>b\n" +
+            "a\\,a => b\\,b";
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);
+    SolrSynonymParser parser = new SolrSynonymParser(true, true, analyzer);
+    parser.parse(new StringReader(testFile));
+    final SynonymMap map = parser.build();
+    analyzer.close();
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.KEYWORD, false);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, false, true));
+      }
+    };
+
+    assertAnalyzesTo(analyzer, "ball",
+        new String[]{"ball"},
+        new int[]{1});
+
+    assertAnalyzesTo(analyzer, "a=>a",
+        new String[]{"b=>b"},
+        new int[]{1});
+
+    assertAnalyzesTo(analyzer, "a,a",
+        new String[]{"b,b"},
+        new int[]{1});
+    analyzer.close();
+  }
+
+  /**
+   * Verify type of token and positionLength after analyzer.
+   */
+  public void testPositionLengthAndTypeSimple() throws Exception {
+    String testFile =
+        "spider man, spiderman";
+
+    Analyzer analyzer = new MockAnalyzer(random());
+    SolrSynonymParser parser = new SolrSynonymParser(true, true, analyzer);
+    parser.parse(new StringReader(testFile));
+    final SynonymMap map = parser.build();
+    analyzer.close();
+
+    analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, true, true));
+      }
+    };
+
+    assertAnalyzesToPositions(analyzer, "spider man",
+        new String[]{"spider", "spiderman", "man"},
+        new String[]{"word", "SYNONYM", "word"},
+        new int[]{1, 0, 1},
+        new int[]{1, 2, 1});
+  }
+
+  private SynonymMap.Builder b;
+  private Tokenizer tokensIn;
+  private SynonymFilter tokensOut;
+  private CharTermAttribute termAtt;
+  private PositionIncrementAttribute posIncrAtt;
+  private PositionLengthAttribute posLenAtt;
+  private OffsetAttribute offsetAtt;
+
+  private void add(String input, String output, boolean keepOrig) {
+    if (VERBOSE) {
+      System.out.println("  add input=" + input + " output=" + output + " keepOrig=" + keepOrig);
+    }
+    CharsRefBuilder inputCharsRef = new CharsRefBuilder();
+    SynonymMap.Builder.join(input.split(" +"), inputCharsRef);
+
+    CharsRefBuilder outputCharsRef = new CharsRefBuilder();
+    SynonymMap.Builder.join(output.split(" +"), outputCharsRef);
+
+    b.add(inputCharsRef.get(), outputCharsRef.get(), keepOrig);
+  }
+
+  // For the output string: separate positions with a space,
+  // and separate multiple tokens at each position with a
+  // /.  If a token should have end offset != the input
+  // token's end offset then add :X to it:
+
+  // TODO: we should probably refactor this guy to use/take analyzer,
+  // the tests are a little messy
+  private void verify(String input, String output) throws Exception {
+    if (VERBOSE) {
+      System.out.println("TEST: verify input=" + input + " expectedOutput=" + output);
+    }
+
+    tokensIn.setReader(new StringReader(input));
+    tokensOut.reset();
+    final String[] expected = output.split(" ");
+    int expectedUpto = 0;
+    while (tokensOut.incrementToken()) {
+
+      if (VERBOSE) {
+        System.out.println("  incr token=" + termAtt.toString() + " posIncr=" + posIncrAtt.getPositionIncrement() + " startOff=" + offsetAtt.startOffset() + " endOff=" + offsetAtt.endOffset());
+      }
+
+      assertTrue(expectedUpto < expected.length);
+      final int startOffset = offsetAtt.startOffset();
+      final int endOffset = offsetAtt.endOffset();
+
+      final String[] expectedAtPos = expected[expectedUpto++].split("/");
+      for (int atPos = 0; atPos < expectedAtPos.length; atPos++) {
+        if (atPos > 0) {
+          assertTrue(tokensOut.incrementToken());
+          if (VERBOSE) {
+            System.out.println("  incr token=" + termAtt.toString() + " posIncr=" + posIncrAtt.getPositionIncrement() + " startOff=" + offsetAtt.startOffset() + " endOff=" + offsetAtt.endOffset());
+          }
+        }
+        final int colonIndex = expectedAtPos[atPos].indexOf(':');
+        final int underbarIndex = expectedAtPos[atPos].indexOf('_');
+        final String expectedToken;
+        final int expectedEndOffset;
+        final int expectedPosLen;
+        if (colonIndex != -1) {
+          expectedToken = expectedAtPos[atPos].substring(0, colonIndex);
+          if (underbarIndex != -1) {
+            expectedEndOffset = Integer.parseInt(expectedAtPos[atPos].substring(1 + colonIndex, underbarIndex));
+            expectedPosLen = Integer.parseInt(expectedAtPos[atPos].substring(1 + underbarIndex));
+          } else {
+            expectedEndOffset = Integer.parseInt(expectedAtPos[atPos].substring(1 + colonIndex));
+            expectedPosLen = 1;
+          }
+        } else if (underbarIndex != -1) {
+          expectedToken = expectedAtPos[atPos].substring(0, underbarIndex);
+          expectedPosLen = Integer.parseInt(expectedAtPos[atPos].substring(1 + underbarIndex));
+          expectedEndOffset = endOffset;
+        } else {
+          expectedToken = expectedAtPos[atPos];
+          expectedEndOffset = endOffset;
+          expectedPosLen = 1;
+        }
+        assertEquals(expectedToken, termAtt.toString());
+        assertEquals(atPos == 0 ? 1 : 0,
+            posIncrAtt.getPositionIncrement());
+        // start/end offset of all tokens at same pos should
+        // be the same:
+        assertEquals(startOffset, offsetAtt.startOffset());
+        assertEquals(expectedEndOffset, offsetAtt.endOffset());
+        assertEquals(expectedPosLen, posLenAtt.getPositionLength());
+      }
+    }
+    tokensOut.end();
+    tokensOut.close();
+    if (VERBOSE) {
+      System.out.println("  incr: END");
+    }
+    assertEquals(expectedUpto, expected.length);
+  }
+
+  public void testDontKeepOrig() throws Exception {
+    b = new SynonymMap.Builder(true);
+    add("a b", "foo", false);
+
+    final SynonymMap map = b.build();
+
+    final Analyzer analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, false, true));
+      }
+    };
+
+    assertAnalyzesTo(analyzer, "a b c",
+        new String[]{"foo", "c"},
+        new int[]{0, 4},
+        new int[]{3, 5},
+        null,
+        new int[]{1, 1},
+        new int[]{1, 1},
+        true);
+    checkAnalysisConsistency(random(), analyzer, false, "a b c");
+    analyzer.close();
+  }
+
+  public void testDoKeepOrig() throws Exception {
+    b = new SynonymMap.Builder(true);
+    add("a b", "foo", true);
+
+    final SynonymMap map = b.build();
+
+    final Analyzer analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, false, true));
+      }
+    };
+
+    assertAnalyzesTo(analyzer, "a b c",
+        new String[]{"a", "foo", "b", "c"},
+        new int[]{0, 0, 2, 4},
+        new int[]{1, 3, 3, 5},
+        null,
+        new int[]{1, 0, 1, 1},
+        new int[]{1, 2, 1, 1},
+        true);
+    checkAnalysisConsistency(random(), analyzer, false, "a b c");
+    analyzer.close();
+  }
+
+  public void testBasic() throws Exception {
+    b = new SynonymMap.Builder(true);
+    add("a", "foo", true);
+    add("a b", "bar fee", true);
+    add("b c", "dog collar", true);
+    add("c d", "dog harness holder extras", true);
+    add("m c e", "dog barks loudly", false);
+    add("i j k", "feep", true);
+
+    add("e f", "foo bar", false);
+    add("e f", "baz bee", false);
+
+    add("z", "boo", false);
+    add("y", "bee", true);
+
+    tokensIn = new MockTokenizer(MockTokenizer.WHITESPACE, true);
+    tokensIn.setReader(new StringReader("a"));
+    tokensIn.reset();
+    assertTrue(tokensIn.incrementToken());
+    assertFalse(tokensIn.incrementToken());
+    tokensIn.end();
+    tokensIn.close();
+
+    tokensOut = new SynonymFilter(tokensIn, b.build(), true, true);
+    termAtt = tokensOut.addAttribute(CharTermAttribute.class);
+    posIncrAtt = tokensOut.addAttribute(PositionIncrementAttribute.class);
+    posLenAtt = tokensOut.addAttribute(PositionLengthAttribute.class);
+    offsetAtt = tokensOut.addAttribute(OffsetAttribute.class);
+
+    verify("a b c", "a/bar b/fee c");
+
+    // syn output extends beyond input tokens
+    verify("x a b c d", "x a/bar b/fee dog/c harness/d_3 holder extras");
+
+    verify("a b a", "a/bar b/fee a/foo");
+
+    // outputs no longer add to one another:
+    verify("c d c d", "dog/c harness/d_3 holder extras dog/c harness/d_3 holder extras");
+
+    // two outputs for same input
+    verify("e f", "foo/baz bar/bee");
+
+    // verify multi-word / single-output offsets:
+    verify("g i j k g", "g i/feep:7_3 j k g");
+
+    // mixed keepOrig true/false:
+    verify("a m c e x", "a/foo dog barks loudly x");
+    verify("c d m c e x", "dog/c harness/d_3 holder extras dog barks loudly x");
+    assertTrue(tokensOut.getCaptureCount() > 0);
+
+    // no captureStates when no syns matched
+    verify("p q r s t", "p q r s t");
+    assertEquals(0, tokensOut.getCaptureCount());
+
+    // no captureStates when only single-input syns, w/ no
+    // lookahead needed, matched
+    verify("p q z y t", "p q boo y/bee t");
+    assertEquals(0, tokensOut.getCaptureCount());
+  }
+
+  private String randomNonEmptyString() {
+    while (true) {
+      final String s = TestUtil.randomUnicodeString(random()).trim();
+      if (s.length() != 0 && s.indexOf('\u0000') == -1) {
+        return s;
+      }
+    }
+  }
+
+  /**
+   * simple random test, doesn't verify correctness.
+   * does verify it doesnt throw exceptions, or that the stream doesn't misbehave
+   */
+  public void testRandom2() throws Exception {
+    final int numIters = atLeast(3);
+    for (int i = 0; i < numIters; i++) {
+      b = new SynonymMap.Builder(random().nextBoolean());
+      final int numEntries = atLeast(10);
+      for (int j = 0; j < numEntries; j++) {
+        add(randomNonEmptyString(), randomNonEmptyString(), random().nextBoolean());
+      }
+      final SynonymMap map = b.build();
+      final boolean ignoreCase = random().nextBoolean();
+
+      final Analyzer analyzer = new Analyzer() {
+        @Override
+        protected TokenStreamComponents createComponents(String fieldName) {
+          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);
+          return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, ignoreCase, true));
+        }
+      };
+
+      checkRandomData(random(), analyzer, 100);
+      analyzer.close();
+    }
+  }
+
+  // Adds MockGraphTokenFilter after SynFilter:
+  public void testRandom2GraphAfter() throws Exception {
+    final int numIters = atLeast(3);
+    Random random = random();
+    for (int i = 0; i < numIters; i++) {
+      b = new SynonymMap.Builder(random.nextBoolean());
+      final int numEntries = atLeast(10);
+      for (int j = 0; j < numEntries; j++) {
+        add(randomNonEmptyString(), randomNonEmptyString(), random.nextBoolean());
+      }
+      final SynonymMap map = b.build();
+      final boolean ignoreCase = random.nextBoolean();
+
+      final Analyzer analyzer = new Analyzer() {
+        @Override
+        protected TokenStreamComponents createComponents(String fieldName) {
+          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);
+          TokenStream syns = new SynonymFilter(tokenizer, map, ignoreCase, true);
+          TokenStream graph = new MockGraphTokenFilter(random(), syns);
+          return new TokenStreamComponents(tokenizer, graph);
+        }
+      };
+
+      checkRandomData(random, analyzer, 100);
+      analyzer.close();
+    }
+  }
+
+  public void testEmptyTerm() throws IOException {
+    Random random = random();
+    final int numIters = atLeast(10);
+    for (int i = 0; i < numIters; i++) {
+      b = new SynonymMap.Builder(random.nextBoolean());
+      final int numEntries = atLeast(10);
+      for (int j = 0; j < numEntries; j++) {
+        add(randomNonEmptyString(), randomNonEmptyString(), random.nextBoolean());
+      }
+      final SynonymMap map = b.build();
+      final boolean ignoreCase = random.nextBoolean();
+
+      final Analyzer analyzer = new Analyzer() {
+        @Override
+        protected TokenStreamComponents createComponents(String fieldName) {
+          Tokenizer tokenizer = new KeywordTokenizer();
+          return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, ignoreCase, true));
+        }
+      };
+
+      checkAnalysisConsistency(random, analyzer, random.nextBoolean(), "");
+      analyzer.close();
+    }
+  }
+
+  /**
+   * simple random test like testRandom2, but for larger docs
+   */
+  public void testRandomHuge() throws Exception {
+    Random random = random();
+    final int numIters = atLeast(3);
+    for (int i = 0; i < numIters; i++) {
+      b = new SynonymMap.Builder(random.nextBoolean());
+      final int numEntries = atLeast(10);
+      if (VERBOSE) {
+        System.out.println("TEST: iter=" + i + " numEntries=" + numEntries);
+      }
+      for (int j = 0; j < numEntries; j++) {
+        add(randomNonEmptyString(), randomNonEmptyString(), random.nextBoolean());
+      }
+      final SynonymMap map = b.build();
+      final boolean ignoreCase = random.nextBoolean();
+
+      final Analyzer analyzer = new Analyzer() {
+        @Override
+        protected TokenStreamComponents createComponents(String fieldName) {
+          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);
+          return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, ignoreCase, true));
+        }
+      };
+
+      checkRandomData(random, analyzer, 100, 1024);
+      analyzer.close();
+    }
+  }
+
+  // LUCENE-3375
+  public void testVanishingTerms() throws Exception {
+    String testFile =
+        "aaa => aaaa1 aaaa2 aaaa3\n" +
+            "bbb => bbbb1 bbbb2\n";
+    Analyzer synAnalyzer = new MockAnalyzer(random());
+    SolrSynonymParser parser = new SolrSynonymParser(true, true, synAnalyzer);
+    parser.parse(new StringReader(testFile));
+    final SynonymMap map = parser.build();
+    synAnalyzer.close();
+
+    Analyzer analyzer = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, true, true));
+      }
+    };
+
+    // where did my pot go?! right there! no longer overlaps.
+    assertAnalyzesTo(analyzer, "xyzzy bbb pot of gold",
+        new String[]{"xyzzy", "bbbb1", "bbbb2", "pot", "of", "gold"});
+
+    // this one no longer nukes 'pot' and 'of'
+    // xyzzy aaa pot of gold -> xyzzy aaaa1 aaaa2 aaaa3 gold
+    assertAnalyzesTo(analyzer, "xyzzy aaa pot of gold",
+        new String[]{"xyzzy", "aaaa1", "aaaa2", "aaaa3", "pot", "of", "gold"});
+    analyzer.close();
+  }
+
+  public void testBasic2() throws Exception {
+    boolean keepOrig = true;
+    do {
+      keepOrig = !keepOrig;
+
+      b = new SynonymMap.Builder(true);
+      add("aaa", "aaaa1 aaaa2 aaaa3", keepOrig);
+      add("bbb", "bbbb1 bbbb2", keepOrig);
+      tokensIn = new MockTokenizer(MockTokenizer.WHITESPACE,
+          true);
+      tokensIn.setReader(new StringReader("a"));
+      tokensIn.reset();
+      assertTrue(tokensIn.incrementToken());
+      assertFalse(tokensIn.incrementToken());
+      tokensIn.end();
+      tokensIn.close();
+
+      tokensOut = new SynonymFilter(tokensIn, b.build(), true, true);
+      termAtt = tokensOut.addAttribute(CharTermAttribute.class);
+      posIncrAtt = tokensOut.addAttribute(PositionIncrementAttribute.class);
+      posLenAtt = tokensOut.addAttribute(PositionLengthAttribute.class);
+      offsetAtt = tokensOut.addAttribute(OffsetAttribute.class);
+
+      if (keepOrig) {
+        verify("xyzzy bbb pot of gold", "xyzzy bbbb1/bbb bbbb2 potof gold");
+        verify("xyzzy aaa pot of gold", "xyzzy aaaa1/aaa aaaa2 aaaa3 pot of gold");
+      } else {
+        verify("xyzzy bbb pot of gold", "xyzzy bbbb1 bbbb2 pot of gold");
+        verify("xyzzy aaa pot of gold", "xyzzy aaaa1 aaaa2 aaaa3 pot of gold");
+      }
+    } while (keepOrig);
+  }
+
+  public void testMatching() throws Exception {
+    b = new SynonymMap.Builder(true);
+    final boolean keepOrig = false;
+    add("a b", "ab", keepOrig);
+    add("a c", "ac", keepOrig);
+    add("a", "aa", keepOrig);
+    add("b", "bb", keepOrig);
+    add("z x c v", "zxcv", keepOrig);
+    add("x c", "xc", keepOrig);
+    final SynonymMap map = b.build();
+    Analyzer a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, true, true));
+      }
+    };
+
+    checkOneTerm(a, "$", "$");
+    checkOneTerm(a, "a", "aa");
+    checkOneTerm(a, "b", "bb");
+
+    assertAnalyzesTo(a, "a $",
+        new String[]{"aa", "$"},
+        new int[]{1, 1});
+
+    assertAnalyzesTo(a, "$ a",
+        new String[]{"$", "aa"},
+        new int[]{1, 1});
+
+    assertAnalyzesTo(a, "a a",
+        new String[]{"aa", "aa"},
+        new int[]{1, 1});
+
+    assertAnalyzesTo(a, "z x c v",
+        new String[]{"zxcv"},
+        new int[]{1});
+
+    assertAnalyzesTo(a, "z x c $",
+        new String[]{"z", "xc", "$"},
+        new int[]{1, 1, 1});
+    a.close();
+  }
+
+  public void testRepeatsOff() throws Exception {
+    b = new SynonymMap.Builder(true);
+    final boolean keepOrig = false;
+    add("a b", "ab", keepOrig);
+    add("a b", "ab", keepOrig);
+    add("a b", "ab", keepOrig);
+    final SynonymMap map = b.build();
+    Analyzer a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, true, true));
+      }
+    };
+
+    assertAnalyzesTo(a, "a b",
+        new String[]{"ab"},
+        new int[]{1});
+    a.close();
+  }
+
+  public void testRepeatsOn() throws Exception {
+    b = new SynonymMap.Builder(false);
+    final boolean keepOrig = false;
+    add("a b", "ab", keepOrig);
+    add("a b", "ab", keepOrig);
+    add("a b", "ab", keepOrig);
+    final SynonymMap map = b.build();
+    Analyzer a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, true, true));
+      }
+    };
+
+    assertAnalyzesTo(a, "a b",
+        new String[]{"ab", "ab", "ab"},
+        new int[]{1, 0, 0});
+    a.close();
+  }
+
+  public void testRecursion() throws Exception {
+    b = new SynonymMap.Builder(true);
+    final boolean keepOrig = false;
+    add("zoo", "zoo", keepOrig);
+    final SynonymMap map = b.build();
+    Analyzer a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, true, true));
+      }
+    };
+
+    assertAnalyzesTo(a, "zoo zoo $ zoo",
+        new String[]{"zoo", "zoo", "$", "zoo"},
+        new int[]{1, 1, 1, 1});
+    a.close();
+  }
+
+  public void testRecursion2() throws Exception {
+    b = new SynonymMap.Builder(true);
+    final boolean keepOrig = false;
+    add("zoo", "zoo", keepOrig);
+    add("zoo", "zoo zoo", keepOrig);
+    final SynonymMap map = b.build();
+    Analyzer a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, true, true));
+      }
+    };
+
+    assertAnalyzesTo(a, "zoo zoo $ zoo",
+        new String[]{"zoo", "zoo", "zoo", "zoo", "zoo", "zoo", "$", "zoo", "zoo", "zoo"},
+        new int[]{1, 0, 1, 1, 0, 1, 1, 1, 0, 1});
+    a.close();
+  }
+
+  public void testOutputHangsOffEnd() throws Exception {
+    b = new SynonymMap.Builder(true);
+    final boolean keepOrig = false;
+    // b hangs off the end (no input token under it):
+    add("a", "a b", keepOrig);
+    tokensIn = new MockTokenizer(MockTokenizer.WHITESPACE, true);
+    tokensIn.setReader(new StringReader("a"));
+    tokensIn.reset();
+    assertTrue(tokensIn.incrementToken());
+    assertFalse(tokensIn.incrementToken());
+    tokensIn.end();
+    tokensIn.close();
+
+    tokensOut = new SynonymFilter(tokensIn, b.build(), true, true);
+    termAtt = tokensOut.addAttribute(CharTermAttribute.class);
+    posIncrAtt = tokensOut.addAttribute(PositionIncrementAttribute.class);
+    offsetAtt = tokensOut.addAttribute(OffsetAttribute.class);
+    posLenAtt = tokensOut.addAttribute(PositionLengthAttribute.class);
+
+    // Make sure endOffset inherits from previous input token:
+    verify("a", "a b:1");
+  }
+
+  public void testIncludeOrig() throws Exception {
+    b = new SynonymMap.Builder(true);
+    final boolean keepOrig = true;
+    add("a b", "ab", keepOrig);
+    add("a c", "ac", keepOrig);
+    add("a", "aa", keepOrig);
+    add("b", "bb", keepOrig);
+    add("z x c v", "zxcv", keepOrig);
+    add("x c", "xc", keepOrig);
+    final SynonymMap map = b.build();
+    Analyzer a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, true, true));
+      }
+    };
+
+    assertAnalyzesTo(a, "$",
+        new String[]{"$"},
+        new int[]{1});
+    assertAnalyzesTo(a, "a",
+        new String[]{"a", "aa"},
+        new int[]{1, 0});
+    assertAnalyzesTo(a, "a",
+        new String[]{"a", "aa"},
+        new int[]{1, 0});
+    assertAnalyzesTo(a, "$ a",
+        new String[]{"$", "a", "aa"},
+        new int[]{1, 1, 0});
+    assertAnalyzesTo(a, "a $",
+        new String[]{"a", "aa", "$"},
+        new int[]{1, 0, 1});
+    assertAnalyzesTo(a, "$ a !",
+        new String[]{"$", "a", "aa", "!"},
+        new int[]{1, 1, 0, 1});
+    assertAnalyzesTo(a, "a a",
+        new String[]{"a", "aa", "a", "aa"},
+        new int[]{1, 0, 1, 0});
+    assertAnalyzesTo(a, "b",
+        new String[]{"b", "bb"},
+        new int[]{1, 0});
+    assertAnalyzesTo(a, "z x c v",
+        new String[]{"z", "zxcv", "x", "c", "v"},
+        new int[]{1, 0, 1, 1, 1});
+    assertAnalyzesTo(a, "z x c $",
+        new String[]{"z", "x", "xc", "c", "$"},
+        new int[]{1, 1, 0, 1, 1});
+    a.close();
+  }
+
+  public void testRecursion3() throws Exception {
+    b = new SynonymMap.Builder(true);
+    final boolean keepOrig = true;
+    add("zoo zoo", "zoo", keepOrig);
+    final SynonymMap map = b.build();
+    Analyzer a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, true, true));
+      }
+    };
+
+    assertAnalyzesTo(a, "zoo zoo $ zoo",
+        new String[]{"zoo", "zoo", "zoo", "$", "zoo"},
+        new int[]{1, 0, 1, 1, 1});
+    a.close();
+  }
+
+  public void testRecursion4() throws Exception {
+    b = new SynonymMap.Builder(true);
+    final boolean keepOrig = true;
+    add("zoo zoo", "zoo", keepOrig);
+    add("zoo", "zoo zoo", keepOrig);
+    final SynonymMap map = b.build();
+    Analyzer a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, true, true));
+      }
+    };
+
+    assertAnalyzesTo(a, "zoo zoo $ zoo",
+        new String[]{"zoo", "zoo", "zoo", "$", "zoo", "zoo", "zoo"},
+        new int[]{1, 0, 1, 1, 1, 0, 1});
+    a.close();
+  }
+
+  public void testMultiwordOffsets() throws Exception {
+    b = new SynonymMap.Builder(true);
+    final boolean keepOrig = true;
+    add("national hockey league", "nhl", keepOrig);
+    final SynonymMap map = b.build();
+    Analyzer a = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(tokenizer, new SynonymFilter(tokenizer, map, true, true));
+      }
+    };
+
+    assertAnalyzesTo(a, "national hockey league",
+        new String[]{"national", "nhl", "hockey", "league"},
+        new int[]{0, 0, 9, 16},
+        new int[]{8, 22, 15, 22},
+        new int[]{1, 0, 1, 1});
+    a.close();
+  }
+
+  public void testEmpty() throws Exception {
+    Tokenizer tokenizer = new MockTokenizer();
+    tokenizer.setReader(new StringReader("aa bb"));
+    try {
+      new SynonymFilter(tokenizer, new SynonymMap.Builder(true).build(), true, true);
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+      assertEquals("fst must be non-null", iae.getMessage());
+    }
+  }
+
+  /**
+   * Helper method to validate all strings that can be generated from a token stream.
+   * Uses {@link TokenStreamToAutomaton} to create an automaton. Asserts the finite strings of the automaton are all
+   * and only the given valid strings.
+   * @param analyzer analyzer containing the SynonymFilter under test.
+   * @param text text to be analyzed.
+   * @param expectedStrings all expected finite strings.
+   */
+  public void assertAllStrings(Analyzer analyzer, String text, String[] expectedStrings) throws IOException {
+    TokenStream tokenStream = analyzer.tokenStream("dummy", text);
+    try {
+      Automaton automaton = new TokenStreamToAutomaton().toAutomaton(tokenStream);
+      Set<IntsRef> finiteStrings = Operations.getFiniteStrings(automaton, -1);
+
+      assertEquals("Invalid resulting strings count. Expected " + expectedStrings.length + " was " + finiteStrings.size(),
+          expectedStrings.length, finiteStrings.size());
+
+      Set<String> expectedStringsSet = new HashSet<>(Arrays.asList(expectedStrings));
+
+      BytesRefBuilder scratchBytesRefBuilder = new BytesRefBuilder();
+      for (IntsRef ir: finiteStrings) {
+        String s = Util.toBytesRef(ir, scratchBytesRefBuilder).utf8ToString().replace((char) TokenStreamToAutomaton.POS_SEP, ' ');
+        assertTrue("Unexpected string found: " + s, expectedStringsSet.contains(s));
+      }
+    } finally {
+      tokenStream.close();
+    }
+  }
+}
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java	(revision 1688508)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java	(revision )
@@ -78,7 +78,15 @@
  * do so.  This limitation is necessary because Lucene's
  * TokenStream (and index) cannot yet represent an arbitrary
  * graph.</p>
- *
+ * 
+ * <p><b>NOTE</b>: 
+ * If <code>fixMultiWordPosLength=true</code>, the matched 
+ * terms no longer generate a sausage, but a better graph.
+ * In order to do this, the longest match (in number of 
+ * terms) is the first output, other rules (and the input,
+ * if <code>keepOrig=true</code>) are stacked on top, with
+ * correct position lengths.</p>
+ * 
  * <p><b>NOTE</b>: If multiple incoming tokens arrive on the
  * same position, only the first token at that position is
  * used for parsing.  Subsequent tokens simply pass through
@@ -108,6 +116,8 @@
 //
 // Another possible solution is described at http://www.cis.uni-muenchen.de/people/Schulz/Pub/dictle5.ps
 
+// TODO Guarantee that the multiword fix solves the PhraseQuery issue.
+
 public final class SynonymFilter extends TokenFilter {
 
   public static final String TYPE_SYNONYM = "SYNONYM";
@@ -117,10 +127,10 @@
   private final boolean ignoreCase;
   private final int rollBufferSize;
 
+  private final boolean fixMultiWordPosLength;
+  
   private int captureCount;
 
-  // TODO: we should set PositionLengthAttr too...
-
   private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
   private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
   private final PositionLengthAttribute posLenAtt = addAttribute(PositionLengthAttribute.class);
@@ -146,13 +156,27 @@
     boolean consumed = true;
     int startOffset;
     int endOffset;
+    int positionLength;
     
     public void reset() {
       state = null;
       consumed = true;
       keepOrig = false;
       matched = false;
+      positionLength = 0;
     }
+
+    public void copyTo(PendingInput dest) {
+      assert dest.consumed : "Can only copy to an empty, consumed input.";
+      dest.term.copyChars(term.get());
+      dest.state = state == null ? null : state.clone();
+      dest.keepOrig = keepOrig;
+      dest.matched = matched;
+      dest.consumed = consumed;
+      dest.startOffset = startOffset;
+      dest.endOffset = endOffset;
+      dest.positionLength = positionLength;
+    }
   };
 
   // Rolling buffer, holding pending input tokens we had to
@@ -170,6 +194,8 @@
     int posIncr = 1;
     int lastEndOffset;
     int lastPosLength;
+    int originalInputIdx = -1;
+    boolean lastIsSynonym;
 
     public PendingOutputs() {
       outputs = new CharsRefBuilder[1];
@@ -180,12 +206,14 @@
     public void reset() {
       upto = count = 0;
       posIncr = 1;
+      originalInputIdx = -1;
     }
 
     public CharsRef pullNext() {
       assert upto < count;
       lastEndOffset = endOffsets[upto];
       lastPosLength = posLengths[upto];
+      lastIsSynonym = originalInputIdx < 0 || upto != originalInputIdx;
       final CharsRefBuilder result = outputs[upto++];
       posIncr = 0;
       if (upto == count) {
@@ -227,8 +255,24 @@
       posLengths[count] = posLength;
       count++;
     }
-  };
 
+    public void copyTo(PendingOutputs dest) {
+      assert dest.upto == 0 && dest.count == 0 : "Can only copy pending output to an empty one.";
+      for (int i = 0; i < count; i++) {
+        CharsRef chars = outputs[i].get();
+        dest.add(chars.chars, chars.offset, chars.length, endOffsets[i], posLengths[i]);
+      }
+      // Should not be necessary ...
+      dest.posIncr = posIncr;
+      dest.lastEndOffset = lastEndOffset;
+      dest.lastPosLength = lastPosLength;
+    }
+
+    public boolean isSynonym() {
+      return lastIsSynonym;
+    }
+  }
+
   private final ByteArrayDataInput bytesReader = new ByteArrayDataInput();
 
   // Rolling buffer, holding stack of pending synonym
@@ -262,20 +306,44 @@
    *                   the input entries when you create the {@link SynonymMap}
    */
   public SynonymFilter(TokenStream input, SynonymMap synonyms, boolean ignoreCase) {
+    this(input, synonyms, ignoreCase, false);
+  }
+
+  /**
+   * @param input input tokenstream
+   * @param synonyms synonym map
+   * @param ignoreCase case-folds input for matching with {@link Character#toLowerCase(int)}.
+   *                   Note, if you set this to true, it's your responsibility to lowercase
+   *                   the input entries when you create the {@link SynonymMap}
+   * @param fixMultiWordPosLength correctly generate a graph for multi word synonyms.
+   *                              This has the side effect that the output may be 
+   *                              longer than the input, with new tokens added in
+   *                              the middle, by synonyms that are longer that the 
+   *                              original input.
+   */
+  public SynonymFilter(TokenStream input, SynonymMap synonyms, boolean ignoreCase, boolean fixMultiWordPosLength) {
     super(input);
     this.synonyms = synonyms;
     this.ignoreCase = ignoreCase;
+    this.fixMultiWordPosLength = fixMultiWordPosLength;
     this.fst = synonyms.fst;
     if (fst == null) {
       throw new IllegalArgumentException("fst must be non-null");
     }
     this.fstReader = fst.getBytesReader();
 
+    if (!fixMultiWordPosLength) {
-    // Must be 1+ so that when roll buffer is at full
-    // lookahead we can distinguish this full buffer from
-    // the empty buffer:
+      // Must be 1+ so that when roll buffer is at full
+      // lookahead we can distinguish this full buffer from
+      // the empty buffer:
-    rollBufferSize = 1+synonyms.maxHorizontalContext;
+      rollBufferSize = 1 + synonyms.maxHorizontalContext;
+    } else {
+      // It's twice max horizontal context to be greater that what will be needed
+      // when adding fake input/output (synonyms have more words thant actual terms)
+      // and lookahead was used.
+      rollBufferSize = 1 + 2 * synonyms.maxHorizontalContext;
+    }
-
+    
     futureInputs = new PendingInput[rollBufferSize];
     futureOutputs = new PendingOutputs[rollBufferSize];
     for(int pos=0;pos<rollBufferSize;pos++) {
@@ -288,6 +356,7 @@
     scratchArc = new FST.Arc<>();
   }
 
+
   private void capture() {
     captureCount++;
     //System.out.println("  capture slot=" + nextWrite);
@@ -458,58 +527,211 @@
     bytesReader.reset(bytes.bytes, bytes.offset, bytes.length);
 
     final int code = bytesReader.readVInt();
-    final boolean keepOrig = (code & 0x1) == 0;
+    boolean keepOrig = (code & 0x1) == 0;
     final int count = code >>> 1;
     //System.out.println("  addOutput count=" + count + " keepOrig=" + keepOrig);
-    for(int outputIDX=0;outputIDX<count;outputIDX++) {
-      synonyms.words.get(bytesReader.readVInt(),
-                         scratchBytes);
-      //System.out.println("    outIDX=" + outputIDX + " bytes=" + scratchBytes.length);
+    final int[] synonymsIdxs = new int[count];
+
+    // If keepOrig, must consider positionLength of matched input to find
+    // the max position length.
+    int synonymMaxPositionLength = keepOrig ? matchInputLength : 0;
+
+    // Calculate the max positions of the synonyms
+    for (int i = 0; i < count; i++) {
+      synonymsIdxs[i] = bytesReader.readVInt();
+
+      int synonymLength = getSynonymLength(synonymsIdxs[i], scratchBytes);
+
+      if (synonymLength > synonymMaxPositionLength) {
+        synonymMaxPositionLength = synonymLength;
+
+        if (fixMultiWordPosLength) {
+          // Make sure the first synonym is the one with the maximum length.
+          // So that when new terms are created, we create the longest first.
+          int aux = synonymsIdxs[0];
+          synonymsIdxs[0] = synonymsIdxs[i];
+          synonymsIdxs[i] = aux;
+        }
+      }
+    }
+
+    boolean moreSynonymTerms = synonymMaxPositionLength > matchInputLength;
+
+    // If keep original but a synonym has more terms, the synonym should appear first on output, so posIncr and posLength are coherent (both 1),
+    // actual terms appear later with type "word".
+    int firstSynonynIdx = 0;
+    if (fixMultiWordPosLength) {
+      if (keepOrig && moreSynonymTerms) {
+        //  change keepOrig to false
+        keepOrig = false;
+
+        // Collect matched terms, and create a fake synonym structure of them, to reuse current methods.
+        final CharsRefBuilder matchChars = new CharsRefBuilder();
+        int upto = nextRead;
+        for (int idx = 0; idx < matchInputLength - 1; idx++) {
+          matchChars.append(futureInputs[upto].term.chars(), 0, futureInputs[upto].term.length());
+          matchChars.append(SynonymMap.WORD_SEPARATOR);
+          futureOutputs[upto].originalInputIdx = 1; // Mark this output as having an original input.
+          upto = rollIncr(upto);
+        }
+
+        // Last term, handled differently ...
+        // TODO Is this the right way to know if we should use current input?
+        if (futureInputs[rollIncr(upto)].consumed && termAtt.length() > 0) {
+          // Last term of the match is current term, not in future inputs.
+          matchChars.append(termAtt.buffer(), 0, termAtt.length());
+        } else {
+          matchChars.append(futureInputs[upto].term.chars(), 0, futureInputs[upto].term.length());
+        }
+        futureOutputs[upto].originalInputIdx = 1;
+
+        // Adjust future inputs for longer synonyms
+        matchInputLength = adjustFutureOutput(matchInputLength, synonymMaxPositionLength);
+
+        //  add longer synonym
+        if (count > 0) { // Sanity check ...
+          synonyms.words.get(synonymsIdxs[0], scratchBytes);
-      scratchChars.copyUTF8Bytes(scratchBytes);
+          scratchChars.copyUTF8Bytes(scratchBytes);
+          addSynonymToOutput(scratchChars, matchInputLength, matchEndOffset, keepOrig, synonymMaxPositionLength);
+          firstSynonynIdx = 1;
+        }
+
+        // add currently matched terms to future outputs. The future output was previously marked as having an original
+        // input at this position, so they will have type "word" instead of "SYNONYM".
+        addSynonymToOutput(matchChars, matchInputLength, matchEndOffset, keepOrig, synonymMaxPositionLength);
+      } else if (moreSynonymTerms) {
+        // Adjust future inputs for longer synonyms
+        matchInputLength = adjustFutureOutput(matchInputLength, synonymMaxPositionLength);
+      }
+    }
+
+    for (int outputIDX = firstSynonynIdx; outputIDX < count; outputIDX++) {
+      synonyms.words.get(synonymsIdxs[outputIDX], scratchBytes);
+      scratchChars.copyUTF8Bytes(scratchBytes);
+      addSynonymToOutput(scratchChars, matchInputLength, matchEndOffset, keepOrig, synonymMaxPositionLength);
+    }
+
+    int upto = nextRead;
+    for (int idx = 0; idx < matchInputLength; idx++) {
+      futureInputs[upto].keepOrig |= keepOrig;
+      futureInputs[upto].matched = true;
+      upto = rollIncr(upto);
+    }
+  }
+
+  private void addSynonymToOutput(CharsRefBuilder synonymChars, int matchInputLength, int matchEndOffset, boolean keepOrig, int synonymMaxPositionLength) {
       int lastStart = 0;
-      final int chEnd = lastStart + scratchChars.length();
+    final int chEnd = lastStart + synonymChars.length();
+    int countWords = 0;
+
       int outputUpto = nextRead;
       for(int chIDX=lastStart;chIDX<=chEnd;chIDX++) {
-        if (chIDX == chEnd || scratchChars.charAt(chIDX) == SynonymMap.WORD_SEPARATOR) {
+        if (chIDX == chEnd || synonymChars.charAt(chIDX) == SynonymMap.WORD_SEPARATOR) {
           final int outputLen = chIDX - lastStart;
-          // Caller is not allowed to have empty string in
-          // the output:
-          assert outputLen > 0: "output contains empty string: " + scratchChars;
+          // Caller is not allowed to have empty string in the output:
+          assert outputLen > 0 : "output contains empty string: " + synonymChars;
           final int endOffset;
           final int posLen;
-          if (chIDX == chEnd && lastStart == 0) {
+        
+          if (chIDX == chEnd && lastStart == 0 && !fixMultiWordPosLength) {
             // This rule had a single output token, so, we set
             // this output's endOffset to the current
             // endOffset (ie, endOffset of the last input
             // token it matched):
             endOffset = matchEndOffset;
             posLen = keepOrig ? matchInputLength : 1;
+          } else if (chIDX == chEnd && fixMultiWordPosLength) {
+            // Always set the end offset of the last output
+            // to the current endOffset (ie, endOffset of the last input
+            // token it matched):
+            endOffset = matchEndOffset;
+            posLen = synonymMaxPositionLength - countWords;
           } else {
             // This rule has more than one output token; we
-            // can't pick any particular endOffset for this
-            // case, so, we inherit the endOffset for the
-            // input token which this output overlaps:
+            // can't pick any particular endOffset for the
+            // intermediary words, so, we inherit the endOffset
+            // for the input token which this output overlaps:
             endOffset = -1;
             posLen = 1;
+            countWords++;
           }
-          futureOutputs[outputUpto].add(scratchChars.chars(), lastStart, outputLen, endOffset, posLen);
-          //System.out.println("      " + new String(scratchChars.chars, lastStart, outputLen) + " outputUpto=" + outputUpto);
+
+          futureOutputs[outputUpto].add(synonymChars.chars(), lastStart, outputLen, endOffset, posLen);
           lastStart = 1+chIDX;
-          //System.out.println("  slot=" + outputUpto + " keepOrig=" + keepOrig);
           outputUpto = rollIncr(outputUpto);
           assert futureOutputs[outputUpto].posIncr == 1: "outputUpto=" + outputUpto + " vs nextWrite=" + nextWrite;
         }
       }
     }
 
-    int upto = nextRead;
-    for(int idx=0;idx<matchInputLength;idx++) {
-      futureInputs[upto].keepOrig |= keepOrig;
-      futureInputs[upto].matched = true;
-      upto = rollIncr(upto);
+  /**
+   * Prepare future output to add the synonyms terms that are longer than the input,
+   * pushing forward the lookahead output that was there previously.
+   * Also push ahead future input on the same position, to keep both arrays in sync.
+   */
+  public int adjustFutureOutput(int matchInputLength, int synonymMaxPositionLength) {
+    // Synonyms shorter or equal to input, nothing to do.
+    if (matchInputLength >= synonymMaxPositionLength) {
+      return matchInputLength;
     }
+
+    int lookaheadIdx = nextRead;
+
+    // Step over current input.
+    for (int i = 0; i < matchInputLength - 1; i++) {
+      lookaheadIdx = rollIncr(lookaheadIdx);
-  }
+    }
 
+    // Two decisions that could use optimizations later ...
+    //  - push both futureOutputs and future inputs, so the indices stay in sync, and the rest of the logic is still valid
+    //  - push one at a time, can probably be done a single time since we know the max lenght of the synonyms.
+    int extraTerms = synonymMaxPositionLength - matchInputLength;
+    for (int i = 0; i < extraTerms; i++) {
+      lookaheadIdx = rollIncr(lookaheadIdx);
+
+      // If there is lookahead input/output ...
+      if (lookaheadIdx != nextWrite) {
+        int newLookaheadIdx = nextWrite;
+        int oldLookaheadIdx;
+        do {
+          oldLookaheadIdx = rollDecr(newLookaheadIdx);
+
+          futureOutputs[oldLookaheadIdx].copyTo(futureOutputs[newLookaheadIdx]);
+          futureInputs[oldLookaheadIdx].copyTo(futureInputs[newLookaheadIdx]);
+          futureOutputs[oldLookaheadIdx].reset();
+          futureInputs[oldLookaheadIdx].reset();
+          newLookaheadIdx = oldLookaheadIdx;
+          assert oldLookaheadIdx != nextWrite && oldLookaheadIdx != nextRead : "oldLookaheadIdx=" + oldLookaheadIdx + " vs nextWrite=" + nextWrite + " & nextRead=" + nextRead;
+        } while (oldLookaheadIdx != lookaheadIdx);
+      }
+
+      futureOutputs[lookaheadIdx].reset();
+      futureInputs[lookaheadIdx].reset();
+      futureInputs[rollDecr(lookaheadIdx)].copyTo(futureInputs[lookaheadIdx]);
+      // Fix this fake input so it does not interfere with others
+      futureInputs[lookaheadIdx].keepOrig = false;
+      futureInputs[lookaheadIdx].matched = true;
+
+      // Has to skip the newly added fake input.
+      inputSkipCount++;
+
+      nextWrite = rollIncr(nextWrite);
+    }
+
+    return matchInputLength + extraTerms;
+  }
+
+  private int getSynonymLength(int synonymsIdx, BytesRef scratchBytes) {
+    synonyms.words.get(synonymsIdx, scratchBytes);
+    int synonymLength = 1;
+    for (int j = scratchBytes.offset; j < scratchBytes.offset + scratchBytes.length; j++) {
+      if (scratchBytes.bytes[j] == SynonymMap.WORD_SEPARATOR) {
+        synonymLength++;
+      }
+    }
+    return synonymLength;
+  }
+
   // ++ mod rollBufferSize
   private int rollIncr(int count) {
     count++;
@@ -520,6 +742,16 @@
     }
   }
 
+  // -- mod rollBufferSize
+  private int rollDecr(int count) {
+    count--;
+    if (count == -1) {
+      return rollBufferSize - 1;
+    } else {
+      return count;
+    }
+  }
+
   // for testing
   int getCaptureCount() {
     return captureCount;
@@ -551,6 +783,9 @@
             // Return a previously saved token (because we
             // had to lookahead):
             restoreState(input.state);
+            if (input.positionLength > 0) {
+              posLenAtt.setPositionLength(input.positionLength);
+            }
           } else {
             // Pass-through case: return token we just pulled
             // but didn't capture:
@@ -572,8 +807,10 @@
           final int posIncr = outputs.posIncr;
           final CharsRef output = outputs.pullNext();
           clearAttributes();
-          termAtt.copyBuffer(output.chars, output.offset, output.length);
+          if (outputs.isSynonym()) {
-          typeAtt.setType(TYPE_SYNONYM);
+            typeAtt.setType(TYPE_SYNONYM);
+          }
+          termAtt.copyBuffer(output.chars, output.offset, output.length);
           int endOffset = outputs.getLastEndOffset();
           if (endOffset == -1) {
             endOffset = input.endOffset;
@@ -613,7 +850,9 @@
           // Keep offset from last input token:
           offsetAtt.setOffset(lastStartOffset, lastEndOffset);
           termAtt.copyBuffer(output.chars, output.offset, output.length);
+          if (outputs.isSynonym()) {
-          typeAtt.setType(TYPE_SYNONYM);
+            typeAtt.setType(TYPE_SYNONYM);
+          }
           //System.out.println("  set posIncr=" + outputs.posIncr + " outputs=" + outputs);
           posIncrAtt.setPositionIncrement(posIncr);
           //System.out.println("  return token=" + termAtt.toString());
