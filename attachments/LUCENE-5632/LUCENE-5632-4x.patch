Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicAnalyzer.java	(working copy)
@@ -137,7 +137,7 @@
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
       Reader reader) {
-    final Tokenizer source = matchVersion.onOrAfter(Version.LUCENE_31) ? 
+    final Tokenizer source = matchVersion.onOrAfter(Version.LUCENE_3_1) ? 
         new StandardTokenizer(matchVersion, reader) : new ArabicLetterTokenizer(matchVersion, reader);
     TokenStream result = new LowerCaseFilter(matchVersion, source);
     // the order here is important: the stopword list is not normalized!
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/ca/CatalanAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/ca/CatalanAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/ca/CatalanAnalyzer.java	(working copy)
@@ -135,7 +135,7 @@
       Reader reader) {
     final Tokenizer source = new StandardTokenizer(matchVersion, reader);
     TokenStream result = new StandardFilter(matchVersion, source);
-    if (matchVersion.onOrAfter(Version.LUCENE_36)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_3_6)) {
       result = new ElisionFilter(result, DEFAULT_ARTICLES);
     }
     result = new LowerCaseFilter(matchVersion, result);
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java	(working copy)
@@ -89,7 +89,7 @@
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
       Reader reader) {
-    if (matchVersion.onOrAfter(Version.LUCENE_36)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_3_6)) {
       final Tokenizer source = new StandardTokenizer(matchVersion, reader);
       // run the widthfilter first before bigramming, it sometimes combines characters.
       TokenStream result = new CJKWidthFilter(source);
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/CompoundWordTokenFilterBase.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/CompoundWordTokenFilterBase.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/CompoundWordTokenFilterBase.java	(working copy)
@@ -157,7 +157,7 @@
       int startOff = CompoundWordTokenFilterBase.this.offsetAtt.startOffset();
       int endOff = CompoundWordTokenFilterBase.this.offsetAtt.endOffset();
       
-      if (matchVersion.onOrAfter(Version.LUCENE_44) ||
+      if (matchVersion.onOrAfter(Version.LUCENE_4_4) ||
           endOff - startOff != CompoundWordTokenFilterBase.this.termAtt.length()) {
         // if length by start + end offsets doesn't match the term text then assume
         // this is a synonym and don't adjust the offsets.
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java	(working copy)
@@ -127,8 +127,8 @@
    * @return {@link org.apache.lucene.analysis.Analyzer.TokenStreamComponents}
    *         built from a {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
-   *         , and {@link CzechStemFilter} (only if version is >= LUCENE_31). If
-   *         a version is >= LUCENE_31 and a stem exclusion set is provided via
+   *         , and {@link CzechStemFilter} (only if version is >= LUCENE_3_1). If
+   *         a version is >= LUCENE_3_1 and a stem exclusion set is provided via
    *         {@link #CzechAnalyzer(Version, CharArraySet, CharArraySet)} a
    *         {@link SetKeywordMarkerFilter} is added before
    *         {@link CzechStemFilter}.
@@ -140,7 +140,7 @@
     TokenStream result = new StandardFilter(matchVersion, source);
     result = new LowerCaseFilter(matchVersion, result);
     result = new StopFilter( matchVersion, result, stopwords);
-    if (matchVersion.onOrAfter(Version.LUCENE_31)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_3_1)) {
       if(!this.stemExclusionTable.isEmpty())
         result = new SetKeywordMarkerFilter(result, stemExclusionTable);
       result = new CzechStemFilter(result);
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanAnalyzer.java	(working copy)
@@ -127,7 +127,7 @@
    */
   public GermanAnalyzer(Version matchVersion) {
     this(matchVersion,
-        matchVersion.onOrAfter(Version.LUCENE_31) ? DefaultSetHolder.DEFAULT_SET
+        matchVersion.onOrAfter(Version.LUCENE_3_1) ? DefaultSetHolder.DEFAULT_SET
             : DefaultSetHolder.DEFAULT_SET_30);
   }
   
@@ -177,10 +177,10 @@
     result = new LowerCaseFilter(matchVersion, result);
     result = new StopFilter( matchVersion, result, stopwords);
     result = new SetKeywordMarkerFilter(result, exclusionSet);
-    if (matchVersion.onOrAfter(Version.LUCENE_36)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_3_6)) {
       result = new GermanNormalizationFilter(result);
       result = new GermanLightStemFilter(result);
-    } else if (matchVersion.onOrAfter(Version.LUCENE_31)) {
+    } else if (matchVersion.onOrAfter(Version.LUCENE_3_1)) {
       result = new SnowballFilter(result, new German2Stemmer());
     } else {
       result = new GermanStemFilter(result);
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/el/GreekAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/el/GreekAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/el/GreekAnalyzer.java	(working copy)
@@ -114,10 +114,10 @@
       Reader reader) {
     final Tokenizer source = new StandardTokenizer(matchVersion, reader);
     TokenStream result = new GreekLowerCaseFilter(matchVersion, source);
-    if (matchVersion.onOrAfter(Version.LUCENE_31))
+    if (matchVersion.onOrAfter(Version.LUCENE_3_1))
       result = new StandardFilter(matchVersion, result);
     result = new StopFilter(matchVersion, result, stopwords);
-    if (matchVersion.onOrAfter(Version.LUCENE_31))
+    if (matchVersion.onOrAfter(Version.LUCENE_3_1))
       result = new GreekStemFilter(result);
     return new TokenStreamComponents(source, result);
   }
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java	(working copy)
@@ -105,7 +105,7 @@
     final Tokenizer source = new StandardTokenizer(matchVersion, reader);
     TokenStream result = new StandardFilter(matchVersion, source);
     // prior to this we get the classic behavior, standardfilter does it for us.
-    if (matchVersion.onOrAfter(Version.LUCENE_31))
+    if (matchVersion.onOrAfter(Version.LUCENE_3_1))
       result = new EnglishPossessiveFilter(matchVersion, result);
     result = new LowerCaseFilter(matchVersion, result);
     result = new StopFilter(matchVersion, result, stopwords);
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishPossessiveFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishPossessiveFilter.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishPossessiveFilter.java	(working copy)
@@ -44,7 +44,7 @@
    */
   @Deprecated
   public EnglishPossessiveFilter(TokenStream input) {
-    this(Version.LUCENE_35, input);
+    this(Version.LUCENE_3_5, input);
   }
 
   public EnglishPossessiveFilter(Version version, TokenStream input) {
@@ -63,7 +63,7 @@
     
     if (bufferLength >= 2 && 
         (buffer[bufferLength-2] == '\'' || 
-         (matchVersion.onOrAfter(Version.LUCENE_36) && (buffer[bufferLength-2] == '\u2019' || buffer[bufferLength-2] == '\uFF07'))) &&
+         (matchVersion.onOrAfter(Version.LUCENE_3_6) && (buffer[bufferLength-2] == '\u2019' || buffer[bufferLength-2] == '\uFF07'))) &&
         (buffer[bufferLength-1] == 's' || buffer[bufferLength-1] == 'S')) {
       termAtt.setLength(bufferLength - 2); // Strip last 2 characters off
     }
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/es/SpanishAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/es/SpanishAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/es/SpanishAnalyzer.java	(working copy)
@@ -133,7 +133,7 @@
     result = new StopFilter(matchVersion, result, stopwords);
     if(!stemExclusionSet.isEmpty())
       result = new SetKeywordMarkerFilter(result, stemExclusionSet);
-    if (matchVersion.onOrAfter(Version.LUCENE_36)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_3_6)) {
       result = new SpanishLightStemFilter(result);
     } else {
       result = new SnowballFilter(result, new SpanishStemmer());
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/fa/PersianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/fa/PersianAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/fa/PersianAnalyzer.java	(working copy)
@@ -118,7 +118,7 @@
   protected TokenStreamComponents createComponents(String fieldName,
       Reader reader) {
     final Tokenizer source;
-    if (matchVersion.onOrAfter(Version.LUCENE_31)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_3_1)) {
       source = new StandardTokenizer(matchVersion, reader);
     } else {
       source = new ArabicLetterTokenizer(matchVersion, reader);
@@ -139,7 +139,7 @@
    */
   @Override
   protected Reader initReader(String fieldName, Reader reader) {
-    return matchVersion.onOrAfter(Version.LUCENE_31) ? 
+    return matchVersion.onOrAfter(Version.LUCENE_3_1) ? 
        new PersianCharFilter(reader) :
        reader;
   }
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java	(working copy)
@@ -141,7 +141,7 @@
    */
   public FrenchAnalyzer(Version matchVersion) {
     this(matchVersion,
-        matchVersion.onOrAfter(Version.LUCENE_31) ? DefaultSetHolder.DEFAULT_STOP_SET
+        matchVersion.onOrAfter(Version.LUCENE_3_1) ? DefaultSetHolder.DEFAULT_STOP_SET
             : DefaultSetHolder.DEFAULT_STOP_SET_30);
   }
   
@@ -189,7 +189,7 @@
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
       Reader reader) {
-    if (matchVersion.onOrAfter(Version.LUCENE_31)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_3_1)) {
       final Tokenizer source = new StandardTokenizer(matchVersion, reader);
       TokenStream result = new StandardFilter(matchVersion, source);
       result = new ElisionFilter(result, DEFAULT_ARTICLES);
@@ -197,7 +197,7 @@
       result = new StopFilter(matchVersion, result, stopwords);
       if(!excltable.isEmpty())
         result = new SetKeywordMarkerFilter(result, excltable);
-      if (matchVersion.onOrAfter(Version.LUCENE_36)) {
+      if (matchVersion.onOrAfter(Version.LUCENE_3_6)) {
         result = new FrenchLightStemFilter(result);
       } else {
         result = new SnowballFilter(result, new org.tartarus.snowball.ext.FrenchStemmer());
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/ga/IrishAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/ga/IrishAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/ga/IrishAnalyzer.java	(working copy)
@@ -138,7 +138,7 @@
     final Tokenizer source = new StandardTokenizer(matchVersion, reader);
     TokenStream result = new StandardFilter(matchVersion, source);
     StopFilter s = new StopFilter(matchVersion, result, HYPHENATIONS);
-    if (!matchVersion.onOrAfter(Version.LUCENE_44)) {
+    if (!matchVersion.onOrAfter(Version.LUCENE_4_4)) {
       s.setEnablePositionIncrements(false);
     }
     result = s;
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/hi/HindiAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/hi/HindiAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/hi/HindiAnalyzer.java	(working copy)
@@ -127,7 +127,7 @@
   protected TokenStreamComponents createComponents(String fieldName,
       Reader reader) {
     final Tokenizer source;
-    if (matchVersion.onOrAfter(Version.LUCENE_36)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_3_6)) {
       source = new StandardTokenizer(matchVersion, reader);
     } else {
       source = new IndicTokenizer(matchVersion, reader);
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/it/ItalianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/it/ItalianAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/it/ItalianAnalyzer.java	(working copy)
@@ -140,14 +140,14 @@
       Reader reader) {
     final Tokenizer source = new StandardTokenizer(matchVersion, reader);
     TokenStream result = new StandardFilter(matchVersion, source);
-    if (matchVersion.onOrAfter(Version.LUCENE_32)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_3_2)) {
       result = new ElisionFilter(result, DEFAULT_ARTICLES);
     }
     result = new LowerCaseFilter(matchVersion, result);
     result = new StopFilter(matchVersion, result, stopwords);
     if(!stemExclusionSet.isEmpty())
       result = new SetKeywordMarkerFilter(result, stemExclusionSet);
-    if (matchVersion.onOrAfter(Version.LUCENE_36)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_3_6)) {
       result = new ItalianLightStemFilter(result);
     } else {
       result = new SnowballFilter(result, new ItalianStemmer());
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/RemoveDuplicatesTokenFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/RemoveDuplicatesTokenFilter.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/RemoveDuplicatesTokenFilter.java	(working copy)
@@ -35,7 +35,7 @@
   private final PositionIncrementAttribute posIncAttribute =  addAttribute(PositionIncrementAttribute.class);
   
   // use a fixed version, as we don't care about case sensitivity.
-  private final CharArraySet previous = new CharArraySet(Version.LUCENE_31, 8, false);
+  private final CharArraySet previous = new CharArraySet(Version.LUCENE_3_1, 8, false);
 
   /**
    * Creates a new RemoveDuplicatesTokenFilter
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/TrimFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/TrimFilter.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/TrimFilter.java	(working copy)
@@ -46,7 +46,7 @@
   @Deprecated
   public TrimFilter(Version version, TokenStream in, boolean updateOffsets) {
     super(in);
-    if (updateOffsets && version.onOrAfter(Version.LUCENE_44)) {
+    if (updateOffsets && version.onOrAfter(Version.LUCENE_4_4)) {
       throw new IllegalArgumentException("updateOffsets=true is not supported anymore as of Lucene 4.4");
     }
     this.updateOffsets = updateOffsets;
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilter.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilter.java	(working copy)
@@ -208,7 +208,7 @@
    */
   public WordDelimiterFilter(Version matchVersion, TokenStream in, byte[] charTypeTable, int configurationFlags, CharArraySet protWords) {
     super(in);
-    if (!matchVersion.onOrAfter(Version.LUCENE_48)) {
+    if (!matchVersion.onOrAfter(Version.LUCENE_4_8)) {
       throw new IllegalArgumentException("This class only works with Lucene 4.8+. To emulate the old (broken) behavior of WordDelimiterFilter, use Lucene47WordDelimiterFilter");
     }
     this.flags = configurationFlags;
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilterFactory.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilterFactory.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilterFactory.java	(working copy)
@@ -118,7 +118,7 @@
 
   @Override
   public TokenFilter create(TokenStream input) {
-    if (luceneMatchVersion.onOrAfter(Version.LUCENE_48)) {
+    if (luceneMatchVersion.onOrAfter(Version.LUCENE_4_8)) {
       return new WordDelimiterFilter(luceneMatchVersion, input, typeTable == null ? WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE : typeTable,
                                    flags, protectedWords);
     } else {
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java	(working copy)
@@ -110,7 +110,7 @@
       throw new IllegalArgumentException("version must not be null");
     }
 
-    if (version.onOrAfter(Version.LUCENE_44) && side == Side.BACK) {
+    if (version.onOrAfter(Version.LUCENE_4_4) && side == Side.BACK) {
       throw new IllegalArgumentException("Side.BACK is not supported anymore as of Lucene 4.4, use ReverseStringFilter up-front and afterward");
     }
 
@@ -127,7 +127,7 @@
     }
 
     this.version = version;
-    this.charUtils = version.onOrAfter(Version.LUCENE_44)
+    this.charUtils = version.onOrAfter(Version.LUCENE_4_4)
         ? CharacterUtils.getInstance(version)
         : CharacterUtils.getJava4Instance();
     this.minGram = minGram;
@@ -174,7 +174,7 @@
           curGramSize = minGram;
           tokStart = offsetAtt.startOffset();
           tokEnd = offsetAtt.endOffset();
-          if (version.onOrAfter(Version.LUCENE_44)) {
+          if (version.onOrAfter(Version.LUCENE_4_4)) {
             // Never update offsets
             updateOffsets = false;
           } else {
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerFactory.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerFactory.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerFactory.java	(working copy)
@@ -52,7 +52,7 @@
 
   @Override
   public Tokenizer create(AttributeFactory factory, Reader input) {
-    if (luceneMatchVersion.onOrAfter(Version.LUCENE_44)) {
+    if (luceneMatchVersion.onOrAfter(Version.LUCENE_4_4)) {
       if (!EdgeNGramTokenFilter.Side.FRONT.getLabel().equals(side)) {
         throw new IllegalArgumentException(EdgeNGramTokenizer.class.getSimpleName() + " does not support backward n-grams as of Lucene 4.4");
       }
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43EdgeNGramTokenizer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43EdgeNGramTokenizer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43EdgeNGramTokenizer.java	(working copy)
@@ -182,7 +182,7 @@
       throw new IllegalArgumentException("minGram must not be greater than maxGram");
     }
 
-    if (version.onOrAfter(Version.LUCENE_44)) {
+    if (version.onOrAfter(Version.LUCENE_4_4)) {
       if (side == Side.BACK) {
         throw new IllegalArgumentException("Side.BACK is not supported anymore as of Lucene 4.4");
       }
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java	(working copy)
@@ -41,7 +41,7 @@
  * increasing length (meaning that "abc" will give "a", "ab", "abc", "b", "bc",
  * "c").</li></ul>
  * <p>You can make this filter use the old behavior by providing a version &lt;
- * {@link Version#LUCENE_44} in the constructor but this is not recommended as
+ * {@link Version#LUCENE_4_4} in the constructor but this is not recommended as
  * it will lead to broken {@link TokenStream}s that will cause highlighting
  * bugs.
  * <p>If you were using this {@link TokenFilter} to perform partial highlighting,
@@ -83,7 +83,7 @@
   public NGramTokenFilter(Version version, TokenStream input, int minGram, int maxGram) {
     super(new CodepointCountFilter(version, input, minGram, Integer.MAX_VALUE));
     this.version = version;
-    this.charUtils = version.onOrAfter(Version.LUCENE_44)
+    this.charUtils = version.onOrAfter(Version.LUCENE_4_4)
         ? CharacterUtils.getInstance(version)
         : CharacterUtils.getJava4Instance();
     if (minGram < 1) {
@@ -94,7 +94,7 @@
     }
     this.minGram = minGram;
     this.maxGram = maxGram;
-    if (version.onOrAfter(Version.LUCENE_44)) {
+    if (version.onOrAfter(Version.LUCENE_4_4)) {
       posIncAtt = addAttribute(PositionIncrementAttribute.class);
       posLenAtt = addAttribute(PositionLengthAttribute.class);
     } else {
@@ -149,7 +149,7 @@
           hasIllegalOffsets = (tokStart + curTermLength) != tokEnd;
         }
       }
-      if (version.onOrAfter(Version.LUCENE_44)) {
+      if (version.onOrAfter(Version.LUCENE_4_4)) {
         if (curGramSize > maxGram || (curPos + curGramSize) > curCodePointCount) {
           ++curPos;
           curGramSize = minGram;
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer.java	(working copy)
@@ -120,10 +120,10 @@
   }
 
   private void init(Version version, int minGram, int maxGram, boolean edgesOnly) {
-    if (!version.onOrAfter(Version.LUCENE_44)) {
+    if (!version.onOrAfter(Version.LUCENE_4_4)) {
       throw new IllegalArgumentException("This class only works with Lucene 4.4+. To emulate the old (broken) behavior of NGramTokenizer, use Lucene43NGramTokenizer/Lucene43EdgeNGramTokenizer");
     }
-    charUtils = version.onOrAfter(Version.LUCENE_44)
+    charUtils = version.onOrAfter(Version.LUCENE_4_4)
         ? CharacterUtils.getInstance(version)
         : CharacterUtils.getJava4Instance();
     if (minGram < 1) {
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizerFactory.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizerFactory.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizerFactory.java	(working copy)
@@ -52,7 +52,7 @@
   /** Creates the {@link TokenStream} of n-grams from the given {@link Reader} and {@link AttributeFactory}. */
   @Override
   public Tokenizer create(AttributeFactory factory, Reader input) {
-    if (luceneMatchVersion.onOrAfter(Version.LUCENE_44)) {
+    if (luceneMatchVersion.onOrAfter(Version.LUCENE_4_4)) {
       return new NGramTokenizer(luceneMatchVersion, factory, input, minGramSize, maxGramSize);
     } else {
       return new Lucene43NGramTokenizer(factory, input, minGramSize, maxGramSize);
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java	(working copy)
@@ -133,7 +133,7 @@
     // historically, this ctor never the stem dict!!!!!
     // so we populate it only for >= 3.6
     this(matchVersion, stopwords, CharArraySet.EMPTY_SET, 
-        matchVersion.onOrAfter(Version.LUCENE_36) 
+        matchVersion.onOrAfter(Version.LUCENE_3_6) 
         ? DefaultSetHolder.DEFAULT_STEM_DICT 
         : CharArrayMap.<String>emptyMap());
   }
@@ -142,7 +142,7 @@
     // historically, this ctor never the stem dict!!!!!
     // so we populate it only for >= 3.6
     this(matchVersion, stopwords, stemExclusionTable,
-        matchVersion.onOrAfter(Version.LUCENE_36)
+        matchVersion.onOrAfter(Version.LUCENE_3_6)
         ? DefaultSetHolder.DEFAULT_STEM_DICT
         : CharArrayMap.<String>emptyMap());
   }
@@ -151,7 +151,7 @@
     this.matchVersion = matchVersion;
     this.stoptable = CharArraySet.unmodifiableSet(CharArraySet.copy(matchVersion, stopwords));
     this.excltable = CharArraySet.unmodifiableSet(CharArraySet.copy(matchVersion, stemExclusionTable));
-    if (stemOverrideDict.isEmpty() || !matchVersion.onOrAfter(Version.LUCENE_31)) {
+    if (stemOverrideDict.isEmpty() || !matchVersion.onOrAfter(Version.LUCENE_3_1)) {
       this.stemdict = null;
       this.origStemdict = CharArrayMap.unmodifiableMap(CharArrayMap.copy(matchVersion, stemOverrideDict));
     } else {
@@ -185,7 +185,7 @@
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
       Reader aReader) {
-    if (matchVersion.onOrAfter(Version.LUCENE_31)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_3_1)) {
       final Tokenizer source = new StandardTokenizer(matchVersion, aReader);
       TokenStream result = new StandardFilter(matchVersion, source);
       result = new LowerCaseFilter(matchVersion, result);
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/position/PositionFilterFactory.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/position/PositionFilterFactory.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/position/PositionFilterFactory.java	(working copy)
@@ -51,7 +51,7 @@
     if (!args.isEmpty()) {
       throw new IllegalArgumentException("Unknown parameters: " + args);
     }
-    if (luceneMatchVersion != null && luceneMatchVersion.onOrAfter(Version.LUCENE_44)) {
+    if (luceneMatchVersion != null && luceneMatchVersion.onOrAfter(Version.LUCENE_4_4)) {
       throw new IllegalArgumentException("PositionFilter is deprecated as of Lucene 4.4. You should either fix your code to not use it or use Lucene 4.3 version compatibility");
     }
   }
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/pt/PortugueseAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/pt/PortugueseAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/pt/PortugueseAnalyzer.java	(working copy)
@@ -133,7 +133,7 @@
     result = new StopFilter(matchVersion, result, stopwords);
     if(!stemExclusionSet.isEmpty())
       result = new SetKeywordMarkerFilter(result, stemExclusionSet);
-    if (matchVersion.onOrAfter(Version.LUCENE_36)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_3_6)) {
       result = new PortugueseLightStemFilter(result);
     } else {
       result = new SnowballFilter(result, new PortugueseStemmer());
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/reverse/ReverseStringFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/reverse/ReverseStringFilter.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/reverse/ReverseStringFilter.java	(working copy)
@@ -176,7 +176,7 @@
    */
   public static void reverse(Version matchVersion, final char[] buffer,
       final int start, final int len) {
-    if (!matchVersion.onOrAfter(Version.LUCENE_31)) {
+    if (!matchVersion.onOrAfter(Version.LUCENE_3_1)) {
       reverseUnicode3(buffer, start, len);
       return;
     }
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianAnalyzer.java	(working copy)
@@ -108,7 +108,7 @@
 
     public RussianAnalyzer(Version matchVersion) {
       this(matchVersion,
-        matchVersion.onOrAfter(Version.LUCENE_31) ? DefaultSetHolder.DEFAULT_STOP_SET
+        matchVersion.onOrAfter(Version.LUCENE_3_1) ? DefaultSetHolder.DEFAULT_STOP_SET
             : DefaultSetHolder.DEFAULT_STOP_SET_30);
     }
   
@@ -152,7 +152,7 @@
     @Override
     protected TokenStreamComponents createComponents(String fieldName,
         Reader reader) {
-      if (matchVersion.onOrAfter(Version.LUCENE_31)) {
+      if (matchVersion.onOrAfter(Version.LUCENE_3_1)) {
         final Tokenizer source = new StandardTokenizer(matchVersion, reader);
         TokenStream result = new StandardFilter(matchVersion, source);
         result = new LowerCaseFilter(matchVersion, result);
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballAnalyzer.java	(working copy)
@@ -71,11 +71,11 @@
     Tokenizer tokenizer = new StandardTokenizer(matchVersion, reader);
     TokenStream result = new StandardFilter(matchVersion, tokenizer);
     // remove the possessive 's for english stemmers
-    if (matchVersion.onOrAfter(Version.LUCENE_31) && 
+    if (matchVersion.onOrAfter(Version.LUCENE_3_1) && 
         (name.equals("English") || name.equals("Porter") || name.equals("Lovins")))
       result = new EnglishPossessiveFilter(result);
     // Use a special lowercase filter for turkish, the stemmer expects it.
-    if (matchVersion.onOrAfter(Version.LUCENE_31) && name.equals("Turkish"))
+    if (matchVersion.onOrAfter(Version.LUCENE_3_1) && name.equals("Turkish"))
       result = new TurkishLowerCaseFilter(result);
     else
       result = new LowerCaseFilter(matchVersion, result);
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardFilter.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardFilter.java	(working copy)
@@ -45,7 +45,7 @@
   
   @Override
   public final boolean incrementToken() throws IOException {
-    if (matchVersion.onOrAfter(Version.LUCENE_31))
+    if (matchVersion.onOrAfter(Version.LUCENE_3_1))
       return input.incrementToken(); // TODO: add some niceties for the new grammar
     else
       return incrementTokenClassic();
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java	(working copy)
@@ -144,13 +144,13 @@
   }
 
   private final void init(Version matchVersion) {
-    if (matchVersion.onOrAfter(Version.LUCENE_47)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_4_7)) {
       this.scanner = new StandardTokenizerImpl(input);
-    } else if (matchVersion.onOrAfter(Version.LUCENE_40)) {
+    } else if (matchVersion.onOrAfter(Version.LUCENE_4_0)) {
       this.scanner = new StandardTokenizerImpl40(input);
-    } else if (matchVersion.onOrAfter(Version.LUCENE_34)) {
+    } else if (matchVersion.onOrAfter(Version.LUCENE_3_4)) {
       this.scanner = new StandardTokenizerImpl34(input);
-    } else if (matchVersion.onOrAfter(Version.LUCENE_31)) {
+    } else if (matchVersion.onOrAfter(Version.LUCENE_3_1)) {
       this.scanner = new StandardTokenizerImpl31(input);
     } else {
       this.scanner = new ClassicTokenizerImpl(input);
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/package.html
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/package.html	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std31/package.html	(working copy)
@@ -17,6 +17,6 @@
 -->
 <html><head></head>
 <body>
-Backwards-compatible implementation to match {@link org.apache.lucene.util.Version#LUCENE_31}
+Backwards-compatible implementation to match {@link org.apache.lucene.util.Version#LUCENE_3_1}
 </body>
 </html>
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/package.html
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/package.html	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std34/package.html	(working copy)
@@ -17,6 +17,6 @@
 -->
 <html><head></head>
 <body>
-Backwards-compatible implementation to match {@link org.apache.lucene.util.Version#LUCENE_34}
+Backwards-compatible implementation to match {@link org.apache.lucene.util.Version#LUCENE_3_4}
 </body>
 </html>
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std36/package.html
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std36/package.html	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std36/package.html	(working copy)
@@ -17,6 +17,6 @@
 -->
 <html><head></head>
 <body>
-Backwards-compatible implementation to match {@link org.apache.lucene.util.Version#LUCENE_36}
+Backwards-compatible implementation to match {@link org.apache.lucene.util.Version#LUCENE_3_6}
 </body>
 </html>
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std40/package.html
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std40/package.html	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std40/package.html	(working copy)
@@ -17,6 +17,6 @@
 -->
 <html><head></head>
 <body>
-Backwards-compatible implementation to match {@link org.apache.lucene.util.Version#LUCENE_40}
+Backwards-compatible implementation to match {@link org.apache.lucene.util.Version#LUCENE_4_0}
 </body>
 </html>
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.java	(working copy)
@@ -124,13 +124,13 @@
 
   private StandardTokenizerInterface getScannerFor(Version matchVersion) {
     // best effort NPE if you dont call reset
-    if (matchVersion.onOrAfter(Version.LUCENE_47)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_4_7)) {
       return new UAX29URLEmailTokenizerImpl(input);
-    } else if (matchVersion.onOrAfter(Version.LUCENE_40)) {
+    } else if (matchVersion.onOrAfter(Version.LUCENE_4_0)) {
       return new UAX29URLEmailTokenizerImpl40(input);
-    } else if (matchVersion.onOrAfter(Version.LUCENE_36)) {
+    } else if (matchVersion.onOrAfter(Version.LUCENE_3_6)) {
       return new UAX29URLEmailTokenizerImpl36(input);
-    } else if (matchVersion.onOrAfter(Version.LUCENE_34)) {
+    } else if (matchVersion.onOrAfter(Version.LUCENE_3_4)) {
       return new UAX29URLEmailTokenizerImpl34(input);
     } else {
       return new UAX29URLEmailTokenizerImpl31(input);
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilterFactory.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilterFactory.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilterFactory.java	(working copy)
@@ -68,7 +68,7 @@
   public SynonymFilterFactory(Map<String,String> args) {
     super(args);
     assureMatchVersion();
-    if (luceneMatchVersion.onOrAfter(Version.LUCENE_34)) {
+    if (luceneMatchVersion.onOrAfter(Version.LUCENE_3_4)) {
       delegator = new FSTSynonymFilterFactory(new HashMap<>(getOriginalArgs()));
     } else {
       // check if you use the new optional arg "format". this makes no sense for the old one, 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiAnalyzer.java	(working copy)
@@ -84,7 +84,7 @@
    * @param matchVersion lucene compatibility version
    */
   public ThaiAnalyzer(Version matchVersion) {
-    this(matchVersion, matchVersion.onOrAfter(Version.LUCENE_36) ? DefaultSetHolder.DEFAULT_STOP_SET : StopAnalyzer.ENGLISH_STOP_WORDS_SET);
+    this(matchVersion, matchVersion.onOrAfter(Version.LUCENE_3_6) ? DefaultSetHolder.DEFAULT_STOP_SET : StopAnalyzer.ENGLISH_STOP_WORDS_SET);
   }
   
   /**
@@ -110,7 +110,7 @@
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
       Reader reader) {
-    if (matchVersion.onOrAfter(Version.LUCENE_48)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_4_8)) {
       final Tokenizer source = new ThaiTokenizer(reader);
       TokenStream result = new LowerCaseFilter(matchVersion, source);
       result = new StopFilter(matchVersion, result, stopwords);
@@ -118,7 +118,7 @@
     } else {
       final Tokenizer source = new StandardTokenizer(matchVersion, reader);
       TokenStream result = new StandardFilter(matchVersion, source);
-      if (matchVersion.onOrAfter(Version.LUCENE_31))
+      if (matchVersion.onOrAfter(Version.LUCENE_3_1))
         result = new LowerCaseFilter(matchVersion, result);
       result = new ThaiWordFilter(matchVersion, result);
       return new TokenStreamComponents(source, new StopFilter(matchVersion,
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java	(working copy)
@@ -69,11 +69,11 @@
 
   /** Creates a new ThaiWordFilter with the specified match version. */
   public ThaiWordFilter(Version matchVersion, TokenStream input) {
-    super(matchVersion.onOrAfter(Version.LUCENE_31) ?
+    super(matchVersion.onOrAfter(Version.LUCENE_3_1) ?
       input : new LowerCaseFilter(matchVersion, input));
     if (!DBBI_AVAILABLE)
       throw new UnsupportedOperationException("This JRE does not have support for Thai segmentation");
-    handlePosIncr = matchVersion.onOrAfter(Version.LUCENE_31);
+    handlePosIncr = matchVersion.onOrAfter(Version.LUCENE_3_1);
   }
   
   @Override
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/tr/TurkishAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/tr/TurkishAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/tr/TurkishAnalyzer.java	(working copy)
@@ -123,7 +123,7 @@
       Reader reader) {
     final Tokenizer source = new StandardTokenizer(matchVersion, reader);
     TokenStream result = new StandardFilter(matchVersion, source);
-    if(matchVersion.onOrAfter(Version.LUCENE_48))
+    if(matchVersion.onOrAfter(Version.LUCENE_4_8))
       result = new ApostropheFilter(result);
     result = new TurkishLowerCaseFilter(result);
     result = new StopFilter(matchVersion, result, stopwords);
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharacterUtils.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharacterUtils.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharacterUtils.java	(working copy)
@@ -43,7 +43,7 @@
    *         {@link Version} instance.
    */
   public static CharacterUtils getInstance(final Version matchVersion) {
-    return matchVersion.onOrAfter(Version.LUCENE_31) ? JAVA_5 : JAVA_4;
+    return matchVersion.onOrAfter(Version.LUCENE_3_1) ? JAVA_5 : JAVA_4;
   }
 
   /** Return a {@link CharacterUtils} instance compatible with Java 1.4. */
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/util/FilteringTokenFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/util/FilteringTokenFilter.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/util/FilteringTokenFilter.java	(working copy)
@@ -36,7 +36,7 @@
 public abstract class FilteringTokenFilter extends TokenFilter {
 
   private static void checkPositionIncrement(Version version, boolean enablePositionIncrements) {
-    if (!enablePositionIncrements && version.onOrAfter(Version.LUCENE_44)) {
+    if (!enablePositionIncrements && version.onOrAfter(Version.LUCENE_4_4)) {
       throw new IllegalArgumentException("enablePositionIncrements=false is not supported anymore as of Lucene 4.4 as it can create broken token streams");
     }
   }
Index: lucene/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java	(working copy)
@@ -105,13 +105,13 @@
    */
   @Deprecated
   public CollationKeyAnalyzer(Collator collator) {
-    this(Version.LUCENE_31, collator);
+    this(Version.LUCENE_3_1, collator);
   }
 
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
       Reader reader) {
-    if (matchVersion.onOrAfter(Version.LUCENE_40)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_4_0)) {
       KeywordTokenizer tokenizer = new KeywordTokenizer(factory, reader, KeywordTokenizer.DEFAULT_BUFFER_SIZE);
       return new TokenStreamComponents(tokenizer, tokenizer);
     } else {
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicLetterTokenizer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicLetterTokenizer.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ar/TestArabicLetterTokenizer.java	(working copy)
@@ -32,7 +32,7 @@
   
   public void testArabicLetterTokenizer() throws IOException {
     StringReader reader = new StringReader("1234567890 Tokenizer \ud801\udc1c\u0300test");
-    ArabicLetterTokenizer tokenizer = new ArabicLetterTokenizer(Version.LUCENE_31,
+    ArabicLetterTokenizer tokenizer = new ArabicLetterTokenizer(Version.LUCENE_3_1,
         reader);
     assertTokenStreamContents(tokenizer, new String[] {"Tokenizer",
         "\ud801\udc1c\u0300test"});
@@ -40,7 +40,7 @@
   
   public void testArabicLetterTokenizerBWCompat() throws IOException {
     StringReader reader = new StringReader("1234567890 Tokenizer \ud801\udc1c\u0300test");
-    ArabicLetterTokenizer tokenizer = new ArabicLetterTokenizer(Version.LUCENE_30,
+    ArabicLetterTokenizer tokenizer = new ArabicLetterTokenizer(Version.LUCENE_3_0,
         reader);
     assertTokenStreamContents(tokenizer, new String[] {"Tokenizer", "\u0300test"});
   }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java	(working copy)
@@ -44,7 +44,7 @@
   }
 
   public void checkCJKToken(final String str, final TestToken[] out_tokens) throws IOException {
-    Analyzer analyzer = new CJKAnalyzer(Version.LUCENE_30);
+    Analyzer analyzer = new CJKAnalyzer(Version.LUCENE_3_0);
     String terms[] = new String[out_tokens.length];
     int startOffsets[] = new int[out_tokens.length];
     int endOffsets[] = new int[out_tokens.length];
@@ -59,7 +59,7 @@
   }
   
   public void checkCJKTokenReusable(final Analyzer a, final String str, final TestToken[] out_tokens) throws IOException {
-    Analyzer analyzer = new CJKAnalyzer(Version.LUCENE_30);
+    Analyzer analyzer = new CJKAnalyzer(Version.LUCENE_3_0);
     String terms[] = new String[out_tokens.length];
     int startOffsets[] = new int[out_tokens.length];
     int endOffsets[] = new int[out_tokens.length];
@@ -215,13 +215,13 @@
   }
   
   public void testTokenStream() throws Exception {
-    Analyzer analyzer = new CJKAnalyzer(Version.LUCENE_30);
+    Analyzer analyzer = new CJKAnalyzer(Version.LUCENE_3_0);
     assertAnalyzesTo(analyzer, "\u4e00\u4e01\u4e02", 
         new String[] { "\u4e00\u4e01", "\u4e01\u4e02"});
   }
   
   public void testReusableTokenStream() throws Exception {
-    Analyzer analyzer = new CJKAnalyzer(Version.LUCENE_30);
+    Analyzer analyzer = new CJKAnalyzer(Version.LUCENE_3_0);
     String str = "\u3042\u3044\u3046\u3048\u304aabc\u304b\u304d\u304f\u3051\u3053";
     
     TestToken[] out_tokens = { 
@@ -276,6 +276,6 @@
   
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
-    checkRandomData(random(), new CJKAnalyzer(Version.LUCENE_30), 10000*RANDOM_MULTIPLIER);
+    checkRandomData(random(), new CJKAnalyzer(Version.LUCENE_3_0), 10000*RANDOM_MULTIPLIER);
   }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestAnalyzers.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestAnalyzers.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestAnalyzers.java	(working copy)
@@ -217,7 +217,7 @@
   @Deprecated
   public void testLowerCaseTokenizerBWCompat() throws IOException {
     StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
-    LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(Version.LUCENE_30,
+    LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(Version.LUCENE_3_0,
         reader);
     assertTokenStreamContents(tokenizer, new String[] { "tokenizer", "test" });
   }
@@ -234,7 +234,7 @@
   @Deprecated
   public void testWhitespaceTokenizerBWCompat() throws IOException {
     StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
-    WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(Version.LUCENE_30,
+    WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(Version.LUCENE_3_0,
         reader);
     assertTokenStreamContents(tokenizer, new String[] { "Tokenizer",
         "\ud801\udc1ctest" });
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer.java	(working copy)
@@ -138,7 +138,7 @@
 
     // 2.4 should not show the bug. But, alas, it's also obsolete,
     // so we check latest released (Robert's gonna break this on 4.0 soon :) )
-    a2 = new ClassicAnalyzer(Version.LUCENE_31);
+    a2 = new ClassicAnalyzer(Version.LUCENE_3_1);
     assertAnalyzesTo(a2, "www.nutch.org.", new String[]{ "www.nutch.org" }, new String[] { "<HOST>" });
   }
 
@@ -245,7 +245,7 @@
   }
 
   public void testJava14BWCompatibility() throws Exception {
-    ClassicAnalyzer sa = new ClassicAnalyzer(Version.LUCENE_30);
+    ClassicAnalyzer sa = new ClassicAnalyzer(Version.LUCENE_3_0);
     assertAnalyzesTo(sa, "test\u02C6test", new String[] { "test", "test" });
   }
 
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer.java	(working copy)
@@ -282,7 +282,7 @@
   /** @deprecated remove this and sophisticated backwards layer in 5.0 */
   @Deprecated
   public void testCombiningMarksBackwards() throws Exception {
-    Analyzer a = new StandardAnalyzer(Version.LUCENE_33);
+    Analyzer a = new StandardAnalyzer(Version.LUCENE_3_3);
     checkOneTerm(a, "ざ", "さ"); // hiragana Bug
     checkOneTerm(a, "ザ", "ザ"); // katakana Works
     checkOneTerm(a, "壹゙", "壹"); // ideographic Bug
@@ -295,7 +295,7 @@
     Analyzer a = new Analyzer() {
       @Override
       protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-        Tokenizer tokenizer = new StandardTokenizer(Version.LUCENE_36, reader);
+        Tokenizer tokenizer = new StandardTokenizer(Version.LUCENE_3_6, reader);
         return new TokenStreamComponents(tokenizer);
       }
     };
@@ -309,7 +309,7 @@
     Analyzer a = new Analyzer() {
       @Override
       protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-        Tokenizer tokenizer = new StandardTokenizer(Version.LUCENE_40, reader);
+        Tokenizer tokenizer = new StandardTokenizer(Version.LUCENE_4_0, reader);
         return new TokenStreamComponents(tokenizer);
       }
     };
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopFilter.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopFilter.java	(working copy)
@@ -71,11 +71,11 @@
     CharArraySet stopSet = StopFilter.makeStopSet(TEST_VERSION_CURRENT, stopWords);
     // with increments
     StringReader reader = new StringReader(sb.toString());
-    StopFilter stpf = new StopFilter(Version.LUCENE_40, new MockTokenizer(reader, MockTokenizer.WHITESPACE, false), stopSet);
+    StopFilter stpf = new StopFilter(Version.LUCENE_4_0, new MockTokenizer(reader, MockTokenizer.WHITESPACE, false), stopSet);
     doTestStopPositons(stpf,true);
     // without increments
     reader = new StringReader(sb.toString());
-    stpf = new StopFilter(Version.LUCENE_43, new MockTokenizer(reader, MockTokenizer.WHITESPACE, false), stopSet);
+    stpf = new StopFilter(Version.LUCENE_4_3, new MockTokenizer(reader, MockTokenizer.WHITESPACE, false), stopSet);
     doTestStopPositons(stpf,false);
     // with increments, concatenating two stop filters
     ArrayList<String> a0 = new ArrayList<>();
@@ -182,7 +182,7 @@
       protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
         Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
         TokenFilter filter = new MockSynonymFilter(tokenizer);
-        StopFilter stopfilter = new StopFilter(Version.LUCENE_43, filter, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
+        StopFilter stopfilter = new StopFilter(Version.LUCENE_4_3, filter, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
         stopfilter.setEnablePositionIncrements(false);
         return new TokenStreamComponents(tokenizer, stopfilter);
       }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestTypeTokenFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestTypeTokenFilter.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestTypeTokenFilter.java	(working copy)
@@ -65,7 +65,7 @@
 
     // without increments
     reader = new StringReader(sb.toString());
-    typeTokenFilter = new TypeTokenFilter(Version.LUCENE_43, false, new StandardTokenizer(TEST_VERSION_CURRENT, reader), stopSet);
+    typeTokenFilter = new TypeTokenFilter(Version.LUCENE_4_3, false, new StandardTokenizer(TEST_VERSION_CURRENT, reader), stopSet);
     testPositons(typeTokenFilter);
 
   }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailAnalyzer.java	(working copy)
@@ -213,7 +213,7 @@
   /** @deprecated remove this and sophisticated backwards layer in 5.0 */
   @Deprecated
   public void testCombiningMarksBackwards() throws Exception {
-    Analyzer a = new UAX29URLEmailAnalyzer(Version.LUCENE_33);
+    Analyzer a = new UAX29URLEmailAnalyzer(Version.LUCENE_3_3);
     checkOneTerm(a, "ざ", "さ"); // hiragana Bug
     checkOneTerm(a, "ザ", "ザ"); // katakana Works
     checkOneTerm(a, "壹゙", "壹"); // ideographic Bug
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailTokenizer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailTokenizer.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailTokenizer.java	(working copy)
@@ -510,7 +510,7 @@
       protected TokenStreamComponents createComponents
         (String fieldName, Reader reader) {
 
-        Tokenizer tokenizer = new UAX29URLEmailTokenizer(Version.LUCENE_31, reader);
+        Tokenizer tokenizer = new UAX29URLEmailTokenizer(Version.LUCENE_3_1, reader);
         return new TokenStreamComponents(tokenizer);
       }
     };
@@ -527,7 +527,7 @@
     Analyzer a = new Analyzer() {
       @Override
       protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-        Tokenizer tokenizer = new UAX29URLEmailTokenizer(Version.LUCENE_34, reader);
+        Tokenizer tokenizer = new UAX29URLEmailTokenizer(Version.LUCENE_3_4, reader);
         return new TokenStreamComponents(tokenizer);
       }
     };
@@ -541,7 +541,7 @@
     Analyzer a = new Analyzer() {
       @Override
       protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-        Tokenizer tokenizer = new UAX29URLEmailTokenizer(Version.LUCENE_36, reader);
+        Tokenizer tokenizer = new UAX29URLEmailTokenizer(Version.LUCENE_3_6, reader);
         return new TokenStreamComponents(tokenizer);
       }
     };
@@ -555,7 +555,7 @@
     Analyzer a = new Analyzer() {
       @Override
       protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-        Tokenizer tokenizer = new UAX29URLEmailTokenizer(Version.LUCENE_40, reader);
+        Tokenizer tokenizer = new UAX29URLEmailTokenizer(Version.LUCENE_4_0, reader);
         return new TokenStreamComponents(tokenizer);
       }
     };
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java	(working copy)
@@ -36,7 +36,7 @@
    */
   @Deprecated
   public void testStopWordLegacy() throws Exception {
-    assertAnalyzesTo(new CzechAnalyzer(Version.LUCENE_30), "Pokud mluvime o volnem", 
+    assertAnalyzesTo(new CzechAnalyzer(Version.LUCENE_3_0), "Pokud mluvime o volnem", 
         new String[] { "mluvime", "volnem" });
   }
   
@@ -50,7 +50,7 @@
    */
   @Deprecated
   public void testReusableTokenStreamLegacy() throws Exception {
-    Analyzer analyzer = new CzechAnalyzer(Version.LUCENE_30);
+    Analyzer analyzer = new CzechAnalyzer(Version.LUCENE_3_0);
     assertAnalyzesTo(analyzer, "Pokud mluvime o volnem", new String[] { "mluvime", "volnem" });
     assertAnalyzesTo(analyzer, "Česká Republika", new String[] { "česká", "republika" });
   }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java	(working copy)
@@ -59,7 +59,7 @@
     checkOneTerm(a, "Schaltflächen", "schaltflach");
     checkOneTerm(a, "Schaltflaechen", "schaltflach");
     // here they are with the old stemmer
-    a = new GermanAnalyzer(Version.LUCENE_30);
+    a = new GermanAnalyzer(Version.LUCENE_3_0);
     checkOneTerm(a, "Schaltflächen", "schaltflach");
     checkOneTerm(a, "Schaltflaechen", "schaltflaech");
   }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java	(working copy)
@@ -56,7 +56,7 @@
    */
   @Deprecated
   public void testAnalyzerBWCompat() throws Exception {
-    Analyzer a = new GreekAnalyzer(Version.LUCENE_30);
+    Analyzer a = new GreekAnalyzer(Version.LUCENE_3_0);
     // Verify the correct analysis of capitals and small accented letters
     assertAnalyzesTo(a, "Μία εξαιρετικά καλή και πλούσια σειρά χαρακτήρων της Ελληνικής γλώσσας",
         new String[] { "μια", "εξαιρετικα", "καλη", "πλουσια", "σειρα", "χαρακτηρων",
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java	(working copy)
@@ -120,7 +120,7 @@
    */
   @Deprecated
   public void testAnalyzer30() throws Exception {
-      FrenchAnalyzer fa = new FrenchAnalyzer(Version.LUCENE_30);
+      FrenchAnalyzer fa = new FrenchAnalyzer(Version.LUCENE_3_0);
 
       assertAnalyzesTo(fa, "", new String[] {
       });
@@ -249,7 +249,7 @@
    */
   @Deprecated
   public void testBuggyStopwordsCasing() throws IOException {
-    FrenchAnalyzer a = new FrenchAnalyzer(Version.LUCENE_30);
+    FrenchAnalyzer a = new FrenchAnalyzer(Version.LUCENE_3_0);
     assertAnalyzesTo(a, "Votre", new String[] { "votr" });
   }
   
@@ -257,7 +257,7 @@
    * Test that stopwords are not case sensitive
    */
   public void testStopwordsCasing() throws IOException {
-    FrenchAnalyzer a = new FrenchAnalyzer(Version.LUCENE_31);
+    FrenchAnalyzer a = new FrenchAnalyzer(Version.LUCENE_3_1);
     assertAnalyzesTo(a, "Votre", new String[] { });
   }
   
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/it/TestItalianAnalyzer.java	(working copy)
@@ -66,7 +66,7 @@
   
   /** test that we don't enable this before 3.2*/
   public void testContractionsBackwards() throws IOException {
-    Analyzer a = new ItalianAnalyzer(Version.LUCENE_31);
+    Analyzer a = new ItalianAnalyzer(Version.LUCENE_3_1);
     assertAnalyzesTo(a, "dell'Italia", new String[] { "dell'ital" });
     assertAnalyzesTo(a, "l'Italiano", new String[] { "l'ital" });
   }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeepWordFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeepWordFilter.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeepWordFilter.java	(working copy)
@@ -53,12 +53,12 @@
     
     // Test Stopwords
     stream = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);
-    stream = new KeepWordFilter(Version.LUCENE_43, false, stream, new CharArraySet(TEST_VERSION_CURRENT, words, true));
+    stream = new KeepWordFilter(Version.LUCENE_4_3, false, stream, new CharArraySet(TEST_VERSION_CURRENT, words, true));
     assertTokenStreamContents(stream, new String[] { "aaa", "BBB" }, new int[] { 1, 1 });
        
     // Now force case
     stream = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);
-    stream = new KeepWordFilter(Version.LUCENE_43, false, stream, new CharArraySet(TEST_VERSION_CURRENT,words, false));
+    stream = new KeepWordFilter(Version.LUCENE_4_3, false, stream, new CharArraySet(TEST_VERSION_CURRENT,words, false));
     assertTokenStreamContents(stream, new String[] { "aaa" }, new int[] { 1 });
   }
   
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java	(working copy)
@@ -38,7 +38,7 @@
   public void testFilterNoPosIncr() throws Exception {
     TokenStream stream = new MockTokenizer(
         new StringReader("short toolong evenmuchlongertext a ab toolong foo"), MockTokenizer.WHITESPACE, false);
-    LengthFilter filter = new LengthFilter(Version.LUCENE_43, false, stream, 2, 6);
+    LengthFilter filter = new LengthFilter(Version.LUCENE_4_3, false, stream, 2, 6);
     assertTokenStreamContents(filter,
       new String[]{"short", "ab", "foo"},
       new int[]{1, 1, 1}
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilterFactory.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilterFactory.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilterFactory.java	(working copy)
@@ -32,7 +32,7 @@
     Reader reader = new StringReader("foo foobar super-duper-trooper");
     TokenStream stream = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
     stream = tokenFilterFactory("Length",
-        Version.LUCENE_43, new ClasspathResourceLoader(getClass()),
+        Version.LUCENE_4_3, new ClasspathResourceLoader(getClass()),
         "min", "4",
         "max", "10",
         "enablePositionIncrements", "false").create(stream);
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestTrimFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestTrimFilter.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestTrimFilter.java	(working copy)
@@ -60,7 +60,7 @@
             new Token(b, 0, b.length, 0, 2),
             new Token(ccc, 0, ccc.length, 0, 3),
             new Token(whitespace, 0, whitespace.length, 0, 3));
-    ts = new TrimFilter(Version.LUCENE_43, ts, true);
+    ts = new TrimFilter(Version.LUCENE_4_3, ts, true);
     
     assertTokenStreamContents(ts, 
         new String[] { "a", "b", "c", "" },
@@ -121,7 +121,7 @@
       @Override
       protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
         Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.KEYWORD, false);
-        return new TokenStreamComponents(tokenizer, new TrimFilter(Version.LUCENE_43, tokenizer, true));
+        return new TokenStreamComponents(tokenizer, new TrimFilter(Version.LUCENE_4_3, tokenizer, true));
       } 
     };
     checkRandomData(random(), a, 1000*RANDOM_MULTIPLIER);
@@ -143,7 +143,7 @@
       protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
         Tokenizer tokenizer = new KeywordTokenizer(reader);
         final boolean updateOffsets = random().nextBoolean();
-        final Version version = updateOffsets ? Version.LUCENE_43 : TEST_VERSION_CURRENT;
+        final Version version = updateOffsets ? Version.LUCENE_4_3 : TEST_VERSION_CURRENT;
         return new TokenStreamComponents(tokenizer, new TrimFilter(version, tokenizer, updateOffsets));
       }
     };
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java	(working copy)
@@ -87,7 +87,7 @@
   }
 
   public void testBackUnigram() throws Exception {
-    EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(Version.LUCENE_43, input, EdgeNGramTokenFilter.Side.BACK, 1, 1);
+    EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(Version.LUCENE_4_3, input, EdgeNGramTokenFilter.Side.BACK, 1, 1);
     assertTokenStreamContents(tokenizer, new String[]{"e"}, new int[]{4}, new int[]{5});
   }
 
@@ -102,7 +102,7 @@
   }
 
   public void testBackRangeOfNgrams() throws Exception {
-    EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(Version.LUCENE_43, input, EdgeNGramTokenFilter.Side.BACK, 1, 3);
+    EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(Version.LUCENE_4_3, input, EdgeNGramTokenFilter.Side.BACK, 1, 3);
     assertTokenStreamContents(tokenizer,
                               new String[]{"e","de","cde"},
                               new int[]{4,3,2},
@@ -197,7 +197,7 @@
       protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
         Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
         TokenFilter filters = new ASCIIFoldingFilter(tokenizer);
-        filters = new EdgeNGramTokenFilter(Version.LUCENE_43, filters, EdgeNGramTokenFilter.Side.FRONT, 2, 15);
+        filters = new EdgeNGramTokenFilter(Version.LUCENE_4_3, filters, EdgeNGramTokenFilter.Side.FRONT, 2, 15);
         return new TokenStreamComponents(tokenizer, filters);
       }
     };
@@ -229,7 +229,7 @@
       protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
         Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
         return new TokenStreamComponents(tokenizer, 
-            new EdgeNGramTokenFilter(Version.LUCENE_43, tokenizer, EdgeNGramTokenFilter.Side.BACK, 2, 4));
+            new EdgeNGramTokenFilter(Version.LUCENE_4_3, tokenizer, EdgeNGramTokenFilter.Side.BACK, 2, 4));
       }    
     };
     checkRandomData(random(), b, 1000*RANDOM_MULTIPLIER, 20, false, false);
@@ -252,7 +252,7 @@
       protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
         Tokenizer tokenizer = new KeywordTokenizer(reader);
         return new TokenStreamComponents(tokenizer, 
-            new EdgeNGramTokenFilter(Version.LUCENE_43, tokenizer, EdgeNGramTokenFilter.Side.BACK, 2, 15));
+            new EdgeNGramTokenFilter(Version.LUCENE_4_3, tokenizer, EdgeNGramTokenFilter.Side.BACK, 2, 15));
       }    
     };
     checkAnalysisConsistency(random, b, random.nextBoolean(), "");
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java	(working copy)
@@ -79,7 +79,7 @@
   }
 
   public void testBackUnigram() throws Exception {
-    Tokenizer tokenizer = new Lucene43EdgeNGramTokenizer(Version.LUCENE_43, input, Lucene43EdgeNGramTokenizer.Side.BACK, 1, 1);
+    Tokenizer tokenizer = new Lucene43EdgeNGramTokenizer(Version.LUCENE_4_3, input, Lucene43EdgeNGramTokenizer.Side.BACK, 1, 1);
     assertTokenStreamContents(tokenizer, new String[]{"e"}, new int[]{4}, new int[]{5}, 5 /* abcde */);
   }
 
@@ -94,7 +94,7 @@
   }
 
   public void testBackRangeOfNgrams() throws Exception {
-    Tokenizer tokenizer = new Lucene43EdgeNGramTokenizer(Version.LUCENE_43, input, Lucene43EdgeNGramTokenizer.Side.BACK, 1, 3);
+    Tokenizer tokenizer = new Lucene43EdgeNGramTokenizer(Version.LUCENE_4_3, input, Lucene43EdgeNGramTokenizer.Side.BACK, 1, 3);
     assertTokenStreamContents(tokenizer, new String[]{"e","de","cde"}, new int[]{4,3,2}, new int[]{5,5,5}, null, null, null, 5 /* abcde */, false);
   }
   
@@ -125,7 +125,7 @@
     Analyzer b = new Analyzer() {
       @Override
       protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-        Tokenizer tokenizer = new Lucene43EdgeNGramTokenizer(Version.LUCENE_43, reader, Lucene43EdgeNGramTokenizer.Side.BACK, 2, 4);
+        Tokenizer tokenizer = new Lucene43EdgeNGramTokenizer(Version.LUCENE_4_3, reader, Lucene43EdgeNGramTokenizer.Side.BACK, 2, 4);
         return new TokenStreamComponents(tokenizer, tokenizer);
       }    
     };
@@ -134,7 +134,7 @@
   }
 
   public void testTokenizerPositions() throws Exception {
-    Tokenizer tokenizer = new Lucene43EdgeNGramTokenizer(Version.LUCENE_43, input, Lucene43EdgeNGramTokenizer.Side.FRONT, 1, 3);
+    Tokenizer tokenizer = new Lucene43EdgeNGramTokenizer(Version.LUCENE_4_3, input, Lucene43EdgeNGramTokenizer.Side.FRONT, 1, 3);
     assertTokenStreamContents(tokenizer,
                               new String[]{"a","ab","abc"},
                               new int[]{0,0,0},
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java	(working copy)
@@ -173,7 +173,7 @@
   }
 
   public void testLucene43() throws IOException {
-    NGramTokenFilter filter = new NGramTokenFilter(Version.LUCENE_43, input, 2, 3);
+    NGramTokenFilter filter = new NGramTokenFilter(Version.LUCENE_4_3, input, 2, 3);
     assertTokenStreamContents(filter,
         new String[]{"ab","bc","cd","de","abc","bcd","cde"},
         new int[]{0,1,2,3,0,1,2},
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/TestNGramFilters.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/TestNGramFilters.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/TestNGramFilters.java	(working copy)
@@ -102,7 +102,7 @@
    */
   public void testEdgeNGramTokenizer3() throws Exception {
     Reader reader = new StringReader("ready");
-    TokenStream stream = tokenizerFactory("EdgeNGram", Version.LUCENE_43,
+    TokenStream stream = tokenizerFactory("EdgeNGram", Version.LUCENE_4_3,
         "side", "back").create(reader);
     assertTokenStreamContents(stream, 
         new String[] { "y" });
@@ -138,7 +138,7 @@
   public void testEdgeNGramFilter3() throws Exception {
     Reader reader = new StringReader("ready");
     TokenStream stream = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
-    stream = tokenFilterFactory("EdgeNGram", Version.LUCENE_43,
+    stream = tokenFilterFactory("EdgeNGram", Version.LUCENE_4_3,
         "side", "back").create(stream);
     assertTokenStreamContents(stream, 
         new String[] { "y" });
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java	(working copy)
@@ -118,7 +118,7 @@
    */
   @Deprecated
   public void testOldBuggyStemmer() throws Exception {
-    Analyzer a = new DutchAnalyzer(Version.LUCENE_30);
+    Analyzer a = new DutchAnalyzer(Version.LUCENE_3_0);
     checkOneTerm(a, "opheffen", "ophef"); // versus snowball 'opheff'
     checkOneTerm(a, "opheffende", "ophef"); // versus snowball 'opheff'
     checkOneTerm(a, "opheffing", "ophef"); // versus snowball 'opheff'
@@ -140,7 +140,7 @@
   }
   
   public void testExclusionTableViaCtor() throws IOException {
-    CharArraySet set = new CharArraySet(Version.LUCENE_30, 1, true);
+    CharArraySet set = new CharArraySet(Version.LUCENE_3_0, 1, true);
     set.add("lichamelijk");
     DutchAnalyzer a = new DutchAnalyzer(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET, set);
     assertAnalyzesTo(a, "lichamelijk lichamelijke", new String[] { "lichamelijk", "licham" });
@@ -164,9 +164,9 @@
    */
   @Deprecated
   public void test30StemOverrides() throws IOException {
-    DutchAnalyzer a = new DutchAnalyzer(Version.LUCENE_30);
+    DutchAnalyzer a = new DutchAnalyzer(Version.LUCENE_3_0);
     checkOneTerm(a, "fiets", "fiets");
-    a = new DutchAnalyzer(Version.LUCENE_30, CharArraySet.EMPTY_SET);
+    a = new DutchAnalyzer(Version.LUCENE_3_0, CharArraySet.EMPTY_SET);
     checkOneTerm(a, "fiets", "fiet"); // only the default ctor populates the dict
   }
 
@@ -183,7 +183,7 @@
    */
   @Deprecated
   public void testBuggyStemOverrides() throws IOException {
-    DutchAnalyzer a = new DutchAnalyzer(Version.LUCENE_35, CharArraySet.EMPTY_SET);
+    DutchAnalyzer a = new DutchAnalyzer(Version.LUCENE_3_5, CharArraySet.EMPTY_SET);
     checkOneTerm(a, "fiets", "fiet");
   }
   
@@ -194,7 +194,7 @@
    */
   @Deprecated
   public void testBuggyStopwordsCasing() throws IOException {
-    DutchAnalyzer a = new DutchAnalyzer(Version.LUCENE_30);
+    DutchAnalyzer a = new DutchAnalyzer(Version.LUCENE_3_0);
     assertAnalyzesTo(a, "Zelf", new String[] { "zelf" });
   }
   
@@ -202,7 +202,7 @@
    * Test that stopwords are not case sensitive
    */
   public void testStopwordsCasing() throws IOException {
-    DutchAnalyzer a = new DutchAnalyzer(Version.LUCENE_31);
+    DutchAnalyzer a = new DutchAnalyzer(Version.LUCENE_3_1);
     assertAnalyzesTo(a, "Zelf", new String[] { });
   }
   
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java	(working copy)
@@ -63,7 +63,7 @@
    */
   @Deprecated
   public void testBackCompat() throws Exception {
-    assertEquals("\uDF05\uD866\uDF05\uD866", ReverseStringFilter.reverse(Version.LUCENE_30, "𩬅𩬅"));
+    assertEquals("\uDF05\uD866\uDF05\uD866", ReverseStringFilter.reverse(Version.LUCENE_3_0, "𩬅𩬅"));
   }
   
   public void testReverseSupplementary() throws Exception {
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java	(working copy)
@@ -40,7 +40,7 @@
     /** @deprecated (3.1) remove this test in Lucene 5.0: stopwords changed */
     @Deprecated
     public void testReusableTokenStream30() throws Exception {
-      Analyzer a = new RussianAnalyzer(Version.LUCENE_30);
+      Analyzer a = new RussianAnalyzer(Version.LUCENE_3_0);
       assertAnalyzesTo(a, "Вместе с тем о силе электромагнитной энергии имели представление еще",
           new String[] { "вмест", "сил", "электромагнитн", "энерг", "имел", "представлен" });
       assertAnalyzesTo(a, "Но знание это хранилось в тайне",
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianLetterTokenizer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianLetterTokenizer.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ru/TestRussianLetterTokenizer.java	(working copy)
@@ -40,7 +40,7 @@
   
   public void testRussianLetterTokenizerBWCompat() throws IOException {
     StringReader reader = new StringReader("1234567890 Вместе \ud801\udc1ctest");
-    RussianLetterTokenizer tokenizer = new RussianLetterTokenizer(Version.LUCENE_30,
+    RussianLetterTokenizer tokenizer = new RussianLetterTokenizer(Version.LUCENE_3_0,
         reader);
     assertTokenStreamContents(tokenizer, new String[] {"1234567890", "Вместе", "test"});
   }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java	(working copy)
@@ -61,7 +61,7 @@
     assertAnalyzesTo(a, "cryogenic", new String[] { "cryogen" });
     assertAnalyzesTo(a, "CRYOGENIC", new String[] { "cryogen" });
     
-    Analyzer b = new SnowballAnalyzer(Version.LUCENE_30, "English");
+    Analyzer b = new SnowballAnalyzer(Version.LUCENE_3_0, "English");
     assertAnalyzesTo(b, "cryogenic", new String[] { "cryogen" });
     assertAnalyzesTo(b, "CRYOGENIC", new String[] { "cryogen" });
   }
@@ -82,7 +82,7 @@
    */
   @Deprecated
   public void testTurkishBWComp() throws Exception {
-    Analyzer a = new SnowballAnalyzer(Version.LUCENE_30, "Turkish");
+    Analyzer a = new SnowballAnalyzer(Version.LUCENE_3_0, "Turkish");
     // AĞACI in turkish lowercases to ağacı, but with lowercase filter ağaci.
     // this fails due to wrong casing, because the stemmer
     // will only remove -ı, not -i
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestUAX29URLEmailTokenizerFactory.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestUAX29URLEmailTokenizerFactory.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestUAX29URLEmailTokenizerFactory.java	(working copy)
@@ -166,7 +166,7 @@
         new String[] {"ざ"});
     
     reader = new StringReader("ざ");
-    stream = tokenizerFactory("UAX29URLEmail", Version.LUCENE_31, new ClasspathResourceLoader(getClass())).create(reader);
+    stream = tokenizerFactory("UAX29URLEmail", Version.LUCENE_3_1, new ClasspathResourceLoader(getClass())).create(reader);
     assertTokenStreamContents(stream, 
         new String[] {"さ"}); // old broken behavior
   }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymFilterFactory.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymFilterFactory.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymFilterFactory.java	(working copy)
@@ -66,7 +66,7 @@
   public void testSynonymsOld() throws Exception {
     Reader reader = new StringReader("GB");
     TokenStream stream = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
-    stream = tokenFilterFactory("Synonym", Version.LUCENE_33, new ClasspathResourceLoader(getClass()),
+    stream = tokenFilterFactory("Synonym", Version.LUCENE_3_3, new ClasspathResourceLoader(getClass()),
         "synonyms", "synonyms.txt").create(stream);
     assertTrue(stream instanceof SlowSynonymFilter);
     assertTokenStreamContents(stream, 
@@ -80,7 +80,7 @@
   public void testMultiwordOffsetsOld() throws Exception {
     Reader reader = new StringReader("national hockey league");
     TokenStream stream = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
-    stream = tokenFilterFactory("Synonym", Version.LUCENE_33, new StringMockResourceLoader("national hockey league, nhl"),
+    stream = tokenFilterFactory("Synonym", Version.LUCENE_3_3, new StringMockResourceLoader("national hockey league, nhl"),
         "synonyms", "synonyms.txt").create(stream);
     // WTF?
     assertTokenStreamContents(stream, 
@@ -127,11 +127,11 @@
   public void testTokenizerFactoryArguments() throws Exception {
     // diff versions produce diff delegator behavior,
     // all should be (mostly) equivilent for our test purposes.
-    doTestTokenizerFactoryArguments(Version.LUCENE_33, 
+    doTestTokenizerFactoryArguments(Version.LUCENE_3_3, 
                                     SlowSynonymFilterFactory.class);
-    doTestTokenizerFactoryArguments(Version.LUCENE_34, 
+    doTestTokenizerFactoryArguments(Version.LUCENE_3_4, 
                                     FSTSynonymFilterFactory.class);
-    doTestTokenizerFactoryArguments(Version.LUCENE_35, 
+    doTestTokenizerFactoryArguments(Version.LUCENE_3_5, 
                                     FSTSynonymFilterFactory.class);
 
     doTestTokenizerFactoryArguments(Version.LUCENE_CURRENT, 
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java	(working copy)
@@ -62,7 +62,7 @@
   }
 
   public void testBackwardsStopWords() throws Exception {
-     assertAnalyzesTo(new ThaiAnalyzer(Version.LUCENE_35), "การที่ได้ต้องแสดงว่างานดี",
+     assertAnalyzesTo(new ThaiAnalyzer(Version.LUCENE_3_5), "การที่ได้ต้องแสดงว่างานดี",
           new String[] { "การ", "ที่", "ได้", "ต้อง", "แสดง", "ว่า", "งาน", "ดี" },
           new int[] { 0, 3, 6, 9, 13, 17, 20, 23 },
           new int[] { 3, 6, 9, 13, 17, 20, 23, 25 });
@@ -74,7 +74,7 @@
     */
   @Deprecated
   public void testBuggyTokenType30() throws Exception {
-    assertAnalyzesTo(new ThaiAnalyzer(Version.LUCENE_30), "การที่ได้ต้องแสดงว่างานดี ๑๒๓",
+    assertAnalyzesTo(new ThaiAnalyzer(Version.LUCENE_3_0), "การที่ได้ต้องแสดงว่างานดี ๑๒๓",
                          new String[] { "การ", "ที่", "ได้", "ต้อง", "แสดง", "ว่า", "งาน", "ดี", "๑๒๓" },
                          new String[] { "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>", 
                                         "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>", 
@@ -84,7 +84,7 @@
   /** @deprecated (3.1) testing backwards behavior */
   @Deprecated
     public void testAnalyzer30() throws Exception {
-        ThaiAnalyzer analyzer = new ThaiAnalyzer(Version.LUCENE_30);
+        ThaiAnalyzer analyzer = new ThaiAnalyzer(Version.LUCENE_3_0);
 
     assertAnalyzesTo(analyzer, "", new String[] {});
 
@@ -143,7 +143,7 @@
   /** @deprecated (3.1) for version back compat */
   @Deprecated
   public void testReusableTokenStream30() throws Exception {
-      ThaiAnalyzer analyzer = new ThaiAnalyzer(Version.LUCENE_30);
+      ThaiAnalyzer analyzer = new ThaiAnalyzer(Version.LUCENE_3_0);
       assertAnalyzesTo(analyzer, "", new String[] {});
 
       assertAnalyzesTo(
@@ -170,7 +170,7 @@
   
   // LUCENE-3044
   public void testAttributeReuse() throws Exception {
-    ThaiAnalyzer analyzer = new ThaiAnalyzer(Version.LUCENE_30);
+    ThaiAnalyzer analyzer = new ThaiAnalyzer(Version.LUCENE_3_0);
     // just consume
     TokenStream ts = analyzer.tokenStream("dummy", "ภาษาไทย");
     assertTokenStreamContents(ts, new String[] { "ภาษา", "ไทย" });
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharacterUtils.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharacterUtils.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharacterUtils.java	(working copy)
@@ -35,7 +35,7 @@
 
   @Test
   public void testCodePointAtCharSequenceInt() {
-    CharacterUtils java4 = CharacterUtils.getInstance(Version.LUCENE_30);
+    CharacterUtils java4 = CharacterUtils.getInstance(Version.LUCENE_3_0);
     String cpAt3 = "Abc\ud801\udc1c";
     String highSurrogateAt3 = "Abc\ud801";
     assertEquals((int) 'A', java4.codePointAt(cpAt3, 0));
@@ -62,7 +62,7 @@
 
   @Test
   public void testCodePointAtCharArrayIntInt() {
-    CharacterUtils java4 = CharacterUtils.getInstance(Version.LUCENE_30);
+    CharacterUtils java4 = CharacterUtils.getInstance(Version.LUCENE_3_0);
     char[] cpAt3 = "Abc\ud801\udc1c".toCharArray();
     char[] highSurrogateAt3 = "Abc\ud801".toCharArray();
     assertEquals((int) 'A', java4.codePointAt(cpAt3, 0, 2));
@@ -159,7 +159,7 @@
 
   @Test
   public void testFillNoHighSurrogate() throws IOException {
-    Version[] versions = new Version[] { Version.LUCENE_30, TEST_VERSION_CURRENT };
+    Version[] versions = new Version[] { Version.LUCENE_3_0, TEST_VERSION_CURRENT };
     for (Version version : versions) {
       CharacterUtils instance = CharacterUtils.getInstance(version);
       Reader reader = new StringReader("helloworld");
@@ -206,7 +206,7 @@
   @Test
   public void testFillJava14() throws IOException {
     String input = "1234\ud801\udc1c789123\ud801\ud801\udc1c\ud801";
-    CharacterUtils instance = CharacterUtils.getInstance(Version.LUCENE_30);
+    CharacterUtils instance = CharacterUtils.getInstance(Version.LUCENE_3_0);
     Reader reader = new StringReader(input);
     CharacterBuffer buffer = CharacterUtils.newCharacterBuffer(5);
     assertTrue(instance.fill(buffer, reader));
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArraySet.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArraySet.java	(revision 1591281)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArraySet.java	(working copy)
@@ -264,7 +264,7 @@
         "\ud801\udc1c\ud801\udc1cCDE", "A\ud801\udc1cB"};
     String[] lowerArr = new String[] {"abc\ud801\udc44",
         "\ud801\udc44\ud801\udc44cde", "a\ud801\udc44b"};
-    CharArraySet set = new CharArraySet(Version.LUCENE_30, Arrays.asList(TEST_STOP_WORDS), true);
+    CharArraySet set = new CharArraySet(Version.LUCENE_3_0, Arrays.asList(TEST_STOP_WORDS), true);
     for (String upper : upperArr) {
       set.add(upper);
     }
@@ -272,7 +272,7 @@
       assertTrue(String.format(Locale.ROOT, missing, upperArr[i]), set.contains(upperArr[i]));
       assertFalse(String.format(Locale.ROOT, falsePos, lowerArr[i]), set.contains(lowerArr[i]));
     }
-    set = new CharArraySet(Version.LUCENE_30, Arrays.asList(TEST_STOP_WORDS), false);
+    set = new CharArraySet(Version.LUCENE_3_0, Arrays.asList(TEST_STOP_WORDS), false);
     for (String upper : upperArr) {
       set.add(upper);
     }
@@ -295,7 +295,7 @@
 
     String[] lowerArr = new String[] { "abc\uD800", "abc\uD800efg",
         "\uD800efg", "\uD800\ud801\udc44b" };
-    CharArraySet set = new CharArraySet(Version.LUCENE_30, Arrays
+    CharArraySet set = new CharArraySet(Version.LUCENE_3_0, Arrays
         .asList(TEST_STOP_WORDS), true);
     for (String upper : upperArr) {
       set.add(upper);
@@ -309,7 +309,7 @@
         assertTrue(String.format(Locale.ROOT, missing, lowerArr[i]), set
             .contains(lowerArr[i]));
     }
-    set = new CharArraySet(Version.LUCENE_30, Arrays.asList(TEST_STOP_WORDS),
+    set = new CharArraySet(Version.LUCENE_3_0, Arrays.asList(TEST_STOP_WORDS),
         false);
     for (String upper : upperArr) {
       set.add(upper);
@@ -500,7 +500,7 @@
     set.add("test2");
     assertTrue(set.toString().contains(", "));
     
-    set = CharArraySet.copy(Version.LUCENE_30, Collections.singleton("test"));
+    set = CharArraySet.copy(Version.LUCENE_3_0, Collections.singleton("test"));
     assertEquals("[test]", set.toString());
     set.add("test2");
     assertTrue(set.toString().contains(", "));
Index: lucene/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationKeyAnalyzer.java
===================================================================
--- lucene/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationKeyAnalyzer.java	(revision 1591281)
+++ lucene/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationKeyAnalyzer.java	(working copy)
@@ -98,12 +98,12 @@
    */
   @Deprecated
   public ICUCollationKeyAnalyzer(Collator collator) {
-    this(Version.LUCENE_31, collator);
+    this(Version.LUCENE_3_1, collator);
   }
 
   @Override
   protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-    if (matchVersion.onOrAfter(Version.LUCENE_40)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_4_0)) {
       KeywordTokenizer tokenizer = new KeywordTokenizer(factory, reader, KeywordTokenizer.DEFAULT_BUFFER_SIZE);
       return new TokenStreamComponents(tokenizer, tokenizer);
     } else {
Index: lucene/analysis/icu/src/java/overview.html
===================================================================
--- lucene/analysis/icu/src/java/overview.html	(revision 1591281)
+++ lucene/analysis/icu/src/java/overview.html	(working copy)
@@ -115,9 +115,9 @@
 <h3>Farsi Range Queries</h3>
 <pre class="prettyprint">
   Collator collator = Collator.getInstance(new ULocale("ar"));
-  ICUCollationKeyAnalyzer analyzer = new ICUCollationKeyAnalyzer(Version.LUCENE_48, collator);
+  ICUCollationKeyAnalyzer analyzer = new ICUCollationKeyAnalyzer(Version.LUCENE_4_9, collator);
   RAMDirectory ramDir = new RAMDirectory();
-  IndexWriter writer = new IndexWriter(ramDir, new IndexWriterConfig(Version.LUCENE_48, analyzer));
+  IndexWriter writer = new IndexWriter(ramDir, new IndexWriterConfig(Version.LUCENE_4_9, analyzer));
   Document doc = new Document();
   doc.add(new Field("content", "\u0633\u0627\u0628", 
                     Field.Store.YES, Field.Index.ANALYZED));
@@ -125,7 +125,7 @@
   writer.close();
   IndexSearcher is = new IndexSearcher(ramDir, true);
 
-  QueryParser aqp = new QueryParser(Version.LUCENE_48, "content", analyzer);
+  QueryParser aqp = new QueryParser(Version.LUCENE_4_9, "content", analyzer);
   aqp.setAnalyzeRangeTerms(true);
     
   // Unicode order would include U+0633 in [ U+062F - U+0698 ], but Farsi
@@ -141,9 +141,9 @@
 <h3>Danish Sorting</h3>
 <pre class="prettyprint">
   Analyzer analyzer 
-    = new ICUCollationKeyAnalyzer(Version.LUCENE_48, Collator.getInstance(new ULocale("da", "dk")));
+    = new ICUCollationKeyAnalyzer(Version.LUCENE_4_9, Collator.getInstance(new ULocale("da", "dk")));
   RAMDirectory indexStore = new RAMDirectory();
-  IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(Version.LUCENE_48, analyzer));
+  IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(Version.LUCENE_4_9, analyzer));
   String[] tracer = new String[] { "A", "B", "C", "D", "E" };
   String[] data = new String[] { "HAT", "HUT", "H\u00C5T", "H\u00D8T", "HOT" };
   String[] sortedTracerOrder = new String[] { "A", "E", "B", "D", "C" };
@@ -169,15 +169,15 @@
 <pre class="prettyprint">
   Collator collator = Collator.getInstance(new ULocale("tr", "TR"));
   collator.setStrength(Collator.PRIMARY);
-  Analyzer analyzer = new ICUCollationKeyAnalyzer(Version.LUCENE_48, collator);
+  Analyzer analyzer = new ICUCollationKeyAnalyzer(Version.LUCENE_4_9, collator);
   RAMDirectory ramDir = new RAMDirectory();
-  IndexWriter writer = new IndexWriter(ramDir, new IndexWriterConfig(Version.LUCENE_48, analyzer));
+  IndexWriter writer = new IndexWriter(ramDir, new IndexWriterConfig(Version.LUCENE_4_9, analyzer));
   Document doc = new Document();
   doc.add(new Field("contents", "DIGY", Field.Store.NO, Field.Index.ANALYZED));
   writer.addDocument(doc);
   writer.close();
   IndexSearcher is = new IndexSearcher(ramDir, true);
-  QueryParser parser = new QueryParser(Version.LUCENE_48, "contents", analyzer);
+  QueryParser parser = new QueryParser(Version.LUCENE_4_9, "contents", analyzer);
   Query query = parser.parse("d\u0131gy");   // U+0131: dotless i
   ScoreDoc[] result = is.search(query, null, 1000).scoreDocs;
   assertEquals("The index Term should be included.", 1, result.length);
Index: lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.java
===================================================================
--- lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.java	(revision 1591281)
+++ lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.java	(working copy)
@@ -138,7 +138,7 @@
   public TokenStreamComponents createComponents(String fieldName, Reader reader) {
     final Tokenizer tokenizer;
     TokenStream result;
-    if (matchVersion.onOrAfter(Version.LUCENE_48)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_4_8)) {
       tokenizer = new HMMChineseTokenizer(reader);
       result = tokenizer;
     } else {
Index: lucene/benchmark/conf/addIndexes.alg
===================================================================
--- lucene/benchmark/conf/addIndexes.alg	(revision 1591281)
+++ lucene/benchmark/conf/addIndexes.alg	(working copy)
@@ -15,7 +15,7 @@
 # * limitations under the License.
 # */
 
-writer.version=LUCENE_40
+writer.version=4.0
 
 analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
 directory=FSDirectory
Index: lucene/benchmark/conf/indexing-flush-by-RAM-multithreaded.alg
===================================================================
--- lucene/benchmark/conf/indexing-flush-by-RAM-multithreaded.alg	(revision 1591281)
+++ lucene/benchmark/conf/indexing-flush-by-RAM-multithreaded.alg	(working copy)
@@ -17,7 +17,7 @@
 # -------------------------------------------------------------------------------------
 # multi val params are iterated by NewRound's, added to reports, start with column name.
 
-writer.version=LUCENE_40
+writer.version=4.0
 #merge.factor=mrg:10:100:10:100:10:100:10:100
 #max.buffered=buf:10:10:100:100:10:10:100:100
 ram.flush.mb=flush:32:40:48:56:32:40:48:56
Index: lucene/benchmark/conf/indexing-flush-by-RAM.alg
===================================================================
--- lucene/benchmark/conf/indexing-flush-by-RAM.alg	(revision 1591281)
+++ lucene/benchmark/conf/indexing-flush-by-RAM.alg	(working copy)
@@ -17,7 +17,7 @@
 # -------------------------------------------------------------------------------------
 # multi val params are iterated by NewRound's, added to reports, start with column name.
 
-writer.version=LUCENE_40
+writer.version=4.0
 #merge.factor=mrg:10:100:10:100:10:100:10:100
 #max.buffered=buf:10:10:100:100:10:10:100:100
 ram.flush.mb=flush:32:40:48:56:32:40:48:56
Index: lucene/benchmark/conf/indexing-multithreaded.alg
===================================================================
--- lucene/benchmark/conf/indexing-multithreaded.alg	(revision 1591281)
+++ lucene/benchmark/conf/indexing-multithreaded.alg	(working copy)
@@ -17,7 +17,7 @@
 # -------------------------------------------------------------------------------------
 # multi val params are iterated by NewRound's, added to reports, start with column name.
 
-writer.version=LUCENE_40
+writer.version=4.0
 merge.factor=mrg:10:100:10:100:10:100:10:100
 max.buffered=buf:10:10:100:100:10:10:100:100
 #ram.flush.mb=flush:32:40:48:56:32:40:48:56
Index: lucene/benchmark/conf/indexing.alg
===================================================================
--- lucene/benchmark/conf/indexing.alg	(revision 1591281)
+++ lucene/benchmark/conf/indexing.alg	(working copy)
@@ -17,7 +17,7 @@
 # -------------------------------------------------------------------------------------
 # multi val params are iterated by NewRound's, added to reports, start with column name.
 
-writer.version=LUCENE_40
+writer.version=4.0
 merge.factor=mrg:10:100:10:100:10:100:10:100
 max.buffered=buf:10:10:100:100:10:10:100:100
 #ram.flush.mb=flush:32:40:48:56:32:40:48:56
Index: lucene/benchmark/conf/standard.alg
===================================================================
--- lucene/benchmark/conf/standard.alg	(revision 1591281)
+++ lucene/benchmark/conf/standard.alg	(working copy)
@@ -17,7 +17,7 @@
 # -------------------------------------------------------------------------------------
 # multi val params are iterated by NewRound's, added to reports, start with column name.
 
-writer.version=LUCENE_40
+writer.version=4.0
 merge.factor=mrg:10:100:10:100:10:100:10:100
 max.buffered=buf:10:10:100:100:10:10:100:100
 compound=cmpnd:true:true:true:true:false:false:false:false
Index: lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AnalyzerFactoryTask.java
===================================================================
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AnalyzerFactoryTask.java	(revision 1591281)
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AnalyzerFactoryTask.java	(working copy)
@@ -68,7 +68,7 @@
  *                      positionIncrementGap:100,
  *                      HTMLStripCharFilter,
  *                      MappingCharFilter(mapping:'mapping-FoldToASCII.txt'),
- *                      WhitespaceTokenizer(luceneMatchVersion:LUCENE_43),
+ *                      WhitespaceTokenizer(luceneMatchVersion:LUCENE_4_3),
  *                      TokenLimitFilter(maxTokenCount:10000, consumeAllTokens:false))
  *     [...]
  *     -NewAnalyzer('strip html, fold to ascii, whitespace tokenize, max 10k tokens')
Index: lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java
===================================================================
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java	(revision 1591281)
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java	(working copy)
@@ -96,8 +96,8 @@
   }
   
   public static IndexWriterConfig createWriterConfig(Config config, PerfRunData runData, OpenMode mode, IndexCommit commit) {
-    // :Post-Release-Update-Version.LUCENE_XY:
-    Version version = Version.valueOf(config.get("writer.version", Version.LUCENE_49.toString()));
+    @SuppressWarnings("deprecation")
+    Version version = Version.parseLeniently(config.get("writer.version", Version.LUCENE_CURRENT.toString()));
     IndexWriterConfig iwConf = new IndexWriterConfig(version, runData.getAnalyzer());
     iwConf.setOpenMode(mode);
     IndexDeletionPolicy indexDeletionPolicy = getIndexDeletionPolicy(config);
Index: lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTaskTest.java
===================================================================
--- lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTaskTest.java	(revision 1591281)
+++ lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTaskTest.java	(working copy)
@@ -38,7 +38,7 @@
   private PerfRunData createPerfRunData(String infoStreamValue) throws Exception {
     Properties props = new Properties();
     // :Post-Release-Update-Version.LUCENE_XY:
-    props.setProperty("writer.version", Version.LUCENE_49.toString());
+    props.setProperty("writer.version", Version.LUCENE_4_9.toString());
     props.setProperty("print.props", "false"); // don't print anything
     props.setProperty("directory", "RAMDirectory");
     if (infoStreamValue != null) {
Index: lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java	(revision 1591281)
+++ lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java	(working copy)
@@ -159,7 +159,7 @@
    * Creates a new config that with defaults that match the specified
    * {@link Version} as well as the default {@link
    * Analyzer}. If matchVersion is >= {@link
-   * Version#LUCENE_32}, {@link TieredMergePolicy} is used
+   * Version#LUCENE_3_2}, {@link TieredMergePolicy} is used
    * for merging; else {@link LogByteSizeMergePolicy}.
    * Note that {@link TieredMergePolicy} is free to select
    * non-contiguous merges, which means docIDs may not
Index: lucene/core/src/java/org/apache/lucene/store/NRTCachingDirectory.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/store/NRTCachingDirectory.java	(revision 1591281)
+++ lucene/core/src/java/org/apache/lucene/store/NRTCachingDirectory.java	(working copy)
@@ -30,7 +30,7 @@
 //   - let subclass dictate policy...?
 //   - rename to MergeCacheingDir?  NRTCachingDir
 
-// :Post-Release-Update-Version.LUCENE_XY: (in <pre> block in javadoc below)
+// :Post-Release-Update-Version.LUCENE_X_Y: (in <pre> block in javadoc below)
 /**
  * Wraps a {@link RAMDirectory}
  * around any provided delegate directory, to
@@ -51,7 +51,7 @@
  * <pre class="prettyprint">
  *   Directory fsDir = FSDirectory.open(new File("/path/to/index"));
  *   NRTCachingDirectory cachedFSDir = new NRTCachingDirectory(fsDir, 5.0, 60.0);
- *   IndexWriterConfig conf = new IndexWriterConfig(Version.LUCENE_49, analyzer);
+ *   IndexWriterConfig conf = new IndexWriterConfig(Version.LUCENE_4_9, analyzer);
  *   IndexWriter writer = new IndexWriter(cachedFSDir, conf);
  * </pre>
  *
Index: lucene/core/src/java/org/apache/lucene/util/Version.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/Version.java	(revision 1591281)
+++ lucene/core/src/java/org/apache/lucene/util/Version.java	(working copy)
@@ -35,114 +35,114 @@
    * @deprecated (4.0) Use latest
    */
   @Deprecated
-  LUCENE_30,
+  LUCENE_3_0,
 
   /**
    * Match settings and bugs in Lucene's 3.1 release.
    * @deprecated (4.0) Use latest
    */
   @Deprecated
-  LUCENE_31,
+  LUCENE_3_1,
   
   /**
    * Match settings and bugs in Lucene's 3.2 release.
    * @deprecated (4.0) Use latest
    */
   @Deprecated
-  LUCENE_32,
+  LUCENE_3_2,
   
   /**
    * Match settings and bugs in Lucene's 3.3 release.
    * @deprecated (4.0) Use latest
    */
   @Deprecated
-  LUCENE_33,
+  LUCENE_3_3,
   
   /**
    * Match settings and bugs in Lucene's 3.4 release.
    * @deprecated (4.0) Use latest
    */
   @Deprecated
-  LUCENE_34,
+  LUCENE_3_4,
   
   /**
    * Match settings and bugs in Lucene's 3.5 release.
    * @deprecated (4.0) Use latest
    */
   @Deprecated
-  LUCENE_35,
+  LUCENE_3_5,
   
   /**
    * Match settings and bugs in Lucene's 3.6 release.
    * @deprecated (4.0) Use latest
    */
   @Deprecated
-  LUCENE_36,
+  LUCENE_3_6,
   
   /**
-   * Match settings and bugs in Lucene's 3.6 release.
+   * Match settings and bugs in Lucene's 4.0 release.
    * @deprecated (4.1) Use latest
    */
   @Deprecated
-  LUCENE_40,
+  LUCENE_4_0,
 
   /** Match settings and bugs in Lucene's 4.1 release.
    * @deprecated (4.2) Use latest
    */
   @Deprecated
-  LUCENE_41,
+  LUCENE_4_1,
 
   /** Match settings and bugs in Lucene's 4.2 release.
    * @deprecated (4.3) Use latest
    */
   @Deprecated
-  LUCENE_42,
+  LUCENE_4_2,
 
   /** Match settings and bugs in Lucene's 4.3 release.
    * @deprecated (4.4) Use latest
    */
   @Deprecated
-  LUCENE_43,
+  LUCENE_4_3,
 
   /** Match settings and bugs in Lucene's 4.4 release.
    * @deprecated (4.5) Use latest
    */
   @Deprecated
-  LUCENE_44,
+  LUCENE_4_4,
 
   /**
    * Match settings and bugs in Lucene's 4.5 release.
    * @deprecated (4.6) Use latest
    */
   @Deprecated
-  LUCENE_45,
+  LUCENE_4_5,
 
   /**
    * Match settings and bugs in Lucene's 4.6 release.
    * @deprecated (4.7) Use latest
    */
   @Deprecated
-  LUCENE_46,
+  LUCENE_4_6,
 
   /** Match settings and bugs in Lucene's 4.7 release.
    * @deprecated (4.8) Use latest
    */
   @Deprecated
-  LUCENE_47,
+  LUCENE_4_7,
 
   /**
    * Match settings and bugs in Lucene's 4.8 release.
    * @deprecated (4.9) Use latest
    */
   @Deprecated
-  LUCENE_48,
+  LUCENE_4_8,
 
   /** Match settings and bugs in Lucene's 4.9 release.
    *  <p>
    *  Use this to get the latest &amp; greatest settings, bug
    *  fixes, etc, for Lucene.
    */
-  LUCENE_49,
+  LUCENE_4_9,
 
   /* Add new constants for later versions **here** to respect order! */
 
@@ -165,12 +165,88 @@
   @Deprecated
   LUCENE_CURRENT;
 
+  
+  // Deprecated old version constants, just for backwards compatibility:
+  // Those are no longer enum constants and don't work in switch statements,
+  // but should fix most uses.
+  // TODO: Do not update them anymore, deprecated in 4.9, so LUCENE_49 is not needed!
+  // Remove in 5.0!
+  
+  /** @deprecated Bad naming of constant; use {@link #LUCENE_3_0} instead. */
+  @Deprecated
+  public static final Version LUCENE_30 = LUCENE_3_0;
+  
+  /** @deprecated Bad naming of constant; use {@link #LUCENE_3_1} instead. */
+  @Deprecated
+  public static final Version LUCENE_31 = LUCENE_3_1;
+  
+  /** @deprecated Bad naming of constant; use {@link #LUCENE_3_2} instead. */
+  @Deprecated
+  public static final Version LUCENE_32 = LUCENE_3_2;
+  
+  /** @deprecated Bad naming of constant; use {@link #LUCENE_3_3} instead. */
+  @Deprecated
+  public static final Version LUCENE_33 = LUCENE_3_3;
+  
+  /** @deprecated Bad naming of constant; use {@link #LUCENE_3_4} instead. */
+  @Deprecated
+  public static final Version LUCENE_34 = LUCENE_3_4;
+  
+  /** @deprecated Bad naming of constant; use {@link #LUCENE_3_5} instead. */
+  @Deprecated
+  public static final Version LUCENE_35 = LUCENE_3_5;
+  
+  /** @deprecated Bad naming of constant; use {@link #LUCENE_3_6} instead. */
+  @Deprecated
+  public static final Version LUCENE_36 = LUCENE_3_6;
+  
+  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_0} instead. */
+  @Deprecated
+  public static final Version LUCENE_40 = LUCENE_4_0;
+  
+  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_1} instead. */
+  @Deprecated
+  public static final Version LUCENE_41 = LUCENE_4_1;
+
+  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_2} instead. */
+  @Deprecated
+  public static final Version LUCENE_42 = LUCENE_4_2;
+
+  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_3} instead. */
+  @Deprecated
+  public static final Version LUCENE_43 = LUCENE_4_3;
+
+  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_4} instead. */
+  @Deprecated
+  public static final Version LUCENE_44 = LUCENE_4_4;
+
+  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_5} instead. */
+  @Deprecated
+  public static final Version LUCENE_45 = LUCENE_4_5;
+
+  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_6} instead. */
+  @Deprecated
+  public static final Version LUCENE_46 = LUCENE_4_6;
+
+  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_7} instead. */
+  @Deprecated
+  public static final Version LUCENE_47 = LUCENE_4_7;
+
+  /** @deprecated Bad naming of constant; use {@link #LUCENE_4_8} instead. */
+  @Deprecated
+  public static final Version LUCENE_48 = LUCENE_4_8;
+  
+  // End: Deprecated version constants -> Remove in 5.0
+
   public boolean onOrAfter(Version other) {
     return compareTo(other) >= 0;
   }
   
   public static Version parseLeniently(String version) {
-    String parsedMatchVersion = version.toUpperCase(Locale.ROOT);
-    return Version.valueOf(parsedMatchVersion.replaceFirst("^(\\d)\\.(\\d)$", "LUCENE_$1$2"));
+    final String parsedMatchVersion = version
+        .toUpperCase(Locale.ROOT)
+        .replaceFirst("^(\\d+)\\.(\\d+)$", "LUCENE_$1_$2")
+        .replaceFirst("^LUCENE_(\\d)(\\d)$", "LUCENE_$1_$2");
+    return Version.valueOf(parsedMatchVersion);
   }
 }
Index: lucene/core/src/test/org/apache/lucene/search/TestControlledRealTimeReopenThread.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestControlledRealTimeReopenThread.java	(revision 1591281)
+++ lucene/core/src/test/org/apache/lucene/search/TestControlledRealTimeReopenThread.java	(working copy)
@@ -474,7 +474,7 @@
 
     final SnapshotDeletionPolicy sdp = new SnapshotDeletionPolicy(new KeepOnlyLastCommitDeletionPolicy());
     final Directory dir = new NRTCachingDirectory(newFSDirectory(createTempDir("nrt")), 5, 128);
-    IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_46,
+    IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_4_6,
                                                      new MockAnalyzer(random()));
     config.setIndexDeletionPolicy(sdp);
     config.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);
Index: lucene/core/src/test/org/apache/lucene/util/TestVersion.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/util/TestVersion.java	(revision 1591281)
+++ lucene/core/src/test/org/apache/lucene/util/TestVersion.java	(working copy)
@@ -23,14 +23,15 @@
     for (Version v : Version.values()) {
       assertTrue("LUCENE_CURRENT must be always onOrAfter("+v+")", Version.LUCENE_CURRENT.onOrAfter(v));
     }
-    assertTrue(Version.LUCENE_40.onOrAfter(Version.LUCENE_31));
-    assertTrue(Version.LUCENE_40.onOrAfter(Version.LUCENE_40));
-    assertFalse(Version.LUCENE_30.onOrAfter(Version.LUCENE_31));
+    assertTrue(Version.LUCENE_4_0.onOrAfter(Version.LUCENE_3_1));
+    assertTrue(Version.LUCENE_4_0.onOrAfter(Version.LUCENE_4_0));
+    assertFalse(Version.LUCENE_3_0.onOrAfter(Version.LUCENE_3_1));
   }
 
   public void testParseLeniently() {
-    assertEquals(Version.LUCENE_40, Version.parseLeniently("4.0"));
-    assertEquals(Version.LUCENE_40, Version.parseLeniently("LUCENE_40"));
+    assertEquals(Version.LUCENE_4_0, Version.parseLeniently("4.0"));
+    assertEquals(Version.LUCENE_4_0, Version.parseLeniently("LUCENE_40"));
+    assertEquals(Version.LUCENE_4_0, Version.parseLeniently("LUCENE_4_0"));
     assertEquals(Version.LUCENE_CURRENT, Version.parseLeniently("LUCENE_CURRENT"));
   }
   
Index: lucene/demo/src/java/org/apache/lucene/demo/facet/FacetExamples.java
===================================================================
--- lucene/demo/src/java/org/apache/lucene/demo/facet/FacetExamples.java	(revision 1591281)
+++ lucene/demo/src/java/org/apache/lucene/demo/facet/FacetExamples.java	(working copy)
@@ -28,6 +28,6 @@
 
   // :Post-Release-Update-Version.LUCENE_XY:
   /** The Lucene {@link Version} used by the example code. */
-  public static final Version EXAMPLES_VER = Version.LUCENE_49;
+  public static final Version EXAMPLES_VER = Version.LUCENE_4_9;
 
 }
Index: lucene/demo/src/java/org/apache/lucene/demo/IndexFiles.java
===================================================================
--- lucene/demo/src/java/org/apache/lucene/demo/IndexFiles.java	(revision 1591281)
+++ lucene/demo/src/java/org/apache/lucene/demo/IndexFiles.java	(working copy)
@@ -88,8 +88,8 @@
 
       Directory dir = FSDirectory.open(new File(indexPath));
       // :Post-Release-Update-Version.LUCENE_XY:
-      Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_49);
-      IndexWriterConfig iwc = new IndexWriterConfig(Version.LUCENE_49, analyzer);
+      Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_4_9);
+      IndexWriterConfig iwc = new IndexWriterConfig(Version.LUCENE_4_9, analyzer);
 
       if (create) {
         // Create a new index in the directory, removing any
Index: lucene/demo/src/java/org/apache/lucene/demo/SearchFiles.java
===================================================================
--- lucene/demo/src/java/org/apache/lucene/demo/SearchFiles.java	(revision 1591281)
+++ lucene/demo/src/java/org/apache/lucene/demo/SearchFiles.java	(working copy)
@@ -91,7 +91,7 @@
     IndexReader reader = DirectoryReader.open(FSDirectory.open(new File(index)));
     IndexSearcher searcher = new IndexSearcher(reader);
     // :Post-Release-Update-Version.LUCENE_XY:
-    Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_49);
+    Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_4_9);
 
     BufferedReader in = null;
     if (queries != null) {
@@ -100,7 +100,7 @@
       in = new BufferedReader(new InputStreamReader(System.in, StandardCharsets.UTF_8));
     }
     // :Post-Release-Update-Version.LUCENE_XY:
-    QueryParser parser = new QueryParser(Version.LUCENE_49, field, analyzer);
+    QueryParser parser = new QueryParser(Version.LUCENE_4_9, field, analyzer);
     while (true) {
       if (queries == null && queryString == null) {                        // prompt the user
         System.out.println("Enter query: ");
Index: lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
===================================================================
--- lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java	(revision 1591281)
+++ lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java	(working copy)
@@ -286,7 +286,7 @@
     // :Post-Release-Update-Version.LUCENE_XY:
     // Make sure we use a MergePolicy which always merges adjacent segments and thus
     // keeps the doc IDs ordered as well (this is crucial for the taxonomy index).
-    return new IndexWriterConfig(Version.LUCENE_49,
+    return new IndexWriterConfig(Version.LUCENE_4_9,
         null).setOpenMode(openMode).setMergePolicy(
         new LogByteSizeMergePolicy());
   }
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java	(revision 1591281)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java	(working copy)
@@ -97,7 +97,7 @@
   public void init(Version matchVersion, String f, Analyzer a) {
     setAnalyzer(a);
     field = f;
-    if (matchVersion.onOrAfter(Version.LUCENE_31)) {
+    if (matchVersion.onOrAfter(Version.LUCENE_3_1)) {
       setAutoGeneratePhraseQueries(false);
     } else {
       setAutoGeneratePhraseQueries(true);
Index: lucene/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiAnalyzer.java
===================================================================
--- lucene/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiAnalyzer.java	(revision 1591281)
+++ lucene/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiAnalyzer.java	(working copy)
@@ -113,7 +113,7 @@
   }
     
   public void testPosIncrementAnalyzer() throws ParseException {
-    QueryParser qp = new QueryParser(Version.LUCENE_40, "", new PosIncrementAnalyzer());
+    QueryParser qp = new QueryParser(Version.LUCENE_4_0, "", new PosIncrementAnalyzer());
     assertEquals("quick brown", qp.parse("the quick brown").toString());
     assertEquals("quick brown fox", qp.parse("the quick brown fox").toString());
   }
Index: lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java	(revision 1591281)
+++ lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java	(working copy)
@@ -373,7 +373,7 @@
    * Use this constant when creating Analyzers and any other version-dependent stuff.
    * <p><b>NOTE:</b> Change this when development starts for new Lucene version:
    */
-  public static final Version TEST_VERSION_CURRENT = Version.LUCENE_49;
+  public static final Version TEST_VERSION_CURRENT = Version.LUCENE_4_9;
 
   /**
    * True if and only if tests are run in verbose mode. If this flag is false
Index: solr/contrib/morphlines-core/src/test-files/solr/collection1/conf/solrconfig.xml
===================================================================
--- solr/contrib/morphlines-core/src/test-files/solr/collection1/conf/solrconfig.xml	(revision 1591281)
+++ solr/contrib/morphlines-core/src/test-files/solr/collection1/conf/solrconfig.xml	(working copy)
@@ -35,7 +35,7 @@
        that you fully re-index after changing this setting as it can
        affect both how text is indexed and queried.
   -->
-  <luceneMatchVersion>LUCENE_43</luceneMatchVersion>
+  <luceneMatchVersion>4.9</luceneMatchVersion>
 
   <!-- lib directives can be used to instruct Solr to load an Jars
        identified and use them to resolve any "plugins" specified in
Index: solr/contrib/morphlines-core/src/test-files/solr/minimr/conf/solrconfig.xml
===================================================================
--- solr/contrib/morphlines-core/src/test-files/solr/minimr/conf/solrconfig.xml	(revision 1591281)
+++ solr/contrib/morphlines-core/src/test-files/solr/minimr/conf/solrconfig.xml	(working copy)
@@ -35,7 +35,7 @@
        that you fully re-index after changing this setting as it can
        affect both how text is indexed and queried.
   -->
-  <luceneMatchVersion>LUCENE_43</luceneMatchVersion>
+  <luceneMatchVersion>4.9</luceneMatchVersion>
 
   <!-- <lib/> directives can be used to instruct Solr to load an Jars
        identified and use them to resolve any "plugins" specified in
Index: solr/contrib/morphlines-core/src/test-files/solr/mrunit/conf/solrconfig.xml
===================================================================
--- solr/contrib/morphlines-core/src/test-files/solr/mrunit/conf/solrconfig.xml	(revision 1591281)
+++ solr/contrib/morphlines-core/src/test-files/solr/mrunit/conf/solrconfig.xml	(working copy)
@@ -35,7 +35,7 @@
        that you fully re-index after changing this setting as it can
        affect both how text is indexed and queried.
   -->
-  <luceneMatchVersion>LUCENE_43</luceneMatchVersion>
+  <luceneMatchVersion>4.9</luceneMatchVersion>
 
   <!-- <lib/> directives can be used to instruct Solr to load an Jars
        identified and use them to resolve any "plugins" specified in
Index: solr/contrib/morphlines-core/src/test-files/solr/solrcelltest/collection1/conf/solrconfig.xml
===================================================================
--- solr/contrib/morphlines-core/src/test-files/solr/solrcelltest/collection1/conf/solrconfig.xml	(revision 1591281)
+++ solr/contrib/morphlines-core/src/test-files/solr/solrcelltest/collection1/conf/solrconfig.xml	(working copy)
@@ -35,7 +35,7 @@
        that you fully re-index after changing this setting as it can
        affect both how text is indexed and queried.
   -->
-  <luceneMatchVersion>LUCENE_43</luceneMatchVersion>
+  <luceneMatchVersion>4.9</luceneMatchVersion>
 
   <!-- lib directives can be used to instruct Solr to load an Jars
        identified and use them to resolve any "plugins" specified in
Index: solr/contrib/morphlines-core/src/test-files/solr/solrcloud/conf/solrconfig.xml
===================================================================
--- solr/contrib/morphlines-core/src/test-files/solr/solrcloud/conf/solrconfig.xml	(revision 1591281)
+++ solr/contrib/morphlines-core/src/test-files/solr/solrcloud/conf/solrconfig.xml	(working copy)
@@ -35,7 +35,7 @@
        that you fully re-index after changing this setting as it can
        affect both how text is indexed and queried.
   -->
-  <luceneMatchVersion>LUCENE_43</luceneMatchVersion>
+  <luceneMatchVersion>4.9</luceneMatchVersion>
 
   <!-- <lib/> directives can be used to instruct Solr to load an Jars
        identified and use them to resolve any "plugins" specified in
Index: solr/core/src/java/org/apache/solr/core/SolrConfig.java
===================================================================
--- solr/core/src/java/org/apache/solr/core/SolrConfig.java	(revision 1591281)
+++ solr/core/src/java/org/apache/solr/core/SolrConfig.java	(working copy)
@@ -163,19 +163,19 @@
     luceneMatchVersion = getLuceneVersion("luceneMatchVersion");
     String indexConfigPrefix;
 
-    // Old indexDefaults and mainIndex sections are deprecated and fails fast for luceneMatchVersion=>LUCENE_40.
+    // Old indexDefaults and mainIndex sections are deprecated and fails fast for luceneMatchVersion=>LUCENE_4_0.
     // For older solrconfig.xml's we allow the old sections, but never mixed with the new <indexConfig>
     boolean hasDeprecatedIndexConfig = (getNode("indexDefaults", false) != null) || (getNode("mainIndex", false) != null);
     boolean hasNewIndexConfig = getNode("indexConfig", false) != null;
     if(hasDeprecatedIndexConfig){
-      if(luceneMatchVersion.onOrAfter(Version.LUCENE_40)) {
+      if(luceneMatchVersion.onOrAfter(Version.LUCENE_4_0)) {
         throw new SolrException(ErrorCode.FORBIDDEN, "<indexDefaults> and <mainIndex> configuration sections are discontinued. Use <indexConfig> instead.");
       } else {
         // Still allow the old sections for older LuceneMatchVersion's
         if(hasNewIndexConfig) {
           throw new SolrException(ErrorCode.FORBIDDEN, "Cannot specify both <indexDefaults>, <mainIndex> and <indexConfig> at the same time. Please use <indexConfig> only.");
         }
-        log.warn("<indexDefaults> and <mainIndex> configuration sections are deprecated and will fail for luceneMatchVersion=LUCENE_40 and later. Please use <indexConfig> instead.");
+        log.warn("<indexDefaults> and <mainIndex> configuration sections are deprecated and will fail for luceneMatchVersion=LUCENE_4_0 and later. Please use <indexConfig> instead.");
         defaultIndexConfig = new SolrIndexConfig(this, "indexDefaults", null);
         mainIndexConfig = new SolrIndexConfig(this, "mainIndex", defaultIndexConfig);
         indexConfigPrefix = "mainIndex";
Index: solr/core/src/java/org/apache/solr/schema/FieldTypePluginLoader.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/FieldTypePluginLoader.java	(revision 1591281)
+++ solr/core/src/java/org/apache/solr/schema/FieldTypePluginLoader.java	(working copy)
@@ -407,7 +407,7 @@
     Version version = (configuredVersion != null) ?
             Config.parseLuceneVersionString(configuredVersion) : schema.getDefaultLuceneMatchVersion();
 
-    if (!version.onOrAfter(Version.LUCENE_40)) {
+    if (!version.onOrAfter(Version.LUCENE_4_0)) {
       log.warn(pluginClassName + " is using deprecated " + version +
         " emulation. You should at some point declare and reindex to at least 4.0, because " +
         "3.x emulation is deprecated and will be removed in 5.0");
Index: solr/core/src/java/org/apache/solr/schema/IndexSchema.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/IndexSchema.java	(revision 1591281)
+++ solr/core/src/java/org/apache/solr/schema/IndexSchema.java	(working copy)
@@ -479,7 +479,7 @@
         similarityFactory = new DefaultSimilarityFactory();
         final NamedList similarityParams = new NamedList();
         Version luceneVersion = getDefaultLuceneMatchVersion();
-        if (!luceneVersion.onOrAfter(Version.LUCENE_47)) {
+        if (!luceneVersion.onOrAfter(Version.LUCENE_4_7)) {
           similarityParams.add(DefaultSimilarityFactory.DISCOUNT_OVERLAPS, false);
         }
         similarityFactory.init(SolrParams.toSolrParams(similarityParams));
Index: solr/core/src/java/org/apache/solr/update/SolrIndexConfig.java
===================================================================
--- solr/core/src/java/org/apache/solr/update/SolrIndexConfig.java	(revision 1591281)
+++ solr/core/src/java/org/apache/solr/update/SolrIndexConfig.java	(working copy)
@@ -127,7 +127,7 @@
     luceneVersion = solrConfig.luceneMatchVersion;
 
     // Assert that end-of-life parameters or syntax is not in our config.
-    // Warn for luceneMatchVersion's before LUCENE_36, fail fast above
+    // Warn for luceneMatchVersion's before LUCENE_3_6, fail fast above
     assertWarnOrFail("The <mergeScheduler>myclass</mergeScheduler> syntax is no longer supported in solrconfig.xml. Please use syntax <mergeScheduler class=\"myclass\"/> instead.",
         !((solrConfig.getNode(prefix+"/mergeScheduler",false) != null) && (solrConfig.get(prefix+"/mergeScheduler/@class",null) == null)),
         true);
Index: solr/core/src/test-files/solr/collection1/conf/schema-luceneMatchVersion.xml
===================================================================
--- solr/core/src/test-files/solr/collection1/conf/schema-luceneMatchVersion.xml	(revision 1591281)
+++ solr/core/src/test-files/solr/collection1/conf/schema-luceneMatchVersion.xml	(working copy)
@@ -38,7 +38,7 @@
     </analyzer>
   </fieldtype>
   <fieldtype name="textStandardAnalyzer40" class="solr.TextField">
-    <analyzer class="org.apache.lucene.analysis.standard.StandardAnalyzer" luceneMatchVersion="LUCENE_40"/>
+    <analyzer class="org.apache.lucene.analysis.standard.StandardAnalyzer" luceneMatchVersion="LUCENE_4_0"/>
   </fieldtype>
   <fieldtype name="textStandardAnalyzerDefault" class="solr.TextField">
     <analyzer class="org.apache.lucene.analysis.standard.StandardAnalyzer"/>
Index: solr/core/src/test-files/solr/collection1/conf/schema-rest-lucene-match-version.xml
===================================================================
--- solr/core/src/test-files/solr/collection1/conf/schema-rest-lucene-match-version.xml	(revision 1591281)
+++ solr/core/src/test-files/solr/collection1/conf/schema-rest-lucene-match-version.xml	(working copy)
@@ -20,8 +20,8 @@
     <fieldtype name="explicitLuceneMatchVersions" class="org.apache.solr.schema.TextField">
       <analyzer>
         <charFilter class="org.apache.solr.analysis.MockCharFilterFactory" luceneMatchVersion="LUCENE_40" remainder="0"/>
-        <tokenizer class="org.apache.solr.analysis.MockTokenizerFactory" luceneMatchVersion="LUCENE_40" />
-        <filter class="org.apache.solr.analysis.MockTokenFilterFactory" luceneMatchVersion="LUCENE_40" stopset="empty"/>
+        <tokenizer class="org.apache.solr.analysis.MockTokenizerFactory" luceneMatchVersion="LUCENE_4_0" />
+        <filter class="org.apache.solr.analysis.MockTokenFilterFactory" luceneMatchVersion="4.0" stopset="empty"/>
       </analyzer>
     </fieldtype>
     <fieldtype name="noLuceneMatchVersions" class="org.apache.solr.schema.TextField">
Index: solr/core/src/test-files/solr/collection1/conf/solrconfig-basic-luceneVersion31.xml
===================================================================
--- solr/core/src/test-files/solr/collection1/conf/solrconfig-basic-luceneVersion31.xml	(revision 1591281)
+++ solr/core/src/test-files/solr/collection1/conf/solrconfig-basic-luceneVersion31.xml	(working copy)
@@ -20,7 +20,7 @@
 <!-- a basic solrconfig that tests can use when they want simple minimal solrconfig/schema
      DO NOT ADD THINGS TO THIS CONFIG! -->
 <config>
-  <luceneMatchVersion>LUCENE_31</luceneMatchVersion>
+  <luceneMatchVersion>LUCENE_3_1</luceneMatchVersion>
   <directoryFactory name="DirectoryFactory" class="${solr.directoryFactory:solr.RAMDirectoryFactory}"/>
   <requestHandler name="standard" class="solr.StandardRequestHandler"></requestHandler>
 </config>
Index: solr/core/src/test-files/solr/collection1/conf/solrconfig-implicitproperties.xml
===================================================================
--- solr/core/src/test-files/solr/collection1/conf/solrconfig-implicitproperties.xml	(revision 1591281)
+++ solr/core/src/test-files/solr/collection1/conf/solrconfig-implicitproperties.xml	(working copy)
@@ -21,7 +21,7 @@
      discovery-based core configuration. Trying a minimal configuration to cut down the setup time.
      use in conjunction with schema-minimal.xml perhaps? -->
 <config>
-  <luceneMatchVersion>LUCENE_41</luceneMatchVersion>
+  <luceneMatchVersion>LUCENE_4_1</luceneMatchVersion>
 
   <dataDir>${solr.data.dir:}</dataDir>
 
Index: solr/core/src/test/org/apache/solr/analysis/TestLuceneMatchVersion.java
===================================================================
--- solr/core/src/test/org/apache/solr/analysis/TestLuceneMatchVersion.java	(revision 1591281)
+++ solr/core/src/test/org/apache/solr/analysis/TestLuceneMatchVersion.java	(working copy)
@@ -53,8 +53,8 @@
 
     type = schema.getFieldType("text40");
     ana = (TokenizerChain) type.getAnalyzer();
-    assertEquals(Version.LUCENE_40, (ana.getTokenizerFactory()).getLuceneMatchVersion());
-    assertEquals(Version.LUCENE_40, (ana.getTokenFilterFactories()[2]).getLuceneMatchVersion());
+    assertEquals(Version.LUCENE_4_0, (ana.getTokenizerFactory()).getLuceneMatchVersion());
+    assertEquals(Version.LUCENE_4_0, (ana.getTokenFilterFactories()[2]).getLuceneMatchVersion());
 
     // this is a hack to get the private matchVersion field in StandardAnalyzer's superclass, may break in later lucene versions - we have no getter :(
     final Field matchVersionField = StandardAnalyzer.class.getSuperclass().getDeclaredField("matchVersion");
@@ -68,6 +68,6 @@
     type = schema.getFieldType("textStandardAnalyzer40");
     ana1 = type.getAnalyzer();
     assertTrue(ana1 instanceof StandardAnalyzer);
-    assertEquals(Version.LUCENE_40, matchVersionField.get(ana1));
+    assertEquals(Version.LUCENE_4_0, matchVersionField.get(ana1));
   }
 }
Index: solr/core/src/test/org/apache/solr/rest/schema/TestSerializedLuceneMatchVersion.java
===================================================================
--- solr/core/src/test/org/apache/solr/rest/schema/TestSerializedLuceneMatchVersion.java	(revision 1591281)
+++ solr/core/src/test/org/apache/solr/rest/schema/TestSerializedLuceneMatchVersion.java	(working copy)
@@ -45,13 +45,13 @@
             "count(/response/lst[@name='fieldType']) = 1",
         
             "//lst[str[@name='class'][.='org.apache.solr.analysis.MockCharFilterFactory']]"
-           +"     [str[@name='luceneMatchVersion'][.='LUCENE_40']]",
+           +"     [str[@name='luceneMatchVersion'][.='LUCENE_4_0']]",
         
             "//lst[str[@name='class'][.='org.apache.solr.analysis.MockTokenizerFactory']]"
-           +"     [str[@name='luceneMatchVersion'][.='LUCENE_40']]",
+           +"     [str[@name='luceneMatchVersion'][.='LUCENE_4_0']]",
         
             "//lst[str[@name='class'][.='org.apache.solr.analysis.MockTokenFilterFactory']]"
-           +"     [str[@name='luceneMatchVersion'][.='LUCENE_40']]");
+           +"     [str[@name='luceneMatchVersion'][.='LUCENE_4_0']]");
   }
 
   @Test
Index: solr/core/src/test/org/apache/solr/search/similarities/TestNonDefinedSimilarityFactory.java
===================================================================
--- solr/core/src/test/org/apache/solr/search/similarities/TestNonDefinedSimilarityFactory.java	(revision 1591281)
+++ solr/core/src/test/org/apache/solr/search/similarities/TestNonDefinedSimilarityFactory.java	(working copy)
@@ -43,14 +43,14 @@
   }
 
   public void test47() throws Exception {
-    System.setProperty("tests.luceneMatchVersion", Version.LUCENE_47.toString());
+    System.setProperty("tests.luceneMatchVersion", Version.LUCENE_4_7.toString());
     initCore("solrconfig-basic.xml","schema-tiny.xml");
     DefaultSimilarity sim = getSimilarity("text", DefaultSimilarity.class);
     assertEquals(true, sim.getDiscountOverlaps());
   }
 
   public void test46() throws Exception {
-    System.setProperty("tests.luceneMatchVersion", Version.LUCENE_46.toString());
+    System.setProperty("tests.luceneMatchVersion", Version.LUCENE_4_6.toString());
     initCore("solrconfig-basic.xml","schema-tiny.xml");
     DefaultSimilarity sim = getSimilarity("text", DefaultSimilarity.class);
     assertEquals(false, sim.getDiscountOverlaps());
