diff --git a/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java b/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
index 81f88865244..a951b172d33 100644
--- a/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
@@ -4615,6 +4615,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable,
         // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we "merged":
         assert merge.info.info.maxDoc() == 0;
         commitMerge(merge, mergeState);
+        success = true;
         return 0;
       }
 
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
index 1db3dd81463..b3845df660d 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -41,6 +41,7 @@ import java.util.Set;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.Semaphore;
 import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicInteger;
 import java.util.function.Predicate;
 import java.util.stream.Collectors;
 
@@ -61,6 +62,7 @@ import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.LongPoint;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.SortedNumericDocValuesField;
@@ -3776,4 +3778,148 @@ public class TestIndexWriter extends LuceneTestCase {
       IOUtils.close(sm, dir);
     }
   }
+
+  public void testMergeOnCommitKeepFullyDeletedSegments() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    iwc.mergePolicy = new FilterMergePolicy(newMergePolicy()) {
+      @Override
+      public boolean keepFullyDeletedSegment(IOSupplier<CodecReader> readerIOSupplier) {
+        return true;
+      }
+
+      @Override
+      public MergeSpecification findFullFlushMerges(MergeTrigger mergeTrigger,
+                                                    SegmentInfos segmentInfos,
+                                                    MergeContext mergeContext) {
+        List<SegmentCommitInfo> fullyDeletedSegments = segmentInfos.asList().stream()
+            .filter(s -> s.info.maxDoc() - s.getDelCount() == 0)
+            .collect(Collectors.toList());
+        if (fullyDeletedSegments.isEmpty()) {
+          return null;
+        }
+        MergeSpecification spec = new MergeSpecification();
+        spec.add(new OneMerge(fullyDeletedSegments));
+        return spec;
+      }
+    };
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document d = new Document();
+    d.add(new StringField("id", "1", Field.Store.YES));
+    w.addDocument(d);
+    w.commit();
+    w.updateDocument(new Term("id", "1"), d);
+    w.commit();
+    try (DirectoryReader reader = w.getReader()) {
+      assertEquals(1, reader.numDocs());
+    }
+    IOUtils.close(w, dir);
+  }
+
+  public void testRandomOperations() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    iwc.setMergePolicy(random().nextBoolean() ? newMergePolicy() : new MockRandomMergePolicy(random()));
+    iwc.setMergePolicy(new FilterMergePolicy(iwc.mergePolicy) {
+      boolean keepFullyDeletedSegment = random().nextBoolean();
+
+      @Override
+      public boolean keepFullyDeletedSegment(IOSupplier<CodecReader> readerIOSupplier) {
+        return keepFullyDeletedSegment;
+      }
+    });
+    IndexWriter writer = new IndexWriter(dir, iwc);
+    SearcherManager sm = new SearcherManager(writer, new SearcherFactory());
+    Semaphore indexedDocs = new Semaphore(100 + random().nextInt(1000));
+    boolean singleDoc = random().nextBoolean();
+    Thread[] threads = new Thread[1 + random().nextInt(4)];
+    CountDownLatch latch = new CountDownLatch(threads.length);
+    for (int i = 0; i < threads.length; i++) {
+      threads[i] = new Thread(() -> {
+        latch.countDown();
+        try {
+          latch.await();
+          while (indexedDocs.tryAcquire()) {
+            String id = singleDoc ? "1" : Integer.toString(random().nextInt(10));
+            Document doc = new Document();
+            doc.add(new StringField("id", id, Field.Store.YES));
+            if (random().nextInt(10) <= 2) {
+              writer.updateDocument(new Term("id", id), doc);
+            } else if (random().nextInt(10) <= 2) {
+              writer.deleteDocuments(new Term("id", id));
+
+            } else {
+              writer.addDocument(doc);
+            }
+            if (random().nextInt(10) <= 1) {
+              sm.maybeRefreshBlocking();
+            }
+            if (rarely(random())) {
+              writer.commit();
+            }
+          }
+        } catch (Exception e) {
+          throw new AssertionError(e);
+        }
+      });
+      threads[i].start();
+    }
+    for (Thread thread : threads) {
+      thread.join();
+    }
+    IOUtils.close(writer, sm, dir);
+  }
+
+  public void testRandomOperationsWithSoftDeletes() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig();
+    AtomicInteger seqNo = new AtomicInteger(-1);
+    AtomicInteger retainingSeqNo = new AtomicInteger();
+    iwc.setMergePolicy(random().nextBoolean() ? newMergePolicy() : new MockRandomMergePolicy(random()));
+    iwc.setSoftDeletesField("soft_deletes");
+    iwc.setMergePolicy(new SoftDeletesRetentionMergePolicy("soft_deletes",
+        () -> LongPoint.newRangeQuery("seq_no", retainingSeqNo.longValue(), Long.MAX_VALUE), iwc.mergePolicy));
+    IndexWriter writer = new IndexWriter(dir, iwc);
+    SearcherManager sm = new SearcherManager(writer, new SearcherFactory());
+    Semaphore indexedDocs = new Semaphore(100 + random().nextInt(5000));
+    Thread[] threads = new Thread[1 + random().nextInt(4)];
+    CountDownLatch latch = new CountDownLatch(threads.length);
+    for (int i = 0; i < threads.length; i++) {
+      threads[i] = new Thread(() -> {
+        latch.countDown();
+        try {
+          latch.await();
+          while (indexedDocs.tryAcquire()) {
+            String id = Integer.toString(random().nextInt(10));
+            Document doc = new Document();
+            doc.add(new StringField("id", id, Field.Store.YES));
+            doc.add(new LongPoint("seq_no", seqNo.getAndIncrement()));
+            if (random().nextInt(10) <= 2) {
+              writer.softUpdateDocument(new Term("id", id), doc, new NumericDocValuesField(iwc.softDeletesField, 1));
+            } else {
+              writer.addDocument(doc);
+            }
+            if (random().nextInt(10) <= 1) {
+              sm.maybeRefreshBlocking();
+            }
+            if (rarely(random())) {
+              int min = retainingSeqNo.get();
+              int max = seqNo.get();
+              if (min < max && random().nextBoolean()) {
+                retainingSeqNo.compareAndSet(min, min - random().nextInt(max - min));
+              }
+              writer.commit();
+            }
+          }
+        } catch (Exception e) {
+          throw new AssertionError(e);
+        }
+      });
+      threads[i].start();
+    }
+    for (Thread thread : threads) {
+      thread.join();
+    }
+    IOUtils.close(writer, sm, dir);
+  }
 }
