Index: lucene/backwards/backwards-readme.txt
===================================================================
--- lucene/backwards/backwards-readme.txt	(revision 958293)
+++ lucene/backwards/backwards-readme.txt	(working copy)
@@ -1,13 +1,14 @@
 This folder contains the src/ folder of the previous Lucene major version.
 
-The test-backwards ANT task compiles the core classes of the previous version and its tests
-against these class files. After that the compiled test classes are run against the new
-lucene-core.jar file.
+The test-backwards ANT task compiles the previous version's tests (bundled) against the
+previous released lucene-core.jar file (bundled). After that the compiled test classes
+are run against the new lucene-core.jar file, created by ANT before.
 
 After branching a new Lucene major version (branch name "lucene_X_Y") do the following:
 
-* svn rm backwards/src/
-* svn cp https://svn.apache.org/repos/asf/lucene/dev/branches/lucene_X_Y/lucene/src/ backwards/src/
+* svn rm backwards/src/test
+* svn cp https://svn.apache.org/repos/asf/lucene/dev/branches/lucene_X_Y/lucene/src/test backwards/src/test
+* Copy the lucene-core.jar from the last release tarball to backwards/lib and delete old one.
 * Check that everything is correct: The backwards folder should contain a src/ folder
-  that now contains java, test, demo,.... The files should be the ones from the branch.
+  that now contains "test". The files should be the ones from the branch.
 * Run "ant test-backwards"
Index: lucene/backwards/lib/lucene-core-3.0.2.jar
===================================================================
Cannot display: file marked as a binary type.
svn:mime-type = application/octet-stream

Property changes on: lucene\backwards\lib\lucene-core-3.0.2.jar
___________________________________________________________________
Added: svn:mime-type
   + application/octet-stream

Index: lucene/backwards/src/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java
===================================================================
--- lucene/backwards/src/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java	(revision 958293)
+++ lucene/backwards/src/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java	(working copy)
@@ -421,7 +421,7 @@
   private IndexWriter newWriter(Directory dir, boolean create)
       throws IOException {
     final IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), create, IndexWriter.MaxFieldLength.UNLIMITED);
-    writer.setMergePolicy(new LogDocMergePolicy());
+    //BACKWARDS TEST PASS WITHOUT THIS: writer.setMergePolicy(new LogDocMergePolicy());
     return writer;
   }
 
@@ -495,7 +495,7 @@
 
     Directory dir = new MockRAMDirectory();
     IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
-    writer.setMergePolicy(new LogByteSizeMergePolicy());
+    //BACKWARDS TEST PASS WITHOUT THIS: writer.setMergePolicy(new LogByteSizeMergePolicy());
     writer.setMaxBufferedDocs(5);
     writer.setUseCompoundFile(false);
     writer.setMergeFactor(100);
@@ -521,9 +521,11 @@
 
     Directory dir2 = new MockRAMDirectory();
     writer = new IndexWriter(dir2, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    /* BACKWARDS TEST PASS WITHOUT THIS: 
     LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();
     lmp.setMinMergeMB(0.0001);
     writer.setMergePolicy(lmp);
+    */
     writer.setMergeFactor(4);
     writer.setUseCompoundFile(false);
     writer.setMergeScheduler(new SerialMergeScheduler());
Index: lucene/backwards/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java
===================================================================
--- lucene/backwards/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java	(revision 958293)
+++ lucene/backwards/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java	(working copy)
@@ -110,6 +110,8 @@
     directory.close();
   }
 
+  //BACKWARDS_BREAK: 
+  /*
   // Test that deletes committed after a merge started and
   // before it finishes, are correctly merged back:
   public void testDeleteMerging() throws IOException {
@@ -152,7 +154,7 @@
     assertEquals(450, reader.numDocs());
     reader.close();
     directory.close();
-  }
+  }*/
 
   public void testNoExtraFiles() throws IOException {
 
Index: lucene/backwards/src/test/org/apache/lucene/index/TestIndexReaderReopen.java
===================================================================
--- lucene/backwards/src/test/org/apache/lucene/index/TestIndexReaderReopen.java	(revision 958293)
+++ lucene/backwards/src/test/org/apache/lucene/index/TestIndexReaderReopen.java	(working copy)
@@ -951,7 +951,7 @@
     IndexWriter.unlock(dir);
     IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
 
-    w.setMergePolicy(new LogDocMergePolicy());
+    //BACKWARDS TEST PASS WITHOUT THIS: w.setMergePolicy(new LogDocMergePolicy());
     
     for (int i = 0; i < 100; i++) {
       w.addDocument(createDocument(i, 4));
Index: lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriter.java
===================================================================
--- lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriter.java	(revision 958293)
+++ lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriter.java	(working copy)
@@ -600,7 +600,8 @@
       dir.close();
     }
 
-    public void testOptimizeMaxNumSegments() throws IOException {
+    //BACKWARDS_BREAK: 
+    /*public void testOptimizeMaxNumSegments() throws IOException {
 
       MockRAMDirectory dir = new MockRAMDirectory();
 
@@ -679,7 +680,7 @@
         else
           assertEquals(7, optSegCount);
       }
-    }
+    }*/
 
     /**
      * Make sure optimize doesn't use any more than 1X
@@ -2751,6 +2752,8 @@
     dir.close();
   }
 
+  //BACKWARDS_BREAK: 
+  /*
   // LUCENE-1168
   public void testTermVectorCorruption() throws IOException {
 
@@ -2887,7 +2890,7 @@
     }
     reader.close();
     dir.close();
-  }
+  }*/
 
   // LUCENE-1084: test user-specified field length
   public void testUserSpecifiedMaxFieldLength() throws IOException {
Index: lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
===================================================================
--- lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java	(revision 958293)
+++ lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java	(working copy)
@@ -1,269 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util._TestUtil;
-
-import org.apache.lucene.util.LuceneTestCase;
-
-public class TestIndexWriterMergePolicy extends LuceneTestCase {
-
-  // Test the normal case
-  public void testNormalCase() throws IOException {
-    Directory dir = new RAMDirectory();
-
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
-    writer.setMaxBufferedDocs(10);
-    writer.setMergeFactor(10);
-    writer.setMergePolicy(new LogDocMergePolicy());
-
-    for (int i = 0; i < 100; i++) {
-      addDoc(writer);
-      checkInvariants(writer);
-    }
-
-    writer.close();
-  }
-
-  // Test to see if there is over merge
-  public void testNoOverMerge() throws IOException {
-    Directory dir = new RAMDirectory();
-
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
-    writer.setMaxBufferedDocs(10);
-    writer.setMergeFactor(10);
-    writer.setMergePolicy(new LogDocMergePolicy());
-
-    boolean noOverMerge = false;
-    for (int i = 0; i < 100; i++) {
-      addDoc(writer);
-      checkInvariants(writer);
-      if (writer.getNumBufferedDocuments() + writer.getSegmentCount() >= 18) {
-        noOverMerge = true;
-      }
-    }
-    assertTrue(noOverMerge);
-
-    writer.close();
-  }
-
-  // Test the case where flush is forced after every addDoc
-  public void testForceFlush() throws IOException {
-    Directory dir = new RAMDirectory();
-
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
-    writer.setMaxBufferedDocs(10);
-    writer.setMergeFactor(10);
-    LogDocMergePolicy mp = new LogDocMergePolicy();
-    mp.setMinMergeDocs(100);
-    writer.setMergePolicy(mp);
-
-    for (int i = 0; i < 100; i++) {
-      addDoc(writer);
-      writer.close();
-
-      writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
-      writer.setMaxBufferedDocs(10);
-      mp = new LogDocMergePolicy();
-      writer.setMergePolicy(mp);
-      mp.setMinMergeDocs(100);
-      writer.setMergeFactor(10);
-      checkInvariants(writer);
-    }
-
-    writer.close();
-  }
-
-  // Test the case where mergeFactor changes
-  public void testMergeFactorChange() throws IOException {
-    Directory dir = new RAMDirectory();
-
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
-    writer.setMaxBufferedDocs(10);
-    writer.setMergeFactor(100);
-    writer.setMergePolicy(new LogDocMergePolicy());
-
-    for (int i = 0; i < 250; i++) {
-      addDoc(writer);
-      checkInvariants(writer);
-    }
-
-    writer.setMergeFactor(5);
-
-    // merge policy only fixes segments on levels where merges
-    // have been triggered, so check invariants after all adds
-    for (int i = 0; i < 10; i++) {
-      addDoc(writer);
-    }
-    checkInvariants(writer);
-
-    writer.close();
-  }
-
-  // Test the case where both mergeFactor and maxBufferedDocs change
-  public void testMaxBufferedDocsChange() throws IOException {
-    Directory dir = new RAMDirectory();
-
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
-    writer.setMaxBufferedDocs(101);
-    writer.setMergeFactor(101);
-    writer.setMergePolicy(new LogDocMergePolicy());
-
-    // leftmost* segment has 1 doc
-    // rightmost* segment has 100 docs
-    for (int i = 1; i <= 100; i++) {
-      for (int j = 0; j < i; j++) {
-        addDoc(writer);
-        checkInvariants(writer);
-      }
-      writer.close();
-
-      writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);
-      writer.setMaxBufferedDocs(101);
-      writer.setMergeFactor(101);
-      writer.setMergePolicy(new LogDocMergePolicy());
-    }
-
-    writer.setMaxBufferedDocs(10);
-    writer.setMergeFactor(10);
-
-    // merge policy only fixes segments on levels where merges
-    // have been triggered, so check invariants after all adds
-    for (int i = 0; i < 100; i++) {
-      addDoc(writer);
-    }
-    checkInvariants(writer);
-
-    for (int i = 100; i < 1000; i++) {
-      addDoc(writer);
-    }
-    writer.commit();
-    ((ConcurrentMergeScheduler) writer.getMergeScheduler()).sync();
-    writer.commit();
-    checkInvariants(writer);
-
-    writer.close();
-  }
-
-  // Test the case where a merge results in no doc at all
-  public void testMergeDocCount0() throws IOException {
-    Directory dir = new RAMDirectory();
-
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
-    writer.setMergePolicy(new LogDocMergePolicy());
-    writer.setMaxBufferedDocs(10);
-    writer.setMergeFactor(100);
-
-    for (int i = 0; i < 250; i++) {
-      addDoc(writer);
-      checkInvariants(writer);
-    }
-    writer.close();
-
-    IndexReader reader = IndexReader.open(dir, false);
-    reader.deleteDocuments(new Term("content", "aaa"));
-    reader.close();
-
-    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);
-    writer.setMergePolicy(new LogDocMergePolicy());
-    writer.setMaxBufferedDocs(10);
-    writer.setMergeFactor(5);
-
-    // merge factor is changed, so check invariants after all adds
-    for (int i = 0; i < 10; i++) {
-      addDoc(writer);
-    }
-    writer.commit();
-    ((ConcurrentMergeScheduler) writer.getMergeScheduler()).sync();
-    writer.commit();
-    checkInvariants(writer);
-    assertEquals(10, writer.maxDoc());
-
-    writer.close();
-  }
-
-  private void addDoc(IndexWriter writer) throws IOException {
-    Document doc = new Document();
-    doc.add(new Field("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
-    writer.addDocument(doc);
-  }
-
-  private void checkInvariants(IndexWriter writer) throws IOException {
-    _TestUtil.syncConcurrentMerges(writer);
-    int maxBufferedDocs = writer.getMaxBufferedDocs();
-    int mergeFactor = writer.getMergeFactor();
-    int maxMergeDocs = writer.getMaxMergeDocs();
-
-    int ramSegmentCount = writer.getNumBufferedDocuments();
-    assertTrue(ramSegmentCount < maxBufferedDocs);
-
-    int lowerBound = -1;
-    int upperBound = maxBufferedDocs;
-    int numSegments = 0;
-
-    int segmentCount = writer.getSegmentCount();
-    for (int i = segmentCount - 1; i >= 0; i--) {
-      int docCount = writer.getDocCount(i);
-      assertTrue(docCount > lowerBound);
-
-      if (docCount <= upperBound) {
-        numSegments++;
-      } else {
-        if (upperBound * mergeFactor <= maxMergeDocs) {
-          assertTrue("maxMergeDocs=" + maxMergeDocs + "; numSegments=" + numSegments + "; upperBound=" + upperBound + "; mergeFactor=" + mergeFactor + "; segs=" + writer.segString(), numSegments < mergeFactor);
-        }
-
-        do {
-          lowerBound = upperBound;
-          upperBound *= mergeFactor;
-        } while (docCount > upperBound);
-        numSegments = 1;
-      }
-    }
-    if (upperBound * mergeFactor <= maxMergeDocs) {
-      assertTrue(numSegments < mergeFactor);
-    }
-
-    String[] files = writer.getDirectory().listAll();
-    int segmentCfsCount = 0;
-    for (int i = 0; i < files.length; i++) {
-      if (files[i].endsWith(".cfs")) {
-        segmentCfsCount++;
-      }
-    }
-    assertEquals(segmentCount, segmentCfsCount);
-  }
-
-  /*
-  private void printSegmentDocCounts(IndexWriter writer) {
-    int segmentCount = writer.getSegmentCount();
-    System.out.println("" + segmentCount + " segments total");
-    for (int i = 0; i < segmentCount; i++) {
-      System.out.println("  segment " + i + " has " + writer.getDocCount(i)
-          + " docs");
-    }
-  }
-  */
-}
Index: lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriterReader.java
===================================================================
--- lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriterReader.java	(revision 958293)
+++ lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriterReader.java	(working copy)
@@ -573,7 +573,7 @@
       boolean multiSegment) throws IOException {
     IndexWriter w = new IndexWriter(dir1, new WhitespaceAnalyzer(),
         IndexWriter.MaxFieldLength.LIMITED);
-    w.setMergePolicy(new LogDocMergePolicy());
+    //BACKWARDS TEST PASS WITHOUT THIS: w.setMergePolicy(new LogDocMergePolicy());
     for (int i = 0; i < 100; i++) {
       w.addDocument(createDocument(i, indexName, 4));
       if (multiSegment && (i % 10) == 0) {
Index: lucene/backwards/src/test/org/apache/lucene/search/CachingWrapperFilterHelper.java
===================================================================
--- lucene/backwards/src/test/org/apache/lucene/search/CachingWrapperFilterHelper.java	(revision 958293)
+++ lucene/backwards/src/test/org/apache/lucene/search/CachingWrapperFilterHelper.java	(working copy)
@@ -1,88 +0,0 @@
-package org.apache.lucene.search;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.BitSet;
-import java.util.WeakHashMap;
-
-import junit.framework.TestCase;
-
-import org.apache.lucene.index.IndexReader;
-
-/**
- * A unit test helper class to test when the filter is getting cached and when it is not.
- */
-public class CachingWrapperFilterHelper extends CachingWrapperFilter {
-  
-  private boolean shouldHaveCache = false;
-
-  /**
-   * @param filter Filter to cache results of
-   */
-  public CachingWrapperFilterHelper(Filter filter) {
-    super(filter);
-  }
-  
-  public void setShouldHaveCache(boolean shouldHaveCache) {
-    this.shouldHaveCache = shouldHaveCache;
-  }
-  
-  @Override
-  public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
-    if (cache == null) {
-      cache = new WeakHashMap();
-    }
-    
-    synchronized (cache) {  // check cache
-      DocIdSet cached = (DocIdSet) cache.get(reader);
-      if (shouldHaveCache) {
-        TestCase.assertNotNull("Cache should have data ", cached);
-      } else {
-        TestCase.assertNull("Cache should be null " + cached , cached);
-      }
-      if (cached != null) {
-        return cached;
-      }
-    }
-
-    final DocIdSet bits = filter.getDocIdSet(reader);
-
-    synchronized (cache) {  // update cache
-      cache.put(reader, bits);
-    }
-
-    return bits;
-  }
-
-  @Override
-  public String toString() {
-    return "CachingWrapperFilterHelper("+filter+")";
-  }
-
-  @Override
-  public boolean equals(Object o) {
-    if (!(o instanceof CachingWrapperFilterHelper)) return false;
-    return this.filter.equals((CachingWrapperFilterHelper)o);
-  }
-  
-  @Override
-  public int hashCode() {
-    return this.filter.hashCode() ^ 0x5525aacb;
-  }
-}
Index: lucene/backwards/src/test/org/apache/lucene/store/MockRAMDirectory.java
===================================================================
--- lucene/backwards/src/test/org/apache/lucene/store/MockRAMDirectory.java	(revision 958293)
+++ lucene/backwards/src/test/org/apache/lucene/store/MockRAMDirectory.java	(working copy)
@@ -28,6 +28,7 @@
 import java.util.Set;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.concurrent.atomic.AtomicLong;
 
 /**
  * This is a subclass of RAMDirectory that adds methods
@@ -213,7 +214,12 @@
       throw new IOException("file " + name + " already exists");
     else {
       if (existing!=null) {
-        sizeInBytes.getAndAdd(-existing.sizeInBytes);
+        //BACKWARDS BREAK in RamDirectory: sizeInBytes -= existing.sizeInBytes;
+        try {
+          ((AtomicLong) getClass().getSuperclass().getDeclaredField("sizeInBytes").get(this)).getAndAdd(-existing.sizeInBytes);
+        } catch (Exception e) {
+          throw new RuntimeException("Backwards-hack failed.", e);
+        }
         existing.directory = null;
       }
 
Index: lucene/backwards/src/test/org/apache/lucene/store/TestHugeRamFile.java
===================================================================
--- lucene/backwards/src/test/org/apache/lucene/store/TestHugeRamFile.java	(revision 958293)
+++ lucene/backwards/src/test/org/apache/lucene/store/TestHugeRamFile.java	(working copy)
@@ -33,7 +33,7 @@
     private long capacity = 0;
     private HashMap singleBuffers = new HashMap();
     @Override
-    byte[] newBuffer(int size) {
+    protected byte[] newBuffer(int size) {
       capacity += size;
       if (capacity <= MAX_VALUE) {
         // below maxint we reuse buffers
Index: lucene/backwards/src/test/org/apache/lucene/TestSnapshotDeletionPolicy.java
===================================================================
--- lucene/backwards/src/test/org/apache/lucene/TestSnapshotDeletionPolicy.java	(revision 958293)
+++ lucene/backwards/src/test/org/apache/lucene/TestSnapshotDeletionPolicy.java	(working copy)
@@ -1,194 +0,0 @@
-package org.apache.lucene;
-// Intentionally not in org.apache.lucene.index, to assert
-// that we do not require any package private access.
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Iterator;
-import java.util.Collection;
-import java.io.File;
-import java.io.IOException;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.MockRAMDirectory;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.index.IndexCommit;
-import org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.TestIndexWriter;
-import org.apache.lucene.index.SnapshotDeletionPolicy;
-import org.apache.lucene.util.ThreadInterruptedException;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
-
-//
-// This was developed for Lucene In Action,
-// http://lucenebook.com
-//
-
-public class TestSnapshotDeletionPolicy extends LuceneTestCase
-{
-  public static final String INDEX_PATH = "test.snapshots";
-
-  public void testSnapshotDeletionPolicy() throws Exception {
-    File dir = _TestUtil.getTempDir(INDEX_PATH);
-    try {
-      Directory fsDir = FSDirectory.open(dir);
-      runTest(fsDir);
-      fsDir.close();
-    } finally {
-      _TestUtil.rmDir(dir);
-    }
-
-    MockRAMDirectory dir2 = new MockRAMDirectory();
-    runTest(dir2);
-    dir2.close();
-  }
-
-  private void runTest(Directory dir) throws Exception {
-    // Run for ~1 seconds
-    final long stopTime = System.currentTimeMillis() + 1000;
-
-    SnapshotDeletionPolicy dp = new SnapshotDeletionPolicy(new KeepOnlyLastCommitDeletionPolicy());
-    final IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), dp, IndexWriter.MaxFieldLength.UNLIMITED);
-    writer.commit();
-    // Force frequent flushes
-    writer.setMaxBufferedDocs(2);
-
-    final Thread t = new Thread() {
-        @Override
-        public void run() {
-          Document doc = new Document();
-          doc.add(new Field("content", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
-          do {
-            for(int i=0;i<27;i++) {
-              try {
-                writer.addDocument(doc);
-              } catch (Throwable t) {
-                t.printStackTrace(System.out);
-                fail("addDocument failed");
-              }
-              if (i%2 == 0) {
-                try {
-                  writer.commit();
-                } catch (Exception e) {
-                  throw new RuntimeException(e);
-                }
-              }
-            }
-            try {
-              Thread.sleep(1);
-            } catch (InterruptedException ie) {
-              throw new ThreadInterruptedException(ie);
-            }
-          } while(System.currentTimeMillis() < stopTime);
-        }
-      };
-
-    t.start();
-
-    // While the above indexing thread is running, take many
-    // backups:
-    do {
-      backupIndex(dir, dp);
-      Thread.sleep(20);
-    } while(t.isAlive());
-
-    t.join();
-
-    // Add one more document to force writer to commit a
-    // final segment, so deletion policy has a chance to
-    // delete again:
-    Document doc = new Document();
-    doc.add(new Field("content", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
-    writer.addDocument(doc);
-
-    // Make sure we don't have any leftover files in the
-    // directory:
-    writer.close();
-    TestIndexWriter.assertNoUnreferencedFiles(dir, "some files were not deleted but should have been");
-  }
-
-  /** Example showing how to use the SnapshotDeletionPolicy
-   *  to take a backup.  This method does not really do a
-   *  backup; instead, it reads every byte of every file
-   *  just to test that the files indeed exist and are
-   *  readable even while the index is changing. */
-  public void backupIndex(Directory dir, SnapshotDeletionPolicy dp) throws Exception {
-    // To backup an index we first take a snapshot:
-    try {
-      copyFiles(dir, (IndexCommit) dp.snapshot("id"));
-    } finally {
-      // Make sure to release the snapshot, otherwise these
-      // files will never be deleted during this IndexWriter
-      // session:
-      dp.release("id");
-    }
-  }
-
-  private void copyFiles(Directory dir, IndexCommit cp) throws Exception {
-
-    // While we hold the snapshot, and nomatter how long
-    // we take to do the backup, the IndexWriter will
-    // never delete the files in the snapshot:
-    Collection files = cp.getFileNames();
-    Iterator it = files.iterator();
-    while(it.hasNext()) {
-      final String fileName = (String) it.next();
-      // NOTE: in a real backup you would not use
-      // readFile; you would need to use something else
-      // that copies the file to a backup location.  This
-      // could even be a spawned shell process (eg "tar",
-      // "zip") that takes the list of files and builds a
-      // backup.
-      readFile(dir, fileName);
-    }
-  }
-
-  byte[] buffer = new byte[4096];
-
-  private void readFile(Directory dir, String name) throws Exception {
-    IndexInput input = dir.openInput(name);
-    try {
-      long size = dir.fileLength(name);
-      long bytesLeft = size;
-      while (bytesLeft > 0) {
-        final int numToRead;
-        if (bytesLeft < buffer.length)
-          numToRead = (int) bytesLeft;
-        else
-          numToRead = buffer.length;
-        input.readBytes(buffer, 0, numToRead, false);
-        bytesLeft -= numToRead;
-      }
-      // Don't do this in your real backups!  This is just
-      // to force a backup to take a somewhat long time, to
-      // make sure we are exercising the fact that the
-      // IndexWriter should not delete this file even when I
-      // take my time reading it.
-      Thread.sleep(1);
-    } finally {
-      input.close();
-    }
-  }
-}
-
Index: lucene/build.xml
===================================================================
--- lucene/build.xml	(revision 958293)
+++ lucene/build.xml	(working copy)
@@ -76,13 +76,11 @@
           description="Runs all unit tests (core, contrib and back-compat)"
   />
 
-  <path id="backwards.compile.classpath">
-    <pathelement location="${build.dir.backwards}/classes/java"/>
-  </path>
-	
   <path id="backwards.test.compile.classpath">
     <path refid="junit-path"/>
-    <pathelement location="${build.dir.backwards}/classes/java"/>
+    <fileset dir="${backwards.dir}/lib">
+      <include name="lucene-core*.jar"/>
+    </fileset>
   </path>
 	
   <path id="backwards.junit.classpath">
@@ -106,17 +104,8 @@
   	description="Runs tests of a previous Lucene version." if="backwards.available">
     <sequential>
       <mkdir dir="${build.dir.backwards}"/>	  
-
-      <!-- first compile branch classes -->
-      <compile
-        srcdir="${backwards.dir}/src/java"
-        destdir="${build.dir.backwards}/classes/java"
-        javac.source="${javac.source.backwards}" javac.target="${javac.target.backwards}"
-      >
-        <classpath refid="backwards.compile.classpath"/>
-      </compile>
           
-      <!-- compile branch tests against branch classpath -->	
+      <!-- compile branch tests against previous version JAR file -->	
       <compile-test-macro srcdir="${backwards.dir}/src/test" destdir="${build.dir.backwards}/classes/test"
                   test.classpath="backwards.test.compile.classpath" javac.source="${javac.source.backwards}" javac.target="${javac.target.backwards}"/>
       
