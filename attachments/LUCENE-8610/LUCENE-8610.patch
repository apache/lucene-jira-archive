diff --git a/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java b/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java
index 4cc981dbd7..622fc8fcfd 100644
--- a/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java
+++ b/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java
@@ -800,83 +800,88 @@ final class DefaultIndexingChain extends DocConsumer {
       try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {
         // reset the TokenStream to the first token
         stream.reset();
-        invertState.setAttributeSource(stream);
-        termsHashPerField.start(field, first);
 
-        while (stream.incrementToken()) {
+        if (!stream.incrementToken()) {
+          stream.end();
+        } else {
+          // some streams, e.g. PreAnalyzedTokenizer, lazily instantiate Attributes
+          invertState.setAttributeSource(stream);
+          termsHashPerField.start(field, first);
+          do {
+
+            // If we hit an exception in stream.next below
+            // (which is fairly common, e.g. if analyzer
+            // chokes on a given document), then it's
+            // non-aborting and (above) this one document
+            // will be marked as deleted, but still
+            // consume a docID
+
+            int posIncr = invertState.posIncrAttribute.getPositionIncrement();
+            invertState.position += posIncr;
+            if (invertState.position < invertState.lastPosition) {
+              if (posIncr == 0) {
+                throw new IllegalArgumentException("first position increment must be > 0 (got 0) for field '" + field.name() + "'");
+              } else if (posIncr < 0) {
+                throw new IllegalArgumentException("position increment must be >= 0 (got " + posIncr + ") for field '" + field.name() + "'");
+              } else {
+                throw new IllegalArgumentException("position overflowed Integer.MAX_VALUE (got posIncr=" + posIncr + " lastPosition=" + invertState.lastPosition + " position=" + invertState.position + ") for field '" + field.name() + "'");
+              }
+            } else if (invertState.position > IndexWriter.MAX_POSITION) {
+              throw new IllegalArgumentException("position " + invertState.position + " is too large for field '" + field.name() + "': max allowed position is " + IndexWriter.MAX_POSITION);
+            }
+            invertState.lastPosition = invertState.position;
+            if (posIncr == 0) {
+              invertState.numOverlap++;
+            }
 
-          // If we hit an exception in stream.next below
-          // (which is fairly common, e.g. if analyzer
-          // chokes on a given document), then it's
-          // non-aborting and (above) this one document
-          // will be marked as deleted, but still
-          // consume a docID
+            int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();
+            int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();
+            if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {
+              throw new IllegalArgumentException("startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards "
+                  + "startOffset=" + startOffset + ",endOffset=" + endOffset + ",lastStartOffset=" + invertState.lastStartOffset + " for field '" + field.name() + "'");
+            }
+            invertState.lastStartOffset = startOffset;
 
-          int posIncr = invertState.posIncrAttribute.getPositionIncrement();
-          invertState.position += posIncr;
-          if (invertState.position < invertState.lastPosition) {
-            if (posIncr == 0) {
-              throw new IllegalArgumentException("first position increment must be > 0 (got 0) for field '" + field.name() + "'");
-            } else if (posIncr < 0) {
-              throw new IllegalArgumentException("position increment must be >= 0 (got " + posIncr + ") for field '" + field.name() + "'");
-            } else {
-              throw new IllegalArgumentException("position overflowed Integer.MAX_VALUE (got posIncr=" + posIncr + " lastPosition=" + invertState.lastPosition + " position=" + invertState.position + ") for field '" + field.name() + "'");
+            try {
+              invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());
+            } catch (ArithmeticException ae) {
+              throw new IllegalArgumentException("too many tokens for field \"" + field.name() + "\"");
             }
-          } else if (invertState.position > IndexWriter.MAX_POSITION) {
-            throw new IllegalArgumentException("position " + invertState.position + " is too large for field '" + field.name() + "': max allowed position is " + IndexWriter.MAX_POSITION);
-          }
-          invertState.lastPosition = invertState.position;
-          if (posIncr == 0) {
-            invertState.numOverlap++;
-          }
-              
-          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();
-          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();
-          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {
-            throw new IllegalArgumentException("startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards "
-                                               + "startOffset=" + startOffset + ",endOffset=" + endOffset + ",lastStartOffset=" + invertState.lastStartOffset + " for field '" + field.name() + "'");
-          }
-          invertState.lastStartOffset = startOffset;
 
-          try {
-            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());
-          } catch (ArithmeticException ae) {
-            throw new IllegalArgumentException("too many tokens for field \"" + field.name() + "\"");
-          }
-          
-          //System.out.println("  term=" + invertState.termAttribute);
-
-          // If we hit an exception in here, we abort
-          // all buffered documents since the last
-          // flush, on the likelihood that the
-          // internal state of the terms hash is now
-          // corrupt and should not be flushed to a
-          // new segment:
-          try {
-            termsHashPerField.add();
-          } catch (MaxBytesLengthExceededException e) {
-            byte[] prefix = new byte[30];
-            BytesRef bigTerm = invertState.termAttribute.getBytesRef();
-            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);
-            String msg = "Document contains at least one immense term in field=\"" + fieldInfo.name + "\" (whose UTF8 encoding is longer than the max length " + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + "), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '" + Arrays.toString(prefix) + "...', original message: " + e.getMessage();
-            if (docState.infoStream.isEnabled("IW")) {
-              docState.infoStream.message("IW", "ERROR: " + msg);
+            //System.out.println("  term=" + invertState.termAttribute);
+
+            // If we hit an exception in here, we abort
+            // all buffered documents since the last
+            // flush, on the likelihood that the
+            // internal state of the terms hash is now
+            // corrupt and should not be flushed to a
+            // new segment:
+            try {
+              termsHashPerField.add();
+            } catch (MaxBytesLengthExceededException e) {
+              byte[] prefix = new byte[30];
+              BytesRef bigTerm = invertState.termAttribute.getBytesRef();
+              System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);
+              String msg = "Document contains at least one immense term in field=\"" + fieldInfo.name + "\" (whose UTF8 encoding is longer than the max length " + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + "), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '" + Arrays.toString(prefix) + "...', original message: " + e.getMessage();
+              if (docState.infoStream.isEnabled("IW")) {
+                docState.infoStream.message("IW", "ERROR: " + msg);
+              }
+              // Document will be deleted above:
+              throw new IllegalArgumentException(msg, e);
+            } catch (Throwable th) {
+              docWriter.onAbortingException(th);
+              throw th;
             }
-            // Document will be deleted above:
-            throw new IllegalArgumentException(msg, e);
-          } catch (Throwable th) {
-            docWriter.onAbortingException(th);
-            throw th;
-          }
-        }
+          } while (stream.incrementToken());
 
-        // trigger streams to perform end-of-stream operations
-        stream.end();
+          // trigger streams to perform end-of-stream operations
+          stream.end();
 
-        // TODO: maybe add some safety? then again, it's already checked 
-        // when we come back around to the field...
-        invertState.position += invertState.posIncrAttribute.getPositionIncrement();
-        invertState.offset += invertState.offsetAttribute.endOffset();
+          // TODO: maybe add some safety? then again, it's already checked 
+          // when we come back around to the field...
+          invertState.position += invertState.posIncrAttribute.getPositionIncrement();
+          invertState.offset += invertState.offsetAttribute.endOffset();
+        }
 
         /* if there is an exception coming through, we won't set this to true here:*/
         succeededInProcessingField = true;
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestNullAttributeCaching.java b/lucene/core/src/test/org/apache/lucene/index/TestNullAttributeCaching.java
new file mode 100644
index 0000000000..fdad0b6d27
--- /dev/null
+++ b/lucene/core/src/test/org/apache/lucene/index/TestNullAttributeCaching.java
@@ -0,0 +1,256 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.index;
+
+
+
+import java.io.IOException;
+import java.lang.reflect.UndeclaredThrowableException;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Attribute;
+import org.apache.lucene.util.AttributeFactory;
+import org.apache.lucene.util.AttributeImpl;
+import org.apache.lucene.util.AttributeSource;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+
+public class TestNullAttributeCaching extends LuceneTestCase {
+
+  protected Directory directory;
+
+  public static final String LAZY_FIELD = "lazy_field";
+  public static final String EAGER_FIELD = "eager_field";
+  public static final String FIELD_VALUE = "value";
+
+  public static FieldType FIELD_TYPE = new FieldType(TextField.TYPE_NOT_STORED);
+
+  @Override
+  public void tearDown() throws Exception {
+    directory.close();
+    super.tearDown();
+  }
+
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    directory = newDirectory();
+  }
+
+  private static final boolean NARROW = false;
+
+  @Test
+  public void testNotWork() throws Exception {
+    if (NARROW) return;
+    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,
+        newIndexWriterConfig(new LazyAttributesAnalyzer()).setMergePolicy(newLogMergePolicy()));
+    Document doc;
+    doc = new Document();
+    doc.add(newField(LAZY_FIELD, "", FIELD_TYPE));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newField(LAZY_FIELD, "something", FIELD_TYPE));
+    writer.addDocument(doc);
+    writer.commit();
+    writer.close();
+  }
+
+  @Test
+  public void testLazyOrderedWork() throws Exception {
+    if (NARROW) return;
+    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,
+        newIndexWriterConfig(new LazyAttributesAnalyzer()).setMergePolicy(newLogMergePolicy()));
+    Document doc;
+    doc = new Document();
+    doc.add(newField(LAZY_FIELD, "something", FIELD_TYPE));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newField(LAZY_FIELD, "", FIELD_TYPE));
+    writer.addDocument(doc);
+    writer.commit();
+    writer.close();
+  }
+
+  @Test
+  public void testLazyEagerOrderedWork() throws Exception {
+    if (NARROW) return;
+    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,
+        newIndexWriterConfig(new LazyAttributesAnalyzer()).setMergePolicy(newLogMergePolicy()));
+    Document doc;
+    doc = new Document();
+    doc.add(newField(EAGER_FIELD, "something", FIELD_TYPE));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newField(LAZY_FIELD, "", FIELD_TYPE));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newField(LAZY_FIELD, "something", FIELD_TYPE));
+    writer.addDocument(doc);
+    writer.commit();
+    writer.close();
+  }
+
+  @Test
+  public void testLazyEagerOrderedPerFieldWork() throws Exception {
+    if (NARROW) return;
+    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,
+        newIndexWriterConfig(new LazyAttributesAnalyzer(Analyzer.PER_FIELD_REUSE_STRATEGY)).setMergePolicy(newLogMergePolicy()));
+    Document doc;
+    doc = new Document();
+    doc.add(newField(EAGER_FIELD, "something", FIELD_TYPE));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newField(LAZY_FIELD, "", FIELD_TYPE));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newField(LAZY_FIELD, "something", FIELD_TYPE));
+    writer.addDocument(doc);
+    writer.commit();
+    writer.close();
+  }
+
+  @Test
+  public void testLazyEagerOrderedPerFieldEnsureCachedAtt() throws Exception {
+    if (NARROW) return;
+    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,
+        newIndexWriterConfig(new LazyAttributesAnalyzer(Analyzer.PER_FIELD_REUSE_STRATEGY)).setMergePolicy(newLogMergePolicy()));
+    Document doc;
+    doc = new Document();
+    doc.add(newField(EAGER_FIELD, "something", FIELD_TYPE));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newField(LAZY_FIELD, "something", FIELD_TYPE));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newField(LAZY_FIELD, "something", FIELD_TYPE));
+    writer.addDocument(doc);
+    writer.commit();
+    writer.close();
+  }
+
+  @Test
+  public void testShouldWork() throws Exception {
+    if (NARROW) return;
+    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,
+        newIndexWriterConfig(new LazyAttributesAnalyzer()).setMergePolicy(newLogMergePolicy()));
+    Document doc;
+    doc = new Document();
+    doc.add(newField(EAGER_FIELD, "", FIELD_TYPE));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newField(EAGER_FIELD, "something", FIELD_TYPE));
+    writer.addDocument(doc);
+    writer.commit();
+    writer.close();
+  }
+
+  private static class LazyAttributesAnalyzer extends Analyzer {
+
+    public LazyAttributesAnalyzer() {
+      super();
+    }
+
+    public LazyAttributesAnalyzer(ReuseStrategy reuseStrategy) {
+      super(reuseStrategy);
+    }
+
+    @Override
+    protected TokenStreamComponents createComponents(String fieldName) {
+      switch (fieldName) {
+        case LAZY_FIELD:
+          return new TokenStreamComponents(new LazyAttributesTokenizer());
+        case EAGER_FIELD:
+          return new TokenStreamComponents(new EagerAttributesTokenizer());
+        default:
+          throw new IllegalArgumentException();
+      }
+    }
+
+  }
+
+  private static class LazyAttributesTokenizer extends Tokenizer {
+
+    private boolean exhausted = true;
+
+    public LazyAttributesTokenizer() {
+      super(new CachingAttributeFactory());
+    }
+
+    @Override
+    public void reset() throws IOException {
+      super.reset();
+      exhausted = this.input.read() == -1;
+      removeAllAttributes();
+    }
+
+    @Override
+    public boolean incrementToken() throws IOException {
+      if (exhausted) {
+        return false;
+      } else {
+        addAttribute(CharTermAttribute.class).append(FIELD_VALUE);
+        addAttribute(PositionIncrementAttribute.class).setPositionIncrement(1);
+        exhausted = true;
+        return true;
+      }
+    }
+
+  }
+
+  private static class EagerAttributesTokenizer extends Tokenizer {
+
+    private boolean exhausted = true;
+    private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+    private final PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);
+
+    @Override
+    public void reset() throws IOException {
+      super.reset();
+      exhausted = this.input.read() == -1;
+    }
+
+    @Override
+    public boolean incrementToken() throws IOException {
+      if (exhausted) {
+        return false;
+      } else {
+        termAtt.append(FIELD_VALUE);
+        posIncAtt.setPositionIncrement(1);
+        exhausted = true;
+        return true;
+      }
+    }
+
+  }
+
+  private static class CachingAttributeFactory extends AttributeFactory {
+
+    private final AttributeSource cachedAttributes = new AttributeSource(AttributeFactory.DEFAULT_ATTRIBUTE_FACTORY);
+
+    @Override
+    public AttributeImpl createAttributeInstance(Class<? extends Attribute> attClass) throws UndeclaredThrowableException {
+      return (AttributeImpl) cachedAttributes.addAttribute(attClass);
+    }
+
+  }
+}
