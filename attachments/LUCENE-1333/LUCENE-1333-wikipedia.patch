Index: lucene/contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java
===================================================================
--- lucene/contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java	(revision 682416)
+++ lucene/contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java	(working copy)
@@ -126,14 +126,13 @@
     tcm.put("3.25", "<NUM>");
     tcm.put("3.50", "<NUM>");
     WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test));
-    Token token = new Token();
     int count = 0;
     int numItalics = 0;
     int numBoldItalics = 0;
     int numCategory = 0;
     int numCitation = 0;
-    while ((token = tf.next(token)) != null) {
-      String tokText = token.termText();
+    for (Token token = tf.next(new Token()); token != null; token = tf.next(token)) {
+      String tokText = token.term();
       //System.out.println("Text: " + tokText + " Type: " + token.type());
       assertTrue("token is null and it shouldn't be", token != null);
       String expectedType = (String) tcm.get(tokText);
@@ -169,73 +168,73 @@
     Token token = new Token();
     token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "click", new String(token.termBuffer(), 0, token.termLength()).equals("click") == true);
+    assertTrue(token.term() + " is not equal to " + "click", token.term().equals("click") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "link", new String(token.termBuffer(), 0, token.termLength()).equals("link") == true);
+    assertTrue(token.term() + " is not equal to " + "link", token.term().equals("link") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "here",
-            new String(token.termBuffer(), 0, token.termLength()).equals("here") == true);
+    assertTrue(token.term() + " is not equal to " + "here",
+            token.term().equals("here") == true);
     //The link, and here should be at the same position for phrases to work
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "again",
-            new String(token.termBuffer(), 0, token.termLength()).equals("again") == true);
+    assertTrue(token.term() + " is not equal to " + "again",
+            token.term().equals("again") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
 
     token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "click",
-            new String(token.termBuffer(), 0, token.termLength()).equals("click") == true);
+    assertTrue(token.term() + " is not equal to " + "click",
+            token.term().equals("click") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
 
     token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "http://lucene.apache.org",
-            new String(token.termBuffer(), 0, token.termLength()).equals("http://lucene.apache.org") == true);
+    assertTrue(token.term() + " is not equal to " + "http://lucene.apache.org",
+            token.term().equals("http://lucene.apache.org") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
 
     token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "here",
-            new String(token.termBuffer(), 0, token.termLength()).equals("here") == true);
+    assertTrue(token.term() + " is not equal to " + "here",
+            token.term().equals("here") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 0, token.getPositionIncrement() == 0);
 
     token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "again",
-            new String(token.termBuffer(), 0, token.termLength()).equals("again") == true);
+    assertTrue(token.term() + " is not equal to " + "again",
+            token.term().equals("again") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
 
     token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "a",
-            new String(token.termBuffer(), 0, token.termLength()).equals("a") == true);
+    assertTrue(token.term() + " is not equal to " + "a",
+            token.term().equals("a") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
 
     token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "b",
-            new String(token.termBuffer(), 0, token.termLength()).equals("b") == true);
+    assertTrue(token.term() + " is not equal to " + "b",
+            token.term().equals("b") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
 
     token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "c",
-            new String(token.termBuffer(), 0, token.termLength()).equals("c") == true);
+    assertTrue(token.term() + " is not equal to " + "c",
+            token.term().equals("c") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
 
     token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "d",
-            new String(token.termBuffer(), 0, token.termLength()).equals("d") == true);
+    assertTrue(token.term() + " is not equal to " + "d",
+            token.term().equals("d") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is not null and it should be", token == null);
   }
 
@@ -245,25 +244,25 @@
     Token token = new Token();
     token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "http://lucene.apache.org/java/docs/index.html#news",
-            new String(token.termBuffer(), 0, token.termLength()).equals("http://lucene.apache.org/java/docs/index.html#news") == true);
+    assertTrue(token.term() + " is not equal to " + "http://lucene.apache.org/java/docs/index.html#news",
+            token.term().equals("http://lucene.apache.org/java/docs/index.html#news") == true);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.EXTERNAL_LINK_URL, token.type().equals(WikipediaTokenizer.EXTERNAL_LINK_URL) == true);
     tf.next(token);//skip here
     token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "http://lucene.apache.org/java/docs/index.html?b=c",
-            new String(token.termBuffer(), 0, token.termLength()).equals("http://lucene.apache.org/java/docs/index.html?b=c") == true);
+    assertTrue(token.term() + " is not equal to " + "http://lucene.apache.org/java/docs/index.html?b=c",
+            token.term().equals("http://lucene.apache.org/java/docs/index.html?b=c") == true);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.EXTERNAL_LINK_URL, token.type().equals(WikipediaTokenizer.EXTERNAL_LINK_URL) == true);
     tf.next(token);//skip here
     token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "https://lucene.apache.org/java/docs/index.html?b=c",
-            new String(token.termBuffer(), 0, token.termLength()).equals("https://lucene.apache.org/java/docs/index.html?b=c") == true);
+    assertTrue(token.term() + " is not equal to " + "https://lucene.apache.org/java/docs/index.html?b=c",
+            token.term().equals("https://lucene.apache.org/java/docs/index.html?b=c") == true);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.EXTERNAL_LINK_URL, token.type().equals(WikipediaTokenizer.EXTERNAL_LINK_URL) == true);
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is not null and it should be", token == null);
 
   }
@@ -277,71 +276,71 @@
     checkLinkPhrases(tf);
     String test = "[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]";
     tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.UNTOKENIZED_ONLY, untoks);
-    Token token;
-    token = tf.next();
+    Token token = new Token();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "a b c d",
-            new String(token.termBuffer(), 0, token.termLength()).equals("a b c d") == true);
+    assertTrue(token.term() + " is not equal to " + "a b c d",
+            token.term().equals("a b c d") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     assertTrue(token.startOffset() + " does not equal: " + 11, token.startOffset() == 11);
     assertTrue(token.endOffset() + " does not equal: " + 18, token.endOffset() == 18);
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "e f g",
-            new String(token.termBuffer(), 0, token.termLength()).equals("e f g") == true);
+    assertTrue(token.term() + " is not equal to " + "e f g",
+            token.term().equals("e f g") == true);
     assertTrue(token.startOffset() + " does not equal: " + 32, token.startOffset() == 32);
     assertTrue(token.endOffset() + " does not equal: " + 37, token.endOffset() == 37);
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "link",
-            new String(token.termBuffer(), 0, token.termLength()).equals("link") == true);
+    assertTrue(token.term() + " is not equal to " + "link",
+            token.term().equals("link") == true);
     assertTrue(token.startOffset() + " does not equal: " + 42, token.startOffset() == 42);
     assertTrue(token.endOffset() + " does not equal: " + 46, token.endOffset() == 46);
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "here",
-            new String(token.termBuffer(), 0, token.termLength()).equals("here") == true);
+    assertTrue(token.term() + " is not equal to " + "here",
+            token.term().equals("here") == true);
     assertTrue(token.startOffset() + " does not equal: " + 47, token.startOffset() == 47);
     assertTrue(token.endOffset() + " does not equal: " + 51, token.endOffset() == 51);
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "link",
-            new String(token.termBuffer(), 0, token.termLength()).equals("link") == true);
+    assertTrue(token.term() + " is not equal to " + "link",
+            token.term().equals("link") == true);
     assertTrue(token.startOffset() + " does not equal: " + 56, token.startOffset() == 56);
     assertTrue(token.endOffset() + " does not equal: " + 60, token.endOffset() == 60);
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "there",
-            new String(token.termBuffer(), 0, token.termLength()).equals("there") == true);
+    assertTrue(token.term() + " is not equal to " + "there",
+            token.term().equals("there") == true);
     assertTrue(token.startOffset() + " does not equal: " + 61, token.startOffset() == 61);
     assertTrue(token.endOffset() + " does not equal: " + 66, token.endOffset() == 66);
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "italics here",
-            new String(token.termBuffer(), 0, token.termLength()).equals("italics here") == true);
+    assertTrue(token.term() + " is not equal to " + "italics here",
+            token.term().equals("italics here") == true);
     assertTrue(token.startOffset() + " does not equal: " + 71, token.startOffset() == 71);
     assertTrue(token.endOffset() + " does not equal: " + 83, token.endOffset() == 83);
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "something",
-            new String(token.termBuffer(), 0, token.termLength()).equals("something") == true);
+    assertTrue(token.term() + " is not equal to " + "something",
+            token.term().equals("something") == true);
     assertTrue(token.startOffset() + " does not equal: " + 86, token.startOffset() == 86);
     assertTrue(token.endOffset() + " does not equal: " + 95, token.endOffset() == 95);
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "more italics",
-            new String(token.termBuffer(), 0, token.termLength()).equals("more italics") == true);
+    assertTrue(token.term() + " is not equal to " + "more italics",
+            token.term().equals("more italics") == true);
     assertTrue(token.startOffset() + " does not equal: " + 98, token.startOffset() == 98);
     assertTrue(token.endOffset() + " does not equal: " + 110, token.endOffset() == 110);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "h   i   j",
-            new String(token.termBuffer(), 0, token.termLength()).equals("h   i   j") == true);
+    assertTrue(token.term() + " is not equal to " + "h   i   j",
+            token.term().equals("h   i   j") == true);
     assertTrue(token.startOffset() + " does not equal: " + 124, token.startOffset() == 124);
     assertTrue(token.endOffset() + " does not equal: " + 133, token.endOffset() == 133);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is not null and it should be", token == null);
   }
 
@@ -352,48 +351,48 @@
     String test = "[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]";
     //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens
     WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);
-    Token token;
-    token = tf.next();
+    Token token = new Token();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "a b c d",
-            new String(token.termBuffer(), 0, token.termLength()).equals("a b c d") == true);
+    assertTrue(token.term() + " is not equal to " + "a b c d",
+            token.term().equals("a b c d") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
     assertTrue(token.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
     assertTrue(token.startOffset() + " does not equal: " + 11, token.startOffset() == 11);
     assertTrue(token.endOffset() + " does not equal: " + 18, token.endOffset() == 18);
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "a",
-            new String(token.termBuffer(), 0, token.termLength()).equals("a") == true);
+    assertTrue(token.term() + " is not equal to " + "a",
+            token.term().equals("a") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 0, token.getPositionIncrement() == 0);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
     assertTrue(token.getFlags() + " equals: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG + " and it shouldn't", token.getFlags() != WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
     assertTrue(token.startOffset() + " does not equal: " + 11, token.startOffset() == 11);
     assertTrue(token.endOffset() + " does not equal: " + 12, token.endOffset() == 12);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "b",
-            new String(token.termBuffer(), 0, token.termLength()).equals("b") == true);
+    assertTrue(token.term() + " is not equal to " + "b",
+            token.term().equals("b") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
     assertTrue(token.startOffset() + " does not equal: " + 13, token.startOffset() == 13);
     assertTrue(token.endOffset() + " does not equal: " + 14, token.endOffset() == 14);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "c",
-            new String(token.termBuffer(), 0, token.termLength()).equals("c") == true);
+    assertTrue(token.term() + " is not equal to " + "c",
+            token.term().equals("c") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
     assertTrue(token.startOffset() + " does not equal: " + 15, token.startOffset() == 15);
     assertTrue(token.endOffset() + " does not equal: " + 16, token.endOffset() == 16);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "d",
-            new String(token.termBuffer(), 0, token.termLength()).equals("d") == true);
+    assertTrue(token.term() + " is not equal to " + "d",
+            token.term().equals("d") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
     assertTrue(token.startOffset() + " does not equal: " + 17, token.startOffset() == 17);
@@ -401,175 +400,175 @@
 
 
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "e f g",
-            new String(token.termBuffer(), 0, token.termLength()).equals("e f g") == true);
+    assertTrue(token.term() + " is not equal to " + "e f g",
+            token.term().equals("e f g") == true);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
     assertTrue(token.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
     assertTrue(token.startOffset() + " does not equal: " + 32, token.startOffset() == 32);
     assertTrue(token.endOffset() + " does not equal: " + 37, token.endOffset() == 37);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "e",
-            new String(token.termBuffer(), 0, token.termLength()).equals("e") == true);
+    assertTrue(token.term() + " is not equal to " + "e",
+            token.term().equals("e") == true);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 0, token.getPositionIncrement() == 0);
     assertTrue(token.startOffset() + " does not equal: " + 32, token.startOffset() == 32);
     assertTrue(token.endOffset() + " does not equal: " + 33, token.endOffset() == 33);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "f",
-            new String(token.termBuffer(), 0, token.termLength()).equals("f") == true);
+    assertTrue(token.term() + " is not equal to " + "f",
+            token.term().equals("f") == true);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     assertTrue(token.startOffset() + " does not equal: " + 34, token.startOffset() == 34);
     assertTrue(token.endOffset() + " does not equal: " + 35, token.endOffset() == 35);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "g",
-            new String(token.termBuffer(), 0, token.termLength()).equals("g") == true);
+    assertTrue(token.term() + " is not equal to " + "g",
+            token.term().equals("g") == true);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     assertTrue(token.startOffset() + " does not equal: " + 36, token.startOffset() == 36);
     assertTrue(token.endOffset() + " does not equal: " + 37, token.endOffset() == 37);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "link",
-            new String(token.termBuffer(), 0, token.termLength()).equals("link") == true);
+    assertTrue(token.term() + " is not equal to " + "link",
+            token.term().equals("link") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
     assertTrue(token.startOffset() + " does not equal: " + 42, token.startOffset() == 42);
     assertTrue(token.endOffset() + " does not equal: " + 46, token.endOffset() == 46);
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "here",
-            new String(token.termBuffer(), 0, token.termLength()).equals("here") == true);
+    assertTrue(token.term() + " is not equal to " + "here",
+            token.term().equals("here") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
     assertTrue(token.startOffset() + " does not equal: " + 47, token.startOffset() == 47);
     assertTrue(token.endOffset() + " does not equal: " + 51, token.endOffset() == 51);
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "link",
-            new String(token.termBuffer(), 0, token.termLength()).equals("link") == true);
+    assertTrue(token.term() + " is not equal to " + "link",
+            token.term().equals("link") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     assertTrue(token.startOffset() + " does not equal: " + 56, token.startOffset() == 56);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
     assertTrue(token.endOffset() + " does not equal: " + 60, token.endOffset() == 60);
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "there",
-            new String(token.termBuffer(), 0, token.termLength()).equals("there") == true);
+    assertTrue(token.term() + " is not equal to " + "there",
+            token.term().equals("there") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
     assertTrue(token.startOffset() + " does not equal: " + 61, token.startOffset() == 61);
     assertTrue(token.endOffset() + " does not equal: " + 66, token.endOffset() == 66);
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "italics here",
-            new String(token.termBuffer(), 0, token.termLength()).equals("italics here") == true);
+    assertTrue(token.term() + " is not equal to " + "italics here",
+            token.term().equals("italics here") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);
     assertTrue(token.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
     assertTrue(token.startOffset() + " does not equal: " + 71, token.startOffset() == 71);
     assertTrue(token.endOffset() + " does not equal: " + 83, token.endOffset() == 83);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "italics",
-            new String(token.termBuffer(), 0, token.termLength()).equals("italics") == true);
+    assertTrue(token.term() + " is not equal to " + "italics",
+            token.term().equals("italics") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 0, token.getPositionIncrement() == 0);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);
     assertTrue(token.startOffset() + " does not equal: " + 71, token.startOffset() == 71);
     assertTrue(token.endOffset() + " does not equal: " + 78, token.endOffset() == 78);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "here",
-            new String(token.termBuffer(), 0, token.termLength()).equals("here") == true);
+    assertTrue(token.term() + " is not equal to " + "here",
+            token.term().equals("here") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);
     assertTrue(token.startOffset() + " does not equal: " + 79, token.startOffset() == 79);
     assertTrue(token.endOffset() + " does not equal: " + 83, token.endOffset() == 83);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "something",
-            new String(token.termBuffer(), 0, token.termLength()).equals("something") == true);
+    assertTrue(token.term() + " is not equal to " + "something",
+            token.term().equals("something") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     assertTrue(token.startOffset() + " does not equal: " + 86, token.startOffset() == 86);
     assertTrue(token.endOffset() + " does not equal: " + 95, token.endOffset() == 95);
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "more italics",
-            new String(token.termBuffer(), 0, token.termLength()).equals("more italics") == true);
+    assertTrue(token.term() + " is not equal to " + "more italics",
+            token.term().equals("more italics") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);
     assertTrue(token.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
     assertTrue(token.startOffset() + " does not equal: " + 98, token.startOffset() == 98);
     assertTrue(token.endOffset() + " does not equal: " + 110, token.endOffset() == 110);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "more",
-            new String(token.termBuffer(), 0, token.termLength()).equals("more") == true);
+    assertTrue(token.term() + " is not equal to " + "more",
+            token.term().equals("more") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 0, token.getPositionIncrement() == 0);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);
     assertTrue(token.startOffset() + " does not equal: " + 98, token.startOffset() == 98);
     assertTrue(token.endOffset() + " does not equal: " + 102, token.endOffset() == 102);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "italics",
-            new String(token.termBuffer(), 0, token.termLength()).equals("italics") == true);
+    assertTrue(token.term() + " is not equal to " + "italics",
+            token.term().equals("italics") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
         assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);
 
     assertTrue(token.startOffset() + " does not equal: " + 103, token.startOffset() == 103);
     assertTrue(token.endOffset() + " does not equal: " + 110, token.endOffset() == 110);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "h   i   j",
-            new String(token.termBuffer(), 0, token.termLength()).equals("h   i   j") == true);
+    assertTrue(token.term() + " is not equal to " + "h   i   j",
+            token.term().equals("h   i   j") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
     assertTrue(token.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
     assertTrue(token.startOffset() + " does not equal: " + 124, token.startOffset() == 124);
     assertTrue(token.endOffset() + " does not equal: " + 133, token.endOffset() == 133);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "h",
-            new String(token.termBuffer(), 0, token.termLength()).equals("h") == true);
+    assertTrue(token.term() + " is not equal to " + "h",
+            token.term().equals("h") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 0, token.getPositionIncrement() == 0);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
     assertTrue(token.startOffset() + " does not equal: " + 124, token.startOffset() == 124);
     assertTrue(token.endOffset() + " does not equal: " + 125, token.endOffset() == 125);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "i",
-            new String(token.termBuffer(), 0, token.termLength()).equals("i") == true);
+    assertTrue(token.term() + " is not equal to " + "i",
+            token.term().equals("i") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
     assertTrue(token.startOffset() + " does not equal: " + 128, token.startOffset() == 128);
     assertTrue(token.endOffset() + " does not equal: " + 129, token.endOffset() == 129);
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "j",
-            new String(token.termBuffer(), 0, token.termLength()).equals("j") == true);
+    assertTrue(token.term() + " is not equal to " + "j",
+            token.term().equals("j") == true);
     assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
     assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
     assertTrue(token.startOffset() + " does not equal: " + 132, token.startOffset() == 132);
     assertTrue(token.endOffset() + " does not equal: " + 133, token.endOffset() == 133);
 
-    token = tf.next();
+    token = tf.next(token);
     assertTrue("token is not null and it should be", token == null);
 
   }
