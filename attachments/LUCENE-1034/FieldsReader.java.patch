*** lucene-2.2.0/src/java/org/apache/lucene/index/FieldsReader.java	2007-06-16 22:20:35.000000000 -0700
--- src/org/apache/lucene/index/FieldsReader.java	2007-10-25 14:52:23.000000000 -0700
***************
*** 142,151 ****
--- 142,210 ----
        }
      }
  
      return doc;
    }
+   
+   public Document[] docs(int[] n, FieldSelector fieldSelector) throws CorruptIndexException, IOException {
+ 	  
+ 	  long[] positions = new long[n.length];
+ 	  Document[] documents = new Document[n.length];
+ 	  
+ 	  for (int x = 0; x < n.length; x++) {
+ 		  indexStream.seek(n[x] * 8L);
+ 		  positions[x] = indexStream.readLong();
+ 	  }
+ 
+ 	  for (int x = 0; x < n.length; x++) {
+ 		  fieldsStream.seek(positions[x]);
+ 
+ 		  Document doc = new Document();
+ 		  int numFields = fieldsStream.readVInt();
+ 		  for (int i = 0; i < numFields; i++) {
+ 		      int fieldNumber = fieldsStream.readVInt();
+ 		      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);
+ 		      FieldSelectorResult acceptField = fieldSelector == null ? FieldSelectorResult.LOAD : fieldSelector.accept(fi.name);
+ 		      
+ 		      byte bits = fieldsStream.readByte();
+ 		      boolean compressed = (bits & FieldsWriter.FIELD_IS_COMPRESSED) != 0;
+ 		      boolean tokenize = (bits & FieldsWriter.FIELD_IS_TOKENIZED) != 0;
+ 		      boolean binary = (bits & FieldsWriter.FIELD_IS_BINARY) != 0;
+ 		      //TODO: Find an alternative approach here if this list continues to grow beyond the
+ 		      //list of 5 or 6 currently here.  See Lucene 762 for discussion
+ 		      if (acceptField.equals(FieldSelectorResult.LOAD)) {
+ 		        addField(doc, fi, binary, compressed, tokenize);
+ 		      }
+ 		      else if (acceptField.equals(FieldSelectorResult.LOAD_FOR_MERGE)) {
+ 		        addFieldForMerge(doc, fi, binary, compressed, tokenize);
+ 		      }
+ 		      else if (acceptField.equals(FieldSelectorResult.LOAD_AND_BREAK)){
+ 		        addField(doc, fi, binary, compressed, tokenize);
+ 		        break;//Get out of this loop
+ 		      }
+ 		      else if (acceptField.equals(FieldSelectorResult.LAZY_LOAD)) {
+ 		        addFieldLazy(doc, fi, binary, compressed, tokenize);
+ 		      }
+ 		      else if (acceptField.equals(FieldSelectorResult.SIZE)){
+ 		        skipField(binary, compressed, addFieldSize(doc, fi, binary, compressed));
+ 		      }
+ 		      else if (acceptField.equals(FieldSelectorResult.SIZE_AND_BREAK)){
+ 		        addFieldSize(doc, fi, binary, compressed);
+ 		        break;
+ 		      }
+ 		      else {
+ 		        skipField(binary, compressed);
+ 		      }  
+ 		  }
+ 
+ 		  documents[x] = doc;
+ 
+ 	  }
+ 
+ 	  return documents;
+ 	  
+   }
  
    /**
     * Skip the field.  We still have to read some of the information about the field, but can skip past the actual content.
     * This will have the most payoff on large fields.
     */
