diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordTokenizerFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordTokenizerFactory.java
index 3654f67bea..46995d65de 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordTokenizerFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordTokenizerFactory.java
@@ -16,7 +16,6 @@
  */
 package org.apache.lucene.analysis.core;
 
-
 import org.apache.lucene.analysis.util.TokenizerFactory;
 import org.apache.lucene.util.AttributeFactory;
 
@@ -27,15 +26,26 @@ import java.util.Map;
  * <pre class="prettyprint">
  * &lt;fieldType name="text_keyword" class="solr.TextField" positionIncrementGap="100"&gt;
  *   &lt;analyzer&gt;
- *     &lt;tokenizer class="solr.KeywordTokenizerFactory"/&gt;
+ *     &lt;tokenizer class="solr.KeywordTokenizerFactory" maxTokenLen="256"/&gt;
  *   &lt;/analyzer&gt;
  * &lt;/fieldType&gt;</pre> 
+ *
+ * Options:
+ * <ul>
+ *   <li>maxTokenLen: max token length, should be greater than 0. It is rare to need to change this
+ *      else {@link KeywordTokenizer}::DEFAULT_BUFFER_SIZE</li>
+ * </ul>
  */
 public class KeywordTokenizerFactory extends TokenizerFactory {
+  private final Integer maxTokenLen;
   
   /** Creates a new KeywordTokenizerFactory */
   public KeywordTokenizerFactory(Map<String,String> args) {
     super(args);
+    maxTokenLen = getInt(args, "maxTokenLen", KeywordTokenizer.DEFAULT_BUFFER_SIZE);
+    if (maxTokenLen <= 0) {
+      throw new IllegalArgumentException("maxTokenLen cannot be equal to or less than 0, passed: " + maxTokenLen);
+    }
     if (!args.isEmpty()) {
       throw new IllegalArgumentException("Unknown parameters: " + args);
     }
@@ -43,6 +53,6 @@ public class KeywordTokenizerFactory extends TokenizerFactory {
   
   @Override
   public KeywordTokenizer create(AttributeFactory factory) {
-    return new KeywordTokenizer(factory, KeywordTokenizer.DEFAULT_BUFFER_SIZE);
+    return new KeywordTokenizer(factory, maxTokenLen);
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizer.java
index df41b3777c..40d9914624 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizer.java
@@ -50,6 +50,17 @@ public class LetterTokenizer extends CharTokenizer {
     super(factory);
   }
   
+  /**
+   * Construct a new LetterTokenizer using a given
+   * {@link org.apache.lucene.util.AttributeFactory}.
+   *
+   * @param factory the attribute factory to use for this {@link Tokenizer}
+   * @param maxTokenLen max token length the tokenizer will emit
+   */
+  public LetterTokenizer(AttributeFactory factory, Integer maxTokenLen) {
+    super(factory, maxTokenLen);
+  }
+
   /** Collects only characters which satisfy
    * {@link Character#isLetter(int)}.*/
   @Override
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizerFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizerFactory.java
index 828d6cf3fe..15aeac401d 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizerFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizerFactory.java
@@ -17,6 +17,7 @@
 package org.apache.lucene.analysis.core;
 
 
+import org.apache.lucene.analysis.util.CharTokenizer;
 import org.apache.lucene.analysis.util.TokenizerFactory;
 import org.apache.lucene.util.AttributeFactory;
 
@@ -27,15 +28,26 @@ import java.util.Map;
  * <pre class="prettyprint">
  * &lt;fieldType name="text_letter" class="solr.TextField" positionIncrementGap="100"&gt;
  *   &lt;analyzer&gt;
- *     &lt;tokenizer class="solr.LetterTokenizerFactory"/&gt;
+ *     &lt;tokenizer class="solr.LetterTokenizerFactory" maxTokenLen="256"/&gt;
  *   &lt;/analyzer&gt;
  * &lt;/fieldType&gt;</pre>
+ *
+ * Options:
+ * <ul>
+ *   <li>maxTokenLen: max token length, must be greater than 0. It is rare to need to change this
+ *      else {@link CharTokenizer}::DEFAULT_MAX_TOKEN_LEN</li>
+ * </ul>
  */
 public class LetterTokenizerFactory extends TokenizerFactory {
+  private final Integer maxTokenLen;
 
   /** Creates a new LetterTokenizerFactory */
   public LetterTokenizerFactory(Map<String,String> args) {
     super(args);
+    maxTokenLen = getInt(args, "maxTokenLen", CharTokenizer.DEFAULT_MAX_WORD_LEN);
+    if (maxTokenLen <= 0) {
+      throw new IllegalArgumentException("maxTokenLen cannot be equal to or less than 0, passed: " + maxTokenLen);
+    }
     if (!args.isEmpty()) {
       throw new IllegalArgumentException("Unknown parameters: " + args);
     }
@@ -43,6 +55,6 @@ public class LetterTokenizerFactory extends TokenizerFactory {
 
   @Override
   public LetterTokenizer create(AttributeFactory factory) {
-    return new LetterTokenizer(factory);
+    return new LetterTokenizer(factory, maxTokenLen);
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizer.java
index 982d356533..2231efb04d 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizer.java
@@ -50,6 +50,17 @@ public final class LowerCaseTokenizer extends LetterTokenizer {
     super(factory);
   }
   
+  /**
+   * Construct a new LowerCaseTokenizer using a given
+   * {@link org.apache.lucene.util.AttributeFactory}.
+   *
+   * @param factory the attribute factory to use for this {@link Tokenizer}
+   * @param maxTokenLen max token length the tokenizer will emit
+   */
+  public LowerCaseTokenizer(AttributeFactory factory, Integer maxTokenLen) {
+    super(factory, maxTokenLen);
+  }
+  
   /** Converts char to lower case
    * {@link Character#toLowerCase(int)}.*/
   @Override
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizerFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizerFactory.java
index 3e29161a92..42aa5069ec 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizerFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizerFactory.java
@@ -18,6 +18,7 @@ package org.apache.lucene.analysis.core;
 
 
 import org.apache.lucene.analysis.util.AbstractAnalysisFactory;
+import org.apache.lucene.analysis.util.CharTokenizer;
 import org.apache.lucene.analysis.util.MultiTermAwareComponent;
 import org.apache.lucene.analysis.util.TokenizerFactory;
 import org.apache.lucene.util.AttributeFactory;
@@ -26,19 +27,32 @@ import java.util.HashMap;
 import java.util.Map;
 
 /**
- * Factory for {@link LowerCaseTokenizer}. 
+ * Factory for {@link LowerCaseTokenizer}.
  * <pre class="prettyprint">
  * &lt;fieldType name="text_lwrcase" class="solr.TextField" positionIncrementGap="100"&gt;
- *   &lt;analyzer&gt;
- *     &lt;tokenizer class="solr.LowerCaseTokenizerFactory"/&gt;
- *   &lt;/analyzer&gt;
+ * &lt;analyzer&gt;
+ * &lt;tokenizer class="solr.LowerCaseTokenizerFactory" maxTokenLen="256"/&gt;
+ * &lt;/analyzer&gt;
  * &lt;/fieldType&gt;</pre>
+ * <p>
+ * Options:
+ * <ul>
+ * <li>maxTokenLen: max token length, should be greater than 0. It is rare to need to change this
+ * else {@link CharTokenizer}::DEFAULT_MAX_WORD_LEN</li>
+ * </ul>
  */
 public class LowerCaseTokenizerFactory extends TokenizerFactory implements MultiTermAwareComponent {
-  
-  /** Creates a new LowerCaseTokenizerFactory */
-  public LowerCaseTokenizerFactory(Map<String,String> args) {
+  private final Integer maxTokenLen;
+
+  /**
+   * Creates a new LowerCaseTokenizerFactory
+   */
+  public LowerCaseTokenizerFactory(Map<String, String> args) {
     super(args);
+    maxTokenLen = getInt(args, "maxTokenLen", CharTokenizer.DEFAULT_MAX_WORD_LEN);
+    if (maxTokenLen <= 0) {
+      throw new IllegalArgumentException("maxTokenLen cannot be equal to or less than 0, passed: " + maxTokenLen);
+    }
     if (!args.isEmpty()) {
       throw new IllegalArgumentException("Unknown parameters: " + args);
     }
@@ -46,11 +60,13 @@ public class LowerCaseTokenizerFactory extends TokenizerFactory implements Multi
 
   @Override
   public LowerCaseTokenizer create(AttributeFactory factory) {
-    return new LowerCaseTokenizer(factory);
+    return new LowerCaseTokenizer(factory, maxTokenLen);
   }
 
   @Override
   public AbstractAnalysisFactory getMultiTermComponent() {
-    return new LowerCaseFilterFactory(new HashMap<>(getOriginalArgs()));
+    Map map = new HashMap<>(getOriginalArgs());
+    map.remove("maxTokenLen"); //removing "maxTokenLen" argument for LowerCaseFilterFactory init
+    return new LowerCaseFilterFactory(map);
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/UnicodeWhitespaceTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/UnicodeWhitespaceTokenizer.java
index 5e4313f6c5..7e9bb3b7c8 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/UnicodeWhitespaceTokenizer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/UnicodeWhitespaceTokenizer.java
@@ -47,6 +47,17 @@ public final class UnicodeWhitespaceTokenizer extends CharTokenizer {
   public UnicodeWhitespaceTokenizer(AttributeFactory factory) {
     super(factory);
   }
+
+  /**
+   * Construct a new UnicodeWhitespaceTokenizer using a given
+   * {@link org.apache.lucene.util.AttributeFactory}.
+   *
+   * @param factory the attribute factory to use for this {@link Tokenizer}
+   * @param maxTokenLen max token length the tokenizer will emit
+   */
+  public UnicodeWhitespaceTokenizer(AttributeFactory factory, Integer maxTokenLen) {
+    super(factory, maxTokenLen);
+  }
   
   /** Collects only characters which do not satisfy Unicode's WHITESPACE property. */
   @Override
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizer.java
index 70f2d620bb..caa27c2018 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizer.java
@@ -46,6 +46,17 @@ public final class WhitespaceTokenizer extends CharTokenizer {
   public WhitespaceTokenizer(AttributeFactory factory) {
     super(factory);
   }
+
+  /**
+   * Construct a new WhitespaceTokenizer using a given
+   * {@link org.apache.lucene.util.AttributeFactory}.
+   *
+   * @param factory the attribute factory to use for this {@link Tokenizer}
+   * @param maxTokenLen max token length the tokenizer will emit
+   */
+  public WhitespaceTokenizer(AttributeFactory factory, Integer maxTokenLen) {
+    super(factory, maxTokenLen);
+  }
   
   /** Collects only characters which do not satisfy
    * {@link Character#isWhitespace(int)}.*/
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizerFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizerFactory.java
index fd38b632ad..4eada4985b 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizerFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizerFactory.java
@@ -22,6 +22,7 @@ import java.util.Collection;
 import java.util.Map;
 
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.util.CharTokenizer;
 import org.apache.lucene.analysis.util.TokenizerFactory;
 import org.apache.lucene.util.AttributeFactory;
 
@@ -30,7 +31,7 @@ import org.apache.lucene.util.AttributeFactory;
  * <pre class="prettyprint">
  * &lt;fieldType name="text_ws" class="solr.TextField" positionIncrementGap="100"&gt;
  *   &lt;analyzer&gt;
- *     &lt;tokenizer class="solr.WhitespaceTokenizerFactory" rule="unicode"/&gt;
+ *     &lt;tokenizer class="solr.WhitespaceTokenizerFactory" rule="unicode"  maxTokenLen="256"/&gt;
  *   &lt;/analyzer&gt;
  * &lt;/fieldType&gt;</pre>
  *
@@ -38,6 +39,8 @@ import org.apache.lucene.util.AttributeFactory;
  * <ul>
  *   <li>rule: either "java" for {@link WhitespaceTokenizer}
  *      or "unicode" for {@link UnicodeWhitespaceTokenizer}</li>
+ *   <li>maxTokenLen: max token length, should be greater than 0. It is rare to need to change this
+ *      else {@link CharTokenizer}::DEFAULT_MAX_TOKEN_LEN</li>
  * </ul>
  */
 public class WhitespaceTokenizerFactory extends TokenizerFactory {
@@ -46,13 +49,17 @@ public class WhitespaceTokenizerFactory extends TokenizerFactory {
   private static final Collection<String> RULE_NAMES = Arrays.asList(RULE_JAVA, RULE_UNICODE);
 
   private final String rule;
+  private final Integer maxTokenLen;
 
   /** Creates a new WhitespaceTokenizerFactory */
   public WhitespaceTokenizerFactory(Map<String,String> args) {
     super(args);
 
     rule = get(args, "rule", RULE_NAMES, RULE_JAVA);
-
+    maxTokenLen = getInt(args, "maxTokenLen", CharTokenizer.DEFAULT_MAX_WORD_LEN);
+    if (maxTokenLen <= 0) {
+      throw new IllegalArgumentException("maxTokenLen cannot be equal to or less than 0, passed: " + maxTokenLen);
+    }
     if (!args.isEmpty()) {
       throw new IllegalArgumentException("Unknown parameters: " + args);
     }
@@ -62,9 +69,9 @@ public class WhitespaceTokenizerFactory extends TokenizerFactory {
   public Tokenizer create(AttributeFactory factory) {
     switch (rule) {
       case RULE_JAVA:
-        return new WhitespaceTokenizer(factory);
+        return new WhitespaceTokenizer(factory, maxTokenLen);
       case RULE_UNICODE:
-        return new UnicodeWhitespaceTokenizer(factory);
+        return new UnicodeWhitespaceTokenizer(factory, maxTokenLen);
       default:
         throw new AssertionError();
     }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java
index 13289bee1b..29dd83b347 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java
@@ -50,6 +50,7 @@ public abstract class CharTokenizer extends Tokenizer {
    * Creates a new {@link CharTokenizer} instance
    */
   public CharTokenizer() {
+    this.maxTokenLen = DEFAULT_MAX_WORD_LEN;
   }
   
   /**
@@ -60,6 +61,21 @@ public abstract class CharTokenizer extends Tokenizer {
    */
   public CharTokenizer(AttributeFactory factory) {
     super(factory);
+    this.maxTokenLen = DEFAULT_MAX_WORD_LEN;
+  }
+  
+  /**
+   * Creates a new {@link CharTokenizer} instance
+   *
+   * @param factory the attribute factory to use for this {@link Tokenizer}
+   * @param maxTokenLen max token length the tokenizer will emit
+   */
+  public CharTokenizer(AttributeFactory factory, Integer maxTokenLen) {
+    super(factory);
+    if (maxTokenLen ==null || maxTokenLen <= 0) {
+      throw new IllegalArgumentException("maxTokenLen cannot be equal to or less than 0, passed: " + maxTokenLen);
+    }
+    this.maxTokenLen = maxTokenLen;
   }
   
   /**
@@ -193,9 +209,10 @@ public abstract class CharTokenizer extends Tokenizer {
   }
   
   private int offset = 0, bufferIndex = 0, dataLen = 0, finalOffset = 0;
-  private static final int MAX_WORD_LEN = 255;
+  public static final int DEFAULT_MAX_WORD_LEN = 255;
   private static final int IO_BUFFER_SIZE = 4096;
-  
+  private final int maxTokenLen;
+
   private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
   private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
   
@@ -256,7 +273,7 @@ public abstract class CharTokenizer extends Tokenizer {
         }
         end += charCount;
         length += Character.toChars(normalize(c), buffer, length); // buffer it, normalized
-        if (length >= MAX_WORD_LEN) { // buffer overflow! make sure to check for >= surrogate pair could break == test
+        if (length >= maxTokenLen) { // buffer overflow! make sure to check for >= surrogate pair could break == test
           break;
         }
       } else if (length > 0) {           // at non-Letter w/ chars
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordTokenizer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordTokenizer.java
new file mode 100644
index 0000000000..434fe2c3ab
--- /dev/null
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordTokenizer.java
@@ -0,0 +1,107 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.analysis.core;
+
+import java.io.IOException;
+import java.io.StringReader;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.util.AttributeFactory;
+
+public class TestKeywordTokenizer extends BaseTokenStreamTestCase {
+
+  public void testSimple() throws IOException {
+    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
+    KeywordTokenizer tokenizer = new KeywordTokenizer();
+    tokenizer.setReader(reader);
+    assertTokenStreamContents(tokenizer, new String[]{"Tokenizer \ud801\udc1ctest"});
+  }
+
+  public void testFactory() {
+    Map<String, String> args = new HashMap<>();
+    KeywordTokenizerFactory factory = new KeywordTokenizerFactory(args);
+    AttributeFactory attributeFactory = newAttributeFactory();
+    Tokenizer tokenizer = factory.create(attributeFactory);
+    assertEquals(KeywordTokenizer.class, tokenizer.getClass());
+  }
+
+  public void testParamsFactory() {
+    Map<String, String> args = new HashMap<>();
+    KeywordTokenizerFactory factory;
+
+    // negative maxTokenLen
+    args = new HashMap<>();
+    args.put("maxTokenLen", "-1");
+    try {
+      factory = new KeywordTokenizerFactory(args);
+    } catch (Exception e) {
+      assertEquals("maxTokenLen cannot be equal to or less than 0, passed: -1", e.getMessage());
+    }
+
+    // zero maxTokenLen
+    args = new HashMap<>();
+    args.put("maxTokenLen", "0");
+    try {
+      factory = new KeywordTokenizerFactory(args);
+    } catch (Exception e) {
+      assertEquals("maxTokenLen cannot be equal to or less than 0, passed: 0", e.getMessage());
+    }
+
+    // Added random param, should throw illegal error
+    args = new HashMap<>();
+    args.put("maxTokenLen", "255");
+    args.put("randomParam", "rValue");
+    try {
+      factory = new KeywordTokenizerFactory(args);
+    } catch (Exception e) {
+      assertEquals("Unknown parameters: {randomParam=rValue}", e.getMessage());
+    }
+
+    // tokeniser will never split, no matter what is passed, 
+    // but the buffer will not be more than length of the token
+    args = new HashMap<>();
+    args.put("maxTokenLen", "5");
+    factory = new KeywordTokenizerFactory(args);
+    AttributeFactory attributeFactory = newAttributeFactory();
+    Tokenizer tokenizer = factory.create(attributeFactory);
+    StringReader reader = new StringReader("Tokenizertest");
+    tokenizer.setReader(reader);
+    try {
+      assertTokenStreamContents(tokenizer, new String[]{"Tokenizertest"});
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+
+    // tokeniser will never split, no matter what is passed, 
+    // but the buffer will not be more than length of the token
+    args = new HashMap<>();
+    args.put("maxTokenLen", "2");
+    factory = new KeywordTokenizerFactory(args);
+    attributeFactory = newAttributeFactory();
+    tokenizer = factory.create(attributeFactory);
+    reader = new StringReader("Tokenizer\u00A0test");
+    tokenizer.setReader(reader);
+    try {
+      assertTokenStreamContents(tokenizer, new String[]{"Tokenizer\u00A0test"});
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+  }
+}
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java
index 34c31d2dc5..58d94cbba1 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java
@@ -106,7 +106,9 @@ import org.junit.BeforeClass;
 import org.tartarus.snowball.SnowballProgram;
 import org.xml.sax.InputSource;
 
-/** tests random analysis chains */
+/**
+ * tests random analysis chains
+ */
 public class TestRandomChains extends BaseTokenStreamTestCase {
 
   static List<Constructor<? extends Tokenizer>> tokenizers;
@@ -115,17 +117,30 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
 
   private static final Predicate<Object[]> ALWAYS = (objects -> true);
 
-  private static final Map<Constructor<?>,Predicate<Object[]>> brokenConstructors = new HashMap<>();
+  private static final Map<Constructor<?>, Predicate<Object[]>> brokenConstructors = new HashMap<>();
+
   static {
     try {
       brokenConstructors.put(
+          LetterTokenizer.class.getConstructor(AttributeFactory.class, java.lang.Integer.class),
+          ALWAYS);
+      brokenConstructors.put(
+          LowerCaseTokenizer.class.getConstructor(AttributeFactory.class, java.lang.Integer.class),
+          ALWAYS);
+      brokenConstructors.put(
+          WhitespaceTokenizer.class.getConstructor(AttributeFactory.class, java.lang.Integer.class),
+          ALWAYS);
+      brokenConstructors.put(
+          UnicodeWhitespaceTokenizer.class.getConstructor(AttributeFactory.class, java.lang.Integer.class),
+          ALWAYS);
+      brokenConstructors.put(
           LimitTokenCountFilter.class.getConstructor(TokenStream.class, int.class),
           ALWAYS);
       brokenConstructors.put(
           LimitTokenCountFilter.class.getConstructor(TokenStream.class, int.class, boolean.class),
           args -> {
-              assert args.length == 3;
-              return !((Boolean) args[2]); // args are broken if consumeAllTokens is false
+            assert args.length == 3;
+            return !((Boolean) args[2]); // args are broken if consumeAllTokens is false
           });
       brokenConstructors.put(
           LimitTokenOffsetFilter.class.getConstructor(TokenStream.class, int.class),
@@ -133,8 +148,8 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
       brokenConstructors.put(
           LimitTokenOffsetFilter.class.getConstructor(TokenStream.class, int.class, boolean.class),
           args -> {
-              assert args.length == 3;
-              return !((Boolean) args[2]); // args are broken if consumeAllTokens is false
+            assert args.length == 3;
+            return !((Boolean) args[2]); // args are broken if consumeAllTokens is false
           });
       brokenConstructors.put(
           LimitTokenPositionFilter.class.getConstructor(TokenStream.class, int.class),
@@ -142,8 +157,8 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
       brokenConstructors.put(
           LimitTokenPositionFilter.class.getConstructor(TokenStream.class, int.class, boolean.class),
           args -> {
-              assert args.length == 3;
-              return !((Boolean) args[2]); // args are broken if consumeAllTokens is false
+            assert args.length == 3;
+            return !((Boolean) args[2]); // args are broken if consumeAllTokens is false
           });
       for (Class<?> c : Arrays.<Class<?>>asList(
           // TODO: can we promote some of these to be only
@@ -154,7 +169,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
           CrankyTokenFilter.class,
           // Not broken: we forcefully add this, so we shouldn't
           // also randomly pick it:
-          ValidatingTokenFilter.class, 
+          ValidatingTokenFilter.class,
           // TODO: needs to be a tokenizer, doesnt handle graph inputs properly (a shingle or similar following will then cause pain)
           WordDelimiterFilter.class,
           // Cannot correct offsets when a char filter had changed them:
@@ -165,7 +180,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
         for (Constructor<?> ctor : c.getConstructors()) {
           brokenConstructors.put(ctor, ALWAYS);
         }
-      }  
+      }
     } catch (Exception e) {
       throw new Error(e);
     }
@@ -173,7 +188,8 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
 
   // TODO: also fix these and remove (maybe):
   // Classes/options that don't produce consistent graph offsets:
-  private static final Map<Constructor<?>,Predicate<Object[]>> brokenOffsetsConstructors = new HashMap<>();
+  private static final Map<Constructor<?>, Predicate<Object[]>> brokenOffsetsConstructors = new HashMap<>();
+
   static {
     try {
       for (Class<?> c : Arrays.<Class<?>>asList(
@@ -208,14 +224,14 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
       final int modifiers = c.getModifiers();
       if (
         // don't waste time with abstract classes or deprecated known-buggy ones
-        Modifier.isAbstract(modifiers) || !Modifier.isPublic(modifiers)
-        || c.isSynthetic() || c.isAnonymousClass() || c.isMemberClass() || c.isInterface()
-        || c.isAnnotationPresent(Deprecated.class)
-        || !(Tokenizer.class.isAssignableFrom(c) || TokenFilter.class.isAssignableFrom(c) || CharFilter.class.isAssignableFrom(c))
-      ) {
+          Modifier.isAbstract(modifiers) || !Modifier.isPublic(modifiers)
+              || c.isSynthetic() || c.isAnonymousClass() || c.isMemberClass() || c.isInterface()
+              || c.isAnnotationPresent(Deprecated.class)
+              || !(Tokenizer.class.isAssignableFrom(c) || TokenFilter.class.isAssignableFrom(c) || CharFilter.class.isAssignableFrom(c))
+          ) {
         continue;
       }
-      
+
       for (final Constructor<?> ctor : c.getConstructors()) {
         // don't test synthetic or deprecated ctors, they likely have known bugs:
         if (ctor.isSynthetic() || ctor.isAnnotationPresent(Deprecated.class) || brokenConstructors.get(ctor) == ALWAYS) {
@@ -223,22 +239,22 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
         }
         if (Tokenizer.class.isAssignableFrom(c)) {
           assertTrue(ctor.toGenericString() + " has unsupported parameter types",
-            allowedTokenizerArgs.containsAll(Arrays.asList(ctor.getParameterTypes())));
+              allowedTokenizerArgs.containsAll(Arrays.asList(ctor.getParameterTypes())));
           tokenizers.add(castConstructor(Tokenizer.class, ctor));
         } else if (TokenFilter.class.isAssignableFrom(c)) {
           assertTrue(ctor.toGenericString() + " has unsupported parameter types",
-            allowedTokenFilterArgs.containsAll(Arrays.asList(ctor.getParameterTypes())));
+              allowedTokenFilterArgs.containsAll(Arrays.asList(ctor.getParameterTypes())));
           tokenfilters.add(castConstructor(TokenFilter.class, ctor));
         } else if (CharFilter.class.isAssignableFrom(c)) {
           assertTrue(ctor.toGenericString() + " has unsupported parameter types",
-            allowedCharFilterArgs.containsAll(Arrays.asList(ctor.getParameterTypes())));
+              allowedCharFilterArgs.containsAll(Arrays.asList(ctor.getParameterTypes())));
           charfilters.add(castConstructor(CharFilter.class, ctor));
         } else {
           fail("Cannot get here");
         }
       }
     }
-    
+
     final Comparator<Constructor<?>> ctorComp = (arg0, arg1) -> arg0.toGenericString().compareTo(arg1.toGenericString());
     Collections.sort(tokenizers, ctorComp);
     Collections.sort(tokenfilters, ctorComp);
@@ -249,28 +265,30 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
       System.out.println("charfilters = " + charfilters);
     }
   }
-  
+
   @AfterClass
   public static void afterClass() {
     tokenizers = null;
     tokenfilters = null;
     charfilters = null;
   }
-  
-  /** Hack to work around the stupidness of Oracle's strict Java backwards compatibility.
-   * {@code Class<T>#getConstructors()} should return unmodifiable {@code List<Constructor<T>>} not array! */
-  @SuppressWarnings("unchecked") 
+
+  /**
+   * Hack to work around the stupidness of Oracle's strict Java backwards compatibility.
+   * {@code Class<T>#getConstructors()} should return unmodifiable {@code List<Constructor<T>>} not array!
+   */
+  @SuppressWarnings("unchecked")
   private static <T> Constructor<T> castConstructor(Class<T> instanceClazz, Constructor<?> ctor) {
     return (Constructor<T>) ctor;
   }
-  
+
   public static List<Class<?>> getClassesForPackage(String pckgname) throws Exception {
     final List<Class<?>> classes = new ArrayList<>();
     collectClassesForPackage(pckgname, classes);
-    assertFalse("No classes found in package '"+pckgname+"'; maybe your test classes are packaged as JAR file?", classes.isEmpty());
+    assertFalse("No classes found in package '" + pckgname + "'; maybe your test classes are packaged as JAR file?", classes.isEmpty());
     return classes;
   }
-  
+
   private static void collectClassesForPackage(String pckgname, List<Class<?>> classes) throws Exception {
     final ClassLoader cld = TestRandomChains.class.getClassLoader();
     final String path = pckgname.replace('.', '/');
@@ -304,168 +322,175 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
       }
     }
   }
-  
-  private static final Map<Class<?>,Function<Random,Object>> argProducers = new IdentityHashMap<Class<?>,Function<Random,Object>>() {{
-    put(int.class, random ->  {
-        // TODO: could cause huge ram usage to use full int range for some filters
-        // (e.g. allocate enormous arrays)
-        // return Integer.valueOf(random.nextInt());
-        return Integer.valueOf(TestUtil.nextInt(random, -50, 50));
+
+  private static final Map<Class<?>, Function<Random, Object>> argProducers = new IdentityHashMap<Class<?>, Function<Random, Object>>() {{
+    put(int.class, random -> {
+      // TODO: could cause huge ram usage to use full int range for some filters
+      // (e.g. allocate enormous arrays)
+      // return Integer.valueOf(random.nextInt());
+      return Integer.valueOf(TestUtil.nextInt(random, -50, 50));
     });
-    put(char.class, random ->  {
-        // TODO: fix any filters that care to throw IAE instead.
-        // also add a unicode validating filter to validate termAtt?
-        // return Character.valueOf((char)random.nextInt(65536));
-        while(true) {
-          char c = (char)random.nextInt(65536);
-          if (c < '\uD800' || c > '\uDFFF') {
-            return Character.valueOf(c);
-          }
+    put(char.class, random -> {
+      // TODO: fix any filters that care to throw IAE instead.
+      // also add a unicode validating filter to validate termAtt?
+      // return Character.valueOf((char)random.nextInt(65536));
+      while (true) {
+        char c = (char) random.nextInt(65536);
+        if (c < '\uD800' || c > '\uDFFF') {
+          return Character.valueOf(c);
         }
+      }
     });
     put(float.class, Random::nextFloat);
     put(boolean.class, Random::nextBoolean);
     put(byte.class, random -> (byte) random.nextInt(256));
-    put(byte[].class, random ->  {
-        byte bytes[] = new byte[random.nextInt(256)];
-        random.nextBytes(bytes);
-        return bytes;
+    put(byte[].class, random -> {
+      byte bytes[] = new byte[random.nextInt(256)];
+      random.nextBytes(bytes);
+      return bytes;
     });
-    put(Random.class, random ->  new Random(random.nextLong()));
+    put(Random.class, random -> new Random(random.nextLong()));
     put(Version.class, random -> Version.LATEST);
     put(AttributeFactory.class, BaseTokenStreamTestCase::newAttributeFactory);
-    put(Set.class,random ->  {
-        // TypeTokenFilter
-        Set<String> set = new HashSet<>();
-        int num = random.nextInt(5);
-        for (int i = 0; i < num; i++) {
-          set.add(StandardTokenizer.TOKEN_TYPES[random.nextInt(StandardTokenizer.TOKEN_TYPES.length)]);
-        }
-        return set;
+    put(Set.class, random -> {
+      // TypeTokenFilter
+      Set<String> set = new HashSet<>();
+      int num = random.nextInt(5);
+      for (int i = 0; i < num; i++) {
+        set.add(StandardTokenizer.TOKEN_TYPES[random.nextInt(StandardTokenizer.TOKEN_TYPES.length)]);
+      }
+      return set;
     });
-    put(Collection.class, random ->  {
-        // CapitalizationFilter
-        Collection<char[]> col = new ArrayList<>();
-        int num = random.nextInt(5);
-        for (int i = 0; i < num; i++) {
-          col.add(TestUtil.randomSimpleString(random).toCharArray());
-        }
-        return col;
+    put(Collection.class, random -> {
+      // CapitalizationFilter
+      Collection<char[]> col = new ArrayList<>();
+      int num = random.nextInt(5);
+      for (int i = 0; i < num; i++) {
+        col.add(TestUtil.randomSimpleString(random).toCharArray());
+      }
+      return col;
     });
-    put(CharArraySet.class, random ->  {
-        int num = random.nextInt(10);
-        CharArraySet set = new CharArraySet(num, random.nextBoolean());
-        for (int i = 0; i < num; i++) {
-          // TODO: make nastier
-          set.add(TestUtil.randomSimpleString(random));
-        }
-        return set;
+    put(CharArraySet.class, random -> {
+      int num = random.nextInt(10);
+      CharArraySet set = new CharArraySet(num, random.nextBoolean());
+      for (int i = 0; i < num; i++) {
+        // TODO: make nastier
+        set.add(TestUtil.randomSimpleString(random));
+      }
+      return set;
     });
     // TODO: don't want to make the exponentially slow ones Dawid documents
     // in TestPatternReplaceFilter, so dont use truly random patterns (for now)
-    put(Pattern.class, random ->  Pattern.compile("a"));
-    put(Pattern[].class, random -> new Pattern[] {Pattern.compile("([a-z]+)"), Pattern.compile("([0-9]+)")});
+    put(Pattern.class, random -> Pattern.compile("a"));
+    put(Pattern[].class, random -> new Pattern[]{Pattern.compile("([a-z]+)"), Pattern.compile("([0-9]+)")});
     put(PayloadEncoder.class, random -> new IdentityEncoder()); // the other encoders will throw exceptions if tokens arent numbers?
     put(Dictionary.class, random -> {
-        // TODO: make nastier
-        InputStream affixStream = TestHunspellStemFilter.class.getResourceAsStream("simple.aff");
-        InputStream dictStream = TestHunspellStemFilter.class.getResourceAsStream("simple.dic");
-        try {
-          return new Dictionary(new RAMDirectory(), "dictionary", affixStream, dictStream);
-        } catch (Exception ex) {
-          Rethrow.rethrow(ex);
-          return null; // unreachable code
-        }
+      // TODO: make nastier
+      InputStream affixStream = TestHunspellStemFilter.class.getResourceAsStream("simple.aff");
+      InputStream dictStream = TestHunspellStemFilter.class.getResourceAsStream("simple.dic");
+      try {
+        return new Dictionary(new RAMDirectory(), "dictionary", affixStream, dictStream);
+      } catch (Exception ex) {
+        Rethrow.rethrow(ex);
+        return null; // unreachable code
+      }
     });
     put(HyphenationTree.class, random -> {
-        // TODO: make nastier
-        try {
-          InputSource is = new InputSource(TestCompoundWordTokenFilter.class.getResource("da_UTF8.xml").toExternalForm());
-          HyphenationTree hyphenator = HyphenationCompoundWordTokenFilter.getHyphenationTree(is);
-          return hyphenator;
-        } catch (Exception ex) {
-          Rethrow.rethrow(ex);
-          return null; // unreachable code
-        }
+      // TODO: make nastier
+      try {
+        InputSource is = new InputSource(TestCompoundWordTokenFilter.class.getResource("da_UTF8.xml").toExternalForm());
+        HyphenationTree hyphenator = HyphenationCompoundWordTokenFilter.getHyphenationTree(is);
+        return hyphenator;
+      } catch (Exception ex) {
+        Rethrow.rethrow(ex);
+        return null; // unreachable code
+      }
     });
-    put(SnowballProgram.class, random ->  {
-        try {
-          String lang = TestSnowball.SNOWBALL_LANGS[random.nextInt(TestSnowball.SNOWBALL_LANGS.length)];
-          Class<? extends SnowballProgram> clazz = Class.forName("org.tartarus.snowball.ext." + lang + "Stemmer").asSubclass(SnowballProgram.class);
-          return clazz.newInstance();
-        } catch (Exception ex) {
-          Rethrow.rethrow(ex);
-          return null; // unreachable code
-        }
+    put(SnowballProgram.class, random -> {
+      try {
+        String lang = TestSnowball.SNOWBALL_LANGS[random.nextInt(TestSnowball.SNOWBALL_LANGS.length)];
+        Class<? extends SnowballProgram> clazz = Class.forName("org.tartarus.snowball.ext." + lang + "Stemmer").asSubclass(SnowballProgram.class);
+        return clazz.newInstance();
+      } catch (Exception ex) {
+        Rethrow.rethrow(ex);
+        return null; // unreachable code
+      }
     });
-    put(String.class, random ->  {
-        // TODO: make nastier
-        if (random.nextBoolean()) {
-          // a token type
-          return StandardTokenizer.TOKEN_TYPES[random.nextInt(StandardTokenizer.TOKEN_TYPES.length)];
-        } else {
-          return TestUtil.randomSimpleString(random);
-        }
+    put(String.class, random -> {
+      // TODO: make nastier
+      if (random.nextBoolean()) {
+        // a token type
+        return StandardTokenizer.TOKEN_TYPES[random.nextInt(StandardTokenizer.TOKEN_TYPES.length)];
+      } else {
+        return TestUtil.randomSimpleString(random);
+      }
     });
     put(NormalizeCharMap.class, random -> {
-        NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();
-        // we can't add duplicate keys, or NormalizeCharMap gets angry
-        Set<String> keys = new HashSet<>();
-        int num = random.nextInt(5);
-        //System.out.println("NormalizeCharMap=");
-        for (int i = 0; i < num; i++) {
-          String key = TestUtil.randomSimpleString(random);
-          if (!keys.contains(key) && key.length() > 0) {
-            String value = TestUtil.randomSimpleString(random);
-            builder.add(key, value);
-            keys.add(key);
-            //System.out.println("mapping: '" + key + "' => '" + value + "'");
-          }
+      NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();
+      // we can't add duplicate keys, or NormalizeCharMap gets angry
+      Set<String> keys = new HashSet<>();
+      int num = random.nextInt(5);
+      //System.out.println("NormalizeCharMap=");
+      for (int i = 0; i < num; i++) {
+        String key = TestUtil.randomSimpleString(random);
+        if (!keys.contains(key) && key.length() > 0) {
+          String value = TestUtil.randomSimpleString(random);
+          builder.add(key, value);
+          keys.add(key);
+          //System.out.println("mapping: '" + key + "' => '" + value + "'");
         }
-        return builder.build();
+      }
+      return builder.build();
     });
     put(CharacterRunAutomaton.class, random -> {
-        // TODO: could probably use a purely random automaton
-        switch(random.nextInt(5)) {
-          case 0: return MockTokenizer.KEYWORD;
-          case 1: return MockTokenizer.SIMPLE;
-          case 2: return MockTokenizer.WHITESPACE;
-          case 3: return MockTokenFilter.EMPTY_STOPSET;
-          default: return MockTokenFilter.ENGLISH_STOPSET;
-        }
+      // TODO: could probably use a purely random automaton
+      switch (random.nextInt(5)) {
+        case 0:
+          return MockTokenizer.KEYWORD;
+        case 1:
+          return MockTokenizer.SIMPLE;
+        case 2:
+          return MockTokenizer.WHITESPACE;
+        case 3:
+          return MockTokenFilter.EMPTY_STOPSET;
+        default:
+          return MockTokenFilter.ENGLISH_STOPSET;
+      }
     });
     put(CharArrayMap.class, random -> {
-        int num = random.nextInt(10);
-        CharArrayMap<String> map = new CharArrayMap<>(num, random.nextBoolean());
-        for (int i = 0; i < num; i++) {
-          // TODO: make nastier
-          map.put(TestUtil.randomSimpleString(random), TestUtil.randomSimpleString(random));
-        }
-        return map;
+      int num = random.nextInt(10);
+      CharArrayMap<String> map = new CharArrayMap<>(num, random.nextBoolean());
+      for (int i = 0; i < num; i++) {
+        // TODO: make nastier
+        map.put(TestUtil.randomSimpleString(random), TestUtil.randomSimpleString(random));
+      }
+      return map;
     });
     put(StemmerOverrideMap.class, random -> {
-        int num = random.nextInt(10);
-        StemmerOverrideFilter.Builder builder = new StemmerOverrideFilter.Builder(random.nextBoolean());
-        for (int i = 0; i < num; i++) {
-          String input = ""; 
-          do {
-            input = TestUtil.randomRealisticUnicodeString(random);
-          } while(input.isEmpty());
-          String out = ""; TestUtil.randomSimpleString(random);
-          do {
-            out = TestUtil.randomRealisticUnicodeString(random);
-          } while(out.isEmpty());
-          builder.add(input, out);
-        }
-        try {
-          return builder.build();
-        } catch (Exception ex) {
-          Rethrow.rethrow(ex);
-          return null; // unreachable code
+      int num = random.nextInt(10);
+      StemmerOverrideFilter.Builder builder = new StemmerOverrideFilter.Builder(random.nextBoolean());
+      for (int i = 0; i < num; i++) {
+        String input = "";
+        do {
+          input = TestUtil.randomRealisticUnicodeString(random);
+        } while (input.isEmpty());
+        String out = "";
+        TestUtil.randomSimpleString(random);
+        do {
+          out = TestUtil.randomRealisticUnicodeString(random);
+        } while (out.isEmpty());
+        builder.add(input, out);
+      }
+      try {
+        return builder.build();
+      } catch (Exception ex) {
+        Rethrow.rethrow(ex);
+        return null; // unreachable code
       }
     });
     put(SynonymMap.class, new Function<Random, Object>() {
-      @Override public Object apply(Random random) {
+      @Override
+      public Object apply(Random random) {
         SynonymMap.Builder b = new SynonymMap.Builder(random.nextBoolean());
         final int numEntries = atLeast(10);
         for (int j = 0; j < numEntries; j++) {
@@ -478,58 +503,59 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
           return null; // unreachable code
         }
       }
-      
+
       private void addSyn(SynonymMap.Builder b, String input, String output, boolean keepOrig) {
         b.add(new CharsRef(input.replaceAll(" +", "\u0000")),
-              new CharsRef(output.replaceAll(" +", "\u0000")),
-              keepOrig);
+            new CharsRef(output.replaceAll(" +", "\u0000")),
+            keepOrig);
       }
-      
+
       private String randomNonEmptyString(Random random) {
-        while(true) {
+        while (true) {
           final String s = TestUtil.randomUnicodeString(random).trim();
           if (s.length() != 0 && s.indexOf('\u0000') == -1) {
             return s;
           }
         }
-      }    
+      }
     });
     put(DateFormat.class, random -> {
-        if (random.nextBoolean()) return null;
-        return DateFormat.getDateInstance(DateFormat.DEFAULT, randomLocale(random));
+      if (random.nextBoolean()) return null;
+      return DateFormat.getDateInstance(DateFormat.DEFAULT, randomLocale(random));
     });
     put(Automaton.class, random -> {
-        return Operations.determinize(new RegExp(AutomatonTestUtil.randomRegexp(random), RegExp.NONE).toAutomaton(), Operations.DEFAULT_MAX_DETERMINIZED_STATES);
+      return Operations.determinize(new RegExp(AutomatonTestUtil.randomRegexp(random), RegExp.NONE).toAutomaton(), Operations.DEFAULT_MAX_DETERMINIZED_STATES);
     });
   }};
-  
+
   static final Set<Class<?>> allowedTokenizerArgs, allowedTokenFilterArgs, allowedCharFilterArgs;
+
   static {
-    allowedTokenizerArgs = Collections.newSetFromMap(new IdentityHashMap<Class<?>,Boolean>());
+    allowedTokenizerArgs = Collections.newSetFromMap(new IdentityHashMap<Class<?>, Boolean>());
     allowedTokenizerArgs.addAll(argProducers.keySet());
     allowedTokenizerArgs.add(Reader.class);
     allowedTokenizerArgs.add(AttributeFactory.class);
     allowedTokenizerArgs.add(AttributeSource.class);
     allowedTokenizerArgs.add(Automaton.class);
 
-    allowedTokenFilterArgs = Collections.newSetFromMap(new IdentityHashMap<Class<?>,Boolean>());
+    allowedTokenFilterArgs = Collections.newSetFromMap(new IdentityHashMap<Class<?>, Boolean>());
     allowedTokenFilterArgs.addAll(argProducers.keySet());
     allowedTokenFilterArgs.add(TokenStream.class);
     // TODO: fix this one, thats broken:
     allowedTokenFilterArgs.add(CommonGramsFilter.class);
-    
-    allowedCharFilterArgs = Collections.newSetFromMap(new IdentityHashMap<Class<?>,Boolean>());
+
+    allowedCharFilterArgs = Collections.newSetFromMap(new IdentityHashMap<Class<?>, Boolean>());
     allowedCharFilterArgs.addAll(argProducers.keySet());
     allowedCharFilterArgs.add(Reader.class);
   }
-  
+
   @SuppressWarnings("unchecked")
   static <T> T newRandomArg(Random random, Class<T> paramType) {
-    final Function<Random,Object> producer = argProducers.get(paramType);
+    final Function<Random, Object> producer = argProducers.get(paramType);
     assertNotNull("No producer for arguments of type " + paramType.getName() + " found", producer);
     return (T) producer.apply(random);
   }
-  
+
   static Object[] newTokenizerArgs(Random random, Class<?>[] paramTypes) {
     Object[] args = new Object[paramTypes.length];
     for (int i = 0; i < args.length; i++) {
@@ -544,7 +570,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
     }
     return args;
   }
-  
+
   static Object[] newCharFilterArgs(Random random, Reader reader, Class<?>[] paramTypes) {
     Object[] args = new Object[paramTypes.length];
     for (int i = 0; i < args.length; i++) {
@@ -557,7 +583,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
     }
     return args;
   }
-  
+
   static Object[] newFilterArgs(Random random, TokenStream stream, Class<?>[] paramTypes) {
     Object[] args = new Object[paramTypes.length];
     for (int i = 0; i < args.length; i++) {
@@ -576,7 +602,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
 
   static class MockRandomAnalyzer extends Analyzer {
     final long seed;
-    
+
     MockRandomAnalyzer(long seed) {
       this.seed = seed;
     }
@@ -588,7 +614,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
       TokenFilterSpec filterSpec = newFilterChain(random, tokenizerSpec.tokenizer, tokenizerSpec.offsetsAreCorrect);
       return filterSpec.offsetsAreCorrect;
     }
-    
+
     @Override
     protected TokenStreamComponents createComponents(String fieldName) {
       Random random = new Random(seed);
@@ -628,7 +654,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
       sb.append(tokenFilterSpec.offsetsAreCorrect);
       return sb.toString();
     }
-    
+
     private <T> T createComponent(Constructor<T> ctor, Object[] args, StringBuilder descr) {
       try {
         final T instance = ctor.newInstance(args);
@@ -640,7 +666,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
         descr.append("\n  ");
         descr.append(ctor.getDeclaringClass().getName());
         String params = Arrays.deepToString(args);
-        params = params.substring(1, params.length()-1);
+        params = params.substring(1, params.length() - 1);
         descr.append("(").append(params).append(")");
         return instance;
       } catch (InvocationTargetException ite) {
@@ -689,7 +715,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
       }
       return spec;
     }
-    
+
     private CharFilterSpec newCharFilterChain(Random random, Reader reader) {
       CharFilterSpec spec = new CharFilterSpec();
       spec.reader = reader;
@@ -712,7 +738,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
       spec.toString = descr.toString();
       return spec;
     }
-    
+
     private TokenFilterSpec newFilterChain(Random random, Tokenizer tokenizer, boolean offsetsAreCorrect) {
       TokenFilterSpec spec = new TokenFilterSpec();
       spec.offsetsAreCorrect = offsetsAreCorrect;
@@ -728,15 +754,15 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
 
         while (true) {
           final Constructor<? extends TokenFilter> ctor = tokenfilters.get(random.nextInt(tokenfilters.size()));
-          
+
           // hack: MockGraph/MockLookahead has assertions that will trip if they follow
           // an offsets violator. so we cant use them after e.g. wikipediatokenizer
           if (!spec.offsetsAreCorrect &&
               (ctor.getDeclaringClass().equals(MockGraphTokenFilter.class)
-               || ctor.getDeclaringClass().equals(MockRandomLookaheadTokenFilter.class))) {
+                  || ctor.getDeclaringClass().equals(MockRandomLookaheadTokenFilter.class))) {
             continue;
           }
-          
+
           final Object args[] = newFilterArgs(random, spec.stream, ctor.getParameterTypes());
           if (broken(ctor, args)) {
             continue;
@@ -759,14 +785,14 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
       return spec;
     }
   }
-  
+
   static class CheckThatYouDidntReadAnythingReaderWrapper extends CharFilter {
     boolean readSomething;
-    
+
     CheckThatYouDidntReadAnythingReaderWrapper(Reader in) {
       super(in);
     }
-    
+
     @Override
     public int correct(int currentOff) {
       return currentOff; // we don't change any offsets
@@ -822,24 +848,24 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
       input.reset();
     }
   }
-  
+
   static class TokenizerSpec {
     Tokenizer tokenizer;
     String toString;
     boolean offsetsAreCorrect = true;
   }
-  
+
   static class TokenFilterSpec {
     TokenStream stream;
     String toString;
     boolean offsetsAreCorrect = true;
   }
-  
+
   static class CharFilterSpec {
     Reader reader;
     String toString;
   }
-  
+
   public void testRandomChains() throws Throwable {
     int numIterations = TEST_NIGHTLY ? atLeast(20) : 3;
     Random random = random();
@@ -850,7 +876,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
         }
         try {
           checkNormalize(a);
-          checkRandomData(random, a, 500*RANDOM_MULTIPLIER, 20, false,
+          checkRandomData(random, a, 500 * RANDOM_MULTIPLIER, 20, false,
               false /* We already validate our own offsets... */);
         } catch (Throwable e) {
           System.err.println("Exception from random analyzer: " + a);
@@ -877,7 +903,7 @@ public class TestRandomChains extends BaseTokenStreamTestCase {
           System.out.println("Creating random analyzer:" + a);
         }
         try {
-          checkRandomData(random, a, 50*RANDOM_MULTIPLIER, 80, false,
+          checkRandomData(random, a, 50 * RANDOM_MULTIPLIER, 80, false,
               false /* We already validate our own offsets... */);
         } catch (Throwable e) {
           System.err.println("Exception from random analyzer: " + a);
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUnicodeWhitespaceTokenizer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUnicodeWhitespaceTokenizer.java
index acdb670f7e..86125e5fde 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUnicodeWhitespaceTokenizer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUnicodeWhitespaceTokenizer.java
@@ -54,4 +54,93 @@ public class TestUnicodeWhitespaceTokenizer extends BaseTokenStreamTestCase {
     assertEquals(UnicodeWhitespaceTokenizer.class, tokenizer.getClass());
   }
 
+  public void testParamsFactory() {
+    Map<String, String> args = new HashMap<>();
+    WhitespaceTokenizerFactory factory;
+
+    // negative maxTokenLen
+    args = new HashMap<>();
+    args.put("rule", "unicode");
+    args.put("maxTokenLen", "-1");
+    try {
+      factory = new WhitespaceTokenizerFactory(args);
+    } catch (Exception e) {
+      assertEquals("maxTokenLen cannot be equal to or less than 0, passed: -1", e.getMessage());
+    }
+
+    // zero maxTokenLen
+    args = new HashMap<>();
+    args.put("rule", "unicode");
+    args.put("maxTokenLen", "0");
+    try {
+      factory = new WhitespaceTokenizerFactory(args);
+    } catch (Exception e) {
+      assertEquals("maxTokenLen cannot be equal to or less than 0, passed: 0", e.getMessage());
+    }
+
+    // Added random param, should throw illegal error
+    args = new HashMap<>();
+    args.put("rule", "unicode");
+    args.put("maxTokenLen", "255");
+    args.put("randomParam", "rValue");
+    try {
+      factory = new WhitespaceTokenizerFactory(args);
+    } catch (Exception e) {
+      assertEquals("Unknown parameters: {randomParam=rValue}", e.getMessage());
+    }
+
+    // tokeniser will split at 5, Token | izer, no matter what happens 
+    args = new HashMap<>();
+    args.put("rule", "unicode");
+    args.put("maxTokenLen", "5");
+    factory = new WhitespaceTokenizerFactory(args);
+    AttributeFactory attributeFactory = newAttributeFactory();
+    Tokenizer tokenizer = factory.create(attributeFactory);
+    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
+    tokenizer.setReader(reader);
+    try {
+      assertTokenStreamContents(tokenizer, new String[]{"Token",
+          "izer", "\ud801\udc1ctes", "t"});
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+
+    // tokeniser will split at 2, To | ke | ni | ze | r, no matter what happens 
+    args = new HashMap<>();
+    args.put("rule", "unicode");
+    args.put("maxTokenLen", "2");
+    factory = new WhitespaceTokenizerFactory(args);
+    attributeFactory = newAttributeFactory();
+    tokenizer = factory.create(attributeFactory);
+    reader = new StringReader("Tokenizer\u00A0test");
+    tokenizer.setReader(reader);
+    try {
+      assertTokenStreamContents(tokenizer, new String[]{"To", "ke", "ni", "ze", "r",
+          "te", "st"});
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+
+    // tokeniser will split at 10, no matter what happens, 
+    // but tokens' length are less than that
+    args = new HashMap<>();
+    args.put("rule", "unicode");
+    args.put("maxTokenLen", "10");
+    factory = new WhitespaceTokenizerFactory(args);
+    attributeFactory = newAttributeFactory();
+    tokenizer = factory.create(attributeFactory);
+    reader = new StringReader("Tokenizer\u00A0test");
+    tokenizer.setReader(reader);
+    try {
+      assertTokenStreamContents(tokenizer, new String[]{"Tokenizer",
+          "test"});
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  public void testFactoryWithSchema() {
+
+  }
+
 }
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers.java
index 783fc3e4b5..ffca21eb91 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers.java
@@ -89,6 +89,43 @@ public class TestCharTokenizers extends BaseTokenStreamTestCase {
     tokenizer.setReader(new StringReader(builder.toString() + builder.toString()));
     assertTokenStreamContents(tokenizer, new String[] {builder.toString().toLowerCase(Locale.ROOT), builder.toString().toLowerCase(Locale.ROOT)});
   }
+
+  /*
+   * tests the max word length passed as parameter - tokenizer will split at the passed position char no matter what happens
+   */
+  public void testCustomMaxTokenLength() throws IOException {
+
+    StringBuilder builder = new StringBuilder();
+    for (int i = 0; i < 100; i++) {
+      builder.append("A");
+    }
+    Tokenizer tokenizer = new LowerCaseTokenizer(newAttributeFactory(), new Integer(100));
+    // Tricky, passing two copies of the string to the reader....
+    tokenizer.setReader(new StringReader(builder.toString() + builder.toString()));
+    assertTokenStreamContents(tokenizer, new String[]{builder.toString().toLowerCase(Locale.ROOT), 
+        builder.toString().toLowerCase(Locale.ROOT) });
+
+    try {
+      tokenizer = new LowerCaseTokenizer(newAttributeFactory(), new Integer(-1));
+    } catch (Exception e) {
+      assertEquals("maxTokenLen cannot be equal to or less than 0, passed: -1", e.getMessage());
+    }
+
+    tokenizer = new LetterTokenizer(newAttributeFactory(), new Integer(100));
+    tokenizer.setReader(new StringReader(builder.toString() + builder.toString()));
+    assertTokenStreamContents(tokenizer, new String[]{builder.toString(), builder.toString()});
+
+
+    // Let's test that we can get a token longer than 255 through.
+    builder.setLength(0);
+    for (int i = 0; i < 500; i++) {
+      builder.append("Z");
+    }
+    tokenizer = new LetterTokenizer(newAttributeFactory(), new Integer(500));
+    tokenizer.setReader(new StringReader(builder.toString()));
+    assertTokenStreamContents(tokenizer, new String[]{builder.toString()});
+
+  }
   
   /*
    * tests the max word length of 255 with a surrogate pair at position 255
diff --git a/solr/core/src/test-files/solr/collection1/conf/schema-tokenizer-test.xml b/solr/core/src/test-files/solr/collection1/conf/schema-tokenizer-test.xml
new file mode 100644
index 0000000000..f3d3196dc8
--- /dev/null
+++ b/solr/core/src/test-files/solr/collection1/conf/schema-tokenizer-test.xml
@@ -0,0 +1,150 @@
+<?xml version="1.0" ?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one or more
+contributor license agreements.  See the NOTICE file distributed with
+this work for additional information regarding copyright ownership.
+The ASF licenses this file to You under the Apache License, Version 2.0
+(the "License"); you may not use this file except in compliance with
+the License.  You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+-->
+
+<!-- The Solr schema file. This file should be named "schema.xml" and
+should be located where the classloader for the Solr webapp can find it.
+
+This schema is used for testing, and as such has everything and the
+kitchen sink thrown in. See example/solr/conf/schema.xml for a
+more concise example.
+
+-->
+
+<schema name="test" version="1.0">
+
+  <!-- field type definitions... note that the "name" attribute is
+  just a label to be used by field definitions.  The "class"
+  attribute and any other attributes determine the real type and
+  behavior of the fieldType.
+  -->
+
+  <!--
+  Default numeric field types. For faster range queries, consider the tint/tfloat/tlong/tdouble types.
+  -->
+  <fieldType name="int" class="solr.TrieIntField" precisionStep="0" positionIncrementGap="0"/>
+  <fieldType name="float" class="solr.TrieFloatField" precisionStep="0" positionIncrementGap="0"/>
+  <fieldType name="long" class="solr.TrieLongField" precisionStep="0" positionIncrementGap="0"/>
+  <fieldType name="double" class="solr.TrieDoubleField" precisionStep="0" positionIncrementGap="0"/>
+
+  <!--
+  Numeric field types that index each value at various levels of precision
+  to accelerate range queries when the number of values between the range
+  endpoints is large. See the javadoc for LegacyNumericRangeQuery for internal
+  implementation details.
+  -->
+
+  <!-- Seperate analyzers for index and query time -->
+
+  <fieldType name="letterfieldType" class="solr.TextField" positionIncrementGap="100">
+      <analyzer type="index">
+        <tokenizer class="solr.LetterTokenizerFactory" maxTokenLen="3" />
+      </analyzer>
+      <analyzer type="query">
+        <tokenizer class="solr.StandardTokenizerFactory"/>
+      </analyzer>
+  </fieldType>
+
+  <fieldType name="lowerCasefieldType" class="solr.TextField" positionIncrementGap="100">
+    <analyzer type="index">
+      <tokenizer class="solr.LowerCaseTokenizerFactory" maxTokenLen="3" />
+    </analyzer>
+    <analyzer type="query">
+      <tokenizer class="solr.StandardTokenizerFactory"/>
+    </analyzer>
+  </fieldType>
+
+  <fieldType name="whiteSpfieldType" class="solr.TextField" positionIncrementGap="100">
+    <analyzer type="index">
+      <tokenizer class="solr.WhitespaceTokenizerFactory" maxTokenLen="3" />
+    </analyzer>
+    <analyzer type="query">
+      <tokenizer class="solr.StandardTokenizerFactory"/>
+    </analyzer>
+  </fieldType>
+
+  <fieldType name="uniWhiteSpfieldType" class="solr.TextField" positionIncrementGap="100">
+    <analyzer type="index">
+      <tokenizer class="solr.WhitespaceTokenizerFactory" maxTokenLen="3" />
+    </analyzer>
+    <analyzer type="query">
+      <tokenizer class="solr.StandardTokenizerFactory"/>
+    </analyzer>
+  </fieldType>
+
+  <fieldType name="keywordfieldType" class="solr.TextField" positionIncrementGap="100">
+    <analyzer index="index">
+      <tokenizer class="solr.KeywordTokenizerFactory" maxTokenLen="3" />
+    </analyzer>
+    <analyzer type="query">
+      <tokenizer class="solr.StandardTokenizerFactory"/>
+    </analyzer>
+  </fieldType>
+
+  <!-- Same analyzers for both index and query time -->
+
+  <fieldType name="letter0fieldType" class="solr.TextField" positionIncrementGap="100">
+    <analyzer>
+      <tokenizer class="solr.LetterTokenizerFactory" maxTokenLen="3" />
+    </analyzer>
+  </fieldType>
+
+  <fieldType name="lowerCase0fieldType" class="solr.TextField" positionIncrementGap="100">
+    <analyzer>
+      <tokenizer class="solr.LowerCaseTokenizerFactory" maxTokenLen="3" />
+    </analyzer>
+  </fieldType>
+
+  <fieldType name="whiteSp0fieldType" class="solr.TextField" positionIncrementGap="100">
+    <analyzer>
+      <tokenizer class="solr.WhitespaceTokenizerFactory" maxTokenLen="3" />
+    </analyzer>
+  </fieldType>
+
+  <fieldType name="uniWhiteSp0fieldType" class="solr.TextField" positionIncrementGap="100">
+    <analyzer>
+      <tokenizer class="solr.WhitespaceTokenizerFactory" maxTokenLen="3" />
+    </analyzer>
+  </fieldType>
+
+  <fieldType name="keyword0fieldType" class="solr.TextField" positionIncrementGap="100">
+    <analyzer>
+      <tokenizer class="solr.KeywordTokenizerFactory"  maxTokenLen="3" />
+    </analyzer>
+  </fieldType>
+
+  <field name="id" type="int" indexed="true" stored="true" multiValued="false" required="true"/>
+
+  <field name="letter" type="letterfieldType" indexed="true" stored="true"/>
+  <field name="lowerCase" type="lowerCasefieldType" indexed="true" stored="true"/>
+  <field name="whiteSpace" type="whiteSpfieldType" indexed="true" stored="true"/>
+  <field name="unicodeWhiteSpace" type="uniWhiteSpfieldType" indexed="true" stored="true"/>
+  <field name="keyword" type="keywordfieldType" indexed="true" stored="true"/>
+
+  <field name="letter0" type="letter0fieldType" indexed="true" stored="true"/>
+  <field name="lowerCase0" type="lowerCase0fieldType" indexed="true" stored="true"/>
+  <field name="whiteSpace0" type="whiteSp0fieldType" indexed="true" stored="true"/>
+  <field name="unicodeWhiteSpace0" type="uniWhiteSp0fieldType" indexed="true" stored="true"/>
+  <field name="keyword0" type="keyword0fieldType" indexed="true" stored="true"/>
+
+  <field name="_version_" type="long" indexed="true" stored="true" multiValued="false"/>
+
+
+  <uniqueKey>id</uniqueKey>
+
+
+</schema>
diff --git a/solr/core/src/test/org/apache/solr/util/TestMaxTokenLenTokenizer.java b/solr/core/src/test/org/apache/solr/util/TestMaxTokenLenTokenizer.java
new file mode 100644
index 0000000000..c7e0dc3c8c
--- /dev/null
+++ b/solr/core/src/test/org/apache/solr/util/TestMaxTokenLenTokenizer.java
@@ -0,0 +1,135 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.solr.util;
+
+import org.apache.solr.SolrTestCaseJ4;
+import org.junit.BeforeClass;
+
+/**
+ * Tests for:
+ * {@link org.apache.lucene.analysis.core.LowerCaseTokenizerFactory}
+ * {@link org.apache.lucene.analysis.core.LetterTokenizerFactory}
+ * {@link org.apache.lucene.analysis.core.KeywordTokenizerFactory}
+ * {@link org.apache.lucene.analysis.core.WhitespaceTokenizerFactory}
+ */
+
+public class TestMaxTokenLenTokenizer extends SolrTestCaseJ4 {
+  /* field names are used in accordance with the solrconfig and schema supplied */
+  private static final String ID = "id";
+
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    initCore("solrconfig-update-processor-chains.xml", "schema-tokenizer-test.xml");
+  }
+
+  public void testSingleFieldDiffAnalyzers() throws Exception {
+
+    clearIndex();
+
+    // using fields with definitions, different tokenizer factories respectively at index time and standard tokenizer at query time.
+
+    updateJ("{\"add\":{\"doc\": {\"id\":1,\"letter\":\"letter\"}},\"commit\":{}}",null);
+    updateJ("{\"add\":{\"doc\": {\"id\":2,\"lowerCase\":\"lowerCase\"}},\"commit\":{}}",null);
+    updateJ("{\"add\":{\"doc\": {\"id\":3,\"whiteSpace\":\"whiteSpace in\"}},\"commit\":{}}",null);
+    updateJ("{\"add\":{\"doc\": {\"id\":4,\"unicodeWhiteSpace\":\"unicode in\"}},\"commit\":{}}",null);
+    updateJ("{\"add\":{\"doc\": {\"id\":5,\"keyword\":\"keyword\"}},\"commit\":{}}",null);
+
+    assertU(commit());
+
+    assertQ("Check the total number of docs", req("q","*:*"), "//result[@numFound=5]");
+
+    //Tokens generated for "letter": "let" "ter" "letter" , maxTokenLen=3
+    assertQ("Check the total number of docs", req("q","letter:let"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","letter:lett"), "//result[@numFound=0]");
+
+    //Tokens generated for "lowerCase": "low" "erC" "ase" "lowerCase" , maxTokenLen=3
+    assertQ("Check the total number of docs", req("q","lowerCase:low"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","lowerCase:l"), "//result[@numFound=0]");
+    assertQ("Check the total number of docs", req("q","lowerCase:lo"), "//result[@numFound=0]");
+    assertQ("Check the total number of docs", req("q","lowerCase:lower"), "//result[@numFound=0]");
+
+    //Tokens generated for "whiteSpace in": "whi" "teS" "pac" "e" "in" "whiteSpace" , maxTokenLen=3
+    assertQ("Check the total number of docs", req("q","whiteSpace:whi"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","whiteSpace:teS"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","whiteSpace:in"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","whiteSpace:white"), "//result[@numFound=0]");
+
+    //Tokens generated for "unicode in": "uni" "cod" "e" "in" "unicode" , maxTokenLen=3
+    assertQ("Check the total number of docs", req("q","unicodeWhiteSpace:uni"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","unicodeWhiteSpace:cod"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","unicodeWhiteSpace:e"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","unicodeWhiteSpace:unico"), "//result[@numFound=0]");
+
+    //Tokens generated for "keyword": "keyword" , maxTokenLen=3
+    assertQ("Check the total number of docs", req("q","keyword:keyword"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","keyword:key"), "//result[@numFound=0]");
+
+  }
+
+  public void testSingleFieldSameAnalyzers() throws Exception {
+
+    clearIndex();
+
+    // using fields with definitions, same tokenizers both at index and query time.
+
+    updateJ("{\"add\":{\"doc\": {\"id\":1,\"letter0\":\"letter\"}},\"commit\":{}}",null);
+    updateJ("{\"add\":{\"doc\": {\"id\":2,\"lowerCase0\":\"lowerCase\"}},\"commit\":{}}",null);
+    updateJ("{\"add\":{\"doc\": {\"id\":3,\"whiteSpace0\":\"whiteSpace in\"}},\"commit\":{}}",null);
+    updateJ("{\"add\":{\"doc\": {\"id\":4,\"unicodeWhiteSpace0\":\"unicode in\"}},\"commit\":{}}",null);
+    updateJ("{\"add\":{\"doc\": {\"id\":5,\"keyword0\":\"keyword\"}},\"commit\":{}}",null);
+
+    assertU(commit());
+
+    assertQ("Check the total number of docs", req("q","*:*"), "//result[@numFound=5]");
+
+    //Tokens generated for "letter": "let" "ter" "letter" , maxTokenLen=3
+    // Anything that matches the first three letters should be found when maxLen=3
+    assertQ("Check the total number of docs", req("q","letter0:l"), "//result[@numFound=0]");
+    assertQ("Check the total number of docs", req("q","letter0:let"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","letter0:lett"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","letter0:letXYZ"), "//result[@numFound=1]");
+
+    //Tokens generated for "lowerCase": "low" "erC" "ase" "lowerCase" , maxTokenLen=3
+    // Anything that matches the first three letters should be found when maxLen=3
+    assertQ("Check the total number of docs", req("q","lowerCase0:low"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","lowerCase0:l"), "//result[@numFound=0]");
+    assertQ("Check the total number of docs", req("q","lowerCase0:lo"), "//result[@numFound=0]");
+    assertQ("Check the total number of docs", req("q","lowerCase0:lowerXYZ"), "//result[@numFound=1]");
+
+    //Tokens generated for "whiteSpace in": "whi" "teS" "pac" "e" "in" "whiteSpace" , maxTokenLen=3
+    // Anything that matches the first three letters should be found when maxLen=3
+    assertQ("Check the total number of docs", req("q","whiteSpace0:h"), "//result[@numFound=0]");
+    assertQ("Check the total number of docs", req("q","whiteSpace0:whi"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","whiteSpace0:teS"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","whiteSpace0:in"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","whiteSpace0:whiteZKY"), "//result[@numFound=1]");
+
+    //Tokens generated for "unicode in": "uni" "cod" "e" "in" "unicode" , maxTokenLen=3
+    // Anything that matches the first three letters should be found when maxLen=3
+    assertQ("Check the total number of docs", req("q","unicodeWhiteSpace0:u"), "//result[@numFound=0]");
+    assertQ("Check the total number of docs", req("q","unicodeWhiteSpace0:uni"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","unicodeWhiteSpace0:cod"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","unicodeWhiteSpace0:e"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","unicodeWhiteSpace0:unicoVBRT"), "//result[@numFound=1]");
+
+    //Tokens generated for "keyword": "keyword" , maxTokenLen=3
+    assertQ("Check the total number of docs", req("q","keyword0:keyword"), "//result[@numFound=1]");
+    assertQ("Check the total number of docs", req("q","keyword0:key"), "//result[@numFound=0]");
+
+  }
+}
