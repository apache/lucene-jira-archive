Index: java/org/apache/lucene/index/DirectoryIndexReader.java
===================================================================
--- java/org/apache/lucene/index/DirectoryIndexReader.java	(revision 695616)
+++ java/org/apache/lucene/index/DirectoryIndexReader.java	(working copy)
@@ -44,6 +44,7 @@
   private Lock writeLock;
   private boolean stale;
   private final HashSet synced = new HashSet();
+  //protected boolean allowCloneWithChanges = false;
 
   /** Used by commit() to record pre-commit state in case
    * rollback is necessary */
@@ -117,23 +118,39 @@
       return (DirectoryIndexReader) finder.doBody(commit.getSegmentsFileName());
     }
   }
-
-  public final synchronized IndexReader reopen() throws CorruptIndexException, IOException {
+  
+  protected IndexReader allowReopen(boolean doClone) throws IOException {
+    if (doClone) {
+      if (hasChanges)
+        throw new IllegalStateException("cannot clone() a reader with uncommitted changes");
+      return null;
+    } else {
+      if (hasChanges || isCurrent())
+        // If we have changes we have the write lock and we
+        // are already "current", or the index in the
+        // directory hasn't changed - nothing to do here
+        return this;
+    }
+    return null;
+  }
+  
+  protected final synchronized IndexReader doReopenOrClone(final boolean doClone) throws CorruptIndexException, IOException {
     ensureOpen();
 
-    if (this.hasChanges || this.isCurrent()) {
-      // this has changes, therefore we have the lock and don't need to reopen
-      // OR: the index in the directory hasn't changed - nothing to do here
-      return this;
-    }
+    IndexReader reader = allowReopen(doClone);
+    if (reader != null) return reader;
 
     return (DirectoryIndexReader) new SegmentInfos.FindSegmentsFile(directory) {
 
       protected Object doBody(String segmentFileName) throws CorruptIndexException, IOException {
-        SegmentInfos infos = new SegmentInfos();
-        infos.read(directory, segmentFileName);
-
-        DirectoryIndexReader newReader = doReopen(infos);
+        SegmentInfos infos = null;
+        if (doClone) {
+          infos = (SegmentInfos) segmentInfos.clone();
+        } else {
+          infos = new SegmentInfos();
+          infos.read(directory, segmentFileName);
+        }
+        DirectoryIndexReader newReader = doReopenOrClone(infos, doClone);
         
         if (DirectoryIndexReader.this != newReader) {
           newReader.init(directory, infos, closeDirectory, readOnly);
@@ -148,7 +165,7 @@
   /**
    * Re-opens the index using the passed-in SegmentInfos 
    */
-  protected abstract DirectoryIndexReader doReopen(SegmentInfos infos) throws CorruptIndexException, IOException;
+  protected abstract DirectoryIndexReader doReopenOrClone(SegmentInfos infos, boolean doClone) throws CorruptIndexException, IOException;
   
   public void setDeletionPolicy(IndexDeletionPolicy deletionPolicy) {
     this.deletionPolicy = deletionPolicy;
Index: java/org/apache/lucene/index/FieldsReader.java
===================================================================
--- java/org/apache/lucene/index/FieldsReader.java	(revision 695616)
+++ java/org/apache/lucene/index/FieldsReader.java	(working copy)
@@ -49,6 +49,7 @@
   private final IndexInput fieldsStream;
 
   private final IndexInput indexStream;
+  private final IndexInput cloneableIndexStream;
   private int numTotalDocs;
   private int size;
   private boolean closed;
@@ -76,8 +77,8 @@
       fieldInfos = fn;
 
       cloneableFieldsStream = d.openInput(segment + "." + IndexFileNames.FIELDS_EXTENSION, readBufferSize);
-      indexStream = d.openInput(segment + "." + IndexFileNames.FIELDS_INDEX_EXTENSION, readBufferSize);
-
+      cloneableIndexStream = d.openInput(segment + "." + IndexFileNames.FIELDS_INDEX_EXTENSION, readBufferSize);
+      indexStream = (IndexInput)cloneableIndexStream.clone();
       // First version of fdx did not include a format
       // header, but, the first int will always be 0 in that
       // case
@@ -131,6 +132,36 @@
   }
 
   /**
+   * For use by clone
+   * @param fieldInfos
+   * @param cloneableFieldsStream
+   * @param fieldsStream
+   * @param indexStream
+   * @param numTotalDocs
+   * @param size
+   * @param format
+   * @param formatSize
+   */
+  private FieldsReader(FieldInfos fieldInfos, IndexInput cloneableFieldsStream, 
+      IndexInput cloneableIndexStream, int numTotalDocs, int size, int format, int formatSize, int docStoreOffset) {
+    this.fieldInfos = fieldInfos;
+    this.cloneableFieldsStream = cloneableFieldsStream;
+    this.cloneableIndexStream = cloneableIndexStream;
+    this.fieldsStream = (IndexInput)cloneableFieldsStream.clone();
+    this.indexStream = (IndexInput)cloneableIndexStream.clone();
+    this.numTotalDocs = numTotalDocs;
+    this.size = size;
+    this.format = format;
+    this.formatSize = formatSize;
+    this.docStoreOffset = docStoreOffset;
+  }
+  
+  public Object clone() {
+    return new FieldsReader(fieldInfos, (IndexInput)cloneableFieldsStream.clone(), 
+                            (IndexInput)cloneableIndexStream.clone(), numTotalDocs, size, format, formatSize, docStoreOffset);
+  }
+  
+  /**
    * @throws AlreadyClosedException if this FieldsReader is closed
    */
   protected final void ensureOpen() throws AlreadyClosedException {
Index: java/org/apache/lucene/index/FilterIndexReader.java
===================================================================
--- java/org/apache/lucene/index/FilterIndexReader.java	(revision 695616)
+++ java/org/apache/lucene/index/FilterIndexReader.java	(working copy)
@@ -104,6 +104,10 @@
     this.in = in;
   }
 
+  public Object clone() {
+    return new FilterIndexReader(in);
+  }
+
   public Directory directory() {
     return in.directory();
   }
Index: java/org/apache/lucene/index/IndexReader.java
===================================================================
--- java/org/apache/lucene/index/IndexReader.java	(revision 695616)
+++ java/org/apache/lucene/index/IndexReader.java	(working copy)
@@ -67,7 +67,7 @@
 
  @version $Id$
 */
-public abstract class IndexReader {
+public abstract class IndexReader implements Cloneable {
 
   // NOTE: in 3.0 this will change to true
   final static boolean READ_ONLY_DEFAULT = false;
@@ -138,7 +138,7 @@
     ensureOpen();
     refCount++;
   }
-
+  
   /**
    * Expert: decreases the refCount of this IndexReader
    * instance.  If the refCount drops to 0, then pending
@@ -150,10 +150,20 @@
    * @see #incRef
    */
   public synchronized void decRef() throws IOException {
+    decRef(true);
+  }
+  
+  /**
+   * Decreases the refCount of this IndexReader instance. If the refCount drops
+   * to 0, then pending changes are committed to the index and this reader is closed.
+   * @param flush Whether or not to flush changes when the refCount is 1
+   * @throws IOException in case an IOException occurs in commit() or doClose()
+   */
+  protected synchronized void decRef(boolean flush) throws IOException {
     assert refCount > 0;
     ensureOpen();
     if (refCount == 1) {
-      commit();
+      if (flush) commit();
       doClose();
     }
     refCount--;
@@ -352,9 +362,21 @@
    * @throws IOException if there is a low-level IO error
    */  
   public synchronized IndexReader reopen() throws CorruptIndexException, IOException {
-    throw new UnsupportedOperationException("This reader does not support reopen().");
+    return doReopenOrClone(false);
   }
+  
+  public synchronized Object clone() {
+    try {
+      return doReopenOrClone(true);
+    } catch (Exception exception) {
+      throw new RuntimeException(exception);
+    }
+  }
 
+  protected IndexReader doReopenOrClone(boolean forced) throws CorruptIndexException, IOException {
+    throw new UnsupportedOperationException("This reader does not support reopen() or clone()");  
+  }
+
   /** 
    * Returns the directory associated with this index.  The Default 
    * implementation returns the directory specified by subclasses when 
@@ -940,8 +962,18 @@
    * @throws IOException if there is a low-level IO error
    */
   public final synchronized void close() throws IOException {
+    close(true);
+  }
+  
+  /**
+   * Closes files associated with this index.
+   * No other methods should be called after this has been called.
+   * @param flush Whether to flush changes to the directory
+   * @throws IOException if there is a low-level IO error
+   */
+  protected synchronized void close(boolean flush) throws IOException {
     if (!closed) {
-      decRef();
+      decRef(flush);
       closed = true;
     }
   }
@@ -1094,8 +1126,14 @@
    *  one commit point.  But if you're using a custom {@link
    *  IndexDeletionPolicy} then there could be many commits.
    *  Once you have a given commit, you can open a reader on
+<<<<<<< .mine
+   *  it by calling {@link IndexReader#open(Directory,
+   *  boolean, IndexDeletionPolicy,
+   *  IndexCommit)}.  There must be at least one commit in
+=======
    *  it by calling {@link IndexReader#open(IndexCommit)}
    *  There must be at least one commit in
+>>>>>>> .r695616
    *  the Directory, else this method throws {@link
    *  java.io.IOException}.  Note that if a commit is in
    *  progress while this method is running, that commit
Index: java/org/apache/lucene/index/MultiReader.java
===================================================================
--- java/org/apache/lucene/index/MultiReader.java	(revision 695616)
+++ java/org/apache/lucene/index/MultiReader.java	(working copy)
@@ -89,7 +89,7 @@
   }
 
   /**
-   * Tries to reopen the subreaders.
+   * Tries to reopen or clone the subreaders.
    * <br>
    * If one or more subreaders could be re-opened (i. e. subReader.reopen() 
    * returned a new instance != subReader), then a new MultiReader instance 
@@ -107,7 +107,7 @@
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error 
    */
-  public IndexReader reopen() throws CorruptIndexException, IOException {
+  protected IndexReader doReopenOrClone(boolean doClone) throws CorruptIndexException, IOException {
     ensureOpen();
     
     boolean reopened = false;
@@ -117,7 +117,10 @@
     boolean success = false;
     try {
       for (int i = 0; i < subReaders.length; i++) {
-        newSubReaders[i] = subReaders[i].reopen();
+        if (doClone)
+          newSubReaders[i] = (IndexReader) subReaders[i].clone();
+        else 
+          newSubReaders[i] = subReaders[i].reopen();
         // if at least one of the subreaders was updated we remember that
         // and return a new MultiReader
         if (newSubReaders[i] != subReaders[i]) {
Index: java/org/apache/lucene/index/MultiSegmentReader.java
===================================================================
--- java/org/apache/lucene/index/MultiSegmentReader.java	(revision 695616)
+++ java/org/apache/lucene/index/MultiSegmentReader.java	(working copy)
@@ -71,9 +71,8 @@
   }
 
   /** This contructor is only used for {@link #reopen()} */
-  MultiSegmentReader(Directory directory, SegmentInfos infos, boolean closeDirectory, SegmentReader[] oldReaders, int[] oldStarts, Map oldNormsCache, boolean readOnly) throws IOException {
+  MultiSegmentReader(Directory directory, SegmentInfos infos, boolean closeDirectory, SegmentReader[] oldReaders, int[] oldStarts, Map oldNormsCache, boolean readOnly, boolean doClone) throws IOException {
     super(directory, infos, closeDirectory, readOnly);
-
     // we put the old SegmentReaders in a map, that allows us
     // to lookup a reader using its segment name
     Map segmentReaders = new HashMap();
@@ -109,7 +108,7 @@
           // this is a new reader; in case we hit an exception we can close it safely
           newReader = SegmentReader.get(readOnly, infos.info(i));
         } else {
-          newReader = (SegmentReader) newReaders[i].reopenSegment(infos.info(i));
+          newReader = (SegmentReader) newReaders[i].reopenSegment(infos.info(i), doClone);
         }
         if (newReader == newReaders[i]) {
           // this reader will be shared between the old and the new one,
@@ -195,15 +194,15 @@
     starts[subReaders.length] = maxDoc;
   }
 
-  protected synchronized DirectoryIndexReader doReopen(SegmentInfos infos) throws CorruptIndexException, IOException {
+  protected synchronized DirectoryIndexReader doReopenOrClone(SegmentInfos infos, boolean doClone) throws CorruptIndexException, IOException {
     if (infos.size() == 1) {
       // The index has only one segment now, so we can't refresh the MultiSegmentReader.
       // Return a new [ReadOnly]SegmentReader instead
       return SegmentReader.get(readOnly, infos, infos.info(0), false);
     } else if (readOnly) {
-      return new ReadOnlyMultiSegmentReader(directory, infos, closeDirectory, subReaders, starts, normsCache);
+      return new ReadOnlyMultiSegmentReader(directory, infos, closeDirectory, subReaders, starts, normsCache, doClone);
     } else {
-      return new MultiSegmentReader(directory, infos, closeDirectory, subReaders, starts, normsCache, false);
+      return new MultiSegmentReader(directory, infos, closeDirectory, subReaders, starts, normsCache, false, doClone);
     }            
   }
 
Index: java/org/apache/lucene/index/ParallelReader.java
===================================================================
--- java/org/apache/lucene/index/ParallelReader.java	(revision 695616)
+++ java/org/apache/lucene/index/ParallelReader.java	(working copy)
@@ -142,7 +142,7 @@
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error 
    */
-  public IndexReader reopen() throws CorruptIndexException, IOException {
+  protected IndexReader doReopenOrClone(boolean doClone) throws CorruptIndexException, IOException {
     ensureOpen();
     
     boolean reopened = false;
@@ -155,7 +155,11 @@
     
       for (int i = 0; i < readers.size(); i++) {
         IndexReader oldReader = (IndexReader) readers.get(i);
-        IndexReader newReader = oldReader.reopen();
+        final IndexReader newReader;
+        if (doClone)
+          newReader = (IndexReader) oldReader.clone();
+        else
+          newReader = oldReader.reopen();
         newReaders.add(newReader);
         // if at least one of the subreaders was updated we remember that
         // and return a new MultiReader
Index: java/org/apache/lucene/index/ReadOnlyMultiSegmentReader.java
===================================================================
--- java/org/apache/lucene/index/ReadOnlyMultiSegmentReader.java	(revision 695616)
+++ java/org/apache/lucene/index/ReadOnlyMultiSegmentReader.java	(working copy)
@@ -27,8 +27,8 @@
     super(directory, sis, closeDirectory, true);
   }
 
-  ReadOnlyMultiSegmentReader(Directory directory, SegmentInfos infos, boolean closeDirectory, SegmentReader[] oldReaders, int[] oldStarts, Map oldNormsCache) throws IOException {
-    super(directory, infos, closeDirectory, oldReaders, oldStarts, oldNormsCache, true);
+  ReadOnlyMultiSegmentReader(Directory directory, SegmentInfos infos, boolean closeDirectory, SegmentReader[] oldReaders, int[] oldStarts, Map oldNormsCache, boolean readOnly) throws IOException {
+    super(directory, infos, closeDirectory, oldReaders, oldStarts, oldNormsCache, true, readOnly);
   }
 
   protected void acquireWriteLock() {
Index: java/org/apache/lucene/index/SegmentReader.java
===================================================================
--- java/org/apache/lucene/index/SegmentReader.java	(revision 695616)
+++ java/org/apache/lucene/index/SegmentReader.java	(working copy)
@@ -42,34 +42,36 @@
  * @version $Id$
  */
 class SegmentReader extends DirectoryIndexReader {
-  private String segment;
-  private SegmentInfo si;
-  private int readBufferSize;
+  protected String segment;
+  protected SegmentInfo si;
+  protected int readBufferSize;
 
   FieldInfos fieldInfos;
-  private FieldsReader fieldsReader;
+  protected FieldsReader fieldsReaderCloneable;
 
   TermInfosReader tis;
   TermVectorsReader termVectorsReaderOrig = null;
-  CloseableThreadLocal termVectorsLocal = new CloseableThreadLocal();
-
+  TermVectorsReaderLocal termVectorsLocal;
+  FieldsReaderLocal fieldsReaderLocal;
+ 
   BitVector deletedDocs = null;
-  private boolean deletedDocsDirty = false;
-  private boolean normsDirty = false;
-  private boolean undeleteAll = false;
-  private int pendingDeleteCount;
+  CopyOnWriteRef deletedDocsCopyOnWriteRef;
+  protected boolean deletedDocsDirty = false;
+  protected boolean normsDirty = false;
+  protected boolean undeleteAll = false;
+  protected int pendingDeleteCount;
 
-  private boolean rollbackDeletedDocsDirty = false;
-  private boolean rollbackNormsDirty = false;
-  private boolean rollbackUndeleteAll = false;
-  private int rollbackPendingDeleteCount;
+  protected boolean rollbackDeletedDocsDirty = false;
+  protected boolean rollbackNormsDirty = false;
+  protected boolean rollbackUndeleteAll = false;
+  protected int rollbackPendingDeleteCount;
   private boolean readOnly;
 
   IndexInput freqStream;
   IndexInput proxStream;
 
   // optionally used for the .nrm file shared by multiple norms
-  private IndexInput singleNormStream;
+  protected IndexInput singleNormStream;
 
   // Compound File Reader when based on a compound file segment
   CompoundFileReader cfsReader = null;
@@ -77,9 +79,31 @@
   
   // indicates the SegmentReader with which the resources are being shared,
   // in case this is a re-opened reader
-  private SegmentReader referencedSegmentReader = null;
+  protected SegmentReader referencedSegmentReader = null;
   
-  private class Norm {
+  protected class CopyOnWriteRef {
+    private int refCount = 0;
+    
+    public synchronized int refCount() {
+      return refCount;
+    }
+    
+    public synchronized void incRef() {
+      refCount++;
+    }
+    
+    public synchronized void decRef() {
+      refCount--;
+    }
+  }
+  
+  protected byte[] cloneNormBytes(byte[] bytes) {
+    byte[] cloneBytes = new byte[bytes.length];
+    System.arraycopy(bytes, 0, cloneBytes, 0, bytes.length);
+    return cloneBytes;
+  }
+  
+  protected class Norm implements Cloneable {
     volatile int refCount;
     boolean useSingleNormStream;
     
@@ -97,6 +121,28 @@
 
     }
     
+    public void copyBytes() {
+      assert copyOnWriteRef != null;
+      if (bytes != null) {
+        byte[] cloneBytes = cloneNormBytes(bytes);
+        bytes = cloneBytes;
+      }
+    }
+    
+    public Norm cloneRefBytes() {
+      Norm clone = new Norm(in, useSingleNormStream, number, normSeek);
+      clone.bytes = bytes;
+      return clone;
+    }
+    
+    public Object clone() {
+      Norm clone = new Norm(in, useSingleNormStream, number, normSeek);
+      if (bytes != null) {
+        clone.bytes = cloneNormBytes(bytes);
+      }
+      return clone;
+    }
+    
     public Norm(IndexInput in, boolean useSingleNormStream, int number, long normSeek)
     {
       refCount = 1;
@@ -106,14 +152,15 @@
       this.useSingleNormStream = useSingleNormStream;
     }
 
-    private IndexInput in;
-    private byte[] bytes;
-    private boolean dirty;
-    private int number;
-    private long normSeek;
-    private boolean rollbackDirty;
+    protected IndexInput in;
+    byte[] bytes;
+    protected boolean dirty;
+    protected int number;
+    protected long normSeek;
+    protected boolean rollbackDirty;
+    protected CopyOnWriteRef copyOnWriteRef;
 
-    private void reWrite(SegmentInfo si) throws IOException {
+    protected void reWrite(SegmentInfo si) throws IOException {
       // NOTE: norms are re-written in regular directory, not cfs
       si.advanceNormGen(this.number);
       IndexOutput out = directory().createOutput(si.getNormFileName(this.number));
@@ -129,7 +176,7 @@
      * It is still valid to access all other norm properties after close is called.
      * @throws IOException
      */
-    private synchronized void close() throws IOException {
+    protected synchronized void close() throws IOException {
       if (in != null && !useSingleNormStream) {
         in.close();
       }
@@ -307,6 +354,10 @@
     return instance;
   }
 
+  /** So a subclass can do its own initializing. */
+  protected void doInitialize() {
+  }
+  
   private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {
     segment = si.name;
     this.si = si;
@@ -354,12 +405,12 @@
         fieldsSegment = segment;
 
       if (doOpenStores) {
-        fieldsReader = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,
+        fieldsReaderCloneable = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,
                                         si.getDocStoreOffset(), si.docCount);
-
+        fieldsReaderLocal = new FieldsReaderLocal(fieldsReaderCloneable);
         // Verify two sources of "maxDoc" agree:
-        if (si.getDocStoreOffset() == -1 && fieldsReader.size() != si.docCount) {
-          throw new CorruptIndexException("doc counts differ for segment " + si.name + ": fieldsReader shows " + fieldsReader.size() + " but segmentInfo shows " + si.docCount);
+        if (si.getDocStoreOffset() == -1 && fieldsReaderCloneable.size() != si.docCount) {
+          throw new CorruptIndexException("doc counts differ for segment " + si.name + ": fieldsReader shows " + fieldsReaderCloneable.size() + " but segmentInfo shows " + si.docCount);
         }
       }
 
@@ -381,7 +432,9 @@
         else
           vectorsSegment = segment;
         termVectorsReaderOrig = new TermVectorsReader(storeDir, vectorsSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);
+        termVectorsLocal = new TermVectorsReaderLocal(termVectorsReaderOrig);
       }
+      doInitialize();
       success = true;
     } finally {
 
@@ -413,13 +466,13 @@
       assert si.getDelCount() == 0;
   }
   
-  protected synchronized DirectoryIndexReader doReopen(SegmentInfos infos) throws CorruptIndexException, IOException {
+  protected synchronized DirectoryIndexReader doReopenOrClone(SegmentInfos infos, boolean doClone) throws CorruptIndexException, IOException {
     DirectoryIndexReader newReader;
-    
+
     if (infos.size() == 1) {
       SegmentInfo si = infos.info(0);
       if (segment.equals(si.name) && si.getUseCompoundFile() == SegmentReader.this.si.getUseCompoundFile()) {
-        newReader = reopenSegment(si);
+        newReader = reopenSegment(si, doClone);
       } else { 
         // segment not referenced anymore, reopen not possible
         // or segment format changed
@@ -427,15 +480,26 @@
       }
     } else {
       if (readOnly)
-        return new ReadOnlyMultiSegmentReader(directory, infos, closeDirectory, new SegmentReader[] {this}, null, null);
+        return new ReadOnlyMultiSegmentReader(directory, infos, closeDirectory, new SegmentReader[] {this}, null, null, doClone);
       else
-        return new MultiSegmentReader(directory, infos, closeDirectory, new SegmentReader[] {this}, null, null, false);
+        return new MultiSegmentReader(directory, infos, closeDirectory, new SegmentReader[] {this}, null, null, false, doClone);
     }
     
     return newReader;
   }
   
-  synchronized SegmentReader reopenSegment(SegmentInfo si) throws CorruptIndexException, IOException {
+  /**
+   * Clone the deleted docs.  The default implementation clones
+   * using the deletedDocs clone method.
+   * @param deletedDocs
+   * @return
+   */
+  protected BitVector cloneDeletedDocs(BitVector deletedDocs) {
+    if (this.deletedDocs == null) return null;
+    return (BitVector)deletedDocs.clone();
+  }
+  
+  synchronized SegmentReader reopenSegment(SegmentInfo si, boolean doClone) throws CorruptIndexException, IOException {
     boolean deletionsUpToDate = (this.si.hasDeletions() == si.hasDeletions()) 
                                   && (!si.hasDeletions() || this.si.getDelFileName().equals(si.getDelFileName()));
     boolean normsUpToDate = true;
@@ -451,18 +515,20 @@
       }
     }
 
-    if (normsUpToDate && deletionsUpToDate) {
+    if ((normsUpToDate && deletionsUpToDate) && !doClone) {
       return this;
     }    
     
-
-      // clone reader
     SegmentReader clone;
-    if (readOnly) 
-      clone = new ReadOnlySegmentReader();
-    else
-      clone = new SegmentReader();
-
+    try {
+      if (readOnly) {
+        clone = (SegmentReader)READONLY_IMPL.newInstance();
+      } else {
+        clone = (SegmentReader)IMPL.newInstance();
+      }
+    } catch (Exception e) {
+      throw new RuntimeException("cannot load SegmentReader class: " + e, e);
+    }
     boolean success = false;
     try {
       clone.readOnly = readOnly;
@@ -472,13 +538,14 @@
       clone.readBufferSize = readBufferSize;
       clone.cfsReader = cfsReader;
       clone.storeCFSReader = storeCFSReader;
-  
+      clone.fieldsReaderCloneable = fieldsReaderCloneable;
       clone.fieldInfos = fieldInfos;
       clone.tis = tis;
       clone.freqStream = freqStream;
       clone.proxStream = proxStream;
       clone.termVectorsReaderOrig = termVectorsReaderOrig;
-  
+      clone.fieldsReaderLocal = fieldsReaderLocal;
+      clone.termVectorsLocal = termVectorsLocal;
       
       // we have to open a new FieldsReader, because it is not thread-safe
       // and can thus not be shared among multiple SegmentReaders
@@ -498,14 +565,8 @@
           storeDir = cfsReader;
         }
       }
-  
-      if (fieldsReader != null) {
-        clone.fieldsReader = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,
-                                        si.getDocStoreOffset(), si.docCount);
-      }
       
-      
-      if (!deletionsUpToDate) {
+      if (!deletionsUpToDate && !doClone) {
         // load deleted docs
         clone.deletedDocs = null;
         clone.loadDeletedDocs();
@@ -513,7 +574,7 @@
         clone.deletedDocs = this.deletedDocs;
       }
   
-      clone.norms = new HashMap();
+      //clone.norms = new HashMap(); already performed at variable declaration
       if (!normsUpToDate) {
         // load norms
         for (int i = 0; i < fieldNormsChanged.length; i++) {
@@ -553,8 +614,40 @@
             }
           }
         }  
-      }    
-  
+      } 
+      // Make norms copyOnWrite and create a new norm object so 
+      // there is no need to have synchronized norms map.
+      // There is a reference to the byte array.  If this reader
+      // or the cloned reader wants to write to the byte array they now 
+      // have to make a copy first.  If this reader 
+      // did not make a copy on write then the updates would be seen
+      // by the cloned reader which is not desired behavior for a clone
+      // which should be a copy at the point clone was called
+      if (doClone) {
+        if (clone.deletedDocs != null) {
+          clone.pendingDeleteCount = pendingDeleteCount;
+          if (deletedDocsCopyOnWriteRef == null) {
+            deletedDocsCopyOnWriteRef = new CopyOnWriteRef();
+            deletedDocsCopyOnWriteRef.incRef();
+          }
+          deletedDocsCopyOnWriteRef.incRef();
+          clone.deletedDocsCopyOnWriteRef = deletedDocsCopyOnWriteRef;
+        }
+        Iterator iterator = clone.norms.entrySet().iterator();
+        while (iterator.hasNext()) {
+          Map.Entry entry = (Map.Entry)iterator.next();
+          String field = (String)entry.getKey();
+          Norm norm = (Norm)entry.getValue();
+          if (norm.copyOnWriteRef == null) {
+            norm.copyOnWriteRef = new CopyOnWriteRef();
+            norm.copyOnWriteRef.incRef();
+          }
+          Norm clonedNorm = norm.cloneRefBytes();
+          norm.copyOnWriteRef.incRef();
+          clonedNorm.copyOnWriteRef = norm.copyOnWriteRef;
+          clone.norms.put(field, clonedNorm); // replace norm object with cloneRefBytes
+        }
+      }
       success = true;
     } finally {
       if (this.referencedSegmentReader != null) {
@@ -606,19 +699,16 @@
         }
       }
     }
+    pendingDeleteCount = 0;
     deletedDocsDirty = false;
     normsDirty = false;
     undeleteAll = false;
   }
 
-  FieldsReader getFieldsReader() {
-    return fieldsReader;
-  }
-
   protected void doClose() throws IOException {
     boolean hasReferencedReader = (referencedSegmentReader != null);
 
-    termVectorsLocal.close();
+    if (termVectorsLocal != null) termVectorsLocal.close();
 
     if (hasReferencedReader) {
       referencedSegmentReader.decRefReaderNotNorms();
@@ -636,17 +726,14 @@
       singleNormStream = null;
     }
     
-    // re-opened SegmentReaders have their own instance of FieldsReader
-    if (fieldsReader != null) {
-      fieldsReader.close();
-    }
-
     if (!hasReferencedReader) { 
       // close everything, nothing is shared anymore with other readers
       if (tis != null) {
         tis.close();
       }
-  
+      if (fieldsReaderCloneable != null) {
+        fieldsReaderCloneable.close();
+      }
       if (freqStream != null)
         freqStream.close();
       if (proxStream != null)
@@ -685,6 +772,16 @@
   }
 
   protected void doDelete(int docNum) {
+    if (deletedDocsCopyOnWriteRef != null && deletedDocs != null) {
+      assert deletedDocsCopyOnWriteRef.refCount() > 0;
+      if (deletedDocsCopyOnWriteRef.refCount() > 1) {
+        deletedDocs = cloneDeletedDocs(deletedDocs);
+        deletedDocsCopyOnWriteRef.decRef();
+      } else {
+        // just use the deletedDocs we have and remove this copyOnWriteRef
+        deletedDocsCopyOnWriteRef = null;
+      }
+    }
     if (deletedDocs == null)
       deletedDocs = new BitVector(maxDoc());
     deletedDocsDirty = true;
@@ -721,11 +818,12 @@
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
    */
-  public synchronized Document document(int n, FieldSelector fieldSelector) throws CorruptIndexException, IOException {
+  public Document document(int n, FieldSelector fieldSelector) throws CorruptIndexException, IOException {
     ensureOpen();
     if (isDeleted(n))
       throw new IllegalArgumentException
               ("attempt to access a deleted document");
+    FieldsReader fieldsReader = getFieldsReader();
     return fieldsReader.doc(n, fieldSelector);
   }
 
@@ -871,7 +969,18 @@
     Norm norm = (Norm) norms.get(field);
     if (norm == null)                             // not an indexed field
       return;
-
+    synchronized (norm) {
+      if (norm.copyOnWriteRef != null) {
+        assert norm.copyOnWriteRef.refCount() > 0;
+        if (norm.copyOnWriteRef.refCount() == 1) {
+          // use the bytes as is, no more references to them
+          norm.copyOnWriteRef = null;
+        } else if (norm.copyOnWriteRef.refCount() > 1) {
+          norm.copyBytes();
+          norm.copyOnWriteRef.decRef();
+        } 
+      }
+    }
     norm.dirty = true;                            // mark it dirty
     normsDirty = true;
 
@@ -973,23 +1082,49 @@
   }
 
   /**
-   * Create a clone from the initial TermVectorsReader and store it in the ThreadLocal.
-   * @return TermVectorsReader
+   * Create a clone from the initial FieldsReader and store it in the ThreadLocal.
+   * @return FieldsReader
    */
-  private TermVectorsReader getTermVectorsReader() {
-    assert termVectorsReaderOrig != null;
-    TermVectorsReader tvReader = (TermVectorsReader)termVectorsLocal.get();
-    if (tvReader == null) {
+  FieldsReader getFieldsReader() {
+    return (FieldsReader) fieldsReaderLocal.get();
+  }
+  
+  public class FieldsReaderLocal extends CloseableThreadLocal {
+    private FieldsReader fieldsReaderCloneable;
+    
+    public FieldsReaderLocal(FieldsReader fieldsReaderCloneable) {
+      this.fieldsReaderCloneable = fieldsReaderCloneable;
+    }
+    
+    protected Object initialValue() {
+      return fieldsReaderCloneable.clone();
+    }
+  }
+  
+  public static class TermVectorsReaderLocal extends CloseableThreadLocal {
+    private TermVectorsReader termVectorsReaderOrig;
+    
+    public TermVectorsReaderLocal(TermVectorsReader termVectorsReaderOrig) {
+      this.termVectorsReaderOrig = termVectorsReaderOrig;
+    }
+    
+    protected Object initialValue() {
       try {
-        tvReader = (TermVectorsReader)termVectorsReaderOrig.clone();
-      } catch (CloneNotSupportedException cnse) {
+        return termVectorsReaderOrig.clone();
+      } catch (Exception ex) {
         return null;
       }
-      termVectorsLocal.set(tvReader);
     }
-    return tvReader;
   }
   
+  /**
+   * Create a clone from the initial TermVectorsReader and store it in the ThreadLocal.
+   * @return TermVectorsReader
+   */
+  private TermVectorsReader getTermVectorsReader() {
+    return (TermVectorsReader) termVectorsLocal.get();
+  }
+  
   /** Return a term frequency vector for the specified document and field. The
    *  vector returned contains term numbers and frequencies for all terms in
    *  the specified field of this document, if the field had storeTermVector
@@ -1022,8 +1157,6 @@
     {
       return;
     }
-
-
     termVectorsReader.get(docNumber, field, mapper);
   }
 
Index: java/org/apache/lucene/util/BitVector.java
===================================================================
--- java/org/apache/lucene/util/BitVector.java	(revision 695616)
+++ java/org/apache/lucene/util/BitVector.java	(working copy)
@@ -35,7 +35,7 @@
 
   @version $Id$
   */
-public final class BitVector {
+public final class BitVector implements Cloneable {
 
   private byte[] bits;
   private int size;
@@ -46,7 +46,30 @@
     size = n;
     bits = new byte[(size >> 3) + 1];
   }
-
+  
+  private BitVector() {}
+  
+  public BitVector(byte[] bits, int size, int count) {
+    this.bits = bits;
+    this.size = size;
+    this.count = count;
+  }
+  
+  public Object clone() {
+    BitVector clone = new BitVector();
+    clone.count = count;
+    clone.size = size;
+    if (bits != null) {
+      clone.bits = new byte[bits.length];
+      System.arraycopy(bits, 0, clone.bits, 0, bits.length);
+    }
+    return clone;
+  }
+  
+  public byte[] getBits() {
+    return bits;
+  }
+  
   /** Sets the value of <code>bit</code> to one. */
   public final void set(int bit) {
     if (bit >= size) {
Index: test/org/apache/lucene/index/TestIndexReaderClone.java
===================================================================
--- test/org/apache/lucene/index/TestIndexReaderClone.java	(revision 0)
+++ test/org/apache/lucene/index/TestIndexReaderClone.java	(revision 0)
@@ -0,0 +1,243 @@
+package org.apache.lucene.index;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Random;
+import java.util.Set;
+
+import org.apache.lucene.index.SegmentReader.Norm;
+import org.apache.lucene.search.Similarity;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.NoLockFactory;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.BitVector;
+import org.apache.lucene.util.LuceneTestCase;
+
+public class TestIndexReaderClone extends LuceneTestCase {
+  static Random random = new Random(System.currentTimeMillis());
+
+  public void testNorms() throws Exception {
+    Map normChanges1 = new HashMap(); // key doc, value byte
+    Map normChanges2 = new HashMap(); // key doc, value byte
+    final Directory directory1 = new RAMDirectory();
+    directory1.setLockFactory(new NoLockFactory());
+    TestIndexReaderReopen.createIndex(directory1, false);
+    IndexReader index1 = IndexReader.open(directory1);
+    for (int x = 0; x < 2; x++) {
+      changeNormsRandom(index1, "field1", normChanges1);
+    }
+    index1.flush();
+    IndexReader index2 = (IndexReader) index1.clone();
+
+    TestIndexReader.assertIndexEquals(index1, index2);
+
+    SegmentReader segmentReader1 = (SegmentReader) index1;
+    SegmentReader segmentReader2 = (SegmentReader) index2;
+    
+    // check to see if norm changes happened to both indexes
+    assertNormChanges(segmentReader1, "field1", normChanges1, true);
+    assertNormChanges(segmentReader2, "field1", normChanges1, true);
+    
+    // change norms in index2
+    for (int x = 0; x < 5; x++) {
+      changeNormsRandom(index2, "field1", normChanges2);
+    }
+    // check to see if changes happened
+    assertNormChanges(segmentReader2, "field1", normChanges2, true);
+    
+    // change norms in index1 the norms changed for index2
+    changeNorms(segmentReader1, "field1", normChanges2.keySet(), segmentReader2);
+    // changes should not be reflected in index1
+    assertNormChanges(segmentReader1, "field1", normChanges2, false, segmentReader2);
+    // should be no CopyOnWriteRef in norms that were modified
+    // because both readers have made modifications to the norms
+    assertNormCopyOnWriteRef(segmentReader1, "field1", false);
+  }
+
+  protected void changeNorms(IndexReader reader, String field, Set set, IndexReader other) throws Exception {
+    byte[] norms = reader.norms(field);
+    byte[] otherNorms = other.norms(field);
+    Iterator i = set.iterator();
+    while (i.hasNext()) {
+      Integer doc = (Integer) i.next();
+      while (true) {
+        float normValue = random.nextFloat();
+        byte normByte = Similarity.encodeNorm(normValue);
+        if (norms[doc.intValue()] != normByte && norms[doc.intValue()] != normByte) {
+          reader.setNorm(doc.intValue(), field, normByte);
+          break;
+        }
+      }
+    }
+  }
+
+  protected void assertNormCopyOnWriteRef(SegmentReader segmentReader, String field, boolean copyOnWriteRef) {
+    Norm norm = (Norm) segmentReader.norms.get(field);
+    if (copyOnWriteRef) {
+      assertTrue(norm.copyOnWriteRef != null);
+    } else {
+      assertTrue(norm.copyOnWriteRef == null);
+    }
+  }
+  
+  protected void assertNormChanges(SegmentReader segmentReader, String field, Map normChanges, boolean isEqual) {
+    assertNormChanges(segmentReader, field, normChanges, isEqual, null);
+  }
+    
+  protected void assertNormChanges(SegmentReader segmentReader, String field, Map normChanges, boolean isEqual, SegmentReader other) {
+    Norm norm = (Norm) segmentReader.norms.get(field);
+    if (other != null) {
+      Norm otherNorm = (Norm)other.norms.get(field);
+      boolean bytesEquals = Arrays.equals(norm.bytes, otherNorm.bytes);
+      if (isEqual) {
+        assertTrue(bytesEquals);
+      } else {
+        if (bytesEquals) {
+          assertTrue(!bytesEquals);
+        }
+      }
+    }
+    Iterator iterator = normChanges.entrySet().iterator();
+    while (iterator.hasNext()) {
+      Map.Entry entry = (Map.Entry) iterator.next();
+      Integer doc = (Integer) entry.getKey();
+      Byte value = (Byte) entry.getValue();
+      if (isEqual)
+        assertEquals(value.byteValue(), norm.bytes[doc.intValue()]);
+      else {
+        byte b1 = value.byteValue();
+        byte b2 = norm.bytes[doc.intValue()];
+        if (b1 == b2) {
+          System.out.println("same "+value.byteValue()+" "+norm.bytes[doc.intValue()]);
+          if (other != null) {
+            Norm otherNorm = (Norm)other.norms.get(field);
+            byte otherByte = otherNorm.bytes[doc.intValue()];
+            if (otherByte == b2) {
+              assertTrue(value.byteValue() != otherByte);
+            }
+          }
+          assertTrue(value.byteValue() != norm.bytes[doc.intValue()]);
+        }
+      }
+    }
+  }
+  
+  // change norms that have not been changed in normChanges
+  protected void changeNormsRandom(IndexReader reader, String field, Map normChanges) throws Exception {
+    int doc = random.nextInt(reader.maxDoc());
+    if (!normChanges.containsKey(new Integer(doc))) {
+      float normValue = random.nextFloat();
+      byte normByte = Similarity.encodeNorm(normValue);
+      byte[] norms = reader.norms(field);
+      if (norms[doc] != normByte) {
+        reader.setNorm(doc, field, normByte);
+        normChanges.put(new Integer(doc), new Byte(normByte));
+      } else {
+        changeNormsRandom(reader, field, normChanges);
+      }
+    } else {
+      changeNormsRandom(reader, field, normChanges);
+    }
+  }
+
+  public void testDeletesMultiClone() throws Exception {
+    final Directory directory1 = new RAMDirectory();
+    directory1.setLockFactory(new NoLockFactory());
+    TestIndexReaderReopen.createIndex(directory1, false);
+    
+    IndexReader index1 = IndexReader.open(directory1);
+    
+    Set deleted1 = new HashSet();
+    for (int x = 0; x < 3; x++) {
+      deleteRandom(index1, deleted1, null);
+    }
+    index1.flush();
+    
+    IndexReader index2 = (IndexReader) index1.clone();
+    IndexReader index3 = (IndexReader) index1.clone();
+    TestIndexReader.assertIndexEquals(index1, index2);
+    TestIndexReader.assertIndexEquals(index1, index3);
+
+    SegmentReader segmentReader1 = (SegmentReader) index1;
+    SegmentReader segmentReader2 = (SegmentReader) index2;
+    SegmentReader segmentReader3 = (SegmentReader) index3;
+
+    assertTrue(segmentReader1.deletedDocsCopyOnWriteRef == segmentReader2.deletedDocsCopyOnWriteRef);
+    assertTrue(segmentReader2.deletedDocsCopyOnWriteRef == segmentReader3.deletedDocsCopyOnWriteRef);
+    assertEquals(segmentReader1.deletedDocsCopyOnWriteRef.refCount(), 3);
+    assertEquals(segmentReader2.deletedDocsCopyOnWriteRef.refCount(), 3);
+    assertEquals(segmentReader3.deletedDocsCopyOnWriteRef.refCount(), 3);
+  }
+
+  public void testDeletes() throws Exception {
+    final Directory directory1 = new RAMDirectory();
+    directory1.setLockFactory(new NoLockFactory());
+    TestIndexReaderReopen.createIndex(directory1, false);
+    Set deleted1 = new HashSet();
+
+    IndexReader index1 = IndexReader.open(directory1);
+    for (int x = 0; x < 3; x++) {
+      deleteRandom(index1, deleted1, null);
+    }
+    index1.flush();
+    IndexReader index2 = (IndexReader) index1.clone();
+    TestIndexReader.assertIndexEquals(index1, index2);
+    Set deleted2 = new HashSet();
+    for (int x = 0; x < 2; x++) {
+      deleteRandom(index2, deleted2, deleted1);
+    }
+
+    SegmentReader segmentReader1 = (SegmentReader) index1;
+    SegmentReader segmentReader2 = (SegmentReader) index2;
+
+    assertEquals(segmentReader1.deletedDocsCopyOnWriteRef.refCount(), 1);
+    assertNull(segmentReader2.deletedDocsCopyOnWriteRef);
+    deleteRandom(index1, new HashSet(), deleted1);
+    assertNull(segmentReader1.deletedDocsCopyOnWriteRef);
+
+    BitVector deletedDocs1 = segmentReader1.deletedDocs;
+    BitVector deletedDocs2 = segmentReader2.deletedDocs;
+    assertTrue(deletedDocs1 != deletedDocs2);
+    assertTrue(segmentReader1.deletedDocs == deletedDocs1);
+
+    index2.flush();
+
+    assertHasDeletedDocs(index1, deleted1);
+    assertHasDeletedDocs(index2, deleted1);
+    assertHasDeletedDocs(index2, deleted2);
+    assertHasNotDeletedDocs(index1, deleted2);
+  }
+
+  public void assertHasNotDeletedDocs(IndexReader index, Set docs1) {
+    Iterator i = docs1.iterator();
+    while (i.hasNext()) {
+      Integer doc = (Integer) i.next();
+      assertFalse(index.isDeleted(doc.intValue()));
+    }
+  }
+
+  public void assertHasDeletedDocs(IndexReader index, Set docs1) {
+    Iterator i = docs1.iterator();
+    while (i.hasNext()) {
+      Integer doc = (Integer) i.next();
+      assertTrue(index.isDeleted(doc.intValue()));
+    }
+  }
+
+  public static void deleteRandom(IndexReader reader, Set deleted, Set exclude) throws IOException {
+    int doc = random.nextInt(reader.maxDoc());
+    if (exclude == null) {
+      reader.deleteDocument(doc);
+      deleted.add(new Integer(doc));
+    } else if (!exclude.contains(new Integer(doc))) {
+      reader.deleteDocument(doc);
+      deleted.add(new Integer(doc));
+    } else {
+      deleteRandom(reader, deleted, exclude);
+    }
+  }
+}
Index: test/org/apache/lucene/index/TestIndexReaderReopen.java
===================================================================
--- test/org/apache/lucene/index/TestIndexReaderReopen.java	(revision 695616)
+++ test/org/apache/lucene/index/TestIndexReaderReopen.java	(working copy)
@@ -853,7 +853,7 @@
     }
   }
   
-  private static void createIndex(Directory dir, boolean multiSegment) throws IOException {
+  public static void createIndex(Directory dir, boolean multiSegment) throws IOException {
     IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
     
     w.setMergePolicy(new LogDocMergePolicy());
