Index: contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java	(revision 636955)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java	(working copy)
@@ -26,6 +26,8 @@
 
 /**
  * Tokenizes the given token into n-grams of given size(s).
+ *
+ * This filter create n-grams from the begging edge or ending edge of a input token.
  * @author Otis Gospodnetic
  */
 public class EdgeNGramTokenFilter extends TokenFilter {
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.java	(revision 636955)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.java	(working copy)
@@ -25,6 +25,9 @@
 
 /**
  * Tokenizes the input from an edge into n-grams of given size(s).
+ *
+ * This tokenizer create n-grams from the begging edge or ending edge of a input token.
+ * MaxGram can't be larger than 1024 because of limitation.
  * @author Otis Gospodnetic
  * @author Adam Hiatt
  */
@@ -90,6 +93,10 @@
       throw new IllegalArgumentException("minGram must be greater than zero");
     }
 
+    if (maxGram > 1024) {
+      throw new IllegalArgumentException("maxGram must be less than 1024 or 1024");
+    }
+
     if (minGram > maxGram) {
       throw new IllegalArgumentException("minGram must not be greater than maxGram");
     }
