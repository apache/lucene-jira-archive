Index: ClassificationResult.java
===================================================================
--- ClassificationResult.java	(revision 1600966)
+++ ClassificationResult.java	(working copy)
@@ -17,18 +17,22 @@
 package org.apache.lucene.classification;
 
 /**
- * The result of a call to {@link Classifier#assignClass(String)} holding an assigned class of type <code>T</code> and a score.
- * @lucene.experimental
+ * The result of a call to {@link Classifier#assignClass(String)} holding an
+ * assigned class of type
+ * <code>T</code> and a score. @lucene.experimental
  */
-public class ClassificationResult<T> {
+public class ClassificationResult<T> implements Comparable{
 
   private final T assignedClass;
-  private final double score;
+    private double score;
 
   /**
    * Constructor
-   * @param assignedClass the class <code>T</code> assigned by a {@link Classifier}
-   * @param score the score for the assignedClass as a <code>double</code>
+     *
+     * @param assignedClass the class
+     * <code>T</code> assigned by a {@link Classifier}
+     * @param score the score for the assignedClass as a
+     * <code>double</code>
    */
   public ClassificationResult(T assignedClass, double score) {
     this.assignedClass = assignedClass;
@@ -37,7 +41,9 @@
 
   /**
    * retrieve the result class
-   * @return a <code>T</code> representing an assigned class
+     *
+     * @return a
+     * <code>T</code> representing an assigned class
    */
   public T getAssignedClass() {
     return assignedClass;
@@ -45,9 +51,24 @@
 
   /**
    * retrieve the result score
-   * @return a <code>double</code> representing a result score
+     *
+     * @return a
+     * <code>double</code> representing a result score
    */
   public double getScore() {
     return score;
   }
+
+    public void setScore(double score) {
+        this.score = score;
 }
+
+    @Override
+    public int compareTo(Object o) {
+        if(o instanceof ClassificationResult){
+            ClassificationResult<T> b = (ClassificationResult<T>) o;
+            return this.getScore() < b.getScore() ? 1 : this.getScore() > b.getScore() ? -1 : 0;
+        }
+        throw new UnsupportedOperationException("Not supported yet.");
+    }
+}
Index: KNearestNeighborClassifier.java
===================================================================
--- KNearestNeighborClassifier.java	(revision 1600966)
+++ KNearestNeighborClassifier.java	(working copy)
@@ -18,6 +18,7 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.mlt.MoreLikeThis;
 import org.apache.lucene.search.BooleanClause;
@@ -31,13 +32,16 @@
 
 import java.io.IOException;
 import java.io.StringReader;
-import java.util.HashMap;
-import java.util.Map;
+import java.util.*;
 
+import org.mockito.internal.listeners.CollectCreatedMocks;
+
 /**
- * A k-Nearest Neighbor classifier (see <code>http://en.wikipedia.org/wiki/K-nearest_neighbors</code>) based
- * on {@link MoreLikeThis}
+ * A k-Nearest Neighbor classifier (see
+ * <code>http://en.wikipedia.org/wiki/K-nearest_neighbors</code>) based on {@link MoreLikeThis}
  *
+ * This is an online classifier if you properly use the {@link IndexWriter#commit()}, or if you use near-realtime readers.
+ *
  * @lucene.experimental
  */
 public class KNearestNeighborClassifier implements Classifier<BytesRef> {
@@ -48,14 +52,14 @@
   private IndexSearcher indexSearcher;
   private final int k;
   private Query query;
-
   private int minDocsFreq;
   private int minTermFreq;
 
   /**
    * Create a {@link Classifier} using kNN algorithm
    *
-   * @param k the number of neighbors to analyze as an <code>int</code>
+     * @param k the number of neighbors to analyze as an
+     * <code>int</code>
    */
   public KNearestNeighborClassifier(int k) {
     this.k = k;
@@ -64,9 +68,12 @@
   /**
    * Create a {@link Classifier} using kNN algorithm
    *
-   * @param k           the number of neighbors to analyze as an <code>int</code>
-   * @param minDocsFreq the minimum number of docs frequency for MLT to be set with {@link MoreLikeThis#setMinDocFreq(int)}
-   * @param minTermFreq the minimum number of term frequency for MLT to be set with {@link MoreLikeThis#setMinTermFreq(int)}
+     * @param k the number of neighbors to analyze as an
+     * <code>int</code>
+     * @param minDocsFreq the minimum number of docs frequency for MLT to be set
+     * with {@link MoreLikeThis#setMinDocFreq(int)}
+     * @param minTermFreq the minimum number of term frequency for MLT to be set
+     * with {@link MoreLikeThis#setMinTermFreq(int)}
    */
   public KNearestNeighborClassifier(int k, int minDocsFreq, int minTermFreq) {
     this.k = k;
@@ -79,6 +86,13 @@
    */
   @Override
   public ClassificationResult<BytesRef> assignClass(String text) throws IOException {
+      List<ClassificationResult<BytesRef>> classificationResultList=assignClassNormalizedList(text);
+      if(!classificationResultList.isEmpty())
+        return classificationResultList.get(0);
+      return null;
+    }
+
+    public List<ClassificationResult<BytesRef>> assignClassNormalizedList(String text) throws IOException {
     if (mlt == null) {
       throw new IOException("You must first call Classifier#train");
     }
@@ -93,9 +107,10 @@
     }
     TopDocs topDocs = indexSearcher.search(mltQuery, k);
     return selectClassFromNeighbors(topDocs);
+
   }
 
-  private ClassificationResult<BytesRef> selectClassFromNeighbors(TopDocs topDocs) throws IOException {
+    private List<ClassificationResult<BytesRef>> selectClassFromNeighbors(TopDocs topDocs) throws IOException {
     // TODO : improve the nearest neighbor selection
     Map<BytesRef, Integer> classCounts = new HashMap<>();
     for (ScoreDoc scoreDoc : topDocs.scoreDocs) {
@@ -107,18 +122,16 @@
         classCounts.put(cl, 1);
       }
     }
-    double max = 0;
+        List<ClassificationResult<BytesRef>> returnList = new ArrayList<ClassificationResult<BytesRef>>();
     BytesRef assignedClass = new BytesRef();
     for (Map.Entry<BytesRef, Integer> entry : classCounts.entrySet()) {
       Integer count = entry.getValue();
-      if (count > max) {
-        max = count;
-        assignedClass = entry.getKey().clone();
+            returnList.add(new ClassificationResult<>(entry.getKey().clone(), count / (double) k));
+
       }
+        Collections.sort(returnList);
+        return returnList;
     }
-    double score = max / (double) k;
-    return new ClassificationResult<>(assignedClass, score);
-  }
 
   /**
    * {@inheritDoc}
Index: SimpleNaiveBayesClassifier.java
===================================================================
--- SimpleNaiveBayesClassifier.java	(revision 1600966)
+++ SimpleNaiveBayesClassifier.java	(working copy)
@@ -20,6 +20,7 @@
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.Terms;
@@ -34,37 +35,38 @@
 import org.apache.lucene.util.BytesRef;
 
 import java.io.IOException;
-import java.util.Collection;
-import java.util.LinkedList;
+import java.util.*;
 
 /**
- * A simplistic Lucene based NaiveBayes classifier, see <code>http://en.wikipedia.org/wiki/Naive_Bayes_classifier</code>
+ * A simplistic Lucene based NaiveBayes classifier, see
+ * <code>http://en.wikipedia.org/wiki/Naive_Bayes_classifier</code>
  *
+ * This is an online classifier if you properly use the {@link IndexWriter#commit()}, or if you use near-realtime readers.
+ * 
  * @lucene.experimental
  */
 public class SimpleNaiveBayesClassifier implements Classifier<BytesRef> {
 
-  private AtomicReader atomicReader;
+  public AtomicReader atomicReader;
   private String[] textFieldNames;
   private String classFieldName;
-  private int docsWithClassSize;
-  private Analyzer analyzer;
-  private IndexSearcher indexSearcher;
+  public Analyzer analyzer;
+  public IndexSearcher indexSearcher;
   private Query query;
 
   /**
-   * Creates a new NaiveBayes classifier.
-   * Note that you must call {@link #train(AtomicReader, String, String, Analyzer) train()} before you can
-   * classify any documents.
+   * Creates a new NaiveBayes classifier. Note that you must call
+   * {@link #train(AtomicReader, String, String, Analyzer) train()} before you
+   * can classify any documents.
    */
-  public SimpleNaiveBayesClassifier() {
-  }
+  public SimpleNaiveBayesClassifier() {}
 
   /**
    * {@inheritDoc}
    */
   @Override
-  public void train(AtomicReader atomicReader, String textFieldName, String classFieldName, Analyzer analyzer) throws IOException {
+  public void train(AtomicReader atomicReader, String textFieldName,
+      String classFieldName, Analyzer analyzer) throws IOException {
     train(atomicReader, textFieldName, classFieldName, analyzer, null);
   }
 
@@ -72,37 +74,38 @@
    * {@inheritDoc}
    */
   @Override
-  public void train(AtomicReader atomicReader, String textFieldName, String classFieldName, Analyzer analyzer, Query query)
-      throws IOException {
-    train(atomicReader, new String[]{textFieldName}, classFieldName, analyzer, query);
+  public void train(AtomicReader atomicReader, String textFieldName,
+      String classFieldName, Analyzer analyzer, Query query) throws IOException {
+    train(atomicReader, new String[] {textFieldName}, classFieldName, analyzer,
+        query);
   }
 
   /**
    * {@inheritDoc}
    */
   @Override
-  public void train(AtomicReader atomicReader, String[] textFieldNames, String classFieldName, Analyzer analyzer, Query query)
-      throws IOException {
+  public void train(AtomicReader atomicReader, String[] textFieldNames,
+      String classFieldName, Analyzer analyzer, Query query) throws IOException {
     this.atomicReader = atomicReader;
     this.indexSearcher = new IndexSearcher(this.atomicReader);
     this.textFieldNames = textFieldNames;
     this.classFieldName = classFieldName;
     this.analyzer = analyzer;
     this.query = query;
-    this.docsWithClassSize = countDocsWithClass();
   }
 
   private int countDocsWithClass() throws IOException {
-    int docCount = MultiFields.getTerms(this.atomicReader, this.classFieldName).getDocCount();
+    int docCount = MultiFields.getTerms(this.atomicReader, this.classFieldName)
+        .getDocCount();
     if (docCount == -1) { // in case codec doesn't support getDocCount
       TotalHitCountCollector totalHitCountCollector = new TotalHitCountCollector();
       BooleanQuery q = new BooleanQuery();
-      q.add(new BooleanClause(new WildcardQuery(new Term(classFieldName, String.valueOf(WildcardQuery.WILDCARD_STRING))), BooleanClause.Occur.MUST));
+      q.add(new BooleanClause(new WildcardQuery(new Term(classFieldName, String
+          .valueOf(WildcardQuery.WILDCARD_STRING))), BooleanClause.Occur.MUST));
       if (query != null) {
         q.add(query, BooleanClause.Occur.MUST);
       }
-      indexSearcher.search(q,
-          totalHitCountCollector);
+      indexSearcher.search(q, totalHitCountCollector);
       docCount = totalHitCountCollector.getTotalHits();
     }
     return docCount;
@@ -112,7 +115,8 @@
     Collection<String> result = new LinkedList<>();
     for (String textFieldName : textFieldNames) {
       try (TokenStream tokenStream = analyzer.tokenStream(textFieldName, doc)) {
-        CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class);
+        CharTermAttribute charTermAttribute = tokenStream
+            .addAttribute(CharTermAttribute.class);
         tokenStream.reset();
         while (tokenStream.incrementToken()) {
           result.add(charTermAttribute.toString());
@@ -127,7 +131,8 @@
    * {@inheritDoc}
    */
   @Override
-  public ClassificationResult<BytesRef> assignClass(String inputDocument) throws IOException {
+  public ClassificationResult<BytesRef> assignClass(String inputDocument)
+      throws IOException {
     if (atomicReader == null) {
       throw new IOException("You must first call Classifier#train");
     }
@@ -138,8 +143,10 @@
     TermsEnum termsEnum = terms.iterator(null);
     BytesRef next;
     String[] tokenizedDoc = tokenizeDoc(inputDocument);
+    int docsWithClassSize = countDocsWithClass();
     while ((next = termsEnum.next()) != null) {
-      double clVal = calculateLogPrior(next) + calculateLogLikelihood(tokenizedDoc, next);
+      double clVal = calculateLogPrior(next,docsWithClassSize)
+          + calculateLogLikelihood(tokenizedDoc, next, docsWithClassSize);
       if (clVal > max) {
         max = clVal;
         foundClass = BytesRef.deepCopyOf(next);
@@ -149,18 +156,75 @@
     return new ClassificationResult<>(foundClass, score);
   }
 
+  public List<ClassificationResult<BytesRef>> assignClassNormalizedList(
+      String inputDocument) throws IOException {
+    if (atomicReader == null) {
+      throw new IOException("You must first call Classifier#train");
+    }
+    List<ClassificationResult<BytesRef>> dataList = new ArrayList<ClassificationResult<BytesRef>>();
 
-  private double calculateLogLikelihood(String[] tokenizedDoc, BytesRef c) throws IOException {
+    Terms terms = MultiFields.getTerms(atomicReader, classFieldName);
+    TermsEnum termsEnum = terms.iterator(null);
+    BytesRef next;
+    String[] tokenizedDoc = tokenizeDoc(inputDocument);
+    int docsWithClassSize = countDocsWithClass();
+    while ((next = termsEnum.next()) != null) {
+      double clVal = calculateLogPrior(next, docsWithClassSize)
+          + calculateLogLikelihood(tokenizedDoc, next, docsWithClassSize);
+      dataList
+          .add(new ClassificationResult<>(BytesRef.deepCopyOf(next), clVal));
+    }
+    
+    // normalization
+    // The values transforms to a 0-1 range
+    ArrayList<ClassificationResult<BytesRef>> returnList = new ArrayList<ClassificationResult<BytesRef>>();
+    if (!dataList.isEmpty()) {
+      Collections.sort(dataList);
+      // this is a negative number closest to 0 = a
+      double smax = dataList.get(0).getScore();
+      
+      double sumLog = 0;
+      // log(sum(exp(x_n-a)))
+      for (ClassificationResult<BytesRef> cr : dataList) {
+        // getScore-smax <=0 (both negative, smax is the smallest abs()
+        sumLog += Math.exp(cr.getScore() - smax);
+      }
+      // loga=a+log(sum(exp(x_n-a))) = log(sum(exp(x_n)))
+      double loga = smax;
+      loga += Math.log(sumLog);
+      
+      // 1/sum*x = exp(log(x))*1/sum = exp(log(x)-log(sum))
+      for (ClassificationResult<BytesRef> cr : dataList) {
+        returnList.add(new ClassificationResult<BytesRef>(
+            cr.getAssignedClass(), Math.exp(cr.getScore() - loga)));
+      }
+    }
+    
+    return returnList;
+  }
+  
+  public ClassificationResult<BytesRef> assignClassNormalized(
+      String inputDocument) throws IOException {
+    List<ClassificationResult<BytesRef>> classificationResultList = assignClassNormalizedList(inputDocument);
+    if (!classificationResultList.isEmpty()) return classificationResultList
+        .get(0);
+    return null;
+  }
+  
+  private double calculateLogLikelihood(String[] tokenizedDoc, BytesRef c,
+      int docsWithClassSize) throws IOException {
     // for each word
     double result = 0d;
     for (String word : tokenizedDoc) {
       // search with text:word AND class:c
       int hits = getWordFreqForClass(word, c);
 
-      // num : count the no of times the word appears in documents of class c (+1)
+      // num : count the no of times the word appears in documents of class c
+      // (+1)
       double num = hits + 1; // +1 is added because of add 1 smoothing
 
-      // den : for the whole dictionary, count the no of times a word appears in documents of class c (+|V|)
+      // den : for the whole dictionary, count the no of times a word appears in
+      // documents of class c (+|V|)
       double den = getTextTermFreqForClass(c) + docsWithClassSize;
 
       // P(w|c) = num/den
@@ -177,20 +241,29 @@
     for (String textFieldName : textFieldNames) {
       Terms terms = MultiFields.getTerms(atomicReader, textFieldName);
       long numPostings = terms.getSumDocFreq(); // number of term/doc pairs
-      avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount(); // avg # of unique terms per doc
+      avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount(); // avg
+                                                                            // #
+                                                                            // of
+                                                                            // unique
+                                                                            // terms
+                                                                            // per
+                                                                            // doc
     }
     int docsWithC = atomicReader.docFreq(new Term(classFieldName, c));
-    return avgNumberOfUniqueTerms * docsWithC; // avg # of unique terms in text fields per doc * # docs with c
+    return avgNumberOfUniqueTerms * docsWithC; // avg # of unique terms in text
+                                               // fields per doc * # docs with c
   }
 
   private int getWordFreqForClass(String word, BytesRef c) throws IOException {
     BooleanQuery booleanQuery = new BooleanQuery();
     BooleanQuery subQuery = new BooleanQuery();
     for (String textFieldName : textFieldNames) {
-     subQuery.add(new BooleanClause(new TermQuery(new Term(textFieldName, word)), BooleanClause.Occur.SHOULD));
+      subQuery.add(new BooleanClause(new TermQuery(
+          new Term(textFieldName, word)), BooleanClause.Occur.SHOULD));
     }
     booleanQuery.add(new BooleanClause(subQuery, BooleanClause.Occur.MUST));
-    booleanQuery.add(new BooleanClause(new TermQuery(new Term(classFieldName, c)), BooleanClause.Occur.MUST));
+    booleanQuery.add(new BooleanClause(new TermQuery(
+        new Term(classFieldName, c)), BooleanClause.Occur.MUST));
     if (query != null) {
       booleanQuery.add(query, BooleanClause.Occur.MUST);
     }
@@ -199,8 +272,10 @@
     return totalHitCountCollector.getTotalHits();
   }
 
-  private double calculateLogPrior(BytesRef currentClass) throws IOException {
-    return Math.log((double) docCount(currentClass)) - Math.log(docsWithClassSize);
+  private double calculateLogPrior(BytesRef currentClass, int docsWithClassSize)
+      throws IOException {
+    return Math.log((double) docCount(currentClass))
+        - Math.log(docsWithClassSize);
   }
 
   private int docCount(BytesRef countedClass) throws IOException {
