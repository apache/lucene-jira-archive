diff --git a/dev-tools/idea/lucene/analysis/common/analysis-common.iml b/dev-tools/idea/lucene/analysis/common/analysis-common.iml
index a98ab53..5a28179 100644
--- a/dev-tools/idea/lucene/analysis/common/analysis-common.iml
+++ b/dev-tools/idea/lucene/analysis/common/analysis-common.iml
@@ -12,6 +12,16 @@
     </content>
     <orderEntry type="inheritedJdk" />
     <orderEntry type="sourceFolder" forTests="false" />
+    <orderEntry type="module-library">
+          <library>
+            <CLASSES>
+              <root url="file://$MODULE_DIR$/lib" />
+            </CLASSES>
+            <JAVADOC />
+            <SOURCES />
+            <jarDirectory url="file://$MODULE_DIR$/lib" recursive="false" />
+          </library>
+        </orderEntry>
     <orderEntry type="library" scope="TEST" name="JUnit" level="project" />
     <orderEntry type="module" scope="TEST" module-name="lucene-test-framework" />
     <orderEntry type="module" module-name="lucene-core" />
diff --git a/lucene/analysis/common/build.xml b/lucene/analysis/common/build.xml
index 670e6ab..e3ded71 100644
--- a/lucene/analysis/common/build.xml
+++ b/lucene/analysis/common/build.xml
@@ -29,6 +29,16 @@
 
   <import file="../analysis-module-build.xml"/>
   
+  <path id="commonjars">
+     <fileset dir="lib"/>
+  </path>
+
+  <path id="classpath">
+    <pathelement path="${analyzers-common.jar}"/>
+    <path refid="commonjars"/>
+    <path refid="base.classpath"/>
+  </path>
+
   <property name="snowball.programs.dir" location="src/java/org/tartarus/snowball/ext"/>  
   
   <property name="unicode-props-file" location="src/java/org/apache/lucene/analysis/util/UnicodeProps.java"/>
diff --git a/lucene/analysis/common/ivy.xml b/lucene/analysis/common/ivy.xml
index 9c5376b..e046ac7 100644
--- a/lucene/analysis/common/ivy.xml
+++ b/lucene/analysis/common/ivy.xml
@@ -18,4 +18,11 @@
 -->
 <ivy-module version="2.0">
   <info organisation="org.apache.lucene" module="analyzers-common"/>
+  <configurations defaultconfmapping="compile->master">
+    <conf name="compile" transitive="false"/>
+  </configurations>
+  <dependencies>
+    <dependency org="com.google.guava" name="guava" rev="${/com.google.guava/guava}" conf="compile"/>
+    <exclude org="*" ext="*" matcher="regexp" type="${ivy.exclude.types}"/> 
+  </dependencies>
 </ivy-module>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/minhash/MinHashFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/minhash/MinHashFilter.java
index c4501f9..9595dfe 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/minhash/MinHashFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/minhash/MinHashFilter.java
@@ -56,17 +56,17 @@ public class MinHashFilter extends TokenFilter {
 
   public static final String MIN_HASH_TYPE = "MIN_HASH";
 
-  ArrayList<FixedSizeTreeSet<ComparableHashCode>> minHashSets;
+  private ArrayList<FixedSizeTreeSet<ComparableHashCode>> minHashSets;
 
-  int hashSetSize = DEFAULT_HASH_SET_SIZE;
+  private int hashSetSize = DEFAULT_HASH_SET_SIZE;
 
-  int hashCount = DEFAULT_HASH_COUNT;
+  private int hashCount = DEFAULT_HASH_COUNT;
 
-  boolean requiresInitialisation = true;
+  private boolean requiresInitialisation = true;
 
   private State endState;
 
-  int position = -1;
+  private int position = -1;
 
   private final CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);
   private final OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);
@@ -97,13 +97,12 @@ public class MinHashFilter extends TokenFilter {
 
 
   @Override
-  final public boolean incrementToken() throws IOException {
+  public final boolean incrementToken() throws IOException {
     // Pull the underlying stream of tokens
     // Hash each token found
     // Generate the required number of variants of this hash
     // Keep the minimum hash value found so far of each variant
 
-
     if (requiresInitialisation) {
       requiresInitialisation = false;
       boolean found = false;
@@ -121,7 +120,7 @@ public class MinHashFilter extends TokenFilter {
       input.end();
       // We need the end state so an underlying shingle filter can have its state restored correctly.
       endState = captureState();
-      if (false == found) {
+      if (!found) {
         return false;
       }
     }
@@ -167,10 +166,6 @@ public class MinHashFilter extends TokenFilter {
     return false;
   }
 
-  /**
-   * @param i
-   * @return
-   */
   private static HashCode getIntHash(int i) {
     if (i < HASH_CACHE_SIZE) {
       return cachedIntHashes[i];
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/minhash/MinHashFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/minhash/MinHashFilterFactory.java
index eeb1062..ccd8ab4 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/minhash/MinHashFilterFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/minhash/MinHashFilterFactory.java
@@ -22,7 +22,7 @@ import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.util.TokenFilterFactory;
 
 /**
- *
+ * {@link TokenFilterFactory} for {@link MinHashFilter}.
  */
 public class MinHashFilterFactory extends TokenFilterFactory {
   private int hashCount = MinHashFilter.DEFAULT_HASH_COUNT;
@@ -30,7 +30,7 @@ public class MinHashFilterFactory extends TokenFilterFactory {
   private int hashSetSize = MinHashFilter.DEFAULT_HASH_SET_SIZE;
 
   /**
-   * @param args
+   * Create a {@link MinHashFilterFactory}.
    */
   public MinHashFilterFactory(Map<String, String> args) {
     super(args);
diff --git a/lucene/analysis/common/src/resources/META-INF/services/org.apache.lucene.analysis.util.TokenFilterFactory b/lucene/analysis/common/src/resources/META-INF/services/org.apache.lucene.analysis.util.TokenFilterFactory
index 8b19cdf..70120c5 100644
--- a/lucene/analysis/common/src/resources/META-INF/services/org.apache.lucene.analysis.util.TokenFilterFactory
+++ b/lucene/analysis/common/src/resources/META-INF/services/org.apache.lucene.analysis.util.TokenFilterFactory
@@ -58,6 +58,7 @@ org.apache.lucene.analysis.id.IndonesianStemFilterFactory
 org.apache.lucene.analysis.in.IndicNormalizationFilterFactory
 org.apache.lucene.analysis.it.ItalianLightStemFilterFactory
 org.apache.lucene.analysis.lv.LatvianStemFilterFactory
+org.apache.lucene.analysis.minhash.MinHashFilterFactory
 org.apache.lucene.analysis.miscellaneous.ASCIIFoldingFilterFactory
 org.apache.lucene.analysis.miscellaneous.CapitalizationFilterFactory
 org.apache.lucene.analysis.miscellaneous.CodepointCountFilterFactory
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/minhash/MinHashFilterTest.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/minhash/MinHashFilterTest.java
index 47f80ab..5741cc6 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/minhash/MinHashFilterTest.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/minhash/MinHashFilterTest.java
@@ -54,6 +54,9 @@ import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.RAMDirectory;
 import org.junit.Test;
 
+/**
+ * Tests for {@link MinHashFilter}
+ */
 public class MinHashFilterTest extends BaseTokenStreamTestCase {
   @Test
   public void testHashOrder() {
@@ -124,11 +127,8 @@ public class MinHashFilterTest extends BaseTokenStreamTestCase {
 
   }
 
-
   @Test
   public void testHashNotReapeated() {
-
-
     FixedSizeTreeSet<ComparableHashCode> minSet = new FixedSizeTreeSet<ComparableHashCode>(500);
     HashSet<ComparableHashCode> unadded = new HashSet<ComparableHashCode>();
     for (int i = 0; i < 10000; i++) {
@@ -159,8 +159,6 @@ public class MinHashFilterTest extends BaseTokenStreamTestCase {
       }
       last = current;
     }
-
-
   }
 
   @Test
@@ -317,9 +315,6 @@ public class MinHashFilterTest extends BaseTokenStreamTestCase {
 
     topDocs = searcher.search(buildQuery("text", "one two three four five six seven eight nine ten", min, 1, 100), 100);
     assertEquals(21, topDocs.totalHits);
-    for (int i = 0; i < topDocs.totalHits; i++) {
-      System.out.println(i + " = " + topDocs.scoreDocs[i]);
-    }
 
     float topScore = 6.0f;
     assertEquals(topDocs.scoreDocs[0].score, topScore, 0.001f);
@@ -346,7 +341,6 @@ public class MinHashFilterTest extends BaseTokenStreamTestCase {
 
   }
 
-
   private void assertAllScores(TopDocs topDocs, float score) {
     for (int i = 0; i < topDocs.totalHits; i++) {
       assertEquals(topDocs.scoreDocs[i].score, score, 0f);
@@ -380,8 +374,7 @@ public class MinHashFilterTest extends BaseTokenStreamTestCase {
     lshffargs.put("hashSetSize", "" + hashSetSize);
     MinHashFilterFactory lshff = new MinHashFilterFactory(lshffargs);
 
-    TokenizerChain chain = new TokenizerChain(new CharFilterFactory[]{}, icutf, new TokenFilterFactory[]{sff, lshff});
-    return chain;
+    return new TokenizerChain(new CharFilterFactory[]{}, icutf, new TokenFilterFactory[]{sff, lshff});
   }
 
   private boolean isLessThan(HashCode hash1, HashCode hash2) {
@@ -399,7 +392,6 @@ public class MinHashFilterTest extends BaseTokenStreamTestCase {
     }
   }
 
-
   /**
    * An analyzer that uses a tokenizer and a list of token filters to
    * create a TokenStream - lifted from SOLR to make this analyzer test lucene only.
@@ -410,7 +402,6 @@ public class MinHashFilterTest extends BaseTokenStreamTestCase {
     final private TokenizerFactory tokenizer;
     final private TokenFilterFactory[] filters;
 
-
     /**
      * Creates a new TokenizerChain.
      *
@@ -462,4 +453,4 @@ public class MinHashFilterTest extends BaseTokenStreamTestCase {
       return sb.toString();
     }
   }
-}
+}
\ No newline at end of file
