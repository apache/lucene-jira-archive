diff --git a/lucene/analysis/common/build.xml b/lucene/analysis/common/build.xml
index 670e6ab..e3ded71 100644
--- a/lucene/analysis/common/build.xml
+++ b/lucene/analysis/common/build.xml
@@ -29,6 +29,16 @@
 
   <import file="../analysis-module-build.xml"/>
   
+  <path id="commonjars">
+     <fileset dir="lib"/>
+  </path>
+
+  <path id="classpath">
+    <pathelement path="${analyzers-common.jar}"/>
+    <path refid="commonjars"/>
+    <path refid="base.classpath"/>
+  </path>
+
   <property name="snowball.programs.dir" location="src/java/org/tartarus/snowball/ext"/>  
   
   <property name="unicode-props-file" location="src/java/org/apache/lucene/analysis/util/UnicodeProps.java"/>
diff --git a/lucene/analysis/common/ivy.xml b/lucene/analysis/common/ivy.xml
index 9c5376b..e046ac7 100644
--- a/lucene/analysis/common/ivy.xml
+++ b/lucene/analysis/common/ivy.xml
@@ -18,4 +18,11 @@
 -->
 <ivy-module version="2.0">
   <info organisation="org.apache.lucene" module="analyzers-common"/>
+  <configurations defaultconfmapping="compile->master">
+    <conf name="compile" transitive="false"/>
+  </configurations>
+  <dependencies>
+    <dependency org="com.google.guava" name="guava" rev="${/com.google.guava/guava}" conf="compile"/>
+    <exclude org="*" ext="*" matcher="regexp" type="${ivy.exclude.types}"/> 
+  </dependencies>
 </ivy-module>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/minhash/MinHashFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/minhash/MinHashFilter.java
new file mode 100644
index 0000000..3d0e320
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/minhash/MinHashFilter.java
@@ -0,0 +1,379 @@
+package org.apache.lucene.analysis.minhash;
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.TreeSet;
+
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionLengthAttribute;
+import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+
+import com.google.common.hash.HashCode;
+import com.google.common.hash.HashCodes;
+import com.google.common.hash.Hashing;
+
+/**
+ *  Generate min hash tokens from an incoming stream of tokens.
+ *  The incoming tokens would typically be 5 word shingles.
+ *  
+ *  The number of hashes used and the number of minimum values for each hash can be set.
+ *  You could have 1 hash and keep the 100 lowest values or 100 hahses and keep the lowest one for each. 
+ *  
+ *  A 128-bit hash is used internally. 
+ *  5 word shingles from 10e5 words generate 10e25 combinations 
+ *  So a 64 bit hash would have collisions (1.8e19) 
+ *  
+ *  When using different hashes 32 bits are used for the hash position leaving scope for 8e28 unique hashes. 
+ *  A single hash will use all 128 bits.
+ *  
+ *
+ */
+public class MinHashFilter extends TokenFilter
+{
+    private static int HASH_CACHE_SIZE = 512;
+    
+    private static HashCode[] cachedIntHashes = new HashCode[HASH_CACHE_SIZE];
+    
+    public static final int DEFAULT_HASH_COUNT = 400;
+    
+    public static final int DEFAULT_HASH_SET_SIZE = 1;
+
+    public static final String MIN_HASH_TYPE = "MIN_HASH";
+
+    ArrayList<FixedSizeTreeSet<ComparableHashCode>> minHashSets;
+
+    int hashSetSize = DEFAULT_HASH_SET_SIZE;
+    
+    int hashCount = DEFAULT_HASH_COUNT;
+    
+    boolean requiresInitialisation = true;
+    
+    private State endState;
+    
+    int position = -1;
+
+    private final CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);
+    private final OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);
+    private final TypeAttribute typeAttribute = addAttribute(TypeAttribute.class);
+    private final PositionIncrementAttribute posIncAttribute = addAttribute(PositionIncrementAttribute.class);
+    private final PositionLengthAttribute posLenAttribute = addAttribute(PositionLengthAttribute.class);
+     
+    static
+    {
+        for(int i = 0; i < HASH_CACHE_SIZE; i++)
+        {
+            cachedIntHashes[i] = Hashing.murmur3_128().hashInt(i);
+        }
+    }
+    
+    protected MinHashFilter(TokenStream input)
+    {
+        this(input, DEFAULT_HASH_COUNT, DEFAULT_HASH_SET_SIZE);
+    }
+
+    protected MinHashFilter(TokenStream input, int hashCount, int hashSetSize)
+    {
+        super(input);
+        this.hashCount = hashCount;
+        this.hashSetSize = hashSetSize;
+        minHashSets = new ArrayList<FixedSizeTreeSet<ComparableHashCode>>(hashCount);
+        for(int i = 0; i < hashCount; i++)
+        {
+           minHashSets.add(new FixedSizeTreeSet<ComparableHashCode>(hashSetSize));
+        }
+        doRest();
+    }
+    
+  
+
+    @Override
+    final public boolean incrementToken() throws IOException
+    {
+        // Pull the underlying stream of tokens
+        // Hash each token found
+        // Generate the required number of variants of this hash
+        // Keep the minimum hash value found so far of each variant
+
+       
+        if(requiresInitialisation)
+        {
+            requiresInitialisation = false;
+            boolean found = false;
+            // First time through so we pull and has everything
+            while (input.incrementToken())
+            {
+                found = true;
+                String current = new String(termAttribute.buffer(), 0, termAttribute.length());
+                
+                for(int i = 0; i < hashCount; i++)
+                {
+                    HashCode hash = Hashing.murmur3_128().hashString(current);
+                    HashCode rehashed = combineOrdered(hash, getIntHash(i));
+                    minHashSets.get(i).add(new ComparableHashCode(rehashed));
+                }
+            }
+            input.end();
+            // We need the end state so an underlying shingle filter can have its state restored correctly. 
+            endState = captureState();
+            if(false == found)
+            {    
+                return false;
+            }
+        }
+
+        clearAttributes();
+        
+        int positionIncrement = 0;
+        while(position < hashCount)
+        {
+            if(position == -1)
+            {
+                position++;
+                positionIncrement = 1;
+            }
+            else
+            {
+                ComparableHashCode hash = minHashSets.get(position).pollFirst();
+                if(hash != null)
+                {
+                    termAttribute.setEmpty();
+                    if(hashCount > 1)
+                    {
+                        termAttribute.append(int0(position));
+                        termAttribute.append(int1(position));
+                    }
+                    long high = secondLong(hash.hashCode);
+                    termAttribute.append(long0(high));
+                    termAttribute.append(long1(high));
+                    termAttribute.append(long2(high));
+                    termAttribute.append(long3(high));
+                    long low = firstLong(hash.hashCode);
+                    termAttribute.append(long0(low));
+                    termAttribute.append(long1(low));
+                    if(hashCount == 1)
+                    {
+                        termAttribute.append(long2(low));
+                        termAttribute.append(long3(low));
+                    }
+                    posIncAttribute.setPositionIncrement(positionIncrement);
+                    offsetAttribute.setOffset(position, position+1);
+                    typeAttribute.setType(MIN_HASH_TYPE);
+                    posLenAttribute.setPositionLength(1);
+                    return true;
+                }
+                else
+                {
+                    position++;
+                    positionIncrement = 1;
+                }
+            }
+        }
+        return false;
+    }
+
+    /**
+     * @param i
+     * @return
+     */
+    private static HashCode getIntHash(int i)
+    {
+        if( i < HASH_CACHE_SIZE)
+        {
+            return cachedIntHashes[i];
+        }
+        else
+        {
+            return Hashing.murmur3_128().hashInt(i);
+        }
+    }
+
+    static long firstLong(HashCode hash)
+    {
+        byte[] bytes = hash.asBytes();
+        return (bytes[0] & 0xFFL)
+                | ((bytes[1] & 0xFFL) << 8)
+                | ((bytes[2] & 0xFFL) << 16)
+                | ((bytes[3] & 0xFFL) << 24)
+                | ((bytes[4] & 0xFFL) << 32)
+                | ((bytes[5] & 0xFFL) << 40)
+                | ((bytes[6] & 0xFFL) << 48)
+                | ((bytes[7] & 0xFFL) << 56);
+    }
+
+    static long secondLong(HashCode hash)
+    {
+        byte[] bytes = hash.asBytes();
+        return (bytes[8] & 0xFFL)
+                | ((bytes[9] & 0xFFL) << 8)
+                | ((bytes[10] & 0xFFL) << 16)
+                | ((bytes[11] & 0xFFL) << 24)
+                | ((bytes[12] & 0xFFL) << 32)
+                | ((bytes[13] & 0xFFL) << 40)
+                | ((bytes[14] & 0xFFL) << 48)
+                | ((bytes[15] & 0xFFL) << 56);
+    }
+
+
+    @Override
+    public void end() throws IOException
+    {
+        restoreState(endState);
+    }
+
+    @Override
+    public void close() throws IOException
+    {
+        super.close();
+    }
+
+    @Override
+    public void reset() throws IOException
+    {
+        super.reset();
+        doRest();
+    }
+
+    private void doRest()
+    {
+        for(int i = 0; i < hashCount; i++)
+        {
+            minHashSets.get(i).clear();
+        }
+        endState = null;
+        position = -1;
+        requiresInitialisation = true;
+    }
+ 
+    private static char long0(long x) { return (char)(x >> 48); }
+    private static char long1(long x) { return (char)(x >> 32); }
+    private static char long2(long x) { return (char)(x >> 16); }
+    private static char long3(long x) { return (char)(x      ); }
+    
+    private static char int0(int x) { return (char)(x >> 16); }
+    private static char int1(int x) { return (char)(x      ); }
+    
+    public static boolean isLessThanUnsigned(long n1, long n2) 
+    {
+        return (n1 < n2) ^ ((n1 < 0) != (n2 < 0));
+    }
+    
+    static class FixedSizeTreeSet<E extends Comparable<E>> extends TreeSet<E>
+    {
+      
+        /**
+         * 
+         */
+        private static final long serialVersionUID = -8237117170340299630L;
+        private int capacity;
+        
+        FixedSizeTreeSet()
+        {
+            this(20);
+        }
+        
+        FixedSizeTreeSet(int capacity)
+        {
+            super();
+            this.capacity = capacity;
+        }
+      
+        
+        @Override
+        public boolean add(final E toAdd) 
+        {
+            if (capacity <= size()) 
+            {
+                final E lastElm = last();
+                if ( toAdd.compareTo(lastElm) > -1) 
+                {
+                    return false;
+                } 
+                else 
+                {
+                    pollLast();
+                }
+            }
+            return super.add(toAdd);
+        }
+    }
+    
+    static class ComparableHashCode implements Comparable<ComparableHashCode>
+    {
+        HashCode hashCode;
+
+        ComparableHashCode( HashCode hashCode)
+        {
+            this.hashCode = hashCode;
+        }
+
+        /* (non-Javadoc)
+         * @see java.lang.Comparable#compareTo(java.lang.Object)
+         */
+        @Override
+        public int compareTo(ComparableHashCode other)
+        {
+            long high1 = secondLong(hashCode);
+            long high2 = secondLong(other.hashCode);
+            
+            if(isLessThanUnsigned(high1, high2))
+            {
+                return -1;
+            }
+            else if(high1 == high2)
+            {
+                long low1 = firstLong(hashCode);
+                long low2 = firstLong(other.hashCode);
+                if(isLessThanUnsigned(low1, low2))
+                {
+                    return -1;
+                }
+                else if(low1 == low2)
+                {
+                    return 0;
+                }
+                else
+                {
+                    return 1;
+                }
+            }
+            else
+            {
+                return 1;
+            }
+        }
+    }
+    
+    public static HashCode combineOrdered(HashCode ... hashCodes) 
+    {
+        int bits = hashCodes[0].bits();
+        byte[] resultBytes = new byte[bits / 8];
+        for (HashCode hashCode : hashCodes) 
+        {
+            byte[] nextBytes = hashCode.asBytes();
+            for (int i = 0; i < nextBytes.length; i++) 
+            {
+                resultBytes[i] = (byte) (resultBytes[i] * 37 ^ nextBytes[i]);
+            }
+        }
+        return HashCodes.fromBytes(resultBytes);
+    }
+}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/minhash/MinHashFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/minhash/MinHashFilterFactory.java
new file mode 100644
index 0000000..0f417f6
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/minhash/MinHashFilterFactory.java
@@ -0,0 +1,53 @@
+package org.apache.lucene.analysis.minhash;
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.util.Map;
+
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.util.TokenFilterFactory;
+
+/**
+ *
+ */
+public class MinHashFilterFactory extends TokenFilterFactory
+{
+    private int hashCount = MinHashFilter.DEFAULT_HASH_COUNT;
+    
+    private int hashSetSize = MinHashFilter.DEFAULT_HASH_SET_SIZE;
+
+    /**
+     * @param args
+     */
+    public MinHashFilterFactory(Map<String, String> args)
+    {
+        super(args);
+        hashCount = getInt(args, "hashCount", MinHashFilter.DEFAULT_HASH_COUNT);
+        hashSetSize = getInt(args, "hashSetSize", MinHashFilter.DEFAULT_HASH_SET_SIZE);
+    }
+
+    /*
+     * (non-Javadoc)
+     * @see org.apache.lucene.analysis.util.TokenFilterFactory#create(org.apache.lucene.analysis.TokenStream)
+     */
+    @Override
+    public TokenStream create(TokenStream input)
+    {
+        MinHashFilter filter = new MinHashFilter(input, hashCount, hashSetSize);
+        return filter;
+    }
+
+}
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/minhash/MinHashFilterTest.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/minhash/MinHashFilterTest.java
new file mode 100644
index 0000000..faf110b
--- /dev/null
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/minhash/MinHashFilterTest.java
@@ -0,0 +1,523 @@
+package org.apache.lucene.analysis.minhash;
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+import java.io.Reader;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizerFactory;
+import org.apache.lucene.analysis.minhash.MinHashFilter.ComparableHashCode;
+import org.apache.lucene.analysis.minhash.MinHashFilter.FixedSizeTreeSet;
+import org.apache.lucene.analysis.shingle.ShingleFilterFactory;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.util.CharFilterFactory;
+import org.apache.lucene.analysis.util.TokenFilterFactory;
+import org.apache.lucene.analysis.util.TokenizerFactory;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+
+import com.google.common.hash.HashCode;
+import com.google.common.hash.Hashing;
+
+public class MinHashFilterTest extends BaseTokenStreamTestCase
+{
+    @Test
+    public void testHashOrder()
+    {
+        assertTrue(!MinHashFilter.isLessThanUnsigned(0l, 0l));
+        assertTrue(MinHashFilter.isLessThanUnsigned(0l, -1l));
+        assertTrue(MinHashFilter.isLessThanUnsigned(1l, -1l));
+        assertTrue(MinHashFilter.isLessThanUnsigned(-2l, -1l));
+        assertTrue(MinHashFilter.isLessThanUnsigned(1l, 2l));
+        assertTrue(MinHashFilter.isLessThanUnsigned(Long.MAX_VALUE, Long.MIN_VALUE));
+
+        FixedSizeTreeSet<ComparableHashCode> minSet = new FixedSizeTreeSet<ComparableHashCode>(500);
+        HashSet<ComparableHashCode> unadded = new HashSet<ComparableHashCode>();
+        for (int i = 0; i < 100; i++)
+        {
+            ComparableHashCode hash = new ComparableHashCode(Hashing.murmur3_128().hashInt(i));
+            ComparableHashCode peek = null;
+            if(minSet.size() > 0)
+            {
+                peek = minSet.last();
+            }
+
+            if (!minSet.add(hash))
+            {
+                unadded.add(hash);
+            }
+            else
+            {
+                if(peek != null)
+                {
+                    if ((minSet.size() == 500) && !peek.equals(minSet.last()))
+                    {
+                        unadded.add(peek);
+                    }
+                }
+            }
+        }
+        assertEquals(100, minSet.size());
+        assertEquals(0, unadded.size());
+
+        HashSet<ComparableHashCode> collisionDetection = new HashSet<ComparableHashCode>();
+        unadded = new HashSet<ComparableHashCode>();
+        minSet = new FixedSizeTreeSet<ComparableHashCode>(500);
+        for (int i = 0; i < 1000000; i++)
+        {
+            ComparableHashCode hash = new ComparableHashCode(Hashing.murmur3_128().hashInt(i));
+            collisionDetection.add(hash);
+            ComparableHashCode peek = null;
+            if(minSet.size() > 0)
+            {
+                peek = minSet.last();
+            }
+
+            if (!minSet.add(hash))
+            {
+                unadded.add(hash);
+            }
+            else
+            {
+                if(peek != null)
+                {
+                    if ((minSet.size() == 500) && !peek.equals(minSet.last()))
+                    {
+                        unadded.add(peek);
+                    }
+                }
+            }   
+        }
+        assertEquals(1000000, collisionDetection.size());
+        assertEquals(500, minSet.size());
+        assertEquals(999500, unadded.size());
+        
+        ComparableHashCode last = null;
+        ComparableHashCode current = null;
+        while((current = minSet.pollLast()) != null)
+        {
+            if(last != null)
+            {
+                assertTrue(isLessThan(current.hashCode, last.hashCode));
+            }
+            else
+            {
+                
+            }
+            last = current;
+        }
+
+    }
+    
+    
+    @Test
+    public void testHashNotReapeated()
+    {
+
+       
+        FixedSizeTreeSet<ComparableHashCode> minSet = new FixedSizeTreeSet<ComparableHashCode>(500);
+        HashSet<ComparableHashCode> unadded = new HashSet<ComparableHashCode>();
+        for (int i = 0; i < 10000; i++)
+        {
+            ComparableHashCode hash = new ComparableHashCode(Hashing.murmur3_128().hashInt(i));
+            ComparableHashCode peek = null;
+            if(minSet.size() > 0)
+            {
+                peek = minSet.last();
+            }
+            if (!minSet.add(hash))
+            {
+                unadded.add(hash);
+            }
+            else
+            {
+                if(peek != null)
+                {
+                    if ((minSet.size() == 500) && !peek.equals(minSet.last()))
+                    {
+                        unadded.add(peek);
+                    }
+                }
+            }
+        }
+        assertEquals(500, minSet.size());
+        
+        ComparableHashCode last = null;
+        ComparableHashCode current = null;
+        while((current = minSet.pollLast()) != null)
+        {
+            if(last != null)
+            {
+                assertTrue(isLessThan(current.hashCode, last.hashCode));
+            }
+            else
+            {
+                
+            }
+            last = current;
+        }
+
+        
+    }
+    
+    @Test
+    public void testTokenStream0() throws IOException
+    {
+        String[] hashes = new String[]{"갏ꉷ榙缏㑝묏"}; 
+        
+        TokenizerChain chain = createMinHashAnalyzer(5, 1, 100);
+        ArrayList<String> tokens = getTokens(chain, "test", "woof woof woof woof woof");
+        chain.close();
+        
+        assertEquals(1,  tokens.size());
+        assertEquals(tokens.get(0), hashes[0]);
+    }
+    
+    @Test
+    public void testTokenStream1() throws IOException
+    {
+        String[] hashes = new String[]{"䟔邻挊蓫㐷谗㺱社", "갏ꉷ榙缏㑝묏"}; 
+        
+        TokenizerChain chain = createMinHashAnalyzer(5, 1, 100);
+        ArrayList<String> tokens = getTokens(chain, "test", "woof woof woof woof woof puff");
+        chain.close();
+        
+        assertEquals(2,  tokens.size());
+        assertEquals(tokens.get(0), hashes[0]);
+        assertEquals(tokens.get(1), hashes[1]);
+    }
+    
+    private ArrayList<String> getTokens(Analyzer analyzer, String field, String value) throws IOException
+    {
+        ArrayList<String> tokens = new ArrayList<String>();
+        
+        TokenStream ts = analyzer.tokenStream(field, value);
+        ts.reset();
+        while(ts.incrementToken())
+        {
+            CharTermAttribute termAttribute = ts.getAttribute(CharTermAttribute.class);
+            String token = new String(termAttribute.buffer(), 0, termAttribute.length());
+            tokens.add(token);
+        }
+        ts.end();
+        ts.close();     
+        
+        return tokens;
+    }
+    
+    @Test
+    public void testTokenStream2() throws IOException
+    {
+        TokenizerChain chain = createMinHashAnalyzer(5, 100, 1);
+        ArrayList<String> tokens = getTokens(chain, "test", "woof woof woof woof woof puff");
+        chain.close();
+        
+        assertEquals(100,  tokens.size());
+    }
+    
+    @Test
+    public void testTokenStream3() throws IOException
+    {
+        TokenizerChain chain = createMinHashAnalyzer(5, 10, 10);
+        ArrayList<String> tokens = getTokens(chain, "test", "woof woof woof woof woof puff");
+        chain.close();
+        
+        assertEquals(20,  tokens.size());
+    }
+    
+    @Test
+    public void testLSHQuery() throws IOException 
+    {
+            Analyzer analyzer = createMinHashAnalyzer(5, 1, 100);
+            IndexWriterConfig config = new IndexWriterConfig(analyzer);
+        
+            RAMDirectory directory = new RAMDirectory();
+            IndexWriter writer = new IndexWriter(directory, config);
+            Document doc = new Document();
+            doc.add(new TextField("text", "woof woof woof woof woof", Store.NO));
+            writer.addDocument(doc);
+        
+            doc = new Document();
+            doc.add(new TextField("text", "woof woof woof woof woof puff", Store.NO));
+            writer.addDocument(doc);
+            
+            doc = new Document();
+            doc.add(new TextField("text", "woof woof woof woof puff", Store.NO));
+            writer.addDocument(doc);
+        
+            writer.commit();
+            writer.close();
+        
+            IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(directory));
+          
+            BooleanQuery.Builder builder = new BooleanQuery.Builder();
+            builder.add(new ConstantScoreQuery(new TermQuery(new Term("text", "䟔邻挊蓫㐷谗㺱社"))), Occur.SHOULD);
+            builder.add(new ConstantScoreQuery(new TermQuery(new Term("text", "갏ꉷ榙缏㑝묏"))), Occur.SHOULD);
+            builder.setDisableCoord(true);
+            TopDocs topDocs = searcher.search(builder.build(), 10);
+        
+            assertEquals(3, topDocs.totalHits);
+            
+            float score = topDocs.scoreDocs[0].score;
+            assertEquals(topDocs.scoreDocs[1].score, score/2, 0f);
+            assertEquals(topDocs.scoreDocs[2].score, score/2, 0f);
+         
+    }
+    
+    
+    
+    @Test
+    public void testLSHQuery2() throws IOException 
+    {
+            String[] parts = new String[]{"one", "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten"};
+            int min = 5;
+        
+            Analyzer analyzer = createMinHashAnalyzer(min, 1, 100);
+            IndexWriterConfig config = new IndexWriterConfig(analyzer);
+        
+            RAMDirectory directory = new RAMDirectory();
+            IndexWriter writer = new IndexWriter(directory, config);
+          
+            for(int i = 0; i < parts.length; i++)
+            {
+                StringBuilder builder = new StringBuilder();
+                for(int j = 0; j < parts.length -i; j++)
+                {
+                    if(builder.length() > 0)
+                    {
+                        builder.append(" ");
+                    }
+                    builder.append(parts[i+j]);
+                    if(j >= min -1)
+                    {
+                        Document doc = new Document();
+                        doc.add(new TextField("text", builder.toString(), Store.NO));
+                        writer.addDocument(doc);
+                    }
+                }    
+            }
+               
+            writer.commit();
+            writer.close();
+        
+             
+            IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(directory));
+          
+            TopDocs topDocs = searcher.search(buildQuery("text", "one two three four five", min, 1, 100), 100);        
+            assertEquals(6, topDocs.totalHits);
+            assertAllScores(topDocs, 1.0f);
+            topDocs = searcher.search(buildQuery("text", "two three four five six", min, 1, 100), 100);        
+            assertEquals(10, topDocs.totalHits);
+            assertAllScores(topDocs, 1.0f);
+            topDocs = searcher.search(buildQuery("text", "three four five six seven", min, 1, 100), 100);        
+            assertEquals(12, topDocs.totalHits);
+            assertAllScores(topDocs, 1.0f);
+            topDocs = searcher.search(buildQuery("text", "four five six seven eight", min, 1, 100), 100);        
+            assertEquals(12, topDocs.totalHits);
+            assertAllScores(topDocs, 1.0f);
+            topDocs = searcher.search(buildQuery("text", "five six seven eight nine", min, 1, 100), 100);        
+            assertEquals(10, topDocs.totalHits);
+            assertAllScores(topDocs, 1.0f);
+            topDocs = searcher.search(buildQuery("text", "six seven eight nine ten", min, 1, 100), 100);        
+            assertEquals(6, topDocs.totalHits);
+            assertAllScores(topDocs, 1.0f);
+            
+            topDocs = searcher.search(buildQuery("text", "one two three four five six", min, 1, 100), 100);        
+            assertEquals(11, topDocs.totalHits);
+            
+            topDocs = searcher.search(buildQuery("text", "one two three four five six seven eight nine ten", min, 1, 100), 100);        
+            assertEquals(21, topDocs.totalHits);
+            for(int i = 0; i < topDocs.totalHits; i++)
+            {
+                System.out.println(i+" = "+topDocs.scoreDocs[i]);
+            }
+            
+            float topScore = 6.0f;
+            assertEquals(topDocs.scoreDocs[0].score, topScore, 0.001f);
+            assertEquals(topDocs.scoreDocs[1].score, topScore * 5/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[2].score, topScore * 5/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[3].score, topScore * 4/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[4].score, topScore * 4/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[5].score, topScore * 4/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[6].score, topScore * 3/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[7].score, topScore * 3/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[8].score, topScore * 3/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[9].score, topScore * 3/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[10].score, topScore * 2/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[11].score, topScore * 2/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[12].score, topScore * 2/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[13].score, topScore * 2/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[14].score, topScore * 2/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[15].score, topScore * 1/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[16].score, topScore * 1/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[17].score, topScore * 1/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[18].score, topScore * 1/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[19].score, topScore * 1/6, 0.001f);
+            assertEquals(topDocs.scoreDocs[20].score, topScore * 1/6, 0.001f);
+         
+    }
+    
+    
+    private void assertAllScores(TopDocs topDocs, float score)
+    {
+        for(int i = 0; i < topDocs.totalHits; i++)
+        {
+            assertEquals(topDocs.scoreDocs[i].score, score, 0f);
+        }
+    }
+    
+    private Query buildQuery(String field, String query, int min, int hashCount, int hashSetSize) throws IOException
+    {
+        TokenizerChain chain = createMinHashAnalyzer(min, hashCount, hashSetSize);
+        ArrayList<String> tokens = getTokens(chain, field, query);
+        chain.close();
+        
+        BooleanQuery.Builder builder = new BooleanQuery.Builder();
+        for(String token : tokens)
+        {
+            builder.add(new ConstantScoreQuery(new TermQuery(new Term("text", token))), Occur.SHOULD);
+        }
+        builder.setDisableCoord(true);
+        return builder.build();
+    }
+    
+    public static TokenizerChain createMinHashAnalyzer(int min, int hashCount, int hashSetSize) 
+    {
+        WhitespaceTokenizerFactory icutf = new WhitespaceTokenizerFactory(Collections.<String, String>emptyMap());
+        HashMap<String, String> sffargs = new HashMap<String, String>();
+        sffargs.put("minShingleSize", ""+min);
+        sffargs.put("maxShingleSize", ""+min);
+        sffargs.put("outputUnigrams", "false");
+        sffargs.put("outputUnigramsIfNoShingles", "false");
+        sffargs.put("tokenSeparator", " ");
+        ShingleFilterFactory sff = new ShingleFilterFactory(sffargs);
+        HashMap<String, String> lshffargs = new HashMap<String, String>();
+        lshffargs.put("hashCount", ""+hashCount);
+        lshffargs.put("hashSetSize", ""+hashSetSize);
+        MinHashFilterFactory lshff = new MinHashFilterFactory(lshffargs);
+
+        TokenizerChain chain = new TokenizerChain(new CharFilterFactory[]{}, icutf, new TokenFilterFactory[]{sff, lshff});
+        return chain;
+    }
+    
+    private boolean isLessThan(HashCode hash1, HashCode hash2)
+    {
+        long high1 = MinHashFilter.secondLong(hash1);
+        long high2 = MinHashFilter.secondLong(hash2);
+        
+        if(MinHashFilter.isLessThanUnsigned(high1, high2))
+        {
+            return true;
+        }
+        else if(high1 == high2)
+        {
+            long low1 = MinHashFilter.firstLong(hash1);
+            long low2 = MinHashFilter.firstLong(hash2);
+            return(MinHashFilter.isLessThanUnsigned(low1, low2));
+        }
+        else
+        {
+            return false;
+        }
+    }
+    
+    
+    /**
+     * An analyzer that uses a tokenizer and a list of token filters to
+     * create a TokenStream - lifted from SOLR to make this analyzer test lucene only. 
+     */
+    public static class TokenizerChain extends Analyzer {
+      
+      final private CharFilterFactory[] charFilters;
+      final private TokenizerFactory tokenizer;
+      final private TokenFilterFactory[] filters;
+
+     
+      /** 
+       * Creates a new TokenizerChain.
+       *
+       * @param charFilters Factories for the CharFilters to use, if any - if null, will be treated as if empty.
+       * @param tokenizer Factory for the Tokenizer to use, must not be null.
+       * @param filters Factories for the TokenFilters to use if any- if null, will be treated as if empty.
+       */
+      public TokenizerChain(CharFilterFactory[] charFilters, TokenizerFactory tokenizer, TokenFilterFactory[] filters) {
+        this.charFilters = charFilters;
+        this.tokenizer = tokenizer;
+        this.filters = filters;
+      }
+
+      @Override
+      public Reader initReader(String fieldName, Reader reader) {
+        if (charFilters != null && charFilters.length > 0) {
+          Reader cs = reader;
+          for (CharFilterFactory charFilter : charFilters) {
+            cs = charFilter.create(cs);
+          }
+          reader = cs;
+        }
+        return reader;
+      }
+
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tk = tokenizer.create();
+        TokenStream ts = tk;
+        for (TokenFilterFactory filter : filters) {
+          ts = filter.create(ts);
+        }
+        return new TokenStreamComponents(tk, ts);
+      }
+
+      @Override
+      public String toString() {
+        StringBuilder sb = new StringBuilder("TokenizerChain(");
+        for (CharFilterFactory filter: charFilters) {
+          sb.append(filter);
+          sb.append(", ");
+        }
+        sb.append(tokenizer);
+        for (TokenFilterFactory filter: filters) {
+          sb.append(", ");
+          sb.append(filter);
+        }
+        sb.append(')');
+        return sb.toString();
+      }
+    }
+}
