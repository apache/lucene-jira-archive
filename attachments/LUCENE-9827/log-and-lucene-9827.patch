diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java
index ce782dad72f..85ecfa5876c 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java
@@ -46,6 +46,8 @@ import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.packed.PackedInts;
 
+import java.util.logging.Logger;
+
 import static org.apache.lucene.search.DocIdSetIterator.NO_MORE_DOCS;
 
 /**
@@ -53,6 +55,7 @@ import static org.apache.lucene.search.DocIdSetIterator.NO_MORE_DOCS;
  * @lucene.experimental
  */
 public final class CompressingStoredFieldsWriter extends StoredFieldsWriter {
+  private Logger log = Logger.getLogger(getClass().getName());
 
   /** Extension of stored fields file */
   public static final String FIELDS_EXTENSION = "fdt";
@@ -556,6 +559,7 @@ public final class CompressingStoredFieldsWriter extends StoredFieldsWriter {
 
       // if its some other format, or an older version of this format, or safety switch:
       if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {
+        log.info("merge=[naive]");
         // naive merge...
         StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];
         if (storedFieldsReader != null) {
@@ -574,7 +578,8 @@ public final class CompressingStoredFieldsWriter extends StoredFieldsWriter {
                  matchingFieldsReader.getChunkSize() == chunkSize && 
                  matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&
                  liveDocs == null &&
-                 !tooDirty(matchingFieldsReader)) { 
+                 !tooDirty(matchingFieldsReader)) {
+        long start = System.nanoTime();
         // optimized merge, raw byte copy
         // its not worth fine-graining this if there are deletions.
         
@@ -635,7 +640,10 @@ public final class CompressingStoredFieldsWriter extends StoredFieldsWriter {
         // since we bulk merged all chunks, we inherit any dirty ones from this segment.
         numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();
         numDirtyDocs += matchingFieldsReader.getNumDirtyDocs();
+        long end = System.nanoTime();
+        log.info("merge=[optimized], tookNs=[" + (end - start) + "], tooDirty=[" + (tooDirty(matchingFieldsReader)) + "], numDirtyDocs=[" + matchingFieldsReader.getNumDirtyDocs() + "], numDocs=[" + matchingFieldsReader.getNumDocs() + "], numDirtyChunks=[" + (matchingFieldsReader.getNumDirtyChunks()) + "], numChunks=[-1]");
       } else {
+        long start = System.nanoTime();
         // optimized merge, we copy serialized (but decompressed) bytes directly
         // even on simple docs (1 stored field), it seems to help by about 20%
         
@@ -654,6 +662,8 @@ public final class CompressingStoredFieldsWriter extends StoredFieldsWriter {
           finishDocument();
           ++docCount;
         }
+        long end = System.nanoTime();
+        log.info("merge=[semi-optimized], tookNs=[" + (end - start) + "], tooDirty=[" + (tooDirty(matchingFieldsReader)) + "], numDirtyDocs=[" + matchingFieldsReader.getNumDirtyDocs() + "], numDocs=[" + matchingFieldsReader.getNumDocs() + "], numDirtyChunks=[" + (matchingFieldsReader.getNumDirtyChunks()) + "], numChunks=[-1]");
       }
     }
     finish(mergeState.mergeFieldInfos, docCount);
@@ -669,8 +679,8 @@ public final class CompressingStoredFieldsWriter extends StoredFieldsWriter {
    */
   boolean tooDirty(CompressingStoredFieldsReader candidate) {
     // more than 1% dirty, or more than hard limit of 1024 dirty chunks
-    return candidate.getNumDirtyChunks() > 1024 || 
-           candidate.getNumDirtyDocs() * 100 > candidate.getNumDocs();
+    return candidate.getNumDirtyChunks() > 1024
+        || (candidate.getNumDirtyChunks() > 1 && candidate.getNumDirtyDocs() * 100 > candidate.getNumDocs());
   }
 
   private static class CompressingStoredFieldsMergeSub extends DocIDMerger.Sub {
