Index: lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java	(revision c9562be943ff40f6c6ed2710c379b0852a4d1577)
+++ lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java	(revision )
@@ -603,11 +603,11 @@
    * @return a query that will return docs like the passed Readers.
    */
   public Query like(String fieldName, Reader... readers) throws IOException {
-    Map<String, Int> words = new HashMap<>();
+    Map<String, Map<String, Int>> field2termFreqMap = new HashMap<>();
     for (Reader r : readers) {
-      addTermFrequencies(r, words, fieldName);
+      addTermFrequencies(r, field2termFreqMap, fieldName);
     }
-    return createQuery(createQueue(words));
+    return createQuery(createQueue(field2termFreqMap));
   }
 
   /**
@@ -642,58 +642,65 @@
   /**
    * Create a PriorityQueue from a word-&gt;tf map.
    *
-   * @param words a map of words keyed on the word(String) with Int objects as the values.
+   * @param field2termFreqMap a map of words keyed on the word(String) with Int objects as the values per field.
    */
-  private PriorityQueue<ScoreTerm> createQueue(Map<String, Int> words) throws IOException {
+  private PriorityQueue<ScoreTerm> createQueue(Map<String, Map<String, Int>> field2termFreqMap) throws IOException {
     // have collected all words in doc and their freqs
     int numDocs = ir.numDocs();
-    final int limit = Math.min(maxQueryTerms, words.size());
-    FreqQ queue = new FreqQ(limit); // will order words by score
+    FreqQ queue;// will order words by score
+    final int limit = Math.min(maxQueryTerms, this.getTermsCount(field2termFreqMap));
+    queue = new FreqQ(limit);
+    for (String fieldName : field2termFreqMap.keySet()) {
+      Map<String, Int> word2termFrequency = field2termFreqMap.get(fieldName);
 
-    for (String word : words.keySet()) { // for every word
-      int tf = words.get(word).x; // term freq in the source doc
+
+      for (String word : word2termFrequency.keySet()) { // for every word
+        int tf = word2termFrequency.get(word).x; // term freq in the source doc
-      if (minTermFreq > 0 && tf < minTermFreq) {
-        continue; // filter out words that don't occur enough times in the source
-      }
+        if (minTermFreq > 0 && tf < minTermFreq) {
+          continue; // filter out words that don't occur enough times in the source
+        }
 
-      // go through all the fields and find the largest document frequency
-      String topField = fieldNames[0];
-      int docFreq = 0;
-      for (String fieldName : fieldNames) {
-        int freq = ir.docFreq(new Term(fieldName, word));
-        topField = (freq > docFreq) ? fieldName : topField;
-        docFreq = (freq > docFreq) ? freq : docFreq;
-      }
+        int docFreq = ir.docFreq(new Term(fieldName, word));
 
-      if (minDocFreq > 0 && docFreq < minDocFreq) {
-        continue; // filter out words that don't occur in enough docs
-      }
+        if (minDocFreq > 0 && docFreq < minDocFreq) {
+          continue; // filter out words that don't occur in enough docs
+        }
 
-      if (docFreq > maxDocFreq) {
-        continue; // filter out words that occur in too many docs
-      }
+        if (docFreq > maxDocFreq) {
+          continue; // filter out words that occur in too many docs
+        }
 
-      if (docFreq == 0) {
-        continue; // index update problem?
-      }
+        if (docFreq == 0) {
+          continue; // index update problem?
+        }
 
-      float idf = similarity.idf(docFreq, numDocs);
-      float score = tf * idf;
+        float idf = similarity.idf(docFreq, numDocs);
+        float score = tf * idf;
 
-      if (queue.size() < limit) {
-        // there is still space in the queue
+        if (queue.size() < limit) {
+          // there is still space in the queue
-        queue.add(new ScoreTerm(word, topField, score, idf, docFreq, tf));
+          queue.add(new ScoreTerm(word, fieldName, score, idf, docFreq, tf));
-      } else {
-        ScoreTerm term = queue.top();
-        if (term.score < score) { // update the smallest in the queue in place and update the queue.
+        } else {
+          ScoreTerm term = queue.top();
+          if (term.score < score) { // update the smallest in the queue in place and update the queue.
-          term.update(word, topField, score, idf, docFreq, tf);
+            term.update(word, fieldName, score, idf, docFreq, tf);
-          queue.updateTop();
-        }
-      }
-    }
+            queue.updateTop();
+          }
+        }
+      }
+    }
     return queue;
   }
 
+  private int getTermsCount(Map<String, Map<String, Int>> field2termFreqMap) {
+    int totalTermsCount = 0;
+    Collection<Map<String, Int>> values = field2termFreqMap.values();
+    for (Map<String, Int> word2termFrequency : values) {
+      totalTermsCount += word2termFrequency.size();
+    }
+    return totalTermsCount;
+  }
+
   /**
    * Describe the parameters that control how the "more like this" query is formed.
    */
@@ -721,7 +728,7 @@
    * @param docNum the id of the lucene document from which to find terms
    */
   private PriorityQueue<ScoreTerm> retrieveTerms(int docNum) throws IOException {
-    Map<String, Int> termFreqMap = new HashMap<>();
+    Map<String, Map<String, Int>> field2termFreqMap = new HashMap<>();
     for (String fieldName : fieldNames) {
       final Fields vectors = ir.getTermVectors(docNum);
       final Terms vector;
@@ -738,43 +745,48 @@
         for (StorableField field : fields) {
           final String stringValue = field.stringValue();
           if (stringValue != null) {
-            addTermFrequencies(new StringReader(stringValue), termFreqMap, fieldName);
+            addTermFrequencies(new StringReader(stringValue), field2termFreqMap, fieldName);
           }
         }
       } else {
-        addTermFrequencies(termFreqMap, vector);
+        addTermFrequencies(field2termFreqMap, vector, fieldName);
       }
     }
 
-    return createQueue(termFreqMap);
+    return createQueue(field2termFreqMap);
   }
 
 
-  private PriorityQueue<ScoreTerm> retrieveTerms(Map<String, Collection<Object>> fields) throws 
+  private PriorityQueue<ScoreTerm> retrieveTerms(Map<String, Collection<Object>> field2fieldValues) throws
       IOException {
-    HashMap<String,Int> termFreqMap = new HashMap<>();
+    Map<String, Map<String, Int>> field2termFreqMap = new HashMap<>();
     for (String fieldName : fieldNames) {
-      for (String field : fields.keySet()) {
-        Collection<Object> fieldValues = fields.get(field);
+      for (String field : field2fieldValues.keySet()) {
+        Collection<Object> fieldValues = field2fieldValues.get(field);
         if(fieldValues == null)
           continue;
         for(Object fieldValue:fieldValues) {
           if (fieldValue != null) {
-            addTermFrequencies(new StringReader(String.valueOf(fieldValue)), termFreqMap,
+            addTermFrequencies(new StringReader(String.valueOf(fieldValue)), field2termFreqMap,
                 fieldName);
           }
         }
       }
     }
-    return createQueue(termFreqMap);
+    return createQueue(field2termFreqMap);
   }
   /**
    * Adds terms and frequencies found in vector into the Map termFreqMap
    *
-   * @param termFreqMap a Map of terms and their frequencies
+   * @param field2termFreqMap a Map of terms and their frequencies per field
    * @param vector List of terms and their frequencies for a doc/field
    */
-  private void addTermFrequencies(Map<String, Int> termFreqMap, Terms vector) throws IOException {
+  private void addTermFrequencies(Map<String, Map<String, Int>> field2termFreqMap, Terms vector, String fieldName) throws IOException {
+    Map<String, Int> termFreqMap = field2termFreqMap.get(fieldName);
+    if (termFreqMap == null) {
+      termFreqMap = new HashMap<>();
+      field2termFreqMap.put(fieldName, termFreqMap);
+    }
     final TermsEnum termsEnum = vector.iterator();
     final CharsRefBuilder spare = new CharsRefBuilder();
     BytesRef text;
@@ -802,15 +814,20 @@
    * Adds term frequencies found by tokenizing text from reader into the Map words
    *
    * @param r a source of text to be tokenized
-   * @param termFreqMap a Map of terms and their frequencies
+   * @param field2termFreqMap a Map of terms and their frequencies per field
    * @param fieldName Used by analyzer for any special per-field analysis
    */
-  private void addTermFrequencies(Reader r, Map<String, Int> termFreqMap, String fieldName)
+  private void addTermFrequencies(Reader r, Map<String, Map<String, Int>> field2termFreqMap, String fieldName)
       throws IOException {
     if (analyzer == null) {
       throw new UnsupportedOperationException("To use MoreLikeThis without " +
           "term vectors, you must provide an Analyzer");
     }
+    Map<String, Int> termFreqMap = field2termFreqMap.get(fieldName);
+    if (termFreqMap == null) {
+      termFreqMap = new HashMap<>();
+      field2termFreqMap.put(fieldName, termFreqMap);
+    }
     try (TokenStream ts = analyzer.tokenStream(fieldName, r)) {
       int tokenCount = 0;
       // for every token
@@ -880,9 +897,9 @@
    * @see #retrieveInterestingTerms
    */
   private PriorityQueue<ScoreTerm> retrieveTerms(Reader r, String fieldName) throws IOException {
-    Map<String, Int> words = new HashMap<>();
-    addTermFrequencies(r, words, fieldName);
-    return createQueue(words);
+    Map<String, Map<String, Int>> field2termFreqMap = new HashMap<>();
+    addTermFrequencies(r, field2termFreqMap, fieldName);
+    return createQueue(field2termFreqMap);
   }
 
   /**
Index: lucene/queries/src/test/org/apache/lucene/queries/mlt/TestMoreLikeThis.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- lucene/queries/src/test/org/apache/lucene/queries/mlt/TestMoreLikeThis.java	(revision c9562be943ff40f6c6ed2710c379b0852a4d1577)
+++ lucene/queries/src/test/org/apache/lucene/queries/mlt/TestMoreLikeThis.java	(revision )
@@ -19,6 +19,7 @@
 
 import java.io.IOException;
 import java.io.StringReader;
+import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.HashMap;
@@ -41,8 +42,13 @@
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
 
 public class TestMoreLikeThis extends LuceneTestCase {
+  public static final String SHOP_TYPE = "type";
+  public static final String FOR_SALE = "weSell";
+  public static final String NOT_FOR_SALE = "weDontSell";
+
   private Directory directory;
   private IndexReader reader;
   private IndexSearcher searcher;
@@ -247,5 +253,74 @@
     return generatedStrings;
   }
 
+  private int addShopDoc(RandomIndexWriter writer, String type, String[] weSell, String[] weDontSell) throws IOException {
+    Document doc = new Document();
+    doc.add(newTextField(SHOP_TYPE, type, Field.Store.YES));
+    for (String item : weSell) {
+      doc.add(newTextField(FOR_SALE, item, Field.Store.YES));
+    }
+    for (String item : weDontSell) {
+      doc.add(newTextField(NOT_FOR_SALE, item, Field.Store.YES));
+    }
+    writer.addDocument(doc);
+    return writer.numDocs() - 1;
+  }
+
+  public void test_multiField_shouldReturnPerFieldBooleanQuery() throws Exception {
+    int maxQueryTerms = 25;
+
+    String[] itShopItemForSale = new String[]{"watch", "ipod", "asrock", "imac", "macbookpro", "monitor", "keyboard", "mouse", "speakers"};
+    String[] itShopItemNotForSale = new String[]{"tie", "trousers", "shoes", "skirt", "hat"};
+
+    String[] clothesShopItemForSale = new String[]{"tie", "trousers", "shoes", "skirt", "hat"};
+    String[] clothesShopItemNotForSale = new String[]{"watch", "ipod", "asrock", "imac", "macbookpro", "monitor", "keyboard", "mouse", "speakers"};
+
+    // add series of shop docs
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    for (int i = 0; i < 100; i++) {
+      addShopDoc(writer, "it", itShopItemForSale, itShopItemNotForSale);
+    }
+    for (int i = 0; i < 10; i++) {
+      addShopDoc(writer, "clothes", clothesShopItemForSale, clothesShopItemNotForSale);
+    }
+    // Input Document is a clothes shop
+    int inputDocId = addShopDoc(writer, "clothes", clothesShopItemForSale, clothesShopItemNotForSale);
+    IndexReader reader = writer.getReader();
+    writer.close();
+
+    // setup MLT query
+    MoreLikeThis mlt = new MoreLikeThis(reader);
+    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false);
+    mlt.setAnalyzer(analyzer);
+    mlt.setMaxQueryTerms(maxQueryTerms);
+    mlt.setMinDocFreq(1);
+    mlt.setMinTermFreq(1);
+    mlt.setMinWordLen(1);
+    mlt.setFieldNames(new String[]{FOR_SALE, NOT_FOR_SALE});
+
+    // perform MLT query
+    BooleanQuery query = (BooleanQuery) mlt.like(inputDocId);
+    Collection<BooleanClause> clauses = query.clauses();
+
+    Collection<BooleanClause> expectedClothesShopClauses = new ArrayList<BooleanClause>();
+    for (String itemForSale : clothesShopItemForSale) {
+      BooleanClause booleanClause = new BooleanClause(new TermQuery(new Term(FOR_SALE, itemForSale)), BooleanClause.Occur.SHOULD);
+      expectedClothesShopClauses.add(booleanClause);
+    }
+    for (String itemNotForSale : clothesShopItemNotForSale) {
+      BooleanClause booleanClause = new BooleanClause(new TermQuery(new Term(NOT_FOR_SALE, itemNotForSale)), BooleanClause.Occur.SHOULD);
+      expectedClothesShopClauses.add(booleanClause);
+    }
+
+    for (BooleanClause expectedClause : expectedClothesShopClauses) {
+      assertTrue(clauses.contains(expectedClause));
+    }
+
+    // clean up
+    reader.close();
+    dir.close();
+    analyzer.close();
+  }
   // TODO: add tests for the MoreLikeThisQuery
 }
