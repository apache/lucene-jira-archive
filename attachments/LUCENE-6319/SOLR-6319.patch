Index: lucene/CHANGES.txt
===================================================================
--- lucene/CHANGES.txt	(revision 1672228)
+++ lucene/CHANGES.txt	(working copy)
@@ -25,6 +25,11 @@
   dictionary, exposed as AutoPrefixPostingsFormat (Adrien Grand,
   Uwe Schindler, Robert Muir, Mike McCandless)
 
+* LUCENE-6319: Added a DelegatingOneMerge to support the ability to
+  decorate merge logic.  SortingOneMerge now extends DelegatingOneMerge
+  and has a second constructor that accepts an AbstractOneMerge delegate.
+  (Elliott Bradshaw)
+
 Optimizations
 
 * LUCENE-6379: IndexWriter.deleteDocuments(Query...) now detects if
@@ -48,6 +53,9 @@
 * LUCENE-6410: Removed unused "reuse" parameter to
   Terms.iterator. (Robert Muir, Mike McCandless)
 
+* LUCENE-6319: Moved OneMerge member variables: segments, rateLimiter and 
+  totalMaxDoc behind public getters.  (Elliott Bradshaw)
+
 ======================= Lucene 5.1.0 =======================
 
 New Features
Index: lucene/core/src/java/org/apache/lucene/index/AbstractOneMerge.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/AbstractOneMerge.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/index/AbstractOneMerge.java	(working copy)
@@ -0,0 +1,228 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+import java.util.List;
+import org.apache.lucene.index.MergePolicy.OneMerge;
+import org.apache.lucene.store.MergeInfo;
+
+/**
+ * <p>
+ * Expert: Base OneMerge class implementing core merge functionality.</p>
+ *
+ * <p>
+ * Implementations should subclass OneMerge or DelegatingOneMerge </p>
+ *
+ * @lucene.experimental
+ */
+public abstract class AbstractOneMerge {
+
+  //Package-Private Access
+  private SegmentCommitInfo info;         // used by IndexWriter
+  private boolean registerDone;           // used by IndexWriter
+  private long mergeGen;                  // used by IndexWriter
+  private boolean isExternal;             // used by IndexWriter
+  private int maxNumSegments = -1;        // used by IndexWriter
+
+  /**
+   * Estimated size in bytes of the merged segment.
+   */
+  private volatile long estimatedMergeBytes;       // used by IndexWriter
+
+  // Sum of sizeInBytes of all SegmentInfos; set by IW.mergeInit
+  private volatile long totalMergeBytes;
+
+  // Nanosecond time of merge start
+  private volatile long mergeStartNS = -1;
+
+  private List<SegmentReader> readers;        // used by IndexWriter
+
+  private Throwable error;
+
+  /** 
+   * Sole constructor.
+   */
+  public AbstractOneMerge() {
+  }
+
+  /**
+   * Record that an exception occurred while executing this merge
+   */
+  synchronized void setException(Throwable error) {
+    this.error = error;
+  }
+
+  /**
+   * Retrieve previous exception set by {@link
+   *  #setException}.
+   */
+  synchronized Throwable getException() {
+    return error;
+  }
+
+  // Package-Private Access
+  boolean isRegisterDone() {
+    return registerDone;
+  }
+
+  void setRegisterDone(boolean registerDone) {
+    this.registerDone = registerDone;
+  }
+
+  long getMergeGen() {
+    return mergeGen;
+  }
+
+  void setMergeGen(long mergeGen) {
+    this.mergeGen = mergeGen;
+  }
+
+  boolean isIsExternal() {
+    return isExternal;
+  }
+
+  void setIsExternal(boolean isExternal) {
+    this.isExternal = isExternal;
+  }
+
+  int getMaxNumSegments() {
+    return maxNumSegments;
+  }
+
+  void setMaxNumSegments(int maxNumSegments) {
+    this.maxNumSegments = maxNumSegments;
+  }
+
+  long getEstimatedMergeBytes() {
+    return estimatedMergeBytes;
+  }
+
+  void setEstimatedMergeBytes(long estimatedMergeBytes) {
+    this.estimatedMergeBytes = estimatedMergeBytes;
+  }
+
+  long getTotalMergeBytes() {
+    return totalMergeBytes;
+  }
+
+  void setTotalMergeBytes(long totalMergeBytes) {
+    this.totalMergeBytes = totalMergeBytes;
+  }
+
+  long getMergeStartNS() {
+    return mergeStartNS;
+  }
+
+  void setMergeStartNS(long mergeStartNS) {
+    this.mergeStartNS = mergeStartNS;
+  }
+
+  List<SegmentReader> getReaders() {
+    return readers;
+  }
+
+  void setReaders(List<SegmentReader> readers) {
+    this.readers = readers;
+  }
+
+  Throwable getError() {
+    return error;
+  }
+
+  void setError(Throwable error) {
+    this.error = error;
+  }
+
+  SegmentCommitInfo getInfo() {
+    return this.info;
+  }
+  
+  /**
+   * Expert: Sets the {@link SegmentCommitInfo} of this {@link OneMerge}. Allows
+   * sub-classes to e.g. set diagnostics properties.
+   */
+  public void setInfo(SegmentCommitInfo info) {
+    this.info = info;
+  }
+
+  /**
+   * Called by {@link IndexWriter} after the merge is done and all readers have
+   * been closed.
+   */
+  public abstract void mergeFinished() throws IOException;
+
+  /**
+   * Expert: Get the list of readers to merge. Note that this list does not
+   * necessarily match the list of segments to merge and should only be used to
+   * feed SegmentMerger to initialize a merge. When a {@link OneMerge} reorders
+   * doc IDs, it must override {@link #getDocMap} too so that deletes that
+   * happened during the merge can be applied to the newly merged segment.
+   */
+  public abstract List<CodecReader> getMergeReaders() throws IOException;
+
+  /**
+   * Expert: If {@link #getMergeReaders()} reorders document IDs, this method
+   * must be overridden to return a mapping from the <i>natural</i> doc ID (the
+   * doc ID that would result from a natural merge) to the actual doc ID. This
+   * mapping is used to apply deletions that happened during the merge to the
+   * new segment.
+   */
+  public abstract MergePolicy.DocMap getDocMap(MergeState mergeState);
+
+  /**
+   * Returns a readable description of the current merge state.
+   */
+  public abstract String segString();
+
+  /**
+   * Returns the total size in bytes of this merge. Note that this does not
+   * indicate the size of the merged segment, but the input total size. This is
+   * only set once the merge is initialized by IndexWriter.
+   */
+  public abstract long totalBytesSize() throws IOException;
+
+  /**
+   * Returns the total number of documents that are included with this merge.
+   * Note that this does not indicate the number of documents after the merge.
+   *
+   */
+  public abstract int totalNumDocs() throws IOException;
+
+  /**
+   * Return {@link MergeInfo} describing this merge.
+   */
+  public abstract MergeInfo getMergeInfo();
+
+  /**
+   * Return list of segments to be merged.
+   */
+  public abstract List<SegmentCommitInfo> getSegments();
+
+  /**
+   * Return private {@link org.apache.lucene.store.RateLimiter} for this merge, used to rate limit
+   * writes and abort.
+   */
+  public abstract MergeRateLimiter getRateLimiter();
+
+  /**
+   * Total number of documents in segments to be merged, not accounting for
+   * deletions.
+   */
+  public abstract int getTotalMaxDoc();
+
+}

Property changes on: lucene/core/src/java/org/apache/lucene/index/AbstractOneMerge.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/index/ConcurrentMergeScheduler.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/ConcurrentMergeScheduler.java	(revision 1672228)
+++ lucene/core/src/java/org/apache/lucene/index/ConcurrentMergeScheduler.java	(working copy)
@@ -262,7 +262,7 @@
 
     for (threadIdx=activeMergeCount-1;threadIdx>=0;threadIdx--) {
       MergeThread mergeThread = activeMerges.get(threadIdx);
-      if (mergeThread.merge.estimatedMergeBytes > MIN_BIG_MERGE_MB*1024*1024) {
+      if (mergeThread.merge.getEstimatedMergeBytes() > MIN_BIG_MERGE_MB*1024*1024) {
         bigMergeCount = 1+threadIdx;
         break;
       }
@@ -289,21 +289,21 @@
       double newMBPerSec;
       if (doPause) {
         newMBPerSec = 0.0;
-      } else if (merge.maxNumSegments != -1) {
+      } else if (merge.getMaxNumSegments() != -1) {
         newMBPerSec = forceMergeMBPerSec;
       } else if (doAutoIOThrottle == false) {
         newMBPerSec = Double.POSITIVE_INFINITY;
-      } else if (merge.estimatedMergeBytes < MIN_BIG_MERGE_MB*1024*1024) {
+      } else if (merge.getEstimatedMergeBytes() < MIN_BIG_MERGE_MB*1024*1024) {
         // Don't rate limit small merges:
         newMBPerSec = Double.POSITIVE_INFINITY;
       } else {
         newMBPerSec = targetMBPerSec;
       }
 
-      double curMBPerSec = merge.rateLimiter.getMBPerSec();
+      double curMBPerSec = merge.getRateLimiter().getMBPerSec();
       
       if (verbose()) {
-        long mergeStartNS = merge.mergeStartNS;
+        long mergeStartNS = merge.getMergeStartNS();
         if (mergeStartNS == -1) {
           // IndexWriter didn't start the merge yet:
           mergeStartNS = now;
@@ -311,12 +311,12 @@
         message.append('\n');
         message.append(String.format(Locale.ROOT, "merge thread %s estSize=%.1f MB (written=%.1f MB) runTime=%.1fs (stopped=%.1fs, paused=%.1fs) rate=%s\n",
                                      mergeThread.getName(),
-                                     bytesToMB(merge.estimatedMergeBytes),
-                                     bytesToMB(merge.rateLimiter.totalBytesWritten),
+                                     bytesToMB(merge.getEstimatedMergeBytes()),
+                                     bytesToMB(merge.getRateLimiter().totalBytesWritten),
                                      nsToSec(now - mergeStartNS),
-                                     nsToSec(merge.rateLimiter.getTotalStoppedNS()),
-                                     nsToSec(merge.rateLimiter.getTotalPausedNS()),
-                                     rateToString(merge.rateLimiter.getMBPerSec())));
+                                     nsToSec(merge.getRateLimiter().getTotalStoppedNS()),
+                                     nsToSec(merge.getRateLimiter().getTotalPausedNS()),
+                                     rateToString(merge.getRateLimiter().getMBPerSec())));
 
         if (newMBPerSec != curMBPerSec) {
           if (newMBPerSec == 0.0) {
@@ -337,7 +337,7 @@
         }
       }
 
-      merge.rateLimiter.setMBPerSec(newMBPerSec);
+      merge.getRateLimiter().setMBPerSec(newMBPerSec);
     }
     if (verbose()) {
       message(message.toString());
@@ -410,7 +410,7 @@
     Thread currentThread = Thread.currentThread();
     int count = 0;
     for (MergeThread mergeThread : mergeThreads) {
-      if (currentThread != mergeThread && mergeThread.isAlive() && mergeThread.merge.rateLimiter.getAbort() == false) {
+      if (currentThread != mergeThread && mergeThread.isAlive() && mergeThread.merge.getRateLimiter().getAbort() == false) {
         count++;
       }
     }
@@ -463,7 +463,7 @@
       boolean success = false;
       try {
         if (verbose()) {
-          message("  consider merge " + writer.segString(merge.segments));
+          message("  consider merge " + writer.segString(merge.getSegments()));
         }
 
         // OK to spawn a new merge thread to handle this
@@ -572,7 +572,7 @@
     @Override
     public int compareTo(MergeThread other) {
       // Larger merges sort first:
-      return Long.compare(other.merge.estimatedMergeBytes, merge.estimatedMergeBytes);
+      return Long.compare(other.merge.getEstimatedMergeBytes(), merge.getEstimatedMergeBytes());
     }
 
     @Override
@@ -663,14 +663,14 @@
   }
 
   private boolean isBacklog(long now, OneMerge merge) {
-    double mergeMB = bytesToMB(merge.estimatedMergeBytes);
+    double mergeMB = bytesToMB(merge.getEstimatedMergeBytes());
     for (MergeThread mergeThread : mergeThreads) {
-      long mergeStartNS = mergeThread.merge.mergeStartNS;
+      long mergeStartNS = mergeThread.merge.getMergeStartNS();
       if (mergeThread.isAlive() && mergeThread.merge != merge &&
           mergeStartNS != -1 &&
-          mergeThread.merge.estimatedMergeBytes >= MIN_BIG_MERGE_MB*1024*1024 &&
+          mergeThread.merge.getEstimatedMergeBytes() >= MIN_BIG_MERGE_MB*1024*1024 &&
           nsToSec(now-mergeStartNS) > 3.0) {
-        double otherMergeMB = bytesToMB(mergeThread.merge.estimatedMergeBytes);
+        double otherMergeMB = bytesToMB(mergeThread.merge.getEstimatedMergeBytes());
         double ratio = otherMergeMB / mergeMB;
         if (ratio > 0.3 && ratio < 3.0) {
           return true;
@@ -687,7 +687,7 @@
       return;
     }
 
-    double mergeMB = bytesToMB(newMerge.estimatedMergeBytes);
+    double mergeMB = bytesToMB(newMerge.getEstimatedMergeBytes());
     if (mergeMB < MIN_BIG_MERGE_MB) {
       // Only watch non-trivial merges for throttling; this is safe because the MP must eventually
       // have to do larger merges:
@@ -756,12 +756,12 @@
 
     double rate;
 
-    if (newMerge.maxNumSegments != -1) {
+    if (newMerge.getMaxNumSegments() != -1) {
       rate = forceMergeMBPerSec;
     } else {
       rate = targetMBPerSec;
     }
-    newMerge.rateLimiter.setMBPerSec(rate);
+    newMerge.getRateLimiter().setMBPerSec(rate);
     targetMBPerSecChanged();
   }
 
Index: lucene/core/src/java/org/apache/lucene/index/DelegatingOneMerge.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/DelegatingOneMerge.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/index/DelegatingOneMerge.java	(working copy)
@@ -0,0 +1,212 @@
+package org.apache.lucene.index;
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+import org.apache.lucene.index.MergePolicy.OneMerge;
+import org.apache.lucene.store.MergeInfo;
+
+/**
+ * <p>
+ * Expert: Used to decorate merge logic.</p>
+ *
+ * @lucene.experimental
+ */
+public class DelegatingOneMerge extends OneMerge {
+
+  private final AbstractOneMerge delegate;
+
+  /**
+   * Construct with a custom OneMerge.
+   *
+   * @param delegate {@link AbstractOneMerge} to delegate to. .
+   */
+  public DelegatingOneMerge(AbstractOneMerge delegate) {
+    this.delegate = delegate;
+  }
+
+  /**
+   * Construct with a default OneMerge as a delegate.
+   *
+   * @param segments List of {@link SegmentCommitInfo}s to be merged.
+   */
+  public DelegatingOneMerge(List<SegmentCommitInfo> segments) {
+    this.delegate = new OneMerge(segments);
+  }
+
+  @Override
+  synchronized void setException(Throwable error) {
+    delegate.setException(error);
+  }
+
+  @Override
+  synchronized Throwable getException() {
+    return delegate.getException();
+  }
+
+  @Override
+  boolean isRegisterDone() {
+    return delegate.isRegisterDone();
+  }
+
+  @Override
+  void setRegisterDone(boolean registerDone) {
+    delegate.setRegisterDone(registerDone);
+  }
+
+  @Override
+  long getMergeGen() {
+    return delegate.getMergeGen();
+  }
+
+  @Override
+  void setMergeGen(long mergeGen) {
+    delegate.setMergeGen(mergeGen);
+  }
+
+  @Override
+  boolean isIsExternal() {
+    return delegate.isIsExternal();
+  }
+
+  @Override
+  void setIsExternal(boolean isExternal) {
+    delegate.setIsExternal(isExternal);
+  }
+
+  @Override
+  int getMaxNumSegments() {
+    return delegate.getMaxNumSegments();
+  }
+
+  @Override
+  void setMaxNumSegments(int maxNumSegments) {
+    delegate.setMaxNumSegments(maxNumSegments);
+  }
+
+  @Override
+  long getEstimatedMergeBytes() {
+    return delegate.getEstimatedMergeBytes();
+  }
+
+  @Override
+  void setEstimatedMergeBytes(long estimatedMergeBytes) {
+    delegate.setEstimatedMergeBytes(estimatedMergeBytes);
+  }
+
+  @Override
+  long getTotalMergeBytes() {
+    return delegate.getTotalMergeBytes();
+  }
+
+  @Override
+  void setTotalMergeBytes(long totalMergeBytes) {
+    delegate.setTotalMergeBytes(totalMergeBytes);
+  }
+
+  @Override
+  long getMergeStartNS() {
+    return delegate.getMergeStartNS();
+  }
+
+  @Override
+  void setMergeStartNS(long mergeStartNS) {
+    delegate.setMergeStartNS(mergeStartNS);
+  }
+
+  @Override
+  List<SegmentReader> getReaders() {
+    return delegate.getReaders();
+  }
+
+  @Override
+  void setReaders(List<SegmentReader> readers) {
+    delegate.setReaders(readers);
+  }
+
+  @Override
+  Throwable getError() {
+    return delegate.getError();
+  }
+
+  @Override
+  void setError(Throwable error) {
+    delegate.setError(error);
+  }
+
+  @Override
+  SegmentCommitInfo getInfo() {
+    return delegate.getInfo();
+  }
+
+  @Override
+  public void mergeFinished() throws IOException {
+    delegate.mergeFinished();
+  }
+
+  @Override
+  public List<CodecReader> getMergeReaders() throws IOException {
+    return delegate.getMergeReaders();
+  }
+
+  @Override
+  public void setInfo(SegmentCommitInfo info) {
+    delegate.setInfo(info);
+  }
+
+  @Override
+  public MergePolicy.DocMap getDocMap(MergeState mergeState) {
+    return delegate.getDocMap(mergeState);
+  }
+
+  @Override
+  public String segString() {
+    return delegate.segString();
+  }
+
+  @Override
+  public long totalBytesSize() throws IOException {
+    return delegate.totalBytesSize();
+  }
+
+  @Override
+  public int totalNumDocs() throws IOException {
+    return delegate.totalNumDocs();
+  }
+
+  @Override
+  public MergeInfo getMergeInfo() {
+    return delegate.getMergeInfo();
+  }
+
+  @Override
+  public final List<SegmentCommitInfo> getSegments() {
+    return delegate.getSegments();
+  }
+
+  @Override
+  public final MergeRateLimiter getRateLimiter() {
+    return delegate.getRateLimiter();
+  }
+
+  @Override
+  public final int getTotalMaxDoc() {
+    return delegate.getTotalMaxDoc();
+  }
+
+}

Property changes on: lucene/core/src/java/org/apache/lucene/index/DelegatingOneMerge.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/IndexWriter.java	(revision 1672228)
+++ lucene/core/src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -1634,13 +1634,13 @@
       // Now mark all pending & running merges for forced
       // merge:
       for(final MergePolicy.OneMerge merge  : pendingMerges) {
-        merge.maxNumSegments = maxNumSegments;
-        segmentsToMerge.put(merge.info, Boolean.TRUE);
+        merge.setMaxNumSegments(maxNumSegments);
+        segmentsToMerge.put(merge.getInfo(), Boolean.TRUE);
       }
 
       for (final MergePolicy.OneMerge merge: runningMerges) {
-        merge.maxNumSegments = maxNumSegments;
-        segmentsToMerge.put(merge.info, Boolean.TRUE);
+        merge.setMaxNumSegments(maxNumSegments);
+        segmentsToMerge.put(merge.getInfo(), Boolean.TRUE);
       }
     }
 
@@ -1660,7 +1660,7 @@
             final int size = mergeExceptions.size();
             for(int i=0;i<size;i++) {
               final MergePolicy.OneMerge merge = mergeExceptions.get(i);
-              if (merge.maxNumSegments != -1) {
+              if (merge.getMaxNumSegments() != -1) {
                 throw new IOException("background merge hit exception: " + merge.segString(), merge.getException());
               }
             }
@@ -1688,12 +1688,12 @@
    *  runningMerges are maxNumSegments merges. */
   private synchronized boolean maxNumSegmentsMergesPending() {
     for (final MergePolicy.OneMerge merge : pendingMerges) {
-      if (merge.maxNumSegments != -1)
+      if (merge.getMaxNumSegments() != -1)
         return true;
     }
 
     for (final MergePolicy.OneMerge merge : runningMerges) {
-      if (merge.maxNumSegments != -1)
+      if (merge.getMaxNumSegments() != -1)
         return true;
     }
 
@@ -1842,7 +1842,7 @@
         final int numMerges = spec.merges.size();
         for(int i=0;i<numMerges;i++) {
           final MergePolicy.OneMerge merge = spec.merges.get(i);
-          merge.maxNumSegments = maxNumSegments;
+          merge.setMaxNumSegments(maxNumSegments);
         }
       }
     } else {
@@ -2123,18 +2123,18 @@
     // Abort all pending & running merges:
     for (final MergePolicy.OneMerge merge : pendingMerges) {
       if (infoStream.isEnabled("IW")) {
-        infoStream.message("IW", "now abort pending merge " + segString(merge.segments));
+        infoStream.message("IW", "now abort pending merge " + segString(merge.getSegments()));
       }
-      merge.rateLimiter.setAbort();
+      merge.getRateLimiter().setAbort();
       mergeFinish(merge);
     }
     pendingMerges.clear();
 
     for (final MergePolicy.OneMerge merge : runningMerges) {
       if (infoStream.isEnabled("IW")) {
-        infoStream.message("IW", "now abort running merge " + segString(merge.segments));
+        infoStream.message("IW", "now abort running merge " + segString(merge.getSegments()));
       }
-      merge.rateLimiter.setAbort();
+      merge.getRateLimiter().setAbort();
     }
 
     // These merges periodically check whether they have
@@ -3069,7 +3069,7 @@
   }
 
   private synchronized void ensureValidMerge(MergePolicy.OneMerge merge) {
-    for(SegmentCommitInfo info : merge.segments) {
+    for(SegmentCommitInfo info : merge.getSegments()) {
       if (!segmentInfos.contains(info)) {
         throw new MergePolicy.MergeException("MergePolicy selected a segment (" + info.info.name + ") that is not in the current index " + segString(), directory);
       }
@@ -3097,9 +3097,9 @@
     
     final void init(ReaderPool readerPool, MergePolicy.OneMerge merge, MergeState mergeState, boolean initWritableLiveDocs) throws IOException {
       if (mergedDeletesAndUpdates == null) {
-        mergedDeletesAndUpdates = readerPool.get(merge.info, true);
+        mergedDeletesAndUpdates = readerPool.get(merge.getInfo(), true);
         docMap = merge.getDocMap(mergeState);
-        assert docMap.isConsistent(merge.info.info.maxDoc());
+        assert docMap.isConsistent(merge.getInfo().info.maxDoc());
       }
       if (initWritableLiveDocs && !initializedWritableLiveDocs) {
         mergedDeletesAndUpdates.initWritableLiveDocs();
@@ -3138,17 +3138,17 @@
    * been flushed to the segments since the merge was started. This method
    * "carries over" such new deletes and updates onto the newly merged segment,
    * and saves the resulting deletes and updates files (incrementing the delete
-   * and DV generations for merge.info). If no deletes were flushed, no new
+   * and DV generations for merge.getInfo()). If no deletes were flushed, no new
    * deletes file is saved.
    */
   synchronized private ReadersAndUpdates commitMergedDeletesAndUpdates(MergePolicy.OneMerge merge, MergeState mergeState) throws IOException {
 
     testPoint("startCommitMergeDeletes");
 
-    final List<SegmentCommitInfo> sourceSegments = merge.segments;
+    final List<SegmentCommitInfo> sourceSegments = merge.getSegments();
 
     if (infoStream.isEnabled("IW")) {
-      infoStream.message("IW", "commitMergeDeletes " + segString(merge.segments));
+      infoStream.message("IW", "commitMergeDeletes " + segString(merge.getSegments()));
     }
 
     // Carefully merge deletes that occurred after we
@@ -3164,7 +3164,7 @@
       SegmentCommitInfo info = sourceSegments.get(i);
       minGen = Math.min(info.getBufferedDeletesGen(), minGen);
       final int maxDoc = info.info.maxDoc();
-      final Bits prevLiveDocs = merge.readers.get(i).getLiveDocs();
+      final Bits prevLiveDocs = merge.getReaders().get(i).getLiveDocs();
       final ReadersAndUpdates rld = readerPool.get(info, false);
       // We hold a ref so it should still be in the pool:
       assert rld != null: "seg=" + info.info.name;
@@ -3286,7 +3286,7 @@
       }
     }
 
-    assert docUpto == merge.info.info.maxDoc();
+    assert docUpto == merge.getInfo().info.maxDoc();
 
     if (mergedDVUpdates.any()) {
 //      System.out.println("[" + Thread.currentThread().getName() + "] IW.commitMergedDeletes: mergedDeletes.info=" + mergedDeletes.info + ", mergedFieldUpdates=" + mergedFieldUpdates);
@@ -3303,7 +3303,7 @@
       } finally {
         if (!success) {
           holder.mergedDeletesAndUpdates.dropChanges();
-          readerPool.drop(merge.info);
+          readerPool.drop(merge.getInfo());
         }
       }
     }
@@ -3321,7 +3321,7 @@
       }
     }
 
-    merge.info.setBufferedDeletesGen(minGen);
+    merge.getInfo().setBufferedDeletesGen(minGen);
 
     return holder.mergedDeletesAndUpdates;
   }
@@ -3335,10 +3335,10 @@
     }
 
     if (infoStream.isEnabled("IW")) {
-      infoStream.message("IW", "commitMerge: " + segString(merge.segments) + " index=" + segString());
+      infoStream.message("IW", "commitMerge: " + segString(merge.getSegments()) + " index=" + segString());
     }
 
-    assert merge.registerDone;
+    assert merge.isRegisterDone();
 
     // If merge was explicitly aborted, or, if rollback() or
     // rollbackTransaction() had been called since our merge
@@ -3346,7 +3346,7 @@
     // deleter.refresh() call that will remove any index
     // file that current segments does not reference), we
     // abort this merge
-    if (merge.rateLimiter.getAbort()) {
+    if (merge.getRateLimiter().getAbort()) {
       if (infoStream.isEnabled("IW")) {
         infoStream.message("IW", "commitMerge: skip: it was aborted");
       }
@@ -3359,12 +3359,12 @@
       // so it will be dropped shortly anyway, but not
       // doing this  makes  MockDirWrapper angry in
       // TestNRTThreads (LUCENE-5434):
-      readerPool.drop(merge.info);
-      deleter.deleteNewFiles(merge.info.files());
+      readerPool.drop(merge.getInfo());
+      deleter.deleteNewFiles(merge.getInfo().files());
       return false;
     }
 
-    final ReadersAndUpdates mergedUpdates = merge.info.info.maxDoc() == 0 ? null : commitMergedDeletesAndUpdates(merge, mergeState);
+    final ReadersAndUpdates mergedUpdates = merge.getInfo().info.maxDoc() == 0 ? null : commitMergedDeletesAndUpdates(merge, mergeState);
 //    System.out.println("[" + Thread.currentThread().getName() + "] IW.commitMerge: mergedDeletes=" + mergedDeletes);
 
     // If the doc store we are using has been closed and
@@ -3372,16 +3372,16 @@
     // started), then we will switch to the compound
     // format as well:
 
-    assert !segmentInfos.contains(merge.info);
+    assert !segmentInfos.contains(merge.getInfo());
 
-    final boolean allDeleted = merge.segments.size() == 0 ||
-      merge.info.info.maxDoc() == 0 ||
+    final boolean allDeleted = merge.getSegments().size() == 0 ||
+      merge.getInfo().info.maxDoc() == 0 ||
       (mergedUpdates != null &&
-       mergedUpdates.getPendingDeleteCount() == merge.info.info.maxDoc());
+       mergedUpdates.getPendingDeleteCount() == merge.getInfo().info.maxDoc());
 
     if (infoStream.isEnabled("IW")) {
       if (allDeleted) {
-        infoStream.message("IW", "merged segment " + merge.info + " is 100% deleted" +  (keepFullyDeletedSegments ? "" : "; skipping insert"));
+        infoStream.message("IW", "merged segment " + merge.getInfo() + " is 100% deleted" +  (keepFullyDeletedSegments ? "" : "; skipping insert"));
       }
     }
 
@@ -3389,9 +3389,9 @@
 
     // If we merged no segments then we better be dropping
     // the new segment:
-    assert merge.segments.size() > 0 || dropSegment;
+    assert merge.getSegments().size() > 0 || dropSegment;
 
-    assert merge.info.info.maxDoc() != 0 || keepFullyDeletedSegments || dropSegment;
+    assert merge.getInfo().info.maxDoc() != 0 || keepFullyDeletedSegments || dropSegment;
 
     if (mergedUpdates != null) {
       boolean success = false;
@@ -3407,7 +3407,7 @@
       } finally {
         if (!success) {
           mergedUpdates.dropChanges();
-          readerPool.drop(merge.info);
+          readerPool.drop(merge.getInfo());
         }
       }
     }
@@ -3420,14 +3420,14 @@
 
     // Now deduct the deleted docs that we just reclaimed from this
     // merge:
-    int delDocCount = merge.totalMaxDoc - merge.info.info.maxDoc();
+    int delDocCount = merge.getTotalMaxDoc() - merge.getInfo().info.maxDoc();
     assert delDocCount >= 0;
     pendingNumDocs.addAndGet(-delDocCount);
 
     if (dropSegment) {
-      assert !segmentInfos.contains(merge.info);
-      readerPool.drop(merge.info);
-      deleter.deleteNewFiles(merge.info.files());
+      assert !segmentInfos.contains(merge.getInfo());
+      readerPool.drop(merge.getInfo());
+      deleter.deleteNewFiles(merge.getInfo().files());
     }
 
     boolean success = false;
@@ -3458,10 +3458,10 @@
       infoStream.message("IW", "after commitMerge: " + segString());
     }
 
-    if (merge.maxNumSegments != -1 && !dropSegment) {
+    if (merge.getMaxNumSegments() != -1 && !dropSegment) {
       // cascade the forceMerge:
-      if (!segmentsToMerge.containsKey(merge.info)) {
-        segmentsToMerge.put(merge.info, Boolean.FALSE);
+      if (!segmentsToMerge.containsKey(merge.getInfo())) {
+        segmentsToMerge.put(merge.getInfo(), Boolean.FALSE);
       }
     }
 
@@ -3471,7 +3471,7 @@
   final private void handleMergeException(Throwable t, MergePolicy.OneMerge merge) throws IOException {
 
     if (infoStream.isEnabled("IW")) {
-      infoStream.message("IW", "handleMergeException: merge=" + segString(merge.segments) + " exc=" + t);
+      infoStream.message("IW", "handleMergeException: merge=" + segString(merge.getSegments()) + " exc=" + t);
     }
 
     // Set the exception on the merge, so if
@@ -3487,7 +3487,7 @@
       // in which case we must throw it so, for example, the
       // rollbackTransaction code in addIndexes* is
       // executed.
-      if (merge.isExternal) {
+      if (merge.isIsExternal()) {
         throw (MergePolicy.MergeAbortedException) t;
       }
     } else {
@@ -3505,7 +3505,7 @@
 
     boolean success = false;
 
-    rateLimiters.set(merge.rateLimiter);
+    rateLimiters.set(merge.getRateLimiter());
 
     final long t0 = System.currentTimeMillis();
 
@@ -3514,12 +3514,12 @@
       try {
         try {
           mergeInit(merge);
-          //if (merge.info != null) {
-          //System.out.println("MERGE: " + merge.info.info.name);
+          //if (merge.getInfo() != null) {
+          //System.out.println("MERGE: " + merge.getInfo().info.name);
           //}
 
           if (infoStream.isEnabled("IW")) {
-            infoStream.message("IW", "now merge\n  merge=" + segString(merge.segments) + "\n  index=" + segString());
+            infoStream.message("IW", "now merge\n  merge=" + segString(merge.getSegments()) + "\n  index=" + segString());
           }
 
           mergeMiddle(merge, mergePolicy);
@@ -3536,25 +3536,25 @@
             if (infoStream.isEnabled("IW")) {
               infoStream.message("IW", "hit exception during merge");
             }
-            if (merge.info != null && !segmentInfos.contains(merge.info)) {
-              deleter.refresh(merge.info.info.name);
+            if (merge.getInfo() != null && !segmentInfos.contains(merge.getInfo())) {
+              deleter.refresh(merge.getInfo().info.name);
             }
           }
 
           // This merge (and, generally, any change to the
           // segments) may now enable new merges, so we call
           // merge policy & update pending merges.
-          if (success && merge.rateLimiter.getAbort() == false && (merge.maxNumSegments != -1 || (!closed && !closing))) {
-            updatePendingMerges(mergePolicy, MergeTrigger.MERGE_FINISHED, merge.maxNumSegments);
+          if (success && merge.getRateLimiter().getAbort() == false && (merge.getMaxNumSegments() != -1 || (!closed && !closing))) {
+            updatePendingMerges(mergePolicy, MergeTrigger.MERGE_FINISHED, merge.getMaxNumSegments());
           }
         }
       }
     } catch (OutOfMemoryError oom) {
       tragicEvent(oom, "merge");
     }
-    if (merge.info != null && merge.rateLimiter.getAbort() == false) {
+    if (merge.getInfo() != null && merge.getRateLimiter().getAbort() == false) {
       if (infoStream.isEnabled("IW")) {
-        infoStream.message("IW", "merge time " + (System.currentTimeMillis()-t0) + " msec for " + merge.info.info.maxDoc() + " docs");
+        infoStream.message("IW", "merge time " + (System.currentTimeMillis()-t0) + " msec for " + merge.getInfo().info.maxDoc() + " docs");
       }
     }
   }
@@ -3571,27 +3571,27 @@
    *  returned. */
   final synchronized boolean registerMerge(MergePolicy.OneMerge merge) throws IOException {
 
-    if (merge.registerDone) {
+    if (merge.isRegisterDone()) {
       return true;
     }
-    assert merge.segments.size() > 0;
+    assert merge.getSegments().size() > 0;
 
     if (stopMerges) {
-      merge.rateLimiter.setAbort();
-      throw new MergePolicy.MergeAbortedException("merge is aborted: " + segString(merge.segments));
+      merge.getRateLimiter().setAbort();
+      throw new MergePolicy.MergeAbortedException("merge is aborted: " + segString(merge.getSegments()));
     }
 
     boolean isExternal = false;
-    for(SegmentCommitInfo info : merge.segments) {
+    for(SegmentCommitInfo info : merge.getSegments()) {
       if (mergingSegments.contains(info)) {
         if (infoStream.isEnabled("IW")) {
-          infoStream.message("IW", "reject merge " + segString(merge.segments) + ": segment " + segString(info) + " is already marked for merge");
+          infoStream.message("IW", "reject merge " + segString(merge.getSegments()) + ": segment " + segString(info) + " is already marked for merge");
         }
         return false;
       }
       if (!segmentInfos.contains(info)) {
         if (infoStream.isEnabled("IW")) {
-          infoStream.message("IW", "reject merge " + segString(merge.segments) + ": segment " + segString(info) + " does not exist in live infos");
+          infoStream.message("IW", "reject merge " + segString(merge.getSegments()) + ": segment " + segString(info) + " does not exist in live infos");
         }
         return false;
       }
@@ -3599,7 +3599,7 @@
         isExternal = true;
       }
       if (segmentsToMerge.containsKey(info)) {
-        merge.maxNumSegments = mergeMaxNumSegments;
+        merge.setMaxNumSegments(mergeMaxNumSegments);
       }
     }
 
@@ -3608,11 +3608,11 @@
     pendingMerges.add(merge);
 
     if (infoStream.isEnabled("IW")) {
-      infoStream.message("IW", "add merge to pendingMerges: " + segString(merge.segments) + " [total " + pendingMerges.size() + " pending]");
+      infoStream.message("IW", "add merge to pendingMerges: " + segString(merge.getSegments()) + " [total " + pendingMerges.size() + " pending]");
     }
 
-    merge.mergeGen = mergeGen;
-    merge.isExternal = isExternal;
+    merge.setMergeGen(mergeGen);
+    merge.setIsExternal(isExternal);
 
     // OK it does not conflict; now record that this merge
     // is running (while synchronized) to avoid race
@@ -3630,27 +3630,27 @@
         infoStream.message("IW", builder.toString());  
       }
     }
-    for(SegmentCommitInfo info : merge.segments) {
+    for(SegmentCommitInfo info : merge.getSegments()) {
       if (infoStream.isEnabled("IW")) {
         infoStream.message("IW", "registerMerge info=" + segString(info));
       }
       mergingSegments.add(info);
     }
 
-    assert merge.estimatedMergeBytes == 0;
-    assert merge.totalMergeBytes == 0;
-    for(SegmentCommitInfo info : merge.segments) {
+    assert merge.getEstimatedMergeBytes() == 0;
+    assert merge.getTotalMergeBytes() == 0;
+    for(SegmentCommitInfo info : merge.getSegments()) {
       if (info.info.maxDoc() > 0) {
         final int delCount = numDeletedDocs(info);
         assert delCount <= info.info.maxDoc();
         final double delRatio = ((double) delCount)/info.info.maxDoc();
-        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);
-        merge.totalMergeBytes += info.sizeInBytes();
+        merge.setEstimatedMergeBytes((long)(merge.getEstimatedMergeBytes() + info.sizeInBytes() * (1.0 - delRatio)));
+        merge.setTotalMergeBytes(merge.getTotalMergeBytes() + info.sizeInBytes());
       }
     }
 
     // Merge is now registered
-    merge.registerDone = true;
+    merge.setRegisterDone(true);
 
     return true;
   }
@@ -3676,19 +3676,19 @@
 
     testPoint("startMergeInit");
 
-    assert merge.registerDone;
-    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;
+    assert merge.isRegisterDone();
+    assert merge.getMaxNumSegments() == -1 || merge.getMaxNumSegments() > 0;
 
     if (tragedy != null) {
       throw new IllegalStateException("this writer hit an unrecoverable error; cannot merge", tragedy);
     }
 
-    if (merge.info != null) {
+    if (merge.getInfo() != null) {
       // mergeInit already done
       return;
     }
 
-    if (merge.rateLimiter.getAbort()) {
+    if (merge.getRateLimiter().getAbort()) {
       return;
     }
 
@@ -3698,11 +3698,11 @@
     // could pre-pool them somehow in that case...
 
     if (infoStream.isEnabled("IW")) {
-      infoStream.message("IW", "now apply deletes for " + merge.segments.size() + " merging segments");
+      infoStream.message("IW", "now apply deletes for " + merge.getSegments().size() + " merging segments");
     }
 
     // Lock order: IW -> BD
-    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.segments);
+    final BufferedUpdatesStream.ApplyDeletesResult result = bufferedUpdatesStream.applyDeletesAndUpdates(readerPool, merge.getSegments());
     
     if (result.anyDeletes) {
       checkpoint();
@@ -3715,9 +3715,9 @@
       for(SegmentCommitInfo info : result.allDeleted) {
         segmentInfos.remove(info);
         pendingNumDocs.addAndGet(-info.info.maxDoc());
-        if (merge.segments.contains(info)) {
+        if (merge.getSegments().contains(info)) {
           mergingSegments.remove(info);
-          merge.segments.remove(info);
+          merge.getSegments().remove(info);
         }
         readerPool.drop(info);
       }
@@ -3730,18 +3730,18 @@
     final String mergeSegmentName = newSegmentName();
     SegmentInfo si = new SegmentInfo(directory, Version.LATEST, mergeSegmentName, -1, false, codec, Collections.<String,String>emptyMap(), StringHelper.randomId(), new HashMap<String,String>());
     Map<String,String> details = new HashMap<>();
-    details.put("mergeMaxNumSegments", "" + merge.maxNumSegments);
-    details.put("mergeFactor", Integer.toString(merge.segments.size()));
+    details.put("mergeMaxNumSegments", "" + merge.getMaxNumSegments());
+    details.put("mergeFactor", Integer.toString(merge.getSegments().size()));
     setDiagnostics(si, SOURCE_MERGE, details);
     merge.setInfo(new SegmentCommitInfo(si, 0, -1L, -1L, -1L));
 
-//    System.out.println("[" + Thread.currentThread().getName() + "] IW._mergeInit: " + segString(merge.segments) + " into " + si);
+//    System.out.println("[" + Thread.currentThread().getName() + "] IW._mergeInit: " + segString(merge.getSegments()) + " into " + si);
 
     // Lock order: IW -> BD
     bufferedUpdatesStream.prune(segmentInfos);
 
     if (infoStream.isEnabled("IW")) {
-      infoStream.message("IW", "merge seg=" + merge.info.info.name + " " + segString(merge.segments));
+      infoStream.message("IW", "merge seg=" + merge.getInfo().info.name + " " + segString(merge.getSegments()));
     }
   }
 
@@ -3775,25 +3775,25 @@
 
     // It's possible we are called twice, eg if there was an
     // exception inside mergeInit
-    if (merge.registerDone) {
-      final List<SegmentCommitInfo> sourceSegments = merge.segments;
+    if (merge.isRegisterDone()) {
+      final List<SegmentCommitInfo> sourceSegments = merge.getSegments();
       for (SegmentCommitInfo info : sourceSegments) {
         mergingSegments.remove(info);
       }
-      merge.registerDone = false;
+      merge.setRegisterDone(false);
     }
 
     runningMerges.remove(merge);
   }
 
   private final synchronized void closeMergeReaders(MergePolicy.OneMerge merge, boolean suppressExceptions) throws IOException {
-    final int numSegments = merge.readers.size();
+    final int numSegments = merge.getReaders().size();
     Throwable th = null;
 
     boolean drop = !suppressExceptions;
     
     for (int i = 0; i < numSegments; i++) {
-      final SegmentReader sr = merge.readers.get(i);
+      final SegmentReader sr = merge.getReaders().get(i);
       if (sr != null) {
         try {
           final ReadersAndUpdates rld = readerPool.get(sr.getSegmentInfo(), false);
@@ -3814,7 +3814,7 @@
             th = t;
           }
         }
-        merge.readers.set(i, null);
+        merge.getReaders().set(i, null);
       }
     }
 
@@ -3837,19 +3837,19 @@
    *  instance */
   private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {
 
-    merge.rateLimiter.checkAbort();
+    merge.getRateLimiter().checkAbort();
 
-    List<SegmentCommitInfo> sourceSegments = merge.segments;
+    List<SegmentCommitInfo> sourceSegments = merge.getSegments();
     
     IOContext context = new IOContext(merge.getMergeInfo());
 
     final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);
 
     if (infoStream.isEnabled("IW")) {
-      infoStream.message("IW", "merging " + segString(merge.segments));
+      infoStream.message("IW", "merging " + segString(merge.getSegments()));
     }
 
-    merge.readers = new ArrayList<>();
+    merge.setReaders(new ArrayList<SegmentReader>());
 
     // This is try/finally to make sure merger's readers are
     // closed:
@@ -3919,23 +3919,23 @@
           reader = newReader;
         }
 
-        merge.readers.add(reader);
+        merge.getReaders().add(reader);
         assert delCount <= info.info.maxDoc(): "delCount=" + delCount + " info.maxDoc=" + info.info.maxDoc() + " rld.pendingDeleteCount=" + rld.getPendingDeleteCount() + " info.getDelCount()=" + info.getDelCount();
         segUpto++;
       }
 
 //      System.out.println("[" + Thread.currentThread().getName() + "] IW.mergeMiddle: merging " + merge.getMergeReaders());
       
-      // we pass merge.getMergeReaders() instead of merge.readers to allow the
+      // we pass merge.getMergeReaders() instead of merge.getReaders() to allow the
       // OneMerge to return a view over the actual segments to merge
       final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),
-                                                     merge.info.info, infoStream, dirWrapper,
+                                                     merge.getInfo().info, infoStream, dirWrapper,
                                                      globalFieldNumberMap, 
                                                      context);
 
-      merge.rateLimiter.checkAbort();
+      merge.getRateLimiter().checkAbort();
 
-      merge.mergeStartNS = System.nanoTime();
+      merge.setMergeStartNS(System.nanoTime());
 
       // This is where all the work happens:
       boolean success3 = false;
@@ -3947,22 +3947,22 @@
       } finally {
         if (!success3) {
           synchronized(this) {  
-            deleter.refresh(merge.info.info.name);
+            deleter.refresh(merge.getInfo().info.name);
           }
         }
       }
       MergeState mergeState = merger.mergeState;
-      assert mergeState.segmentInfo == merge.info.info;
-      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));
+      assert mergeState.segmentInfo == merge.getInfo().info;
+      merge.getInfo().info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));
 
       if (infoStream.isEnabled("IW")) {
         if (merger.shouldMerge()) {
           long t1 = System.nanoTime();
-          double sec = (t1-merge.mergeStartNS)/1000000000.;
-          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);
-          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;
-          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;
-          infoStream.message("IW", "merge codec=" + codec + " maxDoc=" + merge.info.info.maxDoc() + "; merged segment has " +
+          double sec = (t1-merge.getMergeStartNS())/1000000000.;
+          double segmentMB = (merge.getInfo().sizeInBytes()/1024./1024.);
+          double stoppedSec = merge.getRateLimiter().getTotalStoppedNS()/1000000000.;
+          double throttleSec = merge.getRateLimiter().getTotalPausedNS()/1000000000.;
+          infoStream.message("IW", "merge codec=" + codec + " maxDoc=" + merge.getInfo().info.maxDoc() + "; merged segment has " +
                              (mergeState.mergeFieldInfos.hasVectors() ? "vectors" : "no vectors") + "; " +
                              (mergeState.mergeFieldInfos.hasNorms() ? "norms" : "no norms") + "; " + 
                              (mergeState.mergeFieldInfos.hasDocValues() ? "docValues" : "no docValues") + "; " + 
@@ -3982,33 +3982,33 @@
 
       if (merger.shouldMerge() == false) {
         // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we "merged":
-        assert merge.info.info.maxDoc() == 0;
+        assert merge.getInfo().info.maxDoc() == 0;
         commitMerge(merge, mergeState);
         return 0;
       }
 
-      assert merge.info.info.maxDoc() > 0;
+      assert merge.getInfo().info.maxDoc() > 0;
 
       // Very important to do this before opening the reader
       // because codec must know if prox was written for
       // this segment:
-      //System.out.println("merger set hasProx=" + merger.hasProx() + " seg=" + merge.info.name);
+      //System.out.println("merger set hasProx=" + merger.hasProx() + " seg=" + merge.getInfo().name);
       boolean useCompoundFile;
       synchronized (this) { // Guard segmentInfos
-        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);
+        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.getInfo(), this);
       }
 
       if (useCompoundFile) {
         success = false;
 
-        Collection<String> filesToRemove = merge.info.files();
+        Collection<String> filesToRemove = merge.getInfo().files();
         TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);
         try {
-          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);
+          createCompoundFile(infoStream, trackingCFSDir, merge.getInfo().info, context);
           success = true;
         } catch (IOException ioe) {
           synchronized(this) {
-            if (merge.rateLimiter.getAbort()) {
+            if (merge.getRateLimiter().getAbort()) {
               // This can happen if rollback or close(false)
               // is called -- fall through to logic below to
               // remove the partially created CFS:
@@ -4029,7 +4029,7 @@
               for (String cfsFile : cfsFiles) {
                 deleter.deleteFile(cfsFile);
               }
-              deleter.deleteNewFiles(merge.info.files());
+              deleter.deleteNewFiles(merge.getInfo().files());
             }
           }
         }
@@ -4045,7 +4045,7 @@
           // registered with IFD
           deleter.deleteNewFiles(filesToRemove);
 
-          if (merge.rateLimiter.getAbort()) {
+          if (merge.getRateLimiter().getAbort()) {
             if (infoStream.isEnabled("IW")) {
               infoStream.message("IW", "abort merge after building CFS");
             }
@@ -4057,7 +4057,7 @@
           }
         }
 
-        merge.info.info.setUseCompoundFile(true);
+        merge.getInfo().info.setUseCompoundFile(true);
       } else {
         // So that, if we hit exc in commitMerge (later),
         // we close the per-segment readers in the finally
@@ -4071,27 +4071,27 @@
       // above:
       boolean success2 = false;
       try {
-        codec.segmentInfoFormat().write(directory, merge.info.info, context);
+        codec.segmentInfoFormat().write(directory, merge.getInfo().info, context);
         success2 = true;
       } finally {
         if (!success2) {
           synchronized(this) {
-            deleter.deleteNewFiles(merge.info.files());
+            deleter.deleteNewFiles(merge.getInfo().files());
           }
         }
       }
 
-      // TODO: ideally we would freeze merge.info here!!
+      // TODO: ideally we would freeze merge.getInfo() here!!
       // because any changes after writing the .si will be
       // lost... 
 
       if (infoStream.isEnabled("IW")) {
-        infoStream.message("IW", String.format(Locale.ROOT, "merged segment size=%.3f MB vs estimate=%.3f MB", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));
+        infoStream.message("IW", String.format(Locale.ROOT, "merged segment size=%.3f MB vs estimate=%.3f MB", merge.getInfo().sizeInBytes()/1024./1024., merge.getEstimatedMergeBytes()/1024/1024.));
       }
 
       final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();
       if (poolReaders && mergedSegmentWarmer != null) {
-        final ReadersAndUpdates rld = readerPool.get(merge.info, true);
+        final ReadersAndUpdates rld = readerPool.get(merge.getInfo(), true);
         final SegmentReader sr = rld.getReader(IOContext.READ);
         try {
           mergedSegmentWarmer.warm(sr);
@@ -4119,12 +4119,12 @@
       }
     }
 
-    return merge.info.info.maxDoc();
+    return merge.getInfo().info.maxDoc();
   }
 
   synchronized void addMergeException(MergePolicy.OneMerge merge) {
     assert merge.getException() != null;
-    if (!mergeExceptions.contains(merge) && mergeGen == merge.mergeGen) {
+    if (!mergeExceptions.contains(merge) && mergeGen == merge.getMergeGen()) {
       mergeExceptions.add(merge);
     }
   }
Index: lucene/core/src/java/org/apache/lucene/index/MergePolicy.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/MergePolicy.java	(revision 1672228)
+++ lucene/core/src/java/org/apache/lucene/index/MergePolicy.java	(working copy)
@@ -16,7 +16,6 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collections;
@@ -29,44 +28,55 @@
 import org.apache.lucene.util.FixedBitSet;
 
 /**
- * <p>Expert: a MergePolicy determines the sequence of
- * primitive merge operations.</p>
- * 
- * <p>Whenever the segments in an index have been altered by
- * {@link IndexWriter}, either the addition of a newly
- * flushed segment, addition of many segments from
- * addIndexes* calls, or a previous merge that may now need
- * to cascade, {@link IndexWriter} invokes {@link
- * #findMerges} to give the MergePolicy a chance to pick
- * merges that are now required.  This method returns a
- * {@link MergeSpecification} instance describing the set of
- * merges that should be done, or null if no merges are
- * necessary.  When IndexWriter.forceMerge is called, it calls
- * {@link #findForcedMerges(SegmentInfos,int,Map, IndexWriter)} and the MergePolicy should
- * then return the necessary merges.</p>
+ * <p>
+ * Expert: a MergePolicy determines the sequence of primitive merge
+ * operations.</p>
  *
- * <p>Note that the policy can return more than one merge at
- * a time.  In this case, if the writer is using {@link
- * SerialMergeScheduler}, the merges will be run
- * sequentially but if it is using {@link
+ * <p>
+ * Whenever the segments in an index have been altered by {@link IndexWriter},
+ * either the addition of a newly flushed segment, addition of many segments
+ * from addIndexes* calls, or a previous merge that may now need to cascade,
+ * {@link IndexWriter} invokes {@link
+ * #findMerges} to give the MergePolicy a chance to pick merges that are now
+ * required. This method returns a {@link MergeSpecification} instance
+ * describing the set of merges that should be done, or null if no merges are
+ * necessary. When IndexWriter.forceMerge is called, it calls
+ * {@link #findForcedMerges(SegmentInfos,int,Map, IndexWriter)} and the
+ * MergePolicy should then return the necessary merges.</p>
+ *
+ * <p>
+ * Note that the policy can return more than one merge at a time. In this case,
+ * if the writer is using {@link
+ * SerialMergeScheduler}, the merges will be run sequentially but if it is using {@link
  * ConcurrentMergeScheduler} they will be run concurrently.</p>
- * 
- * <p>The default MergePolicy is {@link
+ *
+ * <p>
+ * The default MergePolicy is {@link
  * TieredMergePolicy}.</p>
  *
  * @lucene.experimental
  */
 public abstract class MergePolicy {
 
-  /** A map of doc IDs. */
+  /**
+   * A map of doc IDs.
+   */
   public static abstract class DocMap {
-    /** Sole constructor, typically invoked from sub-classes constructors. */
-    protected DocMap() {}
 
-    /** Return the new doc ID according to its old value. */
+    /**
+     * Sole constructor, typically invoked from sub-classes constructors.
+     */
+    protected DocMap() {
+    }
+
+    /**
+     * Return the new doc ID according to its old value.
+     */
     public abstract int map(int old);
 
-    /** Useful from an assert. */
+    /**
+     * Useful from an assert.
+     */
     boolean isConsistent(int maxDoc) {
       final FixedBitSet targets = new FixedBitSet(maxDoc);
       for (int i = 0; i < maxDoc; ++i) {
@@ -83,44 +93,38 @@
     }
   }
 
-  /** OneMerge provides the information necessary to perform
-   *  an individual primitive merge operation, resulting in
-   *  a single new segment.  The merge spec includes the
-   *  subset of segments to be merged as well as whether the
-   *  new segment should use the compound file format.
+  /**
+   * OneMerge provides the information necessary to perform an individual
+   * primitive merge operation, resulting in a single new segment. The merge
+   * spec includes the subset of segments to be merged as well as whether the
+   * new segment should use the compound file format.
    *
-   * @lucene.experimental */
-  public static class OneMerge {
+   * @lucene.experimental
+   */
+  public static class OneMerge extends AbstractOneMerge {
 
-    SegmentCommitInfo info;         // used by IndexWriter
-    boolean registerDone;           // used by IndexWriter
-    long mergeGen;                  // used by IndexWriter
-    boolean isExternal;             // used by IndexWriter
-    int maxNumSegments = -1;        // used by IndexWriter
+    /**
+     * Segments to be merged.
+     */
+    private List<SegmentCommitInfo> segments;
 
-    /** Estimated size in bytes of the merged segment. */
-    public volatile long estimatedMergeBytes;       // used by IndexWriter
+    /**
+     * A private {@link RateLimiter} for this merge, used to rate limit writes
+     * and abort.
+     */
+    private MergeRateLimiter rateLimiter;
 
-    // Sum of sizeInBytes of all SegmentInfos; set by IW.mergeInit
-    volatile long totalMergeBytes;
+    /**
+     * Total number of documents in segments to be merged, not accounting for
+     * deletions.
+     */
+    private int totalMaxDoc;
 
-    List<SegmentReader> readers;        // used by IndexWriter
-
-    /** Segments to be merged. */
-    public final List<SegmentCommitInfo> segments;
-
-    /** A private {@link RateLimiter} for this merge, used to rate limit writes and abort. */
-    public final MergeRateLimiter rateLimiter;
-
-    volatile long mergeStartNS = -1;
-
-    /** Total number of documents in segments to be merged, not accounting for deletions. */
-    public final int totalMaxDoc;
-    Throwable error;
-
-    /** Sole constructor.
-     * @param segments List of {@link SegmentCommitInfo}s
-     *        to be merged. */
+    /**
+     * Sole constructor.
+     *
+     * @param segments List of {@link SegmentCommitInfo}s to be merged.
+     */
     public OneMerge(List<SegmentCommitInfo> segments) {
       if (0 == segments.size()) {
         throw new RuntimeException("segments must include at least one segment");
@@ -128,7 +132,7 @@
       // clone the list, as the in list may be based off original SegmentInfos and may be modified
       this.segments = new ArrayList<>(segments);
       int count = 0;
-      for(SegmentCommitInfo info : segments) {
+      for (SegmentCommitInfo info : segments) {
         count += info.info.maxDoc();
       }
       totalMaxDoc = count;
@@ -136,42 +140,43 @@
       rateLimiter = new MergeRateLimiter(this);
     }
 
-    /** Called by {@link IndexWriter} after the merge is done and all readers have been closed. */
+    OneMerge() { }
+
+    /**
+     * Called by {@link IndexWriter} after the merge is done and all readers
+     * have been closed.
+     */
     public void mergeFinished() throws IOException {
     }
 
-    /** Expert: Get the list of readers to merge. Note that this list does not
-     *  necessarily match the list of segments to merge and should only be used
-     *  to feed SegmentMerger to initialize a merge. When a {@link OneMerge}
-     *  reorders doc IDs, it must override {@link #getDocMap} too so that
-     *  deletes that happened during the merge can be applied to the newly
-     *  merged segment. */
+    /**
+     * Expert: Get the list of readers to merge. Note that this list does not
+     * necessarily match the list of segments to merge and should only be used
+     * to feed SegmentMerger to initialize a merge. When a {@link OneMerge}
+     * reorders doc IDs, it must override {@link #getDocMap} too so that deletes
+     * that happened during the merge can be applied to the newly merged
+     * segment.
+     */
     public List<CodecReader> getMergeReaders() throws IOException {
-      if (readers == null) {
+      if (this.getReaders() == null) {
         throw new IllegalStateException("IndexWriter has not initialized readers from the segment infos yet");
       }
-      final List<CodecReader> readers = new ArrayList<>(this.readers.size());
-      for (SegmentReader reader : this.readers) {
+      final List<CodecReader> readers = new ArrayList<>(this.getReaders().size());
+      for (SegmentReader reader : this.getReaders()) {
         if (reader.numDocs() > 0) {
           readers.add(reader);
         }
       }
       return Collections.unmodifiableList(readers);
     }
-    
+
     /**
-     * Expert: Sets the {@link SegmentCommitInfo} of this {@link OneMerge}.
-     * Allows sub-classes to e.g. set diagnostics properties.
+     * Expert: If {@link #getMergeReaders()} reorders document IDs, this method
+     * must be overridden to return a mapping from the <i>natural</i> doc ID
+     * (the doc ID that would result from a natural merge) to the actual doc ID.
+     * This mapping is used to apply deletions that happened during the merge to
+     * the new segment.
      */
-    public void setInfo(SegmentCommitInfo info) {
-      this.info = info;
-    }
-
-    /** Expert: If {@link #getMergeReaders()} reorders document IDs, this method
-     *  must be overridden to return a mapping from the <i>natural</i> doc ID
-     *  (the doc ID that would result from a natural merge) to the actual doc
-     *  ID. This mapping is used to apply deletions that happened during the
-     *  merge to the new segment. */
     public DocMap getDocMap(MergeState mergeState) {
       return new DocMap() {
         @Override
@@ -181,55 +186,44 @@
       };
     }
 
-    /** Record that an exception occurred while executing
-     *  this merge */
-    synchronized void setException(Throwable error) {
-      this.error = error;
-    }
-
-    /** Retrieve previous exception set by {@link
-     *  #setException}. */
-    synchronized Throwable getException() {
-      return error;
-    }
-
-    /** Returns a readable description of the current merge
-     *  state. */
+    /**
+     * Returns a readable description of the current merge state.
+     */
     public String segString() {
       StringBuilder b = new StringBuilder();
       final int numSegments = segments.size();
-      for(int i=0;i<numSegments;i++) {
+      for (int i = 0; i < numSegments; i++) {
         if (i > 0) {
           b.append(' ');
         }
         b.append(segments.get(i).toString());
       }
-      if (info != null) {
-        b.append(" into ").append(info.info.name);
+      if (this.getInfo() != null) {
+        b.append(" into ").append(this.getInfo().info.name);
       }
-      if (maxNumSegments != -1) {
-        b.append(" [maxNumSegments=" + maxNumSegments + "]");
+      if (this.getMaxNumSegments() != -1) {
+        b.append(" [maxNumSegments=" + this.getMaxNumSegments() + "]");
       }
       if (rateLimiter.getAbort()) {
         b.append(" [ABORTED]");
       }
       return b.toString();
     }
-    
+
     /**
      * Returns the total size in bytes of this merge. Note that this does not
-     * indicate the size of the merged segment, but the
-     * input total size. This is only set once the merge is
-     * initialized by IndexWriter.
+     * indicate the size of the merged segment, but the input total size. This
+     * is only set once the merge is initialized by IndexWriter.
      */
     public long totalBytesSize() throws IOException {
-      return totalMergeBytes;
+      return this.getTotalMergeBytes();
     }
 
     /**
      * Returns the total number of documents that are included with this merge.
      * Note that this does not indicate the number of documents after the merge.
-     * */
+     *
+     */
     public int totalNumDocs() throws IOException {
       int total = 0;
       for (SegmentCommitInfo info : segments) {
@@ -238,109 +232,142 @@
       return total;
     }
 
-    /** Return {@link MergeInfo} describing this merge. */
+    /**
+     * Return {@link MergeInfo} describing this merge.
+     */
     public MergeInfo getMergeInfo() {
-      return new MergeInfo(totalMaxDoc, estimatedMergeBytes, isExternal, maxNumSegments);
-    }    
+      return new MergeInfo(totalMaxDoc, this.getEstimatedMergeBytes(), this.isIsExternal(), this.getMaxNumSegments());
+    }
+
+    //Public Access
+    public List<SegmentCommitInfo> getSegments() {
+      return segments;
+    }
+
+    public MergeRateLimiter getRateLimiter() {
+      return rateLimiter;
+    }
+
+    public int getTotalMaxDoc() {
+      return totalMaxDoc;
+    }
+
   }
 
   /**
-   * A MergeSpecification instance provides the information
-   * necessary to perform multiple merges.  It simply
-   * contains a list of {@link OneMerge} instances.
+   * A MergeSpecification instance provides the information necessary to perform
+   * multiple merges. It simply contains a list of {@link OneMerge} instances.
    */
-
   public static class MergeSpecification {
 
     /**
      * The subset of segments to be included in the primitive merge.
      */
-
     public final List<OneMerge> merges = new ArrayList<>();
 
-    /** Sole constructor.  Use {@link
-     *  #add(MergePolicy.OneMerge)} to add merges. */
+    /**
+     * Sole constructor. Use {@link
+     *  #add(MergePolicy.OneMerge)} to add merges.
+     */
     public MergeSpecification() {
     }
 
-    /** Adds the provided {@link OneMerge} to this
-     *  specification. */
+    /**
+     * Adds the provided {@link OneMerge} to this specification.
+     */
     public void add(OneMerge merge) {
       merges.add(merge);
     }
 
-    /** Returns a description of the merges in this
-    *  specification. */
+    /**
+     * Returns a description of the merges in this specification.
+     */
     public String segString(Directory dir) {
       StringBuilder b = new StringBuilder();
       b.append("MergeSpec:\n");
       final int count = merges.size();
-      for(int i=0;i<count;i++) {
+      for (int i = 0; i < count; i++) {
         b.append("  ").append(1 + i).append(": ").append(merges.get(i).segString());
       }
       return b.toString();
     }
   }
 
-  /** Exception thrown if there are any problems while
-   *  executing a merge. */
+  /**
+   * Exception thrown if there are any problems while executing a merge.
+   */
   public static class MergeException extends RuntimeException {
+
     private Directory dir;
 
-    /** Create a {@code MergeException}. */
+    /**
+     * Create a {@code MergeException}.
+     */
     public MergeException(String message, Directory dir) {
       super(message);
       this.dir = dir;
     }
 
-    /** Create a {@code MergeException}. */
+    /**
+     * Create a {@code MergeException}.
+     */
     public MergeException(Throwable exc, Directory dir) {
       super(exc);
       this.dir = dir;
     }
 
-    /** Returns the {@link Directory} of the index that hit
-     *  the exception. */
+    /**
+     * Returns the {@link Directory} of the index that hit the exception.
+     */
     public Directory getDirectory() {
       return dir;
     }
   }
 
-  /** Thrown when a merge was explicity aborted because
-   *  {@link IndexWriter#abortMerges} was called.  Normally
-   *  this exception is privately caught and suppresed by
-   *  {@link IndexWriter}. */
+  /**
+   * Thrown when a merge was explicity aborted because
+   * {@link IndexWriter#abortMerges} was called. Normally this exception is
+   * privately caught and suppresed by {@link IndexWriter}.
+   */
   public static class MergeAbortedException extends IOException {
-    /** Create a {@link MergeAbortedException}. */
+
+    /**
+     * Create a {@link MergeAbortedException}.
+     */
     public MergeAbortedException() {
       super("merge is aborted");
     }
 
-    /** Create a {@link MergeAbortedException} with a
-     *  specified message. */
+    /**
+     * Create a {@link MergeAbortedException} with a specified message.
+     */
     public MergeAbortedException(String message) {
       super(message);
     }
   }
-  
+
   /**
-   * Default ratio for compound file system usage. Set to <tt>1.0</tt>, always use 
-   * compound file system.
+   * Default ratio for compound file system usage. Set to <tt>1.0</tt>, always
+   * use compound file system.
    */
   protected static final double DEFAULT_NO_CFS_RATIO = 1.0;
 
   /**
-   * Default max segment size in order to use compound file system. Set to {@link Long#MAX_VALUE}.
+   * Default max segment size in order to use compound file system. Set to
+   * {@link Long#MAX_VALUE}.
    */
   protected static final long DEFAULT_MAX_CFS_SEGMENT_SIZE = Long.MAX_VALUE;
 
-  /** If the size of the merge segment exceeds this ratio of
-   *  the total index size then it will remain in
-   *  non-compound format */
+  /**
+   * If the size of the merge segment exceeds this ratio of the total index size
+   * then it will remain in non-compound format
+   */
   protected double noCFSRatio = DEFAULT_NO_CFS_RATIO;
-  
-  /** If the size of the merged segment exceeds
-   *  this value then it will not use compound file format. */
+
+  /**
+   * If the size of the merged segment exceeds this value then it will not use
+   * compound file format.
+   */
   protected long maxCFSSegmentSize = DEFAULT_MAX_CFS_SEGMENT_SIZE;
 
   /**
@@ -349,11 +376,11 @@
   public MergePolicy() {
     this(DEFAULT_NO_CFS_RATIO, DEFAULT_MAX_CFS_SEGMENT_SIZE);
   }
-  
+
   /**
    * Creates a new merge policy instance with default settings for noCFSRatio
-   * and maxCFSSegmentSize. This ctor should be used by subclasses using different
-   * defaults than the {@link MergePolicy}
+   * and maxCFSSegmentSize. This ctor should be used by subclasses using
+   * different defaults than the {@link MergePolicy}
    */
   protected MergePolicy(double defaultNoCFSRatio, long defaultMaxCFSSegmentSize) {
     this.noCFSRatio = defaultNoCFSRatio;
@@ -365,46 +392,40 @@
    * {@link IndexWriter} calls this whenever there is a change to the segments.
    * This call is always synchronized on the {@link IndexWriter} instance so
    * only one thread at a time will call this method.
+   *
    * @param mergeTrigger the event that triggered the merge
-   * @param segmentInfos
-   *          the total set of segments in the index
+   * @param segmentInfos the total set of segments in the index
    * @param writer the IndexWriter to find the merges on
    */
   public abstract MergeSpecification findMerges(MergeTrigger mergeTrigger, SegmentInfos segmentInfos, IndexWriter writer)
       throws IOException;
 
   /**
-   * Determine what set of merge operations is necessary in
-   * order to merge to {@code <=} the specified segment count. {@link IndexWriter} calls this when its
-   * {@link IndexWriter#forceMerge} method is called. This call is always
+   * Determine what set of merge operations is necessary in order to merge to
+   * {@code <=} the specified segment count. {@link IndexWriter} calls this when
+   * its {@link IndexWriter#forceMerge} method is called. This call is always
    * synchronized on the {@link IndexWriter} instance so only one thread at a
    * time will call this method.
-   * 
-   * @param segmentInfos
-   *          the total set of segments in the index
-   * @param maxSegmentCount
-   *          requested maximum number of segments in the index (currently this
-   *          is always 1)
-   * @param segmentsToMerge
-   *          contains the specific SegmentInfo instances that must be merged
-   *          away. This may be a subset of all
-   *          SegmentInfos.  If the value is True for a
-   *          given SegmentInfo, that means this segment was
-   *          an original segment present in the
-   *          to-be-merged index; else, it was a segment
-   *          produced by a cascaded merge.
+   *
+   * @param segmentInfos the total set of segments in the index
+   * @param maxSegmentCount requested maximum number of segments in the index
+   * (currently this is always 1)
+   * @param segmentsToMerge contains the specific SegmentInfo instances that
+   * must be merged away. This may be a subset of all SegmentInfos. If the value
+   * is True for a given SegmentInfo, that means this segment was an original
+   * segment present in the to-be-merged index; else, it was a segment produced
+   * by a cascaded merge.
    * @param writer the IndexWriter to find the merges on
    */
   public abstract MergeSpecification findForcedMerges(
-          SegmentInfos segmentInfos, int maxSegmentCount, Map<SegmentCommitInfo,Boolean> segmentsToMerge, IndexWriter writer)
+      SegmentInfos segmentInfos, int maxSegmentCount, Map<SegmentCommitInfo, Boolean> segmentsToMerge, IndexWriter writer)
       throws IOException;
 
   /**
    * Determine what set of merge operations is necessary in order to expunge all
    * deletes from the index.
-   * 
-   * @param segmentInfos
-   *          the total set of segments in the index
+   *
+   * @param segmentInfos the total set of segments in the index
    * @param writer the IndexWriter to find the merges on
    */
   public abstract MergeSpecification findForcedDeletesMerges(
@@ -434,41 +455,47 @@
     }
     return mergedInfoSize <= getNoCFSRatio() * totalSize;
   }
-  
-  /** Return the byte size of the provided {@link
-   *  SegmentCommitInfo}, pro-rated by percentage of
-   *  non-deleted documents is set. */
+
+  /**
+   * Return the byte size of the provided {@link
+   *  SegmentCommitInfo}, pro-rated by percentage of non-deleted documents is
+   * set.
+   */
   protected long size(SegmentCommitInfo info, IndexWriter writer) throws IOException {
     long byteSize = info.sizeInBytes();
     int delCount = writer.numDeletedDocs(info);
-    double delRatio = info.info.maxDoc() <= 0 ? 0.0f : (float) delCount / (float) info.info.maxDoc();
+    double delRatio = info.info.maxDoc() <= 0 ? 0.0f : (float)delCount / (float)info.info.maxDoc();
     assert delRatio <= 1.0;
-    return (info.info.maxDoc() <= 0 ? byteSize : (long) (byteSize * (1.0 - delRatio)));
+    return (info.info.maxDoc() <= 0 ? byteSize : (long)(byteSize * (1.0 - delRatio)));
   }
-  
-  /** Returns true if this single info is already fully merged (has no
-   *  pending deletes, is in the same dir as the
-   *  writer, and matches the current compound file setting */
+
+  /**
+   * Returns true if this single info is already fully merged (has no pending
+   * deletes, is in the same dir as the writer, and matches the current compound
+   * file setting
+   */
   protected final boolean isMerged(SegmentInfos infos, SegmentCommitInfo info, IndexWriter writer) throws IOException {
     assert writer != null;
     boolean hasDeletions = writer.numDeletedDocs(info) > 0;
-    return !hasDeletions &&
-      info.info.dir == writer.getDirectory() &&
-      useCompoundFile(infos, info, writer) == info.info.getUseCompoundFile();
+    return !hasDeletions
+        && info.info.dir == writer.getDirectory()
+        && useCompoundFile(infos, info, writer) == info.info.getUseCompoundFile();
   }
-  
-  /** Returns current {@code noCFSRatio}.
+
+  /**
+   * Returns current {@code noCFSRatio}.
    *
-   *  @see #setNoCFSRatio */
+   * @see #setNoCFSRatio
+   */
   public final double getNoCFSRatio() {
     return noCFSRatio;
   }
 
-  /** If a merged segment will be more than this percentage
-   *  of the total size of the index, leave the segment as
-   *  non-compound file even if compound file is enabled.
-   *  Set to 1.0 to always use CFS regardless of merge
-   *  size. */
+  /**
+   * If a merged segment will be more than this percentage of the total size of
+   * the index, leave the segment as non-compound file even if compound file is
+   * enabled. Set to 1.0 to always use CFS regardless of merge size.
+   */
   public final void setNoCFSRatio(double noCFSRatio) {
     if (noCFSRatio < 0.0 || noCFSRatio > 1.0) {
       throw new IllegalArgumentException("noCFSRatio must be 0.0 to 1.0 inclusive; got " + noCFSRatio);
@@ -476,21 +503,24 @@
     this.noCFSRatio = noCFSRatio;
   }
 
-  /** Returns the largest size allowed for a compound file segment */
+  /**
+   * Returns the largest size allowed for a compound file segment
+   */
   public final double getMaxCFSSegmentSizeMB() {
-    return maxCFSSegmentSize/1024/1024.;
+    return maxCFSSegmentSize / 1024 / 1024.;
   }
 
-  /** If a merged segment will be more than this value,
-   *  leave the segment as
-   *  non-compound file even if compound file is enabled.
-   *  Set this to Double.POSITIVE_INFINITY (default) and noCFSRatio to 1.0
-   *  to always use CFS regardless of merge size. */
+  /**
+   * If a merged segment will be more than this value, leave the segment as
+   * non-compound file even if compound file is enabled. Set this to
+   * Double.POSITIVE_INFINITY (default) and noCFSRatio to 1.0 to always use CFS
+   * regardless of merge size.
+   */
   public final void setMaxCFSSegmentSizeMB(double v) {
     if (v < 0.0) {
       throw new IllegalArgumentException("maxCFSSegmentSizeMB must be >=0 (got " + v + ")");
     }
     v *= 1024 * 1024;
-    this.maxCFSSegmentSize = v > Long.MAX_VALUE ? Long.MAX_VALUE : (long) v;
+    this.maxCFSSegmentSize = v > Long.MAX_VALUE ? Long.MAX_VALUE : (long)v;
   }
 }
Index: lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java	(revision 1672228)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java	(working copy)
@@ -866,7 +866,7 @@
   
   /** applies all changes caused by committing a merge to this SegmentInfos */
   void applyMergeChanges(MergePolicy.OneMerge merge, boolean dropSegment) {
-    final Set<SegmentCommitInfo> mergedAway = new HashSet<>(merge.segments);
+    final Set<SegmentCommitInfo> mergedAway = new HashSet<>(merge.getSegments());
     boolean inserted = false;
     int newSegIdx = 0;
     for (int segIdx = 0, cnt = segments.size(); segIdx < cnt; segIdx++) {
@@ -874,7 +874,7 @@
       final SegmentCommitInfo info = segments.get(segIdx);
       if (mergedAway.contains(info)) {
         if (!inserted && !dropSegment) {
-          segments.set(segIdx, merge.info);
+          segments.set(segIdx, merge.getInfo());
           inserted = true;
           newSegIdx++;
         }
@@ -893,7 +893,7 @@
     // be the case that the new segment is also all deleted,
     // we insert it at the beginning if it should not be dropped:
     if (!inserted && !dropSegment) {
-      segments.add(0, merge.info);
+      segments.add(0, merge.getInfo());
     }
   }
 
Index: lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java	(revision 1672228)
+++ lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java	(working copy)
@@ -430,12 +430,12 @@
           }
           final OneMerge merge = new OneMerge(best);
           spec.add(merge);
-          for(SegmentCommitInfo info : merge.segments) {
+          for(SegmentCommitInfo info : merge.getSegments()) {
             toBeMerged.add(info);
           }
 
           if (verbose(writer)) {
-            message("  add merge=" + writer.segString(merge.segments) + " size=" + String.format(Locale.ROOT, "%.3f MB", bestMergeBytes/1024./1024.) + " score=" + String.format(Locale.ROOT, "%.3f", bestScore.getScore()) + " " + bestScore.getExplanation() + (bestTooLarge ? " [max merge]" : ""), writer);
+            message("  add merge=" + writer.segString(merge.getSegments()) + " size=" + String.format(Locale.ROOT, "%.3f MB", bestMergeBytes/1024./1024.) + " score=" + String.format(Locale.ROOT, "%.3f", bestScore.getScore()) + " " + bestScore.getExplanation() + (bestTooLarge ? " [max merge]" : ""), writer);
           }
         } else {
           return spec;
@@ -557,7 +557,7 @@
       }
       final OneMerge merge = new OneMerge(eligible.subList(end-maxMergeAtOnceExplicit, end));
       if (verbose(writer)) {
-        message("add merge=" + writer.segString(merge.segments), writer);
+        message("add merge=" + writer.segString(merge.getSegments()), writer);
       }
       spec.add(merge);
       end -= maxMergeAtOnceExplicit;
@@ -615,7 +615,7 @@
 
       final OneMerge merge = new OneMerge(eligible.subList(start, end));
       if (verbose(writer)) {
-        message("add merge=" + writer.segString(merge.segments), writer);
+        message("add merge=" + writer.segString(merge.getSegments()), writer);
       }
       spec.add(merge);
       start = end;
Index: lucene/core/src/java/org/apache/lucene/index/UpgradeIndexMergePolicy.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/UpgradeIndexMergePolicy.java	(revision 1672228)
+++ lucene/core/src/java/org/apache/lucene/index/UpgradeIndexMergePolicy.java	(working copy)
@@ -98,7 +98,7 @@
       // the resulting set contains all segments that are left over
       // and will be merged to one additional segment:
       for (final OneMerge om : spec.merges) {
-        oldSegments.keySet().removeAll(om.segments);
+        oldSegments.keySet().removeAll(om.getSegments());
       }
     }
 
Index: lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader.java	(revision 1672228)
+++ lucene/core/src/test/org/apache/lucene/index/TestDemoParallelLeafReader.java	(working copy)
@@ -576,7 +576,7 @@
       class ReindexingMergeSpecification extends MergeSpecification {
         @Override
         public void add(OneMerge merge) {
-          super.add(new ReindexingOneMerge(merge.segments));
+          super.add(new ReindexingOneMerge(merge.getSegments()));
         }
 
         @Override
Index: lucene/core/src/test/org/apache/lucene/index/TestForceMergeForever.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestForceMergeForever.java	(revision 1672228)
+++ lucene/core/src/test/org/apache/lucene/index/TestForceMergeForever.java	(working copy)
@@ -41,7 +41,7 @@
 
     @Override
     public void merge(MergePolicy.OneMerge merge) throws IOException {
-      if (merge.maxNumSegments != -1 && (first || merge.segments.size() == 1)) {
+      if (merge.getMaxNumSegments() != -1 && (first || merge.getSegments().size() == 1)) {
         first = false;
         if (VERBOSE) {
           System.out.println("TEST: maxNumSegments merge");
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java	(revision 1672228)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java	(working copy)
@@ -319,8 +319,8 @@
         if (merge == null) {
           break;
         }
-        for(int i=0;i<merge.segments.size();i++) {
-          assert merge.segments.get(i).info.maxDoc() < 20;
+        for(int i=0;i<merge.getSegments().size();i++) {
+          assert merge.getSegments().get(i).info.maxDoc() < 20;
         }
         writer.merge(merge);
       }
Index: lucene/misc/src/java/org/apache/lucene/index/SortingMergePolicy.java
===================================================================
--- lucene/misc/src/java/org/apache/lucene/index/SortingMergePolicy.java	(revision 1672228)
+++ lucene/misc/src/java/org/apache/lucene/index/SortingMergePolicy.java	(working copy)
@@ -1,5 +1,5 @@
 package org.apache.lucene.index;
-
+  
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -23,18 +23,6 @@
 import java.util.List;
 import java.util.Map;
 
-import org.apache.lucene.analysis.Analyzer; // javadocs
-import org.apache.lucene.index.LeafReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.MergePolicy;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.MergeTrigger;
-import org.apache.lucene.index.MultiReader;
-import org.apache.lucene.index.SegmentCommitInfo;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.index.SegmentReader;
-import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.Bits;
@@ -63,7 +51,7 @@
    */
   public static final String SORTER_ID_PROP = "sorter";
   
-  class SortingOneMerge extends OneMerge {
+  class SortingOneMerge extends DelegatingOneMerge {
 
     List<CodecReader> unsortedReaders;
     Sorter.DocMap docMap;
@@ -74,6 +62,11 @@
       super(segments);
       this.infoStream = infoStream;
     }
+    
+    SortingOneMerge(AbstractOneMerge delegate, InfoStream infoStream){
+      super(delegate);
+      this.infoStream = infoStream;
+    }
 
     @Override
     public List<CodecReader> getMergeReaders() throws IOException {
@@ -176,7 +169,7 @@
 
     @Override
     public void add(OneMerge merge) {
-      super.add(new SortingOneMerge(merge.segments, infoStream));
+      super.add(new SortingOneMerge(merge.getSegments(), infoStream));
     }
 
     @Override
Index: lucene/test-framework/src/java/org/apache/lucene/index/MockRandomMergePolicy.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/index/MockRandomMergePolicy.java	(revision 1672228)
+++ lucene/test-framework/src/java/org/apache/lucene/index/MockRandomMergePolicy.java	(working copy)
@@ -117,7 +117,7 @@
 
     if (mergeSpec != null) {
       for(OneMerge merge : mergeSpec.merges) {
-        for(SegmentCommitInfo info : merge.segments) {
+        for(SegmentCommitInfo info : merge.getSegments()) {
           assert segmentsToMerge.containsKey(info);
         }
       }
Index: solr/core/src/java/org/apache/solr/handler/admin/SegmentsInfoRequestHandler.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/admin/SegmentsInfoRequestHandler.java	(revision 1672228)
+++ solr/core/src/java/org/apache/solr/handler/admin/SegmentsInfoRequestHandler.java	(working copy)
@@ -102,7 +102,7 @@
     if (findMerges != null && findMerges.merges != null && findMerges.merges.size() > 0) {
       for (OneMerge merge : findMerges.merges) {
         //TODO: add merge grouping
-        for (SegmentCommitInfo mergeSegmentInfo : merge.segments) {
+        for (SegmentCommitInfo mergeSegmentInfo : merge.getSegments()) {
           result.add(mergeSegmentInfo.info.name);
         }
       }
