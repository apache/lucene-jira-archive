

diff -ruN -x .svn -x build lucene-trunk/lucene/analysis/common/src/java/org/apache/lucene/collation/CollationDocValuesField.java lucene5666/lucene/analysis/common/src/java/org/apache/lucene/collation/CollationDocValuesField.java
--- lucene-trunk/lucene/analysis/common/src/java/org/apache/lucene/collation/CollationDocValuesField.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/analysis/common/src/java/org/apache/lucene/collation/CollationDocValuesField.java	2014-05-14 03:45:16.694644324 -0400
@@ -0,0 +1,70 @@
+package org.apache.lucene.collation;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.text.Collator;
+
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.SortedDocValuesField;
+import org.apache.lucene.search.DocValuesRangeFilter;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * Indexes collation keys as a single-valued {@link SortedDocValuesField}.
+ * <p>
+ * This is more efficient that {@link CollationKeyAnalyzer} if the field 
+ * only has one value: no uninversion is necessary to sort on the field, 
+ * locale-sensitive range queries can still work via {@link DocValuesRangeFilter}, 
+ * and the underlying data structures built at index-time are likely more efficient 
+ * and use less memory than FieldCache.
+ */
+public final class CollationDocValuesField extends Field {
+  private final String name;
+  private final Collator collator;
+  private final BytesRef bytes = new BytesRef();
+  
+  /**
+   * Create a new ICUCollationDocValuesField.
+   * <p>
+   * NOTE: you should not create a new one for each document, instead
+   * just make one and reuse it during your indexing process, setting
+   * the value via {@link #setStringValue(String)}.
+   * @param name field name
+   * @param collator Collator for generating collation keys.
+   */
+  // TODO: can we make this trap-free? maybe just synchronize on the collator
+  // instead? 
+  public CollationDocValuesField(String name, Collator collator) {
+    super(name, SortedDocValuesField.TYPE);
+    this.name = name;
+    this.collator = (Collator) collator.clone();
+    fieldsData = bytes; // so wrong setters cannot be called
+  }
+
+  @Override
+  public String name() {
+    return name;
+  }
+  
+  @Override
+  public void setStringValue(String value) {
+    bytes.bytes = collator.getCollationKey(value).toByteArray();
+    bytes.offset = 0;
+    bytes.length = bytes.bytes.length;
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationDocValuesField.java lucene5666/lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationDocValuesField.java
--- lucene-trunk/lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationDocValuesField.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationDocValuesField.java	2014-05-14 03:45:16.694644324 -0400
@@ -0,0 +1,143 @@
+package org.apache.lucene.collation;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.text.Collator;
+import java.util.Locale;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.DocValuesRangeFilter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.QueryUtils;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.SortField;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
+
+/**
+ * trivial test of CollationDocValuesField
+ */
+@SuppressCodecs("Lucene3x")
+public class TestCollationDocValuesField extends LuceneTestCase {
+  
+  public void testBasic() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    Field field = newField("field", "", StringField.TYPE_STORED);
+    CollationDocValuesField collationField = new CollationDocValuesField("collated", Collator.getInstance(Locale.ENGLISH));
+    doc.add(field);
+    doc.add(collationField);
+
+    field.setStringValue("ABC");
+    collationField.setStringValue("ABC");
+    iw.addDocument(doc);
+    
+    field.setStringValue("abc");
+    collationField.setStringValue("abc");
+    iw.addDocument(doc);
+    
+    IndexReader ir = iw.getReader();
+    iw.shutdown();
+    
+    IndexSearcher is = newSearcher(ir);
+    
+    SortField sortField = new SortField("collated", SortField.Type.STRING);
+    
+    TopDocs td = is.search(new MatchAllDocsQuery(), 5, new Sort(sortField));
+    assertEquals("abc", ir.document(td.scoreDocs[0].doc).get("field"));
+    assertEquals("ABC", ir.document(td.scoreDocs[1].doc).get("field"));
+    ir.close();
+    dir.close();
+  }
+  
+  public void testRanges() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    Field field = newField("field", "", StringField.TYPE_STORED);
+    Collator collator = Collator.getInstance(Locale.getDefault()); // uses -Dtests.locale
+    if (random().nextBoolean()) {
+      collator.setStrength(Collator.PRIMARY);
+    }
+    CollationDocValuesField collationField = new CollationDocValuesField("collated", collator);
+    doc.add(field);
+    doc.add(collationField);
+    
+    int numDocs = atLeast(500);
+    for (int i = 0; i < numDocs; i++) {
+      String value = TestUtil.randomSimpleString(random());
+      field.setStringValue(value);
+      collationField.setStringValue(value);
+      iw.addDocument(doc);
+    }
+    
+    IndexReader ir = iw.getReader();
+    iw.shutdown();
+    IndexSearcher is = newSearcher(ir);
+    
+    int numChecks = atLeast(100);
+    for (int i = 0; i < numChecks; i++) {
+      String start = TestUtil.randomSimpleString(random());
+      String end = TestUtil.randomSimpleString(random());
+      BytesRef lowerVal = new BytesRef(collator.getCollationKey(start).toByteArray());
+      BytesRef upperVal = new BytesRef(collator.getCollationKey(end).toByteArray());
+      Query query = new ConstantScoreQuery(DocValuesRangeFilter.newBytesRefRange("collated", lowerVal, upperVal, true, true));
+      doTestRanges(is, start, end, query, collator);
+    }
+    
+    ir.close();
+    dir.close();
+  }
+  
+  private void doTestRanges(IndexSearcher is, String startPoint, String endPoint, Query query, Collator collator) throws Exception { 
+    QueryUtils.check(query);
+    
+    // positive test
+    TopDocs docs = is.search(query, is.getIndexReader().maxDoc());
+    for (ScoreDoc doc : docs.scoreDocs) {
+      String value = is.doc(doc.doc).get("field");
+      assertTrue(collator.compare(value, startPoint) >= 0);
+      assertTrue(collator.compare(value, endPoint) <= 0);
+    }
+    
+    // negative test
+    BooleanQuery bq = new BooleanQuery();
+    bq.add(new MatchAllDocsQuery(), Occur.SHOULD);
+    bq.add(query, Occur.MUST_NOT);
+    docs = is.search(bq, is.getIndexReader().maxDoc());
+    for (ScoreDoc doc : docs.scoreDocs) {
+      String value = is.doc(doc.doc).get("field");
+      assertTrue(collator.compare(value, startPoint) < 0 || collator.compare(value, endPoint) > 0);
+    }
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java lucene5666/lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java
--- lucene-trunk/lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java	2014-05-14 03:47:28.810646624 -0400
+++ lucene5666/lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java	2014-05-12 13:28:53.424244904 -0400
@@ -60,23 +60,6 @@
        secondRangeBeginning, secondRangeEnd);
   }
   
-  public void testCollationKeySort() throws Exception {
-    Analyzer usAnalyzer 
-      = new CollationKeyAnalyzer(TEST_VERSION_CURRENT, Collator.getInstance(Locale.US));
-    Analyzer franceAnalyzer 
-      = new CollationKeyAnalyzer(TEST_VERSION_CURRENT, Collator.getInstance(Locale.FRANCE));
-    Analyzer swedenAnalyzer 
-      = new CollationKeyAnalyzer(TEST_VERSION_CURRENT, Collator.getInstance(new Locale("sv", "se")));
-    Analyzer denmarkAnalyzer 
-      = new CollationKeyAnalyzer(TEST_VERSION_CURRENT, Collator.getInstance(new Locale("da", "dk")));
-    
-    // The ICU Collator and Sun java.text.Collator implementations differ in their
-    // orderings - "BFJDH" is the ordering for java.text.Collator for Locale.US.
-    testCollationKeySort
-    (usAnalyzer, franceAnalyzer, swedenAnalyzer, denmarkAnalyzer, 
-     oStrokeFirst ? "BFJHD" : "BFJDH", "EACGI", "BJDFH", "BJDHF");
-  }
-  
   public void testThreadSafe() throws Exception {
     int iters = 20 * RANDOM_MULTIPLIER;
     for (int i = 0; i < iters; i++) {


diff -ruN -x .svn -x build lucene-trunk/lucene/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationDocValuesField.java lucene5666/lucene/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationDocValuesField.java
--- lucene-trunk/lucene/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationDocValuesField.java	2014-05-14 03:47:28.822646624 -0400
+++ lucene5666/lucene/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationDocValuesField.java	2014-05-14 03:45:16.694644324 -0400
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.SortedDocValuesField;
-import org.apache.lucene.search.FieldCacheRangeFilter;
+import org.apache.lucene.search.DocValuesRangeFilter;
 import org.apache.lucene.util.BytesRef;
 
 import com.ibm.icu.text.Collator;
@@ -30,7 +30,7 @@
  * <p>
  * This is more efficient that {@link ICUCollationKeyAnalyzer} if the field 
  * only has one value: no uninversion is necessary to sort on the field, 
- * locale-sensitive range queries can still work via {@link FieldCacheRangeFilter}, 
+ * locale-sensitive range queries can still work via {@link DocValuesRangeFilter}, 
  * and the underlying data structures built at index-time are likely more efficient 
  * and use less memory than FieldCache.
  */


diff -ruN -x .svn -x build lucene-trunk/lucene/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationDocValuesField.java lucene5666/lucene/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationDocValuesField.java
--- lucene-trunk/lucene/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationDocValuesField.java	2014-05-14 03:47:28.818646624 -0400
+++ lucene5666/lucene/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationDocValuesField.java	2014-05-14 03:45:16.694644324 -0400
@@ -24,7 +24,7 @@
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.FieldCacheRangeFilter;
+import org.apache.lucene.search.DocValuesRangeFilter;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.Query;
@@ -111,7 +111,7 @@
       String end = TestUtil.randomSimpleString(random());
       BytesRef lowerVal = new BytesRef(collator.getCollationKey(start).toByteArray());
       BytesRef upperVal = new BytesRef(collator.getCollationKey(end).toByteArray());
-      Query query = new ConstantScoreQuery(FieldCacheRangeFilter.newBytesRefRange("collated", lowerVal, upperVal, true, true));
+      Query query = new ConstantScoreQuery(DocValuesRangeFilter.newBytesRefRange("collated", lowerVal, upperVal, true, true));
       doTestRanges(is, start, end, query, collator);
     }
     


diff -ruN -x .svn -x build lucene-trunk/lucene/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyAnalyzer.java lucene5666/lucene/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyAnalyzer.java
--- lucene-trunk/lucene/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyAnalyzer.java	2014-05-14 03:47:28.818646624 -0400
+++ lucene5666/lucene/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyAnalyzer.java	2014-05-12 13:28:45.236244761 -0400
@@ -56,29 +56,6 @@
        secondRangeBeginning, secondRangeEnd);
   }
 
-  // Test using various international locales with accented characters (which
-  // sort differently depending on locale)
-  //
-  // Copied (and slightly modified) from 
-  // org.apache.lucene.search.TestSort.testInternationalSort()
-  //  
-  public void testCollationKeySort() throws Exception {
-    Analyzer usAnalyzer = new ICUCollationKeyAnalyzer
-      (TEST_VERSION_CURRENT, Collator.getInstance(Locale.ROOT));
-    Analyzer franceAnalyzer = new ICUCollationKeyAnalyzer
-      (TEST_VERSION_CURRENT, Collator.getInstance(Locale.FRANCE));
-    Analyzer swedenAnalyzer = new ICUCollationKeyAnalyzer
-      (TEST_VERSION_CURRENT, Collator.getInstance(new Locale("sv", "se")));
-    Analyzer denmarkAnalyzer = new ICUCollationKeyAnalyzer
-      (TEST_VERSION_CURRENT, Collator.getInstance(new Locale("da", "dk")));
-
-    // The ICU Collator and java.text.Collator implementations differ in their
-    // orderings - "BFJHD" is the ordering for the ICU Collator for Locale.ROOT.
-    testCollationKeySort
-    (usAnalyzer, franceAnalyzer, swedenAnalyzer, denmarkAnalyzer, 
-     "BFJHD", "ECAGI", "BJDFH", "BJDHF");
-  }
-  
   public void testThreadSafe() throws Exception {
     int iters = 20 * RANDOM_MULTIPLIER;
     for (int i = 0; i < iters; i++) {


diff -ruN -x .svn -x build lucene-trunk/lucene/benchmark/src/test/org/apache/lucene/benchmark/BenchmarkTestCase.java lucene5666/lucene/benchmark/src/test/org/apache/lucene/benchmark/BenchmarkTestCase.java
--- lucene-trunk/lucene/benchmark/src/test/org/apache/lucene/benchmark/BenchmarkTestCase.java	2014-05-14 03:47:28.778646624 -0400
+++ lucene5666/lucene/benchmark/src/test/org/apache/lucene/benchmark/BenchmarkTestCase.java	2014-05-12 13:28:38.172244638 -0400
@@ -31,6 +31,7 @@
 import org.junit.BeforeClass;
 
 /** Base class for all Benchmark unit tests. */
+@SuppressSysoutChecks(bugUrl = "very noisy")
 public abstract class BenchmarkTestCase extends LuceneTestCase {
   private static File WORKDIR;
   


diff -ruN -x .svn -x build lucene-trunk/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java lucene5666/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java
--- lucene-trunk/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java	2014-05-14 03:47:28.778646624 -0400
+++ lucene5666/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java	2014-05-12 13:28:38.172244638 -0400
@@ -52,17 +52,12 @@
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.SegmentInfos;
 import org.apache.lucene.index.SerialMergeScheduler;
-import org.apache.lucene.index.SlowCompositeReaderWrapper;
-import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.TestUtil;
-import org.apache.lucene.util.TestUtil;
-import org.apache.lucene.util.LuceneTestCase.SuppressSysoutChecks;
 
 /**
  * Test very simply that perf tasks - simple algorithms - are doing what they should.
@@ -328,7 +323,7 @@
         "content.source.forever=true",
         "directory=RAMDirectory",
         "doc.reuse.fields=false",
-        "doc.stored=false",
+        "doc.stored=true",
         "doc.tokenized=false",
         "doc.index.props=true",
         "# ----- alg ",
@@ -344,11 +339,11 @@
     Benchmark benchmark = execBenchmark(algLines);
 
     DirectoryReader r = DirectoryReader.open(benchmark.getRunData().getDirectory());
-    SortedDocValues idx = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(r), "country");
+    
     final int maxDoc = r.maxDoc();
     assertEquals(1000, maxDoc);
     for(int i=0;i<1000;i++) {
-      assertTrue("doc " + i + " has null country", idx.getOrd(i) != -1);
+      assertNotNull("doc " + i + " has null country", r.document(i).getField("country"));
     }
     r.close();
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksParse.java lucene5666/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksParse.java
--- lucene-trunk/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksParse.java	2014-05-14 03:47:28.778646624 -0400
+++ lucene5666/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksParse.java	2014-05-12 13:28:38.260244640 -0400
@@ -42,6 +42,7 @@
 import conf.ConfLoader;
 
 /** Test very simply that perf tasks are parses as expected. */
+@SuppressSysoutChecks(bugUrl = "very noisy")
 public class TestPerfTasksParse extends LuceneTestCase {
 
   static final String NEW_LINE = System.getProperty("line.separator");


diff -ruN -x .svn -x build lucene-trunk/lucene/CHANGES.txt lucene5666/lucene/CHANGES.txt
--- lucene-trunk/lucene/CHANGES.txt	2014-05-14 03:47:28.910646626 -0400
+++ lucene5666/lucene/CHANGES.txt	2014-05-14 03:47:11.118646316 -0400
@@ -65,6 +65,18 @@
   as tokens anymore, and now iterates cells on-demand during indexing instead of
   building a collection.  RPT now has more setters. (David Smiley)
 
+* LUCENE-5666: Change uninverted access (sorting, faceting, grouping, etc)
+  to use the DocValues API instead of FieldCache. For FieldCache functionality,
+  use UninvertingReader in lucene/misc (or implement your own FilterReader).
+  UninvertingReader is more efficient: supports multi-valued numeric fields,
+  detects when a multi-valued field is single-valued, reuses caches
+  of compatible types (e.g. SORTED also supports BINARY and SORTED_SET access
+  without insanity).  "Insanity" is no longer possible unless you explicitly want it. 
+  Rename FieldCache* and DocTermOrds* classes in the search package to DocValues*. 
+  Move SortedSetSortField to core and add SortedSetFieldSource to queries/, which
+  takes the same selectors. Add helper methods to DocValues.java that are better 
+  suited for search code (never return null, etc).  (Mike McCandless, Robert Muir)
+
 Documentation
 
 * LUCENE-5392: Add/improve analysis package documentation to reflect


diff -ruN -x .svn -x build lucene-trunk/lucene/core/build.xml lucene5666/lucene/core/build.xml
--- lucene-trunk/lucene/core/build.xml	2014-05-14 03:47:28.682646622 -0400
+++ lucene5666/lucene/core/build.xml	2014-05-14 03:45:16.694644324 -0400
@@ -31,7 +31,6 @@
   "/>
 
   <property name="forbidden-rue-excludes" value="
-    org/apache/lucene/search/FieldCache$CacheEntry.class
     org/apache/lucene/util/RamUsageEstimator.class
     org/apache/lucene/search/CachingWrapperFilter.class
   "/>


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/document/DoubleDocValuesField.java lucene5666/lucene/core/src/java/org/apache/lucene/document/DoubleDocValuesField.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/document/DoubleDocValuesField.java	2014-05-14 03:47:28.758646623 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/document/DoubleDocValuesField.java	2014-05-12 13:28:35.816244597 -0400
@@ -18,14 +18,13 @@
  */
 
 import org.apache.lucene.index.AtomicReader; // javadocs
-import org.apache.lucene.search.FieldCache; // javadocs
 
 /**
  * Syntactic sugar for encoding doubles as NumericDocValues
  * via {@link Double#doubleToRawLongBits(double)}.
  * <p>
  * Per-document double values can be retrieved via
- * {@link FieldCache#getDoubles(AtomicReader, String, boolean)}.
+ * {@link AtomicReader#getNumericDocValues(String)}.
  * <p>
  * <b>NOTE</b>: In most all cases this will be rather inefficient,
  * requiring eight bytes per document. Consider encoding double


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/document/DoubleField.java lucene5666/lucene/core/src/java/org/apache/lucene/document/DoubleField.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/document/DoubleField.java	2014-05-14 03:47:28.758646623 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/document/DoubleField.java	2014-05-12 13:28:35.816244597 -0400
@@ -18,8 +18,8 @@
  */
 
 import org.apache.lucene.analysis.NumericTokenStream; // javadocs
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.search.FieldCache; // javadocs
 import org.apache.lucene.search.NumericRangeFilter; // javadocs
 import org.apache.lucene.search.NumericRangeQuery; // javadocs
 import org.apache.lucene.util.NumericUtils;
@@ -57,7 +57,7 @@
  * NumericRangeFilter}.  To sort according to a
  * <code>DoubleField</code>, use the normal numeric sort types, eg
  * {@link org.apache.lucene.search.SortField.Type#DOUBLE}. <code>DoubleField</code> 
- * values can also be loaded directly from {@link FieldCache}.</p>
+ * values can also be loaded directly from {@link AtomicReader#getNumericDocValues}.</p>
  *
  * <p>You may add the same field name as an <code>DoubleField</code> to
  * the same document more than once.  Range querying and


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/document/FloatDocValuesField.java lucene5666/lucene/core/src/java/org/apache/lucene/document/FloatDocValuesField.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/document/FloatDocValuesField.java	2014-05-14 03:47:28.758646623 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/document/FloatDocValuesField.java	2014-05-12 13:28:35.816244597 -0400
@@ -18,14 +18,13 @@
  */
 
 import org.apache.lucene.index.AtomicReader; // javadocs
-import org.apache.lucene.search.FieldCache; // javadocs
 
 /**
  * Syntactic sugar for encoding floats as NumericDocValues
  * via {@link Float#floatToRawIntBits(float)}.
  * <p>
  * Per-document floating point values can be retrieved via
- * {@link FieldCache#getFloats(AtomicReader, String, boolean)}.
+ * {@link AtomicReader#getNumericDocValues(String)}.
  * <p>
  * <b>NOTE</b>: In most all cases this will be rather inefficient,
  * requiring four bytes per document. Consider encoding floating


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/document/FloatField.java lucene5666/lucene/core/src/java/org/apache/lucene/document/FloatField.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/document/FloatField.java	2014-05-14 03:47:28.758646623 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/document/FloatField.java	2014-05-12 13:28:35.816244597 -0400
@@ -18,8 +18,8 @@
  */
 
 import org.apache.lucene.analysis.NumericTokenStream; // javadocs
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.search.FieldCache; // javadocs
 import org.apache.lucene.search.NumericRangeFilter; // javadocs
 import org.apache.lucene.search.NumericRangeQuery; // javadocs
 import org.apache.lucene.util.NumericUtils;
@@ -57,7 +57,7 @@
  * NumericRangeFilter}.  To sort according to a
  * <code>FloatField</code>, use the normal numeric sort types, eg
  * {@link org.apache.lucene.search.SortField.Type#FLOAT}. <code>FloatField</code> 
- * values can also be loaded directly from {@link FieldCache}.</p>
+ * values can also be loaded directly from {@link AtomicReader#getNumericDocValues}.</p>
  *
  * <p>You may add the same field name as an <code>FloatField</code> to
  * the same document more than once.  Range querying and


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/document/IntField.java lucene5666/lucene/core/src/java/org/apache/lucene/document/IntField.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/document/IntField.java	2014-05-14 03:47:28.758646623 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/document/IntField.java	2014-05-12 13:28:35.812244597 -0400
@@ -18,8 +18,8 @@
  */
 
 import org.apache.lucene.analysis.NumericTokenStream; // javadocs
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.search.FieldCache; // javadocs
 import org.apache.lucene.search.NumericRangeFilter; // javadocs
 import org.apache.lucene.search.NumericRangeQuery; // javadocs
 import org.apache.lucene.util.NumericUtils;
@@ -57,7 +57,7 @@
  * NumericRangeFilter}.  To sort according to a
  * <code>IntField</code>, use the normal numeric sort types, eg
  * {@link org.apache.lucene.search.SortField.Type#INT}. <code>IntField</code> 
- * values can also be loaded directly from {@link FieldCache}.</p>
+ * values can also be loaded directly from {@link AtomicReader#getNumericDocValues}.</p>
  *
  * <p>You may add the same field name as an <code>IntField</code> to
  * the same document more than once.  Range querying and


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/document/LongField.java lucene5666/lucene/core/src/java/org/apache/lucene/document/LongField.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/document/LongField.java	2014-05-14 03:47:28.758646623 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/document/LongField.java	2014-05-12 13:28:35.816244597 -0400
@@ -18,8 +18,8 @@
  */
 
 import org.apache.lucene.analysis.NumericTokenStream; // javadocs
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.search.FieldCache; // javadocs
 import org.apache.lucene.search.NumericRangeFilter; // javadocs
 import org.apache.lucene.search.NumericRangeQuery; // javadocs
 import org.apache.lucene.util.NumericUtils;
@@ -67,7 +67,7 @@
  * NumericRangeFilter}.  To sort according to a
  * <code>LongField</code>, use the normal numeric sort types, eg
  * {@link org.apache.lucene.search.SortField.Type#LONG}. <code>LongField</code> 
- * values can also be loaded directly from {@link FieldCache}.</p>
+ * values can also be loaded directly from {@link AtomicReader#getNumericDocValues}.</p>
  *
  * <p>You may add the same field name as an <code>LongField</code> to
  * the same document more than once.  Range querying and


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocTermOrds.java lucene5666/lucene/core/src/java/org/apache/lucene/index/DocTermOrds.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocTermOrds.java	2014-05-14 03:47:28.750646623 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/index/DocTermOrds.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,905 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.index;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-
-import org.apache.lucene.codecs.PostingsFormat; // javadocs
-import org.apache.lucene.index.TermsEnum.SeekStatus;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.StringHelper;
-
-/**
- * This class enables fast access to multiple term ords for
- * a specified field across all docIDs.
- *
- * Like FieldCache, it uninverts the index and holds a
- * packed data structure in RAM to enable fast access.
- * Unlike FieldCache, it can handle multi-valued fields,
- * and, it does not hold the term bytes in RAM.  Rather, you
- * must obtain a TermsEnum from the {@link #getOrdTermsEnum}
- * method, and then seek-by-ord to get the term's bytes.
- *
- * While normally term ords are type long, in this API they are
- * int as the internal representation here cannot address
- * more than MAX_INT unique terms.  Also, typically this
- * class is used on fields with relatively few unique terms
- * vs the number of documents.  In addition, there is an
- * internal limit (16 MB) on how many bytes each chunk of
- * documents may consume.  If you trip this limit you'll hit
- * an IllegalStateException.
- *
- * Deleted documents are skipped during uninversion, and if
- * you look them up you'll get 0 ords.
- *
- * The returned per-document ords do not retain their
- * original order in the document.  Instead they are returned
- * in sorted (by ord, ie term's BytesRef comparator) order.  They
- * are also de-dup'd (ie if doc has same term more than once
- * in this field, you'll only get that ord back once).
- *
- * This class tests whether the provided reader is able to
- * retrieve terms by ord (ie, it's single segment, and it
- * uses an ord-capable terms index).  If not, this class
- * will create its own term index internally, allowing to
- * create a wrapped TermsEnum that can handle ord.  The
- * {@link #getOrdTermsEnum} method then provides this
- * wrapped enum, if necessary.
- *
- * The RAM consumption of this class can be high!
- *
- * @lucene.experimental
- */
-
-/*
- * Final form of the un-inverted field:
- *   Each document points to a list of term numbers that are contained in that document.
- *
- *   Term numbers are in sorted order, and are encoded as variable-length deltas from the
- *   previous term number.  Real term numbers start at 2 since 0 and 1 are reserved.  A
- *   term number of 0 signals the end of the termNumber list.
- *
- *   There is a single int[maxDoc()] which either contains a pointer into a byte[] for
- *   the termNumber lists, or directly contains the termNumber list if it fits in the 4
- *   bytes of an integer.  If the first byte in the integer is 1, the next 3 bytes
- *   are a pointer into a byte[] where the termNumber list starts.
- *
- *   There are actually 256 byte arrays, to compensate for the fact that the pointers
- *   into the byte arrays are only 3 bytes long.  The correct byte array for a document
- *   is a function of it's id.
- *
- *   To save space and speed up faceting, any term that matches enough documents will
- *   not be un-inverted... it will be skipped while building the un-inverted field structure,
- *   and will use a set intersection method during faceting.
- *
- *   To further save memory, the terms (the actual string values) are not all stored in
- *   memory, but a TermIndex is used to convert term numbers to term values only
- *   for the terms needed after faceting has completed.  Only every 128th term value
- *   is stored, along with it's corresponding term number, and this is used as an
- *   index to find the closest term and iterate until the desired number is hit (very
- *   much like Lucene's own internal term index).
- *
- */
-
-public class DocTermOrds {
-
-  // Term ords are shifted by this, internally, to reserve
-  // values 0 (end term) and 1 (index is a pointer into byte array)
-  private final static int TNUM_OFFSET = 2;
-
-  /** Every 128th term is indexed, by default. */
-  public final static int DEFAULT_INDEX_INTERVAL_BITS = 7; // decrease to a low number like 2 for testing
-
-  private int indexIntervalBits;
-  private int indexIntervalMask;
-  private int indexInterval;
-
-  /** Don't uninvert terms that exceed this count. */
-  protected final int maxTermDocFreq;
-
-  /** Field we are uninverting. */
-  protected final String field;
-
-  /** Number of terms in the field. */
-  protected int numTermsInField;
-
-  /** Total number of references to term numbers. */
-  protected long termInstances;
-  private long memsz;
-
-  /** Total time to uninvert the field. */
-  protected int total_time;
-
-  /** Time for phase1 of the uninvert process. */
-  protected int phase1_time;
-
-  /** Holds the per-document ords or a pointer to the ords. */
-  protected int[] index;
-
-  /** Holds term ords for documents. */
-  protected byte[][] tnums = new byte[256][];
-
-  /** Total bytes (sum of term lengths) for all indexed terms.*/
-  protected long sizeOfIndexedStrings;
-
-  /** Holds the indexed (by default every 128th) terms. */
-  protected BytesRef[] indexedTermsArray;
-
-  /** If non-null, only terms matching this prefix were
-   *  indexed. */
-  protected BytesRef prefix;
-
-  /** Ordinal of the first term in the field, or 0 if the
-   *  {@link PostingsFormat} does not implement {@link
-   *  TermsEnum#ord}. */
-  protected int ordBase;
-
-  /** Used while uninverting. */
-  protected DocsEnum docsEnum;
-
-  /** Returns total bytes used. */
-  public long ramUsedInBytes() {
-    // can cache the mem size since it shouldn't change
-    if (memsz!=0) return memsz;
-    long sz = 8*8 + 32; // local fields
-    if (index != null) sz += index.length * 4;
-    if (tnums!=null) {
-      for (byte[] arr : tnums)
-        if (arr != null) sz += arr.length;
-    }
-    memsz = sz;
-    return sz;
-  }
-
-  /** Inverts all terms */
-  public DocTermOrds(AtomicReader reader, Bits liveDocs, String field) throws IOException {
-    this(reader, liveDocs, field, null, Integer.MAX_VALUE);
-  }
-
-  /** Inverts only terms starting w/ prefix */
-  public DocTermOrds(AtomicReader reader, Bits liveDocs, String field, BytesRef termPrefix) throws IOException {
-    this(reader, liveDocs, field, termPrefix, Integer.MAX_VALUE);
-  }
-
-  /** Inverts only terms starting w/ prefix, and only terms
-   *  whose docFreq (not taking deletions into account) is
-   *  <=  maxTermDocFreq */
-  public DocTermOrds(AtomicReader reader, Bits liveDocs, String field, BytesRef termPrefix, int maxTermDocFreq) throws IOException {
-    this(reader, liveDocs, field, termPrefix, maxTermDocFreq, DEFAULT_INDEX_INTERVAL_BITS);
-  }
-
-  /** Inverts only terms starting w/ prefix, and only terms
-   *  whose docFreq (not taking deletions into account) is
-   *  <=  maxTermDocFreq, with a custom indexing interval
-   *  (default is every 128nd term). */
-  public DocTermOrds(AtomicReader reader, Bits liveDocs, String field, BytesRef termPrefix, int maxTermDocFreq, int indexIntervalBits) throws IOException {
-    this(field, maxTermDocFreq, indexIntervalBits);
-    uninvert(reader, liveDocs, termPrefix);
-  }
-
-  /** Subclass inits w/ this, but be sure you then call
-   *  uninvert, only once */
-  protected DocTermOrds(String field, int maxTermDocFreq, int indexIntervalBits) {
-    //System.out.println("DTO init field=" + field + " maxTDFreq=" + maxTermDocFreq);
-    this.field = field;
-    this.maxTermDocFreq = maxTermDocFreq;
-    this.indexIntervalBits = indexIntervalBits;
-    indexIntervalMask = 0xffffffff >>> (32-indexIntervalBits);
-    indexInterval = 1 << indexIntervalBits;
-  }
-
-  /** Returns a TermsEnum that implements ord.  If the
-   *  provided reader supports ord, we just return its
-   *  TermsEnum; if it does not, we build a "private" terms
-   *  index internally (WARNING: consumes RAM) and use that
-   *  index to implement ord.  This also enables ord on top
-   *  of a composite reader.  The returned TermsEnum is
-   *  unpositioned.  This returns null if there are no terms.
-   *
-   *  <p><b>NOTE</b>: you must pass the same reader that was
-   *  used when creating this class */
-  public TermsEnum getOrdTermsEnum(AtomicReader reader) throws IOException {
-    if (indexedTermsArray == null) {
-      //System.out.println("GET normal enum");
-      final Fields fields = reader.fields();
-      if (fields == null) {
-        return null;
-      }
-      final Terms terms = fields.terms(field);
-      if (terms == null) {
-        return null;
-      } else {
-        return terms.iterator(null);
-      }
-    } else {
-      //System.out.println("GET wrapped enum ordBase=" + ordBase);
-      return new OrdWrappedTermsEnum(reader);
-    }
-  }
-
-  /**
-   * Returns the number of terms in this field
-   */
-  public int numTerms() {
-    return numTermsInField;
-  }
-
-  /**
-   * Returns {@code true} if no terms were indexed.
-   */
-  public boolean isEmpty() {
-    return index == null;
-  }
-
-  /** Subclass can override this */
-  protected void visitTerm(TermsEnum te, int termNum) throws IOException {
-  }
-
-  /** Invoked during {@link #uninvert(AtomicReader,Bits,BytesRef)}
-   *  to record the document frequency for each uninverted
-   *  term. */
-  protected void setActualDocFreq(int termNum, int df) throws IOException {
-  }
-
-  /** Call this only once (if you subclass!) */
-  protected void uninvert(final AtomicReader reader, Bits liveDocs, final BytesRef termPrefix) throws IOException {
-    final FieldInfo info = reader.getFieldInfos().fieldInfo(field);
-    if (info != null && info.hasDocValues()) {
-      throw new IllegalStateException("Type mismatch: " + field + " was indexed as " + info.getDocValuesType());
-    }
-    //System.out.println("DTO uninvert field=" + field + " prefix=" + termPrefix);
-    final long startTime = System.currentTimeMillis();
-    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);
-
-    final int maxDoc = reader.maxDoc();
-    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number
-    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document
-    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)
-
-    final Fields fields = reader.fields();
-    if (fields == null) {
-      // No terms
-      return;
-    }
-    final Terms terms = fields.terms(field);
-    if (terms == null) {
-      // No terms
-      return;
-    }
-
-    final TermsEnum te = terms.iterator(null);
-    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();
-    //System.out.println("seekStart=" + seekStart.utf8ToString());
-    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {
-      // No terms match
-      return;
-    }
-
-    // If we need our "term index wrapper", these will be
-    // init'd below:
-    List<BytesRef> indexedTerms = null;
-    PagedBytes indexedTermsBytes = null;
-
-    boolean testedOrd = false;
-
-    // we need a minimum of 9 bytes, but round up to 12 since the space would
-    // be wasted with most allocators anyway.
-    byte[] tempArr = new byte[12];
-
-    //
-    // enumerate all terms, and build an intermediate form of the un-inverted field.
-    //
-    // During this intermediate form, every document has a (potential) byte[]
-    // and the int[maxDoc()] array either contains the termNumber list directly
-    // or the *end* offset of the termNumber list in it's byte array (for faster
-    // appending and faster creation of the final form).
-    //
-    // idea... if things are too large while building, we could do a range of docs
-    // at a time (but it would be a fair amount slower to build)
-    // could also do ranges in parallel to take advantage of multiple CPUs
-
-    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)
-    // values.  This requires going over the field first to find the most
-    // frequent terms ahead of time.
-
-    int termNum = 0;
-    docsEnum = null;
-
-    // Loop begins with te positioned to first term (we call
-    // seek above):
-    for (;;) {
-      final BytesRef t = te.term();
-      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {
-        break;
-      }
-      //System.out.println("visit term=" + t.utf8ToString() + " " + t + " termNum=" + termNum);
-
-      if (!testedOrd) {
-        try {
-          ordBase = (int) te.ord();
-          //System.out.println("got ordBase=" + ordBase);
-        } catch (UnsupportedOperationException uoe) {
-          // Reader cannot provide ord support, so we wrap
-          // our own support by creating our own terms index:
-          indexedTerms = new ArrayList<>();
-          indexedTermsBytes = new PagedBytes(15);
-          //System.out.println("NO ORDS");
-        }
-        testedOrd = true;
-      }
-
-      visitTerm(te, termNum);
-
-      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {
-        // Index this term
-        sizeOfIndexedStrings += t.length;
-        BytesRef indexedTerm = new BytesRef();
-        indexedTermsBytes.copy(t, indexedTerm);
-        // TODO: really should 1) strip off useless suffix,
-        // and 2) use FST not array/PagedBytes
-        indexedTerms.add(indexedTerm);
-      }
-
-      final int df = te.docFreq();
-      if (df <= maxTermDocFreq) {
-
-        docsEnum = te.docs(liveDocs, docsEnum, DocsEnum.FLAG_NONE);
-
-        // dF, but takes deletions into account
-        int actualDF = 0;
-
-        for (;;) {
-          int doc = docsEnum.nextDoc();
-          if (doc == DocIdSetIterator.NO_MORE_DOCS) {
-            break;
-          }
-          //System.out.println("  chunk=" + chunk + " docs");
-
-          actualDF ++;
-          termInstances++;
-          
-          //System.out.println("    docID=" + doc);
-          // add TNUM_OFFSET to the term number to make room for special reserved values:
-          // 0 (end term) and 1 (index into byte array follows)
-          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;
-          lastTerm[doc] = termNum;
-          int val = index[doc];
-
-          if ((val & 0xff)==1) {
-            // index into byte array (actually the end of
-            // the doc-specific byte[] when building)
-            int pos = val >>> 8;
-            int ilen = vIntSize(delta);
-            byte[] arr = bytes[doc];
-            int newend = pos+ilen;
-            if (newend > arr.length) {
-              // We avoid a doubling strategy to lower memory usage.
-              // this faceting method isn't for docs with many terms.
-              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.
-              // TODO: figure out what array lengths we can round up to w/o actually using more memory
-              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?
-              // It should be safe to round up to the nearest 32 bits in any case.
-              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment
-              byte[] newarr = new byte[newLen];
-              System.arraycopy(arr, 0, newarr, 0, pos);
-              arr = newarr;
-              bytes[doc] = newarr;
-            }
-            pos = writeInt(delta, arr, pos);
-            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]
-          } else {
-            // OK, this int has data in it... find the end (a zero starting byte - not
-            // part of another number, hence not following a byte with the high bit set).
-            int ipos;
-            if (val==0) {
-              ipos=0;
-            } else if ((val & 0x0000ff80)==0) {
-              ipos=1;
-            } else if ((val & 0x00ff8000)==0) {
-              ipos=2;
-            } else if ((val & 0xff800000)==0) {
-              ipos=3;
-            } else {
-              ipos=4;
-            }
-
-            //System.out.println("      ipos=" + ipos);
-
-            int endPos = writeInt(delta, tempArr, ipos);
-            //System.out.println("      endpos=" + endPos);
-            if (endPos <= 4) {
-              //System.out.println("      fits!");
-              // value will fit in the integer... move bytes back
-              for (int j=ipos; j<endPos; j++) {
-                val |= (tempArr[j] & 0xff) << (j<<3);
-              }
-              index[doc] = val;
-            } else {
-              // value won't fit... move integer into byte[]
-              for (int j=0; j<ipos; j++) {
-                tempArr[j] = (byte)val;
-                val >>>=8;
-              }
-              // point at the end index in the byte[]
-              index[doc] = (endPos<<8) | 1;
-              bytes[doc] = tempArr;
-              tempArr = new byte[12];
-            }
-          }
-        }
-        setActualDocFreq(termNum, actualDF);
-      }
-
-      termNum++;
-      if (te.next() == null) {
-        break;
-      }
-    }
-
-    numTermsInField = termNum;
-
-    long midPoint = System.currentTimeMillis();
-
-    if (termInstances == 0) {
-      // we didn't invert anything
-      // lower memory consumption.
-      tnums = null;
-    } else {
-
-      this.index = index;
-
-      //
-      // transform intermediate form into the final form, building a single byte[]
-      // at a time, and releasing the intermediate byte[]s as we go to avoid
-      // increasing the memory footprint.
-      //
-
-      for (int pass = 0; pass<256; pass++) {
-        byte[] target = tnums[pass];
-        int pos=0;  // end in target;
-        if (target != null) {
-          pos = target.length;
-        } else {
-          target = new byte[4096];
-        }
-
-        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx
-        // where pp is the pass (which array we are building), and xx is all values.
-        // each pass shares the same byte[] for termNumber lists.
-        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {
-          int lim = Math.min(docbase + (1<<16), maxDoc);
-          for (int doc=docbase; doc<lim; doc++) {
-            //System.out.println("  pass=" + pass + " process docID=" + doc);
-            int val = index[doc];
-            if ((val&0xff) == 1) {
-              int len = val >>> 8;
-              //System.out.println("    ptr pos=" + pos);
-              index[doc] = (pos<<8)|1; // change index to point to start of array
-              if ((pos & 0xff000000) != 0) {
-                // we only have 24 bits for the array index
-                throw new IllegalStateException("Too many values for UnInvertedField faceting on field "+field);
-              }
-              byte[] arr = bytes[doc];
-              /*
-              for(byte b : arr) {
-                //System.out.println("      b=" + Integer.toHexString((int) b));
-              }
-              */
-              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM
-              if (target.length <= pos + len) {
-                int newlen = target.length;
-                /*** we don't have to worry about the array getting too large
-                 * since the "pos" param will overflow first (only 24 bits available)
-                if ((newlen<<1) <= 0) {
-                  // overflow...
-                  newlen = Integer.MAX_VALUE;
-                  if (newlen <= pos + len) {
-                    throw new SolrException(400,"Too many terms to uninvert field!");
-                  }
-                } else {
-                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy
-                }
-                ****/
-                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 
-                byte[] newtarget = new byte[newlen];
-                System.arraycopy(target, 0, newtarget, 0, pos);
-                target = newtarget;
-              }
-              System.arraycopy(arr, 0, target, pos, len);
-              pos += len + 1;  // skip single byte at end and leave it 0 for terminator
-            }
-          }
-        }
-
-        // shrink array
-        if (pos < target.length) {
-          byte[] newtarget = new byte[pos];
-          System.arraycopy(target, 0, newtarget, 0, pos);
-          target = newtarget;
-        }
-        
-        tnums[pass] = target;
-
-        if ((pass << 16) > maxDoc)
-          break;
-      }
-
-    }
-    if (indexedTerms != null) {
-      indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);
-    }
-
-    long endTime = System.currentTimeMillis();
-
-    total_time = (int)(endTime-startTime);
-    phase1_time = (int)(midPoint-startTime);
-  }
-
-  /** Number of bytes to represent an unsigned int as a vint. */
-  private static int vIntSize(int x) {
-    if ((x & (0xffffffff << (7*1))) == 0 ) {
-      return 1;
-    }
-    if ((x & (0xffffffff << (7*2))) == 0 ) {
-      return 2;
-    }
-    if ((x & (0xffffffff << (7*3))) == 0 ) {
-      return 3;
-    }
-    if ((x & (0xffffffff << (7*4))) == 0 ) {
-      return 4;
-    }
-    return 5;
-  }
-
-  // todo: if we know the size of the vInt already, we could do
-  // a single switch on the size
-  private static int writeInt(int x, byte[] arr, int pos) {
-    int a;
-    a = (x >>> (7*4));
-    if (a != 0) {
-      arr[pos++] = (byte)(a | 0x80);
-    }
-    a = (x >>> (7*3));
-    if (a != 0) {
-      arr[pos++] = (byte)(a | 0x80);
-    }
-    a = (x >>> (7*2));
-    if (a != 0) {
-      arr[pos++] = (byte)(a | 0x80);
-    }
-    a = (x >>> (7*1));
-    if (a != 0) {
-      arr[pos++] = (byte)(a | 0x80);
-    }
-    arr[pos++] = (byte)(x & 0x7f);
-    return pos;
-  }
-
-  /* Only used if original IndexReader doesn't implement
-   * ord; in this case we "wrap" our own terms index
-   * around it. */
-  private final class OrdWrappedTermsEnum extends TermsEnum {
-    private final TermsEnum termsEnum;
-    private BytesRef term;
-    private long ord = -indexInterval-1;          // force "real" seek
-    
-    public OrdWrappedTermsEnum(AtomicReader reader) throws IOException {
-      assert indexedTermsArray != null;
-      termsEnum = reader.fields().terms(field).iterator(null);
-    }
-
-    @Override    
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-      return termsEnum.docs(liveDocs, reuse, flags);
-    }
-
-    @Override    
-    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-      return termsEnum.docsAndPositions(liveDocs, reuse, flags);
-    }
-
-    @Override
-    public BytesRef term() {
-      return term;
-    }
-
-    @Override
-    public BytesRef next() throws IOException {
-      if (++ord < 0) {
-        ord = 0;
-      }
-      if (termsEnum.next() == null) {
-        term = null;
-        return null;
-      }
-      return setTerm();  // this is extra work if we know we are in bounds...
-    }
-
-    @Override
-    public int docFreq() throws IOException {
-      return termsEnum.docFreq();
-    }
-
-    @Override
-    public long totalTermFreq() throws IOException {
-      return termsEnum.totalTermFreq();
-    }
-
-    @Override
-    public long ord() {
-      return ordBase + ord;
-    }
-
-    @Override
-    public SeekStatus seekCeil(BytesRef target) throws IOException {
-
-      // already here
-      if (term != null && term.equals(target)) {
-        return SeekStatus.FOUND;
-      }
-
-      int startIdx = Arrays.binarySearch(indexedTermsArray, target);
-
-      if (startIdx >= 0) {
-        // we hit the term exactly... lucky us!
-        TermsEnum.SeekStatus seekStatus = termsEnum.seekCeil(target);
-        assert seekStatus == TermsEnum.SeekStatus.FOUND;
-        ord = startIdx << indexIntervalBits;
-        setTerm();
-        assert term != null;
-        return SeekStatus.FOUND;
-      }
-
-      // we didn't hit the term exactly
-      startIdx = -startIdx-1;
-    
-      if (startIdx == 0) {
-        // our target occurs *before* the first term
-        TermsEnum.SeekStatus seekStatus = termsEnum.seekCeil(target);
-        assert seekStatus == TermsEnum.SeekStatus.NOT_FOUND;
-        ord = 0;
-        setTerm();
-        assert term != null;
-        return SeekStatus.NOT_FOUND;
-      }
-
-      // back up to the start of the block
-      startIdx--;
-
-      if ((ord >> indexIntervalBits) == startIdx && term != null && term.compareTo(target) <= 0) {
-        // we are already in the right block and the current term is before the term we want,
-        // so we don't need to seek.
-      } else {
-        // seek to the right block
-        TermsEnum.SeekStatus seekStatus = termsEnum.seekCeil(indexedTermsArray[startIdx]);
-        assert seekStatus == TermsEnum.SeekStatus.FOUND;
-        ord = startIdx << indexIntervalBits;
-        setTerm();
-        assert term != null;  // should be non-null since it's in the index
-      }
-
-      while (term != null && term.compareTo(target) < 0) {
-        next();
-      }
-
-      if (term == null) {
-        return SeekStatus.END;
-      } else if (term.compareTo(target) == 0) {
-        return SeekStatus.FOUND;
-      } else {
-        return SeekStatus.NOT_FOUND;
-      }
-    }
-
-    @Override
-    public void seekExact(long targetOrd) throws IOException {
-      int delta = (int) (targetOrd - ordBase - ord);
-      //System.out.println("  seek(ord) targetOrd=" + targetOrd + " delta=" + delta + " ord=" + ord + " ii=" + indexInterval);
-      if (delta < 0 || delta > indexInterval) {
-        final int idx = (int) (targetOrd >>> indexIntervalBits);
-        final BytesRef base = indexedTermsArray[idx];
-        //System.out.println("  do seek term=" + base.utf8ToString());
-        ord = idx << indexIntervalBits;
-        delta = (int) (targetOrd - ord);
-        final TermsEnum.SeekStatus seekStatus = termsEnum.seekCeil(base);
-        assert seekStatus == TermsEnum.SeekStatus.FOUND;
-      } else {
-        //System.out.println("seek w/in block");
-      }
-
-      while (--delta >= 0) {
-        BytesRef br = termsEnum.next();
-        if (br == null) {
-          assert false;
-          return;
-        }
-        ord++;
-      }
-
-      setTerm();
-      assert term != null;
-    }
-
-    private BytesRef setTerm() throws IOException {
-      term = termsEnum.term();
-      //System.out.println("  setTerm() term=" + term.utf8ToString() + " vs prefix=" + (prefix == null ? "null" : prefix.utf8ToString()));
-      if (prefix != null && !StringHelper.startsWith(term, prefix)) {
-        term = null;
-      }
-      return term;
-    }
-  }
-
-  /** Returns the term ({@link BytesRef}) corresponding to
-   *  the provided ordinal. */
-  public BytesRef lookupTerm(TermsEnum termsEnum, int ord) throws IOException {
-    termsEnum.seekExact(ord);
-    return termsEnum.term();
-  }
-  
-  /** Returns a SortedSetDocValues view of this instance */
-  public SortedSetDocValues iterator(AtomicReader reader) throws IOException {
-    if (isEmpty()) {
-      return DocValues.EMPTY_SORTED_SET;
-    } else {
-      return new Iterator(reader);
-    }
-  }
-  
-  private class Iterator extends SortedSetDocValues {
-    final AtomicReader reader;
-    final TermsEnum te;  // used internally for lookupOrd() and lookupTerm()
-    // currently we read 5 at a time (using the logic of the old iterator)
-    final int buffer[] = new int[5];
-    int bufferUpto;
-    int bufferLength;
-    
-    private int tnum;
-    private int upto;
-    private byte[] arr;
-    
-    Iterator(AtomicReader reader) throws IOException {
-      this.reader = reader;
-      this.te = termsEnum();
-    }
-    
-    @Override
-    public long nextOrd() {
-      while (bufferUpto == bufferLength) {
-        if (bufferLength < buffer.length) {
-          return NO_MORE_ORDS;
-        } else {
-          bufferLength = read(buffer);
-          bufferUpto = 0;
-        }
-      }
-      return buffer[bufferUpto++];
-    }
-    
-    /** Buffer must be at least 5 ints long.  Returns number
-     *  of term ords placed into buffer; if this count is
-     *  less than buffer.length then that is the end. */
-    int read(int[] buffer) {
-      int bufferUpto = 0;
-      if (arr == null) {
-        // code is inlined into upto
-        //System.out.println("inlined");
-        int code = upto;
-        int delta = 0;
-        for (;;) {
-          delta = (delta << 7) | (code & 0x7f);
-          if ((code & 0x80)==0) {
-            if (delta==0) break;
-            tnum += delta - TNUM_OFFSET;
-            buffer[bufferUpto++] = ordBase+tnum;
-            //System.out.println("  tnum=" + tnum);
-            delta = 0;
-          }
-          code >>>= 8;
-        }
-      } else {
-        // code is a pointer
-        for(;;) {
-          int delta = 0;
-          for(;;) {
-            byte b = arr[upto++];
-            delta = (delta << 7) | (b & 0x7f);
-            //System.out.println("    cycle: upto=" + upto + " delta=" + delta + " b=" + b);
-            if ((b & 0x80) == 0) break;
-          }
-          //System.out.println("  delta=" + delta);
-          if (delta == 0) break;
-          tnum += delta - TNUM_OFFSET;
-          //System.out.println("  tnum=" + tnum);
-          buffer[bufferUpto++] = ordBase+tnum;
-          if (bufferUpto == buffer.length) {
-            break;
-          }
-        }
-      }
-
-      return bufferUpto;
-    }
-
-    @Override
-    public void setDocument(int docID) {
-      tnum = 0;
-      final int code = index[docID];
-      if ((code & 0xff)==1) {
-        // a pointer
-        upto = code>>>8;
-        //System.out.println("    pointer!  upto=" + upto);
-        int whichArray = (docID >>> 16) & 0xff;
-        arr = tnums[whichArray];
-      } else {
-        //System.out.println("    inline!");
-        arr = null;
-        upto = code;
-      }
-      bufferUpto = 0;
-      bufferLength = read(buffer);
-    }
-
-    @Override
-    public void lookupOrd(long ord, BytesRef result) {
-      BytesRef ref = null;
-      try {
-        ref = DocTermOrds.this.lookupTerm(te, (int) ord);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-      result.bytes = ref.bytes;
-      result.offset = ref.offset;
-      result.length = ref.length;
-    }
-
-    @Override
-    public long getValueCount() {
-      return numTerms();
-    }
-
-    @Override
-    public long lookupTerm(BytesRef key) {
-      try {
-        if (te.seekCeil(key) == SeekStatus.FOUND) {
-          return te.ord();
-        } else {
-          return -te.ord()-1;
-        }
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-    
-    @Override
-    public TermsEnum termsEnum() {    
-      try {
-        return getOrdTermsEnum(reader);
-      } catch (IOException e) {
-        throw new RuntimeException();
-      }
-    }
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocValues.java lucene5666/lucene/core/src/java/org/apache/lucene/index/DocValues.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocValues.java	2014-05-14 03:47:28.750646623 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/index/DocValues.java	2014-05-12 13:28:37.064244619 -0400
@@ -17,6 +17,8 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 
@@ -159,4 +161,72 @@
       }
     };
   }
+  
+  // some helpers, for transition from fieldcache apis.
+  // as opposed to the AtomicReader apis (which must be strict for consistency), these are lenient
+  
+  /**
+   * Returns NumericDocValues for the reader, or {@link #EMPTY_NUMERIC} if it has none. 
+   */
+  public static NumericDocValues getNumeric(AtomicReader in, String field) throws IOException {
+    NumericDocValues dv = in.getNumericDocValues(field);
+    if (dv == null) {
+      return EMPTY_NUMERIC;
+    } else {
+      return dv;
+    }
+  }
+  
+  /**
+   * Returns BinaryDocValues for the reader, or {@link #EMPTY_BINARY} if it has none. 
+   */
+  public static BinaryDocValues getBinary(AtomicReader in, String field) throws IOException {
+    BinaryDocValues dv = in.getBinaryDocValues(field);
+    if (dv == null) {
+      dv = in.getSortedDocValues(field);
+      if (dv == null) {
+        return EMPTY_BINARY;
+      }
+    }
+    return dv;
+  }
+  
+  /**
+   * Returns SortedDocValues for the reader, or {@link #EMPTY_SORTED} if it has none. 
+   */
+  public static SortedDocValues getSorted(AtomicReader in, String field) throws IOException {
+    SortedDocValues dv = in.getSortedDocValues(field);
+    if (dv == null) {
+      return EMPTY_SORTED;
+    } else {
+      return dv;
+    }
+  }
+  
+  /**
+   * Returns SortedSetDocValues for the reader, or {@link #EMPTY_SORTED_SET} if it has none. 
+   */
+  public static SortedSetDocValues getSortedSet(AtomicReader in, String field) throws IOException {
+    SortedSetDocValues dv = in.getSortedSetDocValues(field);
+    if (dv == null) {
+      SortedDocValues sorted = in.getSortedDocValues(field);
+      if (sorted == null) {
+        return EMPTY_SORTED_SET;
+      }
+      return singleton(sorted);
+    }
+    return dv;
+  }
+  
+  /**
+   * Returns Bits for the reader, or {@link Bits} matching nothing if it has none. 
+   */
+  public static Bits getDocsWithField(AtomicReader in, String field) throws IOException {
+    Bits dv = in.getDocsWithField(field);
+    if (dv == null) {
+      return new Bits.MatchNoBits(in.maxDoc());
+    } else {
+      return dv;
+    }
+  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/FilterAtomicReader.java lucene5666/lucene/core/src/java/org/apache/lucene/index/FilterAtomicReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/FilterAtomicReader.java	2014-05-14 03:47:28.750646623 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/index/FilterAtomicReader.java	2014-05-12 13:28:37.556244627 -0400
@@ -21,7 +21,6 @@
 import java.util.Iterator;
 
 import org.apache.lucene.search.CachingWrapperFilter;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
@@ -38,8 +37,8 @@
  * to override {@link #numDocs()} as well and vice-versa.
  * <p><b>NOTE</b>: If this {@link FilterAtomicReader} does not change the
  * content the contained reader, you could consider overriding
- * {@link #getCoreCacheKey()} so that {@link FieldCache} and
- * {@link CachingWrapperFilter} share the same entries for this atomic reader
+ * {@link #getCoreCacheKey()} so that
+ * {@link CachingWrapperFilter} shares the same entries for this atomic reader
  * and the wrapped one. {@link #getCombinedCoreAndDeletesKey()} could be
  * overridden as well if the {@link #getLiveDocs() live docs} are not changed
  * either.


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/IndexReader.java lucene5666/lucene/core/src/java/org/apache/lucene/index/IndexReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/IndexReader.java	2014-05-14 03:47:28.750646623 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/index/IndexReader.java	2014-05-14 03:45:16.694644324 -0400
@@ -426,7 +426,7 @@
     return getContext().leaves();
   }
 
-  /** Expert: Returns a key for this IndexReader, so FieldCache/CachingWrapperFilter can find
+  /** Expert: Returns a key for this IndexReader, so CachingWrapperFilter can find
    * it again.
    * This key must not have equals()/hashCode() methods, so &quot;equals&quot; means &quot;identical&quot;. */
   public Object getCoreCacheKey() {
@@ -436,7 +436,7 @@
   }
 
   /** Expert: Returns a key for this IndexReader that also includes deletions,
-   * so FieldCache/CachingWrapperFilter can find it again.
+   * so CachingWrapperFilter can find it again.
    * This key must not have equals()/hashCode() methods, so &quot;equals&quot; means &quot;identical&quot;. */
   public Object getCombinedCoreAndDeletesKey() {
     // Don't call ensureOpen since FC calls this (to evict)


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java lucene5666/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java	2014-05-14 03:47:28.750646623 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java	2014-05-14 03:45:16.694644324 -0400
@@ -34,7 +34,7 @@
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.search.FieldCache;
+import org.apache.lucene.search.CachingWrapperFilter;
 import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
@@ -361,7 +361,7 @@
 
   // This is necessary so that cloned SegmentReaders (which
   // share the underlying postings data) will map to the
-  // same entry in the FieldCache.  See LUCENE-1579.
+  // same entry for CachingWrapperFilter.  See LUCENE-1579.
   @Override
   public Object getCoreCacheKey() {
     // NOTE: if this ever changes, be sure to fix
@@ -525,7 +525,7 @@
    * sharing the same core are closed.  At this point it 
    * is safe for apps to evict this reader from any caches 
    * keyed on {@link #getCoreCacheKey}.  This is the same 
-   * interface that {@link FieldCache} uses, internally, 
+   * interface that {@link CachingWrapperFilter} uses, internally, 
    * to evict entries.</p>
    * 
    * @lucene.experimental


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/SingletonSortedSetDocValues.java lucene5666/lucene/core/src/java/org/apache/lucene/index/SingletonSortedSetDocValues.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/SingletonSortedSetDocValues.java	2014-05-14 03:47:28.750646623 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/index/SingletonSortedSetDocValues.java	2014-05-14 03:45:16.694644324 -0400
@@ -23,8 +23,7 @@
  * Exposes multi-valued view over a single-valued instance.
  * <p>
  * This can be used if you want to have one multi-valued implementation
- * against e.g. FieldCache.getDocTermOrds that also works for single-valued 
- * fields.
+ * that works for single or multi-valued types.
  */
 final class SingletonSortedSetDocValues extends SortedSetDocValues {
   private final SortedDocValues in;


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/DocTermOrdsRangeFilter.java lucene5666/lucene/core/src/java/org/apache/lucene/search/DocTermOrdsRangeFilter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/DocTermOrdsRangeFilter.java	2014-05-14 03:47:28.770646624 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/DocTermOrdsRangeFilter.java	2014-05-14 03:45:16.618644322 -0400
@@ -18,15 +18,17 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedSetDocValues;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 
 /**
- * A range filter built on top of a cached multi-valued term field (in {@link FieldCache}).
+ * A range filter built on top of a cached multi-valued term field (from {@link AtomicReader#getSortedSetDocValues}).
  * 
- * <p>Like {@link FieldCacheRangeFilter}, this is just a specialized range query versus
+ * <p>Like {@link DocValuesRangeFilter}, this is just a specialized range query versus
  *    using a TermRangeQuery with {@link DocTermOrdsRewriteMethod}: it will only do
  *    two ordinal to term lookups.</p>
  */
@@ -51,7 +53,7 @@
   public abstract DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException;
   
   /**
-   * Creates a BytesRef range filter using {@link FieldCache#getTermsIndex}. This works with all
+   * Creates a BytesRef range filter using {@link AtomicReader#getSortedSetDocValues}. This works with all
    * fields containing zero or one term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
@@ -59,7 +61,7 @@
     return new DocTermOrdsRangeFilter(field, lowerVal, upperVal, includeLower, includeUpper) {
       @Override
       public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-        final SortedSetDocValues docTermOrds = FieldCache.DEFAULT.getDocTermOrds(context.reader(), field);
+        final SortedSetDocValues docTermOrds = DocValues.getSortedSet(context.reader(), field);
         final long lowerPoint = lowerVal == null ? -1 : docTermOrds.lookupTerm(lowerVal);
         final long upperPoint = upperVal == null ? -1 : docTermOrds.lookupTerm(upperVal);
 
@@ -95,7 +97,7 @@
         
         assert inclusiveLowerPoint >= 0 && inclusiveUpperPoint >= 0;
         
-        return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
+        return new DocValuesDocIdSet(context.reader().maxDoc(), acceptDocs) {
           @Override
           protected final boolean matchDoc(int doc) {
             docTermOrds.setDocument(doc);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/DocTermOrdsRewriteMethod.java lucene5666/lucene/core/src/java/org/apache/lucene/search/DocTermOrdsRewriteMethod.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/DocTermOrdsRewriteMethod.java	2014-05-14 03:47:28.770646624 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/DocTermOrdsRewriteMethod.java	2014-05-14 03:45:16.618644322 -0400
@@ -20,6 +20,7 @@
 import java.io.IOException;
 
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.SortedSetDocValues;
 import org.apache.lucene.index.Terms;
@@ -83,7 +84,7 @@
      */
     @Override
     public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
-      final SortedSetDocValues docTermOrds = FieldCache.DEFAULT.getDocTermOrds(context.reader(), query.field);
+      final SortedSetDocValues docTermOrds = DocValues.getSortedSet(context.reader(), query.field);
       // Cannot use FixedBitSet because we require long index (ord):
       final LongBitSet termSet = new LongBitSet(docTermOrds.getValueCount());
       TermsEnum termsEnum = query.getTermsEnum(new Terms() {
@@ -144,7 +145,7 @@
         return null;
       }
       
-      return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
+      return new DocValuesDocIdSet(context.reader().maxDoc(), acceptDocs) {
         @Override
         protected final boolean matchDoc(int doc) throws ArrayIndexOutOfBoundsException {
           docTermOrds.setDocument(doc);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/DocValuesDocIdSet.java lucene5666/lucene/core/src/java/org/apache/lucene/search/DocValuesDocIdSet.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/DocValuesDocIdSet.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/DocValuesDocIdSet.java	2014-05-14 03:45:16.614644322 -0400
@@ -0,0 +1,167 @@
+package org.apache.lucene.search;
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.OpenBitSet;
+
+/**
+ * Base class for DocIdSet to be used with DocValues. The implementation
+ * of its iterator is very stupid and slow if the implementation of the
+ * {@link #matchDoc} method is not optimized, as iterators simply increment
+ * the document id until {@code matchDoc(int)} returns true. Because of this
+ * {@code matchDoc(int)} must be as fast as possible and in no case do any
+ * I/O.
+ * @lucene.internal
+ */
+public abstract class DocValuesDocIdSet extends DocIdSet {
+
+  protected final int maxDoc;
+  protected final Bits acceptDocs;
+
+  public DocValuesDocIdSet(int maxDoc, Bits acceptDocs) {
+    this.maxDoc = maxDoc;
+    this.acceptDocs = acceptDocs;
+  }
+
+  /**
+   * this method checks, if a doc is a hit
+   */
+  protected abstract boolean matchDoc(int doc);
+
+  /**
+   * this DocIdSet is always cacheable (does not go back
+   * to the reader for iteration)
+   */
+  @Override
+  public final boolean isCacheable() {
+    return true;
+  }
+
+  @Override
+  public final Bits bits() {
+    return (acceptDocs == null) ? new Bits() {
+      @Override
+      public boolean get(int docid) {
+        return matchDoc(docid);
+      }
+
+      @Override
+      public int length() {
+        return maxDoc;
+      }
+    } : new Bits() {
+      @Override
+      public boolean get(int docid) {
+        return matchDoc(docid) && acceptDocs.get(docid);
+      }
+
+      @Override
+      public int length() {
+        return maxDoc;
+      }
+    };
+  }
+
+  @Override
+  public final DocIdSetIterator iterator() throws IOException {
+    if (acceptDocs == null) {
+      // Specialization optimization disregard acceptDocs
+      return new DocIdSetIterator() {
+        private int doc = -1;
+        
+        @Override
+        public int docID() {
+          return doc;
+        }
+      
+        @Override
+        public int nextDoc() {
+          do {
+            doc++;
+            if (doc >= maxDoc) {
+              return doc = NO_MORE_DOCS;
+            }
+          } while (!matchDoc(doc));
+          return doc;
+        }
+      
+        @Override
+        public int advance(int target) {
+          for(doc=target; doc<maxDoc; doc++) {
+            if (matchDoc(doc)) {
+              return doc;
+            }
+          }
+          return doc = NO_MORE_DOCS;
+        }
+
+        @Override
+        public long cost() {
+          return maxDoc;
+        }
+      };
+    } else if (acceptDocs instanceof FixedBitSet || acceptDocs instanceof OpenBitSet) {
+      // special case for FixedBitSet / OpenBitSet: use the iterator and filter it
+      // (used e.g. when Filters are chained by FilteredQuery)
+      return new FilteredDocIdSetIterator(((DocIdSet) acceptDocs).iterator()) {
+        @Override
+        protected boolean match(int doc) {
+          return DocValuesDocIdSet.this.matchDoc(doc);
+        }
+      };
+    } else {
+      // Stupid consultation of acceptDocs and matchDoc()
+      return new DocIdSetIterator() {
+        private int doc = -1;
+        
+        @Override
+        public int docID() {
+          return doc;
+        }
+      
+        @Override
+        public int nextDoc() {
+          do {
+            doc++;
+            if (doc >= maxDoc) {
+              return doc = NO_MORE_DOCS;
+            }
+          } while (!(matchDoc(doc) && acceptDocs.get(doc)));
+          return doc;
+        }
+      
+        @Override
+        public int advance(int target) {
+          for(doc=target; doc<maxDoc; doc++) {
+            if (matchDoc(doc) && acceptDocs.get(doc)) {
+              return doc;
+            }
+          }
+          return doc = NO_MORE_DOCS;
+        }
+
+        @Override
+        public long cost() {
+          return maxDoc;
+        }
+      };
+    }
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/DocValuesRangeFilter.java lucene5666/lucene/core/src/java/org/apache/lucene/search/DocValuesRangeFilter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/DocValuesRangeFilter.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/DocValuesRangeFilter.java	2014-05-14 03:45:16.618644322 -0400
@@ -0,0 +1,427 @@
+package org.apache.lucene.search;
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.document.DoubleField; // for javadocs
+import org.apache.lucene.document.FloatField; // for javadocs
+import org.apache.lucene.document.IntField; // for javadocs
+import org.apache.lucene.document.LongField; // for javadocs
+import org.apache.lucene.index.AtomicReader; // for javadocs
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.NumericUtils;
+
+/**
+ * A range filter built on top of an uninverted single term field 
+ * (from {@link AtomicReader#getNumericDocValues(String)}).
+ * 
+ * <p>{@code DocValuesRangeFilter} builds a single cache for the field the first time it is used.
+ * Each subsequent {@code DocValuesRangeFilter} on the same field then reuses this cache,
+ * even if the range itself changes. 
+ * 
+ * <p>This means that {@code DocValuesRangeFilter} is much faster (sometimes more than 100x as fast) 
+ * as building a {@link TermRangeFilter}, if using a {@link #newStringRange}.
+ * However, if the range never changes it is slower (around 2x as slow) than building
+ * a CachingWrapperFilter on top of a single {@link TermRangeFilter}.
+ *
+ * For numeric data types, this filter may be significantly faster than {@link NumericRangeFilter}.
+ * Furthermore, it does not need the numeric values encoded
+ * by {@link IntField}, {@link FloatField}, {@link
+ * LongField} or {@link DoubleField}. But
+ * it has the problem that it only works with exact one value/document (see below).
+ *
+ * <p>As with all {@link AtomicReader#getNumericDocValues} based functionality, 
+ * {@code DocValuesRangeFilter} is only valid for 
+ * fields which exact one term for each document (except for {@link #newStringRange}
+ * where 0 terms are also allowed). Due to historical reasons, for numeric ranges
+ * all terms that do not have a numeric value, 0 is assumed.
+ *
+ * <p>Thus it works on dates, prices and other single value fields but will not work on
+ * regular text fields. It is preferable to use a <code>NOT_ANALYZED</code> field to ensure that
+ * there is only a single term. 
+ *
+ * <p>This class does not have an constructor, use one of the static factory methods available,
+ * that create a correct instance for different data types.
+ */
+// TODO: use docsWithField to handle empty properly
+public abstract class DocValuesRangeFilter<T> extends Filter {
+  final String field;
+  final T lowerVal;
+  final T upperVal;
+  final boolean includeLower;
+  final boolean includeUpper;
+  
+  private DocValuesRangeFilter(String field, T lowerVal, T upperVal, boolean includeLower, boolean includeUpper) {
+    this.field = field;
+    this.lowerVal = lowerVal;
+    this.upperVal = upperVal;
+    this.includeLower = includeLower;
+    this.includeUpper = includeUpper;
+  }
+  
+  /** This method is implemented for each data type */
+  @Override
+  public abstract DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException;
+
+  /**
+   * Creates a string range filter using {@link AtomicReader#getSortedDocValues(String)}. This works with all
+   * fields containing zero or one term in the field. The range can be half-open by setting one
+   * of the values to <code>null</code>.
+   */
+  public static DocValuesRangeFilter<String> newStringRange(String field, String lowerVal, String upperVal, boolean includeLower, boolean includeUpper) {
+    return new DocValuesRangeFilter<String>(field, lowerVal, upperVal, includeLower, includeUpper) {
+      @Override
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+        final SortedDocValues fcsi = DocValues.getSorted(context.reader(), field);
+        final int lowerPoint = lowerVal == null ? -1 : fcsi.lookupTerm(new BytesRef(lowerVal));
+        final int upperPoint = upperVal == null ? -1 : fcsi.lookupTerm(new BytesRef(upperVal));
+
+        final int inclusiveLowerPoint, inclusiveUpperPoint;
+
+        // Hints:
+        // * binarySearchLookup returns -1, if value was null.
+        // * the value is <0 if no exact hit was found, the returned value
+        //   is (-(insertion point) - 1)
+        if (lowerPoint == -1 && lowerVal == null) {
+          inclusiveLowerPoint = 0;
+        } else if (includeLower && lowerPoint >= 0) {
+          inclusiveLowerPoint = lowerPoint;
+        } else if (lowerPoint >= 0) {
+          inclusiveLowerPoint = lowerPoint + 1;
+        } else {
+          inclusiveLowerPoint = Math.max(0, -lowerPoint - 1);
+        }
+        
+        if (upperPoint == -1 && upperVal == null) {
+          inclusiveUpperPoint = Integer.MAX_VALUE;  
+        } else if (includeUpper && upperPoint >= 0) {
+          inclusiveUpperPoint = upperPoint;
+        } else if (upperPoint >= 0) {
+          inclusiveUpperPoint = upperPoint - 1;
+        } else {
+          inclusiveUpperPoint = -upperPoint - 2;
+        }      
+
+        if (inclusiveUpperPoint < 0 || inclusiveLowerPoint > inclusiveUpperPoint) {
+          return null;
+        }
+        
+        assert inclusiveLowerPoint >= 0 && inclusiveUpperPoint >= 0;
+        
+        return new DocValuesDocIdSet(context.reader().maxDoc(), acceptDocs) {
+          @Override
+          protected final boolean matchDoc(int doc) {
+            final int docOrd = fcsi.getOrd(doc);
+            return docOrd >= inclusiveLowerPoint && docOrd <= inclusiveUpperPoint;
+          }
+        };
+      }
+    };
+  }
+  
+  /**
+   * Creates a BytesRef range filter using {@link AtomicReader#getSortedDocValues(String)}. This works with all
+   * fields containing zero or one term in the field. The range can be half-open by setting one
+   * of the values to <code>null</code>.
+   */
+  // TODO: bogus that newStringRange doesnt share this code... generics hell
+  public static DocValuesRangeFilter<BytesRef> newBytesRefRange(String field, BytesRef lowerVal, BytesRef upperVal, boolean includeLower, boolean includeUpper) {
+    return new DocValuesRangeFilter<BytesRef>(field, lowerVal, upperVal, includeLower, includeUpper) {
+      @Override
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+        final SortedDocValues fcsi = DocValues.getSorted(context.reader(), field);
+        final int lowerPoint = lowerVal == null ? -1 : fcsi.lookupTerm(lowerVal);
+        final int upperPoint = upperVal == null ? -1 : fcsi.lookupTerm(upperVal);
+
+        final int inclusiveLowerPoint, inclusiveUpperPoint;
+
+        // Hints:
+        // * binarySearchLookup returns -1, if value was null.
+        // * the value is <0 if no exact hit was found, the returned value
+        //   is (-(insertion point) - 1)
+        if (lowerPoint == -1 && lowerVal == null) {
+          inclusiveLowerPoint = 0;
+        } else if (includeLower && lowerPoint >= 0) {
+          inclusiveLowerPoint = lowerPoint;
+        } else if (lowerPoint >= 0) {
+          inclusiveLowerPoint = lowerPoint + 1;
+        } else {
+          inclusiveLowerPoint = Math.max(0, -lowerPoint - 1);
+        }
+        
+        if (upperPoint == -1 && upperVal == null) {
+          inclusiveUpperPoint = Integer.MAX_VALUE;  
+        } else if (includeUpper && upperPoint >= 0) {
+          inclusiveUpperPoint = upperPoint;
+        } else if (upperPoint >= 0) {
+          inclusiveUpperPoint = upperPoint - 1;
+        } else {
+          inclusiveUpperPoint = -upperPoint - 2;
+        }      
+
+        if (inclusiveUpperPoint < 0 || inclusiveLowerPoint > inclusiveUpperPoint) {
+          return null;
+        }
+        
+        assert inclusiveLowerPoint >= 0 && inclusiveUpperPoint >= 0;
+        
+        return new DocValuesDocIdSet(context.reader().maxDoc(), acceptDocs) {
+          @Override
+          protected final boolean matchDoc(int doc) {
+            final int docOrd = fcsi.getOrd(doc);
+            return docOrd >= inclusiveLowerPoint && docOrd <= inclusiveUpperPoint;
+          }
+        };
+      }
+    };
+  }
+
+  /**
+   * Creates a numeric range filter using {@link AtomicReader#getSortedDocValues(String)}. This works with all
+   * int fields containing exactly one numeric term in the field. The range can be half-open by setting one
+   * of the values to <code>null</code>.
+   */
+  public static DocValuesRangeFilter<Integer> newIntRange(String field, Integer lowerVal, Integer upperVal, boolean includeLower, boolean includeUpper) {
+    return new DocValuesRangeFilter<Integer>(field, lowerVal, upperVal, includeLower, includeUpper) {
+      @Override
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+        final int inclusiveLowerPoint, inclusiveUpperPoint;
+        if (lowerVal != null) {
+          int i = lowerVal.intValue();
+          if (!includeLower && i == Integer.MAX_VALUE)
+            return null;
+          inclusiveLowerPoint = includeLower ? i : (i + 1);
+        } else {
+          inclusiveLowerPoint = Integer.MIN_VALUE;
+        }
+        if (upperVal != null) {
+          int i = upperVal.intValue();
+          if (!includeUpper && i == Integer.MIN_VALUE)
+            return null;
+          inclusiveUpperPoint = includeUpper ? i : (i - 1);
+        } else {
+          inclusiveUpperPoint = Integer.MAX_VALUE;
+        }
+        
+        if (inclusiveLowerPoint > inclusiveUpperPoint)
+          return null;
+        
+        final NumericDocValues values = DocValues.getNumeric(context.reader(), field);
+        return new DocValuesDocIdSet(context.reader().maxDoc(), acceptDocs) {
+          @Override
+          protected boolean matchDoc(int doc) {
+            final int value = (int) values.get(doc);
+            return value >= inclusiveLowerPoint && value <= inclusiveUpperPoint;
+          }
+        };
+      }
+    };
+  }
+  
+  /**
+   * Creates a numeric range filter using {@link AtomicReader#getNumericDocValues(String)}. This works with all
+   * long fields containing exactly one numeric term in the field. The range can be half-open by setting one
+   * of the values to <code>null</code>.
+   */
+  public static DocValuesRangeFilter<Long> newLongRange(String field, Long lowerVal, Long upperVal, boolean includeLower, boolean includeUpper) {
+    return new DocValuesRangeFilter<Long>(field, lowerVal, upperVal, includeLower, includeUpper) {
+      @Override
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+        final long inclusiveLowerPoint, inclusiveUpperPoint;
+        if (lowerVal != null) {
+          long i = lowerVal.longValue();
+          if (!includeLower && i == Long.MAX_VALUE)
+            return null;
+          inclusiveLowerPoint = includeLower ? i : (i + 1L);
+        } else {
+          inclusiveLowerPoint = Long.MIN_VALUE;
+        }
+        if (upperVal != null) {
+          long i = upperVal.longValue();
+          if (!includeUpper && i == Long.MIN_VALUE)
+            return null;
+          inclusiveUpperPoint = includeUpper ? i : (i - 1L);
+        } else {
+          inclusiveUpperPoint = Long.MAX_VALUE;
+        }
+        
+        if (inclusiveLowerPoint > inclusiveUpperPoint)
+          return null;
+        
+        final NumericDocValues values = DocValues.getNumeric(context.reader(), field);
+        return new DocValuesDocIdSet(context.reader().maxDoc(), acceptDocs) {
+          @Override
+          protected boolean matchDoc(int doc) {
+            final long value = values.get(doc);
+            return value >= inclusiveLowerPoint && value <= inclusiveUpperPoint;
+          }
+        };
+      }
+    };
+  }
+  
+  /**
+   * Creates a numeric range filter using {@link AtomicReader#getNumericDocValues(String)}. This works with all
+   * float fields containing exactly one numeric term in the field. The range can be half-open by setting one
+   * of the values to <code>null</code>.
+   */
+  public static DocValuesRangeFilter<Float> newFloatRange(String field, Float lowerVal, Float upperVal, boolean includeLower, boolean includeUpper) {
+    return new DocValuesRangeFilter<Float>(field, lowerVal, upperVal, includeLower, includeUpper) {
+      @Override
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+        // we transform the floating point numbers to sortable integers
+        // using NumericUtils to easier find the next bigger/lower value
+        final float inclusiveLowerPoint, inclusiveUpperPoint;
+        if (lowerVal != null) {
+          float f = lowerVal.floatValue();
+          if (!includeUpper && f > 0.0f && Float.isInfinite(f))
+            return null;
+          int i = NumericUtils.floatToSortableInt(f);
+          inclusiveLowerPoint = NumericUtils.sortableIntToFloat( includeLower ?  i : (i + 1) );
+        } else {
+          inclusiveLowerPoint = Float.NEGATIVE_INFINITY;
+        }
+        if (upperVal != null) {
+          float f = upperVal.floatValue();
+          if (!includeUpper && f < 0.0f && Float.isInfinite(f))
+            return null;
+          int i = NumericUtils.floatToSortableInt(f);
+          inclusiveUpperPoint = NumericUtils.sortableIntToFloat( includeUpper ? i : (i - 1) );
+        } else {
+          inclusiveUpperPoint = Float.POSITIVE_INFINITY;
+        }
+        
+        if (inclusiveLowerPoint > inclusiveUpperPoint)
+          return null;
+        
+        final NumericDocValues values = DocValues.getNumeric(context.reader(), field);
+        return new DocValuesDocIdSet(context.reader().maxDoc(), acceptDocs) {
+          @Override
+          protected boolean matchDoc(int doc) {
+            final float value = Float.intBitsToFloat((int)values.get(doc));
+            return value >= inclusiveLowerPoint && value <= inclusiveUpperPoint;
+          }
+        };
+      }
+    };
+  }
+  
+  /**
+   * Creates a numeric range filter using {@link AtomicReader#getNumericDocValues(String)}. This works with all
+   * double fields containing exactly one numeric term in the field. The range can be half-open by setting one
+   * of the values to <code>null</code>.
+   */
+  public static DocValuesRangeFilter<Double> newDoubleRange(String field, Double lowerVal, Double upperVal, boolean includeLower, boolean includeUpper) {
+    return new DocValuesRangeFilter<Double>(field, lowerVal, upperVal, includeLower, includeUpper) {
+      @Override
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+        // we transform the floating point numbers to sortable integers
+        // using NumericUtils to easier find the next bigger/lower value
+        final double inclusiveLowerPoint, inclusiveUpperPoint;
+        if (lowerVal != null) {
+          double f = lowerVal.doubleValue();
+          if (!includeUpper && f > 0.0 && Double.isInfinite(f))
+            return null;
+          long i = NumericUtils.doubleToSortableLong(f);
+          inclusiveLowerPoint = NumericUtils.sortableLongToDouble( includeLower ?  i : (i + 1L) );
+        } else {
+          inclusiveLowerPoint = Double.NEGATIVE_INFINITY;
+        }
+        if (upperVal != null) {
+          double f = upperVal.doubleValue();
+          if (!includeUpper && f < 0.0 && Double.isInfinite(f))
+            return null;
+          long i = NumericUtils.doubleToSortableLong(f);
+          inclusiveUpperPoint = NumericUtils.sortableLongToDouble( includeUpper ? i : (i - 1L) );
+        } else {
+          inclusiveUpperPoint = Double.POSITIVE_INFINITY;
+        }
+        
+        if (inclusiveLowerPoint > inclusiveUpperPoint)
+          return null;
+        
+        final NumericDocValues values = DocValues.getNumeric(context.reader(), field);
+        // ignore deleted docs if range doesn't contain 0
+        return new DocValuesDocIdSet(context.reader().maxDoc(), acceptDocs) {
+          @Override
+          protected boolean matchDoc(int doc) {
+            final double value = Double.longBitsToDouble(values.get(doc));
+            return value >= inclusiveLowerPoint && value <= inclusiveUpperPoint;
+          }
+        };
+      }
+    };
+  }
+  
+  @Override
+  public final String toString() {
+    final StringBuilder sb = new StringBuilder(field).append(":");
+    return sb.append(includeLower ? '[' : '{')
+      .append((lowerVal == null) ? "*" : lowerVal.toString())
+      .append(" TO ")
+      .append((upperVal == null) ? "*" : upperVal.toString())
+      .append(includeUpper ? ']' : '}')
+      .toString();
+  }
+
+  @Override
+  @SuppressWarnings({"rawtypes"})
+  public final boolean equals(Object o) {
+    if (this == o) return true;
+    if (!(o instanceof DocValuesRangeFilter)) return false;
+    DocValuesRangeFilter other = (DocValuesRangeFilter) o;
+
+    if (!this.field.equals(other.field)
+        || this.includeLower != other.includeLower
+        || this.includeUpper != other.includeUpper
+    ) { return false; }
+    if (this.lowerVal != null ? !this.lowerVal.equals(other.lowerVal) : other.lowerVal != null) return false;
+    if (this.upperVal != null ? !this.upperVal.equals(other.upperVal) : other.upperVal != null) return false;
+    return true;
+  }
+  
+  @Override
+  public final int hashCode() {
+    int h = field.hashCode();
+    h ^= (lowerVal != null) ? lowerVal.hashCode() : 550356204;
+    h = (h << 1) | (h >>> 31);  // rotate to distinguish lower from upper
+    h ^= (upperVal != null) ? upperVal.hashCode() : -1674416163;
+    h ^= (includeLower ? 1549299360 : -365038026) ^ (includeUpper ? 1721088258 : 1948649653);
+    return h;
+  }
+
+  /** Returns the field name for this filter */
+  public String getField() { return field; }
+
+  /** Returns <code>true</code> if the lower endpoint is inclusive */
+  public boolean includesLower() { return includeLower; }
+  
+  /** Returns <code>true</code> if the upper endpoint is inclusive */
+  public boolean includesUpper() { return includeUpper; }
+
+  /** Returns the lower value of this range filter */
+  public T getLowerVal() { return lowerVal; }
+
+  /** Returns the upper value of this range filter */
+  public T getUpperVal() { return upperVal; }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/DocValuesRewriteMethod.java lucene5666/lucene/core/src/java/org/apache/lucene/search/DocValuesRewriteMethod.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/DocValuesRewriteMethod.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/DocValuesRewriteMethod.java	2014-05-14 03:45:16.618644322 -0400
@@ -0,0 +1,179 @@
+package org.apache.lucene.search;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.LongBitSet;
+
+/**
+ * Rewrites MultiTermQueries into a filter, using DocValues for term enumeration.
+ * <p>
+ * This can be used to perform these queries against an unindexed docvalues field.
+ * @lucene.experimental
+ */
+public final class DocValuesRewriteMethod extends MultiTermQuery.RewriteMethod {
+  
+  @Override
+  public Query rewrite(IndexReader reader, MultiTermQuery query) {
+    Query result = new ConstantScoreQuery(new MultiTermQueryDocValuesWrapperFilter(query));
+    result.setBoost(query.getBoost());
+    return result;
+  }
+  
+  static class MultiTermQueryDocValuesWrapperFilter extends Filter {
+    
+    protected final MultiTermQuery query;
+    
+    /**
+     * Wrap a {@link MultiTermQuery} as a Filter.
+     */
+    protected MultiTermQueryDocValuesWrapperFilter(MultiTermQuery query) {
+      this.query = query;
+    }
+    
+    @Override
+    public String toString() {
+      // query.toString should be ok for the filter, too, if the query boost is 1.0f
+      return query.toString();
+    }
+    
+    @Override
+    public final boolean equals(final Object o) {
+      if (o==this) return true;
+      if (o==null) return false;
+      if (this.getClass().equals(o.getClass())) {
+        return this.query.equals( ((MultiTermQueryDocValuesWrapperFilter)o).query );
+      }
+      return false;
+    }
+    
+    @Override
+    public final int hashCode() {
+      return query.hashCode();
+    }
+    
+    /** Returns the field name for this query */
+    public final String getField() { return query.getField(); }
+    
+    /**
+     * Returns a DocIdSet with documents that should be permitted in search
+     * results.
+     */
+    @Override
+    public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
+      final SortedDocValues fcsi = DocValues.getSorted(context.reader(), query.field);
+      // Cannot use FixedBitSet because we require long index (ord):
+      final LongBitSet termSet = new LongBitSet(fcsi.getValueCount());
+      TermsEnum termsEnum = query.getTermsEnum(new Terms() {
+        
+        @Override
+        public TermsEnum iterator(TermsEnum reuse) {
+          return fcsi.termsEnum();
+        }
+
+        @Override
+        public long getSumTotalTermFreq() {
+          return -1;
+        }
+
+        @Override
+        public long getSumDocFreq() {
+          return -1;
+        }
+
+        @Override
+        public int getDocCount() {
+          return -1;
+        }
+
+        @Override
+        public long size() {
+          return -1;
+        }
+
+        @Override
+        public boolean hasFreqs() {
+          return false;
+        }
+
+        @Override
+        public boolean hasOffsets() {
+          return false;
+        }
+
+        @Override
+        public boolean hasPositions() {
+          return false;
+        }
+        
+        @Override
+        public boolean hasPayloads() {
+          return false;
+        }
+      });
+      
+      assert termsEnum != null;
+      if (termsEnum.next() != null) {
+        // fill into a bitset
+        do {
+          long ord = termsEnum.ord();
+          if (ord >= 0) {
+            termSet.set(ord);
+          }
+        } while (termsEnum.next() != null);
+      } else {
+        return null;
+      }
+      
+      return new DocValuesDocIdSet(context.reader().maxDoc(), acceptDocs) {
+        @Override
+        protected final boolean matchDoc(int doc) throws ArrayIndexOutOfBoundsException {
+          int ord = fcsi.getOrd(doc);
+          if (ord == -1) {
+            return false;
+          }
+          return termSet.get(ord);
+        }
+      };
+    }
+  }
+  
+  @Override
+  public boolean equals(Object obj) {
+    if (this == obj)
+      return true;
+    if (obj == null)
+      return false;
+    if (getClass() != obj.getClass())
+      return false;
+    return true;
+  }
+
+  @Override
+  public int hashCode() {
+    return 641;
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/DocValuesTermsFilter.java lucene5666/lucene/core/src/java/org/apache/lucene/search/DocValuesTermsFilter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/DocValuesTermsFilter.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/DocValuesTermsFilter.java	2014-05-14 03:45:16.618644322 -0400
@@ -0,0 +1,129 @@
+package org.apache.lucene.search;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocsEnum; // javadoc @link
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+
+/**
+ * A {@link Filter} that only accepts documents whose single
+ * term value in the specified field is contained in the
+ * provided set of allowed terms.
+ * 
+ * <p/>
+ * 
+ * This is the same functionality as TermsFilter (from
+ * queries/), except this filter requires that the
+ * field contains only a single term for all documents.
+ * Because of drastically different implementations, they
+ * also have different performance characteristics, as
+ * described below.
+ * 
+ * 
+ * <p/>
+ * 
+ * With each search, this filter translates the specified
+ * set of Terms into a private {@link FixedBitSet} keyed by
+ * term number per unique {@link IndexReader} (normally one
+ * reader per segment).  Then, during matching, the term
+ * number for each docID is retrieved from the cache and
+ * then checked for inclusion using the {@link FixedBitSet}.
+ * Since all testing is done using RAM resident data
+ * structures, performance should be very fast, most likely
+ * fast enough to not require further caching of the
+ * DocIdSet for each possible combination of terms.
+ * However, because docIDs are simply scanned linearly, an
+ * index with a great many small documents may find this
+ * linear scan too costly.
+ * 
+ * <p/>
+ * 
+ * In contrast, TermsFilter builds up an {@link FixedBitSet},
+ * keyed by docID, every time it's created, by enumerating
+ * through all matching docs using {@link DocsEnum} to seek
+ * and scan through each term's docID list.  While there is
+ * no linear scan of all docIDs, besides the allocation of
+ * the underlying array in the {@link FixedBitSet}, this
+ * approach requires a number of "disk seeks" in proportion
+ * to the number of terms, which can be exceptionally costly
+ * when there are cache misses in the OS's IO cache.
+ * 
+ * <p/>
+ * 
+ * Generally, this filter will be slower on the first
+ * invocation for a given field, but subsequent invocations,
+ * even if you change the allowed set of Terms, should be
+ * faster than TermsFilter, especially as the number of
+ * Terms being matched increases.  If you are matching only
+ * a very small number of terms, and those terms in turn
+ * match a very small number of documents, TermsFilter may
+ * perform faster.
+ *
+ * <p/>
+ *
+ * Which filter is best is very application dependent.
+ */
+
+public class DocValuesTermsFilter extends Filter {
+  private String field;
+  private BytesRef[] terms;
+
+  public DocValuesTermsFilter(String field, BytesRef... terms) {
+    this.field = field;
+    this.terms = terms;
+  }
+
+  public DocValuesTermsFilter(String field, String... terms) {
+    this.field = field;
+    this.terms = new BytesRef[terms.length];
+    for (int i = 0; i < terms.length; i++)
+      this.terms[i] = new BytesRef(terms[i]);
+  }
+
+  @Override
+  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    final SortedDocValues fcsi = DocValues.getSorted(context.reader(), field);
+    final FixedBitSet bits = new FixedBitSet(fcsi.getValueCount());
+    for (int i=0;i<terms.length;i++) {
+      int ord = fcsi.lookupTerm(terms[i]);
+      if (ord >= 0) {
+        bits.set(ord);
+      }
+    }
+    return new DocValuesDocIdSet(context.reader().maxDoc(), acceptDocs) {
+      @Override
+      protected final boolean matchDoc(int doc) {
+        int ord = fcsi.getOrd(doc);
+        if (ord == -1) {
+          // missing
+          return false;
+        } else {
+          return bits.get(ord);
+        }
+      }
+    };
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/FieldCacheDocIdSet.java lucene5666/lucene/core/src/java/org/apache/lucene/search/FieldCacheDocIdSet.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/FieldCacheDocIdSet.java	2014-05-14 03:47:28.770646624 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/FieldCacheDocIdSet.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,167 +0,0 @@
-package org.apache.lucene.search;
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.FixedBitSet;
-import org.apache.lucene.util.OpenBitSet;
-
-/**
- * Base class for DocIdSet to be used with FieldCache. The implementation
- * of its iterator is very stupid and slow if the implementation of the
- * {@link #matchDoc} method is not optimized, as iterators simply increment
- * the document id until {@code matchDoc(int)} returns true. Because of this
- * {@code matchDoc(int)} must be as fast as possible and in no case do any
- * I/O.
- * @lucene.internal
- */
-public abstract class FieldCacheDocIdSet extends DocIdSet {
-
-  protected final int maxDoc;
-  protected final Bits acceptDocs;
-
-  public FieldCacheDocIdSet(int maxDoc, Bits acceptDocs) {
-    this.maxDoc = maxDoc;
-    this.acceptDocs = acceptDocs;
-  }
-
-  /**
-   * this method checks, if a doc is a hit
-   */
-  protected abstract boolean matchDoc(int doc);
-
-  /**
-   * this DocIdSet is always cacheable (does not go back
-   * to the reader for iteration)
-   */
-  @Override
-  public final boolean isCacheable() {
-    return true;
-  }
-
-  @Override
-  public final Bits bits() {
-    return (acceptDocs == null) ? new Bits() {
-      @Override
-      public boolean get(int docid) {
-        return matchDoc(docid);
-      }
-
-      @Override
-      public int length() {
-        return maxDoc;
-      }
-    } : new Bits() {
-      @Override
-      public boolean get(int docid) {
-        return matchDoc(docid) && acceptDocs.get(docid);
-      }
-
-      @Override
-      public int length() {
-        return maxDoc;
-      }
-    };
-  }
-
-  @Override
-  public final DocIdSetIterator iterator() throws IOException {
-    if (acceptDocs == null) {
-      // Specialization optimization disregard acceptDocs
-      return new DocIdSetIterator() {
-        private int doc = -1;
-        
-        @Override
-        public int docID() {
-          return doc;
-        }
-      
-        @Override
-        public int nextDoc() {
-          do {
-            doc++;
-            if (doc >= maxDoc) {
-              return doc = NO_MORE_DOCS;
-            }
-          } while (!matchDoc(doc));
-          return doc;
-        }
-      
-        @Override
-        public int advance(int target) {
-          for(doc=target; doc<maxDoc; doc++) {
-            if (matchDoc(doc)) {
-              return doc;
-            }
-          }
-          return doc = NO_MORE_DOCS;
-        }
-
-        @Override
-        public long cost() {
-          return maxDoc;
-        }
-      };
-    } else if (acceptDocs instanceof FixedBitSet || acceptDocs instanceof OpenBitSet) {
-      // special case for FixedBitSet / OpenBitSet: use the iterator and filter it
-      // (used e.g. when Filters are chained by FilteredQuery)
-      return new FilteredDocIdSetIterator(((DocIdSet) acceptDocs).iterator()) {
-        @Override
-        protected boolean match(int doc) {
-          return FieldCacheDocIdSet.this.matchDoc(doc);
-        }
-      };
-    } else {
-      // Stupid consultation of acceptDocs and matchDoc()
-      return new DocIdSetIterator() {
-        private int doc = -1;
-        
-        @Override
-        public int docID() {
-          return doc;
-        }
-      
-        @Override
-        public int nextDoc() {
-          do {
-            doc++;
-            if (doc >= maxDoc) {
-              return doc = NO_MORE_DOCS;
-            }
-          } while (!(matchDoc(doc) && acceptDocs.get(doc)));
-          return doc;
-        }
-      
-        @Override
-        public int advance(int target) {
-          for(doc=target; doc<maxDoc; doc++) {
-            if (matchDoc(doc) && acceptDocs.get(doc)) {
-              return doc;
-            }
-          }
-          return doc = NO_MORE_DOCS;
-        }
-
-        @Override
-        public long cost() {
-          return maxDoc;
-        }
-      };
-    }
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.java lucene5666/lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.java	2014-05-14 03:47:28.770646624 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,1239 +0,0 @@
-package org.apache.lucene.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.PrintStream;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.WeakHashMap;
-
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.DocTermOrds;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.SegmentReader;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FieldCacheSanityChecker;
-import org.apache.lucene.util.FixedBitSet;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.packed.GrowableWriter;
-import org.apache.lucene.util.packed.MonotonicAppendingLongBuffer;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Expert: The default cache implementation, storing all values in memory.
- * A WeakHashMap is used for storage.
- *
- * @since   lucene 1.4
- */
-class FieldCacheImpl implements FieldCache {
-
-  private Map<Class<?>,Cache> caches;
-  FieldCacheImpl() {
-    init();
-  }
-
-  private synchronized void init() {
-    caches = new HashMap<>(9);
-    caches.put(Integer.TYPE, new IntCache(this));
-    caches.put(Float.TYPE, new FloatCache(this));
-    caches.put(Long.TYPE, new LongCache(this));
-    caches.put(Double.TYPE, new DoubleCache(this));
-    caches.put(BinaryDocValues.class, new BinaryDocValuesCache(this));
-    caches.put(SortedDocValues.class, new SortedDocValuesCache(this));
-    caches.put(DocTermOrds.class, new DocTermOrdsCache(this));
-    caches.put(DocsWithFieldCache.class, new DocsWithFieldCache(this));
-  }
-
-  @Override
-  public synchronized void purgeAllCaches() {
-    init();
-  }
-
-  @Override
-  public synchronized void purgeByCacheKey(Object coreCacheKey) {
-    for(Cache c : caches.values()) {
-      c.purgeByCacheKey(coreCacheKey);
-    }
-  }
-
-  @Override
-  public synchronized CacheEntry[] getCacheEntries() {
-    List<CacheEntry> result = new ArrayList<>(17);
-    for(final Map.Entry<Class<?>,Cache> cacheEntry: caches.entrySet()) {
-      final Cache cache = cacheEntry.getValue();
-      final Class<?> cacheType = cacheEntry.getKey();
-      synchronized(cache.readerCache) {
-        for (final Map.Entry<Object,Map<CacheKey, Object>> readerCacheEntry : cache.readerCache.entrySet()) {
-          final Object readerKey = readerCacheEntry.getKey();
-          if (readerKey == null) continue;
-          final Map<CacheKey, Object> innerCache = readerCacheEntry.getValue();
-          for (final Map.Entry<CacheKey, Object> mapEntry : innerCache.entrySet()) {
-            CacheKey entry = mapEntry.getKey();
-            result.add(new CacheEntry(readerKey, entry.field,
-                                      cacheType, entry.custom,
-                                      mapEntry.getValue()));
-          }
-        }
-      }
-    }
-    return result.toArray(new CacheEntry[result.size()]);
-  }
-
-  // per-segment fieldcaches don't purge until the shared core closes.
-  final SegmentReader.CoreClosedListener purgeCore = new SegmentReader.CoreClosedListener() {
-    @Override
-    public void onClose(Object ownerCoreCacheKey) {
-      FieldCacheImpl.this.purgeByCacheKey(ownerCoreCacheKey);
-    }
-  };
-
-  // composite/SlowMultiReaderWrapper fieldcaches don't purge until composite reader is closed.
-  final IndexReader.ReaderClosedListener purgeReader = new IndexReader.ReaderClosedListener() {
-    @Override
-    public void onClose(IndexReader owner) {
-      assert owner instanceof AtomicReader;
-      FieldCacheImpl.this.purgeByCacheKey(((AtomicReader) owner).getCoreCacheKey());
-    }
-  };
-  
-  private void initReader(AtomicReader reader) {
-    if (reader instanceof SegmentReader) {
-      ((SegmentReader) reader).addCoreClosedListener(purgeCore);
-    } else {
-      // we have a slow reader of some sort, try to register a purge event
-      // rather than relying on gc:
-      Object key = reader.getCoreCacheKey();
-      if (key instanceof AtomicReader) {
-        ((AtomicReader)key).addReaderClosedListener(purgeReader); 
-      } else {
-        // last chance
-        reader.addReaderClosedListener(purgeReader);
-      }
-    }
-  }
-
-  /** Expert: Internal cache. */
-  abstract static class Cache {
-
-    Cache(FieldCacheImpl wrapper) {
-      this.wrapper = wrapper;
-    }
-
-    final FieldCacheImpl wrapper;
-
-    final Map<Object,Map<CacheKey,Object>> readerCache = new WeakHashMap<>();
-    
-    protected abstract Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField)
-        throws IOException;
-
-    /** Remove this reader from the cache, if present. */
-    public void purgeByCacheKey(Object coreCacheKey) {
-      synchronized(readerCache) {
-        readerCache.remove(coreCacheKey);
-      }
-    }
-
-    /** Sets the key to the value for the provided reader;
-     *  if the key is already set then this doesn't change it. */
-    public void put(AtomicReader reader, CacheKey key, Object value) {
-      final Object readerKey = reader.getCoreCacheKey();
-      synchronized (readerCache) {
-        Map<CacheKey,Object> innerCache = readerCache.get(readerKey);
-        if (innerCache == null) {
-          // First time this reader is using FieldCache
-          innerCache = new HashMap<>();
-          readerCache.put(readerKey, innerCache);
-          wrapper.initReader(reader);
-        }
-        if (innerCache.get(key) == null) {
-          innerCache.put(key, value);
-        } else {
-          // Another thread beat us to it; leave the current
-          // value
-        }
-      }
-    }
-
-    public Object get(AtomicReader reader, CacheKey key, boolean setDocsWithField) throws IOException {
-      Map<CacheKey,Object> innerCache;
-      Object value;
-      final Object readerKey = reader.getCoreCacheKey();
-      synchronized (readerCache) {
-        innerCache = readerCache.get(readerKey);
-        if (innerCache == null) {
-          // First time this reader is using FieldCache
-          innerCache = new HashMap<>();
-          readerCache.put(readerKey, innerCache);
-          wrapper.initReader(reader);
-          value = null;
-        } else {
-          value = innerCache.get(key);
-        }
-        if (value == null) {
-          value = new CreationPlaceholder();
-          innerCache.put(key, value);
-        }
-      }
-      if (value instanceof CreationPlaceholder) {
-        synchronized (value) {
-          CreationPlaceholder progress = (CreationPlaceholder) value;
-          if (progress.value == null) {
-            progress.value = createValue(reader, key, setDocsWithField);
-            synchronized (readerCache) {
-              innerCache.put(key, progress.value);
-            }
-
-            // Only check if key.custom (the parser) is
-            // non-null; else, we check twice for a single
-            // call to FieldCache.getXXX
-            if (key.custom != null && wrapper != null) {
-              final PrintStream infoStream = wrapper.getInfoStream();
-              if (infoStream != null) {
-                printNewInsanity(infoStream, progress.value);
-              }
-            }
-          }
-          return progress.value;
-        }
-      }
-      return value;
-    }
-
-    private void printNewInsanity(PrintStream infoStream, Object value) {
-      final FieldCacheSanityChecker.Insanity[] insanities = FieldCacheSanityChecker.checkSanity(wrapper);
-      for(int i=0;i<insanities.length;i++) {
-        final FieldCacheSanityChecker.Insanity insanity = insanities[i];
-        final CacheEntry[] entries = insanity.getCacheEntries();
-        for(int j=0;j<entries.length;j++) {
-          if (entries[j].getValue() == value) {
-            // OK this insanity involves our entry
-            infoStream.println("WARNING: new FieldCache insanity created\nDetails: " + insanity.toString());
-            infoStream.println("\nStack:\n");
-            new Throwable().printStackTrace(infoStream);
-            break;
-          }
-        }
-      }
-    }
-  }
-
-  /** Expert: Every composite-key in the internal cache is of this type. */
-  static class CacheKey {
-    final String field;        // which Field
-    final Object custom;       // which custom comparator or parser
-
-    /** Creates one of these objects for a custom comparator/parser. */
-    CacheKey(String field, Object custom) {
-      this.field = field;
-      this.custom = custom;
-    }
-
-    /** Two of these are equal iff they reference the same field and type. */
-    @Override
-    public boolean equals (Object o) {
-      if (o instanceof CacheKey) {
-        CacheKey other = (CacheKey) o;
-        if (other.field.equals(field)) {
-          if (other.custom == null) {
-            if (custom == null) return true;
-          } else if (other.custom.equals (custom)) {
-            return true;
-          }
-        }
-      }
-      return false;
-    }
-
-    /** Composes a hashcode based on the field and type. */
-    @Override
-    public int hashCode() {
-      return field.hashCode() ^ (custom==null ? 0 : custom.hashCode());
-    }
-  }
-
-  private static abstract class Uninvert {
-
-    public Bits docsWithField;
-
-    public void uninvert(AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
-      final int maxDoc = reader.maxDoc();
-      Terms terms = reader.terms(field);
-      if (terms != null) {
-        if (setDocsWithField) {
-          final int termsDocCount = terms.getDocCount();
-          assert termsDocCount <= maxDoc;
-          if (termsDocCount == maxDoc) {
-            // Fast case: all docs have this field:
-            docsWithField = new Bits.MatchAllBits(maxDoc);
-            setDocsWithField = false;
-          }
-        }
-
-        final TermsEnum termsEnum = termsEnum(terms);
-
-        DocsEnum docs = null;
-        FixedBitSet docsWithField = null;
-        while(true) {
-          final BytesRef term = termsEnum.next();
-          if (term == null) {
-            break;
-          }
-          visitTerm(term);
-          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);
-          while (true) {
-            final int docID = docs.nextDoc();
-            if (docID == DocIdSetIterator.NO_MORE_DOCS) {
-              break;
-            }
-            visitDoc(docID);
-            if (setDocsWithField) {
-              if (docsWithField == null) {
-                // Lazy init
-                this.docsWithField = docsWithField = new FixedBitSet(maxDoc);
-              }
-              docsWithField.set(docID);
-            }
-          }
-        }
-      }
-    }
-
-    protected abstract TermsEnum termsEnum(Terms terms) throws IOException;
-    protected abstract void visitTerm(BytesRef term);
-    protected abstract void visitDoc(int docID);
-  }
-
-  // null Bits means no docs matched
-  void setDocsWithField(AtomicReader reader, String field, Bits docsWithField) {
-    final int maxDoc = reader.maxDoc();
-    final Bits bits;
-    if (docsWithField == null) {
-      bits = new Bits.MatchNoBits(maxDoc);
-    } else if (docsWithField instanceof FixedBitSet) {
-      final int numSet = ((FixedBitSet) docsWithField).cardinality();
-      if (numSet >= maxDoc) {
-        // The cardinality of the BitSet is maxDoc if all documents have a value.
-        assert numSet == maxDoc;
-        bits = new Bits.MatchAllBits(maxDoc);
-      } else {
-        bits = docsWithField;
-      }
-    } else {
-      bits = docsWithField;
-    }
-    caches.get(DocsWithFieldCache.class).put(reader, new CacheKey(field, null), bits);
-  }
-
-  @Override
-  public Ints getInts (AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
-    return getInts(reader, field, null, setDocsWithField);
-  }
-
-  @Override
-  public Ints getInts(AtomicReader reader, String field, IntParser parser, boolean setDocsWithField)
-      throws IOException {
-    final NumericDocValues valuesIn = reader.getNumericDocValues(field);
-    if (valuesIn != null) {
-      // Not cached here by FieldCacheImpl (cached instead
-      // per-thread by SegmentReader):
-      return new Ints() {
-        @Override
-        public int get(int docID) {
-          return (int) valuesIn.get(docID);
-        }
-      };
-    } else {
-      final FieldInfo info = reader.getFieldInfos().fieldInfo(field);
-      if (info == null) {
-        return Ints.EMPTY;
-      } else if (info.hasDocValues()) {
-        throw new IllegalStateException("Type mismatch: " + field + " was indexed as " + info.getDocValuesType());
-      } else if (!info.isIndexed()) {
-        return Ints.EMPTY;
-      }
-      return (Ints) caches.get(Integer.TYPE).get(reader, new CacheKey(field, parser), setDocsWithField);
-    }
-  }
-
-  static class IntsFromArray extends Ints {
-    private final PackedInts.Reader values;
-    private final int minValue;
-
-    public IntsFromArray(PackedInts.Reader values, int minValue) {
-      assert values.getBitsPerValue() <= 32;
-      this.values = values;
-      this.minValue = minValue;
-    }
-    
-    @Override
-    public int get(int docID) {
-      final long delta = values.get(docID);
-      return minValue + (int) delta;
-    }
-  }
-
-  private static class HoldsOneThing<T> {
-    private T it;
-
-    public void set(T it) {
-      this.it = it;
-    }
-
-    public T get() {
-      return it;
-    }
-  }
-
-  private static class GrowableWriterAndMinValue {
-    GrowableWriterAndMinValue(GrowableWriter array, long minValue) {
-      this.writer = array;
-      this.minValue = minValue;
-    }
-    public GrowableWriter writer;
-    public long minValue;
-  }
-
-  static final class IntCache extends Cache {
-    IntCache(FieldCacheImpl wrapper) {
-      super(wrapper);
-    }
-
-    @Override
-    protected Object createValue(final AtomicReader reader, CacheKey key, boolean setDocsWithField)
-        throws IOException {
-
-      final IntParser parser = (IntParser) key.custom;
-      if (parser == null) {
-        // Confusing: must delegate to wrapper (vs simply
-        // setting parser = NUMERIC_UTILS_INT_PARSER) so
-        // cache key includes NUMERIC_UTILS_INT_PARSER:
-        return wrapper.getInts(reader, key.field, NUMERIC_UTILS_INT_PARSER, setDocsWithField);
-      }
-
-      final HoldsOneThing<GrowableWriterAndMinValue> valuesRef = new HoldsOneThing<>();
-
-      Uninvert u = new Uninvert() {
-          private int minValue;
-          private int currentValue;
-          private GrowableWriter values;
-
-          @Override
-          public void visitTerm(BytesRef term) {
-            currentValue = parser.parseInt(term);
-            if (values == null) {
-              // Lazy alloc so for the numeric field case
-              // (which will hit a NumberFormatException
-              // when we first try the DEFAULT_INT_PARSER),
-              // we don't double-alloc:
-              int startBitsPerValue;
-              // Make sure than missing values (0) can be stored without resizing
-              if (currentValue < 0) {
-                minValue = currentValue;
-                startBitsPerValue = PackedInts.bitsRequired((-minValue) & 0xFFFFFFFFL);
-              } else {
-                minValue = 0;
-                startBitsPerValue = PackedInts.bitsRequired(currentValue);
-              }
-              values = new GrowableWriter(startBitsPerValue, reader.maxDoc(), PackedInts.FAST);
-              if (minValue != 0) {
-                values.fill(0, values.size(), (-minValue) & 0xFFFFFFFFL); // default value must be 0
-              }
-              valuesRef.set(new GrowableWriterAndMinValue(values, minValue));
-            }
-          }
-
-          @Override
-          public void visitDoc(int docID) {
-            values.set(docID, (currentValue - minValue) & 0xFFFFFFFFL);
-          }
-
-          @Override
-          protected TermsEnum termsEnum(Terms terms) throws IOException {
-            return parser.termsEnum(terms);
-          }
-        };
-
-      u.uninvert(reader, key.field, setDocsWithField);
-
-      if (setDocsWithField) {
-        wrapper.setDocsWithField(reader, key.field, u.docsWithField);
-      }
-      GrowableWriterAndMinValue values = valuesRef.get();
-      if (values == null) {
-        return new IntsFromArray(new PackedInts.NullReader(reader.maxDoc()), 0);
-      }
-      return new IntsFromArray(values.writer.getMutable(), (int) values.minValue);
-    }
-  }
-
-  public Bits getDocsWithField(AtomicReader reader, String field) throws IOException {
-    final FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);
-    if (fieldInfo == null) {
-      // field does not exist or has no value
-      return new Bits.MatchNoBits(reader.maxDoc());
-    } else if (fieldInfo.hasDocValues()) {
-      return reader.getDocsWithField(field);
-    } else if (!fieldInfo.isIndexed()) {
-      return new Bits.MatchNoBits(reader.maxDoc());
-    }
-    return (Bits) caches.get(DocsWithFieldCache.class).get(reader, new CacheKey(field, null), false);
-  }
-
-  static final class DocsWithFieldCache extends Cache {
-    DocsWithFieldCache(FieldCacheImpl wrapper) {
-      super(wrapper);
-    }
-    
-    @Override
-    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)
-    throws IOException {
-      final String field = key.field;
-      final int maxDoc = reader.maxDoc();
-
-      // Visit all docs that have terms for this field
-      FixedBitSet res = null;
-      Terms terms = reader.terms(field);
-      if (terms != null) {
-        final int termsDocCount = terms.getDocCount();
-        assert termsDocCount <= maxDoc;
-        if (termsDocCount == maxDoc) {
-          // Fast case: all docs have this field:
-          return new Bits.MatchAllBits(maxDoc);
-        }
-        final TermsEnum termsEnum = terms.iterator(null);
-        DocsEnum docs = null;
-        while(true) {
-          final BytesRef term = termsEnum.next();
-          if (term == null) {
-            break;
-          }
-          if (res == null) {
-            // lazy init
-            res = new FixedBitSet(maxDoc);
-          }
-
-          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);
-          // TODO: use bulk API
-          while (true) {
-            final int docID = docs.nextDoc();
-            if (docID == DocIdSetIterator.NO_MORE_DOCS) {
-              break;
-            }
-            res.set(docID);
-          }
-        }
-      }
-      if (res == null) {
-        return new Bits.MatchNoBits(maxDoc);
-      }
-      final int numSet = res.cardinality();
-      if (numSet >= maxDoc) {
-        // The cardinality of the BitSet is maxDoc if all documents have a value.
-        assert numSet == maxDoc;
-        return new Bits.MatchAllBits(maxDoc);
-      }
-      return res;
-    }
-  }
-
-  @Override
-  public Floats getFloats (AtomicReader reader, String field, boolean setDocsWithField)
-    throws IOException {
-    return getFloats(reader, field, null, setDocsWithField);
-  }
-
-  @Override
-  public Floats getFloats(AtomicReader reader, String field, FloatParser parser, boolean setDocsWithField)
-    throws IOException {
-    final NumericDocValues valuesIn = reader.getNumericDocValues(field);
-    if (valuesIn != null) {
-      // Not cached here by FieldCacheImpl (cached instead
-      // per-thread by SegmentReader):
-      return new Floats() {
-        @Override
-        public float get(int docID) {
-          return Float.intBitsToFloat((int) valuesIn.get(docID));
-        }
-      };
-    } else {
-      final FieldInfo info = reader.getFieldInfos().fieldInfo(field);
-      if (info == null) {
-        return Floats.EMPTY;
-      } else if (info.hasDocValues()) {
-        throw new IllegalStateException("Type mismatch: " + field + " was indexed as " + info.getDocValuesType());
-      } else if (!info.isIndexed()) {
-        return Floats.EMPTY;
-      }
-      return (Floats) caches.get(Float.TYPE).get(reader, new CacheKey(field, parser), setDocsWithField);
-    }
-  }
-
-  static class FloatsFromArray extends Floats {
-    private final float[] values;
-
-    public FloatsFromArray(float[] values) {
-      this.values = values;
-    }
-    
-    @Override
-    public float get(int docID) {
-      return values[docID];
-    }
-  }
-
-  static final class FloatCache extends Cache {
-    FloatCache(FieldCacheImpl wrapper) {
-      super(wrapper);
-    }
-
-    @Override
-    protected Object createValue(final AtomicReader reader, CacheKey key, boolean setDocsWithField)
-        throws IOException {
-
-      final FloatParser parser = (FloatParser) key.custom;
-      if (parser == null) {
-        // Confusing: must delegate to wrapper (vs simply
-        // setting parser = NUMERIC_UTILS_FLOAT_PARSER) so
-        // cache key includes NUMERIC_UTILS_FLOAT_PARSER:
-        return wrapper.getFloats(reader, key.field, NUMERIC_UTILS_FLOAT_PARSER, setDocsWithField);
-      }
-
-      final HoldsOneThing<float[]> valuesRef = new HoldsOneThing<>();
-
-      Uninvert u = new Uninvert() {
-          private float currentValue;
-          private float[] values;
-
-          @Override
-          public void visitTerm(BytesRef term) {
-            currentValue = parser.parseFloat(term);
-            if (values == null) {
-              // Lazy alloc so for the numeric field case
-              // (which will hit a NumberFormatException
-              // when we first try the DEFAULT_INT_PARSER),
-              // we don't double-alloc:
-              values = new float[reader.maxDoc()];
-              valuesRef.set(values);
-            }
-          }
-
-          @Override
-          public void visitDoc(int docID) {
-            values[docID] = currentValue;
-          }
-          
-          @Override
-          protected TermsEnum termsEnum(Terms terms) throws IOException {
-            return parser.termsEnum(terms);
-          }
-        };
-
-      u.uninvert(reader, key.field, setDocsWithField);
-
-      if (setDocsWithField) {
-        wrapper.setDocsWithField(reader, key.field, u.docsWithField);
-      }
-
-      float[] values = valuesRef.get();
-      if (values == null) {
-        values = new float[reader.maxDoc()];
-      }
-      return new FloatsFromArray(values);
-    }
-  }
-
-  @Override
-  public Longs getLongs(AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
-    return getLongs(reader, field, null, setDocsWithField);
-  }
-  
-  @Override
-  public Longs getLongs(AtomicReader reader, String field, FieldCache.LongParser parser, boolean setDocsWithField)
-      throws IOException {
-    final NumericDocValues valuesIn = reader.getNumericDocValues(field);
-    if (valuesIn != null) {
-      // Not cached here by FieldCacheImpl (cached instead
-      // per-thread by SegmentReader):
-      return new Longs() {
-        @Override
-        public long get(int docID) {
-          return valuesIn.get(docID);
-        }
-      };
-    } else {
-      final FieldInfo info = reader.getFieldInfos().fieldInfo(field);
-      if (info == null) {
-        return Longs.EMPTY;
-      } else if (info.hasDocValues()) {
-        throw new IllegalStateException("Type mismatch: " + field + " was indexed as " + info.getDocValuesType());
-      } else if (!info.isIndexed()) {
-        return Longs.EMPTY;
-      }
-      return (Longs) caches.get(Long.TYPE).get(reader, new CacheKey(field, parser), setDocsWithField);
-    }
-  }
-
-  static class LongsFromArray extends Longs {
-    private final PackedInts.Reader values;
-    private final long minValue;
-
-    public LongsFromArray(PackedInts.Reader values, long minValue) {
-      this.values = values;
-      this.minValue = minValue;
-    }
-    
-    @Override
-    public long get(int docID) {
-      return minValue + values.get(docID);
-    }
-  }
-
-  static final class LongCache extends Cache {
-    LongCache(FieldCacheImpl wrapper) {
-      super(wrapper);
-    }
-
-    @Override
-    protected Object createValue(final AtomicReader reader, CacheKey key, boolean setDocsWithField)
-        throws IOException {
-
-      final LongParser parser = (LongParser) key.custom;
-      if (parser == null) {
-        // Confusing: must delegate to wrapper (vs simply
-        // setting parser = NUMERIC_UTILS_LONG_PARSER) so
-        // cache key includes NUMERIC_UTILS_LONG_PARSER:
-        return wrapper.getLongs(reader, key.field, NUMERIC_UTILS_LONG_PARSER, setDocsWithField);
-      }
-
-      final HoldsOneThing<GrowableWriterAndMinValue> valuesRef = new HoldsOneThing<>();
-
-      Uninvert u = new Uninvert() {
-          private long minValue;
-          private long currentValue;
-          private GrowableWriter values;
-
-          @Override
-          public void visitTerm(BytesRef term) {
-            currentValue = parser.parseLong(term);
-            if (values == null) {
-              // Lazy alloc so for the numeric field case
-              // (which will hit a NumberFormatException
-              // when we first try the DEFAULT_INT_PARSER),
-              // we don't double-alloc:
-              int startBitsPerValue;
-              // Make sure than missing values (0) can be stored without resizing
-              if (currentValue < 0) {
-                minValue = currentValue;
-                startBitsPerValue = minValue == Long.MIN_VALUE ? 64 : PackedInts.bitsRequired(-minValue);
-              } else {
-                minValue = 0;
-                startBitsPerValue = PackedInts.bitsRequired(currentValue);
-              }
-              values = new GrowableWriter(startBitsPerValue, reader.maxDoc(), PackedInts.FAST);
-              if (minValue != 0) {
-                values.fill(0, values.size(), -minValue); // default value must be 0
-              }
-              valuesRef.set(new GrowableWriterAndMinValue(values, minValue));
-            }
-          }
-
-          @Override
-          public void visitDoc(int docID) {
-            values.set(docID, currentValue - minValue);
-          }
-          
-          @Override
-          protected TermsEnum termsEnum(Terms terms) throws IOException {
-            return parser.termsEnum(terms);
-          }
-        };
-
-      u.uninvert(reader, key.field, setDocsWithField);
-
-      if (setDocsWithField) {
-        wrapper.setDocsWithField(reader, key.field, u.docsWithField);
-      }
-      GrowableWriterAndMinValue values = valuesRef.get();
-      if (values == null) {
-        return new LongsFromArray(new PackedInts.NullReader(reader.maxDoc()), 0L);
-      }
-      return new LongsFromArray(values.writer.getMutable(), values.minValue);
-    }
-  }
-
-  @Override
-  public Doubles getDoubles(AtomicReader reader, String field, boolean setDocsWithField)
-    throws IOException {
-    return getDoubles(reader, field, null, setDocsWithField);
-  }
-
-  @Override
-  public Doubles getDoubles(AtomicReader reader, String field, FieldCache.DoubleParser parser, boolean setDocsWithField)
-      throws IOException {
-    final NumericDocValues valuesIn = reader.getNumericDocValues(field);
-    if (valuesIn != null) {
-      // Not cached here by FieldCacheImpl (cached instead
-      // per-thread by SegmentReader):
-      return new Doubles() {
-        @Override
-        public double get(int docID) {
-          return Double.longBitsToDouble(valuesIn.get(docID));
-        }
-      };
-    } else {
-      final FieldInfo info = reader.getFieldInfos().fieldInfo(field);
-      if (info == null) {
-        return Doubles.EMPTY;
-      } else if (info.hasDocValues()) {
-        throw new IllegalStateException("Type mismatch: " + field + " was indexed as " + info.getDocValuesType());
-      } else if (!info.isIndexed()) {
-        return Doubles.EMPTY;
-      }
-      return (Doubles) caches.get(Double.TYPE).get(reader, new CacheKey(field, parser), setDocsWithField);
-    }
-  }
-
-  static class DoublesFromArray extends Doubles {
-    private final double[] values;
-
-    public DoublesFromArray(double[] values) {
-      this.values = values;
-    }
-    
-    @Override
-    public double get(int docID) {
-      return values[docID];
-    }
-  }
-
-  static final class DoubleCache extends Cache {
-    DoubleCache(FieldCacheImpl wrapper) {
-      super(wrapper);
-    }
-
-    @Override
-    protected Object createValue(final AtomicReader reader, CacheKey key, boolean setDocsWithField)
-        throws IOException {
-
-      final DoubleParser parser = (DoubleParser) key.custom;
-      if (parser == null) {
-        // Confusing: must delegate to wrapper (vs simply
-        // setting parser = NUMERIC_UTILS_DOUBLE_PARSER) so
-        // cache key includes NUMERIC_UTILS_DOUBLE_PARSER:
-        return wrapper.getDoubles(reader, key.field, NUMERIC_UTILS_DOUBLE_PARSER, setDocsWithField);
-      }
-
-      final HoldsOneThing<double[]> valuesRef = new HoldsOneThing<>();
-
-      Uninvert u = new Uninvert() {
-          private double currentValue;
-          private double[] values;
-
-          @Override
-          public void visitTerm(BytesRef term) {
-            currentValue = parser.parseDouble(term);
-            if (values == null) {
-              // Lazy alloc so for the numeric field case
-              // (which will hit a NumberFormatException
-              // when we first try the DEFAULT_INT_PARSER),
-              // we don't double-alloc:
-              values = new double[reader.maxDoc()];
-              valuesRef.set(values);
-            }
-          }
-
-          @Override
-          public void visitDoc(int docID) {
-            values[docID] = currentValue;
-          }
-          
-          @Override
-          protected TermsEnum termsEnum(Terms terms) throws IOException {
-            return parser.termsEnum(terms);
-          }
-        };
-
-      u.uninvert(reader, key.field, setDocsWithField);
-
-      if (setDocsWithField) {
-        wrapper.setDocsWithField(reader, key.field, u.docsWithField);
-      }
-      double[] values = valuesRef.get();
-      if (values == null) {
-        values = new double[reader.maxDoc()];
-      }
-      return new DoublesFromArray(values);
-    }
-  }
-
-  public static class SortedDocValuesImpl extends SortedDocValues {
-    private final PagedBytes.Reader bytes;
-    private final MonotonicAppendingLongBuffer termOrdToBytesOffset;
-    private final PackedInts.Reader docToTermOrd;
-    private final int numOrd;
-
-    public SortedDocValuesImpl(PagedBytes.Reader bytes, MonotonicAppendingLongBuffer termOrdToBytesOffset, PackedInts.Reader docToTermOrd, int numOrd) {
-      this.bytes = bytes;
-      this.docToTermOrd = docToTermOrd;
-      this.termOrdToBytesOffset = termOrdToBytesOffset;
-      this.numOrd = numOrd;
-    }
-
-    @Override
-    public int getValueCount() {
-      return numOrd;
-    }
-
-    @Override
-    public int getOrd(int docID) {
-      // Subtract 1, matching the 1+ord we did when
-      // storing, so that missing values, which are 0 in the
-      // packed ints, are returned as -1 ord:
-      return (int) docToTermOrd.get(docID)-1;
-    }
-
-    @Override
-    public void lookupOrd(int ord, BytesRef ret) {
-      if (ord < 0) {
-        throw new IllegalArgumentException("ord must be >=0 (got ord=" + ord + ")");
-      }
-      bytes.fill(ret, termOrdToBytesOffset.get(ord));
-    }
-  }
-
-  public SortedDocValues getTermsIndex(AtomicReader reader, String field) throws IOException {
-    return getTermsIndex(reader, field, PackedInts.FAST);
-  }
-
-  public SortedDocValues getTermsIndex(AtomicReader reader, String field, float acceptableOverheadRatio) throws IOException {
-    SortedDocValues valuesIn = reader.getSortedDocValues(field);
-    if (valuesIn != null) {
-      // Not cached here by FieldCacheImpl (cached instead
-      // per-thread by SegmentReader):
-      return valuesIn;
-    } else {
-      final FieldInfo info = reader.getFieldInfos().fieldInfo(field);
-      if (info == null) {
-        return DocValues.EMPTY_SORTED;
-      } else if (info.hasDocValues()) {
-        // we don't try to build a sorted instance from numeric/binary doc
-        // values because dedup can be very costly
-        throw new IllegalStateException("Type mismatch: " + field + " was indexed as " + info.getDocValuesType());
-      } else if (!info.isIndexed()) {
-        return DocValues.EMPTY_SORTED;
-      }
-      return (SortedDocValues) caches.get(SortedDocValues.class).get(reader, new CacheKey(field, acceptableOverheadRatio), false);
-    }
-  }
-
-  static class SortedDocValuesCache extends Cache {
-    SortedDocValuesCache(FieldCacheImpl wrapper) {
-      super(wrapper);
-    }
-
-    @Override
-    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)
-        throws IOException {
-
-      final int maxDoc = reader.maxDoc();
-
-      Terms terms = reader.terms(key.field);
-
-      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();
-
-      final PagedBytes bytes = new PagedBytes(15);
-
-      int startTermsBPV;
-
-      final int termCountHardLimit;
-      if (maxDoc == Integer.MAX_VALUE) {
-        termCountHardLimit = Integer.MAX_VALUE;
-      } else {
-        termCountHardLimit = maxDoc+1;
-      }
-
-      // TODO: use Uninvert?
-      if (terms != null) {
-        // Try for coarse estimate for number of bits; this
-        // should be an underestimate most of the time, which
-        // is fine -- GrowableWriter will reallocate as needed
-        long numUniqueTerms = terms.size();
-        if (numUniqueTerms != -1L) {
-          if (numUniqueTerms > termCountHardLimit) {
-            // app is misusing the API (there is more than
-            // one term per doc); in this case we make best
-            // effort to load what we can (see LUCENE-2142)
-            numUniqueTerms = termCountHardLimit;
-          }
-
-          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);
-        } else {
-          startTermsBPV = 1;
-        }
-      } else {
-        startTermsBPV = 1;
-      }
-
-      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();
-      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);
-
-      int termOrd = 0;
-
-      // TODO: use Uninvert?
-
-      if (terms != null) {
-        final TermsEnum termsEnum = terms.iterator(null);
-        DocsEnum docs = null;
-
-        while(true) {
-          final BytesRef term = termsEnum.next();
-          if (term == null) {
-            break;
-          }
-          if (termOrd >= termCountHardLimit) {
-            break;
-          }
-
-          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));
-          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);
-          while (true) {
-            final int docID = docs.nextDoc();
-            if (docID == DocIdSetIterator.NO_MORE_DOCS) {
-              break;
-            }
-            // Store 1+ ord into packed bits
-            docToTermOrd.set(docID, 1+termOrd);
-          }
-          termOrd++;
-        }
-      }
-      termOrdToBytesOffset.freeze();
-
-      // maybe an int-only impl?
-      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);
-    }
-  }
-
-  private static class BinaryDocValuesImpl extends BinaryDocValues {
-    private final PagedBytes.Reader bytes;
-    private final PackedInts.Reader docToOffset;
-
-    public BinaryDocValuesImpl(PagedBytes.Reader bytes, PackedInts.Reader docToOffset) {
-      this.bytes = bytes;
-      this.docToOffset = docToOffset;
-    }
-
-    @Override
-    public void get(int docID, BytesRef ret) {
-      final int pointer = (int) docToOffset.get(docID);
-      if (pointer == 0) {
-        ret.bytes = BytesRef.EMPTY_BYTES;
-        ret.offset = 0;
-        ret.length = 0;
-      } else {
-        bytes.fill(ret, pointer);
-      }
-    }
-  }
-
-  // TODO: this if DocTermsIndex was already created, we
-  // should share it...
-  public BinaryDocValues getTerms(AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
-    return getTerms(reader, field, setDocsWithField, PackedInts.FAST);
-  }
-
-  public BinaryDocValues getTerms(AtomicReader reader, String field, boolean setDocsWithField, float acceptableOverheadRatio) throws IOException {
-    BinaryDocValues valuesIn = reader.getBinaryDocValues(field);
-    if (valuesIn == null) {
-      valuesIn = reader.getSortedDocValues(field);
-    }
-
-    if (valuesIn != null) {
-      // Not cached here by FieldCacheImpl (cached instead
-      // per-thread by SegmentReader):
-      return valuesIn;
-    }
-
-    final FieldInfo info = reader.getFieldInfos().fieldInfo(field);
-    if (info == null) {
-      return DocValues.EMPTY_BINARY;
-    } else if (info.hasDocValues()) {
-      throw new IllegalStateException("Type mismatch: " + field + " was indexed as " + info.getDocValuesType());
-    } else if (!info.isIndexed()) {
-      return DocValues.EMPTY_BINARY;
-    }
-
-    return (BinaryDocValues) caches.get(BinaryDocValues.class).get(reader, new CacheKey(field, acceptableOverheadRatio), setDocsWithField);
-  }
-
-  static final class BinaryDocValuesCache extends Cache {
-    BinaryDocValuesCache(FieldCacheImpl wrapper) {
-      super(wrapper);
-    }
-
-    @Override
-    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField)
-        throws IOException {
-
-      // TODO: would be nice to first check if DocTermsIndex
-      // was already cached for this field and then return
-      // that instead, to avoid insanity
-
-      final int maxDoc = reader.maxDoc();
-      Terms terms = reader.terms(key.field);
-
-      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();
-
-      final int termCountHardLimit = maxDoc;
-
-      // Holds the actual term data, expanded.
-      final PagedBytes bytes = new PagedBytes(15);
-
-      int startBPV;
-
-      if (terms != null) {
-        // Try for coarse estimate for number of bits; this
-        // should be an underestimate most of the time, which
-        // is fine -- GrowableWriter will reallocate as needed
-        long numUniqueTerms = terms.size();
-        if (numUniqueTerms != -1L) {
-          if (numUniqueTerms > termCountHardLimit) {
-            numUniqueTerms = termCountHardLimit;
-          }
-          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);
-        } else {
-          startBPV = 1;
-        }
-      } else {
-        startBPV = 1;
-      }
-
-      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);
-      
-      // pointer==0 means not set
-      bytes.copyUsingLengthPrefix(new BytesRef());
-
-      if (terms != null) {
-        int termCount = 0;
-        final TermsEnum termsEnum = terms.iterator(null);
-        DocsEnum docs = null;
-        while(true) {
-          if (termCount++ == termCountHardLimit) {
-            // app is misusing the API (there is more than
-            // one term per doc); in this case we make best
-            // effort to load what we can (see LUCENE-2142)
-            break;
-          }
-
-          final BytesRef term = termsEnum.next();
-          if (term == null) {
-            break;
-          }
-          final long pointer = bytes.copyUsingLengthPrefix(term);
-          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);
-          while (true) {
-            final int docID = docs.nextDoc();
-            if (docID == DocIdSetIterator.NO_MORE_DOCS) {
-              break;
-            }
-            docToOffset.set(docID, pointer);
-          }
-        }
-      }
-
-      final PackedInts.Reader offsetReader = docToOffset.getMutable();
-      if (setDocsWithField) {
-        wrapper.setDocsWithField(reader, key.field, new Bits() {
-          @Override
-          public boolean get(int index) {
-            return offsetReader.get(index) != 0;
-          }
-
-          @Override
-          public int length() {
-            return maxDoc;
-          }
-        });
-      }
-      // maybe an int-only impl?
-      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);
-    }
-  }
-
-  // TODO: this if DocTermsIndex was already created, we
-  // should share it...
-  public SortedSetDocValues getDocTermOrds(AtomicReader reader, String field) throws IOException {
-    SortedSetDocValues dv = reader.getSortedSetDocValues(field);
-    if (dv != null) {
-      return dv;
-    }
-    
-    SortedDocValues sdv = reader.getSortedDocValues(field);
-    if (sdv != null) {
-      return DocValues.singleton(sdv);
-    }
-    
-    final FieldInfo info = reader.getFieldInfos().fieldInfo(field);
-    if (info == null) {
-      return DocValues.EMPTY_SORTED_SET;
-    } else if (info.hasDocValues()) {
-      throw new IllegalStateException("Type mismatch: " + field + " was indexed as " + info.getDocValuesType());
-    } else if (!info.isIndexed()) {
-      return DocValues.EMPTY_SORTED_SET;
-    }
-    
-    DocTermOrds dto = (DocTermOrds) caches.get(DocTermOrds.class).get(reader, new CacheKey(field, null), false);
-    return dto.iterator(reader);
-  }
-
-  static final class DocTermOrdsCache extends Cache {
-    DocTermOrdsCache(FieldCacheImpl wrapper) {
-      super(wrapper);
-    }
-
-    @Override
-    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)
-        throws IOException {
-      return new DocTermOrds(reader, null, key.field);
-    }
-  }
-
-  private volatile PrintStream infoStream;
-
-  public void setInfoStream(PrintStream stream) {
-    infoStream = stream;
-  }
-
-  public PrintStream getInfoStream() {
-    return infoStream;
-  }
-}
-


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/FieldCache.java lucene5666/lucene/core/src/java/org/apache/lucene/search/FieldCache.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/FieldCache.java	2014-05-14 03:47:28.770646624 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/FieldCache.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,571 +0,0 @@
-package org.apache.lucene.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.PrintStream;
-
-import org.apache.lucene.analysis.NumericTokenStream;
-import org.apache.lucene.document.DoubleField;
-import org.apache.lucene.document.FloatField;
-import org.apache.lucene.document.IntField;
-import org.apache.lucene.document.LongField;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.DocTermOrds;
-import org.apache.lucene.index.IndexReader; // javadocs
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.NumericUtils;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/**
- * Expert: Maintains caches of term values.
- *
- * <p>Created: May 19, 2004 11:13:14 AM
- *
- * @since   lucene 1.4
- * @see org.apache.lucene.util.FieldCacheSanityChecker
- *
- * @lucene.internal
- */
-public interface FieldCache {
-
-  /** Field values as 32-bit signed integers */
-  public static abstract class Ints {
-    /** Return an integer representation of this field's value. */
-    public abstract int get(int docID);
-    
-    /** Zero value for every document */
-    public static final Ints EMPTY = new Ints() {
-      @Override
-      public int get(int docID) {
-        return 0;
-      }
-    };
-  }
-
-  /** Field values as 64-bit signed long integers */
-  public static abstract class Longs {
-    /** Return an long representation of this field's value. */
-    public abstract long get(int docID);
-    
-    /** Zero value for every document */
-    public static final Longs EMPTY = new Longs() {
-      @Override
-      public long get(int docID) {
-        return 0;
-      }
-    };
-  }
-
-  /** Field values as 32-bit floats */
-  public static abstract class Floats {
-    /** Return an float representation of this field's value. */
-    public abstract float get(int docID);
-    
-    /** Zero value for every document */
-    public static final Floats EMPTY = new Floats() {
-      @Override
-      public float get(int docID) {
-        return 0;
-      }
-    };
-  }
-
-  /** Field values as 64-bit doubles */
-  public static abstract class Doubles {
-    /** Return an double representation of this field's value. */
-    public abstract double get(int docID);
-    
-    /** Zero value for every document */
-    public static final Doubles EMPTY = new Doubles() {
-      @Override
-      public double get(int docID) {
-        return 0;
-      }
-    };
-  }
-
-  /**
-   * Placeholder indicating creation of this cache is currently in-progress.
-   */
-  public static final class CreationPlaceholder {
-    Object value;
-  }
-
-  /**
-   * Marker interface as super-interface to all parsers. It
-   * is used to specify a custom parser to {@link
-   * SortField#SortField(String, FieldCache.Parser)}.
-   */
-  public interface Parser {
-    
-    /**
-     * Pulls a {@link TermsEnum} from the given {@link Terms}. This method allows certain parsers
-     * to filter the actual TermsEnum before the field cache is filled.
-     * 
-     * @param terms the {@link Terms} instance to create the {@link TermsEnum} from.
-     * @return a possibly filtered {@link TermsEnum} instance, this method must not return <code>null</code>.
-     * @throws IOException if an {@link IOException} occurs
-     */
-    public TermsEnum termsEnum(Terms terms) throws IOException;
-  }
-
-  /** Interface to parse ints from document fields.
-   * @see FieldCache#getInts(AtomicReader, String, FieldCache.IntParser, boolean)
-   */
-  public interface IntParser extends Parser {
-    /** Return an integer representation of this field's value. */
-    public int parseInt(BytesRef term);
-  }
-
-  /** Interface to parse floats from document fields.
-   * @see FieldCache#getFloats(AtomicReader, String, FieldCache.FloatParser, boolean)
-   */
-  public interface FloatParser extends Parser {
-    /** Return an float representation of this field's value. */
-    public float parseFloat(BytesRef term);
-  }
-
-  /** Interface to parse long from document fields.
-   * @see FieldCache#getLongs(AtomicReader, String, FieldCache.LongParser, boolean)
-   */
-  public interface LongParser extends Parser {
-    /** Return an long representation of this field's value. */
-    public long parseLong(BytesRef term);
-  }
-
-  /** Interface to parse doubles from document fields.
-   * @see FieldCache#getDoubles(AtomicReader, String, FieldCache.DoubleParser, boolean)
-   */
-  public interface DoubleParser extends Parser {
-    /** Return an double representation of this field's value. */
-    public double parseDouble(BytesRef term);
-  }
-
-  /** Expert: The cache used internally by sorting and range query classes. */
-  public static FieldCache DEFAULT = new FieldCacheImpl();
-
-  /**
-   * A parser instance for int values encoded by {@link NumericUtils}, e.g. when indexed
-   * via {@link IntField}/{@link NumericTokenStream}.
-   */
-  public static final IntParser NUMERIC_UTILS_INT_PARSER=new IntParser(){
-    @Override
-    public int parseInt(BytesRef term) {
-      return NumericUtils.prefixCodedToInt(term);
-    }
-    
-    @Override
-    public TermsEnum termsEnum(Terms terms) throws IOException {
-      return NumericUtils.filterPrefixCodedInts(terms.iterator(null));
-    }
-    
-    @Override
-    public String toString() { 
-      return FieldCache.class.getName()+".NUMERIC_UTILS_INT_PARSER"; 
-    }
-  };
-
-  /**
-   * A parser instance for float values encoded with {@link NumericUtils}, e.g. when indexed
-   * via {@link FloatField}/{@link NumericTokenStream}.
-   */
-  public static final FloatParser NUMERIC_UTILS_FLOAT_PARSER=new FloatParser(){
-    @Override
-    public float parseFloat(BytesRef term) {
-      return NumericUtils.sortableIntToFloat(NumericUtils.prefixCodedToInt(term));
-    }
-    @Override
-    public String toString() { 
-      return FieldCache.class.getName()+".NUMERIC_UTILS_FLOAT_PARSER"; 
-    }
-    
-    @Override
-    public TermsEnum termsEnum(Terms terms) throws IOException {
-      return NumericUtils.filterPrefixCodedInts(terms.iterator(null));
-    }
-  };
-
-  /**
-   * A parser instance for long values encoded by {@link NumericUtils}, e.g. when indexed
-   * via {@link LongField}/{@link NumericTokenStream}.
-   */
-  public static final LongParser NUMERIC_UTILS_LONG_PARSER = new LongParser(){
-    @Override
-    public long parseLong(BytesRef term) {
-      return NumericUtils.prefixCodedToLong(term);
-    }
-    @Override
-    public String toString() { 
-      return FieldCache.class.getName()+".NUMERIC_UTILS_LONG_PARSER"; 
-    }
-    
-    @Override
-    public TermsEnum termsEnum(Terms terms) throws IOException {
-      return NumericUtils.filterPrefixCodedLongs(terms.iterator(null));
-    }
-  };
-
-  /**
-   * A parser instance for double values encoded with {@link NumericUtils}, e.g. when indexed
-   * via {@link DoubleField}/{@link NumericTokenStream}.
-   */
-  public static final DoubleParser NUMERIC_UTILS_DOUBLE_PARSER = new DoubleParser(){
-    @Override
-    public double parseDouble(BytesRef term) {
-      return NumericUtils.sortableLongToDouble(NumericUtils.prefixCodedToLong(term));
-    }
-    @Override
-    public String toString() { 
-      return FieldCache.class.getName()+".NUMERIC_UTILS_DOUBLE_PARSER"; 
-    }
-    
-    @Override
-    public TermsEnum termsEnum(Terms terms) throws IOException {
-      return NumericUtils.filterPrefixCodedLongs(terms.iterator(null));
-    }
-  };
-  
-  /** Checks the internal cache for an appropriate entry, and if none is found,
-   *  reads the terms in <code>field</code> and returns a bit set at the size of
-   *  <code>reader.maxDoc()</code>, with turned on bits for each docid that 
-   *  does have a value for this field.
-   */
-  public Bits getDocsWithField(AtomicReader reader, String field) throws IOException;
-
-  /**
-   * Returns an {@link Ints} over the values found in documents in the given
-   * field.
-   *
-   * @see #getInts(AtomicReader, String, IntParser, boolean)
-   */
-  public Ints getInts(AtomicReader reader, String field, boolean setDocsWithField) throws IOException;
-
-  /**
-   * Returns an {@link Ints} over the values found in documents in the given
-   * field. If the field was indexed as {@link NumericDocValuesField}, it simply
-   * uses {@link AtomicReader#getNumericDocValues(String)} to read the values.
-   * Otherwise, it checks the internal cache for an appropriate entry, and if
-   * none is found, reads the terms in <code>field</code> as ints and returns
-   * an array of size <code>reader.maxDoc()</code> of the value each document
-   * has in the given field.
-   * 
-   * @param reader
-   *          Used to get field values.
-   * @param field
-   *          Which field contains the longs.
-   * @param parser
-   *          Computes int for string values. May be {@code null} if the
-   *          requested field was indexed as {@link NumericDocValuesField} or
-   *          {@link IntField}.
-   * @param setDocsWithField
-   *          If true then {@link #getDocsWithField} will also be computed and
-   *          stored in the FieldCache.
-   * @return The values in the given field for each document.
-   * @throws IOException
-   *           If any error occurs.
-   */
-  public Ints getInts(AtomicReader reader, String field, IntParser parser, boolean setDocsWithField) throws IOException;
-
-  /**
-   * Returns a {@link Floats} over the values found in documents in the given
-   * field.
-   *
-   * @see #getFloats(AtomicReader, String, FloatParser, boolean)
-   */
-  public Floats getFloats(AtomicReader reader, String field, boolean setDocsWithField) throws IOException;
-
-  /**
-   * Returns a {@link Floats} over the values found in documents in the given
-   * field. If the field was indexed as {@link NumericDocValuesField}, it simply
-   * uses {@link AtomicReader#getNumericDocValues(String)} to read the values.
-   * Otherwise, it checks the internal cache for an appropriate entry, and if
-   * none is found, reads the terms in <code>field</code> as floats and returns
-   * an array of size <code>reader.maxDoc()</code> of the value each document
-   * has in the given field.
-   * 
-   * @param reader
-   *          Used to get field values.
-   * @param field
-   *          Which field contains the floats.
-   * @param parser
-   *          Computes float for string values. May be {@code null} if the
-   *          requested field was indexed as {@link NumericDocValuesField} or
-   *          {@link FloatField}.
-   * @param setDocsWithField
-   *          If true then {@link #getDocsWithField} will also be computed and
-   *          stored in the FieldCache.
-   * @return The values in the given field for each document.
-   * @throws IOException
-   *           If any error occurs.
-   */
-  public Floats getFloats(AtomicReader reader, String field, FloatParser parser, boolean setDocsWithField) throws IOException;
-
-  /**
-   * Returns a {@link Longs} over the values found in documents in the given
-   * field.
-   *
-   * @see #getLongs(AtomicReader, String, LongParser, boolean)
-   */
-  public Longs getLongs(AtomicReader reader, String field, boolean setDocsWithField) throws IOException;
-
-  /**
-   * Returns a {@link Longs} over the values found in documents in the given
-   * field. If the field was indexed as {@link NumericDocValuesField}, it simply
-   * uses {@link AtomicReader#getNumericDocValues(String)} to read the values.
-   * Otherwise, it checks the internal cache for an appropriate entry, and if
-   * none is found, reads the terms in <code>field</code> as longs and returns
-   * an array of size <code>reader.maxDoc()</code> of the value each document
-   * has in the given field.
-   * 
-   * @param reader
-   *          Used to get field values.
-   * @param field
-   *          Which field contains the longs.
-   * @param parser
-   *          Computes long for string values. May be {@code null} if the
-   *          requested field was indexed as {@link NumericDocValuesField} or
-   *          {@link LongField}.
-   * @param setDocsWithField
-   *          If true then {@link #getDocsWithField} will also be computed and
-   *          stored in the FieldCache.
-   * @return The values in the given field for each document.
-   * @throws IOException
-   *           If any error occurs.
-   */
-  public Longs getLongs(AtomicReader reader, String field, LongParser parser, boolean setDocsWithField) throws IOException;
-
-  /**
-   * Returns a {@link Doubles} over the values found in documents in the given
-   * field.
-   *
-   * @see #getDoubles(AtomicReader, String, DoubleParser, boolean)
-   */
-  public Doubles getDoubles(AtomicReader reader, String field, boolean setDocsWithField) throws IOException;
-
-  /**
-   * Returns a {@link Doubles} over the values found in documents in the given
-   * field. If the field was indexed as {@link NumericDocValuesField}, it simply
-   * uses {@link AtomicReader#getNumericDocValues(String)} to read the values.
-   * Otherwise, it checks the internal cache for an appropriate entry, and if
-   * none is found, reads the terms in <code>field</code> as doubles and returns
-   * an array of size <code>reader.maxDoc()</code> of the value each document
-   * has in the given field.
-   * 
-   * @param reader
-   *          Used to get field values.
-   * @param field
-   *          Which field contains the longs.
-   * @param parser
-   *          Computes double for string values. May be {@code null} if the
-   *          requested field was indexed as {@link NumericDocValuesField} or
-   *          {@link DoubleField}.
-   * @param setDocsWithField
-   *          If true then {@link #getDocsWithField} will also be computed and
-   *          stored in the FieldCache.
-   * @return The values in the given field for each document.
-   * @throws IOException
-   *           If any error occurs.
-   */
-  public Doubles getDoubles(AtomicReader reader, String field, DoubleParser parser, boolean setDocsWithField) throws IOException;
-
-  /** Checks the internal cache for an appropriate entry, and if none
-   * is found, reads the term values in <code>field</code>
-   * and returns a {@link BinaryDocValues} instance, providing a
-   * method to retrieve the term (as a BytesRef) per document.
-   * @param reader  Used to get field values.
-   * @param field   Which field contains the strings.
-   * @param setDocsWithField  If true then {@link #getDocsWithField} will
-   *        also be computed and stored in the FieldCache.
-   * @return The values in the given field for each document.
-   * @throws IOException  If any error occurs.
-   */
-  public BinaryDocValues getTerms(AtomicReader reader, String field, boolean setDocsWithField) throws IOException;
-
-  /** Expert: just like {@link #getTerms(AtomicReader,String,boolean)},
-   *  but you can specify whether more RAM should be consumed in exchange for
-   *  faster lookups (default is "true").  Note that the
-   *  first call for a given reader and field "wins",
-   *  subsequent calls will share the same cache entry. */
-  public BinaryDocValues getTerms(AtomicReader reader, String field, boolean setDocsWithField, float acceptableOverheadRatio) throws IOException;
-
-  /** Checks the internal cache for an appropriate entry, and if none
-   * is found, reads the term values in <code>field</code>
-   * and returns a {@link SortedDocValues} instance,
-   * providing methods to retrieve sort ordinals and terms
-   * (as a ByteRef) per document.
-   * @param reader  Used to get field values.
-   * @param field   Which field contains the strings.
-   * @return The values in the given field for each document.
-   * @throws IOException  If any error occurs.
-   */
-  public SortedDocValues getTermsIndex(AtomicReader reader, String field) throws IOException;
-
-  /** Expert: just like {@link
-   *  #getTermsIndex(AtomicReader,String)}, but you can specify
-   *  whether more RAM should be consumed in exchange for
-   *  faster lookups (default is "true").  Note that the
-   *  first call for a given reader and field "wins",
-   *  subsequent calls will share the same cache entry. */
-  public SortedDocValues getTermsIndex(AtomicReader reader, String field, float acceptableOverheadRatio) throws IOException;
-
-  /**
-   * Checks the internal cache for an appropriate entry, and if none is found, reads the term values
-   * in <code>field</code> and returns a {@link DocTermOrds} instance, providing a method to retrieve
-   * the terms (as ords) per document.
-   *
-   * @param reader  Used to build a {@link DocTermOrds} instance
-   * @param field   Which field contains the strings.
-   * @return a {@link DocTermOrds} instance
-   * @throws IOException  If any error occurs.
-   */
-  public SortedSetDocValues getDocTermOrds(AtomicReader reader, String field) throws IOException;
-
-  /**
-   * EXPERT: A unique Identifier/Description for each item in the FieldCache. 
-   * Can be useful for logging/debugging.
-   * @lucene.experimental
-   */
-  public final class CacheEntry {
-
-    private final Object readerKey;
-    private final String fieldName;
-    private final Class<?> cacheType;
-    private final Object custom;
-    private final Object value;
-    private String size;
-
-    public CacheEntry(Object readerKey, String fieldName,
-                      Class<?> cacheType,
-                      Object custom,
-                      Object value) {
-      this.readerKey = readerKey;
-      this.fieldName = fieldName;
-      this.cacheType = cacheType;
-      this.custom = custom;
-      this.value = value;
-    }
-
-    public Object getReaderKey() {
-      return readerKey;
-    }
-
-    public String getFieldName() {
-      return fieldName;
-    }
-
-    public Class<?> getCacheType() {
-      return cacheType;
-    }
-
-    public Object getCustom() {
-      return custom;
-    }
-
-    public Object getValue() {
-      return value;
-    }
-
-    /** 
-     * Computes (and stores) the estimated size of the cache Value 
-     * @see #getEstimatedSize
-     */
-    public void estimateSize() {
-      long bytesUsed = RamUsageEstimator.sizeOf(getValue());
-      size = RamUsageEstimator.humanReadableUnits(bytesUsed);
-    }
-
-    /**
-     * The most recently estimated size of the value, null unless 
-     * estimateSize has been called.
-     */
-    public String getEstimatedSize() {
-      return size;
-    }
-    
-    @Override
-    public String toString() {
-      StringBuilder b = new StringBuilder();
-      b.append("'").append(getReaderKey()).append("'=>");
-      b.append("'").append(getFieldName()).append("',");
-      b.append(getCacheType()).append(",").append(getCustom());
-      b.append("=>").append(getValue().getClass().getName()).append("#");
-      b.append(System.identityHashCode(getValue()));
-      
-      String s = getEstimatedSize();
-      if(null != s) {
-        b.append(" (size =~ ").append(s).append(')');
-      }
-
-      return b.toString();
-    }
-  }
-  
-  /**
-   * EXPERT: Generates an array of CacheEntry objects representing all items 
-   * currently in the FieldCache.
-   * <p>
-   * NOTE: These CacheEntry objects maintain a strong reference to the 
-   * Cached Values.  Maintaining references to a CacheEntry the AtomicIndexReader 
-   * associated with it has garbage collected will prevent the Value itself
-   * from being garbage collected when the Cache drops the WeakReference.
-   * </p>
-   * @lucene.experimental
-   */
-  public CacheEntry[] getCacheEntries();
-
-  /**
-   * <p>
-   * EXPERT: Instructs the FieldCache to forcibly expunge all entries 
-   * from the underlying caches.  This is intended only to be used for 
-   * test methods as a way to ensure a known base state of the Cache 
-   * (with out needing to rely on GC to free WeakReferences).  
-   * It should not be relied on for "Cache maintenance" in general 
-   * application code.
-   * </p>
-   * @lucene.experimental
-   */
-  public void purgeAllCaches();
-
-  /**
-   * Expert: drops all cache entries associated with this
-   * reader {@link IndexReader#getCoreCacheKey}.  NOTE: this cache key must
-   * precisely match the reader that the cache entry is
-   * keyed on. If you pass a top-level reader, it usually
-   * will have no effect as Lucene now caches at the segment
-   * reader level.
-   */
-  public void purgeByCacheKey(Object coreCacheKey);
-
-  /**
-   * If non-null, FieldCacheImpl will warn whenever
-   * entries are created that are not sane according to
-   * {@link org.apache.lucene.util.FieldCacheSanityChecker}.
-   */
-  public void setInfoStream(PrintStream stream);
-
-  /** counterpart of {@link #setInfoStream(PrintStream)} */
-  public PrintStream getInfoStream();
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java lucene5666/lucene/core/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java	2014-05-14 03:47:28.774646624 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,466 +0,0 @@
-package org.apache.lucene.search;
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.document.DoubleField; // for javadocs
-import org.apache.lucene.document.FloatField; // for javadocs
-import org.apache.lucene.document.IntField; // for javadocs
-import org.apache.lucene.document.LongField; // for javadocs
-import org.apache.lucene.index.AtomicReader; // for javadocs
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.NumericUtils;
-
-/**
- * A range filter built on top of a cached single term field (in {@link FieldCache}).
- * 
- * <p>{@code FieldCacheRangeFilter} builds a single cache for the field the first time it is used.
- * Each subsequent {@code FieldCacheRangeFilter} on the same field then reuses this cache,
- * even if the range itself changes. 
- * 
- * <p>This means that {@code FieldCacheRangeFilter} is much faster (sometimes more than 100x as fast) 
- * as building a {@link TermRangeFilter}, if using a {@link #newStringRange}.
- * However, if the range never changes it is slower (around 2x as slow) than building
- * a CachingWrapperFilter on top of a single {@link TermRangeFilter}.
- *
- * For numeric data types, this filter may be significantly faster than {@link NumericRangeFilter}.
- * Furthermore, it does not need the numeric values encoded
- * by {@link IntField}, {@link FloatField}, {@link
- * LongField} or {@link DoubleField}. But
- * it has the problem that it only works with exact one value/document (see below).
- *
- * <p>As with all {@link FieldCache} based functionality, {@code FieldCacheRangeFilter} is only valid for 
- * fields which exact one term for each document (except for {@link #newStringRange}
- * where 0 terms are also allowed). Due to a restriction of {@link FieldCache}, for numeric ranges
- * all terms that do not have a numeric value, 0 is assumed.
- *
- * <p>Thus it works on dates, prices and other single value fields but will not work on
- * regular text fields. It is preferable to use a <code>NOT_ANALYZED</code> field to ensure that
- * there is only a single term. 
- *
- * <p>This class does not have an constructor, use one of the static factory methods available,
- * that create a correct instance for different data types supported by {@link FieldCache}.
- */
-
-public abstract class FieldCacheRangeFilter<T> extends Filter {
-  final String field;
-  final FieldCache.Parser parser;
-  final T lowerVal;
-  final T upperVal;
-  final boolean includeLower;
-  final boolean includeUpper;
-  
-  private FieldCacheRangeFilter(String field, FieldCache.Parser parser, T lowerVal, T upperVal, boolean includeLower, boolean includeUpper) {
-    this.field = field;
-    this.parser = parser;
-    this.lowerVal = lowerVal;
-    this.upperVal = upperVal;
-    this.includeLower = includeLower;
-    this.includeUpper = includeUpper;
-  }
-  
-  /** This method is implemented for each data type */
-  @Override
-  public abstract DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException;
-
-  /**
-   * Creates a string range filter using {@link FieldCache#getTermsIndex}. This works with all
-   * fields containing zero or one term in the field. The range can be half-open by setting one
-   * of the values to <code>null</code>.
-   */
-  public static FieldCacheRangeFilter<String> newStringRange(String field, String lowerVal, String upperVal, boolean includeLower, boolean includeUpper) {
-    return new FieldCacheRangeFilter<String>(field, null, lowerVal, upperVal, includeLower, includeUpper) {
-      @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-        final SortedDocValues fcsi = FieldCache.DEFAULT.getTermsIndex(context.reader(), field);
-        final int lowerPoint = lowerVal == null ? -1 : fcsi.lookupTerm(new BytesRef(lowerVal));
-        final int upperPoint = upperVal == null ? -1 : fcsi.lookupTerm(new BytesRef(upperVal));
-
-        final int inclusiveLowerPoint, inclusiveUpperPoint;
-
-        // Hints:
-        // * binarySearchLookup returns -1, if value was null.
-        // * the value is <0 if no exact hit was found, the returned value
-        //   is (-(insertion point) - 1)
-        if (lowerPoint == -1 && lowerVal == null) {
-          inclusiveLowerPoint = 0;
-        } else if (includeLower && lowerPoint >= 0) {
-          inclusiveLowerPoint = lowerPoint;
-        } else if (lowerPoint >= 0) {
-          inclusiveLowerPoint = lowerPoint + 1;
-        } else {
-          inclusiveLowerPoint = Math.max(0, -lowerPoint - 1);
-        }
-        
-        if (upperPoint == -1 && upperVal == null) {
-          inclusiveUpperPoint = Integer.MAX_VALUE;  
-        } else if (includeUpper && upperPoint >= 0) {
-          inclusiveUpperPoint = upperPoint;
-        } else if (upperPoint >= 0) {
-          inclusiveUpperPoint = upperPoint - 1;
-        } else {
-          inclusiveUpperPoint = -upperPoint - 2;
-        }      
-
-        if (inclusiveUpperPoint < 0 || inclusiveLowerPoint > inclusiveUpperPoint) {
-          return null;
-        }
-        
-        assert inclusiveLowerPoint >= 0 && inclusiveUpperPoint >= 0;
-        
-        return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
-          @Override
-          protected final boolean matchDoc(int doc) {
-            final int docOrd = fcsi.getOrd(doc);
-            return docOrd >= inclusiveLowerPoint && docOrd <= inclusiveUpperPoint;
-          }
-        };
-      }
-    };
-  }
-  
-  /**
-   * Creates a BytesRef range filter using {@link FieldCache#getTermsIndex}. This works with all
-   * fields containing zero or one term in the field. The range can be half-open by setting one
-   * of the values to <code>null</code>.
-   */
-  // TODO: bogus that newStringRange doesnt share this code... generics hell
-  public static FieldCacheRangeFilter<BytesRef> newBytesRefRange(String field, BytesRef lowerVal, BytesRef upperVal, boolean includeLower, boolean includeUpper) {
-    return new FieldCacheRangeFilter<BytesRef>(field, null, lowerVal, upperVal, includeLower, includeUpper) {
-      @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-        final SortedDocValues fcsi = FieldCache.DEFAULT.getTermsIndex(context.reader(), field);
-        final int lowerPoint = lowerVal == null ? -1 : fcsi.lookupTerm(lowerVal);
-        final int upperPoint = upperVal == null ? -1 : fcsi.lookupTerm(upperVal);
-
-        final int inclusiveLowerPoint, inclusiveUpperPoint;
-
-        // Hints:
-        // * binarySearchLookup returns -1, if value was null.
-        // * the value is <0 if no exact hit was found, the returned value
-        //   is (-(insertion point) - 1)
-        if (lowerPoint == -1 && lowerVal == null) {
-          inclusiveLowerPoint = 0;
-        } else if (includeLower && lowerPoint >= 0) {
-          inclusiveLowerPoint = lowerPoint;
-        } else if (lowerPoint >= 0) {
-          inclusiveLowerPoint = lowerPoint + 1;
-        } else {
-          inclusiveLowerPoint = Math.max(0, -lowerPoint - 1);
-        }
-        
-        if (upperPoint == -1 && upperVal == null) {
-          inclusiveUpperPoint = Integer.MAX_VALUE;  
-        } else if (includeUpper && upperPoint >= 0) {
-          inclusiveUpperPoint = upperPoint;
-        } else if (upperPoint >= 0) {
-          inclusiveUpperPoint = upperPoint - 1;
-        } else {
-          inclusiveUpperPoint = -upperPoint - 2;
-        }      
-
-        if (inclusiveUpperPoint < 0 || inclusiveLowerPoint > inclusiveUpperPoint) {
-          return null;
-        }
-        
-        assert inclusiveLowerPoint >= 0 && inclusiveUpperPoint >= 0;
-        
-        return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
-          @Override
-          protected final boolean matchDoc(int doc) {
-            final int docOrd = fcsi.getOrd(doc);
-            return docOrd >= inclusiveLowerPoint && docOrd <= inclusiveUpperPoint;
-          }
-        };
-      }
-    };
-  }
-
-  /**
-   * Creates a numeric range filter using {@link FieldCache#getInts(AtomicReader,String,boolean)}. This works with all
-   * int fields containing exactly one numeric term in the field. The range can be half-open by setting one
-   * of the values to <code>null</code>.
-   */
-  public static FieldCacheRangeFilter<Integer> newIntRange(String field, Integer lowerVal, Integer upperVal, boolean includeLower, boolean includeUpper) {
-    return newIntRange(field, null, lowerVal, upperVal, includeLower, includeUpper);
-  }
-  
-  /**
-   * Creates a numeric range filter using {@link FieldCache#getInts(AtomicReader,String,FieldCache.IntParser,boolean)}. This works with all
-   * int fields containing exactly one numeric term in the field. The range can be half-open by setting one
-   * of the values to <code>null</code>.
-   */
-  public static FieldCacheRangeFilter<Integer> newIntRange(String field, FieldCache.IntParser parser, Integer lowerVal, Integer upperVal, boolean includeLower, boolean includeUpper) {
-    return new FieldCacheRangeFilter<Integer>(field, parser, lowerVal, upperVal, includeLower, includeUpper) {
-      @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-        final int inclusiveLowerPoint, inclusiveUpperPoint;
-        if (lowerVal != null) {
-          int i = lowerVal.intValue();
-          if (!includeLower && i == Integer.MAX_VALUE)
-            return null;
-          inclusiveLowerPoint = includeLower ? i : (i + 1);
-        } else {
-          inclusiveLowerPoint = Integer.MIN_VALUE;
-        }
-        if (upperVal != null) {
-          int i = upperVal.intValue();
-          if (!includeUpper && i == Integer.MIN_VALUE)
-            return null;
-          inclusiveUpperPoint = includeUpper ? i : (i - 1);
-        } else {
-          inclusiveUpperPoint = Integer.MAX_VALUE;
-        }
-        
-        if (inclusiveLowerPoint > inclusiveUpperPoint)
-          return null;
-        
-        final FieldCache.Ints values = FieldCache.DEFAULT.getInts(context.reader(), field, (FieldCache.IntParser) parser, false);
-        return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
-          @Override
-          protected boolean matchDoc(int doc) {
-            final int value = values.get(doc);
-            return value >= inclusiveLowerPoint && value <= inclusiveUpperPoint;
-          }
-        };
-      }
-    };
-  }
-  
-  /**
-   * Creates a numeric range filter using {@link FieldCache#getLongs(AtomicReader,String,boolean)}. This works with all
-   * long fields containing exactly one numeric term in the field. The range can be half-open by setting one
-   * of the values to <code>null</code>.
-   */
-  public static FieldCacheRangeFilter<Long> newLongRange(String field, Long lowerVal, Long upperVal, boolean includeLower, boolean includeUpper) {
-    return newLongRange(field, null, lowerVal, upperVal, includeLower, includeUpper);
-  }
-  
-  /**
-   * Creates a numeric range filter using {@link FieldCache#getLongs(AtomicReader,String,FieldCache.LongParser,boolean)}. This works with all
-   * long fields containing exactly one numeric term in the field. The range can be half-open by setting one
-   * of the values to <code>null</code>.
-   */
-  public static FieldCacheRangeFilter<Long> newLongRange(String field, FieldCache.LongParser parser, Long lowerVal, Long upperVal, boolean includeLower, boolean includeUpper) {
-    return new FieldCacheRangeFilter<Long>(field, parser, lowerVal, upperVal, includeLower, includeUpper) {
-      @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-        final long inclusiveLowerPoint, inclusiveUpperPoint;
-        if (lowerVal != null) {
-          long i = lowerVal.longValue();
-          if (!includeLower && i == Long.MAX_VALUE)
-            return null;
-          inclusiveLowerPoint = includeLower ? i : (i + 1L);
-        } else {
-          inclusiveLowerPoint = Long.MIN_VALUE;
-        }
-        if (upperVal != null) {
-          long i = upperVal.longValue();
-          if (!includeUpper && i == Long.MIN_VALUE)
-            return null;
-          inclusiveUpperPoint = includeUpper ? i : (i - 1L);
-        } else {
-          inclusiveUpperPoint = Long.MAX_VALUE;
-        }
-        
-        if (inclusiveLowerPoint > inclusiveUpperPoint)
-          return null;
-        
-        final FieldCache.Longs values = FieldCache.DEFAULT.getLongs(context.reader(), field, (FieldCache.LongParser) parser, false);
-        return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
-          @Override
-          protected boolean matchDoc(int doc) {
-            final long value = values.get(doc);
-            return value >= inclusiveLowerPoint && value <= inclusiveUpperPoint;
-          }
-        };
-      }
-    };
-  }
-  
-  /**
-   * Creates a numeric range filter using {@link FieldCache#getFloats(AtomicReader,String,boolean)}. This works with all
-   * float fields containing exactly one numeric term in the field. The range can be half-open by setting one
-   * of the values to <code>null</code>.
-   */
-  public static FieldCacheRangeFilter<Float> newFloatRange(String field, Float lowerVal, Float upperVal, boolean includeLower, boolean includeUpper) {
-    return newFloatRange(field, null, lowerVal, upperVal, includeLower, includeUpper);
-  }
-  
-  /**
-   * Creates a numeric range filter using {@link FieldCache#getFloats(AtomicReader,String,FieldCache.FloatParser,boolean)}. This works with all
-   * float fields containing exactly one numeric term in the field. The range can be half-open by setting one
-   * of the values to <code>null</code>.
-   */
-  public static FieldCacheRangeFilter<Float> newFloatRange(String field, FieldCache.FloatParser parser, Float lowerVal, Float upperVal, boolean includeLower, boolean includeUpper) {
-    return new FieldCacheRangeFilter<Float>(field, parser, lowerVal, upperVal, includeLower, includeUpper) {
-      @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-        // we transform the floating point numbers to sortable integers
-        // using NumericUtils to easier find the next bigger/lower value
-        final float inclusiveLowerPoint, inclusiveUpperPoint;
-        if (lowerVal != null) {
-          float f = lowerVal.floatValue();
-          if (!includeUpper && f > 0.0f && Float.isInfinite(f))
-            return null;
-          int i = NumericUtils.floatToSortableInt(f);
-          inclusiveLowerPoint = NumericUtils.sortableIntToFloat( includeLower ?  i : (i + 1) );
-        } else {
-          inclusiveLowerPoint = Float.NEGATIVE_INFINITY;
-        }
-        if (upperVal != null) {
-          float f = upperVal.floatValue();
-          if (!includeUpper && f < 0.0f && Float.isInfinite(f))
-            return null;
-          int i = NumericUtils.floatToSortableInt(f);
-          inclusiveUpperPoint = NumericUtils.sortableIntToFloat( includeUpper ? i : (i - 1) );
-        } else {
-          inclusiveUpperPoint = Float.POSITIVE_INFINITY;
-        }
-        
-        if (inclusiveLowerPoint > inclusiveUpperPoint)
-          return null;
-        
-        final FieldCache.Floats values = FieldCache.DEFAULT.getFloats(context.reader(), field, (FieldCache.FloatParser) parser, false);
-        return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
-          @Override
-          protected boolean matchDoc(int doc) {
-            final float value = values.get(doc);
-            return value >= inclusiveLowerPoint && value <= inclusiveUpperPoint;
-          }
-        };
-      }
-    };
-  }
-  
-  /**
-   * Creates a numeric range filter using {@link FieldCache#getDoubles(AtomicReader,String,boolean)}. This works with all
-   * double fields containing exactly one numeric term in the field. The range can be half-open by setting one
-   * of the values to <code>null</code>.
-   */
-  public static FieldCacheRangeFilter<Double> newDoubleRange(String field, Double lowerVal, Double upperVal, boolean includeLower, boolean includeUpper) {
-    return newDoubleRange(field, null, lowerVal, upperVal, includeLower, includeUpper);
-  }
-  
-  /**
-   * Creates a numeric range filter using {@link FieldCache#getDoubles(AtomicReader,String,FieldCache.DoubleParser,boolean)}. This works with all
-   * double fields containing exactly one numeric term in the field. The range can be half-open by setting one
-   * of the values to <code>null</code>.
-   */
-  public static FieldCacheRangeFilter<Double> newDoubleRange(String field, FieldCache.DoubleParser parser, Double lowerVal, Double upperVal, boolean includeLower, boolean includeUpper) {
-    return new FieldCacheRangeFilter<Double>(field, parser, lowerVal, upperVal, includeLower, includeUpper) {
-      @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-        // we transform the floating point numbers to sortable integers
-        // using NumericUtils to easier find the next bigger/lower value
-        final double inclusiveLowerPoint, inclusiveUpperPoint;
-        if (lowerVal != null) {
-          double f = lowerVal.doubleValue();
-          if (!includeUpper && f > 0.0 && Double.isInfinite(f))
-            return null;
-          long i = NumericUtils.doubleToSortableLong(f);
-          inclusiveLowerPoint = NumericUtils.sortableLongToDouble( includeLower ?  i : (i + 1L) );
-        } else {
-          inclusiveLowerPoint = Double.NEGATIVE_INFINITY;
-        }
-        if (upperVal != null) {
-          double f = upperVal.doubleValue();
-          if (!includeUpper && f < 0.0 && Double.isInfinite(f))
-            return null;
-          long i = NumericUtils.doubleToSortableLong(f);
-          inclusiveUpperPoint = NumericUtils.sortableLongToDouble( includeUpper ? i : (i - 1L) );
-        } else {
-          inclusiveUpperPoint = Double.POSITIVE_INFINITY;
-        }
-        
-        if (inclusiveLowerPoint > inclusiveUpperPoint)
-          return null;
-        
-        final FieldCache.Doubles values = FieldCache.DEFAULT.getDoubles(context.reader(), field, (FieldCache.DoubleParser) parser, false);
-        // ignore deleted docs if range doesn't contain 0
-        return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
-          @Override
-          protected boolean matchDoc(int doc) {
-            final double value = values.get(doc);
-            return value >= inclusiveLowerPoint && value <= inclusiveUpperPoint;
-          }
-        };
-      }
-    };
-  }
-  
-  @Override
-  public final String toString() {
-    final StringBuilder sb = new StringBuilder(field).append(":");
-    return sb.append(includeLower ? '[' : '{')
-      .append((lowerVal == null) ? "*" : lowerVal.toString())
-      .append(" TO ")
-      .append((upperVal == null) ? "*" : upperVal.toString())
-      .append(includeUpper ? ']' : '}')
-      .toString();
-  }
-
-  @Override
-  @SuppressWarnings({"rawtypes"})
-  public final boolean equals(Object o) {
-    if (this == o) return true;
-    if (!(o instanceof FieldCacheRangeFilter)) return false;
-    FieldCacheRangeFilter other = (FieldCacheRangeFilter) o;
-
-    if (!this.field.equals(other.field)
-        || this.includeLower != other.includeLower
-        || this.includeUpper != other.includeUpper
-    ) { return false; }
-    if (this.lowerVal != null ? !this.lowerVal.equals(other.lowerVal) : other.lowerVal != null) return false;
-    if (this.upperVal != null ? !this.upperVal.equals(other.upperVal) : other.upperVal != null) return false;
-    if (this.parser != null ? !this.parser.equals(other.parser) : other.parser != null) return false;
-    return true;
-  }
-  
-  @Override
-  public final int hashCode() {
-    int h = field.hashCode();
-    h ^= (lowerVal != null) ? lowerVal.hashCode() : 550356204;
-    h = (h << 1) | (h >>> 31);  // rotate to distinguish lower from upper
-    h ^= (upperVal != null) ? upperVal.hashCode() : -1674416163;
-    h ^= (parser != null) ? parser.hashCode() : -1572457324;
-    h ^= (includeLower ? 1549299360 : -365038026) ^ (includeUpper ? 1721088258 : 1948649653);
-    return h;
-  }
-
-  /** Returns the field name for this filter */
-  public String getField() { return field; }
-
-  /** Returns <code>true</code> if the lower endpoint is inclusive */
-  public boolean includesLower() { return includeLower; }
-  
-  /** Returns <code>true</code> if the upper endpoint is inclusive */
-  public boolean includesUpper() { return includeUpper; }
-
-  /** Returns the lower value of this range filter */
-  public T getLowerVal() { return lowerVal; }
-
-  /** Returns the upper value of this range filter */
-  public T getUpperVal() { return upperVal; }
-  
-  /** Returns the current numeric parser ({@code null} for {@code T} is {@code String}} */
-  public FieldCache.Parser getParser() { return parser; }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/FieldCacheRewriteMethod.java lucene5666/lucene/core/src/java/org/apache/lucene/search/FieldCacheRewriteMethod.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/FieldCacheRewriteMethod.java	2014-05-14 03:47:28.774646624 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/FieldCacheRewriteMethod.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,178 +0,0 @@
-package org.apache.lucene.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.LongBitSet;
-
-/**
- * Rewrites MultiTermQueries into a filter, using the FieldCache for term enumeration.
- * <p>
- * This can be used to perform these queries against an unindexed docvalues field.
- * @lucene.experimental
- */
-public final class FieldCacheRewriteMethod extends MultiTermQuery.RewriteMethod {
-  
-  @Override
-  public Query rewrite(IndexReader reader, MultiTermQuery query) {
-    Query result = new ConstantScoreQuery(new MultiTermQueryFieldCacheWrapperFilter(query));
-    result.setBoost(query.getBoost());
-    return result;
-  }
-  
-  static class MultiTermQueryFieldCacheWrapperFilter extends Filter {
-    
-    protected final MultiTermQuery query;
-    
-    /**
-     * Wrap a {@link MultiTermQuery} as a Filter.
-     */
-    protected MultiTermQueryFieldCacheWrapperFilter(MultiTermQuery query) {
-      this.query = query;
-    }
-    
-    @Override
-    public String toString() {
-      // query.toString should be ok for the filter, too, if the query boost is 1.0f
-      return query.toString();
-    }
-    
-    @Override
-    public final boolean equals(final Object o) {
-      if (o==this) return true;
-      if (o==null) return false;
-      if (this.getClass().equals(o.getClass())) {
-        return this.query.equals( ((MultiTermQueryFieldCacheWrapperFilter)o).query );
-      }
-      return false;
-    }
-    
-    @Override
-    public final int hashCode() {
-      return query.hashCode();
-    }
-    
-    /** Returns the field name for this query */
-    public final String getField() { return query.getField(); }
-    
-    /**
-     * Returns a DocIdSet with documents that should be permitted in search
-     * results.
-     */
-    @Override
-    public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
-      final SortedDocValues fcsi = FieldCache.DEFAULT.getTermsIndex(context.reader(), query.field);
-      // Cannot use FixedBitSet because we require long index (ord):
-      final LongBitSet termSet = new LongBitSet(fcsi.getValueCount());
-      TermsEnum termsEnum = query.getTermsEnum(new Terms() {
-        
-        @Override
-        public TermsEnum iterator(TermsEnum reuse) {
-          return fcsi.termsEnum();
-        }
-
-        @Override
-        public long getSumTotalTermFreq() {
-          return -1;
-        }
-
-        @Override
-        public long getSumDocFreq() {
-          return -1;
-        }
-
-        @Override
-        public int getDocCount() {
-          return -1;
-        }
-
-        @Override
-        public long size() {
-          return -1;
-        }
-
-        @Override
-        public boolean hasFreqs() {
-          return false;
-        }
-
-        @Override
-        public boolean hasOffsets() {
-          return false;
-        }
-
-        @Override
-        public boolean hasPositions() {
-          return false;
-        }
-        
-        @Override
-        public boolean hasPayloads() {
-          return false;
-        }
-      });
-      
-      assert termsEnum != null;
-      if (termsEnum.next() != null) {
-        // fill into a bitset
-        do {
-          long ord = termsEnum.ord();
-          if (ord >= 0) {
-            termSet.set(ord);
-          }
-        } while (termsEnum.next() != null);
-      } else {
-        return null;
-      }
-      
-      return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
-        @Override
-        protected final boolean matchDoc(int doc) throws ArrayIndexOutOfBoundsException {
-          int ord = fcsi.getOrd(doc);
-          if (ord == -1) {
-            return false;
-          }
-          return termSet.get(ord);
-        }
-      };
-    }
-  }
-  
-  @Override
-  public boolean equals(Object obj) {
-    if (this == obj)
-      return true;
-    if (obj == null)
-      return false;
-    if (getClass() != obj.getClass())
-      return false;
-    return true;
-  }
-
-  @Override
-  public int hashCode() {
-    return 641;
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/FieldCacheTermsFilter.java lucene5666/lucene/core/src/java/org/apache/lucene/search/FieldCacheTermsFilter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/FieldCacheTermsFilter.java	2014-05-14 03:47:28.770646624 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/FieldCacheTermsFilter.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,143 +0,0 @@
-package org.apache.lucene.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocsEnum; // javadoc @link
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FixedBitSet;
-
-/**
- * A {@link Filter} that only accepts documents whose single
- * term value in the specified field is contained in the
- * provided set of allowed terms.
- * 
- * <p/>
- * 
- * This is the same functionality as TermsFilter (from
- * queries/), except this filter requires that the
- * field contains only a single term for all documents.
- * Because of drastically different implementations, they
- * also have different performance characteristics, as
- * described below.
- * 
- * <p/>
- * 
- * The first invocation of this filter on a given field will
- * be slower, since a {@link SortedDocValues} must be
- * created.  Subsequent invocations using the same field
- * will re-use this cache.  However, as with all
- * functionality based on {@link FieldCache}, persistent RAM
- * is consumed to hold the cache, and is not freed until the
- * {@link IndexReader} is closed.  In contrast, TermsFilter
- * has no persistent RAM consumption.
- * 
- * 
- * <p/>
- * 
- * With each search, this filter translates the specified
- * set of Terms into a private {@link FixedBitSet} keyed by
- * term number per unique {@link IndexReader} (normally one
- * reader per segment).  Then, during matching, the term
- * number for each docID is retrieved from the cache and
- * then checked for inclusion using the {@link FixedBitSet}.
- * Since all testing is done using RAM resident data
- * structures, performance should be very fast, most likely
- * fast enough to not require further caching of the
- * DocIdSet for each possible combination of terms.
- * However, because docIDs are simply scanned linearly, an
- * index with a great many small documents may find this
- * linear scan too costly.
- * 
- * <p/>
- * 
- * In contrast, TermsFilter builds up an {@link FixedBitSet},
- * keyed by docID, every time it's created, by enumerating
- * through all matching docs using {@link DocsEnum} to seek
- * and scan through each term's docID list.  While there is
- * no linear scan of all docIDs, besides the allocation of
- * the underlying array in the {@link FixedBitSet}, this
- * approach requires a number of "disk seeks" in proportion
- * to the number of terms, which can be exceptionally costly
- * when there are cache misses in the OS's IO cache.
- * 
- * <p/>
- * 
- * Generally, this filter will be slower on the first
- * invocation for a given field, but subsequent invocations,
- * even if you change the allowed set of Terms, should be
- * faster than TermsFilter, especially as the number of
- * Terms being matched increases.  If you are matching only
- * a very small number of terms, and those terms in turn
- * match a very small number of documents, TermsFilter may
- * perform faster.
- *
- * <p/>
- *
- * Which filter is best is very application dependent.
- */
-
-public class FieldCacheTermsFilter extends Filter {
-  private String field;
-  private BytesRef[] terms;
-
-  public FieldCacheTermsFilter(String field, BytesRef... terms) {
-    this.field = field;
-    this.terms = terms;
-  }
-
-  public FieldCacheTermsFilter(String field, String... terms) {
-    this.field = field;
-    this.terms = new BytesRef[terms.length];
-    for (int i = 0; i < terms.length; i++)
-      this.terms[i] = new BytesRef(terms[i]);
-  }
-
-  public FieldCache getFieldCache() {
-    return FieldCache.DEFAULT;
-  }
-
-  @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-    final SortedDocValues fcsi = getFieldCache().getTermsIndex(context.reader(), field);
-    final FixedBitSet bits = new FixedBitSet(fcsi.getValueCount());
-    for (int i=0;i<terms.length;i++) {
-      int ord = fcsi.lookupTerm(terms[i]);
-      if (ord >= 0) {
-        bits.set(ord);
-      }
-    }
-    return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
-      @Override
-      protected final boolean matchDoc(int doc) {
-        int ord = fcsi.getOrd(doc);
-        if (ord == -1) {
-          // missing
-          return false;
-        } else {
-          return bits.get(ord);
-        }
-      }
-    };
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/FieldComparator.java lucene5666/lucene/core/src/java/org/apache/lucene/search/FieldComparator.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/FieldComparator.java	2014-05-14 03:47:28.774646624 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/FieldComparator.java	2014-05-12 13:28:36.316244606 -0400
@@ -19,13 +19,12 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.search.FieldCache.DoubleParser;
-import org.apache.lucene.search.FieldCache.FloatParser;
-import org.apache.lucene.search.FieldCache.IntParser;
-import org.apache.lucene.search.FieldCache.LongParser;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 
@@ -82,7 +81,7 @@
  *       when the search is switching to the next segment.
  *       You may need to update internal state of the
  *       comparator, for example retrieving new values from
- *       the {@link FieldCache}.
+ *       DocValues.
  *
  *  <li> {@link #value} Return the sort value stored in
  *       the specified slot.  This is only called at the end
@@ -236,7 +235,7 @@
     @Override
     public FieldComparator<T> setNextReader(AtomicReaderContext context) throws IOException {
       if (missingValue != null) {
-        docsWithField = FieldCache.DEFAULT.getDocsWithField(context.reader(), field);
+        docsWithField = DocValues.getDocsWithField(context.reader(), field);
         // optimization to remove unneeded checks on the bit interface:
         if (docsWithField instanceof Bits.MatchAllBits) {
           docsWithField = null;
@@ -249,18 +248,16 @@
   }
 
   /** Parses field's values as double (using {@link
-   *  FieldCache#getDoubles} and sorts by ascending value */
+   *  AtomicReader#getNumericDocValues} and sorts by ascending value */
   public static final class DoubleComparator extends NumericComparator<Double> {
     private final double[] values;
-    private final DoubleParser parser;
-    private FieldCache.Doubles currentReaderValues;
+    private NumericDocValues currentReaderValues;
     private double bottom;
     private double topValue;
 
-    DoubleComparator(int numHits, String field, FieldCache.Parser parser, Double missingValue) {
+    DoubleComparator(int numHits, String field, Double missingValue) {
       super(field, missingValue);
       values = new double[numHits];
-      this.parser = (DoubleParser) parser;
     }
 
     @Override
@@ -270,7 +267,7 @@
 
     @Override
     public int compareBottom(int doc) {
-      double v2 = currentReaderValues.get(doc);
+      double v2 = Double.longBitsToDouble(currentReaderValues.get(doc));
       // Test for v2 == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && v2 == 0 && !docsWithField.get(doc)) {
@@ -282,7 +279,7 @@
 
     @Override
     public void copy(int slot, int doc) {
-      double v2 = currentReaderValues.get(doc);
+      double v2 = Double.longBitsToDouble(currentReaderValues.get(doc));
       // Test for v2 == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && v2 == 0 && !docsWithField.get(doc)) {
@@ -294,9 +291,7 @@
 
     @Override
     public FieldComparator<Double> setNextReader(AtomicReaderContext context) throws IOException {
-      // NOTE: must do this before calling super otherwise
-      // we compute the docsWithField Bits twice!
-      currentReaderValues = FieldCache.DEFAULT.getDoubles(context.reader(), field, parser, missingValue != null);
+      currentReaderValues = DocValues.getNumeric(context.reader(), field);
       return super.setNextReader(context);
     }
     
@@ -317,7 +312,7 @@
 
     @Override
     public int compareTop(int doc) {
-      double docValue = currentReaderValues.get(doc);
+      double docValue = Double.longBitsToDouble(currentReaderValues.get(doc));
       // Test for docValue == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && docValue == 0 && !docsWithField.get(doc)) {
@@ -328,18 +323,16 @@
   }
 
   /** Parses field's values as float (using {@link
-   *  FieldCache#getFloats} and sorts by ascending value */
+   *  AtomicReader#getNumericDocValues(String)} and sorts by ascending value */
   public static final class FloatComparator extends NumericComparator<Float> {
     private final float[] values;
-    private final FloatParser parser;
-    private FieldCache.Floats currentReaderValues;
+    private NumericDocValues currentReaderValues;
     private float bottom;
     private float topValue;
 
-    FloatComparator(int numHits, String field, FieldCache.Parser parser, Float missingValue) {
+    FloatComparator(int numHits, String field, Float missingValue) {
       super(field, missingValue);
       values = new float[numHits];
-      this.parser = (FloatParser) parser;
     }
     
     @Override
@@ -350,7 +343,7 @@
     @Override
     public int compareBottom(int doc) {
       // TODO: are there sneaky non-branch ways to compute sign of float?
-      float v2 = currentReaderValues.get(doc);
+      float v2 = Float.intBitsToFloat((int)currentReaderValues.get(doc));
       // Test for v2 == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && v2 == 0 && !docsWithField.get(doc)) {
@@ -362,7 +355,7 @@
 
     @Override
     public void copy(int slot, int doc) {
-      float v2 = currentReaderValues.get(doc);
+      float v2 =  Float.intBitsToFloat((int)currentReaderValues.get(doc));
       // Test for v2 == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && v2 == 0 && !docsWithField.get(doc)) {
@@ -374,9 +367,7 @@
 
     @Override
     public FieldComparator<Float> setNextReader(AtomicReaderContext context) throws IOException {
-      // NOTE: must do this before calling super otherwise
-      // we compute the docsWithField Bits twice!
-      currentReaderValues = FieldCache.DEFAULT.getFloats(context.reader(), field, parser, missingValue != null);
+      currentReaderValues = DocValues.getNumeric(context.reader(), field);
       return super.setNextReader(context);
     }
     
@@ -397,7 +388,7 @@
 
     @Override
     public int compareTop(int doc) {
-      float docValue = currentReaderValues.get(doc);
+      float docValue = Float.intBitsToFloat((int)currentReaderValues.get(doc));
       // Test for docValue == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && docValue == 0 && !docsWithField.get(doc)) {
@@ -408,18 +399,16 @@
   }
 
   /** Parses field's values as int (using {@link
-   *  FieldCache#getInts} and sorts by ascending value */
+   *  AtomicReader#getNumericDocValues(String)} and sorts by ascending value */
   public static final class IntComparator extends NumericComparator<Integer> {
     private final int[] values;
-    private final IntParser parser;
-    private FieldCache.Ints currentReaderValues;
+    private NumericDocValues currentReaderValues;
     private int bottom;                           // Value of bottom of queue
     private int topValue;
 
-    IntComparator(int numHits, String field, FieldCache.Parser parser, Integer missingValue) {
+    IntComparator(int numHits, String field, Integer missingValue) {
       super(field, missingValue);
       values = new int[numHits];
-      this.parser = (IntParser) parser;
     }
         
     @Override
@@ -429,7 +418,7 @@
 
     @Override
     public int compareBottom(int doc) {
-      int v2 = currentReaderValues.get(doc);
+      int v2 = (int) currentReaderValues.get(doc);
       // Test for v2 == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && v2 == 0 && !docsWithField.get(doc)) {
@@ -441,7 +430,7 @@
 
     @Override
     public void copy(int slot, int doc) {
-      int v2 = currentReaderValues.get(doc);
+      int v2 = (int) currentReaderValues.get(doc);
       // Test for v2 == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && v2 == 0 && !docsWithField.get(doc)) {
@@ -453,9 +442,7 @@
 
     @Override
     public FieldComparator<Integer> setNextReader(AtomicReaderContext context) throws IOException {
-      // NOTE: must do this before calling super otherwise
-      // we compute the docsWithField Bits twice!
-      currentReaderValues = FieldCache.DEFAULT.getInts(context.reader(), field, parser, missingValue != null);
+      currentReaderValues = DocValues.getNumeric(context.reader(), field);
       return super.setNextReader(context);
     }
     
@@ -476,7 +463,7 @@
 
     @Override
     public int compareTop(int doc) {
-      int docValue = currentReaderValues.get(doc);
+      int docValue = (int) currentReaderValues.get(doc);
       // Test for docValue == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && docValue == 0 && !docsWithField.get(doc)) {
@@ -487,18 +474,16 @@
   }
 
   /** Parses field's values as long (using {@link
-   *  FieldCache#getLongs} and sorts by ascending value */
+   *  AtomicReader#getNumericDocValues(String)} and sorts by ascending value */
   public static final class LongComparator extends NumericComparator<Long> {
     private final long[] values;
-    private final LongParser parser;
-    private FieldCache.Longs currentReaderValues;
+    private NumericDocValues currentReaderValues;
     private long bottom;
     private long topValue;
 
-    LongComparator(int numHits, String field, FieldCache.Parser parser, Long missingValue) {
+    LongComparator(int numHits, String field, Long missingValue) {
       super(field, missingValue);
       values = new long[numHits];
-      this.parser = (LongParser) parser;
     }
 
     @Override
@@ -534,9 +519,7 @@
 
     @Override
     public FieldComparator<Long> setNextReader(AtomicReaderContext context) throws IOException {
-      // NOTE: must do this before calling super otherwise
-      // we compute the docsWithField Bits twice!
-      currentReaderValues = FieldCache.DEFAULT.getLongs(context.reader(), field, parser, missingValue != null);
+      currentReaderValues = DocValues.getNumeric(context.reader(), field);
       return super.setNextReader(context);
     }
     
@@ -712,7 +695,7 @@
    *  ordinals.  This is functionally equivalent to {@link
    *  org.apache.lucene.search.FieldComparator.TermValComparator}, but it first resolves the string
    *  to their relative ordinal positions (using the index
-   *  returned by {@link FieldCache#getTermsIndex}), and
+   *  returned by {@link AtomicReader#getSortedDocValues(String)}), and
    *  does most comparisons using the ordinals.  For medium
    *  to large results, this comparator will be much faster
    *  than {@link org.apache.lucene.search.FieldComparator.TermValComparator}.  For very small
@@ -856,7 +839,7 @@
     
     /** Retrieves the SortedDocValues for the field in this segment */
     protected SortedDocValues getSortedDocValues(AtomicReaderContext context, String field) throws IOException {
-      return FieldCache.DEFAULT.getTermsIndex(context.reader(), field);
+      return DocValues.getSorted(context.reader(), field);
     }
     
     @Override
@@ -1029,8 +1012,8 @@
 
     @Override
     public FieldComparator<BytesRef> setNextReader(AtomicReaderContext context) throws IOException {
-      docTerms = FieldCache.DEFAULT.getTerms(context.reader(), field, true);
-      docsWithField = FieldCache.DEFAULT.getDocsWithField(context.reader(), field);
+      docTerms = DocValues.getBinary(context.reader(), field);
+      docsWithField = DocValues.getDocsWithField(context.reader(), field);
       return this;
     }
     


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/FieldValueFilter.java lucene5666/lucene/core/src/java/org/apache/lucene/search/FieldValueFilter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/FieldValueFilter.java	2014-05-14 03:47:28.774646624 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/FieldValueFilter.java	2014-05-14 03:45:16.618644322 -0400
@@ -18,15 +18,17 @@
  */
 import java.io.IOException;
 
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.Bits.MatchAllBits;
 import org.apache.lucene.util.Bits.MatchNoBits;
 
 /**
  * A {@link Filter} that accepts all documents that have one or more values in a
- * given field. This {@link Filter} request {@link Bits} from the
- * {@link FieldCache} and build the bits if not present.
+ * given field. This {@link Filter} request {@link Bits} from
+ * {@link AtomicReader#getDocsWithField}
  */
 public class FieldValueFilter extends Filter {
   private final String field;
@@ -76,13 +78,13 @@
   @Override
   public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs)
       throws IOException {
-    final Bits docsWithField = FieldCache.DEFAULT.getDocsWithField(
+    final Bits docsWithField = DocValues.getDocsWithField(
         context.reader(), field);
     if (negate) {
       if (docsWithField instanceof MatchAllBits) {
         return null;
       }
-      return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
+      return new DocValuesDocIdSet(context.reader().maxDoc(), acceptDocs) {
         @Override
         protected final boolean matchDoc(int doc) {
           return !docsWithField.get(doc);
@@ -97,7 +99,7 @@
         // :-)
         return BitsFilteredDocIdSet.wrap((DocIdSet) docsWithField, acceptDocs);
       }
-      return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
+      return new DocValuesDocIdSet(context.reader().maxDoc(), acceptDocs) {
         @Override
         protected final boolean matchDoc(int doc) {
           return docsWithField.get(doc);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/FieldValueHitQueue.java lucene5666/lucene/core/src/java/org/apache/lucene/search/FieldValueHitQueue.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/FieldValueHitQueue.java	2014-05-14 03:47:28.770646624 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/FieldValueHitQueue.java	2014-05-12 13:28:36.444244608 -0400
@@ -23,13 +23,10 @@
 
 /**
  * Expert: A hit queue for sorting by hits by terms in more than one field.
- * Uses <code>FieldCache.DEFAULT</code> for maintaining
- * internal term lookup tables.
  * 
  * @lucene.experimental
  * @since 2.9
  * @see IndexSearcher#search(Query,Filter,int,Sort)
- * @see FieldCache
  */
 public abstract class FieldValueHitQueue<T extends FieldValueHitQueue.Entry> extends PriorityQueue<T> {
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/package.html lucene5666/lucene/core/src/java/org/apache/lucene/search/package.html
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/package.html	2014-05-14 03:47:28.770646624 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/package.html	2014-05-14 03:45:16.618644322 -0400
@@ -337,8 +337,8 @@
 <p>
 Finally, you can extend the low level {@link org.apache.lucene.search.similarities.Similarity Similarity} directly
 to implement a new retrieval model, or to use external scoring factors particular to your application. For example,
-a custom Similarity can access per-document values via {@link org.apache.lucene.search.FieldCache FieldCache} or
-{@link org.apache.lucene.index.NumericDocValues} and integrate them into the score.
+a custom Similarity can access per-document values via {@link org.apache.lucene.index.NumericDocValues} and 
+integrate them into the score.
 </p>
 <p>
 See the {@link org.apache.lucene.search.similarities} package documentation for information


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/SortedSetSelector.java lucene5666/lucene/core/src/java/org/apache/lucene/search/SortedSetSelector.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/SortedSetSelector.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/SortedSetSelector.java	2014-05-13 00:36:40.960942658 -0400
@@ -0,0 +1,228 @@
+package org.apache.lucene.search;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.RandomAccessOrds;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.util.BytesRef;
+
+/** Selects a value from the document's set to use as the representative value */
+public class SortedSetSelector {
+  
+  /** 
+   * Type of selection to perform.
+   * <p>
+   * Limitations:
+   * <ul>
+   *   <li>Fields containing {@link Integer#MAX_VALUE} or more unique values
+   *       are unsupported.
+   *   <li>Selectors other than ({@link Type#MIN}) require 
+   *       optional codec support. However several codecs provided by Lucene, 
+   *       including the current default codec, support this.
+   * </ul>
+   */
+  public enum Type {
+    /** 
+     * Selects the minimum value in the set 
+     */
+    MIN,
+    /** 
+     * Selects the maximum value in the set 
+     */
+    MAX,
+    /** 
+     * Selects the middle value in the set.
+     * <p>
+     * If the set has an even number of values, the lower of the middle two is chosen.
+     */
+    MIDDLE_MIN,
+    /** 
+     * Selects the middle value in the set.
+     * <p>
+     * If the set has an even number of values, the higher of the middle two is chosen
+     */
+    MIDDLE_MAX
+  }
+  
+  /** Wraps a multi-valued SortedSetDocValues as a single-valued view, using the specified selector */
+  public static SortedDocValues wrap(SortedSetDocValues sortedSet, Type selector) {
+    if (sortedSet.getValueCount() >= Integer.MAX_VALUE) {
+      throw new UnsupportedOperationException("fields containing more than " + (Integer.MAX_VALUE-1) + " unique terms are unsupported");
+    }
+    
+    SortedDocValues singleton = DocValues.unwrapSingleton(sortedSet);
+    if (singleton != null) {
+      // it's actually single-valued in practice, but indexed as multi-valued,
+      // so just sort on the underlying single-valued dv directly.
+      // regardless of selector type, this optimization is safe!
+      return singleton;
+    } else if (selector == Type.MIN) {
+      return new MinValue(sortedSet);
+    } else {
+      if (sortedSet instanceof RandomAccessOrds == false) {
+        throw new UnsupportedOperationException("codec does not support random access ordinals, cannot use selector: " + selector);
+      }
+      RandomAccessOrds randomOrds = (RandomAccessOrds) sortedSet;
+      switch(selector) {
+        case MAX: return new MaxValue(randomOrds);
+        case MIDDLE_MIN: return new MiddleMinValue(randomOrds);
+        case MIDDLE_MAX: return new MiddleMaxValue(randomOrds);
+        case MIN: 
+        default: 
+          throw new AssertionError();
+      }
+    }
+  }
+  
+  /** Wraps a SortedSetDocValues and returns the first ordinal (min) */
+  static class MinValue extends SortedDocValues {
+    final SortedSetDocValues in;
+    
+    MinValue(SortedSetDocValues in) {
+      this.in = in;
+    }
+
+    @Override
+    public int getOrd(int docID) {
+      in.setDocument(docID);
+      return (int) in.nextOrd();
+    }
+
+    @Override
+    public void lookupOrd(int ord, BytesRef result) {
+      in.lookupOrd(ord, result);
+    }
+
+    @Override
+    public int getValueCount() {
+      return (int) in.getValueCount();
+    }
+
+    @Override
+    public int lookupTerm(BytesRef key) {
+      return (int) in.lookupTerm(key);
+    }
+  }
+  
+  /** Wraps a SortedSetDocValues and returns the last ordinal (max) */
+  static class MaxValue extends SortedDocValues {
+    final RandomAccessOrds in;
+    
+    MaxValue(RandomAccessOrds in) {
+      this.in = in;
+    }
+
+    @Override
+    public int getOrd(int docID) {
+      in.setDocument(docID);
+      final int count = in.cardinality();
+      if (count == 0) {
+        return -1;
+      } else {
+        return (int) in.ordAt(count-1);
+      }
+    }
+
+    @Override
+    public void lookupOrd(int ord, BytesRef result) {
+      in.lookupOrd(ord, result);
+    }
+
+    @Override
+    public int getValueCount() {
+      return (int) in.getValueCount();
+    }
+    
+    @Override
+    public int lookupTerm(BytesRef key) {
+      return (int) in.lookupTerm(key);
+    }
+  }
+  
+  /** Wraps a SortedSetDocValues and returns the middle ordinal (or min of the two) */
+  static class MiddleMinValue extends SortedDocValues {
+    final RandomAccessOrds in;
+    
+    MiddleMinValue(RandomAccessOrds in) {
+      this.in = in;
+    }
+
+    @Override
+    public int getOrd(int docID) {
+      in.setDocument(docID);
+      final int count = in.cardinality();
+      if (count == 0) {
+        return -1;
+      } else {
+        return (int) in.ordAt((count-1) >>> 1);
+      }
+    }
+
+    @Override
+    public void lookupOrd(int ord, BytesRef result) {
+      in.lookupOrd(ord, result);
+    }
+
+    @Override
+    public int getValueCount() {
+      return (int) in.getValueCount();
+    }
+    
+    @Override
+    public int lookupTerm(BytesRef key) {
+      return (int) in.lookupTerm(key);
+    }
+  }
+  
+  /** Wraps a SortedSetDocValues and returns the middle ordinal (or max of the two) */
+  static class MiddleMaxValue extends SortedDocValues {
+    final RandomAccessOrds in;
+    
+    MiddleMaxValue(RandomAccessOrds in) {
+      this.in = in;
+    }
+
+    @Override
+    public int getOrd(int docID) {
+      in.setDocument(docID);
+      final int count = in.cardinality();
+      if (count == 0) {
+        return -1;
+      } else {
+        return (int) in.ordAt(count >>> 1);
+      }
+    }
+
+    @Override
+    public void lookupOrd(int ord, BytesRef result) {
+      in.lookupOrd(ord, result);
+    }
+
+    @Override
+    public int getValueCount() {
+      return (int) in.getValueCount();
+    }
+    
+    @Override
+    public int lookupTerm(BytesRef key) {
+      return (int) in.lookupTerm(key);
+    }
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/SortedSetSortField.java lucene5666/lucene/core/src/java/org/apache/lucene/search/SortedSetSortField.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/SortedSetSortField.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/SortedSetSortField.java	2014-05-13 00:37:37.048943634 -0400
@@ -0,0 +1,133 @@
+package org.apache.lucene.search;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.search.FieldComparator;
+import org.apache.lucene.search.SortField;
+
+/** 
+ * SortField for {@link SortedSetDocValues}.
+ * <p>
+ * A SortedSetDocValues contains multiple values for a field, so sorting with
+ * this technique "selects" a value as the representative sort value for the document.
+ * <p>
+ * By default, the minimum value in the set is selected as the sort value, but
+ * this can be customized. Selectors other than the default do have some limitations
+ * to ensure that all selections happen in constant-time for performance.
+ * <p>
+ * Like sorting by string, this also supports sorting missing values as first or last,
+ * via {@link #setMissingValue(Object)}.
+ * <p>
+ * @see SortedSetSelector
+ */
+public class SortedSetSortField extends SortField {
+  
+  private final SortedSetSelector.Type selector;
+  
+  /**
+   * Creates a sort, possibly in reverse, by the minimum value in the set 
+   * for the document.
+   * @param field Name of field to sort by.  Must not be null.
+   * @param reverse True if natural order should be reversed.
+   */
+  public SortedSetSortField(String field, boolean reverse) {
+    this(field, reverse, SortedSetSelector.Type.MIN);
+  }
+
+  /**
+   * Creates a sort, possibly in reverse, specifying how the sort value from 
+   * the document's set is selected.
+   * @param field Name of field to sort by.  Must not be null.
+   * @param reverse True if natural order should be reversed.
+   * @param selector custom selector type for choosing the sort value from the set.
+   * <p>
+   * NOTE: selectors other than {@link SortedSetSelector.Type#MIN} require optional codec support.
+   */
+  public SortedSetSortField(String field, boolean reverse, SortedSetSelector.Type selector) {
+    super(field, SortField.Type.CUSTOM, reverse);
+    if (selector == null) {
+      throw new NullPointerException();
+    }
+    this.selector = selector;
+  }
+  
+  /** Returns the selector in use for this sort */
+  public SortedSetSelector.Type getSelector() {
+    return selector;
+  }
+
+  @Override
+  public int hashCode() {
+    return 31 * super.hashCode() + selector.hashCode();
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (this == obj) return true;
+    if (!super.equals(obj)) return false;
+    if (getClass() != obj.getClass()) return false;
+    SortedSetSortField other = (SortedSetSortField) obj;
+    if (selector != other.selector) return false;
+    return true;
+  }
+  
+  @Override
+  public String toString() {
+    StringBuilder buffer = new StringBuilder();
+    buffer.append("<sortedset" + ": \"").append(getField()).append("\">");
+    if (getReverse()) buffer.append('!');
+    if (missingValue != null) {
+      buffer.append(" missingValue=");
+      buffer.append(missingValue);
+    }
+    buffer.append(" selector=");
+    buffer.append(selector);
+
+    return buffer.toString();
+  }
+
+  /**
+   * Set how missing values (the empty set) are sorted.
+   * <p>
+   * Note that this must be {@link #STRING_FIRST} or {@link #STRING_LAST}.
+   */
+  @Override
+  public void setMissingValue(Object missingValue) {
+    if (missingValue != STRING_FIRST && missingValue != STRING_LAST) {
+      throw new IllegalArgumentException("For SORTED_SET type, missing value must be either STRING_FIRST or STRING_LAST");
+    }
+    this.missingValue = missingValue;
+  }
+  
+  @Override
+  public FieldComparator<?> getComparator(int numHits, int sortPos) throws IOException {
+    return new FieldComparator.TermOrdValComparator(numHits, getField(), missingValue == STRING_LAST) {
+      @Override
+      protected SortedDocValues getSortedDocValues(AtomicReaderContext context, String field) throws IOException {
+        SortedSetDocValues sortedSet = DocValues.getSortedSet(context.reader(), field);
+        return SortedSetSelector.wrap(sortedSet, selector);
+      }
+    };
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/SortField.java lucene5666/lucene/core/src/java/org/apache/lucene/search/SortField.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/SortField.java	2014-05-14 03:47:28.770646624 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/SortField.java	2014-05-12 13:28:36.316244606 -0400
@@ -94,7 +94,6 @@
   private String field;
   private Type type;  // defaults to determining type dynamically
   boolean reverse = false;  // defaults to natural order
-  private FieldCache.Parser parser;
 
   // Used for CUSTOM sort
   private FieldComparatorSource comparatorSource;
@@ -124,44 +123,6 @@
     this.reverse = reverse;
   }
 
-  /** Creates a sort by terms in the given field, parsed
-   * to numeric values using a custom {@link FieldCache.Parser}.
-   * @param field  Name of field to sort by.  Must not be null.
-   * @param parser Instance of a {@link FieldCache.Parser},
-   *  which must subclass one of the existing numeric
-   *  parsers from {@link FieldCache}. Sort type is inferred
-   *  by testing which numeric parser the parser subclasses.
-   * @throws IllegalArgumentException if the parser fails to
-   *  subclass an existing numeric parser, or field is null
-   */
-  public SortField(String field, FieldCache.Parser parser) {
-    this(field, parser, false);
-  }
-
-  /** Creates a sort, possibly in reverse, by terms in the given field, parsed
-   * to numeric values using a custom {@link FieldCache.Parser}.
-   * @param field  Name of field to sort by.  Must not be null.
-   * @param parser Instance of a {@link FieldCache.Parser},
-   *  which must subclass one of the existing numeric
-   *  parsers from {@link FieldCache}. Sort type is inferred
-   *  by testing which numeric parser the parser subclasses.
-   * @param reverse True if natural order should be reversed.
-   * @throws IllegalArgumentException if the parser fails to
-   *  subclass an existing numeric parser, or field is null
-   */
-  public SortField(String field, FieldCache.Parser parser, boolean reverse) {
-    if (parser instanceof FieldCache.IntParser) initFieldType(field, Type.INT);
-    else if (parser instanceof FieldCache.FloatParser) initFieldType(field, Type.FLOAT);
-    else if (parser instanceof FieldCache.LongParser) initFieldType(field, Type.LONG);
-    else if (parser instanceof FieldCache.DoubleParser) initFieldType(field, Type.DOUBLE);
-    else {
-      throw new IllegalArgumentException("Parser instance does not subclass existing numeric parser from FieldCache (got " + parser + ")");
-    }
-
-    this.reverse = reverse;
-    this.parser = parser;
-  }
-
   /** Pass this to {@link #setMissingValue} to have missing
    *  string values sort first. */
   public final static Object STRING_FIRST = new Object() {
@@ -239,14 +200,6 @@
     return type;
   }
 
-  /** Returns the instance of a {@link FieldCache} parser that fits to the given sort type.
-   * May return <code>null</code> if no parser was specified. Sorting is using the default parser then.
-   * @return An instance of a {@link FieldCache} parser, or <code>null</code>.
-   */
-  public FieldCache.Parser getParser() {
-    return parser;
-  }
-
   /** Returns whether the sort should be reversed.
    * @return  True if natural order should be reversed.
    */
@@ -320,8 +273,7 @@
   }
 
   /** Returns true if <code>o</code> is equal to this.  If a
-   *  {@link FieldComparatorSource} or {@link
-   *  FieldCache.Parser} was provided, it must properly
+   *  {@link FieldComparatorSource} was provided, it must properly
    *  implement equals (unless a singleton is always used). */
   @Override
   public boolean equals(Object o) {
@@ -337,8 +289,7 @@
   }
 
   /** Returns true if <code>o</code> is equal to this.  If a
-   *  {@link FieldComparatorSource} or {@link
-   *  FieldCache.Parser} was provided, it must properly
+   *  {@link FieldComparatorSource} was provided, it must properly
    *  implement hashCode (unless a singleton is always
    *  used). */
   @Override
@@ -381,16 +332,16 @@
       return new FieldComparator.DocComparator(numHits);
 
     case INT:
-      return new FieldComparator.IntComparator(numHits, field, parser, (Integer) missingValue);
+      return new FieldComparator.IntComparator(numHits, field, (Integer) missingValue);
 
     case FLOAT:
-      return new FieldComparator.FloatComparator(numHits, field, parser, (Float) missingValue);
+      return new FieldComparator.FloatComparator(numHits, field, (Float) missingValue);
 
     case LONG:
-      return new FieldComparator.LongComparator(numHits, field, parser, (Long) missingValue);
+      return new FieldComparator.LongComparator(numHits, field, (Long) missingValue);
 
     case DOUBLE:
-      return new FieldComparator.DoubleComparator(numHits, field, parser, (Double) missingValue);
+      return new FieldComparator.DoubleComparator(numHits, field, (Double) missingValue);
 
     case CUSTOM:
       assert comparatorSource != null;


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/search/TermRangeFilter.java lucene5666/lucene/core/src/java/org/apache/lucene/search/TermRangeFilter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/search/TermRangeFilter.java	2014-05-14 03:47:28.774646624 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/search/TermRangeFilter.java	2014-05-14 03:45:16.618644322 -0400
@@ -29,7 +29,7 @@
  * for numerical ranges; use {@link NumericRangeFilter} instead.
  *
  * <p>If you construct a large number of range filters with different ranges but on the 
- * same field, {@link FieldCacheRangeFilter} may have significantly better performance. 
+ * same field, {@link DocValuesRangeFilter} may have significantly better performance. 
  * @since 2.9
  */
 public class TermRangeFilter extends MultiTermQueryWrapperFilter<TermRangeQuery> {


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/util/FieldCacheSanityChecker.java lucene5666/lucene/core/src/java/org/apache/lucene/util/FieldCacheSanityChecker.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/util/FieldCacheSanityChecker.java	2014-05-14 03:47:28.754646623 -0400
+++ lucene5666/lucene/core/src/java/org/apache/lucene/util/FieldCacheSanityChecker.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,442 +0,0 @@
-package org.apache.lucene.util;
-/**
- * Copyright 2009 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.index.CompositeReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReaderContext;
-import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.search.FieldCache.CacheEntry;
-import org.apache.lucene.store.AlreadyClosedException;
-
-/** 
- * Provides methods for sanity checking that entries in the FieldCache 
- * are not wasteful or inconsistent.
- * </p>
- * <p>
- * Lucene 2.9 Introduced numerous enhancements into how the FieldCache 
- * is used by the low levels of Lucene searching (for Sorting and 
- * ValueSourceQueries) to improve both the speed for Sorting, as well 
- * as reopening of IndexReaders.  But these changes have shifted the 
- * usage of FieldCache from "top level" IndexReaders (frequently a 
- * MultiReader or DirectoryReader) down to the leaf level SegmentReaders.  
- * As a result, existing applications that directly access the FieldCache 
- * may find RAM usage increase significantly when upgrading to 2.9 or 
- * Later.  This class provides an API for these applications (or their 
- * Unit tests) to check at run time if the FieldCache contains "insane" 
- * usages of the FieldCache.
- * </p>
- * @lucene.experimental
- * @see FieldCache
- * @see FieldCacheSanityChecker.Insanity
- * @see FieldCacheSanityChecker.InsanityType
- */
-public final class FieldCacheSanityChecker {
-
-  private boolean estimateRam;
-
-  public FieldCacheSanityChecker() {
-    /* NOOP */
-  }
-
-  /**
-   * If set, estimate size for all CacheEntry objects will be calculateed.
-   */
-  public void setRamUsageEstimator(boolean flag) {
-    estimateRam = flag;
-  }
-
-
-  /** 
-   * Quick and dirty convenience method
-   * @see #check
-   */
-  public static Insanity[] checkSanity(FieldCache cache) {
-    return checkSanity(cache.getCacheEntries());
-  }
-
-  /** 
-   * Quick and dirty convenience method that instantiates an instance with 
-   * "good defaults" and uses it to test the CacheEntrys
-   * @see #check
-   */
-  public static Insanity[] checkSanity(CacheEntry... cacheEntries) {
-    FieldCacheSanityChecker sanityChecker = new FieldCacheSanityChecker();
-    sanityChecker.setRamUsageEstimator(true);
-    return sanityChecker.check(cacheEntries);
-  }
-
-
-  /**
-   * Tests a CacheEntry[] for indication of "insane" cache usage.
-   * <p>
-   * <B>NOTE:</b>FieldCache CreationPlaceholder objects are ignored.
-   * (:TODO: is this a bad idea? are we masking a real problem?)
-   * </p>
-   */
-  public Insanity[] check(CacheEntry... cacheEntries) {
-    if (null == cacheEntries || 0 == cacheEntries.length) 
-      return new Insanity[0];
-
-    if (estimateRam) {
-      for (int i = 0; i < cacheEntries.length; i++) {
-        cacheEntries[i].estimateSize();
-      }
-    }
-
-    // the indirect mapping lets MapOfSet dedup identical valIds for us
-    //
-    // maps the (valId) identityhashCode of cache values to 
-    // sets of CacheEntry instances
-    final MapOfSets<Integer, CacheEntry> valIdToItems = new MapOfSets<>(new HashMap<Integer, Set<CacheEntry>>(17));
-    // maps ReaderField keys to Sets of ValueIds
-    final MapOfSets<ReaderField, Integer> readerFieldToValIds = new MapOfSets<>(new HashMap<ReaderField, Set<Integer>>(17));
-    //
-
-    // any keys that we know result in more then one valId
-    final Set<ReaderField> valMismatchKeys = new HashSet<>();
-
-    // iterate over all the cacheEntries to get the mappings we'll need
-    for (int i = 0; i < cacheEntries.length; i++) {
-      final CacheEntry item = cacheEntries[i];
-      final Object val = item.getValue();
-
-      // It's OK to have dup entries, where one is eg
-      // float[] and the other is the Bits (from
-      // getDocWithField())
-      if (val instanceof Bits) {
-        continue;
-      }
-
-      if (val instanceof FieldCache.CreationPlaceholder)
-        continue;
-
-      final ReaderField rf = new ReaderField(item.getReaderKey(), 
-                                            item.getFieldName());
-
-      final Integer valId = Integer.valueOf(System.identityHashCode(val));
-
-      // indirect mapping, so the MapOfSet will dedup identical valIds for us
-      valIdToItems.put(valId, item);
-      if (1 < readerFieldToValIds.put(rf, valId)) {
-        valMismatchKeys.add(rf);
-      }
-    }
-
-    final List<Insanity> insanity = new ArrayList<>(valMismatchKeys.size() * 3);
-
-    insanity.addAll(checkValueMismatch(valIdToItems, 
-                                       readerFieldToValIds, 
-                                       valMismatchKeys));
-    insanity.addAll(checkSubreaders(valIdToItems, 
-                                    readerFieldToValIds));
-                    
-    return insanity.toArray(new Insanity[insanity.size()]);
-  }
-
-  /** 
-   * Internal helper method used by check that iterates over 
-   * valMismatchKeys and generates a Collection of Insanity 
-   * instances accordingly.  The MapOfSets are used to populate 
-   * the Insanity objects. 
-   * @see InsanityType#VALUEMISMATCH
-   */
-  private Collection<Insanity> checkValueMismatch(MapOfSets<Integer, CacheEntry> valIdToItems,
-                                        MapOfSets<ReaderField, Integer> readerFieldToValIds,
-                                        Set<ReaderField> valMismatchKeys) {
-
-    final List<Insanity> insanity = new ArrayList<>(valMismatchKeys.size() * 3);
-
-    if (! valMismatchKeys.isEmpty() ) { 
-      // we have multiple values for some ReaderFields
-
-      final Map<ReaderField, Set<Integer>> rfMap = readerFieldToValIds.getMap();
-      final Map<Integer, Set<CacheEntry>> valMap = valIdToItems.getMap();
-      for (final ReaderField rf : valMismatchKeys) {
-        final List<CacheEntry> badEntries = new ArrayList<>(valMismatchKeys.size() * 2);
-        for(final Integer value: rfMap.get(rf)) {
-          for (final CacheEntry cacheEntry : valMap.get(value)) {
-            badEntries.add(cacheEntry);
-          }
-        }
-
-        CacheEntry[] badness = new CacheEntry[badEntries.size()];
-        badness = badEntries.toArray(badness);
-
-        insanity.add(new Insanity(InsanityType.VALUEMISMATCH,
-                                  "Multiple distinct value objects for " + 
-                                  rf.toString(), badness));
-      }
-    }
-    return insanity;
-  }
-
-  /** 
-   * Internal helper method used by check that iterates over 
-   * the keys of readerFieldToValIds and generates a Collection 
-   * of Insanity instances whenever two (or more) ReaderField instances are 
-   * found that have an ancestry relationships.  
-   *
-   * @see InsanityType#SUBREADER
-   */
-  private Collection<Insanity> checkSubreaders( MapOfSets<Integer, CacheEntry>  valIdToItems,
-                                      MapOfSets<ReaderField, Integer> readerFieldToValIds) {
-
-    final List<Insanity> insanity = new ArrayList<>(23);
-
-    Map<ReaderField, Set<ReaderField>> badChildren = new HashMap<>(17);
-    MapOfSets<ReaderField, ReaderField> badKids = new MapOfSets<>(badChildren); // wrapper
-
-    Map<Integer, Set<CacheEntry>> viToItemSets = valIdToItems.getMap();
-    Map<ReaderField, Set<Integer>> rfToValIdSets = readerFieldToValIds.getMap();
-
-    Set<ReaderField> seen = new HashSet<>(17);
-
-    Set<ReaderField> readerFields = rfToValIdSets.keySet();
-    for (final ReaderField rf : readerFields) {
-      
-      if (seen.contains(rf)) continue;
-
-      List<Object> kids = getAllDescendantReaderKeys(rf.readerKey);
-      for (Object kidKey : kids) {
-        ReaderField kid = new ReaderField(kidKey, rf.fieldName);
-        
-        if (badChildren.containsKey(kid)) {
-          // we've already process this kid as RF and found other problems
-          // track those problems as our own
-          badKids.put(rf, kid);
-          badKids.putAll(rf, badChildren.get(kid));
-          badChildren.remove(kid);
-          
-        } else if (rfToValIdSets.containsKey(kid)) {
-          // we have cache entries for the kid
-          badKids.put(rf, kid);
-        }
-        seen.add(kid);
-      }
-      seen.add(rf);
-    }
-
-    // every mapping in badKids represents an Insanity
-    for (final ReaderField parent : badChildren.keySet()) {
-      Set<ReaderField> kids = badChildren.get(parent);
-
-      List<CacheEntry> badEntries = new ArrayList<>(kids.size() * 2);
-
-      // put parent entr(ies) in first
-      {
-        for (final Integer value  : rfToValIdSets.get(parent)) {
-          badEntries.addAll(viToItemSets.get(value));
-        }
-      }
-
-      // now the entries for the descendants
-      for (final ReaderField kid : kids) {
-        for (final Integer value : rfToValIdSets.get(kid)) {
-          badEntries.addAll(viToItemSets.get(value));
-        }
-      }
-
-      CacheEntry[] badness = new CacheEntry[badEntries.size()];
-      badness = badEntries.toArray(badness);
-
-      insanity.add(new Insanity(InsanityType.SUBREADER,
-                                "Found caches for descendants of " + 
-                                parent.toString(),
-                                badness));
-    }
-
-    return insanity;
-
-  }
-
-  /**
-   * Checks if the seed is an IndexReader, and if so will walk
-   * the hierarchy of subReaders building up a list of the objects 
-   * returned by {@code seed.getCoreCacheKey()}
-   */
-  private List<Object> getAllDescendantReaderKeys(Object seed) {
-    List<Object> all = new ArrayList<>(17); // will grow as we iter
-    all.add(seed);
-    for (int i = 0; i < all.size(); i++) {
-      final Object obj = all.get(i);
-      // TODO: We don't check closed readers here (as getTopReaderContext
-      // throws AlreadyClosedException), what should we do? Reflection?
-      if (obj instanceof IndexReader) {
-        try {
-          final List<IndexReaderContext> childs =
-            ((IndexReader) obj).getContext().children();
-          if (childs != null) { // it is composite reader
-            for (final IndexReaderContext ctx : childs) {
-              all.add(ctx.reader().getCoreCacheKey());
-            }
-          }
-        } catch (AlreadyClosedException ace) {
-          // ignore this reader
-        }
-      }
-    }
-    // need to skip the first, because it was the seed
-    return all.subList(1, all.size());
-  }
-
-  /**
-   * Simple pair object for using "readerKey + fieldName" a Map key
-   */
-  private final static class ReaderField {
-    public final Object readerKey;
-    public final String fieldName;
-    public ReaderField(Object readerKey, String fieldName) {
-      this.readerKey = readerKey;
-      this.fieldName = fieldName;
-    }
-    @Override
-    public int hashCode() {
-      return System.identityHashCode(readerKey) * fieldName.hashCode();
-    }
-    @Override
-    public boolean equals(Object that) {
-      if (! (that instanceof ReaderField)) return false;
-
-      ReaderField other = (ReaderField) that;
-      return (this.readerKey == other.readerKey &&
-              this.fieldName.equals(other.fieldName));
-    }
-    @Override
-    public String toString() {
-      return readerKey.toString() + "+" + fieldName;
-    }
-  }
-
-  /**
-   * Simple container for a collection of related CacheEntry objects that 
-   * in conjunction with each other represent some "insane" usage of the 
-   * FieldCache.
-   */
-  public final static class Insanity {
-    private final InsanityType type;
-    private final String msg;
-    private final CacheEntry[] entries;
-    public Insanity(InsanityType type, String msg, CacheEntry... entries) {
-      if (null == type) {
-        throw new IllegalArgumentException
-          ("Insanity requires non-null InsanityType");
-      }
-      if (null == entries || 0 == entries.length) {
-        throw new IllegalArgumentException
-          ("Insanity requires non-null/non-empty CacheEntry[]");
-      }
-      this.type = type;
-      this.msg = msg;
-      this.entries = entries;
-      
-    }
-    /**
-     * Type of insane behavior this object represents
-     */
-    public InsanityType getType() { return type; }
-    /**
-     * Description of hte insane behavior
-     */
-    public String getMsg() { return msg; }
-    /**
-     * CacheEntry objects which suggest a problem
-     */
-    public CacheEntry[] getCacheEntries() { return entries; }
-    /**
-     * Multi-Line representation of this Insanity object, starting with 
-     * the Type and Msg, followed by each CacheEntry.toString() on it's 
-     * own line prefaced by a tab character
-     */
-    @Override
-    public String toString() {
-      StringBuilder buf = new StringBuilder();
-      buf.append(getType()).append(": ");
-
-      String m = getMsg();
-      if (null != m) buf.append(m);
-
-      buf.append('\n');
-
-      CacheEntry[] ce = getCacheEntries();
-      for (int i = 0; i < ce.length; i++) {
-        buf.append('\t').append(ce[i].toString()).append('\n');
-      }
-
-      return buf.toString();
-    }
-  }
-
-  /**
-   * An Enumeration of the different types of "insane" behavior that 
-   * may be detected in a FieldCache.
-   *
-   * @see InsanityType#SUBREADER
-   * @see InsanityType#VALUEMISMATCH
-   * @see InsanityType#EXPECTED
-   */
-  public final static class InsanityType {
-    private final String label;
-    private InsanityType(final String label) {
-      this.label = label;
-    }
-    @Override
-    public String toString() { return label; }
-
-    /** 
-     * Indicates an overlap in cache usage on a given field 
-     * in sub/super readers.
-     */
-    public final static InsanityType SUBREADER 
-      = new InsanityType("SUBREADER");
-
-    /** 
-     * <p>
-     * Indicates entries have the same reader+fieldname but 
-     * different cached values.  This can happen if different datatypes, 
-     * or parsers are used -- and while it's not necessarily a bug 
-     * it's typically an indication of a possible problem.
-     * </p>
-     * <p>
-     * <b>NOTE:</b> Only the reader, fieldname, and cached value are actually 
-     * tested -- if two cache entries have different parsers or datatypes but 
-     * the cached values are the same Object (== not just equal()) this method 
-     * does not consider that a red flag.  This allows for subtle variations 
-     * in the way a Parser is specified (null vs DEFAULT_LONG_PARSER, etc...)
-     * </p>
-     */
-    public final static InsanityType VALUEMISMATCH 
-      = new InsanityType("VALUEMISMATCH");
-
-    /** 
-     * Indicates an expected bit of "insanity".  This may be useful for 
-     * clients that wish to preserve/log information about insane usage 
-     * but indicate that it was expected. 
-     */
-    public final static InsanityType EXPECTED
-      = new InsanityType("EXPECTED");
-  }
-  
-  
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java lucene5666/lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	2014-05-14 03:47:28.706646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	2014-05-14 03:45:16.582644322 -0400
@@ -46,7 +46,6 @@
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.NumericRangeQuery;
 import org.apache.lucene.search.ScoreDoc;
@@ -64,6 +63,7 @@
 import org.apache.lucene.util.InfoStream;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
+import org.apache.lucene.util.NumericUtils;
 import org.apache.lucene.util.StringHelper;
 import org.apache.lucene.util.TestUtil;
 import org.junit.AfterClass;
@@ -819,17 +819,18 @@
       hits = searcher.search(NumericRangeQuery.newLongRange("trieLong", 4, Long.MIN_VALUE, Long.MAX_VALUE, false, false), 100).scoreDocs;
       assertEquals("wrong number of hits", 34, hits.length);
       
-      // check decoding into field cache
-      FieldCache.Ints fci = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(searcher.getIndexReader()), "trieInt", false);
-      int maxDoc = searcher.getIndexReader().maxDoc();
-      for(int doc=0;doc<maxDoc;doc++) {
-        int val = fci.get(doc);
+      // check decoding of terms
+      Terms terms = MultiFields.getTerms(searcher.getIndexReader(), "trieInt");
+      TermsEnum termsEnum = NumericUtils.filterPrefixCodedInts(terms.iterator(null));
+      while (termsEnum.next() != null) {
+        int val = NumericUtils.prefixCodedToInt(termsEnum.term());
         assertTrue("value in id bounds", val >= 0 && val < 35);
       }
       
-      FieldCache.Longs fcl = FieldCache.DEFAULT.getLongs(SlowCompositeReaderWrapper.wrap(searcher.getIndexReader()), "trieLong", false);
-      for(int doc=0;doc<maxDoc;doc++) {
-        long val = fcl.get(doc);
+      terms = MultiFields.getTerms(searcher.getIndexReader(), "trieLong");
+      termsEnum = NumericUtils.filterPrefixCodedLongs(terms.iterator(null));
+      while (termsEnum.next() != null) {
+        long val = NumericUtils.prefixCodedToLong(termsEnum.term());
         assertTrue("value in id bounds", val >= 0L && val < 35L);
       }
       


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java lucene5666/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java	2014-05-14 03:47:28.706646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java	2014-05-12 13:28:35.092244584 -0400
@@ -32,13 +32,11 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.IntField;
 import org.apache.lucene.document.StoredField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.NoSuchDirectoryException;
 import org.apache.lucene.util.Bits;
@@ -753,44 +751,6 @@
     dir.close();
   }
   
-  // LUCENE-1579: Ensure that on a reopened reader, that any
-  // shared segments reuse the doc values arrays in
-  // FieldCache
-  public void testFieldCacheReuseAfterReopen() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriter writer = new IndexWriter(
-        dir,
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).
-            setMergePolicy(newLogMergePolicy(10))
-    );
-    Document doc = new Document();
-    doc.add(new IntField("number", 17, Field.Store.NO));
-    writer.addDocument(doc);
-    writer.commit();
-  
-    // Open reader1
-    DirectoryReader r = DirectoryReader.open(dir);
-    AtomicReader r1 = getOnlySegmentReader(r);
-    final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(r1, "number", false);
-    assertEquals(17, ints.get(0));
-  
-    // Add new segment
-    writer.addDocument(doc);
-    writer.commit();
-  
-    // Reopen reader1 --> reader2
-    DirectoryReader r2 = DirectoryReader.openIfChanged(r);
-    assertNotNull(r2);
-    r.close();
-    AtomicReader sub0 = r2.leaves().get(0).reader();
-    final FieldCache.Ints ints2 = FieldCache.DEFAULT.getInts(sub0, "number", false);
-    r2.close();
-    assertTrue(ints == ints2);
-  
-    writer.shutdown();
-    dir.close();
-  }
-  
   // LUCENE-1586: getUniqueTermCount
   public void testUniqueTermCount() throws Exception {
     Directory dir = newDirectory();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestDocTermOrds.java lucene5666/lucene/core/src/test/org/apache/lucene/index/TestDocTermOrds.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestDocTermOrds.java	2014-05-14 03:47:28.706646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/index/TestDocTermOrds.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,481 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.List;
-import java.util.ArrayList;
-import java.util.HashSet;
-import java.util.Set;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.IntField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.TermsEnum.SeekStatus;
-import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.TestUtil;
-
-// TODO:
-//   - test w/ del docs
-//   - test prefix
-//   - test w/ cutoff
-//   - crank docs way up so we get some merging sometimes
-
-public class TestDocTermOrds extends LuceneTestCase {
-
-  public void testSimple() throws Exception {
-    Directory dir = newDirectory();
-    final RandomIndexWriter w = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
-    Document doc = new Document();
-    Field field = newTextField("field", "", Field.Store.NO);
-    doc.add(field);
-    field.setStringValue("a b c");
-    w.addDocument(doc);
-
-    field.setStringValue("d e f");
-    w.addDocument(doc);
-
-    field.setStringValue("a f");
-    w.addDocument(doc);
-    
-    final IndexReader r = w.getReader();
-    w.shutdown();
-
-    final AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);
-    final DocTermOrds dto = new DocTermOrds(ar, ar.getLiveDocs(), "field");
-    SortedSetDocValues iter = dto.iterator(ar);
-    
-    iter.setDocument(0);
-    assertEquals(0, iter.nextOrd());
-    assertEquals(1, iter.nextOrd());
-    assertEquals(2, iter.nextOrd());
-    assertEquals(SortedSetDocValues.NO_MORE_ORDS, iter.nextOrd());
-    
-    iter.setDocument(1);
-    assertEquals(3, iter.nextOrd());
-    assertEquals(4, iter.nextOrd());
-    assertEquals(5, iter.nextOrd());
-    assertEquals(SortedSetDocValues.NO_MORE_ORDS, iter.nextOrd());
-
-    iter.setDocument(2);
-    assertEquals(0, iter.nextOrd());
-    assertEquals(5, iter.nextOrd());
-    assertEquals(SortedSetDocValues.NO_MORE_ORDS, iter.nextOrd());
-
-    r.close();
-    dir.close();
-  }
-
-  public void testRandom() throws Exception {
-    Directory dir = newDirectory();
-
-    final int NUM_TERMS = atLeast(20);
-    final Set<BytesRef> terms = new HashSet<>();
-    while(terms.size() < NUM_TERMS) {
-      final String s = TestUtil.randomRealisticUnicodeString(random());
-      //final String s = _TestUtil.randomSimpleString(random);
-      if (s.length() > 0) {
-        terms.add(new BytesRef(s));
-      }
-    }
-    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);
-    Arrays.sort(termsArray);
-    
-    final int NUM_DOCS = atLeast(100);
-
-    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-
-    // Sometimes swap in codec that impls ord():
-    if (random().nextInt(10) == 7) {
-      // Make sure terms index has ords:
-      Codec codec = TestUtil.alwaysPostingsFormat(PostingsFormat.forName("Lucene41WithOrds"));
-      conf.setCodec(codec);
-    }
-    
-    final RandomIndexWriter w = new RandomIndexWriter(random(), dir, conf);
-
-    final int[][] idToOrds = new int[NUM_DOCS][];
-    final Set<Integer> ordsForDocSet = new HashSet<>();
-
-    for(int id=0;id<NUM_DOCS;id++) {
-      Document doc = new Document();
-
-      doc.add(new IntField("id", id, Field.Store.YES));
-      
-      final int termCount = TestUtil.nextInt(random(), 0, 20 * RANDOM_MULTIPLIER);
-      while(ordsForDocSet.size() < termCount) {
-        ordsForDocSet.add(random().nextInt(termsArray.length));
-      }
-      final int[] ordsForDoc = new int[termCount];
-      int upto = 0;
-      if (VERBOSE) {
-        System.out.println("TEST: doc id=" + id);
-      }
-      for(int ord : ordsForDocSet) {
-        ordsForDoc[upto++] = ord;
-        Field field = newStringField("field", termsArray[ord].utf8ToString(), Field.Store.NO);
-        if (VERBOSE) {
-          System.out.println("  f=" + termsArray[ord].utf8ToString());
-        }
-        doc.add(field);
-      }
-      ordsForDocSet.clear();
-      Arrays.sort(ordsForDoc);
-      idToOrds[id] = ordsForDoc;
-      w.addDocument(doc);
-    }
-    
-    final DirectoryReader r = w.getReader();
-    w.shutdown();
-
-    if (VERBOSE) {
-      System.out.println("TEST: reader=" + r);
-    }
-
-    for(AtomicReaderContext ctx : r.leaves()) {
-      if (VERBOSE) {
-        System.out.println("\nTEST: sub=" + ctx.reader());
-      }
-      verify(ctx.reader(), idToOrds, termsArray, null);
-    }
-
-    // Also test top-level reader: its enum does not support
-    // ord, so this forces the OrdWrapper to run:
-    if (VERBOSE) {
-      System.out.println("TEST: top reader");
-    }
-    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(r);
-    verify(slowR, idToOrds, termsArray, null);
-
-    FieldCache.DEFAULT.purgeByCacheKey(slowR.getCoreCacheKey());
-
-    r.close();
-    dir.close();
-  }
-
-  public void testRandomWithPrefix() throws Exception {
-    Directory dir = newDirectory();
-
-    final Set<String> prefixes = new HashSet<>();
-    final int numPrefix = TestUtil.nextInt(random(), 2, 7);
-    if (VERBOSE) {
-      System.out.println("TEST: use " + numPrefix + " prefixes");
-    }
-    while(prefixes.size() < numPrefix) {
-      prefixes.add(TestUtil.randomRealisticUnicodeString(random()));
-      //prefixes.add(_TestUtil.randomSimpleString(random));
-    }
-    final String[] prefixesArray = prefixes.toArray(new String[prefixes.size()]);
-
-    final int NUM_TERMS = atLeast(20);
-    final Set<BytesRef> terms = new HashSet<>();
-    while(terms.size() < NUM_TERMS) {
-      final String s = prefixesArray[random().nextInt(prefixesArray.length)] + TestUtil.randomRealisticUnicodeString(random());
-      //final String s = prefixesArray[random.nextInt(prefixesArray.length)] + _TestUtil.randomSimpleString(random);
-      if (s.length() > 0) {
-        terms.add(new BytesRef(s));
-      }
-    }
-    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);
-    Arrays.sort(termsArray);
-    
-    final int NUM_DOCS = atLeast(100);
-
-    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-
-    // Sometimes swap in codec that impls ord():
-    if (random().nextInt(10) == 7) {
-      Codec codec = TestUtil.alwaysPostingsFormat(PostingsFormat.forName("Lucene41WithOrds"));
-      conf.setCodec(codec);
-    }
-    
-    final RandomIndexWriter w = new RandomIndexWriter(random(), dir, conf);
-
-    final int[][] idToOrds = new int[NUM_DOCS][];
-    final Set<Integer> ordsForDocSet = new HashSet<>();
-
-    for(int id=0;id<NUM_DOCS;id++) {
-      Document doc = new Document();
-
-      doc.add(new IntField("id", id, Field.Store.YES));
-      
-      final int termCount = TestUtil.nextInt(random(), 0, 20 * RANDOM_MULTIPLIER);
-      while(ordsForDocSet.size() < termCount) {
-        ordsForDocSet.add(random().nextInt(termsArray.length));
-      }
-      final int[] ordsForDoc = new int[termCount];
-      int upto = 0;
-      if (VERBOSE) {
-        System.out.println("TEST: doc id=" + id);
-      }
-      for(int ord : ordsForDocSet) {
-        ordsForDoc[upto++] = ord;
-        Field field = newStringField("field", termsArray[ord].utf8ToString(), Field.Store.NO);
-        if (VERBOSE) {
-          System.out.println("  f=" + termsArray[ord].utf8ToString());
-        }
-        doc.add(field);
-      }
-      ordsForDocSet.clear();
-      Arrays.sort(ordsForDoc);
-      idToOrds[id] = ordsForDoc;
-      w.addDocument(doc);
-    }
-    
-    final DirectoryReader r = w.getReader();
-    w.shutdown();
-
-    if (VERBOSE) {
-      System.out.println("TEST: reader=" + r);
-    }
-    
-    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(r);
-    for(String prefix : prefixesArray) {
-
-      final BytesRef prefixRef = prefix == null ? null : new BytesRef(prefix);
-
-      final int[][] idToOrdsPrefix = new int[NUM_DOCS][];
-      for(int id=0;id<NUM_DOCS;id++) {
-        final int[] docOrds = idToOrds[id];
-        final List<Integer> newOrds = new ArrayList<>();
-        for(int ord : idToOrds[id]) {
-          if (StringHelper.startsWith(termsArray[ord], prefixRef)) {
-            newOrds.add(ord);
-          }
-        }
-        final int[] newOrdsArray = new int[newOrds.size()];
-        int upto = 0;
-        for(int ord : newOrds) {
-          newOrdsArray[upto++] = ord;
-        }
-        idToOrdsPrefix[id] = newOrdsArray;
-      }
-
-      for(AtomicReaderContext ctx : r.leaves()) {
-        if (VERBOSE) {
-          System.out.println("\nTEST: sub=" + ctx.reader());
-        }
-        verify(ctx.reader(), idToOrdsPrefix, termsArray, prefixRef);
-      }
-
-      // Also test top-level reader: its enum does not support
-      // ord, so this forces the OrdWrapper to run:
-      if (VERBOSE) {
-        System.out.println("TEST: top reader");
-      }
-      verify(slowR, idToOrdsPrefix, termsArray, prefixRef);
-    }
-
-    FieldCache.DEFAULT.purgeByCacheKey(slowR.getCoreCacheKey());
-
-    r.close();
-    dir.close();
-  }
-
-  private void verify(AtomicReader r, int[][] idToOrds, BytesRef[] termsArray, BytesRef prefixRef) throws Exception {
-
-    final DocTermOrds dto = new DocTermOrds(r, r.getLiveDocs(),
-                                            "field",
-                                            prefixRef,
-                                            Integer.MAX_VALUE,
-                                            TestUtil.nextInt(random(), 2, 10));
-                                            
-
-    final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(r, "id", false);
-    /*
-      for(int docID=0;docID<subR.maxDoc();docID++) {
-      System.out.println("  docID=" + docID + " id=" + docIDToID[docID]);
-      }
-    */
-
-    if (VERBOSE) {
-      System.out.println("TEST: verify prefix=" + (prefixRef==null ? "null" : prefixRef.utf8ToString()));
-      System.out.println("TEST: all TERMS:");
-      TermsEnum allTE = MultiFields.getTerms(r, "field").iterator(null);
-      int ord = 0;
-      while(allTE.next() != null) {
-        System.out.println("  ord=" + (ord++) + " term=" + allTE.term().utf8ToString());
-      }
-    }
-
-    //final TermsEnum te = subR.fields().terms("field").iterator();
-    final TermsEnum te = dto.getOrdTermsEnum(r);
-    if (dto.numTerms() == 0) {
-      if (prefixRef == null) {
-        assertNull(MultiFields.getTerms(r, "field"));
-      } else {
-        Terms terms = MultiFields.getTerms(r, "field");
-        if (terms != null) {
-          TermsEnum termsEnum = terms.iterator(null);
-          TermsEnum.SeekStatus result = termsEnum.seekCeil(prefixRef);
-          if (result != TermsEnum.SeekStatus.END) {
-            assertFalse("term=" + termsEnum.term().utf8ToString() + " matches prefix=" + prefixRef.utf8ToString(), StringHelper.startsWith(termsEnum.term(), prefixRef));
-          } else {
-            // ok
-          }
-        } else {
-          // ok
-        }
-      }
-      return;
-    }
-
-    if (VERBOSE) {
-      System.out.println("TEST: TERMS:");
-      te.seekExact(0);
-      while(true) {
-        System.out.println("  ord=" + te.ord() + " term=" + te.term().utf8ToString());
-        if (te.next() == null) {
-          break;
-        }
-      }
-    }
-
-    SortedSetDocValues iter = dto.iterator(r);
-    for(int docID=0;docID<r.maxDoc();docID++) {
-      if (VERBOSE) {
-        System.out.println("TEST: docID=" + docID + " of " + r.maxDoc() + " (id=" + docIDToID.get(docID) + ")");
-      }
-      iter.setDocument(docID);
-      final int[] answers = idToOrds[docIDToID.get(docID)];
-      int upto = 0;
-      long ord;
-      while ((ord = iter.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {
-        te.seekExact(ord);
-        final BytesRef expected = termsArray[answers[upto++]];
-        if (VERBOSE) {
-          System.out.println("  exp=" + expected.utf8ToString() + " actual=" + te.term().utf8ToString());
-        }
-        assertEquals("expected=" + expected.utf8ToString() + " actual=" + te.term().utf8ToString() + " ord=" + ord, expected, te.term());
-      }
-      assertEquals(answers.length, upto);
-    }
-  }
-  
-  public void testBackToTheFuture() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
-    
-    Document doc = new Document();
-    doc.add(newStringField("foo", "bar", Field.Store.NO));
-    iw.addDocument(doc);
-    
-    doc = new Document();
-    doc.add(newStringField("foo", "baz", Field.Store.NO));
-    iw.addDocument(doc);
-    
-    DirectoryReader r1 = DirectoryReader.open(iw, true);
-    
-    iw.deleteDocuments(new Term("foo", "baz"));
-    DirectoryReader r2 = DirectoryReader.open(iw, true);
-    
-    FieldCache.DEFAULT.getDocTermOrds(getOnlySegmentReader(r2), "foo");
-    
-    SortedSetDocValues v = FieldCache.DEFAULT.getDocTermOrds(getOnlySegmentReader(r1), "foo");
-    assertEquals(2, v.getValueCount());
-    v.setDocument(1);
-    assertEquals(1, v.nextOrd());
-    
-    iw.shutdown();
-    r1.close();
-    r2.close();
-    dir.close();
-  }
-  
-  public void testSortedTermsEnum() throws IOException {
-    Directory directory = newDirectory();
-    Analyzer analyzer = new MockAnalyzer(random());
-    IndexWriterConfig iwconfig = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
-    iwconfig.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, iwconfig);
-    
-    Document doc = new Document();
-    doc.add(new StringField("field", "hello", Field.Store.NO));
-    iwriter.addDocument(doc);
-    
-    doc = new Document();
-    doc.add(new StringField("field", "world", Field.Store.NO));
-    iwriter.addDocument(doc);
-
-    doc = new Document();
-    doc.add(new StringField("field", "beer", Field.Store.NO));
-    iwriter.addDocument(doc);
-    iwriter.forceMerge(1);
-    
-    DirectoryReader ireader = iwriter.getReader();
-    iwriter.shutdown();
-
-    AtomicReader ar = getOnlySegmentReader(ireader);
-    SortedSetDocValues dv = FieldCache.DEFAULT.getDocTermOrds(ar, "field");
-    assertEquals(3, dv.getValueCount());
-    
-    TermsEnum termsEnum = dv.termsEnum();
-    
-    // next()
-    assertEquals("beer", termsEnum.next().utf8ToString());
-    assertEquals(0, termsEnum.ord());
-    assertEquals("hello", termsEnum.next().utf8ToString());
-    assertEquals(1, termsEnum.ord());
-    assertEquals("world", termsEnum.next().utf8ToString());
-    assertEquals(2, termsEnum.ord());
-    
-    // seekCeil()
-    assertEquals(SeekStatus.NOT_FOUND, termsEnum.seekCeil(new BytesRef("ha!")));
-    assertEquals("hello", termsEnum.term().utf8ToString());
-    assertEquals(1, termsEnum.ord());
-    assertEquals(SeekStatus.FOUND, termsEnum.seekCeil(new BytesRef("beer")));
-    assertEquals("beer", termsEnum.term().utf8ToString());
-    assertEquals(0, termsEnum.ord());
-    assertEquals(SeekStatus.END, termsEnum.seekCeil(new BytesRef("zzz")));
-    
-    // seekExact()
-    assertTrue(termsEnum.seekExact(new BytesRef("beer")));
-    assertEquals("beer", termsEnum.term().utf8ToString());
-    assertEquals(0, termsEnum.ord());
-    assertTrue(termsEnum.seekExact(new BytesRef("hello")));
-    assertEquals("hello", termsEnum.term().utf8ToString());
-    assertEquals(1, termsEnum.ord());
-    assertTrue(termsEnum.seekExact(new BytesRef("world")));
-    assertEquals("world", termsEnum.term().utf8ToString());
-    assertEquals(2, termsEnum.ord());
-    assertFalse(termsEnum.seekExact(new BytesRef("bogus")));
-    
-    // seek(ord)
-    termsEnum.seekExact(0);
-    assertEquals("beer", termsEnum.term().utf8ToString());
-    assertEquals(0, termsEnum.ord());
-    termsEnum.seekExact(1);
-    assertEquals("hello", termsEnum.term().utf8ToString());
-    assertEquals(1, termsEnum.ord());
-    termsEnum.seekExact(2);
-    assertEquals("world", termsEnum.term().utf8ToString());
-    assertEquals(2, termsEnum.ord());
-    ireader.close();
-    directory.close();
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java lucene5666/lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java	2014-05-14 03:47:28.706646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java	2014-05-12 13:28:35.100244585 -0400
@@ -32,7 +32,6 @@
 import org.apache.lucene.document.SortedSetDocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
@@ -109,7 +108,7 @@
 
     DirectoryReader r = w.getReader();
     w.shutdown();
-    assertEquals(17, FieldCache.DEFAULT.getInts(getOnlySegmentReader(r), "field", false).get(0));
+    assertEquals(17, DocValues.getNumeric(getOnlySegmentReader(r), "field").get(0));
     r.close();
     d.close();
   }
@@ -133,7 +132,7 @@
 
     DirectoryReader r = w.getReader();
     w.shutdown();
-    assertEquals(17, FieldCache.DEFAULT.getInts(getOnlySegmentReader(r), "field", false).get(0));
+    assertEquals(17, DocValues.getNumeric(getOnlySegmentReader(r), "field").get(0));
     r.close();
     d.close();
   }
@@ -176,7 +175,7 @@
     w.addDocument(doc);
     w.forceMerge(1);
     DirectoryReader r = w.getReader();
-    BinaryDocValues s = FieldCache.DEFAULT.getTerms(getOnlySegmentReader(r), "field", false);
+    BinaryDocValues s = DocValues.getSorted(getOnlySegmentReader(r), "field");
 
     BytesRef bytes1 = new BytesRef();
     s.get(0, bytes1);
@@ -783,7 +782,7 @@
     AtomicReader subR = r.leaves().get(0).reader();
     assertEquals(2, subR.numDocs());
 
-    Bits bits = FieldCache.DEFAULT.getDocsWithField(subR, "dv");
+    Bits bits = DocValues.getDocsWithField(subR, "dv");
     assertTrue(bits.get(0));
     assertTrue(bits.get(1));
     r.close();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestDocValuesWithThreads.java lucene5666/lucene/core/src/test/org/apache/lucene/index/TestDocValuesWithThreads.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestDocValuesWithThreads.java	2014-05-14 03:47:28.706646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/index/TestDocValuesWithThreads.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,225 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Random;
-import java.util.Set;
-import java.util.concurrent.CountDownLatch;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.BinaryDocValuesField;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.document.SortedDocValuesField;
-import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-import org.apache.lucene.util.TestUtil;
-
-public class TestDocValuesWithThreads extends LuceneTestCase {
-
-  public void test() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
-
-    final List<Long> numbers = new ArrayList<>();
-    final List<BytesRef> binary = new ArrayList<>();
-    final List<BytesRef> sorted = new ArrayList<>();
-    final int numDocs = atLeast(100);
-    for(int i=0;i<numDocs;i++) {
-      Document d = new Document();
-      long number = random().nextLong();
-      d.add(new NumericDocValuesField("number", number));
-      BytesRef bytes = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));
-      d.add(new BinaryDocValuesField("bytes", bytes));
-      binary.add(bytes);
-      bytes = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));
-      d.add(new SortedDocValuesField("sorted", bytes));
-      sorted.add(bytes);
-      w.addDocument(d);
-      numbers.add(number);
-    }
-
-    w.forceMerge(1);
-    final IndexReader r = w.getReader();
-    w.shutdown();
-
-    assertEquals(1, r.leaves().size());
-    final AtomicReader ar = r.leaves().get(0).reader();
-
-    int numThreads = TestUtil.nextInt(random(), 2, 5);
-    List<Thread> threads = new ArrayList<>();
-    final CountDownLatch startingGun = new CountDownLatch(1);
-    for(int t=0;t<numThreads;t++) {
-      final Random threadRandom = new Random(random().nextLong());
-      Thread thread = new Thread() {
-          @Override
-          public void run() {
-            try {
-              //NumericDocValues ndv = ar.getNumericDocValues("number");
-              FieldCache.Longs ndv = FieldCache.DEFAULT.getLongs(ar, "number", false);
-              //BinaryDocValues bdv = ar.getBinaryDocValues("bytes");
-              BinaryDocValues bdv = FieldCache.DEFAULT.getTerms(ar, "bytes", false);
-              SortedDocValues sdv = FieldCache.DEFAULT.getTermsIndex(ar, "sorted");
-              startingGun.await();
-              int iters = atLeast(1000);
-              BytesRef scratch = new BytesRef();
-              BytesRef scratch2 = new BytesRef();
-              for(int iter=0;iter<iters;iter++) {
-                int docID = threadRandom.nextInt(numDocs);
-                switch(threadRandom.nextInt(4)) {
-                case 0:
-                  assertEquals((int) numbers.get(docID).longValue(), FieldCache.DEFAULT.getInts(ar, "number", false).get(docID));
-                  break;
-                case 1:
-                  assertEquals(numbers.get(docID).longValue(), FieldCache.DEFAULT.getLongs(ar, "number", false).get(docID));
-                  break;
-                case 2:
-                  assertEquals(Float.intBitsToFloat((int) numbers.get(docID).longValue()), FieldCache.DEFAULT.getFloats(ar, "number", false).get(docID), 0.0f);
-                  break;
-                case 3:
-                  assertEquals(Double.longBitsToDouble(numbers.get(docID).longValue()), FieldCache.DEFAULT.getDoubles(ar, "number", false).get(docID), 0.0);
-                  break;
-                }
-                bdv.get(docID, scratch);
-                assertEquals(binary.get(docID), scratch);
-                // Cannot share a single scratch against two "sources":
-                sdv.get(docID, scratch2);
-                assertEquals(sorted.get(docID), scratch2);
-              }
-            } catch (Exception e) {
-              throw new RuntimeException(e);
-            }
-          }
-        };
-      thread.start();
-      threads.add(thread);
-    }
-
-    startingGun.countDown();
-
-    for(Thread thread : threads) {
-      thread.join();
-    }
-
-    r.close();
-    dir.close();
-  }
-  
-  public void test2() throws Exception {
-    Random random = random();
-    final int NUM_DOCS = atLeast(100);
-    final Directory dir = newDirectory();
-    final RandomIndexWriter writer = new RandomIndexWriter(random, dir);
-    final boolean allowDups = random.nextBoolean();
-    final Set<String> seen = new HashSet<>();
-    if (VERBOSE) {
-      System.out.println("TEST: NUM_DOCS=" + NUM_DOCS + " allowDups=" + allowDups);
-    }
-    int numDocs = 0;
-    final List<BytesRef> docValues = new ArrayList<>();
-
-    // TODO: deletions
-    while (numDocs < NUM_DOCS) {
-      final String s;
-      if (random.nextBoolean()) {
-        s = TestUtil.randomSimpleString(random);
-      } else {
-        s = TestUtil.randomUnicodeString(random);
-      }
-      final BytesRef br = new BytesRef(s);
-
-      if (!allowDups) {
-        if (seen.contains(s)) {
-          continue;
-        }
-        seen.add(s);
-      }
-
-      if (VERBOSE) {
-        System.out.println("  " + numDocs + ": s=" + s);
-      }
-      
-      final Document doc = new Document();
-      doc.add(new SortedDocValuesField("stringdv", br));
-      doc.add(new NumericDocValuesField("id", numDocs));
-      docValues.add(br);
-      writer.addDocument(doc);
-      numDocs++;
-
-      if (random.nextInt(40) == 17) {
-        // force flush
-        writer.getReader().close();
-      }
-    }
-
-    writer.forceMerge(1);
-    final DirectoryReader r = writer.getReader();
-    writer.shutdown();
-    
-    final AtomicReader sr = getOnlySegmentReader(r);
-
-    final long END_TIME = System.currentTimeMillis() + (TEST_NIGHTLY ? 30 : 1);
-
-    final int NUM_THREADS = TestUtil.nextInt(random(), 1, 10);
-    Thread[] threads = new Thread[NUM_THREADS];
-    for(int thread=0;thread<NUM_THREADS;thread++) {
-      threads[thread] = new Thread() {
-          @Override
-          public void run() {
-            Random random = random();            
-            final SortedDocValues stringDVDirect;
-            final NumericDocValues docIDToID;
-            try {
-              stringDVDirect = sr.getSortedDocValues("stringdv");
-              docIDToID = sr.getNumericDocValues("id");
-              assertNotNull(stringDVDirect);
-            } catch (IOException ioe) {
-              throw new RuntimeException(ioe);
-            }
-            while(System.currentTimeMillis() < END_TIME) {
-              final SortedDocValues source;
-              source = stringDVDirect;
-              final BytesRef scratch = new BytesRef();
-
-              for(int iter=0;iter<100;iter++) {
-                final int docID = random.nextInt(sr.maxDoc());
-                source.get(docID, scratch);
-                assertEquals(docValues.get((int) docIDToID.get(docID)), scratch);
-              }
-            }
-          }
-        };
-      threads[thread].start();
-    }
-
-    for(Thread thread : threads) {
-      thread.join();
-    }
-
-    r.close();
-    dir.close();
-  }
-
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java lucene5666/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java	2014-05-14 03:47:28.706646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java	2014-05-12 13:28:35.096244584 -0400
@@ -55,7 +55,6 @@
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.PhraseQuery;
@@ -1751,11 +1750,6 @@
     w.shutdown();
     assertEquals(1, reader.docFreq(new Term("content", bigTerm)));
 
-    SortedDocValues dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), "content", random().nextFloat() * PackedInts.FAST);
-    assertEquals(4, dti.getValueCount());
-    BytesRef br = new BytesRef();
-    dti.lookupOrd(2, br);
-    assertEquals(bigTermBytesRef, br);
     reader.close();
     dir.close();
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java lucene5666/lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java	2014-05-14 03:47:28.706646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java	2014-05-12 13:28:34.436244573 -0400
@@ -33,11 +33,11 @@
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.English;
@@ -240,6 +240,7 @@
     for(int docCount=0;docCount<numDocs;docCount++) {
       Document doc = new Document();
       doc.add(new IntField("id", docCount, Field.Store.YES));
+      doc.add(new NumericDocValuesField("id", docCount));
       List<Token> tokens = new ArrayList<>();
       final int numTokens = atLeast(100);
       //final int numTokens = atLeast(20);
@@ -296,7 +297,7 @@
       DocsEnum docs = null;
       DocsAndPositionsEnum docsAndPositions = null;
       DocsAndPositionsEnum docsAndPositionsAndOffsets = null;
-      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(sub, "id", false);
+      final NumericDocValues docIDToID = DocValues.getNumeric(sub, "id");
       for(String term : terms) {
         //System.out.println("  term=" + term);
         if (termsEnum.seekExact(new BytesRef(term))) {
@@ -305,7 +306,7 @@
           int doc;
           //System.out.println("    doc/freq");
           while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
-            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));
+            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));
             //System.out.println("      doc=" + docIDToID.get(doc) + " docID=" + doc + " " + expected.size() + " freq");
             assertNotNull(expected);
             assertEquals(expected.size(), docs.freq());
@@ -316,7 +317,7 @@
           assertNotNull(docsAndPositions);
           //System.out.println("    doc/freq/pos");
           while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
-            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));
+            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));
             //System.out.println("      doc=" + docIDToID.get(doc) + " " + expected.size() + " freq");
             assertNotNull(expected);
             assertEquals(expected.size(), docsAndPositions.freq());
@@ -331,7 +332,7 @@
           assertNotNull(docsAndPositionsAndOffsets);
           //System.out.println("    doc/freq/pos/offs");
           while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
-            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));
+            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));
             //System.out.println("      doc=" + docIDToID.get(doc) + " " + expected.size() + " freq");
             assertNotNull(expected);
             assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java lucene5666/lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java	2014-05-14 03:47:28.706646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java	2014-05-12 13:28:35.096244584 -0400
@@ -24,8 +24,8 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LineFileDocs;
@@ -159,6 +159,7 @@
   private void addDoc(RandomIndexWriter w, Collection<String> terms, Map<BytesRef,Integer> termToID, int id) throws IOException {
     Document doc = new Document();
     doc.add(new IntField("id", id, Field.Store.YES));
+    doc.add(new NumericDocValuesField("id", id));
     if (VERBOSE) {
       System.out.println("TEST: addDoc id:" + id + " terms=" + terms);
     }
@@ -226,8 +227,7 @@
     final IndexReader r = w.getReader();
     w.shutdown();
 
-    // NOTE: intentional insanity!!
-    final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(r), "id", false);
+    final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, "id");
 
     for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/BaseTestRangeFilter.java lucene5666/lucene/core/src/test/org/apache/lucene/search/BaseTestRangeFilter.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/BaseTestRangeFilter.java	2014-05-14 03:47:28.698646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/BaseTestRangeFilter.java	2014-05-12 13:28:34.208244569 -0400
@@ -28,10 +28,13 @@
 import org.apache.lucene.document.FloatField;
 import org.apache.lucene.document.IntField;
 import org.apache.lucene.document.LongField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
 import org.junit.AfterClass;
@@ -120,19 +123,33 @@
     
     Document doc = new Document();
     Field idField = newStringField(random, "id", "", Field.Store.YES);
+    Field idDVField = new SortedDocValuesField("id", new BytesRef());
     Field intIdField = new IntField("id_int", 0, Store.YES);
+    Field intDVField = new NumericDocValuesField("id_int", 0);
     Field floatIdField = new FloatField("id_float", 0, Store.YES);
+    Field floatDVField = new NumericDocValuesField("id_float", 0);
     Field longIdField = new LongField("id_long", 0, Store.YES);
+    Field longDVField = new NumericDocValuesField("id_long", 0);
     Field doubleIdField = new DoubleField("id_double", 0, Store.YES);
+    Field doubleDVField = new NumericDocValuesField("id_double", 0);
     Field randField = newStringField(random, "rand", "", Field.Store.YES);
+    Field randDVField = new SortedDocValuesField("rand", new BytesRef());
     Field bodyField = newStringField(random, "body", "", Field.Store.NO);
+    Field bodyDVField = new SortedDocValuesField("body", new BytesRef());
     doc.add(idField);
+    doc.add(idDVField);
     doc.add(intIdField);
+    doc.add(intDVField);
     doc.add(floatIdField);
+    doc.add(floatDVField);
     doc.add(longIdField);
+    doc.add(longDVField);
     doc.add(doubleIdField);
+    doc.add(doubleDVField);
     doc.add(randField);
+    doc.add(randDVField);
     doc.add(bodyField);
+    doc.add(bodyDVField);
 
     RandomIndexWriter writer = new RandomIndexWriter(random, index.index, 
                                                      newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(random))
@@ -146,10 +163,15 @@
 
       for (int d = minId; d <= maxId; d++) {
         idField.setStringValue(pad(d));
+        idDVField.setBytesValue(new BytesRef(pad(d)));
         intIdField.setIntValue(d);
+        intDVField.setLongValue(d);
         floatIdField.setFloatValue(d);
+        floatDVField.setLongValue(Float.floatToRawIntBits(d));
         longIdField.setLongValue(d);
+        longDVField.setLongValue(d);
         doubleIdField.setDoubleValue(d);
+        doubleDVField.setLongValue(Double.doubleToRawLongBits(d));
         int r = index.allowNegativeRandomInts ? random.nextInt() : random
           .nextInt(Integer.MAX_VALUE);
         if (index.maxR < r) {
@@ -166,7 +188,9 @@
           minCount++;
         }
         randField.setStringValue(pad(r));
+        randDVField.setBytesValue(new BytesRef(pad(r)));
         bodyField.setStringValue("body");
+        bodyDVField.setBytesValue(new BytesRef("body"));
         writer.addDocument(doc);
       }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/JustCompileSearch.java lucene5666/lucene/core/src/test/org/apache/lucene/search/JustCompileSearch.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/JustCompileSearch.java	2014-05-14 03:47:28.694646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/JustCompileSearch.java	2014-05-12 13:28:34.436244573 -0400
@@ -95,34 +95,6 @@
     }
   }
   
-  static final class JustCompileExtendedFieldCacheLongParser implements FieldCache.LongParser {
-
-    @Override
-    public long parseLong(BytesRef string) {
-      throw new UnsupportedOperationException(UNSUPPORTED_MSG);
-    }
-
-    @Override
-    public TermsEnum termsEnum(Terms terms) {
-      throw new UnsupportedOperationException(UNSUPPORTED_MSG);
-    }
-    
-  }
-  
-  static final class JustCompileExtendedFieldCacheDoubleParser implements FieldCache.DoubleParser {
-    
-    @Override
-    public double parseDouble(BytesRef term) {
-      throw new UnsupportedOperationException(UNSUPPORTED_MSG);
-    }
-
-    @Override
-    public TermsEnum termsEnum(Terms terms) {
-      throw new UnsupportedOperationException(UNSUPPORTED_MSG);
-    }
-    
-  }
-
   static final class JustCompileFieldComparator extends FieldComparator<Object> {
 
     @Override


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java	2014-05-14 03:47:28.698646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java	2014-05-14 03:45:16.506644320 -0400
@@ -254,7 +254,7 @@
     // returns default empty docidset, always cacheable:
     assertDocIdSetCacheable(reader, NumericRangeFilter.newIntRange("test", Integer.valueOf(10000), Integer.valueOf(-10000), true, true), true);
     // is cacheable:
-    assertDocIdSetCacheable(reader, FieldCacheRangeFilter.newIntRange("test", Integer.valueOf(10), Integer.valueOf(20), true, true), true);
+    assertDocIdSetCacheable(reader, DocValuesRangeFilter.newIntRange("test", Integer.valueOf(10), Integer.valueOf(20), true, true), true);
     // a fixedbitset filter is always cacheable
     assertDocIdSetCacheable(reader, new Filter() {
       @Override


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestDateSort.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestDateSort.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestDateSort.java	2014-05-14 03:47:28.698646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestDateSort.java	2014-05-12 13:28:34.092244567 -0400
@@ -20,11 +20,13 @@
 import java.util.Arrays;
 
 import org.apache.lucene.index.Term;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LuceneTestCase;
 
 import org.apache.lucene.document.DateTools;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.StoredDocument;
@@ -110,6 +112,7 @@
     String dateTimeString = DateTools.timeToString(time, DateTools.Resolution.SECOND);
     Field dateTimeField = newStringField(DATE_TIME_FIELD, dateTimeString, Field.Store.YES);
     document.add(dateTimeField);
+    document.add(new SortedDocValuesField(DATE_TIME_FIELD, new BytesRef(dateTimeString)));
 
     return document;
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestDocTermOrdsRangeFilter.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestDocTermOrdsRangeFilter.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestDocTermOrdsRangeFilter.java	2014-05-14 03:47:28.694646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestDocTermOrdsRangeFilter.java	2014-05-12 13:28:34.328244571 -0400
@@ -33,12 +33,14 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 import org.apache.lucene.util.TestUtil;
 import org.apache.lucene.util.UnicodeUtil;
 
 /**
  * Tests the DocTermOrdsRangeFilter
  */
+@SuppressCodecs({"Lucene40", "Lucene41", "Lucene42"}) // needs SORTED_SET
 public class TestDocTermOrdsRangeFilter extends LuceneTestCase {
   protected IndexSearcher searcher1;
   protected IndexSearcher searcher2;
@@ -63,10 +65,7 @@
       for (int j = 0; j < numTerms; j++) {
         String s = TestUtil.randomUnicodeString(random());
         doc.add(newStringField(fieldName, s, Field.Store.NO));
-        // if the default codec doesn't support sortedset, we will uninvert at search time
-        if (defaultCodecSupportsSortedSet()) {
-          doc.add(new SortedSetDocValuesField(fieldName, new BytesRef(s)));
-        }
+        doc.add(new SortedSetDocValuesField(fieldName, new BytesRef(s)));
         terms.add(s);
       }
       writer.addDocument(doc);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestDocTermOrdsRewriteMethod.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestDocTermOrdsRewriteMethod.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestDocTermOrdsRewriteMethod.java	2014-05-14 03:47:28.694646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestDocTermOrdsRewriteMethod.java	2014-05-12 13:28:34.324244571 -0400
@@ -51,6 +51,7 @@
   @Override
   public void setUp() throws Exception {
     super.setUp();
+    assumeTrue("requires codec support for SORTED_SET", defaultCodecSupportsSortedSet());
     dir = newDirectory();
     fieldName = random().nextBoolean() ? "field" : ""; // sometimes use an empty string as field name
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir, 
@@ -65,10 +66,7 @@
       for (int j = 0; j < numTerms; j++) {
         String s = TestUtil.randomUnicodeString(random());
         doc.add(newStringField(fieldName, s, Field.Store.NO));
-        // if the default codec doesn't support sortedset, we will uninvert at search time
-        if (defaultCodecSupportsSortedSet()) {
-          doc.add(new SortedSetDocValuesField(fieldName, new BytesRef(s)));
-        }
+        doc.add(new SortedSetDocValuesField(fieldName, new BytesRef(s)));
         terms.add(s);
       }
       writer.addDocument(doc);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestDocValuesScoring.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestDocValuesScoring.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestDocValuesScoring.java	2014-05-14 03:47:28.694646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestDocValuesScoring.java	2014-05-12 13:28:34.316244571 -0400
@@ -23,8 +23,10 @@
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FloatDocValuesField;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.similarities.PerFieldSimilarityWrapper;
@@ -158,12 +160,12 @@
     @Override
     public SimScorer simScorer(SimWeight stats, AtomicReaderContext context) throws IOException {
       final SimScorer sub = sim.simScorer(stats, context);
-      final FieldCache.Floats values = FieldCache.DEFAULT.getFloats(context.reader(), boostField, false);
+      final NumericDocValues values = DocValues.getNumeric(context.reader(), boostField);
       
       return new SimScorer() {
         @Override
         public float score(int doc, float freq) {
-          return values.get(doc) * sub.score(doc, freq);
+          return Float.intBitsToFloat((int)values.get(doc)) * sub.score(doc, freq);
         }
         
         @Override
@@ -178,7 +180,7 @@
 
         @Override
         public Explanation explain(int doc, Explanation freq) {
-          Explanation boostExplanation = new Explanation(values.get(doc), "indexDocValue(" + boostField + ")");
+          Explanation boostExplanation = new Explanation(Float.intBitsToFloat((int)values.get(doc)), "indexDocValue(" + boostField + ")");
           Explanation simExplanation = sub.explain(doc, freq);
           Explanation expl = new Explanation(boostExplanation.getValue() * simExplanation.getValue(), "product of:");
           expl.addDetail(boostExplanation);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java	2014-05-14 03:47:28.694646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java	2014-05-12 13:28:34.324244571 -0400
@@ -20,6 +20,7 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.index.*;
 import org.apache.lucene.search.FieldValueHitQueue.Entry;
 import org.apache.lucene.search.similarities.DefaultSimilarity;
@@ -126,6 +127,9 @@
    Document doc = new Document();
    for (int i = 0; i < vals.length - 2; i += 2) {
      doc.add(newTextField(vals[i], vals[i + 1], Field.Store.YES));
+     if (vals[i].equals("id")) {
+       doc.add(new SortedDocValuesField(vals[i], new BytesRef(vals[i+1])));
+     }
    }
    return doc;
  }
@@ -185,7 +189,7 @@
 
      @Override
      public FieldComparator<Integer> setNextReader(AtomicReaderContext context) throws IOException {
-       idIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldname);
+       idIndex = DocValues.getSorted(context.reader(), fieldname);
        return this;
      }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestExplanations.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestExplanations.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestExplanations.java	2014-05-14 03:47:28.694646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestExplanations.java	2014-05-14 03:45:16.502644320 -0400
@@ -20,6 +20,7 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
@@ -30,6 +31,7 @@
 import org.apache.lucene.search.spans.SpanQuery;
 import org.apache.lucene.search.spans.SpanTermQuery;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LuceneTestCase;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
@@ -73,6 +75,7 @@
     for (int i = 0; i < docFields.length; i++) {
       Document doc = new Document();
       doc.add(newStringField(KEY, ""+i, Field.Store.NO));
+      doc.add(new SortedDocValuesField(KEY, new BytesRef(""+i)));
       Field f = newTextField(FIELD, docFields[i], Field.Store.NO);
       f.setBoost(i);
       doc.add(f);
@@ -110,7 +113,7 @@
   /** 
    * Convenience subclass of FieldCacheTermsFilter
    */
-  public static class ItemizedFilter extends FieldCacheTermsFilter {
+  public static class ItemizedFilter extends DocValuesTermsFilter {
     private static String[] int2str(int [] terms) {
       String [] out = new String[terms.length];
       for (int i = 0; i < terms.length; i++) {
@@ -118,9 +121,6 @@
       }
       return out;
     }
-    public ItemizedFilter(String keyField, int [] keys) {
-      super(keyField, int2str(keys));
-    }
     public ItemizedFilter(int [] keys) {
       super(KEY, int2str(keys));
     }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestFieldCache.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestFieldCache.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestFieldCache.java	2014-05-14 03:47:28.698646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestFieldCache.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,782 +0,0 @@
-package org.apache.lucene.search;
-
-/**
- * Copyright 2004 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintStream;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.LinkedHashSet;
-import java.util.List;
-import java.util.concurrent.CyclicBarrier;
-import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.concurrent.atomic.AtomicInteger;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.BinaryDocValuesField;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.DoubleField;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.FloatField;
-import org.apache.lucene.document.IntField;
-import org.apache.lucene.document.LongField;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.document.SortedDocValuesField;
-import org.apache.lucene.document.SortedSetDocValuesField;
-import org.apache.lucene.document.StoredField;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.DocTermOrds;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.SlowCompositeReaderWrapper;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.search.FieldCache.Doubles;
-import org.apache.lucene.search.FieldCache.Floats;
-import org.apache.lucene.search.FieldCache.Ints;
-import org.apache.lucene.search.FieldCache.Longs;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.NumericUtils;
-import org.apache.lucene.util.TestUtil;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-
-public class TestFieldCache extends LuceneTestCase {
-  private static AtomicReader reader;
-  private static int NUM_DOCS;
-  private static int NUM_ORDS;
-  private static String[] unicodeStrings;
-  private static BytesRef[][] multiValued;
-  private static Directory directory;
-
-  @BeforeClass
-  public static void beforeClass() throws Exception {
-    NUM_DOCS = atLeast(500);
-    NUM_ORDS = atLeast(2);
-    directory = newDirectory();
-    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
-    long theLong = Long.MAX_VALUE;
-    double theDouble = Double.MAX_VALUE;
-    int theInt = Integer.MAX_VALUE;
-    float theFloat = Float.MAX_VALUE;
-    unicodeStrings = new String[NUM_DOCS];
-    multiValued = new BytesRef[NUM_DOCS][NUM_ORDS];
-    if (VERBOSE) {
-      System.out.println("TEST: setUp");
-    }
-    for (int i = 0; i < NUM_DOCS; i++){
-      Document doc = new Document();
-      doc.add(new LongField("theLong", theLong--, Field.Store.NO));
-      doc.add(new DoubleField("theDouble", theDouble--, Field.Store.NO));
-      doc.add(new IntField("theInt", theInt--, Field.Store.NO));
-      doc.add(new FloatField("theFloat", theFloat--, Field.Store.NO));
-      if (i%2 == 0) {
-        doc.add(new IntField("sparse", i, Field.Store.NO));
-      }
-
-      if (i%2 == 0) {
-        doc.add(new IntField("numInt", i, Field.Store.NO));
-      }
-
-      // sometimes skip the field:
-      if (random().nextInt(40) != 17) {
-        unicodeStrings[i] = generateString(i);
-        doc.add(newStringField("theRandomUnicodeString", unicodeStrings[i], Field.Store.YES));
-      }
-
-      // sometimes skip the field:
-      if (random().nextInt(10) != 8) {
-        for (int j = 0; j < NUM_ORDS; j++) {
-          String newValue = generateString(i);
-          multiValued[i][j] = new BytesRef(newValue);
-          doc.add(newStringField("theRandomUnicodeMultiValuedField", newValue, Field.Store.YES));
-        }
-        Arrays.sort(multiValued[i]);
-      }
-      writer.addDocument(doc);
-    }
-    IndexReader r = writer.getReader();
-    reader = SlowCompositeReaderWrapper.wrap(r);
-    writer.shutdown();
-  }
-
-  @AfterClass
-  public static void afterClass() throws Exception {
-    reader.close();
-    reader = null;
-    directory.close();
-    directory = null;
-    unicodeStrings = null;
-    multiValued = null;
-  }
-  
-  public void testInfoStream() throws Exception {
-    try {
-      FieldCache cache = FieldCache.DEFAULT;
-      ByteArrayOutputStream bos = new ByteArrayOutputStream(1024);
-      cache.setInfoStream(new PrintStream(bos, false, IOUtils.UTF_8));
-      cache.getDoubles(reader, "theDouble", false);
-      cache.getFloats(reader, "theDouble", new FieldCache.FloatParser() {
-        @Override
-        public TermsEnum termsEnum(Terms terms) throws IOException {
-          return NumericUtils.filterPrefixCodedLongs(terms.iterator(null));
-        }
-        @Override
-        public float parseFloat(BytesRef term) {
-          return NumericUtils.sortableIntToFloat((int) NumericUtils.prefixCodedToLong(term));
-        }
-      }, false);
-      assertTrue(bos.toString(IOUtils.UTF_8).indexOf("WARNING") != -1);
-    } finally {
-      FieldCache.DEFAULT.setInfoStream(null);
-      FieldCache.DEFAULT.purgeAllCaches();
-    }
-  }
-
-  public void test() throws IOException {
-    FieldCache cache = FieldCache.DEFAULT;
-    FieldCache.Doubles doubles = cache.getDoubles(reader, "theDouble", random().nextBoolean());
-    assertSame("Second request to cache return same array", doubles, cache.getDoubles(reader, "theDouble", random().nextBoolean()));
-    assertSame("Second request with explicit parser return same array", doubles, cache.getDoubles(reader, "theDouble", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean()));
-    for (int i = 0; i < NUM_DOCS; i++) {
-      assertTrue(doubles.get(i) + " does not equal: " + (Double.MAX_VALUE - i), doubles.get(i) == (Double.MAX_VALUE - i));
-    }
-    
-    FieldCache.Longs longs = cache.getLongs(reader, "theLong", random().nextBoolean());
-    assertSame("Second request to cache return same array", longs, cache.getLongs(reader, "theLong", random().nextBoolean()));
-    assertSame("Second request with explicit parser return same array", longs, cache.getLongs(reader, "theLong", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean()));
-    for (int i = 0; i < NUM_DOCS; i++) {
-      assertTrue(longs.get(i) + " does not equal: " + (Long.MAX_VALUE - i) + " i=" + i, longs.get(i) == (Long.MAX_VALUE - i));
-    }
-
-    FieldCache.Ints ints = cache.getInts(reader, "theInt", random().nextBoolean());
-    assertSame("Second request to cache return same array", ints, cache.getInts(reader, "theInt", random().nextBoolean()));
-    assertSame("Second request with explicit parser return same array", ints, cache.getInts(reader, "theInt", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean()));
-    for (int i = 0; i < NUM_DOCS; i++) {
-      assertTrue(ints.get(i) + " does not equal: " + (Integer.MAX_VALUE - i), ints.get(i) == (Integer.MAX_VALUE - i));
-    }
-    
-    FieldCache.Floats floats = cache.getFloats(reader, "theFloat", random().nextBoolean());
-    assertSame("Second request to cache return same array", floats, cache.getFloats(reader, "theFloat", random().nextBoolean()));
-    assertSame("Second request with explicit parser return same array", floats, cache.getFloats(reader, "theFloat", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean()));
-    for (int i = 0; i < NUM_DOCS; i++) {
-      assertTrue(floats.get(i) + " does not equal: " + (Float.MAX_VALUE - i), floats.get(i) == (Float.MAX_VALUE - i));
-    }
-
-    Bits docsWithField = cache.getDocsWithField(reader, "theLong");
-    assertSame("Second request to cache return same array", docsWithField, cache.getDocsWithField(reader, "theLong"));
-    assertTrue("docsWithField(theLong) must be class Bits.MatchAllBits", docsWithField instanceof Bits.MatchAllBits);
-    assertTrue("docsWithField(theLong) Size: " + docsWithField.length() + " is not: " + NUM_DOCS, docsWithField.length() == NUM_DOCS);
-    for (int i = 0; i < docsWithField.length(); i++) {
-      assertTrue(docsWithField.get(i));
-    }
-    
-    docsWithField = cache.getDocsWithField(reader, "sparse");
-    assertSame("Second request to cache return same array", docsWithField, cache.getDocsWithField(reader, "sparse"));
-    assertFalse("docsWithField(sparse) must not be class Bits.MatchAllBits", docsWithField instanceof Bits.MatchAllBits);
-    assertTrue("docsWithField(sparse) Size: " + docsWithField.length() + " is not: " + NUM_DOCS, docsWithField.length() == NUM_DOCS);
-    for (int i = 0; i < docsWithField.length(); i++) {
-      assertEquals(i%2 == 0, docsWithField.get(i));
-    }
-
-    // getTermsIndex
-    SortedDocValues termsIndex = cache.getTermsIndex(reader, "theRandomUnicodeString");
-    assertSame("Second request to cache return same array", termsIndex, cache.getTermsIndex(reader, "theRandomUnicodeString"));
-    final BytesRef br = new BytesRef();
-    for (int i = 0; i < NUM_DOCS; i++) {
-      final BytesRef term;
-      final int ord = termsIndex.getOrd(i);
-      if (ord == -1) {
-        term = null;
-      } else {
-        termsIndex.lookupOrd(ord, br);
-        term = br;
-      }
-      final String s = term == null ? null : term.utf8ToString();
-      assertTrue("for doc " + i + ": " + s + " does not equal: " + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));
-    }
-
-    int nTerms = termsIndex.getValueCount();
-
-    TermsEnum tenum = termsIndex.termsEnum();
-    BytesRef val = new BytesRef();
-    for (int i=0; i<nTerms; i++) {
-      BytesRef val1 = tenum.next();
-      termsIndex.lookupOrd(i, val);
-      // System.out.println("i="+i);
-      assertEquals(val, val1);
-    }
-
-    // seek the enum around (note this isn't a great test here)
-    int num = atLeast(100);
-    for (int i = 0; i < num; i++) {
-      int k = random().nextInt(nTerms);
-      termsIndex.lookupOrd(k, val);
-      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));
-      assertEquals(val, tenum.term());
-    }
-
-    for(int i=0;i<nTerms;i++) {
-      termsIndex.lookupOrd(i, val);
-      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));
-      assertEquals(val, tenum.term());
-    }
-
-    // test bad field
-    termsIndex = cache.getTermsIndex(reader, "bogusfield");
-
-    // getTerms
-    BinaryDocValues terms = cache.getTerms(reader, "theRandomUnicodeString", true);
-    assertSame("Second request to cache return same array", terms, cache.getTerms(reader, "theRandomUnicodeString", true));
-    Bits bits = cache.getDocsWithField(reader, "theRandomUnicodeString");
-    for (int i = 0; i < NUM_DOCS; i++) {
-      terms.get(i, br);
-      final BytesRef term;
-      if (!bits.get(i)) {
-        term = null;
-      } else {
-        term = br;
-      }
-      final String s = term == null ? null : term.utf8ToString();
-      assertTrue("for doc " + i + ": " + s + " does not equal: " + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));
-    }
-
-    // test bad field
-    terms = cache.getTerms(reader, "bogusfield", false);
-
-    // getDocTermOrds
-    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, "theRandomUnicodeMultiValuedField");
-    int numEntries = cache.getCacheEntries().length;
-    // ask for it again, and check that we didnt create any additional entries:
-    termOrds = cache.getDocTermOrds(reader, "theRandomUnicodeMultiValuedField");
-    assertEquals(numEntries, cache.getCacheEntries().length);
-
-    for (int i = 0; i < NUM_DOCS; i++) {
-      termOrds.setDocument(i);
-      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId
-      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));
-      for (BytesRef v : values) {
-        if (v == null) {
-          // why does this test use null values... instead of an empty list: confusing
-          break;
-        }
-        long ord = termOrds.nextOrd();
-        assert ord != SortedSetDocValues.NO_MORE_ORDS;
-        BytesRef scratch = new BytesRef();
-        termOrds.lookupOrd(ord, scratch);
-        assertEquals(v, scratch);
-      }
-      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());
-    }
-
-    // test bad field
-    termOrds = cache.getDocTermOrds(reader, "bogusfield");
-    assertTrue(termOrds.getValueCount() == 0);
-
-    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());
-  }
-
-  public void testEmptyIndex() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriter writer= new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(500));
-    writer.shutdown();
-    IndexReader r = DirectoryReader.open(dir);
-    AtomicReader reader = SlowCompositeReaderWrapper.wrap(r);
-    FieldCache.DEFAULT.getTerms(reader, "foobar", true);
-    FieldCache.DEFAULT.getTermsIndex(reader, "foobar");
-    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());
-    r.close();
-    dir.close();
-  }
-
-  private static String generateString(int i) {
-    String s = null;
-    if (i > 0 && random().nextInt(3) == 1) {
-      // reuse past string -- try to find one that's not null
-      for(int iter = 0; iter < 10 && s == null;iter++) {
-        s = unicodeStrings[random().nextInt(i)];
-      }
-      if (s == null) {
-        s = TestUtil.randomUnicodeString(random());
-      }
-    } else {
-      s = TestUtil.randomUnicodeString(random());
-    }
-    return s;
-  }
-
-  public void testDocsWithField() throws Exception {
-    FieldCache cache = FieldCache.DEFAULT;
-    cache.purgeAllCaches();
-    assertEquals(0, cache.getCacheEntries().length);
-    cache.getDoubles(reader, "theDouble", true);
-
-    // The double[] takes two slots (one w/ null parser, one
-    // w/ real parser), and docsWithField should also
-    // have been populated:
-    assertEquals(3, cache.getCacheEntries().length);
-    Bits bits = cache.getDocsWithField(reader, "theDouble");
-
-    // No new entries should appear:
-    assertEquals(3, cache.getCacheEntries().length);
-    assertTrue(bits instanceof Bits.MatchAllBits);
-
-    FieldCache.Ints ints = cache.getInts(reader, "sparse", true);
-    assertEquals(6, cache.getCacheEntries().length);
-    Bits docsWithField = cache.getDocsWithField(reader, "sparse");
-    assertEquals(6, cache.getCacheEntries().length);
-    for (int i = 0; i < docsWithField.length(); i++) {
-      if (i%2 == 0) {
-        assertTrue(docsWithField.get(i));
-        assertEquals(i, ints.get(i));
-      } else {
-        assertFalse(docsWithField.get(i));
-      }
-    }
-
-    FieldCache.Ints numInts = cache.getInts(reader, "numInt", random().nextBoolean());
-    docsWithField = cache.getDocsWithField(reader, "numInt");
-    for (int i = 0; i < docsWithField.length(); i++) {
-      if (i%2 == 0) {
-        assertTrue(docsWithField.get(i));
-        assertEquals(i, numInts.get(i));
-      } else {
-        assertFalse(docsWithField.get(i));
-      }
-    }
-  }
-  
-  public void testGetDocsWithFieldThreadSafety() throws Exception {
-    final FieldCache cache = FieldCache.DEFAULT;
-    cache.purgeAllCaches();
-
-    int NUM_THREADS = 3;
-    Thread[] threads = new Thread[NUM_THREADS];
-    final AtomicBoolean failed = new AtomicBoolean();
-    final AtomicInteger iters = new AtomicInteger();
-    final int NUM_ITER = 200 * RANDOM_MULTIPLIER;
-    final CyclicBarrier restart = new CyclicBarrier(NUM_THREADS,
-                                                    new Runnable() {
-                                                      @Override
-                                                      public void run() {
-                                                        cache.purgeAllCaches();
-                                                        iters.incrementAndGet();
-                                                      }
-                                                    });
-    for(int threadIDX=0;threadIDX<NUM_THREADS;threadIDX++) {
-      threads[threadIDX] = new Thread() {
-          @Override
-          public void run() {
-
-            try {
-              while(!failed.get()) {
-                final int op = random().nextInt(3);
-                if (op == 0) {
-                  // Purge all caches & resume, once all
-                  // threads get here:
-                  restart.await();
-                  if (iters.get() >= NUM_ITER) {
-                    break;
-                  }
-                } else if (op == 1) {
-                  Bits docsWithField = cache.getDocsWithField(reader, "sparse");
-                  for (int i = 0; i < docsWithField.length(); i++) {
-                    assertEquals(i%2 == 0, docsWithField.get(i));
-                  }
-                } else {
-                  FieldCache.Ints ints = cache.getInts(reader, "sparse", true);
-                  Bits docsWithField = cache.getDocsWithField(reader, "sparse");
-                  for (int i = 0; i < docsWithField.length(); i++) {
-                    if (i%2 == 0) {
-                      assertTrue(docsWithField.get(i));
-                      assertEquals(i, ints.get(i));
-                    } else {
-                      assertFalse(docsWithField.get(i));
-                    }
-                  }
-                }
-              }
-            } catch (Throwable t) {
-              failed.set(true);
-              restart.reset();
-              throw new RuntimeException(t);
-            }
-          }
-        };
-      threads[threadIDX].start();
-    }
-
-    for(int threadIDX=0;threadIDX<NUM_THREADS;threadIDX++) {
-      threads[threadIDX].join();
-    }
-    assertFalse(failed.get());
-  }
-  
-  public void testDocValuesIntegration() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    doc.add(new BinaryDocValuesField("binary", new BytesRef("binary value")));
-    doc.add(new SortedDocValuesField("sorted", new BytesRef("sorted value")));
-    doc.add(new NumericDocValuesField("numeric", 42));
-    if (defaultCodecSupportsSortedSet()) {
-      doc.add(new SortedSetDocValuesField("sortedset", new BytesRef("sortedset value1")));
-      doc.add(new SortedSetDocValuesField("sortedset", new BytesRef("sortedset value2")));
-    }
-    iw.addDocument(doc);
-    DirectoryReader ir = iw.getReader();
-    iw.shutdown();
-    AtomicReader ar = getOnlySegmentReader(ir);
-    
-    BytesRef scratch = new BytesRef();
-    
-    // Binary type: can be retrieved via getTerms()
-    try {
-      FieldCache.DEFAULT.getInts(ar, "binary", false);
-      fail();
-    } catch (IllegalStateException expected) {}
-    
-    BinaryDocValues binary = FieldCache.DEFAULT.getTerms(ar, "binary", true);
-    binary.get(0, scratch);
-    assertEquals("binary value", scratch.utf8ToString());
-    
-    try {
-      FieldCache.DEFAULT.getTermsIndex(ar, "binary");
-      fail();
-    } catch (IllegalStateException expected) {}
-    
-    try {
-      FieldCache.DEFAULT.getDocTermOrds(ar, "binary");
-      fail();
-    } catch (IllegalStateException expected) {}
-    
-    try {
-      new DocTermOrds(ar, null, "binary");
-      fail();
-    } catch (IllegalStateException expected) {}
-    
-    Bits bits = FieldCache.DEFAULT.getDocsWithField(ar, "binary");
-    assertTrue(bits.get(0));
-    
-    // Sorted type: can be retrieved via getTerms(), getTermsIndex(), getDocTermOrds()
-    try {
-      FieldCache.DEFAULT.getInts(ar, "sorted", false);
-      fail();
-    } catch (IllegalStateException expected) {}
-    
-    try {
-      new DocTermOrds(ar, null, "sorted");
-      fail();
-    } catch (IllegalStateException expected) {}
-    
-    binary = FieldCache.DEFAULT.getTerms(ar, "sorted", true);
-    binary.get(0, scratch);
-    assertEquals("sorted value", scratch.utf8ToString());
-    
-    SortedDocValues sorted = FieldCache.DEFAULT.getTermsIndex(ar, "sorted");
-    assertEquals(0, sorted.getOrd(0));
-    assertEquals(1, sorted.getValueCount());
-    sorted.get(0, scratch);
-    assertEquals("sorted value", scratch.utf8ToString());
-    
-    SortedSetDocValues sortedSet = FieldCache.DEFAULT.getDocTermOrds(ar, "sorted");
-    sortedSet.setDocument(0);
-    assertEquals(0, sortedSet.nextOrd());
-    assertEquals(SortedSetDocValues.NO_MORE_ORDS, sortedSet.nextOrd());
-    assertEquals(1, sortedSet.getValueCount());
-    
-    bits = FieldCache.DEFAULT.getDocsWithField(ar, "sorted");
-    assertTrue(bits.get(0));
-    
-    // Numeric type: can be retrieved via getInts() and so on
-    Ints numeric = FieldCache.DEFAULT.getInts(ar, "numeric", false);
-    assertEquals(42, numeric.get(0));
-    
-    try {
-      FieldCache.DEFAULT.getTerms(ar, "numeric", true);
-      fail();
-    } catch (IllegalStateException expected) {}
-    
-    try {
-      FieldCache.DEFAULT.getTermsIndex(ar, "numeric");
-      fail();
-    } catch (IllegalStateException expected) {}
-    
-    try {
-      FieldCache.DEFAULT.getDocTermOrds(ar, "numeric");
-      fail();
-    } catch (IllegalStateException expected) {}
-    
-    try {
-      new DocTermOrds(ar, null, "numeric");
-      fail();
-    } catch (IllegalStateException expected) {}
-    
-    bits = FieldCache.DEFAULT.getDocsWithField(ar, "numeric");
-    assertTrue(bits.get(0));
-    
-    // SortedSet type: can be retrieved via getDocTermOrds() 
-    if (defaultCodecSupportsSortedSet()) {
-      try {
-        FieldCache.DEFAULT.getInts(ar, "sortedset", false);
-        fail();
-      } catch (IllegalStateException expected) {}
-    
-      try {
-        FieldCache.DEFAULT.getTerms(ar, "sortedset", true);
-        fail();
-      } catch (IllegalStateException expected) {}
-    
-      try {
-        FieldCache.DEFAULT.getTermsIndex(ar, "sortedset");
-        fail();
-      } catch (IllegalStateException expected) {}
-      
-      try {
-        new DocTermOrds(ar, null, "sortedset");
-        fail();
-      } catch (IllegalStateException expected) {}
-    
-      sortedSet = FieldCache.DEFAULT.getDocTermOrds(ar, "sortedset");
-      sortedSet.setDocument(0);
-      assertEquals(0, sortedSet.nextOrd());
-      assertEquals(1, sortedSet.nextOrd());
-      assertEquals(SortedSetDocValues.NO_MORE_ORDS, sortedSet.nextOrd());
-      assertEquals(2, sortedSet.getValueCount());
-    
-      bits = FieldCache.DEFAULT.getDocsWithField(ar, "sortedset");
-      assertTrue(bits.get(0));
-    }
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testNonexistantFields() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    iw.addDocument(doc);
-    DirectoryReader ir = iw.getReader();
-    iw.shutdown();
-    
-    AtomicReader ar = getOnlySegmentReader(ir);
-    
-    final FieldCache cache = FieldCache.DEFAULT;
-    cache.purgeAllCaches();
-    assertEquals(0, cache.getCacheEntries().length);
-    
-    Ints ints = cache.getInts(ar, "bogusints", true);
-    assertEquals(0, ints.get(0));
-    
-    Longs longs = cache.getLongs(ar, "boguslongs", true);
-    assertEquals(0, longs.get(0));
-    
-    Floats floats = cache.getFloats(ar, "bogusfloats", true);
-    assertEquals(0, floats.get(0), 0.0f);
-    
-    Doubles doubles = cache.getDoubles(ar, "bogusdoubles", true);
-    assertEquals(0, doubles.get(0), 0.0D);
-    
-    BytesRef scratch = new BytesRef();
-    BinaryDocValues binaries = cache.getTerms(ar, "bogusterms", true);
-    binaries.get(0, scratch);
-    assertEquals(0, scratch.length);
-    
-    SortedDocValues sorted = cache.getTermsIndex(ar, "bogustermsindex");
-    assertEquals(-1, sorted.getOrd(0));
-    sorted.get(0, scratch);
-    assertEquals(0, scratch.length);
-    
-    SortedSetDocValues sortedSet = cache.getDocTermOrds(ar, "bogusmultivalued");
-    sortedSet.setDocument(0);
-    assertEquals(SortedSetDocValues.NO_MORE_ORDS, sortedSet.nextOrd());
-    
-    Bits bits = cache.getDocsWithField(ar, "bogusbits");
-    assertFalse(bits.get(0));
-    
-    // check that we cached nothing
-    assertEquals(0, cache.getCacheEntries().length);
-    ir.close();
-    dir.close();
-  }
-  
-  public void testNonIndexedFields() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new StoredField("bogusbytes", "bogus"));
-    doc.add(new StoredField("bogusshorts", "bogus"));
-    doc.add(new StoredField("bogusints", "bogus"));
-    doc.add(new StoredField("boguslongs", "bogus"));
-    doc.add(new StoredField("bogusfloats", "bogus"));
-    doc.add(new StoredField("bogusdoubles", "bogus"));
-    doc.add(new StoredField("bogusterms", "bogus"));
-    doc.add(new StoredField("bogustermsindex", "bogus"));
-    doc.add(new StoredField("bogusmultivalued", "bogus"));
-    doc.add(new StoredField("bogusbits", "bogus"));
-    iw.addDocument(doc);
-    DirectoryReader ir = iw.getReader();
-    iw.shutdown();
-    
-    AtomicReader ar = getOnlySegmentReader(ir);
-    
-    final FieldCache cache = FieldCache.DEFAULT;
-    cache.purgeAllCaches();
-    assertEquals(0, cache.getCacheEntries().length);
-    
-    Ints ints = cache.getInts(ar, "bogusints", true);
-    assertEquals(0, ints.get(0));
-    
-    Longs longs = cache.getLongs(ar, "boguslongs", true);
-    assertEquals(0, longs.get(0));
-    
-    Floats floats = cache.getFloats(ar, "bogusfloats", true);
-    assertEquals(0, floats.get(0), 0.0f);
-    
-    Doubles doubles = cache.getDoubles(ar, "bogusdoubles", true);
-    assertEquals(0, doubles.get(0), 0.0D);
-    
-    BytesRef scratch = new BytesRef();
-    BinaryDocValues binaries = cache.getTerms(ar, "bogusterms", true);
-    binaries.get(0, scratch);
-    assertEquals(0, scratch.length);
-    
-    SortedDocValues sorted = cache.getTermsIndex(ar, "bogustermsindex");
-    assertEquals(-1, sorted.getOrd(0));
-    sorted.get(0, scratch);
-    assertEquals(0, scratch.length);
-    
-    SortedSetDocValues sortedSet = cache.getDocTermOrds(ar, "bogusmultivalued");
-    sortedSet.setDocument(0);
-    assertEquals(SortedSetDocValues.NO_MORE_ORDS, sortedSet.nextOrd());
-    
-    Bits bits = cache.getDocsWithField(ar, "bogusbits");
-    assertFalse(bits.get(0));
-    
-    // check that we cached nothing
-    assertEquals(0, cache.getCacheEntries().length);
-    ir.close();
-    dir.close();
-  }
-
-  // Make sure that the use of GrowableWriter doesn't prevent from using the full long range
-  public void testLongFieldCache() throws IOException {
-    Directory dir = newDirectory();
-    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    cfg.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, cfg);
-    Document doc = new Document();
-    LongField field = new LongField("f", 0L, Store.YES);
-    doc.add(field);
-    final long[] values = new long[TestUtil.nextInt(random(), 1, 10)];
-    for (int i = 0; i < values.length; ++i) {
-      final long v;
-      switch (random().nextInt(10)) {
-        case 0:
-          v = Long.MIN_VALUE;
-          break;
-        case 1:
-          v = 0;
-          break;
-        case 2:
-          v = Long.MAX_VALUE;
-          break;
-        default:
-          v = TestUtil.nextLong(random(), -10, 10);
-          break;
-      }
-      values[i] = v;
-      if (v == 0 && random().nextBoolean()) {
-        // missing
-        iw.addDocument(new Document());
-      } else {
-        field.setLongValue(v);
-        iw.addDocument(doc);
-      }
-    }
-    iw.forceMerge(1);
-    final DirectoryReader reader = iw.getReader();
-    final FieldCache.Longs longs = FieldCache.DEFAULT.getLongs(getOnlySegmentReader(reader), "f", false);
-    for (int i = 0; i < values.length; ++i) {
-      assertEquals(values[i], longs.get(i));
-    }
-    reader.close();
-    iw.shutdown();
-    dir.close();
-  }
-
-  // Make sure that the use of GrowableWriter doesn't prevent from using the full int range
-  public void testIntFieldCache() throws IOException {
-    Directory dir = newDirectory();
-    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    cfg.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, cfg);
-    Document doc = new Document();
-    IntField field = new IntField("f", 0, Store.YES);
-    doc.add(field);
-    final int[] values = new int[TestUtil.nextInt(random(), 1, 10)];
-    for (int i = 0; i < values.length; ++i) {
-      final int v;
-      switch (random().nextInt(10)) {
-        case 0:
-          v = Integer.MIN_VALUE;
-          break;
-        case 1:
-          v = 0;
-          break;
-        case 2:
-          v = Integer.MAX_VALUE;
-          break;
-        default:
-          v = TestUtil.nextInt(random(), -10, 10);
-          break;
-      }
-      values[i] = v;
-      if (v == 0 && random().nextBoolean()) {
-        // missing
-        iw.addDocument(new Document());
-      } else {
-        field.setIntValue(v);
-        iw.addDocument(doc);
-      }
-    }
-    iw.forceMerge(1);
-    final DirectoryReader reader = iw.getReader();
-    final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(getOnlySegmentReader(reader), "f", false);
-    for (int i = 0; i < values.length; ++i) {
-      assertEquals(values[i], ints.get(i));
-    }
-    reader.close();
-    iw.shutdown();
-    dir.close();
-  }
-
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java	2014-05-14 03:47:28.698646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java	2014-05-14 03:45:16.506644320 -0400
@@ -23,6 +23,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
@@ -63,67 +64,67 @@
     Query q = new TermQuery(new Term("body","body"));
 
     // test id, bounded on both ends
-    result = search.search(q, FieldCacheRangeFilter.newStringRange("id",minIP,maxIP,T,T), numDocs).scoreDocs;
+    result = search.search(q, DocValuesRangeFilter.newStringRange("id",minIP,maxIP,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",minIP,maxIP,T,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",minIP,maxIP,T,F), numDocs).scoreDocs;
     assertEquals("all but last", numDocs-1, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",minIP,maxIP,F,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",minIP,maxIP,F,T), numDocs).scoreDocs;
     assertEquals("all but first", numDocs-1, result.length);
         
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",minIP,maxIP,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",minIP,maxIP,F,F), numDocs).scoreDocs;
     assertEquals("all but ends", numDocs-2, result.length);
     
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",medIP,maxIP,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",medIP,maxIP,T,T), numDocs).scoreDocs;
     assertEquals("med and up", 1+ maxId-medId, result.length);
         
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",minIP,medIP,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",minIP,medIP,T,T), numDocs).scoreDocs;
     assertEquals("up to med", 1+ medId-minId, result.length);
 
     // unbounded id
 
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",null,null,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",null,null,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",minIP,null,T,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",minIP,null,T,F), numDocs).scoreDocs;
     assertEquals("min and up", numDocs, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",null,maxIP,F,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",null,maxIP,F,T), numDocs).scoreDocs;
     assertEquals("max and down", numDocs, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",minIP,null,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",minIP,null,F,F), numDocs).scoreDocs;
     assertEquals("not min, but up", numDocs-1, result.length);
         
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",null,maxIP,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",null,maxIP,F,F), numDocs).scoreDocs;
     assertEquals("not max, but down", numDocs-1, result.length);
         
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",medIP,maxIP,T,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",medIP,maxIP,T,F), numDocs).scoreDocs;
     assertEquals("med and up, not max", maxId-medId, result.length);
         
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",minIP,medIP,F,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",minIP,medIP,F,T), numDocs).scoreDocs;
     assertEquals("not min, up to med", medId-minId, result.length);
 
     // very small sets
 
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",minIP,minIP,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",minIP,minIP,F,F), numDocs).scoreDocs;
     assertEquals("min,min,F,F", 0, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",medIP,medIP,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",medIP,medIP,F,F), numDocs).scoreDocs;
     assertEquals("med,med,F,F", 0, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",maxIP,maxIP,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",maxIP,maxIP,F,F), numDocs).scoreDocs;
     assertEquals("max,max,F,F", 0, result.length);
                      
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",minIP,minIP,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",minIP,minIP,T,T), numDocs).scoreDocs;
     assertEquals("min,min,T,T", 1, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",null,minIP,F,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",null,minIP,F,T), numDocs).scoreDocs;
     assertEquals("nul,min,F,T", 1, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",maxIP,maxIP,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",maxIP,maxIP,T,T), numDocs).scoreDocs;
     assertEquals("max,max,T,T", 1, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",maxIP,null,T,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",maxIP,null,T,F), numDocs).scoreDocs;
     assertEquals("max,nul,T,T", 1, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("id",medIP,medIP,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("id",medIP,medIP,T,T), numDocs).scoreDocs;
     assertEquals("med,med,T,T", 1, result.length);
   }
 
@@ -145,47 +146,47 @@
 
     // test extremes, bounded on both ends
         
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("rand",minRP,maxRP,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("rand",minRP,maxRP,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("rand",minRP,maxRP,T,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("rand",minRP,maxRP,T,F), numDocs).scoreDocs;
     assertEquals("all but biggest", numDocs-1, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("rand",minRP,maxRP,F,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("rand",minRP,maxRP,F,T), numDocs).scoreDocs;
     assertEquals("all but smallest", numDocs-1, result.length);
         
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("rand",minRP,maxRP,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("rand",minRP,maxRP,F,F), numDocs).scoreDocs;
     assertEquals("all but extremes", numDocs-2, result.length);
     
     // unbounded
 
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("rand",minRP,null,T,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("rand",minRP,null,T,F), numDocs).scoreDocs;
     assertEquals("smallest and up", numDocs, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("rand",null,maxRP,F,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("rand",null,maxRP,F,T), numDocs).scoreDocs;
     assertEquals("biggest and down", numDocs, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("rand",minRP,null,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("rand",minRP,null,F,F), numDocs).scoreDocs;
     assertEquals("not smallest, but up", numDocs-1, result.length);
         
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("rand",null,maxRP,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("rand",null,maxRP,F,F), numDocs).scoreDocs;
     assertEquals("not biggest, but down", numDocs-1, result.length);
         
     // very small sets
 
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("rand",minRP,minRP,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("rand",minRP,minRP,F,F), numDocs).scoreDocs;
     assertEquals("min,min,F,F", 0, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("rand",maxRP,maxRP,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("rand",maxRP,maxRP,F,F), numDocs).scoreDocs;
     assertEquals("max,max,F,F", 0, result.length);
                      
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("rand",minRP,minRP,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("rand",minRP,minRP,T,T), numDocs).scoreDocs;
     assertEquals("min,min,T,T", 1, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("rand",null,minRP,F,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("rand",null,minRP,F,T), numDocs).scoreDocs;
     assertEquals("nul,min,F,T", 1, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("rand",maxRP,maxRP,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("rand",maxRP,maxRP,T,T), numDocs).scoreDocs;
     assertEquals("max,max,T,T", 1, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newStringRange("rand",maxRP,null,T,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newStringRange("rand",maxRP,null,T,F), numDocs).scoreDocs;
     assertEquals("max,nul,T,T", 1, result.length);
   }
   
@@ -208,75 +209,75 @@
 
     // test id, bounded on both ends
         
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",minIdO,maxIdO,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",minIdO,maxIdO,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",minIdO,maxIdO,T,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",minIdO,maxIdO,T,F), numDocs).scoreDocs;
     assertEquals("all but last", numDocs-1, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",minIdO,maxIdO,F,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",minIdO,maxIdO,F,T), numDocs).scoreDocs;
     assertEquals("all but first", numDocs-1, result.length);
         
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",minIdO,maxIdO,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",minIdO,maxIdO,F,F), numDocs).scoreDocs;
     assertEquals("all but ends", numDocs-2, result.length);
     
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",medIdO,maxIdO,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",medIdO,maxIdO,T,T), numDocs).scoreDocs;
     assertEquals("med and up", 1+ maxId-medId, result.length);
         
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",minIdO,medIdO,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",minIdO,medIdO,T,T), numDocs).scoreDocs;
     assertEquals("up to med", 1+ medId-minId, result.length);
     
     // unbounded id
 
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",null,null,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",null,null,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",minIdO,null,T,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",minIdO,null,T,F), numDocs).scoreDocs;
     assertEquals("min and up", numDocs, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",null,maxIdO,F,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",null,maxIdO,F,T), numDocs).scoreDocs;
     assertEquals("max and down", numDocs, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",minIdO,null,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",minIdO,null,F,F), numDocs).scoreDocs;
     assertEquals("not min, but up", numDocs-1, result.length);
         
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",null,maxIdO,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",null,maxIdO,F,F), numDocs).scoreDocs;
     assertEquals("not max, but down", numDocs-1, result.length);
         
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",medIdO,maxIdO,T,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",medIdO,maxIdO,T,F), numDocs).scoreDocs;
     assertEquals("med and up, not max", maxId-medId, result.length);
         
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",minIdO,medIdO,F,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",minIdO,medIdO,F,T), numDocs).scoreDocs;
     assertEquals("not min, up to med", medId-minId, result.length);
 
     // very small sets
 
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",minIdO,minIdO,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",minIdO,minIdO,F,F), numDocs).scoreDocs;
     assertEquals("min,min,F,F", 0, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",medIdO,medIdO,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",medIdO,medIdO,F,F), numDocs).scoreDocs;
     assertEquals("med,med,F,F", 0, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",maxIdO,maxIdO,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",maxIdO,maxIdO,F,F), numDocs).scoreDocs;
     assertEquals("max,max,F,F", 0, result.length);
                      
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",minIdO,minIdO,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",minIdO,minIdO,T,T), numDocs).scoreDocs;
     assertEquals("min,min,T,T", 1, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",null,minIdO,F,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",null,minIdO,F,T), numDocs).scoreDocs;
     assertEquals("nul,min,F,T", 1, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",maxIdO,maxIdO,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",maxIdO,maxIdO,T,T), numDocs).scoreDocs;
     assertEquals("max,max,T,T", 1, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",maxIdO,null,T,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",maxIdO,null,T,F), numDocs).scoreDocs;
     assertEquals("max,nul,T,T", 1, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",medIdO,medIdO,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",medIdO,medIdO,T,T), numDocs).scoreDocs;
     assertEquals("med,med,T,T", 1, result.length);
     
     // special cases
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",Integer.valueOf(Integer.MAX_VALUE),null,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",Integer.valueOf(Integer.MAX_VALUE),null,F,F), numDocs).scoreDocs;
     assertEquals("overflow special case", 0, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",null,Integer.valueOf(Integer.MIN_VALUE),F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",null,Integer.valueOf(Integer.MIN_VALUE),F,F), numDocs).scoreDocs;
     assertEquals("overflow special case", 0, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",maxIdO,minIdO,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",maxIdO,minIdO,T,T), numDocs).scoreDocs;
     assertEquals("inverse range", 0, result.length);
   }
   
@@ -299,75 +300,75 @@
 
     // test id, bounded on both ends
         
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",minIdO,maxIdO,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",minIdO,maxIdO,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",minIdO,maxIdO,T,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",minIdO,maxIdO,T,F), numDocs).scoreDocs;
     assertEquals("all but last", numDocs-1, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",minIdO,maxIdO,F,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",minIdO,maxIdO,F,T), numDocs).scoreDocs;
     assertEquals("all but first", numDocs-1, result.length);
         
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",minIdO,maxIdO,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",minIdO,maxIdO,F,F), numDocs).scoreDocs;
     assertEquals("all but ends", numDocs-2, result.length);
     
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",medIdO,maxIdO,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",medIdO,maxIdO,T,T), numDocs).scoreDocs;
     assertEquals("med and up", 1+ maxId-medId, result.length);
         
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",minIdO,medIdO,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",minIdO,medIdO,T,T), numDocs).scoreDocs;
     assertEquals("up to med", 1+ medId-minId, result.length);
     
     // unbounded id
 
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",null,null,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",null,null,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",minIdO,null,T,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",minIdO,null,T,F), numDocs).scoreDocs;
     assertEquals("min and up", numDocs, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",null,maxIdO,F,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",null,maxIdO,F,T), numDocs).scoreDocs;
     assertEquals("max and down", numDocs, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",minIdO,null,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",minIdO,null,F,F), numDocs).scoreDocs;
     assertEquals("not min, but up", numDocs-1, result.length);
         
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",null,maxIdO,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",null,maxIdO,F,F), numDocs).scoreDocs;
     assertEquals("not max, but down", numDocs-1, result.length);
         
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",medIdO,maxIdO,T,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",medIdO,maxIdO,T,F), numDocs).scoreDocs;
     assertEquals("med and up, not max", maxId-medId, result.length);
         
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",minIdO,medIdO,F,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",minIdO,medIdO,F,T), numDocs).scoreDocs;
     assertEquals("not min, up to med", medId-minId, result.length);
 
     // very small sets
 
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",minIdO,minIdO,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",minIdO,minIdO,F,F), numDocs).scoreDocs;
     assertEquals("min,min,F,F", 0, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",medIdO,medIdO,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",medIdO,medIdO,F,F), numDocs).scoreDocs;
     assertEquals("med,med,F,F", 0, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",maxIdO,maxIdO,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",maxIdO,maxIdO,F,F), numDocs).scoreDocs;
     assertEquals("max,max,F,F", 0, result.length);
                      
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",minIdO,minIdO,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",minIdO,minIdO,T,T), numDocs).scoreDocs;
     assertEquals("min,min,T,T", 1, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",null,minIdO,F,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",null,minIdO,F,T), numDocs).scoreDocs;
     assertEquals("nul,min,F,T", 1, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",maxIdO,maxIdO,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",maxIdO,maxIdO,T,T), numDocs).scoreDocs;
     assertEquals("max,max,T,T", 1, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",maxIdO,null,T,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",maxIdO,null,T,F), numDocs).scoreDocs;
     assertEquals("max,nul,T,T", 1, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",medIdO,medIdO,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",medIdO,medIdO,T,T), numDocs).scoreDocs;
     assertEquals("med,med,T,T", 1, result.length);
     
     // special cases
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",Long.valueOf(Long.MAX_VALUE),null,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",Long.valueOf(Long.MAX_VALUE),null,F,F), numDocs).scoreDocs;
     assertEquals("overflow special case", 0, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",null,Long.valueOf(Long.MIN_VALUE),F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",null,Long.valueOf(Long.MIN_VALUE),F,F), numDocs).scoreDocs;
     assertEquals("overflow special case", 0, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newLongRange("id_long",maxIdO,minIdO,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newLongRange("id_long",maxIdO,minIdO,T,T), numDocs).scoreDocs;
     assertEquals("inverse range", 0, result.length);
   }
   
@@ -386,19 +387,19 @@
     ScoreDoc[] result;
     Query q = new TermQuery(new Term("body","body"));
 
-    result = search.search(q,FieldCacheRangeFilter.newFloatRange("id_float",minIdO,medIdO,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newFloatRange("id_float",minIdO,medIdO,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs/2, result.length);
     int count = 0;
-    result = search.search(q,FieldCacheRangeFilter.newFloatRange("id_float",null,medIdO,F,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newFloatRange("id_float",null,medIdO,F,T), numDocs).scoreDocs;
     count += result.length;
-    result = search.search(q,FieldCacheRangeFilter.newFloatRange("id_float",medIdO,null,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newFloatRange("id_float",medIdO,null,F,F), numDocs).scoreDocs;
     count += result.length;
     assertEquals("sum of two concenatted ranges", numDocs, count);
-    result = search.search(q,FieldCacheRangeFilter.newFloatRange("id_float",null,null,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newFloatRange("id_float",null,null,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newFloatRange("id_float",Float.valueOf(Float.POSITIVE_INFINITY),null,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newFloatRange("id_float",Float.valueOf(Float.POSITIVE_INFINITY),null,F,F), numDocs).scoreDocs;
     assertEquals("infinity special case", 0, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newFloatRange("id_float",null,Float.valueOf(Float.NEGATIVE_INFINITY),F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newFloatRange("id_float",null,Float.valueOf(Float.NEGATIVE_INFINITY),F,F), numDocs).scoreDocs;
     assertEquals("infinity special case", 0, result.length);
   }
   
@@ -415,19 +416,19 @@
     ScoreDoc[] result;
     Query q = new TermQuery(new Term("body","body"));
 
-    result = search.search(q,FieldCacheRangeFilter.newDoubleRange("id_double",minIdO,medIdO,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newDoubleRange("id_double",minIdO,medIdO,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs/2, result.length);
     int count = 0;
-    result = search.search(q,FieldCacheRangeFilter.newDoubleRange("id_double",null,medIdO,F,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newDoubleRange("id_double",null,medIdO,F,T), numDocs).scoreDocs;
     count += result.length;
-    result = search.search(q,FieldCacheRangeFilter.newDoubleRange("id_double",medIdO,null,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newDoubleRange("id_double",medIdO,null,F,F), numDocs).scoreDocs;
     count += result.length;
     assertEquals("sum of two concenatted ranges", numDocs, count);
-    result = search.search(q,FieldCacheRangeFilter.newDoubleRange("id_double",null,null,T,T), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newDoubleRange("id_double",null,null,T,T), numDocs).scoreDocs;
     assertEquals("find all", numDocs, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newDoubleRange("id_double",Double.valueOf(Double.POSITIVE_INFINITY),null,F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newDoubleRange("id_double",Double.valueOf(Double.POSITIVE_INFINITY),null,F,F), numDocs).scoreDocs;
     assertEquals("infinity special case", 0, result.length);
-    result = search.search(q,FieldCacheRangeFilter.newDoubleRange("id_double",null, Double.valueOf(Double.NEGATIVE_INFINITY),F,F), numDocs).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newDoubleRange("id_double",null, Double.valueOf(Double.NEGATIVE_INFINITY),F,F), numDocs).scoreDocs;
     assertEquals("infinity special case", 0, result.length);
   }
   
@@ -440,6 +441,7 @@
     for (int d = -20; d <= 20; d++) {
       Document doc = new Document();
       doc.add(new IntField("id_int", d, Field.Store.NO));
+      doc.add(new NumericDocValuesField("id_int", d));
       doc.add(newStringField("body", "body", Field.Store.NO));
       writer.addDocument(doc);
     }
@@ -457,19 +459,19 @@
     ScoreDoc[] result;
     Query q = new TermQuery(new Term("body","body"));
 
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",-20,20,T,T), 100).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",-20,20,T,T), 100).scoreDocs;
     assertEquals("find all", 40, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",0,20,T,T), 100).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",0,20,T,T), 100).scoreDocs;
     assertEquals("find all", 20, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",-20,0,T,T), 100).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",-20,0,T,T), 100).scoreDocs;
     assertEquals("find all", 20, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",10,20,T,T), 100).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",10,20,T,T), 100).scoreDocs;
     assertEquals("find all", 11, result.length);
 
-    result = search.search(q,FieldCacheRangeFilter.newIntRange("id_int",-20,-10,T,T), 100).scoreDocs;
+    result = search.search(q,DocValuesRangeFilter.newIntRange("id_int",-20,-10,T,T), 100).scoreDocs;
     assertEquals("find all", 11, result.length);
     reader.close();
     dir.close();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRewriteMethod.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRewriteMethod.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRewriteMethod.java	2014-05-14 03:47:28.694646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRewriteMethod.java	2014-05-14 03:45:16.502644320 -0400
@@ -31,7 +31,7 @@
   @Override
   protected void assertSame(String regexp) throws IOException {   
     RegexpQuery fieldCache = new RegexpQuery(new Term(fieldName, regexp), RegExp.NONE);
-    fieldCache.setRewriteMethod(new FieldCacheRewriteMethod());
+    fieldCache.setRewriteMethod(new DocValuesRewriteMethod());
     
     RegexpQuery filter = new RegexpQuery(new Term(fieldName, regexp), RegExp.NONE);
     filter.setRewriteMethod(MultiTermQuery.CONSTANT_SCORE_FILTER_REWRITE);
@@ -49,9 +49,9 @@
     assertEquals(a1, a2);
     assertFalse(a1.equals(b));
     
-    a1.setRewriteMethod(new FieldCacheRewriteMethod());
-    a2.setRewriteMethod(new FieldCacheRewriteMethod());
-    b.setRewriteMethod(new FieldCacheRewriteMethod());
+    a1.setRewriteMethod(new DocValuesRewriteMethod());
+    a2.setRewriteMethod(new DocValuesRewriteMethod());
+    b.setRewriteMethod(new DocValuesRewriteMethod());
     assertEquals(a1, a2);
     assertFalse(a1.equals(b));
     QueryUtils.check(a1);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheTermsFilter.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheTermsFilter.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheTermsFilter.java	2014-05-14 03:47:28.698646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheTermsFilter.java	2014-05-14 03:45:16.506644320 -0400
@@ -18,6 +18,8 @@
  */
 
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.SortedDocValuesField;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LuceneTestCase;
 
 import org.apache.lucene.document.Document;
@@ -31,10 +33,11 @@
 /**
  * A basic unit test for FieldCacheTermsFilter
  *
- * @see org.apache.lucene.search.FieldCacheTermsFilter
+ * @see org.apache.lucene.search.DocValuesTermsFilter
  */
 public class TestFieldCacheTermsFilter extends LuceneTestCase {
   public void testMissingTerms() throws Exception {
+    assumeTrue("requires support for missing values", defaultCodecSupportsMissingDocValues());
     String fieldName = "field1";
     Directory rd = newDirectory();
     RandomIndexWriter w = new RandomIndexWriter(random(), rd);
@@ -42,6 +45,7 @@
       Document doc = new Document();
       int term = i * 10; //terms are units of 10;
       doc.add(newStringField(fieldName, "" + term, Field.Store.YES));
+      doc.add(new SortedDocValuesField(fieldName, new BytesRef("" + term)));
       w.addDocument(doc);
     }
     IndexReader reader = w.getReader();
@@ -54,18 +58,18 @@
 
     List<String> terms = new ArrayList<>();
     terms.add("5");
-    results = searcher.search(q, new FieldCacheTermsFilter(fieldName,  terms.toArray(new String[0])), numDocs).scoreDocs;
+    results = searcher.search(q, new DocValuesTermsFilter(fieldName,  terms.toArray(new String[0])), numDocs).scoreDocs;
     assertEquals("Must match nothing", 0, results.length);
 
     terms = new ArrayList<>();
     terms.add("10");
-    results = searcher.search(q, new FieldCacheTermsFilter(fieldName,  terms.toArray(new String[0])), numDocs).scoreDocs;
+    results = searcher.search(q, new DocValuesTermsFilter(fieldName,  terms.toArray(new String[0])), numDocs).scoreDocs;
     assertEquals("Must match 1", 1, results.length);
 
     terms = new ArrayList<>();
     terms.add("10");
     terms.add("20");
-    results = searcher.search(q, new FieldCacheTermsFilter(fieldName,  terms.toArray(new String[0])), numDocs).scoreDocs;
+    results = searcher.search(q, new DocValuesTermsFilter(fieldName,  terms.toArray(new String[0])), numDocs).scoreDocs;
     assertEquals("Must match 2", 2, results.length);
 
     reader.close();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestFieldValueFilter.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestFieldValueFilter.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestFieldValueFilter.java	2014-05-14 03:47:28.694646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestFieldValueFilter.java	2014-05-12 13:28:34.208244569 -0400
@@ -21,16 +21,20 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 
 /**
  * 
  */
+@SuppressCodecs({"Lucene40", "Lucene41", "Lucene42"}) // suppress codecs without missing
 public class TestFieldValueFilter extends LuceneTestCase {
 
   public void testFieldValueFilterNoValue() throws IOException {
@@ -96,9 +100,12 @@
       if (random().nextBoolean()) {
         docStates[i] = 1;
         doc.add(newTextField("some", "value", Field.Store.YES));
+        doc.add(new SortedDocValuesField("some", new BytesRef("value")));
       }
       doc.add(newTextField("all", "test", Field.Store.NO));
+      doc.add(new SortedDocValuesField("all", new BytesRef("test")));
       doc.add(newTextField("id", "" + i, Field.Store.YES));
+      doc.add(new SortedDocValuesField("id", new BytesRef("" + i)));
       writer.addDocument(doc);
     }
     writer.commit();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java	2014-05-14 03:47:28.694646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java	2014-05-12 13:28:34.324244571 -0400
@@ -565,46 +565,6 @@
     testFloatRange(2);
   }
   
-  private void testSorting(int precisionStep) throws Exception {
-    String field="field"+precisionStep;
-    // 10 random tests, the index order is ascending,
-    // so using a reverse sort field should retun descending documents
-    int num = TestUtil.nextInt(random(), 10, 20);
-    for (int i = 0; i < num; i++) {
-      int lower=(int)(random().nextDouble()*noDocs*distance)+startOffset;
-      int upper=(int)(random().nextDouble()*noDocs*distance)+startOffset;
-      if (lower>upper) {
-        int a=lower; lower=upper; upper=a;
-      }
-      Query tq=NumericRangeQuery.newIntRange(field, precisionStep, lower, upper, true, true);
-      TopDocs topDocs = searcher.search(tq, null, noDocs, new Sort(new SortField(field, SortField.Type.INT, true)));
-      if (topDocs.totalHits==0) continue;
-      ScoreDoc[] sd = topDocs.scoreDocs;
-      assertNotNull(sd);
-      int last = searcher.doc(sd[0].doc).getField(field).numericValue().intValue();
-      for (int j=1; j<sd.length; j++) {
-        int act = searcher.doc(sd[j].doc).getField(field).numericValue().intValue();
-        assertTrue("Docs should be sorted backwards", last>act );
-        last=act;
-      }
-    }
-  }
-
-  @Test
-  public void testSorting_8bit() throws Exception {
-    testSorting(8);
-  }
-  
-  @Test
-  public void testSorting_4bit() throws Exception {
-    testSorting(4);
-  }
-  
-  @Test
-  public void testSorting_2bit() throws Exception {
-    testSorting(2);
-  }
-  
   @Test
   public void testEqualsAndHash() throws Exception {
     QueryUtils.checkHashEquals(NumericRangeQuery.newIntRange("test1", 4, 10, 20, true, true));


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java	2014-05-14 03:47:28.694646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java	2014-05-12 13:28:34.324244571 -0400
@@ -38,7 +38,6 @@
 import org.apache.lucene.util.NumericUtils;
 import org.apache.lucene.util.TestNumericUtils; // NaN arrays
 import org.apache.lucene.util.TestUtil;
-import org.apache.lucene.util.TestUtil;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
@@ -608,51 +607,6 @@
     testDoubleRange(2);
   }
   
-  private void testSorting(int precisionStep) throws Exception {
-    String field="field"+precisionStep;
-    // 10 random tests, the index order is ascending,
-    // so using a reverse sort field should retun descending documents
-    int num = TestUtil.nextInt(random(), 10, 20);
-    for (int i = 0; i < num; i++) {
-      long lower=(long)(random().nextDouble()*noDocs*distance)+startOffset;
-      long upper=(long)(random().nextDouble()*noDocs*distance)+startOffset;
-      if (lower>upper) {
-        long a=lower; lower=upper; upper=a;
-      }
-      Query tq=NumericRangeQuery.newLongRange(field, precisionStep, lower, upper, true, true);
-      TopDocs topDocs = searcher.search(tq, null, noDocs, new Sort(new SortField(field, SortField.Type.LONG, true)));
-      if (topDocs.totalHits==0) continue;
-      ScoreDoc[] sd = topDocs.scoreDocs;
-      assertNotNull(sd);
-      long last=searcher.doc(sd[0].doc).getField(field).numericValue().longValue();
-      for (int j=1; j<sd.length; j++) {
-        long act=searcher.doc(sd[j].doc).getField(field).numericValue().longValue();
-        assertTrue("Docs should be sorted backwards", last>act );
-        last=act;
-      }
-    }
-  }
-
-  @Test
-  public void testSorting_8bit() throws Exception {
-    testSorting(8);
-  }
-  
-  @Test
-  public void testSorting_6bit() throws Exception {
-    testSorting(6);
-  }
-  
-  @Test
-  public void testSorting_4bit() throws Exception {
-    testSorting(4);
-  }
-  
-  @Test
-  public void testSorting_2bit() throws Exception {
-    testSorting(2);
-  }
-  
   @Test
   public void testEqualsAndHash() throws Exception {
     QueryUtils.checkHashEquals(NumericRangeQuery.newLongRange("test1", 4, 10L, 20L, true, true));


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestRegexpRandom2.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestRegexpRandom2.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestRegexpRandom2.java	2014-05-14 03:47:28.694646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestRegexpRandom2.java	2014-05-12 13:28:34.328244571 -0400
@@ -26,6 +26,7 @@
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.index.FilteredTermsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -66,11 +67,14 @@
     Document doc = new Document();
     Field field = newStringField(fieldName, "", Field.Store.NO);
     doc.add(field);
+    Field dvField = new SortedDocValuesField(fieldName, new BytesRef());
+    doc.add(dvField);
     List<String> terms = new ArrayList<>();
     int num = atLeast(200);
     for (int i = 0; i < num; i++) {
       String s = TestUtil.randomUnicodeString(random());
       field.setStringValue(s);
+      dvField.setBytesValue(new BytesRef(s));
       terms.add(s);
       writer.addDocument(doc);
     }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestSortDocValues.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestSortDocValues.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestSortDocValues.java	2014-05-14 03:47:28.698646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestSortDocValues.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,804 +0,0 @@
-package org.apache.lucene.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.document.BinaryDocValuesField;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.DoubleDocValuesField;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FloatDocValuesField;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.document.SortedDocValuesField;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
-
-/** Tests basic sorting on docvalues fields.
- * These are mostly like TestSort's tests, except each test
- * indexes the field up-front as docvalues, and checks no fieldcaches were made */
-@SuppressCodecs({"Lucene40", "Lucene41", "Lucene42"}) // avoid codecs that don't support "missing"
-public class TestSortDocValues extends LuceneTestCase {
-  
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    // ensure there is nothing in fieldcache before test starts
-    FieldCache.DEFAULT.purgeAllCaches();
-  }
-  
-  private void assertNoFieldCaches() {
-    // docvalues sorting should NOT create any fieldcache entries!
-    assertEquals(0, FieldCache.DEFAULT.getCacheEntries().length);
-  }
-
-  /** Tests sorting on type string */
-  public void testString() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new SortedDocValuesField("value", new BytesRef("foo")));
-    doc.add(newStringField("value", "foo", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedDocValuesField("value", new BytesRef("bar")));
-    doc.add(newStringField("value", "bar", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.STRING));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'bar' comes before 'foo'
-    assertEquals("bar", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("foo", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertNoFieldCaches();
-    
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests reverse sorting on type string */
-  public void testStringReverse() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new SortedDocValuesField("value", new BytesRef("bar")));
-    doc.add(newStringField("value", "bar", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedDocValuesField("value", new BytesRef("foo")));
-    doc.add(newStringField("value", "foo", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.STRING, true));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'foo' comes after 'bar' in reverse order
-    assertEquals("foo", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("bar", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type string_val */
-  public void testStringVal() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new BinaryDocValuesField("value", new BytesRef("foo")));
-    doc.add(newStringField("value", "foo", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new BinaryDocValuesField("value", new BytesRef("bar")));
-    doc.add(newStringField("value", "bar", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.STRING_VAL));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'bar' comes before 'foo'
-    assertEquals("bar", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("foo", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests reverse sorting on type string_val */
-  public void testStringValReverse() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new BinaryDocValuesField("value", new BytesRef("bar")));
-    doc.add(newStringField("value", "bar", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new BinaryDocValuesField("value", new BytesRef("foo")));
-    doc.add(newStringField("value", "foo", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.STRING_VAL, true));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'foo' comes after 'bar' in reverse order
-    assertEquals("foo", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("bar", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type string_val, but with a SortedDocValuesField */
-  public void testStringValSorted() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new SortedDocValuesField("value", new BytesRef("foo")));
-    doc.add(newStringField("value", "foo", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedDocValuesField("value", new BytesRef("bar")));
-    doc.add(newStringField("value", "bar", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.STRING_VAL));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'bar' comes before 'foo'
-    assertEquals("bar", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("foo", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests reverse sorting on type string_val, but with a SortedDocValuesField */
-  public void testStringValReverseSorted() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new SortedDocValuesField("value", new BytesRef("bar")));
-    doc.add(newStringField("value", "bar", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedDocValuesField("value", new BytesRef("foo")));
-    doc.add(newStringField("value", "foo", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.STRING_VAL, true));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'foo' comes after 'bar' in reverse order
-    assertEquals("foo", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("bar", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type int */
-  public void testInt() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new NumericDocValuesField("value", 300000));
-    doc.add(newStringField("value", "300000", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new NumericDocValuesField("value", -1));
-    doc.add(newStringField("value", "-1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new NumericDocValuesField("value", 4));
-    doc.add(newStringField("value", "4", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.INT));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // numeric order
-    assertEquals("-1", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("4", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("300000", searcher.doc(td.scoreDocs[2].doc).get("value"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type int in reverse */
-  public void testIntReverse() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new NumericDocValuesField("value", 300000));
-    doc.add(newStringField("value", "300000", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new NumericDocValuesField("value", -1));
-    doc.add(newStringField("value", "-1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new NumericDocValuesField("value", 4));
-    doc.add(newStringField("value", "4", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.INT, true));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // reverse numeric order
-    assertEquals("300000", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("4", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("-1", searcher.doc(td.scoreDocs[2].doc).get("value"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type int with a missing value */
-  public void testIntMissing() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new NumericDocValuesField("value", -1));
-    doc.add(newStringField("value", "-1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new NumericDocValuesField("value", 4));
-    doc.add(newStringField("value", "4", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.INT));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // null is treated as a 0
-    assertEquals("-1", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertNull(searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("4", searcher.doc(td.scoreDocs[2].doc).get("value"));
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type int, specifying the missing value should be treated as Integer.MAX_VALUE */
-  public void testIntMissingLast() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new NumericDocValuesField("value", -1));
-    doc.add(newStringField("value", "-1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new NumericDocValuesField("value", 4));
-    doc.add(newStringField("value", "4", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    SortField sortField = new SortField("value", SortField.Type.INT);
-    sortField.setMissingValue(Integer.MAX_VALUE);
-    Sort sort = new Sort(sortField);
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // null is treated as a Integer.MAX_VALUE
-    assertEquals("-1", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("4", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertNull(searcher.doc(td.scoreDocs[2].doc).get("value"));
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type long */
-  public void testLong() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new NumericDocValuesField("value", 3000000000L));
-    doc.add(newStringField("value", "3000000000", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new NumericDocValuesField("value", -1));
-    doc.add(newStringField("value", "-1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new NumericDocValuesField("value", 4));
-    doc.add(newStringField("value", "4", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.LONG));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // numeric order
-    assertEquals("-1", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("4", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("3000000000", searcher.doc(td.scoreDocs[2].doc).get("value"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type long in reverse */
-  public void testLongReverse() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new NumericDocValuesField("value", 3000000000L));
-    doc.add(newStringField("value", "3000000000", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new NumericDocValuesField("value", -1));
-    doc.add(newStringField("value", "-1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new NumericDocValuesField("value", 4));
-    doc.add(newStringField("value", "4", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.LONG, true));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // reverse numeric order
-    assertEquals("3000000000", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("4", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("-1", searcher.doc(td.scoreDocs[2].doc).get("value"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type long with a missing value */
-  public void testLongMissing() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new NumericDocValuesField("value", -1));
-    doc.add(newStringField("value", "-1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new NumericDocValuesField("value", 4));
-    doc.add(newStringField("value", "4", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.LONG));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // null is treated as 0
-    assertEquals("-1", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertNull(searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("4", searcher.doc(td.scoreDocs[2].doc).get("value"));
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type long, specifying the missing value should be treated as Long.MAX_VALUE */
-  public void testLongMissingLast() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new NumericDocValuesField("value", -1));
-    doc.add(newStringField("value", "-1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new NumericDocValuesField("value", 4));
-    doc.add(newStringField("value", "4", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    SortField sortField = new SortField("value", SortField.Type.LONG);
-    sortField.setMissingValue(Long.MAX_VALUE);
-    Sort sort = new Sort(sortField);
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // null is treated as Long.MAX_VALUE
-    assertEquals("-1", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("4", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertNull(searcher.doc(td.scoreDocs[2].doc).get("value"));
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type float */
-  public void testFloat() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new FloatDocValuesField("value", 30.1F));
-    doc.add(newStringField("value", "30.1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new FloatDocValuesField("value", -1.3F));
-    doc.add(newStringField("value", "-1.3", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new FloatDocValuesField("value", 4.2F));
-    doc.add(newStringField("value", "4.2", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.FLOAT));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // numeric order
-    assertEquals("-1.3", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("4.2", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("30.1", searcher.doc(td.scoreDocs[2].doc).get("value"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type float in reverse */
-  public void testFloatReverse() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new FloatDocValuesField("value", 30.1F));
-    doc.add(newStringField("value", "30.1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new FloatDocValuesField("value", -1.3F));
-    doc.add(newStringField("value", "-1.3", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new FloatDocValuesField("value", 4.2F));
-    doc.add(newStringField("value", "4.2", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.FLOAT, true));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // reverse numeric order
-    assertEquals("30.1", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("4.2", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("-1.3", searcher.doc(td.scoreDocs[2].doc).get("value"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type float with a missing value */
-  public void testFloatMissing() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new FloatDocValuesField("value", -1.3F));
-    doc.add(newStringField("value", "-1.3", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new FloatDocValuesField("value", 4.2F));
-    doc.add(newStringField("value", "4.2", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.FLOAT));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // null is treated as 0
-    assertEquals("-1.3", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertNull(searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("4.2", searcher.doc(td.scoreDocs[2].doc).get("value"));
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type float, specifying the missing value should be treated as Float.MAX_VALUE */
-  public void testFloatMissingLast() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new FloatDocValuesField("value", -1.3F));
-    doc.add(newStringField("value", "-1.3", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new FloatDocValuesField("value", 4.2F));
-    doc.add(newStringField("value", "4.2", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    SortField sortField = new SortField("value", SortField.Type.FLOAT);
-    sortField.setMissingValue(Float.MAX_VALUE);
-    Sort sort = new Sort(sortField);
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // null is treated as Float.MAX_VALUE
-    assertEquals("-1.3", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("4.2", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertNull(searcher.doc(td.scoreDocs[2].doc).get("value"));
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type double */
-  public void testDouble() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new DoubleDocValuesField("value", 30.1));
-    doc.add(newStringField("value", "30.1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new DoubleDocValuesField("value", -1.3));
-    doc.add(newStringField("value", "-1.3", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new DoubleDocValuesField("value", 4.2333333333333));
-    doc.add(newStringField("value", "4.2333333333333", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new DoubleDocValuesField("value", 4.2333333333332));
-    doc.add(newStringField("value", "4.2333333333332", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.DOUBLE));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(4, td.totalHits);
-    // numeric order
-    assertEquals("-1.3", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("4.2333333333332", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("4.2333333333333", searcher.doc(td.scoreDocs[2].doc).get("value"));
-    assertEquals("30.1", searcher.doc(td.scoreDocs[3].doc).get("value"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type double with +/- zero */
-  public void testDoubleSignedZero() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new DoubleDocValuesField("value", +0D));
-    doc.add(newStringField("value", "+0", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new DoubleDocValuesField("value", -0D));
-    doc.add(newStringField("value", "-0", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.DOUBLE));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // numeric order
-    assertEquals("-0", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("+0", searcher.doc(td.scoreDocs[1].doc).get("value"));
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type double in reverse */
-  public void testDoubleReverse() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new DoubleDocValuesField("value", 30.1));
-    doc.add(newStringField("value", "30.1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new DoubleDocValuesField("value", -1.3));
-    doc.add(newStringField("value", "-1.3", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new DoubleDocValuesField("value", 4.2333333333333));
-    doc.add(newStringField("value", "4.2333333333333", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new DoubleDocValuesField("value", 4.2333333333332));
-    doc.add(newStringField("value", "4.2333333333332", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.DOUBLE, true));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(4, td.totalHits);
-    // numeric order
-    assertEquals("30.1", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("4.2333333333333", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("4.2333333333332", searcher.doc(td.scoreDocs[2].doc).get("value"));
-    assertEquals("-1.3", searcher.doc(td.scoreDocs[3].doc).get("value"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type double with a missing value */
-  public void testDoubleMissing() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new DoubleDocValuesField("value", -1.3));
-    doc.add(newStringField("value", "-1.3", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new DoubleDocValuesField("value", 4.2333333333333));
-    doc.add(newStringField("value", "4.2333333333333", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new DoubleDocValuesField("value", 4.2333333333332));
-    doc.add(newStringField("value", "4.2333333333332", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.DOUBLE));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(4, td.totalHits);
-    // null treated as a 0
-    assertEquals("-1.3", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertNull(searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("4.2333333333332", searcher.doc(td.scoreDocs[2].doc).get("value"));
-    assertEquals("4.2333333333333", searcher.doc(td.scoreDocs[3].doc).get("value"));
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type double, specifying the missing value should be treated as Double.MAX_VALUE */
-  public void testDoubleMissingLast() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new DoubleDocValuesField("value", -1.3));
-    doc.add(newStringField("value", "-1.3", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new DoubleDocValuesField("value", 4.2333333333333));
-    doc.add(newStringField("value", "4.2333333333333", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new DoubleDocValuesField("value", 4.2333333333332));
-    doc.add(newStringField("value", "4.2333333333332", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    SortField sortField = new SortField("value", SortField.Type.DOUBLE);
-    sortField.setMissingValue(Double.MAX_VALUE);
-    Sort sort = new Sort(sortField);
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(4, td.totalHits);
-    // null treated as Double.MAX_VALUE
-    assertEquals("-1.3", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("4.2333333333332", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("4.2333333333333", searcher.doc(td.scoreDocs[2].doc).get("value"));
-    assertNull(searcher.doc(td.scoreDocs[3].doc).get("value"));
-
-    ir.close();
-    dir.close();
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestSortedSetSelector.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestSortedSetSelector.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestSortedSetSelector.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestSortedSetSelector.java	2014-05-13 00:40:46.260946930 -0400
@@ -0,0 +1,556 @@
+package org.apache.lucene.search;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.diskdv.DiskDocValuesFormat;
+import org.apache.lucene.codecs.lucene45.Lucene45DocValuesFormat;
+import org.apache.lucene.codecs.memory.DirectDocValuesFormat;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.SortedSetDocValuesField;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.SortField;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+
+/** Tests for SortedSetSortField selectors other than MIN,
+ *  these require optional codec support (random access to ordinals) */
+public class TestSortedSetSelector extends LuceneTestCase {
+  static Codec savedCodec;
+  
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    savedCodec = Codec.getDefault();
+    // currently only these codecs that support random access ordinals
+    int victim = random().nextInt(3);
+    switch(victim) {
+      case 0:  Codec.setDefault(TestUtil.alwaysDocValuesFormat(new DirectDocValuesFormat()));
+      case 1:  Codec.setDefault(TestUtil.alwaysDocValuesFormat(new DiskDocValuesFormat()));
+      default: Codec.setDefault(TestUtil.alwaysDocValuesFormat(new Lucene45DocValuesFormat()));
+    }
+  }
+  
+  @AfterClass
+  public static void afterClass() throws Exception {
+    Codec.setDefault(savedCodec);
+  }
+  
+  public void testMax() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("foo")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+
+    // slow wrapper does not support random access ordinals (there is no need for that!)
+    IndexSearcher searcher = newSearcher(ir, false);
+    
+    Sort sort = new Sort(new SortedSetSortField("value", false, SortedSetSelector.Type.MAX));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // 'baz' comes before 'foo'
+    assertEquals("2", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    assertEquals("1", searcher.doc(td.scoreDocs[1].doc).get("id"));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMaxReverse() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("foo")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    // slow wrapper does not support random access ordinals (there is no need for that!)
+    IndexSearcher searcher = newSearcher(ir, false);
+    
+    Sort sort = new Sort(new SortedSetSortField("value", true, SortedSetSelector.Type.MAX));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // 'baz' comes before 'foo'
+    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMaxMissingFirst() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("foo")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
+    doc.add(newStringField("id", "3", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+
+    // slow wrapper does not support random access ordinals (there is no need for that!)
+    IndexSearcher searcher = newSearcher(ir, false);
+    
+    SortField sortField = new SortedSetSortField("value", false, SortedSetSelector.Type.MAX);
+    sortField.setMissingValue(SortField.STRING_FIRST);
+    Sort sort = new Sort(sortField);
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // null comes first
+    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    // 'baz' comes before 'foo'
+    assertEquals("3", searcher.doc(td.scoreDocs[1].doc).get("id"));
+    assertEquals("2", searcher.doc(td.scoreDocs[2].doc).get("id"));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMaxMissingLast() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("foo")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
+    doc.add(newStringField("id", "3", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+
+    // slow wrapper does not support random access ordinals (there is no need for that!)
+    IndexSearcher searcher = newSearcher(ir, false);
+    
+    SortField sortField = new SortedSetSortField("value", false, SortedSetSelector.Type.MAX);
+    sortField.setMissingValue(SortField.STRING_LAST);
+    Sort sort = new Sort(sortField);
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // 'baz' comes before 'foo'
+    assertEquals("3", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
+    // null comes last
+    assertEquals("1", searcher.doc(td.scoreDocs[2].doc).get("id"));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMaxSingleton() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    // slow wrapper does not support random access ordinals (there is no need for that!)
+    IndexSearcher searcher = newSearcher(ir, false);
+    Sort sort = new Sort(new SortedSetSortField("value", false, SortedSetSelector.Type.MAX));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // 'bar' comes before 'baz'
+    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMiddleMin() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("a")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("d")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    // slow wrapper does not support random access ordinals (there is no need for that!)
+    IndexSearcher searcher = newSearcher(ir, false);
+    Sort sort = new Sort(new SortedSetSortField("value", false, SortedSetSelector.Type.MIDDLE_MIN));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // 'b' comes before 'c'
+    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMiddleMinReverse() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("a")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("d")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    // slow wrapper does not support random access ordinals (there is no need for that!)
+    IndexSearcher searcher = newSearcher(ir, false);
+    Sort sort = new Sort(new SortedSetSortField("value", true, SortedSetSelector.Type.MIDDLE_MIN));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // 'b' comes before 'c'
+    assertEquals("2", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    assertEquals("1", searcher.doc(td.scoreDocs[1].doc).get("id"));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMiddleMinMissingFirst() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(newStringField("id", "3", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("a")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("d")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    // slow wrapper does not support random access ordinals (there is no need for that!)
+    IndexSearcher searcher = newSearcher(ir, false);
+    SortField sortField = new SortedSetSortField("value", false, SortedSetSelector.Type.MIDDLE_MIN);
+    sortField.setMissingValue(SortField.STRING_FIRST);
+    Sort sort = new Sort(sortField);
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // null comes first
+    assertEquals("3", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    // 'b' comes before 'c'
+    assertEquals("1", searcher.doc(td.scoreDocs[1].doc).get("id"));
+    assertEquals("2", searcher.doc(td.scoreDocs[2].doc).get("id"));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMiddleMinMissingLast() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(newStringField("id", "3", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("a")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("d")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    // slow wrapper does not support random access ordinals (there is no need for that!)
+    IndexSearcher searcher = newSearcher(ir, false);
+    SortField sortField = new SortedSetSortField("value", false, SortedSetSelector.Type.MIDDLE_MIN);
+    sortField.setMissingValue(SortField.STRING_LAST);
+    Sort sort = new Sort(sortField);
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // 'b' comes before 'c'
+    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
+    // null comes last
+    assertEquals("3", searcher.doc(td.scoreDocs[2].doc).get("id"));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMiddleMinSingleton() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    // slow wrapper does not support random access ordinals (there is no need for that!)
+    IndexSearcher searcher = newSearcher(ir, false);
+    Sort sort = new Sort(new SortedSetSortField("value", false, SortedSetSelector.Type.MIDDLE_MIN));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // 'bar' comes before 'baz'
+    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMiddleMax() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("a")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("d")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    // slow wrapper does not support random access ordinals (there is no need for that!)
+    IndexSearcher searcher = newSearcher(ir, false);
+    Sort sort = new Sort(new SortedSetSortField("value", false, SortedSetSelector.Type.MIDDLE_MAX));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // 'b' comes before 'c'
+    assertEquals("2", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    assertEquals("1", searcher.doc(td.scoreDocs[1].doc).get("id"));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMiddleMaxReverse() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("a")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("d")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    // slow wrapper does not support random access ordinals (there is no need for that!)
+    IndexSearcher searcher = newSearcher(ir, false);
+    Sort sort = new Sort(new SortedSetSortField("value", true, SortedSetSelector.Type.MIDDLE_MAX));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // 'b' comes before 'c'
+    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMiddleMaxMissingFirst() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(newStringField("id", "3", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("a")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("d")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    // slow wrapper does not support random access ordinals (there is no need for that!)
+    IndexSearcher searcher = newSearcher(ir, false);
+    SortField sortField = new SortedSetSortField("value", false, SortedSetSelector.Type.MIDDLE_MAX);
+    sortField.setMissingValue(SortField.STRING_FIRST);
+    Sort sort = new Sort(sortField);
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // null comes first
+    assertEquals("3", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    // 'b' comes before 'c'
+    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
+    assertEquals("1", searcher.doc(td.scoreDocs[2].doc).get("id"));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMiddleMaxMissingLast() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(newStringField("id", "3", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("a")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("d")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    // slow wrapper does not support random access ordinals (there is no need for that!)
+    IndexSearcher searcher = newSearcher(ir, false);
+    SortField sortField = new SortedSetSortField("value", false, SortedSetSelector.Type.MIDDLE_MAX);
+    sortField.setMissingValue(SortField.STRING_LAST);
+    Sort sort = new Sort(sortField);
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // 'b' comes before 'c'
+    assertEquals("2", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    assertEquals("1", searcher.doc(td.scoreDocs[1].doc).get("id"));
+    // null comes last
+    assertEquals("3", searcher.doc(td.scoreDocs[2].doc).get("id"));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMiddleMaxSingleton() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    // slow wrapper does not support random access ordinals (there is no need for that!)
+    IndexSearcher searcher = newSearcher(ir, false);
+    Sort sort = new Sort(new SortedSetSortField("value", false, SortedSetSelector.Type.MIDDLE_MAX));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // 'bar' comes before 'baz'
+    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
+
+    ir.close();
+    dir.close();
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestSortedSetSortField.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestSortedSetSortField.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestSortedSetSortField.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestSortedSetSortField.java	2014-05-13 00:38:20.124944385 -0400
@@ -0,0 +1,229 @@
+package org.apache.lucene.search;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.SortedSetDocValuesField;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.SortField;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
+
+/** Simple tests for SortedSetSortField, indexing the sortedset up front */
+@SuppressCodecs({"Lucene40", "Lucene41"}) // avoid codecs that don't support sortedset
+public class TestSortedSetSortField extends LuceneTestCase {
+  
+  public void testEmptyIndex() throws Exception {
+    IndexSearcher empty = newSearcher(new MultiReader());
+    Query query = new TermQuery(new Term("contents", "foo"));
+  
+    Sort sort = new Sort();
+    sort.setSort(new SortedSetSortField("sortedset", false));
+    TopDocs td = empty.search(query, null, 10, sort, true, true);
+    assertEquals(0, td.totalHits);
+    
+    // for an empty index, any selector should work
+    for (SortedSetSelector.Type v : SortedSetSelector.Type.values()) {
+      sort.setSort(new SortedSetSortField("sortedset", false, v));
+      td = empty.search(query, null, 10, sort, true, true);
+      assertEquals(0, td.totalHits);
+    }
+  }
+  
+  public void testEquals() throws Exception {
+    SortField sf = new SortedSetSortField("a", false);
+    assertFalse(sf.equals(null));
+    
+    assertEquals(sf, sf);
+    
+    SortField sf2 = new SortedSetSortField("a", false);
+    assertEquals(sf, sf2);
+    assertEquals(sf.hashCode(), sf2.hashCode());
+    
+    assertFalse(sf.equals(new SortedSetSortField("a", true)));
+    assertFalse(sf.equals(new SortedSetSortField("b", false)));
+    assertFalse(sf.equals(new SortedSetSortField("a", false, SortedSetSelector.Type.MAX)));
+    assertFalse(sf.equals("foo"));
+  }
+  
+  public void testForward() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("foo")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortedSetSortField("value", false));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // 'bar' comes before 'baz'
+    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testReverse() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("foo")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortedSetSortField("value", true));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // 'bar' comes before 'baz'
+    assertEquals("2", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    assertEquals("1", searcher.doc(td.scoreDocs[1].doc).get("id"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMissingFirst() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("foo")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("id", "3", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    SortField sortField = new SortedSetSortField("value", false);
+    sortField.setMissingValue(SortField.STRING_FIRST);
+    Sort sort = new Sort(sortField);
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // 'bar' comes before 'baz'
+    // null comes first
+    assertEquals("3", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    assertEquals("1", searcher.doc(td.scoreDocs[1].doc).get("id"));
+    assertEquals("2", searcher.doc(td.scoreDocs[2].doc).get("id"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMissingLast() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("foo")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("id", "3", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    SortField sortField = new SortedSetSortField("value", false);
+    sortField.setMissingValue(SortField.STRING_LAST);
+    Sort sort = new Sort(sortField);
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // 'bar' comes before 'baz'
+    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
+    // null comes last
+    assertEquals("3", searcher.doc(td.scoreDocs[2].doc).get("id"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  public void testSingleton() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortedSetSortField("value", false));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // 'bar' comes before 'baz'
+    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
+    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
+
+    ir.close();
+    dir.close();
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestSort.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestSort.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestSort.java	2014-05-14 03:47:28.698646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestSort.java	2014-05-12 13:28:34.092244567 -0400
@@ -18,31 +18,20 @@
  */
 
 import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.List;
 
-import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.document.DoubleField;
+import org.apache.lucene.document.DoubleDocValuesField;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FloatField;
-import org.apache.lucene.document.IntField;
-import org.apache.lucene.document.LongField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.document.FloatDocValuesField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.MultiReader;
 import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 
 /*
  * Very simple tests of sorting.
@@ -59,16 +48,19 @@
  *        |
  *       \./
  */
+@SuppressCodecs({"Lucene40", "Lucene41", "Lucene42"}) // avoid codecs that don't support "missing"
 public class TestSort extends LuceneTestCase {
-
+  
   /** Tests sorting on type string */
   public void testString() throws IOException {
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     Document doc = new Document();
+    doc.add(new SortedDocValuesField("value", new BytesRef("foo")));
     doc.add(newStringField("value", "foo", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
+    doc.add(new SortedDocValuesField("value", new BytesRef("bar")));
     doc.add(newStringField("value", "bar", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
@@ -82,36 +74,7 @@
     // 'bar' comes before 'foo'
     assertEquals("bar", searcher.doc(td.scoreDocs[0].doc).get("value"));
     assertEquals("foo", searcher.doc(td.scoreDocs[1].doc).get("value"));
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on type string with a missing value */
-  public void testStringMissing() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("value", "foo", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("value", "bar", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
     
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.STRING));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // null comes first
-    assertNull(searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("bar", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("foo", searcher.doc(td.scoreDocs[2].doc).get("value"));
-
     ir.close();
     dir.close();
   }
@@ -121,9 +84,11 @@
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     Document doc = new Document();
+    doc.add(new SortedDocValuesField("value", new BytesRef("bar")));
     doc.add(newStringField("value", "bar", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
+    doc.add(new SortedDocValuesField("value", new BytesRef("foo")));
     doc.add(newStringField("value", "foo", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
@@ -147,9 +112,11 @@
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     Document doc = new Document();
+    doc.add(new BinaryDocValuesField("value", new BytesRef("foo")));
     doc.add(newStringField("value", "foo", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
+    doc.add(new BinaryDocValuesField("value", new BytesRef("bar")));
     doc.add(newStringField("value", "bar", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
@@ -168,169 +135,72 @@
     dir.close();
   }
   
-  /** Tests sorting on type string_val with a missing value */
-  public void testStringValMissing() throws IOException {
+  /** Tests reverse sorting on type string_val */
+  public void testStringValReverse() throws IOException {
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     Document doc = new Document();
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("value", "foo", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
+    doc.add(new BinaryDocValuesField("value", new BytesRef("bar")));
     doc.add(newStringField("value", "bar", Field.Store.YES));
     writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.STRING_VAL));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // null comes first
-    assertNull(searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("bar", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("foo", searcher.doc(td.scoreDocs[2].doc).get("value"));
-
-    ir.close();
-    dir.close();
-  }
-
-  /** Tests sorting on type string with a missing
-   *  value sorted first */
-  public void testStringMissingSortedFirst() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    writer.addDocument(doc);
     doc = new Document();
+    doc.add(new BinaryDocValuesField("value", new BytesRef("foo")));
     doc.add(newStringField("value", "foo", Field.Store.YES));
     writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("value", "bar", Field.Store.YES));
-    writer.addDocument(doc);
     IndexReader ir = writer.getReader();
     writer.shutdown();
     
     IndexSearcher searcher = newSearcher(ir);
-    SortField sf = new SortField("value", SortField.Type.STRING);
-    Sort sort = new Sort(sf);
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // null comes first
-    assertNull(searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("bar", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("foo", searcher.doc(td.scoreDocs[2].doc).get("value"));
-
-    ir.close();
-    dir.close();
-  }
-
-  /** Tests reverse sorting on type string with a missing
-   *  value sorted first */
-  public void testStringMissingSortedFirstReverse() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("value", "foo", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("value", "bar", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    SortField sf = new SortField("value", SortField.Type.STRING, true);
-    Sort sort = new Sort(sf);
+    Sort sort = new Sort(new SortField("value", SortField.Type.STRING_VAL, true));
 
     TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
+    assertEquals(2, td.totalHits);
+    // 'foo' comes after 'bar' in reverse order
     assertEquals("foo", searcher.doc(td.scoreDocs[0].doc).get("value"));
     assertEquals("bar", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    // null comes last
-    assertNull(searcher.doc(td.scoreDocs[2].doc).get("value"));
 
     ir.close();
     dir.close();
   }
-
-  /** Tests sorting on type string with a missing
-   *  value sorted last */
-  public void testStringValMissingSortedLast() throws IOException {
+  
+  /** Tests sorting on type string_val, but with a SortedDocValuesField */
+  public void testStringValSorted() throws IOException {
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     Document doc = new Document();
-    writer.addDocument(doc);
-    doc = new Document();
+    doc.add(new SortedDocValuesField("value", new BytesRef("foo")));
     doc.add(newStringField("value", "foo", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
+    doc.add(new SortedDocValuesField("value", new BytesRef("bar")));
     doc.add(newStringField("value", "bar", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
     writer.shutdown();
     
     IndexSearcher searcher = newSearcher(ir);
-    SortField sf = new SortField("value", SortField.Type.STRING);
-    sf.setMissingValue(SortField.STRING_LAST);
-    Sort sort = new Sort(sf);
+    Sort sort = new Sort(new SortField("value", SortField.Type.STRING_VAL));
 
     TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
+    assertEquals(2, td.totalHits);
+    // 'bar' comes before 'foo'
     assertEquals("bar", searcher.doc(td.scoreDocs[0].doc).get("value"));
     assertEquals("foo", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    // null comes last
-    assertNull(searcher.doc(td.scoreDocs[2].doc).get("value"));
-
-    ir.close();
-    dir.close();
-  }
-
-  /** Tests reverse sorting on type string with a missing
-   *  value sorted last */
-  public void testStringValMissingSortedLastReverse() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("value", "foo", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("value", "bar", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    SortField sf = new SortField("value", SortField.Type.STRING, true);
-    sf.setMissingValue(SortField.STRING_LAST);
-    Sort sort = new Sort(sf);
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // null comes first
-    assertNull(searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("foo", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("bar", searcher.doc(td.scoreDocs[2].doc).get("value"));
 
     ir.close();
     dir.close();
   }
   
-  /** Tests reverse sorting on type string_val */
-  public void testStringValReverse() throws IOException {
+  /** Tests reverse sorting on type string_val, but with a SortedDocValuesField */
+  public void testStringValReverseSorted() throws IOException {
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     Document doc = new Document();
+    doc.add(new SortedDocValuesField("value", new BytesRef("bar")));
     doc.add(newStringField("value", "bar", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
+    doc.add(new SortedDocValuesField("value", new BytesRef("foo")));
     doc.add(newStringField("value", "foo", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
@@ -349,142 +219,67 @@
     dir.close();
   }
   
-  /** Tests sorting on internal docid order */
-  public void testFieldDoc() throws Exception {
+  /** Tests sorting on type int */
+  public void testInt() throws IOException {
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     Document doc = new Document();
-    doc.add(newStringField("value", "foo", Field.Store.NO));
+    doc.add(new NumericDocValuesField("value", 300000));
+    doc.add(newStringField("value", "300000", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(newStringField("value", "bar", Field.Store.NO));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(SortField.FIELD_DOC);
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // docid 0, then docid 1
-    assertEquals(0, td.scoreDocs[0].doc);
-    assertEquals(1, td.scoreDocs[1].doc);
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting on reverse internal docid order */
-  public void testFieldDocReverse() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(newStringField("value", "foo", Field.Store.NO));
+    doc.add(new NumericDocValuesField("value", -1));
+    doc.add(newStringField("value", "-1", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(newStringField("value", "bar", Field.Store.NO));
+    doc.add(new NumericDocValuesField("value", 4));
+    doc.add(newStringField("value", "4", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
     writer.shutdown();
     
     IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField(null, SortField.Type.DOC, true));
+    Sort sort = new Sort(new SortField("value", SortField.Type.INT));
 
     TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // docid 1, then docid 0
-    assertEquals(1, td.scoreDocs[0].doc);
-    assertEquals(0, td.scoreDocs[1].doc);
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests default sort (by score) */
-  public void testFieldScore() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(newTextField("value", "foo bar bar bar bar", Field.Store.NO));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newTextField("value", "foo foo foo foo foo", Field.Store.NO));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort();
-
-    TopDocs actual = searcher.search(new TermQuery(new Term("value", "foo")), 10, sort);
-    assertEquals(2, actual.totalHits);
-
-    TopDocs expected = searcher.search(new TermQuery(new Term("value", "foo")), 10);
-    // the two topdocs should be the same
-    assertEquals(expected.totalHits, actual.totalHits);
-    for (int i = 0; i < actual.scoreDocs.length; i++) {
-      assertEquals(actual.scoreDocs[i].doc, expected.scoreDocs[i].doc);
-    }
+    assertEquals(3, td.totalHits);
+    // numeric order
+    assertEquals("-1", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("4", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("300000", searcher.doc(td.scoreDocs[2].doc).get("value"));
 
     ir.close();
     dir.close();
   }
   
-  /** Tests default sort (by score) in reverse */
-  public void testFieldScoreReverse() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(newTextField("value", "foo bar bar bar bar", Field.Store.NO));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newTextField("value", "foo foo foo foo foo", Field.Store.NO));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField(null, SortField.Type.SCORE, true));
-
-    TopDocs actual = searcher.search(new TermQuery(new Term("value", "foo")), 10, sort);
-    assertEquals(2, actual.totalHits);
-
-    TopDocs expected = searcher.search(new TermQuery(new Term("value", "foo")), 10);
-    // the two topdocs should be the reverse of each other
-    assertEquals(expected.totalHits, actual.totalHits);
-    assertEquals(actual.scoreDocs[0].doc, expected.scoreDocs[1].doc);
-    assertEquals(actual.scoreDocs[1].doc, expected.scoreDocs[0].doc);
-
-    ir.close();
-    dir.close();
-  }
-
-  /** Tests sorting on type int */
-  public void testInt() throws IOException {
+  /** Tests sorting on type int in reverse */
+  public void testIntReverse() throws IOException {
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     Document doc = new Document();
-    doc.add(new IntField("value", 300000, Field.Store.YES));
+    doc.add(new NumericDocValuesField("value", 300000));
+    doc.add(newStringField("value", "300000", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new IntField("value", -1, Field.Store.YES));
+    doc.add(new NumericDocValuesField("value", -1));
+    doc.add(newStringField("value", "-1", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new IntField("value", 4, Field.Store.YES));
+    doc.add(new NumericDocValuesField("value", 4));
+    doc.add(newStringField("value", "4", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
     writer.shutdown();
     
     IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.INT));
+    Sort sort = new Sort(new SortField("value", SortField.Type.INT, true));
 
     TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
     assertEquals(3, td.totalHits);
-    // numeric order
-    assertEquals("-1", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    // reverse numeric order
+    assertEquals("300000", searcher.doc(td.scoreDocs[0].doc).get("value"));
     assertEquals("4", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("300000", searcher.doc(td.scoreDocs[2].doc).get("value"));
+    assertEquals("-1", searcher.doc(td.scoreDocs[2].doc).get("value"));
 
     ir.close();
     dir.close();
@@ -497,10 +292,12 @@
     Document doc = new Document();
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new IntField("value", -1, Field.Store.YES));
+    doc.add(new NumericDocValuesField("value", -1));
+    doc.add(newStringField("value", "-1", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new IntField("value", 4, Field.Store.YES));
+    doc.add(new NumericDocValuesField("value", 4));
+    doc.add(newStringField("value", "4", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
     writer.shutdown();
@@ -526,10 +323,12 @@
     Document doc = new Document();
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new IntField("value", -1, Field.Store.YES));
+    doc.add(new NumericDocValuesField("value", -1));
+    doc.add(newStringField("value", "-1", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new IntField("value", 4, Field.Store.YES));
+    doc.add(new NumericDocValuesField("value", 4));
+    doc.add(newStringField("value", "4", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
     writer.shutdown();
@@ -550,61 +349,67 @@
     dir.close();
   }
   
-  /** Tests sorting on type int in reverse */
-  public void testIntReverse() throws IOException {
+  /** Tests sorting on type long */
+  public void testLong() throws IOException {
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     Document doc = new Document();
-    doc.add(new IntField("value", 300000, Field.Store.YES));
+    doc.add(new NumericDocValuesField("value", 3000000000L));
+    doc.add(newStringField("value", "3000000000", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new IntField("value", -1, Field.Store.YES));
+    doc.add(new NumericDocValuesField("value", -1));
+    doc.add(newStringField("value", "-1", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new IntField("value", 4, Field.Store.YES));
+    doc.add(new NumericDocValuesField("value", 4));
+    doc.add(newStringField("value", "4", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
     writer.shutdown();
     
     IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.INT, true));
+    Sort sort = new Sort(new SortField("value", SortField.Type.LONG));
 
     TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
     assertEquals(3, td.totalHits);
-    // reverse numeric order
-    assertEquals("300000", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    // numeric order
+    assertEquals("-1", searcher.doc(td.scoreDocs[0].doc).get("value"));
     assertEquals("4", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("-1", searcher.doc(td.scoreDocs[2].doc).get("value"));
+    assertEquals("3000000000", searcher.doc(td.scoreDocs[2].doc).get("value"));
 
     ir.close();
     dir.close();
   }
   
-  /** Tests sorting on type long */
-  public void testLong() throws IOException {
+  /** Tests sorting on type long in reverse */
+  public void testLongReverse() throws IOException {
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     Document doc = new Document();
-    doc.add(new LongField("value", 3000000000L, Field.Store.YES));
+    doc.add(new NumericDocValuesField("value", 3000000000L));
+    doc.add(newStringField("value", "3000000000", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new LongField("value", -1, Field.Store.YES));
+    doc.add(new NumericDocValuesField("value", -1));
+    doc.add(newStringField("value", "-1", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new LongField("value", 4, Field.Store.YES));
+    doc.add(new NumericDocValuesField("value", 4));
+    doc.add(newStringField("value", "4", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
     writer.shutdown();
     
     IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.LONG));
+    Sort sort = new Sort(new SortField("value", SortField.Type.LONG, true));
 
     TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
     assertEquals(3, td.totalHits);
-    // numeric order
-    assertEquals("-1", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    // reverse numeric order
+    assertEquals("3000000000", searcher.doc(td.scoreDocs[0].doc).get("value"));
     assertEquals("4", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("3000000000", searcher.doc(td.scoreDocs[2].doc).get("value"));
+    assertEquals("-1", searcher.doc(td.scoreDocs[2].doc).get("value"));
 
     ir.close();
     dir.close();
@@ -617,10 +422,12 @@
     Document doc = new Document();
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new LongField("value", -1, Field.Store.YES));
+    doc.add(new NumericDocValuesField("value", -1));
+    doc.add(newStringField("value", "-1", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new LongField("value", 4, Field.Store.YES));
+    doc.add(new NumericDocValuesField("value", 4));
+    doc.add(newStringField("value", "4", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
     writer.shutdown();
@@ -646,10 +453,12 @@
     Document doc = new Document();
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new LongField("value", -1, Field.Store.YES));
+    doc.add(new NumericDocValuesField("value", -1));
+    doc.add(newStringField("value", "-1", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new LongField("value", 4, Field.Store.YES));
+    doc.add(new NumericDocValuesField("value", 4));
+    doc.add(newStringField("value", "4", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
     writer.shutdown();
@@ -670,61 +479,67 @@
     dir.close();
   }
   
-  /** Tests sorting on type long in reverse */
-  public void testLongReverse() throws IOException {
+  /** Tests sorting on type float */
+  public void testFloat() throws IOException {
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     Document doc = new Document();
-    doc.add(new LongField("value", 3000000000L, Field.Store.YES));
+    doc.add(new FloatDocValuesField("value", 30.1F));
+    doc.add(newStringField("value", "30.1", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new LongField("value", -1, Field.Store.YES));
+    doc.add(new FloatDocValuesField("value", -1.3F));
+    doc.add(newStringField("value", "-1.3", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new LongField("value", 4, Field.Store.YES));
+    doc.add(new FloatDocValuesField("value", 4.2F));
+    doc.add(newStringField("value", "4.2", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
     writer.shutdown();
     
     IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.LONG, true));
+    Sort sort = new Sort(new SortField("value", SortField.Type.FLOAT));
 
     TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
     assertEquals(3, td.totalHits);
-    // reverse numeric order
-    assertEquals("3000000000", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("4", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("-1", searcher.doc(td.scoreDocs[2].doc).get("value"));
+    // numeric order
+    assertEquals("-1.3", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("4.2", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("30.1", searcher.doc(td.scoreDocs[2].doc).get("value"));
 
     ir.close();
     dir.close();
   }
   
-  /** Tests sorting on type float */
-  public void testFloat() throws IOException {
+  /** Tests sorting on type float in reverse */
+  public void testFloatReverse() throws IOException {
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     Document doc = new Document();
-    doc.add(new FloatField("value", 30.1f, Field.Store.YES));
+    doc.add(new FloatDocValuesField("value", 30.1F));
+    doc.add(newStringField("value", "30.1", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new FloatField("value", -1.3f, Field.Store.YES));
+    doc.add(new FloatDocValuesField("value", -1.3F));
+    doc.add(newStringField("value", "-1.3", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new FloatField("value", 4.2f, Field.Store.YES));
+    doc.add(new FloatDocValuesField("value", 4.2F));
+    doc.add(newStringField("value", "4.2", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
     writer.shutdown();
     
     IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.FLOAT));
+    Sort sort = new Sort(new SortField("value", SortField.Type.FLOAT, true));
 
     TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
     assertEquals(3, td.totalHits);
-    // numeric order
-    assertEquals("-1.3", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    // reverse numeric order
+    assertEquals("30.1", searcher.doc(td.scoreDocs[0].doc).get("value"));
     assertEquals("4.2", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("30.1", searcher.doc(td.scoreDocs[2].doc).get("value"));
+    assertEquals("-1.3", searcher.doc(td.scoreDocs[2].doc).get("value"));
 
     ir.close();
     dir.close();
@@ -737,10 +552,12 @@
     Document doc = new Document();
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new FloatField("value", -1.3f, Field.Store.YES));
+    doc.add(new FloatDocValuesField("value", -1.3F));
+    doc.add(newStringField("value", "-1.3", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new FloatField("value", 4.2f, Field.Store.YES));
+    doc.add(new FloatDocValuesField("value", 4.2F));
+    doc.add(newStringField("value", "4.2", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
     writer.shutdown();
@@ -766,10 +583,12 @@
     Document doc = new Document();
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new FloatField("value", -1.3f, Field.Store.YES));
+    doc.add(new FloatDocValuesField("value", -1.3F));
+    doc.add(newStringField("value", "-1.3", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new FloatField("value", 4.2f, Field.Store.YES));
+    doc.add(new FloatDocValuesField("value", 4.2F));
+    doc.add(newStringField("value", "4.2", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
     writer.shutdown();
@@ -790,51 +609,25 @@
     dir.close();
   }
   
-  /** Tests sorting on type float in reverse */
-  public void testFloatReverse() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new FloatField("value", 30.1f, Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new FloatField("value", -1.3f, Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new FloatField("value", 4.2f, Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.FLOAT, true));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // reverse numeric order
-    assertEquals("30.1", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("4.2", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("-1.3", searcher.doc(td.scoreDocs[2].doc).get("value"));
-
-    ir.close();
-    dir.close();
-  }
-  
   /** Tests sorting on type double */
   public void testDouble() throws IOException {
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     Document doc = new Document();
-    doc.add(new DoubleField("value", 30.1, Field.Store.YES));
+    doc.add(new DoubleDocValuesField("value", 30.1));
+    doc.add(newStringField("value", "30.1", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new DoubleField("value", -1.3, Field.Store.YES));
+    doc.add(new DoubleDocValuesField("value", -1.3));
+    doc.add(newStringField("value", "-1.3", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new DoubleField("value", 4.2333333333333, Field.Store.YES));
+    doc.add(new DoubleDocValuesField("value", 4.2333333333333));
+    doc.add(newStringField("value", "4.2333333333333", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new DoubleField("value", 4.2333333333332, Field.Store.YES));
+    doc.add(new DoubleDocValuesField("value", 4.2333333333332));
+    doc.add(newStringField("value", "4.2333333333332", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
     writer.shutdown();
@@ -859,10 +652,12 @@
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     Document doc = new Document();
-    doc.add(new DoubleField("value", +0d, Field.Store.YES));
+    doc.add(new DoubleDocValuesField("value", +0D));
+    doc.add(newStringField("value", "+0", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new DoubleField("value", -0d, Field.Store.YES));
+    doc.add(new DoubleDocValuesField("value", -0D));
+    doc.add(newStringField("value", "-0", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
     IndexReader ir = writer.getReader();
@@ -874,506 +669,120 @@
     TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
     assertEquals(2, td.totalHits);
     // numeric order
-    double v0 = searcher.doc(td.scoreDocs[0].doc).getField("value").numericValue().doubleValue();
-    double v1 = searcher.doc(td.scoreDocs[1].doc).getField("value").numericValue().doubleValue();
-    assertEquals(0, v0, 0d);
-    assertEquals(0, v1, 0d);
-    // check sign bits
-    assertEquals(1, Double.doubleToLongBits(v0) >>> 63);
-    assertEquals(0, Double.doubleToLongBits(v1) >>> 63);
+    assertEquals("-0", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("+0", searcher.doc(td.scoreDocs[1].doc).get("value"));
 
     ir.close();
     dir.close();
   }
   
-  /** Tests sorting on type double with a missing value */
-  public void testDoubleMissing() throws IOException {
+  /** Tests sorting on type double in reverse */
+  public void testDoubleReverse() throws IOException {
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     Document doc = new Document();
+    doc.add(new DoubleDocValuesField("value", 30.1));
+    doc.add(newStringField("value", "30.1", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new DoubleField("value", -1.3, Field.Store.YES));
+    doc.add(new DoubleDocValuesField("value", -1.3));
+    doc.add(newStringField("value", "-1.3", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new DoubleField("value", 4.2333333333333, Field.Store.YES));
+    doc.add(new DoubleDocValuesField("value", 4.2333333333333));
+    doc.add(newStringField("value", "4.2333333333333", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new DoubleField("value", 4.2333333333332, Field.Store.YES));
+    doc.add(new DoubleDocValuesField("value", 4.2333333333332));
+    doc.add(newStringField("value", "4.2333333333332", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
     writer.shutdown();
     
     IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.DOUBLE));
+    Sort sort = new Sort(new SortField("value", SortField.Type.DOUBLE, true));
 
     TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
     assertEquals(4, td.totalHits);
-    // null treated as a 0
-    assertEquals("-1.3", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertNull(searcher.doc(td.scoreDocs[1].doc).get("value"));
+    // numeric order
+    assertEquals("30.1", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("4.2333333333333", searcher.doc(td.scoreDocs[1].doc).get("value"));
     assertEquals("4.2333333333332", searcher.doc(td.scoreDocs[2].doc).get("value"));
-    assertEquals("4.2333333333333", searcher.doc(td.scoreDocs[3].doc).get("value"));
+    assertEquals("-1.3", searcher.doc(td.scoreDocs[3].doc).get("value"));
 
     ir.close();
     dir.close();
   }
   
-  /** Tests sorting on type double, specifying the missing value should be treated as Double.MAX_VALUE */
-  public void testDoubleMissingLast() throws IOException {
+  /** Tests sorting on type double with a missing value */
+  public void testDoubleMissing() throws IOException {
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     Document doc = new Document();
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new DoubleField("value", -1.3, Field.Store.YES));
+    doc.add(new DoubleDocValuesField("value", -1.3));
+    doc.add(newStringField("value", "-1.3", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new DoubleField("value", 4.2333333333333, Field.Store.YES));
+    doc.add(new DoubleDocValuesField("value", 4.2333333333333));
+    doc.add(newStringField("value", "4.2333333333333", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new DoubleField("value", 4.2333333333332, Field.Store.YES));
+    doc.add(new DoubleDocValuesField("value", 4.2333333333332));
+    doc.add(newStringField("value", "4.2333333333332", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
     writer.shutdown();
     
     IndexSearcher searcher = newSearcher(ir);
-    SortField sortField = new SortField("value", SortField.Type.DOUBLE);
-    sortField.setMissingValue(Double.MAX_VALUE);
-    Sort sort = new Sort(sortField);
+    Sort sort = new Sort(new SortField("value", SortField.Type.DOUBLE));
 
     TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
     assertEquals(4, td.totalHits);
-    // null treated as Double.MAX_VALUE
+    // null treated as a 0
     assertEquals("-1.3", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("4.2333333333332", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("4.2333333333333", searcher.doc(td.scoreDocs[2].doc).get("value"));
-    assertNull(searcher.doc(td.scoreDocs[3].doc).get("value"));
+    assertNull(searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("4.2333333333332", searcher.doc(td.scoreDocs[2].doc).get("value"));
+    assertEquals("4.2333333333333", searcher.doc(td.scoreDocs[3].doc).get("value"));
 
     ir.close();
     dir.close();
   }
   
-  /** Tests sorting on type double in reverse */
-  public void testDoubleReverse() throws IOException {
+  /** Tests sorting on type double, specifying the missing value should be treated as Double.MAX_VALUE */
+  public void testDoubleMissingLast() throws IOException {
     Directory dir = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
     Document doc = new Document();
-    doc.add(new DoubleField("value", 30.1, Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new DoubleField("value", -1.3, Field.Store.YES));
+    doc.add(new DoubleDocValuesField("value", -1.3));
+    doc.add(newStringField("value", "-1.3", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new DoubleField("value", 4.2333333333333, Field.Store.YES));
+    doc.add(new DoubleDocValuesField("value", 4.2333333333333));
+    doc.add(newStringField("value", "4.2333333333333", Field.Store.YES));
     writer.addDocument(doc);
     doc = new Document();
-    doc.add(new DoubleField("value", 4.2333333333332, Field.Store.YES));
+    doc.add(new DoubleDocValuesField("value", 4.2333333333332));
+    doc.add(newStringField("value", "4.2333333333332", Field.Store.YES));
     writer.addDocument(doc);
     IndexReader ir = writer.getReader();
     writer.shutdown();
     
     IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.DOUBLE, true));
+    SortField sortField = new SortField("value", SortField.Type.DOUBLE);
+    sortField.setMissingValue(Double.MAX_VALUE);
+    Sort sort = new Sort(sortField);
 
     TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
     assertEquals(4, td.totalHits);
-    // numeric order
-    assertEquals("30.1", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("4.2333333333333", searcher.doc(td.scoreDocs[1].doc).get("value"));
-    assertEquals("4.2333333333332", searcher.doc(td.scoreDocs[2].doc).get("value"));
-    assertEquals("-1.3", searcher.doc(td.scoreDocs[3].doc).get("value"));
-
-    ir.close();
-    dir.close();
-  }
-  
-  public void testEmptyStringVsNullStringSort() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(
-                        TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    Document doc = new Document();
-    doc.add(newStringField("f", "", Field.Store.NO));
-    doc.add(newStringField("t", "1", Field.Store.NO));
-    w.addDocument(doc);
-    w.commit();
-    doc = new Document();
-    doc.add(newStringField("t", "1", Field.Store.NO));
-    w.addDocument(doc);
-
-    IndexReader r = DirectoryReader.open(w, true);
-    w.shutdown();
-    IndexSearcher s = newSearcher(r);
-    TopDocs hits = s.search(new TermQuery(new Term("t", "1")), null, 10, new Sort(new SortField("f", SortField.Type.STRING)));
-    assertEquals(2, hits.totalHits);
-    // null sorts first
-    assertEquals(1, hits.scoreDocs[0].doc);
-    assertEquals(0, hits.scoreDocs[1].doc);
-    r.close();
-    dir.close();
-  }
-  
-  /** test that we don't throw exception on multi-valued field (LUCENE-2142) */
-  public void testMultiValuedField() throws IOException {
-    Directory indexStore = newDirectory();
-    IndexWriter writer = new IndexWriter(indexStore, newIndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    for(int i=0; i<5; i++) {
-        Document doc = new Document();
-        doc.add(new StringField("string", "a"+i, Field.Store.NO));
-        doc.add(new StringField("string", "b"+i, Field.Store.NO));
-        writer.addDocument(doc);
-    }
-    writer.forceMerge(1); // enforce one segment to have a higher unique term count in all cases
-    writer.shutdown();
-    Sort sort = new Sort(
-        new SortField("string", SortField.Type.STRING),
-        SortField.FIELD_DOC);
-    // this should not throw AIOOBE or RuntimeEx
-    IndexReader reader = DirectoryReader.open(indexStore);
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(new MatchAllDocsQuery(), null, 500, sort);
-    reader.close();
-    indexStore.close();
-  }
-  
-  public void testMaxScore() throws Exception {
-    Directory d = newDirectory();
-    // Not RIW because we need exactly 2 segs:
-    IndexWriter w = new IndexWriter(d, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    int id = 0;
-    for(int seg=0;seg<2;seg++) {
-      for(int docIDX=0;docIDX<10;docIDX++) {
-        Document doc = new Document();
-        doc.add(new IntField("id", docIDX, Field.Store.YES));
-        StringBuilder sb = new StringBuilder();
-        for(int i=0;i<id;i++) {
-          sb.append(' ');
-          sb.append("text");
-        }
-        doc.add(newTextField("body", sb.toString(), Field.Store.NO));
-        w.addDocument(doc);
-        id++;
-      }
-      w.commit();
-    }
-
-    IndexReader r = DirectoryReader.open(w, true);
-    w.shutdown();
-    Query q = new TermQuery(new Term("body", "text"));
-    IndexSearcher s = newSearcher(r);
-    float maxScore = s.search(q , 10).getMaxScore();
-    assertEquals(maxScore, s.search(q, null, 3, Sort.INDEXORDER, random().nextBoolean(), true).getMaxScore(), 0.0);
-    assertEquals(maxScore, s.search(q, null, 3, Sort.RELEVANCE, random().nextBoolean(), true).getMaxScore(), 0.0);
-    assertEquals(maxScore, s.search(q, null, 3, new Sort(new SortField[] {new SortField("id", SortField.Type.INT, false)}), random().nextBoolean(), true).getMaxScore(), 0.0);
-    assertEquals(maxScore, s.search(q, null, 3, new Sort(new SortField[] {new SortField("id", SortField.Type.INT, true)}), random().nextBoolean(), true).getMaxScore(), 0.0);
-    r.close();
-    d.close();
-  }
-  
-  /** test sorts when there's nothing in the index */
-  public void testEmptyIndex() throws Exception {
-    IndexSearcher empty = newSearcher(new MultiReader());
-    Query query = new TermQuery(new Term("contents", "foo"));
-  
-    Sort sort = new Sort();
-    TopDocs td = empty.search(query, null, 10, sort, true, true);
-    assertEquals(0, td.totalHits);
-
-    sort.setSort(SortField.FIELD_DOC);
-    td = empty.search(query, null, 10, sort, true, true);
-    assertEquals(0, td.totalHits);
-
-    sort.setSort(new SortField("int", SortField.Type.INT), SortField.FIELD_DOC);
-    td = empty.search(query, null, 10, sort, true, true);
-    assertEquals(0, td.totalHits);
-    
-    sort.setSort(new SortField("string", SortField.Type.STRING, true), SortField.FIELD_DOC);
-    td = empty.search(query, null, 10, sort, true, true);
-    assertEquals(0, td.totalHits);
-    
-    sort.setSort(new SortField("string_val", SortField.Type.STRING_VAL, true), SortField.FIELD_DOC);
-    td = empty.search(query, null, 10, sort, true, true);
-    assertEquals(0, td.totalHits);
-
-    sort.setSort(new SortField("float", SortField.Type.FLOAT), new SortField("string", SortField.Type.STRING));
-    td = empty.search(query, null, 10, sort, true, true);
-    assertEquals(0, td.totalHits);
-  }
-  
-  /** 
-   * test sorts for a custom int parser that uses a simple char encoding 
-   */
-  public void testCustomIntParser() throws Exception {
-    List<String> letters = Arrays.asList(new String[] { "A", "B", "C", "D", "E", "F", "G", "H", "I", "J" });
-    Collections.shuffle(letters, random());
-
-    Directory dir = newDirectory();
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);
-    for (String letter : letters) {
-      Document doc = new Document();
-      doc.add(newStringField("parser", letter, Field.Store.YES));
-      iw.addDocument(doc);
-    }
-    
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("parser", new FieldCache.IntParser() {
-      @Override
-      public int parseInt(BytesRef term) {
-        return (term.bytes[term.offset]-'A') * 123456;
-      }
-      
-      @Override
-      public TermsEnum termsEnum(Terms terms) throws IOException {
-        return terms.iterator(null);
-      }
-    }), SortField.FIELD_DOC );
-    
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-
-    // results should be in alphabetical order
-    assertEquals(10, td.totalHits);
-    Collections.sort(letters);
-    for (int i = 0; i < letters.size(); i++) {
-      assertEquals(letters.get(i), searcher.doc(td.scoreDocs[i].doc).get("parser"));
-    }
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** 
-   * test sorts for a custom long parser that uses a simple char encoding 
-   */
-  public void testCustomLongParser() throws Exception {
-    List<String> letters = Arrays.asList(new String[] { "A", "B", "C", "D", "E", "F", "G", "H", "I", "J" });
-    Collections.shuffle(letters, random());
-
-    Directory dir = newDirectory();
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);
-    for (String letter : letters) {
-      Document doc = new Document();
-      doc.add(newStringField("parser", letter, Field.Store.YES));
-      iw.addDocument(doc);
-    }
-    
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("parser", new FieldCache.LongParser() {
-      @Override
-      public long parseLong(BytesRef term) {
-        return (term.bytes[term.offset]-'A') * 1234567890L;
-      }
-      
-      @Override
-      public TermsEnum termsEnum(Terms terms) throws IOException {
-        return terms.iterator(null);
-      }
-    }), SortField.FIELD_DOC );
-    
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-
-    // results should be in alphabetical order
-    assertEquals(10, td.totalHits);
-    Collections.sort(letters);
-    for (int i = 0; i < letters.size(); i++) {
-      assertEquals(letters.get(i), searcher.doc(td.scoreDocs[i].doc).get("parser"));
-    }
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** 
-   * test sorts for a custom float parser that uses a simple char encoding 
-   */
-  public void testCustomFloatParser() throws Exception {
-    List<String> letters = Arrays.asList(new String[] { "A", "B", "C", "D", "E", "F", "G", "H", "I", "J" });
-    Collections.shuffle(letters, random());
-
-    Directory dir = newDirectory();
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);
-    for (String letter : letters) {
-      Document doc = new Document();
-      doc.add(newStringField("parser", letter, Field.Store.YES));
-      iw.addDocument(doc);
-    }
-    
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("parser", new FieldCache.FloatParser() {
-      @Override
-      public float parseFloat(BytesRef term) {
-        return (float) Math.sqrt(term.bytes[term.offset]);
-      }
-      
-      @Override
-      public TermsEnum termsEnum(Terms terms) throws IOException {
-        return terms.iterator(null);
-      }
-    }), SortField.FIELD_DOC );
-    
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-
-    // results should be in alphabetical order
-    assertEquals(10, td.totalHits);
-    Collections.sort(letters);
-    for (int i = 0; i < letters.size(); i++) {
-      assertEquals(letters.get(i), searcher.doc(td.scoreDocs[i].doc).get("parser"));
-    }
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** 
-   * test sorts for a custom double parser that uses a simple char encoding 
-   */
-  public void testCustomDoubleParser() throws Exception {
-    List<String> letters = Arrays.asList(new String[] { "A", "B", "C", "D", "E", "F", "G", "H", "I", "J" });
-    Collections.shuffle(letters, random());
-
-    Directory dir = newDirectory();
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);
-    for (String letter : letters) {
-      Document doc = new Document();
-      doc.add(newStringField("parser", letter, Field.Store.YES));
-      iw.addDocument(doc);
-    }
-    
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("parser", new FieldCache.DoubleParser() {
-      @Override
-      public double parseDouble(BytesRef term) {
-        return Math.pow(term.bytes[term.offset], (term.bytes[term.offset]-'A'));
-      }
-      
-      @Override
-      public TermsEnum termsEnum(Terms terms) throws IOException {
-        return terms.iterator(null);
-      }
-    }), SortField.FIELD_DOC );
-    
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-
-    // results should be in alphabetical order
-    assertEquals(10, td.totalHits);
-    Collections.sort(letters);
-    for (int i = 0; i < letters.size(); i++) {
-      assertEquals(letters.get(i), searcher.doc(td.scoreDocs[i].doc).get("parser"));
-    }
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting a single document */
-  public void testSortOneDocument() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(newStringField("value", "foo", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.STRING));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(1, td.totalHits);
-    assertEquals("foo", searcher.doc(td.scoreDocs[0].doc).get("value"));
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting a single document with scores */
-  public void testSortOneDocumentWithScores() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(newStringField("value", "foo", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortField("value", SortField.Type.STRING));
-
-    TopDocs expected = searcher.search(new TermQuery(new Term("value", "foo")), 10);
-    assertEquals(1, expected.totalHits);
-    TopDocs actual = searcher.search(new TermQuery(new Term("value", "foo")), null, 10, sort, true, true);
-    
-    assertEquals(expected.totalHits, actual.totalHits);
-    assertEquals(expected.scoreDocs[0].score, actual.scoreDocs[0].score, 0F);
-
-    ir.close();
-    dir.close();
-  }
-  
-  /** Tests sorting with two fields */
-  public void testSortTwoFields() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(newStringField("tievalue", "tied", Field.Store.NO));
-    doc.add(newStringField("value", "foo", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("tievalue", "tied", Field.Store.NO));
-    doc.add(newStringField("value", "bar", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    // tievalue, then value
-    Sort sort = new Sort(new SortField("tievalue", SortField.Type.STRING),
-                         new SortField("value", SortField.Type.STRING));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'bar' comes before 'foo'
-    assertEquals("bar", searcher.doc(td.scoreDocs[0].doc).get("value"));
-    assertEquals("foo", searcher.doc(td.scoreDocs[1].doc).get("value"));
-
-    ir.close();
-    dir.close();
-  }
-
-  public void testScore() throws IOException {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(newStringField("value", "bar", Field.Store.NO));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("value", "foo", Field.Store.NO));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(SortField.FIELD_SCORE);
-
-    final BooleanQuery bq = new BooleanQuery();
-    bq.add(new TermQuery(new Term("value", "foo")), Occur.SHOULD);
-    bq.add(new MatchAllDocsQuery(), Occur.SHOULD);
-    TopDocs td = searcher.search(bq, 10, sort);
-    assertEquals(2, td.totalHits);
-    assertEquals(1, td.scoreDocs[0].doc);
-    assertEquals(0, td.scoreDocs[1].doc);
+    // null treated as Double.MAX_VALUE
+    assertEquals("-1.3", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("4.2333333333332", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("4.2333333333333", searcher.doc(td.scoreDocs[2].doc).get("value"));
+    assertNull(searcher.doc(td.scoreDocs[3].doc).get("value"));
 
     ir.close();
     dir.close();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestSortRandom.java lucene5666/lucene/core/src/test/org/apache/lucene/search/TestSortRandom.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestSortRandom.java	2014-05-14 03:47:28.694646622 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/search/TestSortRandom.java	2014-05-12 13:28:34.320244571 -0400
@@ -32,7 +32,9 @@
 import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.StoredField;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.store.Directory;
@@ -87,7 +89,6 @@
 
         br = new BytesRef(s);
         doc.add(new SortedDocValuesField("stringdv", br));
-        doc.add(newStringField("string", s, Field.Store.NO));
         docValues.add(br);
 
       } else {
@@ -124,17 +125,12 @@
       final SortField sf;
       final boolean sortMissingLast;
       final boolean missingIsNull;
-      if (random.nextBoolean()) {
-        sf = new SortField("stringdv", SortField.Type.STRING, reverse);
-        // Can only use sort missing if the DVFormat
-        // supports docsWithField:
-        sortMissingLast = defaultCodecSupportsDocsWithField() && random().nextBoolean();
-        missingIsNull = defaultCodecSupportsDocsWithField();
-      } else {
-        sf = new SortField("string", SortField.Type.STRING, reverse);
-        sortMissingLast = random().nextBoolean();
-        missingIsNull = true;
-      }
+      sf = new SortField("stringdv", SortField.Type.STRING, reverse);
+      // Can only use sort missing if the DVFormat
+      // supports docsWithField:
+      sortMissingLast = defaultCodecSupportsDocsWithField() && random().nextBoolean();
+      missingIsNull = defaultCodecSupportsDocsWithField();
+
       if (sortMissingLast) {
         sf.setMissingValue(SortField.STRING_LAST);
       }
@@ -264,14 +260,14 @@
     @Override
     public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
       final int maxDoc = context.reader().maxDoc();
-      final FieldCache.Ints idSource = FieldCache.DEFAULT.getInts(context.reader(), "id", false);
+      final NumericDocValues idSource = DocValues.getNumeric(context.reader(), "id");
       assertNotNull(idSource);
       final FixedBitSet bits = new FixedBitSet(maxDoc);
       for(int docID=0;docID<maxDoc;docID++) {
         if (random.nextFloat() <= density && (acceptDocs == null || acceptDocs.get(docID))) {
           bits.set(docID);
           //System.out.println("  acc id=" + idSource.getInt(docID) + " docID=" + docID);
-          matchValues.add(docValues.get(idSource.get(docID)));
+          matchValues.add(docValues.get((int) idSource.get(docID)));
         }
       }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/util/junitcompat/TestFailOnFieldCacheInsanity.java lucene5666/lucene/core/src/test/org/apache/lucene/util/junitcompat/TestFailOnFieldCacheInsanity.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/util/junitcompat/TestFailOnFieldCacheInsanity.java	2014-05-14 03:47:28.710646623 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/util/junitcompat/TestFailOnFieldCacheInsanity.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,80 +0,0 @@
-package org.apache.lucene.util.junitcompat;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.RAMDirectory;
-import org.junit.Assert;
-import org.junit.Test;
-import org.junit.runner.JUnitCore;
-import org.junit.runner.Result;
-import org.junit.runner.notification.Failure;
-
-public class TestFailOnFieldCacheInsanity extends WithNestedTests {
-  public TestFailOnFieldCacheInsanity() {
-    super(true);
-  }
-  
-  public static class Nested1 extends WithNestedTests.AbstractNestedTest {
-    private Directory d;
-    private IndexReader r;
-    private AtomicReader subR;
-
-    private void makeIndex() throws Exception {
-      // we use RAMDirectory here, because we dont want to stay on open files on Windows:
-      d = new RAMDirectory();
-      @SuppressWarnings("resource") RandomIndexWriter w =
-          new RandomIndexWriter(random(), d);
-      Document doc = new Document();
-      doc.add(newField("ints", "1", StringField.TYPE_NOT_STORED));
-      w.addDocument(doc);
-      w.forceMerge(1);
-      r = w.getReader();
-      w.shutdown();
-
-      subR = r.leaves().get(0).reader();
-    }
-
-    public void testDummy() throws Exception {
-      makeIndex();
-      assertNotNull(FieldCache.DEFAULT.getTermsIndex(subR, "ints"));
-      assertNotNull(FieldCache.DEFAULT.getTerms(subR, "ints", false));
-      // NOTE: do not close reader/directory, else it
-      // purges FC entries
-    }
-  }
-
-  @Test
-  public void testFailOnFieldCacheInsanity() {
-    Result r = JUnitCore.runClasses(Nested1.class);
-    boolean insane = false;
-    for(Failure f : r.getFailures()) {
-      if (f.getMessage().indexOf("Insane") != -1) {
-        insane = true;
-        break;
-      }
-    }
-    Assert.assertTrue(insane);
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/util/TestFieldCacheSanityChecker.java lucene5666/lucene/core/src/test/org/apache/lucene/util/TestFieldCacheSanityChecker.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/util/TestFieldCacheSanityChecker.java	2014-05-14 03:47:28.710646623 -0400
+++ lucene5666/lucene/core/src/test/org/apache/lucene/util/TestFieldCacheSanityChecker.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,166 +0,0 @@
-package org.apache.lucene.util;
-
-/**
- * Copyright 2009 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.DoubleField;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FloatField;
-import org.apache.lucene.document.IntField;
-import org.apache.lucene.document.LongField;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.MultiReader;
-import org.apache.lucene.index.SlowCompositeReaderWrapper;
-import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.FieldCacheSanityChecker.Insanity;
-import org.apache.lucene.util.FieldCacheSanityChecker.InsanityType;
-
-public class TestFieldCacheSanityChecker extends LuceneTestCase {
-
-  protected AtomicReader readerA;
-  protected AtomicReader readerB;
-  protected AtomicReader readerX;
-  protected AtomicReader readerAclone;
-  protected Directory dirA, dirB;
-  private static final int NUM_DOCS = 1000;
-
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    dirA = newDirectory();
-    dirB = newDirectory();
-
-    IndexWriter wA = new IndexWriter(dirA, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    IndexWriter wB = new IndexWriter(dirB, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-
-    long theLong = Long.MAX_VALUE;
-    double theDouble = Double.MAX_VALUE;
-    int theInt = Integer.MAX_VALUE;
-    float theFloat = Float.MAX_VALUE;
-    for (int i = 0; i < NUM_DOCS; i++){
-      Document doc = new Document();
-      doc.add(new LongField("theLong", theLong--, Field.Store.NO));
-      doc.add(new DoubleField("theDouble", theDouble--, Field.Store.NO));
-      doc.add(new IntField("theInt", theInt--, Field.Store.NO));
-      doc.add(new FloatField("theFloat", theFloat--, Field.Store.NO));
-      if (0 == i % 3) {
-        wA.addDocument(doc);
-      } else {
-        wB.addDocument(doc);
-      }
-    }
-    wA.shutdown();
-    wB.shutdown();
-    DirectoryReader rA = DirectoryReader.open(dirA);
-    readerA = SlowCompositeReaderWrapper.wrap(rA);
-    readerAclone = SlowCompositeReaderWrapper.wrap(rA);
-    readerA = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dirA));
-    readerB = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dirB));
-    readerX = SlowCompositeReaderWrapper.wrap(new MultiReader(readerA, readerB));
-  }
-
-  @Override
-  public void tearDown() throws Exception {
-    readerA.close();
-    readerAclone.close();
-    readerB.close();
-    readerX.close();
-    dirA.close();
-    dirB.close();
-    super.tearDown();
-  }
-
-  public void testSanity() throws IOException {
-    FieldCache cache = FieldCache.DEFAULT;
-    cache.purgeAllCaches();
-
-    cache.getDoubles(readerA, "theDouble", false);
-    cache.getDoubles(readerA, "theDouble", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, false);
-    cache.getDoubles(readerAclone, "theDouble", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, false);
-    cache.getDoubles(readerB, "theDouble", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, false);
-
-    cache.getInts(readerX, "theInt", false);
-    cache.getInts(readerX, "theInt", FieldCache.NUMERIC_UTILS_INT_PARSER, false);
-
-    // // // 
-
-    Insanity[] insanity = 
-      FieldCacheSanityChecker.checkSanity(cache.getCacheEntries());
-    
-    if (0 < insanity.length)
-      dumpArray(getTestClass().getName() + "#" + getTestName() 
-          + " INSANITY", insanity, System.err);
-
-    assertEquals("shouldn't be any cache insanity", 0, insanity.length);
-    cache.purgeAllCaches();
-  }
-
-  public void testInsanity1() throws IOException {
-    FieldCache cache = FieldCache.DEFAULT;
-    cache.purgeAllCaches();
-
-    cache.getInts(readerX, "theInt", FieldCache.NUMERIC_UTILS_INT_PARSER, false);
-    cache.getTerms(readerX, "theInt", false);
-
-    // // // 
-
-    Insanity[] insanity = 
-      FieldCacheSanityChecker.checkSanity(cache.getCacheEntries());
-
-    assertEquals("wrong number of cache errors", 1, insanity.length);
-    assertEquals("wrong type of cache error", 
-                 InsanityType.VALUEMISMATCH,
-                 insanity[0].getType());
-    assertEquals("wrong number of entries in cache error", 2,
-                 insanity[0].getCacheEntries().length);
-
-    // we expect bad things, don't let tearDown complain about them
-    cache.purgeAllCaches();
-  }
-
-  public void testInsanity2() throws IOException {
-    FieldCache cache = FieldCache.DEFAULT;
-    cache.purgeAllCaches();
-
-    cache.getTerms(readerA, "theInt", false);
-    cache.getTerms(readerB, "theInt", false);
-    cache.getTerms(readerX, "theInt", false);
-
-
-    // // // 
-
-    Insanity[] insanity = 
-      FieldCacheSanityChecker.checkSanity(cache.getCacheEntries());
-    
-    assertEquals("wrong number of cache errors", 1, insanity.length);
-    assertEquals("wrong type of cache error", 
-                 InsanityType.SUBREADER,
-                 insanity[0].getType());
-    assertEquals("wrong number of entries in cache error", 3,
-                 insanity[0].getCacheEntries().length);
-
-    // we expect bad things, don't let tearDown complain about them
-    cache.purgeAllCaches();
-  }
-
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/demo/src/java/org/apache/lucene/demo/facet/DistanceFacetsExample.java lucene5666/lucene/demo/src/java/org/apache/lucene/demo/facet/DistanceFacetsExample.java
--- lucene-trunk/lucene/demo/src/java/org/apache/lucene/demo/facet/DistanceFacetsExample.java	2014-05-14 03:47:28.906646626 -0400
+++ lucene5666/lucene/demo/src/java/org/apache/lucene/demo/facet/DistanceFacetsExample.java	2014-05-12 13:28:34.092244567 -0400
@@ -25,6 +25,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.DoubleField;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.expressions.Expression;
 import org.apache.lucene.expressions.SimpleBindings;
 import org.apache.lucene.expressions.js.JavascriptCompiler;
@@ -92,19 +93,26 @@
     // TODO: we could index in radians instead ... saves all the conversions in getBoundingBoxFilter
 
     // Add documents with latitude/longitude location:
+    // we index these both as DoubleFields (for bounding box/ranges) and as NumericDocValuesFields (for scoring)
     Document doc = new Document();
     doc.add(new DoubleField("latitude", 40.759011, Field.Store.NO));
+    doc.add(new NumericDocValuesField("latitude", Double.doubleToRawLongBits(40.759011)));
     doc.add(new DoubleField("longitude", -73.9844722, Field.Store.NO));
+    doc.add(new NumericDocValuesField("longitude", Double.doubleToRawLongBits(-73.9844722)));
     writer.addDocument(doc);
     
     doc = new Document();
     doc.add(new DoubleField("latitude", 40.718266, Field.Store.NO));
+    doc.add(new NumericDocValuesField("latitude", Double.doubleToRawLongBits(40.718266)));
     doc.add(new DoubleField("longitude", -74.007819, Field.Store.NO));
+    doc.add(new NumericDocValuesField("longitude", Double.doubleToRawLongBits(-74.007819)));
     writer.addDocument(doc);
     
     doc = new Document();
     doc.add(new DoubleField("latitude", 40.7051157, Field.Store.NO));
+    doc.add(new NumericDocValuesField("latitude", Double.doubleToRawLongBits(40.7051157)));
     doc.add(new DoubleField("longitude", -74.0088305, Field.Store.NO));
+    doc.add(new NumericDocValuesField("longitude", Double.doubleToRawLongBits(-74.0088305)));
     writer.addDocument(doc);
 
     // Open near-real-time searcher


diff -ruN -x .svn -x build lucene-trunk/lucene/expressions/src/java/org/apache/lucene/expressions/SimpleBindings.java lucene5666/lucene/expressions/src/java/org/apache/lucene/expressions/SimpleBindings.java
--- lucene-trunk/lucene/expressions/src/java/org/apache/lucene/expressions/SimpleBindings.java	2014-05-14 03:47:28.902646626 -0400
+++ lucene5666/lucene/expressions/src/java/org/apache/lucene/expressions/SimpleBindings.java	2014-05-12 13:28:33.840244563 -0400
@@ -25,10 +25,6 @@
 import org.apache.lucene.queries.function.valuesource.FloatFieldSource;
 import org.apache.lucene.queries.function.valuesource.IntFieldSource;
 import org.apache.lucene.queries.function.valuesource.LongFieldSource;
-import org.apache.lucene.search.FieldCache.DoubleParser;
-import org.apache.lucene.search.FieldCache.FloatParser;
-import org.apache.lucene.search.FieldCache.IntParser;
-import org.apache.lucene.search.FieldCache.LongParser;
 import org.apache.lucene.search.SortField;
 
 /**
@@ -87,13 +83,13 @@
     SortField field = (SortField) o;
     switch(field.getType()) {
       case INT:
-        return new IntFieldSource(field.getField(), (IntParser) field.getParser());
+        return new IntFieldSource(field.getField());
       case LONG:
-        return new LongFieldSource(field.getField(), (LongParser) field.getParser());
+        return new LongFieldSource(field.getField());
       case FLOAT:
-        return new FloatFieldSource(field.getField(), (FloatParser) field.getParser());
+        return new FloatFieldSource(field.getField());
       case DOUBLE:
-        return new DoubleFieldSource(field.getField(), (DoubleParser) field.getParser());
+        return new DoubleFieldSource(field.getField());
       case SCORE:
         return getScoreValueSource();
       default:


diff -ruN -x .svn -x build lucene-trunk/lucene/expressions/src/test/org/apache/lucene/expressions/TestDemoExpressions.java lucene5666/lucene/expressions/src/test/org/apache/lucene/expressions/TestDemoExpressions.java
--- lucene-trunk/lucene/expressions/src/test/org/apache/lucene/expressions/TestDemoExpressions.java	2014-05-14 03:47:28.898646626 -0400
+++ lucene5666/lucene/expressions/src/test/org/apache/lucene/expressions/TestDemoExpressions.java	2014-05-12 13:28:33.724244561 -0400
@@ -1,7 +1,6 @@
 package org.apache.lucene.expressions;
 
 import org.apache.lucene.document.Document;
-import org.apache.lucene.document.DoubleField;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.expressions.js.JavascriptCompiler;
@@ -53,24 +52,24 @@
     doc.add(newStringField("id", "1", Field.Store.YES));
     doc.add(newTextField("body", "some contents and more contents", Field.Store.NO));
     doc.add(new NumericDocValuesField("popularity", 5));
-    doc.add(new DoubleField("latitude", 40.759011, Field.Store.NO));
-    doc.add(new DoubleField("longitude", -73.9844722, Field.Store.NO));
+    doc.add(new NumericDocValuesField("latitude", Double.doubleToRawLongBits(40.759011)));
+    doc.add(new NumericDocValuesField("longitude", Double.doubleToRawLongBits(-73.9844722)));
     iw.addDocument(doc);
     
     doc = new Document();
     doc.add(newStringField("id", "2", Field.Store.YES));
     doc.add(newTextField("body", "another document with different contents", Field.Store.NO));
     doc.add(new NumericDocValuesField("popularity", 20));
-    doc.add(new DoubleField("latitude", 40.718266, Field.Store.NO));
-    doc.add(new DoubleField("longitude", -74.007819, Field.Store.NO));
+    doc.add(new NumericDocValuesField("latitude", Double.doubleToRawLongBits(40.718266)));
+    doc.add(new NumericDocValuesField("longitude", Double.doubleToRawLongBits(-74.007819)));
     iw.addDocument(doc);
     
     doc = new Document();
     doc.add(newStringField("id", "3", Field.Store.YES));
     doc.add(newTextField("body", "crappy contents", Field.Store.NO));
     doc.add(new NumericDocValuesField("popularity", 2));
-    doc.add(new DoubleField("latitude", 40.7051157, Field.Store.NO));
-    doc.add(new DoubleField("longitude", -74.0088305, Field.Store.NO));
+    doc.add(new NumericDocValuesField("latitude", Double.doubleToRawLongBits(40.7051157)));
+    doc.add(new NumericDocValuesField("longitude", Double.doubleToRawLongBits(-74.0088305)));
     iw.addDocument(doc);
     
     reader = iw.getReader();


diff -ruN -x .svn -x build lucene-trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetSumValueSource.java lucene5666/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetSumValueSource.java
--- lucene-trunk/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetSumValueSource.java	2014-05-14 03:47:28.894646626 -0400
+++ lucene5666/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetSumValueSource.java	2014-05-12 13:28:32.020244531 -0400
@@ -80,27 +80,27 @@
     // Reused across documents, to add the necessary facet
     // fields:
     Document doc = new Document();
-    doc.add(new IntField("num", 10, Field.Store.NO));
+    doc.add(new NumericDocValuesField("num", 10));
     doc.add(new FacetField("Author", "Bob"));
     writer.addDocument(config.build(taxoWriter, doc));
 
     doc = new Document();
-    doc.add(new IntField("num", 20, Field.Store.NO));
+    doc.add(new NumericDocValuesField("num", 20));
     doc.add(new FacetField("Author", "Lisa"));
     writer.addDocument(config.build(taxoWriter, doc));
 
     doc = new Document();
-    doc.add(new IntField("num", 30, Field.Store.NO));
+    doc.add(new NumericDocValuesField("num", 30));
     doc.add(new FacetField("Author", "Lisa"));
     writer.addDocument(config.build(taxoWriter, doc));
 
     doc = new Document();
-    doc.add(new IntField("num", 40, Field.Store.NO));
+    doc.add(new NumericDocValuesField("num", 40));
     doc.add(new FacetField("Author", "Susan"));
     writer.addDocument(config.build(taxoWriter, doc));
 
     doc = new Document();
-    doc.add(new IntField("num", 45, Field.Store.NO));
+    doc.add(new NumericDocValuesField("num", 45));
     doc.add(new FacetField("Author", "Frank"));
     writer.addDocument(config.build(taxoWriter, doc));
 
@@ -145,7 +145,7 @@
     FacetsConfig config = new FacetsConfig();
 
     Document doc = new Document();
-    doc.add(new IntField("num", 10, Field.Store.NO));
+    doc.add(new NumericDocValuesField("num", 10));
     doc.add(new FacetField("a", "foo1"));
     writer.addDocument(config.build(taxoWriter, doc));
 
@@ -154,7 +154,7 @@
     }
 
     doc = new Document();
-    doc.add(new IntField("num", 20, Field.Store.NO));
+    doc.add(new NumericDocValuesField("num", 20));
     doc.add(new FacetField("a", "foo2"));
     doc.add(new FacetField("b", "bar1"));
     writer.addDocument(config.build(taxoWriter, doc));
@@ -164,7 +164,7 @@
     }
 
     doc = new Document();
-    doc.add(new IntField("num", 30, Field.Store.NO));
+    doc.add(new NumericDocValuesField("num", 30));
     doc.add(new FacetField("a", "foo3"));
     doc.add(new FacetField("b", "bar2"));
     doc.add(new FacetField("c", "baz1"));


diff -ruN -x .svn -x build lucene-trunk/lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java lucene5666/lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java
--- lucene-trunk/lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java	2014-05-14 03:47:28.898646626 -0400
+++ lucene5666/lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java	2014-05-12 13:28:31.972244530 -0400
@@ -30,6 +30,7 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.facet.DrillSideways.DrillSidewaysResult;
 import org.apache.lucene.facet.sortedset.DefaultSortedSetDocValuesReaderState;
@@ -497,6 +498,7 @@
     for(Doc rawDoc : docs) {
       Document doc = new Document();
       doc.add(newStringField("id", rawDoc.id, Field.Store.YES));
+      doc.add(new SortedDocValuesField("id", new BytesRef(rawDoc.id)));
       doc.add(newStringField("content", rawDoc.contentToken, Field.Store.NO));
 
       if (VERBOSE) {


diff -ruN -x .svn -x build lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/BlockGroupingCollector.java lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/BlockGroupingCollector.java
--- lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/BlockGroupingCollector.java	2014-05-14 03:47:28.842646625 -0400
+++ lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/BlockGroupingCollector.java	2014-05-14 03:45:16.694644324 -0400
@@ -300,7 +300,7 @@
    *  This is normally not a problem, as you can obtain the
    *  value just like you obtain other values for each
    *  matching document (eg, via stored fields, via
-   *  FieldCache, etc.)
+   *  DocValues, etc.)
    *
    *  @param withinGroupSort The {@link Sort} used to sort
    *    documents within each group.  Passing null is


diff -ruN -x .svn -x build lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/GroupingSearch.java lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/GroupingSearch.java
--- lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/GroupingSearch.java	2014-05-14 03:47:28.842646625 -0400
+++ lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/GroupingSearch.java	2014-05-12 13:28:55.604244942 -0400
@@ -20,7 +20,6 @@
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.CachingCollector;
 import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MultiCollector;
@@ -78,7 +77,7 @@
   private Bits matchingGroupHeads;
 
   /**
-   * Constructs a <code>GroupingSearch</code> instance that groups documents by index terms using the {@link FieldCache}.
+   * Constructs a <code>GroupingSearch</code> instance that groups documents by index terms using DocValues.
    * The group field can only have one token per document. This means that the field must not be analysed.
    *
    * @param groupField The name of the field to group by.


diff -ruN -x .svn -x build lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/package.html lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/package.html
--- lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/package.html	2014-05-14 03:47:28.842646625 -0400
+++ lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/package.html	2014-05-14 03:45:16.694644324 -0400
@@ -80,8 +80,7 @@
 <p>Known limitations:</p>
 <ul>
   <li> For the two-pass grouping search, the group field must be a
-    single-valued indexed field (or indexed as a {@link org.apache.lucene.document.SortedDocValuesField}).
-    {@link org.apache.lucene.search.FieldCache} is used to load the {@link org.apache.lucene.index.SortedDocValues} for this field.
+    indexed as a {@link org.apache.lucene.document.SortedDocValuesField}).
   <li> Although Solr support grouping by function and this module has abstraction of what a group is, there are currently only
     implementations for grouping based on terms.
   <li> Sharding is not directly supported, though is not too


diff -ruN -x .svn -x build lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/package.html lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/package.html
--- lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/package.html	2014-05-14 03:47:28.842646625 -0400
+++ lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/package.html	2014-05-14 03:45:16.694644324 -0400
@@ -16,6 +16,6 @@
 -->
 <html>
 <body>
-Support for grouping by indexed terms via {@link org.apache.lucene.search.FieldCache}.
+Support for grouping by indexed terms via {@link org.apache.lucene.index.DocValues}.
 </body>
 </html>


diff -ruN -x .svn -x build lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupHeadsCollector.java lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupHeadsCollector.java
--- lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupHeadsCollector.java	2014-05-14 03:47:28.842646625 -0400
+++ lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupHeadsCollector.java	2014-05-12 13:28:55.600244942 -0400
@@ -18,9 +18,8 @@
  */
 
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.search.LeafCollector;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.FieldComparator;
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.Sort;
@@ -161,7 +160,7 @@
     @Override
     protected void doSetNextReader(AtomicReaderContext context) throws IOException {
       this.readerContext = context;
-      groupIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), groupField);
+      groupIndex = DocValues.getSorted(context.reader(), groupField);
 
       for (GroupHead groupHead : groups.values()) {
         for (int i = 0; i < groupHead.comparators.length; i++) {
@@ -276,13 +275,13 @@
     @Override
     protected void doSetNextReader(AtomicReaderContext context) throws IOException {
       this.readerContext = context;
-      groupIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), groupField);
+      groupIndex = DocValues.getSorted(context.reader(), groupField);
       for (int i = 0; i < fields.length; i++) {
         if (fields[i].getType() == SortField.Type.SCORE) {
           continue;
         }
 
-        sortsIndex[i] = FieldCache.DEFAULT.getTermsIndex(context.reader(), fields[i].getField());
+        sortsIndex[i] = DocValues.getSorted(context.reader(), fields[i].getField());
       }
 
       // Clear ordSet and fill it with previous encountered groups that can occur in the current segment.
@@ -444,9 +443,9 @@
     @Override
     protected void doSetNextReader(AtomicReaderContext context) throws IOException {
       this.readerContext = context;
-      groupIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), groupField);
+      groupIndex = DocValues.getSorted(context.reader(), groupField);
       for (int i = 0; i < fields.length; i++) {
-        sortsIndex[i] = FieldCache.DEFAULT.getTermsIndex(context.reader(), fields[i].getField());
+        sortsIndex[i] = DocValues.getSorted(context.reader(), fields[i].getField());
       }
 
       // Clear ordSet and fill it with previous encountered groups that can occur in the current segment.
@@ -587,7 +586,7 @@
     @Override
     protected void doSetNextReader(AtomicReaderContext context) throws IOException {
       this.readerContext = context;
-      groupIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), groupField);
+      groupIndex = DocValues.getSorted(context.reader(), groupField);
 
       // Clear ordSet and fill it with previous encountered groups that can occur in the current segment.
       ordSet.clear();


diff -ruN -x .svn -x build lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupsCollector.java lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupsCollector.java
--- lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupsCollector.java	2014-05-14 03:47:28.842646625 -0400
+++ lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupsCollector.java	2014-05-12 13:28:56.932244965 -0400
@@ -18,9 +18,9 @@
  */
 
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.LeafCollector;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.grouping.AbstractAllGroupsCollector;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.SentinelIntSet;
@@ -105,7 +105,7 @@
 
   @Override
   protected void doSetNextReader(AtomicReaderContext context) throws IOException {
-    index = FieldCache.DEFAULT.getTermsIndex(context.reader(), groupField);
+    index = DocValues.getSorted(context.reader(), groupField);
 
     // Clear ordSet and fill it with previous encountered groups that can occur in the current segment.
     ordSet.clear();


diff -ruN -x .svn -x build lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermDistinctValuesCollector.java lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermDistinctValuesCollector.java
--- lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermDistinctValuesCollector.java	2014-05-14 03:47:28.842646625 -0400
+++ lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermDistinctValuesCollector.java	2014-05-12 13:28:55.604244942 -0400
@@ -18,9 +18,9 @@
  */
 
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.LeafCollector;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.grouping.AbstractDistinctValuesCollector;
 import org.apache.lucene.search.grouping.SearchGroup;
 import org.apache.lucene.util.BytesRef;
@@ -109,8 +109,8 @@
 
   @Override
   protected void doSetNextReader(AtomicReaderContext context) throws IOException {
-    groupFieldTermIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), groupField);
-    countFieldTermIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), countField);
+    groupFieldTermIndex = DocValues.getSorted(context.reader(), groupField);
+    countFieldTermIndex = DocValues.getSorted(context.reader(), countField);
     ordSet.clear();
     for (GroupCount group : groups) {
       int groupOrd = group.groupValue == null ? -1 : groupFieldTermIndex.lookupTerm(group.groupValue);


diff -ruN -x .svn -x build lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermFirstPassGroupingCollector.java lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermFirstPassGroupingCollector.java
--- lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermFirstPassGroupingCollector.java	2014-05-14 03:47:28.842646625 -0400
+++ lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermFirstPassGroupingCollector.java	2014-05-14 03:45:16.694644324 -0400
@@ -20,9 +20,9 @@
 import java.io.IOException;
 
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.LeafCollector;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.grouping.AbstractFirstPassGroupingCollector;
 import org.apache.lucene.util.BytesRef;
@@ -46,7 +46,7 @@
    *
    *  @param groupField The field used to group
    *    documents. This field must be single-valued and
-   *    indexed (FieldCache is used to access its value
+   *    indexed (DocValues is used to access its value
    *    per-document).
    *  @param groupSort The {@link Sort} used to sort the
    *    groups.  The top sorted document within each group
@@ -88,6 +88,6 @@
   @Override
   protected void doSetNextReader(AtomicReaderContext readerContext) throws IOException {
     super.doSetNextReader(readerContext);
-    index = FieldCache.DEFAULT.getTermsIndex(readerContext.reader(), groupField);
+    index = DocValues.getSorted(readerContext.reader(), groupField);
   }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermGroupFacetCollector.java lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermGroupFacetCollector.java
--- lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermGroupFacetCollector.java	2014-05-14 03:47:28.842646625 -0400
+++ lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermGroupFacetCollector.java	2014-05-12 13:28:55.604244942 -0400
@@ -18,11 +18,11 @@
  */
 
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.SortedSetDocValues;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.LeafCollector;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.grouping.AbstractGroupFacetCollector;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.SentinelIntSet;
@@ -34,7 +34,7 @@
 
 /**
  * An implementation of {@link AbstractGroupFacetCollector} that computes grouped facets based on the indexed terms
- * from the {@link FieldCache}.
+ * from DocValues.
  *
  * @lucene.experimental
  */
@@ -128,8 +128,8 @@
         segmentResults.add(createSegmentResult());
       }
 
-      groupFieldTermsIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), groupField);
-      facetFieldTermsIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), facetField);
+      groupFieldTermsIndex = DocValues.getSorted(context.reader(), groupField);
+      facetFieldTermsIndex = DocValues.getSorted(context.reader(), facetField);
 
       // 1+ to allow for the -1 "not set":
       segmentFacetCounts = new int[facetFieldTermsIndex.getValueCount()+1];
@@ -283,8 +283,8 @@
         segmentResults.add(createSegmentResult());
       }
 
-      groupFieldTermsIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), groupField);
-      facetFieldDocTermOrds = FieldCache.DEFAULT.getDocTermOrds(context.reader(), facetField);
+      groupFieldTermsIndex = DocValues.getSorted(context.reader(), groupField);
+      facetFieldDocTermOrds = DocValues.getSortedSet(context.reader(), facetField);
       facetFieldNumTerms = (int) facetFieldDocTermOrds.getValueCount();
       if (facetFieldNumTerms == 0) {
         facetOrdTermsEnum = null;


diff -ruN -x .svn -x build lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermSecondPassGroupingCollector.java lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermSecondPassGroupingCollector.java
--- lucene-trunk/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermSecondPassGroupingCollector.java	2014-05-14 03:47:28.842646625 -0400
+++ lucene5666/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermSecondPassGroupingCollector.java	2014-05-12 13:28:55.604244942 -0400
@@ -21,9 +21,9 @@
 import java.util.Collection;
 
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.LeafCollector;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.grouping.AbstractSecondPassGroupingCollector;
 import org.apache.lucene.search.grouping.SearchGroup;
@@ -56,7 +56,7 @@
   @Override
   protected void doSetNextReader(AtomicReaderContext readerContext) throws IOException {
     super.doSetNextReader(readerContext);
-    index = FieldCache.DEFAULT.getTermsIndex(readerContext.reader(), groupField);
+    index = DocValues.getSorted(readerContext.reader(), groupField);
 
     // Rebuild ordSet
     ordSet.clear();


diff -ruN -x .svn -x build lucene-trunk/lucene/grouping/src/test/org/apache/lucene/search/grouping/AllGroupHeadsCollectorTest.java lucene5666/lucene/grouping/src/test/org/apache/lucene/search/grouping/AllGroupHeadsCollectorTest.java
--- lucene-trunk/lucene/grouping/src/test/org/apache/lucene/search/grouping/AllGroupHeadsCollectorTest.java	2014-05-14 03:47:28.830646625 -0400
+++ lucene5666/lucene/grouping/src/test/org/apache/lucene/search/grouping/AllGroupHeadsCollectorTest.java	2014-05-14 03:45:16.694644324 -0400
@@ -17,23 +17,35 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiDocValues;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.BytesRefFieldSource;
 import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.QueryUtils;
 import org.apache.lucene.search.ScoreDoc;
@@ -48,22 +60,8 @@
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
 
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-
 public class AllGroupHeadsCollectorTest extends LuceneTestCase {
 
-  private static final DocValuesType[] vts = new DocValuesType[]{
-      DocValuesType.BINARY, DocValuesType.SORTED
-  };
-
   public void testBasic() throws Exception {
     final String groupField = "author";
     Directory dir = newDirectory();
@@ -72,30 +70,30 @@
         dir,
         newIndexWriterConfig(TEST_VERSION_CURRENT,
             new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
-    DocValuesType valueType = vts[random().nextInt(vts.length)];
+    DocValuesType valueType = DocValuesType.SORTED;
 
     // 0
     Document doc = new Document();
     addGroupField(doc, groupField, "author1", valueType);
     doc.add(newTextField("content", "random text", Field.Store.NO));
-    doc.add(new IntField("id_1", 1, Field.Store.NO));
-    doc.add(newStringField("id_2", "1", Field.Store.NO));
+    doc.add(new NumericDocValuesField("id_1", 1));
+    doc.add(new SortedDocValuesField("id_2", new BytesRef("1")));
     w.addDocument(doc);
 
     // 1
     doc = new Document();
     addGroupField(doc, groupField, "author1", valueType);
     doc.add(newTextField("content", "some more random text blob", Field.Store.NO));
-    doc.add(new IntField("id_1", 2, Field.Store.NO));
-    doc.add(newStringField("id_2", "2", Field.Store.NO));
+    doc.add(new NumericDocValuesField("id_1", 2));
+    doc.add(new SortedDocValuesField("id_2", new BytesRef("2")));
     w.addDocument(doc);
 
     // 2
     doc = new Document();
     addGroupField(doc, groupField, "author1", valueType);
     doc.add(newTextField("content", "some more random textual data", Field.Store.NO));
-    doc.add(new IntField("id_1", 3, Field.Store.NO));
-    doc.add(newStringField("id_2", "3", Field.Store.NO));
+    doc.add(new NumericDocValuesField("id_1", 3));
+    doc.add(new SortedDocValuesField("id_2", new BytesRef("3")));
     w.addDocument(doc);
     w.commit(); // To ensure a second segment
 
@@ -103,38 +101,38 @@
     doc = new Document();
     addGroupField(doc, groupField, "author2", valueType);
     doc.add(newTextField("content", "some random text", Field.Store.NO));
-    doc.add(new IntField("id_1", 4, Field.Store.NO));
-    doc.add(newStringField("id_2", "4", Field.Store.NO));
+    doc.add(new NumericDocValuesField("id_1", 4));
+    doc.add(new SortedDocValuesField("id_2", new BytesRef("4")));
     w.addDocument(doc);
 
     // 4
     doc = new Document();
     addGroupField(doc, groupField, "author3", valueType);
     doc.add(newTextField("content", "some more random text", Field.Store.NO));
-    doc.add(new IntField("id_1", 5, Field.Store.NO));
-    doc.add(newStringField("id_2", "5", Field.Store.NO));
+    doc.add(new NumericDocValuesField("id_1", 5));
+    doc.add(new SortedDocValuesField("id_2", new BytesRef("5")));
     w.addDocument(doc);
 
     // 5
     doc = new Document();
     addGroupField(doc, groupField, "author3", valueType);
     doc.add(newTextField("content", "random blob", Field.Store.NO));
-    doc.add(new IntField("id_1", 6, Field.Store.NO));
-    doc.add(newStringField("id_2", "6", Field.Store.NO));
+    doc.add(new NumericDocValuesField("id_1", 6));
+    doc.add(new SortedDocValuesField("id_2", new BytesRef("6")));
     w.addDocument(doc);
 
     // 6 -- no author field
     doc = new Document();
     doc.add(newTextField("content", "random word stuck in alot of other text", Field.Store.NO));
-    doc.add(new IntField("id_1", 6, Field.Store.NO));
-    doc.add(newStringField("id_2", "6", Field.Store.NO));
+    doc.add(new NumericDocValuesField("id_1", 6));
+    doc.add(new SortedDocValuesField("id_2", new BytesRef("6")));
     w.addDocument(doc);
 
     // 7 -- no author field
     doc = new Document();
     doc.add(newTextField("content", "random word stuck in alot of other text", Field.Store.NO));
-    doc.add(new IntField("id_1", 7, Field.Store.NO));
-    doc.add(newStringField("id_2", "7", Field.Store.NO));
+    doc.add(new NumericDocValuesField("id_1", 7));
+    doc.add(new SortedDocValuesField("id_2", new BytesRef("7")));
     w.addDocument(doc);
 
     IndexReader reader = w.getReader();
@@ -198,6 +196,7 @@
           // B/c of DV based impl we can't see the difference between an empty string and a null value.
           // For that reason we don't generate empty string groups.
           randomValue = TestUtil.randomRealisticUnicodeString(random());
+          //randomValue = TestUtil.randomSimpleString(random());
         } while ("".equals(randomValue));
         groups.add(new BytesRef(randomValue));
       }
@@ -224,31 +223,20 @@
           dir,
           newIndexWriterConfig(TEST_VERSION_CURRENT,
               new MockAnalyzer(random())));
-      DocValuesType valueType = vts[random().nextInt(vts.length)];
+      DocValuesType valueType = DocValuesType.SORTED;
 
       Document doc = new Document();
       Document docNoGroup = new Document();
-      Field group = newStringField("group", "", Field.Store.NO);
-      doc.add(group);
       Field valuesField = null;
-      switch(valueType) {
-        case BINARY:
-          valuesField = new BinaryDocValuesField("group_dv", new BytesRef());
-          break;
-        case SORTED:
-          valuesField = new SortedDocValuesField("group_dv", new BytesRef());
-          break;
-        default:
-          fail("unhandled type");
-      }
+      valuesField = new SortedDocValuesField("group", new BytesRef());
       doc.add(valuesField);
-      Field sort1 = newStringField("sort1", "", Field.Store.NO);
+      Field sort1 = new SortedDocValuesField("sort1", new BytesRef());
       doc.add(sort1);
       docNoGroup.add(sort1);
-      Field sort2 = newStringField("sort2", "", Field.Store.NO);
+      Field sort2 = new SortedDocValuesField("sort2", new BytesRef());
       doc.add(sort2);
       docNoGroup.add(sort2);
-      Field sort3 = newStringField("sort3", "", Field.Store.NO);
+      Field sort3 = new SortedDocValuesField("sort3", new BytesRef());
       doc.add(sort3);
       docNoGroup.add(sort3);
       Field content = newTextField("content", "", Field.Store.NO);
@@ -257,6 +245,9 @@
       IntField id = new IntField("id", 0, Field.Store.NO);
       doc.add(id);
       docNoGroup.add(id);
+      NumericDocValuesField idDV = new NumericDocValuesField("id", 0);
+      doc.add(idDV);
+      docNoGroup.add(idDV);
       final GroupDoc[] groupDocs = new GroupDoc[numDocs];
       for (int i = 0; i < numDocs; i++) {
         final BytesRef groupValue;
@@ -283,14 +274,14 @@
 
         groupDocs[i] = groupDoc;
         if (groupDoc.group != null) {
-          group.setStringValue(groupDoc.group.utf8ToString());
           valuesField.setBytesValue(new BytesRef(groupDoc.group.utf8ToString()));
         }
-        sort1.setStringValue(groupDoc.sort1.utf8ToString());
-        sort2.setStringValue(groupDoc.sort2.utf8ToString());
-        sort3.setStringValue(groupDoc.sort3.utf8ToString());
+        sort1.setBytesValue(groupDoc.sort1);
+        sort2.setBytesValue(groupDoc.sort2);
+        sort3.setBytesValue(groupDoc.sort3);
         content.setStringValue(groupDoc.content);
         id.setIntValue(groupDoc.id);
+        idDV.setLongValue(groupDoc.id);
         if (groupDoc.group == null) {
           w.addDocument(docNoGroup);
         } else {
@@ -301,91 +292,86 @@
       final DirectoryReader r = w.getReader();
       w.shutdown();
 
-      // NOTE: intentional but temporary field cache insanity!
-      final FieldCache.Ints docIdToFieldId = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(r), "id", false);
+      final NumericDocValues docIdToFieldId = MultiDocValues.getNumericValues(r, "id");
       final int[] fieldIdToDocID = new int[numDocs];
       for (int i = 0; i < numDocs; i++) {
-        int fieldId = docIdToFieldId.get(i);
+        int fieldId = (int) docIdToFieldId.get(i);
         fieldIdToDocID[fieldId] = i;
       }
 
-      try {
-        final IndexSearcher s = newSearcher(r);
-
-        for (int contentID = 0; contentID < 3; contentID++) {
-          final ScoreDoc[] hits = s.search(new TermQuery(new Term("content", "real" + contentID)), numDocs).scoreDocs;
-          for (ScoreDoc hit : hits) {
-            final GroupDoc gd = groupDocs[docIdToFieldId.get(hit.doc)];
-            assertTrue(gd.score == 0.0);
-            gd.score = hit.score;
-            int docId = gd.id;
-            assertEquals(docId, docIdToFieldId.get(hit.doc));
-          }
-        }
-
-        for (GroupDoc gd : groupDocs) {
-          assertTrue(gd.score != 0.0);
+      final IndexSearcher s = newSearcher(r);
+      
+      for (int contentID = 0; contentID < 3; contentID++) {
+        final ScoreDoc[] hits = s.search(new TermQuery(new Term("content", "real" + contentID)), numDocs).scoreDocs;
+        for (ScoreDoc hit : hits) {
+          final GroupDoc gd = groupDocs[(int) docIdToFieldId.get(hit.doc)];
+          assertTrue(gd.score == 0.0);
+          gd.score = hit.score;
+          int docId = gd.id;
+          assertEquals(docId, docIdToFieldId.get(hit.doc));
+        }
+      }
+      
+      for (GroupDoc gd : groupDocs) {
+        assertTrue(gd.score != 0.0);
+      }
+      
+      for (int searchIter = 0; searchIter < 100; searchIter++) {
+        
+        if (VERBOSE) {
+          System.out.println("TEST: searchIter=" + searchIter);
         }
-
-        for (int searchIter = 0; searchIter < 100; searchIter++) {
-
-          if (VERBOSE) {
-            System.out.println("TEST: searchIter=" + searchIter);
+        
+        final String searchTerm = "real" + random().nextInt(3);
+        boolean sortByScoreOnly = random().nextBoolean();
+        Sort sortWithinGroup = getRandomSort(sortByScoreOnly);
+        AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = createRandomCollector("group", sortWithinGroup);
+        s.search(new TermQuery(new Term("content", searchTerm)), allGroupHeadsCollector);
+        int[] expectedGroupHeads = createExpectedGroupHeads(searchTerm, groupDocs, sortWithinGroup, sortByScoreOnly, fieldIdToDocID);
+        int[] actualGroupHeads = allGroupHeadsCollector.retrieveGroupHeads();
+        // The actual group heads contains Lucene ids. Need to change them into our id value.
+        for (int i = 0; i < actualGroupHeads.length; i++) {
+          actualGroupHeads[i] = (int) docIdToFieldId.get(actualGroupHeads[i]);
+        }
+        // Allows us the easily iterate and assert the actual and expected results.
+        Arrays.sort(expectedGroupHeads);
+        Arrays.sort(actualGroupHeads);
+        
+        if (VERBOSE) {
+          System.out.println("Collector: " + allGroupHeadsCollector.getClass().getSimpleName());
+          System.out.println("Sort within group: " + sortWithinGroup);
+          System.out.println("Num group: " + numGroups);
+          System.out.println("Num doc: " + numDocs);
+          System.out.println("\n=== Expected: \n");
+          for (int expectedDocId : expectedGroupHeads) {
+            GroupDoc expectedGroupDoc = groupDocs[expectedDocId];
+            String expectedGroup = expectedGroupDoc.group == null ? null : expectedGroupDoc.group.utf8ToString();
+            System.out.println(
+                String.format(Locale.ROOT,
+                    "Group:%10s score%5f Sort1:%10s Sort2:%10s Sort3:%10s doc:%5d",
+                    expectedGroup, expectedGroupDoc.score, expectedGroupDoc.sort1.utf8ToString(),
+                    expectedGroupDoc.sort2.utf8ToString(), expectedGroupDoc.sort3.utf8ToString(), expectedDocId
+                    )
+                );
           }
-
-          final String searchTerm = "real" + random().nextInt(3);
-          boolean sortByScoreOnly = random().nextBoolean();
-          Sort sortWithinGroup = getRandomSort(sortByScoreOnly);
-          AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = createRandomCollector("group", sortWithinGroup);
-          s.search(new TermQuery(new Term("content", searchTerm)), allGroupHeadsCollector);
-          int[] expectedGroupHeads = createExpectedGroupHeads(searchTerm, groupDocs, sortWithinGroup, sortByScoreOnly, fieldIdToDocID);
-          int[] actualGroupHeads = allGroupHeadsCollector.retrieveGroupHeads();
-          // The actual group heads contains Lucene ids. Need to change them into our id value.
-          for (int i = 0; i < actualGroupHeads.length; i++) {
-            actualGroupHeads[i] = docIdToFieldId.get(actualGroupHeads[i]);
+          System.out.println("\n=== Actual: \n");
+          for (int actualDocId : actualGroupHeads) {
+            GroupDoc actualGroupDoc = groupDocs[actualDocId];
+            String actualGroup = actualGroupDoc.group == null ? null : actualGroupDoc.group.utf8ToString();
+            System.out.println(
+                String.format(Locale.ROOT,
+                    "Group:%10s score%5f Sort1:%10s Sort2:%10s Sort3:%10s doc:%5d",
+                    actualGroup, actualGroupDoc.score, actualGroupDoc.sort1.utf8ToString(),
+                    actualGroupDoc.sort2.utf8ToString(), actualGroupDoc.sort3.utf8ToString(), actualDocId
+                    )
+                );
           }
-          // Allows us the easily iterate and assert the actual and expected results.
-          Arrays.sort(expectedGroupHeads);
-          Arrays.sort(actualGroupHeads);
-
-          if (VERBOSE) {
-            System.out.println("Collector: " + allGroupHeadsCollector.getClass().getSimpleName());
-            System.out.println("Sort within group: " + sortWithinGroup);
-            System.out.println("Num group: " + numGroups);
-            System.out.println("Num doc: " + numDocs);
-            System.out.println("\n=== Expected: \n");
-            for (int expectedDocId : expectedGroupHeads) {
-              GroupDoc expectedGroupDoc = groupDocs[expectedDocId];
-              String expectedGroup = expectedGroupDoc.group == null ? null : expectedGroupDoc.group.utf8ToString();
-              System.out.println(
-                  String.format(Locale.ROOT,
-                      "Group:%10s score%5f Sort1:%10s Sort2:%10s Sort3:%10s doc:%5d",
-                      expectedGroup, expectedGroupDoc.score, expectedGroupDoc.sort1.utf8ToString(),
-                      expectedGroupDoc.sort2.utf8ToString(), expectedGroupDoc.sort3.utf8ToString(), expectedDocId
-                  )
-              );
-            }
-            System.out.println("\n=== Actual: \n");
-            for (int actualDocId : actualGroupHeads) {
-              GroupDoc actualGroupDoc = groupDocs[actualDocId];
-              String actualGroup = actualGroupDoc.group == null ? null : actualGroupDoc.group.utf8ToString();
-              System.out.println(
-                  String.format(Locale.ROOT,
-                      "Group:%10s score%5f Sort1:%10s Sort2:%10s Sort3:%10s doc:%5d",
-                      actualGroup, actualGroupDoc.score, actualGroupDoc.sort1.utf8ToString(),
-                      actualGroupDoc.sort2.utf8ToString(), actualGroupDoc.sort3.utf8ToString(), actualDocId
-                  )
-              );
-            }
-            System.out.println("\n===================================================================================");
-          }
-
-          assertArrayEquals(expectedGroupHeads, actualGroupHeads);
+          System.out.println("\n===================================================================================");
         }
-      } finally {
-        QueryUtils.purgeFieldCache(r);
+        
+        assertArrayEquals(expectedGroupHeads, actualGroupHeads);
       }
-
+      
       r.close();
       dir.close();
     }
@@ -542,14 +528,13 @@
   }
 
   private void addGroupField(Document doc, String groupField, String value, DocValuesType valueType) {
-    doc.add(new TextField(groupField, value, Field.Store.NO));
     Field valuesField = null;
     switch(valueType) {
       case BINARY:
-        valuesField = new BinaryDocValuesField(groupField + "_dv", new BytesRef(value));
+        valuesField = new BinaryDocValuesField(groupField, new BytesRef(value));
         break;
       case SORTED:
-        valuesField = new SortedDocValuesField(groupField + "_dv", new BytesRef(value));
+        valuesField = new SortedDocValuesField(groupField, new BytesRef(value));
         break;
       default:
         fail("unhandled type");


diff -ruN -x .svn -x build lucene-trunk/lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest.java lucene5666/lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest.java
--- lucene-trunk/lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest.java	2014-05-14 03:47:28.830646625 -0400
+++ lucene5666/lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest.java	2014-05-12 13:28:55.596244941 -0400
@@ -17,16 +17,27 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import java.util.Random;
+import java.util.Set;
+
 import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.StoredDocument;
 import org.apache.lucene.index.Term;
@@ -41,92 +52,71 @@
 import org.apache.lucene.search.grouping.term.TermFirstPassGroupingCollector;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 import org.apache.lucene.util.TestUtil;
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueStr;
 
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Random;
-import java.util.Set;
 
+@SuppressCodecs({"Lucene40", "Lucene41", "Lucene42"}) // we need missing support... i think?
 public class DistinctValuesCollectorTest extends AbstractGroupingTestCase {
 
   private final static NullComparator nullComparator = new NullComparator();
   
   private final String groupField = "author";
-  private final String dvGroupField = "author_dv";
   private final String countField = "publisher";
-  private final String dvCountField = "publisher_dv";
 
   public void testSimple() throws Exception {
     Random random = random();
-    DocValuesType[] dvTypes = new DocValuesType[]{
-        DocValuesType.NUMERIC,
-        DocValuesType.BINARY,
-        DocValuesType.SORTED,
-    };
     Directory dir = newDirectory();
     RandomIndexWriter w = new RandomIndexWriter(
         random,
         dir,
         newIndexWriterConfig(TEST_VERSION_CURRENT,
             new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));
-    boolean canUseDV = true;
-    DocValuesType dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;
-
     Document doc = new Document();
-    addField(doc, groupField, "1", dvType);
-    addField(doc, countField, "1", dvType);
+    addField(doc, groupField, "1");
+    addField(doc, countField, "1");
     doc.add(new TextField("content", "random text", Field.Store.NO));
     doc.add(new StringField("id", "1", Field.Store.NO));
     w.addDocument(doc);
 
     // 1
     doc = new Document();
-    addField(doc, groupField, "1", dvType);
-    addField(doc, countField, "1", dvType);
+    addField(doc, groupField, "1");
+    addField(doc, countField, "1");
     doc.add(new TextField("content", "some more random text blob", Field.Store.NO));
     doc.add(new StringField("id", "2", Field.Store.NO));
     w.addDocument(doc);
 
     // 2
     doc = new Document();
-    addField(doc, groupField, "1", dvType);
-    addField(doc, countField, "2", dvType);
+    addField(doc, groupField, "1");
+    addField(doc, countField, "2");
     doc.add(new TextField("content", "some more random textual data", Field.Store.NO));
     doc.add(new StringField("id", "3", Field.Store.NO));
     w.addDocument(doc);
     w.commit(); // To ensure a second segment
 
-    // 3
+    // 3 -- no count field
     doc = new Document();
-    addField(doc, groupField, "2", dvType);
+    addField(doc, groupField, "2");
     doc.add(new TextField("content", "some random text", Field.Store.NO));
     doc.add(new StringField("id", "4", Field.Store.NO));
     w.addDocument(doc);
 
     // 4
     doc = new Document();
-    addField(doc, groupField, "3", dvType);
-    addField(doc, countField, "1", dvType);
+    addField(doc, groupField, "3");
+    addField(doc, countField, "1");
     doc.add(new TextField("content", "some more random text", Field.Store.NO));
     doc.add(new StringField("id", "5", Field.Store.NO));
     w.addDocument(doc);
 
     // 5
     doc = new Document();
-    addField(doc, groupField, "3", dvType);
-    addField(doc, countField, "1", dvType);
+    addField(doc, groupField, "3");
+    addField(doc, countField, "1");
     doc.add(new TextField("content", "random blob", Field.Store.NO));
     doc.add(new StringField("id", "6", Field.Store.NO));
     w.addDocument(doc);
@@ -134,7 +124,7 @@
     // 6 -- no author field
     doc = new Document();
     doc.add(new TextField("content", "random word stuck in alot of other text", Field.Store.YES));
-    addField(doc, countField, "1", dvType);
+    addField(doc, countField, "1");
     doc.add(new StringField("id", "6", Field.Store.NO));
     w.addDocument(doc);
 
@@ -160,13 +150,13 @@
     };
 
     // === Search for content:random
-    AbstractFirstPassGroupingCollector<Comparable<Object>> firstCollector = createRandomFirstPassCollector(dvType, new Sort(), groupField, 10);
+    AbstractFirstPassGroupingCollector<Comparable<Object>> firstCollector = createRandomFirstPassCollector(new Sort(), groupField, 10);
     indexSearcher.search(new TermQuery(new Term("content", "random")), firstCollector);
     AbstractDistinctValuesCollector<? extends AbstractDistinctValuesCollector.GroupCount<Comparable<Object>>> distinctValuesCollector
-        = createDistinctCountCollector(firstCollector, groupField, countField, dvType);
+        = createDistinctCountCollector(firstCollector, groupField, countField);
     indexSearcher.search(new TermQuery(new Term("content", "random")), distinctValuesCollector);
 
-    List<? extends AbstractDistinctValuesCollector.GroupCount<Comparable<Object>>> gcs =  distinctValuesCollector.getGroups();
+    List<? extends AbstractDistinctValuesCollector.GroupCount<Comparable<Object>>> gcs = distinctValuesCollector.getGroups();
     Collections.sort(gcs, cmp);
     assertEquals(4, gcs.size());
 
@@ -193,9 +183,9 @@
     compare("1", countValues.get(0));
 
     // === Search for content:some
-    firstCollector = createRandomFirstPassCollector(dvType, new Sort(), groupField, 10);
+    firstCollector = createRandomFirstPassCollector(new Sort(), groupField, 10);
     indexSearcher.search(new TermQuery(new Term("content", "some")), firstCollector);
-    distinctValuesCollector = createDistinctCountCollector(firstCollector, groupField, countField, dvType);
+    distinctValuesCollector = createDistinctCountCollector(firstCollector, groupField, countField);
     indexSearcher.search(new TermQuery(new Term("content", "some")), distinctValuesCollector);
 
     gcs = distinctValuesCollector.getGroups();
@@ -220,9 +210,9 @@
     compare("1", countValues.get(0));
 
      // === Search for content:blob
-    firstCollector = createRandomFirstPassCollector(dvType, new Sort(), groupField, 10);
+    firstCollector = createRandomFirstPassCollector(new Sort(), groupField, 10);
     indexSearcher.search(new TermQuery(new Term("content", "blob")), firstCollector);
-    distinctValuesCollector = createDistinctCountCollector(firstCollector, groupField, countField, dvType);
+    distinctValuesCollector = createDistinctCountCollector(firstCollector, groupField, countField);
     indexSearcher.search(new TermQuery(new Term("content", "blob")), distinctValuesCollector);
 
     gcs = distinctValuesCollector.getGroups();
@@ -251,18 +241,16 @@
       IndexContext context = createIndexContext();
       for (int searchIter = 0; searchIter < 100; searchIter++) {
         final IndexSearcher searcher = newSearcher(context.indexReader);
-        boolean useDv = context.dvType != null && random.nextBoolean();
-        DocValuesType dvType = useDv ? context.dvType : null;
         String term = context.contentStrings[random.nextInt(context.contentStrings.length)];
         Sort groupSort = new Sort(new SortField("id", SortField.Type.STRING));
         int topN = 1 + random.nextInt(10);
 
         List<AbstractDistinctValuesCollector.GroupCount<Comparable<?>>> expectedResult = createExpectedResult(context, term, groupSort, topN);
 
-        AbstractFirstPassGroupingCollector<Comparable<?>> firstCollector = createRandomFirstPassCollector(dvType, groupSort, groupField, topN);
+        AbstractFirstPassGroupingCollector<Comparable<?>> firstCollector = createRandomFirstPassCollector(groupSort, groupField, topN);
         searcher.search(new TermQuery(new Term("content", term)), firstCollector);
         AbstractDistinctValuesCollector<? extends AbstractDistinctValuesCollector.GroupCount<Comparable<?>>> distinctValuesCollector
-            = createDistinctCountCollector(firstCollector, groupField, countField, dvType);
+            = createDistinctCountCollector(firstCollector, groupField, countField);
         searcher.search(new TermQuery(new Term("content", term)), distinctValuesCollector);
         @SuppressWarnings("unchecked")
         List<AbstractDistinctValuesCollector.GroupCount<Comparable<?>>> actualResult = (List<AbstractDistinctValuesCollector.GroupCount<Comparable<?>>>) distinctValuesCollector.getGroups();
@@ -273,7 +261,6 @@
           System.out.println("1st pass collector class name=" + firstCollector.getClass().getName());
           System.out.println("2nd pass collector class name=" + distinctValuesCollector.getClass().getName());
           System.out.println("Search term=" + term);
-          System.out.println("DVType=" + dvType);
           System.out.println("1st pass groups=" + firstCollector.getTopGroups(0, false));
           System.out.println("Expected:");      
           printGroups(expectedResult);
@@ -363,33 +350,14 @@
     }
   }
 
-  private void addField(Document doc, String field, String value, DocValuesType type) {
-    doc.add(new StringField(field, value, Field.Store.YES));
-    if (type == null) {
-      return;
-    }
-    String dvField = field + "_dv";
-
-    Field valuesField = null;
-    switch (type) {
-      case NUMERIC:
-        valuesField = new NumericDocValuesField(dvField, Integer.parseInt(value));
-        break;
-      case BINARY:
-        valuesField = new BinaryDocValuesField(dvField, new BytesRef(value));
-        break;
-      case SORTED:
-        valuesField = new SortedDocValuesField(dvField, new BytesRef(value));
-        break;
-    }
-    doc.add(valuesField);
+  private void addField(Document doc, String field, String value) {
+    doc.add(new SortedDocValuesField(field, new BytesRef(value)));
   }
 
   @SuppressWarnings({"unchecked","rawtypes"})
   private <T extends Comparable> AbstractDistinctValuesCollector<AbstractDistinctValuesCollector.GroupCount<T>> createDistinctCountCollector(AbstractFirstPassGroupingCollector<T> firstPassGroupingCollector,
                                                                       String groupField,
-                                                                      String countField,
-                                                                      DocValuesType dvType) {
+                                                                      String countField) {
     Random random = random();
     Collection<SearchGroup<T>> searchGroups = firstPassGroupingCollector.getTopGroups(0, false);
     if (FunctionFirstPassGroupingCollector.class.isAssignableFrom(firstPassGroupingCollector.getClass())) {
@@ -400,20 +368,12 @@
   }
 
   @SuppressWarnings({"unchecked","rawtypes"})
-  private <T> AbstractFirstPassGroupingCollector<T> createRandomFirstPassCollector(DocValuesType dvType, Sort groupSort, String groupField, int topNGroups) throws IOException {
+  private <T> AbstractFirstPassGroupingCollector<T> createRandomFirstPassCollector(Sort groupSort, String groupField, int topNGroups) throws IOException {
     Random random = random();
-    if (dvType != null) {
-      if (random.nextBoolean()) {
-        return (AbstractFirstPassGroupingCollector<T>) new FunctionFirstPassGroupingCollector(new BytesRefFieldSource(groupField), new HashMap<>(), groupSort, topNGroups);
-      } else {
-        return (AbstractFirstPassGroupingCollector<T>) new TermFirstPassGroupingCollector(groupField, groupSort, topNGroups);
-      }
+    if (random.nextBoolean()) {
+      return (AbstractFirstPassGroupingCollector<T>) new FunctionFirstPassGroupingCollector(new BytesRefFieldSource(groupField), new HashMap<>(), groupSort, topNGroups);
     } else {
-      if (random.nextBoolean()) {
-        return (AbstractFirstPassGroupingCollector<T>) new FunctionFirstPassGroupingCollector(new BytesRefFieldSource(groupField), new HashMap<>(), groupSort, topNGroups);
-      } else {
-        return (AbstractFirstPassGroupingCollector<T>) new TermFirstPassGroupingCollector(groupField, groupSort, topNGroups);
-      }
+      return (AbstractFirstPassGroupingCollector<T>) new TermFirstPassGroupingCollector(groupField, groupSort, topNGroups);
     }
   }
 
@@ -444,10 +404,6 @@
 
   private IndexContext createIndexContext() throws Exception {
     Random random = random();
-    DocValuesType[] dvTypes = new DocValuesType[]{
-        DocValuesType.BINARY,
-        DocValuesType.SORTED
-    };
 
     Directory dir = newDirectory();
     RandomIndexWriter w = new RandomIndexWriter(
@@ -457,9 +413,6 @@
         new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy())
       );
 
-    boolean canUseDV = true;
-    DocValuesType dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;
-
     int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;
     String[] groupValues = new String[numDocs / 5];
     String[] countValues = new String[numDocs / 10];
@@ -492,10 +445,10 @@
       Document doc = new Document();
       doc.add(new StringField("id", String.format(Locale.ROOT, "%09d", i), Field.Store.YES));
       if (groupValue != null) {
-        addField(doc, groupField, groupValue, dvType);
+        addField(doc, groupField, groupValue);
       }
       if (countValue != null) {
-        addField(doc, countField, countValue, dvType);
+        addField(doc, countField, countValue);
       }
       doc.add(new TextField("content", content, Field.Store.YES));
       w.addDocument(doc);
@@ -510,22 +463,20 @@
     }
 
     w.shutdown();
-    return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));
+    return new IndexContext(dir, reader, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));
   }
 
   private static class IndexContext {
 
     final Directory directory;
     final DirectoryReader indexReader;
-    final DocValuesType dvType;
     final Map<String, Map<String, Set<String>>> searchTermToGroupCounts;
     final String[] contentStrings;
 
-    IndexContext(Directory directory, DirectoryReader indexReader, DocValuesType dvType,
+    IndexContext(Directory directory, DirectoryReader indexReader, 
                  Map<String, Map<String, Set<String>>> searchTermToGroupCounts, String[] contentStrings) {
       this.directory = directory;
       this.indexReader = indexReader;
-      this.dvType = dvType;
       this.searchTermToGroupCounts = searchTermToGroupCounts;
       this.contentStrings = contentStrings;
     }


diff -ruN -x .svn -x build lucene-trunk/lucene/grouping/src/test/org/apache/lucene/search/grouping/GroupFacetCollectorTest.java lucene5666/lucene/grouping/src/test/org/apache/lucene/search/grouping/GroupFacetCollectorTest.java
--- lucene-trunk/lucene/grouping/src/test/org/apache/lucene/search/grouping/GroupFacetCollectorTest.java	2014-05-14 03:47:28.830646625 -0400
+++ lucene5666/lucene/grouping/src/test/org/apache/lucene/search/grouping/GroupFacetCollectorTest.java	2014-05-12 13:28:56.928244965 -0400
@@ -17,11 +17,26 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import java.util.NavigableSet;
+import java.util.Random;
+import java.util.Set;
+import java.util.TreeSet;
+
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.SortedDocValuesField;
+import org.apache.lucene.document.SortedSetDocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.NoMergePolicy;
@@ -33,22 +48,12 @@
 import org.apache.lucene.search.grouping.term.TermGroupFacetCollector;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 import org.apache.lucene.util.TestUtil;
 
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.NavigableSet;
-import java.util.Random;
-import java.util.Set;
-import java.util.TreeSet;
 
+// Need SSDV
+@SuppressCodecs({"Lucene40", "Lucene41"})
 public class GroupFacetCollectorTest extends AbstractGroupingTestCase {
 
   public void testSimple() throws Exception {
@@ -62,7 +67,7 @@
         dir,
         newIndexWriterConfig(TEST_VERSION_CURRENT,
             new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
-    boolean useDv = random().nextBoolean();
+    boolean useDv = true;
 
     // 0
     Document doc = new Document();
@@ -287,7 +292,7 @@
         dir,
         newIndexWriterConfig(TEST_VERSION_CURRENT,
             new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.INSTANCE));
-    boolean useDv = false;
+    boolean useDv = true;
 
     // Cannot assert this since we use NoMergePolicy:
     w.setDoRandomForceMergeAssert(false);
@@ -300,7 +305,7 @@
     // 1
     doc = new Document();
     addField(doc, groupField, "a", useDv);
-    doc.add(new StringField("airport", "ams", Field.Store.NO));
+    doc.add(new SortedSetDocValuesField("airport", new BytesRef("ams")));
     w.addDocument(doc);
 
     w.commit();
@@ -309,32 +314,32 @@
     // 2
     doc = new Document();
     addField(doc, groupField, "a", useDv);
-    doc.add(new StringField("airport", "ams", Field.Store.NO));
+    doc.add(new SortedSetDocValuesField("airport", new BytesRef("ams")));
     w.addDocument(doc);
 
     // 3
     doc = new Document();
     addField(doc, groupField, "a", useDv);
-    doc.add(new StringField("airport", "dus", Field.Store.NO));
+    doc.add(new SortedSetDocValuesField("airport", new BytesRef("dus")));
 
     w.addDocument(doc);
 
     // 4
     doc = new Document();
     addField(doc, groupField, "b", useDv);
-    doc.add(new StringField("airport", "ams", Field.Store.NO));
+    doc.add(new SortedSetDocValuesField("airport", new BytesRef("ams")));
     w.addDocument(doc);
 
     // 5
     doc = new Document();
     addField(doc, groupField, "b", useDv);
-    doc.add(new StringField("airport", "ams", Field.Store.NO));
+    doc.add(new SortedSetDocValuesField("airport", new BytesRef("ams")));
     w.addDocument(doc);
 
     // 6
     doc = new Document();
     addField(doc, groupField, "b", useDv);
-    doc.add(new StringField("airport", "ams", Field.Store.NO));
+    doc.add(new SortedSetDocValuesField("airport", new BytesRef("ams")));
     w.addDocument(doc);
     w.commit();
 
@@ -346,7 +351,7 @@
 
     w.shutdown();
     IndexSearcher indexSearcher = newSearcher(DirectoryReader.open(dir));
-    AbstractGroupFacetCollector groupedAirportFacetCollector = createRandomCollector(groupField, "airport", null, true);
+    AbstractGroupFacetCollector groupedAirportFacetCollector = createRandomCollector(groupField + "_dv", "airport", null, true);
     indexSearcher.search(new MatchAllDocsQuery(), groupedAirportFacetCollector);
     TermGroupFacetCollector.GroupedFacetResult airportResult = groupedAirportFacetCollector.mergeSegmentResults(10, 0, false);
     assertEquals(3, airportResult.getTotalCount());
@@ -364,10 +369,8 @@
   }
 
   private void addField(Document doc, String field, String value, boolean canUseIDV) {
-    doc.add(new StringField(field, value, Field.Store.NO));
-    if (canUseIDV) {
-      doc.add(new SortedDocValuesField(field + "_dv", new BytesRef(value)));
-    }
+    assert canUseIDV;
+    doc.add(new SortedDocValuesField(field + "_dv", new BytesRef(value)));
   }
 
   public void testRandom() throws Exception {
@@ -386,7 +389,6 @@
         if (VERBOSE) {
           System.out.println("TEST: searchIter=" + searchIter);
         }
-        boolean useDv = !multipleFacetsPerDocument && context.useDV && random.nextBoolean();
         String searchTerm = context.contentStrings[random.nextInt(context.contentStrings.length)];
         int limit = random.nextInt(context.facetValues.size());
         int offset = random.nextInt(context.facetValues.size() - limit);
@@ -409,7 +411,7 @@
         }
 
         GroupedFacetResult expectedFacetResult = createExpectedFacetResult(searchTerm, context, offset, limit, minCount, orderByCount, facetPrefix);
-        AbstractGroupFacetCollector groupFacetCollector = createRandomCollector(useDv ? "group_dv" : "group", useDv ? "facet_dv" : "facet", facetPrefix, multipleFacetsPerDocument);
+        AbstractGroupFacetCollector groupFacetCollector = createRandomCollector("group", "facet", facetPrefix, multipleFacetsPerDocument);
         searcher.search(new TermQuery(new Term("content", searchTerm)), groupFacetCollector);
         TermGroupFacetCollector.GroupedFacetResult actualFacetResult = groupFacetCollector.mergeSegmentResults(size, minCount, orderByCount);
 
@@ -417,7 +419,6 @@
         List<TermGroupFacetCollector.FacetEntry> actualFacetEntries = actualFacetResult.getFacetEntries(offset, limit);
 
         if (VERBOSE) {
-          System.out.println("Use DV: " + useDv);
           System.out.println("Collector: " + groupFacetCollector.getClass().getSimpleName());
           System.out.println("Num group: " + context.numGroups);
           System.out.println("Num doc: " + context.numDocs);
@@ -514,35 +515,29 @@
             new MockAnalyzer(random)
         )
     );
-    boolean canUseDV = true;
-    boolean useDv = canUseDV && !multipleFacetValuesPerDocument && random.nextBoolean();
-
     Document doc = new Document();
     Document docNoGroup = new Document();
     Document docNoFacet = new Document();
     Document docNoGroupNoFacet = new Document();
     Field group = newStringField("group", "", Field.Store.NO);
-    Field groupDc = new SortedDocValuesField("group_dv", new BytesRef());
-    if (useDv) {
-      doc.add(groupDc);
-      docNoFacet.add(groupDc);
-    }
+    Field groupDc = new SortedDocValuesField("group", new BytesRef());
+    doc.add(groupDc);
+    docNoFacet.add(groupDc);
     doc.add(group);
     docNoFacet.add(group);
     Field[] facetFields;
-    if (useDv) {
-      assert !multipleFacetValuesPerDocument;
+    if (multipleFacetValuesPerDocument == false) {
       facetFields = new Field[2];
       facetFields[0] = newStringField("facet", "", Field.Store.NO);
       doc.add(facetFields[0]);
       docNoGroup.add(facetFields[0]);
-      facetFields[1] = new SortedDocValuesField("facet_dv", new BytesRef());
+      facetFields[1] = new SortedDocValuesField("facet", new BytesRef());
       doc.add(facetFields[1]);
       docNoGroup.add(facetFields[1]);
     } else {
       facetFields = multipleFacetValuesPerDocument ? new Field[2 + random.nextInt(6)] : new Field[1];
       for (int i = 0; i < facetFields.length; i++) {
-        facetFields[i] = newStringField("facet", "", Field.Store.NO);
+        facetFields[i] = new SortedSetDocValuesField("facet", new BytesRef());
         doc.add(facetFields[i]);
         docNoGroup.add(facetFields[i]);
       }
@@ -576,11 +571,7 @@
       if (random.nextInt(24) == 17) {
         // So we test the "doc doesn't have the group'd
         // field" case:
-        if (useDv) {
-          groupValue = "";
-        } else {
-          groupValue = null;
-        }
+        groupValue = "";
       } else {
         groupValue = groups.get(random.nextInt(groups.size()));
       }
@@ -592,8 +583,22 @@
       Map<String, Set<String>> facetToGroups = searchTermToFacetToGroups.get(contentStr);
 
       List<String> facetVals = new ArrayList<>();
-      if (useDv || random.nextInt(24) != 18) {
-        if (useDv) {
+      if (multipleFacetValuesPerDocument == false) {
+        String facetValue = facetValues.get(random.nextInt(facetValues.size()));
+        uniqueFacetValues.add(facetValue);
+        if (!facetToGroups.containsKey(facetValue)) {
+          facetToGroups.put(facetValue, new HashSet<String>());
+        }
+        Set<String> groupsInFacet = facetToGroups.get(facetValue);
+        groupsInFacet.add(groupValue);
+        if (groupsInFacet.size() > facetWithMostGroups) {
+          facetWithMostGroups = groupsInFacet.size();
+        }
+        facetFields[0].setStringValue(facetValue);
+        facetFields[1].setBytesValue(new BytesRef(facetValue));
+        facetVals.add(facetValue);
+      } else {
+        for (Field facetField : facetFields) {
           String facetValue = facetValues.get(random.nextInt(facetValues.size()));
           uniqueFacetValues.add(facetValue);
           if (!facetToGroups.containsKey(facetValue)) {
@@ -604,34 +609,8 @@
           if (groupsInFacet.size() > facetWithMostGroups) {
             facetWithMostGroups = groupsInFacet.size();
           }
-          facetFields[0].setStringValue(facetValue);
-          facetFields[1].setBytesValue(new BytesRef(facetValue));
+          facetField.setBytesValue(new BytesRef(facetValue));
           facetVals.add(facetValue);
-        } else {
-          for (Field facetField : facetFields) {
-            String facetValue = facetValues.get(random.nextInt(facetValues.size()));
-            uniqueFacetValues.add(facetValue);
-            if (!facetToGroups.containsKey(facetValue)) {
-              facetToGroups.put(facetValue, new HashSet<String>());
-            }
-            Set<String> groupsInFacet = facetToGroups.get(facetValue);
-            groupsInFacet.add(groupValue);
-            if (groupsInFacet.size() > facetWithMostGroups) {
-              facetWithMostGroups = groupsInFacet.size();
-            }
-            facetField.setStringValue(facetValue);
-            facetVals.add(facetValue);
-          }
-        }
-      } else {
-        uniqueFacetValues.add(null);
-        if (!facetToGroups.containsKey(null)) {
-          facetToGroups.put(null, new HashSet<String>());
-        }
-        Set<String> groupsInFacet = facetToGroups.get(null);
-        groupsInFacet.add(groupValue);
-        if (groupsInFacet.size() > facetWithMostGroups) {
-          facetWithMostGroups = groupsInFacet.size();
         }
       }
 
@@ -640,11 +619,10 @@
       }
 
       if (groupValue != null) {
-        if (useDv) {
-          groupDc.setBytesValue(new BytesRef(groupValue));
-        }
+        groupDc.setBytesValue(new BytesRef(groupValue));
         group.setStringValue(groupValue);
-      } else if (useDv) {
+      } else {
+        // TODO: not true
         // DV cannot have missing values:
         groupDc.setBytesValue(new BytesRef());
       }
@@ -663,7 +641,7 @@
     DirectoryReader reader = writer.getReader();
     writer.shutdown();
 
-    return new IndexContext(searchTermToFacetToGroups, reader, numDocs, dir, facetWithMostGroups, numGroups, contentBrs, uniqueFacetValues, useDv);
+    return new IndexContext(searchTermToFacetToGroups, reader, numDocs, dir, facetWithMostGroups, numGroups, contentBrs, uniqueFacetValues);
   }
 
   private GroupedFacetResult createExpectedFacetResult(String searchTerm, IndexContext context, int offset, int limit, int minCount, final boolean orderByCount, String facetPrefix) {
@@ -738,8 +716,6 @@
 
   private AbstractGroupFacetCollector createRandomCollector(String groupField, String facetField, String facetPrefix, boolean multipleFacetsPerDocument) {
     BytesRef facetPrefixBR = facetPrefix == null ? null : new BytesRef(facetPrefix);
-    // DocValues cannot be multi-valued:
-    assert !multipleFacetsPerDocument || !groupField.endsWith("_dv");
     return TermGroupFacetCollector.createTermGroupFacetCollector(groupField, facetField, multipleFacetsPerDocument, facetPrefixBR, random().nextInt(1024));
   }
 
@@ -764,10 +740,9 @@
     final int facetWithMostGroups;
     final int numGroups;
     final String[] contentStrings;
-    final boolean useDV;
 
     public IndexContext(Map<String, Map<String, Set<String>>> searchTermToFacetGroups, DirectoryReader r,
-                        int numDocs, Directory dir, int facetWithMostGroups, int numGroups, String[] contentStrings, NavigableSet<String> facetValues, boolean useDV) {
+                        int numDocs, Directory dir, int facetWithMostGroups, int numGroups, String[] contentStrings, NavigableSet<String> facetValues) {
       this.searchTermToFacetGroups = searchTermToFacetGroups;
       this.indexReader = r;
       this.numDocs = numDocs;
@@ -776,7 +751,6 @@
       this.numGroups = numGroups;
       this.contentStrings = contentStrings;
       this.facetValues = facetValues;
-      this.useDV = useDV;
     }
   }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java lucene5666/lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java
--- lucene-trunk/lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java	2014-05-14 03:47:28.830646625 -0400
+++ lucene5666/lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java	2014-05-14 03:45:16.694644324 -0400
@@ -21,6 +21,8 @@
 import org.apache.lucene.document.*;
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.MultiDocValues;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -39,6 +41,7 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 import org.apache.lucene.util.TestUtil;
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueStr;
@@ -52,6 +55,7 @@
 //   - test ties
 //   - test compound sort
 
+@SuppressCodecs({"Lucene40", "Lucene41", "Lucene42"}) // we need missing support... i think?
 public class TestGrouping extends LuceneTestCase {
 
   public void testBasic() throws Exception {
@@ -120,10 +124,6 @@
 
     final Sort groupSort = Sort.RELEVANCE;
 
-    if (random().nextBoolean()) {
-      groupField += "_dv";
-    }
-
     final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, 10);
     indexSearcher.search(new TermQuery(new Term("content", "random")), c1);
 
@@ -172,8 +172,7 @@
   }
 
   private void addGroupField(Document doc, String groupField, String value) {
-    doc.add(new TextField(groupField, value, Field.Store.YES));
-    doc.add(new SortedDocValuesField(groupField + "_dv", new BytesRef(value)));
+    doc.add(new SortedDocValuesField(groupField, new BytesRef(value)));
   }
 
   private AbstractFirstPassGroupingCollector<?> createRandomFirstPassCollector(String groupField, Sort groupSort, int topDocs) throws IOException {
@@ -569,10 +568,14 @@
         docs.add(doc);
         if (groupValue.group != null) {
           doc.add(newStringField("group", groupValue.group.utf8ToString(), Field.Store.YES));
+          doc.add(new SortedDocValuesField("group", BytesRef.deepCopyOf(groupValue.group)));
         }
         doc.add(newStringField("sort1", groupValue.sort1.utf8ToString(), Field.Store.NO));
+        doc.add(new SortedDocValuesField("sort1", BytesRef.deepCopyOf(groupValue.sort1)));
         doc.add(newStringField("sort2", groupValue.sort2.utf8ToString(), Field.Store.NO));
+        doc.add(new SortedDocValuesField("sort2", BytesRef.deepCopyOf(groupValue.sort2)));
         doc.add(new IntField("id", groupValue.id, Field.Store.NO));
+        doc.add(new NumericDocValuesField("id", groupValue.id));
         doc.add(newTextField("content", groupValue.content, Field.Store.NO));
         //System.out.println("TEST:     doc content=" + groupValue.content + " group=" + (groupValue.group == null ? "null" : groupValue.group.utf8ToString()) + " sort1=" + groupValue.sort1.utf8ToString() + " id=" + groupValue.id);
       }
@@ -642,7 +645,7 @@
           // For that reason we don't generate empty string
           // groups.
           randomValue = TestUtil.randomRealisticUnicodeString(random());
-          //randomValue = _TestUtil.randomSimpleString(random());
+          //randomValue = TestUtil.randomSimpleString(random());
         } while ("".equals(randomValue));
 
         groups.add(new BytesRef(randomValue));
@@ -670,22 +673,18 @@
                                                   dir,
                                                   newIndexWriterConfig(TEST_VERSION_CURRENT,
                                                                        new MockAnalyzer(random())));
-      boolean canUseIDV = true;
-
       Document doc = new Document();
       Document docNoGroup = new Document();
-      Field idvGroupField = new SortedDocValuesField("group_dv", new BytesRef());
-      if (canUseIDV) {
-        doc.add(idvGroupField);
-        docNoGroup.add(idvGroupField);
-      }
+      Field idvGroupField = new SortedDocValuesField("group", new BytesRef());
+      doc.add(idvGroupField);
+      docNoGroup.add(idvGroupField);
 
       Field group = newStringField("group", "", Field.Store.NO);
       doc.add(group);
-      Field sort1 = newStringField("sort1", "", Field.Store.NO);
+      Field sort1 = new SortedDocValuesField("sort1", new BytesRef());
       doc.add(sort1);
       docNoGroup.add(sort1);
-      Field sort2 = newStringField("sort2", "", Field.Store.NO);
+      Field sort2 = new SortedDocValuesField("sort2", new BytesRef());
       doc.add(sort2);
       docNoGroup.add(sort2);
       Field content = newTextField("content", "", Field.Store.NO);
@@ -693,7 +692,10 @@
       docNoGroup.add(content);
       IntField id = new IntField("id", 0, Field.Store.NO);
       doc.add(id);
+      NumericDocValuesField idDV = new NumericDocValuesField("id", 0);
+      doc.add(idDV);
       docNoGroup.add(id);
+      docNoGroup.add(idDV);
       final GroupDoc[] groupDocs = new GroupDoc[numDocs];
       for(int i=0;i<numDocs;i++) {
         final BytesRef groupValue;
@@ -716,19 +718,19 @@
         groupDocs[i] = groupDoc;
         if (groupDoc.group != null) {
           group.setStringValue(groupDoc.group.utf8ToString());
-          if (canUseIDV) {
-            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));
-          }
-        } else if (canUseIDV) {
+          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));
+        } else {
+          // TODO: not true
           // Must explicitly set empty string, else eg if
           // the segment has all docs missing the field then
           // we get null back instead of empty BytesRef:
           idvGroupField.setBytesValue(new BytesRef());
         }
-        sort1.setStringValue(groupDoc.sort1.utf8ToString());
-        sort2.setStringValue(groupDoc.sort2.utf8ToString());
+        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));
+        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));
         content.setStringValue(groupDoc.content);
         id.setIntValue(groupDoc.id);
+        idDV.setLongValue(groupDoc.id);
         if (groupDoc.group == null) {
           w.addDocument(docNoGroup);
         } else {
@@ -742,405 +744,387 @@
       final DirectoryReader r = w.getReader();
       w.shutdown();
 
-      // NOTE: intentional but temporary field cache insanity!
-      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(r), "id", false);
+      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, "id");
       DirectoryReader rBlocks = null;
       Directory dirBlocks = null;
 
-      try {
-        final IndexSearcher s = newSearcher(r);
+      final IndexSearcher s = newSearcher(r);
+      if (VERBOSE) {
+        System.out.println("\nTEST: searcher=" + s);
+      }
+      
+      final ShardState shards = new ShardState(s);
+      
+      for(int contentID=0;contentID<3;contentID++) {
+        final ScoreDoc[] hits = s.search(new TermQuery(new Term("content", "real"+contentID)), numDocs).scoreDocs;
+        for(ScoreDoc hit : hits) {
+          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];
+          assertTrue(gd.score == 0.0);
+          gd.score = hit.score;
+          assertEquals(gd.id, docIDToID.get(hit.doc));
+        }
+      }
+      
+      for(GroupDoc gd : groupDocs) {
+        assertTrue(gd.score != 0.0);
+      }
+      
+      // Build 2nd index, where docs are added in blocks by
+      // group, so we can use single pass collector
+      dirBlocks = newDirectory();
+      rBlocks = getDocBlockReader(dirBlocks, groupDocs);
+      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term("groupend", "x"))));
+      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, "id");
+      assertNotNull(docIDToIDBlocks);
+      
+      final IndexSearcher sBlocks = newSearcher(rBlocks);
+      final ShardState shardsBlocks = new ShardState(sBlocks);
+      
+      // ReaderBlocks only increases maxDoc() vs reader, which
+      // means a monotonic shift in scores, so we can
+      // reliably remap them w/ Map:
+      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();
+      
+      // Tricky: must separately set .score2, because the doc
+      // block index was created with possible deletions!
+      //System.out.println("fixup score2");
+      for(int contentID=0;contentID<3;contentID++) {
+        //System.out.println("  term=real" + contentID);
+        final Map<Float,Float> termScoreMap = new HashMap<>();
+        scoreMap.put("real"+contentID, termScoreMap);
+        //System.out.println("term=real" + contentID + " dfold=" + s.docFreq(new Term("content", "real"+contentID)) +
+        //" dfnew=" + sBlocks.docFreq(new Term("content", "real"+contentID)));
+        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term("content", "real"+contentID)), numDocs).scoreDocs;
+        for(ScoreDoc hit : hits) {
+          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];
+          assertTrue(gd.score2 == 0.0);
+          gd.score2 = hit.score;
+          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));
+          //System.out.println("    score=" + gd.score + " score2=" + hit.score + " id=" + docIDToIDBlocks.get(hit.doc));
+          termScoreMap.put(gd.score, gd.score2);
+        }
+      }
+      
+      for(int searchIter=0;searchIter<100;searchIter++) {
+        
         if (VERBOSE) {
-          System.out.println("\nTEST: searcher=" + s);
+          System.out.println("\nTEST: searchIter=" + searchIter);
         }
-
-        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {
-          canUseIDV = false;
-        } else {
-          canUseIDV = true;
-        }
-        final ShardState shards = new ShardState(s);
-
-        for(int contentID=0;contentID<3;contentID++) {
-          final ScoreDoc[] hits = s.search(new TermQuery(new Term("content", "real"+contentID)), numDocs).scoreDocs;
-          for(ScoreDoc hit : hits) {
-            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];
-            assertTrue(gd.score == 0.0);
-            gd.score = hit.score;
-            assertEquals(gd.id, docIDToID.get(hit.doc));
+        
+        final String searchTerm = "real" + random().nextInt(3);
+        final boolean fillFields = random().nextBoolean();
+        boolean getScores = random().nextBoolean();
+        final boolean getMaxScores = random().nextBoolean();
+        final Sort groupSort = getRandomSort();
+        //final Sort groupSort = new Sort(new SortField[] {new SortField("sort1", SortField.STRING), new SortField("id", SortField.INT)});
+        // TODO: also test null (= sort by relevance)
+        final Sort docSort = getRandomSort();
+        
+        for(SortField sf : docSort.getSort()) {
+          if (sf.getType() == SortField.Type.SCORE) {
+            getScores = true;
+            break;
           }
         }
-
-        for(GroupDoc gd : groupDocs) {
-          assertTrue(gd.score != 0.0);
-        }
-
-        // Build 2nd index, where docs are added in blocks by
-        // group, so we can use single pass collector
-        dirBlocks = newDirectory();
-        rBlocks = getDocBlockReader(dirBlocks, groupDocs);
-        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term("groupend", "x"))));
-        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(rBlocks), "id", false);
-
-        final IndexSearcher sBlocks = newSearcher(rBlocks);
-        final ShardState shardsBlocks = new ShardState(sBlocks);
-
-        // ReaderBlocks only increases maxDoc() vs reader, which
-        // means a monotonic shift in scores, so we can
-        // reliably remap them w/ Map:
-        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();
-
-        // Tricky: must separately set .score2, because the doc
-        // block index was created with possible deletions!
-        //System.out.println("fixup score2");
-        for(int contentID=0;contentID<3;contentID++) {
-          //System.out.println("  term=real" + contentID);
-          final Map<Float,Float> termScoreMap = new HashMap<>();
-          scoreMap.put("real"+contentID, termScoreMap);
-          //System.out.println("term=real" + contentID + " dfold=" + s.docFreq(new Term("content", "real"+contentID)) +
-          //" dfnew=" + sBlocks.docFreq(new Term("content", "real"+contentID)));
-          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term("content", "real"+contentID)), numDocs).scoreDocs;
-          for(ScoreDoc hit : hits) {
-            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];
-            assertTrue(gd.score2 == 0.0);
-            gd.score2 = hit.score;
-            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));
-            //System.out.println("    score=" + gd.score + " score2=" + hit.score + " id=" + docIDToIDBlocks.get(hit.doc));
-            termScoreMap.put(gd.score, gd.score2);
+        
+        for(SortField sf : groupSort.getSort()) {
+          if (sf.getType() == SortField.Type.SCORE) {
+            getScores = true;
+            break;
           }
         }
-
-        for(int searchIter=0;searchIter<100;searchIter++) {
-
+        
+        final int topNGroups = TestUtil.nextInt(random(), 1, 30);
+        //final int topNGroups = 10;
+        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);
+        
+        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);
+        //final int groupOffset = 0;
+        
+        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);
+        //final int docOffset = 0;
+        
+        final boolean doCache = random().nextBoolean();
+        final boolean doAllGroups = random().nextBoolean();
+        if (VERBOSE) {
+          System.out.println("TEST: groupSort=" + groupSort + " docSort=" + docSort + " searchTerm=" + searchTerm + " dF=" + r.docFreq(new Term("content", searchTerm))  +" dFBlock=" + rBlocks.docFreq(new Term("content", searchTerm)) + " topNGroups=" + topNGroups + " groupOffset=" + groupOffset + " docOffset=" + docOffset + " doCache=" + doCache + " docsPerGroup=" + docsPerGroup + " doAllGroups=" + doAllGroups + " getScores=" + getScores + " getMaxScores=" + getMaxScores);
+        }
+        
+        String groupField = "group";
+        if (VERBOSE) {
+          System.out.println("  groupField=" + groupField);
+        }
+        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);
+        final CachingCollector cCache;
+        final Collector c;
+        
+        final AbstractAllGroupsCollector<?> allGroupsCollector;
+        if (doAllGroups) {
+          allGroupsCollector = createAllGroupsCollector(c1, groupField);
+        } else {
+          allGroupsCollector = null;
+        }
+        
+        final boolean useWrappingCollector = random().nextBoolean();
+        
+        if (doCache) {
+          final double maxCacheMB = random().nextDouble();
           if (VERBOSE) {
-            System.out.println("\nTEST: searchIter=" + searchIter);
+            System.out.println("TEST: maxCacheMB=" + maxCacheMB);
           }
-
-          final String searchTerm = "real" + random().nextInt(3);
-          final boolean fillFields = random().nextBoolean();
-          boolean getScores = random().nextBoolean();
-          final boolean getMaxScores = random().nextBoolean();
-          final Sort groupSort = getRandomSort();
-          //final Sort groupSort = new Sort(new SortField[] {new SortField("sort1", SortField.STRING), new SortField("id", SortField.INT)});
-          // TODO: also test null (= sort by relevance)
-          final Sort docSort = getRandomSort();
-
-          for(SortField sf : docSort.getSort()) {
-            if (sf.getType() == SortField.Type.SCORE) {
-              getScores = true;
-              break;
-            }
-          }
-
-          for(SortField sf : groupSort.getSort()) {
-            if (sf.getType() == SortField.Type.SCORE) {
-              getScores = true;
-              break;
+          
+          if (useWrappingCollector) {
+            if (doAllGroups) {
+              cCache = CachingCollector.create(c1, true, maxCacheMB);
+              c = MultiCollector.wrap(cCache, allGroupsCollector);
+            } else {
+              c = cCache = CachingCollector.create(c1, true, maxCacheMB);
             }
+          } else {
+            // Collect only into cache, then replay multiple times:
+            c = cCache = CachingCollector.create(false, true, maxCacheMB);
           }
-
-          final int topNGroups = TestUtil.nextInt(random(), 1, 30);
-          //final int topNGroups = 10;
-          final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);
-
-          final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);
-          //final int groupOffset = 0;
-
-          final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);
-          //final int docOffset = 0;
-
-          final boolean doCache = random().nextBoolean();
-          final boolean doAllGroups = random().nextBoolean();
-          if (VERBOSE) {
-            System.out.println("TEST: groupSort=" + groupSort + " docSort=" + docSort + " searchTerm=" + searchTerm + " dF=" + r.docFreq(new Term("content", searchTerm))  +" dFBlock=" + rBlocks.docFreq(new Term("content", searchTerm)) + " topNGroups=" + topNGroups + " groupOffset=" + groupOffset + " docOffset=" + docOffset + " doCache=" + doCache + " docsPerGroup=" + docsPerGroup + " doAllGroups=" + doAllGroups + " getScores=" + getScores + " getMaxScores=" + getMaxScores);
-          }
-
-          String groupField = "group";
-          if (canUseIDV && random().nextBoolean()) {
-            groupField += "_dv";
-          }
-          if (VERBOSE) {
-            System.out.println("  groupField=" + groupField);
-          }
-          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);
-          final CachingCollector cCache;
-          final Collector c;
-
-          final AbstractAllGroupsCollector<?> allGroupsCollector;
+        } else {
+          cCache = null;
           if (doAllGroups) {
-            allGroupsCollector = createAllGroupsCollector(c1, groupField);
+            c = MultiCollector.wrap(c1, allGroupsCollector);
           } else {
-            allGroupsCollector = null;
+            c = c1;
           }
-
-          final boolean useWrappingCollector = random().nextBoolean();
-
-          if (doCache) {
-            final double maxCacheMB = random().nextDouble();
-            if (VERBOSE) {
-              System.out.println("TEST: maxCacheMB=" + maxCacheMB);
-            }
-
-            if (useWrappingCollector) {
-              if (doAllGroups) {
-                cCache = CachingCollector.create(c1, true, maxCacheMB);
-                c = MultiCollector.wrap(cCache, allGroupsCollector);
-              } else {
-                c = cCache = CachingCollector.create(c1, true, maxCacheMB);
-              }
-            } else {
-              // Collect only into cache, then replay multiple times:
-              c = cCache = CachingCollector.create(false, true, maxCacheMB);
+        }
+        
+        // Search top reader:
+        final Query query = new TermQuery(new Term("content", searchTerm));
+        
+        s.search(query, c);
+        
+        if (doCache && !useWrappingCollector) {
+          if (cCache.isCached()) {
+            // Replay for first-pass grouping
+            cCache.replay(c1);
+            if (doAllGroups) {
+              // Replay for all groups:
+              cCache.replay(allGroupsCollector);
             }
           } else {
-            cCache = null;
+            // Replay by re-running search:
+            s.search(query, c1);
             if (doAllGroups) {
-              c = MultiCollector.wrap(c1, allGroupsCollector);
-            } else {
-              c = c1;
+              s.search(query, allGroupsCollector);
             }
           }
-
-          // Search top reader:
-          final Query query = new TermQuery(new Term("content", searchTerm));
-
-          s.search(query, c);
-
-          if (doCache && !useWrappingCollector) {
-            if (cCache.isCached()) {
-              // Replay for first-pass grouping
-              cCache.replay(c1);
-              if (doAllGroups) {
-                // Replay for all groups:
-                cCache.replay(allGroupsCollector);
-              }
-            } else {
-              // Replay by re-running search:
-              s.search(query, c1);
-              if (doAllGroups) {
-                s.search(query, allGroupsCollector);
-              }
+        }
+        
+        // Get 1st pass top groups
+        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);
+        final TopGroups<BytesRef> groupsResult;
+        if (VERBOSE) {
+          System.out.println("TEST: first pass topGroups");
+          if (topGroups == null) {
+            System.out.println("  null");
+          } else {
+            for (SearchGroup<BytesRef> searchGroup : topGroups) {
+              System.out.println("  " + (searchGroup.groupValue == null ? "null" : searchGroup.groupValue) + ": " + Arrays.deepToString(searchGroup.sortValues));
             }
           }
-
-          // Get 1st pass top groups
-          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);
-          final TopGroups<BytesRef> groupsResult;
+        }
+        
+        // Get 1st pass top groups using shards
+        
+        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,
+            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);
+        final AbstractSecondPassGroupingCollector<?> c2;
+        if (topGroups != null) {
+          
           if (VERBOSE) {
-            System.out.println("TEST: first pass topGroups");
-            if (topGroups == null) {
-              System.out.println("  null");
-            } else {
-              for (SearchGroup<BytesRef> searchGroup : topGroups) {
-                System.out.println("  " + (searchGroup.groupValue == null ? "null" : searchGroup.groupValue) + ": " + Arrays.deepToString(searchGroup.sortValues));
-              }
+            System.out.println("TEST: topGroups");
+            for (SearchGroup<BytesRef> searchGroup : topGroups) {
+              System.out.println("  " + (searchGroup.groupValue == null ? "null" : searchGroup.groupValue.utf8ToString()) + ": " + Arrays.deepToString(searchGroup.sortValues));
             }
           }
-
-          // Get 1st pass top groups using shards
-
-          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);
-          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,
-              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);
-          final AbstractSecondPassGroupingCollector<?> c2;
-          if (topGroups != null) {
-
-            if (VERBOSE) {
-              System.out.println("TEST: topGroups");
-              for (SearchGroup<BytesRef> searchGroup : topGroups) {
-                System.out.println("  " + (searchGroup.groupValue == null ? "null" : searchGroup.groupValue.utf8ToString()) + ": " + Arrays.deepToString(searchGroup.sortValues));
-              }
-            }
-
-            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);
-            if (doCache) {
-              if (cCache.isCached()) {
-                if (VERBOSE) {
-                  System.out.println("TEST: cache is intact");
-                }
-                cCache.replay(c2);
-              } else {
-                if (VERBOSE) {
-                  System.out.println("TEST: cache was too large");
-                }
-                s.search(query, c2);
+          
+          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);
+          if (doCache) {
+            if (cCache.isCached()) {
+              if (VERBOSE) {
+                System.out.println("TEST: cache is intact");
               }
+              cCache.replay(c2);
             } else {
+              if (VERBOSE) {
+                System.out.println("TEST: cache was too large");
+              }
               s.search(query, c2);
             }
-
-            if (doAllGroups) {
-              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);
-              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());
-            } else {
-              groupsResult = getTopGroups(c2, docOffset);
-            }
           } else {
-            c2 = null;
-            groupsResult = null;
-            if (VERBOSE) {
-              System.out.println("TEST:   no results");
-            }
+            s.search(query, c2);
           }
-
-          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);
-
+          
+          if (doAllGroups) {
+            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);
+            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());
+          } else {
+            groupsResult = getTopGroups(c2, docOffset);
+          }
+        } else {
+          c2 = null;
+          groupsResult = null;
           if (VERBOSE) {
-            if (expectedGroups == null) {
-              System.out.println("TEST: no expected groups");
-            } else {
-              System.out.println("TEST: expected groups totalGroupedHitCount=" + expectedGroups.totalGroupedHitCount);
-              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {
-                System.out.println("  group=" + (gd.groupValue == null ? "null" : gd.groupValue) + " totalHits=" + gd.totalHits + " scoreDocs.len=" + gd.scoreDocs.length);
-                for(ScoreDoc sd : gd.scoreDocs) {
-                  System.out.println("    id=" + sd.doc + " score=" + sd.score);
-                }
+            System.out.println("TEST:   no results");
+          }
+        }
+        
+        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);
+        
+        if (VERBOSE) {
+          if (expectedGroups == null) {
+            System.out.println("TEST: no expected groups");
+          } else {
+            System.out.println("TEST: expected groups totalGroupedHitCount=" + expectedGroups.totalGroupedHitCount);
+            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {
+              System.out.println("  group=" + (gd.groupValue == null ? "null" : gd.groupValue) + " totalHits=" + gd.totalHits + " scoreDocs.len=" + gd.scoreDocs.length);
+              for(ScoreDoc sd : gd.scoreDocs) {
+                System.out.println("    id=" + sd.doc + " score=" + sd.score);
               }
             }
-
-            if (groupsResult == null) {
-              System.out.println("TEST: no matched groups");
-            } else {
-              System.out.println("TEST: matched groups totalGroupedHitCount=" + groupsResult.totalGroupedHitCount);
-              for(GroupDocs<BytesRef> gd : groupsResult.groups) {
-                System.out.println("  group=" + (gd.groupValue == null ? "null" : gd.groupValue) + " totalHits=" + gd.totalHits);
-                for(ScoreDoc sd : gd.scoreDocs) {
-                  System.out.println("    id=" + docIDToID.get(sd.doc) + " score=" + sd.score);
-                }
-              }
-
-              if (searchIter == 14) {
-                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {
-                  System.out.println("ID=" + docIDToID.get(docIDX) + " explain=" + s.explain(query, docIDX));
-                }
+          }
+          
+          if (groupsResult == null) {
+            System.out.println("TEST: no matched groups");
+          } else {
+            System.out.println("TEST: matched groups totalGroupedHitCount=" + groupsResult.totalGroupedHitCount);
+            for(GroupDocs<BytesRef> gd : groupsResult.groups) {
+              System.out.println("  group=" + (gd.groupValue == null ? "null" : gd.groupValue) + " totalHits=" + gd.totalHits);
+              for(ScoreDoc sd : gd.scoreDocs) {
+                System.out.println("    id=" + docIDToID.get(sd.doc) + " score=" + sd.score);
               }
             }
-
-            if (topGroupsShards == null) {
-              System.out.println("TEST: no matched-merged groups");
-            } else {
-              System.out.println("TEST: matched-merged groups totalGroupedHitCount=" + topGroupsShards.totalGroupedHitCount);
-              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {
-                System.out.println("  group=" + (gd.groupValue == null ? "null" : gd.groupValue) + " totalHits=" + gd.totalHits);
-                for(ScoreDoc sd : gd.scoreDocs) {
-                  System.out.println("    id=" + docIDToID.get(sd.doc) + " score=" + sd.score);
-                }
+            
+            if (searchIter == 14) {
+              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {
+                System.out.println("ID=" + docIDToID.get(docIDX) + " explain=" + s.explain(query, docIDX));
               }
             }
           }
-
-          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith("_dv"));
-
-          // Confirm merged shards match:
-          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);
-          if (topGroupsShards != null) {
-            verifyShards(shards.docStarts, topGroupsShards);
-          }
-
-          final boolean needsScores = getScores || getMaxScores || docSort == null;
-          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);
-          final TermAllGroupsCollector allGroupsCollector2;
-          final Collector c4;
-          if (doAllGroups) {
-            // NOTE: must be "group" and not "group_dv"
-            // (groupField) because we didn't index doc
-            // values in the block index:
-            allGroupsCollector2 = new TermAllGroupsCollector("group");
-            c4 = MultiCollector.wrap(c3, allGroupsCollector2);
+          
+          if (topGroupsShards == null) {
+            System.out.println("TEST: no matched-merged groups");
           } else {
-            allGroupsCollector2 = null;
-            c4 = c3;
+            System.out.println("TEST: matched-merged groups totalGroupedHitCount=" + topGroupsShards.totalGroupedHitCount);
+            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {
+              System.out.println("  group=" + (gd.groupValue == null ? "null" : gd.groupValue) + " totalHits=" + gd.totalHits);
+              for(ScoreDoc sd : gd.scoreDocs) {
+                System.out.println("    id=" + docIDToID.get(sd.doc) + " score=" + sd.score);
+              }
+            }
           }
-          // Get block grouping result:
-          sBlocks.search(query, c4);
-          @SuppressWarnings({"unchecked","rawtypes"})
-          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);
-          final TopGroups<BytesRef> groupsResultBlocks;
-          if (doAllGroups && tempTopGroupsBlocks != null) {
-            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());
-            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());
+        }
+        
+        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);
+        
+        // Confirm merged shards match:
+        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);
+        if (topGroupsShards != null) {
+          verifyShards(shards.docStarts, topGroupsShards);
+        }
+        
+        final boolean needsScores = getScores || getMaxScores || docSort == null;
+        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);
+        final TermAllGroupsCollector allGroupsCollector2;
+        final Collector c4;
+        if (doAllGroups) {
+          // NOTE: must be "group" and not "group_dv"
+          // (groupField) because we didn't index doc
+          // values in the block index:
+          allGroupsCollector2 = new TermAllGroupsCollector("group");
+          c4 = MultiCollector.wrap(c3, allGroupsCollector2);
+        } else {
+          allGroupsCollector2 = null;
+          c4 = c3;
+        }
+        // Get block grouping result:
+        sBlocks.search(query, c4);
+        @SuppressWarnings({"unchecked","rawtypes"})
+        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);
+        final TopGroups<BytesRef> groupsResultBlocks;
+        if (doAllGroups && tempTopGroupsBlocks != null) {
+          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());
+          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());
+        } else {
+          groupsResultBlocks = tempTopGroupsBlocks;
+        }
+        
+        if (VERBOSE) {
+          if (groupsResultBlocks == null) {
+            System.out.println("TEST: no block groups");
           } else {
-            groupsResultBlocks = tempTopGroupsBlocks;
-          }
-
-          if (VERBOSE) {
-            if (groupsResultBlocks == null) {
-              System.out.println("TEST: no block groups");
-            } else {
-              System.out.println("TEST: block groups totalGroupedHitCount=" + groupsResultBlocks.totalGroupedHitCount);
-              boolean first = true;
-              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {
-                System.out.println("  group=" + (gd.groupValue == null ? "null" : gd.groupValue.utf8ToString()) + " totalHits=" + gd.totalHits);
-                for(ScoreDoc sd : gd.scoreDocs) {
-                  System.out.println("    id=" + docIDToIDBlocks.get(sd.doc) + " score=" + sd.score);
-                  if (first) {
-                    System.out.println("explain: " + sBlocks.explain(query, sd.doc));
-                    first = false;
-                  }
+            System.out.println("TEST: block groups totalGroupedHitCount=" + groupsResultBlocks.totalGroupedHitCount);
+            boolean first = true;
+            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {
+              System.out.println("  group=" + (gd.groupValue == null ? "null" : gd.groupValue.utf8ToString()) + " totalHits=" + gd.totalHits);
+              for(ScoreDoc sd : gd.scoreDocs) {
+                System.out.println("    id=" + docIDToIDBlocks.get(sd.doc) + " score=" + sd.score);
+                if (first) {
+                  System.out.println("explain: " + sBlocks.explain(query, sd.doc));
+                  first = false;
                 }
               }
             }
           }
-
-          // Get shard'd block grouping result:
-          // Block index does not index DocValues so we pass
-          // false for canUseIDV:
-          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,
-              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(false));
-
-          if (expectedGroups != null) {
-            // Fixup scores for reader2
-            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {
-              for(ScoreDoc hit : groupDocsHits.scoreDocs) {
-                final GroupDoc gd = groupDocsByID[hit.doc];
-                assertEquals(gd.id, hit.doc);
-                //System.out.println("fixup score " + hit.score + " to " + gd.score2 + " vs " + gd.score);
-                hit.score = gd.score2;
-              }
+        }
+        
+        // Get shard'd block grouping result:
+        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,
+            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);
+        
+        if (expectedGroups != null) {
+          // Fixup scores for reader2
+          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {
+            for(ScoreDoc hit : groupDocsHits.scoreDocs) {
+              final GroupDoc gd = groupDocsByID[hit.doc];
+              assertEquals(gd.id, hit.doc);
+              //System.out.println("fixup score " + hit.score + " to " + gd.score2 + " vs " + gd.score);
+              hit.score = gd.score2;
             }
-
-            final SortField[] sortFields = groupSort.getSort();
-            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);
-            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {
-              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {
-                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {
-                  if (groupDocsHits.groupSortValues != null) {
-                    //System.out.println("remap " + groupDocsHits.groupSortValues[groupSortIDX] + " to " + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));
-                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);
-                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);
-                  }
+          }
+          
+          final SortField[] sortFields = groupSort.getSort();
+          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);
+          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {
+            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {
+              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {
+                if (groupDocsHits.groupSortValues != null) {
+                  //System.out.println("remap " + groupDocsHits.groupSortValues[groupSortIDX] + " to " + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));
+                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);
+                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);
                 }
               }
             }
-
-            final SortField[] docSortFields = docSort.getSort();
-            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {
-              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {
-                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {
-                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {
-                    FieldDoc hit = (FieldDoc) _hit;
-                    if (hit.fields != null) {
-                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);
-                      assertNotNull(hit.fields[docSortIDX]);
-                    }
+          }
+          
+          final SortField[] docSortFields = docSort.getSort();
+          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {
+            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {
+              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {
+                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {
+                  FieldDoc hit = (FieldDoc) _hit;
+                  if (hit.fields != null) {
+                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);
+                    assertNotNull(hit.fields[docSortIDX]);
                   }
                 }
               }
             }
           }
-
-          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);
-          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);
-        }
-      } finally {
-        QueryUtils.purgeFieldCache(r);
-        if (rBlocks != null) {
-          QueryUtils.purgeFieldCache(rBlocks);
         }
+        
+        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);
+        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);
       }
-
+      
       r.close();
       dir.close();
-
+      
       rBlocks.close();
       dirBlocks.close();
     }
@@ -1158,7 +1142,7 @@
   }
 
   private TopGroups<BytesRef> searchShards(IndexSearcher topSearcher, ShardSearcher[] subSearchers, Query query, Sort groupSort, Sort docSort, int groupOffset, int topNGroups, int docOffset,
-                                           int topNDocs, boolean getScores, boolean getMaxScores, boolean canUseIDV, boolean preFlex, ValueHolder<Boolean> usedIdvBasedImpl) throws Exception {
+                                           int topNDocs, boolean getScores, boolean getMaxScores, boolean canUseIDV, boolean preFlex) throws Exception {
 
     // TODO: swap in caching, all groups collector hereassertEquals(expected.totalHitCount, actual.totalHitCount);
     // too...
@@ -1182,10 +1166,6 @@
     }
 
     String groupField = "group";
-    if (shardsCanUseIDV && random().nextBoolean()) {
-      groupField += "_dv";
-      usedIdvBasedImpl.value = true;
-    }
 
     for(int shardIDX=0;shardIDX<subSearchers.length;shardIDX++) {
 
@@ -1257,7 +1237,7 @@
     }
   }
 
-  private void assertEquals(FieldCache.Ints docIDtoID, TopGroups<BytesRef> expected, TopGroups<BytesRef> actual, boolean verifyGroupValues, boolean verifyTotalGroupCount, boolean verifySortValues, boolean testScores, boolean idvBasedImplsUsed) {
+  private void assertEquals(NumericDocValues docIDtoID, TopGroups<BytesRef> expected, TopGroups<BytesRef> actual, boolean verifyGroupValues, boolean verifyTotalGroupCount, boolean verifySortValues, boolean testScores, boolean idvBasedImplsUsed) {
     if (expected == null) {
       assertNull(actual);
       return;


diff -ruN -x .svn -x build lucene-trunk/lucene/join/src/java/org/apache/lucene/search/join/TermsCollector.java lucene5666/lucene/join/src/java/org/apache/lucene/search/join/TermsCollector.java
--- lucene-trunk/lucene/join/src/java/org/apache/lucene/search/join/TermsCollector.java	2014-05-14 03:47:28.806646624 -0400
+++ lucene5666/lucene/join/src/java/org/apache/lucene/search/join/TermsCollector.java	2014-05-12 13:28:40.608244680 -0400
@@ -21,10 +21,10 @@
 
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedSetDocValues;
 import org.apache.lucene.search.LeafCollector;
 import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.SimpleCollector;
 import org.apache.lucene.util.BytesRef;
@@ -85,7 +85,7 @@
 
     @Override
     protected void doSetNextReader(AtomicReaderContext context) throws IOException {
-      docTermOrds = FieldCache.DEFAULT.getDocTermOrds(context.reader(), field);
+      docTermOrds = DocValues.getSortedSet(context.reader(), field);
     }
   }
 
@@ -107,7 +107,7 @@
 
     @Override
     protected void doSetNextReader(AtomicReaderContext context) throws IOException {
-      fromDocTerms = FieldCache.DEFAULT.getTerms(context.reader(), field, false);
+      fromDocTerms = DocValues.getBinary(context.reader(), field);
     }
   }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/join/src/java/org/apache/lucene/search/join/TermsWithScoreCollector.java lucene5666/lucene/join/src/java/org/apache/lucene/search/join/TermsWithScoreCollector.java
--- lucene-trunk/lucene/join/src/java/org/apache/lucene/search/join/TermsWithScoreCollector.java	2014-05-14 03:47:28.806646624 -0400
+++ lucene5666/lucene/join/src/java/org/apache/lucene/search/join/TermsWithScoreCollector.java	2014-05-12 13:28:40.608244680 -0400
@@ -21,10 +21,10 @@
 
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedSetDocValues;
 import org.apache.lucene.search.LeafCollector;
 import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.SimpleCollector;
 import org.apache.lucene.util.ArrayUtil;
@@ -131,7 +131,7 @@
 
     @Override
     protected void doSetNextReader(AtomicReaderContext context) throws IOException {
-      fromDocTerms = FieldCache.DEFAULT.getTerms(context.reader(), field, false);
+      fromDocTerms = DocValues.getBinary(context.reader(), field);
     }
 
     static class Avg extends SV {
@@ -217,7 +217,7 @@
 
     @Override
     protected void doSetNextReader(AtomicReaderContext context) throws IOException {
-      fromDocTermOrds = FieldCache.DEFAULT.getDocTermOrds(context.reader(), field);
+      fromDocTermOrds = DocValues.getSortedSet(context.reader(), field);
     }
 
     static class Avg extends MV {


diff -ruN -x .svn -x build lucene-trunk/lucene/join/src/test/org/apache/lucene/search/join/TestBlockJoinSorting.java lucene5666/lucene/join/src/test/org/apache/lucene/search/join/TestBlockJoinSorting.java
--- lucene-trunk/lucene/join/src/test/org/apache/lucene/search/join/TestBlockJoinSorting.java	2014-05-14 03:47:28.802646624 -0400
+++ lucene5666/lucene/join/src/test/org/apache/lucene/search/join/TestBlockJoinSorting.java	2014-05-12 13:28:40.608244680 -0400
@@ -20,6 +20,7 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.NoMergePolicy;
@@ -58,14 +59,17 @@
     List<Document> docs = new ArrayList<>();
     Document document = new Document();
     document.add(new StringField("field2", "a", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("a")));
     document.add(new StringField("filter_1", "T", Field.Store.NO));
     docs.add(document);
     document = new Document();
     document.add(new StringField("field2", "b", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("b")));
     document.add(new StringField("filter_1", "T", Field.Store.NO));
     docs.add(document);
     document = new Document();
     document.add(new StringField("field2", "c", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("c")));
     document.add(new StringField("filter_1", "T", Field.Store.NO));
     docs.add(document);
     document = new Document();
@@ -78,14 +82,17 @@
     docs.clear();
     document = new Document();
     document.add(new StringField("field2", "c", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("c")));
     document.add(new StringField("filter_1", "T", Field.Store.NO));
     docs.add(document);
     document = new Document();
     document.add(new StringField("field2", "d", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("d")));
     document.add(new StringField("filter_1", "T", Field.Store.NO));
     docs.add(document);
     document = new Document();
     document.add(new StringField("field2", "e", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("e")));
     document.add(new StringField("filter_1", "T", Field.Store.NO));
     docs.add(document);
     document = new Document();
@@ -97,14 +104,17 @@
     docs.clear();
     document = new Document();
     document.add(new StringField("field2", "e", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("e")));
     document.add(new StringField("filter_1", "T", Field.Store.NO));
     docs.add(document);
     document = new Document();
     document.add(new StringField("field2", "f", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("f")));
     document.add(new StringField("filter_1", "T", Field.Store.NO));
     docs.add(document);
     document = new Document();
     document.add(new StringField("field2", "g", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("g")));
     document.add(new StringField("filter_1", "T", Field.Store.NO));
     docs.add(document);
     document = new Document();
@@ -116,14 +126,17 @@
     docs.clear();
     document = new Document();
     document.add(new StringField("field2", "g", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("g")));
     document.add(new StringField("filter_1", "T", Field.Store.NO));
     docs.add(document);
     document = new Document();
     document.add(new StringField("field2", "h", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("h")));
     document.add(new StringField("filter_1", "F", Field.Store.NO));
     docs.add(document);
     document = new Document();
     document.add(new StringField("field2", "i", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("i")));
     document.add(new StringField("filter_1", "F", Field.Store.NO));
     docs.add(document);
     document = new Document();
@@ -136,14 +149,17 @@
     docs.clear();
     document = new Document();
     document.add(new StringField("field2", "i", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("i")));
     document.add(new StringField("filter_1", "F", Field.Store.NO));
     docs.add(document);
     document = new Document();
     document.add(new StringField("field2", "j", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("j")));
     document.add(new StringField("filter_1", "F", Field.Store.NO));
     docs.add(document);
     document = new Document();
     document.add(new StringField("field2", "k", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("k")));
     document.add(new StringField("filter_1", "F", Field.Store.NO));
     docs.add(document);
     document = new Document();
@@ -155,14 +171,17 @@
     docs.clear();
     document = new Document();
     document.add(new StringField("field2", "k", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("k")));
     document.add(new StringField("filter_1", "T", Field.Store.NO));
     docs.add(document);
     document = new Document();
     document.add(new StringField("field2", "l", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("l")));
     document.add(new StringField("filter_1", "T", Field.Store.NO));
     docs.add(document);
     document = new Document();
     document.add(new StringField("field2", "m", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("m")));
     document.add(new StringField("filter_1", "T", Field.Store.NO));
     docs.add(document);
     document = new Document();
@@ -180,14 +199,17 @@
     docs.clear();
     document = new Document();
     document.add(new StringField("field2", "m", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("m")));
     document.add(new StringField("filter_1", "T", Field.Store.NO));
     docs.add(document);
     document = new Document();
     document.add(new StringField("field2", "n", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("n")));
     document.add(new StringField("filter_1", "F", Field.Store.NO));
     docs.add(document);
     document = new Document();
     document.add(new StringField("field2", "o", Field.Store.NO));
+    document.add(new SortedDocValuesField("field2", new BytesRef("o")));
     document.add(new StringField("filter_1", "F", Field.Store.NO));
     docs.add(document);
     document = new Document();


diff -ruN -x .svn -x build lucene-trunk/lucene/join/src/test/org/apache/lucene/search/join/TestJoinUtil.java lucene5666/lucene/join/src/test/org/apache/lucene/search/join/TestJoinUtil.java
--- lucene-trunk/lucene/join/src/test/org/apache/lucene/search/join/TestJoinUtil.java	2014-05-14 03:47:28.802646624 -0400
+++ lucene5666/lucene/join/src/test/org/apache/lucene/search/join/TestJoinUtil.java	2014-05-12 13:28:39.488244661 -0400
@@ -34,10 +34,13 @@
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.SortedDocValuesField;
+import org.apache.lucene.document.SortedSetDocValuesField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
@@ -53,7 +56,6 @@
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Explanation;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.Query;
@@ -68,9 +70,11 @@
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 import org.apache.lucene.util.TestUtil;
 import org.junit.Test;
 
+@SuppressCodecs({"Lucene40", "Lucene41", "Lucene42"}) // we need SortedSet, docsWithField
 public class TestJoinUtil extends LuceneTestCase {
 
   public void testSimple() throws Exception {
@@ -89,20 +93,25 @@
     doc.add(new TextField("description", "random text", Field.Store.NO));
     doc.add(new TextField("name", "name1", Field.Store.NO));
     doc.add(new TextField(idField, "1", Field.Store.NO));
+    doc.add(new SortedDocValuesField(idField, new BytesRef("1")));
     w.addDocument(doc);
 
     // 1
     doc = new Document();
     doc.add(new TextField("price", "10.0", Field.Store.NO));
     doc.add(new TextField(idField, "2", Field.Store.NO));
+    doc.add(new SortedDocValuesField(idField, new BytesRef("2")));
     doc.add(new TextField(toField, "1", Field.Store.NO));
+    doc.add(new SortedDocValuesField(toField, new BytesRef("1")));
     w.addDocument(doc);
 
     // 2
     doc = new Document();
     doc.add(new TextField("price", "20.0", Field.Store.NO));
     doc.add(new TextField(idField, "3", Field.Store.NO));
+    doc.add(new SortedDocValuesField(idField, new BytesRef("3")));
     doc.add(new TextField(toField, "1", Field.Store.NO));
+    doc.add(new SortedDocValuesField(toField, new BytesRef("1")));
     w.addDocument(doc);
 
     // 3
@@ -110,6 +119,7 @@
     doc.add(new TextField("description", "more random text", Field.Store.NO));
     doc.add(new TextField("name", "name2", Field.Store.NO));
     doc.add(new TextField(idField, "4", Field.Store.NO));
+    doc.add(new SortedDocValuesField(idField, new BytesRef("4")));
     w.addDocument(doc);
     w.commit();
 
@@ -117,14 +127,18 @@
     doc = new Document();
     doc.add(new TextField("price", "10.0", Field.Store.NO));
     doc.add(new TextField(idField, "5", Field.Store.NO));
+    doc.add(new SortedDocValuesField(idField, new BytesRef("5")));
     doc.add(new TextField(toField, "4", Field.Store.NO));
+    doc.add(new SortedDocValuesField(toField, new BytesRef("4")));
     w.addDocument(doc);
 
     // 5
     doc = new Document();
     doc.add(new TextField("price", "20.0", Field.Store.NO));
     doc.add(new TextField(idField, "6", Field.Store.NO));
+    doc.add(new SortedDocValuesField(idField, new BytesRef("6")));
     doc.add(new TextField(toField, "4", Field.Store.NO));
+    doc.add(new SortedDocValuesField(toField, new BytesRef("4")));
     w.addDocument(doc);
 
     IndexSearcher indexSearcher = new IndexSearcher(w.getReader());
@@ -180,16 +194,18 @@
     doc.add(new TextField("description", "random text", Field.Store.NO));
     doc.add(new TextField("name", "name1", Field.Store.NO));
     doc.add(new TextField(idField, "0", Field.Store.NO));
+    doc.add(new SortedDocValuesField(idField, new BytesRef("0")));
     w.addDocument(doc);
 
     doc = new Document();
     doc.add(new TextField("price", "10.0", Field.Store.NO));
-    for(int i=0;i<300;i++){
-      doc.add(new TextField(toField, ""+i, Field.Store.NO));
-      if(!multipleValues){
-        w.addDocument(doc);
-        doc.removeFields(toField);
+
+    if (multipleValues) {
+      for(int i=0;i<300;i++) {
+        doc.add(new SortedSetDocValuesField(toField, new BytesRef(""+i)));
       }
+    } else {
+      doc.add(new SortedDocValuesField(toField, new BytesRef("0")));
     }
     w.addDocument(doc);
 
@@ -317,20 +333,25 @@
     doc.add(new TextField("description", "A random movie", Field.Store.NO));
     doc.add(new TextField("name", "Movie 1", Field.Store.NO));
     doc.add(new TextField(idField, "1", Field.Store.NO));
+    doc.add(new SortedDocValuesField(idField, new BytesRef("1")));
     w.addDocument(doc);
 
     // 1
     doc = new Document();
     doc.add(new TextField("subtitle", "The first subtitle of this movie", Field.Store.NO));
     doc.add(new TextField(idField, "2", Field.Store.NO));
+    doc.add(new SortedDocValuesField(idField, new BytesRef("2")));
     doc.add(new TextField(toField, "1", Field.Store.NO));
+    doc.add(new SortedDocValuesField(toField, new BytesRef("1")));
     w.addDocument(doc);
 
     // 2
     doc = new Document();
     doc.add(new TextField("subtitle", "random subtitle; random event movie", Field.Store.NO));
     doc.add(new TextField(idField, "3", Field.Store.NO));
+    doc.add(new SortedDocValuesField(idField, new BytesRef("3")));
     doc.add(new TextField(toField, "1", Field.Store.NO));
+    doc.add(new SortedDocValuesField(toField, new BytesRef("1")));
     w.addDocument(doc);
 
     // 3
@@ -338,6 +359,7 @@
     doc.add(new TextField("description", "A second random movie", Field.Store.NO));
     doc.add(new TextField("name", "Movie 2", Field.Store.NO));
     doc.add(new TextField(idField, "4", Field.Store.NO));
+    doc.add(new SortedDocValuesField(idField, new BytesRef("4")));
     w.addDocument(doc);
     w.commit();
 
@@ -345,14 +367,18 @@
     doc = new Document();
     doc.add(new TextField("subtitle", "a very random event happened during christmas night", Field.Store.NO));
     doc.add(new TextField(idField, "5", Field.Store.NO));
+    doc.add(new SortedDocValuesField(idField, new BytesRef("5")));
     doc.add(new TextField(toField, "4", Field.Store.NO));
+    doc.add(new SortedDocValuesField(toField, new BytesRef("4")));
     w.addDocument(doc);
 
     // 5
     doc = new Document();
     doc.add(new TextField("subtitle", "movie end movie test 123 test 123 random", Field.Store.NO));
     doc.add(new TextField(idField, "6", Field.Store.NO));
+    doc.add(new SortedDocValuesField(idField, new BytesRef("6")));
     doc.add(new TextField(toField, "4", Field.Store.NO));
+    doc.add(new SortedDocValuesField(toField, new BytesRef("4")));
     w.addDocument(doc);
 
     IndexSearcher indexSearcher = new IndexSearcher(w.getReader());
@@ -572,6 +598,11 @@
           context.fromDocuments.get(linkValue).add(docs[i]);
           context.randomValueFromDocs.get(value).add(docs[i]);
           document.add(newTextField(random(), "from", linkValue, Field.Store.NO));
+          if (multipleValuesPerDocument) {
+            document.add(new SortedSetDocValuesField("from", new BytesRef(linkValue)));
+          } else {
+            document.add(new SortedDocValuesField("from", new BytesRef(linkValue)));
+          }
         } else {
           if (!context.toDocuments.containsKey(linkValue)) {
             context.toDocuments.put(linkValue, new ArrayList<RandomDoc>());
@@ -583,6 +614,11 @@
           context.toDocuments.get(linkValue).add(docs[i]);
           context.randomValueToDocs.get(value).add(docs[i]);
           document.add(newTextField(random(), "to", linkValue, Field.Store.NO));
+          if (multipleValuesPerDocument) {
+            document.add(new SortedSetDocValuesField("to", new BytesRef(linkValue)));
+          } else {
+            document.add(new SortedDocValuesField("to", new BytesRef(linkValue)));
+          }
         }
       }
 
@@ -644,7 +680,7 @@
 
           @Override
           protected void doSetNextReader(AtomicReaderContext context) throws IOException {
-            docTermOrds = FieldCache.DEFAULT.getDocTermOrds(context.reader(), fromField);
+            docTermOrds = DocValues.getSortedSet(context.reader(), fromField);
           }
 
           @Override
@@ -682,8 +718,8 @@
 
           @Override
           protected void doSetNextReader(AtomicReaderContext context) throws IOException {
-            terms = FieldCache.DEFAULT.getTerms(context.reader(), fromField, true);
-            docsWithField = FieldCache.DEFAULT.getDocsWithField(context.reader(), fromField);
+            terms = DocValues.getBinary(context.reader(), fromField);
+            docsWithField = DocValues.getDocsWithField(context.reader(), fromField);
           }
 
           @Override
@@ -753,7 +789,7 @@
             @Override
             protected void doSetNextReader(AtomicReaderContext context) throws IOException {
               docBase = context.docBase;
-              docTermOrds = FieldCache.DEFAULT.getDocTermOrds(context.reader(), toField);
+              docTermOrds = DocValues.getSortedSet(context.reader(), toField);
             }
 
             @Override
@@ -781,7 +817,7 @@
 
           @Override
           protected void doSetNextReader(AtomicReaderContext context) throws IOException {
-            terms = FieldCache.DEFAULT.getTerms(context.reader(), toField, false);
+            terms = DocValues.getBinary(context.reader(), toField);
             docBase = context.docBase;
           }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/build.xml lucene5666/lucene/misc/build.xml
--- lucene-trunk/lucene/misc/build.xml	2014-05-14 03:47:28.846646625 -0400
+++ lucene5666/lucene/misc/build.xml	2014-05-14 03:45:16.698644324 -0400
@@ -32,6 +32,10 @@
     org/apache/lucene/misc/IndexMergeTool.class
   "/>
 
+  <property name="forbidden-rue-excludes" value="
+    org/apache/lucene/uninverting/FieldCache$CacheEntry.class
+  "/>
+
   <import file="../module-build.xml"/>
 
   <target name="install-cpptasks" unless="cpptasks.uptodate" depends="ivy-availability-check,ivy-fail,ivy-configure">


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/java/org/apache/lucene/uninverting/DocTermOrds.java lucene5666/lucene/misc/src/java/org/apache/lucene/uninverting/DocTermOrds.java
--- lucene-trunk/lucene/misc/src/java/org/apache/lucene/uninverting/DocTermOrds.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/misc/src/java/org/apache/lucene/uninverting/DocTermOrds.java	2014-05-12 13:48:00.048264871 -0400
@@ -0,0 +1,916 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.uninverting;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+import org.apache.lucene.codecs.PostingsFormat; // javadocs
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.TermsEnum.SeekStatus;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.StringHelper;
+
+/**
+ * This class enables fast access to multiple term ords for
+ * a specified field across all docIDs.
+ *
+ * Like FieldCache, it uninverts the index and holds a
+ * packed data structure in RAM to enable fast access.
+ * Unlike FieldCache, it can handle multi-valued fields,
+ * and, it does not hold the term bytes in RAM.  Rather, you
+ * must obtain a TermsEnum from the {@link #getOrdTermsEnum}
+ * method, and then seek-by-ord to get the term's bytes.
+ *
+ * While normally term ords are type long, in this API they are
+ * int as the internal representation here cannot address
+ * more than MAX_INT unique terms.  Also, typically this
+ * class is used on fields with relatively few unique terms
+ * vs the number of documents.  In addition, there is an
+ * internal limit (16 MB) on how many bytes each chunk of
+ * documents may consume.  If you trip this limit you'll hit
+ * an IllegalStateException.
+ *
+ * Deleted documents are skipped during uninversion, and if
+ * you look them up you'll get 0 ords.
+ *
+ * The returned per-document ords do not retain their
+ * original order in the document.  Instead they are returned
+ * in sorted (by ord, ie term's BytesRef comparator) order.  They
+ * are also de-dup'd (ie if doc has same term more than once
+ * in this field, you'll only get that ord back once).
+ *
+ * This class tests whether the provided reader is able to
+ * retrieve terms by ord (ie, it's single segment, and it
+ * uses an ord-capable terms index).  If not, this class
+ * will create its own term index internally, allowing to
+ * create a wrapped TermsEnum that can handle ord.  The
+ * {@link #getOrdTermsEnum} method then provides this
+ * wrapped enum, if necessary.
+ *
+ * The RAM consumption of this class can be high!
+ *
+ * @lucene.experimental
+ */
+
+/*
+ * Final form of the un-inverted field:
+ *   Each document points to a list of term numbers that are contained in that document.
+ *
+ *   Term numbers are in sorted order, and are encoded as variable-length deltas from the
+ *   previous term number.  Real term numbers start at 2 since 0 and 1 are reserved.  A
+ *   term number of 0 signals the end of the termNumber list.
+ *
+ *   There is a single int[maxDoc()] which either contains a pointer into a byte[] for
+ *   the termNumber lists, or directly contains the termNumber list if it fits in the 4
+ *   bytes of an integer.  If the first byte in the integer is 1, the next 3 bytes
+ *   are a pointer into a byte[] where the termNumber list starts.
+ *
+ *   There are actually 256 byte arrays, to compensate for the fact that the pointers
+ *   into the byte arrays are only 3 bytes long.  The correct byte array for a document
+ *   is a function of it's id.
+ *
+ *   To save space and speed up faceting, any term that matches enough documents will
+ *   not be un-inverted... it will be skipped while building the un-inverted field structure,
+ *   and will use a set intersection method during faceting.
+ *
+ *   To further save memory, the terms (the actual string values) are not all stored in
+ *   memory, but a TermIndex is used to convert term numbers to term values only
+ *   for the terms needed after faceting has completed.  Only every 128th term value
+ *   is stored, along with it's corresponding term number, and this is used as an
+ *   index to find the closest term and iterate until the desired number is hit (very
+ *   much like Lucene's own internal term index).
+ *
+ */
+
+public class DocTermOrds {
+
+  // Term ords are shifted by this, internally, to reserve
+  // values 0 (end term) and 1 (index is a pointer into byte array)
+  private final static int TNUM_OFFSET = 2;
+
+  /** Every 128th term is indexed, by default. */
+  public final static int DEFAULT_INDEX_INTERVAL_BITS = 7; // decrease to a low number like 2 for testing
+
+  private int indexIntervalBits;
+  private int indexIntervalMask;
+  private int indexInterval;
+
+  /** Don't uninvert terms that exceed this count. */
+  protected final int maxTermDocFreq;
+
+  /** Field we are uninverting. */
+  protected final String field;
+
+  /** Number of terms in the field. */
+  protected int numTermsInField;
+
+  /** Total number of references to term numbers. */
+  protected long termInstances;
+  private long memsz;
+
+  /** Total time to uninvert the field. */
+  protected int total_time;
+
+  /** Time for phase1 of the uninvert process. */
+  protected int phase1_time;
+
+  /** Holds the per-document ords or a pointer to the ords. */
+  protected int[] index;
+
+  /** Holds term ords for documents. */
+  protected byte[][] tnums = new byte[256][];
+
+  /** Total bytes (sum of term lengths) for all indexed terms.*/
+  protected long sizeOfIndexedStrings;
+
+  /** Holds the indexed (by default every 128th) terms. */
+  protected BytesRef[] indexedTermsArray;
+
+  /** If non-null, only terms matching this prefix were
+   *  indexed. */
+  protected BytesRef prefix;
+
+  /** Ordinal of the first term in the field, or 0 if the
+   *  {@link PostingsFormat} does not implement {@link
+   *  TermsEnum#ord}. */
+  protected int ordBase;
+
+  /** Used while uninverting. */
+  protected DocsEnum docsEnum;
+
+  /** Returns total bytes used. */
+  public long ramUsedInBytes() {
+    // can cache the mem size since it shouldn't change
+    if (memsz!=0) return memsz;
+    long sz = 8*8 + 32; // local fields
+    if (index != null) sz += index.length * 4;
+    if (tnums!=null) {
+      for (byte[] arr : tnums)
+        if (arr != null) sz += arr.length;
+    }
+    memsz = sz;
+    return sz;
+  }
+
+  /** Inverts all terms */
+  public DocTermOrds(AtomicReader reader, Bits liveDocs, String field) throws IOException {
+    this(reader, liveDocs, field, null, Integer.MAX_VALUE);
+  }
+  
+  // TODO: instead of all these ctors and options, take termsenum!
+
+  /** Inverts only terms starting w/ prefix */
+  public DocTermOrds(AtomicReader reader, Bits liveDocs, String field, BytesRef termPrefix) throws IOException {
+    this(reader, liveDocs, field, termPrefix, Integer.MAX_VALUE);
+  }
+
+  /** Inverts only terms starting w/ prefix, and only terms
+   *  whose docFreq (not taking deletions into account) is
+   *  <=  maxTermDocFreq */
+  public DocTermOrds(AtomicReader reader, Bits liveDocs, String field, BytesRef termPrefix, int maxTermDocFreq) throws IOException {
+    this(reader, liveDocs, field, termPrefix, maxTermDocFreq, DEFAULT_INDEX_INTERVAL_BITS);
+  }
+
+  /** Inverts only terms starting w/ prefix, and only terms
+   *  whose docFreq (not taking deletions into account) is
+   *  <=  maxTermDocFreq, with a custom indexing interval
+   *  (default is every 128nd term). */
+  public DocTermOrds(AtomicReader reader, Bits liveDocs, String field, BytesRef termPrefix, int maxTermDocFreq, int indexIntervalBits) throws IOException {
+    this(field, maxTermDocFreq, indexIntervalBits);
+    uninvert(reader, liveDocs, termPrefix);
+  }
+
+  /** Subclass inits w/ this, but be sure you then call
+   *  uninvert, only once */
+  protected DocTermOrds(String field, int maxTermDocFreq, int indexIntervalBits) {
+    //System.out.println("DTO init field=" + field + " maxTDFreq=" + maxTermDocFreq);
+    this.field = field;
+    this.maxTermDocFreq = maxTermDocFreq;
+    this.indexIntervalBits = indexIntervalBits;
+    indexIntervalMask = 0xffffffff >>> (32-indexIntervalBits);
+    indexInterval = 1 << indexIntervalBits;
+  }
+
+  /** Returns a TermsEnum that implements ord.  If the
+   *  provided reader supports ord, we just return its
+   *  TermsEnum; if it does not, we build a "private" terms
+   *  index internally (WARNING: consumes RAM) and use that
+   *  index to implement ord.  This also enables ord on top
+   *  of a composite reader.  The returned TermsEnum is
+   *  unpositioned.  This returns null if there are no terms.
+   *
+   *  <p><b>NOTE</b>: you must pass the same reader that was
+   *  used when creating this class */
+  public TermsEnum getOrdTermsEnum(AtomicReader reader) throws IOException {
+    if (indexedTermsArray == null) {
+      //System.out.println("GET normal enum");
+      final Fields fields = reader.fields();
+      if (fields == null) {
+        return null;
+      }
+      final Terms terms = fields.terms(field);
+      if (terms == null) {
+        return null;
+      } else {
+        return terms.iterator(null);
+      }
+    } else {
+      //System.out.println("GET wrapped enum ordBase=" + ordBase);
+      return new OrdWrappedTermsEnum(reader);
+    }
+  }
+
+  /**
+   * Returns the number of terms in this field
+   */
+  public int numTerms() {
+    return numTermsInField;
+  }
+
+  /**
+   * Returns {@code true} if no terms were indexed.
+   */
+  public boolean isEmpty() {
+    return index == null;
+  }
+
+  /** Subclass can override this */
+  protected void visitTerm(TermsEnum te, int termNum) throws IOException {
+  }
+
+  /** Invoked during {@link #uninvert(AtomicReader,Bits,BytesRef)}
+   *  to record the document frequency for each uninverted
+   *  term. */
+  protected void setActualDocFreq(int termNum, int df) throws IOException {
+  }
+
+  /** Call this only once (if you subclass!) */
+  protected void uninvert(final AtomicReader reader, Bits liveDocs, final BytesRef termPrefix) throws IOException {
+    final FieldInfo info = reader.getFieldInfos().fieldInfo(field);
+    if (info != null && info.hasDocValues()) {
+      throw new IllegalStateException("Type mismatch: " + field + " was indexed as " + info.getDocValuesType());
+    }
+    //System.out.println("DTO uninvert field=" + field + " prefix=" + termPrefix);
+    final long startTime = System.currentTimeMillis();
+    prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);
+
+    final int maxDoc = reader.maxDoc();
+    final int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number
+    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document
+    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)
+
+    final Fields fields = reader.fields();
+    if (fields == null) {
+      // No terms
+      return;
+    }
+    final Terms terms = fields.terms(field);
+    if (terms == null) {
+      // No terms
+      return;
+    }
+
+    final TermsEnum te = terms.iterator(null);
+    final BytesRef seekStart = termPrefix != null ? termPrefix : new BytesRef();
+    //System.out.println("seekStart=" + seekStart.utf8ToString());
+    if (te.seekCeil(seekStart) == TermsEnum.SeekStatus.END) {
+      // No terms match
+      return;
+    }
+
+    // If we need our "term index wrapper", these will be
+    // init'd below:
+    List<BytesRef> indexedTerms = null;
+    PagedBytes indexedTermsBytes = null;
+
+    boolean testedOrd = false;
+
+    // we need a minimum of 9 bytes, but round up to 12 since the space would
+    // be wasted with most allocators anyway.
+    byte[] tempArr = new byte[12];
+
+    //
+    // enumerate all terms, and build an intermediate form of the un-inverted field.
+    //
+    // During this intermediate form, every document has a (potential) byte[]
+    // and the int[maxDoc()] array either contains the termNumber list directly
+    // or the *end* offset of the termNumber list in it's byte array (for faster
+    // appending and faster creation of the final form).
+    //
+    // idea... if things are too large while building, we could do a range of docs
+    // at a time (but it would be a fair amount slower to build)
+    // could also do ranges in parallel to take advantage of multiple CPUs
+
+    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)
+    // values.  This requires going over the field first to find the most
+    // frequent terms ahead of time.
+
+    int termNum = 0;
+    docsEnum = null;
+
+    // Loop begins with te positioned to first term (we call
+    // seek above):
+    for (;;) {
+      final BytesRef t = te.term();
+      if (t == null || (termPrefix != null && !StringHelper.startsWith(t, termPrefix))) {
+        break;
+      }
+      //System.out.println("visit term=" + t.utf8ToString() + " " + t + " termNum=" + termNum);
+
+      if (!testedOrd) {
+        try {
+          ordBase = (int) te.ord();
+          //System.out.println("got ordBase=" + ordBase);
+        } catch (UnsupportedOperationException uoe) {
+          // Reader cannot provide ord support, so we wrap
+          // our own support by creating our own terms index:
+          indexedTerms = new ArrayList<>();
+          indexedTermsBytes = new PagedBytes(15);
+          //System.out.println("NO ORDS");
+        }
+        testedOrd = true;
+      }
+
+      visitTerm(te, termNum);
+
+      if (indexedTerms != null && (termNum & indexIntervalMask) == 0) {
+        // Index this term
+        sizeOfIndexedStrings += t.length;
+        BytesRef indexedTerm = new BytesRef();
+        indexedTermsBytes.copy(t, indexedTerm);
+        // TODO: really should 1) strip off useless suffix,
+        // and 2) use FST not array/PagedBytes
+        indexedTerms.add(indexedTerm);
+      }
+
+      final int df = te.docFreq();
+      if (df <= maxTermDocFreq) {
+
+        docsEnum = te.docs(liveDocs, docsEnum, DocsEnum.FLAG_NONE);
+
+        // dF, but takes deletions into account
+        int actualDF = 0;
+
+        for (;;) {
+          int doc = docsEnum.nextDoc();
+          if (doc == DocIdSetIterator.NO_MORE_DOCS) {
+            break;
+          }
+          //System.out.println("  chunk=" + chunk + " docs");
+
+          actualDF ++;
+          termInstances++;
+          
+          //System.out.println("    docID=" + doc);
+          // add TNUM_OFFSET to the term number to make room for special reserved values:
+          // 0 (end term) and 1 (index into byte array follows)
+          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;
+          lastTerm[doc] = termNum;
+          int val = index[doc];
+
+          if ((val & 0xff)==1) {
+            // index into byte array (actually the end of
+            // the doc-specific byte[] when building)
+            int pos = val >>> 8;
+            int ilen = vIntSize(delta);
+            byte[] arr = bytes[doc];
+            int newend = pos+ilen;
+            if (newend > arr.length) {
+              // We avoid a doubling strategy to lower memory usage.
+              // this faceting method isn't for docs with many terms.
+              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.
+              // TODO: figure out what array lengths we can round up to w/o actually using more memory
+              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?
+              // It should be safe to round up to the nearest 32 bits in any case.
+              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment
+              byte[] newarr = new byte[newLen];
+              System.arraycopy(arr, 0, newarr, 0, pos);
+              arr = newarr;
+              bytes[doc] = newarr;
+            }
+            pos = writeInt(delta, arr, pos);
+            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]
+          } else {
+            // OK, this int has data in it... find the end (a zero starting byte - not
+            // part of another number, hence not following a byte with the high bit set).
+            int ipos;
+            if (val==0) {
+              ipos=0;
+            } else if ((val & 0x0000ff80)==0) {
+              ipos=1;
+            } else if ((val & 0x00ff8000)==0) {
+              ipos=2;
+            } else if ((val & 0xff800000)==0) {
+              ipos=3;
+            } else {
+              ipos=4;
+            }
+
+            //System.out.println("      ipos=" + ipos);
+
+            int endPos = writeInt(delta, tempArr, ipos);
+            //System.out.println("      endpos=" + endPos);
+            if (endPos <= 4) {
+              //System.out.println("      fits!");
+              // value will fit in the integer... move bytes back
+              for (int j=ipos; j<endPos; j++) {
+                val |= (tempArr[j] & 0xff) << (j<<3);
+              }
+              index[doc] = val;
+            } else {
+              // value won't fit... move integer into byte[]
+              for (int j=0; j<ipos; j++) {
+                tempArr[j] = (byte)val;
+                val >>>=8;
+              }
+              // point at the end index in the byte[]
+              index[doc] = (endPos<<8) | 1;
+              bytes[doc] = tempArr;
+              tempArr = new byte[12];
+            }
+          }
+        }
+        setActualDocFreq(termNum, actualDF);
+      }
+
+      termNum++;
+      if (te.next() == null) {
+        break;
+      }
+    }
+
+    numTermsInField = termNum;
+
+    long midPoint = System.currentTimeMillis();
+
+    if (termInstances == 0) {
+      // we didn't invert anything
+      // lower memory consumption.
+      tnums = null;
+    } else {
+
+      this.index = index;
+
+      //
+      // transform intermediate form into the final form, building a single byte[]
+      // at a time, and releasing the intermediate byte[]s as we go to avoid
+      // increasing the memory footprint.
+      //
+
+      for (int pass = 0; pass<256; pass++) {
+        byte[] target = tnums[pass];
+        int pos=0;  // end in target;
+        if (target != null) {
+          pos = target.length;
+        } else {
+          target = new byte[4096];
+        }
+
+        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx
+        // where pp is the pass (which array we are building), and xx is all values.
+        // each pass shares the same byte[] for termNumber lists.
+        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {
+          int lim = Math.min(docbase + (1<<16), maxDoc);
+          for (int doc=docbase; doc<lim; doc++) {
+            //System.out.println("  pass=" + pass + " process docID=" + doc);
+            int val = index[doc];
+            if ((val&0xff) == 1) {
+              int len = val >>> 8;
+              //System.out.println("    ptr pos=" + pos);
+              index[doc] = (pos<<8)|1; // change index to point to start of array
+              if ((pos & 0xff000000) != 0) {
+                // we only have 24 bits for the array index
+                throw new IllegalStateException("Too many values for UnInvertedField faceting on field "+field);
+              }
+              byte[] arr = bytes[doc];
+              /*
+              for(byte b : arr) {
+                //System.out.println("      b=" + Integer.toHexString((int) b));
+              }
+              */
+              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM
+              if (target.length <= pos + len) {
+                int newlen = target.length;
+                /*** we don't have to worry about the array getting too large
+                 * since the "pos" param will overflow first (only 24 bits available)
+                if ((newlen<<1) <= 0) {
+                  // overflow...
+                  newlen = Integer.MAX_VALUE;
+                  if (newlen <= pos + len) {
+                    throw new SolrException(400,"Too many terms to uninvert field!");
+                  }
+                } else {
+                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy
+                }
+                ****/
+                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 
+                byte[] newtarget = new byte[newlen];
+                System.arraycopy(target, 0, newtarget, 0, pos);
+                target = newtarget;
+              }
+              System.arraycopy(arr, 0, target, pos, len);
+              pos += len + 1;  // skip single byte at end and leave it 0 for terminator
+            }
+          }
+        }
+
+        // shrink array
+        if (pos < target.length) {
+          byte[] newtarget = new byte[pos];
+          System.arraycopy(target, 0, newtarget, 0, pos);
+          target = newtarget;
+        }
+        
+        tnums[pass] = target;
+
+        if ((pass << 16) > maxDoc)
+          break;
+      }
+
+    }
+    if (indexedTerms != null) {
+      indexedTermsArray = indexedTerms.toArray(new BytesRef[indexedTerms.size()]);
+    }
+
+    long endTime = System.currentTimeMillis();
+
+    total_time = (int)(endTime-startTime);
+    phase1_time = (int)(midPoint-startTime);
+  }
+
+  /** Number of bytes to represent an unsigned int as a vint. */
+  private static int vIntSize(int x) {
+    if ((x & (0xffffffff << (7*1))) == 0 ) {
+      return 1;
+    }
+    if ((x & (0xffffffff << (7*2))) == 0 ) {
+      return 2;
+    }
+    if ((x & (0xffffffff << (7*3))) == 0 ) {
+      return 3;
+    }
+    if ((x & (0xffffffff << (7*4))) == 0 ) {
+      return 4;
+    }
+    return 5;
+  }
+
+  // todo: if we know the size of the vInt already, we could do
+  // a single switch on the size
+  private static int writeInt(int x, byte[] arr, int pos) {
+    int a;
+    a = (x >>> (7*4));
+    if (a != 0) {
+      arr[pos++] = (byte)(a | 0x80);
+    }
+    a = (x >>> (7*3));
+    if (a != 0) {
+      arr[pos++] = (byte)(a | 0x80);
+    }
+    a = (x >>> (7*2));
+    if (a != 0) {
+      arr[pos++] = (byte)(a | 0x80);
+    }
+    a = (x >>> (7*1));
+    if (a != 0) {
+      arr[pos++] = (byte)(a | 0x80);
+    }
+    arr[pos++] = (byte)(x & 0x7f);
+    return pos;
+  }
+
+  /* Only used if original IndexReader doesn't implement
+   * ord; in this case we "wrap" our own terms index
+   * around it. */
+  private final class OrdWrappedTermsEnum extends TermsEnum {
+    private final TermsEnum termsEnum;
+    private BytesRef term;
+    private long ord = -indexInterval-1;          // force "real" seek
+    
+    public OrdWrappedTermsEnum(AtomicReader reader) throws IOException {
+      assert indexedTermsArray != null;
+      termsEnum = reader.fields().terms(field).iterator(null);
+    }
+
+    @Override    
+    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+      return termsEnum.docs(liveDocs, reuse, flags);
+    }
+
+    @Override    
+    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
+      return termsEnum.docsAndPositions(liveDocs, reuse, flags);
+    }
+
+    @Override
+    public BytesRef term() {
+      return term;
+    }
+
+    @Override
+    public BytesRef next() throws IOException {
+      if (++ord < 0) {
+        ord = 0;
+      }
+      if (termsEnum.next() == null) {
+        term = null;
+        return null;
+      }
+      return setTerm();  // this is extra work if we know we are in bounds...
+    }
+
+    @Override
+    public int docFreq() throws IOException {
+      return termsEnum.docFreq();
+    }
+
+    @Override
+    public long totalTermFreq() throws IOException {
+      return termsEnum.totalTermFreq();
+    }
+
+    @Override
+    public long ord() {
+      return ordBase + ord;
+    }
+
+    @Override
+    public SeekStatus seekCeil(BytesRef target) throws IOException {
+
+      // already here
+      if (term != null && term.equals(target)) {
+        return SeekStatus.FOUND;
+      }
+
+      int startIdx = Arrays.binarySearch(indexedTermsArray, target);
+
+      if (startIdx >= 0) {
+        // we hit the term exactly... lucky us!
+        TermsEnum.SeekStatus seekStatus = termsEnum.seekCeil(target);
+        assert seekStatus == TermsEnum.SeekStatus.FOUND;
+        ord = startIdx << indexIntervalBits;
+        setTerm();
+        assert term != null;
+        return SeekStatus.FOUND;
+      }
+
+      // we didn't hit the term exactly
+      startIdx = -startIdx-1;
+    
+      if (startIdx == 0) {
+        // our target occurs *before* the first term
+        TermsEnum.SeekStatus seekStatus = termsEnum.seekCeil(target);
+        assert seekStatus == TermsEnum.SeekStatus.NOT_FOUND;
+        ord = 0;
+        setTerm();
+        assert term != null;
+        return SeekStatus.NOT_FOUND;
+      }
+
+      // back up to the start of the block
+      startIdx--;
+
+      if ((ord >> indexIntervalBits) == startIdx && term != null && term.compareTo(target) <= 0) {
+        // we are already in the right block and the current term is before the term we want,
+        // so we don't need to seek.
+      } else {
+        // seek to the right block
+        TermsEnum.SeekStatus seekStatus = termsEnum.seekCeil(indexedTermsArray[startIdx]);
+        assert seekStatus == TermsEnum.SeekStatus.FOUND;
+        ord = startIdx << indexIntervalBits;
+        setTerm();
+        assert term != null;  // should be non-null since it's in the index
+      }
+
+      while (term != null && term.compareTo(target) < 0) {
+        next();
+      }
+
+      if (term == null) {
+        return SeekStatus.END;
+      } else if (term.compareTo(target) == 0) {
+        return SeekStatus.FOUND;
+      } else {
+        return SeekStatus.NOT_FOUND;
+      }
+    }
+
+    @Override
+    public void seekExact(long targetOrd) throws IOException {
+      int delta = (int) (targetOrd - ordBase - ord);
+      //System.out.println("  seek(ord) targetOrd=" + targetOrd + " delta=" + delta + " ord=" + ord + " ii=" + indexInterval);
+      if (delta < 0 || delta > indexInterval) {
+        final int idx = (int) (targetOrd >>> indexIntervalBits);
+        final BytesRef base = indexedTermsArray[idx];
+        //System.out.println("  do seek term=" + base.utf8ToString());
+        ord = idx << indexIntervalBits;
+        delta = (int) (targetOrd - ord);
+        final TermsEnum.SeekStatus seekStatus = termsEnum.seekCeil(base);
+        assert seekStatus == TermsEnum.SeekStatus.FOUND;
+      } else {
+        //System.out.println("seek w/in block");
+      }
+
+      while (--delta >= 0) {
+        BytesRef br = termsEnum.next();
+        if (br == null) {
+          assert false;
+          return;
+        }
+        ord++;
+      }
+
+      setTerm();
+      assert term != null;
+    }
+
+    private BytesRef setTerm() throws IOException {
+      term = termsEnum.term();
+      //System.out.println("  setTerm() term=" + term.utf8ToString() + " vs prefix=" + (prefix == null ? "null" : prefix.utf8ToString()));
+      if (prefix != null && !StringHelper.startsWith(term, prefix)) {
+        term = null;
+      }
+      return term;
+    }
+  }
+
+  /** Returns the term ({@link BytesRef}) corresponding to
+   *  the provided ordinal. */
+  public BytesRef lookupTerm(TermsEnum termsEnum, int ord) throws IOException {
+    termsEnum.seekExact(ord);
+    return termsEnum.term();
+  }
+  
+  /** Returns a SortedSetDocValues view of this instance */
+  public SortedSetDocValues iterator(AtomicReader reader) throws IOException {
+    if (isEmpty()) {
+      return DocValues.EMPTY_SORTED_SET;
+    } else {
+      return new Iterator(reader);
+    }
+  }
+  
+  private class Iterator extends SortedSetDocValues {
+    final AtomicReader reader;
+    final TermsEnum te;  // used internally for lookupOrd() and lookupTerm()
+    // currently we read 5 at a time (using the logic of the old iterator)
+    final int buffer[] = new int[5];
+    int bufferUpto;
+    int bufferLength;
+    
+    private int tnum;
+    private int upto;
+    private byte[] arr;
+    
+    Iterator(AtomicReader reader) throws IOException {
+      this.reader = reader;
+      this.te = termsEnum();
+    }
+    
+    @Override
+    public long nextOrd() {
+      while (bufferUpto == bufferLength) {
+        if (bufferLength < buffer.length) {
+          return NO_MORE_ORDS;
+        } else {
+          bufferLength = read(buffer);
+          bufferUpto = 0;
+        }
+      }
+      return buffer[bufferUpto++];
+    }
+    
+    /** Buffer must be at least 5 ints long.  Returns number
+     *  of term ords placed into buffer; if this count is
+     *  less than buffer.length then that is the end. */
+    int read(int[] buffer) {
+      int bufferUpto = 0;
+      if (arr == null) {
+        // code is inlined into upto
+        //System.out.println("inlined");
+        int code = upto;
+        int delta = 0;
+        for (;;) {
+          delta = (delta << 7) | (code & 0x7f);
+          if ((code & 0x80)==0) {
+            if (delta==0) break;
+            tnum += delta - TNUM_OFFSET;
+            buffer[bufferUpto++] = ordBase+tnum;
+            //System.out.println("  tnum=" + tnum);
+            delta = 0;
+          }
+          code >>>= 8;
+        }
+      } else {
+        // code is a pointer
+        for(;;) {
+          int delta = 0;
+          for(;;) {
+            byte b = arr[upto++];
+            delta = (delta << 7) | (b & 0x7f);
+            //System.out.println("    cycle: upto=" + upto + " delta=" + delta + " b=" + b);
+            if ((b & 0x80) == 0) break;
+          }
+          //System.out.println("  delta=" + delta);
+          if (delta == 0) break;
+          tnum += delta - TNUM_OFFSET;
+          //System.out.println("  tnum=" + tnum);
+          buffer[bufferUpto++] = ordBase+tnum;
+          if (bufferUpto == buffer.length) {
+            break;
+          }
+        }
+      }
+
+      return bufferUpto;
+    }
+
+    @Override
+    public void setDocument(int docID) {
+      tnum = 0;
+      final int code = index[docID];
+      if ((code & 0xff)==1) {
+        // a pointer
+        upto = code>>>8;
+        //System.out.println("    pointer!  upto=" + upto);
+        int whichArray = (docID >>> 16) & 0xff;
+        arr = tnums[whichArray];
+      } else {
+        //System.out.println("    inline!");
+        arr = null;
+        upto = code;
+      }
+      bufferUpto = 0;
+      bufferLength = read(buffer);
+    }
+
+    @Override
+    public void lookupOrd(long ord, BytesRef result) {
+      BytesRef ref = null;
+      try {
+        ref = DocTermOrds.this.lookupTerm(te, (int) ord);
+      } catch (IOException e) {
+        throw new RuntimeException(e);
+      }
+      result.bytes = ref.bytes;
+      result.offset = ref.offset;
+      result.length = ref.length;
+    }
+
+    @Override
+    public long getValueCount() {
+      return numTerms();
+    }
+
+    @Override
+    public long lookupTerm(BytesRef key) {
+      try {
+        if (te.seekCeil(key) == SeekStatus.FOUND) {
+          return te.ord();
+        } else {
+          return -te.ord()-1;
+        }
+      } catch (IOException e) {
+        throw new RuntimeException(e);
+      }
+    }
+    
+    @Override
+    public TermsEnum termsEnum() {    
+      try {
+        return getOrdTermsEnum(reader);
+      } catch (IOException e) {
+        throw new RuntimeException();
+      }
+    }
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.java lucene5666/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.java
--- lucene-trunk/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.java	2014-05-12 17:17:08.708483399 -0400
@@ -0,0 +1,908 @@
+package org.apache.lucene.uninverting;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.WeakHashMap;
+
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentReader;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.packed.GrowableWriter;
+import org.apache.lucene.util.packed.MonotonicAppendingLongBuffer;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Expert: The default cache implementation, storing all values in memory.
+ * A WeakHashMap is used for storage.
+ *
+ * @since   lucene 1.4
+ */
+class FieldCacheImpl implements FieldCache {
+
+  private Map<Class<?>,Cache> caches;
+  FieldCacheImpl() {
+    init();
+  }
+
+  private synchronized void init() {
+    caches = new HashMap<>(6);
+    caches.put(Long.TYPE, new LongCache(this));
+    caches.put(BinaryDocValues.class, new BinaryDocValuesCache(this));
+    caches.put(SortedDocValues.class, new SortedDocValuesCache(this));
+    caches.put(DocTermOrds.class, new DocTermOrdsCache(this));
+    caches.put(DocsWithFieldCache.class, new DocsWithFieldCache(this));
+  }
+
+  @Override
+  public synchronized void purgeAllCaches() {
+    init();
+  }
+
+  @Override
+  public synchronized void purgeByCacheKey(Object coreCacheKey) {
+    for(Cache c : caches.values()) {
+      c.purgeByCacheKey(coreCacheKey);
+    }
+  }
+
+  @Override
+  public synchronized CacheEntry[] getCacheEntries() {
+    List<CacheEntry> result = new ArrayList<>(17);
+    for(final Map.Entry<Class<?>,Cache> cacheEntry: caches.entrySet()) {
+      final Cache cache = cacheEntry.getValue();
+      final Class<?> cacheType = cacheEntry.getKey();
+      synchronized(cache.readerCache) {
+        for (final Map.Entry<Object,Map<CacheKey, Object>> readerCacheEntry : cache.readerCache.entrySet()) {
+          final Object readerKey = readerCacheEntry.getKey();
+          if (readerKey == null) continue;
+          final Map<CacheKey, Object> innerCache = readerCacheEntry.getValue();
+          for (final Map.Entry<CacheKey, Object> mapEntry : innerCache.entrySet()) {
+            CacheKey entry = mapEntry.getKey();
+            result.add(new CacheEntry(readerKey, entry.field,
+                                      cacheType, entry.custom,
+                                      mapEntry.getValue()));
+          }
+        }
+      }
+    }
+    return result.toArray(new CacheEntry[result.size()]);
+  }
+
+  // per-segment fieldcaches don't purge until the shared core closes.
+  final SegmentReader.CoreClosedListener purgeCore = new SegmentReader.CoreClosedListener() {
+    @Override
+    public void onClose(Object ownerCoreCacheKey) {
+      FieldCacheImpl.this.purgeByCacheKey(ownerCoreCacheKey);
+    }
+  };
+
+  // composite/SlowMultiReaderWrapper fieldcaches don't purge until composite reader is closed.
+  final IndexReader.ReaderClosedListener purgeReader = new IndexReader.ReaderClosedListener() {
+    @Override
+    public void onClose(IndexReader owner) {
+      assert owner instanceof AtomicReader;
+      FieldCacheImpl.this.purgeByCacheKey(((AtomicReader) owner).getCoreCacheKey());
+    }
+  };
+  
+  private void initReader(AtomicReader reader) {
+    if (reader instanceof SegmentReader) {
+      ((SegmentReader) reader).addCoreClosedListener(purgeCore);
+    } else {
+      // we have a slow reader of some sort, try to register a purge event
+      // rather than relying on gc:
+      Object key = reader.getCoreCacheKey();
+      if (key instanceof AtomicReader) {
+        ((AtomicReader)key).addReaderClosedListener(purgeReader); 
+      } else {
+        // last chance
+        reader.addReaderClosedListener(purgeReader);
+      }
+    }
+  }
+
+  /** Expert: Internal cache. */
+  abstract static class Cache {
+
+    Cache(FieldCacheImpl wrapper) {
+      this.wrapper = wrapper;
+    }
+
+    final FieldCacheImpl wrapper;
+
+    final Map<Object,Map<CacheKey,Object>> readerCache = new WeakHashMap<>();
+    
+    protected abstract Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField)
+        throws IOException;
+
+    /** Remove this reader from the cache, if present. */
+    public void purgeByCacheKey(Object coreCacheKey) {
+      synchronized(readerCache) {
+        readerCache.remove(coreCacheKey);
+      }
+    }
+
+    /** Sets the key to the value for the provided reader;
+     *  if the key is already set then this doesn't change it. */
+    public void put(AtomicReader reader, CacheKey key, Object value) {
+      final Object readerKey = reader.getCoreCacheKey();
+      synchronized (readerCache) {
+        Map<CacheKey,Object> innerCache = readerCache.get(readerKey);
+        if (innerCache == null) {
+          // First time this reader is using FieldCache
+          innerCache = new HashMap<>();
+          readerCache.put(readerKey, innerCache);
+          wrapper.initReader(reader);
+        }
+        if (innerCache.get(key) == null) {
+          innerCache.put(key, value);
+        } else {
+          // Another thread beat us to it; leave the current
+          // value
+        }
+      }
+    }
+
+    public Object get(AtomicReader reader, CacheKey key, boolean setDocsWithField) throws IOException {
+      Map<CacheKey,Object> innerCache;
+      Object value;
+      final Object readerKey = reader.getCoreCacheKey();
+      synchronized (readerCache) {
+        innerCache = readerCache.get(readerKey);
+        if (innerCache == null) {
+          // First time this reader is using FieldCache
+          innerCache = new HashMap<>();
+          readerCache.put(readerKey, innerCache);
+          wrapper.initReader(reader);
+          value = null;
+        } else {
+          value = innerCache.get(key);
+        }
+        if (value == null) {
+          value = new CreationPlaceholder();
+          innerCache.put(key, value);
+        }
+      }
+      if (value instanceof CreationPlaceholder) {
+        synchronized (value) {
+          CreationPlaceholder progress = (CreationPlaceholder) value;
+          if (progress.value == null) {
+            progress.value = createValue(reader, key, setDocsWithField);
+            synchronized (readerCache) {
+              innerCache.put(key, progress.value);
+            }
+
+            // Only check if key.custom (the parser) is
+            // non-null; else, we check twice for a single
+            // call to FieldCache.getXXX
+            if (key.custom != null && wrapper != null) {
+              final PrintStream infoStream = wrapper.getInfoStream();
+              if (infoStream != null) {
+                printNewInsanity(infoStream, progress.value);
+              }
+            }
+          }
+          return progress.value;
+        }
+      }
+      return value;
+    }
+
+    private void printNewInsanity(PrintStream infoStream, Object value) {
+      final FieldCacheSanityChecker.Insanity[] insanities = FieldCacheSanityChecker.checkSanity(wrapper);
+      for(int i=0;i<insanities.length;i++) {
+        final FieldCacheSanityChecker.Insanity insanity = insanities[i];
+        final CacheEntry[] entries = insanity.getCacheEntries();
+        for(int j=0;j<entries.length;j++) {
+          if (entries[j].getValue() == value) {
+            // OK this insanity involves our entry
+            infoStream.println("WARNING: new FieldCache insanity created\nDetails: " + insanity.toString());
+            infoStream.println("\nStack:\n");
+            new Throwable().printStackTrace(infoStream);
+            break;
+          }
+        }
+      }
+    }
+  }
+
+  /** Expert: Every composite-key in the internal cache is of this type. */
+  static class CacheKey {
+    final String field;        // which Field
+    final Object custom;       // which custom comparator or parser
+
+    /** Creates one of these objects for a custom comparator/parser. */
+    CacheKey(String field, Object custom) {
+      this.field = field;
+      this.custom = custom;
+    }
+
+    /** Two of these are equal iff they reference the same field and type. */
+    @Override
+    public boolean equals (Object o) {
+      if (o instanceof CacheKey) {
+        CacheKey other = (CacheKey) o;
+        if (other.field.equals(field)) {
+          if (other.custom == null) {
+            if (custom == null) return true;
+          } else if (other.custom.equals (custom)) {
+            return true;
+          }
+        }
+      }
+      return false;
+    }
+
+    /** Composes a hashcode based on the field and type. */
+    @Override
+    public int hashCode() {
+      return field.hashCode() ^ (custom==null ? 0 : custom.hashCode());
+    }
+  }
+
+  private static abstract class Uninvert {
+
+    public Bits docsWithField;
+
+    public void uninvert(AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
+      final int maxDoc = reader.maxDoc();
+      Terms terms = reader.terms(field);
+      if (terms != null) {
+        if (setDocsWithField) {
+          final int termsDocCount = terms.getDocCount();
+          assert termsDocCount <= maxDoc;
+          if (termsDocCount == maxDoc) {
+            // Fast case: all docs have this field:
+            docsWithField = new Bits.MatchAllBits(maxDoc);
+            setDocsWithField = false;
+          }
+        }
+
+        final TermsEnum termsEnum = termsEnum(terms);
+
+        DocsEnum docs = null;
+        FixedBitSet docsWithField = null;
+        while(true) {
+          final BytesRef term = termsEnum.next();
+          if (term == null) {
+            break;
+          }
+          visitTerm(term);
+          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);
+          while (true) {
+            final int docID = docs.nextDoc();
+            if (docID == DocIdSetIterator.NO_MORE_DOCS) {
+              break;
+            }
+            visitDoc(docID);
+            if (setDocsWithField) {
+              if (docsWithField == null) {
+                // Lazy init
+                this.docsWithField = docsWithField = new FixedBitSet(maxDoc);
+              }
+              docsWithField.set(docID);
+            }
+          }
+        }
+      }
+    }
+
+    protected abstract TermsEnum termsEnum(Terms terms) throws IOException;
+    protected abstract void visitTerm(BytesRef term);
+    protected abstract void visitDoc(int docID);
+  }
+
+  // null Bits means no docs matched
+  void setDocsWithField(AtomicReader reader, String field, Bits docsWithField) {
+    final int maxDoc = reader.maxDoc();
+    final Bits bits;
+    if (docsWithField == null) {
+      bits = new Bits.MatchNoBits(maxDoc);
+    } else if (docsWithField instanceof FixedBitSet) {
+      final int numSet = ((FixedBitSet) docsWithField).cardinality();
+      if (numSet >= maxDoc) {
+        // The cardinality of the BitSet is maxDoc if all documents have a value.
+        assert numSet == maxDoc;
+        bits = new Bits.MatchAllBits(maxDoc);
+      } else {
+        bits = docsWithField;
+      }
+    } else {
+      bits = docsWithField;
+    }
+    caches.get(DocsWithFieldCache.class).put(reader, new CacheKey(field, null), bits);
+  }
+
+  private static class HoldsOneThing<T> {
+    private T it;
+
+    public void set(T it) {
+      this.it = it;
+    }
+
+    public T get() {
+      return it;
+    }
+  }
+
+  private static class GrowableWriterAndMinValue {
+    GrowableWriterAndMinValue(GrowableWriter array, long minValue) {
+      this.writer = array;
+      this.minValue = minValue;
+    }
+    public GrowableWriter writer;
+    public long minValue;
+  }
+
+  public Bits getDocsWithField(AtomicReader reader, String field) throws IOException {
+    final FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);
+    if (fieldInfo == null) {
+      // field does not exist or has no value
+      return new Bits.MatchNoBits(reader.maxDoc());
+    } else if (fieldInfo.hasDocValues()) {
+      return reader.getDocsWithField(field);
+    } else if (!fieldInfo.isIndexed()) {
+      return new Bits.MatchNoBits(reader.maxDoc());
+    }
+    return (Bits) caches.get(DocsWithFieldCache.class).get(reader, new CacheKey(field, null), false);
+  }
+
+  static final class DocsWithFieldCache extends Cache {
+    DocsWithFieldCache(FieldCacheImpl wrapper) {
+      super(wrapper);
+    }
+    
+    @Override
+    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)
+    throws IOException {
+      final String field = key.field;
+      final int maxDoc = reader.maxDoc();
+
+      // Visit all docs that have terms for this field
+      FixedBitSet res = null;
+      Terms terms = reader.terms(field);
+      if (terms != null) {
+        final int termsDocCount = terms.getDocCount();
+        assert termsDocCount <= maxDoc;
+        if (termsDocCount == maxDoc) {
+          // Fast case: all docs have this field:
+          return new Bits.MatchAllBits(maxDoc);
+        }
+        final TermsEnum termsEnum = terms.iterator(null);
+        DocsEnum docs = null;
+        while(true) {
+          final BytesRef term = termsEnum.next();
+          if (term == null) {
+            break;
+          }
+          if (res == null) {
+            // lazy init
+            res = new FixedBitSet(maxDoc);
+          }
+
+          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);
+          // TODO: use bulk API
+          while (true) {
+            final int docID = docs.nextDoc();
+            if (docID == DocIdSetIterator.NO_MORE_DOCS) {
+              break;
+            }
+            res.set(docID);
+          }
+        }
+      }
+      if (res == null) {
+        return new Bits.MatchNoBits(maxDoc);
+      }
+      final int numSet = res.cardinality();
+      if (numSet >= maxDoc) {
+        // The cardinality of the BitSet is maxDoc if all documents have a value.
+        assert numSet == maxDoc;
+        return new Bits.MatchAllBits(maxDoc);
+      }
+      return res;
+    }
+  }
+  
+  @Override
+  public NumericDocValues getNumerics(AtomicReader reader, String field, Parser parser, boolean setDocsWithField) throws IOException {
+    if (parser == null) {
+      throw new NullPointerException();
+    }
+    final NumericDocValues valuesIn = reader.getNumericDocValues(field);
+    if (valuesIn != null) {
+      // Not cached here by FieldCacheImpl (cached instead
+      // per-thread by SegmentReader):
+      return valuesIn;
+    } else {
+      final FieldInfo info = reader.getFieldInfos().fieldInfo(field);
+      if (info == null) {
+        return DocValues.EMPTY_NUMERIC;
+      } else if (info.hasDocValues()) {
+        throw new IllegalStateException("Type mismatch: " + field + " was indexed as " + info.getDocValuesType());
+      } else if (!info.isIndexed()) {
+        return DocValues.EMPTY_NUMERIC;
+      }
+      return (NumericDocValues) caches.get(Long.TYPE).get(reader, new CacheKey(field, parser), setDocsWithField);
+    }
+  }
+
+  static class LongsFromArray extends NumericDocValues {
+    private final PackedInts.Reader values;
+    private final long minValue;
+
+    public LongsFromArray(PackedInts.Reader values, long minValue) {
+      this.values = values;
+      this.minValue = minValue;
+    }
+    
+    @Override
+    public long get(int docID) {
+      return minValue + values.get(docID);
+    }
+  }
+
+  static final class LongCache extends Cache {
+    LongCache(FieldCacheImpl wrapper) {
+      super(wrapper);
+    }
+
+    @Override
+    protected Object createValue(final AtomicReader reader, CacheKey key, boolean setDocsWithField)
+        throws IOException {
+
+      final Parser parser = (Parser) key.custom;
+
+      final HoldsOneThing<GrowableWriterAndMinValue> valuesRef = new HoldsOneThing<>();
+
+      Uninvert u = new Uninvert() {
+          private long minValue;
+          private long currentValue;
+          private GrowableWriter values;
+
+          @Override
+          public void visitTerm(BytesRef term) {
+            currentValue = parser.parseValue(term);
+            if (values == null) {
+              // Lazy alloc so for the numeric field case
+              // (which will hit a NumberFormatException
+              // when we first try the DEFAULT_INT_PARSER),
+              // we don't double-alloc:
+              int startBitsPerValue;
+              // Make sure than missing values (0) can be stored without resizing
+              if (currentValue < 0) {
+                minValue = currentValue;
+                startBitsPerValue = minValue == Long.MIN_VALUE ? 64 : PackedInts.bitsRequired(-minValue);
+              } else {
+                minValue = 0;
+                startBitsPerValue = PackedInts.bitsRequired(currentValue);
+              }
+              values = new GrowableWriter(startBitsPerValue, reader.maxDoc(), PackedInts.FAST);
+              if (minValue != 0) {
+                values.fill(0, values.size(), -minValue); // default value must be 0
+              }
+              valuesRef.set(new GrowableWriterAndMinValue(values, minValue));
+            }
+          }
+
+          @Override
+          public void visitDoc(int docID) {
+            values.set(docID, currentValue - minValue);
+          }
+          
+          @Override
+          protected TermsEnum termsEnum(Terms terms) throws IOException {
+            return parser.termsEnum(terms);
+          }
+        };
+
+      u.uninvert(reader, key.field, setDocsWithField);
+
+      if (setDocsWithField) {
+        wrapper.setDocsWithField(reader, key.field, u.docsWithField);
+      }
+      GrowableWriterAndMinValue values = valuesRef.get();
+      if (values == null) {
+        return new LongsFromArray(new PackedInts.NullReader(reader.maxDoc()), 0L);
+      }
+      return new LongsFromArray(values.writer.getMutable(), values.minValue);
+    }
+  }
+
+  public static class SortedDocValuesImpl extends SortedDocValues {
+    private final PagedBytes.Reader bytes;
+    private final MonotonicAppendingLongBuffer termOrdToBytesOffset;
+    private final PackedInts.Reader docToTermOrd;
+    private final int numOrd;
+
+    public SortedDocValuesImpl(PagedBytes.Reader bytes, MonotonicAppendingLongBuffer termOrdToBytesOffset, PackedInts.Reader docToTermOrd, int numOrd) {
+      this.bytes = bytes;
+      this.docToTermOrd = docToTermOrd;
+      this.termOrdToBytesOffset = termOrdToBytesOffset;
+      this.numOrd = numOrd;
+    }
+
+    @Override
+    public int getValueCount() {
+      return numOrd;
+    }
+
+    @Override
+    public int getOrd(int docID) {
+      // Subtract 1, matching the 1+ord we did when
+      // storing, so that missing values, which are 0 in the
+      // packed ints, are returned as -1 ord:
+      return (int) docToTermOrd.get(docID)-1;
+    }
+
+    @Override
+    public void lookupOrd(int ord, BytesRef ret) {
+      if (ord < 0) {
+        throw new IllegalArgumentException("ord must be >=0 (got ord=" + ord + ")");
+      }
+      bytes.fill(ret, termOrdToBytesOffset.get(ord));
+    }
+  }
+
+  public SortedDocValues getTermsIndex(AtomicReader reader, String field) throws IOException {
+    return getTermsIndex(reader, field, PackedInts.FAST);
+  }
+
+  public SortedDocValues getTermsIndex(AtomicReader reader, String field, float acceptableOverheadRatio) throws IOException {
+    SortedDocValues valuesIn = reader.getSortedDocValues(field);
+    if (valuesIn != null) {
+      // Not cached here by FieldCacheImpl (cached instead
+      // per-thread by SegmentReader):
+      return valuesIn;
+    } else {
+      final FieldInfo info = reader.getFieldInfos().fieldInfo(field);
+      if (info == null) {
+        return DocValues.EMPTY_SORTED;
+      } else if (info.hasDocValues()) {
+        // we don't try to build a sorted instance from numeric/binary doc
+        // values because dedup can be very costly
+        throw new IllegalStateException("Type mismatch: " + field + " was indexed as " + info.getDocValuesType());
+      } else if (!info.isIndexed()) {
+        return DocValues.EMPTY_SORTED;
+      }
+      return (SortedDocValues) caches.get(SortedDocValues.class).get(reader, new CacheKey(field, acceptableOverheadRatio), false);
+    }
+  }
+
+  static class SortedDocValuesCache extends Cache {
+    SortedDocValuesCache(FieldCacheImpl wrapper) {
+      super(wrapper);
+    }
+
+    @Override
+    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)
+        throws IOException {
+
+      final int maxDoc = reader.maxDoc();
+
+      Terms terms = reader.terms(key.field);
+
+      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();
+
+      final PagedBytes bytes = new PagedBytes(15);
+
+      int startTermsBPV;
+
+      final int termCountHardLimit;
+      if (maxDoc == Integer.MAX_VALUE) {
+        termCountHardLimit = Integer.MAX_VALUE;
+      } else {
+        termCountHardLimit = maxDoc+1;
+      }
+
+      // TODO: use Uninvert?
+      if (terms != null) {
+        // Try for coarse estimate for number of bits; this
+        // should be an underestimate most of the time, which
+        // is fine -- GrowableWriter will reallocate as needed
+        long numUniqueTerms = terms.size();
+        if (numUniqueTerms != -1L) {
+          if (numUniqueTerms > termCountHardLimit) {
+            // app is misusing the API (there is more than
+            // one term per doc); in this case we make best
+            // effort to load what we can (see LUCENE-2142)
+            numUniqueTerms = termCountHardLimit;
+          }
+
+          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);
+        } else {
+          startTermsBPV = 1;
+        }
+      } else {
+        startTermsBPV = 1;
+      }
+
+      MonotonicAppendingLongBuffer termOrdToBytesOffset = new MonotonicAppendingLongBuffer();
+      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);
+
+      int termOrd = 0;
+
+      // TODO: use Uninvert?
+
+      if (terms != null) {
+        final TermsEnum termsEnum = terms.iterator(null);
+        DocsEnum docs = null;
+
+        while(true) {
+          final BytesRef term = termsEnum.next();
+          if (term == null) {
+            break;
+          }
+          if (termOrd >= termCountHardLimit) {
+            break;
+          }
+
+          termOrdToBytesOffset.add(bytes.copyUsingLengthPrefix(term));
+          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);
+          while (true) {
+            final int docID = docs.nextDoc();
+            if (docID == DocIdSetIterator.NO_MORE_DOCS) {
+              break;
+            }
+            // Store 1+ ord into packed bits
+            docToTermOrd.set(docID, 1+termOrd);
+          }
+          termOrd++;
+        }
+      }
+      termOrdToBytesOffset.freeze();
+
+      // maybe an int-only impl?
+      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset, docToTermOrd.getMutable(), termOrd);
+    }
+  }
+
+  private static class BinaryDocValuesImpl extends BinaryDocValues {
+    private final PagedBytes.Reader bytes;
+    private final PackedInts.Reader docToOffset;
+
+    public BinaryDocValuesImpl(PagedBytes.Reader bytes, PackedInts.Reader docToOffset) {
+      this.bytes = bytes;
+      this.docToOffset = docToOffset;
+    }
+
+    @Override
+    public void get(int docID, BytesRef ret) {
+      final int pointer = (int) docToOffset.get(docID);
+      if (pointer == 0) {
+        ret.bytes = BytesRef.EMPTY_BYTES;
+        ret.offset = 0;
+        ret.length = 0;
+      } else {
+        bytes.fill(ret, pointer);
+      }
+    }
+  }
+
+  // TODO: this if DocTermsIndex was already created, we
+  // should share it...
+  public BinaryDocValues getTerms(AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
+    return getTerms(reader, field, setDocsWithField, PackedInts.FAST);
+  }
+
+  public BinaryDocValues getTerms(AtomicReader reader, String field, boolean setDocsWithField, float acceptableOverheadRatio) throws IOException {
+    BinaryDocValues valuesIn = reader.getBinaryDocValues(field);
+    if (valuesIn == null) {
+      valuesIn = reader.getSortedDocValues(field);
+    }
+
+    if (valuesIn != null) {
+      // Not cached here by FieldCacheImpl (cached instead
+      // per-thread by SegmentReader):
+      return valuesIn;
+    }
+
+    final FieldInfo info = reader.getFieldInfos().fieldInfo(field);
+    if (info == null) {
+      return DocValues.EMPTY_BINARY;
+    } else if (info.hasDocValues()) {
+      throw new IllegalStateException("Type mismatch: " + field + " was indexed as " + info.getDocValuesType());
+    } else if (!info.isIndexed()) {
+      return DocValues.EMPTY_BINARY;
+    }
+
+    return (BinaryDocValues) caches.get(BinaryDocValues.class).get(reader, new CacheKey(field, acceptableOverheadRatio), setDocsWithField);
+  }
+
+  static final class BinaryDocValuesCache extends Cache {
+    BinaryDocValuesCache(FieldCacheImpl wrapper) {
+      super(wrapper);
+    }
+
+    @Override
+    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField)
+        throws IOException {
+
+      // TODO: would be nice to first check if DocTermsIndex
+      // was already cached for this field and then return
+      // that instead, to avoid insanity
+
+      final int maxDoc = reader.maxDoc();
+      Terms terms = reader.terms(key.field);
+
+      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();
+
+      final int termCountHardLimit = maxDoc;
+
+      // Holds the actual term data, expanded.
+      final PagedBytes bytes = new PagedBytes(15);
+
+      int startBPV;
+
+      if (terms != null) {
+        // Try for coarse estimate for number of bits; this
+        // should be an underestimate most of the time, which
+        // is fine -- GrowableWriter will reallocate as needed
+        long numUniqueTerms = terms.size();
+        if (numUniqueTerms != -1L) {
+          if (numUniqueTerms > termCountHardLimit) {
+            numUniqueTerms = termCountHardLimit;
+          }
+          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);
+        } else {
+          startBPV = 1;
+        }
+      } else {
+        startBPV = 1;
+      }
+
+      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);
+      
+      // pointer==0 means not set
+      bytes.copyUsingLengthPrefix(new BytesRef());
+
+      if (terms != null) {
+        int termCount = 0;
+        final TermsEnum termsEnum = terms.iterator(null);
+        DocsEnum docs = null;
+        while(true) {
+          if (termCount++ == termCountHardLimit) {
+            // app is misusing the API (there is more than
+            // one term per doc); in this case we make best
+            // effort to load what we can (see LUCENE-2142)
+            break;
+          }
+
+          final BytesRef term = termsEnum.next();
+          if (term == null) {
+            break;
+          }
+          final long pointer = bytes.copyUsingLengthPrefix(term);
+          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);
+          while (true) {
+            final int docID = docs.nextDoc();
+            if (docID == DocIdSetIterator.NO_MORE_DOCS) {
+              break;
+            }
+            docToOffset.set(docID, pointer);
+          }
+        }
+      }
+
+      final PackedInts.Reader offsetReader = docToOffset.getMutable();
+      if (setDocsWithField) {
+        wrapper.setDocsWithField(reader, key.field, new Bits() {
+          @Override
+          public boolean get(int index) {
+            return offsetReader.get(index) != 0;
+          }
+
+          @Override
+          public int length() {
+            return maxDoc;
+          }
+        });
+      }
+      // maybe an int-only impl?
+      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);
+    }
+  }
+
+  // TODO: this if DocTermsIndex was already created, we
+  // should share it...
+  public SortedSetDocValues getDocTermOrds(AtomicReader reader, String field, BytesRef prefix) throws IOException {
+    // not a general purpose filtering mechanism...
+    assert prefix == null || prefix == INT32_TERM_PREFIX || prefix == INT64_TERM_PREFIX;
+    
+    SortedSetDocValues dv = reader.getSortedSetDocValues(field);
+    if (dv != null) {
+      return dv;
+    }
+    
+    SortedDocValues sdv = reader.getSortedDocValues(field);
+    if (sdv != null) {
+      return DocValues.singleton(sdv);
+    }
+    
+    final FieldInfo info = reader.getFieldInfos().fieldInfo(field);
+    if (info == null) {
+      return DocValues.EMPTY_SORTED_SET;
+    } else if (info.hasDocValues()) {
+      throw new IllegalStateException("Type mismatch: " + field + " was indexed as " + info.getDocValuesType());
+    } else if (!info.isIndexed()) {
+      return DocValues.EMPTY_SORTED_SET;
+    }
+    
+    // ok we need to uninvert. check if we can optimize a bit.
+    
+    Terms terms = reader.terms(field);
+    if (terms == null) {
+      return DocValues.EMPTY_SORTED_SET;
+    } else {
+      // if #postings = #docswithfield we know that the field is "single valued enough".
+      // its possible the same term might appear twice in the same document, but SORTED_SET discards frequency.
+      // its still ok with filtering (which we limit to numerics), it just means precisionStep = Inf
+      long numPostings = terms.getSumDocFreq();
+      if (numPostings != -1 && numPostings == terms.getDocCount()) {
+        return DocValues.singleton(getTermsIndex(reader, field));
+      }
+    }
+    
+    DocTermOrds dto = (DocTermOrds) caches.get(DocTermOrds.class).get(reader, new CacheKey(field, prefix), false);
+    return dto.iterator(reader);
+  }
+
+  static final class DocTermOrdsCache extends Cache {
+    DocTermOrdsCache(FieldCacheImpl wrapper) {
+      super(wrapper);
+    }
+
+    @Override
+    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)
+        throws IOException {
+      BytesRef prefix = (BytesRef) key.custom;
+      return new DocTermOrds(reader, null, key.field, prefix);
+    }
+  }
+
+  private volatile PrintStream infoStream;
+
+  public void setInfoStream(PrintStream stream) {
+    infoStream = stream;
+  }
+
+  public PrintStream getInfoStream() {
+    return infoStream;
+  }
+}
+


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCache.java lucene5666/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCache.java
--- lucene-trunk/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCache.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCache.java	2014-05-12 16:48:24.308453370 -0400
@@ -0,0 +1,387 @@
+package org.apache.lucene.uninverting;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.PrintStream;
+
+import org.apache.lucene.analysis.NumericTokenStream;
+import org.apache.lucene.document.DoubleField;
+import org.apache.lucene.document.FloatField;
+import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.LongField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.IndexReader; // javadocs
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.NumericUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/**
+ * Expert: Maintains caches of term values.
+ *
+ * <p>Created: May 19, 2004 11:13:14 AM
+ *
+ * @since   lucene 1.4
+ * @see FieldCacheSanityChecker
+ *
+ * @lucene.internal
+ */
+interface FieldCache {
+
+  /**
+   * Placeholder indicating creation of this cache is currently in-progress.
+   */
+  public static final class CreationPlaceholder {
+    Object value;
+  }
+
+  /**
+   * interface to all parsers. It is used to parse different numeric types.
+   */
+  public interface Parser {
+    
+    /**
+     * Pulls a {@link TermsEnum} from the given {@link Terms}. This method allows certain parsers
+     * to filter the actual TermsEnum before the field cache is filled.
+     * 
+     * @param terms the {@link Terms} instance to create the {@link TermsEnum} from.
+     * @return a possibly filtered {@link TermsEnum} instance, this method must not return <code>null</code>.
+     * @throws IOException if an {@link IOException} occurs
+     */
+    public TermsEnum termsEnum(Terms terms) throws IOException;
+    
+    /** Parse's this field's value */
+    public long parseValue(BytesRef term);
+  }
+
+  /** Expert: The cache used internally by sorting and range query classes. */
+  public static FieldCache DEFAULT = new FieldCacheImpl();
+
+  /**
+   * A parser instance for int values encoded by {@link NumericUtils}, e.g. when indexed
+   * via {@link IntField}/{@link NumericTokenStream}.
+   */
+  public static final Parser NUMERIC_UTILS_INT_PARSER = new Parser() {
+    @Override
+    public long parseValue(BytesRef term) {
+      return NumericUtils.prefixCodedToInt(term);
+    }
+    
+    @Override
+    public TermsEnum termsEnum(Terms terms) throws IOException {
+      return NumericUtils.filterPrefixCodedInts(terms.iterator(null));
+    }
+    
+    @Override
+    public String toString() { 
+      return FieldCache.class.getName()+".NUMERIC_UTILS_INT_PARSER"; 
+    }
+  };
+
+  /**
+   * A parser instance for float values encoded with {@link NumericUtils}, e.g. when indexed
+   * via {@link FloatField}/{@link NumericTokenStream}.
+   */
+  public static final Parser NUMERIC_UTILS_FLOAT_PARSER = new Parser() {
+    @Override
+    public long parseValue(BytesRef term) {
+      int val = NumericUtils.prefixCodedToInt(term);
+      if (val<0) val ^= 0x7fffffff;
+      return val;
+    }
+    
+    @Override
+    public String toString() { 
+      return FieldCache.class.getName()+".NUMERIC_UTILS_FLOAT_PARSER"; 
+    }
+    
+    @Override
+    public TermsEnum termsEnum(Terms terms) throws IOException {
+      return NumericUtils.filterPrefixCodedInts(terms.iterator(null));
+    }
+  };
+
+  /**
+   * A parser instance for long values encoded by {@link NumericUtils}, e.g. when indexed
+   * via {@link LongField}/{@link NumericTokenStream}.
+   */
+  public static final Parser NUMERIC_UTILS_LONG_PARSER = new Parser() {
+    @Override
+    public long parseValue(BytesRef term) {
+      return NumericUtils.prefixCodedToLong(term);
+    }
+    @Override
+    public String toString() { 
+      return FieldCache.class.getName()+".NUMERIC_UTILS_LONG_PARSER"; 
+    }
+    
+    @Override
+    public TermsEnum termsEnum(Terms terms) throws IOException {
+      return NumericUtils.filterPrefixCodedLongs(terms.iterator(null));
+    }
+  };
+
+  /**
+   * A parser instance for double values encoded with {@link NumericUtils}, e.g. when indexed
+   * via {@link DoubleField}/{@link NumericTokenStream}.
+   */
+  public static final Parser NUMERIC_UTILS_DOUBLE_PARSER = new Parser() {
+    @Override
+    public long parseValue(BytesRef term) {
+      long val = NumericUtils.prefixCodedToLong(term);
+      if (val<0) val ^= 0x7fffffffffffffffL;
+      return val;
+    }
+    @Override
+    public String toString() { 
+      return FieldCache.class.getName()+".NUMERIC_UTILS_DOUBLE_PARSER"; 
+    }
+    
+    @Override
+    public TermsEnum termsEnum(Terms terms) throws IOException {
+      return NumericUtils.filterPrefixCodedLongs(terms.iterator(null));
+    }
+  };
+  
+  /** Checks the internal cache for an appropriate entry, and if none is found,
+   *  reads the terms in <code>field</code> and returns a bit set at the size of
+   *  <code>reader.maxDoc()</code>, with turned on bits for each docid that 
+   *  does have a value for this field.
+   */
+  public Bits getDocsWithField(AtomicReader reader, String field) throws IOException;
+
+  /**
+   * Returns a {@link NumericDocValues} over the values found in documents in the given
+   * field. If the field was indexed as {@link NumericDocValuesField}, it simply
+   * uses {@link AtomicReader#getNumericDocValues(String)} to read the values.
+   * Otherwise, it checks the internal cache for an appropriate entry, and if
+   * none is found, reads the terms in <code>field</code> as longs and returns
+   * an array of size <code>reader.maxDoc()</code> of the value each document
+   * has in the given field.
+   * 
+   * @param reader
+   *          Used to get field values.
+   * @param field
+   *          Which field contains the longs.
+   * @param parser
+   *          Computes long for string values. May be {@code null} if the
+   *          requested field was indexed as {@link NumericDocValuesField} or
+   *          {@link LongField}.
+   * @param setDocsWithField
+   *          If true then {@link #getDocsWithField} will also be computed and
+   *          stored in the FieldCache.
+   * @return The values in the given field for each document.
+   * @throws IOException
+   *           If any error occurs.
+   */
+  public NumericDocValues getNumerics(AtomicReader reader, String field, Parser parser, boolean setDocsWithField) throws IOException;
+
+  /** Checks the internal cache for an appropriate entry, and if none
+   * is found, reads the term values in <code>field</code>
+   * and returns a {@link BinaryDocValues} instance, providing a
+   * method to retrieve the term (as a BytesRef) per document.
+   * @param reader  Used to get field values.
+   * @param field   Which field contains the strings.
+   * @param setDocsWithField  If true then {@link #getDocsWithField} will
+   *        also be computed and stored in the FieldCache.
+   * @return The values in the given field for each document.
+   * @throws IOException  If any error occurs.
+   */
+  public BinaryDocValues getTerms(AtomicReader reader, String field, boolean setDocsWithField) throws IOException;
+
+  /** Expert: just like {@link #getTerms(AtomicReader,String,boolean)},
+   *  but you can specify whether more RAM should be consumed in exchange for
+   *  faster lookups (default is "true").  Note that the
+   *  first call for a given reader and field "wins",
+   *  subsequent calls will share the same cache entry. */
+  public BinaryDocValues getTerms(AtomicReader reader, String field, boolean setDocsWithField, float acceptableOverheadRatio) throws IOException;
+
+  /** Checks the internal cache for an appropriate entry, and if none
+   * is found, reads the term values in <code>field</code>
+   * and returns a {@link SortedDocValues} instance,
+   * providing methods to retrieve sort ordinals and terms
+   * (as a ByteRef) per document.
+   * @param reader  Used to get field values.
+   * @param field   Which field contains the strings.
+   * @return The values in the given field for each document.
+   * @throws IOException  If any error occurs.
+   */
+  public SortedDocValues getTermsIndex(AtomicReader reader, String field) throws IOException;
+
+  /** Expert: just like {@link
+   *  #getTermsIndex(AtomicReader,String)}, but you can specify
+   *  whether more RAM should be consumed in exchange for
+   *  faster lookups (default is "true").  Note that the
+   *  first call for a given reader and field "wins",
+   *  subsequent calls will share the same cache entry. */
+  public SortedDocValues getTermsIndex(AtomicReader reader, String field, float acceptableOverheadRatio) throws IOException;
+
+  /** Can be passed to {@link #getDocTermOrds} to filter for 32-bit numeric terms */
+  public static final BytesRef INT32_TERM_PREFIX = new BytesRef(new byte[] { NumericUtils.SHIFT_START_INT });
+  /** Can be passed to {@link #getDocTermOrds} to filter for 64-bit numeric terms */
+  public static final BytesRef INT64_TERM_PREFIX = new BytesRef(new byte[] { NumericUtils.SHIFT_START_LONG });
+  
+  /**
+   * Checks the internal cache for an appropriate entry, and if none is found, reads the term values
+   * in <code>field</code> and returns a {@link DocTermOrds} instance, providing a method to retrieve
+   * the terms (as ords) per document.
+   *
+   * @param reader  Used to build a {@link DocTermOrds} instance
+   * @param field   Which field contains the strings.
+   * @param prefix  prefix for a subset of the terms which should be uninverted. Can be null or
+   *                {@link #INT32_TERM_PREFIX} or {@link #INT64_TERM_PREFIX}
+   *                
+   * @return a {@link DocTermOrds} instance
+   * @throws IOException  If any error occurs.
+   */
+  public SortedSetDocValues getDocTermOrds(AtomicReader reader, String field, BytesRef prefix) throws IOException;
+
+  /**
+   * EXPERT: A unique Identifier/Description for each item in the FieldCache. 
+   * Can be useful for logging/debugging.
+   * @lucene.experimental
+   */
+  public final class CacheEntry {
+
+    private final Object readerKey;
+    private final String fieldName;
+    private final Class<?> cacheType;
+    private final Object custom;
+    private final Object value;
+    private String size;
+
+    public CacheEntry(Object readerKey, String fieldName,
+                      Class<?> cacheType,
+                      Object custom,
+                      Object value) {
+      this.readerKey = readerKey;
+      this.fieldName = fieldName;
+      this.cacheType = cacheType;
+      this.custom = custom;
+      this.value = value;
+    }
+
+    public Object getReaderKey() {
+      return readerKey;
+    }
+
+    public String getFieldName() {
+      return fieldName;
+    }
+
+    public Class<?> getCacheType() {
+      return cacheType;
+    }
+
+    public Object getCustom() {
+      return custom;
+    }
+
+    public Object getValue() {
+      return value;
+    }
+
+    /** 
+     * Computes (and stores) the estimated size of the cache Value 
+     * @see #getEstimatedSize
+     */
+    public void estimateSize() {
+      long bytesUsed = RamUsageEstimator.sizeOf(getValue());
+      size = RamUsageEstimator.humanReadableUnits(bytesUsed);
+    }
+
+    /**
+     * The most recently estimated size of the value, null unless 
+     * estimateSize has been called.
+     */
+    public String getEstimatedSize() {
+      return size;
+    }
+    
+    @Override
+    public String toString() {
+      StringBuilder b = new StringBuilder();
+      b.append("'").append(getReaderKey()).append("'=>");
+      b.append("'").append(getFieldName()).append("',");
+      b.append(getCacheType()).append(",").append(getCustom());
+      b.append("=>").append(getValue().getClass().getName()).append("#");
+      b.append(System.identityHashCode(getValue()));
+      
+      String s = getEstimatedSize();
+      if(null != s) {
+        b.append(" (size =~ ").append(s).append(')');
+      }
+
+      return b.toString();
+    }
+  }
+  
+  /**
+   * EXPERT: Generates an array of CacheEntry objects representing all items 
+   * currently in the FieldCache.
+   * <p>
+   * NOTE: These CacheEntry objects maintain a strong reference to the 
+   * Cached Values.  Maintaining references to a CacheEntry the AtomicIndexReader 
+   * associated with it has garbage collected will prevent the Value itself
+   * from being garbage collected when the Cache drops the WeakReference.
+   * </p>
+   * @lucene.experimental
+   */
+  public CacheEntry[] getCacheEntries();
+
+  /**
+   * <p>
+   * EXPERT: Instructs the FieldCache to forcibly expunge all entries 
+   * from the underlying caches.  This is intended only to be used for 
+   * test methods as a way to ensure a known base state of the Cache 
+   * (with out needing to rely on GC to free WeakReferences).  
+   * It should not be relied on for "Cache maintenance" in general 
+   * application code.
+   * </p>
+   * @lucene.experimental
+   */
+  public void purgeAllCaches();
+
+  /**
+   * Expert: drops all cache entries associated with this
+   * reader {@link IndexReader#getCoreCacheKey}.  NOTE: this cache key must
+   * precisely match the reader that the cache entry is
+   * keyed on. If you pass a top-level reader, it usually
+   * will have no effect as Lucene now caches at the segment
+   * reader level.
+   */
+  public void purgeByCacheKey(Object coreCacheKey);
+
+  /**
+   * If non-null, FieldCacheImpl will warn whenever
+   * entries are created that are not sane according to
+   * {@link FieldCacheSanityChecker}.
+   */
+  public void setInfoStream(PrintStream stream);
+
+  /** counterpart of {@link #setInfoStream(PrintStream)} */
+  public PrintStream getInfoStream();
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheSanityChecker.java lucene5666/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheSanityChecker.java
--- lucene-trunk/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheSanityChecker.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheSanityChecker.java	2014-05-12 13:28:56.496244957 -0400
@@ -0,0 +1,442 @@
+package org.apache.lucene.uninverting;
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexReaderContext;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.uninverting.FieldCache.CacheEntry;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.MapOfSets;
+
+/** 
+ * Provides methods for sanity checking that entries in the FieldCache 
+ * are not wasteful or inconsistent.
+ * </p>
+ * <p>
+ * Lucene 2.9 Introduced numerous enhancements into how the FieldCache 
+ * is used by the low levels of Lucene searching (for Sorting and 
+ * ValueSourceQueries) to improve both the speed for Sorting, as well 
+ * as reopening of IndexReaders.  But these changes have shifted the 
+ * usage of FieldCache from "top level" IndexReaders (frequently a 
+ * MultiReader or DirectoryReader) down to the leaf level SegmentReaders.  
+ * As a result, existing applications that directly access the FieldCache 
+ * may find RAM usage increase significantly when upgrading to 2.9 or 
+ * Later.  This class provides an API for these applications (or their 
+ * Unit tests) to check at run time if the FieldCache contains "insane" 
+ * usages of the FieldCache.
+ * </p>
+ * @lucene.experimental
+ * @see FieldCache
+ * @see FieldCacheSanityChecker.Insanity
+ * @see FieldCacheSanityChecker.InsanityType
+ */
+final class FieldCacheSanityChecker {
+
+  private boolean estimateRam;
+
+  public FieldCacheSanityChecker() {
+    /* NOOP */
+  }
+
+  /**
+   * If set, estimate size for all CacheEntry objects will be calculateed.
+   */
+  public void setRamUsageEstimator(boolean flag) {
+    estimateRam = flag;
+  }
+
+
+  /** 
+   * Quick and dirty convenience method
+   * @see #check
+   */
+  public static Insanity[] checkSanity(FieldCache cache) {
+    return checkSanity(cache.getCacheEntries());
+  }
+
+  /** 
+   * Quick and dirty convenience method that instantiates an instance with 
+   * "good defaults" and uses it to test the CacheEntrys
+   * @see #check
+   */
+  public static Insanity[] checkSanity(CacheEntry... cacheEntries) {
+    FieldCacheSanityChecker sanityChecker = new FieldCacheSanityChecker();
+    sanityChecker.setRamUsageEstimator(true);
+    return sanityChecker.check(cacheEntries);
+  }
+
+
+  /**
+   * Tests a CacheEntry[] for indication of "insane" cache usage.
+   * <p>
+   * <B>NOTE:</b>FieldCache CreationPlaceholder objects are ignored.
+   * (:TODO: is this a bad idea? are we masking a real problem?)
+   * </p>
+   */
+  public Insanity[] check(CacheEntry... cacheEntries) {
+    if (null == cacheEntries || 0 == cacheEntries.length) 
+      return new Insanity[0];
+
+    if (estimateRam) {
+      for (int i = 0; i < cacheEntries.length; i++) {
+        cacheEntries[i].estimateSize();
+      }
+    }
+
+    // the indirect mapping lets MapOfSet dedup identical valIds for us
+    //
+    // maps the (valId) identityhashCode of cache values to 
+    // sets of CacheEntry instances
+    final MapOfSets<Integer, CacheEntry> valIdToItems = new MapOfSets<>(new HashMap<Integer, Set<CacheEntry>>(17));
+    // maps ReaderField keys to Sets of ValueIds
+    final MapOfSets<ReaderField, Integer> readerFieldToValIds = new MapOfSets<>(new HashMap<ReaderField, Set<Integer>>(17));
+    //
+
+    // any keys that we know result in more then one valId
+    final Set<ReaderField> valMismatchKeys = new HashSet<>();
+
+    // iterate over all the cacheEntries to get the mappings we'll need
+    for (int i = 0; i < cacheEntries.length; i++) {
+      final CacheEntry item = cacheEntries[i];
+      final Object val = item.getValue();
+
+      // It's OK to have dup entries, where one is eg
+      // float[] and the other is the Bits (from
+      // getDocWithField())
+      if (val instanceof Bits) {
+        continue;
+      }
+
+      if (val instanceof FieldCache.CreationPlaceholder)
+        continue;
+
+      final ReaderField rf = new ReaderField(item.getReaderKey(), 
+                                            item.getFieldName());
+
+      final Integer valId = Integer.valueOf(System.identityHashCode(val));
+
+      // indirect mapping, so the MapOfSet will dedup identical valIds for us
+      valIdToItems.put(valId, item);
+      if (1 < readerFieldToValIds.put(rf, valId)) {
+        valMismatchKeys.add(rf);
+      }
+    }
+
+    final List<Insanity> insanity = new ArrayList<>(valMismatchKeys.size() * 3);
+
+    insanity.addAll(checkValueMismatch(valIdToItems, 
+                                       readerFieldToValIds, 
+                                       valMismatchKeys));
+    insanity.addAll(checkSubreaders(valIdToItems, 
+                                    readerFieldToValIds));
+                    
+    return insanity.toArray(new Insanity[insanity.size()]);
+  }
+
+  /** 
+   * Internal helper method used by check that iterates over 
+   * valMismatchKeys and generates a Collection of Insanity 
+   * instances accordingly.  The MapOfSets are used to populate 
+   * the Insanity objects. 
+   * @see InsanityType#VALUEMISMATCH
+   */
+  private Collection<Insanity> checkValueMismatch(MapOfSets<Integer, CacheEntry> valIdToItems,
+                                        MapOfSets<ReaderField, Integer> readerFieldToValIds,
+                                        Set<ReaderField> valMismatchKeys) {
+
+    final List<Insanity> insanity = new ArrayList<>(valMismatchKeys.size() * 3);
+
+    if (! valMismatchKeys.isEmpty() ) { 
+      // we have multiple values for some ReaderFields
+
+      final Map<ReaderField, Set<Integer>> rfMap = readerFieldToValIds.getMap();
+      final Map<Integer, Set<CacheEntry>> valMap = valIdToItems.getMap();
+      for (final ReaderField rf : valMismatchKeys) {
+        final List<CacheEntry> badEntries = new ArrayList<>(valMismatchKeys.size() * 2);
+        for(final Integer value: rfMap.get(rf)) {
+          for (final CacheEntry cacheEntry : valMap.get(value)) {
+            badEntries.add(cacheEntry);
+          }
+        }
+
+        CacheEntry[] badness = new CacheEntry[badEntries.size()];
+        badness = badEntries.toArray(badness);
+
+        insanity.add(new Insanity(InsanityType.VALUEMISMATCH,
+                                  "Multiple distinct value objects for " + 
+                                  rf.toString(), badness));
+      }
+    }
+    return insanity;
+  }
+
+  /** 
+   * Internal helper method used by check that iterates over 
+   * the keys of readerFieldToValIds and generates a Collection 
+   * of Insanity instances whenever two (or more) ReaderField instances are 
+   * found that have an ancestry relationships.  
+   *
+   * @see InsanityType#SUBREADER
+   */
+  private Collection<Insanity> checkSubreaders( MapOfSets<Integer, CacheEntry>  valIdToItems,
+                                      MapOfSets<ReaderField, Integer> readerFieldToValIds) {
+
+    final List<Insanity> insanity = new ArrayList<>(23);
+
+    Map<ReaderField, Set<ReaderField>> badChildren = new HashMap<>(17);
+    MapOfSets<ReaderField, ReaderField> badKids = new MapOfSets<>(badChildren); // wrapper
+
+    Map<Integer, Set<CacheEntry>> viToItemSets = valIdToItems.getMap();
+    Map<ReaderField, Set<Integer>> rfToValIdSets = readerFieldToValIds.getMap();
+
+    Set<ReaderField> seen = new HashSet<>(17);
+
+    Set<ReaderField> readerFields = rfToValIdSets.keySet();
+    for (final ReaderField rf : readerFields) {
+      
+      if (seen.contains(rf)) continue;
+
+      List<Object> kids = getAllDescendantReaderKeys(rf.readerKey);
+      for (Object kidKey : kids) {
+        ReaderField kid = new ReaderField(kidKey, rf.fieldName);
+        
+        if (badChildren.containsKey(kid)) {
+          // we've already process this kid as RF and found other problems
+          // track those problems as our own
+          badKids.put(rf, kid);
+          badKids.putAll(rf, badChildren.get(kid));
+          badChildren.remove(kid);
+          
+        } else if (rfToValIdSets.containsKey(kid)) {
+          // we have cache entries for the kid
+          badKids.put(rf, kid);
+        }
+        seen.add(kid);
+      }
+      seen.add(rf);
+    }
+
+    // every mapping in badKids represents an Insanity
+    for (final ReaderField parent : badChildren.keySet()) {
+      Set<ReaderField> kids = badChildren.get(parent);
+
+      List<CacheEntry> badEntries = new ArrayList<>(kids.size() * 2);
+
+      // put parent entr(ies) in first
+      {
+        for (final Integer value  : rfToValIdSets.get(parent)) {
+          badEntries.addAll(viToItemSets.get(value));
+        }
+      }
+
+      // now the entries for the descendants
+      for (final ReaderField kid : kids) {
+        for (final Integer value : rfToValIdSets.get(kid)) {
+          badEntries.addAll(viToItemSets.get(value));
+        }
+      }
+
+      CacheEntry[] badness = new CacheEntry[badEntries.size()];
+      badness = badEntries.toArray(badness);
+
+      insanity.add(new Insanity(InsanityType.SUBREADER,
+                                "Found caches for descendants of " + 
+                                parent.toString(),
+                                badness));
+    }
+
+    return insanity;
+
+  }
+
+  /**
+   * Checks if the seed is an IndexReader, and if so will walk
+   * the hierarchy of subReaders building up a list of the objects 
+   * returned by {@code seed.getCoreCacheKey()}
+   */
+  private List<Object> getAllDescendantReaderKeys(Object seed) {
+    List<Object> all = new ArrayList<>(17); // will grow as we iter
+    all.add(seed);
+    for (int i = 0; i < all.size(); i++) {
+      final Object obj = all.get(i);
+      // TODO: We don't check closed readers here (as getTopReaderContext
+      // throws AlreadyClosedException), what should we do? Reflection?
+      if (obj instanceof IndexReader) {
+        try {
+          final List<IndexReaderContext> childs =
+            ((IndexReader) obj).getContext().children();
+          if (childs != null) { // it is composite reader
+            for (final IndexReaderContext ctx : childs) {
+              all.add(ctx.reader().getCoreCacheKey());
+            }
+          }
+        } catch (AlreadyClosedException ace) {
+          // ignore this reader
+        }
+      }
+    }
+    // need to skip the first, because it was the seed
+    return all.subList(1, all.size());
+  }
+
+  /**
+   * Simple pair object for using "readerKey + fieldName" a Map key
+   */
+  private final static class ReaderField {
+    public final Object readerKey;
+    public final String fieldName;
+    public ReaderField(Object readerKey, String fieldName) {
+      this.readerKey = readerKey;
+      this.fieldName = fieldName;
+    }
+    @Override
+    public int hashCode() {
+      return System.identityHashCode(readerKey) * fieldName.hashCode();
+    }
+    @Override
+    public boolean equals(Object that) {
+      if (! (that instanceof ReaderField)) return false;
+
+      ReaderField other = (ReaderField) that;
+      return (this.readerKey == other.readerKey &&
+              this.fieldName.equals(other.fieldName));
+    }
+    @Override
+    public String toString() {
+      return readerKey.toString() + "+" + fieldName;
+    }
+  }
+
+  /**
+   * Simple container for a collection of related CacheEntry objects that 
+   * in conjunction with each other represent some "insane" usage of the 
+   * FieldCache.
+   */
+  public final static class Insanity {
+    private final InsanityType type;
+    private final String msg;
+    private final CacheEntry[] entries;
+    public Insanity(InsanityType type, String msg, CacheEntry... entries) {
+      if (null == type) {
+        throw new IllegalArgumentException
+          ("Insanity requires non-null InsanityType");
+      }
+      if (null == entries || 0 == entries.length) {
+        throw new IllegalArgumentException
+          ("Insanity requires non-null/non-empty CacheEntry[]");
+      }
+      this.type = type;
+      this.msg = msg;
+      this.entries = entries;
+      
+    }
+    /**
+     * Type of insane behavior this object represents
+     */
+    public InsanityType getType() { return type; }
+    /**
+     * Description of hte insane behavior
+     */
+    public String getMsg() { return msg; }
+    /**
+     * CacheEntry objects which suggest a problem
+     */
+    public CacheEntry[] getCacheEntries() { return entries; }
+    /**
+     * Multi-Line representation of this Insanity object, starting with 
+     * the Type and Msg, followed by each CacheEntry.toString() on it's 
+     * own line prefaced by a tab character
+     */
+    @Override
+    public String toString() {
+      StringBuilder buf = new StringBuilder();
+      buf.append(getType()).append(": ");
+
+      String m = getMsg();
+      if (null != m) buf.append(m);
+
+      buf.append('\n');
+
+      CacheEntry[] ce = getCacheEntries();
+      for (int i = 0; i < ce.length; i++) {
+        buf.append('\t').append(ce[i].toString()).append('\n');
+      }
+
+      return buf.toString();
+    }
+  }
+
+  /**
+   * An Enumeration of the different types of "insane" behavior that 
+   * may be detected in a FieldCache.
+   *
+   * @see InsanityType#SUBREADER
+   * @see InsanityType#VALUEMISMATCH
+   * @see InsanityType#EXPECTED
+   */
+  public final static class InsanityType {
+    private final String label;
+    private InsanityType(final String label) {
+      this.label = label;
+    }
+    @Override
+    public String toString() { return label; }
+
+    /** 
+     * Indicates an overlap in cache usage on a given field 
+     * in sub/super readers.
+     */
+    public final static InsanityType SUBREADER 
+      = new InsanityType("SUBREADER");
+
+    /** 
+     * <p>
+     * Indicates entries have the same reader+fieldname but 
+     * different cached values.  This can happen if different datatypes, 
+     * or parsers are used -- and while it's not necessarily a bug 
+     * it's typically an indication of a possible problem.
+     * </p>
+     * <p>
+     * <b>NOTE:</b> Only the reader, fieldname, and cached value are actually 
+     * tested -- if two cache entries have different parsers or datatypes but 
+     * the cached values are the same Object (== not just equal()) this method 
+     * does not consider that a red flag.  This allows for subtle variations 
+     * in the way a Parser is specified (null vs DEFAULT_LONG_PARSER, etc...)
+     * </p>
+     */
+    public final static InsanityType VALUEMISMATCH 
+      = new InsanityType("VALUEMISMATCH");
+
+    /** 
+     * Indicates an expected bit of "insanity".  This may be useful for 
+     * clients that wish to preserve/log information about insane usage 
+     * but indicate that it was expected. 
+     */
+    public final static InsanityType EXPECTED
+      = new InsanityType("EXPECTED");
+  }
+  
+  
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/java/org/apache/lucene/uninverting/package.html lucene5666/lucene/misc/src/java/org/apache/lucene/uninverting/package.html
--- lucene-trunk/lucene/misc/src/java/org/apache/lucene/uninverting/package.html	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/misc/src/java/org/apache/lucene/uninverting/package.html	2014-05-12 13:28:56.496244957 -0400
@@ -0,0 +1,21 @@
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<body>
+Support for creating docvalues on-the-fly from the inverted index at runtime.
+</body>
+</html>
\ No newline at end of file


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/java/org/apache/lucene/uninverting/UninvertingReader.java lucene5666/lucene/misc/src/java/org/apache/lucene/uninverting/UninvertingReader.java
--- lucene-trunk/lucene/misc/src/java/org/apache/lucene/uninverting/UninvertingReader.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/misc/src/java/org/apache/lucene/uninverting/UninvertingReader.java	2014-05-14 03:45:22.014644416 -0400
@@ -0,0 +1,326 @@
+package org.apache.lucene.uninverting;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Map;
+
+import org.apache.lucene.document.IntField; // javadocs
+import org.apache.lucene.document.LongField; // javadocs
+import org.apache.lucene.document.FloatField; // javadocs
+import org.apache.lucene.document.DoubleField; // javadocs
+import org.apache.lucene.document.BinaryDocValuesField; // javadocs
+import org.apache.lucene.document.NumericDocValuesField; // javadocs
+import org.apache.lucene.document.SortedDocValuesField; // javadocs
+import org.apache.lucene.document.SortedSetDocValuesField; // javadocs
+import org.apache.lucene.document.StringField; // javadocs
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.FilterAtomicReader;
+import org.apache.lucene.index.FilterDirectoryReader;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.uninverting.FieldCache.CacheEntry;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.NumericUtils;
+
+/**
+ * A FilterReader that exposes <i>indexed</i> values as if they also had
+ * docvalues.
+ * <p>
+ * This is accomplished by "inverting the inverted index" or "uninversion".
+ * <p>
+ * The uninversion process happens lazily: upon the first request for the 
+ * field's docvalues (e.g. via {@link AtomicReader#getNumericDocValues(String)} 
+ * or similar), it will create the docvalues on-the-fly if needed and cache it,
+ * based on the core cache key of the wrapped AtomicReader.
+ */
+public class UninvertingReader extends FilterAtomicReader {
+  
+  /**
+   * Specifies the type of uninversion to apply for the field. 
+   */
+  public static enum Type {
+    /** 
+     * Single-valued Integer, (e.g. indexed with {@link IntField})
+     * <p>
+     * Fields with this type act as if they were indexed with
+     * {@link NumericDocValuesField}.
+     */
+    INTEGER,
+    /** 
+     * Single-valued Long, (e.g. indexed with {@link LongField}) 
+     * <p>
+     * Fields with this type act as if they were indexed with
+     * {@link NumericDocValuesField}.
+     */
+    LONG,
+    /** 
+     * Single-valued Float, (e.g. indexed with {@link FloatField}) 
+     * <p>
+     * Fields with this type act as if they were indexed with
+     * {@link NumericDocValuesField}.
+     */
+    FLOAT,
+    /** 
+     * Single-valued Double, (e.g. indexed with {@link DoubleField}) 
+     * <p>
+     * Fields with this type act as if they were indexed with
+     * {@link NumericDocValuesField}.
+     */
+    DOUBLE,
+    /** 
+     * Single-valued Binary, (e.g. indexed with {@link StringField}) 
+     * <p>
+     * Fields with this type act as if they were indexed with
+     * {@link BinaryDocValuesField}.
+     */
+    BINARY,
+    /** 
+     * Single-valued Binary, (e.g. indexed with {@link StringField}) 
+     * <p>
+     * Fields with this type act as if they were indexed with
+     * {@link SortedDocValuesField}.
+     */
+    SORTED,
+    /** 
+     * Multi-valued Binary, (e.g. indexed with {@link StringField}) 
+     * <p>
+     * Fields with this type act as if they were indexed with
+     * {@link SortedSetDocValuesField}.
+     */
+    SORTED_SET_BINARY,
+    /** 
+     * Multi-valued Integer, (e.g. indexed with {@link IntField}) 
+     * <p>
+     * Fields with this type act as if they were indexed with
+     * {@link SortedSetDocValuesField}.
+     */
+    SORTED_SET_INTEGER,
+    /** 
+     * Multi-valued Float, (e.g. indexed with {@link FloatField}) 
+     * <p>
+     * Fields with this type act as if they were indexed with
+     * {@link SortedSetDocValuesField}.
+     */
+    SORTED_SET_FLOAT,
+    /** 
+     * Multi-valued Long, (e.g. indexed with {@link LongField}) 
+     * <p>
+     * Fields with this type act as if they were indexed with
+     * {@link SortedSetDocValuesField}.
+     */
+    SORTED_SET_LONG,
+    /** 
+     * Multi-valued Double, (e.g. indexed with {@link DoubleField}) 
+     * <p>
+     * Fields with this type act as if they were indexed with
+     * {@link SortedSetDocValuesField}.
+     */
+    SORTED_SET_DOUBLE
+  }
+  
+  /**
+   * Wraps a provided DirectoryReader. Note that for convenience, the returned reader
+   * can be used normally (e.g. passed to {@link DirectoryReader#openIfChanged(DirectoryReader)})
+   * and so on. 
+   */
+  public static DirectoryReader wrap(DirectoryReader in, final Map<String,Type> mapping) {
+    return new UninvertingDirectoryReader(in, mapping);
+  }
+  
+  static class UninvertingDirectoryReader extends FilterDirectoryReader {
+    final Map<String,Type> mapping;
+    
+    public UninvertingDirectoryReader(DirectoryReader in, final Map<String,Type> mapping) {
+      super(in, new FilterDirectoryReader.SubReaderWrapper() {
+        @Override
+        public AtomicReader wrap(AtomicReader reader) {
+          return new UninvertingReader(reader, mapping);
+        }
+      });
+      this.mapping = mapping;
+    }
+
+    @Override
+    protected DirectoryReader doWrapDirectoryReader(DirectoryReader in) {
+      return new UninvertingDirectoryReader(in, mapping);
+    }
+  }
+  
+  final Map<String,Type> mapping;
+  final FieldInfos fieldInfos;
+  
+  /** 
+   * Create a new UninvertingReader with the specified mapping 
+   * <p>
+   * Expert: This should almost never be used. Use {@link #wrap(DirectoryReader, Map)}
+   * instead.
+   *  
+   * @lucene.internal
+   */
+  public UninvertingReader(AtomicReader in, Map<String,Type> mapping) {
+    super(in);
+    this.mapping = mapping;
+    ArrayList<FieldInfo> filteredInfos = new ArrayList<>();
+    for (FieldInfo fi : in.getFieldInfos()) {
+      FieldInfo.DocValuesType type = fi.getDocValuesType();
+      if (fi.isIndexed() && !fi.hasDocValues()) {
+        Type t = mapping.get(fi.name);
+        if (t != null) {
+          switch(t) {
+            case INTEGER:
+            case LONG:
+            case FLOAT:
+            case DOUBLE:
+              type = FieldInfo.DocValuesType.NUMERIC;
+              break;
+            case BINARY:
+              type = FieldInfo.DocValuesType.BINARY;
+              break;
+            case SORTED:
+              type = FieldInfo.DocValuesType.SORTED;
+              break;
+            case SORTED_SET_BINARY:
+            case SORTED_SET_INTEGER:
+            case SORTED_SET_FLOAT:
+            case SORTED_SET_LONG:
+            case SORTED_SET_DOUBLE:
+              type = FieldInfo.DocValuesType.SORTED_SET;
+              break;
+            default:
+              throw new AssertionError();
+          }
+        }
+      }
+      filteredInfos.add(new FieldInfo(fi.name, fi.isIndexed(), fi.number, fi.hasVectors(), fi.omitsNorms(),
+                                      fi.hasPayloads(), fi.getIndexOptions(), type, fi.getNormType(), null));
+    }
+    fieldInfos = new FieldInfos(filteredInfos.toArray(new FieldInfo[filteredInfos.size()]));
+  }
+
+  @Override
+  public FieldInfos getFieldInfos() {
+    return fieldInfos;
+  }
+
+  @Override
+  public NumericDocValues getNumericDocValues(String field) throws IOException {
+    Type v = mapping.get(field);
+    if (v != null) {
+      switch (v) {
+        case INTEGER: return FieldCache.DEFAULT.getNumerics(in, field, FieldCache.NUMERIC_UTILS_INT_PARSER, true);
+        case FLOAT: return FieldCache.DEFAULT.getNumerics(in, field, FieldCache.NUMERIC_UTILS_FLOAT_PARSER, true);
+        case LONG: return FieldCache.DEFAULT.getNumerics(in, field, FieldCache.NUMERIC_UTILS_LONG_PARSER, true);
+        case DOUBLE: return FieldCache.DEFAULT.getNumerics(in, field, FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, true);
+        default:
+          throw new IllegalStateException("Type mismatch: " + field + " was indexed as " + v);
+      }
+    }
+    return super.getNumericDocValues(field);
+  }
+
+  @Override
+  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
+    Type v = mapping.get(field);
+    if (v == Type.BINARY) {
+      return FieldCache.DEFAULT.getTerms(in, field, true);
+    } else if (v != null && v != Type.SORTED) {
+      throw new IllegalStateException("Type mismatch: " + field + " was indexed as " + v);
+    } else {
+      return in.getBinaryDocValues(field);
+    }
+  }
+
+  @Override
+  public SortedDocValues getSortedDocValues(String field) throws IOException {
+    Type v = mapping.get(field);
+    if (v == Type.SORTED) {
+      return FieldCache.DEFAULT.getTermsIndex(in, field);
+    } else if (v != null) {
+      throw new IllegalStateException("Type mismatch: " + field + " was indexed as " + v);
+    } else {
+      return in.getSortedDocValues(field);
+    }
+  }
+  
+  @Override
+  public SortedSetDocValues getSortedSetDocValues(String field) throws IOException {
+    Type v = mapping.get(field);
+    if (v != null) {
+      switch (v) {
+        case SORTED_SET_INTEGER:
+        case SORTED_SET_FLOAT: 
+          return FieldCache.DEFAULT.getDocTermOrds(in, field, FieldCache.INT32_TERM_PREFIX);
+        case SORTED_SET_LONG:
+        case SORTED_SET_DOUBLE:
+          return FieldCache.DEFAULT.getDocTermOrds(in, field, FieldCache.INT64_TERM_PREFIX);
+        case SORTED_SET_BINARY:
+          return FieldCache.DEFAULT.getDocTermOrds(in, field, null);
+        default:
+          if (v != Type.SORTED) {
+            throw new IllegalStateException("Type mismatch: " + field + " was indexed as " + v);
+          }
+      }
+    }
+    return in.getSortedSetDocValues(field);
+  }
+
+  @Override
+  public Bits getDocsWithField(String field) throws IOException {
+    if (mapping.containsKey(field)) {
+      return FieldCache.DEFAULT.getDocsWithField(in, field);
+    } else {
+      return in.getDocsWithField(field);
+    }
+  }
+
+  @Override
+  public Object getCoreCacheKey() {
+    return in.getCoreCacheKey();
+  }
+
+  @Override
+  public Object getCombinedCoreAndDeletesKey() {
+    return in.getCombinedCoreAndDeletesKey();
+  }
+
+  @Override
+  public String toString() {
+    return "Uninverting(" + in.toString() + ")";
+  }
+  
+  /** 
+   * Return information about the backing cache
+   * @lucene.internal 
+   */
+  public static String[] getUninvertedStats() {
+    CacheEntry[] entries = FieldCache.DEFAULT.getCacheEntries();
+    String[] info = new String[entries.length];
+    for (int i = 0; i < entries.length; i++) {
+      info[i] = entries[i].toString();
+    }
+    return info;
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestDocTermOrds.java lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestDocTermOrds.java
--- lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestDocTermOrds.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestDocTermOrds.java	2014-05-12 17:18:32.608484861 -0400
@@ -0,0 +1,643 @@
+package org.apache.lucene.uninverting;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.List;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.LongField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.TermsEnum.SeekStatus;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.NumericUtils;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.TestUtil;
+
+// TODO:
+//   - test w/ del docs
+//   - test prefix
+//   - test w/ cutoff
+//   - crank docs way up so we get some merging sometimes
+
+public class TestDocTermOrds extends LuceneTestCase {
+
+  public void testSimple() throws Exception {
+    Directory dir = newDirectory();
+    final RandomIndexWriter w = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
+    Document doc = new Document();
+    Field field = newTextField("field", "", Field.Store.NO);
+    doc.add(field);
+    field.setStringValue("a b c");
+    w.addDocument(doc);
+
+    field.setStringValue("d e f");
+    w.addDocument(doc);
+
+    field.setStringValue("a f");
+    w.addDocument(doc);
+    
+    final IndexReader r = w.getReader();
+    w.shutdown();
+
+    final AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);
+    final DocTermOrds dto = new DocTermOrds(ar, ar.getLiveDocs(), "field");
+    SortedSetDocValues iter = dto.iterator(ar);
+    
+    iter.setDocument(0);
+    assertEquals(0, iter.nextOrd());
+    assertEquals(1, iter.nextOrd());
+    assertEquals(2, iter.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, iter.nextOrd());
+    
+    iter.setDocument(1);
+    assertEquals(3, iter.nextOrd());
+    assertEquals(4, iter.nextOrd());
+    assertEquals(5, iter.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, iter.nextOrd());
+
+    iter.setDocument(2);
+    assertEquals(0, iter.nextOrd());
+    assertEquals(5, iter.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, iter.nextOrd());
+
+    r.close();
+    dir.close();
+  }
+
+  public void testRandom() throws Exception {
+    Directory dir = newDirectory();
+
+    final int NUM_TERMS = atLeast(20);
+    final Set<BytesRef> terms = new HashSet<>();
+    while(terms.size() < NUM_TERMS) {
+      final String s = TestUtil.randomRealisticUnicodeString(random());
+      //final String s = _TestUtil.randomSimpleString(random);
+      if (s.length() > 0) {
+        terms.add(new BytesRef(s));
+      }
+    }
+    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);
+    Arrays.sort(termsArray);
+    
+    final int NUM_DOCS = atLeast(100);
+
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+
+    // Sometimes swap in codec that impls ord():
+    if (random().nextInt(10) == 7) {
+      // Make sure terms index has ords:
+      Codec codec = TestUtil.alwaysPostingsFormat(PostingsFormat.forName("Lucene41WithOrds"));
+      conf.setCodec(codec);
+    }
+    
+    final RandomIndexWriter w = new RandomIndexWriter(random(), dir, conf);
+
+    final int[][] idToOrds = new int[NUM_DOCS][];
+    final Set<Integer> ordsForDocSet = new HashSet<>();
+
+    for(int id=0;id<NUM_DOCS;id++) {
+      Document doc = new Document();
+
+      doc.add(new IntField("id", id, Field.Store.YES));
+      
+      final int termCount = TestUtil.nextInt(random(), 0, 20 * RANDOM_MULTIPLIER);
+      while(ordsForDocSet.size() < termCount) {
+        ordsForDocSet.add(random().nextInt(termsArray.length));
+      }
+      final int[] ordsForDoc = new int[termCount];
+      int upto = 0;
+      if (VERBOSE) {
+        System.out.println("TEST: doc id=" + id);
+      }
+      for(int ord : ordsForDocSet) {
+        ordsForDoc[upto++] = ord;
+        Field field = newStringField("field", termsArray[ord].utf8ToString(), Field.Store.NO);
+        if (VERBOSE) {
+          System.out.println("  f=" + termsArray[ord].utf8ToString());
+        }
+        doc.add(field);
+      }
+      ordsForDocSet.clear();
+      Arrays.sort(ordsForDoc);
+      idToOrds[id] = ordsForDoc;
+      w.addDocument(doc);
+    }
+    
+    final DirectoryReader r = w.getReader();
+    w.shutdown();
+
+    if (VERBOSE) {
+      System.out.println("TEST: reader=" + r);
+    }
+
+    for(AtomicReaderContext ctx : r.leaves()) {
+      if (VERBOSE) {
+        System.out.println("\nTEST: sub=" + ctx.reader());
+      }
+      verify(ctx.reader(), idToOrds, termsArray, null);
+    }
+
+    // Also test top-level reader: its enum does not support
+    // ord, so this forces the OrdWrapper to run:
+    if (VERBOSE) {
+      System.out.println("TEST: top reader");
+    }
+    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(r);
+    verify(slowR, idToOrds, termsArray, null);
+
+    FieldCache.DEFAULT.purgeByCacheKey(slowR.getCoreCacheKey());
+
+    r.close();
+    dir.close();
+  }
+
+  public void testRandomWithPrefix() throws Exception {
+    Directory dir = newDirectory();
+
+    final Set<String> prefixes = new HashSet<>();
+    final int numPrefix = TestUtil.nextInt(random(), 2, 7);
+    if (VERBOSE) {
+      System.out.println("TEST: use " + numPrefix + " prefixes");
+    }
+    while(prefixes.size() < numPrefix) {
+      prefixes.add(TestUtil.randomRealisticUnicodeString(random()));
+      //prefixes.add(_TestUtil.randomSimpleString(random));
+    }
+    final String[] prefixesArray = prefixes.toArray(new String[prefixes.size()]);
+
+    final int NUM_TERMS = atLeast(20);
+    final Set<BytesRef> terms = new HashSet<>();
+    while(terms.size() < NUM_TERMS) {
+      final String s = prefixesArray[random().nextInt(prefixesArray.length)] + TestUtil.randomRealisticUnicodeString(random());
+      //final String s = prefixesArray[random.nextInt(prefixesArray.length)] + _TestUtil.randomSimpleString(random);
+      if (s.length() > 0) {
+        terms.add(new BytesRef(s));
+      }
+    }
+    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);
+    Arrays.sort(termsArray);
+    
+    final int NUM_DOCS = atLeast(100);
+
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+
+    // Sometimes swap in codec that impls ord():
+    if (random().nextInt(10) == 7) {
+      Codec codec = TestUtil.alwaysPostingsFormat(PostingsFormat.forName("Lucene41WithOrds"));
+      conf.setCodec(codec);
+    }
+    
+    final RandomIndexWriter w = new RandomIndexWriter(random(), dir, conf);
+
+    final int[][] idToOrds = new int[NUM_DOCS][];
+    final Set<Integer> ordsForDocSet = new HashSet<>();
+
+    for(int id=0;id<NUM_DOCS;id++) {
+      Document doc = new Document();
+
+      doc.add(new IntField("id", id, Field.Store.YES));
+      
+      final int termCount = TestUtil.nextInt(random(), 0, 20 * RANDOM_MULTIPLIER);
+      while(ordsForDocSet.size() < termCount) {
+        ordsForDocSet.add(random().nextInt(termsArray.length));
+      }
+      final int[] ordsForDoc = new int[termCount];
+      int upto = 0;
+      if (VERBOSE) {
+        System.out.println("TEST: doc id=" + id);
+      }
+      for(int ord : ordsForDocSet) {
+        ordsForDoc[upto++] = ord;
+        Field field = newStringField("field", termsArray[ord].utf8ToString(), Field.Store.NO);
+        if (VERBOSE) {
+          System.out.println("  f=" + termsArray[ord].utf8ToString());
+        }
+        doc.add(field);
+      }
+      ordsForDocSet.clear();
+      Arrays.sort(ordsForDoc);
+      idToOrds[id] = ordsForDoc;
+      w.addDocument(doc);
+    }
+    
+    final DirectoryReader r = w.getReader();
+    w.shutdown();
+
+    if (VERBOSE) {
+      System.out.println("TEST: reader=" + r);
+    }
+    
+    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(r);
+    for(String prefix : prefixesArray) {
+
+      final BytesRef prefixRef = prefix == null ? null : new BytesRef(prefix);
+
+      final int[][] idToOrdsPrefix = new int[NUM_DOCS][];
+      for(int id=0;id<NUM_DOCS;id++) {
+        final int[] docOrds = idToOrds[id];
+        final List<Integer> newOrds = new ArrayList<>();
+        for(int ord : idToOrds[id]) {
+          if (StringHelper.startsWith(termsArray[ord], prefixRef)) {
+            newOrds.add(ord);
+          }
+        }
+        final int[] newOrdsArray = new int[newOrds.size()];
+        int upto = 0;
+        for(int ord : newOrds) {
+          newOrdsArray[upto++] = ord;
+        }
+        idToOrdsPrefix[id] = newOrdsArray;
+      }
+
+      for(AtomicReaderContext ctx : r.leaves()) {
+        if (VERBOSE) {
+          System.out.println("\nTEST: sub=" + ctx.reader());
+        }
+        verify(ctx.reader(), idToOrdsPrefix, termsArray, prefixRef);
+      }
+
+      // Also test top-level reader: its enum does not support
+      // ord, so this forces the OrdWrapper to run:
+      if (VERBOSE) {
+        System.out.println("TEST: top reader");
+      }
+      verify(slowR, idToOrdsPrefix, termsArray, prefixRef);
+    }
+
+    FieldCache.DEFAULT.purgeByCacheKey(slowR.getCoreCacheKey());
+
+    r.close();
+    dir.close();
+  }
+
+  private void verify(AtomicReader r, int[][] idToOrds, BytesRef[] termsArray, BytesRef prefixRef) throws Exception {
+
+    final DocTermOrds dto = new DocTermOrds(r, r.getLiveDocs(),
+                                            "field",
+                                            prefixRef,
+                                            Integer.MAX_VALUE,
+                                            TestUtil.nextInt(random(), 2, 10));
+                                            
+
+    final NumericDocValues docIDToID = FieldCache.DEFAULT.getNumerics(r, "id", FieldCache.NUMERIC_UTILS_INT_PARSER, false);
+    /*
+      for(int docID=0;docID<subR.maxDoc();docID++) {
+      System.out.println("  docID=" + docID + " id=" + docIDToID[docID]);
+      }
+    */
+
+    if (VERBOSE) {
+      System.out.println("TEST: verify prefix=" + (prefixRef==null ? "null" : prefixRef.utf8ToString()));
+      System.out.println("TEST: all TERMS:");
+      TermsEnum allTE = MultiFields.getTerms(r, "field").iterator(null);
+      int ord = 0;
+      while(allTE.next() != null) {
+        System.out.println("  ord=" + (ord++) + " term=" + allTE.term().utf8ToString());
+      }
+    }
+
+    //final TermsEnum te = subR.fields().terms("field").iterator();
+    final TermsEnum te = dto.getOrdTermsEnum(r);
+    if (dto.numTerms() == 0) {
+      if (prefixRef == null) {
+        assertNull(MultiFields.getTerms(r, "field"));
+      } else {
+        Terms terms = MultiFields.getTerms(r, "field");
+        if (terms != null) {
+          TermsEnum termsEnum = terms.iterator(null);
+          TermsEnum.SeekStatus result = termsEnum.seekCeil(prefixRef);
+          if (result != TermsEnum.SeekStatus.END) {
+            assertFalse("term=" + termsEnum.term().utf8ToString() + " matches prefix=" + prefixRef.utf8ToString(), StringHelper.startsWith(termsEnum.term(), prefixRef));
+          } else {
+            // ok
+          }
+        } else {
+          // ok
+        }
+      }
+      return;
+    }
+
+    if (VERBOSE) {
+      System.out.println("TEST: TERMS:");
+      te.seekExact(0);
+      while(true) {
+        System.out.println("  ord=" + te.ord() + " term=" + te.term().utf8ToString());
+        if (te.next() == null) {
+          break;
+        }
+      }
+    }
+
+    SortedSetDocValues iter = dto.iterator(r);
+    for(int docID=0;docID<r.maxDoc();docID++) {
+      if (VERBOSE) {
+        System.out.println("TEST: docID=" + docID + " of " + r.maxDoc() + " (id=" + docIDToID.get(docID) + ")");
+      }
+      iter.setDocument(docID);
+      final int[] answers = idToOrds[(int) docIDToID.get(docID)];
+      int upto = 0;
+      long ord;
+      while ((ord = iter.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {
+        te.seekExact(ord);
+        final BytesRef expected = termsArray[answers[upto++]];
+        if (VERBOSE) {
+          System.out.println("  exp=" + expected.utf8ToString() + " actual=" + te.term().utf8ToString());
+        }
+        assertEquals("expected=" + expected.utf8ToString() + " actual=" + te.term().utf8ToString() + " ord=" + ord, expected, te.term());
+      }
+      assertEquals(answers.length, upto);
+    }
+  }
+  
+  public void testBackToTheFuture() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
+    
+    Document doc = new Document();
+    doc.add(newStringField("foo", "bar", Field.Store.NO));
+    iw.addDocument(doc);
+    
+    doc = new Document();
+    doc.add(newStringField("foo", "baz", Field.Store.NO));
+    // we need a second value for a doc, or we don't actually test DocTermOrds!
+    doc.add(newStringField("foo", "car", Field.Store.NO));
+    iw.addDocument(doc);
+    
+    DirectoryReader r1 = DirectoryReader.open(iw, true);
+    
+    iw.deleteDocuments(new Term("foo", "baz"));
+    DirectoryReader r2 = DirectoryReader.open(iw, true);
+    
+    FieldCache.DEFAULT.getDocTermOrds(getOnlySegmentReader(r2), "foo", null);
+    
+    SortedSetDocValues v = FieldCache.DEFAULT.getDocTermOrds(getOnlySegmentReader(r1), "foo", null);
+    assertEquals(3, v.getValueCount());
+    v.setDocument(1);
+    assertEquals(1, v.nextOrd());
+    
+    iw.shutdown();
+    r1.close();
+    r2.close();
+    dir.close();
+  }
+  
+  public void testNumericEncoded32() throws IOException {
+    Directory dir = newDirectory();
+    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
+    
+    Document doc = new Document();
+    doc.add(new IntField("foo", 5, Field.Store.NO));
+    iw.addDocument(doc);
+    
+    doc = new Document();
+    doc.add(new IntField("foo", 5, Field.Store.NO));
+    doc.add(new IntField("foo", -3, Field.Store.NO));
+    iw.addDocument(doc);
+    
+    iw.forceMerge(1);
+    iw.shutdown();
+    
+    DirectoryReader ir = DirectoryReader.open(dir);
+    AtomicReader ar = getOnlySegmentReader(ir);
+    
+    SortedSetDocValues v = FieldCache.DEFAULT.getDocTermOrds(ar, "foo", FieldCache.INT32_TERM_PREFIX);
+    assertEquals(2, v.getValueCount());
+    
+    v.setDocument(0);
+    assertEquals(1, v.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, v.nextOrd());
+    
+    v.setDocument(1);
+    assertEquals(0, v.nextOrd());
+    assertEquals(1, v.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, v.nextOrd());
+    
+    BytesRef value = new BytesRef();
+    v.lookupOrd(0, value);
+    assertEquals(-3, NumericUtils.prefixCodedToInt(value));
+    
+    v.lookupOrd(1, value);
+    assertEquals(5, NumericUtils.prefixCodedToInt(value));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testNumericEncoded64() throws IOException {
+    Directory dir = newDirectory();
+    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
+    
+    Document doc = new Document();
+    doc.add(new LongField("foo", 5, Field.Store.NO));
+    iw.addDocument(doc);
+    
+    doc = new Document();
+    doc.add(new LongField("foo", 5, Field.Store.NO));
+    doc.add(new LongField("foo", -3, Field.Store.NO));
+    iw.addDocument(doc);
+    
+    iw.forceMerge(1);
+    iw.shutdown();
+    
+    DirectoryReader ir = DirectoryReader.open(dir);
+    AtomicReader ar = getOnlySegmentReader(ir);
+    
+    SortedSetDocValues v = FieldCache.DEFAULT.getDocTermOrds(ar, "foo", FieldCache.INT64_TERM_PREFIX);
+    assertEquals(2, v.getValueCount());
+    
+    v.setDocument(0);
+    assertEquals(1, v.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, v.nextOrd());
+    
+    v.setDocument(1);
+    assertEquals(0, v.nextOrd());
+    assertEquals(1, v.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, v.nextOrd());
+    
+    BytesRef value = new BytesRef();
+    v.lookupOrd(0, value);
+    assertEquals(-3, NumericUtils.prefixCodedToLong(value));
+    
+    v.lookupOrd(1, value);
+    assertEquals(5, NumericUtils.prefixCodedToLong(value));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testSortedTermsEnum() throws IOException {
+    Directory directory = newDirectory();
+    Analyzer analyzer = new MockAnalyzer(random());
+    IndexWriterConfig iwconfig = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    iwconfig.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, iwconfig);
+    
+    Document doc = new Document();
+    doc.add(new StringField("field", "hello", Field.Store.NO));
+    iwriter.addDocument(doc);
+    
+    doc = new Document();
+    doc.add(new StringField("field", "world", Field.Store.NO));
+    // we need a second value for a doc, or we don't actually test DocTermOrds!
+    doc.add(new StringField("field", "hello", Field.Store.NO));
+    iwriter.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new StringField("field", "beer", Field.Store.NO));
+    iwriter.addDocument(doc);
+    iwriter.forceMerge(1);
+    
+    DirectoryReader ireader = iwriter.getReader();
+    iwriter.shutdown();
+
+    AtomicReader ar = getOnlySegmentReader(ireader);
+    SortedSetDocValues dv = FieldCache.DEFAULT.getDocTermOrds(ar, "field", null);
+    assertEquals(3, dv.getValueCount());
+    
+    TermsEnum termsEnum = dv.termsEnum();
+    
+    // next()
+    assertEquals("beer", termsEnum.next().utf8ToString());
+    assertEquals(0, termsEnum.ord());
+    assertEquals("hello", termsEnum.next().utf8ToString());
+    assertEquals(1, termsEnum.ord());
+    assertEquals("world", termsEnum.next().utf8ToString());
+    assertEquals(2, termsEnum.ord());
+    
+    // seekCeil()
+    assertEquals(SeekStatus.NOT_FOUND, termsEnum.seekCeil(new BytesRef("ha!")));
+    assertEquals("hello", termsEnum.term().utf8ToString());
+    assertEquals(1, termsEnum.ord());
+    assertEquals(SeekStatus.FOUND, termsEnum.seekCeil(new BytesRef("beer")));
+    assertEquals("beer", termsEnum.term().utf8ToString());
+    assertEquals(0, termsEnum.ord());
+    assertEquals(SeekStatus.END, termsEnum.seekCeil(new BytesRef("zzz")));
+    
+    // seekExact()
+    assertTrue(termsEnum.seekExact(new BytesRef("beer")));
+    assertEquals("beer", termsEnum.term().utf8ToString());
+    assertEquals(0, termsEnum.ord());
+    assertTrue(termsEnum.seekExact(new BytesRef("hello")));
+    assertEquals("hello", termsEnum.term().utf8ToString());
+    assertEquals(1, termsEnum.ord());
+    assertTrue(termsEnum.seekExact(new BytesRef("world")));
+    assertEquals("world", termsEnum.term().utf8ToString());
+    assertEquals(2, termsEnum.ord());
+    assertFalse(termsEnum.seekExact(new BytesRef("bogus")));
+    
+    // seek(ord)
+    termsEnum.seekExact(0);
+    assertEquals("beer", termsEnum.term().utf8ToString());
+    assertEquals(0, termsEnum.ord());
+    termsEnum.seekExact(1);
+    assertEquals("hello", termsEnum.term().utf8ToString());
+    assertEquals(1, termsEnum.ord());
+    termsEnum.seekExact(2);
+    assertEquals("world", termsEnum.term().utf8ToString());
+    assertEquals(2, termsEnum.ord());
+    ireader.close();
+    directory.close();
+  }
+  
+  public void testActuallySingleValued() throws IOException {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwconfig =  newIndexWriterConfig(TEST_VERSION_CURRENT, null);
+    iwconfig.setMergePolicy(newLogMergePolicy());
+    IndexWriter iw = new IndexWriter(dir, iwconfig);
+    
+    Document doc = new Document();
+    doc.add(new StringField("foo", "bar", Field.Store.NO));
+    iw.addDocument(doc);
+    
+    doc = new Document();
+    doc.add(new StringField("foo", "baz", Field.Store.NO));
+    iw.addDocument(doc);
+    
+    doc = new Document();
+    iw.addDocument(doc);
+    
+    doc = new Document();
+    doc.add(new StringField("foo", "baz", Field.Store.NO));
+    doc.add(new StringField("foo", "baz", Field.Store.NO));
+    iw.addDocument(doc);
+    
+    iw.forceMerge(1);
+    iw.shutdown();
+    
+    DirectoryReader ir = DirectoryReader.open(dir);
+    AtomicReader ar = getOnlySegmentReader(ir);
+    
+    SortedSetDocValues v = FieldCache.DEFAULT.getDocTermOrds(ar, "foo", null);
+    assertNotNull(DocValues.unwrapSingleton(v)); // actually a single-valued field
+    assertEquals(2, v.getValueCount());
+    
+    v.setDocument(0);
+    assertEquals(0, v.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, v.nextOrd());
+    
+    v.setDocument(1);
+    assertEquals(1, v.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, v.nextOrd());
+    
+    v.setDocument(2);
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, v.nextOrd());
+    
+    v.setDocument(3);
+    assertEquals(1, v.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, v.nextOrd());
+    
+    BytesRef value = new BytesRef();
+    v.lookupOrd(0, value);
+    assertEquals("bar", value.utf8ToString());
+    
+    v.lookupOrd(1, value);
+    assertEquals("baz", value.utf8ToString());
+    
+    ir.close();
+    dir.close();
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache.java lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache.java
--- lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache.java	2014-05-12 14:43:45.868323137 -0400
@@ -0,0 +1,775 @@
+package org.apache.lucene.uninverting;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.concurrent.CyclicBarrier;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.BinaryDocValuesField;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.DoubleField;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.document.FloatField;
+import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.LongField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
+import org.apache.lucene.document.SortedSetDocValuesField;
+import org.apache.lucene.document.StoredField;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.NumericUtils;
+import org.apache.lucene.util.TestUtil;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+
+public class TestFieldCache extends LuceneTestCase {
+  private static AtomicReader reader;
+  private static int NUM_DOCS;
+  private static int NUM_ORDS;
+  private static String[] unicodeStrings;
+  private static BytesRef[][] multiValued;
+  private static Directory directory;
+
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    NUM_DOCS = atLeast(500);
+    NUM_ORDS = atLeast(2);
+    directory = newDirectory();
+    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
+    long theLong = Long.MAX_VALUE;
+    double theDouble = Double.MAX_VALUE;
+    int theInt = Integer.MAX_VALUE;
+    float theFloat = Float.MAX_VALUE;
+    unicodeStrings = new String[NUM_DOCS];
+    multiValued = new BytesRef[NUM_DOCS][NUM_ORDS];
+    if (VERBOSE) {
+      System.out.println("TEST: setUp");
+    }
+    for (int i = 0; i < NUM_DOCS; i++){
+      Document doc = new Document();
+      doc.add(new LongField("theLong", theLong--, Field.Store.NO));
+      doc.add(new DoubleField("theDouble", theDouble--, Field.Store.NO));
+      doc.add(new IntField("theInt", theInt--, Field.Store.NO));
+      doc.add(new FloatField("theFloat", theFloat--, Field.Store.NO));
+      if (i%2 == 0) {
+        doc.add(new IntField("sparse", i, Field.Store.NO));
+      }
+
+      if (i%2 == 0) {
+        doc.add(new IntField("numInt", i, Field.Store.NO));
+      }
+
+      // sometimes skip the field:
+      if (random().nextInt(40) != 17) {
+        unicodeStrings[i] = generateString(i);
+        doc.add(newStringField("theRandomUnicodeString", unicodeStrings[i], Field.Store.YES));
+      }
+
+      // sometimes skip the field:
+      if (random().nextInt(10) != 8) {
+        for (int j = 0; j < NUM_ORDS; j++) {
+          String newValue = generateString(i);
+          multiValued[i][j] = new BytesRef(newValue);
+          doc.add(newStringField("theRandomUnicodeMultiValuedField", newValue, Field.Store.YES));
+        }
+        Arrays.sort(multiValued[i]);
+      }
+      writer.addDocument(doc);
+    }
+    IndexReader r = writer.getReader();
+    reader = SlowCompositeReaderWrapper.wrap(r);
+    writer.shutdown();
+  }
+
+  @AfterClass
+  public static void afterClass() throws Exception {
+    reader.close();
+    reader = null;
+    directory.close();
+    directory = null;
+    unicodeStrings = null;
+    multiValued = null;
+  }
+  
+  public void testInfoStream() throws Exception {
+    try {
+      FieldCache cache = FieldCache.DEFAULT;
+      ByteArrayOutputStream bos = new ByteArrayOutputStream(1024);
+      cache.setInfoStream(new PrintStream(bos, false, IOUtils.UTF_8));
+      cache.getNumerics(reader, "theDouble", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, false);
+      cache.getNumerics(reader, "theDouble", new FieldCache.Parser() {
+        @Override
+        public TermsEnum termsEnum(Terms terms) throws IOException {
+          return NumericUtils.filterPrefixCodedLongs(terms.iterator(null));
+        }
+        @Override
+        public long parseValue(BytesRef term) {
+          int val = (int) NumericUtils.prefixCodedToLong(term);
+          if (val<0) val ^= 0x7fffffff;
+          return val;
+        }
+      }, false);
+      assertTrue(bos.toString(IOUtils.UTF_8).indexOf("WARNING") != -1);
+    } finally {
+      FieldCache.DEFAULT.setInfoStream(null);
+      FieldCache.DEFAULT.purgeAllCaches();
+    }
+  }
+
+  public void test() throws IOException {
+    FieldCache cache = FieldCache.DEFAULT;
+    NumericDocValues doubles = cache.getNumerics(reader, "theDouble", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean());
+    assertSame("Second request to cache return same array", doubles, cache.getNumerics(reader, "theDouble", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean()));
+    for (int i = 0; i < NUM_DOCS; i++) {
+      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));
+    }
+    
+    NumericDocValues longs = cache.getNumerics(reader, "theLong", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean());
+    assertSame("Second request to cache return same array", longs, cache.getNumerics(reader, "theLong", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean()));
+    for (int i = 0; i < NUM_DOCS; i++) {
+      assertEquals(Long.MAX_VALUE - i, longs.get(i));
+    }
+
+    NumericDocValues ints = cache.getNumerics(reader, "theInt", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean());
+    assertSame("Second request to cache return same array", ints, cache.getNumerics(reader, "theInt", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean()));
+    for (int i = 0; i < NUM_DOCS; i++) {
+      assertEquals(Integer.MAX_VALUE - i, ints.get(i));
+    }
+    
+    NumericDocValues floats = cache.getNumerics(reader, "theFloat", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean());
+    assertSame("Second request to cache return same array", floats, cache.getNumerics(reader, "theFloat", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean()));
+    for (int i = 0; i < NUM_DOCS; i++) {
+      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));
+    }
+
+    Bits docsWithField = cache.getDocsWithField(reader, "theLong");
+    assertSame("Second request to cache return same array", docsWithField, cache.getDocsWithField(reader, "theLong"));
+    assertTrue("docsWithField(theLong) must be class Bits.MatchAllBits", docsWithField instanceof Bits.MatchAllBits);
+    assertTrue("docsWithField(theLong) Size: " + docsWithField.length() + " is not: " + NUM_DOCS, docsWithField.length() == NUM_DOCS);
+    for (int i = 0; i < docsWithField.length(); i++) {
+      assertTrue(docsWithField.get(i));
+    }
+    
+    docsWithField = cache.getDocsWithField(reader, "sparse");
+    assertSame("Second request to cache return same array", docsWithField, cache.getDocsWithField(reader, "sparse"));
+    assertFalse("docsWithField(sparse) must not be class Bits.MatchAllBits", docsWithField instanceof Bits.MatchAllBits);
+    assertTrue("docsWithField(sparse) Size: " + docsWithField.length() + " is not: " + NUM_DOCS, docsWithField.length() == NUM_DOCS);
+    for (int i = 0; i < docsWithField.length(); i++) {
+      assertEquals(i%2 == 0, docsWithField.get(i));
+    }
+
+    // getTermsIndex
+    SortedDocValues termsIndex = cache.getTermsIndex(reader, "theRandomUnicodeString");
+    assertSame("Second request to cache return same array", termsIndex, cache.getTermsIndex(reader, "theRandomUnicodeString"));
+    final BytesRef br = new BytesRef();
+    for (int i = 0; i < NUM_DOCS; i++) {
+      final BytesRef term;
+      final int ord = termsIndex.getOrd(i);
+      if (ord == -1) {
+        term = null;
+      } else {
+        termsIndex.lookupOrd(ord, br);
+        term = br;
+      }
+      final String s = term == null ? null : term.utf8ToString();
+      assertTrue("for doc " + i + ": " + s + " does not equal: " + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));
+    }
+
+    int nTerms = termsIndex.getValueCount();
+
+    TermsEnum tenum = termsIndex.termsEnum();
+    BytesRef val = new BytesRef();
+    for (int i=0; i<nTerms; i++) {
+      BytesRef val1 = tenum.next();
+      termsIndex.lookupOrd(i, val);
+      // System.out.println("i="+i);
+      assertEquals(val, val1);
+    }
+
+    // seek the enum around (note this isn't a great test here)
+    int num = atLeast(100);
+    for (int i = 0; i < num; i++) {
+      int k = random().nextInt(nTerms);
+      termsIndex.lookupOrd(k, val);
+      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));
+      assertEquals(val, tenum.term());
+    }
+
+    for(int i=0;i<nTerms;i++) {
+      termsIndex.lookupOrd(i, val);
+      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));
+      assertEquals(val, tenum.term());
+    }
+
+    // test bad field
+    termsIndex = cache.getTermsIndex(reader, "bogusfield");
+
+    // getTerms
+    BinaryDocValues terms = cache.getTerms(reader, "theRandomUnicodeString", true);
+    assertSame("Second request to cache return same array", terms, cache.getTerms(reader, "theRandomUnicodeString", true));
+    Bits bits = cache.getDocsWithField(reader, "theRandomUnicodeString");
+    for (int i = 0; i < NUM_DOCS; i++) {
+      terms.get(i, br);
+      final BytesRef term;
+      if (!bits.get(i)) {
+        term = null;
+      } else {
+        term = br;
+      }
+      final String s = term == null ? null : term.utf8ToString();
+      assertTrue("for doc " + i + ": " + s + " does not equal: " + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));
+    }
+
+    // test bad field
+    terms = cache.getTerms(reader, "bogusfield", false);
+
+    // getDocTermOrds
+    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, "theRandomUnicodeMultiValuedField", null);
+    int numEntries = cache.getCacheEntries().length;
+    // ask for it again, and check that we didnt create any additional entries:
+    termOrds = cache.getDocTermOrds(reader, "theRandomUnicodeMultiValuedField", null);
+    assertEquals(numEntries, cache.getCacheEntries().length);
+
+    for (int i = 0; i < NUM_DOCS; i++) {
+      termOrds.setDocument(i);
+      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId
+      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));
+      for (BytesRef v : values) {
+        if (v == null) {
+          // why does this test use null values... instead of an empty list: confusing
+          break;
+        }
+        long ord = termOrds.nextOrd();
+        assert ord != SortedSetDocValues.NO_MORE_ORDS;
+        BytesRef scratch = new BytesRef();
+        termOrds.lookupOrd(ord, scratch);
+        assertEquals(v, scratch);
+      }
+      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());
+    }
+
+    // test bad field
+    termOrds = cache.getDocTermOrds(reader, "bogusfield", null);
+    assertTrue(termOrds.getValueCount() == 0);
+
+    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());
+  }
+
+  public void testEmptyIndex() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriter writer= new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(500));
+    writer.shutdown();
+    IndexReader r = DirectoryReader.open(dir);
+    AtomicReader reader = SlowCompositeReaderWrapper.wrap(r);
+    FieldCache.DEFAULT.getTerms(reader, "foobar", true);
+    FieldCache.DEFAULT.getTermsIndex(reader, "foobar");
+    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());
+    r.close();
+    dir.close();
+  }
+
+  private static String generateString(int i) {
+    String s = null;
+    if (i > 0 && random().nextInt(3) == 1) {
+      // reuse past string -- try to find one that's not null
+      for(int iter = 0; iter < 10 && s == null;iter++) {
+        s = unicodeStrings[random().nextInt(i)];
+      }
+      if (s == null) {
+        s = TestUtil.randomUnicodeString(random());
+      }
+    } else {
+      s = TestUtil.randomUnicodeString(random());
+    }
+    return s;
+  }
+
+  public void testDocsWithField() throws Exception {
+    FieldCache cache = FieldCache.DEFAULT;
+    cache.purgeAllCaches();
+    assertEquals(0, cache.getCacheEntries().length);
+    cache.getNumerics(reader, "theDouble", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, true);
+
+    // The double[] takes one slots, and docsWithField should also
+    // have been populated:
+    assertEquals(2, cache.getCacheEntries().length);
+    Bits bits = cache.getDocsWithField(reader, "theDouble");
+
+    // No new entries should appear:
+    assertEquals(2, cache.getCacheEntries().length);
+    assertTrue(bits instanceof Bits.MatchAllBits);
+
+    NumericDocValues ints = cache.getNumerics(reader, "sparse", FieldCache.NUMERIC_UTILS_INT_PARSER, true);
+    assertEquals(4, cache.getCacheEntries().length);
+    Bits docsWithField = cache.getDocsWithField(reader, "sparse");
+    assertEquals(4, cache.getCacheEntries().length);
+    for (int i = 0; i < docsWithField.length(); i++) {
+      if (i%2 == 0) {
+        assertTrue(docsWithField.get(i));
+        assertEquals(i, ints.get(i));
+      } else {
+        assertFalse(docsWithField.get(i));
+      }
+    }
+
+    NumericDocValues numInts = cache.getNumerics(reader, "numInt", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean());
+    docsWithField = cache.getDocsWithField(reader, "numInt");
+    for (int i = 0; i < docsWithField.length(); i++) {
+      if (i%2 == 0) {
+        assertTrue(docsWithField.get(i));
+        assertEquals(i, numInts.get(i));
+      } else {
+        assertFalse(docsWithField.get(i));
+      }
+    }
+  }
+  
+  public void testGetDocsWithFieldThreadSafety() throws Exception {
+    final FieldCache cache = FieldCache.DEFAULT;
+    cache.purgeAllCaches();
+
+    int NUM_THREADS = 3;
+    Thread[] threads = new Thread[NUM_THREADS];
+    final AtomicBoolean failed = new AtomicBoolean();
+    final AtomicInteger iters = new AtomicInteger();
+    final int NUM_ITER = 200 * RANDOM_MULTIPLIER;
+    final CyclicBarrier restart = new CyclicBarrier(NUM_THREADS,
+                                                    new Runnable() {
+                                                      @Override
+                                                      public void run() {
+                                                        cache.purgeAllCaches();
+                                                        iters.incrementAndGet();
+                                                      }
+                                                    });
+    for(int threadIDX=0;threadIDX<NUM_THREADS;threadIDX++) {
+      threads[threadIDX] = new Thread() {
+          @Override
+          public void run() {
+
+            try {
+              while(!failed.get()) {
+                final int op = random().nextInt(3);
+                if (op == 0) {
+                  // Purge all caches & resume, once all
+                  // threads get here:
+                  restart.await();
+                  if (iters.get() >= NUM_ITER) {
+                    break;
+                  }
+                } else if (op == 1) {
+                  Bits docsWithField = cache.getDocsWithField(reader, "sparse");
+                  for (int i = 0; i < docsWithField.length(); i++) {
+                    assertEquals(i%2 == 0, docsWithField.get(i));
+                  }
+                } else {
+                  NumericDocValues ints = cache.getNumerics(reader, "sparse", FieldCache.NUMERIC_UTILS_INT_PARSER, true);
+                  Bits docsWithField = cache.getDocsWithField(reader, "sparse");
+                  for (int i = 0; i < docsWithField.length(); i++) {
+                    if (i%2 == 0) {
+                      assertTrue(docsWithField.get(i));
+                      assertEquals(i, ints.get(i));
+                    } else {
+                      assertFalse(docsWithField.get(i));
+                    }
+                  }
+                }
+              }
+            } catch (Throwable t) {
+              failed.set(true);
+              restart.reset();
+              throw new RuntimeException(t);
+            }
+          }
+        };
+      threads[threadIDX].start();
+    }
+
+    for(int threadIDX=0;threadIDX<NUM_THREADS;threadIDX++) {
+      threads[threadIDX].join();
+    }
+    assertFalse(failed.get());
+  }
+  
+  public void testDocValuesIntegration() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryDocValuesField("binary", new BytesRef("binary value")));
+    doc.add(new SortedDocValuesField("sorted", new BytesRef("sorted value")));
+    doc.add(new NumericDocValuesField("numeric", 42));
+    if (defaultCodecSupportsSortedSet()) {
+      doc.add(new SortedSetDocValuesField("sortedset", new BytesRef("sortedset value1")));
+      doc.add(new SortedSetDocValuesField("sortedset", new BytesRef("sortedset value2")));
+    }
+    iw.addDocument(doc);
+    DirectoryReader ir = iw.getReader();
+    iw.shutdown();
+    AtomicReader ar = getOnlySegmentReader(ir);
+    
+    BytesRef scratch = new BytesRef();
+    
+    // Binary type: can be retrieved via getTerms()
+    try {
+      FieldCache.DEFAULT.getNumerics(ar, "binary", FieldCache.NUMERIC_UTILS_INT_PARSER, false);
+      fail();
+    } catch (IllegalStateException expected) {}
+    
+    BinaryDocValues binary = FieldCache.DEFAULT.getTerms(ar, "binary", true);
+    binary.get(0, scratch);
+    assertEquals("binary value", scratch.utf8ToString());
+    
+    try {
+      FieldCache.DEFAULT.getTermsIndex(ar, "binary");
+      fail();
+    } catch (IllegalStateException expected) {}
+    
+    try {
+      FieldCache.DEFAULT.getDocTermOrds(ar, "binary", null);
+      fail();
+    } catch (IllegalStateException expected) {}
+    
+    try {
+      new DocTermOrds(ar, null, "binary");
+      fail();
+    } catch (IllegalStateException expected) {}
+    
+    Bits bits = FieldCache.DEFAULT.getDocsWithField(ar, "binary");
+    assertTrue(bits.get(0));
+    
+    // Sorted type: can be retrieved via getTerms(), getTermsIndex(), getDocTermOrds()
+    try {
+      FieldCache.DEFAULT.getNumerics(ar, "sorted", FieldCache.NUMERIC_UTILS_INT_PARSER, false);
+      fail();
+    } catch (IllegalStateException expected) {}
+    
+    try {
+      new DocTermOrds(ar, null, "sorted");
+      fail();
+    } catch (IllegalStateException expected) {}
+    
+    binary = FieldCache.DEFAULT.getTerms(ar, "sorted", true);
+    binary.get(0, scratch);
+    assertEquals("sorted value", scratch.utf8ToString());
+    
+    SortedDocValues sorted = FieldCache.DEFAULT.getTermsIndex(ar, "sorted");
+    assertEquals(0, sorted.getOrd(0));
+    assertEquals(1, sorted.getValueCount());
+    sorted.get(0, scratch);
+    assertEquals("sorted value", scratch.utf8ToString());
+    
+    SortedSetDocValues sortedSet = FieldCache.DEFAULT.getDocTermOrds(ar, "sorted", null);
+    sortedSet.setDocument(0);
+    assertEquals(0, sortedSet.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, sortedSet.nextOrd());
+    assertEquals(1, sortedSet.getValueCount());
+    
+    bits = FieldCache.DEFAULT.getDocsWithField(ar, "sorted");
+    assertTrue(bits.get(0));
+    
+    // Numeric type: can be retrieved via getInts() and so on
+    NumericDocValues numeric = FieldCache.DEFAULT.getNumerics(ar, "numeric", FieldCache.NUMERIC_UTILS_INT_PARSER, false);
+    assertEquals(42, numeric.get(0));
+    
+    try {
+      FieldCache.DEFAULT.getTerms(ar, "numeric", true);
+      fail();
+    } catch (IllegalStateException expected) {}
+    
+    try {
+      FieldCache.DEFAULT.getTermsIndex(ar, "numeric");
+      fail();
+    } catch (IllegalStateException expected) {}
+    
+    try {
+      FieldCache.DEFAULT.getDocTermOrds(ar, "numeric", null);
+      fail();
+    } catch (IllegalStateException expected) {}
+    
+    try {
+      new DocTermOrds(ar, null, "numeric");
+      fail();
+    } catch (IllegalStateException expected) {}
+    
+    bits = FieldCache.DEFAULT.getDocsWithField(ar, "numeric");
+    assertTrue(bits.get(0));
+    
+    // SortedSet type: can be retrieved via getDocTermOrds() 
+    if (defaultCodecSupportsSortedSet()) {
+      try {
+        FieldCache.DEFAULT.getNumerics(ar, "sortedset", FieldCache.NUMERIC_UTILS_INT_PARSER, false);
+        fail();
+      } catch (IllegalStateException expected) {}
+    
+      try {
+        FieldCache.DEFAULT.getTerms(ar, "sortedset", true);
+        fail();
+      } catch (IllegalStateException expected) {}
+    
+      try {
+        FieldCache.DEFAULT.getTermsIndex(ar, "sortedset");
+        fail();
+      } catch (IllegalStateException expected) {}
+      
+      try {
+        new DocTermOrds(ar, null, "sortedset");
+        fail();
+      } catch (IllegalStateException expected) {}
+    
+      sortedSet = FieldCache.DEFAULT.getDocTermOrds(ar, "sortedset", null);
+      sortedSet.setDocument(0);
+      assertEquals(0, sortedSet.nextOrd());
+      assertEquals(1, sortedSet.nextOrd());
+      assertEquals(SortedSetDocValues.NO_MORE_ORDS, sortedSet.nextOrd());
+      assertEquals(2, sortedSet.getValueCount());
+    
+      bits = FieldCache.DEFAULT.getDocsWithField(ar, "sortedset");
+      assertTrue(bits.get(0));
+    }
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testNonexistantFields() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    iw.addDocument(doc);
+    DirectoryReader ir = iw.getReader();
+    iw.shutdown();
+    
+    AtomicReader ar = getOnlySegmentReader(ir);
+    
+    final FieldCache cache = FieldCache.DEFAULT;
+    cache.purgeAllCaches();
+    assertEquals(0, cache.getCacheEntries().length);
+    
+    NumericDocValues ints = cache.getNumerics(ar, "bogusints", FieldCache.NUMERIC_UTILS_INT_PARSER, true);
+    assertEquals(0, ints.get(0));
+    
+    NumericDocValues longs = cache.getNumerics(ar, "boguslongs", FieldCache.NUMERIC_UTILS_LONG_PARSER, true);
+    assertEquals(0, longs.get(0));
+    
+    NumericDocValues floats = cache.getNumerics(ar, "bogusfloats", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, true);
+    assertEquals(0, floats.get(0));
+    
+    NumericDocValues doubles = cache.getNumerics(ar, "bogusdoubles", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, true);
+    assertEquals(0, doubles.get(0));
+    
+    BytesRef scratch = new BytesRef();
+    BinaryDocValues binaries = cache.getTerms(ar, "bogusterms", true);
+    binaries.get(0, scratch);
+    assertEquals(0, scratch.length);
+    
+    SortedDocValues sorted = cache.getTermsIndex(ar, "bogustermsindex");
+    assertEquals(-1, sorted.getOrd(0));
+    sorted.get(0, scratch);
+    assertEquals(0, scratch.length);
+    
+    SortedSetDocValues sortedSet = cache.getDocTermOrds(ar, "bogusmultivalued", null);
+    sortedSet.setDocument(0);
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, sortedSet.nextOrd());
+    
+    Bits bits = cache.getDocsWithField(ar, "bogusbits");
+    assertFalse(bits.get(0));
+    
+    // check that we cached nothing
+    assertEquals(0, cache.getCacheEntries().length);
+    ir.close();
+    dir.close();
+  }
+  
+  public void testNonIndexedFields() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new StoredField("bogusbytes", "bogus"));
+    doc.add(new StoredField("bogusshorts", "bogus"));
+    doc.add(new StoredField("bogusints", "bogus"));
+    doc.add(new StoredField("boguslongs", "bogus"));
+    doc.add(new StoredField("bogusfloats", "bogus"));
+    doc.add(new StoredField("bogusdoubles", "bogus"));
+    doc.add(new StoredField("bogusterms", "bogus"));
+    doc.add(new StoredField("bogustermsindex", "bogus"));
+    doc.add(new StoredField("bogusmultivalued", "bogus"));
+    doc.add(new StoredField("bogusbits", "bogus"));
+    iw.addDocument(doc);
+    DirectoryReader ir = iw.getReader();
+    iw.shutdown();
+    
+    AtomicReader ar = getOnlySegmentReader(ir);
+    
+    final FieldCache cache = FieldCache.DEFAULT;
+    cache.purgeAllCaches();
+    assertEquals(0, cache.getCacheEntries().length);
+    
+    NumericDocValues ints = cache.getNumerics(ar, "bogusints", FieldCache.NUMERIC_UTILS_INT_PARSER, true);
+    assertEquals(0, ints.get(0));
+    
+    NumericDocValues longs = cache.getNumerics(ar, "boguslongs", FieldCache.NUMERIC_UTILS_LONG_PARSER, true);
+    assertEquals(0, longs.get(0));
+    
+    NumericDocValues floats = cache.getNumerics(ar, "bogusfloats", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, true);
+    assertEquals(0, floats.get(0));
+    
+    NumericDocValues doubles = cache.getNumerics(ar, "bogusdoubles", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, true);
+    assertEquals(0, doubles.get(0));
+    
+    BytesRef scratch = new BytesRef();
+    BinaryDocValues binaries = cache.getTerms(ar, "bogusterms", true);
+    binaries.get(0, scratch);
+    assertEquals(0, scratch.length);
+    
+    SortedDocValues sorted = cache.getTermsIndex(ar, "bogustermsindex");
+    assertEquals(-1, sorted.getOrd(0));
+    sorted.get(0, scratch);
+    assertEquals(0, scratch.length);
+    
+    SortedSetDocValues sortedSet = cache.getDocTermOrds(ar, "bogusmultivalued", null);
+    sortedSet.setDocument(0);
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, sortedSet.nextOrd());
+    
+    Bits bits = cache.getDocsWithField(ar, "bogusbits");
+    assertFalse(bits.get(0));
+    
+    // check that we cached nothing
+    assertEquals(0, cache.getCacheEntries().length);
+    ir.close();
+    dir.close();
+  }
+
+  // Make sure that the use of GrowableWriter doesn't prevent from using the full long range
+  public void testLongFieldCache() throws IOException {
+    Directory dir = newDirectory();
+    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    cfg.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, cfg);
+    Document doc = new Document();
+    LongField field = new LongField("f", 0L, Store.YES);
+    doc.add(field);
+    final long[] values = new long[TestUtil.nextInt(random(), 1, 10)];
+    for (int i = 0; i < values.length; ++i) {
+      final long v;
+      switch (random().nextInt(10)) {
+        case 0:
+          v = Long.MIN_VALUE;
+          break;
+        case 1:
+          v = 0;
+          break;
+        case 2:
+          v = Long.MAX_VALUE;
+          break;
+        default:
+          v = TestUtil.nextLong(random(), -10, 10);
+          break;
+      }
+      values[i] = v;
+      if (v == 0 && random().nextBoolean()) {
+        // missing
+        iw.addDocument(new Document());
+      } else {
+        field.setLongValue(v);
+        iw.addDocument(doc);
+      }
+    }
+    iw.forceMerge(1);
+    final DirectoryReader reader = iw.getReader();
+    final NumericDocValues longs = FieldCache.DEFAULT.getNumerics(getOnlySegmentReader(reader), "f", FieldCache.NUMERIC_UTILS_LONG_PARSER, false);
+    for (int i = 0; i < values.length; ++i) {
+      assertEquals(values[i], longs.get(i));
+    }
+    reader.close();
+    iw.shutdown();
+    dir.close();
+  }
+
+  // Make sure that the use of GrowableWriter doesn't prevent from using the full int range
+  public void testIntFieldCache() throws IOException {
+    Directory dir = newDirectory();
+    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    cfg.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, cfg);
+    Document doc = new Document();
+    IntField field = new IntField("f", 0, Store.YES);
+    doc.add(field);
+    final int[] values = new int[TestUtil.nextInt(random(), 1, 10)];
+    for (int i = 0; i < values.length; ++i) {
+      final int v;
+      switch (random().nextInt(10)) {
+        case 0:
+          v = Integer.MIN_VALUE;
+          break;
+        case 1:
+          v = 0;
+          break;
+        case 2:
+          v = Integer.MAX_VALUE;
+          break;
+        default:
+          v = TestUtil.nextInt(random(), -10, 10);
+          break;
+      }
+      values[i] = v;
+      if (v == 0 && random().nextBoolean()) {
+        // missing
+        iw.addDocument(new Document());
+      } else {
+        field.setIntValue(v);
+        iw.addDocument(doc);
+      }
+    }
+    iw.forceMerge(1);
+    final DirectoryReader reader = iw.getReader();
+    final NumericDocValues ints = FieldCache.DEFAULT.getNumerics(getOnlySegmentReader(reader), "f", FieldCache.NUMERIC_UTILS_INT_PARSER, false);
+    for (int i = 0; i < values.length; ++i) {
+      assertEquals(values[i], ints.get(i));
+    }
+    reader.close();
+    iw.shutdown();
+    dir.close();
+  }
+
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheReopen.java lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheReopen.java
--- lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheReopen.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheReopen.java	2014-05-12 13:28:55.940244947 -0400
@@ -0,0 +1,72 @@
+package org.apache.lucene.uninverting;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.IntField;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+
+public class TestFieldCacheReopen extends LuceneTestCase {
+  
+  // TODO: make a version of this that tests the same thing with UninvertingReader.wrap()
+  
+  // LUCENE-1579: Ensure that on a reopened reader, that any
+  // shared segments reuse the doc values arrays in
+  // FieldCache
+  public void testFieldCacheReuseAfterReopen() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriter writer = new IndexWriter(
+        dir,
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).
+            setMergePolicy(newLogMergePolicy(10))
+    );
+    Document doc = new Document();
+    doc.add(new IntField("number", 17, Field.Store.NO));
+    writer.addDocument(doc);
+    writer.commit();
+  
+    // Open reader1
+    DirectoryReader r = DirectoryReader.open(dir);
+    AtomicReader r1 = getOnlySegmentReader(r);
+    final NumericDocValues ints = FieldCache.DEFAULT.getNumerics(r1, "number", FieldCache.NUMERIC_UTILS_INT_PARSER, false);
+    assertEquals(17, ints.get(0));
+  
+    // Add new segment
+    writer.addDocument(doc);
+    writer.commit();
+  
+    // Reopen reader1 --> reader2
+    DirectoryReader r2 = DirectoryReader.openIfChanged(r);
+    assertNotNull(r2);
+    r.close();
+    AtomicReader sub0 = r2.leaves().get(0).reader();
+    final NumericDocValues ints2 = FieldCache.DEFAULT.getNumerics(sub0, "number", FieldCache.NUMERIC_UTILS_INT_PARSER, false);
+    r2.close();
+    assertTrue(ints == ints2);
+  
+    writer.shutdown();
+    dir.close();
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheSanityChecker.java lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheSanityChecker.java
--- lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheSanityChecker.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheSanityChecker.java	2014-05-12 13:28:57.008244966 -0400
@@ -0,0 +1,164 @@
+package org.apache.lucene.uninverting;
+
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.DoubleField;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FloatField;
+import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.LongField;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.MultiReader;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.uninverting.FieldCacheSanityChecker.Insanity;
+import org.apache.lucene.uninverting.FieldCacheSanityChecker.InsanityType;
+import org.apache.lucene.util.LuceneTestCase;
+
+public class TestFieldCacheSanityChecker extends LuceneTestCase {
+
+  protected AtomicReader readerA;
+  protected AtomicReader readerB;
+  protected AtomicReader readerX;
+  protected AtomicReader readerAclone;
+  protected Directory dirA, dirB;
+  private static final int NUM_DOCS = 1000;
+
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    dirA = newDirectory();
+    dirB = newDirectory();
+
+    IndexWriter wA = new IndexWriter(dirA, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    IndexWriter wB = new IndexWriter(dirB, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+
+    long theLong = Long.MAX_VALUE;
+    double theDouble = Double.MAX_VALUE;
+    int theInt = Integer.MAX_VALUE;
+    float theFloat = Float.MAX_VALUE;
+    for (int i = 0; i < NUM_DOCS; i++){
+      Document doc = new Document();
+      doc.add(new LongField("theLong", theLong--, Field.Store.NO));
+      doc.add(new DoubleField("theDouble", theDouble--, Field.Store.NO));
+      doc.add(new IntField("theInt", theInt--, Field.Store.NO));
+      doc.add(new FloatField("theFloat", theFloat--, Field.Store.NO));
+      if (0 == i % 3) {
+        wA.addDocument(doc);
+      } else {
+        wB.addDocument(doc);
+      }
+    }
+    wA.shutdown();
+    wB.shutdown();
+    DirectoryReader rA = DirectoryReader.open(dirA);
+    readerA = SlowCompositeReaderWrapper.wrap(rA);
+    readerAclone = SlowCompositeReaderWrapper.wrap(rA);
+    readerA = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dirA));
+    readerB = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dirB));
+    readerX = SlowCompositeReaderWrapper.wrap(new MultiReader(readerA, readerB));
+  }
+
+  @Override
+  public void tearDown() throws Exception {
+    readerA.close();
+    readerAclone.close();
+    readerB.close();
+    readerX.close();
+    dirA.close();
+    dirB.close();
+    super.tearDown();
+  }
+
+  public void testSanity() throws IOException {
+    FieldCache cache = FieldCache.DEFAULT;
+    cache.purgeAllCaches();
+
+    cache.getNumerics(readerA, "theDouble", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, false);
+    cache.getNumerics(readerAclone, "theDouble", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, false);
+    cache.getNumerics(readerB, "theDouble", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, false);
+
+    cache.getNumerics(readerX, "theInt", FieldCache.NUMERIC_UTILS_INT_PARSER, false);
+
+    // // // 
+
+    Insanity[] insanity = 
+      FieldCacheSanityChecker.checkSanity(cache.getCacheEntries());
+    
+    if (0 < insanity.length)
+      dumpArray(getTestClass().getName() + "#" + getTestName() 
+          + " INSANITY", insanity, System.err);
+
+    assertEquals("shouldn't be any cache insanity", 0, insanity.length);
+    cache.purgeAllCaches();
+  }
+
+  public void testInsanity1() throws IOException {
+    FieldCache cache = FieldCache.DEFAULT;
+    cache.purgeAllCaches();
+
+    cache.getNumerics(readerX, "theInt", FieldCache.NUMERIC_UTILS_INT_PARSER, false);
+    cache.getTerms(readerX, "theInt", false);
+
+    // // // 
+
+    Insanity[] insanity = 
+      FieldCacheSanityChecker.checkSanity(cache.getCacheEntries());
+
+    assertEquals("wrong number of cache errors", 1, insanity.length);
+    assertEquals("wrong type of cache error", 
+                 InsanityType.VALUEMISMATCH,
+                 insanity[0].getType());
+    assertEquals("wrong number of entries in cache error", 2,
+                 insanity[0].getCacheEntries().length);
+
+    // we expect bad things, don't let tearDown complain about them
+    cache.purgeAllCaches();
+  }
+
+  public void testInsanity2() throws IOException {
+    FieldCache cache = FieldCache.DEFAULT;
+    cache.purgeAllCaches();
+
+    cache.getTerms(readerA, "theInt", false);
+    cache.getTerms(readerB, "theInt", false);
+    cache.getTerms(readerX, "theInt", false);
+
+
+    // // // 
+
+    Insanity[] insanity = 
+      FieldCacheSanityChecker.checkSanity(cache.getCacheEntries());
+    
+    assertEquals("wrong number of cache errors", 1, insanity.length);
+    assertEquals("wrong type of cache error", 
+                 InsanityType.SUBREADER,
+                 insanity[0].getType());
+    assertEquals("wrong number of entries in cache error", 3,
+                 insanity[0].getCacheEntries().length);
+
+    // we expect bad things, don't let tearDown complain about them
+    cache.purgeAllCaches();
+  }
+
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheSort.java lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheSort.java
--- lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheSort.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheSort.java	2014-05-12 13:28:56.084244950 -0400
@@ -0,0 +1,1235 @@
+package org.apache.lucene.uninverting;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.DoubleField;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FloatField;
+import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.LongField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.MultiReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.SortField;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
+import org.apache.lucene.util.LuceneTestCase;
+
+/*
+ * Tests sorting (but with fieldcache instead of docvalues)
+ */
+public class TestFieldCacheSort extends LuceneTestCase {
+
+  /** Tests sorting on type string */
+  public void testString() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(newStringField("value", "foo", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "bar", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.SORTED));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.STRING));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // 'bar' comes before 'foo'
+    assertEquals("bar", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("foo", searcher.doc(td.scoreDocs[1].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type string with a missing value */
+  public void testStringMissing() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "foo", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "bar", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.SORTED));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.STRING));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // null comes first
+    assertNull(searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("bar", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("foo", searcher.doc(td.scoreDocs[2].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests reverse sorting on type string */
+  public void testStringReverse() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(newStringField("value", "bar", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "foo", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.SORTED));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.STRING, true));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // 'foo' comes after 'bar' in reverse order
+    assertEquals("foo", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("bar", searcher.doc(td.scoreDocs[1].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type string_val */
+  public void testStringVal() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(newStringField("value", "foo", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "bar", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.BINARY));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.STRING_VAL));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // 'bar' comes before 'foo'
+    assertEquals("bar", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("foo", searcher.doc(td.scoreDocs[1].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type string_val with a missing value */
+  public void testStringValMissing() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "foo", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "bar", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.BINARY));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.STRING_VAL));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // null comes first
+    assertNull(searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("bar", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("foo", searcher.doc(td.scoreDocs[2].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+
+  /** Tests sorting on type string with a missing
+   *  value sorted first */
+  public void testStringMissingSortedFirst() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "foo", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "bar", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.SORTED));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    SortField sf = new SortField("value", SortField.Type.STRING);
+    Sort sort = new Sort(sf);
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // null comes first
+    assertNull(searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("bar", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("foo", searcher.doc(td.scoreDocs[2].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+
+  /** Tests reverse sorting on type string with a missing
+   *  value sorted first */
+  public void testStringMissingSortedFirstReverse() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "foo", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "bar", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.SORTED));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    SortField sf = new SortField("value", SortField.Type.STRING, true);
+    Sort sort = new Sort(sf);
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    assertEquals("foo", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("bar", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    // null comes last
+    assertNull(searcher.doc(td.scoreDocs[2].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+
+  /** Tests sorting on type string with a missing
+   *  value sorted last */
+  public void testStringValMissingSortedLast() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "foo", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "bar", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.SORTED));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    SortField sf = new SortField("value", SortField.Type.STRING);
+    sf.setMissingValue(SortField.STRING_LAST);
+    Sort sort = new Sort(sf);
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    assertEquals("bar", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("foo", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    // null comes last
+    assertNull(searcher.doc(td.scoreDocs[2].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+
+  /** Tests reverse sorting on type string with a missing
+   *  value sorted last */
+  public void testStringValMissingSortedLastReverse() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "foo", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "bar", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.SORTED));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    SortField sf = new SortField("value", SortField.Type.STRING, true);
+    sf.setMissingValue(SortField.STRING_LAST);
+    Sort sort = new Sort(sf);
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // null comes first
+    assertNull(searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("foo", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("bar", searcher.doc(td.scoreDocs[2].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests reverse sorting on type string_val */
+  public void testStringValReverse() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(newStringField("value", "bar", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "foo", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.BINARY));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.STRING_VAL, true));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // 'foo' comes after 'bar' in reverse order
+    assertEquals("foo", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("bar", searcher.doc(td.scoreDocs[1].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on internal docid order */
+  public void testFieldDoc() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(newStringField("value", "foo", Field.Store.NO));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "bar", Field.Store.NO));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(SortField.FIELD_DOC);
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // docid 0, then docid 1
+    assertEquals(0, td.scoreDocs[0].doc);
+    assertEquals(1, td.scoreDocs[1].doc);
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on reverse internal docid order */
+  public void testFieldDocReverse() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(newStringField("value", "foo", Field.Store.NO));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "bar", Field.Store.NO));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField(null, SortField.Type.DOC, true));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // docid 1, then docid 0
+    assertEquals(1, td.scoreDocs[0].doc);
+    assertEquals(0, td.scoreDocs[1].doc);
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests default sort (by score) */
+  public void testFieldScore() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(newTextField("value", "foo bar bar bar bar", Field.Store.NO));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newTextField("value", "foo foo foo foo foo", Field.Store.NO));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort();
+
+    TopDocs actual = searcher.search(new TermQuery(new Term("value", "foo")), 10, sort);
+    assertEquals(2, actual.totalHits);
+
+    TopDocs expected = searcher.search(new TermQuery(new Term("value", "foo")), 10);
+    // the two topdocs should be the same
+    assertEquals(expected.totalHits, actual.totalHits);
+    for (int i = 0; i < actual.scoreDocs.length; i++) {
+      assertEquals(actual.scoreDocs[i].doc, expected.scoreDocs[i].doc);
+    }
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests default sort (by score) in reverse */
+  public void testFieldScoreReverse() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(newTextField("value", "foo bar bar bar bar", Field.Store.NO));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newTextField("value", "foo foo foo foo foo", Field.Store.NO));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField(null, SortField.Type.SCORE, true));
+
+    TopDocs actual = searcher.search(new TermQuery(new Term("value", "foo")), 10, sort);
+    assertEquals(2, actual.totalHits);
+
+    TopDocs expected = searcher.search(new TermQuery(new Term("value", "foo")), 10);
+    // the two topdocs should be the reverse of each other
+    assertEquals(expected.totalHits, actual.totalHits);
+    assertEquals(actual.scoreDocs[0].doc, expected.scoreDocs[1].doc);
+    assertEquals(actual.scoreDocs[1].doc, expected.scoreDocs[0].doc);
+
+    ir.close();
+    dir.close();
+  }
+
+  /** Tests sorting on type int */
+  public void testInt() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new IntField("value", 300000, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new IntField("value", -1, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new IntField("value", 4, Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.INTEGER));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.INT));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // numeric order
+    assertEquals("-1", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("4", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("300000", searcher.doc(td.scoreDocs[2].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type int with a missing value */
+  public void testIntMissing() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new IntField("value", -1, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new IntField("value", 4, Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.INTEGER));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.INT));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // null is treated as a 0
+    assertEquals("-1", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertNull(searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("4", searcher.doc(td.scoreDocs[2].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type int, specifying the missing value should be treated as Integer.MAX_VALUE */
+  public void testIntMissingLast() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new IntField("value", -1, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new IntField("value", 4, Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.INTEGER));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    SortField sortField = new SortField("value", SortField.Type.INT);
+    sortField.setMissingValue(Integer.MAX_VALUE);
+    Sort sort = new Sort(sortField);
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // null is treated as a Integer.MAX_VALUE
+    assertEquals("-1", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("4", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertNull(searcher.doc(td.scoreDocs[2].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type int in reverse */
+  public void testIntReverse() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new IntField("value", 300000, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new IntField("value", -1, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new IntField("value", 4, Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.INTEGER));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.INT, true));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // reverse numeric order
+    assertEquals("300000", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("4", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("-1", searcher.doc(td.scoreDocs[2].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type long */
+  public void testLong() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new LongField("value", 3000000000L, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new LongField("value", -1, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new LongField("value", 4, Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.LONG));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.LONG));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // numeric order
+    assertEquals("-1", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("4", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("3000000000", searcher.doc(td.scoreDocs[2].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type long with a missing value */
+  public void testLongMissing() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new LongField("value", -1, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new LongField("value", 4, Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.LONG));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.LONG));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // null is treated as 0
+    assertEquals("-1", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertNull(searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("4", searcher.doc(td.scoreDocs[2].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type long, specifying the missing value should be treated as Long.MAX_VALUE */
+  public void testLongMissingLast() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new LongField("value", -1, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new LongField("value", 4, Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.LONG));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    SortField sortField = new SortField("value", SortField.Type.LONG);
+    sortField.setMissingValue(Long.MAX_VALUE);
+    Sort sort = new Sort(sortField);
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // null is treated as Long.MAX_VALUE
+    assertEquals("-1", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("4", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertNull(searcher.doc(td.scoreDocs[2].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type long in reverse */
+  public void testLongReverse() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new LongField("value", 3000000000L, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new LongField("value", -1, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new LongField("value", 4, Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.LONG));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.LONG, true));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // reverse numeric order
+    assertEquals("3000000000", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("4", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("-1", searcher.doc(td.scoreDocs[2].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type float */
+  public void testFloat() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new FloatField("value", 30.1f, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new FloatField("value", -1.3f, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new FloatField("value", 4.2f, Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.FLOAT));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.FLOAT));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // numeric order
+    assertEquals("-1.3", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("4.2", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("30.1", searcher.doc(td.scoreDocs[2].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type float with a missing value */
+  public void testFloatMissing() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new FloatField("value", -1.3f, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new FloatField("value", 4.2f, Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.FLOAT));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.FLOAT));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // null is treated as 0
+    assertEquals("-1.3", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertNull(searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("4.2", searcher.doc(td.scoreDocs[2].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type float, specifying the missing value should be treated as Float.MAX_VALUE */
+  public void testFloatMissingLast() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new FloatField("value", -1.3f, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new FloatField("value", 4.2f, Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.FLOAT));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    SortField sortField = new SortField("value", SortField.Type.FLOAT);
+    sortField.setMissingValue(Float.MAX_VALUE);
+    Sort sort = new Sort(sortField);
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // null is treated as Float.MAX_VALUE
+    assertEquals("-1.3", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("4.2", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertNull(searcher.doc(td.scoreDocs[2].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type float in reverse */
+  public void testFloatReverse() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new FloatField("value", 30.1f, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new FloatField("value", -1.3f, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new FloatField("value", 4.2f, Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.FLOAT));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.FLOAT, true));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(3, td.totalHits);
+    // reverse numeric order
+    assertEquals("30.1", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("4.2", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("-1.3", searcher.doc(td.scoreDocs[2].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type double */
+  public void testDouble() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new DoubleField("value", 30.1, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new DoubleField("value", -1.3, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new DoubleField("value", 4.2333333333333, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new DoubleField("value", 4.2333333333332, Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.DOUBLE));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.DOUBLE));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(4, td.totalHits);
+    // numeric order
+    assertEquals("-1.3", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("4.2333333333332", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("4.2333333333333", searcher.doc(td.scoreDocs[2].doc).get("value"));
+    assertEquals("30.1", searcher.doc(td.scoreDocs[3].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type double with +/- zero */
+  public void testDoubleSignedZero() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new DoubleField("value", +0d, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new DoubleField("value", -0d, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.DOUBLE));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.DOUBLE));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // numeric order
+    double v0 = searcher.doc(td.scoreDocs[0].doc).getField("value").numericValue().doubleValue();
+    double v1 = searcher.doc(td.scoreDocs[1].doc).getField("value").numericValue().doubleValue();
+    assertEquals(0, v0, 0d);
+    assertEquals(0, v1, 0d);
+    // check sign bits
+    assertEquals(1, Double.doubleToLongBits(v0) >>> 63);
+    assertEquals(0, Double.doubleToLongBits(v1) >>> 63);
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type double with a missing value */
+  public void testDoubleMissing() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new DoubleField("value", -1.3, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new DoubleField("value", 4.2333333333333, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new DoubleField("value", 4.2333333333332, Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.DOUBLE));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.DOUBLE));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(4, td.totalHits);
+    // null treated as a 0
+    assertEquals("-1.3", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertNull(searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("4.2333333333332", searcher.doc(td.scoreDocs[2].doc).get("value"));
+    assertEquals("4.2333333333333", searcher.doc(td.scoreDocs[3].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type double, specifying the missing value should be treated as Double.MAX_VALUE */
+  public void testDoubleMissingLast() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new DoubleField("value", -1.3, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new DoubleField("value", 4.2333333333333, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new DoubleField("value", 4.2333333333332, Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.DOUBLE));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    SortField sortField = new SortField("value", SortField.Type.DOUBLE);
+    sortField.setMissingValue(Double.MAX_VALUE);
+    Sort sort = new Sort(sortField);
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(4, td.totalHits);
+    // null treated as Double.MAX_VALUE
+    assertEquals("-1.3", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("4.2333333333332", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("4.2333333333333", searcher.doc(td.scoreDocs[2].doc).get("value"));
+    assertNull(searcher.doc(td.scoreDocs[3].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting on type double in reverse */
+  public void testDoubleReverse() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(new DoubleField("value", 30.1, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new DoubleField("value", -1.3, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new DoubleField("value", 4.2333333333333, Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new DoubleField("value", 4.2333333333332, Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), 
+                     Collections.singletonMap("value", Type.DOUBLE));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.DOUBLE, true));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(4, td.totalHits);
+    // numeric order
+    assertEquals("30.1", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("4.2333333333333", searcher.doc(td.scoreDocs[1].doc).get("value"));
+    assertEquals("4.2333333333332", searcher.doc(td.scoreDocs[2].doc).get("value"));
+    assertEquals("-1.3", searcher.doc(td.scoreDocs[3].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  public void testEmptyStringVsNullStringSort() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(
+                        TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    Document doc = new Document();
+    doc.add(newStringField("f", "", Field.Store.NO));
+    doc.add(newStringField("t", "1", Field.Store.NO));
+    w.addDocument(doc);
+    w.commit();
+    doc = new Document();
+    doc.add(newStringField("t", "1", Field.Store.NO));
+    w.addDocument(doc);
+
+    IndexReader r = UninvertingReader.wrap(DirectoryReader.open(w, true), 
+                    Collections.singletonMap("f", Type.SORTED));
+    w.shutdown();
+    IndexSearcher s = newSearcher(r);
+    TopDocs hits = s.search(new TermQuery(new Term("t", "1")), null, 10, new Sort(new SortField("f", SortField.Type.STRING)));
+    assertEquals(2, hits.totalHits);
+    // null sorts first
+    assertEquals(1, hits.scoreDocs[0].doc);
+    assertEquals(0, hits.scoreDocs[1].doc);
+    r.close();
+    dir.close();
+  }
+  
+  /** test that we don't throw exception on multi-valued field (LUCENE-2142) */
+  public void testMultiValuedField() throws IOException {
+    Directory indexStore = newDirectory();
+    IndexWriter writer = new IndexWriter(indexStore, newIndexWriterConfig(
+        TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    for(int i=0; i<5; i++) {
+        Document doc = new Document();
+        doc.add(new StringField("string", "a"+i, Field.Store.NO));
+        doc.add(new StringField("string", "b"+i, Field.Store.NO));
+        writer.addDocument(doc);
+    }
+    writer.forceMerge(1); // enforce one segment to have a higher unique term count in all cases
+    writer.shutdown();
+    Sort sort = new Sort(
+        new SortField("string", SortField.Type.STRING),
+        SortField.FIELD_DOC);
+    // this should not throw AIOOBE or RuntimeEx
+    IndexReader reader = UninvertingReader.wrap(DirectoryReader.open(indexStore),
+                         Collections.singletonMap("string", Type.SORTED));
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(new MatchAllDocsQuery(), null, 500, sort);
+    reader.close();
+    indexStore.close();
+  }
+  
+  public void testMaxScore() throws Exception {
+    Directory d = newDirectory();
+    // Not RIW because we need exactly 2 segs:
+    IndexWriter w = new IndexWriter(d, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    int id = 0;
+    for(int seg=0;seg<2;seg++) {
+      for(int docIDX=0;docIDX<10;docIDX++) {
+        Document doc = new Document();
+        doc.add(new IntField("id", docIDX, Field.Store.YES));
+        StringBuilder sb = new StringBuilder();
+        for(int i=0;i<id;i++) {
+          sb.append(' ');
+          sb.append("text");
+        }
+        doc.add(newTextField("body", sb.toString(), Field.Store.NO));
+        w.addDocument(doc);
+        id++;
+      }
+      w.commit();
+    }
+
+    IndexReader r = UninvertingReader.wrap(DirectoryReader.open(w, true),
+                    Collections.singletonMap("id", Type.INTEGER));
+    w.shutdown();
+    Query q = new TermQuery(new Term("body", "text"));
+    IndexSearcher s = newSearcher(r);
+    float maxScore = s.search(q , 10).getMaxScore();
+    assertEquals(maxScore, s.search(q, null, 3, Sort.INDEXORDER, random().nextBoolean(), true).getMaxScore(), 0.0);
+    assertEquals(maxScore, s.search(q, null, 3, Sort.RELEVANCE, random().nextBoolean(), true).getMaxScore(), 0.0);
+    assertEquals(maxScore, s.search(q, null, 3, new Sort(new SortField[] {new SortField("id", SortField.Type.INT, false)}), random().nextBoolean(), true).getMaxScore(), 0.0);
+    assertEquals(maxScore, s.search(q, null, 3, new Sort(new SortField[] {new SortField("id", SortField.Type.INT, true)}), random().nextBoolean(), true).getMaxScore(), 0.0);
+    r.close();
+    d.close();
+  }
+  
+  /** test sorts when there's nothing in the index */
+  public void testEmptyIndex() throws Exception {
+    IndexSearcher empty = newSearcher(new MultiReader());
+    Query query = new TermQuery(new Term("contents", "foo"));
+  
+    Sort sort = new Sort();
+    TopDocs td = empty.search(query, null, 10, sort, true, true);
+    assertEquals(0, td.totalHits);
+
+    sort.setSort(SortField.FIELD_DOC);
+    td = empty.search(query, null, 10, sort, true, true);
+    assertEquals(0, td.totalHits);
+
+    sort.setSort(new SortField("int", SortField.Type.INT), SortField.FIELD_DOC);
+    td = empty.search(query, null, 10, sort, true, true);
+    assertEquals(0, td.totalHits);
+    
+    sort.setSort(new SortField("string", SortField.Type.STRING, true), SortField.FIELD_DOC);
+    td = empty.search(query, null, 10, sort, true, true);
+    assertEquals(0, td.totalHits);
+    
+    sort.setSort(new SortField("string_val", SortField.Type.STRING_VAL, true), SortField.FIELD_DOC);
+    td = empty.search(query, null, 10, sort, true, true);
+    assertEquals(0, td.totalHits);
+
+    sort.setSort(new SortField("float", SortField.Type.FLOAT), new SortField("string", SortField.Type.STRING));
+    td = empty.search(query, null, 10, sort, true, true);
+    assertEquals(0, td.totalHits);
+  }
+  
+  /** Tests sorting a single document */
+  public void testSortOneDocument() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(newStringField("value", "foo", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(),
+                     Collections.singletonMap("value", Type.SORTED));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.STRING));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(1, td.totalHits);
+    assertEquals("foo", searcher.doc(td.scoreDocs[0].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting a single document with scores */
+  public void testSortOneDocumentWithScores() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(newStringField("value", "foo", Field.Store.YES));
+    writer.addDocument(doc);
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(),
+                     Collections.singletonMap("value", Type.SORTED));
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(new SortField("value", SortField.Type.STRING));
+
+    TopDocs expected = searcher.search(new TermQuery(new Term("value", "foo")), 10);
+    assertEquals(1, expected.totalHits);
+    TopDocs actual = searcher.search(new TermQuery(new Term("value", "foo")), null, 10, sort, true, true);
+    
+    assertEquals(expected.totalHits, actual.totalHits);
+    assertEquals(expected.scoreDocs[0].score, actual.scoreDocs[0].score, 0F);
+
+    ir.close();
+    dir.close();
+  }
+  
+  /** Tests sorting with two fields */
+  public void testSortTwoFields() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(newStringField("tievalue", "tied", Field.Store.NO));
+    doc.add(newStringField("value", "foo", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("tievalue", "tied", Field.Store.NO));
+    doc.add(newStringField("value", "bar", Field.Store.YES));
+    writer.addDocument(doc);
+    Map<String,Type> mappings = new HashMap<>();
+    mappings.put("tievalue", Type.SORTED);
+    mappings.put("value", Type.SORTED);
+    
+    IndexReader ir = UninvertingReader.wrap(writer.getReader(), mappings);
+    writer.shutdown();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    // tievalue, then value
+    Sort sort = new Sort(new SortField("tievalue", SortField.Type.STRING),
+                         new SortField("value", SortField.Type.STRING));
+
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
+    assertEquals(2, td.totalHits);
+    // 'bar' comes before 'foo'
+    assertEquals("bar", searcher.doc(td.scoreDocs[0].doc).get("value"));
+    assertEquals("foo", searcher.doc(td.scoreDocs[1].doc).get("value"));
+
+    ir.close();
+    dir.close();
+  }
+
+  public void testScore() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    doc.add(newStringField("value", "bar", Field.Store.NO));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(newStringField("value", "foo", Field.Store.NO));
+    writer.addDocument(doc);
+    IndexReader ir = writer.getReader();
+    writer.shutdown();
+
+    IndexSearcher searcher = newSearcher(ir);
+    Sort sort = new Sort(SortField.FIELD_SCORE);
+
+    final BooleanQuery bq = new BooleanQuery();
+    bq.add(new TermQuery(new Term("value", "foo")), Occur.SHOULD);
+    bq.add(new MatchAllDocsQuery(), Occur.SHOULD);
+    TopDocs td = searcher.search(bq, 10, sort);
+    assertEquals(2, td.totalHits);
+    assertEquals(1, td.scoreDocs[0].doc);
+    assertEquals(0, td.scoreDocs[1].doc);
+
+    ir.close();
+    dir.close();
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues.java lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues.java
--- lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues.java	2014-05-12 14:43:59.672323377 -0400
@@ -0,0 +1,595 @@
+package org.apache.lucene.uninverting;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.index.SortedSetDocValues.NO_MORE_ORDS;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.lucene42.Lucene42DocValuesFormat;
+import org.apache.lucene.document.BinaryDocValuesField;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
+import org.apache.lucene.document.SortedSetDocValuesField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.index.StoredDocument;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.TermsEnum.SeekStatus;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
+
+public class TestFieldCacheVsDocValues extends LuceneTestCase {
+  
+  public void testByteMissingVsFieldCache() throws Exception {
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      doTestMissingVsFieldCache(Byte.MIN_VALUE, Byte.MAX_VALUE);
+    }
+  }
+  
+  public void testShortMissingVsFieldCache() throws Exception {
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      doTestMissingVsFieldCache(Short.MIN_VALUE, Short.MAX_VALUE);
+    }
+  }
+  
+  public void testIntMissingVsFieldCache() throws Exception {
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      doTestMissingVsFieldCache(Integer.MIN_VALUE, Integer.MAX_VALUE);
+    }
+  }
+  
+  public void testLongMissingVsFieldCache() throws Exception {
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      doTestMissingVsFieldCache(Long.MIN_VALUE, Long.MAX_VALUE);
+    }
+  }
+  
+  public void testSortedFixedLengthVsFieldCache() throws Exception {
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      int fixedLength = TestUtil.nextInt(random(), 1, 10);
+      doTestSortedVsFieldCache(fixedLength, fixedLength);
+    }
+  }
+  
+  public void testSortedVariableLengthVsFieldCache() throws Exception {
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      doTestSortedVsFieldCache(1, 10);
+    }
+  }
+  
+  public void testSortedSetFixedLengthVsUninvertedField() throws Exception {
+    assumeTrue("Codec does not support SORTED_SET", defaultCodecSupportsSortedSet());
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      int fixedLength = TestUtil.nextInt(random(), 1, 10);
+      doTestSortedSetVsUninvertedField(fixedLength, fixedLength);
+    }
+  }
+  
+  public void testSortedSetVariableLengthVsUninvertedField() throws Exception {
+    assumeTrue("Codec does not support SORTED_SET", defaultCodecSupportsSortedSet());
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      doTestSortedSetVsUninvertedField(1, 10);
+    }
+  }
+  
+  // LUCENE-4853
+  public void testHugeBinaryValues() throws Exception {
+    Analyzer analyzer = new MockAnalyzer(random());
+    // FSDirectory because SimpleText will consume gobbs of
+    // space when storing big binary values:
+    Directory d = newFSDirectory(createTempDir("hugeBinaryValues"));
+    boolean doFixed = random().nextBoolean();
+    int numDocs;
+    int fixedLength = 0;
+    if (doFixed) {
+      // Sometimes make all values fixed length since some
+      // codecs have different code paths for this:
+      numDocs = TestUtil.nextInt(random(), 10, 20);
+      fixedLength = TestUtil.nextInt(random(), 65537, 256 * 1024);
+    } else {
+      numDocs = TestUtil.nextInt(random(), 100, 200);
+    }
+    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
+    List<byte[]> docBytes = new ArrayList<>();
+    long totalBytes = 0;
+    for(int docID=0;docID<numDocs;docID++) {
+      // we don't use RandomIndexWriter because it might add
+      // more docvalues than we expect !!!!
+
+      // Must be > 64KB in size to ensure more than 2 pages in
+      // PagedBytes would be needed:
+      int numBytes;
+      if (doFixed) {
+        numBytes = fixedLength;
+      } else if (docID == 0 || random().nextInt(5) == 3) {
+        numBytes = TestUtil.nextInt(random(), 65537, 3 * 1024 * 1024);
+      } else {
+        numBytes = TestUtil.nextInt(random(), 1, 1024 * 1024);
+      }
+      totalBytes += numBytes;
+      if (totalBytes > 5 * 1024*1024) {
+        break;
+      }
+      byte[] bytes = new byte[numBytes];
+      random().nextBytes(bytes);
+      docBytes.add(bytes);
+      Document doc = new Document();      
+      BytesRef b = new BytesRef(bytes);
+      b.length = bytes.length;
+      doc.add(new BinaryDocValuesField("field", b));
+      doc.add(new StringField("id", ""+docID, Field.Store.YES));
+      try {
+        w.addDocument(doc);
+      } catch (IllegalArgumentException iae) {
+        if (iae.getMessage().indexOf("is too large") == -1) {
+          throw iae;
+        } else {
+          // OK: some codecs can't handle binary DV > 32K
+          assertFalse(codecAcceptsHugeBinaryValues("field"));
+          w.rollback();
+          d.close();
+          return;
+        }
+      }
+    }
+    
+    DirectoryReader r;
+    try {
+      r = DirectoryReader.open(w, true);
+    } catch (IllegalArgumentException iae) {
+      if (iae.getMessage().indexOf("is too large") == -1) {
+        throw iae;
+      } else {
+        assertFalse(codecAcceptsHugeBinaryValues("field"));
+
+        // OK: some codecs can't handle binary DV > 32K
+        w.rollback();
+        d.close();
+        return;
+      }
+    }
+    w.shutdown();
+
+    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);
+
+    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, "field", false);
+    for(int docID=0;docID<docBytes.size();docID++) {
+      StoredDocument doc = ar.document(docID);
+      BytesRef bytes = new BytesRef();
+      s.get(docID, bytes);
+      byte[] expected = docBytes.get(Integer.parseInt(doc.get("id")));
+      assertEquals(expected.length, bytes.length);
+      assertEquals(new BytesRef(expected), bytes);
+    }
+
+    assertTrue(codecAcceptsHugeBinaryValues("field"));
+
+    ar.close();
+    d.close();
+  }
+
+  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)
+  public void testHugeBinaryValueLimit() throws Exception {
+    // We only test DVFormats that have a limit
+    assumeFalse("test requires codec with limits on max binary field length", codecAcceptsHugeBinaryValues("field"));
+    Analyzer analyzer = new MockAnalyzer(random());
+    // FSDirectory because SimpleText will consume gobbs of
+    // space when storing big binary values:
+    Directory d = newFSDirectory(createTempDir("hugeBinaryValues"));
+    boolean doFixed = random().nextBoolean();
+    int numDocs;
+    int fixedLength = 0;
+    if (doFixed) {
+      // Sometimes make all values fixed length since some
+      // codecs have different code paths for this:
+      numDocs = TestUtil.nextInt(random(), 10, 20);
+      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;
+    } else {
+      numDocs = TestUtil.nextInt(random(), 100, 200);
+    }
+    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
+    List<byte[]> docBytes = new ArrayList<>();
+    long totalBytes = 0;
+    for(int docID=0;docID<numDocs;docID++) {
+      // we don't use RandomIndexWriter because it might add
+      // more docvalues than we expect !!!!
+
+      // Must be > 64KB in size to ensure more than 2 pages in
+      // PagedBytes would be needed:
+      int numBytes;
+      if (doFixed) {
+        numBytes = fixedLength;
+      } else if (docID == 0 || random().nextInt(5) == 3) {
+        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;
+      } else {
+        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);
+      }
+      totalBytes += numBytes;
+      if (totalBytes > 5 * 1024*1024) {
+        break;
+      }
+      byte[] bytes = new byte[numBytes];
+      random().nextBytes(bytes);
+      docBytes.add(bytes);
+      Document doc = new Document();      
+      BytesRef b = new BytesRef(bytes);
+      b.length = bytes.length;
+      doc.add(new BinaryDocValuesField("field", b));
+      doc.add(new StringField("id", ""+docID, Field.Store.YES));
+      w.addDocument(doc);
+    }
+    
+    DirectoryReader r = DirectoryReader.open(w, true);
+    w.shutdown();
+
+    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);
+
+    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, "field", false);
+    for(int docID=0;docID<docBytes.size();docID++) {
+      StoredDocument doc = ar.document(docID);
+      BytesRef bytes = new BytesRef();
+      s.get(docID, bytes);
+      byte[] expected = docBytes.get(Integer.parseInt(doc.get("id")));
+      assertEquals(expected.length, bytes.length);
+      assertEquals(new BytesRef(expected), bytes);
+    }
+
+    ar.close();
+    d.close();
+  }
+  
+  private void doTestSortedVsFieldCache(int minLength, int maxLength) throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);
+    Document doc = new Document();
+    Field idField = new StringField("id", "", Field.Store.NO);
+    Field indexedField = new StringField("indexed", "", Field.Store.NO);
+    Field dvField = new SortedDocValuesField("dv", new BytesRef());
+    doc.add(idField);
+    doc.add(indexedField);
+    doc.add(dvField);
+    
+    // index some docs
+    int numDocs = atLeast(300);
+    for (int i = 0; i < numDocs; i++) {
+      idField.setStringValue(Integer.toString(i));
+      final int length;
+      if (minLength == maxLength) {
+        length = minLength; // fixed length
+      } else {
+        length = TestUtil.nextInt(random(), minLength, maxLength);
+      }
+      String value = TestUtil.randomSimpleString(random(), length);
+      indexedField.setStringValue(value);
+      dvField.setBytesValue(new BytesRef(value));
+      writer.addDocument(doc);
+      if (random().nextInt(31) == 0) {
+        writer.commit();
+      }
+    }
+    
+    // delete some docs
+    int numDeletions = random().nextInt(numDocs/10);
+    for (int i = 0; i < numDeletions; i++) {
+      int id = random().nextInt(numDocs);
+      writer.deleteDocuments(new Term("id", Integer.toString(id)));
+    }
+    writer.shutdown();
+    
+    // compare
+    DirectoryReader ir = DirectoryReader.open(dir);
+    for (AtomicReaderContext context : ir.leaves()) {
+      AtomicReader r = context.reader();
+      SortedDocValues expected = FieldCache.DEFAULT.getTermsIndex(r, "indexed");
+      SortedDocValues actual = r.getSortedDocValues("dv");
+      assertEquals(r.maxDoc(), expected, actual);
+    }
+    ir.close();
+    dir.close();
+  }
+  
+  private void doTestSortedSetVsUninvertedField(int minLength, int maxLength) throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);
+    
+    // index some docs
+    int numDocs = atLeast(300);
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      Field idField = new StringField("id", Integer.toString(i), Field.Store.NO);
+      doc.add(idField);
+      final int length;
+      if (minLength == maxLength) {
+        length = minLength; // fixed length
+      } else {
+        length = TestUtil.nextInt(random(), minLength, maxLength);
+      }
+      int numValues = random().nextInt(17);
+      // create a random list of strings
+      List<String> values = new ArrayList<>();
+      for (int v = 0; v < numValues; v++) {
+        values.add(TestUtil.randomSimpleString(random(), length));
+      }
+      
+      // add in any order to the indexed field
+      ArrayList<String> unordered = new ArrayList<>(values);
+      Collections.shuffle(unordered, random());
+      for (String v : values) {
+        doc.add(newStringField("indexed", v, Field.Store.NO));
+      }
+
+      // add in any order to the dv field
+      ArrayList<String> unordered2 = new ArrayList<>(values);
+      Collections.shuffle(unordered2, random());
+      for (String v : unordered2) {
+        doc.add(new SortedSetDocValuesField("dv", new BytesRef(v)));
+      }
+
+      writer.addDocument(doc);
+      if (random().nextInt(31) == 0) {
+        writer.commit();
+      }
+    }
+    
+    // delete some docs
+    int numDeletions = random().nextInt(numDocs/10);
+    for (int i = 0; i < numDeletions; i++) {
+      int id = random().nextInt(numDocs);
+      writer.deleteDocuments(new Term("id", Integer.toString(id)));
+    }
+    
+    // compare per-segment
+    DirectoryReader ir = writer.getReader();
+    for (AtomicReaderContext context : ir.leaves()) {
+      AtomicReader r = context.reader();
+      SortedSetDocValues expected = FieldCache.DEFAULT.getDocTermOrds(r, "indexed", null);
+      SortedSetDocValues actual = r.getSortedSetDocValues("dv");
+      assertEquals(r.maxDoc(), expected, actual);
+    }
+    ir.close();
+    
+    writer.forceMerge(1);
+    
+    // now compare again after the merge
+    ir = writer.getReader();
+    AtomicReader ar = getOnlySegmentReader(ir);
+    SortedSetDocValues expected = FieldCache.DEFAULT.getDocTermOrds(ar, "indexed", null);
+    SortedSetDocValues actual = ar.getSortedSetDocValues("dv");
+    assertEquals(ir.maxDoc(), expected, actual);
+    ir.close();
+    
+    writer.shutdown();
+    dir.close();
+  }
+  
+  private void doTestMissingVsFieldCache(LongProducer longs) throws Exception {
+    assumeTrue("Codec does not support getDocsWithField", defaultCodecSupportsDocsWithField());
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);
+    Field idField = new StringField("id", "", Field.Store.NO);
+    Field indexedField = newStringField("indexed", "", Field.Store.NO);
+    Field dvField = new NumericDocValuesField("dv", 0);
+
+    
+    // index some docs
+    int numDocs = atLeast(300);
+    // numDocs should be always > 256 so that in case of a codec that optimizes
+    // for numbers of values <= 256, all storage layouts are tested
+    assert numDocs > 256;
+    for (int i = 0; i < numDocs; i++) {
+      idField.setStringValue(Integer.toString(i));
+      long value = longs.next();
+      indexedField.setStringValue(Long.toString(value));
+      dvField.setLongValue(value);
+      Document doc = new Document();
+      doc.add(idField);
+      // 1/4 of the time we neglect to add the fields
+      if (random().nextInt(4) > 0) {
+        doc.add(indexedField);
+        doc.add(dvField);
+      }
+      writer.addDocument(doc);
+      if (random().nextInt(31) == 0) {
+        writer.commit();
+      }
+    }
+    
+    // delete some docs
+    int numDeletions = random().nextInt(numDocs/10);
+    for (int i = 0; i < numDeletions; i++) {
+      int id = random().nextInt(numDocs);
+      writer.deleteDocuments(new Term("id", Integer.toString(id)));
+    }
+
+    // merge some segments and ensure that at least one of them has more than
+    // 256 values
+    writer.forceMerge(numDocs / 256);
+
+    writer.shutdown();
+    
+    // compare
+    DirectoryReader ir = DirectoryReader.open(dir);
+    for (AtomicReaderContext context : ir.leaves()) {
+      AtomicReader r = context.reader();
+      Bits expected = FieldCache.DEFAULT.getDocsWithField(r, "indexed");
+      Bits actual = FieldCache.DEFAULT.getDocsWithField(r, "dv");
+      assertEquals(expected, actual);
+    }
+    ir.close();
+    dir.close();
+  }
+  
+  private void doTestMissingVsFieldCache(final long minValue, final long maxValue) throws Exception {
+    doTestMissingVsFieldCache(new LongProducer() {
+      @Override
+      long next() {
+        return TestUtil.nextLong(random(), minValue, maxValue);
+      }
+    });
+  }
+  
+  static abstract class LongProducer {
+    abstract long next();
+  }
+
+  private void assertEquals(Bits expected, Bits actual) throws Exception {
+    assertEquals(expected.length(), actual.length());
+    for (int i = 0; i < expected.length(); i++) {
+      assertEquals(expected.get(i), actual.get(i));
+    }
+  }
+  
+  private void assertEquals(int maxDoc, SortedDocValues expected, SortedDocValues actual) throws Exception {
+    assertEquals(maxDoc, DocValues.singleton(expected), DocValues.singleton(actual));
+  }
+  
+  private void assertEquals(int maxDoc, SortedSetDocValues expected, SortedSetDocValues actual) throws Exception {
+    // can be null for the segment if no docs actually had any SortedDocValues
+    // in this case FC.getDocTermsOrds returns EMPTY
+    if (actual == null) {
+      assertEquals(DocValues.EMPTY_SORTED_SET, expected);
+      return;
+    }
+    assertEquals(expected.getValueCount(), actual.getValueCount());
+    // compare ord lists
+    for (int i = 0; i < maxDoc; i++) {
+      expected.setDocument(i);
+      actual.setDocument(i);
+      long expectedOrd;
+      while ((expectedOrd = expected.nextOrd()) != NO_MORE_ORDS) {
+        assertEquals(expectedOrd, actual.nextOrd());
+      }
+      assertEquals(NO_MORE_ORDS, actual.nextOrd());
+    }
+    
+    // compare ord dictionary
+    BytesRef expectedBytes = new BytesRef();
+    BytesRef actualBytes = new BytesRef();
+    for (long i = 0; i < expected.getValueCount(); i++) {
+      expected.lookupTerm(expectedBytes);
+      actual.lookupTerm(actualBytes);
+      assertEquals(expectedBytes, actualBytes);
+    }
+    
+    // compare termsenum
+    assertEquals(expected.getValueCount(), expected.termsEnum(), actual.termsEnum());
+  }
+  
+  private void assertEquals(long numOrds, TermsEnum expected, TermsEnum actual) throws Exception {
+    BytesRef ref;
+    
+    // sequential next() through all terms
+    while ((ref = expected.next()) != null) {
+      assertEquals(ref, actual.next());
+      assertEquals(expected.ord(), actual.ord());
+      assertEquals(expected.term(), actual.term());
+    }
+    assertNull(actual.next());
+    
+    // sequential seekExact(ord) through all terms
+    for (long i = 0; i < numOrds; i++) {
+      expected.seekExact(i);
+      actual.seekExact(i);
+      assertEquals(expected.ord(), actual.ord());
+      assertEquals(expected.term(), actual.term());
+    }
+    
+    // sequential seekExact(BytesRef) through all terms
+    for (long i = 0; i < numOrds; i++) {
+      expected.seekExact(i);
+      assertTrue(actual.seekExact(expected.term()));
+      assertEquals(expected.ord(), actual.ord());
+      assertEquals(expected.term(), actual.term());
+    }
+    
+    // sequential seekCeil(BytesRef) through all terms
+    for (long i = 0; i < numOrds; i++) {
+      expected.seekExact(i);
+      assertEquals(SeekStatus.FOUND, actual.seekCeil(expected.term()));
+      assertEquals(expected.ord(), actual.ord());
+      assertEquals(expected.term(), actual.term());
+    }
+    
+    // random seekExact(ord)
+    for (long i = 0; i < numOrds; i++) {
+      long randomOrd = TestUtil.nextLong(random(), 0, numOrds - 1);
+      expected.seekExact(randomOrd);
+      actual.seekExact(randomOrd);
+      assertEquals(expected.ord(), actual.ord());
+      assertEquals(expected.term(), actual.term());
+    }
+    
+    // random seekExact(BytesRef)
+    for (long i = 0; i < numOrds; i++) {
+      long randomOrd = TestUtil.nextLong(random(), 0, numOrds - 1);
+      expected.seekExact(randomOrd);
+      actual.seekExact(expected.term());
+      assertEquals(expected.ord(), actual.ord());
+      assertEquals(expected.term(), actual.term());
+    }
+    
+    // random seekCeil(BytesRef)
+    for (long i = 0; i < numOrds; i++) {
+      BytesRef target = new BytesRef(TestUtil.randomUnicodeString(random()));
+      SeekStatus expectedStatus = expected.seekCeil(target);
+      assertEquals(expectedStatus, actual.seekCeil(target));
+      if (expectedStatus != SeekStatus.END) {
+        assertEquals(expected.ord(), actual.ord());
+        assertEquals(expected.term(), actual.term());
+      }
+    }
+  }
+  
+  protected boolean codecAcceptsHugeBinaryValues(String field) {
+    String name = Codec.getDefault().getName();
+    return !(name.equals("Lucene40") || name.equals("Lucene41") || name.equals("Lucene42") || name.equals("Memory") || name.equals("Direct"));
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheWithThreads.java lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheWithThreads.java
--- lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheWithThreads.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheWithThreads.java	2014-05-12 13:28:55.836244946 -0400
@@ -0,0 +1,231 @@
+package org.apache.lucene.uninverting;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Random;
+import java.util.Set;
+import java.util.concurrent.CountDownLatch;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.BinaryDocValuesField;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
+
+public class TestFieldCacheWithThreads extends LuceneTestCase {
+
+  public void test() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
+
+    final List<Long> numbers = new ArrayList<>();
+    final List<BytesRef> binary = new ArrayList<>();
+    final List<BytesRef> sorted = new ArrayList<>();
+    final int numDocs = atLeast(100);
+    for(int i=0;i<numDocs;i++) {
+      Document d = new Document();
+      long number = random().nextLong();
+      d.add(new NumericDocValuesField("number", number));
+      BytesRef bytes = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));
+      d.add(new BinaryDocValuesField("bytes", bytes));
+      binary.add(bytes);
+      bytes = new BytesRef(TestUtil.randomRealisticUnicodeString(random()));
+      d.add(new SortedDocValuesField("sorted", bytes));
+      sorted.add(bytes);
+      w.addDocument(d);
+      numbers.add(number);
+    }
+
+    w.forceMerge(1);
+    final IndexReader r = DirectoryReader.open(w, true);
+    w.shutdown();
+
+    assertEquals(1, r.leaves().size());
+    final AtomicReader ar = r.leaves().get(0).reader();
+
+    int numThreads = TestUtil.nextInt(random(), 2, 5);
+    List<Thread> threads = new ArrayList<>();
+    final CountDownLatch startingGun = new CountDownLatch(1);
+    for(int t=0;t<numThreads;t++) {
+      final Random threadRandom = new Random(random().nextLong());
+      Thread thread = new Thread() {
+          @Override
+          public void run() {
+            try {
+              //NumericDocValues ndv = ar.getNumericDocValues("number");
+              NumericDocValues ndv = FieldCache.DEFAULT.getNumerics(ar, "number", FieldCache.NUMERIC_UTILS_LONG_PARSER, false);
+              //BinaryDocValues bdv = ar.getBinaryDocValues("bytes");
+              BinaryDocValues bdv = FieldCache.DEFAULT.getTerms(ar, "bytes", false);
+              SortedDocValues sdv = FieldCache.DEFAULT.getTermsIndex(ar, "sorted");
+              startingGun.await();
+              int iters = atLeast(1000);
+              BytesRef scratch = new BytesRef();
+              BytesRef scratch2 = new BytesRef();
+              for(int iter=0;iter<iters;iter++) {
+                int docID = threadRandom.nextInt(numDocs);
+                switch(threadRandom.nextInt(4)) {
+                case 0:
+                  assertEquals(numbers.get(docID).longValue(), FieldCache.DEFAULT.getNumerics(ar, "number", FieldCache.NUMERIC_UTILS_INT_PARSER, false).get(docID));
+                  break;
+                case 1:
+                  assertEquals(numbers.get(docID).longValue(), FieldCache.DEFAULT.getNumerics(ar, "number", FieldCache.NUMERIC_UTILS_LONG_PARSER, false).get(docID));
+                  break;
+                case 2:
+                  assertEquals(numbers.get(docID).longValue(), FieldCache.DEFAULT.getNumerics(ar, "number", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, false).get(docID));
+                  break;
+                case 3:
+                  assertEquals(numbers.get(docID).longValue(), FieldCache.DEFAULT.getNumerics(ar, "number", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, false).get(docID));
+                  break;
+                }
+                bdv.get(docID, scratch);
+                assertEquals(binary.get(docID), scratch);
+                // Cannot share a single scratch against two "sources":
+                sdv.get(docID, scratch2);
+                assertEquals(sorted.get(docID), scratch2);
+              }
+            } catch (Exception e) {
+              throw new RuntimeException(e);
+            }
+          }
+        };
+      thread.start();
+      threads.add(thread);
+    }
+
+    startingGun.countDown();
+
+    for(Thread thread : threads) {
+      thread.join();
+    }
+
+    r.close();
+    dir.close();
+  }
+  
+  public void test2() throws Exception {
+    Random random = random();
+    final int NUM_DOCS = atLeast(100);
+    final Directory dir = newDirectory();
+    final RandomIndexWriter writer = new RandomIndexWriter(random, dir);
+    final boolean allowDups = random.nextBoolean();
+    final Set<String> seen = new HashSet<>();
+    if (VERBOSE) {
+      System.out.println("TEST: NUM_DOCS=" + NUM_DOCS + " allowDups=" + allowDups);
+    }
+    int numDocs = 0;
+    final List<BytesRef> docValues = new ArrayList<>();
+
+    // TODO: deletions
+    while (numDocs < NUM_DOCS) {
+      final String s;
+      if (random.nextBoolean()) {
+        s = TestUtil.randomSimpleString(random);
+      } else {
+        s = TestUtil.randomUnicodeString(random);
+      }
+      final BytesRef br = new BytesRef(s);
+
+      if (!allowDups) {
+        if (seen.contains(s)) {
+          continue;
+        }
+        seen.add(s);
+      }
+
+      if (VERBOSE) {
+        System.out.println("  " + numDocs + ": s=" + s);
+      }
+      
+      final Document doc = new Document();
+      doc.add(new SortedDocValuesField("stringdv", br));
+      doc.add(new NumericDocValuesField("id", numDocs));
+      docValues.add(br);
+      writer.addDocument(doc);
+      numDocs++;
+
+      if (random.nextInt(40) == 17) {
+        // force flush
+        writer.getReader().close();
+      }
+    }
+
+    writer.forceMerge(1);
+    final DirectoryReader r = writer.getReader();
+    writer.shutdown();
+    
+    final AtomicReader sr = getOnlySegmentReader(r);
+
+    final long END_TIME = System.currentTimeMillis() + (TEST_NIGHTLY ? 30 : 1);
+
+    final int NUM_THREADS = TestUtil.nextInt(random(), 1, 10);
+    Thread[] threads = new Thread[NUM_THREADS];
+    for(int thread=0;thread<NUM_THREADS;thread++) {
+      threads[thread] = new Thread() {
+          @Override
+          public void run() {
+            Random random = random();            
+            final SortedDocValues stringDVDirect;
+            final NumericDocValues docIDToID;
+            try {
+              stringDVDirect = sr.getSortedDocValues("stringdv");
+              docIDToID = sr.getNumericDocValues("id");
+              assertNotNull(stringDVDirect);
+            } catch (IOException ioe) {
+              throw new RuntimeException(ioe);
+            }
+            while(System.currentTimeMillis() < END_TIME) {
+              final SortedDocValues source;
+              source = stringDVDirect;
+              final BytesRef scratch = new BytesRef();
+
+              for(int iter=0;iter<100;iter++) {
+                final int docID = random.nextInt(sr.maxDoc());
+                source.get(docID, scratch);
+                assertEquals(docValues.get((int) docIDToID.get(docID)), scratch);
+              }
+            }
+          }
+        };
+      threads[thread].start();
+    }
+
+    for(Thread thread : threads) {
+      thread.join();
+    }
+
+    r.close();
+    dir.close();
+  }
+
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms32.java lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms32.java
--- lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms32.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms32.java	2014-05-12 13:28:55.836244946 -0400
@@ -0,0 +1,156 @@
+package org.apache.lucene.uninverting;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.IntField;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.NumericRangeQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.SortField;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class TestNumericTerms32 extends LuceneTestCase {
+  // distance of entries
+  private static int distance;
+  // shift the starting of the values to the left, to also have negative values:
+  private static final int startOffset = - 1 << 15;
+  // number of docs to generate for testing
+  private static int noDocs;
+  
+  private static Directory directory = null;
+  private static IndexReader reader = null;
+  private static IndexSearcher searcher = null;
+  
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    noDocs = atLeast(4096);
+    distance = (1 << 30) / noDocs;
+    directory = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))
+        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))
+        .setMergePolicy(newLogMergePolicy()));
+    
+    final FieldType storedInt = new FieldType(IntField.TYPE_NOT_STORED);
+    storedInt.setStored(true);
+    storedInt.freeze();
+
+    final FieldType storedInt8 = new FieldType(storedInt);
+    storedInt8.setNumericPrecisionStep(8);
+
+    final FieldType storedInt4 = new FieldType(storedInt);
+    storedInt4.setNumericPrecisionStep(4);
+
+    final FieldType storedInt2 = new FieldType(storedInt);
+    storedInt2.setNumericPrecisionStep(2);
+
+    IntField
+      field8 = new IntField("field8", 0, storedInt8),
+      field4 = new IntField("field4", 0, storedInt4),
+      field2 = new IntField("field2", 0, storedInt2);
+    
+    Document doc = new Document();
+    // add fields, that have a distance to test general functionality
+    doc.add(field8); doc.add(field4); doc.add(field2);
+    
+    // Add a series of noDocs docs with increasing int values
+    for (int l=0; l<noDocs; l++) {
+      int val=distance*l+startOffset;
+      field8.setIntValue(val);
+      field4.setIntValue(val);
+      field2.setIntValue(val);
+
+      val=l-(noDocs/2);
+      writer.addDocument(doc);
+    }
+  
+    Map<String,Type> map = new HashMap<>();
+    map.put("field2", Type.INTEGER);
+    map.put("field4", Type.INTEGER);
+    map.put("field8", Type.INTEGER);
+    reader = UninvertingReader.wrap(writer.getReader(), map);
+    searcher=newSearcher(reader);
+    writer.shutdown();
+  }
+  
+  @AfterClass
+  public static void afterClass() throws Exception {
+    searcher = null;
+    reader.close();
+    reader = null;
+    directory.close();
+    directory = null;
+  }
+  
+  private void testSorting(int precisionStep) throws Exception {
+    String field="field"+precisionStep;
+    // 10 random tests, the index order is ascending,
+    // so using a reverse sort field should retun descending documents
+    int num = TestUtil.nextInt(random(), 10, 20);
+    for (int i = 0; i < num; i++) {
+      int lower=(int)(random().nextDouble()*noDocs*distance)+startOffset;
+      int upper=(int)(random().nextDouble()*noDocs*distance)+startOffset;
+      if (lower>upper) {
+        int a=lower; lower=upper; upper=a;
+      }
+      Query tq=NumericRangeQuery.newIntRange(field, precisionStep, lower, upper, true, true);
+      TopDocs topDocs = searcher.search(tq, null, noDocs, new Sort(new SortField(field, SortField.Type.INT, true)));
+      if (topDocs.totalHits==0) continue;
+      ScoreDoc[] sd = topDocs.scoreDocs;
+      assertNotNull(sd);
+      int last = searcher.doc(sd[0].doc).getField(field).numericValue().intValue();
+      for (int j=1; j<sd.length; j++) {
+        int act = searcher.doc(sd[j].doc).getField(field).numericValue().intValue();
+        assertTrue("Docs should be sorted backwards", last>act );
+        last=act;
+      }
+    }
+  }
+
+  @Test
+  public void testSorting_8bit() throws Exception {
+    testSorting(8);
+  }
+  
+  @Test
+  public void testSorting_4bit() throws Exception {
+    testSorting(4);
+  }
+  
+  @Test
+  public void testSorting_2bit() throws Exception {
+    testSorting(2);
+  }  
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64.java lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64.java
--- lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestNumericTerms64.java	2014-05-12 13:28:57.008244966 -0400
@@ -0,0 +1,166 @@
+package org.apache.lucene.uninverting;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.LongField;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.NumericRangeQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.SortField;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class TestNumericTerms64 extends LuceneTestCase {
+  // distance of entries
+  private static long distance;
+  // shift the starting of the values to the left, to also have negative values:
+  private static final long startOffset = - 1L << 31;
+  // number of docs to generate for testing
+  private static int noDocs;
+  
+  private static Directory directory = null;
+  private static IndexReader reader = null;
+  private static IndexSearcher searcher = null;
+  
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    noDocs = atLeast(4096);
+    distance = (1L << 60) / noDocs;
+    directory = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))
+        .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))
+        .setMergePolicy(newLogMergePolicy()));
+
+    final FieldType storedLong = new FieldType(LongField.TYPE_NOT_STORED);
+    storedLong.setStored(true);
+    storedLong.freeze();
+
+    final FieldType storedLong8 = new FieldType(storedLong);
+    storedLong8.setNumericPrecisionStep(8);
+
+    final FieldType storedLong4 = new FieldType(storedLong);
+    storedLong4.setNumericPrecisionStep(4);
+
+    final FieldType storedLong6 = new FieldType(storedLong);
+    storedLong6.setNumericPrecisionStep(6);
+
+    final FieldType storedLong2 = new FieldType(storedLong);
+    storedLong2.setNumericPrecisionStep(2);
+
+    LongField
+      field8 = new LongField("field8", 0L, storedLong8),
+      field6 = new LongField("field6", 0L, storedLong6),
+      field4 = new LongField("field4", 0L, storedLong4),
+      field2 = new LongField("field2", 0L, storedLong2);
+
+    Document doc = new Document();
+    // add fields, that have a distance to test general functionality
+    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2);
+    
+    // Add a series of noDocs docs with increasing long values, by updating the fields
+    for (int l=0; l<noDocs; l++) {
+      long val=distance*l+startOffset;
+      field8.setLongValue(val);
+      field6.setLongValue(val);
+      field4.setLongValue(val);
+      field2.setLongValue(val);
+
+      val=l-(noDocs/2);
+      writer.addDocument(doc);
+    }
+    Map<String,Type> map = new HashMap<>();
+    map.put("field2", Type.LONG);
+    map.put("field4", Type.LONG);
+    map.put("field6", Type.LONG);
+    map.put("field8", Type.LONG);
+    reader = UninvertingReader.wrap(writer.getReader(), map);
+    searcher=newSearcher(reader);
+    writer.shutdown();
+  }
+  
+  @AfterClass
+  public static void afterClass() throws Exception {
+    searcher = null;
+    reader.close();
+    reader = null;
+    directory.close();
+    directory = null;
+  }
+  
+  private void testSorting(int precisionStep) throws Exception {
+    String field="field"+precisionStep;
+    // 10 random tests, the index order is ascending,
+    // so using a reverse sort field should retun descending documents
+    int num = TestUtil.nextInt(random(), 10, 20);
+    for (int i = 0; i < num; i++) {
+      long lower=(long)(random().nextDouble()*noDocs*distance)+startOffset;
+      long upper=(long)(random().nextDouble()*noDocs*distance)+startOffset;
+      if (lower>upper) {
+        long a=lower; lower=upper; upper=a;
+      }
+      Query tq=NumericRangeQuery.newLongRange(field, precisionStep, lower, upper, true, true);
+      TopDocs topDocs = searcher.search(tq, null, noDocs, new Sort(new SortField(field, SortField.Type.LONG, true)));
+      if (topDocs.totalHits==0) continue;
+      ScoreDoc[] sd = topDocs.scoreDocs;
+      assertNotNull(sd);
+      long last=searcher.doc(sd[0].doc).getField(field).numericValue().longValue();
+      for (int j=1; j<sd.length; j++) {
+        long act=searcher.doc(sd[j].doc).getField(field).numericValue().longValue();
+        assertTrue("Docs should be sorted backwards", last>act );
+        last=act;
+      }
+    }
+  }
+
+  @Test
+  public void testSorting_8bit() throws Exception {
+    testSorting(8);
+  }
+  
+  @Test
+  public void testSorting_6bit() throws Exception {
+    testSorting(6);
+  }
+  
+  @Test
+  public void testSorting_4bit() throws Exception {
+    testSorting(4);
+  }
+  
+  @Test
+  public void testSorting_2bit() throws Exception {
+    testSorting(2);
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestUninvertingReader.java lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestUninvertingReader.java
--- lucene-trunk/lucene/misc/src/test/org/apache/lucene/uninverting/TestUninvertingReader.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/misc/src/test/org/apache/lucene/uninverting/TestUninvertingReader.java	2014-05-13 10:07:55.577539531 -0400
@@ -0,0 +1,248 @@
+package org.apache.lucene.uninverting;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.EnumSet;
+import java.util.Set;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.LongField;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.NumericUtils;
+
+public class TestUninvertingReader extends LuceneTestCase {
+  
+  public void testSortedSetInteger() throws IOException {
+    Directory dir = newDirectory();
+    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
+    
+    Document doc = new Document();
+    doc.add(new IntField("foo", 5, Field.Store.NO));
+    iw.addDocument(doc);
+    
+    doc = new Document();
+    doc.add(new IntField("foo", 5, Field.Store.NO));
+    doc.add(new IntField("foo", -3, Field.Store.NO));
+    iw.addDocument(doc);
+    
+    iw.forceMerge(1);
+    iw.shutdown();
+    
+    DirectoryReader ir = UninvertingReader.wrap(DirectoryReader.open(dir), 
+                         Collections.singletonMap("foo", Type.SORTED_SET_INTEGER));
+    AtomicReader ar = ir.leaves().get(0).reader();
+    assertNoSilentInsanity(ar, "foo", DocValuesType.SORTED_SET);
+    SortedSetDocValues v = ar.getSortedSetDocValues("foo");
+    assertNoSilentInsanity(ar, "foo", DocValuesType.SORTED_SET);
+    assertEquals(2, v.getValueCount());
+    
+    v.setDocument(0);
+    assertEquals(1, v.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, v.nextOrd());
+    
+    v.setDocument(1);
+    assertEquals(0, v.nextOrd());
+    assertEquals(1, v.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, v.nextOrd());
+    
+    BytesRef value = new BytesRef();
+    v.lookupOrd(0, value);
+    assertEquals(-3, NumericUtils.prefixCodedToInt(value));
+    
+    v.lookupOrd(1, value);
+    assertEquals(5, NumericUtils.prefixCodedToInt(value));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testSortedSetFloat() throws IOException {
+    Directory dir = newDirectory();
+    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
+    
+    Document doc = new Document();
+    doc.add(new IntField("foo", Float.floatToRawIntBits(5f), Field.Store.NO));
+    iw.addDocument(doc);
+    
+    doc = new Document();
+    doc.add(new IntField("foo", Float.floatToRawIntBits(5f), Field.Store.NO));
+    doc.add(new IntField("foo", Float.floatToRawIntBits(-3f), Field.Store.NO));
+    iw.addDocument(doc);
+    
+    iw.forceMerge(1);
+    iw.shutdown();
+    
+    DirectoryReader ir = UninvertingReader.wrap(DirectoryReader.open(dir), 
+                         Collections.singletonMap("foo", Type.SORTED_SET_FLOAT));
+    AtomicReader ar = ir.leaves().get(0).reader();
+    
+    assertNoSilentInsanity(ar, "foo", DocValuesType.SORTED_SET);
+    SortedSetDocValues v = ar.getSortedSetDocValues("foo");
+    assertNoSilentInsanity(ar, "foo", DocValuesType.SORTED_SET);
+    assertEquals(2, v.getValueCount());
+    
+    v.setDocument(0);
+    assertEquals(1, v.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, v.nextOrd());
+    
+    v.setDocument(1);
+    assertEquals(0, v.nextOrd());
+    assertEquals(1, v.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, v.nextOrd());
+    
+    BytesRef value = new BytesRef();
+    v.lookupOrd(0, value);
+    assertEquals(Float.floatToRawIntBits(-3f), NumericUtils.prefixCodedToInt(value));
+    
+    v.lookupOrd(1, value);
+    assertEquals(Float.floatToRawIntBits(5f), NumericUtils.prefixCodedToInt(value));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testSortedSetLong() throws IOException {
+    Directory dir = newDirectory();
+    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
+    
+    Document doc = new Document();
+    doc.add(new LongField("foo", 5, Field.Store.NO));
+    iw.addDocument(doc);
+    
+    doc = new Document();
+    doc.add(new LongField("foo", 5, Field.Store.NO));
+    doc.add(new LongField("foo", -3, Field.Store.NO));
+    iw.addDocument(doc);
+    
+    iw.forceMerge(1);
+    iw.shutdown();
+    
+    DirectoryReader ir = UninvertingReader.wrap(DirectoryReader.open(dir), 
+        Collections.singletonMap("foo", Type.SORTED_SET_LONG));
+    AtomicReader ar = ir.leaves().get(0).reader();
+    assertNoSilentInsanity(ar, "foo", DocValuesType.SORTED_SET);
+    SortedSetDocValues v = ar.getSortedSetDocValues("foo");
+    assertNoSilentInsanity(ar, "foo", DocValuesType.SORTED_SET);
+    assertEquals(2, v.getValueCount());
+    
+    v.setDocument(0);
+    assertEquals(1, v.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, v.nextOrd());
+    
+    v.setDocument(1);
+    assertEquals(0, v.nextOrd());
+    assertEquals(1, v.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, v.nextOrd());
+    
+    BytesRef value = new BytesRef();
+    v.lookupOrd(0, value);
+    assertEquals(-3, NumericUtils.prefixCodedToLong(value));
+    
+    v.lookupOrd(1, value);
+    assertEquals(5, NumericUtils.prefixCodedToLong(value));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testSortedSetDouble() throws IOException {
+    Directory dir = newDirectory();
+    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
+    
+    Document doc = new Document();
+    doc.add(new LongField("foo", Double.doubleToRawLongBits(5d), Field.Store.NO));
+    iw.addDocument(doc);
+    
+    doc = new Document();
+    doc.add(new LongField("foo", Double.doubleToRawLongBits(5d), Field.Store.NO));
+    doc.add(new LongField("foo", Double.doubleToRawLongBits(-3d), Field.Store.NO));
+    iw.addDocument(doc);
+    
+    iw.forceMerge(1);
+    iw.shutdown();
+    
+    DirectoryReader ir = UninvertingReader.wrap(DirectoryReader.open(dir), 
+        Collections.singletonMap("foo", Type.SORTED_SET_DOUBLE));
+    AtomicReader ar = ir.leaves().get(0).reader();
+    assertNoSilentInsanity(ar, "foo", DocValuesType.SORTED_SET);
+    SortedSetDocValues v = ar.getSortedSetDocValues("foo");
+    assertNoSilentInsanity(ar, "foo", DocValuesType.SORTED_SET);
+    assertEquals(2, v.getValueCount());
+    
+    v.setDocument(0);
+    assertEquals(1, v.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, v.nextOrd());
+    
+    v.setDocument(1);
+    assertEquals(0, v.nextOrd());
+    assertEquals(1, v.nextOrd());
+    assertEquals(SortedSetDocValues.NO_MORE_ORDS, v.nextOrd());
+    
+    BytesRef value = new BytesRef();
+    v.lookupOrd(0, value);
+    assertEquals(Double.doubleToRawLongBits(-3d), NumericUtils.prefixCodedToLong(value));
+    
+    v.lookupOrd(1, value);
+    assertEquals(Double.doubleToRawLongBits(5d), NumericUtils.prefixCodedToLong(value));
+    
+    ir.close();
+    dir.close();
+  }
+  
+  private void assertNoSilentInsanity(AtomicReader reader, String field, DocValuesType type) throws IOException {
+    Set<DocValuesType> insaneTypes = EnumSet.allOf(DocValuesType.class);
+    insaneTypes.remove(type);
+    
+    for (DocValuesType t : insaneTypes) {
+      tryToBeInsane(reader, field, type, t);
+    }
+  }
+  
+  private void tryToBeInsane(AtomicReader reader, String field, DocValuesType actualType, DocValuesType insaneType) throws IOException {
+    try {
+      switch(insaneType) {
+        case NUMERIC:
+          reader.getNumericDocValues(field);
+          break;
+        case SORTED:
+          reader.getSortedDocValues(field);
+          break;
+        case BINARY:
+          reader.getBinaryDocValues(field);
+          break;
+        case SORTED_SET:
+          reader.getSortedSetDocValues(field);
+        default:
+          throw new AssertionError();
+      }
+      fail("didn't get expected exception: actual=" + actualType + ",insane=" + insaneType);
+    } catch (IllegalStateException expected) {}
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/CustomScoreProvider.java lucene5666/lucene/queries/src/java/org/apache/lucene/queries/CustomScoreProvider.java
--- lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/CustomScoreProvider.java	2014-05-14 03:47:28.882646626 -0400
+++ lucene5666/lucene/queries/src/java/org/apache/lucene/queries/CustomScoreProvider.java	2014-05-12 13:28:31.956244530 -0400
@@ -23,7 +23,6 @@
 import org.apache.lucene.index.IndexReader; // for javadocs
 import org.apache.lucene.queries.function.FunctionQuery;
 import org.apache.lucene.search.Explanation;
-import org.apache.lucene.search.FieldCache; // for javadocs
 
 /**
  * An instance of this subclass should be returned by
@@ -32,7 +31,7 @@
  * <p>Since Lucene 2.9, queries operate on each segment of an index separately,
  * so the protected {@link #context} field can be used to resolve doc IDs,
  * as the supplied <code>doc</code> ID is per-segment and without knowledge
- * of the IndexReader you cannot access the document or {@link FieldCache}.
+ * of the IndexReader you cannot access the document or DocValues.
  * 
  * @lucene.experimental
  * @since 2.9.2


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/docvalues/DocTermsIndexDocValues.java lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/docvalues/DocTermsIndexDocValues.java
--- lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/docvalues/DocTermsIndexDocValues.java	2014-05-14 03:47:28.890646626 -0400
+++ lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/docvalues/DocTermsIndexDocValues.java	2014-05-13 08:08:07.433414354 -0400
@@ -20,12 +20,12 @@
 import java.io.IOException;
 
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.ValueSourceScorer;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.UnicodeUtil;
@@ -44,12 +44,12 @@
   protected final CharsRef spareChars = new CharsRef();
 
   public DocTermsIndexDocValues(ValueSource vs, AtomicReaderContext context, String field) throws IOException {
-    try {
-      termsIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), field);
-    } catch (RuntimeException e) {
-      throw new DocTermsIndexException(field, e);
-    }
+    this(vs, open(context, field));
+  }
+  
+  protected DocTermsIndexDocValues(ValueSource vs, SortedDocValues termsIndex) {
     this.vs = vs;
+    this.termsIndex = termsIndex;
   }
 
   protected abstract String toTerm(String readableValue);
@@ -162,6 +162,15 @@
     };
   }
 
+  // TODO: why?
+  static SortedDocValues open(AtomicReaderContext context, String field) throws IOException {
+    try {
+      return DocValues.getSorted(context.reader(), field);
+    } catch (RuntimeException e) {
+      throw new DocTermsIndexException(field, e);
+    }
+  }
+  
   /**
    * Custom Exception to be thrown when the DocTermsIndex for a field cannot be generated
    */


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/BytesRefFieldSource.java lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/BytesRefFieldSource.java
--- lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/BytesRefFieldSource.java	2014-05-14 03:47:28.890646626 -0400
+++ lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/BytesRefFieldSource.java	2014-05-12 13:28:31.960244530 -0400
@@ -22,13 +22,15 @@
 
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.DocTermsIndexDocValues;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.mutable.MutableValue;
+import org.apache.lucene.util.mutable.MutableValueStr;
 
 /**
  * An implementation for retrieving {@link FunctionValues} instances for string based fields.
@@ -42,11 +44,12 @@
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     final FieldInfo fieldInfo = readerContext.reader().getFieldInfos().fieldInfo(field);
+
     // To be sorted or not to be sorted, that is the question
     // TODO: do it cleaner?
     if (fieldInfo != null && fieldInfo.getDocValuesType() == DocValuesType.BINARY) {
-      final BinaryDocValues binaryValues = FieldCache.DEFAULT.getTerms(readerContext.reader(), field, true);
-      final Bits docsWithField = FieldCache.DEFAULT.getDocsWithField(readerContext.reader(), field);
+      final BinaryDocValues binaryValues = DocValues.getBinary(readerContext.reader(), field);
+      final Bits docsWithField = DocValues.getDocsWithField(readerContext.reader(), field);
       return new FunctionValues() {
 
         @Override
@@ -76,6 +79,31 @@
         public String toString(int doc) {
           return description() + '=' + strVal(doc);
         }
+
+        @Override
+        public ValueFiller getValueFiller() {
+          return new ValueFiller() {
+            private final MutableValueStr mval = new MutableValueStr();
+
+            @Override
+            public MutableValue getValue() {
+              return mval;
+            }
+
+            @Override
+            public void fillValue(int doc) {
+              mval.exists = docsWithField.get(doc);
+              if (mval.exists) {
+                binaryValues.get(doc, mval.value);
+              } else {
+                mval.value.bytes = BytesRef.EMPTY_BYTES;
+                mval.value.offset = 0;
+                mval.value.length = 0;
+              }
+            }
+          };
+        }
+
       };
     } else {
       return new DocTermsIndexDocValues(this, readerContext, field) {


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleFieldSource.java lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleFieldSource.java
--- lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleFieldSource.java	2014-05-14 03:47:28.890646626 -0400
+++ lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleFieldSource.java	2014-05-12 13:28:31.968244530 -0400
@@ -20,31 +20,24 @@
 import java.io.IOException;
 import java.util.Map;
 
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSourceScorer;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueDouble;
 
 /**
- * Obtains double field values from {@link FieldCache#getDoubles} and makes
+ * Obtains double field values from {@link AtomicReader#getNumericDocValues} and makes
  * those values available as other numeric types, casting as needed.
  */
 public class DoubleFieldSource extends FieldCacheSource {
 
-  protected final FieldCache.DoubleParser parser;
-
   public DoubleFieldSource(String field) {
-    this(field, null);
-  }
-
-  public DoubleFieldSource(String field, FieldCache.DoubleParser parser) {
     super(field);
-    this.parser = parser;
   }
 
   @Override
@@ -54,12 +47,12 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final FieldCache.Doubles arr = cache.getDoubles(readerContext.reader(), field, parser, true);
-    final Bits valid = cache.getDocsWithField(readerContext.reader(), field);
+    final NumericDocValues arr = DocValues.getNumeric(readerContext.reader(), field);
+    final Bits valid = DocValues.getDocsWithField(readerContext.reader(), field);
     return new DoubleDocValues(this) {
       @Override
       public double doubleVal(int doc) {
-        return arr.get(doc);
+        return Double.longBitsToDouble(arr.get(doc));
       }
 
       @Override
@@ -79,29 +72,24 @@
 
           @Override
           public void fillValue(int doc) {
-            mval.value = arr.get(doc);
+            mval.value = doubleVal(doc);
             mval.exists = mval.value != 0 || valid.get(doc);
           }
         };
       }
-
-
-      };
-
+    };
   }
 
   @Override
   public boolean equals(Object o) {
     if (o.getClass() != DoubleFieldSource.class) return false;
     DoubleFieldSource other = (DoubleFieldSource) o;
-    return super.equals(other)
-      && (this.parser == null ? other.parser == null :
-          this.parser.getClass() == other.parser.getClass());
+    return super.equals(other);
   }
 
   @Override
   public int hashCode() {
-    int h = parser == null ? Double.class.hashCode() : parser.getClass().hashCode();
+    int h = Double.class.hashCode();
     h += super.hashCode();
     return h;
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/EnumFieldSource.java lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/EnumFieldSource.java
--- lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/EnumFieldSource.java	2014-05-14 03:47:28.890646626 -0400
+++ lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/EnumFieldSource.java	2014-05-12 13:28:31.956244530 -0400
@@ -20,31 +20,31 @@
 import java.io.IOException;
 import java.util.Map;
 
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSourceScorer;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueInt;
 
 /**
- * Obtains int field values from {@link FieldCache#getInts} and makes
+ * Obtains int field values from {@link AtomicReader#getNumericDocValues} and makes
  * those values available as other numeric types, casting as needed.
  * strVal of the value is not the int value, but its string (displayed) value
  */
 public class EnumFieldSource extends FieldCacheSource {
   static final Integer DEFAULT_VALUE = -1;
 
-  final FieldCache.IntParser parser;
   final Map<Integer, String> enumIntToStringMap;
   final Map<String, Integer> enumStringToIntMap;
 
-  public EnumFieldSource(String field, FieldCache.IntParser parser, Map<Integer, String> enumIntToStringMap, Map<String, Integer> enumStringToIntMap) {
+  public EnumFieldSource(String field, Map<Integer, String> enumIntToStringMap, Map<String, Integer> enumStringToIntMap) {
     super(field);
-    this.parser = parser;
     this.enumIntToStringMap = enumIntToStringMap;
     this.enumStringToIntMap = enumStringToIntMap;
   }
@@ -98,55 +98,29 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final FieldCache.Ints arr = cache.getInts(readerContext.reader(), field, parser, true);
-    final Bits valid = cache.getDocsWithField(readerContext.reader(), field);
+    final NumericDocValues arr = DocValues.getNumeric(readerContext.reader(), field);
+    final Bits valid = DocValues.getDocsWithField(readerContext.reader(), field);
 
     return new IntDocValues(this) {
       final MutableValueInt val = new MutableValueInt();
 
       @Override
-      public float floatVal(int doc) {
-        return (float) arr.get(doc);
-      }
-
-      @Override
       public int intVal(int doc) {
-        return arr.get(doc);
-      }
-
-      @Override
-      public long longVal(int doc) {
-        return (long) arr.get(doc);
-      }
-
-      @Override
-      public double doubleVal(int doc) {
-        return (double) arr.get(doc);
+        return (int) arr.get(doc);
       }
 
       @Override
       public String strVal(int doc) {
-        Integer intValue = arr.get(doc);
+        Integer intValue = intVal(doc);
         return intValueToStringValue(intValue);
       }
 
       @Override
-      public Object objectVal(int doc) {
-        return valid.get(doc) ? arr.get(doc) : null;
-      }
-
-      @Override
       public boolean exists(int doc) {
         return valid.get(doc);
       }
 
       @Override
-      public String toString(int doc) {
-        return description() + '=' + strVal(doc);
-      }
-
-
-      @Override
       public ValueSourceScorer getRangeScorer(IndexReader reader, String lowerVal, String upperVal, boolean includeLower, boolean includeUpper) {
         Integer lower = stringValueToIntValue(lowerVal);
         Integer upper = stringValueToIntValue(upperVal);
@@ -171,7 +145,7 @@
         return new ValueSourceScorer(reader, this) {
           @Override
           public boolean matchesValue(int doc) {
-            int val = arr.get(doc);
+            int val = intVal(doc);
             // only check for deleted if it's the default value
             // if (val==0 && reader.isDeleted(doc)) return false;
             return val >= ll && val <= uu;
@@ -191,13 +165,11 @@
 
           @Override
           public void fillValue(int doc) {
-            mval.value = arr.get(doc);
+            mval.value = intVal(doc);
             mval.exists = valid.get(doc);
           }
         };
       }
-
-
     };
   }
 
@@ -211,7 +183,6 @@
 
     if (!enumIntToStringMap.equals(that.enumIntToStringMap)) return false;
     if (!enumStringToIntMap.equals(that.enumStringToIntMap)) return false;
-    if (!parser.equals(that.parser)) return false;
 
     return true;
   }
@@ -219,7 +190,6 @@
   @Override
   public int hashCode() {
     int result = super.hashCode();
-    result = 31 * result + parser.hashCode();
     result = 31 * result + enumIntToStringMap.hashCode();
     result = 31 * result + enumStringToIntMap.hashCode();
     return result;


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/FieldCacheSource.java lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/FieldCacheSource.java
--- lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/FieldCacheSource.java	2014-05-14 03:47:28.890646626 -0400
+++ lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/FieldCacheSource.java	2014-05-12 13:28:31.956244530 -0400
@@ -18,26 +18,20 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.search.FieldCache;
 
 /**
  * A base class for ValueSource implementations that retrieve values for
- * a single field from the {@link org.apache.lucene.search.FieldCache}.
+ * a single field from DocValues.
  *
  *
  */
 public abstract class FieldCacheSource extends ValueSource {
   protected final String field;
-  protected final FieldCache cache = FieldCache.DEFAULT;
 
   public FieldCacheSource(String field) {
     this.field=field;
   }
 
-  public FieldCache getFieldCache() {
-    return cache;
-  }
-
   public String getField() {
     return field;
   }
@@ -51,13 +45,12 @@
   public boolean equals(Object o) {
     if (!(o instanceof FieldCacheSource)) return false;
     FieldCacheSource other = (FieldCacheSource)o;
-    return this.field.equals(other.field)
-           && this.cache == other.cache;
+    return this.field.equals(other.field);
   }
 
   @Override
   public int hashCode() {
-    return cache.hashCode() + field.hashCode();
+    return field.hashCode();
   }
 
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/FloatFieldSource.java lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/FloatFieldSource.java
--- lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/FloatFieldSource.java	2014-05-14 03:47:28.890646626 -0400
+++ lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/FloatFieldSource.java	2014-05-12 13:28:31.960244530 -0400
@@ -20,29 +20,24 @@
 import java.io.IOException;
 import java.util.Map;
 
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueFloat;
 
 /**
- * Obtains float field values from {@link FieldCache#getFloats} and makes those
+ * Obtains float field values from {@link AtomicReader#getNumericDocValues} and makes those
  * values available as other numeric types, casting as needed.
  */
 public class FloatFieldSource extends FieldCacheSource {
 
-  protected final FieldCache.FloatParser parser;
-
   public FloatFieldSource(String field) {
-    this(field, null);
-  }
-
-  public FloatFieldSource(String field, FieldCache.FloatParser parser) {
     super(field);
-    this.parser = parser;
   }
 
   @Override
@@ -52,18 +47,13 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final FieldCache.Floats arr = cache.getFloats(readerContext.reader(), field, parser, true);
-    final Bits valid = cache.getDocsWithField(readerContext.reader(), field);
+    final NumericDocValues arr = DocValues.getNumeric(readerContext.reader(), field);
+    final Bits valid = DocValues.getDocsWithField(readerContext.reader(), field);
 
     return new FloatDocValues(this) {
       @Override
       public float floatVal(int doc) {
-        return arr.get(doc);
-      }
-
-      @Override
-      public Object objectVal(int doc) {
-        return valid.get(doc) ? arr.get(doc) : null;
+        return Float.intBitsToFloat((int)arr.get(doc));
       }
 
       @Override
@@ -83,7 +73,7 @@
 
           @Override
           public void fillValue(int doc) {
-            mval.value = arr.get(doc);
+            mval.value = floatVal(doc);
             mval.exists = mval.value != 0 || valid.get(doc);
           }
         };
@@ -96,14 +86,12 @@
   public boolean equals(Object o) {
     if (o.getClass() !=  FloatFieldSource.class) return false;
     FloatFieldSource other = (FloatFieldSource)o;
-    return super.equals(other)
-      && (this.parser==null ? other.parser==null :
-          this.parser.getClass() == other.parser.getClass());
+    return super.equals(other);
   }
 
   @Override
   public int hashCode() {
-    int h = parser==null ? Float.class.hashCode() : parser.getClass().hashCode();
+    int h = Float.class.hashCode();
     h += super.hashCode();
     return h;
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IntFieldSource.java lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IntFieldSource.java
--- lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IntFieldSource.java	2014-05-14 03:47:28.890646626 -0400
+++ lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IntFieldSource.java	2014-05-12 13:28:31.956244530 -0400
@@ -20,30 +20,26 @@
 import java.io.IOException;
 import java.util.Map;
 
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSourceScorer;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueInt;
 
 /**
- * Obtains int field values from {@link FieldCache#getInts} and makes those
+ * Obtains int field values from {@link AtomicReader#getNumericDocValues} and makes those
  * values available as other numeric types, casting as needed.
  */
 public class IntFieldSource extends FieldCacheSource {
-  final FieldCache.IntParser parser;
 
   public IntFieldSource(String field) {
-    this(field, null);
-  }
-
-  public IntFieldSource(String field, FieldCache.IntParser parser) {
     super(field);
-    this.parser = parser;
   }
 
   @Override
@@ -54,40 +50,20 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final FieldCache.Ints arr = cache.getInts(readerContext.reader(), field, parser, true);
-    final Bits valid = cache.getDocsWithField(readerContext.reader(), field);
+    final NumericDocValues arr = DocValues.getNumeric(readerContext.reader(), field);
+    final Bits valid = DocValues.getDocsWithField(readerContext.reader(), field);
     
     return new IntDocValues(this) {
       final MutableValueInt val = new MutableValueInt();
-      
-      @Override
-      public float floatVal(int doc) {
-        return (float)arr.get(doc);
-      }
 
       @Override
       public int intVal(int doc) {
-        return arr.get(doc);
-      }
-
-      @Override
-      public long longVal(int doc) {
-        return (long)arr.get(doc);
-      }
-
-      @Override
-      public double doubleVal(int doc) {
-        return (double)arr.get(doc);
+        return (int) arr.get(doc);
       }
 
       @Override
       public String strVal(int doc) {
-        return Integer.toString(arr.get(doc));
-      }
-
-      @Override
-      public Object objectVal(int doc) {
-        return valid.get(doc) ? arr.get(doc) : null;
+        return Integer.toString(intVal(doc));
       }
 
       @Override
@@ -96,11 +72,6 @@
       }
 
       @Override
-      public String toString(int doc) {
-        return description() + '=' + intVal(doc);
-      }
-
-      @Override
       public ValueFiller getValueFiller() {
         return new ValueFiller() {
           private final MutableValueInt mval = new MutableValueInt();
@@ -112,13 +83,11 @@
 
           @Override
           public void fillValue(int doc) {
-            mval.value = arr.get(doc);
+            mval.value = intVal(doc);
             mval.exists = mval.value != 0 || valid.get(doc);
           }
         };
       }
-
-      
     };
   }
 
@@ -126,14 +95,12 @@
   public boolean equals(Object o) {
     if (o.getClass() !=  IntFieldSource.class) return false;
     IntFieldSource other = (IntFieldSource)o;
-    return super.equals(other)
-      && (this.parser==null ? other.parser==null :
-          this.parser.getClass() == other.parser.getClass());
+    return super.equals(other);
   }
 
   @Override
   public int hashCode() {
-    int h = parser==null ? Integer.class.hashCode() : parser.getClass().hashCode();
+    int h = Integer.class.hashCode();
     h += super.hashCode();
     return h;
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java
--- lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java	2014-05-14 03:47:28.890646626 -0400
+++ lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java	2014-05-12 13:28:31.956244530 -0400
@@ -22,6 +22,7 @@
 
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.ReaderUtil;
@@ -30,7 +31,6 @@
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.packed.PackedInts;
 
 /**
  * Use a field value and find the Document Frequency within another field.
@@ -56,7 +56,7 @@
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException
   {
-    final BinaryDocValues terms = cache.getTerms(readerContext.reader(), field, false, PackedInts.FAST);
+    final BinaryDocValues terms = DocValues.getBinary(readerContext.reader(), field);
     final IndexReader top = ReaderUtil.getTopLevelContext(readerContext).reader();
     Terms t = MultiFields.getTerms(top, qfield);
     final TermsEnum termsEnum = t == null ? TermsEnum.EMPTY : t.iterator(null);


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java
--- lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java	2014-05-14 03:47:28.890646626 -0400
+++ lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java	2014-05-12 13:28:31.956244530 -0400
@@ -20,31 +20,24 @@
 import java.io.IOException;
 import java.util.Map;
 
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSourceScorer;
 import org.apache.lucene.queries.function.docvalues.LongDocValues;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueLong;
 
 /**
- * Obtains long field values from {@link FieldCache#getLongs} and makes those
+ * Obtains long field values from {@link AtomicReader#getNumericDocValues} and makes those
  * values available as other numeric types, casting as needed.
  */
 public class LongFieldSource extends FieldCacheSource {
 
-  protected final FieldCache.LongParser parser;
-
   public LongFieldSource(String field) {
-    this(field, null);
-  }
-
-  public LongFieldSource(String field, FieldCache.LongParser parser) {
     super(field);
-    this.parser = parser;
   }
 
   @Override
@@ -66,8 +59,8 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final FieldCache.Longs arr = cache.getLongs(readerContext.reader(), field, parser, true);
-    final Bits valid = cache.getDocsWithField(readerContext.reader(), field);
+    final NumericDocValues arr = DocValues.getNumeric(readerContext.reader(), field);
+    final Bits valid = DocValues.getDocsWithField(readerContext.reader(), field);
     
     return new LongDocValues(this) {
       @Override
@@ -124,14 +117,12 @@
   public boolean equals(Object o) {
     if (o.getClass() != this.getClass()) return false;
     LongFieldSource other = (LongFieldSource) o;
-    return super.equals(other)
-      && (this.parser == null ? other.parser == null :
-          this.parser.getClass() == other.parser.getClass());
+    return super.equals(other);
   }
 
   @Override
   public int hashCode() {
-    int h = parser == null ? this.getClass().hashCode() : parser.getClass().hashCode();
+    int h = getClass().hashCode();
     h += super.hashCode();
     return h;
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/OrdFieldSource.java lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/OrdFieldSource.java
--- lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/OrdFieldSource.java	2014-05-14 03:47:28.890646626 -0400
+++ lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/OrdFieldSource.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,128 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.queries.function.valuesource;
-
-import java.io.IOException;
-import java.util.Map;
-
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.CompositeReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.ReaderUtil;
-import org.apache.lucene.index.SlowCompositeReaderWrapper;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.docvalues.IntDocValues;
-import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.util.mutable.MutableValue;
-import org.apache.lucene.util.mutable.MutableValueInt;
-
-/**
- * Obtains the ordinal of the field value from the default Lucene {@link org.apache.lucene.search.FieldCache} using getStringIndex().
- * <br>
- * The native lucene index order is used to assign an ordinal value for each field value.
- * <br>Field values (terms) are lexicographically ordered by unicode value, and numbered starting at 1.
- * <br>
- * Example:<br>
- *  If there were only three field values: "apple","banana","pear"
- * <br>then ord("apple")=1, ord("banana")=2, ord("pear")=3
- * <p>
- * WARNING: ord() depends on the position in an index and can thus change when other documents are inserted or deleted,
- *  or if a MultiSearcher is used.
- * <br>WARNING: as of Solr 1.4, ord() and rord() can cause excess memory use since they must use a FieldCache entry
- * at the top level reader, while sorting and function queries now use entries at the segment level.  Hence sorting
- * or using a different function query, in addition to ord()/rord() will double memory use.
- *
- */
-
-public class OrdFieldSource extends ValueSource {
-  protected final String field;
-
-  public OrdFieldSource(String field) {
-    this.field = field;
-  }
-
-  @Override
-  public String description() {
-    return "ord(" + field + ')';
-  }
-
-
-  // TODO: this is trappy? perhaps this query instead should make you pass a slow reader yourself?
-  @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final int off = readerContext.docBase;
-    final IndexReader topReader = ReaderUtil.getTopLevelContext(readerContext).reader();
-    final AtomicReader r = SlowCompositeReaderWrapper.wrap(topReader);
-    final SortedDocValues sindex = FieldCache.DEFAULT.getTermsIndex(r, field);
-    return new IntDocValues(this) {
-      protected String toTerm(String readableValue) {
-        return readableValue;
-      }
-      @Override
-      public int intVal(int doc) {
-        return sindex.getOrd(doc+off);
-      }
-      @Override
-      public int ordVal(int doc) {
-        return sindex.getOrd(doc+off);
-      }
-      @Override
-      public int numOrd() {
-        return sindex.getValueCount();
-      }
-
-      @Override
-      public boolean exists(int doc) {
-        return sindex.getOrd(doc+off) != 0;
-      }
-
-      @Override
-      public ValueFiller getValueFiller() {
-        return new ValueFiller() {
-          private final MutableValueInt mval = new MutableValueInt();
-
-          @Override
-          public MutableValue getValue() {
-            return mval;
-          }
-
-          @Override
-          public void fillValue(int doc) {
-            mval.value = sindex.getOrd(doc);
-            mval.exists = mval.value!=0;
-          }
-        };
-      }
-    };
-  }
-
-  @Override
-  public boolean equals(Object o) {
-    return o != null && o.getClass() == OrdFieldSource.class && this.field.equals(((OrdFieldSource)o).field);
-  }
-
-  private static final int hcode = OrdFieldSource.class.hashCode();
-  @Override
-  public int hashCode() {
-    return hcode + field.hashCode();
-  }
-
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ReverseOrdFieldSource.java lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ReverseOrdFieldSource.java
--- lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ReverseOrdFieldSource.java	2014-05-14 03:47:28.890646626 -0400
+++ lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ReverseOrdFieldSource.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,99 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.queries.function.valuesource;
-
-import java.io.IOException;
-import java.util.Map;
-
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.CompositeReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.ReaderUtil;
-import org.apache.lucene.index.SlowCompositeReaderWrapper;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.docvalues.IntDocValues;
-import org.apache.lucene.search.FieldCache;
-
-/**
- * Obtains the ordinal of the field value from the default Lucene {@link org.apache.lucene.search.FieldCache} using getTermsIndex()
- * and reverses the order.
- * <br>
- * The native lucene index order is used to assign an ordinal value for each field value.
- * <br>Field values (terms) are lexicographically ordered by unicode value, and numbered starting at 1.
- * <br>
- * Example of reverse ordinal (rord):<br>
- *  If there were only three field values: "apple","banana","pear"
- * <br>then rord("apple")=3, rord("banana")=2, ord("pear")=1
- * <p>
- *  WARNING: ord() depends on the position in an index and can thus change when other documents are inserted or deleted,
- *  or if a MultiSearcher is used.
- * <br>
- *  WARNING: as of Solr 1.4, ord() and rord() can cause excess memory use since they must use a FieldCache entry
- * at the top level reader, while sorting and function queries now use entries at the segment level.  Hence sorting
- * or using a different function query, in addition to ord()/rord() will double memory use.
- * 
- *
- */
-
-public class ReverseOrdFieldSource extends ValueSource {
-  public final String field;
-
-  public ReverseOrdFieldSource(String field) {
-    this.field = field;
-  }
-
-  @Override
-  public String description() {
-    return "rord("+field+')';
-  }
-
-  // TODO: this is trappy? perhaps this query instead should make you pass a slow reader yourself?
-  @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final IndexReader topReader = ReaderUtil.getTopLevelContext(readerContext).reader();
-    final AtomicReader r = SlowCompositeReaderWrapper.wrap(topReader);
-    final int off = readerContext.docBase;
-
-    final SortedDocValues sindex = FieldCache.DEFAULT.getTermsIndex(r, field);
-    final int end = sindex.getValueCount();
-
-    return new IntDocValues(this) {
-     @Override
-      public int intVal(int doc) {
-        return (end - sindex.getOrd(doc+off) - 1);
-      }
-    };
-  }
-
-  @Override
-  public boolean equals(Object o) {
-    if (o == null || (o.getClass() !=  ReverseOrdFieldSource.class)) return false;
-    ReverseOrdFieldSource other = (ReverseOrdFieldSource)o;
-    return this.field.equals(other.field);
-  }
-
-  private static final int hcode = ReverseOrdFieldSource.class.hashCode();
-  @Override
-  public int hashCode() {
-    return hcode + field.hashCode();
-  }
-
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SortedSetFieldSource.java lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SortedSetFieldSource.java
--- lucene-trunk/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SortedSetFieldSource.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SortedSetFieldSource.java	2014-05-13 08:13:27.273419924 -0400
@@ -0,0 +1,90 @@
+package org.apache.lucene.queries.function.valuesource;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Map;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.docvalues.DocTermsIndexDocValues;
+import org.apache.lucene.search.SortedSetSelector;
+
+/**
+ * Retrieves {@link FunctionValues} instances for multi-valued string based fields.
+ * <p>
+ * A SortedSetDocValues contains multiple values for a field, so this 
+ * technique "selects" a value as the representative value for the document.
+ * 
+ * @see SortedSetSelector
+ */
+public class SortedSetFieldSource extends FieldCacheSource {
+  protected final SortedSetSelector.Type selector;
+  
+  public SortedSetFieldSource(String field) {
+    this(field, SortedSetSelector.Type.MIN);
+  }
+  
+  public SortedSetFieldSource(String field, SortedSetSelector.Type selector) {
+    super(field);
+    this.selector = selector;
+  }
+
+  @Override
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    SortedSetDocValues sortedSet = DocValues.getSortedSet(readerContext.reader(), field);
+    SortedDocValues view = SortedSetSelector.wrap(sortedSet, selector);
+    return new DocTermsIndexDocValues(this, view) {
+      @Override
+      protected String toTerm(String readableValue) {
+        return readableValue;
+      }
+
+      @Override
+      public Object objectVal(int doc) {
+        return strVal(doc);
+      }
+    };
+  }
+  
+  @Override
+  public String description() {
+    return "sortedset(" + field + ",selector=" + selector + ')';
+  }
+
+  @Override
+  public int hashCode() {
+    final int prime = 31;
+    int result = super.hashCode();
+    result = prime * result + ((selector == null) ? 0 : selector.hashCode());
+    return result;
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (this == obj) return true;
+    if (!super.equals(obj)) return false;
+    if (getClass() != obj.getClass()) return false;
+    SortedSetFieldSource other = (SortedSetFieldSource) obj;
+    if (selector != other.selector) return false;
+    return true;
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/test/org/apache/lucene/queries/function/FunctionTestSetup.java lucene5666/lucene/queries/src/test/org/apache/lucene/queries/function/FunctionTestSetup.java
--- lucene-trunk/lucene/queries/src/test/org/apache/lucene/queries/function/FunctionTestSetup.java	2014-05-14 03:47:28.882646626 -0400
+++ lucene5666/lucene/queries/src/test/org/apache/lucene/queries/function/FunctionTestSetup.java	2014-05-12 13:28:31.424244521 -0400
@@ -1,7 +1,5 @@
 package org.apache.lucene.queries.function;
 
-import java.io.IOException;
-
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
@@ -9,15 +7,14 @@
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.FloatField;
 import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.queries.function.valuesource.FloatFieldSource;
 import org.apache.lucene.queries.function.valuesource.IntFieldSource;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LuceneTestCase;
@@ -60,21 +57,7 @@
   protected static final String INT_FIELD = "iii";
   protected static final String FLOAT_FIELD = "fff";
 
-  private static final FieldCache.FloatParser CUSTOM_FLOAT_PARSER = new FieldCache.FloatParser() {
-    
-    @Override
-    public TermsEnum termsEnum(Terms terms) throws IOException {
-      return FieldCache.NUMERIC_UTILS_INT_PARSER.termsEnum(terms);
-    }
-    
-    @Override
-    public float parseFloat(BytesRef term) {
-      return (float) FieldCache.NUMERIC_UTILS_INT_PARSER.parseInt(term);
-    }
-  };
-
   protected ValueSource INT_VALUESOURCE = new IntFieldSource(INT_FIELD);
-  protected ValueSource INT_AS_FLOAT_VALUESOURCE = new FloatFieldSource(INT_FIELD, CUSTOM_FLOAT_PARSER);
   protected ValueSource FLOAT_VALUESOURCE = new FloatFieldSource(FLOAT_FIELD);
 
   private static final String DOC_TEXT_LINES[] = {
@@ -152,6 +135,7 @@
     
     f = newField(ID_FIELD, id2String(scoreAndID), customType); // for debug purposes
     d.add(f);
+    d.add(new SortedDocValuesField(ID_FIELD, new BytesRef(id2String(scoreAndID))));
 
     FieldType customType2 = new FieldType(TextField.TYPE_NOT_STORED);
     customType2.setOmitNorms(true);
@@ -160,9 +144,11 @@
 
     f = new IntField(INT_FIELD, scoreAndID, Store.YES); // for function scoring
     d.add(f);
+    d.add(new NumericDocValuesField(INT_FIELD, scoreAndID));
 
     f = new FloatField(FLOAT_FIELD, scoreAndID, Store.YES); // for function scoring
     d.add(f);
+    d.add(new NumericDocValuesField(FLOAT_FIELD, Float.floatToRawIntBits(scoreAndID)));
 
     iw.addDocument(d);
     log("added: " + d);


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/test/org/apache/lucene/queries/function/TestFieldScoreQuery.java lucene5666/lucene/queries/src/test/org/apache/lucene/queries/function/TestFieldScoreQuery.java
--- lucene-trunk/lucene/queries/src/test/org/apache/lucene/queries/function/TestFieldScoreQuery.java	2014-05-14 03:47:28.882646626 -0400
+++ lucene5666/lucene/queries/src/test/org/apache/lucene/queries/function/TestFieldScoreQuery.java	2014-05-12 13:28:31.424244521 -0400
@@ -53,8 +53,6 @@
   /** Test that FieldScoreQuery of Type.FLOAT returns docs in expected order. */
   @Test
   public void testRankFloat () throws Exception {
-    // INT field can be parsed as float
-    doTestRank(INT_AS_FLOAT_VALUESOURCE);
     // same values, but in flot format
     doTestRank(FLOAT_VALUESOURCE);
   }
@@ -88,8 +86,6 @@
   /** Test that FieldScoreQuery of Type.FLOAT returns the expected scores. */
   @Test
   public void testExactScoreFloat () throws  Exception {
-    // INT field can be parsed as float
-    doTestExactScore(INT_AS_FLOAT_VALUESOURCE);
     // same values, but in flot format
     doTestExactScore(FLOAT_VALUESOURCE);
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/test/org/apache/lucene/queries/function/TestFunctionQuerySort.java lucene5666/lucene/queries/src/test/org/apache/lucene/queries/function/TestFunctionQuerySort.java
--- lucene-trunk/lucene/queries/src/test/org/apache/lucene/queries/function/TestFunctionQuerySort.java	2014-05-14 03:47:28.878646625 -0400
+++ lucene5666/lucene/queries/src/test/org/apache/lucene/queries/function/TestFunctionQuerySort.java	2014-05-12 13:28:31.420244520 -0400
@@ -22,6 +22,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -48,12 +49,15 @@
 
     Document doc = new Document();
     Field field = new IntField("value", 0, Field.Store.YES);
+    Field dvField = new NumericDocValuesField("value", 0);
     doc.add(field);
+    doc.add(dvField);
 
     // Save docs unsorted (decreasing value n, n-1, ...)
     final int NUM_VALS = 5;
     for (int val = NUM_VALS; val > 0; val--) {
       field.setIntValue(val);
+      dvField.setLongValue(val);
       writer.addDocument(doc);
     }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/test/org/apache/lucene/queries/function/TestOrdValues.java lucene5666/lucene/queries/src/test/org/apache/lucene/queries/function/TestOrdValues.java
--- lucene-trunk/lucene/queries/src/test/org/apache/lucene/queries/function/TestOrdValues.java	2014-05-14 03:47:28.882646626 -0400
+++ lucene5666/lucene/queries/src/test/org/apache/lucene/queries/function/TestOrdValues.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,155 +0,0 @@
-package org.apache.lucene.queries.function;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.queries.function.valuesource.OrdFieldSource;
-import org.apache.lucene.queries.function.valuesource.ReverseOrdFieldSource;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.QueryUtils;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.TopDocs;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * Test search based on OrdFieldSource and ReverseOrdFieldSource.
- * <p/>
- * Tests here create an index with a few documents, each having
- * an indexed "id" field.
- * The ord values of this field are later used for scoring.
- * <p/>
- * The order tests use Hits to verify that docs are ordered as expected.
- * <p/>
- * The exact score tests use TopDocs top to verify the exact score.
- */
-public class TestOrdValues extends FunctionTestSetup {
-
-  @BeforeClass
-  public static void beforeClass() throws Exception {
-    createIndex(false);
-  }
-
-  /**
-   * Test OrdFieldSource
-   */
-  @Test
-  public void testOrdFieldRank() throws Exception {
-    doTestRank(ID_FIELD, true);
-  }
-
-  /**
-   * Test ReverseOrdFieldSource
-   */
-  @Test
-  public void testReverseOrdFieldRank() throws Exception {
-    doTestRank(ID_FIELD, false);
-  }
-
-  // Test that queries based on reverse/ordFieldScore scores correctly
-  private void doTestRank(String field, boolean inOrder) throws Exception {
-    IndexReader r = DirectoryReader.open(dir);
-    IndexSearcher s = newSearcher(r);
-    ValueSource vs;
-    if (inOrder) {
-      vs = new OrdFieldSource(field);
-    } else {
-      vs = new ReverseOrdFieldSource(field);
-    }
-
-    Query q = new FunctionQuery(vs);
-    log("test: " + q);
-    QueryUtils.check(random(), q, s);
-    ScoreDoc[] h = s.search(q, null, 1000).scoreDocs;
-    assertEquals("All docs should be matched!", N_DOCS, h.length);
-    String prevID = inOrder
-            ? "IE"   // greater than all ids of docs in this test ("ID0001", etc.)
-            : "IC";  // smaller than all ids of docs in this test ("ID0001", etc.)
-
-    for (int i = 0; i < h.length; i++) {
-      String resID = s.doc(h[i].doc).get(ID_FIELD);
-      log(i + ".   score=" + h[i].score + "  -  " + resID);
-      log(s.explain(q, h[i].doc));
-      if (inOrder) {
-        assertTrue("res id " + resID + " should be < prev res id " + prevID, resID.compareTo(prevID) < 0);
-      } else {
-        assertTrue("res id " + resID + " should be > prev res id " + prevID, resID.compareTo(prevID) > 0);
-      }
-      prevID = resID;
-    }
-    r.close();
-  }
-
-  /**
-   * Test exact score for OrdFieldSource
-   */
-  @Test
-  public void testOrdFieldExactScore() throws Exception {
-    doTestExactScore(ID_FIELD, true);
-  }
-
-  /**
-   * Test exact score for ReverseOrdFieldSource
-   */
-  @Test
-  public void testReverseOrdFieldExactScore() throws Exception {
-    doTestExactScore(ID_FIELD, false);
-  }
-
-
-  // Test that queries based on reverse/ordFieldScore returns docs with expected score.
-  private void doTestExactScore(String field, boolean inOrder) throws Exception {
-    IndexReader r = DirectoryReader.open(dir);
-    IndexSearcher s = newSearcher(r);
-    ValueSource vs;
-    if (inOrder) {
-      vs = new OrdFieldSource(field);
-    } else {
-      vs = new ReverseOrdFieldSource(field);
-    }
-    Query q = new FunctionQuery(vs);
-    TopDocs td = s.search(q, null, 1000);
-    assertEquals("All docs should be matched!", N_DOCS, td.totalHits);
-    ScoreDoc sd[] = td.scoreDocs;
-    for (int i = 0; i < sd.length; i++) {
-      float score = sd[i].score;
-      String id = s.getIndexReader().document(sd[i].doc).get(ID_FIELD);
-      log("-------- " + i + ". Explain doc " + id);
-      log(s.explain(q, sd[i].doc));
-      float expectedScore = N_DOCS - i - 1;
-      assertEquals("score of result " + i + " shuould be " + expectedScore + " != " + score, expectedScore, score, TEST_SCORE_TOLERANCE_DELTA);
-      String expectedId = inOrder
-              ? id2String(N_DOCS - i) // in-order ==> larger  values first
-              : id2String(i + 1);     // reverse  ==> smaller values first
-      assertTrue("id of result " + i + " shuould be " + expectedId + " != " + score, expectedId.equals(id));
-    }
-    r.close();
-  }
-  
-  // LUCENE-1250
-  public void testEqualsNull() throws Exception {
-    OrdFieldSource ofs = new OrdFieldSource("f");
-    assertFalse(ofs.equals(null));
-    
-    ReverseOrdFieldSource rofs = new ReverseOrdFieldSource("f");
-    assertFalse(rofs.equals(null));
-  }
-
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/test/org/apache/lucene/queries/function/TestSortedSetFieldSource.java lucene5666/lucene/queries/src/test/org/apache/lucene/queries/function/TestSortedSetFieldSource.java
--- lucene-trunk/lucene/queries/src/test/org/apache/lucene/queries/function/TestSortedSetFieldSource.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/lucene/queries/src/test/org/apache/lucene/queries/function/TestSortedSetFieldSource.java	2014-05-13 08:21:12.689428029 -0400
@@ -0,0 +1,61 @@
+package org.apache.lucene.queries.function;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Collections;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.SortedSetDocValuesField;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.queries.function.valuesource.SortedSetFieldSource;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
+
+@SuppressCodecs({"Lucene40", "Lucene41"}) // avoid codecs that don't support sortedset
+public class TestSortedSetFieldSource extends LuceneTestCase {
+  public void testSimple() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
+    doc.add(newStringField("id", "2", Field.Store.YES));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("foo")));
+    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
+    doc.add(newStringField("id", "1", Field.Store.YES));
+    writer.addDocument(doc);
+    writer.forceMerge(1);
+    writer.shutdown();
+
+    DirectoryReader ir = DirectoryReader.open(dir);
+    AtomicReader ar = getOnlySegmentReader(ir);
+    
+    ValueSource vs = new SortedSetFieldSource("value");
+    FunctionValues values = vs.getValues(Collections.emptyMap(), ar.getContext());
+    assertEquals("baz", values.strVal(0));
+    assertEquals("bar", values.strVal(1)); 
+    ir.close();
+    dir.close();
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/test/org/apache/lucene/queries/function/TestValueSources.java lucene5666/lucene/queries/src/test/org/apache/lucene/queries/function/TestValueSources.java
--- lucene-trunk/lucene/queries/src/test/org/apache/lucene/queries/function/TestValueSources.java	2014-05-14 03:47:28.882646626 -0400
+++ lucene5666/lucene/queries/src/test/org/apache/lucene/queries/function/TestValueSources.java	2014-05-12 13:28:31.420244520 -0400
@@ -27,6 +27,8 @@
 import org.apache.lucene.document.FloatField;
 import org.apache.lucene.document.IntField;
 import org.apache.lucene.document.LongField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.IndexReader;
@@ -101,26 +103,44 @@
     Document document = new Document();
     Field idField = new StringField("id", "", Field.Store.NO);
     document.add(idField);
+    Field idDVField = new SortedDocValuesField("id", new BytesRef());
+    document.add(idDVField);
     Field doubleField = new DoubleField("double", 0d, Field.Store.NO);
     document.add(doubleField);
+    Field doubleDVField = new NumericDocValuesField("double", 0);
+    document.add(doubleDVField);
     Field floatField = new FloatField("float", 0f, Field.Store.NO);
     document.add(floatField);
+    Field floatDVField = new NumericDocValuesField("float", 0);
+    document.add(floatDVField);
     Field intField = new IntField("int", 0, Field.Store.NO);
     document.add(intField);
+    Field intDVField = new NumericDocValuesField("int", 0);
+    document.add(intDVField);
     Field longField = new LongField("long", 0L, Field.Store.NO);
     document.add(longField);
+    Field longDVField = new NumericDocValuesField("long", 0);
+    document.add(longDVField);
     Field stringField = new StringField("string", "", Field.Store.NO);
     document.add(stringField);
+    Field stringDVField = new SortedDocValuesField("string", new BytesRef());
+    document.add(stringDVField);
     Field textField = new TextField("text", "", Field.Store.NO);
     document.add(textField);
     
     for (String [] doc : documents) {
       idField.setStringValue(doc[0]);
+      idDVField.setBytesValue(new BytesRef(doc[0]));
       doubleField.setDoubleValue(Double.valueOf(doc[1]));
+      doubleDVField.setLongValue(Double.doubleToRawLongBits(Double.valueOf(doc[1])));
       floatField.setFloatValue(Float.valueOf(doc[2]));
+      floatDVField.setLongValue(Float.floatToRawIntBits(Float.valueOf(doc[2])));
       intField.setIntValue(Integer.valueOf(doc[3]));
+      intDVField.setLongValue(Integer.valueOf(doc[3]));
       longField.setLongValue(Long.valueOf(doc[4]));
+      longDVField.setLongValue(Long.valueOf(doc[4]));
       stringField.setStringValue(doc[5]);
+      stringDVField.setBytesValue(new BytesRef(doc[5]));
       textField.setStringValue(doc[6]);
       iw.addDocument(document);
     }


diff -ruN -x .svn -x build lucene-trunk/lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java lucene5666/lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java
--- lucene-trunk/lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java	2014-05-14 03:47:28.878646625 -0400
+++ lucene5666/lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java	2014-05-12 13:28:30.544244505 -0400
@@ -24,7 +24,6 @@
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.CheckHits;
 import org.apache.lucene.search.Explanation;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.QueryUtils;
@@ -39,7 +38,9 @@
 
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.Term;
 
 /**
@@ -66,11 +67,6 @@
    */
   @Test
   public void testCustomScoreFloat() throws Exception {
-    // INT field can be parsed as float
-    doTestCustomScore(INT_AS_FLOAT_VALUESOURCE, 1.0);
-    doTestCustomScore(INT_AS_FLOAT_VALUESOURCE, 5.0);
-
-    // same values, but in float format
     doTestCustomScore(FLOAT_VALUESOURCE, 1.0);
     doTestCustomScore(FLOAT_VALUESOURCE, 6.0);
   }
@@ -164,7 +160,7 @@
 
     @Override
     protected CustomScoreProvider getCustomScoreProvider(AtomicReaderContext context) throws IOException {
-      final FieldCache.Ints values = FieldCache.DEFAULT.getInts(context.reader(), INT_FIELD, false);
+      final NumericDocValues values = DocValues.getNumeric(context.reader(), INT_FIELD);
       return new CustomScoreProvider(context) {
         @Override
         public float customScore(int doc, float subScore, float valSrcScore) {


diff -ruN -x .svn -x build lucene-trunk/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedStringComparator.java lucene5666/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedStringComparator.java
--- lucene-trunk/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedStringComparator.java	2014-05-14 03:47:28.862646625 -0400
+++ lucene5666/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedStringComparator.java	2014-05-12 13:28:57.280244971 -0400
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.search.FieldCache;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.search.FieldComparator;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
@@ -95,8 +95,8 @@
 
   @Override
   public FieldComparator<String> setNextReader(AtomicReaderContext context) throws IOException {
-    currentDocTerms = FieldCache.DEFAULT.getTerms(context.reader(), field, true);
-    docsWithField = FieldCache.DEFAULT.getDocsWithField(context.reader(), field);
+    currentDocTerms = DocValues.getBinary(context.reader(), field);
+    docsWithField = DocValues.getDocsWithField(context.reader(), field);
     return this;
   }
   


diff -ruN -x .svn -x build lucene-trunk/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedTermRangeFilter.java lucene5666/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedTermRangeFilter.java
--- lucene-trunk/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedTermRangeFilter.java	2014-05-14 03:47:28.862646625 -0400
+++ lucene5666/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedTermRangeFilter.java	2014-05-14 03:45:16.490644320 -0400
@@ -21,7 +21,7 @@
 
 import org.apache.lucene.search.MultiTermQueryWrapperFilter;
 import org.apache.lucene.search.NumericRangeFilter; // javadoc
-import org.apache.lucene.search.FieldCacheRangeFilter; // javadoc
+import org.apache.lucene.search.DocValuesRangeFilter; // javadoc
 
 /**
  * A Filter that restricts search results to a range of term
@@ -33,7 +33,7 @@
  * for numerical ranges; use {@link NumericRangeFilter} instead.
  *
  * <p>If you construct a large number of range filters with different ranges but on the 
- * same field, {@link FieldCacheRangeFilter} may have significantly better performance. 
+ * same field, {@link DocValuesRangeFilter} may have significantly better performance. 
  * @deprecated Index collation keys with CollationKeyAnalyzer or ICUCollationKeyAnalyzer instead.
  * This class will be removed in Lucene 5.0
  */


diff -ruN -x .svn -x build lucene-trunk/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SortedSetSortField.java lucene5666/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SortedSetSortField.java
--- lucene-trunk/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SortedSetSortField.java	2014-05-14 03:47:28.862646625 -0400
+++ lucene5666/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SortedSetSortField.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,328 +0,0 @@
-package org.apache.lucene.sandbox.queries;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.RandomAccessOrds;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.search.FieldComparator;
-import org.apache.lucene.search.SortField;
-import org.apache.lucene.util.BytesRef;
-
-/** 
- * SortField for {@link SortedSetDocValues}.
- * <p>
- * A SortedSetDocValues contains multiple values for a field, so sorting with
- * this technique "selects" a value as the representative sort value for the document.
- * <p>
- * By default, the minimum value in the set is selected as the sort value, but
- * this can be customized. Selectors other than the default do have some limitations
- * (see below) to ensure that all selections happen in constant-time for performance.
- * <p>
- * Like sorting by string, this also supports sorting missing values as first or last,
- * via {@link #setMissingValue(Object)}.
- * <p>
- * Limitations:
- * <ul>
- *   <li>Fields containing {@link Integer#MAX_VALUE} or more unique values
- *       are unsupported.
- *   <li>Selectors other than the default ({@link Selector#MIN}) require 
- *       optional codec support. However several codecs provided by Lucene, 
- *       including the current default codec, support this.
- * </ul>
- */
-public class SortedSetSortField extends SortField {
-  
-  /** Selects a value from the document's set to use as the sort value */
-  public static enum Selector {
-    /** 
-     * Selects the minimum value in the set 
-     */
-    MIN,
-    /** 
-     * Selects the maximum value in the set 
-     */
-    MAX,
-    /** 
-     * Selects the middle value in the set.
-     * <p>
-     * If the set has an even number of values, the lower of the middle two is chosen.
-     */
-    MIDDLE_MIN,
-    /** 
-     * Selects the middle value in the set.
-     * <p>
-     * If the set has an even number of values, the higher of the middle two is chosen
-     */
-    MIDDLE_MAX
-  }
-  
-  private final Selector selector;
-  
-  /**
-   * Creates a sort, possibly in reverse, by the minimum value in the set 
-   * for the document.
-   * @param field Name of field to sort by.  Must not be null.
-   * @param reverse True if natural order should be reversed.
-   */
-  public SortedSetSortField(String field, boolean reverse) {
-    this(field, reverse, Selector.MIN);
-  }
-
-  /**
-   * Creates a sort, possibly in reverse, specifying how the sort value from 
-   * the document's set is selected.
-   * @param field Name of field to sort by.  Must not be null.
-   * @param reverse True if natural order should be reversed.
-   * @param selector custom selector for choosing the sort value from the set.
-   * <p>
-   * NOTE: selectors other than {@link Selector#MIN} require optional codec support.
-   */
-  public SortedSetSortField(String field, boolean reverse, Selector selector) {
-    super(field, SortField.Type.CUSTOM, reverse);
-    if (selector == null) {
-      throw new NullPointerException();
-    }
-    this.selector = selector;
-  }
-  
-  /** Returns the selector in use for this sort */
-  public Selector getSelector() {
-    return selector;
-  }
-
-  @Override
-  public int hashCode() {
-    return 31 * super.hashCode() + selector.hashCode();
-  }
-
-  @Override
-  public boolean equals(Object obj) {
-    if (this == obj) return true;
-    if (!super.equals(obj)) return false;
-    if (getClass() != obj.getClass()) return false;
-    SortedSetSortField other = (SortedSetSortField) obj;
-    if (selector != other.selector) return false;
-    return true;
-  }
-  
-  @Override
-  public String toString() {
-    StringBuilder buffer = new StringBuilder();
-    buffer.append("<sortedset" + ": \"").append(getField()).append("\">");
-    if (getReverse()) buffer.append('!');
-    if (missingValue != null) {
-      buffer.append(" missingValue=");
-      buffer.append(missingValue);
-    }
-    buffer.append(" selector=");
-    buffer.append(selector);
-
-    return buffer.toString();
-  }
-
-  /**
-   * Set how missing values (the empty set) are sorted.
-   * <p>
-   * Note that this must be {@link #STRING_FIRST} or {@link #STRING_LAST}.
-   */
-  @Override
-  public void setMissingValue(Object missingValue) {
-    if (missingValue != STRING_FIRST && missingValue != STRING_LAST) {
-      throw new IllegalArgumentException("For SORTED_SET type, missing value must be either STRING_FIRST or STRING_LAST");
-    }
-    this.missingValue = missingValue;
-  }
-  
-  @Override
-  public FieldComparator<?> getComparator(int numHits, int sortPos) throws IOException {
-    return new FieldComparator.TermOrdValComparator(numHits, getField(), missingValue == STRING_LAST) {
-      @Override
-      protected SortedDocValues getSortedDocValues(AtomicReaderContext context, String field) throws IOException {
-        SortedSetDocValues sortedSet = FieldCache.DEFAULT.getDocTermOrds(context.reader(), field);
-        
-        if (sortedSet.getValueCount() >= Integer.MAX_VALUE) {
-          throw new UnsupportedOperationException("fields containing more than " + (Integer.MAX_VALUE-1) + " unique terms are unsupported");
-        }
-        
-        SortedDocValues singleton = DocValues.unwrapSingleton(sortedSet);
-        if (singleton != null) {
-          // it's actually single-valued in practice, but indexed as multi-valued,
-          // so just sort on the underlying single-valued dv directly.
-          // regardless of selector type, this optimization is safe!
-          return singleton;
-        } else if (selector == Selector.MIN) {
-          return new MinValue(sortedSet);
-        } else {
-          if (sortedSet instanceof RandomAccessOrds == false) {
-            throw new UnsupportedOperationException("codec does not support random access ordinals, cannot use selector: " + selector);
-          }
-          RandomAccessOrds randomOrds = (RandomAccessOrds) sortedSet;
-          switch(selector) {
-            case MAX: return new MaxValue(randomOrds);
-            case MIDDLE_MIN: return new MiddleMinValue(randomOrds);
-            case MIDDLE_MAX: return new MiddleMaxValue(randomOrds);
-            case MIN: 
-            default: 
-              throw new AssertionError();
-          }
-        }
-      }
-    };
-  }
-  
-  /** Wraps a SortedSetDocValues and returns the first ordinal (min) */
-  static class MinValue extends SortedDocValues {
-    final SortedSetDocValues in;
-    
-    MinValue(SortedSetDocValues in) {
-      this.in = in;
-    }
-
-    @Override
-    public int getOrd(int docID) {
-      in.setDocument(docID);
-      return (int) in.nextOrd();
-    }
-
-    @Override
-    public void lookupOrd(int ord, BytesRef result) {
-      in.lookupOrd(ord, result);
-    }
-
-    @Override
-    public int getValueCount() {
-      return (int) in.getValueCount();
-    }
-
-    @Override
-    public int lookupTerm(BytesRef key) {
-      return (int) in.lookupTerm(key);
-    }
-  }
-  
-  /** Wraps a SortedSetDocValues and returns the last ordinal (max) */
-  static class MaxValue extends SortedDocValues {
-    final RandomAccessOrds in;
-    
-    MaxValue(RandomAccessOrds in) {
-      this.in = in;
-    }
-
-    @Override
-    public int getOrd(int docID) {
-      in.setDocument(docID);
-      final int count = in.cardinality();
-      if (count == 0) {
-        return -1;
-      } else {
-        return (int) in.ordAt(count-1);
-      }
-    }
-
-    @Override
-    public void lookupOrd(int ord, BytesRef result) {
-      in.lookupOrd(ord, result);
-    }
-
-    @Override
-    public int getValueCount() {
-      return (int) in.getValueCount();
-    }
-    
-    @Override
-    public int lookupTerm(BytesRef key) {
-      return (int) in.lookupTerm(key);
-    }
-  }
-  
-  /** Wraps a SortedSetDocValues and returns the middle ordinal (or min of the two) */
-  static class MiddleMinValue extends SortedDocValues {
-    final RandomAccessOrds in;
-    
-    MiddleMinValue(RandomAccessOrds in) {
-      this.in = in;
-    }
-
-    @Override
-    public int getOrd(int docID) {
-      in.setDocument(docID);
-      final int count = in.cardinality();
-      if (count == 0) {
-        return -1;
-      } else {
-        return (int) in.ordAt((count-1) >>> 1);
-      }
-    }
-
-    @Override
-    public void lookupOrd(int ord, BytesRef result) {
-      in.lookupOrd(ord, result);
-    }
-
-    @Override
-    public int getValueCount() {
-      return (int) in.getValueCount();
-    }
-    
-    @Override
-    public int lookupTerm(BytesRef key) {
-      return (int) in.lookupTerm(key);
-    }
-  }
-  
-  /** Wraps a SortedSetDocValues and returns the middle ordinal (or max of the two) */
-  static class MiddleMaxValue extends SortedDocValues {
-    final RandomAccessOrds in;
-    
-    MiddleMaxValue(RandomAccessOrds in) {
-      this.in = in;
-    }
-
-    @Override
-    public int getOrd(int docID) {
-      in.setDocument(docID);
-      final int count = in.cardinality();
-      if (count == 0) {
-        return -1;
-      } else {
-        return (int) in.ordAt(count >>> 1);
-      }
-    }
-
-    @Override
-    public void lookupOrd(int ord, BytesRef result) {
-      in.lookupOrd(ord, result);
-    }
-
-    @Override
-    public int getValueCount() {
-      return (int) in.getValueCount();
-    }
-    
-    @Override
-    public int lookupTerm(BytesRef key) {
-      return (int) in.lookupTerm(key);
-    }
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSlowCollationMethods.java lucene5666/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSlowCollationMethods.java
--- lucene-trunk/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSlowCollationMethods.java	2014-05-14 03:47:28.854646625 -0400
+++ lucene5666/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSlowCollationMethods.java	2014-05-12 13:28:57.272244971 -0400
@@ -5,11 +5,13 @@
 
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.search.*;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
 import org.junit.AfterClass;
@@ -58,6 +60,8 @@
       String value = TestUtil.randomUnicodeString(random());
       Field field = newStringField("field", value, Field.Store.YES);
       doc.add(field);
+      Field dvField = new SortedDocValuesField("field", new BytesRef(value));
+      doc.add(dvField);
       iw.addDocument(doc);
     }
     splitDoc = TestUtil.randomUnicodeString(random());


diff -ruN -x .svn -x build lucene-trunk/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSortedSetSortFieldDocValues.java lucene5666/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSortedSetSortFieldDocValues.java
--- lucene-trunk/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSortedSetSortFieldDocValues.java	2014-05-14 03:47:28.858646625 -0400
+++ lucene5666/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSortedSetSortFieldDocValues.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,210 +0,0 @@
-package org.apache.lucene.sandbox.queries;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.SortedSetDocValuesField;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.SortField;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
-
-/** Simple tests for SortedSetSortField, indexing the sortedset up front */
-@SuppressCodecs({"Lucene40", "Lucene41"}) // avoid codecs that don't support sortedset
-public class TestSortedSetSortFieldDocValues extends LuceneTestCase {
-  
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    // ensure there is nothing in fieldcache before test starts
-    FieldCache.DEFAULT.purgeAllCaches();
-  }
-  
-  private void assertNoFieldCaches() {
-    // docvalues sorting should NOT create any fieldcache entries!
-    assertEquals(0, FieldCache.DEFAULT.getCacheEntries().length);
-  }
-  
-  public void testForward() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("foo")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortedSetSortField("value", false));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'bar' comes before 'baz'
-    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    assertNoFieldCaches();
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testReverse() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("foo")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortedSetSortField("value", true));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'bar' comes before 'baz'
-    assertEquals("2", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("1", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMissingFirst() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("foo")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("id", "3", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    SortField sortField = new SortedSetSortField("value", false);
-    sortField.setMissingValue(SortField.STRING_FIRST);
-    Sort sort = new Sort(sortField);
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // 'bar' comes before 'baz'
-    // null comes first
-    assertEquals("3", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("1", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    assertEquals("2", searcher.doc(td.scoreDocs[2].doc).get("id"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMissingLast() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("foo")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("id", "3", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    SortField sortField = new SortedSetSortField("value", false);
-    sortField.setMissingValue(SortField.STRING_LAST);
-    Sort sort = new Sort(sortField);
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // 'bar' comes before 'baz'
-    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    // null comes last
-    assertEquals("3", searcher.doc(td.scoreDocs[2].doc).get("id"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-  
-  public void testSingleton() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortedSetSortField("value", false));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'bar' comes before 'baz'
-    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSortedSetSortField.java lucene5666/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSortedSetSortField.java
--- lucene-trunk/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSortedSetSortField.java	2014-05-14 03:47:28.854646625 -0400
+++ lucene5666/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSortedSetSortField.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,225 +0,0 @@
-package org.apache.lucene.sandbox.queries;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.SortField;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.LuceneTestCase;
-
-/** Simple tests for SortedSetSortField */
-public class TestSortedSetSortField extends LuceneTestCase {
-  
-  public void testForward() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(newStringField("value", "baz", Field.Store.NO));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("value", "foo", Field.Store.NO));
-    doc.add(newStringField("value", "bar", Field.Store.NO));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortedSetSortField("value", false));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'bar' comes before 'baz'
-    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testReverse() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(newStringField("value", "foo", Field.Store.NO));
-    doc.add(newStringField("value", "bar", Field.Store.NO));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("value", "baz", Field.Store.NO));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortedSetSortField("value", true));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'bar' comes before 'baz'
-    assertEquals("2", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("1", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMissingFirst() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(newStringField("value", "baz", Field.Store.NO));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("value", "foo", Field.Store.NO));
-    doc.add(newStringField("value", "bar", Field.Store.NO));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("id", "3", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    SortField sortField = new SortedSetSortField("value", false);
-    sortField.setMissingValue(SortField.STRING_FIRST);
-    Sort sort = new Sort(sortField);
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // 'bar' comes before 'baz'
-    // null comes first
-    assertEquals("3", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("1", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    assertEquals("2", searcher.doc(td.scoreDocs[2].doc).get("id"));
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMissingLast() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(newStringField("value", "baz", Field.Store.NO));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("value", "foo", Field.Store.NO));
-    doc.add(newStringField("value", "bar", Field.Store.NO));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("id", "3", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    SortField sortField = new SortedSetSortField("value", false);
-    sortField.setMissingValue(SortField.STRING_LAST);
-    Sort sort = new Sort(sortField);
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // 'bar' comes before 'baz'
-    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    // null comes last
-    assertEquals("3", searcher.doc(td.scoreDocs[2].doc).get("id"));
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testSingleton() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(newStringField("value", "baz", Field.Store.NO));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(newStringField("value", "bar", Field.Store.NO));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    Sort sort = new Sort(new SortedSetSortField("value", false));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'bar' comes before 'baz'
-    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testEmptyIndex() throws Exception {
-    IndexSearcher empty = newSearcher(new MultiReader());
-    Query query = new TermQuery(new Term("contents", "foo"));
-  
-    Sort sort = new Sort();
-    sort.setSort(new SortedSetSortField("sortedset", false));
-    TopDocs td = empty.search(query, null, 10, sort, true, true);
-    assertEquals(0, td.totalHits);
-    
-    // for an empty index, any selector should work
-    for (SortedSetSortField.Selector v : SortedSetSortField.Selector.values()) {
-      sort.setSort(new SortedSetSortField("sortedset", false, v));
-      td = empty.search(query, null, 10, sort, true, true);
-      assertEquals(0, td.totalHits);
-    }
-  }
-  
-  public void testEquals() throws Exception {
-    SortField sf = new SortedSetSortField("a", false);
-    assertFalse(sf.equals(null));
-    
-    assertEquals(sf, sf);
-    
-    SortField sf2 = new SortedSetSortField("a", false);
-    assertEquals(sf, sf2);
-    assertEquals(sf.hashCode(), sf2.hashCode());
-    
-    assertFalse(sf.equals(new SortedSetSortField("a", true)));
-    assertFalse(sf.equals(new SortedSetSortField("b", false)));
-    assertFalse(sf.equals(new SortedSetSortField("a", false, SortedSetSortField.Selector.MAX)));
-    assertFalse(sf.equals("foo"));
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSortedSetSortFieldSelectors.java lucene5666/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSortedSetSortFieldSelectors.java
--- lucene-trunk/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSortedSetSortFieldSelectors.java	2014-05-14 03:47:28.854646625 -0400
+++ lucene5666/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/TestSortedSetSortFieldSelectors.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,584 +0,0 @@
-package org.apache.lucene.sandbox.queries;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.diskdv.DiskDocValuesFormat;
-import org.apache.lucene.codecs.lucene45.Lucene45DocValuesFormat;
-import org.apache.lucene.codecs.memory.DirectDocValuesFormat;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.SortedSetDocValuesField;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.SortField;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-
-/** Tests for SortedSetSortField selectors other than MIN,
- *  these require optional codec support (random access to ordinals) */
-public class TestSortedSetSortFieldSelectors extends LuceneTestCase {
-  static Codec savedCodec;
-  
-  @BeforeClass
-  public static void beforeClass() throws Exception {
-    savedCodec = Codec.getDefault();
-    // currently only these codecs that support random access ordinals
-    int victim = random().nextInt(3);
-    switch(victim) {
-      case 0:  Codec.setDefault(TestUtil.alwaysDocValuesFormat(new DirectDocValuesFormat()));
-      case 1:  Codec.setDefault(TestUtil.alwaysDocValuesFormat(new DiskDocValuesFormat()));
-      default: Codec.setDefault(TestUtil.alwaysDocValuesFormat(new Lucene45DocValuesFormat()));
-    }
-  }
-  
-  @AfterClass
-  public static void afterClass() throws Exception {
-    Codec.setDefault(savedCodec);
-  }
-  
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    // ensure there is nothing in fieldcache before test starts
-    FieldCache.DEFAULT.purgeAllCaches();
-  }
-  
-  private void assertNoFieldCaches() {
-    // docvalues sorting should NOT create any fieldcache entries!
-    assertEquals(0, FieldCache.DEFAULT.getCacheEntries().length);
-  }
-  
-  public void testMax() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("foo")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-
-    // slow wrapper does not support random access ordinals (there is no need for that!)
-    IndexSearcher searcher = newSearcher(ir, false);
-    
-    Sort sort = new Sort(new SortedSetSortField("value", false, SortedSetSortField.Selector.MAX));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'baz' comes before 'foo'
-    assertEquals("2", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("1", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    assertNoFieldCaches();
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMaxReverse() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("foo")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    // slow wrapper does not support random access ordinals (there is no need for that!)
-    IndexSearcher searcher = newSearcher(ir, false);
-    
-    Sort sort = new Sort(new SortedSetSortField("value", true, SortedSetSortField.Selector.MAX));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'baz' comes before 'foo'
-    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    assertNoFieldCaches();
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMaxMissingFirst() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("foo")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
-    doc.add(newStringField("id", "3", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-
-    // slow wrapper does not support random access ordinals (there is no need for that!)
-    IndexSearcher searcher = newSearcher(ir, false);
-    
-    SortField sortField = new SortedSetSortField("value", false, SortedSetSortField.Selector.MAX);
-    sortField.setMissingValue(SortField.STRING_FIRST);
-    Sort sort = new Sort(sortField);
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // null comes first
-    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    // 'baz' comes before 'foo'
-    assertEquals("3", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    assertEquals("2", searcher.doc(td.scoreDocs[2].doc).get("id"));
-    assertNoFieldCaches();
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMaxMissingLast() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("foo")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
-    doc.add(newStringField("id", "3", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-
-    // slow wrapper does not support random access ordinals (there is no need for that!)
-    IndexSearcher searcher = newSearcher(ir, false);
-    
-    SortField sortField = new SortedSetSortField("value", false, SortedSetSortField.Selector.MAX);
-    sortField.setMissingValue(SortField.STRING_LAST);
-    Sort sort = new Sort(sortField);
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // 'baz' comes before 'foo'
-    assertEquals("3", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    // null comes last
-    assertEquals("1", searcher.doc(td.scoreDocs[2].doc).get("id"));
-    assertNoFieldCaches();
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMaxSingleton() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    // slow wrapper does not support random access ordinals (there is no need for that!)
-    IndexSearcher searcher = newSearcher(ir, false);
-    Sort sort = new Sort(new SortedSetSortField("value", false, SortedSetSortField.Selector.MAX));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'bar' comes before 'baz'
-    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMiddleMin() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("a")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("d")));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    // slow wrapper does not support random access ordinals (there is no need for that!)
-    IndexSearcher searcher = newSearcher(ir, false);
-    Sort sort = new Sort(new SortedSetSortField("value", false, SortedSetSortField.Selector.MIDDLE_MIN));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'b' comes before 'c'
-    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    assertNoFieldCaches();
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMiddleMinReverse() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("a")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("d")));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    // slow wrapper does not support random access ordinals (there is no need for that!)
-    IndexSearcher searcher = newSearcher(ir, false);
-    Sort sort = new Sort(new SortedSetSortField("value", true, SortedSetSortField.Selector.MIDDLE_MIN));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'b' comes before 'c'
-    assertEquals("2", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("1", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    assertNoFieldCaches();
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMiddleMinMissingFirst() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(newStringField("id", "3", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("a")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("d")));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    // slow wrapper does not support random access ordinals (there is no need for that!)
-    IndexSearcher searcher = newSearcher(ir, false);
-    SortField sortField = new SortedSetSortField("value", false, SortedSetSortField.Selector.MIDDLE_MIN);
-    sortField.setMissingValue(SortField.STRING_FIRST);
-    Sort sort = new Sort(sortField);
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // null comes first
-    assertEquals("3", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    // 'b' comes before 'c'
-    assertEquals("1", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    assertEquals("2", searcher.doc(td.scoreDocs[2].doc).get("id"));
-    assertNoFieldCaches();
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMiddleMinMissingLast() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(newStringField("id", "3", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("a")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("d")));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    // slow wrapper does not support random access ordinals (there is no need for that!)
-    IndexSearcher searcher = newSearcher(ir, false);
-    SortField sortField = new SortedSetSortField("value", false, SortedSetSortField.Selector.MIDDLE_MIN);
-    sortField.setMissingValue(SortField.STRING_LAST);
-    Sort sort = new Sort(sortField);
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // 'b' comes before 'c'
-    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    // null comes last
-    assertEquals("3", searcher.doc(td.scoreDocs[2].doc).get("id"));
-    assertNoFieldCaches();
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMiddleMinSingleton() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    // slow wrapper does not support random access ordinals (there is no need for that!)
-    IndexSearcher searcher = newSearcher(ir, false);
-    Sort sort = new Sort(new SortedSetSortField("value", false, SortedSetSortField.Selector.MIDDLE_MIN));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'bar' comes before 'baz'
-    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMiddleMax() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("a")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("d")));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    // slow wrapper does not support random access ordinals (there is no need for that!)
-    IndexSearcher searcher = newSearcher(ir, false);
-    Sort sort = new Sort(new SortedSetSortField("value", false, SortedSetSortField.Selector.MIDDLE_MAX));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'b' comes before 'c'
-    assertEquals("2", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("1", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    assertNoFieldCaches();
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMiddleMaxReverse() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("a")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("d")));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    // slow wrapper does not support random access ordinals (there is no need for that!)
-    IndexSearcher searcher = newSearcher(ir, false);
-    Sort sort = new Sort(new SortedSetSortField("value", true, SortedSetSortField.Selector.MIDDLE_MAX));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'b' comes before 'c'
-    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    assertNoFieldCaches();
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMiddleMaxMissingFirst() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(newStringField("id", "3", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("a")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("d")));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    // slow wrapper does not support random access ordinals (there is no need for that!)
-    IndexSearcher searcher = newSearcher(ir, false);
-    SortField sortField = new SortedSetSortField("value", false, SortedSetSortField.Selector.MIDDLE_MAX);
-    sortField.setMissingValue(SortField.STRING_FIRST);
-    Sort sort = new Sort(sortField);
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // null comes first
-    assertEquals("3", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    // 'b' comes before 'c'
-    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    assertEquals("1", searcher.doc(td.scoreDocs[2].doc).get("id"));
-    assertNoFieldCaches();
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMiddleMaxMissingLast() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(newStringField("id", "3", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("a")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("c")));
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("d")));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("b")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    // slow wrapper does not support random access ordinals (there is no need for that!)
-    IndexSearcher searcher = newSearcher(ir, false);
-    SortField sortField = new SortedSetSortField("value", false, SortedSetSortField.Selector.MIDDLE_MAX);
-    sortField.setMissingValue(SortField.STRING_LAST);
-    Sort sort = new Sort(sortField);
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(3, td.totalHits);
-    // 'b' comes before 'c'
-    assertEquals("2", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("1", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    // null comes last
-    assertEquals("3", searcher.doc(td.scoreDocs[2].doc).get("id"));
-    assertNoFieldCaches();
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMiddleMaxSingleton() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("baz")));
-    doc.add(newStringField("id", "2", Field.Store.YES));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new SortedSetDocValuesField("value", new BytesRef("bar")));
-    doc.add(newStringField("id", "1", Field.Store.YES));
-    writer.addDocument(doc);
-    IndexReader ir = writer.getReader();
-    writer.shutdown();
-    
-    // slow wrapper does not support random access ordinals (there is no need for that!)
-    IndexSearcher searcher = newSearcher(ir, false);
-    Sort sort = new Sort(new SortedSetSortField("value", false, SortedSetSortField.Selector.MIDDLE_MAX));
-
-    TopDocs td = searcher.search(new MatchAllDocsQuery(), 10, sort);
-    assertEquals(2, td.totalHits);
-    // 'bar' comes before 'baz'
-    assertEquals("1", searcher.doc(td.scoreDocs[0].doc).get("id"));
-    assertEquals("2", searcher.doc(td.scoreDocs[1].doc).get("id"));
-    assertNoFieldCaches();
-
-    ir.close();
-    dir.close();
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/spatial/build.xml lucene5666/lucene/spatial/build.xml
--- lucene-trunk/lucene/spatial/build.xml	2014-05-14 03:47:28.786646624 -0400
+++ lucene5666/lucene/spatial/build.xml	2014-05-12 13:28:38.364244641 -0400
@@ -32,6 +32,7 @@
     <path refid="base.classpath"/>
     <path refid="spatialjar"/>
     <pathelement path="${queries.jar}" />
+    <pathelement path="${misc.jar}" />
   </path>
 
   <path id="test.classpath">
@@ -40,12 +41,13 @@
     <pathelement path="src/test-files" />
   </path>
 
-  <target name="compile-core" depends="jar-queries,common.compile-core" />
+  <target name="compile-core" depends="jar-queries,jar-misc,common.compile-core" />
 
-  <target name="javadocs" depends="javadocs-queries,compile-core">
+  <target name="javadocs" depends="javadocs-queries,javadocs-misc,compile-core">
     <invoke-module-javadoc>
       <links>
         <link href="../queries"/>
+        <link href="../misc"/>
       </links>
     </invoke-module-javadoc>
   </target>


diff -ruN -x .svn -x build lucene-trunk/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxSimilarityValueSource.java lucene5666/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxSimilarityValueSource.java
--- lucene-trunk/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxSimilarityValueSource.java	2014-05-14 03:47:28.794646624 -0400
+++ lucene5666/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxSimilarityValueSource.java	2014-05-12 13:28:38.664244647 -0400
@@ -20,10 +20,11 @@
 import com.spatial4j.core.shape.Rectangle;
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.Explanation;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.Bits;
 
 import java.io.IOException;
@@ -64,13 +65,13 @@
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     AtomicReader reader = readerContext.reader();
-    final FieldCache.Doubles minX = FieldCache.DEFAULT.getDoubles(reader, strategy.field_minX, true);
-    final FieldCache.Doubles minY = FieldCache.DEFAULT.getDoubles(reader, strategy.field_minY, true);
-    final FieldCache.Doubles maxX = FieldCache.DEFAULT.getDoubles(reader, strategy.field_maxX, true);
-    final FieldCache.Doubles maxY = FieldCache.DEFAULT.getDoubles(reader, strategy.field_maxY, true);
+    final NumericDocValues minX = DocValues.getNumeric(reader, strategy.field_minX);
+    final NumericDocValues minY = DocValues.getNumeric(reader, strategy.field_minY);
+    final NumericDocValues maxX = DocValues.getNumeric(reader, strategy.field_maxX);
+    final NumericDocValues maxY = DocValues.getNumeric(reader, strategy.field_maxY);
 
-    final Bits validMinX = FieldCache.DEFAULT.getDocsWithField(reader, strategy.field_minX);
-    final Bits validMaxX = FieldCache.DEFAULT.getDocsWithField(reader, strategy.field_maxX);
+    final Bits validMinX = DocValues.getDocsWithField(reader, strategy.field_minX);
+    final Bits validMaxX = DocValues.getDocsWithField(reader, strategy.field_maxX);
 
     return new FunctionValues() {
       //reused
@@ -78,13 +79,13 @@
 
       @Override
       public float floatVal(int doc) {
-        double minXVal = minX.get(doc);
-        double maxXVal = maxX.get(doc);
+        double minXVal = Double.longBitsToDouble(minX.get(doc));
+        double maxXVal = Double.longBitsToDouble(maxX.get(doc));
         // make sure it has minX and area
         if ((minXVal != 0 || validMinX.get(doc)) && (maxXVal != 0 || validMaxX.get(doc))) {
           rect.reset(
               minXVal, maxXVal,
-              minY.get(doc), maxY.get(doc));
+              Double.longBitsToDouble(minY.get(doc)), Double.longBitsToDouble(maxY.get(doc)));
           return (float) similarity.score(rect, null);
         } else {
           return (float) similarity.score(null, null);
@@ -96,8 +97,8 @@
         // make sure it has minX and area
         if (validMinX.get(doc) && validMaxX.get(doc)) {
           rect.reset(
-              minX.get(doc), maxX.get(doc),
-              minY.get(doc), maxY.get(doc));
+              Double.longBitsToDouble(minX.get(doc)), Double.longBitsToDouble(maxX.get(doc)),
+              Double.longBitsToDouble(minY.get(doc)), Double.longBitsToDouble(maxY.get(doc)));
           Explanation exp = new Explanation();
           similarity.score(rect, exp);
           return exp;


diff -ruN -x .svn -x build lucene-trunk/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxStrategy.java lucene5666/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxStrategy.java
--- lucene-trunk/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxStrategy.java	2014-05-14 03:47:28.794646624 -0400
+++ lucene5666/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxStrategy.java	2014-05-12 13:28:38.664244647 -0400
@@ -25,6 +25,7 @@
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.function.FunctionQuery;
 import org.apache.lucene.queries.function.ValueSource;
@@ -64,7 +65,7 @@
  * The {@link #makeBBoxAreaSimilarityValueSource(com.spatial4j.core.shape.Rectangle)}
  * works by calculating the query bbox overlap percentage against the indexed
  * shape overlap percentage. The indexed shape's coordinates are retrieved from
- * the {@link org.apache.lucene.search.FieldCache}.
+ * {@link AtomicReader#getNumericDocValues}
  *
  * @lucene.experimental
  */


diff -ruN -x .svn -x build lucene-trunk/lucene/spatial/src/java/org/apache/lucene/spatial/DisjointSpatialFilter.java lucene5666/lucene/spatial/src/java/org/apache/lucene/spatial/DisjointSpatialFilter.java
--- lucene-trunk/lucene/spatial/src/java/org/apache/lucene/spatial/DisjointSpatialFilter.java	2014-05-14 03:47:28.790646624 -0400
+++ lucene5666/lucene/spatial/src/java/org/apache/lucene/spatial/DisjointSpatialFilter.java	2014-05-12 13:28:38.664244647 -0400
@@ -17,11 +17,12 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.queries.ChainedFilter;
 import org.apache.lucene.search.BitsFilteredDocIdSet;
 import org.apache.lucene.search.DocIdSet;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.spatial.query.SpatialArgs;
 import org.apache.lucene.spatial.query.SpatialOperation;
@@ -48,7 +49,7 @@
    * @param strategy Needed to compute intersects
    * @param args Used in spatial intersection
    * @param field This field is used to determine which docs have spatial data via
-   *               {@link org.apache.lucene.search.FieldCache#getDocsWithField(org.apache.lucene.index.AtomicReader, String)}.
+   *               {@link AtomicReader#getDocsWithField(String)}.
    *              Passing null will assume all docs have spatial data.
    */
   public DisjointSpatialFilter(SpatialStrategy strategy, SpatialArgs args, String field) {
@@ -92,7 +93,7 @@
       // which is nice but loading it in this way might be slower than say using an
       // intersects filter against the world bounds. So do we add a method to the
       // strategy, perhaps?  But the strategy can't cache it.
-      docsWithField = FieldCache.DEFAULT.getDocsWithField(context.reader(), field);
+      docsWithField = DocValues.getDocsWithField(context.reader(), field);
 
       final int maxDoc = context.reader().maxDoc();
       if (docsWithField.length() != maxDoc )


diff -ruN -x .svn -x build lucene-trunk/lucene/spatial/src/java/org/apache/lucene/spatial/SpatialStrategy.java lucene5666/lucene/spatial/src/java/org/apache/lucene/spatial/SpatialStrategy.java
--- lucene-trunk/lucene/spatial/src/java/org/apache/lucene/spatial/SpatialStrategy.java	2014-05-14 03:47:28.794646624 -0400
+++ lucene5666/lucene/spatial/src/java/org/apache/lucene/spatial/SpatialStrategy.java	2014-05-12 13:28:39.484244661 -0400
@@ -41,8 +41,7 @@
  *   <li>What types of query shapes can be used?</li>
  *   <li>What types of query operations are supported?
  *   This might vary per shape.</li>
- *   <li>Does it use the {@link org.apache.lucene.search.FieldCache},
- *   or some other type of cache?  When?
+ *   <li>Does it use some type of cache?  When?
  * </ul>
  * If a strategy only supports certain shapes at index or query time, then in
  * general it will throw an exception if given an incompatible one.  It will not


diff -ruN -x .svn -x build lucene-trunk/lucene/spatial/src/java/org/apache/lucene/spatial/vector/DistanceValueSource.java lucene5666/lucene/spatial/src/java/org/apache/lucene/spatial/vector/DistanceValueSource.java
--- lucene-trunk/lucene/spatial/src/java/org/apache/lucene/spatial/vector/DistanceValueSource.java	2014-05-14 03:47:28.790646624 -0400
+++ lucene5666/lucene/spatial/src/java/org/apache/lucene/spatial/vector/DistanceValueSource.java	2014-05-12 13:28:40.604244680 -0400
@@ -21,9 +21,10 @@
 import com.spatial4j.core.shape.Point;
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.Bits;
 
 import java.io.IOException;
@@ -65,10 +66,10 @@
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     AtomicReader reader = readerContext.reader();
 
-    final FieldCache.Doubles ptX = FieldCache.DEFAULT.getDoubles(reader, strategy.getFieldNameX(), true);
-    final FieldCache.Doubles ptY = FieldCache.DEFAULT.getDoubles(reader, strategy.getFieldNameY(), true);
-    final Bits validX =  FieldCache.DEFAULT.getDocsWithField(reader, strategy.getFieldNameX());
-    final Bits validY =  FieldCache.DEFAULT.getDocsWithField(reader, strategy.getFieldNameY());
+    final NumericDocValues ptX = DocValues.getNumeric(reader, strategy.getFieldNameX());
+    final NumericDocValues ptY = DocValues.getNumeric(reader, strategy.getFieldNameY());
+    final Bits validX =  DocValues.getDocsWithField(reader, strategy.getFieldNameX());
+    final Bits validY =  DocValues.getDocsWithField(reader, strategy.getFieldNameY());
 
     return new FunctionValues() {
 
@@ -87,7 +88,7 @@
         // make sure it has minX and area
         if (validX.get(doc)) {
           assert validY.get(doc);
-          return calculator.distance(from, ptX.get(doc), ptY.get(doc)) * multiplier;
+          return calculator.distance(from, Double.longBitsToDouble(ptX.get(doc)), Double.longBitsToDouble(ptY.get(doc))) * multiplier;
         }
         return nullValue;
       }


diff -ruN -x .svn -x build lucene-trunk/lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestCase.java lucene5666/lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestCase.java
--- lucene-trunk/lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestCase.java	2014-05-14 03:47:28.798646624 -0400
+++ lucene5666/lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestCase.java	2014-05-12 13:28:38.436244643 -0400
@@ -32,6 +32,8 @@
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.uninverting.UninvertingReader;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
@@ -42,7 +44,9 @@
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.List;
+import java.util.Map;
 import java.util.Random;
 
 import static com.carrotsearch.randomizedtesting.RandomizedTest.randomGaussian;
@@ -59,15 +63,26 @@
 
   protected SpatialContext ctx;//subclass must initialize
 
+  Map<String,Type> uninvertMap = new HashMap<>();
+  
   @Override
   @Before
   public void setUp() throws Exception {
     super.setUp();
-
+    // TODO: change this module to index docvalues instead of uninverting
+    uninvertMap.clear();
+    uninvertMap.put("bbox__minX", Type.DOUBLE);
+    uninvertMap.put("bbox__maxX", Type.DOUBLE);
+    uninvertMap.put("bbox__minY", Type.DOUBLE);
+    uninvertMap.put("bbox__maxY", Type.DOUBLE);
+    uninvertMap.put("pointvector__x", Type.DOUBLE);
+    uninvertMap.put("pointvector__y", Type.DOUBLE);
+    uninvertMap.put("SpatialOpRecursivePrefixTreeTest", Type.SORTED);
+    
     directory = newDirectory();
     final Random random = random();
     indexWriter = new RandomIndexWriter(random,directory, newIndexWriterConfig(random));
-    indexReader = indexWriter.getReader();
+    indexReader = UninvertingReader.wrap(indexWriter.getReader(), uninvertMap);
     indexSearcher = newSearcher(indexReader);
   }
 
@@ -110,8 +125,11 @@
 
   protected void commit() throws IOException {
     indexWriter.commit();
-    IOUtils.close(indexReader);
-    indexReader = indexWriter.getReader();
+    DirectoryReader newReader = DirectoryReader.openIfChanged(indexReader);
+    if (newReader != null) {
+      IOUtils.close(indexReader);
+      indexReader = newReader;
+    }
     indexSearcher = newSearcher(indexReader);
   }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase.java lucene5666/lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase.java	2014-05-14 03:47:28.866646625 -0400
+++ lucene5666/lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase.java	2014-05-12 13:28:58.536244993 -0400
@@ -154,83 +154,6 @@
     farsiIndex.close();
   }
   
-  // Test using various international locales with accented characters (which
-  // sort differently depending on locale)
-  //
-  // Copied (and slightly modified) from 
-  // org.apache.lucene.search.TestSort.testInternationalSort()
-  //  
-  // TODO: this test is really fragile. there are already 3 different cases,
-  // depending upon unicode version.
-  public void testCollationKeySort(Analyzer usAnalyzer,
-                                   Analyzer franceAnalyzer,
-                                   Analyzer swedenAnalyzer,
-                                   Analyzer denmarkAnalyzer,
-                                   String usResult,
-                                   String frResult,
-                                   String svResult,
-                                   String dkResult) throws Exception {
-    Directory indexStore = newDirectory();
-    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
-
-    // document data:
-    // the tracer field is used to determine which document was hit
-    String[][] sortData = new String[][] {
-      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)
-      {  "A",   "x",     "p\u00EAche",      "p\u00EAche",      "p\u00EAche",      "p\u00EAche"      },
-      {  "B",   "y",     "HAT",             "HAT",             "HAT",             "HAT"             },
-      {  "C",   "x",     "p\u00E9ch\u00E9", "p\u00E9ch\u00E9", "p\u00E9ch\u00E9", "p\u00E9ch\u00E9" },
-      {  "D",   "y",     "HUT",             "HUT",             "HUT",             "HUT"             },
-      {  "E",   "x",     "peach",           "peach",           "peach",           "peach"           },
-      {  "F",   "y",     "H\u00C5T",        "H\u00C5T",        "H\u00C5T",        "H\u00C5T"        },
-      {  "G",   "x",     "sin",             "sin",             "sin",             "sin"             },
-      {  "H",   "y",     "H\u00D8T",        "H\u00D8T",        "H\u00D8T",        "H\u00D8T"        },
-      {  "I",   "x",     "s\u00EDn",        "s\u00EDn",        "s\u00EDn",        "s\u00EDn"        },
-      {  "J",   "y",     "HOT",             "HOT",             "HOT",             "HOT"             },
-    };
-
-    FieldType customType = new FieldType();
-    customType.setStored(true);
-    
-    for (int i = 0 ; i < sortData.length ; ++i) {
-      Document doc = new Document();
-      doc.add(new Field("tracer", sortData[i][0], customType));
-      doc.add(new TextField("contents", sortData[i][1], Field.Store.NO));
-      if (sortData[i][2] != null) 
-        doc.add(new TextField("US", usAnalyzer.tokenStream("US", sortData[i][2])));
-      if (sortData[i][3] != null) 
-        doc.add(new TextField("France", franceAnalyzer.tokenStream("France", sortData[i][3])));
-      if (sortData[i][4] != null)
-        doc.add(new TextField("Sweden", swedenAnalyzer.tokenStream("Sweden", sortData[i][4])));
-      if (sortData[i][5] != null) 
-        doc.add(new TextField("Denmark", denmarkAnalyzer.tokenStream("Denmark", sortData[i][5])));
-      writer.addDocument(doc);
-    }
-    writer.forceMerge(1);
-    writer.shutdown();
-    IndexReader reader = DirectoryReader.open(indexStore);
-    IndexSearcher searcher = new IndexSearcher(reader);
-
-    Sort sort = new Sort();
-    Query queryX = new TermQuery(new Term ("contents", "x"));
-    Query queryY = new TermQuery(new Term ("contents", "y"));
-    
-    sort.setSort(new SortField("US", SortField.Type.STRING));
-    assertMatches(searcher, queryY, sort, usResult);
-
-    sort.setSort(new SortField("France", SortField.Type.STRING));
-    assertMatches(searcher, queryX, sort, frResult);
-
-    sort.setSort(new SortField("Sweden", SortField.Type.STRING));
-    assertMatches(searcher, queryY, sort, svResult);
-
-    sort.setSort(new SortField("Denmark", SortField.Type.STRING));
-    assertMatches(searcher, queryY, sort, dkResult);
-    reader.close();
-    indexStore.close();
-  }
-    
   // Make sure the documents returned by the search match the expected list
   // Copied from TestSort.java
   private void assertMatches(IndexSearcher searcher, Query query, Sort sort, 


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java lucene5666/lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java	2014-05-14 03:47:28.870646625 -0400
+++ lucene5666/lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java	2014-05-12 13:28:58.288244988 -0400
@@ -23,7 +23,6 @@
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashMap;
-import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
 import java.util.Set;
@@ -33,7 +32,6 @@
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.lucene42.Lucene42DocValuesFormat;
 import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -47,7 +45,6 @@
 import org.apache.lucene.index.TermsEnum.SeekStatus;
 import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.ScoreDoc;
@@ -1278,73 +1275,6 @@
     dir.close();
   }
   
-  private void doTestMissingVsFieldCache(final long minValue, final long maxValue) throws Exception {
-    doTestMissingVsFieldCache(new LongProducer() {
-      @Override
-      long next() {
-        return TestUtil.nextLong(random(), minValue, maxValue);
-      }
-    });
-  }
-  
-  private void doTestMissingVsFieldCache(LongProducer longs) throws Exception {
-    assumeTrue("Codec does not support getDocsWithField", defaultCodecSupportsDocsWithField());
-    Directory dir = newDirectory();
-    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);
-    Field idField = new StringField("id", "", Field.Store.NO);
-    Field indexedField = newStringField("indexed", "", Field.Store.NO);
-    Field dvField = new NumericDocValuesField("dv", 0);
-
-    
-    // index some docs
-    int numDocs = atLeast(300);
-    // numDocs should be always > 256 so that in case of a codec that optimizes
-    // for numbers of values <= 256, all storage layouts are tested
-    assert numDocs > 256;
-    for (int i = 0; i < numDocs; i++) {
-      idField.setStringValue(Integer.toString(i));
-      long value = longs.next();
-      indexedField.setStringValue(Long.toString(value));
-      dvField.setLongValue(value);
-      Document doc = new Document();
-      doc.add(idField);
-      // 1/4 of the time we neglect to add the fields
-      if (random().nextInt(4) > 0) {
-        doc.add(indexedField);
-        doc.add(dvField);
-      }
-      writer.addDocument(doc);
-      if (random().nextInt(31) == 0) {
-        writer.commit();
-      }
-    }
-    
-    // delete some docs
-    int numDeletions = random().nextInt(numDocs/10);
-    for (int i = 0; i < numDeletions; i++) {
-      int id = random().nextInt(numDocs);
-      writer.deleteDocuments(new Term("id", Integer.toString(id)));
-    }
-
-    // merge some segments and ensure that at least one of them has more than
-    // 256 values
-    writer.forceMerge(numDocs / 256);
-
-    writer.shutdown();
-    
-    // compare
-    DirectoryReader ir = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
-      Bits expected = FieldCache.DEFAULT.getDocsWithField(r, "indexed");
-      Bits actual = FieldCache.DEFAULT.getDocsWithField(r, "dv");
-      assertEquals(expected, actual);
-    }
-    ir.close();
-    dir.close();
-  }
-  
   public void testBooleanNumericsVsStoredFields() throws Exception {
     int numIterations = atLeast(1);
     for (int i = 0; i < numIterations; i++) {
@@ -1359,13 +1289,6 @@
     }
   }
   
-  public void testByteMissingVsFieldCache() throws Exception {
-    int numIterations = atLeast(1);
-    for (int i = 0; i < numIterations; i++) {
-      doTestMissingVsFieldCache(Byte.MIN_VALUE, Byte.MAX_VALUE);
-    }
-  }
-  
   public void testShortNumericsVsStoredFields() throws Exception {
     int numIterations = atLeast(1);
     for (int i = 0; i < numIterations; i++) {
@@ -1373,13 +1296,6 @@
     }
   }
   
-  public void testShortMissingVsFieldCache() throws Exception {
-    int numIterations = atLeast(1);
-    for (int i = 0; i < numIterations; i++) {
-      doTestMissingVsFieldCache(Short.MIN_VALUE, Short.MAX_VALUE);
-    }
-  }
-  
   public void testIntNumericsVsStoredFields() throws Exception {
     int numIterations = atLeast(1);
     for (int i = 0; i < numIterations; i++) {
@@ -1387,13 +1303,6 @@
     }
   }
   
-  public void testIntMissingVsFieldCache() throws Exception {
-    int numIterations = atLeast(1);
-    for (int i = 0; i < numIterations; i++) {
-      doTestMissingVsFieldCache(Integer.MIN_VALUE, Integer.MAX_VALUE);
-    }
-  }
-  
   public void testLongNumericsVsStoredFields() throws Exception {
     int numIterations = atLeast(1);
     for (int i = 0; i < numIterations; i++) {
@@ -1401,13 +1310,6 @@
     }
   }
   
-  public void testLongMissingVsFieldCache() throws Exception {
-    int numIterations = atLeast(1);
-    for (int i = 0; i < numIterations; i++) {
-      doTestMissingVsFieldCache(Long.MIN_VALUE, Long.MAX_VALUE);
-    }
-  }
-  
   private void doTestBinaryVsStoredFields(int minLength, int maxLength) throws Exception {
     Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
@@ -1535,57 +1437,6 @@
     dir.close();
   }
   
-  private void doTestSortedVsFieldCache(int minLength, int maxLength) throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);
-    Document doc = new Document();
-    Field idField = new StringField("id", "", Field.Store.NO);
-    Field indexedField = new StringField("indexed", "", Field.Store.NO);
-    Field dvField = new SortedDocValuesField("dv", new BytesRef());
-    doc.add(idField);
-    doc.add(indexedField);
-    doc.add(dvField);
-    
-    // index some docs
-    int numDocs = atLeast(300);
-    for (int i = 0; i < numDocs; i++) {
-      idField.setStringValue(Integer.toString(i));
-      final int length;
-      if (minLength == maxLength) {
-        length = minLength; // fixed length
-      } else {
-        length = TestUtil.nextInt(random(), minLength, maxLength);
-      }
-      String value = TestUtil.randomSimpleString(random(), length);
-      indexedField.setStringValue(value);
-      dvField.setBytesValue(new BytesRef(value));
-      writer.addDocument(doc);
-      if (random().nextInt(31) == 0) {
-        writer.commit();
-      }
-    }
-    
-    // delete some docs
-    int numDeletions = random().nextInt(numDocs/10);
-    for (int i = 0; i < numDeletions; i++) {
-      int id = random().nextInt(numDocs);
-      writer.deleteDocuments(new Term("id", Integer.toString(id)));
-    }
-    writer.shutdown();
-    
-    // compare
-    DirectoryReader ir = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
-      SortedDocValues expected = FieldCache.DEFAULT.getTermsIndex(r, "indexed");
-      SortedDocValues actual = r.getSortedDocValues("dv");
-      assertEquals(r.maxDoc(), expected, actual);
-    }
-    ir.close();
-    dir.close();
-  }
-  
   public void testSortedFixedLengthVsStoredFields() throws Exception {
     int numIterations = atLeast(1);
     for (int i = 0; i < numIterations; i++) {
@@ -1594,21 +1445,6 @@
     }
   }
   
-  public void testSortedFixedLengthVsFieldCache() throws Exception {
-    int numIterations = atLeast(1);
-    for (int i = 0; i < numIterations; i++) {
-      int fixedLength = TestUtil.nextInt(random(), 1, 10);
-      doTestSortedVsFieldCache(fixedLength, fixedLength);
-    }
-  }
-  
-  public void testSortedVariableLengthVsFieldCache() throws Exception {
-    int numIterations = atLeast(1);
-    for (int i = 0; i < numIterations; i++) {
-      doTestSortedVsFieldCache(1, 10);
-    }
-  }
-  
   public void testSortedVariableLengthVsStoredFields() throws Exception {
     int numIterations = atLeast(1);
     for (int i = 0; i < numIterations; i++) {
@@ -2173,206 +2009,6 @@
     }
   }
 
-  private void assertEquals(Bits expected, Bits actual) throws Exception {
-    assertEquals(expected.length(), actual.length());
-    for (int i = 0; i < expected.length(); i++) {
-      assertEquals(expected.get(i), actual.get(i));
-    }
-  }
-  
-  private void assertEquals(int maxDoc, SortedDocValues expected, SortedDocValues actual) throws Exception {
-    assertEquals(maxDoc, new SingletonSortedSetDocValues(expected), new SingletonSortedSetDocValues(actual));
-  }
-  
-  private void assertEquals(int maxDoc, SortedSetDocValues expected, SortedSetDocValues actual) throws Exception {
-    // can be null for the segment if no docs actually had any SortedDocValues
-    // in this case FC.getDocTermsOrds returns EMPTY
-    if (actual == null) {
-      assertEquals(DocValues.EMPTY_SORTED_SET, expected);
-      return;
-    }
-    assertEquals(expected.getValueCount(), actual.getValueCount());
-    // compare ord lists
-    for (int i = 0; i < maxDoc; i++) {
-      expected.setDocument(i);
-      actual.setDocument(i);
-      long expectedOrd;
-      while ((expectedOrd = expected.nextOrd()) != NO_MORE_ORDS) {
-        assertEquals(expectedOrd, actual.nextOrd());
-      }
-      assertEquals(NO_MORE_ORDS, actual.nextOrd());
-    }
-    
-    // compare ord dictionary
-    BytesRef expectedBytes = new BytesRef();
-    BytesRef actualBytes = new BytesRef();
-    for (long i = 0; i < expected.getValueCount(); i++) {
-      expected.lookupTerm(expectedBytes);
-      actual.lookupTerm(actualBytes);
-      assertEquals(expectedBytes, actualBytes);
-    }
-    
-    // compare termsenum
-    assertEquals(expected.getValueCount(), expected.termsEnum(), actual.termsEnum());
-  }
-  
-  private void assertEquals(long numOrds, TermsEnum expected, TermsEnum actual) throws Exception {
-    BytesRef ref;
-    
-    // sequential next() through all terms
-    while ((ref = expected.next()) != null) {
-      assertEquals(ref, actual.next());
-      assertEquals(expected.ord(), actual.ord());
-      assertEquals(expected.term(), actual.term());
-    }
-    assertNull(actual.next());
-    
-    // sequential seekExact(ord) through all terms
-    for (long i = 0; i < numOrds; i++) {
-      expected.seekExact(i);
-      actual.seekExact(i);
-      assertEquals(expected.ord(), actual.ord());
-      assertEquals(expected.term(), actual.term());
-    }
-    
-    // sequential seekExact(BytesRef) through all terms
-    for (long i = 0; i < numOrds; i++) {
-      expected.seekExact(i);
-      assertTrue(actual.seekExact(expected.term()));
-      assertEquals(expected.ord(), actual.ord());
-      assertEquals(expected.term(), actual.term());
-    }
-    
-    // sequential seekCeil(BytesRef) through all terms
-    for (long i = 0; i < numOrds; i++) {
-      expected.seekExact(i);
-      assertEquals(SeekStatus.FOUND, actual.seekCeil(expected.term()));
-      assertEquals(expected.ord(), actual.ord());
-      assertEquals(expected.term(), actual.term());
-    }
-    
-    // random seekExact(ord)
-    for (long i = 0; i < numOrds; i++) {
-      long randomOrd = TestUtil.nextLong(random(), 0, numOrds - 1);
-      expected.seekExact(randomOrd);
-      actual.seekExact(randomOrd);
-      assertEquals(expected.ord(), actual.ord());
-      assertEquals(expected.term(), actual.term());
-    }
-    
-    // random seekExact(BytesRef)
-    for (long i = 0; i < numOrds; i++) {
-      long randomOrd = TestUtil.nextLong(random(), 0, numOrds - 1);
-      expected.seekExact(randomOrd);
-      actual.seekExact(expected.term());
-      assertEquals(expected.ord(), actual.ord());
-      assertEquals(expected.term(), actual.term());
-    }
-    
-    // random seekCeil(BytesRef)
-    for (long i = 0; i < numOrds; i++) {
-      BytesRef target = new BytesRef(TestUtil.randomUnicodeString(random()));
-      SeekStatus expectedStatus = expected.seekCeil(target);
-      assertEquals(expectedStatus, actual.seekCeil(target));
-      if (expectedStatus != SeekStatus.END) {
-        assertEquals(expected.ord(), actual.ord());
-        assertEquals(expected.term(), actual.term());
-      }
-    }
-  }
-  
-  private void doTestSortedSetVsUninvertedField(int minLength, int maxLength) throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);
-    
-    // index some docs
-    int numDocs = atLeast(300);
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      Field idField = new StringField("id", Integer.toString(i), Field.Store.NO);
-      doc.add(idField);
-      final int length;
-      if (minLength == maxLength) {
-        length = minLength; // fixed length
-      } else {
-        length = TestUtil.nextInt(random(), minLength, maxLength);
-      }
-      int numValues = random().nextInt(17);
-      // create a random list of strings
-      List<String> values = new ArrayList<>();
-      for (int v = 0; v < numValues; v++) {
-        values.add(TestUtil.randomSimpleString(random(), length));
-      }
-      
-      // add in any order to the indexed field
-      ArrayList<String> unordered = new ArrayList<>(values);
-      Collections.shuffle(unordered, random());
-      for (String v : values) {
-        doc.add(newStringField("indexed", v, Field.Store.NO));
-      }
-
-      // add in any order to the dv field
-      ArrayList<String> unordered2 = new ArrayList<>(values);
-      Collections.shuffle(unordered2, random());
-      for (String v : unordered2) {
-        doc.add(new SortedSetDocValuesField("dv", new BytesRef(v)));
-      }
-
-      writer.addDocument(doc);
-      if (random().nextInt(31) == 0) {
-        writer.commit();
-      }
-    }
-    
-    // delete some docs
-    int numDeletions = random().nextInt(numDocs/10);
-    for (int i = 0; i < numDeletions; i++) {
-      int id = random().nextInt(numDocs);
-      writer.deleteDocuments(new Term("id", Integer.toString(id)));
-    }
-    
-    // compare per-segment
-    DirectoryReader ir = writer.getReader();
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
-      SortedSetDocValues expected = FieldCache.DEFAULT.getDocTermOrds(r, "indexed");
-      SortedSetDocValues actual = r.getSortedSetDocValues("dv");
-      assertEquals(r.maxDoc(), expected, actual);
-    }
-    ir.close();
-    
-    writer.forceMerge(1);
-    
-    // now compare again after the merge
-    ir = writer.getReader();
-    AtomicReader ar = getOnlySegmentReader(ir);
-    SortedSetDocValues expected = FieldCache.DEFAULT.getDocTermOrds(ar, "indexed");
-    SortedSetDocValues actual = ar.getSortedSetDocValues("dv");
-    assertEquals(ir.maxDoc(), expected, actual);
-    ir.close();
-    
-    writer.shutdown();
-    dir.close();
-  }
-  
-  public void testSortedSetFixedLengthVsUninvertedField() throws Exception {
-    assumeTrue("Codec does not support SORTED_SET", defaultCodecSupportsSortedSet());
-    int numIterations = atLeast(1);
-    for (int i = 0; i < numIterations; i++) {
-      int fixedLength = TestUtil.nextInt(random(), 1, 10);
-      doTestSortedSetVsUninvertedField(fixedLength, fixedLength);
-    }
-  }
-  
-  public void testSortedSetVariableLengthVsUninvertedField() throws Exception {
-    assumeTrue("Codec does not support SORTED_SET", defaultCodecSupportsSortedSet());
-    int numIterations = atLeast(1);
-    for (int i = 0; i < numIterations; i++) {
-      doTestSortedSetVsUninvertedField(1, 10);
-    }
-  }
-
   public void testGCDCompression() throws Exception {
     int numIterations = atLeast(1);
     for (int i = 0; i < numIterations; i++) {
@@ -2606,172 +2242,6 @@
     ir.close();
     directory.close();
   }
-
-  // LUCENE-4853
-  public void testHugeBinaryValues() throws Exception {
-    Analyzer analyzer = new MockAnalyzer(random());
-    // FSDirectory because SimpleText will consume gobbs of
-    // space when storing big binary values:
-    Directory d = newFSDirectory(createTempDir("hugeBinaryValues"));
-    boolean doFixed = random().nextBoolean();
-    int numDocs;
-    int fixedLength = 0;
-    if (doFixed) {
-      // Sometimes make all values fixed length since some
-      // codecs have different code paths for this:
-      numDocs = TestUtil.nextInt(random(), 10, 20);
-      fixedLength = TestUtil.nextInt(random(), 65537, 256 * 1024);
-    } else {
-      numDocs = TestUtil.nextInt(random(), 100, 200);
-    }
-    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
-    List<byte[]> docBytes = new ArrayList<>();
-    long totalBytes = 0;
-    for(int docID=0;docID<numDocs;docID++) {
-      // we don't use RandomIndexWriter because it might add
-      // more docvalues than we expect !!!!
-
-      // Must be > 64KB in size to ensure more than 2 pages in
-      // PagedBytes would be needed:
-      int numBytes;
-      if (doFixed) {
-        numBytes = fixedLength;
-      } else if (docID == 0 || random().nextInt(5) == 3) {
-        numBytes = TestUtil.nextInt(random(), 65537, 3 * 1024 * 1024);
-      } else {
-        numBytes = TestUtil.nextInt(random(), 1, 1024 * 1024);
-      }
-      totalBytes += numBytes;
-      if (totalBytes > 5 * 1024*1024) {
-        break;
-      }
-      byte[] bytes = new byte[numBytes];
-      random().nextBytes(bytes);
-      docBytes.add(bytes);
-      Document doc = new Document();      
-      BytesRef b = new BytesRef(bytes);
-      b.length = bytes.length;
-      doc.add(new BinaryDocValuesField("field", b));
-      doc.add(new StringField("id", ""+docID, Field.Store.YES));
-      try {
-        w.addDocument(doc);
-      } catch (IllegalArgumentException iae) {
-        if (iae.getMessage().indexOf("is too large") == -1) {
-          throw iae;
-        } else {
-          // OK: some codecs can't handle binary DV > 32K
-          assertFalse(codecAcceptsHugeBinaryValues("field"));
-          w.rollback();
-          d.close();
-          return;
-        }
-      }
-    }
-    
-    DirectoryReader r;
-    try {
-      r = w.getReader();
-    } catch (IllegalArgumentException iae) {
-      if (iae.getMessage().indexOf("is too large") == -1) {
-        throw iae;
-      } else {
-        assertFalse(codecAcceptsHugeBinaryValues("field"));
-
-        // OK: some codecs can't handle binary DV > 32K
-        w.rollback();
-        d.close();
-        return;
-      }
-    }
-    w.shutdown();
-
-    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);
-
-    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, "field", false);
-    for(int docID=0;docID<docBytes.size();docID++) {
-      StoredDocument doc = ar.document(docID);
-      BytesRef bytes = new BytesRef();
-      s.get(docID, bytes);
-      byte[] expected = docBytes.get(Integer.parseInt(doc.get("id")));
-      assertEquals(expected.length, bytes.length);
-      assertEquals(new BytesRef(expected), bytes);
-    }
-
-    assertTrue(codecAcceptsHugeBinaryValues("field"));
-
-    ar.close();
-    d.close();
-  }
-
-  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)
-  public void testHugeBinaryValueLimit() throws Exception {
-    // We only test DVFormats that have a limit
-    assumeFalse("test requires codec with limits on max binary field length", codecAcceptsHugeBinaryValues("field"));
-    Analyzer analyzer = new MockAnalyzer(random());
-    // FSDirectory because SimpleText will consume gobbs of
-    // space when storing big binary values:
-    Directory d = newFSDirectory(createTempDir("hugeBinaryValues"));
-    boolean doFixed = random().nextBoolean();
-    int numDocs;
-    int fixedLength = 0;
-    if (doFixed) {
-      // Sometimes make all values fixed length since some
-      // codecs have different code paths for this:
-      numDocs = TestUtil.nextInt(random(), 10, 20);
-      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;
-    } else {
-      numDocs = TestUtil.nextInt(random(), 100, 200);
-    }
-    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
-    List<byte[]> docBytes = new ArrayList<>();
-    long totalBytes = 0;
-    for(int docID=0;docID<numDocs;docID++) {
-      // we don't use RandomIndexWriter because it might add
-      // more docvalues than we expect !!!!
-
-      // Must be > 64KB in size to ensure more than 2 pages in
-      // PagedBytes would be needed:
-      int numBytes;
-      if (doFixed) {
-        numBytes = fixedLength;
-      } else if (docID == 0 || random().nextInt(5) == 3) {
-        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;
-      } else {
-        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);
-      }
-      totalBytes += numBytes;
-      if (totalBytes > 5 * 1024*1024) {
-        break;
-      }
-      byte[] bytes = new byte[numBytes];
-      random().nextBytes(bytes);
-      docBytes.add(bytes);
-      Document doc = new Document();      
-      BytesRef b = new BytesRef(bytes);
-      b.length = bytes.length;
-      doc.add(new BinaryDocValuesField("field", b));
-      doc.add(new StringField("id", ""+docID, Field.Store.YES));
-      w.addDocument(doc);
-    }
-    
-    DirectoryReader r = w.getReader();
-    w.shutdown();
-
-    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);
-
-    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, "field", false);
-    for(int docID=0;docID<docBytes.size();docID++) {
-      StoredDocument doc = ar.document(docID);
-      BytesRef bytes = new BytesRef();
-      s.get(docID, bytes);
-      byte[] expected = docBytes.get(Integer.parseInt(doc.get("id")));
-      assertEquals(expected.length, bytes.length);
-      assertEquals(new BytesRef(expected), bytes);
-    }
-
-    ar.close();
-    d.close();
-  }
   
   /** Tests dv against stored fields with threads (binary/numeric/sorted, no missing) */
   public void testThreads() throws Exception {


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java lucene5666/lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java	2014-05-14 03:47:28.870646625 -0400
+++ lucene5666/lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java	2014-05-12 13:28:58.632244994 -0400
@@ -43,10 +43,10 @@
 import org.apache.lucene.document.FloatField;
 import org.apache.lucene.document.IntField;
 import org.apache.lucene.document.LongField;
+import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.StoredField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.NumericRangeQuery;
 import org.apache.lucene.search.Query;
@@ -289,6 +289,7 @@
       FieldType ft = new FieldType(IntField.TYPE_STORED);
       ft.setNumericPrecisionStep(Integer.MAX_VALUE);
       doc.add(new IntField("id", id, ft));
+      doc.add(new NumericDocValuesField("id", id));
       w.addDocument(doc);
     }
     final DirectoryReader r = w.getReader();
@@ -298,12 +299,12 @@
 
     for(AtomicReaderContext ctx : r.leaves()) {
       final AtomicReader sub = ctx.reader();
-      final FieldCache.Ints ids = FieldCache.DEFAULT.getInts(sub, "id", false);
+      final NumericDocValues ids = DocValues.getNumeric(sub, "id");
       for(int docID=0;docID<sub.numDocs();docID++) {
         final StoredDocument doc = sub.document(docID);
         final Field f = (Field) doc.getField("nf");
         assertTrue("got f=" + f, f instanceof StoredField);
-        assertEquals(answers[ids.get(docID)], f.numericValue());
+        assertEquals(answers[(int) ids.get(docID)], f.numericValue());
       }
     }
     r.close();


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/search/QueryUtils.java lucene5666/lucene/test-framework/src/java/org/apache/lucene/search/QueryUtils.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/search/QueryUtils.java	2014-05-14 03:47:28.866646625 -0400
+++ lucene5666/lucene/test-framework/src/java/org/apache/lucene/search/QueryUtils.java	2014-05-14 03:45:16.722644324 -0400
@@ -130,11 +130,6 @@
     }
   }
   
-  public static void purgeFieldCache(IndexReader r) throws IOException {
-    // this is just a hack, to get an atomic reader that contains all subreaders for insanity checks
-    FieldCache.DEFAULT.purgeByCacheKey(SlowCompositeReaderWrapper.wrap(r).getCoreCacheKey());
-  }
-  
   /** This is a MultiReader that can be used for randomly wrapping other readers
    * without creating FieldCache insanity.
    * The trick is to use an opaque/fake cache key. */


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java lucene5666/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java	2014-05-14 03:47:28.874646625 -0400
+++ lucene5666/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java	2014-05-14 03:45:16.722644324 -0400
@@ -105,8 +105,6 @@
 import org.apache.lucene.index.TieredMergePolicy;
 import org.apache.lucene.search.AssertingIndexSearcher;
 import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.FieldCache.CacheEntry;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.QueryUtils.FCInvisibleMultiReader;
 import org.apache.lucene.store.BaseDirectoryWrapper;
@@ -121,7 +119,6 @@
 import org.apache.lucene.store.MockDirectoryWrapper;
 import org.apache.lucene.store.NRTCachingDirectory;
 import org.apache.lucene.store.RateLimitedDirectoryWrapper;
-import org.apache.lucene.util.FieldCacheSanityChecker.Insanity;
 import org.apache.lucene.util.automaton.AutomatonTestUtil;
 import org.apache.lucene.util.automaton.CompiledAutomaton;
 import org.apache.lucene.util.automaton.RegExp;
@@ -630,7 +627,6 @@
     .around(threadAndTestNameRule)
     .around(new SystemPropertiesInvariantRule(IGNORED_INVARIANT_PROPERTIES))
     .around(new TestRuleSetupAndRestoreInstanceEnv())
-    .around(new TestRuleFieldCacheSanity())
     .around(parentChainCallRule);
 
   private static final Map<String,FieldType> fieldToType = new HashMap<String,FieldType>();
@@ -742,48 +738,6 @@
   }
 
   /**
-   * Asserts that FieldCacheSanityChecker does not detect any
-   * problems with FieldCache.DEFAULT.
-   * <p>
-   * If any problems are found, they are logged to System.err
-   * (allong with the msg) when the Assertion is thrown.
-   * </p>
-   * <p>
-   * This method is called by tearDown after every test method,
-   * however IndexReaders scoped inside test methods may be garbage
-   * collected prior to this method being called, causing errors to
-   * be overlooked. Tests are encouraged to keep their IndexReaders
-   * scoped at the class level, or to explicitly call this method
-   * directly in the same scope as the IndexReader.
-   * </p>
-   *
-   * @see org.apache.lucene.util.FieldCacheSanityChecker
-   */
-  protected static void assertSaneFieldCaches(final String msg) {
-    final CacheEntry[] entries = FieldCache.DEFAULT.getCacheEntries();
-    Insanity[] insanity = null;
-    try {
-      try {
-        insanity = FieldCacheSanityChecker.checkSanity(entries);
-      } catch (RuntimeException e) {
-        dumpArray(msg + ": FieldCache", entries, System.err);
-        throw e;
-      }
-
-      assertEquals(msg + ": Insane FieldCache usage(s) found",
-                   0, insanity.length);
-      insanity = null;
-    } finally {
-
-      // report this in the event of any exception/failure
-      // if no failure, then insanity will be null anyway
-      if (null != insanity) {
-        dumpArray(msg + ": Insane FieldCache usage(s)", insanity, System.err);
-      }
-    }
-  }
-
-  /**
    * Returns a number of at least <code>i</code>
    * <p>
    * The actual number returned will be influenced by whether {@link #TEST_NIGHTLY}


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleFieldCacheSanity.java lucene5666/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleFieldCacheSanity.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleFieldCacheSanity.java	2014-05-14 03:47:28.874646625 -0400
+++ lucene5666/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleFieldCacheSanity.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,68 +0,0 @@
-package org.apache.lucene.util;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.util.FieldCacheSanityChecker; // javadocs
-import org.junit.rules.TestRule;
-import org.junit.runner.Description;
-import org.junit.runners.model.Statement;
-
-/**
- * This rule will fail the test if it has insane field caches.
- * <p>
- * calling assertSaneFieldCaches here isn't as useful as having test
- * classes call it directly from the scope where the index readers
- * are used, because they could be gc'ed just before this tearDown
- * method is called.
- * <p>
- * But it's better then nothing.
- * <p>
- * If you are testing functionality that you know for a fact
- * "violates" FieldCache sanity, then you should either explicitly
- * call purgeFieldCache at the end of your test method, or refactor
- * your Test class so that the inconsistent FieldCache usages are
- * isolated in distinct test methods
- * 
- * @see FieldCacheSanityChecker
- */
-public class TestRuleFieldCacheSanity implements TestRule {
-  
-  @Override
-  public Statement apply(final Statement s, final Description d) {
-    return new Statement() {
-      @Override
-      public void evaluate() throws Throwable {
-        s.evaluate();
-
-        Throwable problem = null;
-        try {
-          LuceneTestCase.assertSaneFieldCaches(d.getDisplayName());
-        } catch (Throwable t) {
-          problem = t;
-        }
-
-        FieldCache.DEFAULT.purgeAllCaches();
-
-        if (problem != null) {
-          Rethrow.rethrow(problem);
-        }
-      }
-    };
-  }  
-}


diff -ruN -x .svn -x build lucene-trunk/solr/contrib/analysis-extras/src/java/org/apache/solr/schema/ICUCollationField.java lucene5666/solr/contrib/analysis-extras/src/java/org/apache/solr/schema/ICUCollationField.java
--- lucene-trunk/solr/contrib/analysis-extras/src/java/org/apache/solr/schema/ICUCollationField.java	2014-05-14 03:47:28.974646627 -0400
+++ lucene5666/solr/contrib/analysis-extras/src/java/org/apache/solr/schema/ICUCollationField.java	2014-05-14 03:45:16.722644324 -0400
@@ -34,10 +34,11 @@
 import org.apache.lucene.index.StorableField;
 import org.apache.lucene.search.ConstantScoreQuery;
 import org.apache.lucene.search.DocTermOrdsRangeFilter;
-import org.apache.lucene.search.FieldCacheRangeFilter;
+import org.apache.lucene.search.DocValuesRangeFilter;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.SortField;
 import org.apache.lucene.search.TermRangeQuery;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Version;
 import org.apache.lucene.analysis.util.ResourceLoader;
@@ -224,6 +225,15 @@
   public SortField getSortField(SchemaField field, boolean top) {
     return getStringSort(field, top);
   }
+  
+  @Override
+  public Type getUninversionType(SchemaField sf) {
+    if (sf.multiValued()) {
+      return Type.SORTED_SET_BINARY; 
+    } else {
+      return Type.SORTED;
+    }
+  }
 
   @Override
   public Analyzer getIndexAnalyzer() {
@@ -270,7 +280,7 @@
           return new ConstantScoreQuery(DocTermOrdsRangeFilter.newBytesRefRange(
               field.getName(), low, high, minInclusive, maxInclusive));
         } else {
-          return new ConstantScoreQuery(FieldCacheRangeFilter.newBytesRefRange(
+          return new ConstantScoreQuery(DocValuesRangeFilter.newBytesRefRange(
               field.getName(), low, high, minInclusive, maxInclusive));
         } 
     } else {


diff -ruN -x .svn -x build lucene-trunk/solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/ClusteringComponent.java lucene5666/solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/ClusteringComponent.java
--- lucene-trunk/solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/ClusteringComponent.java	2014-04-15 21:09:59.642843035 -0400
+++ lucene5666/solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/ClusteringComponent.java	2014-05-12 13:29:02.152245056 -0400
@@ -267,7 +267,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/ClusteringComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/ClusteringComponent.java $";
   }
 
   /**


diff -ruN -x .svn -x build lucene-trunk/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java lucene5666/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java
--- lucene-trunk/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java	2014-05-05 18:51:55.454050171 -0400
+++ lucene5666/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java	2014-05-12 13:29:02.552245063 -0400
@@ -313,7 +313,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java $";
   }
 
   public static final String ENABLE_DEBUG = "enableDebug";


diff -ruN -x .svn -x build lucene-trunk/solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java lucene5666/solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java
--- lucene-trunk/solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java	2014-04-15 21:09:59.658843035 -0400
+++ lucene5666/solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java	2014-05-12 13:29:02.304245058 -0400
@@ -125,7 +125,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/analytics/statistics/StatsCollectorSupplierFactory.java lucene5666/solr/core/src/java/org/apache/solr/analytics/statistics/StatsCollectorSupplierFactory.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/analytics/statistics/StatsCollectorSupplierFactory.java	2014-05-14 03:47:28.946646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/analytics/statistics/StatsCollectorSupplierFactory.java	2014-05-12 13:29:11.060245211 -0400
@@ -33,7 +33,6 @@
 import org.apache.lucene.queries.function.valuesource.FloatFieldSource;
 import org.apache.lucene.queries.function.valuesource.IntFieldSource;
 import org.apache.lucene.queries.function.valuesource.LongFieldSource;
-import org.apache.lucene.search.FieldCache;
 import org.apache.solr.analytics.expression.ExpressionFactory;
 import org.apache.solr.analytics.request.ExpressionRequest;
 import org.apache.solr.analytics.util.AnalyticsParams;
@@ -357,7 +356,7 @@
       if (sourceType!=NUMBER_TYPE&&sourceType!=FIELD_TYPE) {
         return null;
       }
-      return new IntFieldSource(expressionString, FieldCache.NUMERIC_UTILS_INT_PARSER) {
+      return new IntFieldSource(expressionString) {
         public String description() {
           return field;
         }
@@ -366,7 +365,7 @@
       if (sourceType!=NUMBER_TYPE&&sourceType!=FIELD_TYPE) {
         return null;
       }
-      return new LongFieldSource(expressionString, FieldCache.NUMERIC_UTILS_LONG_PARSER) {
+      return new LongFieldSource(expressionString) {
         public String description() {
           return field;
         }
@@ -375,7 +374,7 @@
       if (sourceType!=NUMBER_TYPE&&sourceType!=FIELD_TYPE) {
         return null;
       }
-      return new FloatFieldSource(expressionString, FieldCache.NUMERIC_UTILS_FLOAT_PARSER) {
+      return new FloatFieldSource(expressionString) {
         public String description() {
           return field;
         }
@@ -384,7 +383,7 @@
       if (sourceType!=NUMBER_TYPE&&sourceType!=FIELD_TYPE) {
         return null;
       }
-      return new DoubleFieldSource(expressionString, FieldCache.NUMERIC_UTILS_DOUBLE_PARSER) {
+      return new DoubleFieldSource(expressionString) {
         public String description() {
           return field;
         }
@@ -393,7 +392,7 @@
       if (sourceType!=DATE_TYPE&&sourceType!=FIELD_TYPE) {
         return null;
       }
-      return new DateFieldSource(expressionString, AnalyticsParsers.DEFAULT_DATE_PARSER) {
+      return new DateFieldSource(expressionString) {
         public String description() {
           return field;
         }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/analytics/util/AnalyticsParsers.java lucene5666/solr/core/src/java/org/apache/solr/analytics/util/AnalyticsParsers.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/analytics/util/AnalyticsParsers.java	2014-05-14 03:47:28.946646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/analytics/util/AnalyticsParsers.java	2014-05-12 13:29:11.032245210 -0400
@@ -18,14 +18,9 @@
 package org.apache.solr.analytics.util;
 
 import java.io.IOException;
-import java.text.ParseException;
 import java.util.Arrays;
 import java.util.Date;
 
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.search.FieldCache.LongParser;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.NumericUtils;
 import org.apache.solr.schema.FieldType;
@@ -61,31 +56,7 @@
       return AnalyticsParsers.STRING_PARSER;
     }
   }
-  
-  /** Long Parser that takes in String representations of dates and
-   *  converts them into longs
-   */
-  public final static LongParser DEFAULT_DATE_PARSER = new LongParser() {
-    @SuppressWarnings("deprecation")
-    @Override
-    public long parseLong(BytesRef term) {
-      try {
-        return TrieDateField.parseDate(term.utf8ToString()).getTime();
-      } catch (ParseException e) {
-        System.err.println("Cannot parse date "+term.utf8ToString());
-        return 0;
-      }
-    }
-    @Override
-    public String toString() { 
-      return FieldCache.class.getName()+".DEFAULT_DATE_PARSER"; 
-    }
-    @Override
-    public TermsEnum termsEnum(Terms terms) throws IOException {
-      return terms.iterator(null);
-    }
-  };
-  
+
   /**
    * For use in classes that grab values by docValue.
    * Converts a BytesRef object into the correct readable text.


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/analytics/util/valuesource/DateFieldSource.java lucene5666/solr/core/src/java/org/apache/solr/analytics/util/valuesource/DateFieldSource.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/analytics/util/valuesource/DateFieldSource.java	2014-05-14 03:47:28.950646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/analytics/util/valuesource/DateFieldSource.java	2014-05-12 13:29:11.180245213 -0400
@@ -18,17 +18,18 @@
 package org.apache.solr.analytics.util.valuesource;
 
 import java.io.IOException;
-import java.text.ParseException;
 import java.util.Date;
 import java.util.Map;
 
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.LongDocValues;
 import org.apache.lucene.queries.function.valuesource.LongFieldSource;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.NumericUtils;
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueDate;
 import org.apache.solr.schema.TrieDateField;
@@ -39,16 +40,12 @@
  */
 public class DateFieldSource extends LongFieldSource {
 
-  public DateFieldSource(String field) throws ParseException {
-    super(field, null);
-  }
-
-  public DateFieldSource(String field, FieldCache.LongParser parser) {
-    super(field, parser);
+  public DateFieldSource(String field) {
+    super(field);
   }
 
   public long externalToLong(String extVal) {
-    return parser.parseLong(new BytesRef(extVal));
+    return NumericUtils.prefixCodedToLong(new BytesRef(extVal));
   }
 
   public Object longToObject(long val) {
@@ -62,8 +59,8 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final FieldCache.Longs arr = cache.getLongs(readerContext.reader(), field, parser, true);
-    final Bits valid = cache.getDocsWithField(readerContext.reader(), field);
+    final NumericDocValues arr = DocValues.getNumeric(readerContext.reader(), field);
+    final Bits valid = DocValues.getDocsWithField(readerContext.reader(), field);
     return new LongDocValues(this) {
       @Override
       public long longVal(int doc) {
@@ -110,16 +107,12 @@
   public boolean equals(Object o) {
     if (o.getClass() != this.getClass()) return false;
     DateFieldSource other = (DateFieldSource) o;
-    if (parser==null) {
-      return field.equals(other.field);
-    } else {
-      return field.equals(other.field) && parser.equals(other.parser);
-    }
+    return field.equals(other.field);
   }
 
   @Override
   public int hashCode() {
-    int h = parser == null ? this.getClass().hashCode() : parser.getClass().hashCode();
+    int h = this.getClass().hashCode();
     h += super.hashCode();
     return h;
   }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/core/RequestHandlers.java lucene5666/solr/core/src/java/org/apache/solr/core/RequestHandlers.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/core/RequestHandlers.java	2014-04-15 21:10:01.494843030 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/core/RequestHandlers.java	2014-05-12 13:29:11.288245215 -0400
@@ -291,7 +291,7 @@
 
     @Override
     public String getSource() {
-      String rev = "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/core/RequestHandlers.java $";
+      String rev = "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/core/RequestHandlers.java $";
       if( _handler != null ) {
         rev += "\n" + _handler.getSource();
       }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/core/SolrCore.java lucene5666/solr/core/src/java/org/apache/solr/core/SolrCore.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/core/SolrCore.java	2014-05-14 03:47:28.950646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/core/SolrCore.java	2014-05-14 03:45:16.846644326 -0400
@@ -1461,7 +1461,7 @@
       if (newestSearcher != null && (nrt || indexDirFile.equals(newIndexDirFile))) {
 
         DirectoryReader newReader;
-        DirectoryReader currentReader = newestSearcher.get().getIndexReader();
+        DirectoryReader currentReader = newestSearcher.get().getRawReader();
 
         // SolrCore.verbose("start reopen from",previousSearcher,"writer=",writer);
         
@@ -2388,7 +2388,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/core/SolrCore.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/core/SolrCore.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/AdminHandlers.java lucene5666/solr/core/src/java/org/apache/solr/handler/admin/AdminHandlers.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/AdminHandlers.java	2014-03-11 11:09:46.932273735 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/admin/AdminHandlers.java	2014-05-12 13:29:10.196245196 -0400
@@ -123,7 +123,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/AdminHandlers.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/admin/AdminHandlers.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java	2014-05-08 16:23:46.014409192 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java	2014-05-12 13:29:10.200245196 -0400
@@ -1275,7 +1275,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java $";
   }
 
   /**


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/LoggingHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/admin/LoggingHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/LoggingHandler.java	2014-04-15 21:10:00.942843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/admin/LoggingHandler.java	2014-05-12 13:29:10.196245196 -0400
@@ -158,6 +158,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/LoggingHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/admin/LoggingHandler.java $";
   }
 }
\ No newline at end of file


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java	2014-05-03 10:00:10.550485348 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java	2014-05-12 13:29:10.200245196 -0400
@@ -659,7 +659,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java	2014-04-15 21:10:00.942843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java	2014-05-12 13:29:10.200245196 -0400
@@ -92,6 +92,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java	2014-04-15 21:10:00.942843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java	2014-05-12 13:29:10.196245196 -0400
@@ -56,6 +56,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java	2014-04-15 21:10:00.942843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java	2014-05-12 13:29:10.200245196 -0400
@@ -383,6 +383,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java	2014-04-15 21:10:00.942843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java	2014-05-12 13:29:10.204245196 -0400
@@ -299,6 +299,6 @@
 
   @Override
   public String getSource() {    
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java	2014-04-15 21:10:00.942843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java	2014-05-12 13:29:10.200245196 -0400
@@ -346,7 +346,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java $";
   }
   
   private static final long ONE_KB = 1024;


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java	2014-04-15 21:10:00.942843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java	2014-05-12 13:29:10.200245196 -0400
@@ -132,6 +132,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java	2014-03-11 11:09:47.036273733 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java	2014-05-12 13:29:10.112245194 -0400
@@ -44,6 +44,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java lucene5666/solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java	2014-05-05 18:51:55.454050171 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java	2014-05-12 13:29:10.096245194 -0400
@@ -361,7 +361,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/ExpandComponent.java lucene5666/solr/core/src/java/org/apache/solr/handler/component/ExpandComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/ExpandComponent.java	2014-05-14 03:47:28.942646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/component/ExpandComponent.java	2014-05-12 13:29:10.096245194 -0400
@@ -19,6 +19,7 @@
 
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.LeafCollector;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -27,7 +28,6 @@
 import org.apache.lucene.search.SimpleCollector;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.TopDocsCollector;
@@ -188,7 +188,7 @@
 
     SolrIndexSearcher searcher = req.getSearcher();
     AtomicReader reader = searcher.getAtomicReader();
-    SortedDocValues values = FieldCache.DEFAULT.getTermsIndex(reader, field);
+    SortedDocValues values = DocValues.getSorted(reader, field);
     FixedBitSet groupBits = new FixedBitSet(values.getValueCount());
     DocList docList = rb.getResults().docList;
     IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java lucene5666/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java	2014-04-15 21:10:00.958843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java	2014-05-12 13:29:10.096245194 -0400
@@ -618,7 +618,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/FieldFacetStats.java lucene5666/solr/core/src/java/org/apache/solr/handler/component/FieldFacetStats.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/FieldFacetStats.java	2014-05-14 03:47:28.942646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/component/FieldFacetStats.java	2014-05-12 13:29:10.100245194 -0400
@@ -25,10 +25,10 @@
 
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.BytesRef;
 import org.apache.solr.schema.SchemaField;
 import org.apache.solr.search.SolrIndexSearcher;
@@ -100,7 +100,7 @@
   // Currently only used by UnInvertedField stats
   public boolean facetTermNum(int docID, int statsTermNum) throws IOException {
     if (topLevelSortedValues == null) {
-      topLevelSortedValues = FieldCache.DEFAULT.getTermsIndex(topLevelReader, name);
+      topLevelSortedValues = DocValues.getSorted(topLevelReader, name);
     }
     
     int term = topLevelSortedValues.getOrd(docID);


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java lucene5666/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java	2014-04-15 21:10:00.958843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java	2014-05-12 13:29:10.096245194 -0400
@@ -212,7 +212,7 @@
   
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java $";
   }
   
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java lucene5666/solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java	2014-04-15 21:10:00.962843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java	2014-05-12 13:29:10.104245194 -0400
@@ -405,7 +405,7 @@
   
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java $";
   }
   
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java lucene5666/solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java	2014-04-15 21:10:00.958843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java	2014-05-12 13:29:10.100245194 -0400
@@ -269,6 +269,6 @@
 //  }
 //
 //  public String getSource() {
-//    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java $";
+//    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java $";
 //  }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java lucene5666/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java	2014-04-20 07:24:30.489953380 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java	2014-05-12 13:29:10.100245194 -0400
@@ -1175,7 +1175,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java lucene5666/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java	2014-04-15 21:10:00.958843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java	2014-05-12 13:29:10.096245194 -0400
@@ -535,7 +535,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java lucene5666/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java	2014-04-15 21:10:00.962843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java	2014-05-12 13:29:10.104245194 -0400
@@ -494,7 +494,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java	2014-04-15 21:10:00.958843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java	2014-05-12 13:29:10.100245194 -0400
@@ -386,7 +386,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java lucene5666/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java	2014-05-05 18:51:55.454050171 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java	2014-05-12 13:29:10.096245194 -0400
@@ -761,7 +761,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java $";
   }
 
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java lucene5666/solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java	2014-05-14 03:47:28.942646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java	2014-05-14 03:45:16.846644326 -0400
@@ -37,6 +37,7 @@
 import org.apache.solr.common.util.NamedList;
 import org.apache.solr.common.util.SimpleOrderedMap;
 import org.apache.solr.common.util.StrUtils;
+import org.apache.solr.request.DocValuesStats;
 import org.apache.solr.request.SolrQueryRequest;
 import org.apache.solr.request.UnInvertedField;
 import org.apache.solr.schema.FieldType;
@@ -172,7 +173,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java $";
   }
 
 }
@@ -315,9 +316,8 @@
         NamedList<?> stv;
 
         if (sf.multiValued() || ft.multiValuedFieldCache()) {
-          //use UnInvertedField for multivalued fields
-          UnInvertedField uif = UnInvertedField.getUnInvertedField(statsField, searcher);
-          stv = uif.getStats(searcher, docs, calcDistinct, facets).getStatsValues();
+          // TODO: should this also be used for single-valued string fields? (should work fine)
+          stv = DocValuesStats.getCounts(searcher, sf.getName(), docs, calcDistinct, facets).getStatsValues();
         } else {
           stv = getFieldCacheStats(statsField, calcDistinct, facets);
         }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/SuggestComponent.java lucene5666/solr/core/src/java/org/apache/solr/handler/component/SuggestComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/SuggestComponent.java	2014-04-15 21:10:00.958843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/component/SuggestComponent.java	2014-05-12 13:29:10.100245194 -0400
@@ -320,7 +320,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/SuggestComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/component/SuggestComponent.java $";
   }
   
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java lucene5666/solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java	2014-04-15 21:10:00.958843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java	2014-05-12 13:29:10.100245194 -0400
@@ -476,7 +476,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java lucene5666/solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java	2014-04-15 21:10:00.962843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java	2014-05-12 13:29:10.100245194 -0400
@@ -470,7 +470,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/CSVRequestHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/CSVRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/CSVRequestHandler.java	2014-03-11 11:09:46.900273736 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/CSVRequestHandler.java	2014-05-12 13:29:10.108245194 -0400
@@ -40,7 +40,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/CSVRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/CSVRequestHandler.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java	2014-05-05 18:51:55.454050171 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java	2014-05-12 13:29:10.108245194 -0400
@@ -124,7 +124,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java $";
   }
 
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java	2014-04-15 21:10:00.958843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java	2014-05-12 13:29:09.980245192 -0400
@@ -69,6 +69,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java	2014-05-03 10:00:10.550485348 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java	2014-05-12 13:29:10.196245196 -0400
@@ -108,7 +108,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java $";
   }
 
   // ================================================= Helper methods ================================================


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/JsonUpdateRequestHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/JsonUpdateRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/JsonUpdateRequestHandler.java	2014-03-11 11:09:47.036273733 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/JsonUpdateRequestHandler.java	2014-05-12 13:29:10.100245194 -0400
@@ -41,7 +41,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/JsonUpdateRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/JsonUpdateRequestHandler.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java	2014-05-03 10:00:10.550485348 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java	2014-05-12 13:29:10.104245194 -0400
@@ -458,7 +458,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java	2014-04-15 21:10:00.942843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java	2014-05-12 13:29:10.200245196 -0400
@@ -296,6 +296,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/RealTimeGetHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/RealTimeGetHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/RealTimeGetHandler.java	2014-04-15 21:10:00.942843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/RealTimeGetHandler.java	2014-05-12 13:29:10.196245196 -0400
@@ -42,7 +42,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/RealTimeGetHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/RealTimeGetHandler.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java	2014-05-07 10:35:18.800540496 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java	2014-05-12 13:29:10.200245196 -0400
@@ -551,7 +551,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java $";
   }
 
   /** 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/StandardRequestHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/StandardRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/StandardRequestHandler.java	2014-03-11 11:09:46.928273735 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/StandardRequestHandler.java	2014-05-12 13:29:10.200245196 -0400
@@ -52,7 +52,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/StandardRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/StandardRequestHandler.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/UpdateRequestHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/UpdateRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/UpdateRequestHandler.java	2014-04-15 21:10:01.334843031 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/UpdateRequestHandler.java	2014-05-12 13:29:10.108245194 -0400
@@ -159,7 +159,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/UpdateRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/UpdateRequestHandler.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java	2014-03-11 11:09:46.928273735 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java	2014-05-12 13:29:09.980245192 -0400
@@ -43,7 +43,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/XsltUpdateRequestHandler.java lucene5666/solr/core/src/java/org/apache/solr/handler/XsltUpdateRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/XsltUpdateRequestHandler.java	2014-03-11 11:09:46.900273736 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/handler/XsltUpdateRequestHandler.java	2014-05-12 13:29:10.200245196 -0400
@@ -43,6 +43,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/XsltUpdateRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/handler/XsltUpdateRequestHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/BreakIteratorBoundaryScanner.java lucene5666/solr/core/src/java/org/apache/solr/highlight/BreakIteratorBoundaryScanner.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/BreakIteratorBoundaryScanner.java	2014-03-11 11:09:46.548273743 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/highlight/BreakIteratorBoundaryScanner.java	2014-05-12 13:29:09.732245188 -0400
@@ -77,6 +77,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/BreakIteratorBoundaryScanner.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/highlight/BreakIteratorBoundaryScanner.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/DefaultEncoder.java lucene5666/solr/core/src/java/org/apache/solr/highlight/DefaultEncoder.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/DefaultEncoder.java	2014-03-11 11:09:46.616273742 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/highlight/DefaultEncoder.java	2014-05-12 13:29:09.736245188 -0400
@@ -43,6 +43,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/DefaultEncoder.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/highlight/DefaultEncoder.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/GapFragmenter.java lucene5666/solr/core/src/java/org/apache/solr/highlight/GapFragmenter.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/GapFragmenter.java	2014-03-11 11:09:46.616273742 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/highlight/GapFragmenter.java	2014-05-12 13:29:09.732245188 -0400
@@ -49,7 +49,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/GapFragmenter.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/highlight/GapFragmenter.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/HtmlEncoder.java lucene5666/solr/core/src/java/org/apache/solr/highlight/HtmlEncoder.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/HtmlEncoder.java	2014-03-11 11:09:46.632273741 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/highlight/HtmlEncoder.java	2014-05-12 13:29:09.732245188 -0400
@@ -43,6 +43,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/HtmlEncoder.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/highlight/HtmlEncoder.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/HtmlFormatter.java lucene5666/solr/core/src/java/org/apache/solr/highlight/HtmlFormatter.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/HtmlFormatter.java	2014-03-11 11:09:46.616273742 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/highlight/HtmlFormatter.java	2014-05-12 13:29:09.732245188 -0400
@@ -48,6 +48,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/HtmlFormatter.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/highlight/HtmlFormatter.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/RegexFragmenter.java lucene5666/solr/core/src/java/org/apache/solr/highlight/RegexFragmenter.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/RegexFragmenter.java	2014-04-15 21:10:00.914843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/highlight/RegexFragmenter.java	2014-05-12 13:29:09.732245188 -0400
@@ -96,7 +96,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/RegexFragmenter.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/highlight/RegexFragmenter.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/ScoreOrderFragmentsBuilder.java lucene5666/solr/core/src/java/org/apache/solr/highlight/ScoreOrderFragmentsBuilder.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/ScoreOrderFragmentsBuilder.java	2014-03-11 11:09:46.616273742 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/highlight/ScoreOrderFragmentsBuilder.java	2014-05-12 13:29:09.736245188 -0400
@@ -43,6 +43,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/ScoreOrderFragmentsBuilder.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/highlight/ScoreOrderFragmentsBuilder.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/SimpleBoundaryScanner.java lucene5666/solr/core/src/java/org/apache/solr/highlight/SimpleBoundaryScanner.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/SimpleBoundaryScanner.java	2014-03-11 11:09:46.544273743 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/highlight/SimpleBoundaryScanner.java	2014-05-12 13:29:09.736245188 -0400
@@ -46,6 +46,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/SimpleBoundaryScanner.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/highlight/SimpleBoundaryScanner.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/SimpleFragListBuilder.java lucene5666/solr/core/src/java/org/apache/solr/highlight/SimpleFragListBuilder.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/SimpleFragListBuilder.java	2014-03-11 11:09:46.620273742 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/highlight/SimpleFragListBuilder.java	2014-05-12 13:29:09.732245188 -0400
@@ -45,6 +45,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/SimpleFragListBuilder.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/highlight/SimpleFragListBuilder.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/SimpleFragmentsBuilder.java lucene5666/solr/core/src/java/org/apache/solr/highlight/SimpleFragmentsBuilder.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/SimpleFragmentsBuilder.java	2014-03-11 11:09:46.548273743 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/highlight/SimpleFragmentsBuilder.java	2014-05-12 13:29:09.732245188 -0400
@@ -43,6 +43,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/SimpleFragmentsBuilder.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/highlight/SimpleFragmentsBuilder.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/SingleFragListBuilder.java lucene5666/solr/core/src/java/org/apache/solr/highlight/SingleFragListBuilder.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/SingleFragListBuilder.java	2014-03-11 11:09:46.612273742 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/highlight/SingleFragListBuilder.java	2014-05-12 13:29:09.732245188 -0400
@@ -45,6 +45,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/SingleFragListBuilder.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/highlight/SingleFragListBuilder.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/request/DocValuesFacets.java lucene5666/solr/core/src/java/org/apache/solr/request/DocValuesFacets.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/request/DocValuesFacets.java	2014-05-14 03:47:28.954646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/request/DocValuesFacets.java	2014-05-14 03:45:16.846644326 -0400
@@ -60,10 +60,13 @@
     SchemaField schemaField = searcher.getSchema().getField(fieldName);
     FieldType ft = schemaField.getType();
     NamedList<Integer> res = new NamedList<>();
+    
+    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?
+    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();
 
     final SortedSetDocValues si; // for term lookups only
     OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones
-    if (schemaField.multiValued()) {
+    if (multiValued) {
       si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);
       if (si instanceof MultiSortedSetDocValues) {
         ordinalMap = ((MultiSortedSetDocValues)si).mapping;
@@ -126,7 +129,7 @@
           disi = dis.iterator();
         }
         if (disi != null) {
-          if (schemaField.multiValued()) {
+          if (multiValued) {
             SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);
             if (sub == null) {
               sub = DocValues.EMPTY_SORTED_SET;


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/request/DocValuesStats.java lucene5666/solr/core/src/java/org/apache/solr/request/DocValuesStats.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/request/DocValuesStats.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/solr/core/src/java/org/apache/solr/request/DocValuesStats.java	2014-05-14 03:45:17.142644331 -0400
@@ -0,0 +1,198 @@
+package org.apache.solr.request;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.MultiDocValues.MultiSortedDocValues;
+import org.apache.lucene.index.MultiDocValues.MultiSortedSetDocValues;
+import org.apache.lucene.index.MultiDocValues.OrdinalMap;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TermRangeQuery;
+import org.apache.lucene.util.BytesRef;
+import org.apache.solr.handler.component.FieldFacetStats;
+import org.apache.solr.handler.component.StatsValues;
+import org.apache.solr.handler.component.StatsValuesFactory;
+import org.apache.solr.schema.FieldType;
+import org.apache.solr.schema.SchemaField;
+import org.apache.solr.search.DocSet;
+import org.apache.solr.search.SolrIndexSearcher;
+
+/**
+ * Computes term stats for docvalues field (single or multivalued).
+ * <p>
+ * Instead of working on a top-level reader view (binary-search per docid),
+ * it collects per-segment, but maps ordinals to global ordinal space using
+ * MultiDocValues' OrdinalMap.
+ */
+public class DocValuesStats {
+  private DocValuesStats() {}
+  
+  public static StatsValues getCounts(SolrIndexSearcher searcher, String fieldName, DocSet docs, boolean calcDistinct, String[] facet) throws IOException {
+    SchemaField schemaField = searcher.getSchema().getField(fieldName);
+    FieldType ft = schemaField.getType();
+    StatsValues res = StatsValuesFactory.createStatsValues(schemaField, calcDistinct);
+    
+    //Initialize facetstats, if facets have been passed in
+    final FieldFacetStats[] facetStats = new FieldFacetStats[facet.length];
+    int upto = 0;
+    for (String facetField : facet) {
+      SchemaField facetSchemaField = searcher.getSchema().getField(facetField);
+      facetStats[upto++] = new FieldFacetStats(searcher, facetField, schemaField, facetSchemaField, calcDistinct);
+    }
+    
+    // TODO: remove multiValuedFieldCache(), check dv type / uninversion type?
+    final boolean multiValued = schemaField.multiValued() || ft.multiValuedFieldCache();
+
+    SortedSetDocValues si; // for term lookups only
+    OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones
+    if (multiValued) {
+      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);
+      if (si instanceof MultiSortedSetDocValues) {
+        ordinalMap = ((MultiSortedSetDocValues)si).mapping;
+      }
+    } else {
+      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);
+      si = single == null ? null : DocValues.singleton(single);
+      if (single instanceof MultiSortedDocValues) {
+        ordinalMap = ((MultiSortedDocValues)single).mapping;
+      }
+    }
+    if (si == null) {
+      si = DocValues.EMPTY_SORTED_SET;
+    }
+    if (si.getValueCount() >= Integer.MAX_VALUE) {
+      throw new UnsupportedOperationException("Currently this stats method is limited to " + Integer.MAX_VALUE + " unique terms");
+    }
+
+    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(fieldName, null, null, false, false)));
+
+    final int nTerms = (int) si.getValueCount();   
+    
+    // count collection array only needs to be as big as the number of terms we are
+    // going to collect counts for.
+    final int[] counts = new int[nTerms];
+    
+    Filter filter = docs.getTopFilter();
+    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();
+    for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {
+      AtomicReaderContext leaf = leaves.get(subIndex);
+      DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs
+      DocIdSetIterator disi = null;
+      if (dis != null) {
+        disi = dis.iterator();
+      }
+      if (disi != null) {
+        int docBase = leaf.docBase;
+        if (multiValued) {
+          SortedSetDocValues sub = leaf.reader().getSortedSetDocValues(fieldName);
+          if (sub == null) {
+            sub = DocValues.EMPTY_SORTED_SET;
+          }
+          final SortedDocValues singleton = DocValues.unwrapSingleton(sub);
+          if (singleton != null) {
+            // some codecs may optimize SORTED_SET storage for single-valued fields
+            accumSingle(counts, docBase, facetStats, singleton, disi, subIndex, ordinalMap);
+          } else {
+            accumMulti(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);
+          }
+        } else {
+          SortedDocValues sub = leaf.reader().getSortedDocValues(fieldName);
+          if (sub == null) {
+            sub = DocValues.EMPTY_SORTED;
+          }
+          accumSingle(counts, docBase, facetStats, sub, disi, subIndex, ordinalMap);
+        }
+      }
+    }
+    
+    // add results in index order
+    BytesRef value = new BytesRef();
+    for (int ord = 0; ord < counts.length; ord++) {
+      int count = counts[ord];
+      if (count > 0) {
+        si.lookupOrd(ord, value);
+        res.accumulate(value, count);
+        for (FieldFacetStats f : facetStats) {
+          f.accumulateTermNum(ord, value);
+        }
+      }
+    }
+
+    res.addMissing(missing.size());
+    if (facetStats.length > 0) {
+      for (FieldFacetStats f : facetStats) {
+        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;
+        FieldType facetType = searcher.getSchema().getFieldType(f.name);
+        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {
+          String termLabel = entry.getKey();
+          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);
+          entry.getValue().addMissing(missingCount);
+        }
+        res.addFacet(f.name, facetStatsValues);
+      }
+    }
+    return res;
+  }
+
+  /** accumulates per-segment single-valued stats */
+  static void accumSingle(int counts[], int docBase, FieldFacetStats[] facetStats, SortedDocValues si, DocIdSetIterator disi, int subIndex, OrdinalMap map) throws IOException {
+    int doc;
+    while ((doc = disi.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+      int term = si.getOrd(doc);
+      if (term >= 0) {
+        if (map != null) {
+          term = (int) map.getGlobalOrd(subIndex, term);
+        }
+        counts[term]++;
+        for (FieldFacetStats f : facetStats) {
+          f.facetTermNum(docBase + doc, term);
+        }
+      }
+    }
+  }
+  
+  /** accumulates per-segment multi-valued stats */
+  static void accumMulti(int counts[], int docBase, FieldFacetStats[] facetStats, SortedSetDocValues si, DocIdSetIterator disi, int subIndex, OrdinalMap map) throws IOException {
+    int doc;
+    while ((doc = disi.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+      si.setDocument(doc);
+      long ord;
+      while ((ord = si.nextOrd()) != SortedSetDocValues.NO_MORE_ORDS) {
+        int term = (int) ord;
+        if (map != null) {
+          term = (int) map.getGlobalOrd(subIndex, term);
+        }
+        counts[term]++;
+        for (FieldFacetStats f : facetStats) {
+          f.facetTermNum(docBase + doc, term);
+        }
+      }
+    }
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/request/NumericFacets.java lucene5666/solr/core/src/java/org/apache/solr/request/NumericFacets.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/request/NumericFacets.java	2014-05-14 03:47:28.954646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/request/NumericFacets.java	2014-05-12 13:29:11.180245213 -0400
@@ -30,16 +30,16 @@
 
 import org.apache.lucene.document.FieldType.NumericType;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
-import org.apache.lucene.util.NumericUtils;
 import org.apache.lucene.util.PriorityQueue;
 import org.apache.lucene.util.StringHelper;
 import org.apache.solr.common.params.FacetParams;
@@ -144,7 +144,7 @@
     final HashTable hashTable = new HashTable();
     final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();
     AtomicReaderContext ctx = null;
-    FieldCache.Longs longs = null;
+    NumericDocValues longs = null;
     Bits docsWithField = null;
     int missingCount = 0;
     for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {
@@ -156,39 +156,39 @@
         assert doc >= ctx.docBase;
         switch (numericType) {
           case LONG:
-            longs = FieldCache.DEFAULT.getLongs(ctx.reader(), fieldName, true);
+            longs = DocValues.getNumeric(ctx.reader(), fieldName);
             break;
           case INT:
-            final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(ctx.reader(), fieldName, true);
-            longs = new FieldCache.Longs() {
-              @Override
-              public long get(int docID) {
-                return ints.get(docID);
-              }
-            };
+            longs = DocValues.getNumeric(ctx.reader(), fieldName);
             break;
           case FLOAT:
-            final FieldCache.Floats floats = FieldCache.DEFAULT.getFloats(ctx.reader(), fieldName, true);
-            longs = new FieldCache.Longs() {
+            final NumericDocValues floats = DocValues.getNumeric(ctx.reader(), fieldName);
+            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator
+            longs = new NumericDocValues() {
               @Override
               public long get(int docID) {
-                return NumericUtils.floatToSortableInt(floats.get(docID));
+                long bits = floats.get(docID);
+                if (bits<0) bits ^= 0x7fffffffffffffffL;
+                return bits;
               }
             };
             break;
           case DOUBLE:
-            final FieldCache.Doubles doubles = FieldCache.DEFAULT.getDoubles(ctx.reader(), fieldName, true);
-            longs = new FieldCache.Longs() {
+            final NumericDocValues doubles = DocValues.getNumeric(ctx.reader(), fieldName);
+            // TODO: this bit flipping should probably be moved to tie-break in the PQ comparator
+            longs = new NumericDocValues() {
               @Override
               public long get(int docID) {
-                return NumericUtils.doubleToSortableLong(doubles.get(docID));
+                long bits = doubles.get(docID);
+                if (bits<0) bits ^= 0x7fffffffffffffffL;
+                return bits;
               }
             };
             break;
           default:
             throw new AssertionError();
         }
-        docsWithField = FieldCache.DEFAULT.getDocsWithField(ctx.reader(), fieldName);
+        docsWithField = DocValues.getDocsWithField(ctx.reader(), fieldName);
       }
       long v = longs.get(doc - ctx.docBase);
       if (v != 0 || docsWithField.get(doc - ctx.docBase)) {


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java lucene5666/solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java	2014-05-14 03:47:28.954646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java	2014-05-12 13:29:11.180245213 -0400
@@ -22,11 +22,11 @@
 import java.util.concurrent.*;
 
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
@@ -236,7 +236,7 @@
     BytesRef tempBR = new BytesRef();
 
     void countTerms() throws IOException {
-      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);
+      si = DocValues.getSorted(context.reader(), fieldName);
       // SolrCore.log.info("reader= " + reader + "  FC=" + System.identityHashCode(si));
 
       if (prefix!=null) {


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/request/SimpleFacets.java lucene5666/solr/core/src/java/org/apache/solr/request/SimpleFacets.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/request/SimpleFacets.java	2014-05-14 03:47:28.954646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/request/SimpleFacets.java	2014-05-14 03:45:17.158644332 -0400
@@ -38,6 +38,8 @@
 import java.util.concurrent.TimeUnit;
 
 import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.MultiDocsEnum;
@@ -46,8 +48,9 @@
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.FilterCollector;
+import org.apache.lucene.search.LeafCollector;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.TermQuery;
@@ -83,6 +86,7 @@
 import org.apache.solr.search.DocSet;
 import org.apache.solr.search.Grouping;
 import org.apache.solr.search.HashDocSet;
+import org.apache.solr.search.Insanity;
 import org.apache.solr.search.QParser;
 import org.apache.solr.search.QueryParsing;
 import org.apache.solr.search.SolrIndexSearcher;
@@ -378,18 +382,13 @@
 
     final boolean multiToken = sf.multiValued() || ft.multiValuedFieldCache();
     
-    if (method == null && ft.getNumericType() != null && !sf.multiValued()) {
+    if (ft.getNumericType() != null && !sf.multiValued()) {
       // the per-segment approach is optimal for numeric field types since there
       // are no global ords to merge and no need to create an expensive
       // top-level reader
       method = FacetMethod.FCS;
     }
 
-    if (ft.getNumericType() != null && sf.hasDocValues()) {
-      // only fcs is able to leverage the numeric field caches
-      method = FacetMethod.FCS;
-    }
-
     if (method == null) {
       // TODO: default to per-segment or not?
       method = FacetMethod.FC;
@@ -430,14 +429,7 @@
           }
           break;
         case FC:
-          if (sf.hasDocValues()) {
-            counts = DocValuesFacets.getCounts(searcher, base, field, offset,limit, mincount, missing, sort, prefix);
-          } else if (multiToken || TrieField.getMainValuePrefix(ft) != null) {
-            UnInvertedField uif = UnInvertedField.getUnInvertedField(field, searcher);
-            counts = uif.getCounts(searcher, base, offset, limit, mincount,missing,sort,prefix);
-          } else {
-            counts = getFieldCacheCounts(searcher, base, field, offset,limit, mincount, missing, sort, prefix);
-          }
+          counts = DocValuesFacets.getCounts(searcher, base, field, offset,limit, mincount, missing, sort, prefix);
           break;
         default:
           throw new AssertionError();
@@ -458,7 +450,7 @@
                                              String sort,
                                              String prefix) throws IOException {
     GroupingSpecification groupingSpecification = rb.getGroupingSpec();
-    String groupField  = groupingSpecification != null ? groupingSpecification.getFields()[0] : null;
+    final String groupField  = groupingSpecification != null ? groupingSpecification.getFields()[0] : null;
     if (groupField == null) {
       throw new SolrException (
           SolrException.ErrorCode.BAD_REQUEST,
@@ -467,8 +459,24 @@
     }
 
     BytesRef prefixBR = prefix != null ? new BytesRef(prefix) : null;
-    TermGroupFacetCollector collector = TermGroupFacetCollector.createTermGroupFacetCollector(groupField, field, multiToken, prefixBR, 128);
-    searcher.search(new MatchAllDocsQuery(), base.getTopFilter(), collector);
+    final TermGroupFacetCollector collector = TermGroupFacetCollector.createTermGroupFacetCollector(groupField, field, multiToken, prefixBR, 128);
+    
+    SchemaField sf = searcher.getSchema().getFieldOrNull(groupField);
+    
+    if (sf != null && sf.hasDocValues() == false && sf.multiValued() == false && sf.getType().getNumericType() != null) {
+      // its a single-valued numeric field: we must currently create insanity :(
+      // there isnt a GroupedFacetCollector that works on numerics right now...
+      searcher.search(new MatchAllDocsQuery(), base.getTopFilter(), new FilterCollector(collector) {
+        @Override
+        public LeafCollector getLeafCollector(AtomicReaderContext context) throws IOException {
+          AtomicReader insane = Insanity.wrapInsanity(context.reader(), groupField);
+          return in.getLeafCollector(insane.getContext());
+        }
+      });
+    } else {
+      searcher.search(new MatchAllDocsQuery(), base.getTopFilter(), collector);
+    }
+    
     boolean orderByCount = sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY);
     TermGroupFacetCollector.GroupedFacetResult result 
       = collector.mergeSegmentResults(limit < 0 ? Integer.MAX_VALUE : 
@@ -622,152 +630,6 @@
     return docs.andNotSize(hasVal);
   }
 
-
-  /**
-   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.
-   * The field must have at most one indexed token per document.
-   */
-  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {
-    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,
-    //  we should use an alternate strategy to avoid
-    //  1) creating another huge int[] for the counts
-    //  2) looping over that huge int[] looking for the rare non-zeros.
-    //
-    // Yet another variation: if docs.size() is small and termvectors are stored,
-    // then use them instead of the FieldCache.
-    //
-
-    // TODO: this function is too big and could use some refactoring, but
-    // we also need a facet cache, and refactoring of SimpleFacets instead of
-    // trying to pass all the various params around.
-
-    FieldType ft = searcher.getSchema().getFieldType(fieldName);
-    NamedList<Integer> res = new NamedList<>();
-
-    SortedDocValues si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);
-
-    final BytesRef br = new BytesRef();
-
-    final BytesRef prefixRef;
-    if (prefix == null) {
-      prefixRef = null;
-    } else if (prefix.length()==0) {
-      prefix = null;
-      prefixRef = null;
-    } else {
-      prefixRef = new BytesRef(prefix);
-    }
-
-    int startTermIndex, endTermIndex;
-    if (prefix!=null) {
-      startTermIndex = si.lookupTerm(prefixRef);
-      if (startTermIndex<0) startTermIndex=-startTermIndex-1;
-      prefixRef.append(UnicodeUtil.BIG_TERM);
-      endTermIndex = si.lookupTerm(prefixRef);
-      assert endTermIndex < 0;
-      endTermIndex = -endTermIndex-1;
-    } else {
-      startTermIndex=-1;
-      endTermIndex=si.getValueCount();
-    }
-
-    final int nTerms=endTermIndex-startTermIndex;
-    int missingCount = -1; 
-    final CharsRef charsRef = new CharsRef(10);
-    if (nTerms>0 && docs.size() >= mincount) {
-
-      // count collection array only needs to be as big as the number of terms we are
-      // going to collect counts for.
-      final int[] counts = new int[nTerms];
-
-      DocIterator iter = docs.iterator();
-
-      while (iter.hasNext()) {
-        int term = si.getOrd(iter.nextDoc());
-        int arrIdx = term-startTermIndex;
-        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;
-      }
-
-      if (startTermIndex == -1) {
-        missingCount = counts[0];
-      }
-
-      // IDEA: we could also maintain a count of "other"... everything that fell outside
-      // of the top 'N'
-
-      int off=offset;
-      int lim=limit>=0 ? limit : Integer.MAX_VALUE;
-
-      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {
-        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;
-        maxsize = Math.min(maxsize, nTerms);
-        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);
-
-        int min=mincount-1;  // the smallest value in the top 'N' values
-        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {
-          int c = counts[i];
-          if (c>min) {
-            // NOTE: we use c>min rather than c>=min as an optimization because we are going in
-            // index order, so we already know that the keys are ordered.  This can be very
-            // important if a lot of the counts are repeated (like zero counts would be).
-
-            // smaller term numbers sort higher, so subtract the term number instead
-            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);
-            boolean displaced = queue.insert(pair);
-            if (displaced) min=(int)(queue.top() >>> 32);
-          }
-        }
-
-        // if we are deep paging, we don't have to order the highest "offset" counts.
-        int collectCount = Math.max(0, queue.size() - off);
-        assert collectCount <= lim;
-
-        // the start and end indexes of our list "sorted" (starting with the highest value)
-        int sortedIdxStart = queue.size() - (collectCount - 1);
-        int sortedIdxEnd = queue.size() + 1;
-        final long[] sorted = queue.sort(collectCount);
-
-        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {
-          long pair = sorted[i];
-          int c = (int)(pair >>> 32);
-          int tnum = Integer.MAX_VALUE - (int)pair;
-          si.lookupOrd(startTermIndex+tnum, br);
-          ft.indexedToReadable(br, charsRef);
-          res.add(charsRef.toString(), c);
-        }
-      
-      } else {
-        // add results in index order
-        int i=(startTermIndex==-1)?1:0;
-        if (mincount<=0) {
-          // if mincount<=0, then we won't discard any terms and we know exactly
-          // where to start.
-          i+=off;
-          off=0;
-        }
-
-        for (; i<nTerms; i++) {          
-          int c = counts[i];
-          if (c<mincount || --off>=0) continue;
-          if (--lim<0) break;
-          si.lookupOrd(startTermIndex+i, br);
-          ft.indexedToReadable(br, charsRef);
-          res.add(charsRef.toString(), c);
-        }
-      }
-    }
-
-    if (missing) {
-      if (missingCount < 0) {
-        missingCount = getFieldMissingCount(searcher,docs,fieldName);
-      }
-      res.add(null, missingCount);
-    }
-    
-    return res;
-  }
-
-
   /**
    * Returns a list of terms in the specified field along with the 
    * corresponding count of documents in the set that match that constraint.


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/request/UnInvertedField.java lucene5666/solr/core/src/java/org/apache/solr/request/UnInvertedField.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/request/UnInvertedField.java	2014-05-14 03:47:28.954646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/request/UnInvertedField.java	2014-05-12 13:29:11.300245215 -0400
@@ -23,12 +23,12 @@
 import java.util.concurrent.atomic.AtomicLong;
 
 import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.DocTermOrds;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.TermRangeQuery;
+import org.apache.lucene.uninverting.DocTermOrds;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.FixedBitSet;


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/schema/AbstractSpatialFieldType.java lucene5666/solr/core/src/java/org/apache/solr/schema/AbstractSpatialFieldType.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/schema/AbstractSpatialFieldType.java	2014-05-14 03:47:28.970646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/schema/AbstractSpatialFieldType.java	2014-05-14 03:45:17.066644330 -0400
@@ -40,6 +40,7 @@
 import org.apache.lucene.spatial.query.SpatialArgs;
 import org.apache.lucene.spatial.query.SpatialArgsParser;
 import org.apache.lucene.spatial.query.SpatialOperation;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.params.SolrParams;
 import org.apache.solr.response.TextResponseWriter;
@@ -129,6 +130,11 @@
   }
 
   @Override
+  public Type getUninversionType(SchemaField sf) {
+    return null;
+  }
+
+  @Override
   public List<StorableField> createFields(SchemaField field, Object val, float boost) {
     String shapeStr = null;
     Shape shape = null;


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/schema/BinaryField.java lucene5666/solr/core/src/java/org/apache/solr/schema/BinaryField.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/schema/BinaryField.java	2014-05-14 03:47:28.970646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/schema/BinaryField.java	2014-05-14 03:45:17.066644330 -0400
@@ -23,6 +23,7 @@
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.StorableField;
 import org.apache.lucene.search.SortField;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
 import org.apache.lucene.util.BytesRef;
 import org.apache.solr.common.util.Base64;
 import org.apache.solr.response.TextResponseWriter;
@@ -44,6 +45,15 @@
     throw new RuntimeException("Cannot sort on a Binary field");
   }
 
+  @Override
+  public Type getUninversionType(SchemaField sf) {
+    // TODO: maybe just return null?
+    if (sf.multiValued()) {
+      return Type.SORTED_SET_BINARY;
+    } else {
+      return Type.BINARY;
+    }
+  }
 
   @Override
   public String toExternal(StorableField f) {


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/schema/BoolField.java lucene5666/solr/core/src/java/org/apache/solr/schema/BoolField.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/schema/BoolField.java	2014-05-14 03:47:28.970646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/schema/BoolField.java	2014-05-14 03:45:17.066644330 -0400
@@ -25,15 +25,15 @@
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.GeneralField;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.StorableField;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.BoolDocValues;
-import org.apache.lucene.queries.function.valuesource.OrdFieldSource;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.SortField;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.mutable.MutableValue;
@@ -41,6 +41,7 @@
 import org.apache.solr.analysis.SolrAnalyzer;
 import org.apache.solr.response.TextResponseWriter;
 import org.apache.solr.search.QParser;
+import org.apache.solr.search.function.OrdFieldSource;
 /**
  *
  */
@@ -52,6 +53,15 @@
   }
 
   @Override
+  public Type getUninversionType(SchemaField sf) {
+    if (sf.multiValued()) {
+      return Type.SORTED_SET_BINARY;
+    } else {
+      return Type.SORTED;
+    }
+  }
+
+  @Override
   public ValueSource getValueSource(SchemaField field, QParser qparser) {
     field.checkFieldCacheSource(qparser);
     return new BoolFieldSource(field.name);
@@ -179,7 +189,7 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final SortedDocValues sindex = FieldCache.DEFAULT.getTermsIndex(readerContext.reader(), field);
+    final SortedDocValues sindex = DocValues.getSorted(readerContext.reader(), field);
 
     // figure out what ord maps to true
     int nord = sindex.getValueCount();


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/schema/CollationField.java lucene5666/solr/core/src/java/org/apache/solr/schema/CollationField.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/schema/CollationField.java	2014-05-14 03:47:28.970646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/schema/CollationField.java	2014-05-14 03:45:16.782644325 -0400
@@ -38,10 +38,11 @@
 import org.apache.lucene.index.StorableField;
 import org.apache.lucene.search.ConstantScoreQuery;
 import org.apache.lucene.search.DocTermOrdsRangeFilter;
-import org.apache.lucene.search.FieldCacheRangeFilter;
+import org.apache.lucene.search.DocValuesRangeFilter;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.SortField;
 import org.apache.lucene.search.TermRangeQuery;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Version;
 import org.apache.lucene.analysis.util.ResourceLoader;
@@ -199,6 +200,15 @@
   public SortField getSortField(SchemaField field, boolean top) {
     return getStringSort(field, top);
   }
+  
+  @Override
+  public Type getUninversionType(SchemaField sf) {
+    if (sf.multiValued()) {
+      return Type.SORTED_SET_BINARY;
+    } else {
+      return Type.SORTED;
+    }
+  }
 
   @Override
   public Analyzer getIndexAnalyzer() {
@@ -245,7 +255,7 @@
           return new ConstantScoreQuery(DocTermOrdsRangeFilter.newBytesRefRange(
               field.getName(), low, high, minInclusive, maxInclusive));
         } else {
-          return new ConstantScoreQuery(FieldCacheRangeFilter.newBytesRefRange(
+          return new ConstantScoreQuery(DocValuesRangeFilter.newBytesRefRange(
               field.getName(), low, high, minInclusive, maxInclusive));
         } 
     } else {


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/schema/CurrencyField.java lucene5666/solr/core/src/java/org/apache/solr/schema/CurrencyField.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/schema/CurrencyField.java	2014-05-14 03:47:28.970646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/schema/CurrencyField.java	2014-05-14 03:45:16.782644325 -0400
@@ -26,6 +26,7 @@
 import org.apache.lucene.search.SortField;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.FieldValueFilter;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
 import org.apache.lucene.queries.ChainedFilter;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrException.ErrorCode;
@@ -343,6 +344,11 @@
     // Convert all values to default currency for sorting.
     return (new RawCurrencyValueSource(field, defaultCurrency, null)).getSortField(reverse);
   }
+  
+  @Override
+  public Type getUninversionType(SchemaField sf) {
+    return null;
+  }
 
   @Override
   public void write(TextResponseWriter writer, String name, StorableField field) throws IOException {


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/schema/EnumField.java lucene5666/solr/core/src/java/org/apache/solr/schema/EnumField.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/schema/EnumField.java	2014-05-14 03:47:28.970646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/schema/EnumField.java	2014-05-14 03:45:17.066644330 -0400
@@ -22,6 +22,7 @@
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.EnumFieldSource;
 import org.apache.lucene.search.*;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.NumericUtils;
@@ -178,10 +179,19 @@
   public SortField getSortField(SchemaField field, boolean top) {
     field.checkSortability();
     final Object missingValue = Integer.MIN_VALUE;
-    SortField sf = new SortField(field.getName(), FieldCache.NUMERIC_UTILS_INT_PARSER, top);
+    SortField sf = new SortField(field.getName(), SortField.Type.INT, top);
     sf.setMissingValue(missingValue);
     return sf;
   }
+  
+  @Override
+  public Type getUninversionType(SchemaField sf) {
+    if (sf.multiValued()) {
+      return Type.SORTED_SET_INTEGER;
+    } else {
+      return Type.INTEGER;
+    }
+  }
 
   /**
    * {@inheritDoc}
@@ -189,7 +199,7 @@
   @Override
   public ValueSource getValueSource(SchemaField field, QParser qparser) {
     field.checkFieldCacheSource(qparser);
-    return new EnumFieldSource(field.getName(), FieldCache.NUMERIC_UTILS_INT_PARSER, enumIntToStringMap, enumStringToIntMap);
+    return new EnumFieldSource(field.getName(), enumIntToStringMap, enumStringToIntMap);
   }
 
   /**
@@ -230,7 +240,7 @@
     Query query = null;
     final boolean matchOnly = field.hasDocValues() && !field.indexed();
     if (matchOnly) {
-      query = new ConstantScoreQuery(FieldCacheRangeFilter.newIntRange(field.getName(),
+      query = new ConstantScoreQuery(DocValuesRangeFilter.newIntRange(field.getName(),
               min == null ? null : minValue,
               max == null ? null : maxValue,
               minInclusive, maxInclusive));


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/schema/ExternalFileField.java lucene5666/solr/core/src/java/org/apache/solr/schema/ExternalFileField.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/schema/ExternalFileField.java	2014-05-14 03:47:28.966646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/schema/ExternalFileField.java	2014-05-14 03:45:17.066644330 -0400
@@ -19,6 +19,7 @@
 import org.apache.lucene.index.StorableField;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.SortField;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.response.TextResponseWriter;
 import org.apache.solr.search.QParser;
@@ -90,6 +91,11 @@
     FileFloatSource source = getFileFloatSource(field);
     return source.getSortField(reverse);
   }
+  
+  @Override
+  public Type getUninversionType(SchemaField sf) {
+    return null;
+  }
 
   @Override
   public ValueSource getValueSource(SchemaField field, QParser parser) {


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/schema/FieldType.java lucene5666/solr/core/src/java/org/apache/solr/schema/FieldType.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/schema/FieldType.java	2014-05-14 03:47:28.970646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/schema/FieldType.java	2014-05-14 03:45:17.066644330 -0400
@@ -32,8 +32,8 @@
 import org.apache.lucene.search.ConstantScoreQuery;
 import org.apache.lucene.search.DocTermOrdsRangeFilter;
 import org.apache.lucene.search.DocTermOrdsRewriteMethod;
-import org.apache.lucene.search.FieldCacheRangeFilter;
-import org.apache.lucene.search.FieldCacheRewriteMethod;
+import org.apache.lucene.search.DocValuesRangeFilter;
+import org.apache.lucene.search.DocValuesRewriteMethod;
 import org.apache.lucene.search.MultiTermQuery;
 import org.apache.lucene.search.PrefixQuery;
 import org.apache.lucene.search.Query;
@@ -41,6 +41,7 @@
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.TermRangeQuery;
 import org.apache.lucene.search.similarities.Similarity;
+import org.apache.lucene.uninverting.UninvertingReader;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.UnicodeUtil;
@@ -446,6 +447,16 @@
     query.setRewriteMethod(sf.getType().getRewriteMethod(parser, sf));
     return query;
   }
+  
+  /**
+   * DocValues is not enabled for a field, but its indexed, docvalues can be constructed 
+   * on the fly (uninverted, aka fieldcache) on the first request to sort, facet, etc. 
+   * This specifies the structure to use.
+   * 
+   * @param sf field instance
+   * @return type to uninvert, or {@code null} (to disallow uninversion for the field)
+   */
+  public abstract UninvertingReader.Type getUninversionType(SchemaField sf);
 
   /**
    * Default analyzer for types that only produce 1 verbatim token...
@@ -687,7 +698,7 @@
             part2 == null ? null : new BytesRef(toInternal(part2)),
             minInclusive, maxInclusive));
       } else {
-        return new ConstantScoreQuery(FieldCacheRangeFilter.newStringRange(
+        return new ConstantScoreQuery(DocValuesRangeFilter.newStringRange(
             field.getName(), 
             part1 == null ? null : toInternal(part1),
             part2 == null ? null : toInternal(part2),
@@ -731,7 +742,7 @@
    */
   public MultiTermQuery.RewriteMethod getRewriteMethod(QParser parser, SchemaField field) {
     if (!field.indexed() && field.hasDocValues()) {
-      return field.multiValued() ? new DocTermOrdsRewriteMethod() : new FieldCacheRewriteMethod();
+      return field.multiValued() ? new DocTermOrdsRewriteMethod() : new DocValuesRewriteMethod();
     } else {
       return MultiTermQuery.CONSTANT_SCORE_AUTO_REWRITE_DEFAULT;
     }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/schema/GeoHashField.java lucene5666/solr/core/src/java/org/apache/solr/schema/GeoHashField.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/schema/GeoHashField.java	2014-05-14 03:47:28.970646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/schema/GeoHashField.java	2014-05-14 03:45:16.782644325 -0400
@@ -22,6 +22,8 @@
 import org.apache.lucene.index.StorableField;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.SortField;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
+
 import com.spatial4j.core.context.SpatialContext;
 import com.spatial4j.core.io.GeohashUtils;
 import com.spatial4j.core.shape.Point;
@@ -47,6 +49,15 @@
   public SortField getSortField(SchemaField field, boolean top) {
     return getStringSort(field, top);
   }
+  
+  @Override
+  public Type getUninversionType(SchemaField sf) {
+    if (sf.multiValued()) {
+      return Type.SORTED_SET_BINARY;
+    } else {
+      return Type.SORTED;
+    }
+  }
 
     //QUESTION: Should we do a fast and crude one?  Or actually check distances
   //Fast and crude could use EdgeNGrams, but that would require a different


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/schema/IndexSchema.java lucene5666/solr/core/src/java/org/apache/solr/schema/IndexSchema.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/schema/IndexSchema.java	2014-05-14 03:47:28.966646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/schema/IndexSchema.java	2014-05-14 03:45:16.842644326 -0400
@@ -19,10 +19,15 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.AnalyzerWrapper;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexableField;
+import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.StorableField;
 import org.apache.lucene.index.StoredDocument;
 import org.apache.lucene.search.similarities.Similarity;
+import org.apache.lucene.uninverting.UninvertingReader;
 import org.apache.lucene.util.Version;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrException.ErrorCode;
@@ -357,6 +362,22 @@
     indexAnalyzer = new SolrIndexAnalyzer();
     queryAnalyzer = new SolrQueryAnalyzer();
   }
+  
+  public Map<String,UninvertingReader.Type> getUninversionMap(IndexReader reader) {
+    Map<String,UninvertingReader.Type> map = new HashMap<>();
+    for (FieldInfo f : MultiFields.getMergedFieldInfos(reader)) {
+      if (f.hasDocValues() == false && f.isIndexed()) {
+        SchemaField sf = getFieldOrNull(f.name);
+        if (sf != null) {
+          UninvertingReader.Type type = sf.getType().getUninversionType(sf);
+          if (type != null) {
+            map.put(f.name, type);
+          }
+        }
+      }
+    }
+    return map;
+  }
 
   /**
    * Writes the schema in schema.xml format to the given writer 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/schema/LatLonType.java lucene5666/solr/core/src/java/org/apache/solr/schema/LatLonType.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/schema/LatLonType.java	2014-05-14 03:47:28.970646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/schema/LatLonType.java	2014-05-14 03:45:17.066644330 -0400
@@ -41,6 +41,7 @@
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.SortField;
 import org.apache.lucene.search.Weight;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
 import org.apache.lucene.util.Bits;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.response.TextResponseWriter;
@@ -241,6 +242,11 @@
   public SortField getSortField(SchemaField field, boolean top) {
     throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, "Sorting not supported on LatLonType " + field.getName());
   }
+  
+  @Override
+  public Type getUninversionType(SchemaField sf) {
+    return null;
+  }
 
 
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/schema/PointType.java lucene5666/solr/core/src/java/org/apache/solr/schema/PointType.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/schema/PointType.java	2014-05-14 03:47:28.970646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/schema/PointType.java	2014-05-14 03:45:17.066644330 -0400
@@ -25,6 +25,7 @@
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.SortField;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.params.MapSolrParams;
 import org.apache.solr.common.params.SolrParams;
@@ -119,6 +120,11 @@
   public SortField getSortField(SchemaField field, boolean top) {
     throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, "Sorting not supported on PointType " + field.getName());
   }
+  
+  @Override
+  public Type getUninversionType(SchemaField sf) {
+    return null;
+  }
 
   @Override
   /**


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/schema/PreAnalyzedField.java lucene5666/solr/core/src/java/org/apache/solr/schema/PreAnalyzedField.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/schema/PreAnalyzedField.java	2014-05-14 03:47:28.970646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/schema/PreAnalyzedField.java	2014-05-14 03:45:17.066644330 -0400
@@ -30,12 +30,18 @@
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.StorableField;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.valuesource.SortedSetFieldSource;
 import org.apache.lucene.search.SortField;
+import org.apache.lucene.search.SortedSetSortField;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
 import org.apache.lucene.util.AttributeFactory;
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.AttributeSource.State;
 import org.apache.solr.analysis.SolrAnalyzer;
 import org.apache.solr.response.TextResponseWriter;
+import org.apache.solr.search.QParser;
+import org.apache.solr.search.Sorting;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -112,10 +118,21 @@
     }
     return f;
   }
-
+  
   @Override
   public SortField getSortField(SchemaField field, boolean top) {
-    return getStringSort(field, top);
+    field.checkSortability();
+    return Sorting.getTextSortField(field.getName(), top, field.sortMissingLast(), field.sortMissingFirst());
+  }
+  
+  @Override
+  public ValueSource getValueSource(SchemaField field, QParser parser) {
+    return new SortedSetFieldSource(field.getName());
+  }
+
+  @Override
+  public Type getUninversionType(SchemaField sf) {
+    return Type.SORTED_SET_BINARY;
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/schema/RandomSortField.java lucene5666/solr/core/src/java/org/apache/solr/schema/RandomSortField.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/schema/RandomSortField.java	2014-05-14 03:47:28.970646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/schema/RandomSortField.java	2014-05-14 03:45:16.842644326 -0400
@@ -30,6 +30,7 @@
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
 import org.apache.lucene.search.*;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
 import org.apache.solr.response.TextResponseWriter;
 import org.apache.solr.search.QParser;
 
@@ -92,6 +93,11 @@
   public SortField getSortField(SchemaField field, boolean reverse) {
     return new SortField(field.getName(), randomComparatorSource, reverse);
   }
+  
+  @Override
+  public Type getUninversionType(SchemaField sf) {
+    return null;
+  }
 
   @Override
   public ValueSource getValueSource(SchemaField field, QParser qparser) {


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/schema/StrField.java lucene5666/solr/core/src/java/org/apache/solr/schema/StrField.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/schema/StrField.java	2014-05-14 03:47:28.966646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/schema/StrField.java	2014-05-14 03:45:17.066644330 -0400
@@ -28,6 +28,7 @@
 import org.apache.lucene.index.StorableField;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.SortField;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
 import org.apache.lucene.util.BytesRef;
 import org.apache.solr.response.TextResponseWriter;
 import org.apache.solr.search.QParser;
@@ -63,6 +64,15 @@
   }
 
   @Override
+  public Type getUninversionType(SchemaField sf) {
+    if (sf.multiValued()) {
+      return Type.SORTED_SET_BINARY;
+    } else {
+      return Type.SORTED;
+    }
+  }
+
+  @Override
   public void write(TextResponseWriter writer, String name, StorableField f) throws IOException {
     writer.writeStr(name, f.stringValue(), true);
   }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/schema/TextField.java lucene5666/solr/core/src/java/org/apache/solr/schema/TextField.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/schema/TextField.java	2014-05-14 03:47:28.970646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/schema/TextField.java	2014-05-14 03:45:17.066644330 -0400
@@ -18,15 +18,19 @@
 package org.apache.solr.schema;
 
 import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.valuesource.SortedSetFieldSource;
 import org.apache.lucene.search.*;
 import org.apache.lucene.index.StorableField;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.QueryBuilder;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.response.TextResponseWriter;
 import org.apache.solr.search.QParser;
+import org.apache.solr.search.Sorting;
 
 import java.util.Map;
 import java.io.IOException;
@@ -93,7 +97,18 @@
   @Override
   public SortField getSortField(SchemaField field, boolean reverse) {
     /* :TODO: maybe warn if isTokenized(), but doesn't use LimitTokenCountFilter in it's chain? */
-    return getStringSort(field, reverse);
+    field.checkSortability();
+    return Sorting.getTextSortField(field.getName(), reverse, field.sortMissingLast(), field.sortMissingFirst());
+  }
+  
+  @Override
+  public ValueSource getValueSource(SchemaField field, QParser parser) {
+    return new SortedSetFieldSource(field.getName());
+  }
+  
+  @Override
+  public Type getUninversionType(SchemaField sf) {
+    return Type.SORTED_SET_BINARY;
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/schema/TrieDateField.java lucene5666/solr/core/src/java/org/apache/solr/schema/TrieDateField.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/schema/TrieDateField.java	2014-05-14 03:47:28.970646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/schema/TrieDateField.java	2014-05-14 03:45:17.066644330 -0400
@@ -30,6 +30,7 @@
 import org.apache.lucene.search.SortField;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.NumericRangeQuery;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
 
@@ -365,6 +366,11 @@
   }
 
   @Override
+  public Type getUninversionType(SchemaField sf) {
+    return wrappedField.getUninversionType(sf);
+  }
+
+  @Override
   public Object marshalSortValue(Object value) {
     return value;
   }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/schema/TrieField.java lucene5666/solr/core/src/java/org/apache/solr/schema/TrieField.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/schema/TrieField.java	2014-05-14 03:47:28.970646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/schema/TrieField.java	2014-05-14 03:45:16.782644325 -0400
@@ -39,11 +39,11 @@
 import org.apache.lucene.queries.function.valuesource.IntFieldSource;
 import org.apache.lucene.queries.function.valuesource.LongFieldSource;
 import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.search.FieldCacheRangeFilter;
+import org.apache.lucene.search.DocValuesRangeFilter;
 import org.apache.lucene.search.NumericRangeQuery;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.SortField;
+import org.apache.lucene.uninverting.UninvertingReader.Type;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.NumericUtils;
@@ -153,7 +153,7 @@
         else if( sortMissingFirst ) {
           missingValue = top ? Integer.MAX_VALUE : Integer.MIN_VALUE;
         }
-        sf = new SortField( field.getName(), FieldCache.NUMERIC_UTILS_INT_PARSER, top);
+        sf = new SortField( field.getName(), SortField.Type.INT, top);
         sf.setMissingValue(missingValue);
         return sf;
       
@@ -164,7 +164,7 @@
         else if( sortMissingFirst ) {
           missingValue = top ? Float.POSITIVE_INFINITY : Float.NEGATIVE_INFINITY;
         }
-        sf = new SortField( field.getName(), FieldCache.NUMERIC_UTILS_FLOAT_PARSER, top);
+        sf = new SortField( field.getName(), SortField.Type.FLOAT, top);
         sf.setMissingValue(missingValue);
         return sf;
       
@@ -176,7 +176,7 @@
         else if( sortMissingFirst ) {
           missingValue = top ? Long.MAX_VALUE : Long.MIN_VALUE;
         }
-        sf = new SortField( field.getName(), FieldCache.NUMERIC_UTILS_LONG_PARSER, top);
+        sf = new SortField( field.getName(), SortField.Type.LONG, top);
         sf.setMissingValue(missingValue);
         return sf;
         
@@ -187,7 +187,7 @@
         else if( sortMissingFirst ) {
           missingValue = top ? Double.POSITIVE_INFINITY : Double.NEGATIVE_INFINITY;
         }
-        sf = new SortField( field.getName(), FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, top);
+        sf = new SortField( field.getName(), SortField.Type.DOUBLE, top);
         sf.setMissingValue(missingValue);
         return sf;
         
@@ -195,21 +195,54 @@
         throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Unknown type for trie field: " + field.name);
     }
   }
+  
+  @Override
+  public Type getUninversionType(SchemaField sf) {
+    if (sf.multiValued()) {
+      switch (type) {
+        case INTEGER:
+          return Type.SORTED_SET_INTEGER;
+        case LONG:
+        case DATE:
+          return Type.SORTED_SET_LONG;
+        case FLOAT:
+          return Type.SORTED_SET_FLOAT;
+        case DOUBLE:
+          return Type.SORTED_SET_DOUBLE;
+        default:
+          throw new AssertionError();
+      }
+    } else {
+      switch (type) {
+        case INTEGER:
+          return Type.INTEGER;
+        case LONG:
+        case DATE:
+          return Type.LONG;
+        case FLOAT:
+          return Type.FLOAT;
+        case DOUBLE:
+          return Type.DOUBLE;
+        default:
+          throw new AssertionError();
+      }
+    }
+  }
 
   @Override
   public ValueSource getValueSource(SchemaField field, QParser qparser) {
     field.checkFieldCacheSource(qparser);
     switch (type) {
       case INTEGER:
-        return new IntFieldSource( field.getName(), FieldCache.NUMERIC_UTILS_INT_PARSER );
+        return new IntFieldSource( field.getName());
       case FLOAT:
-        return new FloatFieldSource( field.getName(), FieldCache.NUMERIC_UTILS_FLOAT_PARSER );
+        return new FloatFieldSource( field.getName());
       case DATE:
-        return new TrieDateFieldSource( field.getName(), FieldCache.NUMERIC_UTILS_LONG_PARSER );        
+        return new TrieDateFieldSource( field.getName());        
       case LONG:
-        return new LongFieldSource( field.getName(), FieldCache.NUMERIC_UTILS_LONG_PARSER );
+        return new LongFieldSource( field.getName());
       case DOUBLE:
-        return new DoubleFieldSource( field.getName(), FieldCache.NUMERIC_UTILS_DOUBLE_PARSER );
+        return new DoubleFieldSource( field.getName());
       default:
         throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Unknown type for trie field: " + field.name);
     }
@@ -274,7 +307,7 @@
     switch (type) {
       case INTEGER:
         if (matchOnly) {
-          query = new ConstantScoreQuery(FieldCacheRangeFilter.newIntRange(field.getName(),
+          query = new ConstantScoreQuery(DocValuesRangeFilter.newIntRange(field.getName(),
                 min == null ? null : Integer.parseInt(min),
                 max == null ? null : Integer.parseInt(max),
                 minInclusive, maxInclusive));
@@ -287,7 +320,7 @@
         break;
       case FLOAT:
         if (matchOnly) {
-          query = new ConstantScoreQuery(FieldCacheRangeFilter.newFloatRange(field.getName(),
+          query = new ConstantScoreQuery(DocValuesRangeFilter.newFloatRange(field.getName(),
                 min == null ? null : Float.parseFloat(min),
                 max == null ? null : Float.parseFloat(max),
                 minInclusive, maxInclusive));
@@ -300,7 +333,7 @@
         break;
       case LONG:
         if (matchOnly) {
-          query = new ConstantScoreQuery(FieldCacheRangeFilter.newLongRange(field.getName(),
+          query = new ConstantScoreQuery(DocValuesRangeFilter.newLongRange(field.getName(),
                 min == null ? null : Long.parseLong(min),
                 max == null ? null : Long.parseLong(max),
                 minInclusive, maxInclusive));
@@ -313,7 +346,7 @@
         break;
       case DOUBLE:
         if (matchOnly) {
-          query = new ConstantScoreQuery(FieldCacheRangeFilter.newDoubleRange(field.getName(),
+          query = new ConstantScoreQuery(DocValuesRangeFilter.newDoubleRange(field.getName(),
                 min == null ? null : Double.parseDouble(min),
                 max == null ? null : Double.parseDouble(max),
                 minInclusive, maxInclusive));
@@ -326,7 +359,7 @@
         break;
       case DATE:
         if (matchOnly) {
-          query = new ConstantScoreQuery(FieldCacheRangeFilter.newLongRange(field.getName(),
+          query = new ConstantScoreQuery(DocValuesRangeFilter.newLongRange(field.getName(),
                 min == null ? null : dateField.parseMath(null, min).getTime(),
                 max == null ? null : dateField.parseMath(null, max).getTime(),
                 minInclusive, maxInclusive));
@@ -706,8 +739,8 @@
 
 class TrieDateFieldSource extends LongFieldSource {
 
-  public TrieDateFieldSource(String field, FieldCache.LongParser parser) {
-    super(field, parser);
+  public TrieDateFieldSource(String field) {
+    super(field);
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/CollapsingQParserPlugin.java lucene5666/solr/core/src/java/org/apache/solr/search/CollapsingQParserPlugin.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/CollapsingQParserPlugin.java	2014-05-14 03:47:28.930646626 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/search/CollapsingQParserPlugin.java	2014-05-12 13:29:09.412245182 -0400
@@ -27,7 +27,9 @@
 
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
@@ -37,7 +39,6 @@
 import org.apache.lucene.search.LeafCollector;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.FilterCollector;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
@@ -288,11 +289,7 @@
 
         SortedDocValues docValues = null;
         FunctionQuery funcQuery = null;
-        if(schemaField.hasDocValues()) {
-          docValues = searcher.getAtomicReader().getSortedDocValues(this.field);
-        } else {
-          docValues = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), this.field);
-        }
+        docValues = DocValues.getSorted(searcher.getAtomicReader(), this.field);
 
         FieldType fieldType = null;
 
@@ -794,7 +791,7 @@
 
   private class IntValueCollapse extends FieldValueCollapse {
 
-    private FieldCache.Ints vals;
+    private NumericDocValues vals;
     private IntCompare comp;
     private int nullVal;
     private int[] ordVals;
@@ -829,11 +826,11 @@
     }
 
     public void setNextReader(AtomicReaderContext context) throws IOException {
-      this.vals = FieldCache.DEFAULT.getInts(context.reader(), this.field, false);
+      this.vals = DocValues.getNumeric(context.reader(), this.field);
     }
 
     public void collapse(int ord, int contextDoc, int globalDoc) throws IOException {
-      int val = vals.get(contextDoc);
+      int val = (int) vals.get(contextDoc);
       if(ord > -1) {
         if(comp.test(val, ordVals[ord])) {
           ords[ord] = globalDoc;
@@ -863,7 +860,7 @@
 
   private class LongValueCollapse extends FieldValueCollapse {
 
-    private FieldCache.Longs vals;
+    private NumericDocValues vals;
     private LongCompare comp;
     private long nullVal;
     private long[] ordVals;
@@ -897,7 +894,7 @@
     }
 
     public void setNextReader(AtomicReaderContext context) throws IOException {
-      this.vals = FieldCache.DEFAULT.getLongs(context.reader(), this.field, false);
+      this.vals = DocValues.getNumeric(context.reader(), this.field);
     }
 
     public void collapse(int ord, int contextDoc, int globalDoc) throws IOException {
@@ -931,7 +928,7 @@
 
   private class FloatValueCollapse extends FieldValueCollapse {
 
-    private FieldCache.Floats vals;
+    private NumericDocValues vals;
     private FloatCompare comp;
     private float nullVal;
     private float[] ordVals;
@@ -966,11 +963,11 @@
     }
 
     public void setNextReader(AtomicReaderContext context) throws IOException {
-      this.vals = FieldCache.DEFAULT.getFloats(context.reader(), this.field, false);
+      this.vals = DocValues.getNumeric(context.reader(), this.field);
     }
 
     public void collapse(int ord, int contextDoc, int globalDoc) throws IOException {
-      float val = vals.get(contextDoc);
+      float val = Float.intBitsToFloat((int)vals.get(contextDoc));
       if(ord > -1) {
         if(comp.test(val, ordVals[ord])) {
           ords[ord] = globalDoc;


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/FastLRUCache.java lucene5666/solr/core/src/java/org/apache/solr/search/FastLRUCache.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/FastLRUCache.java	2014-04-15 21:10:00.898843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/search/FastLRUCache.java	2014-05-12 13:29:09.448245183 -0400
@@ -191,7 +191,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/FastLRUCache.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/search/FastLRUCache.java $";
   }
 
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java lucene5666/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java	2014-04-15 21:10:00.862843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java	2014-05-12 13:29:09.416245182 -0400
@@ -355,7 +355,7 @@
 
     @Override
     public String getSource() {
-      return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java $";
+      return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java $";
     }
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/function/OrdFieldSource.java lucene5666/solr/core/src/java/org/apache/solr/search/function/OrdFieldSource.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/function/OrdFieldSource.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/solr/core/src/java/org/apache/solr/search/function/OrdFieldSource.java	2014-05-14 03:45:16.842644326 -0400
@@ -0,0 +1,153 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.solr.search.function;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiReader;
+import org.apache.lucene.index.ReaderUtil;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.docvalues.IntDocValues;
+import org.apache.lucene.search.SortedSetSelector;
+import org.apache.lucene.util.mutable.MutableValue;
+import org.apache.lucene.util.mutable.MutableValueInt;
+import org.apache.solr.schema.SchemaField;
+import org.apache.solr.search.Insanity;
+import org.apache.solr.search.SolrIndexSearcher;
+
+/**
+ * Obtains the ordinal of the field value from {@link AtomicReader#getSortedDocValues}.
+ * <br>
+ * The native lucene index order is used to assign an ordinal value for each field value.
+ * <br>Field values (terms) are lexicographically ordered by unicode value, and numbered starting at 1.
+ * <br>
+ * Example:<br>
+ *  If there were only three field values: "apple","banana","pear"
+ * <br>then ord("apple")=1, ord("banana")=2, ord("pear")=3
+ * <p>
+ * WARNING: ord() depends on the position in an index and can thus change when other documents are inserted or deleted,
+ *  or if a MultiSearcher is used.
+ * <br>WARNING: as of Solr 1.4, ord() and rord() can cause excess memory use since they must use a FieldCache entry
+ * at the top level reader, while sorting and function queries now use entries at the segment level.  Hence sorting
+ * or using a different function query, in addition to ord()/rord() will double memory use.
+ *
+ */
+
+public class OrdFieldSource extends ValueSource {
+  protected final String field;
+
+  public OrdFieldSource(String field) {
+    this.field = field;
+  }
+
+  @Override
+  public String description() {
+    return "ord(" + field + ')';
+  }
+
+
+  @Override
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    final int off = readerContext.docBase;
+    final AtomicReader r;
+    Object o = context.get("searcher");
+    if (o instanceof SolrIndexSearcher) {
+      SolrIndexSearcher is = (SolrIndexSearcher) o;
+      SchemaField sf = is.getSchema().getFieldOrNull(field);
+      if (sf != null && sf.hasDocValues() == false && sf.multiValued() == false && sf.getType().getNumericType() != null) {
+        // its a single-valued numeric field: we must currently create insanity :(
+        List<AtomicReaderContext> leaves = is.getIndexReader().leaves();
+        AtomicReader insaneLeaves[] = new AtomicReader[leaves.size()];
+        int upto = 0;
+        for (AtomicReaderContext raw : leaves) {
+          insaneLeaves[upto++] = Insanity.wrapInsanity(raw.reader(), field);
+        }
+        r = SlowCompositeReaderWrapper.wrap(new MultiReader(insaneLeaves));
+      } else {
+        // reuse ordinalmap
+        r = ((SolrIndexSearcher)o).getAtomicReader();
+      }
+    } else {
+      IndexReader topReader = ReaderUtil.getTopLevelContext(readerContext).reader();
+      r = SlowCompositeReaderWrapper.wrap(topReader);
+    }
+    // if its e.g. tokenized/multivalued, emulate old behavior of single-valued fc
+    final SortedDocValues sindex = SortedSetSelector.wrap(DocValues.getSortedSet(r, field), SortedSetSelector.Type.MIN);
+    return new IntDocValues(this) {
+      protected String toTerm(String readableValue) {
+        return readableValue;
+      }
+      @Override
+      public int intVal(int doc) {
+        return sindex.getOrd(doc+off);
+      }
+      @Override
+      public int ordVal(int doc) {
+        return sindex.getOrd(doc+off);
+      }
+      @Override
+      public int numOrd() {
+        return sindex.getValueCount();
+      }
+
+      @Override
+      public boolean exists(int doc) {
+        return sindex.getOrd(doc+off) != 0;
+      }
+
+      @Override
+      public ValueFiller getValueFiller() {
+        return new ValueFiller() {
+          private final MutableValueInt mval = new MutableValueInt();
+
+          @Override
+          public MutableValue getValue() {
+            return mval;
+          }
+
+          @Override
+          public void fillValue(int doc) {
+            mval.value = sindex.getOrd(doc);
+            mval.exists = mval.value!=0;
+          }
+        };
+      }
+    };
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    return o != null && o.getClass() == OrdFieldSource.class && this.field.equals(((OrdFieldSource)o).field);
+  }
+
+  private static final int hcode = OrdFieldSource.class.hashCode();
+  @Override
+  public int hashCode() {
+    return hcode + field.hashCode();
+  }
+
+}


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/function/ReverseOrdFieldSource.java lucene5666/solr/core/src/java/org/apache/solr/search/function/ReverseOrdFieldSource.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/function/ReverseOrdFieldSource.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/solr/core/src/java/org/apache/solr/search/function/ReverseOrdFieldSource.java	2014-05-14 03:45:16.842644326 -0400
@@ -0,0 +1,124 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.solr.search.function;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.CompositeReader;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiReader;
+import org.apache.lucene.index.ReaderUtil;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.docvalues.IntDocValues;
+import org.apache.lucene.search.SortedSetSelector;
+import org.apache.solr.schema.SchemaField;
+import org.apache.solr.search.Insanity;
+import org.apache.solr.search.SolrIndexSearcher;
+
+/**
+ * Obtains the ordinal of the field value from {@link AtomicReader#getSortedDocValues}
+ * and reverses the order.
+ * <br>
+ * The native lucene index order is used to assign an ordinal value for each field value.
+ * <br>Field values (terms) are lexicographically ordered by unicode value, and numbered starting at 1.
+ * <br>
+ * Example of reverse ordinal (rord):<br>
+ *  If there were only three field values: "apple","banana","pear"
+ * <br>then rord("apple")=3, rord("banana")=2, ord("pear")=1
+ * <p>
+ *  WARNING: ord() depends on the position in an index and can thus change when other documents are inserted or deleted,
+ *  or if a MultiSearcher is used.
+ * <br>
+ *  WARNING: as of Solr 1.4, ord() and rord() can cause excess memory use since they must use a FieldCache entry
+ * at the top level reader, while sorting and function queries now use entries at the segment level.  Hence sorting
+ * or using a different function query, in addition to ord()/rord() will double memory use.
+ * 
+ *
+ */
+
+public class ReverseOrdFieldSource extends ValueSource {
+  public final String field;
+
+  public ReverseOrdFieldSource(String field) {
+    this.field = field;
+  }
+
+  @Override
+  public String description() {
+    return "rord("+field+')';
+  }
+
+  @Override
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    final int off = readerContext.docBase;
+    final AtomicReader r;
+    Object o = context.get("searcher");
+    if (o instanceof SolrIndexSearcher) {
+      SolrIndexSearcher is = (SolrIndexSearcher) o;
+      SchemaField sf = is.getSchema().getFieldOrNull(field);
+      if (sf != null && sf.hasDocValues() == false && sf.multiValued() == false && sf.getType().getNumericType() != null) {
+        // its a single-valued numeric field: we must currently create insanity :(
+        List<AtomicReaderContext> leaves = is.getIndexReader().leaves();
+        AtomicReader insaneLeaves[] = new AtomicReader[leaves.size()];
+        int upto = 0;
+        for (AtomicReaderContext raw : leaves) {
+          insaneLeaves[upto++] = Insanity.wrapInsanity(raw.reader(), field);
+        }
+        r = SlowCompositeReaderWrapper.wrap(new MultiReader(insaneLeaves));
+      } else {
+        // reuse ordinalmap
+        r = ((SolrIndexSearcher)o).getAtomicReader();
+      }
+    } else {
+      IndexReader topReader = ReaderUtil.getTopLevelContext(readerContext).reader();
+      r = SlowCompositeReaderWrapper.wrap(topReader);
+    }
+    // if its e.g. tokenized/multivalued, emulate old behavior of single-valued fc
+    final SortedDocValues sindex = SortedSetSelector.wrap(DocValues.getSortedSet(r, field), SortedSetSelector.Type.MIN);
+    final int end = sindex.getValueCount();
+
+    return new IntDocValues(this) {
+     @Override
+      public int intVal(int doc) {
+        return (end - sindex.getOrd(doc+off) - 1);
+      }
+    };
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (o == null || (o.getClass() !=  ReverseOrdFieldSource.class)) return false;
+    ReverseOrdFieldSource other = (ReverseOrdFieldSource)o;
+    return this.field.equals(other.field);
+  }
+
+  private static final int hcode = ReverseOrdFieldSource.class.hashCode();
+  @Override
+  public int hashCode() {
+    return hcode + field.hashCode();
+  }
+
+}


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/grouping/CommandHandler.java lucene5666/solr/core/src/java/org/apache/solr/search/grouping/CommandHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/grouping/CommandHandler.java	2014-05-14 03:47:28.938646626 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/search/grouping/CommandHandler.java	2014-05-14 03:45:16.842644326 -0400
@@ -19,8 +19,10 @@
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.List;
 
+import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.MultiCollector;
@@ -28,8 +30,12 @@
 import org.apache.lucene.search.TimeLimitingCollector;
 import org.apache.lucene.search.TotalHitCountCollector;
 import org.apache.lucene.search.grouping.AbstractAllGroupHeadsCollector;
+import org.apache.lucene.search.grouping.function.FunctionAllGroupHeadsCollector;
+import org.apache.lucene.search.grouping.function.FunctionAllGroupsCollector;
 import org.apache.lucene.search.grouping.term.TermAllGroupHeadsCollector;
 import org.apache.solr.common.util.NamedList;
+import org.apache.solr.schema.FieldType;
+import org.apache.solr.schema.SchemaField;
 import org.apache.solr.search.BitDocSet;
 import org.apache.solr.search.DocSet;
 import org.apache.solr.search.DocSetCollector;
@@ -157,16 +163,25 @@
 
   private DocSet computeGroupedDocSet(Query query, ProcessedFilter filter, List<Collector> collectors) throws IOException {
     Command firstCommand = commands.get(0);
-    AbstractAllGroupHeadsCollector termAllGroupHeadsCollector =
-        TermAllGroupHeadsCollector.create(firstCommand.getKey(), firstCommand.getSortWithinGroup());
+    String field = firstCommand.getKey();
+    SchemaField sf = searcher.getSchema().getField(field);
+    FieldType fieldType = sf.getType();
+    
+    final AbstractAllGroupHeadsCollector allGroupHeadsCollector;
+    if (fieldType.getNumericType() != null) {
+      ValueSource vs = fieldType.getValueSource(sf, null);
+      allGroupHeadsCollector = new FunctionAllGroupHeadsCollector(vs, new HashMap<Object,Object>(), firstCommand.getSortWithinGroup());
+    } else {
+      allGroupHeadsCollector = TermAllGroupHeadsCollector.create(firstCommand.getKey(), firstCommand.getSortWithinGroup());
+    }
     if (collectors.isEmpty()) {
-      searchWithTimeLimiter(query, filter, termAllGroupHeadsCollector);
+      searchWithTimeLimiter(query, filter, allGroupHeadsCollector);
     } else {
-      collectors.add(termAllGroupHeadsCollector);
+      collectors.add(allGroupHeadsCollector);
       searchWithTimeLimiter(query, filter, MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()])));
     }
 
-    return new BitDocSet(termAllGroupHeadsCollector.retrieveGroupHeads(searcher.maxDoc()));
+    return new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(searcher.maxDoc()));
   }
 
   private DocSet computeDocSet(Query query, ProcessedFilter filter, List<Collector> collectors) throws IOException {


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/grouping/distributed/command/GroupConverter.java lucene5666/solr/core/src/java/org/apache/solr/search/grouping/distributed/command/GroupConverter.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/grouping/distributed/command/GroupConverter.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/solr/core/src/java/org/apache/solr/search/grouping/distributed/command/GroupConverter.java	2014-05-14 03:45:16.842644326 -0400
@@ -0,0 +1,160 @@
+package org.apache.solr.search.grouping.distributed.command;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Date;
+import java.util.List;
+
+import org.apache.lucene.search.grouping.GroupDocs;
+import org.apache.lucene.search.grouping.SearchGroup;
+import org.apache.lucene.search.grouping.TopGroups;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.mutable.MutableValue;
+import org.apache.lucene.util.mutable.MutableValueDate;
+import org.apache.lucene.util.mutable.MutableValueDouble;
+import org.apache.lucene.util.mutable.MutableValueFloat;
+import org.apache.lucene.util.mutable.MutableValueInt;
+import org.apache.lucene.util.mutable.MutableValueLong;
+import org.apache.solr.schema.FieldType;
+import org.apache.solr.schema.SchemaField;
+import org.apache.solr.schema.TrieField;
+
+/** 
+ * this is a transition class: for numeric types we use function-based distributed grouping,
+ * otherwise term-based. so for now we internally use function-based but pretend like we did 
+ * it all with bytes, to not change any wire serialization etc.
+ */
+class GroupConverter {
+  
+  static Collection<SearchGroup<BytesRef>> fromMutable(SchemaField field, Collection<SearchGroup<MutableValue>> values) {
+    if (values == null) {
+      return null;
+    }
+    FieldType fieldType = field.getType();
+    List<SearchGroup<BytesRef>> result = new ArrayList<>(values.size());
+    for (SearchGroup<MutableValue> original : values) {
+      SearchGroup<BytesRef> converted = new SearchGroup<BytesRef>();
+      converted.sortValues = original.sortValues;
+      if (original.groupValue.exists) {
+        BytesRef binary = new BytesRef();
+        fieldType.readableToIndexed(original.groupValue.toString(), binary);
+        converted.groupValue = binary;
+      } else {
+        converted.groupValue = null;
+      }
+      result.add(converted);
+    }
+    return result;
+  }
+  
+  static Collection<SearchGroup<MutableValue>> toMutable(SchemaField field, Collection<SearchGroup<BytesRef>> values) {
+    FieldType fieldType = field.getType();
+    List<SearchGroup<MutableValue>> result = new ArrayList<>(values.size());
+    for (SearchGroup<BytesRef> original : values) {
+      SearchGroup<MutableValue> converted = new SearchGroup<MutableValue>();
+      converted.sortValues = original.sortValues; // ?
+      TrieField.TrieTypes type = ((TrieField)fieldType).getType();
+      final MutableValue v;
+      switch (type) {
+        case INTEGER:
+          MutableValueInt mutableInt = new MutableValueInt();
+          if (original.groupValue == null) {
+            mutableInt.value = 0;
+            mutableInt.exists = false;
+          } else {
+            mutableInt.value = (Integer) fieldType.toObject(field, original.groupValue);
+          }
+          v = mutableInt;
+          break;
+        case FLOAT:
+          MutableValueFloat mutableFloat = new MutableValueFloat();
+          if (original.groupValue == null) {
+            mutableFloat.value = 0;
+            mutableFloat.exists = false;
+          } else {
+            mutableFloat.value = (Float) fieldType.toObject(field, original.groupValue);
+          }
+          v = mutableFloat;
+          break;
+        case DOUBLE:
+          MutableValueDouble mutableDouble = new MutableValueDouble();
+          if (original.groupValue == null) {
+            mutableDouble.value = 0;
+            mutableDouble.exists = false;
+          } else {
+            mutableDouble.value = (Double) fieldType.toObject(field, original.groupValue);
+          }
+          v = mutableDouble;
+          break;
+        case LONG:
+          MutableValueLong mutableLong = new MutableValueLong();
+          if (original.groupValue == null) {
+            mutableLong.value = 0;
+            mutableLong.exists = false;
+          } else {
+            mutableLong.value = (Long) fieldType.toObject(field, original.groupValue);
+          }
+          v = mutableLong;
+          break;
+        case DATE:
+          MutableValueDate mutableDate = new MutableValueDate();
+          if (original.groupValue == null) {
+            mutableDate.value = 0;
+            mutableDate.exists = false;
+          } else {
+            mutableDate.value = ((Date)fieldType.toObject(field, original.groupValue)).getTime();
+          }
+          v = mutableDate;
+          break;
+        default:
+          throw new AssertionError();
+      }
+      converted.groupValue = v;
+      result.add(converted);
+    }
+    return result;
+  }
+  
+  static TopGroups<BytesRef> fromMutable(SchemaField field, TopGroups<MutableValue> values) {
+    if (values == null) {
+      return null;
+    }
+    
+    FieldType fieldType = field.getType();
+    
+    @SuppressWarnings("unchecked")
+    GroupDocs<BytesRef> groupDocs[] = new GroupDocs[values.groups.length];
+    
+    for (int i = 0; i < values.groups.length; i++) {
+      GroupDocs<MutableValue> original = values.groups[i];
+      final BytesRef groupValue;
+      if (original.groupValue.exists) {
+        BytesRef binary = new BytesRef();
+        fieldType.readableToIndexed(original.groupValue.toString(), binary);
+        groupValue = binary;
+      } else {
+        groupValue = null;
+      }
+      groupDocs[i] = new GroupDocs<BytesRef>(original.score, original.maxScore, original.totalHits, original.scoreDocs, groupValue, original.groupSortValues);
+    }
+    
+    return new TopGroups<BytesRef>(values.groupSort, values.withinGroupSort, values.totalHitCount, values.totalGroupedHitCount, groupDocs, values.maxScore);
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/grouping/distributed/command/SearchGroupsFieldCommand.java lucene5666/solr/core/src/java/org/apache/solr/search/grouping/distributed/command/SearchGroupsFieldCommand.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/grouping/distributed/command/SearchGroupsFieldCommand.java	2014-05-14 03:47:28.938646626 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/search/grouping/distributed/command/SearchGroupsFieldCommand.java	2014-05-14 03:45:17.142644331 -0400
@@ -17,12 +17,18 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.grouping.AbstractAllGroupsCollector;
+import org.apache.lucene.search.grouping.AbstractFirstPassGroupingCollector;
 import org.apache.lucene.search.grouping.SearchGroup;
+import org.apache.lucene.search.grouping.function.FunctionAllGroupsCollector;
+import org.apache.lucene.search.grouping.function.FunctionFirstPassGroupingCollector;
 import org.apache.lucene.search.grouping.term.TermAllGroupsCollector;
 import org.apache.lucene.search.grouping.term.TermFirstPassGroupingCollector;
 import org.apache.lucene.util.BytesRef;
+import org.apache.solr.schema.FieldType;
 import org.apache.solr.schema.SchemaField;
 import org.apache.solr.search.grouping.Command;
 
@@ -76,8 +82,8 @@
   private final int topNGroups;
   private final boolean includeGroupCount;
 
-  private TermFirstPassGroupingCollector firstPassGroupingCollector;
-  private TermAllGroupsCollector allGroupsCollector;
+  private AbstractFirstPassGroupingCollector firstPassGroupingCollector;
+  private AbstractAllGroupsCollector allGroupsCollector;
 
   private SearchGroupsFieldCommand(SchemaField field, Sort groupSort, int topNGroups, boolean includeGroupCount) {
     this.field = field;
@@ -89,12 +95,23 @@
   @Override
   public List<Collector> create() throws IOException {
     List<Collector> collectors = new ArrayList<>();
+    FieldType fieldType = field.getType();
     if (topNGroups > 0) {
-      firstPassGroupingCollector = new TermFirstPassGroupingCollector(field.getName(), groupSort, topNGroups);
+      if (fieldType.getNumericType() != null) {
+        ValueSource vs = fieldType.getValueSource(field, null);
+        firstPassGroupingCollector = new FunctionFirstPassGroupingCollector(vs, new HashMap<Object,Object>(), groupSort, topNGroups);
+      } else {
+        firstPassGroupingCollector = new TermFirstPassGroupingCollector(field.getName(), groupSort, topNGroups);
+      }
       collectors.add(firstPassGroupingCollector);
     }
     if (includeGroupCount) {
-      allGroupsCollector = new TermAllGroupsCollector(field.getName());
+      if (fieldType.getNumericType() != null) {
+        ValueSource vs = fieldType.getValueSource(field, null);
+        allGroupsCollector = new FunctionAllGroupsCollector(vs, new HashMap<Object,Object>());
+      } else {
+        allGroupsCollector = new TermAllGroupsCollector(field.getName());
+      }
       collectors.add(allGroupsCollector);
     }
     return collectors;
@@ -104,7 +121,11 @@
   public Pair<Integer, Collection<SearchGroup<BytesRef>>> result() {
     final Collection<SearchGroup<BytesRef>> topGroups;
     if (topNGroups > 0) {
-      topGroups = firstPassGroupingCollector.getTopGroups(0, true);
+      if (field.getType().getNumericType() != null) {
+        topGroups = GroupConverter.fromMutable(field, firstPassGroupingCollector.getTopGroups(0, true));
+      } else {
+        topGroups = firstPassGroupingCollector.getTopGroups(0, true);
+      }
     } else {
       topGroups = Collections.emptyList();
     }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/grouping/distributed/command/TopGroupsFieldCommand.java lucene5666/solr/core/src/java/org/apache/solr/search/grouping/distributed/command/TopGroupsFieldCommand.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/grouping/distributed/command/TopGroupsFieldCommand.java	2014-05-14 03:47:28.938646626 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/search/grouping/distributed/command/TopGroupsFieldCommand.java	2014-05-14 03:45:17.142644331 -0400
@@ -17,13 +17,18 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.grouping.AbstractSecondPassGroupingCollector;
 import org.apache.lucene.search.grouping.GroupDocs;
 import org.apache.lucene.search.grouping.SearchGroup;
 import org.apache.lucene.search.grouping.TopGroups;
+import org.apache.lucene.search.grouping.function.FunctionSecondPassGroupingCollector;
 import org.apache.lucene.search.grouping.term.TermSecondPassGroupingCollector;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.mutable.MutableValue;
+import org.apache.solr.schema.FieldType;
 import org.apache.solr.schema.SchemaField;
 import org.apache.solr.search.grouping.Command;
 
@@ -31,6 +36,7 @@
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.List;
 
 /**
@@ -101,7 +107,7 @@
   private final int maxDocPerGroup;
   private final boolean needScores;
   private final boolean needMaxScore;
-  private TermSecondPassGroupingCollector secondPassCollector;
+  private AbstractSecondPassGroupingCollector secondPassCollector;
 
   private TopGroupsFieldCommand(SchemaField field,
                                 Sort groupSort,
@@ -126,9 +132,18 @@
     }
 
     List<Collector> collectors = new ArrayList<>();
-    secondPassCollector = new TermSecondPassGroupingCollector(
+    FieldType fieldType = field.getType();
+    if (fieldType.getNumericType() != null) {
+      ValueSource vs = fieldType.getValueSource(field, null);
+      Collection<SearchGroup<MutableValue>> v = GroupConverter.toMutable(field, firstPhaseGroups);
+      secondPassCollector = new FunctionSecondPassGroupingCollector(
+          v, groupSort, sortWithinGroup, maxDocPerGroup, needScores, needMaxScore, true, vs, new HashMap<Object,Object>()
+      );
+    } else {
+      secondPassCollector = new TermSecondPassGroupingCollector(
           field.getName(), firstPhaseGroups, groupSort, sortWithinGroup, maxDocPerGroup, needScores, needMaxScore, true
-    );
+      );
+    }
     collectors.add(secondPassCollector);
     return collectors;
   }
@@ -140,7 +155,12 @@
       return new TopGroups<>(groupSort.getSort(), sortWithinGroup.getSort(), 0, 0, new GroupDocs[0], Float.NaN);
     }
 
-    return secondPassCollector.getTopGroups(0);
+    FieldType fieldType = field.getType();
+    if (fieldType.getNumericType() != null) {
+      return GroupConverter.fromMutable(field, secondPassCollector.getTopGroups(0));
+    } else {
+      return secondPassCollector.getTopGroups(0);
+    }
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/Insanity.java lucene5666/solr/core/src/java/org/apache/solr/search/Insanity.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/Insanity.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/solr/core/src/java/org/apache/solr/search/Insanity.java	2014-05-14 03:45:17.142644331 -0400
@@ -0,0 +1,129 @@
+package org.apache.solr.search;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.FilterAtomicReader;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.uninverting.UninvertingReader;
+
+/** 
+ * Lucene 5.0 removes "accidental" insanity, so you must explicitly
+ * create it.
+ * <p>
+ * This class creates insanity for two specific situations:
+ * <ul>
+ *   <li>calling {@code ord} or {@code rord} functions on a single-valued numeric field.
+ *   <li>doing grouped faceting ({@code group.facet}) on a single-valued numeric field.
+ * </ul>
+ */
+@Deprecated
+public class Insanity {
+  
+  /** 
+   * Returns a view over {@code sane} where {@code insaneField} is a string
+   * instead of a numeric.
+   */
+  public static AtomicReader wrapInsanity(AtomicReader sane, String insaneField) {
+    return new UninvertingReader(new InsaneReader(sane, insaneField),
+                                 Collections.singletonMap(insaneField, UninvertingReader.Type.SORTED));
+  }
+  
+  /** Hides the proper numeric dv type for the field */
+  private static class InsaneReader extends FilterAtomicReader {
+    final String insaneField;
+    final FieldInfos fieldInfos;
+    
+    InsaneReader(AtomicReader in, String insaneField) {
+      super(in);
+      this.insaneField = insaneField;
+      ArrayList<FieldInfo> filteredInfos = new ArrayList<>();
+      for (FieldInfo fi : in.getFieldInfos()) {
+        if (fi.name.equals(insaneField)) {
+          filteredInfos.add(new FieldInfo(fi.name, fi.isIndexed(), fi.number, fi.hasVectors(), fi.omitsNorms(),
+                                          fi.hasPayloads(), fi.getIndexOptions(), null, fi.getNormType(), null));
+        } else {
+          filteredInfos.add(fi);
+        }
+      }
+      fieldInfos = new FieldInfos(filteredInfos.toArray(new FieldInfo[filteredInfos.size()]));
+    }
+
+    @Override
+    public NumericDocValues getNumericDocValues(String field) throws IOException {
+      if (insaneField.equals(field)) {
+        return null;
+      } else {
+        return in.getNumericDocValues(field);
+      }
+    }
+
+    @Override
+    public BinaryDocValues getBinaryDocValues(String field) throws IOException {
+      if (insaneField.equals(field)) {
+        return null;
+      } else {
+        return in.getBinaryDocValues(field);
+      }
+    }
+
+    @Override
+    public SortedDocValues getSortedDocValues(String field) throws IOException {
+      if (insaneField.equals(field)) {
+        return null;
+      } else {
+        return in.getSortedDocValues(field);
+      }
+    }
+
+    @Override
+    public SortedSetDocValues getSortedSetDocValues(String field) throws IOException {
+      if (insaneField.equals(field)) {
+        return null;
+      } else {
+        return in.getSortedSetDocValues(field);
+      }
+    }
+
+    @Override
+    public FieldInfos getFieldInfos() {
+      return fieldInfos;
+    }
+
+    // important to override these, so fieldcaches are shared on what we wrap
+    
+    @Override
+    public Object getCoreCacheKey() {
+      return in.getCoreCacheKey();
+    }
+
+    @Override
+    public Object getCombinedCoreAndDeletesKey() {
+      return in.getCombinedCoreAndDeletesKey();
+    }
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/LFUCache.java lucene5666/solr/core/src/java/org/apache/solr/search/LFUCache.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/LFUCache.java	2014-04-15 21:10:00.866843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/search/LFUCache.java	2014-05-12 13:29:09.408245182 -0400
@@ -223,7 +223,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/LFUCache.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/search/LFUCache.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/LRUCache.java lucene5666/solr/core/src/java/org/apache/solr/search/LRUCache.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/LRUCache.java	2014-05-01 04:30:05.971131245 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/search/LRUCache.java	2014-05-12 13:29:09.412245182 -0400
@@ -219,7 +219,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/LRUCache.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/search/LRUCache.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/QParserPlugin.java lucene5666/solr/core/src/java/org/apache/solr/search/QParserPlugin.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/QParserPlugin.java	2014-04-15 21:10:00.862843032 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/search/QParserPlugin.java	2014-05-12 13:29:09.640245186 -0400
@@ -90,7 +90,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/QParserPlugin.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/search/QParserPlugin.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java lucene5666/solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java	2014-05-14 03:47:28.934646626 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java	2014-05-14 03:45:17.142644331 -0400
@@ -19,39 +19,32 @@
 
 import java.net.URL;
 
+import org.apache.lucene.uninverting.UninvertingReader;
 import org.apache.solr.common.util.NamedList;
 import org.apache.solr.common.util.SimpleOrderedMap;
 
 import org.apache.solr.core.SolrCore;
 import org.apache.solr.core.SolrInfoMBean;
 
-import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.search.FieldCache.CacheEntry;
-import org.apache.lucene.util.FieldCacheSanityChecker;
-import org.apache.lucene.util.FieldCacheSanityChecker.Insanity;
-
 /**
- * A SolrInfoMBean that provides introspection of the Lucene FieldCache, this is <b>NOT</b> a cache that is managed by Solr.
+ * A SolrInfoMBean that provides introspection of the Solr FieldCache
  *
  */
 public class SolrFieldCacheMBean implements SolrInfoMBean {
 
-  protected FieldCacheSanityChecker checker = new FieldCacheSanityChecker();
-
   @Override
   public String getName() { return this.getClass().getName(); }
   @Override
   public String getVersion() { return SolrCore.version; }
   @Override
   public String getDescription() {
-    return "Provides introspection of the Lucene FieldCache, "
-      +    "this is **NOT** a cache that is managed by Solr.";
+    return "Provides introspection of the Solr FieldCache ";
   }
   @Override
   public Category getCategory() { return Category.CACHE; } 
   @Override
   public String getSource() { 
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java $";
   }
   @Override
   public URL[] getDocs() {
@@ -60,27 +53,10 @@
   @Override
   public NamedList getStatistics() {
     NamedList stats = new SimpleOrderedMap();
-    CacheEntry[] entries = FieldCache.DEFAULT.getCacheEntries();
+    String[] entries = UninvertingReader.getUninvertedStats();
     stats.add("entries_count", entries.length);
     for (int i = 0; i < entries.length; i++) {
-      CacheEntry e = entries[i];
-      stats.add("entry#" + i, e.toString());
-    }
-
-    Insanity[] insanity = checker.check(entries);
-
-    stats.add("insanity_count", insanity.length);
-    for (int i = 0; i < insanity.length; i++) {
-
-      /** RAM estimation is both CPU and memory intensive... we don't want to do it unless asked.
-      // we only estimate the size of insane entries
-      for (CacheEntry e : insanity[i].getCacheEntries()) {
-        // don't re-estimate if we've already done it.
-        if (null == e.getEstimatedSize()) e.estimateSize();
-      }
-      **/
-      
-      stats.add("insanity#" + i, insanity[i].toString());
+      stats.add("entry#" + i, entries[i]);
     }
     return stats;
   }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java lucene5666/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java	2014-05-14 03:47:28.938646626 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java	2014-05-14 03:45:17.142644331 -0400
@@ -89,6 +89,7 @@
 import org.apache.lucene.search.TotalHitCountCollector;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.uninverting.UninvertingReader;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.FixedBitSet;
@@ -169,9 +170,12 @@
   private DirectoryFactory directoryFactory;
   
   private final AtomicReader atomicReader;
+  // only for addIndexes etc (no fieldcache)
+  private final DirectoryReader rawReader;
+  
   private String path;
   private final boolean reserveDirectory;
-  private final boolean createdDirectory; 
+  private boolean createdDirectory; 
   
   private static DirectoryReader getReader(SolrCore core, SolrIndexConfig config, DirectoryFactory directoryFactory, String path) throws IOException {
     DirectoryReader reader = null;
@@ -184,18 +188,27 @@
     }
     return reader;
   }
+  
+  // TODO: wrap elsewhere and return a "map" from the schema that overrides get() ?
+  // this reader supports reopen
+  private static DirectoryReader wrapReader(SolrCore core, DirectoryReader reader) {
+    assert reader != null;
+    return UninvertingReader.wrap(reader, core.getLatestSchema().getUninversionMap(reader));
+  }
 
   public SolrIndexSearcher(SolrCore core, String path, IndexSchema schema, SolrIndexConfig config, String name, boolean enableCache, DirectoryFactory directoryFactory) throws IOException {
     // we don't need to reserve the directory because we get it from the factory
-    this(core, path, schema, config, name, null, true, enableCache, false, directoryFactory);
+    this(core, path, schema, config, name, getReader(core, config, directoryFactory, path), true, enableCache, false, directoryFactory);
+    this.createdDirectory = true;
   }
 
   public SolrIndexSearcher(SolrCore core, String path, IndexSchema schema, SolrIndexConfig config, String name, DirectoryReader r, boolean closeReader, boolean enableCache, boolean reserveDirectory, DirectoryFactory directoryFactory) throws IOException {
-    super(r == null ? getReader(core, config, directoryFactory, path) : r);
+    super(wrapReader(core, r));
 
     this.path = path;
     this.directoryFactory = directoryFactory;
     this.reader = (DirectoryReader) super.readerContext.reader();
+    this.rawReader = r;
     this.atomicReader = SlowCompositeReaderWrapper.wrap(this.reader);
     this.core = core;
     this.schema = schema;
@@ -211,7 +224,6 @@
     Directory dir = getIndexReader().directory();
     
     this.reserveDirectory = reserveDirectory;
-    this.createdDirectory = r == null;
     if (reserveDirectory) {
       // keep the directory from being released while we use it
       directoryFactory.incRef(dir);
@@ -303,6 +315,11 @@
     return atomicReader;
   }
   
+  /** Raw reader (no fieldcaches etc). Useful for operations like addIndexes */
+  public final DirectoryReader getRawReader() {
+    return rawReader;
+  }
+  
   @Override
   public final DirectoryReader getIndexReader() {
     assert reader == super.getIndexReader();
@@ -351,7 +368,7 @@
     
     long cpg = reader.getIndexCommit().getGeneration();
     try {
-      if (closeReader) reader.decRef();
+      if (closeReader) rawReader.decRef();
     } catch (Exception e) {
       SolrException.log(log, "Problem dec ref'ing reader", e);
     }
@@ -2230,7 +2247,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/Sorting.java lucene5666/solr/core/src/java/org/apache/solr/search/Sorting.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/Sorting.java	2014-05-14 03:47:28.934646626 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/search/Sorting.java	2014-05-14 03:45:17.142644331 -0400
@@ -40,12 +40,23 @@
    * @return SortField
    */
   public static SortField getStringSortField(String fieldName, boolean reverse, boolean nullLast, boolean nullFirst) {
+    SortField sortField = new SortField(fieldName, SortField.Type.STRING, reverse);
+    applyMissingFirstLast(sortField, reverse, nullLast, nullFirst);
+    return sortField;
+  }
+
+  /** Like {@link #getStringSortField}) except safe for tokenized fields */
+  public static SortField getTextSortField(String fieldName, boolean reverse, boolean nullLast, boolean nullFirst) {
+    SortField sortField = new SortedSetSortField(fieldName, reverse);
+    applyMissingFirstLast(sortField, reverse, nullLast, nullFirst);
+    return sortField;
+  }
+  
+  private static void applyMissingFirstLast(SortField in, boolean reverse, boolean nullLast, boolean nullFirst) {
     if (nullFirst && nullLast) {
       throw new IllegalArgumentException("Cannot specify missing values as both first and last");
     }
-
-    SortField sortField = new SortField(fieldName, SortField.Type.STRING, reverse);
-
+    
     // 4 cases:
     // missingFirst / forward: default lucene behavior
     // missingFirst / reverse: set sortMissingLast
@@ -53,12 +64,11 @@
     // missingLast  / reverse: default lucene behavior
     
     if (nullFirst && reverse) {
-      sortField.setMissingValue(SortField.STRING_LAST);
+      in.setMissingValue(SortField.STRING_LAST);
     } else if (nullLast && !reverse) {
-      sortField.setMissingValue(SortField.STRING_LAST);
+      in.setMissingValue(SortField.STRING_LAST);
     }
-
-    return sortField;
   }
+    
 }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/ValueSourceParser.java lucene5666/solr/core/src/java/org/apache/solr/search/ValueSourceParser.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/ValueSourceParser.java	2014-05-14 03:47:28.934646626 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/search/ValueSourceParser.java	2014-05-14 03:45:16.842644326 -0400
@@ -42,6 +42,8 @@
 import org.apache.solr.schema.*;
 
 import org.apache.solr.search.function.CollapseScoreFunction;
+import org.apache.solr.search.function.OrdFieldSource;
+import org.apache.solr.search.function.ReverseOrdFieldSource;
 import org.apache.solr.search.function.distance.*;
 import org.apache.solr.util.plugin.NamedListInitializedPlugin;
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/update/DeleteByQueryWrapper.java lucene5666/solr/core/src/java/org/apache/solr/update/DeleteByQueryWrapper.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/update/DeleteByQueryWrapper.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/solr/core/src/java/org/apache/solr/update/DeleteByQueryWrapper.java	2014-05-14 03:45:17.142644331 -0400
@@ -0,0 +1,119 @@
+package org.apache.solr.update;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.Explanation;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Weight;
+import org.apache.lucene.uninverting.UninvertingReader;
+import org.apache.lucene.util.Bits;
+import org.apache.solr.schema.IndexSchema;
+
+/** 
+ * Allows access to uninverted docvalues by delete-by-queries.
+ * this is used e.g. to implement versioning constraints in solr.
+ * <p>
+ * Even though we wrap for each query, UninvertingReader's core 
+ * cache key is the inner one, so it still reuses fieldcaches and so on.
+ */
+final class DeleteByQueryWrapper extends Query {
+  final Query in;
+  final IndexSchema schema;
+  
+  DeleteByQueryWrapper(Query in, IndexSchema schema) {
+    this.in = in;
+    this.schema = schema;
+  }
+  
+  AtomicReader wrap(AtomicReader reader) {
+    return new UninvertingReader(reader, schema.getUninversionMap(reader));
+  }
+  
+  // we try to be well-behaved, but we are not (and IW's applyQueryDeletes isn't much better...)
+  
+  @Override
+  public Query rewrite(IndexReader reader) throws IOException {
+    Query rewritten = in.rewrite(reader);
+    if (rewritten != in) {
+      return new DeleteByQueryWrapper(rewritten, schema);
+    } else {
+      return this;
+    }
+  }
+  
+  @Override
+  public Weight createWeight(IndexSearcher searcher) throws IOException {
+    final AtomicReader wrapped = wrap((AtomicReader) searcher.getIndexReader());
+    final IndexSearcher privateContext = new IndexSearcher(wrapped);
+    final Weight inner = in.createWeight(privateContext);
+    return new Weight() {
+      @Override
+      public Explanation explain(AtomicReaderContext context, int doc) throws IOException { throw new UnsupportedOperationException(); }
+
+      @Override
+      public Query getQuery() { return DeleteByQueryWrapper.this; }
+
+      @Override
+      public float getValueForNormalization() throws IOException { return inner.getValueForNormalization(); }
+
+      @Override
+      public void normalize(float norm, float topLevelBoost) { inner.normalize(norm, topLevelBoost); }
+
+      @Override
+      public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+        return inner.scorer(privateContext.getIndexReader().leaves().get(0), acceptDocs);
+      }
+    };
+  }
+
+  @Override
+  public String toString(String field) {
+    return "Uninverting(" + in.toString(field) + ")";
+  }
+
+  @Override
+  public int hashCode() {
+    final int prime = 31;
+    int result = super.hashCode();
+    result = prime * result + ((in == null) ? 0 : in.hashCode());
+    result = prime * result + ((schema == null) ? 0 : schema.hashCode());
+    return result;
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (this == obj) return true;
+    if (!super.equals(obj)) return false;
+    if (getClass() != obj.getClass()) return false;
+    DeleteByQueryWrapper other = (DeleteByQueryWrapper) obj;
+    if (in == null) {
+      if (other.in != null) return false;
+    } else if (!in.equals(other.in)) return false;
+    if (schema == null) {
+      if (other.schema != null) return false;
+    } else if (!schema.equals(other.schema)) return false;
+    return true;
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java lucene5666/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java	2014-05-14 03:47:28.946646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java	2014-05-14 03:45:16.842644326 -0400
@@ -242,7 +242,7 @@
               bq.add(new BooleanClause(new TermQuery(updateTerm),
                   Occur.MUST_NOT));
               bq.add(new BooleanClause(new TermQuery(idTerm), Occur.MUST));
-              writer.deleteDocuments(bq);
+              writer.deleteDocuments(new DeleteByQueryWrapper(bq, core.getLatestSchema()));
             }
             
             // Add to the transaction log *after* successfully adding to the
@@ -402,7 +402,7 @@
         } else {
           RefCounted<IndexWriter> iw = solrCoreState.getIndexWriter(core);
           try {
-            iw.get().deleteDocuments(q);
+            iw.get().deleteDocuments(new DeleteByQueryWrapper(q, core.getLatestSchema()));
           } finally {
             iw.decref();
           }
@@ -440,7 +440,7 @@
             .getIndexAnalyzer());
         
         for (Query q : dbqList) {
-          writer.deleteDocuments(q);
+          writer.deleteDocuments(new DeleteByQueryWrapper(q, core.getLatestSchema()));
         }
       } finally {
         iw.decref();
@@ -833,7 +833,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5666/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/update/SolrIndexSplitter.java lucene5666/solr/core/src/java/org/apache/solr/update/SolrIndexSplitter.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/update/SolrIndexSplitter.java	2014-05-14 03:47:28.946646627 -0400
+++ lucene5666/solr/core/src/java/org/apache/solr/update/SolrIndexSplitter.java	2014-05-14 03:45:17.142644331 -0400
@@ -89,7 +89,7 @@
 
   public void split() throws IOException {
 
-    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();
+    List<AtomicReaderContext> leaves = searcher.getRawReader().leaves();
     List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());
 
     log.info("SolrIndexSplitter: partitions=" + numPieces + " segments="+leaves.size());


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/test/org/apache/solr/core/TestMergePolicyConfig.java lucene5666/solr/core/src/test/org/apache/solr/core/TestMergePolicyConfig.java
--- lucene-trunk/solr/core/src/test/org/apache/solr/core/TestMergePolicyConfig.java	2014-05-14 03:47:28.918646626 -0400
+++ lucene5666/solr/core/src/test/org/apache/solr/core/TestMergePolicyConfig.java	2014-05-14 03:45:16.770644325 -0400
@@ -196,7 +196,7 @@
   public static void assertCompoundSegments(SolrCore core, boolean compound) {
     RefCounted<SolrIndexSearcher> searcherRef = core.getRegisteredSearcher();
     try {
-      assertCompoundSegments(searcherRef.get().getIndexReader(), compound);
+      assertCompoundSegments(searcherRef.get().getRawReader(), compound);
     } finally {
       searcherRef.decref();
     }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/test/org/apache/solr/core/TestNonNRTOpen.java lucene5666/solr/core/src/test/org/apache/solr/core/TestNonNRTOpen.java
--- lucene-trunk/solr/core/src/test/org/apache/solr/core/TestNonNRTOpen.java	2014-05-14 03:47:28.918646626 -0400
+++ lucene5666/solr/core/src/test/org/apache/solr/core/TestNonNRTOpen.java	2014-05-14 03:45:16.770644325 -0400
@@ -138,7 +138,7 @@
     RefCounted<SolrIndexSearcher> searcher = core.getSearcher();
     try {
       SolrIndexSearcher s = searcher.get();
-      DirectoryReader ir = s.getIndexReader();
+      DirectoryReader ir = s.getRawReader();
       assertEquals("SOLR-5815? : wrong maxDoc: core=" + core.toString() +" searcher=" + s.toString(),
                    maxDoc, ir.maxDoc());
       assertFalse("SOLR-5815? : expected non-NRT reader, got: " + ir, ir.toString().contains(":nrt"));
@@ -151,7 +151,7 @@
     RefCounted<SolrIndexSearcher> searcher = h.getCore().getSearcher();
     Set<Object> set = Collections.newSetFromMap(new IdentityHashMap<Object,Boolean>());
     try {
-      DirectoryReader ir = searcher.get().getIndexReader();
+      DirectoryReader ir = searcher.get().getRawReader();
       for (AtomicReaderContext context : ir.leaves()) {
         set.add(context.reader().getCoreCacheKey());
       }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/test/org/apache/solr/core/TestNRTOpen.java lucene5666/solr/core/src/test/org/apache/solr/core/TestNRTOpen.java
--- lucene-trunk/solr/core/src/test/org/apache/solr/core/TestNRTOpen.java	2014-05-14 03:47:28.918646626 -0400
+++ lucene5666/solr/core/src/test/org/apache/solr/core/TestNRTOpen.java	2014-05-14 03:45:16.766644325 -0400
@@ -129,7 +129,7 @@
   static void assertNRT(int maxDoc) {
     RefCounted<SolrIndexSearcher> searcher = h.getCore().getSearcher();
     try {
-      DirectoryReader ir = searcher.get().getIndexReader();
+      DirectoryReader ir = searcher.get().getRawReader();
       assertEquals(maxDoc, ir.maxDoc());
       assertTrue("expected NRT reader, got: " + ir, ir.toString().contains(":nrt"));
     } finally {
@@ -141,7 +141,7 @@
     RefCounted<SolrIndexSearcher> searcher = h.getCore().getSearcher();
     Set<Object> set = Collections.newSetFromMap(new IdentityHashMap<Object,Boolean>());
     try {
-      DirectoryReader ir = searcher.get().getIndexReader();
+      DirectoryReader ir = searcher.get().getRawReader();
       for (AtomicReaderContext context : ir.leaves()) {
         set.add(context.reader().getCoreCacheKey());
       }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/test/org/apache/solr/request/TestFaceting.java lucene5666/solr/core/src/test/org/apache/solr/request/TestFaceting.java
--- lucene-trunk/solr/core/src/test/org/apache/solr/request/TestFaceting.java	2014-05-14 03:47:28.918646626 -0400
+++ lucene5666/solr/core/src/test/org/apache/solr/request/TestFaceting.java	2014-05-14 03:45:16.782644325 -0400
@@ -22,9 +22,12 @@
 import java.util.Locale;
 import java.util.Random;
 
-import org.apache.lucene.index.DocTermOrds;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedSetDocValues;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.uninverting.DocTermOrds;
 import org.apache.lucene.util.BytesRef;
 import org.apache.solr.SolrTestCaseJ4;
 import org.apache.solr.common.params.FacetParams;
@@ -81,12 +84,11 @@
     createIndex(size);
     req = lrf.makeRequest("q","*:*");
 
-    UnInvertedField uif = new UnInvertedField(proto.field(), req.getSearcher());
+    SortedSetDocValues dv = DocValues.getSortedSet(req.getSearcher().getAtomicReader(), proto.field());
 
-    assertEquals(size, uif.getNumTerms());
+    assertEquals(size, dv.getValueCount());
 
-    TermsEnum te = uif.getOrdTermsEnum(req.getSearcher().getAtomicReader());
-    assertEquals(size == 0, te == null);
+    TermsEnum te = dv.termsEnum();
 
     Random r = new Random(size);
     // test seeking by term string
@@ -763,16 +765,16 @@
     RefCounted<SolrIndexSearcher> currentSearcherRef = h.getCore().getSearcher();
     try {
       SolrIndexSearcher currentSearcher = currentSearcherRef.get();
-      UnInvertedField ui0 = UnInvertedField.getUnInvertedField("f0_ws", currentSearcher);
-      UnInvertedField ui1 = UnInvertedField.getUnInvertedField("f1_ws", currentSearcher);
-      UnInvertedField ui2 = UnInvertedField.getUnInvertedField("f2_ws", currentSearcher);
-      UnInvertedField ui3 = UnInvertedField.getUnInvertedField("f3_ws", currentSearcher);
-      UnInvertedField ui4 = UnInvertedField.getUnInvertedField("f4_ws", currentSearcher);
-      UnInvertedField ui5 = UnInvertedField.getUnInvertedField("f5_ws", currentSearcher);
-      UnInvertedField ui6 = UnInvertedField.getUnInvertedField("f6_ws", currentSearcher);
-      UnInvertedField ui7 = UnInvertedField.getUnInvertedField("f7_ws", currentSearcher);
-      UnInvertedField ui8 = UnInvertedField.getUnInvertedField("f8_ws", currentSearcher);
-      UnInvertedField ui9 = UnInvertedField.getUnInvertedField("f9_ws", currentSearcher);
+      SortedSetDocValues ui0 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f0_ws");
+      SortedSetDocValues ui1 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f1_ws");
+      SortedSetDocValues ui2 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f2_ws");
+      SortedSetDocValues ui3 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f3_ws");
+      SortedSetDocValues ui4 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f4_ws");
+      SortedSetDocValues ui5 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f5_ws");
+      SortedSetDocValues ui6 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f6_ws");
+      SortedSetDocValues ui7 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f7_ws");
+      SortedSetDocValues ui8 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f8_ws");
+      SortedSetDocValues ui9 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f9_ws");
 
       assertQ("check threading, more threads than fields",
           req("q", "id:*", "indent", "true", "fl", "id", "rows", "1"
@@ -924,28 +926,39 @@
       // Now, are all the UnInvertedFields still the same? Meaning they weren't re-fetched even when a bunch were
       // requested at the same time?
       assertEquals("UnInvertedField coming back from the seacher should not have changed! ",
-          ui0, UnInvertedField.getUnInvertedField("f0_ws", currentSearcher));
+          ui0, DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f0_ws"));
       assertEquals("UnInvertedField coming back from the seacher should not have changed! ",
-          ui1, UnInvertedField.getUnInvertedField("f1_ws", currentSearcher));
+          ui1, DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f1_ws"));
       assertEquals("UnInvertedField coming back from the seacher should not have changed! ",
-          ui2, UnInvertedField.getUnInvertedField("f2_ws", currentSearcher));
+          ui2, DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f2_ws"));
       assertEquals("UnInvertedField coming back from the seacher should not have changed! ",
-          ui3, UnInvertedField.getUnInvertedField("f3_ws", currentSearcher));
+          ui3, DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f3_ws"));
       assertEquals("UnInvertedField coming back from the seacher should not have changed! ",
-          ui4, UnInvertedField.getUnInvertedField("f4_ws", currentSearcher));
+          ui4, DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f4_ws"));
       assertEquals("UnInvertedField coming back from the seacher should not have changed! ",
-          ui5, UnInvertedField.getUnInvertedField("f5_ws", currentSearcher));
+          ui5, DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f5_ws"));
       assertEquals("UnInvertedField coming back from the seacher should not have changed! ",
-          ui6, UnInvertedField.getUnInvertedField("f6_ws", currentSearcher));
+          ui6, DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f6_ws"));
       assertEquals("UnInvertedField coming back from the seacher should not have changed! ",
-          ui7, UnInvertedField.getUnInvertedField("f7_ws", currentSearcher));
+          ui7, DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f7_ws"));
       assertEquals("UnInvertedField coming back from the seacher should not have changed! ",
-          ui8, UnInvertedField.getUnInvertedField("f8_ws", currentSearcher));
+          ui8, DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f8_ws"));
       assertEquals("UnInvertedField coming back from the seacher should not have changed! ",
-          ui9, UnInvertedField.getUnInvertedField("f9_ws", currentSearcher));
+          ui9, DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f9_ws"));
     } finally {
       currentSearcherRef.decref();
     }
   }
+  
+  // assert same instance: either same object, or both wrapping same single-valued object
+  private void assertEquals(String msg, SortedSetDocValues dv1, SortedSetDocValues dv2) {
+    SortedDocValues singleton1 = DocValues.unwrapSingleton(dv1);
+    SortedDocValues singleton2 = DocValues.unwrapSingleton(dv2);
+    if (singleton1 == null || singleton2 == null) {
+      assertSame(dv1, dv2);
+    } else {
+      assertSame(singleton1, singleton2);
+    }
+  }
 }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/test/org/apache/solr/search/function/SortByFunctionTest.java lucene5666/solr/core/src/test/org/apache/solr/search/function/SortByFunctionTest.java
--- lucene-trunk/solr/core/src/test/org/apache/solr/search/function/SortByFunctionTest.java	2014-05-14 03:47:28.926646626 -0400
+++ lucene5666/solr/core/src/test/org/apache/solr/search/function/SortByFunctionTest.java	2014-05-14 03:45:16.774644325 -0400
@@ -99,11 +99,11 @@
   
   public void testSortJoinDocFreq() throws Exception
   {
-    assertU(adoc("id", "4", "id_s", "D", "links_mfacet", "A", "links_mfacet", "B", "links_mfacet", "C" ) );
-    assertU(adoc("id", "3", "id_s", "C", "links_mfacet", "A", "links_mfacet", "B" ) );
+    assertU(adoc("id", "4", "id_s1", "D", "links_mfacet", "A", "links_mfacet", "B", "links_mfacet", "C" ) );
+    assertU(adoc("id", "3", "id_s1", "C", "links_mfacet", "A", "links_mfacet", "B" ) );
     assertU(commit()); // Make sure it uses two readers
-    assertU(adoc("id", "2", "id_s", "B", "links_mfacet", "A" ) );
-    assertU(adoc("id", "1", "id_s", "A"  ) );
+    assertU(adoc("id", "2", "id_s1", "B", "links_mfacet", "A" ) );
+    assertU(adoc("id", "1", "id_s1", "A"  ) );
     assertU(commit());
 
     assertQ(req("q", "links_mfacet:B", "fl", "id", "sort", "id asc"),
@@ -112,7 +112,7 @@
             "//result/doc[2]/int[@name='id'][.='4']"
     );
     
-    assertQ(req("q", "*:*", "fl", "id", "sort", "joindf(id_s, links_mfacet) desc"),
+    assertQ(req("q", "*:*", "fl", "id", "sort", "joindf(id_s1, links_mfacet) desc"),
             "//*[@numFound='4']",
             "//result/doc[1]/int[@name='id'][.='1']",
             "//result/doc[2]/int[@name='id'][.='2']",
@@ -120,7 +120,7 @@
             "//result/doc[4]/int[@name='id'][.='4']"
     );
 
-    assertQ(req("q", "*:*", "fl", "id", "sort", "joindf(id_s, links_mfacet) asc"),
+    assertQ(req("q", "*:*", "fl", "id", "sort", "joindf(id_s1, links_mfacet) asc"),
             "//*[@numFound='4']",
             "//result/doc[1]/int[@name='id'][.='4']",
             "//result/doc[2]/int[@name='id'][.='3']",


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/test/org/apache/solr/search/function/TestFunctionQuery.java lucene5666/solr/core/src/test/org/apache/solr/search/function/TestFunctionQuery.java
--- lucene-trunk/solr/core/src/test/org/apache/solr/search/function/TestFunctionQuery.java	2014-05-14 03:47:28.926646626 -0400
+++ lucene5666/solr/core/src/test/org/apache/solr/search/function/TestFunctionQuery.java	2014-05-14 03:45:16.774644325 -0400
@@ -19,7 +19,6 @@
 
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.index.FieldInvertState;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.similarities.DefaultSimilarity;
 import org.apache.lucene.search.similarities.TFIDFSimilarity;
 import org.apache.solr.SolrTestCaseJ4;
@@ -202,8 +201,6 @@
     singleTest(field,"sum(query($v1,5),query($v1,7))",
             Arrays.asList("v1","\0:[* TO *]"),  88,12
             );
-
-    FieldCache.DEFAULT.purgeAllCaches();   // avoid FC insanity
   }
 
   @Test
@@ -283,9 +280,7 @@
 
       singleTest(field, "\0", answers);
       // System.out.println("Done test "+i);
-    }
-
-    FieldCache.DEFAULT.purgeAllCaches();   // avoid FC insanity    
+    }  
   }
 
   @Test
@@ -422,9 +417,6 @@
            ,"*//doc[1]/float[.='120.0']"
            ,"*//doc[2]/float[.='121.0']"
     );
-
-
-    FieldCache.DEFAULT.purgeAllCaches();   // avoid FC insanity
   }
 
   /**
@@ -640,9 +632,7 @@
     assertU(adoc("id", "10000")); // will get same reader if no index change
     assertU(commit());   
     singleTest(fieldAsFunc, "sqrt(\0)");
-    assertTrue(orig != FileFloatSource.onlyForTesting);
-
-    FieldCache.DEFAULT.purgeAllCaches();   // avoid FC insanity    
+    assertTrue(orig != FileFloatSource.onlyForTesting);  
   }
 
   /**
@@ -667,9 +657,7 @@
                100,100,  -4,-4,  0,0,  10,10,  25,25,  5,5,  77,77,  1,1);
     singleTest(fieldAsFunc, "sqrt(\0)", 
                100,10,  25,5,  0,0,   1,1);
-    singleTest(fieldAsFunc, "log(\0)",  1,0);
-
-    FieldCache.DEFAULT.purgeAllCaches();   // avoid FC insanity    
+    singleTest(fieldAsFunc, "log(\0)",  1,0); 
   }
 
     @Test


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/test/org/apache/solr/search/function/TestOrdValues.java lucene5666/solr/core/src/test/org/apache/solr/search/function/TestOrdValues.java
--- lucene-trunk/solr/core/src/test/org/apache/solr/search/function/TestOrdValues.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene5666/solr/core/src/test/org/apache/solr/search/function/TestOrdValues.java	2014-05-14 03:45:16.774644325 -0400
@@ -0,0 +1,310 @@
+package org.apache.solr.search.function;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.FloatField;
+import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.queries.function.FunctionQuery;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.valuesource.FloatFieldSource;
+import org.apache.lucene.queries.function.valuesource.IntFieldSource;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.QueryUtils;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+/**
+ * Test search based on OrdFieldSource and ReverseOrdFieldSource.
+ * <p/>
+ * Tests here create an index with a few documents, each having
+ * an indexed "id" field.
+ * The ord values of this field are later used for scoring.
+ * <p/>
+ * The order tests use Hits to verify that docs are ordered as expected.
+ * <p/>
+ * The exact score tests use TopDocs top to verify the exact score.
+ */
+public class TestOrdValues extends LuceneTestCase {
+
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    createIndex(false);
+  }
+
+  /**
+   * Test OrdFieldSource
+   */
+  @Test
+  public void testOrdFieldRank() throws Exception {
+    doTestRank(ID_FIELD, true);
+  }
+
+  /**
+   * Test ReverseOrdFieldSource
+   */
+  @Test
+  public void testReverseOrdFieldRank() throws Exception {
+    doTestRank(ID_FIELD, false);
+  }
+
+  // Test that queries based on reverse/ordFieldScore scores correctly
+  private void doTestRank(String field, boolean inOrder) throws Exception {
+    IndexReader r = DirectoryReader.open(dir);
+    IndexSearcher s = newSearcher(r);
+    ValueSource vs;
+    if (inOrder) {
+      vs = new OrdFieldSource(field);
+    } else {
+      vs = new ReverseOrdFieldSource(field);
+    }
+
+    Query q = new FunctionQuery(vs);
+    log("test: " + q);
+    QueryUtils.check(random(), q, s);
+    ScoreDoc[] h = s.search(q, null, 1000).scoreDocs;
+    assertEquals("All docs should be matched!", N_DOCS, h.length);
+    String prevID = inOrder
+            ? "IE"   // greater than all ids of docs in this test ("ID0001", etc.)
+            : "IC";  // smaller than all ids of docs in this test ("ID0001", etc.)
+
+    for (int i = 0; i < h.length; i++) {
+      String resID = s.doc(h[i].doc).get(ID_FIELD);
+      log(i + ".   score=" + h[i].score + "  -  " + resID);
+      log(s.explain(q, h[i].doc));
+      if (inOrder) {
+        assertTrue("res id " + resID + " should be < prev res id " + prevID, resID.compareTo(prevID) < 0);
+      } else {
+        assertTrue("res id " + resID + " should be > prev res id " + prevID, resID.compareTo(prevID) > 0);
+      }
+      prevID = resID;
+    }
+    r.close();
+  }
+
+  /**
+   * Test exact score for OrdFieldSource
+   */
+  @Test
+  public void testOrdFieldExactScore() throws Exception {
+    doTestExactScore(ID_FIELD, true);
+  }
+
+  /**
+   * Test exact score for ReverseOrdFieldSource
+   */
+  @Test
+  public void testReverseOrdFieldExactScore() throws Exception {
+    doTestExactScore(ID_FIELD, false);
+  }
+
+
+  // Test that queries based on reverse/ordFieldScore returns docs with expected score.
+  private void doTestExactScore(String field, boolean inOrder) throws Exception {
+    IndexReader r = DirectoryReader.open(dir);
+    IndexSearcher s = newSearcher(r);
+    ValueSource vs;
+    if (inOrder) {
+      vs = new OrdFieldSource(field);
+    } else {
+      vs = new ReverseOrdFieldSource(field);
+    }
+    Query q = new FunctionQuery(vs);
+    TopDocs td = s.search(q, null, 1000);
+    assertEquals("All docs should be matched!", N_DOCS, td.totalHits);
+    ScoreDoc sd[] = td.scoreDocs;
+    for (int i = 0; i < sd.length; i++) {
+      float score = sd[i].score;
+      String id = s.getIndexReader().document(sd[i].doc).get(ID_FIELD);
+      log("-------- " + i + ". Explain doc " + id);
+      log(s.explain(q, sd[i].doc));
+      float expectedScore = N_DOCS - i - 1;
+      assertEquals("score of result " + i + " shuould be " + expectedScore + " != " + score, expectedScore, score, TEST_SCORE_TOLERANCE_DELTA);
+      String expectedId = inOrder
+              ? id2String(N_DOCS - i) // in-order ==> larger  values first
+              : id2String(i + 1);     // reverse  ==> smaller values first
+      assertTrue("id of result " + i + " shuould be " + expectedId + " != " + score, expectedId.equals(id));
+    }
+    r.close();
+  }
+  
+  // LUCENE-1250
+  public void testEqualsNull() throws Exception {
+    OrdFieldSource ofs = new OrdFieldSource("f");
+    assertFalse(ofs.equals(null));
+    
+    ReverseOrdFieldSource rofs = new ReverseOrdFieldSource("f");
+    assertFalse(rofs.equals(null));
+  }
+  
+  /**
+   * Actual score computation order is slightly different than assumptios
+   * this allows for a small amount of variation
+   */
+  protected static float TEST_SCORE_TOLERANCE_DELTA = 0.001f;
+
+  protected static final int N_DOCS = 17; // select a primary number > 2
+
+  protected static final String ID_FIELD = "id";
+  protected static final String TEXT_FIELD = "text";
+  protected static final String INT_FIELD = "iii";
+  protected static final String FLOAT_FIELD = "fff";
+
+  protected ValueSource INT_VALUESOURCE = new IntFieldSource(INT_FIELD);
+  protected ValueSource FLOAT_VALUESOURCE = new FloatFieldSource(FLOAT_FIELD);
+
+  private static final String DOC_TEXT_LINES[] = {
+          "Well, this is just some plain text we use for creating the ",
+          "test documents. It used to be a text from an online collection ",
+          "devoted to first aid, but if there was there an (online) lawyers ",
+          "first aid collection with legal advices, \"it\" might have quite ",
+          "probably advised one not to include \"it\"'s text or the text of ",
+          "any other online collection in one's code, unless one has money ",
+          "that one don't need and one is happy to donate for lawyers ",
+          "charity. Anyhow at some point, rechecking the usage of this text, ",
+          "it became uncertain that this text is free to use, because ",
+          "the web site in the disclaimer of he eBook containing that text ",
+          "was not responding anymore, and at the same time, in projGut, ",
+          "searching for first aid no longer found that eBook as well. ",
+          "So here we are, with a perhaps much less interesting ",
+          "text for the test, but oh much much safer. ",
+  };
+
+  protected static Directory dir;
+  protected static Analyzer anlzr;
+
+  @AfterClass
+  public static void afterClassFunctionTestSetup() throws Exception {
+    dir.close();
+    dir = null;
+    anlzr = null;
+  }
+
+  protected static void createIndex(boolean doMultiSegment) throws Exception {
+    if (VERBOSE) {
+      System.out.println("TEST: setUp");
+    }
+    // prepare a small index with just a few documents.
+    dir = newDirectory();
+    anlzr = new MockAnalyzer(random());
+    IndexWriterConfig iwc = newIndexWriterConfig( TEST_VERSION_CURRENT, anlzr).setMergePolicy(newLogMergePolicy());
+    if (doMultiSegment) {
+      iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 2, 7));
+    }
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    // add docs not exactly in natural ID order, to verify we do check the order of docs by scores
+    int remaining = N_DOCS;
+    boolean done[] = new boolean[N_DOCS];
+    int i = 0;
+    while (remaining > 0) {
+      if (done[i]) {
+        throw new Exception("to set this test correctly N_DOCS=" + N_DOCS + " must be primary and greater than 2!");
+      }
+      addDoc(iw, i);
+      done[i] = true;
+      i = (i + 4) % N_DOCS;
+      remaining --;
+    }
+    if (!doMultiSegment) {
+      if (VERBOSE) {
+        System.out.println("TEST: setUp full merge");
+      }
+      iw.forceMerge(1);
+    }
+    iw.shutdown();
+    if (VERBOSE) {
+      System.out.println("TEST: setUp done close");
+    }
+  }
+
+  private static void addDoc(RandomIndexWriter iw, int i) throws Exception {
+    Document d = new Document();
+    Field f;
+    int scoreAndID = i + 1;
+
+    FieldType customType = new FieldType(TextField.TYPE_STORED);
+    customType.setTokenized(false);
+    customType.setOmitNorms(true);
+    
+    f = newField(ID_FIELD, id2String(scoreAndID), customType); // for debug purposes
+    d.add(f);
+    d.add(new SortedDocValuesField(ID_FIELD, new BytesRef(id2String(scoreAndID))));
+
+    FieldType customType2 = new FieldType(TextField.TYPE_NOT_STORED);
+    customType2.setOmitNorms(true);
+    f = newField(TEXT_FIELD, "text of doc" + scoreAndID + textLine(i), customType2); // for regular search
+    d.add(f);
+
+    f = new IntField(INT_FIELD, scoreAndID, Store.YES); // for function scoring
+    d.add(f);
+    d.add(new NumericDocValuesField(INT_FIELD, scoreAndID));
+
+    f = new FloatField(FLOAT_FIELD, scoreAndID, Store.YES); // for function scoring
+    d.add(f);
+    d.add(new NumericDocValuesField(FLOAT_FIELD, Float.floatToRawIntBits(scoreAndID)));
+
+    iw.addDocument(d);
+    log("added: " + d);
+  }
+
+  // 17 --> ID00017
+  protected static String id2String(int scoreAndID) {
+    String s = "000000000" + scoreAndID;
+    int n = ("" + N_DOCS).length() + 3;
+    int k = s.length() - n;
+    return "ID" + s.substring(k);
+  }
+
+  // some text line for regular search
+  private static String textLine(int docNum) {
+    return DOC_TEXT_LINES[docNum % DOC_TEXT_LINES.length];
+  }
+
+  // extract expected doc score from its ID Field: "ID7" --> 7.0
+  protected static float expectedFieldScore(String docIDFieldVal) {
+    return Float.parseFloat(docIDFieldVal.substring(2));
+  }
+
+  // debug messages (change DBG to true for anything to print)
+  protected static void log(Object o) {
+    if (VERBOSE) {
+      System.out.println(o.toString());
+    }
+  }
+
+}


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/test/org/apache/solr/search/TestIndexSearcher.java lucene5666/solr/core/src/test/org/apache/solr/search/TestIndexSearcher.java
--- lucene-trunk/solr/core/src/test/org/apache/solr/search/TestIndexSearcher.java	2014-05-14 03:47:28.922646626 -0400
+++ lucene5666/solr/core/src/test/org/apache/solr/search/TestIndexSearcher.java	2014-05-14 03:45:16.782644325 -0400
@@ -16,6 +16,7 @@
  */
 package org.apache.solr.search;
 
+import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.LogDocMergePolicy;
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReaderContext;
@@ -71,7 +72,7 @@
     assertU(commit());
 
     SolrQueryRequest sr1 = req("q","foo");
-    IndexReaderContext rCtx1 = sr1.getSearcher().getTopReaderContext();
+    IndexReader r1 = sr1.getSearcher().getRawReader();
 
     String sval1 = getStringVal(sr1, "v_s1",0);
     assertEquals("string1", sval1);
@@ -81,28 +82,28 @@
     assertU(commit());
 
     SolrQueryRequest sr2 = req("q","foo");
-    IndexReaderContext rCtx2 = sr2.getSearcher().getTopReaderContext();
+    IndexReader r2 = sr2.getSearcher().getRawReader();
 
     // make sure the readers share the first segment
     // Didn't work w/ older versions of lucene2.9 going from segment -> multi
-    assertEquals(rCtx1.leaves().get(0).reader(), rCtx2.leaves().get(0).reader());
+    assertEquals(r1.leaves().get(0).reader(), r2.leaves().get(0).reader());
 
     assertU(adoc("id","5", "v_f","3.14159"));
     assertU(adoc("id","6", "v_f","8983", "v_s1","string6"));
     assertU(commit());
 
     SolrQueryRequest sr3 = req("q","foo");
-    IndexReaderContext rCtx3 = sr3.getSearcher().getTopReaderContext();
+    IndexReader r3 = sr3.getSearcher().getRawReader();
     // make sure the readers share segments
     // assertEquals(r1.getLeafReaders()[0], r3.getLeafReaders()[0]);
-    assertEquals(rCtx2.leaves().get(0).reader(), rCtx3.leaves().get(0).reader());
-    assertEquals(rCtx2.leaves().get(1).reader(), rCtx3.leaves().get(1).reader());
+    assertEquals(r2.leaves().get(0).reader(), r3.leaves().get(0).reader());
+    assertEquals(r2.leaves().get(1).reader(), r3.leaves().get(1).reader());
 
     sr1.close();
     sr2.close();            
 
     // should currently be 1, but this could change depending on future index management
-    int baseRefCount = rCtx3.reader().getRefCount();
+    int baseRefCount = r3.getRefCount();
     assertEquals(1, baseRefCount);
 
     Object sr3SearcherRegAt = sr3.getSearcher().getStatistics().get("registeredAt");
@@ -112,7 +113,7 @@
                sr3.getSearcher(), sr4.getSearcher());
     assertEquals("nothing changed, searcher should not have been re-registered",
                  sr3SearcherRegAt, sr4.getSearcher().getStatistics().get("registeredAt"));
-    IndexReaderContext rCtx4 = sr4.getSearcher().getTopReaderContext();
+    IndexReader r4 = sr4.getSearcher().getRawReader();
 
     // force an index change so the registered searcher won't be the one we are testing (and
     // then we should be able to test the refCount going all the way to 0
@@ -120,12 +121,12 @@
     assertU(commit()); 
 
     // test that reader didn't change
-    assertSame(rCtx3.reader(), rCtx4.reader());
-    assertEquals(baseRefCount, rCtx4.reader().getRefCount());
+    assertSame(r3, r4);
+    assertEquals(baseRefCount, r4.getRefCount());
     sr3.close();
-    assertEquals(baseRefCount, rCtx4.reader().getRefCount());
+    assertEquals(baseRefCount, r4.getRefCount());
     sr4.close();
-    assertEquals(baseRefCount-1, rCtx4.reader().getRefCount());
+    assertEquals(baseRefCount-1, r4.getRefCount());
 
 
     SolrQueryRequest sr5 = req("q","foo");


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/test/org/apache/solr/search/TestSort.java lucene5666/solr/core/src/test/org/apache/solr/search/TestSort.java
--- lucene-trunk/solr/core/src/test/org/apache/solr/search/TestSort.java	2014-05-14 03:47:28.922646626 -0400
+++ lucene5666/solr/core/src/test/org/apache/solr/search/TestSort.java	2014-05-14 03:45:16.774644325 -0400
@@ -21,7 +21,9 @@
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.Comparator;
+import java.util.HashMap;
 import java.util.List;
+import java.util.Map;
 import java.util.Random;
 
 import org.apache.lucene.analysis.core.SimpleAnalyzer;
@@ -50,6 +52,7 @@
 import org.apache.lucene.search.TopFieldCollector;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.uninverting.UninvertingReader;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.TestUtil;
@@ -221,8 +224,11 @@
       }
       iw.shutdown();
 
+      Map<String,UninvertingReader.Type> mapping = new HashMap<>();
+      mapping.put("f", UninvertingReader.Type.SORTED);
+      mapping.put("f2", UninvertingReader.Type.SORTED);
 
-      DirectoryReader reader = DirectoryReader.open(dir);
+      DirectoryReader reader = UninvertingReader.wrap(DirectoryReader.open(dir), mapping);
       IndexSearcher searcher = new IndexSearcher(reader);
       // System.out.println("segments="+searcher.getIndexReader().getSequentialSubReaders().length);
       assertTrue(reader.leaves().size() > 1);


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/test/org/apache/solr/TestDistributedSearch.java lucene5666/solr/core/src/test/org/apache/solr/TestDistributedSearch.java
--- lucene-trunk/solr/core/src/test/org/apache/solr/TestDistributedSearch.java	2014-05-14 03:47:28.918646626 -0400
+++ lucene5666/solr/core/src/test/org/apache/solr/TestDistributedSearch.java	2014-05-14 03:45:16.782644325 -0400
@@ -23,7 +23,6 @@
 import java.util.Map;
 
 import org.apache.commons.lang.StringUtils;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.apache.solr.client.solrj.SolrServer;
 import org.apache.solr.client.solrj.SolrServerException;
@@ -422,8 +421,6 @@
     
     // Thread.sleep(10000000000L);
 
-    FieldCache.DEFAULT.purgeAllCaches();   // avoid FC insanity
-
     del("*:*"); // delete all docs and test stats request
     commit();
     try {


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/test/org/apache/solr/TestGroupingSearch.java lucene5666/solr/core/src/test/org/apache/solr/TestGroupingSearch.java
--- lucene-trunk/solr/core/src/test/org/apache/solr/TestGroupingSearch.java	2014-05-14 03:47:28.922646626 -0400
+++ lucene5666/solr/core/src/test/org/apache/solr/TestGroupingSearch.java	2014-05-14 03:45:16.782644325 -0400
@@ -17,7 +17,6 @@
 
 package org.apache.solr;
 
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.index.LogDocMergePolicy;
 import org.noggit.JSONUtil;
 import org.noggit.ObjectBuilder;
@@ -518,7 +517,6 @@
       ,"/grouped/"+f+"/matches==10"
       ,"/facet_counts/facet_fields/"+f+"==['1',3, '2',3, '3',2, '4',1, '5',1]"
     );
-    FieldCache.DEFAULT.purgeAllCaches();   // avoid FC insanity
 
     // test that grouping works with highlighting
     assertJQ(req("fq",filt,  "q","{!func}"+f2, "group","true", "group.field",f, "fl","id"


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/test/org/apache/solr/TestRandomDVFaceting.java lucene5666/solr/core/src/test/org/apache/solr/TestRandomDVFaceting.java
--- lucene-trunk/solr/core/src/test/org/apache/solr/TestRandomDVFaceting.java	2014-05-14 03:47:28.922646626 -0400
+++ lucene5666/solr/core/src/test/org/apache/solr/TestRandomDVFaceting.java	2014-05-14 03:45:16.782644325 -0400
@@ -23,7 +23,6 @@
 import java.util.Map;
 import java.util.Random;
 
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.TestUtil;
 import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
@@ -114,27 +113,23 @@
 
   @Test
   public void testRandomFaceting() throws Exception {
-    try {
-      Random rand = random();
-      int iter = atLeast(100);
-      init();
-      addMoreDocs(0);
-
-      for (int i=0; i<iter; i++) {
-        doFacetTests();
-
-        if (rand.nextInt(100) < 5) {
-          init();
-        }
-
-        addMoreDocs(rand.nextInt(indexSize) + 1);
-
-        if (rand.nextInt(100) < 50) {
-          deleteSomeDocs();
-        }
+    Random rand = random();
+    int iter = atLeast(100);
+    init();
+    addMoreDocs(0);
+    
+    for (int i=0; i<iter; i++) {
+      doFacetTests();
+      
+      if (rand.nextInt(100) < 5) {
+        init();
+      }
+      
+      addMoreDocs(rand.nextInt(indexSize) + 1);
+      
+      if (rand.nextInt(100) < 50) {
+        deleteSomeDocs();
       }
-    } finally {
-      FieldCache.DEFAULT.purgeAllCaches();   // avoid FC insanity
     }
   }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/test/org/apache/solr/TestRandomFaceting.java lucene5666/solr/core/src/test/org/apache/solr/TestRandomFaceting.java
--- lucene-trunk/solr/core/src/test/org/apache/solr/TestRandomFaceting.java	2014-05-14 03:47:28.918646626 -0400
+++ lucene5666/solr/core/src/test/org/apache/solr/TestRandomFaceting.java	2014-05-14 03:45:16.774644325 -0400
@@ -17,7 +17,6 @@
 
 package org.apache.solr;
 
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.TestUtil;
 import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.apache.solr.common.params.ModifiableSolrParams;
@@ -113,27 +112,23 @@
 
   @Test
   public void testRandomFaceting() throws Exception {
-    try {
-      Random rand = random();
-      int iter = atLeast(100);
-      init();
-      addMoreDocs(0);
-
-      for (int i=0; i<iter; i++) {
-        doFacetTests();
-
-        if (rand.nextInt(100) < 5) {
-          init();
-        }
-
-        addMoreDocs(rand.nextInt(indexSize) + 1);
-
-        if (rand.nextInt(100) < 50) {
-          deleteSomeDocs();
-        }
+    Random rand = random();
+    int iter = atLeast(100);
+    init();
+    addMoreDocs(0);
+    
+    for (int i=0; i<iter; i++) {
+      doFacetTests();
+      
+      if (rand.nextInt(100) < 5) {
+        init();
+      }
+      
+      addMoreDocs(rand.nextInt(indexSize) + 1);
+      
+      if (rand.nextInt(100) < 50) {
+        deleteSomeDocs();
       }
-    } finally {
-      FieldCache.DEFAULT.purgeAllCaches();   // avoid FC insanity
     }
   }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/test-files/solr/collection1/conf/schema.xml lucene5666/solr/core/src/test-files/solr/collection1/conf/schema.xml
--- lucene-trunk/solr/core/src/test-files/solr/collection1/conf/schema.xml	2014-05-14 03:47:28.926646626 -0400
+++ lucene5666/solr/core/src/test-files/solr/collection1/conf/schema.xml	2014-05-14 03:45:16.782644325 -0400
@@ -500,7 +500,7 @@
 
 
    <field name="cat" type="string" indexed="true" stored="true" multiValued="true"/>
-   <field name="price"  type="float" indexed="true" stored="true"/>
+   <field name="price"  type="float" indexed="true" stored="true" multiValued="false"/>
    <field name="inStock" type="boolean" indexed="true" stored="true" />
 
    <field name="subword" type="subword" indexed="true" stored="true"/>


diff -ruN -x .svn -x build lucene-trunk/solr/test-framework/src/java/org/apache/solr/BaseDistributedSearchTestCase.java lucene5666/solr/test-framework/src/java/org/apache/solr/BaseDistributedSearchTestCase.java
--- lucene-trunk/solr/test-framework/src/java/org/apache/solr/BaseDistributedSearchTestCase.java	2014-05-14 03:47:28.978646627 -0400
+++ lucene5666/solr/test-framework/src/java/org/apache/solr/BaseDistributedSearchTestCase.java	2014-05-14 03:45:16.722644324 -0400
@@ -35,7 +35,6 @@
 import junit.framework.Assert;
 
 import org.apache.commons.io.FileUtils;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.Constants;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
@@ -284,7 +283,6 @@
   @Override
   public void tearDown() throws Exception {
     destroyServers();
-    FieldCache.DEFAULT.purgeAllCaches();   // avoid FC insanity
     super.tearDown();
   }
 
