Index: lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterMatchedFieldsTest.java
===================================================================
--- lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterMatchedFieldsTest.java	(revision 0)
+++ lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterMatchedFieldsTest.java	(working copy)
@@ -0,0 +1,136 @@
+package org.apache.lucene.search.vectorhighlight;
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.AbstractMatchedFieldsHighlighterTestCase;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.highlight.DefaultEncoder;
+import org.apache.lucene.search.highlight.Encoder;
+
+/**
+ * Test the FVH's matched fields.  Note that the FVH supports boosted terms and boosting phrase matches
+ * so those are tested here instead of in the parent.
+ */
+public class FastVectorHighlighterMatchedFieldsTest extends AbstractMatchedFieldsHighlighterTestCase {
+  /**
+   * Tests that boosts are included when sorting snippets.  This is here rather than in the parent because
+   * the PostingsHighlighter doesn't support it.
+   */
+  public void testBoosts() throws IOException {   
+    // And the highlighter respects the boosts on matched fields when sorting fragments
+    matchedFieldsTestCase( "cat cat junk junk junk junk junk Junk junk a cat junk junk",
+      "Junk junk <b>a cat</b> junk junk",
+      clause( "field", "cat" ), clause( "field_exact", 5, "a", "cat" ) );
+    matchedFieldsTestCase( "cat cat junk junk junk junk Junk junk junk a cat junk junk",
+      "<b>cat cat</b> junk junk junk junk",
+      clause( "field", "cat" ), clause( "field_exact", "a", "cat" ) );
+
+    // The same thing works across three fields as well
+    matchedFieldsTestCase( "cat cat CAT junk junk junk junk junk junk junk a cat junk junk",
+      "junk junk <b>a cat</b> junk junk",
+      clause( "field", "cat" ), clause( "field_exact", 200, "a", "cat" ), clause( "field_super_exact", 5, "CAT" ) );
+    matchedFieldsTestCase( "a cat cat junk junk junk junk junk junk junk a CAT junk junk",
+      "junk junk <b>a CAT</b> junk junk",
+      clause( "field", "cat" ), clause( "field_exact", 5, "a", "cat" ), clause( "field_super_exact", 200, "a", "CAT" ) );
+
+    // And across fields with different tokenizers!
+    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
+      "junk junk <b>a cat</b> junk junk",
+      clause( "field_exact", 5, "a", "cat" ), clause( "field_characters", "c" ) );
+    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
+      "<b>cat cat</b> junk junk junk junk",
+      clause( "field", "cat" ), clause( "field_characters", 5, "c" ) );
+    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
+      "junk junk <b>a cat</b> junk junk",
+      clause( "field", "cat" ), clause( "field_characters", 5, "a", " ", "c", "a", "t" ) );
+
+    // Multiple matches add to the score of the segment
+    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
+      "<b>cat cat</b> junk junk junk junk",
+      clause( "field", "cat" ), clause( "field_sliced", "cat" ), clause( "field_exact", 2, "a", "cat" ) );
+    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
+      "junk junk <b>a cat</b> junk junk",
+      clause( "field", "cat" ), clause( "field_sliced", "cat" ), clause( "field_exact", 4, "a", "cat" ) );
+
+    // Even fields with tokens on top of one another are ok
+    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
+      "<b>cat</b> cat junk junk junk junk",
+      clause( "field_der_red", 2, "der" ), clause( "field_exact", "a", "cat" ) );
+    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
+      "<b>cat</b> cat junk junk junk junk",
+      clause( "field_der_red", 2, "red" ), clause( "field_exact", "a", "cat" ) );
+    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
+      "<b>cat</b> cat junk junk junk junk",
+      clause( "field_der_red", "red" ), clause( "field_der_red", "der" ), clause( "field_exact", "a", "cat" ) );
+  }
+
+  public void testRequireFieldMatchWithBoosts() throws IOException {
+    // Everything works pretty well even if you don't require a field match
+    matchedFieldsTestCase( true, false, "cat cat junk junk junk junk junk junk junk a cat junk junk",
+      "junk junk <b>a cat</b> junk junk",
+      clause( "field", "cat" ), clause( "field_characters", 10, "a", " ", "c", "a", "t" ) );
+
+    // Even boosts keep themselves pretty much intact
+    matchedFieldsTestCase( true, false, "a cat cat junk junk junk junk junk junk junk a CAT junk junk",
+      "junk junk <b>a CAT</b> junk junk",
+      clause( "field", "cat" ), clause( "field_exact", 5, "a", "cat" ), clause( "field_super_exact", 200, "a", "CAT" ) );
+    matchedFieldsTestCase( true, false, "cat cat CAT junk junk junk junk junk junk junk a cat junk junk",
+      "junk junk <b>a cat</b> junk junk",
+      clause( "field", "cat" ), clause( "field_exact", 200, "a", "cat" ), clause( "field_super_exact", 5, "CAT" ) );
+
+    // Except that all the matched field matches apply even if they aren't mentioned in the query
+    // which can make for some confusing scoring.  This isn't too big a deal, just something you
+    // need to think about when you don't force a field match.
+    matchedFieldsTestCase( true, false, "cat cat junk junk junk junk junk junk junk a cat junk junk",
+      "<b>cat cat</b> junk junk junk junk",
+      clause( "field", "cat" ), clause( "field_characters", 4, "a", " ", "c", "a", "t" ) );
+  }
+
+  @Override
+  protected FieldType prepare( FieldType fieldType ) {
+    fieldType.setStoreTermVectorOffsets( true );
+    fieldType.setStoreTermVectorPositions( true );
+    fieldType.setStoreTermVectors( true );
+    return fieldType;
+  }
+
+  @Override
+  protected String[] getBestFragments( IndexReader reader, Query query, boolean fieldMatch, Set< String > matchedFields
+      ) throws IOException {
+    FastVectorHighlighter highlighter = new FastVectorHighlighter();
+    FragListBuilder fragListBuilder = new SimpleFragListBuilder();
+    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();
+    
+    String[] preTags = new String[] { "<b>" };
+    String[] postTags = new String[] { "</b>" };
+    Encoder encoder = new DefaultEncoder();
+    int docId = 0;
+    FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );
+    if ( matchedFields != null ) {
+      return highlighter.getBestFragments( fieldQuery, reader, docId, "field", matchedFields, 25, 1,
+        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );
+    } else {
+      return highlighter.getBestFragments( fieldQuery, reader, docId, "field", 25, 1,
+        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );
+    }
+  }
+}
Index: lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest.java
===================================================================
--- lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest.java	(revision 1568049)
+++ lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest.java	(working copy)
@@ -51,6 +51,7 @@
 import org.apache.lucene.util.automaton.CharacterRunAutomaton;
 import org.apache.lucene.util.automaton.RegExp;
 
+import static org.apache.lucene.search.AbstractMatchedFieldsHighlighterTestCase.*;
 
 public class FastVectorHighlighterTest extends LuceneTestCase {
   
@@ -345,118 +346,7 @@
     writer.close();
     dir.close();
   }
-  
-  public void testMatchedFields() throws IOException {
-    // Searching just on the stored field doesn't highlight a stopword
-    matchedFieldsTestCase( false, true, "a match", "a <b>match</b>",
-      clause( "field", "a" ), clause( "field", "match" ) );
 
-    // Even if you add an unqueried matched field that would match it
-    matchedFieldsTestCase( "a match", "a <b>match</b>",
-      clause( "field", "a" ), clause( "field", "match" ) );
-
-    // Nor if you query the field but don't add it as a matched field to the highlighter
-    matchedFieldsTestCase( false, false, "a match", "a <b>match</b>",
-      clause( "field_exact", "a" ), clause( "field", "match" ) );
-
-    // But if you query the field and add it as a matched field to the highlighter then it is highlighted
-    matchedFieldsTestCase( "a match", "<b>a</b> <b>match</b>",
-      clause( "field_exact", "a" ), clause( "field", "match" ) );
-
-    // It is also ok to match just the matched field but get highlighting from the stored field
-    matchedFieldsTestCase( "a match", "<b>a</b> <b>match</b>",
-      clause( "field_exact", "a" ), clause( "field_exact", "match" ) );
-
-    // Boosted matched fields work too
-    matchedFieldsTestCase( "a match", "<b>a</b> <b>match</b>",
-      clause( "field_exact", 5, "a" ), clause( "field", "match" ) );
-
-    // It is also ok if both the stored and the matched field match the term
-    matchedFieldsTestCase( "a match", "a <b>match</b>",
-      clause( "field_exact", "match" ), clause( "field", "match" ) );
-
-    // And the highlighter respects the boosts on matched fields when sorting fragments
-    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
-      "junk junk <b>a cat</b> junk junk",
-      clause( "field", "cat" ), clause( "field_exact", 5, "a", "cat" ) );
-    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
-      "<b>cat</b> <b>cat</b> junk junk junk junk",
-      clause( "field", "cat" ), clause( "field_exact", "a", "cat" ) );
-
-    // The same thing works across three fields as well
-    matchedFieldsTestCase( "cat cat CAT junk junk junk junk junk junk junk a cat junk junk",
-      "junk junk <b>a cat</b> junk junk",
-      clause( "field", "cat" ), clause( "field_exact", 200, "a", "cat" ), clause( "field_super_exact", 5, "CAT" ) );
-    matchedFieldsTestCase( "a cat cat junk junk junk junk junk junk junk a CAT junk junk",
-      "junk junk <b>a CAT</b> junk junk",
-      clause( "field", "cat" ), clause( "field_exact", 5, "a", "cat" ), clause( "field_super_exact", 200, "a", "CAT" ) );
-
-    // And across fields with different tokenizers!
-    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
-      "junk junk <b>a cat</b> junk junk",
-      clause( "field_exact", 5, "a", "cat" ), clause( "field_characters", "c" ) );
-    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
-      "<b>c</b>at <b>c</b>at junk junk junk junk",
-      clause( "field_exact", "a", "cat" ), clause( "field_characters", "c" ) );
-    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
-      "ca<b>t</b> ca<b>t</b> junk junk junk junk",
-      clause( "field_exact", "a", "cat" ), clause( "field_characters", "t" ) );
-    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
-      "<b>cat</b> <b>cat</b> junk junk junk junk", // See how the phrases are joined?
-      clause( "field", "cat" ), clause( "field_characters", 5, "c" ) );
-    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
-      "junk junk <b>a cat</b> junk junk",
-      clause( "field", "cat" ), clause( "field_characters", 5, "a", " ", "c", "a", "t" ) );
-
-    // Phrases and tokens inside one another are joined
-    matchedFieldsTestCase( "cats wow", "<b>cats w</b>ow",
-      clause( "field", "cats" ), clause( "field_tripples", "s w" ) );
-
-    // Everything works pretty well even if you don't require a field match
-    matchedFieldsTestCase( true, false, "cat cat junk junk junk junk junk junk junk a cat junk junk",
-      "junk junk <b>a cat</b> junk junk",
-      clause( "field", "cat" ), clause( "field_characters", 10, "a", " ", "c", "a", "t" ) );
-
-    // Even boosts keep themselves pretty much intact
-    matchedFieldsTestCase( true, false, "a cat cat junk junk junk junk junk junk junk a CAT junk junk",
-      "junk junk <b>a CAT</b> junk junk",
-      clause( "field", "cat" ), clause( "field_exact", 5, "a", "cat" ), clause( "field_super_exact", 200, "a", "CAT" ) );
-    matchedFieldsTestCase( true, false, "cat cat CAT junk junk junk junk junk junk junk a cat junk junk",
-      "junk junk <b>a cat</b> junk junk",
-      clause( "field", "cat" ), clause( "field_exact", 200, "a", "cat" ), clause( "field_super_exact", 5, "CAT" ) );
-
-    // Except that all the matched field matches apply even if they aren't mentioned in the query
-    // which can make for some confusing scoring.  This isn't too big a deal, just something you
-    // need to think about when you don't force a field match.
-    matchedFieldsTestCase( true, false, "cat cat junk junk junk junk junk junk junk a cat junk junk",
-      "<b>cat</b> <b>cat</b> junk junk junk junk",
-      clause( "field", "cat" ), clause( "field_characters", 4, "a", " ", "c", "a", "t" ) );
-
-    // It is also cool to match fields that don't have _exactly_ the same text so long as you are careful.
-    // In this case field_sliced is a prefix of field.
-    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
-      "<b>cat</b> <b>cat</b> junk junk junk junk", clause( "field_sliced", "cat" ) );
-
-    // Multiple matches add to the score of the segment
-    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
-      "<b>cat</b> <b>cat</b> junk junk junk junk",
-      clause( "field", "cat" ), clause( "field_sliced", "cat" ), clause( "field_exact", 2, "a", "cat" ) );
-    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
-      "junk junk <b>a cat</b> junk junk",
-      clause( "field", "cat" ), clause( "field_sliced", "cat" ), clause( "field_exact", 4, "a", "cat" ) );
-
-    // Even fields with tokens on top of one another are ok
-    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
-      "<b>cat</b> cat junk junk junk junk",
-      clause( "field_der_red", 2, "der" ), clause( "field_exact", "a", "cat" ) );
-    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
-      "<b>cat</b> cat junk junk junk junk",
-      clause( "field_der_red", 2, "red" ), clause( "field_exact", "a", "cat" ) );
-    matchedFieldsTestCase( "cat cat junk junk junk junk junk junk junk a cat junk junk",
-      "<b>cat</b> cat junk junk junk junk",
-      clause( "field_der_red", "red" ), clause( "field_der_red", "der" ), clause( "field_exact", "a", "cat" ) );
-  }
-
   public void testMultiValuedSortByScore() throws IOException {
     Directory dir = newDirectory();
     IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer( random() ) ) );
@@ -504,111 +394,4 @@
     writer.close();
     dir.close();
   }
-
-  private void matchedFieldsTestCase( String fieldValue, String expected, Query... queryClauses ) throws IOException {
-    matchedFieldsTestCase( true, true, fieldValue, expected, queryClauses );
-  }
-
-  private void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {
-    Document doc = new Document();
-    FieldType stored = new FieldType( TextField.TYPE_STORED );
-    stored.setStoreTermVectorOffsets( true );
-    stored.setStoreTermVectorPositions( true );
-    stored.setStoreTermVectors( true );
-    stored.freeze();
-    FieldType matched = new FieldType( TextField.TYPE_NOT_STORED );
-    matched.setStoreTermVectorOffsets( true );
-    matched.setStoreTermVectorPositions( true );
-    matched.setStoreTermVectors( true );
-    matched.freeze();
-    doc.add( new Field( "field", fieldValue, stored ) );               // Whitespace tokenized with English stop words
-    doc.add( new Field( "field_exact", fieldValue, matched ) );        // Whitespace tokenized without stop words
-    doc.add( new Field( "field_super_exact", fieldValue, matched ) );  // Whitespace tokenized without toLower
-    doc.add( new Field( "field_characters", fieldValue, matched ) );   // Each letter is a token
-    doc.add( new Field( "field_tripples", fieldValue, matched ) );     // Every three letters is a token
-    doc.add( new Field( "field_sliced", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field
-      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );
-    doc.add( new Field( "field_der_red", new CannedTokenStream(        // Hacky field containing "der" and "red" at pos = 0
-          token( "der", 1, 0, 3 ),
-          token( "red", 0, 0, 3 )
-        ), matched ) );
-
-    final Map<String, Analyzer> fieldAnalyzers = new TreeMap<String, Analyzer>();
-    fieldAnalyzers.put( "field", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );
-    fieldAnalyzers.put( "field_exact", new MockAnalyzer( random() ) );
-    fieldAnalyzers.put( "field_super_exact", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );
-    fieldAnalyzers.put( "field_characters", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(".").toAutomaton() ), true ) );
-    fieldAnalyzers.put( "field_tripples", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp("...").toAutomaton() ), true ) );
-    fieldAnalyzers.put( "field_sliced", fieldAnalyzers.get( "field" ) );
-    fieldAnalyzers.put( "field_der_red", fieldAnalyzers.get( "field" ) );  // This is required even though we provide a token stream
-    Analyzer analyzer = new AnalyzerWrapper() {
-      public Analyzer getWrappedAnalyzer(String fieldName) {
-        return fieldAnalyzers.get( fieldName );
-      }
-    };
-
-    Directory dir = newDirectory();
-    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer ) );
-    writer.addDocument( doc );
-
-    FastVectorHighlighter highlighter = new FastVectorHighlighter();
-    FragListBuilder fragListBuilder = new SimpleFragListBuilder();
-    FragmentsBuilder fragmentsBuilder = new ScoreOrderFragmentsBuilder();
-    IndexReader reader = DirectoryReader.open( writer, true );
-    String[] preTags = new String[] { "<b>" };
-    String[] postTags = new String[] { "</b>" };
-    Encoder encoder = new DefaultEncoder();
-    int docId = 0;
-    BooleanQuery query = new BooleanQuery();
-    for ( Query clause : queryClauses ) {
-      query.add( clause, Occur.MUST );
-    }
-    FieldQuery fieldQuery = new FieldQuery( query, reader, true, fieldMatch );
-    String[] bestFragments;
-    if ( useMatchedFields ) {
-      Set< String > matchedFields = new HashSet< String >();
-      matchedFields.add( "field" );
-      matchedFields.add( "field_exact" );
-      matchedFields.add( "field_super_exact" );
-      matchedFields.add( "field_characters" );
-      matchedFields.add( "field_tripples" );
-      matchedFields.add( "field_sliced" );
-      matchedFields.add( "field_der_red" );
-      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, "field", matchedFields, 25, 1,
-        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );
-    } else {
-      bestFragments = highlighter.getBestFragments( fieldQuery, reader, docId, "field", 25, 1,
-        fragListBuilder, fragmentsBuilder, preTags, postTags, encoder );
-    }
-    assertEquals( expected, bestFragments[ 0 ] );
-
-    reader.close();
-    writer.close();
-    dir.close();
-  }
-
-  private Query clause( String field, String... terms ) {
-    return clause( field, 1, terms );
-  }
-
-  private Query clause( String field, float boost, String... terms ) {
-    Query q;
-    if ( terms.length == 1 ) {
-      q = new TermQuery( new Term( field, terms[ 0 ] ) );
-    } else {
-      PhraseQuery pq = new PhraseQuery();
-      for ( String term: terms ) {
-        pq.add( new Term( field, term ) );
-      }
-      q = pq;
-    }
-    q.setBoost( boost );
-    return q;
-  }
-
-  private static Token token( String term, int posInc, int startOffset, int endOffset ) {
-    Token t = new Token( term, startOffset, endOffset );
-    t.setPositionIncrement( posInc );
-    return t;
-  }
 }
Index: lucene/highlighter/src/test/org/apache/lucene/search/AbstractMatchedFieldsHighlighterTestCase.java
===================================================================
--- lucene/highlighter/src/test/org/apache/lucene/search/AbstractMatchedFieldsHighlighterTestCase.java	(revision 0)
+++ lucene/highlighter/src/test/org/apache/lucene/search/AbstractMatchedFieldsHighlighterTestCase.java	(working copy)
@@ -0,0 +1,227 @@
+package org.apache.lucene.search;
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.AnalyzerWrapper;
+import org.apache.lucene.analysis.CannedTokenStream;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenFilter;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.queries.CommonTermsQuery;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.highlight.DefaultEncoder;
+import org.apache.lucene.search.highlight.Encoder;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.automaton.CharacterRunAutomaton;
+import org.apache.lucene.util.automaton.RegExp;
+
+/**
+ * Base test case for verifying highlighters that can combine highlights from matched
+ * fields into a single highlighted field.
+ */
+public abstract class AbstractMatchedFieldsHighlighterTestCase extends LuceneTestCase {
+  /**
+   * Prepare the fieldType to store whatever extra data is required by the highlighter.
+   * @param fieldType to prepare
+   * @return the field type
+   */
+  protected abstract FieldType prepare( FieldType fieldType );
+  protected abstract String[] getBestFragments( IndexReader reader, Query query, boolean fieldMatch,
+      Set< String > matchedFields ) throws IOException;
+
+  /**
+   * Basic searching with matched fields.
+   */
+  public void testSameTokenizers() throws IOException {
+    // Searching just on the stored field doesn't highlight a stopword
+    matchedFieldsTestCase( false, true, "a match", "a <b>match</b>",
+      clause( "field", "a" ), clause( "field", "match" ) );
+
+    // Even if you add an unqueried matched field that would match it
+    matchedFieldsTestCase( "a match", "a <b>match</b>",
+      clause( "field", "a" ), clause( "field", "match" ) );
+
+    // Nor if you query the field but don't add it as a matched field to the highlighter
+    matchedFieldsTestCase( false, false, "a match", "a <b>match</b>",
+      clause( "field_exact", "a" ), clause( "field", "match" ) );
+
+    // But if you query the field and add it as a matched field to the highlighter then it is highlighted
+    matchedFieldsTestCase( "a match", "<b>a match</b>",
+      clause( "field_exact", "a" ), clause( "field", "match" ) );
+
+    // It is also ok to match just the matched field but get highlighting from the stored field
+    matchedFieldsTestCase( "a match", "<b>a match</b>",
+      clause( "field_exact", "a" ), clause( "field_exact", "match" ) );
+
+    // Boosted matched fields don't break anything
+    matchedFieldsTestCase( "a match", "<b>a match</b>",
+      clause( "field_exact", 5, "a" ), clause( "field", "match" ) );
+
+    // It is also ok if both the stored and the matched field match the term
+    matchedFieldsTestCase( "a match", "a <b>match</b>",
+      clause( "field_exact", "match" ), clause( "field", "match" ) );
+  }
+
+  /**
+   * Matched fields that don't share the same tokenizer!
+   */
+  public void testDifferentTokenizers() throws IOException {
+    // And across fields with different tokenizers!
+    matchedFieldsTestCase( "cat cat junk junk junk junk. Junk junk junk a cat junk junk",
+      "<b>c</b>at <b>c</b>at <b>junk junk junk</b> junk",
+      clause( "field_exact", "junk" ), clause( "field_characters", "c" ) );
+    matchedFieldsTestCase( "cat cat junk junk junk junk. Junk junk junk a cat junk junk",
+      "ca<b>t</b> ca<b>t junk junk junk</b> junk",
+      clause( "field_exact", "junk" ), clause( "field_characters", "t" ) );
+
+    // Phrases and tokens inside one another are joined
+    matchedFieldsTestCase( "cats wow", "<b>cats w</b>ow",
+      clause( "field", "cats" ), clause( "field_tripples", "s w" ) );
+  }
+
+  /**
+   * Matched fields that use that actually have a different source.
+   */
+  public void testDifferentSource() throws IOException {
+    // It is also cool to match fields that don't have _exactly_ the same text so long as you are careful.
+    // In this case field_sliced is a prefix of field.
+    matchedFieldsTestCase( "cat cat junk junk junk junk. Junk junk junk cat cat cat cat junk junk",
+      "<b>cat cat</b> junk junk junk junk", clause( "field_sliced", "cat" ) );
+
+    // Even fields with tokens on top of one another are ok
+    matchedFieldsTestCase( "cat cat junk junk junk junk. Junk junk junk a cat junk junk",
+      "<b>cat</b> cat junk junk junk junk",
+      clause( "field_der_red", "red" ), clause( "field_der_red", "der" ) );
+  }
+
+  protected void matchedFieldsTestCase( String fieldValue, String expected, Query... queryClauses ) throws IOException {
+    matchedFieldsTestCase( true, true, fieldValue, expected, queryClauses );
+  }
+
+  protected void matchedFieldsTestCase( boolean useMatchedFields, boolean fieldMatch, String fieldValue, String expected, Query... queryClauses ) throws IOException {
+    System.err.println("  " + fieldValue + "   " + expected);
+
+    Document doc = new Document();
+    FieldType stored = prepare( new FieldType( TextField.TYPE_STORED ) );
+    stored.freeze();
+    FieldType matched = prepare( new FieldType( TextField.TYPE_NOT_STORED ) );
+    matched.freeze();
+    doc.add( new Field( "field", fieldValue, stored ) );               // Whitespace tokenized with English stop words
+    doc.add( new Field( "field_exact", fieldValue, matched ) );        // Whitespace tokenized without stop words
+    doc.add( new Field( "field_super_exact", fieldValue, matched ) );  // Whitespace tokenized without toLower
+    doc.add( new Field( "field_characters", fieldValue, matched ) );   // Each letter is a token
+    doc.add( new Field( "field_tripples", fieldValue, matched ) );     // Every three letters is a token
+    doc.add( new Field( "field_sliced", fieldValue.substring( 0,       // Sliced at 10 chars then analyzed just like field
+      Math.min( fieldValue.length() - 1 , 10 ) ), matched ) );
+    doc.add( new Field( "field_der_red", new CannedTokenStream(        // Hacky field containing "der" and "red" at pos = 0
+          token( "der", 1, 0, 3 ),
+          token( "red", 0, 0, 3 )
+        ), matched ) );
+
+    final Map<String, Analyzer> fieldAnalyzers = new HashMap<String, Analyzer>();
+    fieldAnalyzers.put( "field", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET ) );
+    fieldAnalyzers.put( "field_exact", new MockAnalyzer( random() ) );
+    fieldAnalyzers.put( "field_super_exact", new MockAnalyzer( random(), MockTokenizer.WHITESPACE, false ) );
+    fieldAnalyzers.put( "field_characters", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp(".").toAutomaton() ), true ) );
+    fieldAnalyzers.put( "field_tripples", new MockAnalyzer( random(), new CharacterRunAutomaton( new RegExp("...").toAutomaton() ), true ) );
+    fieldAnalyzers.put( "field_sliced", fieldAnalyzers.get( "field" ) );
+    fieldAnalyzers.put( "field_der_red", fieldAnalyzers.get( "field" ) );  // This is required even though we provide a token stream
+    Analyzer analyzer = new AnalyzerWrapper(Analyzer.PER_FIELD_REUSE_STRATEGY) {
+      public Analyzer getWrappedAnalyzer(String fieldName) {
+        return fieldAnalyzers.get( fieldName );
+      }
+    };
+
+    Directory dir = newDirectory();
+    IndexWriter writer = new IndexWriter( dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer ) );
+    writer.addDocument( doc );
+
+    IndexReader reader = DirectoryReader.open( writer, true );
+    BooleanQuery query = new BooleanQuery();
+    for ( Query clause : queryClauses ) {
+      query.add( clause, Occur.SHOULD );
+    }
+    Set< String > matchedFields = null;
+    if ( useMatchedFields ) {
+      matchedFields = new HashSet< String >();
+      matchedFields.add( "field" );
+      matchedFields.add( "field_exact" );
+      matchedFields.add( "field_super_exact" );
+      matchedFields.add( "field_characters" );
+      matchedFields.add( "field_tripples" );
+      matchedFields.add( "field_sliced" );
+      matchedFields.add( "field_der_red" );
+    }
+    String[] bestFragments = getBestFragments( reader, query, fieldMatch, matchedFields );
+    assertEquals( expected, bestFragments[ 0 ]
+      .replaceAll( "</b>( )?<b>", "$1" ) // Highlighters doesn't always combine highlights next to one another.
+      .trim()
+      .replaceAll( "\\.$", "" )          // Some highlighters strip the trailing period, some don't.
+    );
+
+    reader.close();
+    writer.close();
+    dir.close();
+  }
+
+  public static Query clause( String field, String... terms ) {
+    return clause( field, 1, terms );
+  }
+
+  public static Query clause( String field, float boost, String... terms ) {
+    Query q;
+    if ( terms.length == 1 ) {
+      q = new TermQuery( new Term( field, terms[ 0 ] ) );
+    } else {
+      PhraseQuery pq = new PhraseQuery();
+      for ( String term: terms ) {
+        pq.add( new Term( field, term ) );
+      }
+      q = pq;
+    }
+    q.setBoost( boost );
+    return q;
+  }
+
+  public static Token token( String term, int posInc, int startOffset, int endOffset ) {
+    Token t = new Token( term, startOffset, endOffset );
+    t.setPositionIncrement( posInc );
+    return t;
+  }
+}
Index: lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterMatchedFields.java
===================================================================
--- lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterMatchedFields.java	(revision 0)
+++ lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterMatchedFields.java	(working copy)
@@ -0,0 +1,56 @@
+package org.apache.lucene.search.postingshighlight;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.search.AbstractMatchedFieldsHighlighterTestCase;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
+
+@SuppressCodecs({"MockFixedIntBlock", "MockVariableIntBlock", "MockSep", "MockRandom"})
+public class TestPostingsHighlighterMatchedFields extends AbstractMatchedFieldsHighlighterTestCase {
+	@Override
+  protected FieldType prepare(FieldType fieldType) {
+    fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
+    return fieldType;
+  }
+
+  @Override
+  protected String[] getBestFragments(IndexReader reader, Query query, boolean fieldMatch, Set<String> matchedFields
+      ) throws IOException {
+    PostingsHighlighter highlighter = new PostingsHighlighter();
+    IndexSearcher searcher = newSearcher(reader);
+    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
+    assertEquals(1, topDocs.totalHits);
+
+    if ( matchedFields == null ) {
+      return highlighter.highlight("field", query, searcher, topDocs);
+    } else {
+      return highlighter.highlight("field", matchedFields, query, searcher,
+          topDocs, 1);
+    }
+  }
+}
Index: lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighter.java
===================================================================
--- lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighter.java	(revision 1568049)
+++ lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighter.java	(working copy)
@@ -1111,7 +1111,7 @@
     assertEquals(1, topDocs.totalHits);
     int[] docIDs = new int[1];
     docIDs[0] = topDocs.scoreDocs[0].doc;
-    Map<String,Object[]> snippets = highlighter.highlightFieldsAsObjects(new String[]{"body"}, query, searcher, docIDs, new int[] {1});
+    Map<String,Object[]> snippets = highlighter.highlightFieldsAsObjects(new String[]{"body"}, null, query, searcher, docIDs, new int[] {1});
     Object[] bodySnippets = snippets.get("body");
     assertEquals(1, bodySnippets.length);
     assertTrue(Arrays.equals(new String[] {"blah blah", "Just a test <b>highlighting</b> from postings. "}, (String[]) bodySnippets[0]));
Index: lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter.java	(revision 1568049)
+++ lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter.java	(working copy)
@@ -21,12 +21,14 @@
 import java.text.BreakIterator;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.Comparator;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Locale;
 import java.util.Map;
 import java.util.PriorityQueue;
+import java.util.Set;
 import java.util.SortedSet;
 import java.util.TreeSet;
 
@@ -205,11 +207,35 @@
    *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}
    */
   public String[] highlight(String field, Query query, IndexSearcher searcher, TopDocs topDocs, int maxPassages) throws IOException {
-    Map<String,String[]> res = highlightFields(new String[] { field }, query, searcher, topDocs, new int[] { maxPassages });
+    Map<String,String[]> res = highlightFields(new String[] { field }, null, query, searcher, topDocs, new int[] { maxPassages });
     return res.get(field);
   }
-  
+
   /**
+   * Highlights the top-N passages from a single field.
+   * 
+   * @param field field name to highlight. 
+   *        Must have a stored string value and also be indexed with offsets.
+   * @param query query to highlight.
+   * @param searcher searcher that was previously used to execute the query.
+   * @param topDocs TopDocs containing the summary result documents to highlight.
+   * @param maxPassages The maximum number of top-N ranked passages used to 
+   *        form the highlighted snippets.
+   * @return Array of formatted snippets corresponding to the documents in <code>topDocs</code>. 
+   *         If no highlights were found for a document, the
+   *         first {@code maxPassages} sentences from the
+   *         field will be returned.
+   * @throws IOException if an I/O error occurred during processing
+   * @throws IllegalArgumentException if <code>field</code> was indexed without 
+   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}
+   */
+  public String[] highlight(String field, Set<String> matchedFields, Query query, IndexSearcher searcher, TopDocs topDocs, int maxPassages) throws IOException {
+    Map<String,String[]> res = highlightFields(new String[] { field }, Collections.singletonMap(field, matchedFields),
+        query, searcher, topDocs, new int[] { maxPassages });
+    return res.get(field);
+  }
+
+  /**
    * Highlights the top passages from multiple fields.
    * <p>
    * Conceptually, this behaves as a more efficient form of:
@@ -237,9 +263,42 @@
   public Map<String,String[]> highlightFields(String fields[], Query query, IndexSearcher searcher, TopDocs topDocs) throws IOException {
     int maxPassages[] = new int[fields.length];
     Arrays.fill(maxPassages, 1);
+    return highlightFields(fields, null, query, searcher, topDocs, maxPassages);
+  }
+
+  /**
+   * Highlights the top-N passages from multiple fields.
+   * <p>
+   * Conceptually, this behaves as a more efficient form of:
+   * <pre class="prettyprint">
+   * Map m = new HashMap();
+   * for (String field : fields) {
+   *   m.put(field, highlight(field, query, searcher, topDocs, maxPassages));
+   * }
+   * return m;
+   * </pre>
+   * 
+   * @param fields field names to highlight. 
+   *        Must have a stored string value and also be indexed with offsets.
+   * @param query query to highlight.
+   * @param searcher searcher that was previously used to execute the query.
+   * @param topDocs TopDocs containing the summary result documents to highlight.
+   * @param maxPassages The maximum number of top-N ranked passages per-field used to 
+   *        form the highlighted snippets.
+   * @return Map keyed on field name, containing the array of formatted snippets 
+   *         corresponding to the documents in <code>topDocs</code>. 
+   *         If no highlights were found for a document, the
+   *         first {@code maxPassages} sentences from the
+   *         field will be returned.
+   * @throws IOException if an I/O error occurred during processing
+   * @throws IllegalArgumentException if <code>field</code> was indexed without 
+   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}
+   */
+  public Map<String,String[]> highlightFields(String fields[], Query query, IndexSearcher searcher, TopDocs topDocs,
+      int maxPassages[]) throws IOException {
     return highlightFields(fields, query, searcher, topDocs, maxPassages);
   }
-  
+
   /**
    * Highlights the top-N passages from multiple fields.
    * <p>
@@ -268,14 +327,15 @@
    * @throws IllegalArgumentException if <code>field</code> was indexed without 
    *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}
    */
-  public Map<String,String[]> highlightFields(String fields[], Query query, IndexSearcher searcher, TopDocs topDocs, int maxPassages[]) throws IOException {
+  public Map<String,String[]> highlightFields(String fields[], Map<String, Set<String>> fieldToMatchedFields,
+      Query query, IndexSearcher searcher, TopDocs topDocs, int maxPassages[]) throws IOException {
     final ScoreDoc scoreDocs[] = topDocs.scoreDocs;
     int docids[] = new int[scoreDocs.length];
     for (int i = 0; i < docids.length; i++) {
       docids[i] = scoreDocs[i].doc;
     }
 
-    return highlightFields(fields, query, searcher, docids, maxPassages);
+    return highlightFields(fields, fieldToMatchedFields, query, searcher, docids, maxPassages);
   }
 
   /**
@@ -299,8 +359,35 @@
    *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}
    */
   public Map<String,String[]> highlightFields(String fieldsIn[], Query query, IndexSearcher searcher, int[] docidsIn, int maxPassagesIn[]) throws IOException {
+    return highlightFields(fieldsIn, null, query, searcher, docidsIn, maxPassagesIn);
+  }
+
+  /**
+   * Highlights the top-N passages from multiple fields,
+   * for the provided int[] docids.
+   * 
+   * @param fieldsIn field names to highlight. 
+   *        Must have a stored string value and also be indexed with offsets.
+   * @param query query to highlight.
+   * @param searcher searcher that was previously used to execute the query.
+   * @param docidsIn containing the document IDs to highlight.
+   * @param maxPassagesIn The maximum number of top-N ranked passages per-field used to 
+   *        form the highlighted snippets.
+   * @return Map keyed on field name, containing the array of formatted snippets 
+   *         corresponding to the documents in <code>docidsIn</code>. 
+   *         If no highlights were found for a document, the
+   *         first {@code maxPassages} from the field will
+   *         be returned.
+   * @throws IOException if an I/O error occurred during processing
+   * @throws IllegalArgumentException if <code>field</code> was indexed without 
+   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}
+   */
+  public Map<String,String[]> highlightFields(String fieldsIn[], Map<String, Set<String>> fieldToMatchedFields,
+      Query query, IndexSearcher searcher, int[] docidsIn, int maxPassagesIn[]) throws IOException {
     Map<String,String[]> snippets = new HashMap<String,String[]>();
-    for(Map.Entry<String,Object[]> ent : highlightFieldsAsObjects(fieldsIn, query, searcher, docidsIn, maxPassagesIn).entrySet()) {
+    Map<String,Object[]> highlights = highlightFieldsAsObjects(fieldsIn, fieldToMatchedFields, query, searcher,
+        docidsIn, maxPassagesIn);
+    for(Map.Entry<String,Object[]> ent : highlights.entrySet()) {
       Object[] snippetObjects = ent.getValue();
       String[] snippetStrings = new String[snippetObjects.length];
       snippets.put(ent.getKey(), snippetStrings);
@@ -337,7 +424,9 @@
    * @throws IllegalArgumentException if <code>field</code> was indexed without 
    *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}
    */
-  protected Map<String,Object[]> highlightFieldsAsObjects(String fieldsIn[], Query query, IndexSearcher searcher, int[] docidsIn, int maxPassagesIn[]) throws IOException {
+  protected Map<String,Object[]> highlightFieldsAsObjects(String fieldsIn[],
+      Map<String, Set<String>> fieldToMatchedFields, Query query, IndexSearcher searcher, int[] docidsIn,
+      int maxPassagesIn[]) throws IOException {
     if (fieldsIn.length < 1) {
       throw new IllegalArgumentException("fieldsIn must not be empty");
     }
@@ -388,18 +477,33 @@
     for (int i = 0; i < fields.length; i++) {
       String field = fields[i];
       int numPassages = maxPassages[i];
-      Term floor = new Term(field, "");
-      Term ceiling = new Term(field, UnicodeUtil.BIG_TERM);
-      SortedSet<Term> fieldTerms = queryTerms.subSet(floor, ceiling);
+      List<MatchedFieldAndTerms> terms;
+      int termsSize;
+      Set<String> matchedFields;
+      if (fieldToMatchedFields == null || (matchedFields = fieldToMatchedFields.get(field)) == null) {
+        BytesRef[] termsForField = termsForField(field, queryTerms);
+        terms = Collections.singletonList(new MatchedFieldAndTerms(field, termsForField));
+        termsSize = termsForField.length;
+      } else {
+        terms = new ArrayList<MatchedFieldAndTerms>(matchedFields.size());
+        termsSize = 0;
+        String[] matchedFieldsSorted = new String[matchedFields.size()];
+        int matchedFieldsSortedUpTo = 0;
+        for (String matchedField: matchedFields) {
+          matchedFieldsSorted[matchedFieldsSortedUpTo++] = matchedField;
+        }
+        Arrays.sort(matchedFieldsSorted);
+        for (String matchedField: matchedFields) {
+          BytesRef[] termsForField = termsForField(matchedField, queryTerms);
+          terms.add(new MatchedFieldAndTerms(matchedField, termsForField));
+          termsSize += termsForField.length;
+        }
+      }
       // TODO: should we have some reasonable defaults for term pruning? (e.g. stopwords)
 
       // Strip off the redundant field:
-      BytesRef terms[] = new BytesRef[fieldTerms.size()];
-      int termUpto = 0;
-      for(Term term : fieldTerms) {
-        terms[termUpto++] = term.bytes();
-      }
-      Map<Integer,Object> fieldHighlights = highlightField(field, contents[i], getBreakIterator(field), terms, docids, leaves, numPassages, query);
+      Map<Integer,Object> fieldHighlights = highlightField(field, contents[i], getBreakIterator(field), terms,
+          termsSize, docids, leaves, numPassages, query);
         
       Object[] result = new Object[docids.length];
       for (int j = 0; j < docidsIn.length; j++) {
@@ -410,6 +514,18 @@
     return highlights;
   }
 
+  private BytesRef[] termsForField(String field, SortedSet<Term> queryTerms) {
+    Term floor = new Term(field, "");
+    Term ceiling = new Term(field, UnicodeUtil.BIG_TERM);
+    SortedSet<Term> fieldTerms = queryTerms.subSet(floor, ceiling);
+    BytesRef[] terms = new BytesRef[fieldTerms.size()];
+    int termUpto = 0;
+    for(Term term : fieldTerms) {
+      terms[termUpto++] = term.bytes();
+    }
+    return terms;
+  }
+
   /** Loads the String values for each field X docID to be
    *  highlighted.  By default this loads from stored
    *  fields, but a subclass can change the source.  This
@@ -453,12 +569,13 @@
     return null;
   }
     
-  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<AtomicReaderContext> leaves, int maxPassages, Query query) throws IOException {  
+  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi,
+      List<MatchedFieldAndTerms> terms, int termsLength, int[] docids, List<AtomicReaderContext> leaves, int maxPassages,
+      Query query) throws IOException {  
     Map<Integer,Object> highlights = new HashMap<Integer,Object>();
     
     // reuse in the real sense... for docs in same segment we just advance our old enum
     DocsAndPositionsEnum postings[] = null;
-    TermsEnum termsEnum = null;
     int lastLeaf = -1;
 
     PassageFormatter fieldFormatter = getFormatter(field);
@@ -472,13 +589,12 @@
     if (analyzer != null) {
       automata = MultiTermHighlighting.extractAutomata(query, field);
     }
-    
-    final BytesRef allTerms[];
+
+    final DocsAndPositionsEnumSource allTerms[];
     if (automata.length > 0) {
-      allTerms = new BytesRef[terms.length + 1];
-      System.arraycopy(terms, 0, allTerms, 0, terms.length);
+      allTerms = new DocsAndPositionsEnumSource[termsLength + 1];
     } else {
-      allTerms = terms;
+      allTerms = new DocsAndPositionsEnumSource[termsLength];
     }
 
     for (int i = 0; i < docids.length; i++) {
@@ -491,20 +607,27 @@
       int leaf = ReaderUtil.subIndex(doc, leaves);
       AtomicReaderContext subContext = leaves.get(leaf);
       AtomicReader r = subContext.reader();
-      Terms t = r.terms(field);
-      if (t == null) {
-        continue; // nothing to do
-      }
       if (leaf != lastLeaf) {
-        termsEnum = t.iterator(null);
+        // We're on a new leaf so we need a new set of TermsEnums for the matched fields
+        int termUpTo = 0;
+        for (MatchedFieldAndTerms matchedFieldAndTerms: terms) {
+          Terms t = r.terms(matchedFieldAndTerms.matchedField);
+          if (t == null) {
+            continue; // nothing to do
+          }
+          TermsEnum termsEnum = t.iterator(null);
+          for (BytesRef term: matchedFieldAndTerms.terms) {
+            allTerms[termUpTo++] = new DocsAndPositionsEnumSource(matchedFieldAndTerms.matchedField, termsEnum, term);
+          }
+        }
         postings = new DocsAndPositionsEnum[allTerms.length];
       }
       if (automata.length > 0) {
         DocsAndPositionsEnum dp = MultiTermHighlighting.getDocsEnum(analyzer.tokenStream(field, content), automata);
         dp.advance(doc - subContext.docBase);
-        postings[terms.length] = dp;
+        postings[termsLength] = dp;
       }
-      Passage passages[] = highlightDoc(field, allTerms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);
+      Passage passages[] = highlightDoc(field, allTerms, content.length(), bi, doc - subContext.docBase, postings, maxPassages);
       if (passages.length == 0) {
         passages = getEmptyHighlight(field, bi, maxPassages);
       }
@@ -522,8 +645,8 @@
   // algorithm: treat sentence snippets as miniature documents
   // we can intersect these with the postings lists via BreakIterator.preceding(offset),s
   // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))
-  private Passage[] highlightDoc(String field, BytesRef terms[], int contentLength, BreakIterator bi, int doc, 
-      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {
+  private Passage[] highlightDoc(String field, DocsAndPositionsEnumSource terms[], int contentLength, BreakIterator bi, int doc, 
+      DocsAndPositionsEnum[] postings, int n) throws IOException {
     PassageScorer scorer = getScorer(field);
     if (scorer == null) {
       throw new NullPointerException("PassageScorer cannot be null");
@@ -538,14 +661,12 @@
         continue;
       } else if (de == null) {
         postings[i] = EMPTY; // initially
-        if (!termsEnum.seekExact(terms[i])) {
+
+        de = terms[i].getDocsAndPositionsEnum();
+        if (de == null) {
           continue; // term not found
         }
-        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);
-        if (de == null) {
-          // no positions available
-          throw new IllegalArgumentException("field '" + field + "' was indexed without offsets, cannot highlight");
-        }
+        postings[i] = de;
         pDoc = de.advance(doc);
       } else {
         pDoc = de.docID();
@@ -633,11 +754,14 @@
       int tf = 0;
       while (true) {
         tf++;
-        BytesRef term = terms[off.id];
-        if (term == null) {
+        DocsAndPositionsEnumSource termSource = terms[off.id];
+        BytesRef term;
+        if (termSource == null) {
           // multitermquery match, pull from payload
           term = off.dp.getPayload();
           assert term != null;
+        } else {
+          term = termSource.term;
         }
         current.addMatch(start, end, term);
         if (off.pos == dp.freq()) {
@@ -712,6 +836,40 @@
       }
     }
   }
+
+  private static class DocsAndPositionsEnumSource {
+    private final String field;
+    private final TermsEnum termsEnum;
+    private final BytesRef term;
+
+    public DocsAndPositionsEnumSource(String field, TermsEnum termsEnum, BytesRef term) {
+      this.field = field;
+      this.termsEnum = termsEnum;
+      this.term = term;
+    }
+
+    public DocsAndPositionsEnum getDocsAndPositionsEnum() throws IOException {
+      if (!termsEnum.seekExact(term)) {
+        return null; // term not found
+      }
+      DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);
+      if (de == null) {
+        // no positions available
+        throw new IllegalArgumentException("field '" + field + "' was indexed without offsets, cannot highlight");
+      }
+      return de;
+    }
+  }
+
+  private static class MatchedFieldAndTerms {
+    private final String matchedField;
+    private final BytesRef[] terms;
+
+    private MatchedFieldAndTerms(String matchedField, BytesRef[] terms) {
+      this.matchedField = matchedField;
+      this.terms = terms;
+    }
+  }
   
   private static final DocsAndPositionsEnum EMPTY = new DocsAndPositionsEnum() {
 
