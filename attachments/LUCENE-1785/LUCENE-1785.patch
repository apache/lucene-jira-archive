Index: src/test/org/apache/lucene/search/TestMergeFieldCache.java
===================================================================
--- src/test/org/apache/lucene/search/TestMergeFieldCache.java	(revision 0)
+++ src/test/org/apache/lucene/search/TestMergeFieldCache.java	(revision 0)
@@ -0,0 +1,226 @@
+package org.apache.lucene.search;
+
+import java.io.IOException;
+import java.lang.reflect.Array;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.SegmentReader;
+import org.apache.lucene.index.SerialMergeScheduler;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.FieldCache.Parser;
+import org.apache.lucene.search.FieldCacheImpl.Cache;
+import org.apache.lucene.search.FieldCacheImpl.Entry;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.MockRAMDirectory;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.ReaderUtil;
+
+public class TestMergeFieldCache extends LuceneTestCase {
+  public static final int NUM_DOCS = 1000;
+
+  static FieldCacheImpl fci = (FieldCacheImpl) FieldCacheImpl.DEFAULT;
+
+  public static int[] sortFields = new int[] { SortField.BYTE,
+      SortField.DOUBLE, SortField.FLOAT, SortField.INT, SortField.LONG,
+      SortField.SHORT, SortField.STRING_VAL };
+
+  public static String[] fields = new String[] { "theByte", "theDouble",
+      "theFloat", "theInt", "theLong", "theShort" };
+
+  public static Parser[] parsers = new Parser[] {
+      FieldCache.DEFAULT_BYTE_PARSER, FieldCache.DEFAULT_DOUBLE_PARSER,
+      FieldCache.DEFAULT_FLOAT_PARSER, FieldCache.DEFAULT_INT_PARSER,
+      FieldCache.DEFAULT_LONG_PARSER, FieldCache.DEFAULT_SHORT_PARSER };
+
+  public static Cache[] caches = new Cache[7];
+  static {
+    caches[0] = fci.caches.get(Byte.TYPE);
+    caches[1] = fci.caches.get(Double.TYPE);
+    caches[2] = fci.caches.get(Float.TYPE);
+    caches[3] = fci.caches.get(Integer.TYPE);
+    caches[4] = fci.caches.get(Long.TYPE);
+    caches[5] = fci.caches.get(Short.TYPE);
+    caches[6] = fci.caches.get(String.class);
+  }
+
+  private static void printSegments(String name, IndexReader reader)
+      throws IOException {
+    List<IndexReader> subReaders = new ArrayList<IndexReader>();
+    ReaderUtil.gatherSubReaders(subReaders, reader);
+    List<String> segments = new ArrayList<String>();
+    for (IndexReader r : subReaders) {
+      segments.add(((SegmentReader) r).getSegmentName());
+    }
+    System.out.println(name + " " + segments);
+  }
+
+  /**
+   * Create a 2 segment index. Search with sort on each field. Optimize, which
+   * will merge the field caches.
+   * 
+   * @throws Exception
+   */
+  public void test() throws Exception {
+    IndexWriter writer = createIndex();
+    Directory dir = writer.getDirectory();
+    // writer.setInfoStream(System.out);
+    assertEquals(NUM_DOCS, writer.numDocs());
+    IndexReader reader = writer.getReader();
+    int numDocs = reader.numDocs();
+    assertEquals(2, reader.getSequentialSubReaders().length);
+    // printSegments("reader", reader);
+    IndexSearcher searcher = new IndexSearcher(reader);
+    // create all the field caches
+    for (int x = 0; x < fields.length; x++) {
+      search(fields[x], sortFields[x], parsers[x], caches[x], searcher);
+    }
+    reader.close();
+    writer.deleteDocuments(new Term("theInt", Integer.MAX_VALUE - 2 + ""));
+    writer.getReader().close();
+    writer.expungeDeletes();
+
+    IndexReader r2 = writer.getReader();
+    //printSegments("r2", r2);
+    assertEquals(2, r2.getSequentialSubReaders().length);
+    assertEquals(numDocs - 1, r2.numDocs());
+    r2.close();
+
+    writer.optimize();
+
+    IndexReader opreader = writer.getReader();
+    //printSegments("opreader", opreader);
+    IndexReader[] subreaders = opreader.getSequentialSubReaders();
+    // we should have one reader
+    assertEquals(1, subreaders.length);
+    IndexReader opreaderSeq = subreaders[0];
+    for (int x = 0; x < fields.length; x++) {
+      Entry entry = new Entry(fields[x], parsers[x]);
+      Object value = caches[x].getIfExists(opreaderSeq, entry);
+      assertNotNull(value);
+      int length = Array.getLength(value);
+      assertEquals(NUM_DOCS - 1, length);
+    }
+    searcher.close();
+    opreader.close();
+
+    writer.close();
+    dir.close();
+  }
+
+  public void search(String field, int sf, Parser parser, Cache cache,
+      IndexSearcher searcher) throws IOException {
+    SortField sortField = new SortField(field, sf);
+    TopDocs td = searcher.search(new MatchAllDocsQuery(), null, 10, new Sort(
+        sortField));
+    Entry entry = new Entry(field, parser);
+    Entry entry2 = new Entry(field, null);
+    IndexReader[] subreaders = searcher.getIndexReader()
+        .getSequentialSubReaders();
+    assertTrue(subreaders.length > 1);
+    for (int x = 0; x < subreaders.length; x++) {
+      Object value = cache.getIfExists(subreaders[x], entry);
+      assertNotNull(field, value);
+      int length = Array.getLength(value);
+      assertEquals(NUM_DOCS / 2, length);
+
+      // for some reason there's 2 keys pointing to the same
+      // value, when this assert breaks, it means this
+      // is fixed
+      Object value2 = cache.getIfExists(subreaders[x], entry2);
+      assertNotNull(field, value2);
+      assertTrue(value == value2);
+    }
+  }
+
+  public void testIntegrity() throws IOException {
+    RAMDirectory directory = new MockRAMDirectory();
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(),
+        true, IndexWriter.MaxFieldLength.LIMITED);
+    writer.setMergeScheduler(new SerialMergeScheduler());
+    for (int x = 0; x < 5; x++) {
+      Document doc = new Document();
+      doc.add(new Field("integer", String.valueOf(x), Field.Store.YES,
+          Field.Index.NOT_ANALYZED));
+      writer.addDocument(doc);
+    }
+    writer.getReader().close();
+    for (int x = 5; x < 10; x++) {
+      Document doc = new Document();
+      doc.add(new Field("integer", String.valueOf(x), Field.Store.YES,
+          Field.Index.NOT_ANALYZED));
+      writer.addDocument(doc);
+    }
+    writer.getReader().close();
+    for (int x = 10; x < 15; x++) {
+      Document doc = new Document();
+      doc.add(new Field("integer", String.valueOf(x), Field.Store.YES,
+          Field.Index.NOT_ANALYZED));
+      writer.addDocument(doc);
+    }
+    IndexReader r = writer.getReader();
+    assertEquals(3, r.getSequentialSubReaders().length);
+    SortField sortField = new SortField("integer", SortField.INT);
+    TopDocs td = new IndexSearcher(r).search(new MatchAllDocsQuery(), null, 10,
+        new Sort(sortField));
+    r.close();
+    writer.deleteDocuments(new Term("integer", "5"));
+    writer.optimize();
+    r = writer.getReader();
+    IndexReader r2 = r.getSequentialSubReaders()[0];
+    td = new IndexSearcher(r).search(new MatchAllDocsQuery(), null, 10,
+        new Sort(sortField));
+    assertEquals(14, r2.maxDoc());
+    int[] ints = FieldCacheImpl.DEFAULT.getInts(r2, "integer", FieldCache.DEFAULT_INT_PARSER);
+    int[] match = new int[] {0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14};
+    assertTrue(Arrays.equals(ints, match));
+    r.close();
+    writer.close();
+    directory.close();
+  }
+
+  /**
+   * Create 2 segments with the different primitive fields
+   * 
+   * @return
+   * @throws Exception
+   */
+  private IndexWriter createIndex() throws Exception {
+    RAMDirectory directory = new MockRAMDirectory();
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(),
+        true, IndexWriter.MaxFieldLength.LIMITED);
+    writer.setMergeScheduler(new SerialMergeScheduler());
+    writer.setMaxBufferedDocs(500);
+    long theLong = Long.MAX_VALUE;
+    double theDouble = Double.MAX_VALUE;
+    byte theByte = Byte.MAX_VALUE;
+    short theShort = Short.MAX_VALUE;
+    int theInt = Integer.MAX_VALUE;
+    float theFloat = Float.MAX_VALUE;
+    for (int i = 0; i < NUM_DOCS; i++) {
+      Document doc = new Document();
+      doc.add(new Field("theLong", String.valueOf(theLong--), Field.Store.NO,
+          Field.Index.NOT_ANALYZED));
+      doc.add(new Field("theDouble", String.valueOf(theDouble--),
+          Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(new Field("theByte", String.valueOf(theByte--), Field.Store.NO,
+          Field.Index.NOT_ANALYZED));
+      
+      doc.add(new Field("theShort", String.valueOf(theShort--), Field.Store.NO,
+          Field.Index.NOT_ANALYZED));
+      doc.add(new Field("theInt", String.valueOf(theInt--), Field.Store.NO,
+          Field.Index.NOT_ANALYZED));
+      doc.add(new Field("theFloat", String.valueOf(theFloat--), Field.Store.NO,
+          Field.Index.NOT_ANALYZED));
+      writer.addDocument(doc);
+    }
+    return writer;
+  }
+}
Index: src/java/org/apache/lucene/search/FieldCacheImpl.java
===================================================================
--- src/java/org/apache/lucene/search/FieldCacheImpl.java	(revision 885059)
+++ src/java/org/apache/lucene/search/FieldCacheImpl.java	(working copy)
@@ -21,16 +21,19 @@
 import java.io.PrintStream;
 import java.util.ArrayList;
 import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 import java.util.WeakHashMap;
 
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermDocs;
 import org.apache.lucene.index.TermEnum;
+import org.apache.lucene.util.FieldCacheSanityChecker;
 import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.FieldCacheSanityChecker;
 
 /**
  * Expert: The default cache implementation, storing all values in memory.
@@ -40,9 +43,9 @@
  *
  * @since   lucene 1.4
  */
-class FieldCacheImpl implements FieldCache {
+public class FieldCacheImpl implements FieldCache {
 	
-  private Map<Class<?>,Cache> caches;
+  Map<Class<?>,Cache> caches;
   FieldCacheImpl() {
     init();
   }
@@ -143,7 +146,24 @@
     
     protected abstract Object createValue(IndexReader reader, Entry key)
         throws IOException;
-
+    
+    public abstract Object merge(IndexReader[] readers, Object[] values, int maxDoc);
+    
+    public Object getIfExists(IndexReader reader, Object key) throws IOException {
+      Map innerCache;
+      Object value;
+      final Object readerKey = reader.getFieldCacheKey();
+      synchronized (readerCache) {
+        innerCache = (Map) readerCache.get(readerKey);
+        if (innerCache == null) {
+          value = null;
+        } else {
+          value = innerCache.get(key);
+        }
+      }
+      return value;
+    }
+    
     public Object get(IndexReader reader, Entry key) throws IOException {
       Map<Entry,Object> innerCache;
       Object value;
@@ -216,6 +236,10 @@
       this.custom = custom;
     }
 
+    public String toString() {
+      return "field:"+field+" custom:"+custom;
+    }
+
     /** Two of these are equal iff they reference the same field and type. */
     @Override
     public boolean equals (Object o) {
@@ -254,7 +278,25 @@
     ByteCache(FieldCache wrapper) {
       super(wrapper);
     }
+    
     @Override
+    public Object merge(IndexReader[] readers, Object[] values, int maxDoc) {
+      byte[] results = new byte[maxDoc];
+      int upTo = 0;
+      for (int i = 0; i < values.length; i++) {
+        int max = readers[i].maxDoc();
+        byte[] src = (byte[]) values[i];
+        IndexReader r = readers[i];
+        for (int doc=0; doc < max; doc++) {
+          if (!r.isDeleted(doc)) {
+            results[upTo++] = src[doc];
+          }
+        }
+      }
+      return results;
+    }
+    
+    @Override
     protected Object createValue(IndexReader reader, Entry entryKey)
         throws IOException {
       Entry entry = entryKey;
@@ -300,6 +342,23 @@
     ShortCache(FieldCache wrapper) {
       super(wrapper);
     }
+    
+    @Override
+    public Object merge(IndexReader[] readers, Object[] values, int maxDoc) {
+      short[] results = new short[maxDoc];
+      int upTo = 0;
+      for (int i = 0; i < values.length; i++) {
+        int max = readers[i].maxDoc();
+        short[] src = (short[]) values[i];
+        IndexReader r = readers[i];
+        for (int doc=0; doc < max; doc++) {
+          if (!r.isDeleted(doc)) {
+            results[upTo++] = src[doc];
+          }
+        }
+      }
+      return results;
+    }
 
     @Override
     protected Object createValue(IndexReader reader, Entry entryKey)
@@ -349,6 +408,23 @@
     }
 
     @Override
+    public Object merge(IndexReader[] readers, Object[] values, int maxDoc) {
+      int[] results = new int[maxDoc];
+      int upTo = 0;
+      for (int i = 0; i < values.length; i++) {
+        int max = readers[i].maxDoc();
+        int[] src = (int[]) values[i];
+        IndexReader r = readers[i];
+        for (int doc=0; doc < max; doc++) {
+          if (!r.isDeleted(doc)) {
+            results[upTo++] = src[doc];
+          }
+        }
+      }
+      return results;
+    }
+
+    @Override
     protected Object createValue(IndexReader reader, Entry entryKey)
         throws IOException {
       Entry entry = entryKey;
@@ -407,6 +483,23 @@
     }
 
     @Override
+    public Object merge(IndexReader[] readers, Object[] values, int maxDoc) {
+      float[] results = new float[maxDoc];
+      int upTo = 0;
+      for (int i = 0; i < values.length; i++) {
+        int max = readers[i].maxDoc();
+        float[] src = (float[]) values[i];
+        IndexReader r = readers[i];
+        for (int doc=0; doc < max; doc++) {
+          if (!r.isDeleted(doc)) {
+            results[upTo++] = src[doc];
+          }
+        }
+      }
+      return results;
+    }
+
+    @Override
     protected Object createValue(IndexReader reader, Entry entryKey)
         throws IOException {
       Entry entry = entryKey;
@@ -462,6 +555,23 @@
     }
 
     @Override
+    public Object merge(IndexReader[] readers, Object[] values, int maxDoc) {
+      long[] results = new long[maxDoc];
+      int upTo = 0;
+      for (int i = 0; i < values.length; i++) {
+        int max = readers[i].maxDoc();
+        long[] src = (long[]) values[i];
+        IndexReader r = readers[i];
+        for (int doc=0; doc < max; doc++) {
+          if (!r.isDeleted(doc)) {
+            results[upTo++] = src[doc];
+          }
+        }
+      }
+      return results;
+    }
+
+    @Override
     protected Object createValue(IndexReader reader, Entry entry)
         throws IOException {
       String field = entry.field;
@@ -517,6 +627,23 @@
     }
 
     @Override
+    public Object merge(IndexReader[] readers, Object[] values, int maxDoc) {
+      double[] results = new double[maxDoc];
+      int upTo = 0;
+      for (int i = 0; i < values.length; i++) {
+        int max = readers[i].maxDoc();
+        double[] src = (double[]) values[i];
+        IndexReader r = readers[i];
+        for (int doc=0; doc < max; doc++) {
+          if (!r.isDeleted(doc)) {
+            results[upTo++] = src[doc];
+          }
+        }
+      }
+      return results;
+    }
+
+    @Override
     protected Object createValue(IndexReader reader, Entry entryKey)
         throws IOException {
       Entry entry = entryKey;
@@ -567,6 +694,23 @@
     }
 
     @Override
+    public Object merge(IndexReader[] readers, Object[] values, int maxDoc) {
+      String[] results = new String[maxDoc];
+      int upTo = 0;
+      for (int i = 0; i < values.length; i++) {
+        int max = readers[i].maxDoc();
+        String[] src = (String[]) values[i];
+        IndexReader r = readers[i];
+        for (int doc=0; doc < max; doc++) {
+          if (!r.isDeleted(doc)) {
+            results[upTo++] = src[doc];
+          }
+        }
+      }
+      return results;
+    }
+
+    @Override
     protected Object createValue(IndexReader reader, Entry entryKey)
         throws IOException {
       String field = StringHelper.intern(entryKey.field);
@@ -601,8 +745,13 @@
     StringIndexCache(FieldCache wrapper) {
       super(wrapper);
     }
-
+    
     @Override
+    public Object merge(IndexReader[] readers, Object[] values, int maxDoc) {
+      throw new UnsupportedOperationException("StringIndex cannot be merged");
+    }
+    
+    @Override
     protected Object createValue(IndexReader reader, Entry entryKey)
         throws IOException {
       String field = StringHelper.intern(entryKey.field);
@@ -668,5 +817,68 @@
   public PrintStream getInfoStream() {
     return infoStream;
   }
+  
+  public void mergeCaches(IndexReader newReader, IndexReader[] readers) throws IOException {
+    mergeCache(newReader, readers, caches.get(Byte.TYPE));
+    //mergeCache(newReader, readers, customCache);
+    mergeCache(newReader, readers, caches.get(Double.TYPE));
+    mergeCache(newReader, readers, caches.get(Float.TYPE));
+    mergeCache(newReader, readers, caches.get(Integer.TYPE));
+    mergeCache(newReader, readers, caches.get(Long.TYPE));
+    mergeCache(newReader, readers, caches.get(Short.TYPE));
+    mergeCache(newReader, readers, caches.get(String.class));
+    //mergeCache(newReader, readers, caches.get(StringIndex.class));
+  }
+
+  public void mergeCache(IndexReader newReader, IndexReader[] readers, Cache cache) throws IOException {
+    Map entryMap = new HashMap();
+    for (int x=0; x < readers.length; x++) {
+      final Object readerKey = readers[x].getFieldCacheKey();
+      synchronized (cache.readerCache) { 
+        Map<Entry,Object> innerCache = cache.readerCache.get(readerKey);
+        if (innerCache == null) {
+          continue;
+        }
+        Iterator<Entry> it = innerCache.keySet().iterator();
+        while (it.hasNext()) {
+          Entry entry = it.next();
+          Set rset = (Set)entryMap.get(entry);
+          if (rset == null) {
+            rset = new HashSet();
+            entryMap.put(entry, rset);
+          }
+          rset.add(readerKey);
+        }
+      }
+    }
+    
+    Iterator it = entryMap.keySet().iterator();
+    while (it.hasNext()) {
+      Entry entry = (Entry)it.next();
+      Set set = (Set)entryMap.get(entry);
+      assert set.size() == readers.length;
+      if (entry.custom != null) {
+        mergeEntry(newReader, readers, entry, cache);
+      }
+    }
+  }
+
+  private void mergeEntry(IndexReader mergedReader, IndexReader[] readers, Entry entry, Cache cache) throws IOException {
+    Object[] values = new Object[readers.length];
+    for (int x=0; x < readers.length; x++) {
+      values[x] = cache.get(readers[x], entry);
+    }
+    Object readerKey = mergedReader.getFieldCacheKey();
+    Object mergedValue = cache.merge(readers, values, mergedReader.maxDoc());
+    Map<Entry,Object> innerCache;
+    synchronized (cache.readerCache) {
+      innerCache = (Map<Entry,Object>) cache.readerCache.get(readerKey);
+      if (innerCache == null) {
+        innerCache = new HashMap<Entry,Object>();
+        cache.readerCache.put(readerKey, innerCache);
+      }
+      innerCache.put(entry, mergedValue);
+    }
+  }
 }
 
Index: src/java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- src/java/org/apache/lucene/index/IndexWriter.java	(revision 885059)
+++ src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -17,32 +17,33 @@
  * limitations under the License.
  */
 
+import java.io.Closeable;
+import java.io.IOException;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.DocumentsWriter.IndexingChain;
+import org.apache.lucene.search.FieldCacheImpl;
+import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Similarity;
-import org.apache.lucene.search.Query;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.BufferedIndexInput;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.Lock;
 import org.apache.lucene.store.LockObtainFailedException;
-import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.BufferedIndexInput;
 import org.apache.lucene.util.Constants;
 import org.apache.lucene.util.ThreadInterruptedException;
 
-import java.io.IOException;
-import java.io.Closeable;
-import java.io.PrintStream;
-import java.util.List;
-import java.util.Collection;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.Set;
-import java.util.HashSet;
-import java.util.LinkedList;
-import java.util.Iterator;
-import java.util.Map;
-
 /**
   An <code>IndexWriter</code> creates and maintains an index.
 
@@ -3904,7 +3905,7 @@
             message("now merge\n  merge=" + merge.segString(directory) + "\n  merge=" + merge + "\n  index=" + segString());
 
           mergeMiddle(merge);
-          mergeSuccess(merge);
+          //mergeSuccess(merge);
           success = true;
         } catch (Throwable t) {
           handleMergeException(t, merge);
@@ -3936,9 +3937,17 @@
   }
 
   /** Hook that's called when the specified merge is complete. */
-  void mergeSuccess(MergePolicy.OneMerge merge) {
+  void mergeSuccess(MergePolicy.OneMerge merge) throws IOException {
+    // by default we merge the field caches of the segment
+    // readers
+    SegmentReader newReader = readerPool.get(merge.info, false);
+    try {
+      ((FieldCacheImpl)FieldCacheImpl.DEFAULT).mergeCaches(newReader, merge.readersClone);
+    } finally {
+      readerPool.release(newReader);
+    }
   }
-  
+
   /** Checks whether this merge involves any segments
    *  already participating in a merge.  If not, this merge
    *  is "registered", meaning we record that its segments
@@ -4279,7 +4288,6 @@
 
       // This is where all the work happens:
       mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);
-
       assert mergedDocCount == totDocCount;
 
       // TODO: in the non-realtime case, we may want to only
@@ -4287,6 +4295,7 @@
       // when we just need deletes)
 
       final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);
+      mergeSuccess(merge);
       try {
         if (poolReaders && mergedSegmentWarmer != null) {
           mergedSegmentWarmer.warm(mergedReader);
