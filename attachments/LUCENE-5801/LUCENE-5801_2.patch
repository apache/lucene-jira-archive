Index: facet/src/test/org/apache/lucene/facet/taxonomy/utils/OrdinalMappingReaderTest.java
===================================================================
--- facet/src/test/org/apache/lucene/facet/taxonomy/utils/OrdinalMappingReaderTest.java	(revision 0)
+++ facet/src/test/org/apache/lucene/facet/taxonomy/utils/OrdinalMappingReaderTest.java	(revision 0)
@@ -0,0 +1,127 @@
+package org.apache.lucene.facet.taxonomy.utils;
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.LabelAndValue;
+import org.apache.lucene.facet.taxonomy.FastTaxonomyFacetCounts;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.MemoryOrdinalMap;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.MultiReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class OrdinalMappingReaderTest extends FacetTestCase {
+	  
+	  private static final int NUM_DOCS = 100;
+	  FacetsConfig facetConfig = new FacetsConfig();
+	  {
+		  facetConfig.setMultiValued("tag", true);
+	  }
+	  
+	  @Test
+	  public void testTaxonomyMergeUtils() throws Exception {
+	    Directory dir = newDirectory();
+	    System.out.println("idx1 dir: "+dir);
+	    Directory taxDir = newDirectory();
+	    buildIndexWithFacets(dir, taxDir, true);
+	    
+	    Directory dir1 = newDirectory();
+	    Directory taxDir1 = newDirectory();
+	    buildIndexWithFacets(dir1, taxDir1, false);
+	    
+	    IndexWriter destIndexWriter = new IndexWriter(dir1, new IndexWriterConfig(TEST_VERSION_CURRENT, null));
+	    DirectoryTaxonomyWriter destTaxWriter = new DirectoryTaxonomyWriter(taxDir1);
+	    try {
+	      TaxonomyMergeUtils.merge(dir, taxDir, new MemoryOrdinalMap(), destIndexWriter, destTaxWriter);
+	    } finally {
+	      IOUtils.close(destIndexWriter, destTaxWriter);
+	    }
+	    
+	    verifyResults(dir1, taxDir1);
+	    dir1.close();
+	    taxDir1.close();
+	    dir.close();
+	    taxDir.close();
+	  }
+
+	  private void verifyResults(Directory dir, Directory taxDir) throws IOException {
+	    DirectoryReader reader1 = DirectoryReader.open(dir);
+	    DirectoryTaxonomyReader taxReader = new DirectoryTaxonomyReader(taxDir);
+	    IndexSearcher searcher = newSearcher(reader1);
+	    
+	    FacetsCollector collector = new FacetsCollector();
+	    FacetsCollector.search(searcher, new MatchAllDocsQuery(), 10, collector);
+	    Facets facets = new FastTaxonomyFacetCounts(taxReader, facetConfig, collector);
+	    FacetResult result = facets.getTopChildren(10, "tag");
+
+	    for (LabelAndValue lv: result.labelValues) {
+	      int weight = lv.value.intValue();
+	      String label = lv.label;
+	      if (VERBOSE) {
+	        System.out.println(label + ": " + weight);
+	      }
+	      assertEquals(NUM_DOCS ,weight);
+	    }
+	    reader1.close();
+	    taxReader.close();
+	  }
+
+	  private void buildIndexWithFacets(Directory dir, Directory taxDir, boolean asc) throws IOException {
+	    IndexWriterConfig config = newIndexWriterConfig(TEST_VERSION_CURRENT, 
+	        new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false));
+	    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, config);
+	    
+	    DirectoryTaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxDir);
+	    for (int i = 1; i <= NUM_DOCS; i++) {
+	      Document doc = new Document();
+	      for (int j = i; j <= NUM_DOCS; j++) {
+	        int facetValue = asc? j: NUM_DOCS - j;
+	        doc.add(new FacetField("tag", Integer.toString(facetValue)));
+	      }
+	      writer.addDocument(facetConfig.build(taxonomyWriter, doc));
+	    }
+	    taxonomyWriter.commit();
+	    taxonomyWriter.close();
+	    writer.commit();
+	    writer.close();
+	  }
+	  
+}
Index: facet/src/java/org/apache/lucene/facet/taxonomy/utils/TaxonomyMergeUtils.java
===================================================================
--- facet/src/java/org/apache/lucene/facet/taxonomy/utils/TaxonomyMergeUtils.java	(revision 0)
+++ facet/src/java/org/apache/lucene/facet/taxonomy/utils/TaxonomyMergeUtils.java	(revision 0)
@@ -0,0 +1,66 @@
+package org.apache.lucene.facet.taxonomy.utils;
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.MultiReader;
+import org.apache.lucene.store.Directory;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Utility methods for merging index and taxonomy directories.
+ * @lucene.experimental
+ */
+public class TaxonomyMergeUtils {
+
+  /**
+   * Merges the given taxonomy and index directories and commits the changes to
+   * the given writers.
+   */
+  public static void merge(Directory srcIndexDir, Directory srcTaxDir, OrdinalMap map, IndexWriter destIndexWriter, 
+      DirectoryTaxonomyWriter destTaxWriter) throws IOException {
+
+	// merge the taxonomies
+    destTaxWriter.addTaxonomy(srcTaxDir, map);
+    int ordinalMap[] = map.getMap();
+    DirectoryReader reader = DirectoryReader.open(srcIndexDir);
+    List<AtomicReaderContext> leaves = reader.leaves();
+    int numReaders = leaves.size();
+    AtomicReader wrappedLeaves[] = new AtomicReader[numReaders];
+    for (int i = 0; i < numReaders; i++) {
+      wrappedLeaves[i] = new OrdinalMappingAtomicReader(leaves.get(i).reader(), ordinalMap);
+    }
+    try {
+      destIndexWriter.addIndexes(new MultiReader(wrappedLeaves));
+      
+      // commit changes to taxonomy and index respectively.
+      destTaxWriter.commit();
+      destIndexWriter.commit();
+    } finally {
+      reader.close();
+    }
+  }
+  
+}
\ No newline at end of file
Index: facet/src/java/org/apache/lucene/facet/taxonomy/utils/OrdinalMappingAtomicReader.java
===================================================================
--- facet/src/java/org/apache/lucene/facet/taxonomy/utils/OrdinalMappingAtomicReader.java	(revision 0)
+++ facet/src/java/org/apache/lucene/facet/taxonomy/utils/OrdinalMappingAtomicReader.java	(revision 0)
@@ -0,0 +1,142 @@
+package org.apache.lucene.facet.taxonomy.utils;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.taxonomy.DocValuesOrdinalsReader;
+import org.apache.lucene.facet.taxonomy.OrdinalsReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.FilterAtomicReader;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/**
+ * A {@link FilterAtomicReader} for updating facets ordinal references,
+ * based on an ordinal map. You should use this code in conjunction with merging
+ * taxonomies - after you merge taxonomies, you receive an {@link OrdinalMap}
+ * which maps the 'old' ordinals to the 'new' ones. You can use that map to
+ * re-map the doc values which contain the facets information (ordinals) either
+ * before or while merging the indexes.
+ * <p>
+ * For re-mapping the ordinals during index merge, do the following:
+ * 
+ * <pre class="prettyprint">
+ * // merge the old taxonomy with the new one.
+ * OrdinalMap map = new MemoryOrdinalMap();
+ * DirectoryTaxonomyWriter.addTaxonomy(srcTaxoDir, map);
+ * int[] ordmap = map.getMap();
+ * 
+ * // Add the index and re-map ordinals on the go
+ * DirectoryReader reader = DirectoryReader.open(oldDir);
+ * IndexWriterConfig conf = new IndexWriterConfig(VER, ANALYZER);
+ * IndexWriter writer = new IndexWriter(newDir, conf);
+ * List&lt;AtomicReaderContext&gt; leaves = reader.leaves();
+ *   AtomicReader wrappedLeaves[] = new AtomicReader[leaves.size()];
+ *   for (int i = 0; i < leaves.size(); i++) {
+ *     wrappedLeaves[i] = new OrdinalMappingAtomicReader(leaves.get(i).reader(), ordmap);
+ *   }
+ * writer.addIndexes(new MultiReader(wrappedLeaves));
+ * writer.commit();
+ * </pre>
+ * 
+ * @lucene.experimental
+ */
+public class OrdinalMappingAtomicReader extends FilterAtomicReader {
+  
+  private final int[] ordinalMap;
+  private final InnerFacetsConfig facetsConfig;
+  
+  /**
+   * Wraps an AtomicReader, mapping ordinals according to the ordinalMap,
+   * using the provided indexingParams.
+   */
+  public OrdinalMappingAtomicReader(AtomicReader in, int[] ordinalMap) {
+    super(in);
+    this.ordinalMap = ordinalMap;
+    facetsConfig = new InnerFacetsConfig();
+  }
+
+  @Override
+  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
+	OrdinalsReader inner = getOrdinalsReader(field);
+
+	return new OrdinalMappingBinaryDocValues(inner);
+  }
+  
+  private class OrdinalMappingBinaryDocValues extends BinaryDocValues {
+
+    private final IntsRef ordinals = new IntsRef(32);
+    private final OrdinalsReader delegate;
+
+
+    protected OrdinalMappingBinaryDocValues(OrdinalsReader delegate) {
+        this.delegate = delegate;
+     }
+    
+    @SuppressWarnings("synthetic-access")
+    @Override
+    public BytesRef get(int docID) {
+      BytesRef result = null;
+      // NOTE: this isn't quite koscher, because in general
+      // multiple threads can call BinaryDV.get which would
+      // then conflict on the single ordinals instance, but
+      // because this impl is only used for merging, we know
+      // only 1 thread calls us:
+      try {
+        delegate.getReader(in.getContext()).get(docID, ordinals);
+        
+        // map the ordinals
+        for (int i = 0; i < ordinals.length; i++) {
+          ordinals.ints[i] = ordinalMap[ordinals.ints[i]];
+        }
+        
+        result = encode(ordinals);
+      } catch (IOException e) {
+    	  e.printStackTrace();
+      }
+        
+      return result;
+    }
+  }
+ 
+  /** Subclass and override to change the field value decoding */
+  protected OrdinalsReader getOrdinalsReader(String field) {
+	  return new DocValuesOrdinalsReader(field);
+  }
+
+  /** Encodes ordinals into a BytesRef; expert: subclass can
+   *  override this to change encoding. */
+  protected BytesRef encode(IntsRef ordinals) {
+	  return facetsConfig.dedupAndEncode(ordinals);
+  }
+  
+  /** Inner class which exposes {@link FacetsConfig#dedupAndEncode()} */
+  private class InnerFacetsConfig extends FacetsConfig {
+
+	@Override
+	public BytesRef dedupAndEncode(IntsRef ordinals) {
+		return super.dedupAndEncode(ordinals);
+	}
+	  
+  }
+}
+
