Index: lucene/core/src/java/org/apache/lucene/index/AtomicReaderContext.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/AtomicReaderContext.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/index/AtomicReaderContext.java	(working copy)
@@ -1,5 +1,8 @@
 package org.apache.lucene.index;
 
+import java.util.Collections;
+import java.util.List;
+
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -28,7 +31,7 @@
   public final int docBase;
   
   private final AtomicReader reader;
-  private final AtomicReaderContext[] leaves;
+  private final List<AtomicReaderContext> leaves;
   
   /**
    * Creates a new {@link AtomicReaderContext} 
@@ -39,7 +42,7 @@
     this.ord = leafOrd;
     this.docBase = leafDocBase;
     this.reader = reader;
-    this.leaves = isTopLevel ? new AtomicReaderContext[] { this } : null;
+    this.leaves = isTopLevel ? Collections.singletonList(this) : null;
   }
   
   AtomicReaderContext(AtomicReader atomicReader) {
@@ -47,12 +50,15 @@
   }
   
   @Override
-  public AtomicReaderContext[] leaves() {
+  public List<AtomicReaderContext> leaves() {
+    if (!isTopLevel)
+      throw new UnsupportedOperationException("This is not a top-level context.");
+    assert leaves != null;
     return leaves;
   }
   
   @Override
-  public IndexReaderContext[] children() {
+  public List<IndexReaderContext> children() {
     return null;
   }
   
Index: lucene/core/src/java/org/apache/lucene/index/BaseCompositeReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/BaseCompositeReader.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/index/BaseCompositeReader.java	(working copy)
@@ -18,6 +18,9 @@
  */
 
 import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
 
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.ReaderUtil;
@@ -47,12 +50,16 @@
  * @lucene.internal
  */
 public abstract class BaseCompositeReader<R extends IndexReader> extends CompositeReader {
-  protected final R[] subReaders;
-  protected final int[] starts;       // 1st docno for each reader
+  private final R[] subReaders;
+  private final int[] starts;       // 1st docno for each reader
   private final int maxDoc;
   private final int numDocs;
   private final boolean hasDeletions;
-  
+
+  /** List view solely for {@link #getSequentialSubReaders()},
+   * for effectiveness the array is used internally. */
+  private final List<R> subReadersList;
+
   /**
    * Constructs a {@code BaseCompositeReader} on the given subReaders.
    * @param subReaders the wrapped sub-readers. This array is returned by
@@ -63,6 +70,7 @@
    */
   protected BaseCompositeReader(R[] subReaders) throws IOException {
     this.subReaders = subReaders;
+    this.subReadersList = Collections.unmodifiableList(Arrays.asList(subReaders));
     starts = new int[subReaders.length + 1];    // build starts array
     int maxDoc = 0, numDocs = 0;
     boolean hasDeletions = false;
@@ -135,8 +143,16 @@
     return ReaderUtil.subIndex(docID, this.starts);
   }
   
+  /** Helper method for subclasses to get the docBase of the given sub-reader index. */
+  protected final int readerBase(int readerIndex) {
+    if (readerIndex < 0 || readerIndex >= subReaders.length) {
+      throw new IllegalArgumentException("readerIndex must be >= 0 and < getSequentialSubReaders().size()");
+    }
+    return this.starts[readerIndex];
+  }
+  
   @Override
-  public final R[] getSequentialSubReaders() {
-    return subReaders;
+  public final List<? extends R> getSequentialSubReaders() {
+    return subReadersList;
   }
 }
Index: lucene/core/src/java/org/apache/lucene/index/CompositeReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/CompositeReader.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/index/CompositeReader.java	(working copy)
@@ -17,6 +17,8 @@
  * limitations under the License.
  */
 
+import java.util.List;
+
 import org.apache.lucene.search.SearcherManager; // javadocs
 import org.apache.lucene.store.*;
 
@@ -63,12 +65,12 @@
     final StringBuilder buffer = new StringBuilder();
     buffer.append(getClass().getSimpleName());
     buffer.append('(');
-    final IndexReader[] subReaders = getSequentialSubReaders();
+    final List<? extends IndexReader> subReaders = getSequentialSubReaders();
     assert subReaders != null;
-    if (subReaders.length > 0) {
-      buffer.append(subReaders[0]);
-      for (int i = 1; i < subReaders.length; ++i) {
-        buffer.append(" ").append(subReaders[i]);
+    if (!subReaders.isEmpty()) {
+      buffer.append(subReaders.get(0));
+      for (int i = 1, c = subReaders.size(); i < c; ++i) {
+        buffer.append(" ").append(subReaders.get(i));
       }
     }
     buffer.append(')');
@@ -81,11 +83,8 @@
    *  If this method returns an empty array, that means this
    *  reader is a null reader (for example a MultiReader
    *  that has no sub readers).
-   *  <p><b>Warning:</b> Don't modify the returned array!
-   *  Doing so will corrupt the internal structure of this
-   *  {@code CompositeReader}.
    */
-  public abstract IndexReader[] getSequentialSubReaders();
+  public abstract List<? extends IndexReader> getSequentialSubReaders();
 
   @Override
   public final CompositeReaderContext getTopReaderContext() {
Index: lucene/core/src/java/org/apache/lucene/index/CompositeReaderContext.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/CompositeReaderContext.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/index/CompositeReaderContext.java	(working copy)
@@ -17,16 +17,18 @@
  * limitations under the License.
  */
 
-import java.io.IOException;
-import org.apache.lucene.util.ReaderUtil;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
 
 /**
  * {@link IndexReaderContext} for {@link CompositeReader} instance.
  * @lucene.experimental
  */
 public final class CompositeReaderContext extends IndexReaderContext {
-  private final IndexReaderContext[] children;
-  private final AtomicReaderContext[] leaves;
+  private final List<IndexReaderContext> children;
+  private final List<AtomicReaderContext> leaves;
   private final CompositeReader reader;
   
   static CompositeReaderContext create(CompositeReader reader) {
@@ -38,34 +40,37 @@
    * not top-level readers in the current context
    */
   CompositeReaderContext(CompositeReaderContext parent, CompositeReader reader,
-      int ordInParent, int docbaseInParent, IndexReaderContext[] children) {
+      int ordInParent, int docbaseInParent, List<IndexReaderContext> children) {
     this(parent, reader, ordInParent, docbaseInParent, children, null);
   }
   
   /**
    * Creates a {@link CompositeReaderContext} for top-level readers with parent set to <code>null</code>
    */
-  CompositeReaderContext(CompositeReader reader, IndexReaderContext[] children, AtomicReaderContext[] leaves) {
+  CompositeReaderContext(CompositeReader reader, List<IndexReaderContext> children, List<AtomicReaderContext> leaves) {
     this(null, reader, 0, 0, children, leaves);
   }
   
   private CompositeReaderContext(CompositeReaderContext parent, CompositeReader reader,
-      int ordInParent, int docbaseInParent, IndexReaderContext[] children,
-      AtomicReaderContext[] leaves) {
+      int ordInParent, int docbaseInParent, List<IndexReaderContext> children,
+      List<AtomicReaderContext> leaves) {
     super(parent, ordInParent, docbaseInParent);
-    this.children = children;
-    this.leaves = leaves;
+    this.children = Collections.unmodifiableList(children);
+    this.leaves = leaves == null ? null : Collections.unmodifiableList(leaves);
     this.reader = reader;
   }
 
   @Override
-  public AtomicReaderContext[] leaves() {
+  public List<AtomicReaderContext> leaves() {
+    if (!isTopLevel)
+      throw new UnsupportedOperationException("This is not a top-level context.");
+    assert leaves != null;
     return leaves;
   }
   
   
   @Override
-  public IndexReaderContext[] children() {
+  public List<IndexReaderContext> children() {
     return children;
   }
   
@@ -76,13 +81,11 @@
   
   private static final class Builder {
     private final CompositeReader reader;
-    private final AtomicReaderContext[] leaves;
-    private int leafOrd = 0;
+    private final List<AtomicReaderContext> leaves = new ArrayList<AtomicReaderContext>();
     private int leafDocBase = 0;
     
     public Builder(CompositeReader reader) {
       this.reader = reader;
-      leaves = new AtomicReaderContext[numLeaves(reader)];
     }
     
     public CompositeReaderContext build() {
@@ -92,14 +95,14 @@
     private IndexReaderContext build(CompositeReaderContext parent, IndexReader reader, int ord, int docBase) {
       if (reader instanceof AtomicReader) {
         final AtomicReader ar = (AtomicReader) reader;
-        final AtomicReaderContext atomic = new AtomicReaderContext(parent, ar, ord, docBase, leafOrd, leafDocBase);
-        leaves[leafOrd++] = atomic;
+        final AtomicReaderContext atomic = new AtomicReaderContext(parent, ar, ord, docBase, leaves.size(), leafDocBase);
+        leaves.add(atomic);
         leafDocBase += reader.maxDoc();
         return atomic;
       } else {
         final CompositeReader cr = (CompositeReader) reader;
-        final IndexReader[] sequentialSubReaders = cr.getSequentialSubReaders();
-        final IndexReaderContext[] children = new IndexReaderContext[sequentialSubReaders.length];
+        final List<? extends IndexReader> sequentialSubReaders = cr.getSequentialSubReaders();
+        final List<IndexReaderContext> children = Arrays.asList(new IndexReaderContext[sequentialSubReaders.size()]);
         final CompositeReaderContext newParent;
         if (parent == null) {
           newParent = new CompositeReaderContext(cr, children, leaves);
@@ -107,31 +110,15 @@
           newParent = new CompositeReaderContext(parent, cr, ord, docBase, children);
         }
         int newDocBase = 0;
-        for (int i = 0; i < sequentialSubReaders.length; i++) {
-          children[i] = build(newParent, sequentialSubReaders[i], i, newDocBase);
-          newDocBase += sequentialSubReaders[i].maxDoc();
+        for (int i = 0, c = sequentialSubReaders.size(); i < c; i++) {
+          final IndexReader r = sequentialSubReaders.get(i);
+          children.set(i, build(newParent, r, i, newDocBase));
+          newDocBase += r.maxDoc();
         }
         assert newDocBase == cr.maxDoc();
         return newParent;
       }
     }
-    
-    private int numLeaves(IndexReader reader) {
-      final int[] numLeaves = new int[1];
-      try {
-        new ReaderUtil.Gather(reader) {
-          @Override
-          protected void add(int base, AtomicReader r) {
-            numLeaves[0]++;
-          }
-        }.run();
-      } catch (IOException ioe) {
-        // won't happen
-        throw new RuntimeException(ioe);
-      }
-      return numLeaves[0];
-    }
-    
   }
 
 }
\ No newline at end of file
Index: lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java	(working copy)
@@ -22,7 +22,6 @@
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.List;
-import java.util.Map;
 
 import org.apache.lucene.search.SearcherManager; // javadocs
 import org.apache.lucene.store.Directory;
Index: lucene/core/src/java/org/apache/lucene/index/IndexReaderContext.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/IndexReaderContext.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/index/IndexReaderContext.java	(working copy)
@@ -1,5 +1,7 @@
 package org.apache.lucene.index;
 
+import java.util.List;
+
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -45,23 +47,19 @@
   public abstract IndexReader reader();
   
   /**
-   * Returns the context's leaves if this context is a top-level context
-   * otherwise <code>null</code>. For convenience, if this is an
-   * {@link AtomicReaderContext} this returns itsself as the only leaf.
+   * Returns the context's leaves if this context is a top-level context.
+   * For convenience, if this is an {@link AtomicReaderContext} this
+   * returns itself as the only leaf.
    * <p>Note: this is convenience method since leaves can always be obtained by
-   * walking the context tree.
-   * <p><b>Warning:</b> Don't modify the returned array!
-   * Doing so will corrupt the internal structure of this
-   * {@code IndexReaderContext}.
+   * walking the context tree using {@link #children()}.
+   * @throws UnsupportedOperationExceception if this is not a top-level context.
+   * @see #children()
    */
-  public abstract AtomicReaderContext[] leaves();
+  public abstract List<AtomicReaderContext> leaves();
   
   /**
    * Returns the context's children iff this context is a composite context
    * otherwise <code>null</code>.
-   * <p><b>Warning:</b> Don't modify the returned array!
-   * Doing so will corrupt the internal structure of this
-   * {@code IndexReaderContext}.
    */
-  public abstract IndexReaderContext[] children();
+  public abstract List<IndexReaderContext> children();
 }
\ No newline at end of file
Index: lucene/core/src/java/org/apache/lucene/index/MultiDocsAndPositionsEnum.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/MultiDocsAndPositionsEnum.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/index/MultiDocsAndPositionsEnum.java	(working copy)
@@ -17,8 +17,8 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.util.ReaderUtil;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.ReaderSlice;
 
 import java.io.IOException;
 
@@ -148,7 +148,7 @@
   // TODO: implement bulk read more efficiently than super
   public final static class EnumWithSlice {
     public DocsAndPositionsEnum docsAndPositionsEnum;
-    public ReaderUtil.Slice slice;
+    public ReaderSlice slice;
   }
 }
 
Index: lucene/core/src/java/org/apache/lucene/index/MultiDocsEnum.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/MultiDocsEnum.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/index/MultiDocsEnum.java	(working copy)
@@ -17,7 +17,8 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.util.ReaderUtil;
+import org.apache.lucene.util.ReaderSlice;
+
 import java.io.IOException;
 import java.util.Arrays;
 
@@ -123,7 +124,7 @@
   // TODO: implement bulk read more efficiently than super
   public final static class EnumWithSlice {
     public DocsEnum docsEnum;
-    public ReaderUtil.Slice slice;
+    public ReaderSlice slice;
     
     @Override
     public String toString() {
Index: lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java	(working copy)
@@ -29,13 +29,18 @@
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.PagedBytes;
 import org.apache.lucene.util.ReaderUtil;
-import org.apache.lucene.util.ReaderUtil.Gather;
 import org.apache.lucene.util.packed.PackedInts.Reader;
 
 /**
  * A wrapper for CompositeIndexReader providing access to per segment
  * {@link DocValues}
  * 
+ * <p><b>NOTE</b>: for multi readers, you'll get better
+ * performance by gathering the sub readers using
+ * {@link IndexReader#getTopReaderContext()} to get the
+ * atomic leaves and then operate per-AtomicReader,
+ * instead of using this class.
+ *
  * @lucene.experimental
  * @lucene.internal
  */
@@ -69,6 +74,8 @@
   }
   
   private static class DocValuesPuller {
+    public DocValuesPuller() {}
+
     public DocValues pull(AtomicReader reader, String field) throws IOException {
       return reader.docValues(field);
     }
@@ -94,8 +101,9 @@
    * their values on the fly.
    * 
    * <p>
-   * <b>NOTE</b>: this is a slow way to access DocValues. It's better to get the
-   * sub-readers (using {@link Gather}) and iterate through them yourself.
+   * <b>NOTE</b>: this is a slow way to access DocValues.
+   * It's better to get the sub-readers and iterate through them
+   * yourself.
    */
   public static DocValues getDocValues(IndexReader r, final String field) throws IOException {
     return getDocValues(r, field, DEFAULT_PULLER);
@@ -106,80 +114,74 @@
    * their values on the fly.
    * 
    * <p>
-   * <b>NOTE</b>: this is a slow way to access DocValues. It's better to get the
-   * sub-readers (using {@link Gather}) and iterate through them yourself.
+   * <b>NOTE</b>: this is a slow way to access DocValues.
+   * It's better to get the sub-readers and iterate through them
+   * yourself.
    */
   public static DocValues getNormDocValues(IndexReader r, final String field) throws IOException {
     return getDocValues(r, field, NORMS_PULLER);
   }
   
  
-  private static DocValues getDocValues(IndexReader r, final String field, final DocValuesPuller puller) throws IOException {
-    if (r instanceof AtomicReader) {
+  private static DocValues getDocValues(IndexReader reader, final String field, final DocValuesPuller puller) throws IOException {
+    if (reader instanceof AtomicReader) {
       // already an atomic reader
-      return puller.pull((AtomicReader) r, field);
+      return puller.pull((AtomicReader) reader, field);
     }
-    assert r instanceof CompositeReader;
-    final IndexReader[] subs = ((CompositeReader) r).getSequentialSubReaders();
-    if (subs.length == 0) {
-      // no fields
-      return null;
-    } else if (subs.length == 1) {
-      return getDocValues(subs[0], field, puller);
-    } else {      
-      final List<DocValuesSlice> slices = new ArrayList<DocValuesSlice>();
-      
-      final TypePromoter promotedType[] = new TypePromoter[1];
-      promotedType[0] = TypePromoter.getIdentityPromoter();
-      
-      // gather all docvalues fields, accumulating a promoted type across 
-      // potentially incompatible types
-      
-      new ReaderUtil.Gather(r) {
-        boolean stop = false;
-        @Override
-        protected void add(int base, AtomicReader r) throws IOException {
-          if (stop) {
-            return;
-          }
+    assert reader instanceof CompositeReader;
+    final List<AtomicReaderContext> leaves = reader.getTopReaderContext().leaves();
+    switch (leaves.size()) {
+      case 0:
+        // no fields
+        return null;
+      case 1:
+        // already an atomic reader / reader with one leave
+        return getDocValues(leaves.get(0).reader(), field, puller);
+      default:
+        final List<DocValuesSlice> slices = new ArrayList<DocValuesSlice>();
+        
+        TypePromoter promotedType =  TypePromoter.getIdentityPromoter();
+        
+        // gather all docvalues fields, accumulating a promoted type across 
+        // potentially incompatible types
+        for (final AtomicReaderContext ctx : leaves) {
+          final AtomicReader r = ctx.reader();
           final DocValues d = puller.pull(r, field);
           if (d != null) {
             TypePromoter incoming = TypePromoter.create(d.getType(), d.getValueSize());
-            promotedType[0] = promotedType[0].promote(incoming);
+            promotedType = promotedType.promote(incoming);
           } else if (puller.stopLoadingOnNull(r, field)){
-            promotedType[0] = TypePromoter.getIdentityPromoter(); // set to identity to return null
-            stop = true;
+            return null;
           }
-          slices.add(new DocValuesSlice(d, base, r.maxDoc()));
+          slices.add(new DocValuesSlice(d, ctx.docBase, r.maxDoc()));
         }
-      }.run();
-      
-      // return null if no docvalues encountered anywhere
-      if (promotedType[0] == TypePromoter.getIdentityPromoter()) {
-        return null;
-      }
-           
-      // populate starts and fill gaps with empty docvalues 
-      int starts[] = new int[slices.size()];
-      for (int i = 0; i < slices.size(); i++) {
-        DocValuesSlice slice = slices.get(i);
-        starts[i] = slice.start;
-        if (slice.docValues == null) {
-          Type promoted = promotedType[0].type();
-          switch(promoted) {
-            case BYTES_FIXED_DEREF:
-            case BYTES_FIXED_STRAIGHT:
-            case BYTES_FIXED_SORTED:
-              assert promotedType[0].getValueSize() >= 0;
-              slice.docValues = new EmptyFixedDocValues(slice.length, promoted, promotedType[0].getValueSize());
-              break;
-            default:
-              slice.docValues = new EmptyDocValues(slice.length, promoted);
+        
+        // return null if no docvalues encountered anywhere
+        if (promotedType == TypePromoter.getIdentityPromoter()) {
+          return null;
+        }
+             
+        // populate starts and fill gaps with empty docvalues 
+        int starts[] = new int[slices.size()];
+        for (int i = 0; i < slices.size(); i++) {
+          DocValuesSlice slice = slices.get(i);
+          starts[i] = slice.start;
+          if (slice.docValues == null) {
+            Type promoted = promotedType.type();
+            switch(promoted) {
+              case BYTES_FIXED_DEREF:
+              case BYTES_FIXED_STRAIGHT:
+              case BYTES_FIXED_SORTED:
+                assert promotedType.getValueSize() >= 0;
+                slice.docValues = new EmptyFixedDocValues(slice.length, promoted, promotedType.getValueSize());
+                break;
+              default:
+                slice.docValues = new EmptyDocValues(slice.length, promoted);
+            }
           }
         }
-      }
-      
-      return new MultiDocValues(slices.toArray(new DocValuesSlice[slices.size()]), starts, promotedType[0]);
+        
+        return new MultiDocValues(slices.toArray(new DocValuesSlice[slices.size()]), starts, promotedType);
     }
   }
 
Index: lucene/core/src/java/org/apache/lucene/index/MultiFields.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/MultiFields.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/index/MultiFields.java	(working copy)
@@ -28,8 +28,7 @@
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.MultiBits;
-import org.apache.lucene.util.ReaderUtil.Gather;  // for javadocs
-import org.apache.lucene.util.ReaderUtil;
+import org.apache.lucene.util.ReaderSlice;
 
 /**
  * Exposes flex API, merged from flex API of sub-segments.
@@ -38,9 +37,10 @@
  * sub-readers (eg {@link DirectoryReader} or {@link
  * MultiReader}).
  *
- * <p><b>NOTE</b>: for multi readers, you'll get better
- * performance by gathering the sub readers using {@link
- * ReaderUtil#gatherSubReaders} and then operate per-reader,
+ * <p><b>NOTE</b>: for composite readers, you'll get better
+ * performance by gathering the sub readers using
+ * {@link IndexReader#getTopReaderContext()} to get the
+ * atomic leaves and then operate per-AtomicReader,
  * instead of using this class.
  *
  * @lucene.experimental
@@ -48,7 +48,7 @@
 
 public final class MultiFields extends Fields {
   private final Fields[] subs;
-  private final ReaderUtil.Slice[] subSlices;
+  private final ReaderSlice[] subSlices;
   private final Map<String,Terms> terms = new ConcurrentHashMap<String,Terms>();
 
   /** Returns a single {@link Fields} instance for this
@@ -57,72 +57,57 @@
    *  has no postings.
    *
    *  <p><b>NOTE</b>: this is a slow way to access postings.
-   *  It's better to get the sub-readers (using {@link
-   *  Gather}) and iterate through them
+   *  It's better to get the sub-readers and iterate through them
    *  yourself. */
-  public static Fields getFields(IndexReader r) throws IOException {
-    if (r instanceof AtomicReader) {
-      // already an atomic reader
-      return ((AtomicReader) r).fields();
-    }
-    assert r instanceof CompositeReader;
-    final IndexReader[] subs = ((CompositeReader) r).getSequentialSubReaders();
-    if (subs.length == 0) {
-      // no fields
-      return null;
-    } else {
-      final List<Fields> fields = new ArrayList<Fields>();
-      final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();
-
-      new ReaderUtil.Gather(r) {
-        @Override
-        protected void add(int base, AtomicReader r) throws IOException {
+  public static Fields getFields(IndexReader reader) throws IOException {
+    final List<AtomicReaderContext> leaves = reader.getTopReaderContext().leaves();
+    switch (leaves.size()) {
+      case 0:
+        // no fields
+        return null;
+      case 1:
+        // already an atomic reader / reader with one leave
+        return leaves.get(0).reader().fields();
+      default:
+        final List<Fields> fields = new ArrayList<Fields>();
+        final List<ReaderSlice> slices = new ArrayList<ReaderSlice>();
+        for (final AtomicReaderContext ctx : leaves) {
+          final AtomicReader r = ctx.reader();
           final Fields f = r.fields();
           if (f != null) {
             fields.add(f);
-            slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));
+            slices.add(new ReaderSlice(ctx.docBase, r.maxDoc(), fields.size()-1));
           }
         }
-      }.run();
-
-      if (fields.isEmpty()) {
-        return null;
-      } else if (fields.size() == 1) {
-        return fields.get(0);
-      } else {
-        return new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),
-                                       slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY));
-      }
+        if (fields.isEmpty()) {
+          return null;
+        } else if (fields.size() == 1) {
+          return fields.get(0);
+        } else {
+          return new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),
+                                         slices.toArray(ReaderSlice.EMPTY_ARRAY));
+        }
     }
   }
 
-  public static Bits getLiveDocs(IndexReader r) {
-    if (r.hasDeletions()) {
-      final List<Bits> liveDocs = new ArrayList<Bits>();
-      final List<Integer> starts = new ArrayList<Integer>();
-
-      try {
-        final int maxDoc = new ReaderUtil.Gather(r) {
-            @Override
-            protected void add(int base, AtomicReader r) throws IOException {
-              // record all liveDocs, even if they are null
-              liveDocs.add(r.getLiveDocs());
-              starts.add(base);
-            }
-          }.run();
-        starts.add(maxDoc);
-      } catch (IOException ioe) {
-        // should not happen
-        throw new RuntimeException(ioe);
+  public static Bits getLiveDocs(IndexReader reader) {
+    if (reader.hasDeletions()) {
+      final List<AtomicReaderContext> leaves = reader.getTopReaderContext().leaves();
+      final int size = leaves.size();
+      assert size > 0 : "A reader with deletions must have at least one leave";
+      if (size == 1) {
+        return leaves.get(0).reader().getLiveDocs();
       }
-
-      assert liveDocs.size() > 0;
-      if (liveDocs.size() == 1) {
-        // Only one actual sub reader -- optimize this case
-        return liveDocs.get(0);
-      } else {
-        return new MultiBits(liveDocs, starts, true);
+      final Bits[] liveDocs = new Bits[size];
+      final int[] starts = new int[size + 1];
+      for (int i = 0; i < size; i++) {
+        // record all liveDocs, even if they are null
+        final AtomicReaderContext ctx = leaves.get(i);
+        liveDocs[i] = ctx.reader().getLiveDocs();
+        starts[i] = ctx.docBase;
       }
+      starts[size] = reader.maxDoc();
+      return new MultiBits(liveDocs, starts, true);
     } else {
       return null;
     }
@@ -170,7 +155,7 @@
     return null;
   }
 
-  public MultiFields(Fields[] subs, ReaderUtil.Slice[] subSlices) {
+  public MultiFields(Fields[] subs, ReaderSlice[] subSlices) {
     this.subs = subs;
     this.subSlices = subSlices;
   }
@@ -179,7 +164,7 @@
   public FieldsEnum iterator() throws IOException {
 
     final List<FieldsEnum> fieldsEnums = new ArrayList<FieldsEnum>();
-    final List<ReaderUtil.Slice> fieldsSlices = new ArrayList<ReaderUtil.Slice>();
+    final List<ReaderSlice> fieldsSlices = new ArrayList<ReaderSlice>();
     for(int i=0;i<subs.length;i++) {
       fieldsEnums.add(subs[i].iterator());
       fieldsSlices.add(subSlices[i]);
@@ -189,13 +174,12 @@
     } else {
       return new MultiFieldsEnum(this,
                                  fieldsEnums.toArray(FieldsEnum.EMPTY_ARRAY),
-                                 fieldsSlices.toArray(ReaderUtil.Slice.EMPTY_ARRAY));
+                                 fieldsSlices.toArray(ReaderSlice.EMPTY_ARRAY));
     }
   }
 
   @Override
   public Terms terms(String field) throws IOException {
-
     Terms result = terms.get(field);
     if (result != null)
       return result;
@@ -204,7 +188,7 @@
     // Lazy init: first time this field is requested, we
     // create & add to terms:
     final List<Terms> subs2 = new ArrayList<Terms>();
-    final List<ReaderUtil.Slice> slices2 = new ArrayList<ReaderUtil.Slice>();
+    final List<ReaderSlice> slices2 = new ArrayList<ReaderSlice>();
 
     // Gather all sub-readers that share this field
     for(int i=0;i<subs.length;i++) {
@@ -220,7 +204,7 @@
       // is unbounded.
     } else {
       result = new MultiTerms(subs2.toArray(Terms.EMPTY_ARRAY),
-          slices2.toArray(ReaderUtil.Slice.EMPTY_ARRAY));
+          slices2.toArray(ReaderSlice.EMPTY_ARRAY));
       terms.put(field, result);
     }
 
@@ -252,18 +236,16 @@
    *  will be unavailable.
    */
   public static FieldInfos getMergedFieldInfos(IndexReader reader) {
-    final List<AtomicReader> subReaders = new ArrayList<AtomicReader>();
-    ReaderUtil.gatherSubReaders(subReaders, reader);
     final FieldInfos.Builder builder = new FieldInfos.Builder();
-    for(AtomicReader subReader : subReaders) {
-      builder.add(subReader.getFieldInfos());
+    for(final AtomicReaderContext ctx : reader.getTopReaderContext().leaves()) {
+      builder.add(ctx.reader().getFieldInfos());
     }
     return builder.finish();
   }
 
   public static Collection<String> getIndexedFields(IndexReader reader) {
     final Collection<String> fields = new HashSet<String>();
-    for(FieldInfo fieldInfo : getMergedFieldInfos(reader)) {
+    for(final FieldInfo fieldInfo : getMergedFieldInfos(reader)) {
       if (fieldInfo.isIndexed()) {
         fields.add(fieldInfo.name);
       }
Index: lucene/core/src/java/org/apache/lucene/index/MultiFieldsEnum.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/MultiFieldsEnum.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/index/MultiFieldsEnum.java	(working copy)
@@ -18,7 +18,7 @@
  */
 
 import org.apache.lucene.util.PriorityQueue;
-import org.apache.lucene.util.ReaderUtil;
+import org.apache.lucene.util.ReaderSlice;
 
 import java.io.IOException;
 import java.util.List;
@@ -47,7 +47,7 @@
 
   /** The subs array must be newly initialized FieldsEnum
    *  (ie, {@link FieldsEnum#next} has not been called. */
-  public MultiFieldsEnum(MultiFields fields, FieldsEnum[] subs, ReaderUtil.Slice[] subSlices) throws IOException {
+  public MultiFieldsEnum(MultiFields fields, FieldsEnum[] subs, ReaderSlice[] subSlices) throws IOException {
     this.fields = fields;
     queue = new FieldMergeQueue(subs.length);
     top = new FieldsEnumWithSlice[subs.length];
@@ -107,11 +107,11 @@
   public final static class FieldsEnumWithSlice {
     public static final FieldsEnumWithSlice[] EMPTY_ARRAY = new FieldsEnumWithSlice[0];
     final FieldsEnum fields;
-    final ReaderUtil.Slice slice;
+    final ReaderSlice slice;
     final int index;
     String current;
 
-    public FieldsEnumWithSlice(FieldsEnum fields, ReaderUtil.Slice slice, int index) throws IOException {
+    public FieldsEnumWithSlice(FieldsEnum fields, ReaderSlice slice, int index) throws IOException {
       this.slice = slice;
       this.index = index;
       assert slice.length >= 0: "length=" + slice.length;
Index: lucene/core/src/java/org/apache/lucene/index/MultiReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/MultiReader.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/index/MultiReader.java	(working copy)
@@ -68,12 +68,12 @@
   @Override
   protected synchronized void doClose() throws IOException {
     IOException ioe = null;
-    for (int i = 0; i < subReaders.length; i++) {
+    for (final IndexReader r : getSequentialSubReaders()) {
       try {
         if (closeSubReaders) {
-          subReaders[i].close();
+          r.close();
         } else {
-          subReaders[i].decRef();
+          r.decRef();
         }
       } catch (IOException e) {
         if (ioe == null) ioe = e;
Index: lucene/core/src/java/org/apache/lucene/index/MultiTerms.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/MultiTerms.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/index/MultiTerms.java	(working copy)
@@ -23,7 +23,7 @@
 import java.util.List;
 
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.ReaderUtil;
+import org.apache.lucene.util.ReaderSlice;
 
 import org.apache.lucene.util.automaton.CompiledAutomaton;
 
@@ -36,10 +36,10 @@
 
 public final class MultiTerms extends Terms {
   private final Terms[] subs;
-  private final ReaderUtil.Slice[] subSlices;
+  private final ReaderSlice[] subSlices;
   private final Comparator<BytesRef> termComp;
 
-  public MultiTerms(Terms[] subs, ReaderUtil.Slice[] subSlices) throws IOException {
+  public MultiTerms(Terms[] subs, ReaderSlice[] subSlices) throws IOException {
     this.subs = subs;
     this.subSlices = subSlices;
     
Index: lucene/core/src/java/org/apache/lucene/index/MultiTermsEnum.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/MultiTermsEnum.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/index/MultiTermsEnum.java	(working copy)
@@ -22,7 +22,7 @@
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BitsSlice;
 import org.apache.lucene.util.MultiBits;
-import org.apache.lucene.util.ReaderUtil;
+import org.apache.lucene.util.ReaderSlice;
 
 import java.io.IOException;
 import java.util.Arrays;
@@ -71,7 +71,7 @@
     return top;
   }
 
-  public MultiTermsEnum(ReaderUtil.Slice[] slices) {
+  public MultiTermsEnum(ReaderSlice[] slices) {
     queue = new TermMergeQueue(slices.length);
     top = new TermsEnumWithSlice[slices.length];
     subs = new TermsEnumWithSlice[slices.length];
@@ -494,12 +494,12 @@
   }
 
   private final static class TermsEnumWithSlice {
-    private final ReaderUtil.Slice subSlice;
+    private final ReaderSlice subSlice;
     private TermsEnum terms;
     public BytesRef current;
     final int index;
 
-    public TermsEnumWithSlice(int index, ReaderUtil.Slice subSlice) {
+    public TermsEnumWithSlice(int index, ReaderSlice subSlice) {
       this.subSlice = subSlice;
       this.index = index;
       assert subSlice.length >= 0: "length=" + subSlice.length;
Index: lucene/core/src/java/org/apache/lucene/index/ParallelCompositeReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/ParallelCompositeReader.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/index/ParallelCompositeReader.java	(working copy)
@@ -21,6 +21,7 @@
 import java.util.Collections;
 import java.util.IdentityHashMap;
 import java.util.Iterator;
+import java.util.List;
 import java.util.Set;
 
 /** An {@link CompositeReader} which reads multiple, parallel indexes.  Each index added
@@ -85,44 +86,45 @@
         throw new IllegalArgumentException("There must be at least one main reader if storedFieldsReaders are used.");
       return new IndexReader[0];
     } else {
-      final IndexReader[] firstSubReaders = readers[0].getSequentialSubReaders();
+      final List<? extends IndexReader> firstSubReaders = readers[0].getSequentialSubReaders();
 
       // check compatibility:
-      final int maxDoc = readers[0].maxDoc();
-      final int[] childMaxDoc = new int[firstSubReaders.length];
-      final boolean[] childAtomic = new boolean[firstSubReaders.length];
-      for (int i = 0; i < firstSubReaders.length; i++) {
-        childMaxDoc[i] = firstSubReaders[i].maxDoc();
-        childAtomic[i] = firstSubReaders[i] instanceof AtomicReader;
+      final int maxDoc = readers[0].maxDoc(), noSubs = firstSubReaders.size();
+      final int[] childMaxDoc = new int[noSubs];
+      final boolean[] childAtomic = new boolean[noSubs];
+      for (int i = 0; i < noSubs; i++) {
+        final IndexReader r = firstSubReaders.get(i);
+        childMaxDoc[i] = r.maxDoc();
+        childAtomic[i] = r instanceof AtomicReader;
       }
       validate(readers, maxDoc, childMaxDoc, childAtomic);
       validate(storedFieldsReaders, maxDoc, childMaxDoc, childAtomic);
 
       // hierarchically build the same subreader structure as the first CompositeReader with Parallel*Readers:
-      final IndexReader[] subReaders = new IndexReader[firstSubReaders.length];
+      final IndexReader[] subReaders = new IndexReader[noSubs];
       for (int i = 0; i < subReaders.length; i++) {
-        if (firstSubReaders[i] instanceof AtomicReader) {
+        if (firstSubReaders.get(i) instanceof AtomicReader) {
           final AtomicReader[] atomicSubs = new AtomicReader[readers.length];
           for (int j = 0; j < readers.length; j++) {
-            atomicSubs[j] = (AtomicReader) readers[j].getSequentialSubReaders()[i];
+            atomicSubs[j] = (AtomicReader) readers[j].getSequentialSubReaders().get(i);
           }
           final AtomicReader[] storedSubs = new AtomicReader[storedFieldsReaders.length];
           for (int j = 0; j < storedFieldsReaders.length; j++) {
-            storedSubs[j] = (AtomicReader) storedFieldsReaders[j].getSequentialSubReaders()[i];
+            storedSubs[j] = (AtomicReader) storedFieldsReaders[j].getSequentialSubReaders().get(i);
           }
           // we simply enable closing of subReaders, to prevent incRefs on subReaders
           // -> for synthetic subReaders, close() is never
           // called by our doClose()
           subReaders[i] = new ParallelAtomicReader(true, atomicSubs, storedSubs);
         } else {
-          assert firstSubReaders[i] instanceof CompositeReader;
+          assert firstSubReaders.get(i) instanceof CompositeReader;
           final CompositeReader[] compositeSubs = new CompositeReader[readers.length];
           for (int j = 0; j < readers.length; j++) {
-            compositeSubs[j] = (CompositeReader) readers[j].getSequentialSubReaders()[i];
+            compositeSubs[j] = (CompositeReader) readers[j].getSequentialSubReaders().get(i);
           }
           final CompositeReader[] storedSubs = new CompositeReader[storedFieldsReaders.length];
           for (int j = 0; j < storedFieldsReaders.length; j++) {
-            storedSubs[j] = (CompositeReader) storedFieldsReaders[j].getSequentialSubReaders()[i];
+            storedSubs[j] = (CompositeReader) storedFieldsReaders[j].getSequentialSubReaders().get(i);
           }
           // we simply enable closing of subReaders, to prevent incRefs on subReaders
           // -> for synthetic subReaders, close() is never called by our doClose()
@@ -136,18 +138,20 @@
   private static void validate(CompositeReader[] readers, int maxDoc, int[] childMaxDoc, boolean[] childAtomic) {
     for (int i = 0; i < readers.length; i++) {
       final CompositeReader reader = readers[i];
-      final IndexReader[] subs = reader.getSequentialSubReaders();
+      final List<? extends IndexReader> subs = reader.getSequentialSubReaders();
       if (reader.maxDoc() != maxDoc) {
         throw new IllegalArgumentException("All readers must have same maxDoc: "+maxDoc+"!="+reader.maxDoc());
       }
-      if (subs.length != childMaxDoc.length) {
+      final int noSubs = subs.size();
+      if (noSubs != childMaxDoc.length) {
         throw new IllegalArgumentException("All readers must have same number of subReaders");
       }
-      for (int subIDX = 0; subIDX < subs.length; subIDX++) {
-        if (subs[subIDX].maxDoc() != childMaxDoc[subIDX]) {
+      for (int subIDX = 0; subIDX < noSubs; subIDX++) {
+        final IndexReader r = subs.get(subIDX);
+        if (r.maxDoc() != childMaxDoc[subIDX]) {
           throw new IllegalArgumentException("All readers must have same corresponding subReader maxDoc");
         }
-        if (!(childAtomic[subIDX] ? (subs[subIDX] instanceof AtomicReader) : (subs[subIDX] instanceof CompositeReader))) {
+        if (!(childAtomic[subIDX] ? (r instanceof AtomicReader) : (r instanceof CompositeReader))) {
           throw new IllegalArgumentException("All readers must have same corresponding subReader types (atomic or composite)");
         }
       }
Index: lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java	(working copy)
@@ -35,6 +35,7 @@
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.InfoStream;
 import org.apache.lucene.util.ReaderUtil;
+import org.apache.lucene.util.ReaderSlice;
 
 /**
  * The SegmentMerger class combines two or more Segments, represented by an IndexReader ({@link #add},
@@ -76,16 +77,9 @@
    * @param reader
    */
   final void add(IndexReader reader) {
-    try {
-      new ReaderUtil.Gather(reader) {
-        @Override
-        protected void add(int base, AtomicReader r) {
-          mergeState.readers.add(new MergeState.IndexReaderAndLiveDocs(r, r.getLiveDocs(), r.numDeletedDocs()));
-        }
-      }.run();
-    } catch (IOException ioe) {
-      // won't happen
-      throw new RuntimeException(ioe);
+    for (final AtomicReaderContext ctx : reader.getTopReaderContext().leaves()) {
+      final AtomicReader r = ctx.reader();
+      mergeState.readers.add(new MergeState.IndexReaderAndLiveDocs(r, r.getLiveDocs(), r.numDeletedDocs()));
     }
   }
 
@@ -311,7 +305,7 @@
   private final void mergeTerms(SegmentWriteState segmentWriteState) throws CorruptIndexException, IOException {
     
     final List<Fields> fields = new ArrayList<Fields>();
-    final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();
+    final List<ReaderSlice> slices = new ArrayList<ReaderSlice>();
 
     int docBase = 0;
 
@@ -320,7 +314,7 @@
       final Fields f = r.reader.fields();
       final int maxDoc = r.reader.maxDoc();
       if (f != null) {
-        slices.add(new ReaderUtil.Slice(docBase, maxDoc, readerIndex));
+        slices.add(new ReaderSlice(docBase, maxDoc, readerIndex));
         fields.add(f);
       }
       docBase += maxDoc;
@@ -331,7 +325,7 @@
     try {
       consumer.merge(mergeState,
                      new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),
-                                     slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY)));
+                                     slices.toArray(ReaderSlice.EMPTY_ARRAY)));
       success = true;
     } finally {
       if (success) {
Index: lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java	(working copy)
@@ -22,7 +22,6 @@
 import java.util.Map;
 
 import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.ReaderUtil; // javadoc
 
 import org.apache.lucene.index.DirectoryReader; // javadoc
 import org.apache.lucene.index.MultiReader; // javadoc
@@ -32,17 +31,16 @@
  * MultiReader} or {@link DirectoryReader}) to emulate an
  * atomic reader.  This requires implementing the postings
  * APIs on-the-fly, using the static methods in {@link
- * MultiFields}, {@link MultiDocValues}, 
- * by stepping through the sub-readers to merge fields/terms, 
- * appending docs, etc.
+ * MultiFields}, {@link MultiDocValues}, by stepping through
+ * the sub-readers to merge fields/terms, appending docs, etc.
  *
  * <p><b>NOTE</b>: this class almost always results in a
  * performance hit.  If this is important to your use case,
- * it's better to get the sequential sub readers (see {@link
- * ReaderUtil#gatherSubReaders}, instead, and iterate through them
- * yourself.</p>
+ * you'll get better performance by gathering the sub readers using
+ * {@link IndexReader#getTopReaderContext()} to get the
+ * atomic leaves and then operate per-AtomicReader,
+ * instead of using this class.
  */
-
 public final class SlowCompositeReaderWrapper extends AtomicReader {
 
   private final CompositeReader in;
Index: lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java	(working copy)
@@ -118,8 +118,8 @@
       writer, segmentInfos, writer.getConfig().getReaderTermsIndexDivisor(), applyAllDeletes);
   }
 
-  /** This constructor is only used for {@link #doOpenIfChanged()} */
-  private static DirectoryReader open(Directory directory, IndexWriter writer, SegmentInfos infos, AtomicReader[] oldReaders,
+  /** This constructor is only used for {@link #doOpenIfChanged(SegmentInfos, IndexWriter)} */
+  private static DirectoryReader open(Directory directory, IndexWriter writer, SegmentInfos infos, List<? extends AtomicReader> oldReaders,
     int termInfosIndexDivisor) throws IOException {
     // we put the old SegmentReaders in a map, that allows us
     // to lookup a reader using its segment name
@@ -127,8 +127,9 @@
 
     if (oldReaders != null) {
       // create a Map SegmentName->SegmentReader
-      for (int i = 0; i < oldReaders.length; i++) {
-        segmentReaders.put(((SegmentReader) oldReaders[i]).getSegmentName(), Integer.valueOf(i));
+      for (int i = 0, c = oldReaders.size(); i < c; i++) {
+        final SegmentReader sr = (SegmentReader) oldReaders.get(i);
+        segmentReaders.put(sr.getSegmentName(), Integer.valueOf(i));
       }
     }
     
@@ -146,7 +147,7 @@
         newReaders[i] = null;
       } else {
         // there is an old reader for this segment - we'll try to reopen it
-        newReaders[i] = (SegmentReader) oldReaders[oldReaderIndex.intValue()];
+        newReaders[i] = (SegmentReader) oldReaders.get(oldReaderIndex.intValue());
       }
 
       boolean success = false;
@@ -216,9 +217,9 @@
     if (writer != null) {
       buffer.append(":nrt");
     }
-    for(int i=0;i<subReaders.length;i++) {
+    for (final IndexReader r : getSequentialSubReaders()) {
       buffer.append(' ');
-      buffer.append(subReaders[i]);
+      buffer.append(r);
     }
     buffer.append(')');
     return buffer.toString();
@@ -298,7 +299,7 @@
   }
 
   synchronized DirectoryReader doOpenIfChanged(SegmentInfos infos, IndexWriter writer) throws CorruptIndexException, IOException {
-    return StandardDirectoryReader.open(directory, writer, infos, subReaders, termInfosIndexDivisor);
+    return StandardDirectoryReader.open(directory, writer, infos, getSequentialSubReaders(), termInfosIndexDivisor);
   }
 
   @Override
@@ -329,10 +330,10 @@
   @Override
   protected synchronized void doClose() throws IOException {
     IOException ioe = null;
-    for (int i = 0; i < subReaders.length; i++) {
+    for (final IndexReader r : getSequentialSubReaders()) {
       // try to close each reader, even if an exception is thrown
       try {
-        subReaders[i].decRef();
+        r.decRef();
       } catch (IOException e) {
         if (ioe == null) ioe = e;
       }
Index: lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java	(working copy)
@@ -18,7 +18,9 @@
  */
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.Iterator;
+import java.util.List;
 import java.util.NoSuchElementException;
 import java.util.Set;
 import java.util.concurrent.Callable;
@@ -80,7 +82,7 @@
   // NOTE: these members might change in incompatible ways
   // in the next release
   protected final IndexReaderContext readerContext;
-  protected final AtomicReaderContext[] leafContexts;
+  protected final List<AtomicReaderContext> leafContexts;
   // used with executor - each slice holds a set of leafs executed within one thread
   protected final LeafSlice[] leafSlices;
 
@@ -165,10 +167,10 @@
    * Each {@link LeafSlice} is executed in a single thread. By default there
    * will be one {@link LeafSlice} per leaf ({@link AtomicReaderContext}).
    */
-  protected LeafSlice[] slices(AtomicReaderContext...leaves) {
-    LeafSlice[] slices = new LeafSlice[leaves.length];
+  protected LeafSlice[] slices(List<AtomicReaderContext> leaves) {
+    LeafSlice[] slices = new LeafSlice[leaves.size()];
     for (int i = 0; i < slices.length; i++) {
-      slices[i] = new LeafSlice(leaves[i]);
+      slices[i] = new LeafSlice(leaves.get(i));
     }
     return slices;
   }
@@ -440,7 +442,7 @@
    * {@link IndexSearcher#search(Query,Filter,int)} instead.
    * @throws BooleanQuery.TooManyClauses
    */
-  protected TopDocs search(AtomicReaderContext[] leaves, Weight weight, ScoreDoc after, int nDocs) throws IOException {
+  protected TopDocs search(List<AtomicReaderContext> leaves, Weight weight, ScoreDoc after, int nDocs) throws IOException {
     // single thread
     int limit = reader.maxDoc();
     if (limit == 0) {
@@ -477,7 +479,7 @@
    * <p>NOTE: this does not compute scores by default.  If you
    * need scores, create a {@link TopFieldCollector}
    * instance by calling {@link TopFieldCollector#create} and
-   * then pass that to {@link #search(AtomicReaderContext[], Weight,
+   * then pass that to {@link #search(List, Weight,
    * Collector)}.</p>
    */
   protected TopFieldDocs search(Weight weight, FieldDoc after, int nDocs,
@@ -525,7 +527,7 @@
    * whether or not the fields in the returned {@link FieldDoc} instances should
    * be set by specifying fillFields.
    */
-  protected TopFieldDocs search(AtomicReaderContext[] leaves, Weight weight, FieldDoc after, int nDocs,
+  protected TopFieldDocs search(List<AtomicReaderContext> leaves, Weight weight, FieldDoc after, int nDocs,
                                 Sort sort, boolean fillFields, boolean doDocScores, boolean doMaxScore) throws IOException {
     // single thread
     int limit = reader.maxDoc();
@@ -559,15 +561,15 @@
    *          to receive hits
    * @throws BooleanQuery.TooManyClauses
    */
-  protected void search(AtomicReaderContext[] leaves, Weight weight, Collector collector)
+  protected void search(List<AtomicReaderContext> leaves, Weight weight, Collector collector)
       throws IOException {
 
     // TODO: should we make this
     // threaded...?  the Collector could be sync'd?
     // always use single thread:
-    for (int i = 0; i < leaves.length; i++) { // search each subreader
-      collector.setNextReader(leaves[i]);
-      Scorer scorer = weight.scorer(leaves[i], !collector.acceptsDocsOutOfOrder(), true, leaves[i].reader().getLiveDocs());
+    for (AtomicReaderContext ctx : leaves) { // search each subreader
+      collector.setNextReader(ctx);
+      Scorer scorer = weight.scorer(ctx, !collector.acceptsDocsOutOfOrder(), true, ctx.reader().getLiveDocs());
       if (scorer != null) {
         scorer.score(collector);
       }
@@ -611,9 +613,10 @@
    */
   protected Explanation explain(Weight weight, int doc) throws IOException {
     int n = ReaderUtil.subIndex(doc, leafContexts);
-    int deBasedDoc = doc - leafContexts[n].docBase;
+    final AtomicReaderContext ctx = leafContexts.get(n);
+    int deBasedDoc = doc - ctx.docBase;
     
-    return weight.explain(leafContexts[n], deBasedDoc);
+    return weight.explain(ctx, deBasedDoc);
   }
 
   /**
@@ -669,7 +672,7 @@
     }
 
     public TopDocs call() throws IOException {
-      final TopDocs docs = searcher.search(slice.leaves, weight, after, nDocs);
+      final TopDocs docs = searcher.search(Arrays.asList(slice.leaves), weight, after, nDocs);
       final ScoreDoc[] scoreDocs = docs.scoreDocs;
       //it would be so nice if we had a thread-safe insert 
       lock.lock();
@@ -757,11 +760,13 @@
 
     public TopFieldDocs call() throws IOException {
       assert slice.leaves.length == 1;
-      final TopFieldDocs docs = searcher.search(slice.leaves, weight, after, nDocs, sort, true, doDocScores, doMaxScore);
+      final TopFieldDocs docs = searcher.search(Arrays.asList(slice.leaves),
+          weight, after, nDocs, sort, true, doDocScores, doMaxScore);
       lock.lock();
       try {
-        final int base = slice.leaves[0].docBase;
-        hq.setNextReader(slice.leaves[0]);
+        final AtomicReaderContext ctx = slice.leaves[0];
+        final int base = ctx.docBase;
+        hq.setNextReader(ctx);
         hq.setScorer(fakeScorer);
         for(ScoreDoc scoreDoc : docs.scoreDocs) {
           fakeScorer.doc = scoreDoc.doc - base;
@@ -837,7 +842,7 @@
   public static class LeafSlice {
     final AtomicReaderContext[] leaves;
     
-    public LeafSlice(AtomicReaderContext...leaves) {
+    public LeafSlice(AtomicReaderContext... leaves) {
       this.leaves = leaves;
     }
   }
Index: lucene/core/src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java	(working copy)
@@ -184,8 +184,7 @@
     for (Term term : terms) {
       termContexts.put(term, TermContext.build(context, term, true));
     }
-    final AtomicReaderContext[] leaves = context.leaves();
-    for (AtomicReaderContext atomicReaderContext : leaves) {
+    for (AtomicReaderContext atomicReaderContext : context.leaves()) {
       final Spans spans = query.getSpans(atomicReaderContext, atomicReaderContext.reader().getLiveDocs(), termContexts);
       while (spans.next() == true) {
         if (spans.isPayloadAvailable()) {
Index: lucene/core/src/java/org/apache/lucene/search/TermCollectingRewrite.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/TermCollectingRewrite.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/search/TermCollectingRewrite.java	(working copy)
@@ -48,8 +48,7 @@
   final void collectTerms(IndexReader reader, MultiTermQuery query, TermCollector collector) throws IOException {
     IndexReaderContext topReaderContext = reader.getTopReaderContext();
     Comparator<BytesRef> lastTermComp = null;
-    final AtomicReaderContext[] leaves = topReaderContext.leaves();
-    for (AtomicReaderContext context : leaves) {
+    for (AtomicReaderContext context : topReaderContext.leaves()) {
       final Fields fields = context.reader().fields();
       if (fields == null) {
         // reader has no fields
Index: lucene/core/src/java/org/apache/lucene/util/BitsSlice.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/BitsSlice.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/util/BitsSlice.java	(working copy)
@@ -25,7 +25,7 @@
   private final int length;
 
   // start is inclusive; end is exclusive (length = end-start)
-  public BitsSlice(Bits parent, ReaderUtil.Slice slice) {
+  public BitsSlice(Bits parent, ReaderSlice slice) {
     this.parent = parent;
     this.start = slice.start;
     this.length = slice.length;
Index: lucene/core/src/java/org/apache/lucene/util/FieldCacheSanityChecker.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/FieldCacheSanityChecker.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/util/FieldCacheSanityChecker.java	(working copy)
@@ -280,9 +280,9 @@
     for (int i = 0; i < all.size(); i++) {
       Object obj = all.get(i);
       if (obj instanceof CompositeReader) {
-        IndexReader[] subs = ((CompositeReader)obj).getSequentialSubReaders();
-        for (int j = 0; (null != subs) && (j < subs.length); j++) {
-          all.add(subs[j].getCoreCacheKey());
+        List<? extends IndexReader> subs = ((CompositeReader)obj).getSequentialSubReaders();
+        for (int j = 0; (null != subs) && (j < subs.size()); j++) {
+          all.add(subs.get(j).getCoreCacheKey());
         }
       }
       
Index: lucene/core/src/java/org/apache/lucene/util/MultiBits.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/MultiBits.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/util/MultiBits.java	(working copy)
@@ -17,8 +17,6 @@
  * limitations under the License.
  */
 
-import java.util.List;
-
 /**
  * Concatenates multiple Bits together, on every lookup.
  *
@@ -36,13 +34,10 @@
 
   private final boolean defaultValue;
 
-  public MultiBits(List<Bits> bits, List<Integer> starts, boolean defaultValue) {
-    assert starts.size() == 1+bits.size();
-    this.subs = bits.toArray(Bits.EMPTY_ARRAY);
-    this.starts = new int[starts.size()];
-    for(int i=0;i<this.starts.length;i++) {
-      this.starts[i] = starts.get(i);
-    }
+  public MultiBits(Bits[] subs, int[] starts, boolean defaultValue) {
+    assert starts.length == 1+subs.length;
+    this.subs = subs;
+    this.starts = starts;
     this.defaultValue = defaultValue;
   }
 
@@ -84,7 +79,7 @@
 
   /**
    * Represents a sub-Bits from 
-   * {@link MultiBits#getMatchingSub(org.apache.lucene.util.ReaderUtil.Slice) getMatchingSub()}.
+   * {@link MultiBits#getMatchingSub(org.apache.lucene.util.ReaderSlice) getMatchingSub()}.
    */
   public final static class SubResult {
     public boolean matches;
@@ -99,7 +94,7 @@
    * {@link SubResult#matches} instead to ensure the sub was 
    * actually found.
    */
-  public SubResult getMatchingSub(ReaderUtil.Slice slice) {
+  public SubResult getMatchingSub(ReaderSlice slice) {
     int reader = ReaderUtil.subIndex(slice.start, starts);
     assert reader != -1;
     assert reader < subs.length: "slice=" + slice + " starts[-1]=" + starts[starts.length-1];
Index: lucene/core/src/java/org/apache/lucene/util/ReaderSlice.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/ReaderSlice.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/util/ReaderSlice.java	(working copy)
@@ -0,0 +1,39 @@
+package org.apache.lucene.util;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Subreader slice from a parent composite reader.
+ */
+public final class ReaderSlice {
+  public static final ReaderSlice[] EMPTY_ARRAY = new ReaderSlice[0];
+  public final int start;
+  public final int length;
+  public final int readerIndex;
+
+  public ReaderSlice(int start, int length, int readerIndex) {
+    this.start = start;
+    this.length = length;
+    this.readerIndex = readerIndex;
+  }
+
+  @Override
+  public String toString() {
+    return "slice start=" + start + " length=" + length + " readerIndex=" + readerIndex;
+  }
+}
\ No newline at end of file
Index: lucene/core/src/java/org/apache/lucene/util/ReaderSlice.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/ReaderSlice.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/util/ReaderSlice.java	(working copy)

Property changes on: lucene/core/src/java/org/apache/lucene/util/ReaderSlice.java
___________________________________________________________________
Added: svn:keywords
## -0,0 +1 ##
+Date Author Id Revision HeadURL
\ No newline at end of property
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/util/ReaderUtil.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/ReaderUtil.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/util/ReaderUtil.java	(working copy)
@@ -18,11 +18,8 @@
  */
 
 import java.util.List;
-import java.io.IOException;
 
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.CompositeReader;
-import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReaderContext;
 
@@ -36,92 +33,6 @@
   private ReaderUtil() {} // no instance
 
   /**
-   * Subreader slice from a parent composite reader.
-   */
-  public static class Slice {
-    public static final Slice[] EMPTY_ARRAY = new Slice[0];
-    public final int start;
-    public final int length;
-    public final int readerIndex;
-
-    public Slice(int start, int length, int readerIndex) {
-      this.start = start;
-      this.length = length;
-      this.readerIndex = readerIndex;
-    }
-
-    @Override
-    public String toString() {
-      return "slice start=" + start + " length=" + length + " readerIndex=" + readerIndex;
-    }
-  }
-
-  /**
-   * Gathers sub-readers from reader into a List.  See
-   * {@link Gather} for are more general way to gather
-   * whatever you need to, per reader.
-   *
-   * @lucene.experimental
-   * 
-   * @param allSubReaders
-   * @param reader
-   */
-
-  public static void gatherSubReaders(final List<AtomicReader> allSubReaders, IndexReader reader) {
-    try {
-      new Gather(reader) {
-        @Override
-        protected void add(int base, AtomicReader r) {
-          allSubReaders.add(r);
-        }
-      }.run();
-    } catch (IOException ioe) {
-      // won't happen
-      throw new RuntimeException(ioe);
-    }
-  }
-
-  /** Recursively visits all sub-readers of a reader.  You
-   *  should subclass this and override the add method to
-   *  gather what you need.
-   *
-   * @lucene.experimental */
-  public static abstract class Gather {
-    private final IndexReader topReader;
-
-    public Gather(IndexReader r) {
-      topReader = r;
-    }
-
-    public int run() throws IOException {
-      return run(0, topReader);
-    }
-
-    public int run(int docBase) throws IOException {
-      return run(docBase, topReader);
-    }
-
-    private int run(final int base, final IndexReader reader) throws IOException {
-      if (reader instanceof AtomicReader) {
-        // atomic reader
-        add(base, (AtomicReader) reader);
-        return base + reader.maxDoc();
-      } else {
-        assert reader instanceof CompositeReader : "must be a composite reader";
-        int newBase = base;
-        IndexReader[] subReaders = ((CompositeReader) reader).getSequentialSubReaders();
-        for (int i = 0; i < subReaders.length; i++) {
-          newBase = run(newBase, subReaders[i]);
-        }
-        assert newBase == base + reader.maxDoc();
-        return newBase;
-      }
-    }
-
-    protected abstract void add(int base, AtomicReader r) throws IOException;
-  }
-  
-  /**
    * Walks up the reader tree and return the given context's top level reader
    * context, or in other words the reader tree's root context.
    */
@@ -162,20 +73,20 @@
    * Returns index of the searcher/reader for document <code>n</code> in the
    * array used to construct this searcher/reader.
    */
-  public static int subIndex(int n, AtomicReaderContext[] leaves) { // find
+  public static int subIndex(int n, List<AtomicReaderContext> leaves) { // find
     // searcher/reader for doc n:
-    int size = leaves.length;
+    int size = leaves.size();
     int lo = 0; // search starts array
     int hi = size - 1; // for first element less than n, return its index
     while (hi >= lo) {
       int mid = (lo + hi) >>> 1;
-      int midValue = leaves[mid].docBase;
+      int midValue = leaves.get(mid).docBase;
       if (n < midValue)
         hi = mid - 1;
       else if (n > midValue)
         lo = mid + 1;
       else { // found a match
-        while (mid + 1 < size && leaves[mid + 1].docBase == midValue) {
+        while (mid + 1 < size && leaves.get(mid + 1).docBase == midValue) {
           mid++; // scan to last match
         }
         return mid;
Index: lucene/core/src/java/org/apache/lucene/util/TermContext.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/TermContext.java	(revision 1351260)
+++ lucene/core/src/java/org/apache/lucene/util/TermContext.java	(working copy)
@@ -57,7 +57,7 @@
     if (context.leaves() == null) {
       len = 1;
     } else {
-      len = context.leaves().length;
+      len = context.leaves().size();
     }
     states = new TermState[len];
   }
@@ -85,11 +85,10 @@
     final String field = term.field();
     final BytesRef bytes = term.bytes();
     final TermContext perReaderTermState = new TermContext(context);
-    final AtomicReaderContext[] leaves = context.leaves();
     //if (DEBUG) System.out.println("prts.build term=" + term);
-    for (int i = 0; i < leaves.length; i++) {
+    for (final AtomicReaderContext ctx : context.leaves()) {
       //if (DEBUG) System.out.println("  r=" + leaves[i].reader);
-      final Fields fields = leaves[i].reader().fields();
+      final Fields fields = ctx.reader().fields();
       if (fields != null) {
         final Terms terms = fields.terms(field);
         if (terms != null) {
@@ -97,7 +96,7 @@
           if (termsEnum.seekExact(bytes, cache)) { 
             final TermState termState = termsEnum.termState();
             //if (DEBUG) System.out.println("    found");
-            perReaderTermState.register(termState, leaves[i].ord, termsEnum.docFreq(), termsEnum.totalTermFreq());
+            perReaderTermState.register(termState, ctx.ord, termsEnum.docFreq(), termsEnum.totalTermFreq());
           }
         }
       }
Index: lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java	(working copy)
@@ -17,6 +17,7 @@
  */
 import java.io.IOException;
 import java.util.IdentityHashMap;
+import java.util.List;
 import java.util.Random;
 
 import org.apache.lucene.analysis.MockAnalyzer;
@@ -35,7 +36,6 @@
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LineFileDocs;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.ReaderUtil;
 import org.apache.lucene.util._TestUtil;
 
 public class TestReuseDocsEnum extends LuceneTestCase {
@@ -50,21 +50,18 @@
     writer.commit();
 
     DirectoryReader open = DirectoryReader.open(dir);
-    new ReaderUtil.Gather(open) {
-      @Override
-      protected void add(int base, AtomicReader r) throws IOException {
-        Terms terms = r.terms("body");
-        TermsEnum iterator = terms.iterator(null);
-        IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();
-        MatchNoBits bits = new Bits.MatchNoBits(r.maxDoc());
-        while ((iterator.next()) != null) {
-          DocsEnum docs = iterator.docs(random().nextBoolean() ? bits : new Bits.MatchNoBits(r.maxDoc()), null, random().nextBoolean());
-          enums.put(docs, true);
-        }
-        
-        assertEquals(terms.size(), enums.size());
+    for (AtomicReader indexReader : open.getSequentialSubReaders()) {
+      Terms terms = indexReader.terms("body");
+      TermsEnum iterator = terms.iterator(null);
+      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();
+      MatchNoBits bits = new Bits.MatchNoBits(indexReader.maxDoc());
+      while ((iterator.next()) != null) {
+        DocsEnum docs = iterator.docs(random().nextBoolean() ? bits : new Bits.MatchNoBits(indexReader.maxDoc()), null, random().nextBoolean());
+        enums.put(docs, true);
       }
-    }.run();
+      
+      assertEquals(terms.size(), enums.size());
+    }
     IOUtils.close(writer, open, dir);
   }
   
@@ -79,9 +76,8 @@
     writer.commit();
 
     DirectoryReader open = DirectoryReader.open(dir);
-    IndexReader[] sequentialSubReaders = open.getSequentialSubReaders();
-    for (IndexReader indexReader : sequentialSubReaders) {
-      Terms terms = ((AtomicReader) indexReader).terms("body");
+    for (AtomicReader indexReader : open.getSequentialSubReaders()) {
+      Terms terms = indexReader.terms("body");
       TermsEnum iterator = terms.iterator(null);
       IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();
       MatchNoBits bits = new Bits.MatchNoBits(open.maxDoc());
@@ -125,8 +121,8 @@
 
     DirectoryReader firstReader = DirectoryReader.open(dir);
     DirectoryReader secondReader = DirectoryReader.open(dir);
-    IndexReader[] sequentialSubReaders = firstReader.getSequentialSubReaders();
-    IndexReader[] sequentialSubReaders2 = secondReader.getSequentialSubReaders();
+    List<? extends AtomicReader> sequentialSubReaders = firstReader.getSequentialSubReaders();
+    List<? extends AtomicReader> sequentialSubReaders2 = secondReader.getSequentialSubReaders();
     
     for (IndexReader indexReader : sequentialSubReaders) {
       Terms terms = ((AtomicReader) indexReader).terms("body");
@@ -154,11 +150,11 @@
     IOUtils.close(writer, firstReader, secondReader, dir);
   }
   
-  public DocsEnum randomDocsEnum(String field, BytesRef term, IndexReader[] readers, Bits bits) throws IOException {
+  public DocsEnum randomDocsEnum(String field, BytesRef term, List<? extends AtomicReader> readers, Bits bits) throws IOException {
     if (random().nextInt(10) == 0) {
       return null;
     }
-    AtomicReader indexReader = (AtomicReader) readers[random().nextInt(readers.length)];
+    AtomicReader indexReader = (AtomicReader) readers.get(random().nextInt(readers.size()));
     return indexReader.termDocsEnum(bits, field, term, random().nextBoolean());
   }
 
Index: lucene/core/src/test/org/apache/lucene/index/TestCustomNorms.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestCustomNorms.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/index/TestCustomNorms.java	(working copy)
@@ -145,26 +145,25 @@
     writer.close();
     assertEquals(numAdded, reader.numDocs());
     IndexReaderContext topReaderContext = reader.getTopReaderContext();
-    AtomicReaderContext[] leaves = topReaderContext.leaves();
-    for (int j = 0; j < leaves.length; j++) {
-      AtomicReader atomicReader = leaves[j].reader();
-    Source source = random().nextBoolean() ? atomicReader.normValues("foo").getSource() : atomicReader.normValues("foo").getDirectSource();
-    Bits liveDocs = atomicReader.getLiveDocs();
-    Type t = source.getType();
-    for (int i = 0; i < atomicReader.maxDoc(); i++) {
-        assertEquals(0, source.getFloat(i), 0.000f);
-    }
-    
-
-    source = random().nextBoolean() ? atomicReader.normValues("bar").getSource() : atomicReader.normValues("bar").getDirectSource();
-    for (int i = 0; i < atomicReader.maxDoc(); i++) {
-      if (liveDocs == null || liveDocs.get(i)) {
-        assertEquals("type: " + t, 1, source.getFloat(i), 0.000f);
-      } else {
-        assertEquals("type: " + t, 0, source.getFloat(i), 0.000f);
+    for (final AtomicReaderContext ctx : topReaderContext.leaves()) {
+      AtomicReader atomicReader = ctx.reader();
+      Source source = random().nextBoolean() ? atomicReader.normValues("foo").getSource() : atomicReader.normValues("foo").getDirectSource();
+      Bits liveDocs = atomicReader.getLiveDocs();
+      Type t = source.getType();
+      for (int i = 0; i < atomicReader.maxDoc(); i++) {
+          assertEquals(0, source.getFloat(i), 0.000f);
       }
+      
+  
+      source = random().nextBoolean() ? atomicReader.normValues("bar").getSource() : atomicReader.normValues("bar").getDirectSource();
+      for (int i = 0; i < atomicReader.maxDoc(); i++) {
+        if (liveDocs == null || liveDocs.get(i)) {
+          assertEquals("type: " + t, 1, source.getFloat(i), 0.000f);
+        } else {
+          assertEquals("type: " + t, 0, source.getFloat(i), 0.000f);
+        }
+      }
     }
-    }
     reader.close();
     dir.close();
   }
Index: lucene/core/src/test/org/apache/lucene/index/TestDeletionPolicy.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDeletionPolicy.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/index/TestDeletionPolicy.java	(working copy)
@@ -67,7 +67,7 @@
     public void onCommit(List<? extends IndexCommit> commits) throws IOException {
       IndexCommit lastCommit =  commits.get(commits.size()-1);
       DirectoryReader r = DirectoryReader.open(dir);
-      assertEquals("lastCommit.segmentCount()=" + lastCommit.getSegmentCount() + " vs IndexReader.segmentCount=" + r.getSequentialSubReaders().length, r.getSequentialSubReaders().length, lastCommit.getSegmentCount());
+      assertEquals("lastCommit.segmentCount()=" + lastCommit.getSegmentCount() + " vs IndexReader.segmentCount=" + r.getSequentialSubReaders().size(), r.getSequentialSubReaders().size(), lastCommit.getSegmentCount());
       r.close();
       verifyCommitOrder(commits);
       numOnCommit++;
@@ -325,7 +325,7 @@
       final boolean needsMerging;
       {
         DirectoryReader r = DirectoryReader.open(dir);
-        needsMerging = r.getSequentialSubReaders().length != 1;
+        needsMerging = r.getSequentialSubReaders().size() != 1;
         r.close();
       }
       if (needsMerging) {
@@ -442,7 +442,7 @@
 
     DirectoryReader r = DirectoryReader.open(dir);
     // Still merged, still 11 docs
-    assertEquals(1, r.getSequentialSubReaders().length);
+    assertEquals(1, r.getSequentialSubReaders().size());
     assertEquals(11, r.numDocs());
     r.close();
 
@@ -458,7 +458,7 @@
     r = DirectoryReader.open(dir);
     // Not fully merged because we rolled it back, and now only
     // 10 docs
-    assertTrue(r.getSequentialSubReaders().length > 1);
+    assertTrue(r.getSequentialSubReaders().size() > 1);
     assertEquals(10, r.numDocs());
     r.close();
 
@@ -468,7 +468,7 @@
     writer.close();
 
     r = DirectoryReader.open(dir);
-    assertEquals(1, r.getSequentialSubReaders().length);
+    assertEquals(1, r.getSequentialSubReaders().size());
     assertEquals(10, r.numDocs());
     r.close();
 
@@ -480,7 +480,7 @@
     // Reader still sees fully merged index, because writer
     // opened on the prior commit has not yet committed:
     r = DirectoryReader.open(dir);
-    assertEquals(1, r.getSequentialSubReaders().length);
+    assertEquals(1, r.getSequentialSubReaders().size());
     assertEquals(10, r.numDocs());
     r.close();
 
@@ -488,7 +488,7 @@
 
     // Now reader sees not-fully-merged index:
     r = DirectoryReader.open(dir);
-    assertTrue(r.getSequentialSubReaders().length > 1);
+    assertTrue(r.getSequentialSubReaders().size() > 1);
     assertEquals(10, r.numDocs());
     r.close();
 
Index: lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java	(working copy)
@@ -550,7 +550,7 @@
     assertEquals("IndexReaders have different values for numDocs.", index1.numDocs(), index2.numDocs());
     assertEquals("IndexReaders have different values for maxDoc.", index1.maxDoc(), index2.maxDoc());
     assertEquals("Only one IndexReader has deletions.", index1.hasDeletions(), index2.hasDeletions());
-    assertEquals("Single segment test differs.", index1.getSequentialSubReaders().length == 1, index2.getSequentialSubReaders().length == 1);
+    assertEquals("Single segment test differs.", index1.getSequentialSubReaders().size() == 1, index2.getSequentialSubReaders().size() == 1);
     
     // check field names
     FieldInfos fieldInfos1 = MultiFields.getMergedFieldInfos(index1);
@@ -785,7 +785,7 @@
     DirectoryReader r2 = DirectoryReader.openIfChanged(r);
     assertNotNull(r2);
     r.close();
-    AtomicReader sub0 = r2.getSequentialSubReaders()[0];
+    AtomicReader sub0 = r2.getSequentialSubReaders().get(0);
     final int[] ints2 = FieldCache.DEFAULT.getInts(sub0, "number", false);
     r2.close();
     assertTrue(ints == ints2);
@@ -814,9 +814,9 @@
     assertNotNull(r2);
     r.close();
   
-    IndexReader[] subs = r2.getSequentialSubReaders();
-    for(int i=0;i<subs.length;i++) {
-      assertEquals(36, ((AtomicReader) subs[i]).getUniqueTermCount());
+    List<? extends AtomicReader> subs = r2.getSequentialSubReaders();
+    for(AtomicReader s : subs) {
+      assertEquals(36, s.getUniqueTermCount());
     }
     r2.close();
     writer.close();
@@ -842,7 +842,7 @@
       // expected
     }
   
-    assertEquals(-1, ((SegmentReader) r.getSequentialSubReaders()[0]).getTermInfosIndexDivisor());
+    assertEquals(-1, ((SegmentReader) r.getSequentialSubReaders().get(0)).getTermInfosIndexDivisor());
     writer = new IndexWriter(
         dir,
         newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).
@@ -857,11 +857,11 @@
     assertNotNull(r2);
     assertNull(DirectoryReader.openIfChanged(r2));
     r.close();
-    IndexReader[] subReaders = r2.getSequentialSubReaders();
-    assertEquals(2, subReaders.length);
-    for(int i=0;i<2;i++) {
+    List<? extends AtomicReader> subReaders = r2.getSequentialSubReaders();
+    assertEquals(2, subReaders.size());
+    for(AtomicReader s : subReaders) {
       try {
-        subReaders[i].docFreq(new Term("field", "f"));
+        s.docFreq(new Term("field", "f"));
         fail("did not hit expected exception");
       } catch (IllegalStateException ise) {
         // expected
Index: lucene/core/src/test/org/apache/lucene/index/TestDirectoryReaderReopen.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDirectoryReaderReopen.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/index/TestDirectoryReaderReopen.java	(working copy)
@@ -469,9 +469,9 @@
 
     DirectoryReader r = DirectoryReader.open(dir);
     if (multiSegment) {
-      assertTrue(r.getSequentialSubReaders().length > 1);
+      assertTrue(r.getSequentialSubReaders().size() > 1);
     } else {
-      assertTrue(r.getSequentialSubReaders().length == 1);
+      assertTrue(r.getSequentialSubReaders().size() == 1);
     }
     r.close();
   }
@@ -542,9 +542,9 @@
     }
     
     if (checkSubReaders && reader instanceof CompositeReader) {
-      IndexReader[] subReaders = ((CompositeReader) reader).getSequentialSubReaders();
-      for (int i = 0; i < subReaders.length; i++) {
-        assertReaderClosed(subReaders[i], checkSubReaders, checkNormsClosed);
+      List<? extends IndexReader> subReaders = ((CompositeReader) reader).getSequentialSubReaders();
+      for (IndexReader r : subReaders) {
+        assertReaderClosed(r, checkSubReaders, checkNormsClosed);
       }
     }
   }
Index: lucene/core/src/test/org/apache/lucene/index/TestDocsAndPositions.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDocsAndPositions.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/index/TestDocsAndPositions.java	(working copy)
@@ -64,8 +64,7 @@
     for (int i = 0; i < num; i++) {
       BytesRef bytes = new BytesRef("1");
       IndexReaderContext topReaderContext = reader.getTopReaderContext();
-      AtomicReaderContext[] leaves = topReaderContext.leaves();
-      for (AtomicReaderContext atomicReaderContext : leaves) {
+      for (AtomicReaderContext atomicReaderContext : topReaderContext.leaves()) {
         DocsAndPositionsEnum docsAndPosEnum = getDocsAndPositions(
             atomicReaderContext.reader(), bytes, null);
         assertNotNull(docsAndPosEnum);
@@ -140,8 +139,7 @@
     for (int i = 0; i < num; i++) {
       BytesRef bytes = new BytesRef("" + term);
       IndexReaderContext topReaderContext = reader.getTopReaderContext();
-      AtomicReaderContext[] leaves = topReaderContext.leaves();
-      for (AtomicReaderContext atomicReaderContext : leaves) {
+      for (AtomicReaderContext atomicReaderContext : topReaderContext.leaves()) {
         DocsAndPositionsEnum docsAndPosEnum = getDocsAndPositions(
             atomicReaderContext.reader(), bytes, null);
         assertNotNull(docsAndPosEnum);
@@ -216,8 +214,7 @@
     for (int i = 0; i < num; i++) {
       BytesRef bytes = new BytesRef("" + term);
       IndexReaderContext topReaderContext = reader.getTopReaderContext();
-      AtomicReaderContext[] leaves = topReaderContext.leaves();
-      for (AtomicReaderContext context : leaves) {
+      for (AtomicReaderContext context : topReaderContext.leaves()) {
         int maxDoc = context.reader().maxDoc();
         DocsEnum docsEnum = _TestUtil.docs(random(), context.reader(), fieldName, bytes, null, null, true);
         if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {
@@ -295,8 +292,7 @@
       BytesRef bytes = new BytesRef("even");
 
       IndexReaderContext topReaderContext = reader.getTopReaderContext();
-      AtomicReaderContext[] leaves = topReaderContext.leaves();
-      for (AtomicReaderContext atomicReaderContext : leaves) {
+      for (AtomicReaderContext atomicReaderContext : topReaderContext.leaves()) {
         DocsAndPositionsEnum docsAndPosEnum = getDocsAndPositions(
             atomicReaderContext.reader(), bytes, null);
         assertNotNull(docsAndPosEnum);
Index: lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java	(working copy)
@@ -77,7 +77,7 @@
     writer.close(true);
 
     DirectoryReader reader = DirectoryReader.open(dir, 1);
-    assertEquals(1, reader.getSequentialSubReaders().length);
+    assertEquals(1, reader.getSequentialSubReaders().size());
 
     IndexSearcher searcher = new IndexSearcher(reader);
 
@@ -750,7 +750,7 @@
     w.forceMerge(1);
     DirectoryReader r = w.getReader();
     w.close();
-    assertEquals(17, r.getSequentialSubReaders()[0].docValues("field").load().getInt(0));
+    assertEquals(17, getOnlySegmentReader(r).docValues("field").load().getInt(0));
     r.close();
     d.close();
   }
@@ -994,8 +994,9 @@
     w.addDocument(doc);
     bytes[0] = 1;
     w.addDocument(doc);
+    w.forceMerge(1);
     DirectoryReader r = w.getReader();
-    Source s = r.getSequentialSubReaders()[0].docValues("field").getSource();
+    Source s = getOnlySegmentReader(r).docValues("field").getSource();
 
     BytesRef bytes1 = s.getBytes(0, new BytesRef());
     assertEquals(bytes.length, bytes1.length);
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit.java	(working copy)
@@ -282,7 +282,7 @@
 
     // Reader should see index as multi-seg at this
     // point:
-    assertTrue("Reader incorrectly sees one segment", reader.getSequentialSubReaders().length > 1);
+    assertTrue("Reader incorrectly sees one segment", reader.getSequentialSubReaders().size() > 1);
     reader.close();
 
     // Abort the writer:
@@ -293,7 +293,7 @@
     reader = DirectoryReader.open(dir);
 
     // Reader should still see index as multi-segment
-    assertTrue("Reader incorrectly sees one segment", reader.getSequentialSubReaders().length > 1);
+    assertTrue("Reader incorrectly sees one segment", reader.getSequentialSubReaders().size() > 1);
     reader.close();
 
     if (VERBOSE) {
@@ -312,7 +312,7 @@
     reader = DirectoryReader.open(dir);
 
     // Reader should see index as one segment
-    assertEquals("Reader incorrectly sees more than one segment", 1, reader.getSequentialSubReaders().length);
+    assertEquals("Reader incorrectly sees more than one segment", 1, reader.getSequentialSubReaders().size());
     reader.close();
     dir.close();
   }
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriterForceMerge.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterForceMerge.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterForceMerge.java	(working copy)
@@ -187,7 +187,7 @@
       if (0 == pass) {
         writer.close();
         DirectoryReader reader = DirectoryReader.open(dir);
-        assertEquals(1, reader.getSequentialSubReaders().length);
+        assertEquals(1, reader.getSequentialSubReaders().size());
         reader.close();
       } else {
         // Get another segment to flush so we can verify it is
@@ -197,7 +197,7 @@
         writer.close();
 
         DirectoryReader reader = DirectoryReader.open(dir);
-        assertTrue(reader.getSequentialSubReaders().length > 1);
+        assertTrue(reader.getSequentialSubReaders().size() > 1);
         reader.close();
 
         SegmentInfos infos = new SegmentInfos();
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java	(working copy)
@@ -982,7 +982,7 @@
     Document doc = new Document();
     doc.add(new TextField("f", "val", Field.Store.NO));
     w.addDocument(doc);
-    IndexReader r = DirectoryReader.open(w, true).getSequentialSubReaders()[0];
+    SegmentReader r = getOnlySegmentReader(DirectoryReader.open(w, true));
     try {
       _TestUtil.docs(random(), r, "f", new BytesRef("val"), null, null, false);
       fail("should have failed to seek since terms index was not loaded.");
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java	(working copy)
@@ -30,7 +30,6 @@
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.ReaderUtil;
 import org.apache.lucene.util.UnicodeUtil;
 
 public class TestIndexWriterUnicode extends LuceneTestCase {
@@ -316,12 +315,9 @@
     IndexReader r = writer.getReader();
 
     // Test each sub-segment
-    new ReaderUtil.Gather(r) {
-      @Override
-      protected void add(int base, AtomicReader r) throws IOException {
-        checkTermsOrder(r, allTerms, false);
-      }
-    }.run();
+    for (AtomicReaderContext ctx : r.getTopReaderContext().leaves()) {
+      checkTermsOrder(ctx.reader(), allTerms, false);
+    }
     checkTermsOrder(r, allTerms, true);
 
     // Test multi segment
Index: lucene/core/src/test/org/apache/lucene/index/TestParallelCompositeReader.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestParallelCompositeReader.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/index/TestParallelCompositeReader.java	(working copy)
@@ -339,13 +339,13 @@
     if (compositeComposite) {
       rd1 = new MultiReader(DirectoryReader.open(dir1), DirectoryReader.open(dir1));
       rd2 = new MultiReader(DirectoryReader.open(dir2), DirectoryReader.open(dir2));
-      assertEquals(2, rd1.getSequentialSubReaders().length);
-      assertEquals(2, rd2.getSequentialSubReaders().length);
+      assertEquals(2, rd1.getSequentialSubReaders().size());
+      assertEquals(2, rd2.getSequentialSubReaders().size());
     } else {
       rd1 = DirectoryReader.open(dir1);
       rd2 = DirectoryReader.open(dir2);
-      assertEquals(3, rd1.getSequentialSubReaders().length);
-      assertEquals(3, rd2.getSequentialSubReaders().length);
+      assertEquals(3, rd1.getSequentialSubReaders().size());
+      assertEquals(3, rd2.getSequentialSubReaders().size());
     }
     ParallelCompositeReader pr = new ParallelCompositeReader(rd1, rd2);
     return newSearcher(pr);
Index: lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java	(working copy)
@@ -281,7 +281,7 @@
   }
 
   private static void printDocs(DirectoryReader r) throws Throwable {
-    IndexReader[] subs = r.getSequentialSubReaders();
+    List<? extends AtomicReader> subs = r.getSequentialSubReaders();
     for(IndexReader sub : subs) {
       // TODO: improve this
       Bits liveDocs = ((AtomicReader)sub).getLiveDocs();
Index: lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java	(working copy)
@@ -742,7 +742,7 @@
     w.forceMerge(1);
     DirectoryReader r = w.getReader();
     w.close();
-    AtomicReader sub = r.getSequentialSubReaders()[0];
+    AtomicReader sub = getOnlySegmentReader(r);
     Terms terms = sub.fields().terms("field");
     Automaton automaton = new RegExp(".*", RegExp.NONE).toAutomaton();    
     CompiledAutomaton ca = new CompiledAutomaton(automaton, false, false);    
Index: lucene/core/src/test/org/apache/lucene/index/TestThreadedForceMerge.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestThreadedForceMerge.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/index/TestThreadedForceMerge.java	(working copy)
@@ -130,7 +130,7 @@
           OpenMode.APPEND).setMaxBufferedDocs(2));
       
       DirectoryReader reader = DirectoryReader.open(directory);
-      assertEquals("reader=" + reader, 1, reader.getSequentialSubReaders().length);
+      assertEquals("reader=" + reader, 1, reader.getSequentialSubReaders().size());
       assertEquals(expectedDocCount, reader.numDocs());
       reader.close();
     }
Index: lucene/core/src/test/org/apache/lucene/index/TestTypePromotion.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestTypePromotion.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/index/TestTypePromotion.java	(working copy)
@@ -19,6 +19,7 @@
 
 import java.io.IOException;
 import java.util.EnumSet;
+import java.util.List;
 import java.util.Random;
 
 import org.apache.lucene.analysis.MockAnalyzer;
@@ -120,11 +121,11 @@
   private void assertValues(TestType type, Directory dir, long[] values, Type[] sourceType)
       throws CorruptIndexException, IOException {
     DirectoryReader reader = DirectoryReader.open(dir);
-    assertEquals(1, reader.getSequentialSubReaders().length);
+    assertEquals(1, reader.getSequentialSubReaders().size());
     IndexReaderContext topReaderContext = reader.getTopReaderContext();
-    AtomicReaderContext[] children = topReaderContext.leaves();
-    assertEquals(1, children.length);
-    DocValues docValues = children[0].reader().docValues("promote");
+    List<AtomicReaderContext> leaves = topReaderContext.leaves();
+    assertEquals(1, leaves.size());
+    DocValues docValues = leaves.get(0).reader().docValues("promote");
     Source directSource = docValues.getDirectSource();
     for (int i = 0; i < values.length; i++) {
       int id = Integer.parseInt(reader.document(i).get("id"));
@@ -372,10 +373,10 @@
     writer.forceMerge(1);
     writer.close();
     DirectoryReader reader = DirectoryReader.open(dir);
-    assertEquals(1, reader.getSequentialSubReaders().length);
+    assertEquals(1, reader.getSequentialSubReaders().size());
     IndexReaderContext topReaderContext = reader.getTopReaderContext();
-    AtomicReaderContext[] children = topReaderContext.leaves();
-    DocValues docValues = children[0].reader().docValues("promote");
+    List<AtomicReaderContext> leaves = topReaderContext.leaves();
+    DocValues docValues = leaves.get(0).reader().docValues("promote");
     assertNotNull(docValues);
     assertValues(TestType.Byte, dir, values, sourceType);
     assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());
Index: lucene/core/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java	(working copy)
@@ -21,11 +21,11 @@
 import java.util.Collection;
 import java.util.Collections;
 import java.util.HashMap;
+import java.util.List;
 import java.util.Map;
 import java.util.TreeSet;
 
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -42,14 +42,16 @@
 public class MultiSpansWrapper extends Spans { // can't be package private due to payloads
 
   private SpanQuery query;
-  private AtomicReaderContext[] leaves;
+  private List<AtomicReaderContext> leaves;
   private int leafOrd = 0;
   private Spans current;
   private Map<Term,TermContext> termContexts;
+  private final int numLeaves;
 
-  private MultiSpansWrapper(AtomicReaderContext[] leaves, SpanQuery query, Map<Term,TermContext> termContexts) {
+  private MultiSpansWrapper(List<AtomicReaderContext> leaves, SpanQuery query, Map<Term,TermContext> termContexts) {
     this.query = query;
     this.leaves = leaves;
+    this.numLeaves = leaves.size();
     this.termContexts = termContexts;
 
   }
@@ -61,27 +63,30 @@
     for (Term term : terms) {
       termContexts.put(term, TermContext.build(topLevelReaderContext, term, true));
     }
-    AtomicReaderContext[] leaves = topLevelReaderContext.leaves();
-    if(leaves.length == 1) {
-      return query.getSpans(leaves[0], leaves[0].reader().getLiveDocs(), termContexts);
+    final List<AtomicReaderContext> leaves = topLevelReaderContext.leaves();
+    if(leaves.size() == 1) {
+      final AtomicReaderContext ctx = leaves.get(0);
+      return query.getSpans(ctx, ctx.reader().getLiveDocs(), termContexts);
     }
     return new MultiSpansWrapper(leaves, query, termContexts);
   }
 
   @Override
   public boolean next() throws IOException {
-    if (leafOrd >= leaves.length) {
+    if (leafOrd >= numLeaves) {
       return false;
     }
     if (current == null) {
-      current = query.getSpans(leaves[leafOrd], leaves[leafOrd].reader().getLiveDocs(), termContexts);
+      final AtomicReaderContext ctx = leaves.get(leafOrd);
+      current = query.getSpans(ctx, ctx.reader().getLiveDocs(), termContexts);
     }
     while(true) {
       if (current.next()) {
         return true;
       }
-      if (++leafOrd < leaves.length) {
-        current = query.getSpans(leaves[leafOrd], leaves[leafOrd].reader().getLiveDocs(), termContexts);
+      if (++leafOrd < numLeaves) {
+        final AtomicReaderContext ctx = leaves.get(leafOrd);
+        current = query.getSpans(ctx, ctx.reader().getLiveDocs(), termContexts);
       } else {
         current = null;
         break;
@@ -92,27 +97,30 @@
 
   @Override
   public boolean skipTo(int target) throws IOException {
-    if (leafOrd >= leaves.length) {
+    if (leafOrd >= numLeaves) {
       return false;
     }
 
     int subIndex = ReaderUtil.subIndex(target, leaves);
     assert subIndex >= leafOrd;
     if (subIndex != leafOrd) {
-      current = query.getSpans(leaves[subIndex], leaves[subIndex].reader().getLiveDocs(), termContexts);
+      final AtomicReaderContext ctx = leaves.get(subIndex);
+      current = query.getSpans(ctx, ctx.reader().getLiveDocs(), termContexts);
       leafOrd = subIndex;
     } else if (current == null) {
-      current = query.getSpans(leaves[leafOrd], leaves[leafOrd].reader().getLiveDocs(), termContexts);
+      final AtomicReaderContext ctx = leaves.get(leafOrd);
+      current = query.getSpans(ctx, ctx.reader().getLiveDocs(), termContexts);
     }
     while (true) {
-      if (current.skipTo(target - leaves[leafOrd].docBase)) {
+      if (current.skipTo(target - leaves.get(leafOrd).docBase)) {
         return true;
       }
-      if (++leafOrd < leaves.length) {
-        current = query.getSpans(leaves[leafOrd], leaves[leafOrd].reader().getLiveDocs(), termContexts);
+      if (++leafOrd < numLeaves) {
+        final AtomicReaderContext ctx = leaves.get(leafOrd);
+        current = query.getSpans(ctx, ctx.reader().getLiveDocs(), termContexts);
       } else {
-          current = null;
-          break;
+        current = null;
+        break;
       }
     }
 
@@ -124,7 +132,7 @@
     if (current == null) {
       return DocIdSetIterator.NO_MORE_DOCS;
     }
-    return current.doc() + leaves[leafOrd].docBase;
+    return current.doc() + leaves.get(leafOrd).docBase;
   }
 
   @Override
Index: lucene/core/src/test/org/apache/lucene/search/spans/TestNearSpansOrdered.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/spans/TestNearSpansOrdered.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/search/spans/TestNearSpansOrdered.java	(working copy)
@@ -166,8 +166,8 @@
     SpanNearQuery q = makeQuery();
     Weight w = searcher.createNormalizedWeight(q);
     IndexReaderContext topReaderContext = searcher.getTopReaderContext();
-    AtomicReaderContext[] leaves = topReaderContext.leaves();
-    Scorer s = w.scorer(leaves[0], true, false, leaves[0].reader().getLiveDocs());
+    AtomicReaderContext leave = topReaderContext.leaves().get(0);
+    Scorer s = w.scorer(leave, true, false, leave.reader().getLiveDocs());
     assertEquals(1, s.advance(1));
   }
   
Index: lucene/core/src/test/org/apache/lucene/search/spans/TestSpans.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/spans/TestSpans.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/search/spans/TestSpans.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 import java.io.IOException;
+import java.util.List;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
@@ -404,10 +405,10 @@
     boolean ordered = true;
     int slop = 1;
     IndexReaderContext topReaderContext = searcher.getTopReaderContext();
-    AtomicReaderContext[] leaves = topReaderContext.leaves();
+    List<AtomicReaderContext> leaves = topReaderContext.leaves();
     int subIndex = ReaderUtil.subIndex(11, leaves);
-    for (int i = 0; i < leaves.length; i++) {
-      
+    for (int i = 0, c = leaves.size(); i < c; i++) {
+      final AtomicReaderContext ctx = leaves.get(i);
      
       final Similarity sim = new DefaultSimilarity() {
         @Override
@@ -427,13 +428,13 @@
                                 slop,
                                 ordered);
   
-        spanScorer = searcher.createNormalizedWeight(snq).scorer(leaves[i], true, false, leaves[i].reader().getLiveDocs());
+        spanScorer = searcher.createNormalizedWeight(snq).scorer(ctx, true, false, ctx.reader().getLiveDocs());
       } finally {
         searcher.setSimilarity(oldSim);
       }
       if (i == subIndex) {
         assertTrue("first doc", spanScorer.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
-        assertEquals("first doc number", spanScorer.docID() + leaves[i].docBase, 11);
+        assertEquals("first doc number", spanScorer.docID() + ctx.docBase, 11);
         float score = spanScorer.score();
         assertTrue("first doc score should be zero, " + score, score == 0.0f);
       }  else {
Index: lucene/core/src/test/org/apache/lucene/search/TestBooleanQuery.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestBooleanQuery.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/search/TestBooleanQuery.java	(working copy)
@@ -226,7 +226,7 @@
 
       Weight weight = s.createNormalizedWeight(q);
 
-      Scorer scorer = weight.scorer(s.leafContexts[0],
+      Scorer scorer = weight.scorer(s.leafContexts.get(0),
                                           true, false, null);
 
       // First pass: just use .nextDoc() to gather all hits
@@ -244,7 +244,7 @@
       for(int iter2=0;iter2<10;iter2++) {
 
         weight = s.createNormalizedWeight(q);
-        scorer = weight.scorer(s.leafContexts[0],
+        scorer = weight.scorer(s.leafContexts.get(0),
                                true, false, null);
 
         if (VERBOSE) {
Index: lucene/core/src/test/org/apache/lucene/search/TestMatchAllDocsQuery.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestMatchAllDocsQuery.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/search/TestMatchAllDocsQuery.java	(working copy)
@@ -54,7 +54,7 @@
 
     IndexSearcher is = newSearcher(ir);
     ScoreDoc[] hits;
-
+    
     hits = is.search(new MatchAllDocsQuery(), null, 1000).scoreDocs;
     assertEquals(3, hits.length);
     assertEquals("one", is.doc(hits[0].doc).get("key"));
Index: lucene/core/src/test/org/apache/lucene/search/TestShardSearching.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestShardSearching.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/search/TestShardSearching.java	(working copy)
@@ -311,13 +311,13 @@
 
     final int numNodes = shardSearcher.nodeVersions.length;
     int[] base = new int[numNodes];
-    final IndexReader[] subs = ((CompositeReader) mockSearcher.getIndexReader()).getSequentialSubReaders();
-    assertEquals(numNodes, subs.length);
+    final List<? extends IndexReader> subs = ((CompositeReader) mockSearcher.getIndexReader()).getSequentialSubReaders();
+    assertEquals(numNodes, subs.size());
 
     int docCount = 0;
     for(int nodeID=0;nodeID<numNodes;nodeID++) {
       base[nodeID] = docCount;
-      docCount += subs[nodeID].maxDoc();
+      docCount += subs.get(nodeID).maxDoc();
     }
 
     if (VERBOSE) {
Index: lucene/core/src/test/org/apache/lucene/search/TestTopDocsMerge.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestTopDocsMerge.java	(revision 1351260)
+++ lucene/core/src/test/org/apache/lucene/search/TestTopDocsMerge.java	(working copy)
@@ -19,6 +19,7 @@
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.List;
 
 import org.apache.lucene.document.Document;
@@ -39,11 +40,11 @@
 public class TestTopDocsMerge extends LuceneTestCase {
 
   private static class ShardSearcher extends IndexSearcher {
-    private final AtomicReaderContext[] ctx;
+    private final List<AtomicReaderContext> ctx;
 
     public ShardSearcher(AtomicReaderContext ctx, IndexReaderContext parent) {
       super(parent);
-      this.ctx = new AtomicReaderContext[] {ctx};
+      this.ctx = Collections.singletonList(ctx);
     }
 
     public void search(Weight weight, Collector collector) throws IOException {
@@ -56,7 +57,7 @@
 
     @Override
     public String toString() {
-      return "ShardSearcher(" + ctx[0] + ")";
+      return "ShardSearcher(" + ctx.get(0) + ")";
     }
   }
 
@@ -131,13 +132,15 @@
       docStarts[0] = 0;
     } else {
       final CompositeReaderContext compCTX = (CompositeReaderContext) ctx;
-      subSearchers = new ShardSearcher[compCTX.leaves().length];
-      docStarts = new int[compCTX.leaves().length];
+      final int size = compCTX.leaves().size();
+      subSearchers = new ShardSearcher[size];
+      docStarts = new int[size];
       int docBase = 0;
-      for(int searcherIDX=0;searcherIDX<subSearchers.length;searcherIDX++) { 
-        subSearchers[searcherIDX] = new ShardSearcher(compCTX.leaves()[searcherIDX], compCTX);
+      for(int searcherIDX=0;searcherIDX<subSearchers.length;searcherIDX++) {
+        final AtomicReaderContext leave = compCTX.leaves().get(searcherIDX);
+        subSearchers[searcherIDX] = new ShardSearcher(leave, compCTX);
         docStarts[searcherIDX] = docBase;
-        docBase += compCTX.leaves()[searcherIDX].reader().maxDoc();
+        docBase += leave.reader().maxDoc();
       }
     }
 
Index: lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java
===================================================================
--- lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java	(revision 1351260)
+++ lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java	(working copy)
@@ -629,25 +629,16 @@
     public final int[] docStarts;
 
     public ShardState(IndexSearcher s) {
-      List<AtomicReader> subReaders = new ArrayList<AtomicReader>();
-      ReaderUtil.gatherSubReaders(subReaders, s.getIndexReader());
-      subSearchers = new ShardSearcher[subReaders.size()];
       final IndexReaderContext ctx = s.getTopReaderContext();
-      if (ctx instanceof AtomicReaderContext) {
-        assert subSearchers.length == 1;
-        subSearchers[0] = new ShardSearcher((AtomicReaderContext) ctx, ctx);
-      } else {
-        final CompositeReaderContext compCTX = (CompositeReaderContext) ctx;
-        for(int searcherIDX=0;searcherIDX<subSearchers.length;searcherIDX++) {
-          subSearchers[searcherIDX] = new ShardSearcher(compCTX.leaves()[searcherIDX], compCTX);
-        }
+      final List<AtomicReaderContext> leaves = ctx.leaves();
+      subSearchers = new ShardSearcher[leaves.size()];
+      for(int searcherIDX=0;searcherIDX<subSearchers.length;searcherIDX++) {
+        subSearchers[searcherIDX] = new ShardSearcher(leaves.get(searcherIDX), ctx);
       }
 
       docStarts = new int[subSearchers.length];
-      int docBase = 0;
       for(int subIDX=0;subIDX<docStarts.length;subIDX++) {
-        docStarts[subIDX] = docBase;
-        docBase += subReaders.get(subIDX).maxDoc();
+        docStarts[subIDX] = leaves.get(subIDX).docBase;
         //System.out.println("docStarts[" + subIDX + "]=" + docStarts[subIDX]);
       }
     }
@@ -1315,24 +1306,20 @@
   }
 
   private static class ShardSearcher extends IndexSearcher {
-    private final AtomicReaderContext[] ctx;
+    private final List<AtomicReaderContext> ctx;
 
     public ShardSearcher(AtomicReaderContext ctx, IndexReaderContext parent) {
       super(parent);
-      this.ctx = new AtomicReaderContext[] {ctx};
+      this.ctx = Collections.singletonList(ctx);
     }
 
     public void search(Weight weight, Collector collector) throws IOException {
       search(ctx, weight, collector);
     }
 
-    public TopDocs search(Weight weight, int topN) throws IOException {
-      return search(ctx, weight, null, topN);
-    }
-
     @Override
     public String toString() {
-      return "ShardSearcher(" + ctx[0].reader() + ")";
+      return "ShardSearcher(" + ctx.get(0).reader() + ")";
     }
   }
 
Index: lucene/join/src/test/org/apache/lucene/search/join/TestBlockJoin.java
===================================================================
--- lucene/join/src/test/org/apache/lucene/search/join/TestBlockJoin.java	(revision 1351260)
+++ lucene/join/src/test/org/apache/lucene/search/join/TestBlockJoin.java	(working copy)
@@ -247,9 +247,9 @@
   }
   
   private Document getParentDoc(IndexReader reader, Filter parents, int childDocID) throws IOException {
-    final AtomicReaderContext[] leaves = reader.getTopReaderContext().leaves();
+    final List<AtomicReaderContext> leaves = reader.getTopReaderContext().leaves();
     final int subIndex = ReaderUtil.subIndex(childDocID, leaves);
-    final AtomicReaderContext leaf = leaves[subIndex];
+    final AtomicReaderContext leaf = leaves.get(subIndex);
     final FixedBitSet bits = (FixedBitSet) parents.getDocIdSet(leaf, null);
     return leaf.reader().document(bits.nextSetBit(childDocID - leaf.docBase));
   }
@@ -961,7 +961,7 @@
 
     ToParentBlockJoinQuery q = new ToParentBlockJoinQuery(tq, parentFilter, ScoreMode.Avg);
     Weight weight = s.createNormalizedWeight(q);
-    DocIdSetIterator disi = weight.scorer(s.getIndexReader().getTopReaderContext().leaves()[0], true, true, null);
+    DocIdSetIterator disi = weight.scorer(s.getIndexReader().getTopReaderContext().leaves().get(0), true, true, null);
     assertEquals(1, disi.advance(1));
     r.close();
     dir.close();
@@ -995,7 +995,7 @@
 
     ToParentBlockJoinQuery q = new ToParentBlockJoinQuery(tq, parentFilter, ScoreMode.Avg);
     Weight weight = s.createNormalizedWeight(q);
-    DocIdSetIterator disi = weight.scorer(s.getIndexReader().getTopReaderContext().leaves()[0], true, true, null);
+    DocIdSetIterator disi = weight.scorer(s.getIndexReader().getTopReaderContext().leaves().get(0), true, true, null);
     assertEquals(2, disi.advance(0));
     r.close();
     dir.close();
Index: lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java
===================================================================
--- lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java	(revision 1351260)
+++ lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java	(working copy)
@@ -20,13 +20,13 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.List;
 
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.FSDirectory;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.ReaderUtil;
 import org.apache.lucene.util.Version;
 
 /**
@@ -101,7 +101,8 @@
           .setOpenMode(OpenMode.CREATE));
       System.err.println("Writing part " + (i + 1) + " ...");
       // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date
-      w.addIndexes(input.getSequentialSubReaders());
+      final List<? extends FakeDeleteAtomicIndexReader> sr = input.getSequentialSubReaders();
+      w.addIndexes(sr.toArray(new IndexReader[sr.size()])); // TODO: maybe take List<IR> here?
       w.close();
     }
     System.err.println("Done.");
@@ -177,34 +178,36 @@
   /**
    * This class emulates deletions on the underlying index.
    */
-  private static final class FakeDeleteIndexReader extends MultiReader {
+  private static final class FakeDeleteIndexReader extends BaseCompositeReader<FakeDeleteAtomicIndexReader> {
 
     public FakeDeleteIndexReader(IndexReader reader) throws IOException {
       super(initSubReaders(reader));
     }
     
-    private static AtomicReader[] initSubReaders(IndexReader reader) throws IOException {
-      final ArrayList<AtomicReader> subs = new ArrayList<AtomicReader>();
-      new ReaderUtil.Gather(reader) {
-        @Override
-        protected void add(int base, AtomicReader r) {
-          subs.add(new FakeDeleteAtomicIndexReader(r));
-        }
-      }.run();
-      return subs.toArray(new AtomicReader[subs.size()]);
+    private static FakeDeleteAtomicIndexReader[] initSubReaders(IndexReader reader) throws IOException {
+      final List<AtomicReaderContext> leaves = reader.getTopReaderContext().leaves();
+      final FakeDeleteAtomicIndexReader[] subs = new FakeDeleteAtomicIndexReader[leaves.size()];
+      int i = 0;
+      for (final AtomicReaderContext ctx : leaves) {
+        subs[i++] = new FakeDeleteAtomicIndexReader(ctx.reader());
+      }
+      return subs;
     }
         
     public void deleteDocument(int docID) {
       final int i = readerIndex(docID);
-      ((FakeDeleteAtomicIndexReader) subReaders[i]).deleteDocument(docID - starts[i]);
+      getSequentialSubReaders().get(i).deleteDocument(docID - readerBase(i));
     }
 
     public void undeleteAll()  {
-      for (IndexReader r : subReaders) {
-        ((FakeDeleteAtomicIndexReader) r).undeleteAll();
+      for (FakeDeleteAtomicIndexReader r : getSequentialSubReaders()) {
+        r.undeleteAll();
       }
     }
 
+    @Override
+    protected void doClose() throws IOException {}
+
     // no need to override numDocs/hasDeletions,
     // as we pass the subreaders directly to IW.addIndexes().
   }
Index: lucene/misc/src/java/org/apache/lucene/index/PKIndexSplitter.java
===================================================================
--- lucene/misc/src/java/org/apache/lucene/index/PKIndexSplitter.java	(revision 1351260)
+++ lucene/misc/src/java/org/apache/lucene/index/PKIndexSplitter.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 import java.io.IOException;
+import java.util.List;
 
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.search.DocIdSet;
@@ -101,10 +102,11 @@
     boolean success = false;
     final IndexWriter w = new IndexWriter(target, config);
     try {
-      final AtomicReaderContext[] leaves = reader.getTopReaderContext().leaves();
-      final IndexReader[] subReaders = new IndexReader[leaves.length];
-      for (int i = 0; i < leaves.length; i++) {
-        subReaders[i] = new DocumentFilteredAtomicIndexReader(leaves[i], preserveFilter, negateFilter);
+      final List<AtomicReaderContext> leaves = reader.getTopReaderContext().leaves();
+      final IndexReader[] subReaders = new IndexReader[leaves.size()];
+      int i = 0;
+      for (final AtomicReaderContext ctx : leaves) {
+        subReaders[i++] = new DocumentFilteredAtomicIndexReader(ctx, preserveFilter, negateFilter);
       }
       w.addIndexes(subReaders);
       success = true;
Index: lucene/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java
===================================================================
--- lucene/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java	(revision 1351260)
+++ lucene/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
@@ -184,33 +185,29 @@
   }
   
   public static long getTotalTermFreq(IndexReader reader, final String field, final BytesRef termText) throws Exception {   
-    final long totalTF[] = new long[1];
-    
-    new ReaderUtil.Gather(reader) {
-
-      @Override
-      protected void add(int base, AtomicReader r) throws IOException {
-        Bits liveDocs = r.getLiveDocs();
-        if (liveDocs == null) {
-          // TODO: we could do this up front, during the scan
-          // (next()), instead of after-the-fact here w/ seek,
-          // if the codec supports it and there are no del
-          // docs...
-          final long totTF = r.totalTermFreq(field, termText);
-          if (totTF != -1) {
-            totalTF[0] += totTF;
-            return;
-          }
-        }
-        DocsEnum de = r.termDocsEnum(liveDocs, field, termText, true);
-        if (de != null) {
-          while (de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS)
-            totalTF[0] += de.freq();
-        }
+    long totalTF = 0L;
+    for (final AtomicReaderContext ctx : reader.getTopReaderContext().leaves()) {
+      AtomicReader r = ctx.reader();
+      Bits liveDocs = r.getLiveDocs();
+      if (liveDocs == null) {
+        // TODO: we could do this up front, during the scan
+        // (next()), instead of after-the-fact here w/ seek,
+        // if the codec supports it and there are no del
+        // docs...
+        final long totTF = r.totalTermFreq(field, termText);
+        if (totTF != -1) {
+          totalTF += totTF;
+          continue;
+        } // otherwise we fall-through
       }
-    }.run();
+      DocsEnum de = r.termDocsEnum(liveDocs, field, termText, true);
+      if (de != null) {
+        while (de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS)
+          totalTF += de.freq();
+      }
+    }
     
-    return totalTF[0];
+    return totalTF;
   }
  }
 
Index: lucene/misc/src/test/org/apache/lucene/index/TestBalancedSegmentMergePolicy.java
===================================================================
--- lucene/misc/src/test/org/apache/lucene/index/TestBalancedSegmentMergePolicy.java	(revision 1351260)
+++ lucene/misc/src/test/org/apache/lucene/index/TestBalancedSegmentMergePolicy.java	(working copy)
@@ -67,7 +67,7 @@
     int numSegments = _TestUtil.nextInt(random(), 1, 4);
     iw.forceMerge(numSegments);
     DirectoryReader ir = iw.getReader();
-    assertTrue(ir.getSequentialSubReaders().length <= numSegments);
+    assertTrue(ir.getSequentialSubReaders().size() <= numSegments);
     ir.close();
   }
   
Index: lucene/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java
===================================================================
--- lucene/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java	(revision 1351260)
+++ lucene/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java	(working copy)
@@ -59,7 +59,7 @@
     }
     iw.commit();
     DirectoryReader iwReader = iw.getReader();
-    assertEquals(3, iwReader.getSequentialSubReaders().length);
+    assertEquals(3, iwReader.getSequentialSubReaders().size());
     iwReader.close();
     iw.close();
     // we should have 2 segments now
@@ -87,7 +87,7 @@
     // now remove the copied segment from src
     IndexSplitter.main(new String[] {dir.getAbsolutePath(), "-d", splitSegName});
     r = DirectoryReader.open(fsDir);
-    assertEquals(2, r.getSequentialSubReaders().length);
+    assertEquals(2, r.getSequentialSubReaders().size());
     r.close();
     fsDir.close();
   }
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java	(revision 1351260)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java	(working copy)
@@ -25,6 +25,7 @@
 import org.apache.lucene.util.ReaderUtil;
 
 import java.io.IOException;
+import java.util.List;
 import java.util.Map;
 
 /**
@@ -60,7 +61,7 @@
   }
 
   private ScaleInfo createScaleInfo(Map context, AtomicReaderContext readerContext) throws IOException {
-    final AtomicReaderContext[] leaves = ReaderUtil.getTopLevelContext(readerContext).leaves();
+    final List<AtomicReaderContext> leaves = ReaderUtil.getTopLevelContext(readerContext).leaves();
 
     float minVal = Float.POSITIVE_INFINITY;
     float maxVal = Float.NEGATIVE_INFINITY;
Index: lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestParser.java
===================================================================
--- lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestParser.java	(revision 1351260)
+++ lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestParser.java	(working copy)
@@ -44,6 +44,7 @@
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.InputStreamReader;
+import java.util.List;
 
 
 public class TestParser extends LuceneTestCase {
@@ -194,8 +195,8 @@
   }
 
   public void testDuplicateFilterQueryXML() throws ParserException, IOException {
-    AtomicReaderContext leaves[] = searcher.getTopReaderContext().leaves();
-    Assume.assumeTrue(leaves == null || leaves.length == 1);
+    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();
+    Assume.assumeTrue(leaves.size() == 1);
     Query q = parse("DuplicateFilterQuery.xml");
     int h = searcher.search(q, null, 1000).totalHits;
     assertEquals("DuplicateFilterQuery should produce 1 result ", 1, h);
Index: lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java	(revision 1351260)
+++ lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java	(working copy)
@@ -28,6 +28,7 @@
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
@@ -497,14 +498,11 @@
 
       final IndexReader reader = searcher.getIndexReader();
       if (reader.maxDoc() > 0) {
-        new ReaderUtil.Gather(reader) {
-          @Override
-          protected void add(int base, AtomicReader r) throws IOException {
-            Terms terms = r.terms(F_WORD);
-            if (terms != null)
-              termsEnums.add(terms.iterator(null));
-          }
-        }.run();
+        for (final AtomicReaderContext ctx : reader.getTopReaderContext().leaves()) {
+          Terms terms = ctx.reader().terms(F_WORD);
+          if (terms != null)
+            termsEnums.add(terms.iterator(null));
+        }
       }
       
       boolean isEmpty = termsEnums.isEmpty();
Index: lucene/test-framework/src/java/org/apache/lucene/search/QueryUtils.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/search/QueryUtils.java	(revision 1351260)
+++ lucene/test-framework/src/java/org/apache/lucene/search/QueryUtils.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 import java.io.IOException;
+import java.util.List;
 import java.util.Random;
 
 import junit.framework.Assert;
@@ -233,7 +234,7 @@
    */
   public static void checkSkipTo(final Query q, final IndexSearcher s) throws IOException {
     //System.out.println("Checking "+q);
-    final AtomicReaderContext[] readerContextArray = s.getTopReaderContext().leaves();
+    final List<AtomicReaderContext> readerContextArray = s.getTopReaderContext().leaves();
     if (s.createNormalizedWeight(q).scoresDocsOutOfOrder()) return;  // in this case order of skipTo() might differ from that of next().
 
     final int skip_op = 0;
@@ -278,7 +279,7 @@
             try {
               if (scorer == null) {
                 Weight w = s.createNormalizedWeight(q);
-                AtomicReaderContext context = readerContextArray[leafPtr];
+                AtomicReaderContext context = readerContextArray.get(leafPtr);
                 scorer = w.scorer(context, true, false, context.reader().getLiveDocs());
               }
               
@@ -334,7 +335,7 @@
               leafPtr++;
             }
             lastReader[0] = context.reader();
-            assert readerContextArray[leafPtr].reader() == context.reader();
+            assert readerContextArray.get(leafPtr).reader() == context.reader();
             this.scorer = null;
             lastDoc[0] = -1;
           }
@@ -368,7 +369,7 @@
     final float maxDiff = 1e-3f;
     final int lastDoc[] = {-1};
     final AtomicReader lastReader[] = {null};
-    final AtomicReaderContext[] context = s.getTopReaderContext().leaves();
+    final List<AtomicReaderContext> context = s.getTopReaderContext().leaves();
     s.search(q,new Collector() {
       private Scorer scorer;
       private int leafPtr;
@@ -384,7 +385,7 @@
           long startMS = System.currentTimeMillis();
           for (int i=lastDoc[0]+1; i<=doc; i++) {
             Weight w = s.createNormalizedWeight(q);
-            Scorer scorer = w.scorer(context[leafPtr], true, false, liveDocs);
+            Scorer scorer = w.scorer(context.get(leafPtr), true, false, liveDocs);
             Assert.assertTrue("query collected "+doc+" but skipTo("+i+") says no more docs!",scorer.advance(i) != DocIdSetIterator.NO_MORE_DOCS);
             Assert.assertEquals("query collected "+doc+" but skipTo("+i+") got to "+scorer.docID(),doc,scorer.docID());
             float skipToScore = scorer.score();
Index: lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java	(revision 1351260)
+++ lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java	(working copy)
@@ -435,11 +435,12 @@
    * do tests on that segment's reader. This is an utility method to help them.
    */
   public static SegmentReader getOnlySegmentReader(DirectoryReader reader) {
-    IndexReader[] subReaders = reader.getSequentialSubReaders();
-    if (subReaders.length != 1)
-      throw new IllegalArgumentException(reader + " has " + subReaders.length + " segments instead of exactly one");
-    assertTrue(subReaders[0] instanceof SegmentReader);
-    return (SegmentReader) subReaders[0];
+    List<? extends IndexReader> subReaders = reader.getSequentialSubReaders();
+    if (subReaders.size() != 1)
+      throw new IllegalArgumentException(reader + " has " + subReaders.size() + " segments instead of exactly one");
+    final IndexReader r = subReaders.get(0);
+    assertTrue(r instanceof SegmentReader);
+    return (SegmentReader) r;
   }
 
   /**
Index: solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java	(revision 1351260)
+++ solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java	(working copy)
@@ -544,7 +544,7 @@
     indexInfo.add("maxDoc", reader.maxDoc());
 
     indexInfo.add("version", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?
-    indexInfo.add("segmentCount", reader.getSequentialSubReaders().length);
+    indexInfo.add("segmentCount", reader.getTopReaderContext().leaves().size());
     indexInfo.add("current", reader.isCurrent() );
     indexInfo.add("hasDeletions", reader.hasDeletions() );
     indexInfo.add("directory", dir );
Index: solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java	(revision 1351260)
+++ solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java	(working copy)
@@ -437,11 +437,11 @@
       NamedList<Object[]> sortVals = new NamedList<Object[]>(); // order is important for the sort fields
       Field field = new StringField("dummy", "", Field.Store.NO); // a dummy Field
       IndexReaderContext topReaderContext = searcher.getTopReaderContext();
-      AtomicReaderContext[] leaves = topReaderContext.leaves();
+      List<AtomicReaderContext> leaves = topReaderContext.leaves();
       AtomicReaderContext currentLeaf = null;
-      if (leaves.length==1) {
+      if (leaves.size()==1) {
         // if there is a single segment, use that subReader and avoid looking up each time
-        currentLeaf = leaves[0];
+        currentLeaf = leaves.get(0);
         leaves=null;
       }
 
@@ -478,7 +478,7 @@
 
           if (leaves != null) {
             idx = ReaderUtil.subIndex(doc, leaves);
-            currentLeaf = leaves[idx];
+            currentLeaf = leaves.get(idx);
             if (idx != lastIdx) {
               // we switched segments.  invalidate comparator.
               comparator = null;
Index: solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java	(revision 1351260)
+++ solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java	(working copy)
@@ -679,7 +679,7 @@
         if (buildOnCommit)  {
           buildSpellIndex(newSearcher);
         } else if (buildOnOptimize) {
-          if (newSearcher.getIndexReader().getSequentialSubReaders().length == 1)  {
+          if (newSearcher.getIndexReader().getSequentialSubReaders().size() == 1)  {
             buildSpellIndex(newSearcher);
           } else  {
             LOG.info("Index is not optimized therefore skipping building spell check index for: " + checker.getDictionaryName());
Index: solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java
===================================================================
--- solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java	(revision 1351260)
+++ solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java	(working copy)
@@ -82,7 +82,7 @@
     // reuse the translation logic to go from top level set to per-segment set
     baseSet = docs.getTopFilter();
 
-    final AtomicReaderContext[] leaves = searcher.getTopReaderContext().leaves();
+    final List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();
     // The list of pending tasks that aren't immediately submitted
     // TODO: Is there a completion service, or a delegating executor that can
     // limit the number of concurrent tasks submitted to a bigger executor?
@@ -90,8 +90,8 @@
 
     int threads = nThreads <= 0 ? Integer.MAX_VALUE : nThreads;
 
-    for (int i=0; i<leaves.length; i++) {
-      final SegFacet segFacet = new SegFacet(leaves[i]);
+    for (final AtomicReaderContext leave : leaves) {
+      final SegFacet segFacet = new SegFacet(leave);
 
       Callable<SegFacet> task = new Callable<SegFacet>() {
         public SegFacet call() throws Exception {
@@ -111,7 +111,7 @@
 
 
     // now merge the per-segment results
-    PriorityQueue<SegFacet> queue = new PriorityQueue<SegFacet>(leaves.length) {
+    PriorityQueue<SegFacet> queue = new PriorityQueue<SegFacet>(leaves.size()) {
       @Override
       protected boolean lessThan(SegFacet a, SegFacet b) {
         return a.tempBR.compareTo(b.tempBR) < 0;
@@ -121,7 +121,7 @@
 
     boolean hasMissingCount=false;
     int missingCount=0;
-    for (int i=0; i<leaves.length; i++) {
+    for (int i=0, c=leaves.size(); i<c; i++) {
       SegFacet seg = null;
 
       try {
Index: solr/core/src/java/org/apache/solr/response/transform/ValueSourceAugmenter.java
===================================================================
--- solr/core/src/java/org/apache/solr/response/transform/ValueSourceAugmenter.java	(revision 1351260)
+++ solr/core/src/java/org/apache/solr/response/transform/ValueSourceAugmenter.java	(working copy)
@@ -17,6 +17,7 @@
 package org.apache.solr.response.transform;
 
 import java.io.IOException;
+import java.util.List;
 import java.util.Map;
 
 import org.apache.lucene.index.AtomicReaderContext;
@@ -26,7 +27,6 @@
 import org.apache.lucene.util.ReaderUtil;
 import org.apache.solr.common.SolrDocument;
 import org.apache.solr.common.SolrException;
-import org.apache.solr.request.SolrQueryRequest;
 import org.apache.solr.search.QParser;
 import org.apache.solr.search.SolrIndexSearcher;
 
@@ -64,7 +64,7 @@
     try {
       IndexReader reader = qparser.getReq().getSearcher().getIndexReader();
       readerContexts = reader.getTopReaderContext().leaves();
-      docValuesArr = new FunctionValues[readerContexts.length];
+      docValuesArr = new FunctionValues[readerContexts.size()];
 
       searcher = qparser.getReq().getSearcher();
       fcontext = ValueSource.newContext(searcher);
@@ -77,7 +77,7 @@
 
   Map fcontext;
   SolrIndexSearcher searcher;
-  AtomicReaderContext[] readerContexts;
+  List<AtomicReaderContext> readerContexts;
   FunctionValues docValuesArr[];
 
 
@@ -89,7 +89,7 @@
 
       // TODO: calculate this stuff just once across diff functions
       int idx = ReaderUtil.subIndex(docid, readerContexts);
-      AtomicReaderContext rcontext = readerContexts[idx];
+      AtomicReaderContext rcontext = readerContexts.get(idx);
       FunctionValues values = docValuesArr[idx];
       if (values == null) {
         docValuesArr[idx] = values = valueSource.getValues(fcontext, rcontext);
Index: solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java	(revision 1351260)
+++ solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java	(working copy)
@@ -602,11 +602,9 @@
    */
   public long lookupId(BytesRef idBytes) throws IOException {
     String field = schema.getUniqueKeyField().getName();
-    final AtomicReaderContext[] leaves = leafContexts;
 
-
-    for (int i=0; i<leaves.length; i++) {
-      final AtomicReaderContext leaf = leaves[i];
+    for (int i=0, c=leafContexts.size(); i<c; i++) {
+      final AtomicReaderContext leaf = leafContexts.get(i);
       final AtomicReader reader = leaf.reader();
 
       final Fields fields = reader.fields();
@@ -756,11 +754,7 @@
       collector = pf.postFilter;
     }
 
-    final AtomicReaderContext[] leaves = leafContexts;
-
-
-    for (int i=0; i<leaves.length; i++) {
-      final AtomicReaderContext leaf = leaves[i];
+    for (final AtomicReaderContext leaf : leafContexts) {
       final AtomicReader reader = leaf.reader();
       final Bits liveDocs = reader.getLiveDocs();   // TODO: the filter may already only have liveDocs...
       DocIdSet idSet = null;
@@ -989,10 +983,7 @@
     if (filter==null) {
       if (query instanceof TermQuery) {
         Term t = ((TermQuery)query).getTerm();
-        final AtomicReaderContext[] leaves = leafContexts;
-
-        for (int i=0; i<leaves.length; i++) {
-          final AtomicReaderContext leaf = leaves[i];
+        for (final AtomicReaderContext leaf : leafContexts) {
           final AtomicReader reader = leaf.reader();
           collector.setNextReader(leaf);
           Fields fields = reader.fields();
@@ -1799,7 +1790,7 @@
     while (iter.hasNext()) {
       int doc = iter.nextDoc();
       while (doc>=end) {
-        AtomicReaderContext leaf = leafContexts[readerIndex++];
+        AtomicReaderContext leaf = leafContexts.get(readerIndex++);
         base = leaf.docBase;
         end = base + leaf.reader().maxDoc();
         topCollector.setNextReader(leaf);
Index: solr/core/src/java/org/apache/solr/update/VersionInfo.java
===================================================================
--- solr/core/src/java/org/apache/solr/update/VersionInfo.java	(revision 1351260)
+++ solr/core/src/java/org/apache/solr/update/VersionInfo.java	(working copy)
@@ -159,7 +159,7 @@
       ValueSource vs = versionField.getType().getValueSource(versionField, null);
       Map context = ValueSource.newContext(searcher);
       vs.createWeight(context, searcher);
-      FunctionValues fv = vs.getValues(context, searcher.getTopReaderContext().leaves()[(int)(lookup>>32)]);
+      FunctionValues fv = vs.getValues(context, searcher.getTopReaderContext().leaves().get((int)(lookup>>32)));
       long ver = fv.longVal((int)lookup);
       return ver;
 
Index: solr/core/src/test/org/apache/solr/search/TestDocSet.java
===================================================================
--- solr/core/src/test/org/apache/solr/search/TestDocSet.java	(revision 1351260)
+++ solr/core/src/test/org/apache/solr/search/TestDocSet.java	(working copy)
@@ -19,6 +19,7 @@
 
 import java.io.IOException;
 import java.util.Arrays;
+import java.util.List;
 import java.util.Random;
 
 import org.apache.lucene.index.FieldInfo;
@@ -471,7 +472,7 @@
 
     DocIdSet da;
     DocIdSet db;
-    AtomicReaderContext[] leaves = topLevelContext.leaves();
+    List<AtomicReaderContext> leaves = topLevelContext.leaves();
 
     // first test in-sequence sub readers
     for (AtomicReaderContext readerContext : leaves) {
@@ -480,10 +481,10 @@
       doTestIteratorEqual(da, db);
     }  
 
-    int nReaders = leaves.length;
+    int nReaders = leaves.size();
     // now test out-of-sequence sub readers
     for (int i=0; i<nReaders; i++) {
-      AtomicReaderContext readerContext = leaves[rand.nextInt(nReaders)];
+      AtomicReaderContext readerContext = leaves.get(rand.nextInt(nReaders));
       da = fa.getDocIdSet(readerContext, null);
       db = fb.getDocIdSet(readerContext, null);
       doTestIteratorEqual(da, db);
Index: solr/core/src/test/org/apache/solr/search/TestIndexSearcher.java
===================================================================
--- solr/core/src/test/org/apache/solr/search/TestIndexSearcher.java	(revision 1351260)
+++ solr/core/src/test/org/apache/solr/search/TestIndexSearcher.java	(working copy)
@@ -26,6 +26,7 @@
 import org.apache.solr.schema.SchemaField;
 import org.junit.BeforeClass;
 
+import java.util.List;
 import java.util.Map;
 import java.io.IOException;
 
@@ -50,9 +51,9 @@
     Map context = ValueSource.newContext(sqr.getSearcher());
     vs.createWeight(context, sqr.getSearcher());
     IndexReaderContext topReaderContext = sqr.getSearcher().getTopReaderContext();
-    AtomicReaderContext[] leaves = topReaderContext.leaves();
+    List<AtomicReaderContext> leaves = topReaderContext.leaves();
     int idx = ReaderUtil.subIndex(doc, leaves);
-    AtomicReaderContext leaf = leaves[idx];
+    AtomicReaderContext leaf = leaves.get(idx);
     FunctionValues vals = vs.getValues(context, leaf);
     return vals.strVal(doc-leaf.docBase);
   }
@@ -78,7 +79,7 @@
 
     // make sure the readers share the first segment
     // Didn't work w/ older versions of lucene2.9 going from segment -> multi
-    assertEquals(rCtx1.leaves()[0].reader(), rCtx2.leaves()[0].reader());
+    assertEquals(rCtx1.leaves().get(0).reader(), rCtx2.leaves().get(0).reader());
 
     assertU(adoc("id","5", "v_f","3.14159"));
     assertU(adoc("id","6", "v_f","8983", "v_s1","string6"));
@@ -88,8 +89,8 @@
     IndexReaderContext rCtx3 = sr3.getSearcher().getTopReaderContext();
     // make sure the readers share segments
     // assertEquals(r1.getLeafReaders()[0], r3.getLeafReaders()[0]);
-    assertEquals(rCtx2.leaves()[0].reader(), rCtx3.leaves()[0].reader());
-    assertEquals(rCtx2.leaves()[1].reader(), rCtx3.leaves()[1].reader());
+    assertEquals(rCtx2.leaves().get(0).reader(), rCtx3.leaves().get(0).reader());
+    assertEquals(rCtx2.leaves().get(1).reader(), rCtx3.leaves().get(1).reader());
 
     sr1.close();
     sr2.close();            
@@ -123,8 +124,8 @@
     assertU(commit());
     SolrQueryRequest sr6 = req("q","foo");
     IndexReaderContext rCtx6 = sr6.getSearcher().getTopReaderContext();
-    assertEquals(1, rCtx6.leaves()[0].reader().numDocs()); // only a single doc left in the first segment
-    assertTrue( !rCtx5.leaves()[0].reader().equals(rCtx6.leaves()[0].reader()) );  // readers now different
+    assertEquals(1, rCtx6.leaves().get(0).reader().numDocs()); // only a single doc left in the first segment
+    assertTrue( !rCtx5.leaves().get(0).reader().equals(rCtx6.leaves().get(0).reader()) );  // readers now different
 
     sr5.close();
     sr6.close();
Index: solr/core/src/test/org/apache/solr/search/TestSort.java
===================================================================
--- solr/core/src/test/org/apache/solr/search/TestSort.java	(revision 1351260)
+++ solr/core/src/test/org/apache/solr/search/TestSort.java	(working copy)
@@ -198,7 +198,7 @@
       DirectoryReader reader = DirectoryReader.open(dir);
       IndexSearcher searcher = new IndexSearcher(reader);
       // System.out.println("segments="+searcher.getIndexReader().getSequentialSubReaders().length);
-      assertTrue(reader.getSequentialSubReaders().length > 1);
+      assertTrue(reader.getSequentialSubReaders().size() > 1);
 
       for (int i=0; i<qiter; i++) {
         Filter filt = new Filter() {
