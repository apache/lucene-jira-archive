From d3a0a1308ea4d0e83ec464191184dcf95fbb6fb4 Mon Sep 17 00:00:00 2001
From: Mike Drob <mdrob@cloudera.com>
Date: Thu, 3 Mar 2016 21:49:02 -0600
Subject: [PATCH] lucene-6993

---
 lucene/analysis/common/build.xml                   |  49 +-
 .../analysis/charfilter/HTMLStripCharFilter.jflex  |   2 +-
 .../charfilter/HTMLStripCharFilterFactory.java     |  24 +-
 .../charfilter/cf50/HTMLCharacterEntities50.jflex  | 162 ++++
 .../charfilter/cf50/HTMLStripCharFilter50.jflex    | 937 +++++++++++++++++++++
 .../analysis/charfilter/cf50/package-info.java     |  58 ++
 .../analysis/standard/ClassicTokenizerFactory.java |  17 +-
 .../analysis/standard/ClassicTokenizerImpl.jflex   |   2 +-
 .../standard/StandardTokenizerFactory.java         |  17 +-
 .../analysis/standard/StandardTokenizerImpl.jflex  |   2 +-
 .../standard/UAX29URLEmailTokenizerFactory.java    |  18 +-
 .../standard/UAX29URLEmailTokenizerImpl.jflex      |   4 +-
 .../analysis/standard/std50/ASCIITLD.jflex-macro   | 366 ++++++++
 .../standard/std50/ClassicTokenizer50.java         | 186 ++++
 .../standard/std50/ClassicTokenizerImpl50.jflex    | 128 +++
 .../standard/std50/StandardTokenizer50.java        | 204 +++++
 .../standard/std50/StandardTokenizerImpl50.jflex   | 202 +++++
 .../standard/std50/UAX29URLEmailTokenizer50.java   | 173 ++++
 .../std50/UAX29URLEmailTokenizerImpl50.jflex       | 316 +++++++
 .../analysis/standard/std50/package-info.java      |  63 ++
 .../lucene/analysis/standard/std50/package.html    |  22 +
 .../wikipedia/WikipediaTokenizerFactory.java       |  14 +-
 .../wikipedia/WikipediaTokenizerImpl.jflex         |   2 +-
 .../wikipedia/wp50/WikipediaTokenizer50.java       | 318 +++++++
 .../wikipedia/wp50/WikipediaTokenizerImpl50.jflex  | 343 ++++++++
 .../analysis/wikipedia/wp50/package-info.java      |  21 +
 .../analysis/standard/TestStandardAnalyzer.java    |   2 +-
 .../standard/TestUAX29URLEmailTokenizer.java       |   4 +-
 .../standard/generateJavaUnicodeWordBreakTest.pl   |   5 +-
 .../analysis/standard/random.text.with.urls.txt    |   4 +-
 .../standard/urls.from.random.text.with.urls.txt   |   4 +-
 lucene/common-build.xml                            |   3 +-
 lucene/ivy-versions.properties                     |   1 +
 33 files changed, 3631 insertions(+), 42 deletions(-)
 create mode 100644 lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/cf50/HTMLCharacterEntities50.jflex
 create mode 100644 lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/cf50/HTMLStripCharFilter50.jflex
 create mode 100644 lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/cf50/package-info.java
 create mode 100644 lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/ASCIITLD.jflex-macro
 create mode 100644 lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/ClassicTokenizer50.java
 create mode 100644 lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/ClassicTokenizerImpl50.jflex
 create mode 100644 lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/StandardTokenizer50.java
 create mode 100644 lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/StandardTokenizerImpl50.jflex
 create mode 100644 lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/UAX29URLEmailTokenizer50.java
 create mode 100644 lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/UAX29URLEmailTokenizerImpl50.jflex
 create mode 100644 lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/package-info.java
 create mode 100644 lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/package.html
 create mode 100644 lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/wp50/WikipediaTokenizer50.java
 create mode 100644 lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/wp50/WikipediaTokenizerImpl50.jflex
 create mode 100644 lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/wp50/package-info.java

diff --git a/lucene/analysis/common/build.xml b/lucene/analysis/common/build.xml
index 670e6ab..e990054 100644
--- a/lucene/analysis/common/build.xml
+++ b/lucene/analysis/common/build.xml
@@ -86,7 +86,6 @@
     <attribute name="name"/>
     <sequential>
       <jflex file="@{dir}/@{name}.jflex" outdir="@{dir}" nobak="on" inputstreamctor="false"/>
-      <!-- LUCENE-5897: Disallow scanner buffer expansion -->
       <replaceregexp file="@{dir}/@{name}.java"
                      match="[ \t]*/\* is the buffer big enough\? \*/\s+if \(zzCurrentPos >= zzBuffer\.length.*?\}[ \t]*\r?\n"
                      replace="" flags="s" />
@@ -97,8 +96,8 @@
                      match="int requested = zzBuffer.length - zzEndRead;"
                      replace="int requested = zzBuffer.length - zzEndRead - zzFinalHighSurrogate;"/>
       <replaceregexp file="@{dir}/@{name}.java"
-                     match="(zzFinalHighSurrogate = 1;)(\r?\n)"
-                     replace="\1\2          if (totalRead == 1) { return true; }\2"/>
+                     match="throw new java.io.IOException\(.Reader returned 0 characters. See JFlex examples for workaround..\)"
+                     replace="return true" flags="s" />
     </sequential>
   </macrodef>
 
@@ -110,11 +109,53 @@
       <fileset dir="src/java/org/apache/lucene/analysis/wikipedia" includes="*.java">
         <containsregexp expression="generated.*by.*JFlex"/>
       </fileset>
-      <fileset dir="src/java/org/apache/lucene/analysis/standard" includes="**/*.java">
+      <fileset dir="src/java/org/apache/lucene/analysis/standard" includes="*.java">
         <containsregexp expression="generated.*by.*JFlex"/>
       </fileset>
     </delete>
   </target>
+
+  <target name="jflex-legacy" depends="-install-jflex,clean-jflex-legacy,-jflex-legacy-StandardAnalyzer,
+      -jflex-legacy-UAX29URLEmailTokenizer,-jflex-legacy-wiki-tokenizer,-jflex-legacy-HTMLStripCharFilter"/>
+
+  <target name="clean-jflex-legacy">
+    <delete>
+      <fileset dir="src/java/org/apache/lucene/analysis/charfilter/cf50" includes="*.java">
+        <containsregexp expression="generated.*by.*JFlex"/>
+      </fileset>
+      <fileset dir="src/java/org/apache/lucene/analysis/wikipedia/wp50" includes="*.java">
+        <containsregexp expression="generated.*by.*JFlex"/>
+      </fileset>
+      <fileset dir="src/java/org/apache/lucene/analysis/standard/std50" includes="*.java">
+        <containsregexp expression="generated.*by.*JFlex"/>
+      </fileset>
+    </delete>
+  </target>
+
+  <target name="-jflex-legacy-StandardAnalyzer" depends="init,-install-jflex">
+    <run-jflex-and-disable-buffer-expansion
+        dir="src/java/org/apache/lucene/analysis/standard/std50" name="StandardTokenizerImpl50"/>
+    <run-jflex dir="src/java/org/apache/lucene/analysis/standard/std50" name="ClassicTokenizerImpl50"/>
+  </target>
+
+  <target name="-jflex-legacy-UAX29URLEmailTokenizer" depends="init,-install-jflex">
+    <run-jflex-and-disable-buffer-expansion
+        dir="src/java/org/apache/lucene/analysis/standard/std50" name="UAX29URLEmailTokenizerImpl50"/>
+  </target>
+
+  <target name="-jflex-legacy-wiki-tokenizer" depends="init,-install-jflex">
+    <run-jflex dir="src/java/org/apache/lucene/analysis/wikipedia/wp50" name="WikipediaTokenizerImpl50"/>
+  </target>
+
+  <target name="-jflex-legacy-HTMLStripCharFilter" depends="init,-install-jflex">
+    <jflex file="src/java/org/apache/lucene/analysis/charfilter/cf50/HTMLStripCharFilter50.jflex"
+           outdir="src/java/org/apache/lucene/analysis/charfilter/cf50"
+           nobak="on" inputstreamctor="false"/>
+    <!-- Remove the inappropriate JFlex-generated constructor -->
+    <replaceregexp file="src/java/org/apache/lucene/analysis/charfilter/cf50/HTMLStripCharFilter50.java"
+                   match="/\*\*\s*\*\s*Creates a new scanner\s*\*\s*\*\s*@param\s*in\s*the java.io.Reader to read input from\.\s*\*/\s*public HTMLStripCharFilter50\(java\.io\.Reader in\)\s*\{\s*this.zzReader = in;\s*\}"
+                   replace="" flags="s"/>
+  </target>
   
   <target xmlns:ivy="antlib:org.apache.ivy.ant" name="-resolve-icu4j" unless="icu4j.resolved" depends="ivy-availability-check,ivy-configure">
     <loadproperties prefix="ivyversions" srcFile="${common.dir}/ivy-versions.properties"/>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex
index 352ede7..432b8e0 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex
@@ -33,7 +33,7 @@ import org.apache.lucene.analysis.util.OpenStringBuilder;
 @SuppressWarnings("fallthrough")
 %%
 
-%unicode 6.3
+%unicode 7.0
 %apiprivate
 %type int
 %final
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilterFactory.java
index 9a237c2..03a475a 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilterFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilterFactory.java
@@ -17,7 +17,9 @@
 package org.apache.lucene.analysis.charfilter;
 
 
+import org.apache.lucene.analysis.charfilter.cf50.HTMLStripCharFilter50;
 import org.apache.lucene.analysis.util.CharFilterFactory;
+import org.apache.lucene.util.Version;
 
 import java.io.Reader;
 import java.util.Map;
@@ -48,13 +50,23 @@ public class HTMLStripCharFilterFactory extends CharFilterFactory {
   }
 
   @Override
-  public HTMLStripCharFilter create(Reader input) {
-    HTMLStripCharFilter charFilter;
-    if (null == escapedTags) {
-      charFilter = new HTMLStripCharFilter(input);
+  public BaseCharFilter create(Reader input) {
+    if (luceneMatchVersion.onOrAfter(Version.LUCENE_6_0_0)) {
+      HTMLStripCharFilter charFilter;
+      if (null == escapedTags) {
+        charFilter = new HTMLStripCharFilter(input);
+      } else {
+        charFilter = new HTMLStripCharFilter(input, escapedTags);
+      }
+      return charFilter;
     } else {
-      charFilter = new HTMLStripCharFilter(input, escapedTags);
+      HTMLStripCharFilter50 charFilter;
+      if (null == escapedTags) {
+        charFilter = new HTMLStripCharFilter50(input);
+      } else {
+        charFilter = new HTMLStripCharFilter50(input, escapedTags);
+      }
+      return charFilter;
     }
-    return charFilter;
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/cf50/HTMLCharacterEntities50.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/cf50/HTMLCharacterEntities50.jflex
new file mode 100644
index 0000000..c2fdc1d
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/cf50/HTMLCharacterEntities50.jflex
@@ -0,0 +1,162 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+CharacterEntities = ( "AElig" | "Aacute" | "Acirc" | "Agrave" | "Alpha"
+                    | "Aring" | "Atilde" | "Auml" | "Beta" | "Ccedil" | "Chi"
+                    | "Dagger" | "Delta" | "ETH" | "Eacute" | "Ecirc"
+                    | "Egrave" | "Epsilon" | "Eta" | "Euml" | "Gamma"
+                    | "Iacute" | "Icirc" | "Igrave" | "Iota" | "Iuml" | "Kappa"
+                    | "Lambda" | "Mu" | "Ntilde" | "Nu" | "OElig" | "Oacute"
+                    | "Ocirc" | "Ograve" | "Omega" | "Omicron" | "Oslash"
+                    | "Otilde" | "Ouml" | "Phi" | "Pi" | "Prime" | "Psi"
+                    | "Rho" | "Scaron" | "Sigma" | "THORN" | "Tau" | "Theta"
+                    | "Uacute" | "Ucirc" | "Ugrave" | "Upsilon" | "Uuml" | "Xi"
+                    | "Yacute" | "Yuml" | "Zeta" | "aacute" | "acirc" | "acute"
+                    | "aelig" | "agrave" | "alefsym" | "alpha" | "amp" | "AMP"
+                    | "and" | "ang" | "apos" | "aring" | "asymp" | "atilde"
+                    | "auml" | "bdquo" | "beta" | "brvbar" | "bull" | "cap"
+                    | "ccedil" | "cedil" | "cent" | "chi" | "circ" | "clubs"
+                    | "cong" | "copy" | "COPY" | "crarr" | "cup" | "curren"
+                    | "dArr" | "dagger" | "darr" | "deg" | "delta" | "diams"
+                    | "divide" | "eacute" | "ecirc" | "egrave" | "empty"
+                    | "emsp" | "ensp" | "epsilon" | "equiv" | "eta" | "eth"
+                    | "euml" | "euro" | "exist" | "fnof" | "forall" | "frac12"
+                    | "frac14" | "frac34" | "frasl" | "gamma" | "ge" | "gt"
+                    | "GT" | "hArr" | "harr" | "hearts" | "hellip" | "iacute"
+                    | "icirc" | "iexcl" | "igrave" | "image" | "infin" | "int"
+                    | "iota" | "iquest" | "isin" | "iuml" | "kappa" | "lArr"
+                    | "lambda" | "lang" | "laquo" | "larr" | "lceil" | "ldquo"
+                    | "le" | "lfloor" | "lowast" | "loz" | "lrm" | "lsaquo"
+                    | "lsquo" | "lt" | "LT" | "macr" | "mdash" | "micro"
+                    | "middot" | "minus" | "mu" | "nabla" | "nbsp" | "ndash"
+                    | "ne" | "ni" | "not" | "notin" | "nsub" | "ntilde" | "nu"
+                    | "oacute" | "ocirc" | "oelig" | "ograve" | "oline"
+                    | "omega" | "omicron" | "oplus" | "or" | "ordf" | "ordm"
+                    | "oslash" | "otilde" | "otimes" | "ouml" | "para" | "part"
+                    | "permil" | "perp" | "phi" | "pi" | "piv" | "plusmn"
+                    | "pound" | "prime" | "prod" | "prop" | "psi" | "quot"
+                    | "QUOT" | "rArr" | "radic" | "rang" | "raquo" | "rarr"
+                    | "rceil" | "rdquo" | "real" | "reg" | "REG" | "rfloor"
+                    | "rho" | "rlm" | "rsaquo" | "rsquo" | "sbquo" | "scaron"
+                    | "sdot" | "sect" | "shy" | "sigma" | "sigmaf" | "sim"
+                    | "spades" | "sub" | "sube" | "sum" | "sup" | "sup1"
+                    | "sup2" | "sup3" | "supe" | "szlig" | "tau" | "there4"
+                    | "theta" | "thetasym" | "thinsp" | "thorn" | "tilde"
+                    | "times" | "trade" | "uArr" | "uacute" | "uarr" | "ucirc"
+                    | "ugrave" | "uml" | "upsih" | "upsilon" | "uuml"
+                    | "weierp" | "xi" | "yacute" | "yen" | "yuml" | "zeta"
+                    | "zwj" | "zwnj" )
+%{
+  private static final Map<String,String> upperCaseVariantsAccepted
+      = new HashMap<>();
+  static {
+    upperCaseVariantsAccepted.put("quot", "QUOT");
+    upperCaseVariantsAccepted.put("copy", "COPY");
+    upperCaseVariantsAccepted.put("gt", "GT");
+    upperCaseVariantsAccepted.put("lt", "LT");
+    upperCaseVariantsAccepted.put("reg", "REG");
+    upperCaseVariantsAccepted.put("amp", "AMP");
+  }
+  private static final CharArrayMap<Character> entityValues
+      = new CharArrayMap<>(253, false);
+  static {
+    String[] entities = {
+      "AElig", "\u00C6", "Aacute", "\u00C1", "Acirc", "\u00C2",
+      "Agrave", "\u00C0", "Alpha", "\u0391", "Aring", "\u00C5",
+      "Atilde", "\u00C3", "Auml", "\u00C4", "Beta", "\u0392",
+      "Ccedil", "\u00C7", "Chi", "\u03A7", "Dagger", "\u2021",
+      "Delta", "\u0394", "ETH", "\u00D0", "Eacute", "\u00C9",
+      "Ecirc", "\u00CA", "Egrave", "\u00C8", "Epsilon", "\u0395",
+      "Eta", "\u0397", "Euml", "\u00CB", "Gamma", "\u0393", "Iacute", "\u00CD",
+      "Icirc", "\u00CE", "Igrave", "\u00CC", "Iota", "\u0399",
+      "Iuml", "\u00CF", "Kappa", "\u039A", "Lambda", "\u039B", "Mu", "\u039C",
+      "Ntilde", "\u00D1", "Nu", "\u039D", "OElig", "\u0152",
+      "Oacute", "\u00D3", "Ocirc", "\u00D4", "Ograve", "\u00D2",
+      "Omega", "\u03A9", "Omicron", "\u039F", "Oslash", "\u00D8",
+      "Otilde", "\u00D5", "Ouml", "\u00D6", "Phi", "\u03A6", "Pi", "\u03A0",
+      "Prime", "\u2033", "Psi", "\u03A8", "Rho", "\u03A1", "Scaron", "\u0160",
+      "Sigma", "\u03A3", "THORN", "\u00DE", "Tau", "\u03A4", "Theta", "\u0398",
+      "Uacute", "\u00DA", "Ucirc", "\u00DB", "Ugrave", "\u00D9",
+      "Upsilon", "\u03A5", "Uuml", "\u00DC", "Xi", "\u039E",
+      "Yacute", "\u00DD", "Yuml", "\u0178", "Zeta", "\u0396",
+      "aacute", "\u00E1", "acirc", "\u00E2", "acute", "\u00B4",
+      "aelig", "\u00E6", "agrave", "\u00E0", "alefsym", "\u2135",
+      "alpha", "\u03B1", "amp", "\u0026", "and", "\u2227", "ang", "\u2220",
+      "apos", "\u0027", "aring", "\u00E5", "asymp", "\u2248",
+      "atilde", "\u00E3", "auml", "\u00E4", "bdquo", "\u201E",
+      "beta", "\u03B2", "brvbar", "\u00A6", "bull", "\u2022", "cap", "\u2229",
+      "ccedil", "\u00E7", "cedil", "\u00B8", "cent", "\u00A2", "chi", "\u03C7",
+      "circ", "\u02C6", "clubs", "\u2663", "cong", "\u2245", "copy", "\u00A9",
+      "crarr", "\u21B5", "cup", "\u222A", "curren", "\u00A4", "dArr", "\u21D3",
+      "dagger", "\u2020", "darr", "\u2193", "deg", "\u00B0", "delta", "\u03B4",
+      "diams", "\u2666", "divide", "\u00F7", "eacute", "\u00E9",
+      "ecirc", "\u00EA", "egrave", "\u00E8", "empty", "\u2205",
+      "emsp", "\u2003", "ensp", "\u2002", "epsilon", "\u03B5",
+      "equiv", "\u2261", "eta", "\u03B7", "eth", "\u00F0", "euml", "\u00EB",
+      "euro", "\u20AC", "exist", "\u2203", "fnof", "\u0192",
+      "forall", "\u2200", "frac12", "\u00BD", "frac14", "\u00BC",
+      "frac34", "\u00BE", "frasl", "\u2044", "gamma", "\u03B3", "ge", "\u2265",
+      "gt", "\u003E", "hArr", "\u21D4", "harr", "\u2194", "hearts", "\u2665",
+      "hellip", "\u2026", "iacute", "\u00ED", "icirc", "\u00EE",
+      "iexcl", "\u00A1", "igrave", "\u00EC", "image", "\u2111",
+      "infin", "\u221E", "int", "\u222B", "iota", "\u03B9", "iquest", "\u00BF",
+      "isin", "\u2208", "iuml", "\u00EF", "kappa", "\u03BA", "lArr", "\u21D0",
+      "lambda", "\u03BB", "lang", "\u2329", "laquo", "\u00AB",
+      "larr", "\u2190", "lceil", "\u2308", "ldquo", "\u201C", "le", "\u2264",
+      "lfloor", "\u230A", "lowast", "\u2217", "loz", "\u25CA", "lrm", "\u200E",
+      "lsaquo", "\u2039", "lsquo", "\u2018", "lt", "\u003C", "macr", "\u00AF",
+      "mdash", "\u2014", "micro", "\u00B5", "middot", "\u00B7",
+      "minus", "\u2212", "mu", "\u03BC", "nabla", "\u2207", "nbsp", " ",
+      "ndash", "\u2013", "ne", "\u2260", "ni", "\u220B", "not", "\u00AC",
+      "notin", "\u2209", "nsub", "\u2284", "ntilde", "\u00F1", "nu", "\u03BD",
+      "oacute", "\u00F3", "ocirc", "\u00F4", "oelig", "\u0153",
+      "ograve", "\u00F2", "oline", "\u203E", "omega", "\u03C9",
+      "omicron", "\u03BF", "oplus", "\u2295", "or", "\u2228", "ordf", "\u00AA",
+      "ordm", "\u00BA", "oslash", "\u00F8", "otilde", "\u00F5",
+      "otimes", "\u2297", "ouml", "\u00F6", "para", "\u00B6", "part", "\u2202",
+      "permil", "\u2030", "perp", "\u22A5", "phi", "\u03C6", "pi", "\u03C0",
+      "piv", "\u03D6", "plusmn", "\u00B1", "pound", "\u00A3",
+      "prime", "\u2032", "prod", "\u220F", "prop", "\u221D", "psi", "\u03C8",
+      "quot", "\"", "rArr", "\u21D2", "radic", "\u221A", "rang", "\u232A",
+      "raquo", "\u00BB", "rarr", "\u2192", "rceil", "\u2309",
+      "rdquo", "\u201D", "real", "\u211C", "reg", "\u00AE", "rfloor", "\u230B",
+      "rho", "\u03C1", "rlm", "\u200F", "rsaquo", "\u203A", "rsquo", "\u2019",
+      "sbquo", "\u201A", "scaron", "\u0161", "sdot", "\u22C5",
+      "sect", "\u00A7", "shy", "\u00AD", "sigma", "\u03C3", "sigmaf", "\u03C2",
+      "sim", "\u223C", "spades", "\u2660", "sub", "\u2282", "sube", "\u2286",
+      "sum", "\u2211", "sup", "\u2283", "sup1", "\u00B9", "sup2", "\u00B2",
+      "sup3", "\u00B3", "supe", "\u2287", "szlig", "\u00DF", "tau", "\u03C4",
+      "there4", "\u2234", "theta", "\u03B8", "thetasym", "\u03D1",
+      "thinsp", "\u2009", "thorn", "\u00FE", "tilde", "\u02DC",
+      "times", "\u00D7", "trade", "\u2122", "uArr", "\u21D1",
+      "uacute", "\u00FA", "uarr", "\u2191", "ucirc", "\u00FB",
+      "ugrave", "\u00F9", "uml", "\u00A8", "upsih", "\u03D2",
+      "upsilon", "\u03C5", "uuml", "\u00FC", "weierp", "\u2118",
+      "xi", "\u03BE", "yacute", "\u00FD", "yen", "\u00A5", "yuml", "\u00FF",
+      "zeta", "\u03B6", "zwj", "\u200D", "zwnj", "\u200C"
+    };
+    for (int i = 0 ; i < entities.length ; i += 2) {
+      Character value = entities[i + 1].charAt(0);
+      entityValues.put(entities[i], value);
+      String upperCaseVariant = upperCaseVariantsAccepted.get(entities[i]);
+      if (upperCaseVariant != null) {
+        entityValues.put(upperCaseVariant, value);
+      }
+    }
+  }
+%}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/cf50/HTMLStripCharFilter50.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/cf50/HTMLStripCharFilter50.jflex
new file mode 100644
index 0000000..ea406ea
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/cf50/HTMLStripCharFilter50.jflex
@@ -0,0 +1,937 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.analysis.charfilter.cf50;
+
+import java.io.IOException;
+import java.io.Reader;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.analysis.charfilter.BaseCharFilter;
+import org.apache.lucene.analysis.util.CharArrayMap;
+import org.apache.lucene.analysis.util.CharArraySet;
+import org.apache.lucene.analysis.util.OpenStringBuilder;
+
+/**
+ * A CharFilter that wraps another Reader and attempts to strip out HTML constructs.
+ */
+@SuppressWarnings("fallthrough")
+%%
+
+%unicode 6.3
+%apiprivate
+%type int
+%final
+%public
+%char
+%function nextChar
+%class HTMLStripCharFilter50
+%extends BaseCharFilter
+%xstate AMPERSAND, NUMERIC_CHARACTER, CHARACTER_REFERENCE_TAIL
+%xstate LEFT_ANGLE_BRACKET, BANG, COMMENT, SCRIPT, SCRIPT_COMMENT
+%xstate LEFT_ANGLE_BRACKET_SLASH, LEFT_ANGLE_BRACKET_SPACE, CDATA
+%xstate SERVER_SIDE_INCLUDE, SINGLE_QUOTED_STRING, DOUBLE_QUOTED_STRING
+%xstate END_TAG_TAIL_INCLUDE, END_TAG_TAIL_EXCLUDE, END_TAG_TAIL_SUBSTITUTE
+%xstate START_TAG_TAIL_INCLUDE, START_TAG_TAIL_EXCLUDE, START_TAG_TAIL_SUBSTITUTE
+%xstate STYLE, STYLE_COMMENT
+
+// From XML 1.0 <http://www.w3.org/TR/xml/>:
+//
+//    [4]  NameStartChar ::= ":" | [A-Z] | "_" | [a-z] | [...]
+//    [4a] NameChar      ::= NameStartChar | "-" | "." | [0-9] | [...]
+//    [5]  Name          ::= NameStartChar (NameChar)*
+//
+// From UAX #31: Unicode Identifier and Pattern Syntax
+// <http://unicode.org/reports/tr31/>:
+//
+//    D1. Default Identifier Syntax
+//
+//        <identifier> := <ID_Start> <ID_Continue>*
+//
+Name = [:_\p{ID_Start}] [-.:_\p{ID_Continue}]*
+
+// From Apache httpd mod_include documentation
+// <http://httpd.apache.org/docs/current/mod/mod_include.html>:
+//
+// Basic Elements
+//
+//    The document is parsed as an HTML document, with special commands
+//    embedded as SGML comments. A command has the syntax:
+//
+//       <!--#element attribute=value attribute=value ... -->
+//
+//    The value will often be enclosed in double quotes, but single quotes (')
+//    and backticks (`) are also possible. Many commands only allow a single
+//    attribute-value pair. Note that the comment terminator (-->) should be
+//    preceded by whitespace to ensure that it isn't considered part of an SSI
+//    token. Note that the leading <!--# is one token and may not contain any
+//    whitespaces.
+//
+
+EventAttributeSuffixes = ( [aA][bB][oO][rR][tT]                 |
+                           [bB][lL][uU][rR]                     |
+                           [cC][hH][aA][nN][gG][eE]             |
+                           [cC][lL][iI][cC][kK]                 |
+                           [dD][bB][lL][cC][lL][iI][cC][kK]     |
+                           [eE][rR][rR][oO][rR]                 |
+                           [fF][oO][cC][uU][sS]                 |
+                           [kK][eE][yY][dD][oO][wW][nN]         |
+                           [kK][eE][yY][pP][rR][eE][sS][sS]     |
+                           [kK][eE][yY][uU][pP]                 |
+                           [lL][oO][aA][dD]                     |
+                           [mM][oO][uU][sS][eE][dD][oO][wW][nN] |
+                           [mM][oO][uU][sS][eE][mM][oO][vV][eE] |
+                           [mM][oO][uU][sS][eE][oO][uU][tT]     |
+                           [mM][oO][uU][sS][eE][oO][vV][eE][rR] |
+                           [mM][oO][uU][sS][eE][uU][pP]         |
+                           [rR][eE][sS][eE][tT]                 |
+                           [sS][eE][lL][eE][cC][tT]             |
+                           [sS][uU][bB][mM][iI][tT]             |
+                           [uU][nN][lL][oO][aA][dD]             )
+
+SingleQuoted = ( "'" ( "\\'" | [^']* )* "'" )
+DoubleQuoted = ( "\"" ( "\\\"" | [^\"]* )* "\"" )
+ServerSideInclude = ( "<!--#" ( [^'\"] | {SingleQuoted} | {DoubleQuoted} )* "-->" )
+EventAttribute = [oO][nN] {EventAttributeSuffixes} \s* "=" \s* ( {SingleQuoted} | {DoubleQuoted} )
+OpenTagContent = ( {EventAttribute} | [^<>] | {ServerSideInclude} )*
+
+InlineElment = ( [aAbBiIqQsSuU]                   |
+                 [aA][bB][bB][rR]                 |
+                 [aA][cC][rR][oO][nN][yY][mM]     |
+                 [bB][aA][sS][eE][fF][oO][nN][tT] |
+                 [bB][dD][oO]                     |
+                 [bB][iI][gG]                     |
+                 [cC][iI][tT][eE]                 |
+                 [cC][oO][dD][eE]                 |
+                 [dD][fF][nN]                     |
+                 [eE][mM]                         |
+                 [fF][oO][nN][tT]                 |
+                 [iI][mM][gG]                     |
+                 [iI][nN][pP][uU][tT]             |
+                 [kK][bB][dD]                     |
+                 [lL][aA][bB][eE][lL]             |
+                 [sS][aA][mM][pP]                 |
+                 [sS][eE][lL][eE][cC][tT]         |
+                 [sS][mM][aA][lL][lL]             |
+                 [sS][pP][aA][nN]                 |
+                 [sS][tT][rR][iI][kK][eE]         |
+                 [sS][tT][rR][oO][nN][gG]         |
+                 [sS][uU][bB]                     |
+                 [sS][uU][pP]                     |
+                 [tT][eE][xX][tT][aA][rR][eE][aA] |
+                 [tT][tT]                         |
+                 [vV][aA][rR]                     )
+
+
+%include HTMLCharacterEntities50.jflex
+
+%{
+  private static final int INITIAL_INPUT_SEGMENT_SIZE = 1024;
+  private static final char BLOCK_LEVEL_START_TAG_REPLACEMENT = '\n';
+  private static final char BLOCK_LEVEL_END_TAG_REPLACEMENT = '\n';
+  private static final char BR_START_TAG_REPLACEMENT = '\n';
+  private static final char BR_END_TAG_REPLACEMENT = '\n';
+  private static final char SCRIPT_REPLACEMENT = '\n';
+  private static final char STYLE_REPLACEMENT = '\n';
+  private static final char REPLACEMENT_CHARACTER = '\uFFFD';
+
+  private CharArraySet escapedTags = null;
+  private int inputStart;
+  private int cumulativeDiff;
+  private boolean escapeBR = false;
+  private boolean escapeSCRIPT = false;
+  private boolean escapeSTYLE = false;
+  private int restoreState;
+  private int previousRestoreState;
+  private int outputCharCount;
+  private int eofReturnValue;
+  private TextSegment inputSegment
+      = new TextSegment(INITIAL_INPUT_SEGMENT_SIZE);
+  private TextSegment outputSegment = inputSegment;
+  private TextSegment entitySegment = new TextSegment(2);
+
+  /**
+   * Creates a new HTMLStripCharFilter50 over the provided Reader.
+   * @param source Reader to strip html tags from.
+   */
+  public HTMLStripCharFilter50(Reader source) {
+    super(source);
+    this.zzReader = source;
+  }
+
+  /**
+   * Creates a new HTMLStripCharFilter50 over the provided Reader
+   * with the specified start and end tags.
+   * @param source Reader to strip html tags from.
+   * @param escapedTags Tags in this set (both start and end tags)
+   *  will not be filtered out.
+   */
+  public HTMLStripCharFilter50(Reader source, Set<String> escapedTags) {
+    super(source);
+    this.zzReader = source;
+    if (null != escapedTags) {
+      for (String tag : escapedTags) {
+        if (tag.equalsIgnoreCase("BR")) {
+          escapeBR = true;
+        } else if (tag.equalsIgnoreCase("SCRIPT")) {
+          escapeSCRIPT = true;
+        } else if (tag.equalsIgnoreCase("STYLE")) {
+          escapeSTYLE = true;
+        } else {
+          if (null == this.escapedTags) {
+            this.escapedTags = new CharArraySet(16, true);
+          }
+          this.escapedTags.add(tag);
+        }
+      }
+    }
+  }
+
+  @Override
+  public int read() throws IOException {
+    if (outputSegment.isRead()) {
+      if (zzAtEOF) {
+        return -1;
+      }
+      int ch = nextChar();
+      ++outputCharCount;
+      return ch;
+    }
+    int ch = outputSegment.nextChar();
+    ++outputCharCount;
+    return ch;
+  }
+
+  @Override
+  public int read(char cbuf[], int off, int len) throws IOException {
+    int i = 0;
+    for ( ; i < len ; ++i) {
+      int ch = read();
+      if (ch == -1) break;
+      cbuf[off++] = (char)ch;
+    }
+    return i > 0 ? i : (len == 0 ? 0 : -1);
+  }
+
+  @Override
+  public void close() throws IOException {
+    yyclose();
+  }
+
+  static int getInitialBufferSize() {  // Package private, for testing purposes
+    return ZZ_BUFFERSIZE;
+  }
+
+  private class TextSegment extends OpenStringBuilder {
+    /** The position from which the next char will be read. */
+    int pos = 0;
+
+    /** Wraps the given buffer and sets this.len to the given length. */
+    TextSegment(char[] buffer, int length) {
+      super(buffer, length);
+    }
+
+    /** Allocates an internal buffer of the given size. */
+    TextSegment(int size) {
+      super(size);
+    }
+
+    /** Sets len = 0 and pos = 0. */
+    void clear() {
+      reset();
+      restart();
+    }
+
+    /** Sets pos = 0 */
+    void restart() {
+      pos = 0;
+    }
+
+    /** Returns the next char in the segment. */
+    int nextChar() {
+      assert (! isRead()): "Attempting to read past the end of a segment.";
+      return buf[pos++];
+    }
+
+    /** Returns true when all characters in the text segment have been read */
+    boolean isRead() {
+      return pos >= len;
+    }
+  }
+%}
+
+%eofval{
+  return eofReturnValue;
+%eofval}
+%eof{
+  switch (zzLexicalState) {
+    case SCRIPT:
+    case COMMENT:
+    case SCRIPT_COMMENT:
+    case STYLE:
+    case STYLE_COMMENT:
+    case SINGLE_QUOTED_STRING:
+    case DOUBLE_QUOTED_STRING:
+    case END_TAG_TAIL_EXCLUDE:
+    case END_TAG_TAIL_SUBSTITUTE:
+    case START_TAG_TAIL_EXCLUDE:
+    case SERVER_SIDE_INCLUDE:
+    case START_TAG_TAIL_SUBSTITUTE: { // Exclude
+      // add (length of input that won't be output) [ - (substitution length) = 0 ]
+      cumulativeDiff += yychar - inputStart;
+      // position the correction at (already output length) [ + (substitution length) = 0 ]
+      addOffCorrectMap(outputCharCount, cumulativeDiff);
+      outputSegment.clear();
+      eofReturnValue = -1;
+      break;
+    }
+    case CHARACTER_REFERENCE_TAIL: {        // Substitute
+      // At end of file, allow char refs without semicolons
+      // add (length of input that won't be output) - (substitution length)
+      cumulativeDiff += inputSegment.length() - outputSegment.length();
+      // position the correction at (already output length) + (substitution length)
+      addOffCorrectMap(outputCharCount + outputSegment.length(), cumulativeDiff);
+      eofReturnValue = ( ! outputSegment.isRead()) ? outputSegment.nextChar() : -1;
+      break;
+    }
+    case BANG:
+    case CDATA:
+    case AMPERSAND:
+    case NUMERIC_CHARACTER:
+    case END_TAG_TAIL_INCLUDE:
+    case START_TAG_TAIL_INCLUDE:
+    case LEFT_ANGLE_BRACKET:
+    case LEFT_ANGLE_BRACKET_SLASH:
+    case LEFT_ANGLE_BRACKET_SPACE: {        // Include
+      outputSegment = inputSegment;
+      eofReturnValue = ( ! outputSegment.isRead()) ? outputSegment.nextChar() : -1;
+      break;
+    }
+    default: {
+      eofReturnValue = -1;
+    }
+  }
+%eof}
+
+%%
+
+"&" {
+  inputStart = yychar;
+  inputSegment.clear();
+  inputSegment.append('&');
+  yybegin(AMPERSAND);
+}
+
+"<" {
+  inputStart = yychar;
+  inputSegment.clear();
+  inputSegment.append('<');
+  yybegin(LEFT_ANGLE_BRACKET);
+}
+
+<AMPERSAND> {
+  {CharacterEntities} {
+    int length = yylength();
+    inputSegment.write(zzBuffer, zzStartRead, length);
+    entitySegment.clear();
+    char ch = entityValues.get(zzBuffer, zzStartRead, length).charValue();
+    entitySegment.append(ch);
+    outputSegment = entitySegment;
+    yybegin(CHARACTER_REFERENCE_TAIL);
+  }
+  "#" { inputSegment.append('#'); yybegin(NUMERIC_CHARACTER); }
+
+//                                             1   1       11              11
+// 0  1   2   3       45              678  9   0   1       23              45
+  "#" [xX][dD][89aAbB][0-9a-fA-F]{2} ";&#" [xX][dD][c-fC-F][0-9a-fA-F]{2} ";" {
+    // Handle paired UTF-16 surrogates.
+    outputSegment = entitySegment;
+    outputSegment.clear();
+    String surrogatePair = yytext();
+    char highSurrogate = '\u0000';
+    try {
+      highSurrogate = (char)Integer.parseInt(surrogatePair.substring(2, 6), 16);
+    } catch(Exception e) { // should never happen
+      assert false: "Exception parsing high surrogate '"
+                  + surrogatePair.substring(2, 6) + "'";
+    }
+    try {
+      outputSegment.unsafeWrite
+          ((char)Integer.parseInt(surrogatePair.substring(10, 14), 16));
+    } catch(Exception e) { // should never happen
+      assert false: "Exception parsing low surrogate '"
+                  + surrogatePair.substring(10, 14) + "'";
+    }
+    // add (previously matched input length) + (this match length) - (substitution length)
+    cumulativeDiff += inputSegment.length() + yylength() - 2;
+    // position the correction at (already output length) + (substitution length)
+    addOffCorrectMap(outputCharCount + 2, cumulativeDiff);
+    inputSegment.clear();
+    yybegin(YYINITIAL);
+    return highSurrogate;
+  }
+
+//                          1   1       11              11
+// 01  2    345    678  9   0   1       23              45
+  "#5" [56] \d{3} ";&#" [xX][dD][c-fC-F][0-9a-fA-F]{2} ";" {
+    // Handle paired UTF-16 surrogates.
+    String surrogatePair = yytext();
+    char highSurrogate = '\u0000';
+    try { // High surrogates are in decimal range [55296, 56319]
+      highSurrogate = (char)Integer.parseInt(surrogatePair.substring(1, 6));
+    } catch(Exception e) { // should never happen
+      assert false: "Exception parsing high surrogate '"
+                  + surrogatePair.substring(1, 6) + "'";
+    }
+    if (Character.isHighSurrogate(highSurrogate)) {
+      outputSegment = entitySegment;
+      outputSegment.clear();
+      try {
+        outputSegment.unsafeWrite
+            ((char)Integer.parseInt(surrogatePair.substring(10, 14), 16));
+      } catch(Exception e) { // should never happen
+        assert false: "Exception parsing low surrogate '"
+                    + surrogatePair.substring(10, 14) + "'";
+      }
+      // add (previously matched input length) + (this match length) - (substitution length)
+      cumulativeDiff += inputSegment.length() + yylength() - 2;
+      // position the correction at (already output length) + (substitution length)
+      addOffCorrectMap(outputCharCount + 2, cumulativeDiff);
+      inputSegment.clear();
+      yybegin(YYINITIAL);
+      return highSurrogate;
+    }
+    yypushback(surrogatePair.length() - 1); // Consume only '#'
+    inputSegment.append('#');
+    yybegin(NUMERIC_CHARACTER);
+  }
+
+//                                          1    111     11
+// 0  1   2   3       45              6789  0    123     45
+  "#" [xX][dD][89aAbB][0-9a-fA-F]{2} ";&#5" [67] \d{3}  ";" {
+    // Handle paired UTF-16 surrogates.
+    String surrogatePair = yytext();
+    char highSurrogate = '\u0000';
+    char lowSurrogate = '\u0000';
+    try {
+      highSurrogate = (char)Integer.parseInt(surrogatePair.substring(2, 6), 16);
+    } catch(Exception e) { // should never happen
+      assert false: "Exception parsing high surrogate '"
+                  + surrogatePair.substring(2, 6) + "'";
+    }
+    try { // Low surrogates are in decimal range [56320, 57343]
+      lowSurrogate = (char)Integer.parseInt(surrogatePair.substring(9, 14));
+    } catch(Exception e) { // should never happen
+      assert false: "Exception parsing low surrogate '"
+                  + surrogatePair.substring(9, 14) + "'";
+    }
+    if (Character.isLowSurrogate(lowSurrogate)) {
+      outputSegment = entitySegment;
+      outputSegment.clear();
+      outputSegment.unsafeWrite(lowSurrogate);
+      // add (previously matched input length) + (this match length) - (substitution length)
+      cumulativeDiff += inputSegment.length() + yylength() - 2;
+      // position the correction at (already output length) + (substitution length)
+      addOffCorrectMap(outputCharCount + 2, cumulativeDiff);
+      inputSegment.clear();
+      yybegin(YYINITIAL);
+      return highSurrogate;
+    }
+    yypushback(surrogatePair.length() - 1); // Consume only '#'
+    inputSegment.append('#');
+    yybegin(NUMERIC_CHARACTER);
+  }
+
+//                       1    111     11
+// 01  2    345    6789  0    123     45
+  "#5" [56] \d{3} ";&#5" [67] \d{3}  ";" {
+    // Handle paired UTF-16 surrogates.
+    String surrogatePair = yytext();
+    char highSurrogate = '\u0000';
+    try { // High surrogates are in decimal range [55296, 56319]
+      highSurrogate = (char)Integer.parseInt(surrogatePair.substring(1, 6));
+    } catch(Exception e) { // should never happen
+      assert false: "Exception parsing high surrogate '"
+                  + surrogatePair.substring(1, 6) + "'";
+    }
+    if (Character.isHighSurrogate(highSurrogate)) {
+      char lowSurrogate = '\u0000';
+      try { // Low surrogates are in decimal range [56320, 57343]
+        lowSurrogate = (char)Integer.parseInt(surrogatePair.substring(9, 14));
+      } catch(Exception e) { // should never happen
+        assert false: "Exception parsing low surrogate '"
+                    + surrogatePair.substring(9, 14) + "'";
+      }
+      if (Character.isLowSurrogate(lowSurrogate)) {
+        outputSegment = entitySegment;
+        outputSegment.clear();
+        outputSegment.unsafeWrite(lowSurrogate);
+        // add (previously matched input length) + (this match length) - (substitution length)
+        cumulativeDiff += inputSegment.length() + yylength() - 2;
+        // position the correction at (already output length) + (substitution length)
+        addOffCorrectMap(outputCharCount + 2, cumulativeDiff);
+        inputSegment.clear();
+        yybegin(YYINITIAL);
+        return highSurrogate;
+      }
+    }
+    yypushback(surrogatePair.length() - 1); // Consume only '#'
+    inputSegment.append('#');
+    yybegin(NUMERIC_CHARACTER);
+  }
+}
+
+<NUMERIC_CHARACTER> {
+  [xX] [0-9A-Fa-f]+ {
+    int matchLength = yylength();
+    inputSegment.write(zzBuffer, zzStartRead, matchLength);
+    if (matchLength <= 6) { // 10FFFF: max 6 hex chars
+      String hexCharRef
+          = new String(zzBuffer, zzStartRead + 1, matchLength - 1);
+      int codePoint = 0;
+      try {
+        codePoint = Integer.parseInt(hexCharRef, 16);
+      } catch(Exception e) {
+        assert false: "Exception parsing hex code point '" + hexCharRef + "'";
+      }
+      if (codePoint <= 0x10FFFF) {
+        outputSegment = entitySegment;
+        outputSegment.clear();
+        if (codePoint >= Character.MIN_SURROGATE
+            && codePoint <= Character.MAX_SURROGATE) {
+          outputSegment.unsafeWrite(REPLACEMENT_CHARACTER);
+        } else {
+          outputSegment.setLength
+              (Character.toChars(codePoint, outputSegment.getArray(), 0));
+        }
+        yybegin(CHARACTER_REFERENCE_TAIL);
+      } else {
+        outputSegment = inputSegment;
+        yybegin(YYINITIAL);
+        return outputSegment.nextChar();
+      }
+    } else {
+      outputSegment = inputSegment;
+      yybegin(YYINITIAL);
+      return outputSegment.nextChar();
+    }
+  }
+  [0-9]+ {
+    int matchLength = yylength();
+    inputSegment.write(zzBuffer, zzStartRead, matchLength);
+    if (matchLength <= 7) { // 0x10FFFF = 1114111: max 7 decimal chars
+      String decimalCharRef = yytext();
+      int codePoint = 0;
+      try {
+        codePoint = Integer.parseInt(decimalCharRef);
+      } catch(Exception e) {
+        assert false: "Exception parsing code point '" + decimalCharRef + "'";
+      }
+      if (codePoint <= 0x10FFFF) {
+        outputSegment = entitySegment;
+        outputSegment.clear();
+        if (codePoint >= Character.MIN_SURROGATE
+            && codePoint <= Character.MAX_SURROGATE) {
+          outputSegment.unsafeWrite(REPLACEMENT_CHARACTER);
+        } else {
+          outputSegment.setLength
+              (Character.toChars(codePoint, outputSegment.getArray(), 0));
+        }
+        yybegin(CHARACTER_REFERENCE_TAIL);
+      } else {
+        outputSegment = inputSegment;
+        yybegin(YYINITIAL);
+        return outputSegment.nextChar();
+      }
+    } else {
+      outputSegment = inputSegment;
+      yybegin(YYINITIAL);
+      return outputSegment.nextChar();
+    }
+  }
+}
+
+<CHARACTER_REFERENCE_TAIL> {
+  ";" {
+    // add (previously matched input length) + (this match length) - (substitution length)
+    cumulativeDiff += inputSegment.length() + yylength() - outputSegment.length();
+    // position the correction at (already output length) + (substitution length)
+    addOffCorrectMap(outputCharCount + outputSegment.length(), cumulativeDiff);
+    yybegin(YYINITIAL);
+    return outputSegment.nextChar();
+  }
+}
+
+<LEFT_ANGLE_BRACKET_SLASH> {
+  \s+ { inputSegment.write(zzBuffer, zzStartRead, yylength()); }
+  [bB][rR] \s* ">" {
+    yybegin(YYINITIAL);
+    if (escapeBR) {
+      inputSegment.write(zzBuffer, zzStartRead, yylength());
+      outputSegment = inputSegment;
+      return outputSegment.nextChar();
+    } else {
+      // add (previously matched input length) + (this match length) - (substitution length)
+      cumulativeDiff += inputSegment.length() + yylength() - 1;
+      // position the correction at (already output length) + (substitution length)
+      addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
+      inputSegment.reset();
+      return BR_END_TAG_REPLACEMENT;
+    }
+  }
+  {InlineElment} {
+    inputSegment.write(zzBuffer, zzStartRead, yylength());
+    if (null != escapedTags
+        && escapedTags.contains(zzBuffer, zzStartRead, yylength())) {
+      yybegin(END_TAG_TAIL_INCLUDE);
+    } else {
+      yybegin(END_TAG_TAIL_EXCLUDE);
+    }
+  }
+  {Name} {
+    inputSegment.write(zzBuffer, zzStartRead, yylength());
+    if (null != escapedTags
+        && escapedTags.contains(zzBuffer, zzStartRead, yylength())) {
+      yybegin(END_TAG_TAIL_INCLUDE);
+    } else {
+      yybegin(END_TAG_TAIL_SUBSTITUTE);
+    }
+  }
+}
+
+<END_TAG_TAIL_INCLUDE> {
+   \s* ">" {
+     inputSegment.write(zzBuffer, zzStartRead, yylength());
+     outputSegment = inputSegment;
+     yybegin(YYINITIAL);
+     return outputSegment.nextChar();
+   }
+}
+
+<END_TAG_TAIL_EXCLUDE> {
+  \s* ">" {
+    // add (previously matched input length) + (this match length) [ - (substitution length) = 0 ]
+    cumulativeDiff += inputSegment.length() + yylength();
+    // position the correction at (already output length) [ + (substitution length) = 0 ]
+    addOffCorrectMap(outputCharCount, cumulativeDiff);
+    inputSegment.clear();
+    yybegin(YYINITIAL);
+  }
+}
+
+<END_TAG_TAIL_SUBSTITUTE> {
+  \s* ">" {
+    // add (previously matched input length) + (this match length) - (substitution length)
+    cumulativeDiff += inputSegment.length() + yylength() - 1;
+    // position the correction at (already output length) + (substitution length)
+    addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
+    inputSegment.clear();
+    yybegin(YYINITIAL);
+    return BLOCK_LEVEL_END_TAG_REPLACEMENT;
+  }
+}
+
+<LEFT_ANGLE_BRACKET> {
+  "!" { inputSegment.append('!'); yybegin(BANG); }
+  "/" { inputSegment.append('/'); yybegin(LEFT_ANGLE_BRACKET_SLASH); }
+  \s+ {
+    inputSegment.write(zzBuffer, zzStartRead, yylength());
+    yybegin(LEFT_ANGLE_BRACKET_SPACE);
+  }
+  "?" [^>]* [/?] ">" {
+    // add (previously matched input length) + (this match length) [ - (substitution length) = 0 ]
+    cumulativeDiff += inputSegment.length() + yylength();
+    // position the correction at (already output length) [ + (substitution length) = 0 ]
+    addOffCorrectMap(outputCharCount, cumulativeDiff);
+    inputSegment.clear();
+    yybegin(YYINITIAL);
+  }
+  \s* [bB][rR] ( ( "="\s* | \s+ ) {OpenTagContent} )? \s* "/"? ">" {
+    yybegin(YYINITIAL);
+    if (escapeBR) {
+      inputSegment.write(zzBuffer, zzStartRead, yylength());
+      outputSegment = inputSegment;
+      return outputSegment.nextChar();
+    } else {
+      // add (previously matched input length) + (this match length) - (substitution length)
+      cumulativeDiff += inputSegment.length() + yylength() - 1;
+      // position the correction at (already output length) + (substitution length)
+      addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
+      inputSegment.reset();
+      return BR_START_TAG_REPLACEMENT;
+    }
+  }
+  \s* [sS][cC][rR][iI][pP][tT] ( \s+ {OpenTagContent} )? \s*  ">" {
+    yybegin(SCRIPT);
+    if (escapeSCRIPT) {
+      inputSegment.write(zzBuffer, zzStartRead, yylength());
+      outputSegment = inputSegment;
+      inputStart += 1 + yylength();
+      return outputSegment.nextChar();
+    }
+  }
+  \s* [sS][tT][yY][lL][eE] ( \s+ {OpenTagContent} )? \s* ">" {
+    yybegin(STYLE);
+    if (escapeSTYLE) {
+      inputSegment.write(zzBuffer, zzStartRead, yylength());
+      outputSegment = inputSegment;
+      inputStart += 1 + yylength();
+      return outputSegment.nextChar();
+    }
+  }
+}
+
+<LEFT_ANGLE_BRACKET, LEFT_ANGLE_BRACKET_SPACE> {
+  {InlineElment} {
+    inputSegment.write(zzBuffer, zzStartRead, yylength());
+    if (null != escapedTags
+        && escapedTags.contains(zzBuffer, zzStartRead, yylength())) {
+      yybegin(START_TAG_TAIL_INCLUDE);
+    } else {
+      yybegin(START_TAG_TAIL_EXCLUDE);
+    }
+  }
+  {Name} {
+    inputSegment.write(zzBuffer, zzStartRead, yylength());
+    if (null != escapedTags
+        && escapedTags.contains(zzBuffer, zzStartRead, yylength())) {
+      yybegin(START_TAG_TAIL_INCLUDE);
+    } else {
+      yybegin(START_TAG_TAIL_SUBSTITUTE);
+    }
+  }
+}
+
+<START_TAG_TAIL_INCLUDE> {
+   ( ( "="\s* | \s+ ) {OpenTagContent} )? \s* "/"? ">" {
+     inputSegment.write(zzBuffer, zzStartRead, yylength());
+     outputSegment = inputSegment;
+     yybegin(YYINITIAL);
+     return outputSegment.nextChar();
+   }
+}
+
+<START_TAG_TAIL_EXCLUDE> {
+   ( ( "="\s* | \s+ ) {OpenTagContent} )? \s* "/"? ">" {
+    // add (previously matched input length) + (this match length) [ - (substitution length) = 0 ]
+    cumulativeDiff += inputSegment.length() + yylength();
+    // position the correction at (already output length) [ + (substitution length) = 0 ]
+    addOffCorrectMap(outputCharCount, cumulativeDiff);
+    inputSegment.clear();
+    outputSegment = inputSegment;
+    yybegin(YYINITIAL);
+  }
+}
+
+<START_TAG_TAIL_SUBSTITUTE> {
+  ( ( "="\s* | \s+ ) {OpenTagContent} )? \s*  "/"? ">" {
+    // add (previously matched input length) + (this match length) - (substitution length)
+    cumulativeDiff += inputSegment.length() + yylength() - 1;
+    // position the correction at (already output length) + (substitution length)
+    addOffCorrectMap(outputCharCount + 1, cumulativeDiff);
+    inputSegment.clear();
+    yybegin(YYINITIAL);
+    return BLOCK_LEVEL_START_TAG_REPLACEMENT;
+  }
+}
+
+<BANG> {
+  "--" {
+    if (inputSegment.length() > 2) { // Chars between "<!" and "--" - this is not a comment
+      inputSegment.append(yytext());
+    } else {
+      yybegin(COMMENT);
+    }
+  }
+  ">" {
+    // add (previously matched input length) + (this match length) [ - (substitution length) = 0 ]
+    cumulativeDiff += inputSegment.length() + yylength();
+    // position the correction at (already output length) [ + (substitution length) = 0 ]
+    addOffCorrectMap(outputCharCount, cumulativeDiff);
+    inputSegment.clear();
+    yybegin(YYINITIAL);
+  }
+  // From XML 1.0 <http://www.w3.org/TR/xml/>:
+  //
+  // [18] CDSect  ::= CDStart CData CDEnd
+  // [19] CDStart ::= '<![CDATA['
+  // [20] CData   ::= (Char* - (Char* ']]>' Char*))
+  // [21] CDEnd   ::= ']]>'
+  //
+  "[CDATA[" {
+    if (inputSegment.length() > 2) { // Chars between "<!" and "[CDATA[" - this is not a CDATA section
+      inputSegment.append(yytext());
+    } else {
+      // add (previously matched input length) + (this match length) [ - (substitution length) = 0 ]
+      cumulativeDiff += inputSegment.length() + yylength();
+      // position the correction at (already output length) [ + (substitution length) = 0 ]
+      addOffCorrectMap(outputCharCount, cumulativeDiff);
+      inputSegment.clear();
+      yybegin(CDATA);
+    }
+  }
+  [^] {
+    inputSegment.append(yytext());
+  }
+}
+
+<CDATA> {
+  "]]>" {
+    // add (this match length) [ - (substitution length) = 0 ]
+    cumulativeDiff += yylength();
+    // position the correction at (already output length) [ + (substitution length) = 0 ]
+    addOffCorrectMap(outputCharCount, cumulativeDiff);
+    yybegin(YYINITIAL);
+  }
+  [^] { 
+    if (yylength() == 1) {
+      return zzBuffer[zzStartRead];
+    } else {
+      outputSegment.append(yytext()); return outputSegment.nextChar();
+    }
+  }
+}
+
+<COMMENT> {
+  "<!--#" { restoreState = COMMENT; yybegin(SERVER_SIDE_INCLUDE); }
+  "-->" {
+    // add (previously matched input length) + (this match length) [ - (substitution length) = 0]
+    cumulativeDiff += yychar - inputStart + yylength();
+    // position the correction at (already output length) [ + (substitution length) = 0]
+    addOffCorrectMap(outputCharCount, cumulativeDiff);
+    inputSegment.clear();
+    yybegin(YYINITIAL);
+  }
+  [^] { }
+}
+
+<SERVER_SIDE_INCLUDE> {
+  "-->" { yybegin(restoreState); }
+  "'" {
+    previousRestoreState = restoreState;
+    restoreState = SERVER_SIDE_INCLUDE;
+    yybegin(SINGLE_QUOTED_STRING);
+  }
+  "\"" {
+    previousRestoreState = restoreState;
+    restoreState = SERVER_SIDE_INCLUDE;
+    yybegin(DOUBLE_QUOTED_STRING);
+  }
+  [^] { }
+}
+
+<SCRIPT_COMMENT> {
+  "<!--#" { restoreState = SCRIPT_COMMENT; yybegin(SERVER_SIDE_INCLUDE); }
+  "'"     { restoreState = SCRIPT_COMMENT; yybegin(SINGLE_QUOTED_STRING); }
+  "\""    { restoreState = SCRIPT_COMMENT; yybegin(DOUBLE_QUOTED_STRING); }
+  "-->"   { yybegin(SCRIPT); }
+  [^] { }
+}
+
+<STYLE_COMMENT> {
+  "<!--#" { restoreState = STYLE_COMMENT; yybegin(SERVER_SIDE_INCLUDE); }
+  "'"     { restoreState = STYLE_COMMENT; yybegin(SINGLE_QUOTED_STRING); }
+  "\""    { restoreState = STYLE_COMMENT; yybegin(DOUBLE_QUOTED_STRING); }
+  "-->"   { yybegin(STYLE); }
+  [^] { }
+}
+
+<SINGLE_QUOTED_STRING> {
+  "\\" [^] { }
+  "'" { yybegin(restoreState); restoreState = previousRestoreState; }
+  [^] { }
+}
+
+<DOUBLE_QUOTED_STRING> {
+  "\\" [^] { }
+  "\"" { yybegin(restoreState); restoreState = previousRestoreState; }
+  [^] { }
+}
+
+<SCRIPT> {
+  "<!--" { yybegin(SCRIPT_COMMENT); }
+  "</" \s* [sS][cC][rR][iI][pP][tT] \s* ">" {
+    inputSegment.clear();
+    yybegin(YYINITIAL);
+    // add (previously matched input length) -- current match and substitution handled below
+    cumulativeDiff += yychar - inputStart;
+    // position at (already output length) -- substitution handled below
+    int offsetCorrectionPos = outputCharCount;
+    int returnValue;
+    if (escapeSCRIPT) {
+      inputSegment.write(zzBuffer, zzStartRead, yylength());
+      outputSegment = inputSegment;
+      returnValue = outputSegment.nextChar();
+    } else {
+      // add (this match length) - (substitution length)
+      cumulativeDiff += yylength() - 1;
+      // add (substitution length)
+      ++offsetCorrectionPos;
+      returnValue = SCRIPT_REPLACEMENT;
+    }
+    addOffCorrectMap(offsetCorrectionPos, cumulativeDiff);
+    return returnValue;
+  }
+  [^] { }
+}
+
+<STYLE> {
+  "<!--" { yybegin(STYLE_COMMENT); }
+  "</" \s* [sS][tT][yY][lL][eE] \s* ">" {
+    inputSegment.clear();
+    yybegin(YYINITIAL);
+    // add (previously matched input length) -- current match and substitution handled below
+    cumulativeDiff += yychar - inputStart;
+    // position the offset correction at (already output length) -- substitution handled below
+    int offsetCorrectionPos = outputCharCount;
+    int returnValue;
+    if (escapeSTYLE) {
+      inputSegment.write(zzBuffer, zzStartRead, yylength());
+      outputSegment = inputSegment;
+      returnValue = outputSegment.nextChar();
+    } else {
+      // add (this match length) - (substitution length)
+      cumulativeDiff += yylength() - 1;
+      // add (substitution length)
+      ++offsetCorrectionPos;
+      returnValue = STYLE_REPLACEMENT;
+    }
+    addOffCorrectMap(offsetCorrectionPos, cumulativeDiff);
+    return returnValue;
+  }
+  [^] { }
+}
+
+<AMPERSAND,NUMERIC_CHARACTER,CHARACTER_REFERENCE_TAIL,LEFT_ANGLE_BRACKET_SLASH,END_TAG_TAIL_INCLUDE,END_TAG_TAIL_EXCLUDE,END_TAG_TAIL_SUBSTITUTE,LEFT_ANGLE_BRACKET,LEFT_ANGLE_BRACKET_SPACE,START_TAG_TAIL_INCLUDE,START_TAG_TAIL_EXCLUDE,START_TAG_TAIL_SUBSTITUTE,BANG> {
+  [^] {
+    yypushback(yylength());
+    outputSegment = inputSegment;
+    outputSegment.restart();
+    yybegin(YYINITIAL);
+    return outputSegment.nextChar();
+  }
+}
+
+[^] { 
+  if (yylength() == 1) {
+    return zzBuffer[zzStartRead];
+  } else {
+    outputSegment.append(yytext()); return outputSegment.nextChar();
+  }
+}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/cf50/package-info.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/cf50/package-info.java
new file mode 100644
index 0000000..7f66bd2
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/cf50/package-info.java
@@ -0,0 +1,58 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Normalization of text before the tokenizer.
+ * <p>
+ *   CharFilters are chainable filters that normalize text before tokenization 
+ *   and provide mappings between normalized text offsets and the corresponding 
+ *   offset in the original text.
+ * </p>
+ * <H2>CharFilter offset mappings</H2>
+ * <p>
+ *   CharFilters modify an input stream via a series of substring
+ *   replacements (including deletions and insertions) to produce an output
+ *   stream. There are three possible replacement cases: the replacement
+ *   string has the same length as the original substring; the replacement
+ *   is shorter; and the replacement is longer. In the latter two cases
+ *   (when the replacement has a different length than the original),
+ *   one or more offset correction mappings are required.
+ * </p>
+ * <p>
+ *   When the replacement is shorter than the original (e.g. when the
+ *   replacement is the empty string), a single offset correction mapping
+ *   should be added at the replacement's end offset in the output stream.
+ *   The <code>cumulativeDiff</code> parameter to the
+ *   <code>addOffCorrectMapping()</code> method will be the sum of all
+ *   previous replacement offset adjustments, with the addition of the
+ *   difference between the lengths of the original substring and the
+ *   replacement string (a positive value).
+ * </p>
+ * <p>
+ *   When the replacement is longer than the original (e.g. when the
+ *   original is the empty string), you should add as many offset
+ *   correction mappings as the difference between the lengths of the
+ *   replacement string and the original substring, starting at the
+ *   end offset the original substring would have had in the output stream.
+ *   The <code>cumulativeDiff</code> parameter to the
+ *   <code>addOffCorrectMapping()</code> method will be the sum of all
+ *   previous replacement offset adjustments, with the addition of the
+ *   difference between the lengths of the original substring and the
+ *   replacement string so far (a negative value).
+ * </p>
+ */
+package org.apache.lucene.analysis.charfilter.cf50;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerFactory.java
index b2a7134..23d606f 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerFactory.java
@@ -17,8 +17,11 @@
 package org.apache.lucene.analysis.standard;
 
 
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.standard.std50.ClassicTokenizer50;
 import org.apache.lucene.analysis.util.TokenizerFactory;
 import org.apache.lucene.util.AttributeFactory;
+import org.apache.lucene.util.Version;
 
 import java.util.Map;
 
@@ -44,9 +47,15 @@ public class ClassicTokenizerFactory extends TokenizerFactory {
   }
 
   @Override
-  public ClassicTokenizer create(AttributeFactory factory) {
-    ClassicTokenizer tokenizer = new ClassicTokenizer(factory);
-    tokenizer.setMaxTokenLength(maxTokenLength);
-    return tokenizer;
+  public Tokenizer create(AttributeFactory factory) {
+    if (luceneMatchVersion.onOrAfter(Version.LUCENE_6_0_0)) {
+      ClassicTokenizer tokenizer = new ClassicTokenizer(factory);
+      tokenizer.setMaxTokenLength(maxTokenLength);
+      return tokenizer;
+    } else {
+      ClassicTokenizer50 tokenizer = new ClassicTokenizer50(factory);
+      tokenizer.setMaxTokenLength(maxTokenLength);
+      return tokenizer;
+    }
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex
index de65d26..e41d849 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex
@@ -26,7 +26,7 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 %%
 
 %class ClassicTokenizerImpl
-%unicode 3.0
+%unicode 7.0
 %integer
 %function getNextToken
 %pack
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerFactory.java
index c74c55c..5bca0ea 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerFactory.java
@@ -17,8 +17,11 @@
 package org.apache.lucene.analysis.standard;
 
 
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.standard.std50.StandardTokenizer50;
 import org.apache.lucene.analysis.util.TokenizerFactory;
 import org.apache.lucene.util.AttributeFactory;
+import org.apache.lucene.util.Version;
 
 import java.util.Map;
 
@@ -44,9 +47,15 @@ public class StandardTokenizerFactory extends TokenizerFactory {
   }
 
   @Override
-  public StandardTokenizer create(AttributeFactory factory) {
-    StandardTokenizer tokenizer = new StandardTokenizer(factory);
-    tokenizer.setMaxTokenLength(maxTokenLength);
-    return tokenizer;
+  public Tokenizer create(AttributeFactory factory) {
+    if (luceneMatchVersion.onOrAfter(Version.LUCENE_6_0_0)) {
+      StandardTokenizer tokenizer = new StandardTokenizer(factory);
+      tokenizer.setMaxTokenLength(maxTokenLength);
+      return tokenizer;
+    } else {
+      StandardTokenizer50 tokenizer = new StandardTokenizer50(factory);
+      tokenizer.setMaxTokenLength(maxTokenLength);
+      return tokenizer;
+    }
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.jflex
index 34f4ead..2182fb4 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.jflex
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.jflex
@@ -38,7 +38,7 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 @SuppressWarnings("fallthrough")
 %%
 
-%unicode 6.3
+%unicode 7.0
 %integer
 %final
 %public
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerFactory.java
index ee0bf45..554f35a 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerFactory.java
@@ -17,8 +17,11 @@
 package org.apache.lucene.analysis.standard;
 
 
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.standard.std50.UAX29URLEmailTokenizer50;
 import org.apache.lucene.analysis.util.TokenizerFactory;
 import org.apache.lucene.util.AttributeFactory;
+import org.apache.lucene.util.Version;
 
 import java.io.Reader;
 import java.util.Map;
@@ -44,10 +47,17 @@ public class UAX29URLEmailTokenizerFactory extends TokenizerFactory {
     }
   }
 
+  @SuppressWarnings("deprecation")
   @Override
-  public UAX29URLEmailTokenizer create(AttributeFactory factory) {
-    UAX29URLEmailTokenizer tokenizer = new UAX29URLEmailTokenizer(factory);
-    tokenizer.setMaxTokenLength(maxTokenLength);
-    return tokenizer;
+  public Tokenizer create(AttributeFactory factory) {
+    if (luceneMatchVersion.onOrAfter(Version.LUCENE_6_0_0)) {
+      UAX29URLEmailTokenizer tokenizer = new UAX29URLEmailTokenizer(factory);
+      tokenizer.setMaxTokenLength(maxTokenLength);
+      return tokenizer;
+    } else {
+      UAX29URLEmailTokenizer50 tokenizer = new UAX29URLEmailTokenizer50(factory);
+      tokenizer.setMaxTokenLength(maxTokenLength);
+      return tokenizer;
+    }
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.jflex
index 32af631..c39b7ce 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.jflex
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.jflex
@@ -41,7 +41,7 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 @SuppressWarnings("fallthrough")
 %%
 
-%unicode 6.3
+%unicode 7.0
 %integer
 %final
 %public
@@ -311,4 +311,4 @@ EMAIL = {EMAILlocalPart} "@" ({DomainNameStrict} | {EMAILbracketedHost})
   //
   {RegionalIndicatorEx} {RegionalIndicatorEx}+ | [^]
     { yybegin(YYINITIAL); /* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */ }
-}
\ No newline at end of file
+}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/ASCIITLD.jflex-macro b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/ASCIITLD.jflex-macro
new file mode 100644
index 0000000..5d78558
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/ASCIITLD.jflex-macro
@@ -0,0 +1,366 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+// Generated from IANA Root Zone Database <http://www.internic.net/zones/root.zone>
+// file version from Friday, December 6, 2013 4:34:10 AM UTC
+// generated on Friday, December 6, 2013 3:21:59 PM UTC
+// by org.apache.lucene.analysis.standard.GenerateJflexTLDMacros
+
+ASCIITLD = "." (
+	  [aA][cC]
+	| [aA][dD]
+	| [aA][eE]
+	| [aA][eE][rR][oO]
+	| [aA][fF]
+	| [aA][gG]
+	| [aA][iI]
+	| [aA][lL]
+	| [aA][mM]
+	| [aA][nN]
+	| [aA][oO]
+	| [aA][qQ]
+	| [aA][rR]
+	| [aA][rR][pP][aA]
+	| [aA][sS]
+	| [aA][sS][iI][aA]
+	| [aA][tT]
+	| [aA][uU]
+	| [aA][wW]
+	| [aA][xX]
+	| [aA][zZ]
+	| [bB][aA]
+	| [bB][bB]
+	| [bB][dD]
+	| [bB][eE]
+	| [bB][fF]
+	| [bB][gG]
+	| [bB][hH]
+	| [bB][iI]
+	| [bB][iI][kK][eE]
+	| [bB][iI][zZ]
+	| [bB][jJ]
+	| [bB][mM]
+	| [bB][nN]
+	| [bB][oO]
+	| [bB][rR]
+	| [bB][sS]
+	| [bB][tT]
+	| [bB][vV]
+	| [bB][wW]
+	| [bB][yY]
+	| [bB][zZ]
+	| [cC][aA]
+	| [cC][aA][mM][eE][rR][aA]
+	| [cC][aA][tT]
+	| [cC][cC]
+	| [cC][dD]
+	| [cC][fF]
+	| [cC][gG]
+	| [cC][hH]
+	| [cC][iI]
+	| [cC][kK]
+	| [cC][lL]
+	| [cC][lL][oO][tT][hH][iI][nN][gG]
+	| [cC][mM]
+	| [cC][nN]
+	| [cC][oO]
+	| [cC][oO][mM]
+	| [cC][oO][nN][sS][tT][rR][uU][cC][tT][iI][oO][nN]
+	| [cC][oO][nN][tT][rR][aA][cC][tT][oO][rR][sS]
+	| [cC][oO][oO][pP]
+	| [cC][rR]
+	| [cC][uU]
+	| [cC][vV]
+	| [cC][wW]
+	| [cC][xX]
+	| [cC][yY]
+	| [cC][zZ]
+	| [dD][eE]
+	| [dD][iI][aA][mM][oO][nN][dD][sS]
+	| [dD][iI][rR][eE][cC][tT][oO][rR][yY]
+	| [dD][jJ]
+	| [dD][kK]
+	| [dD][mM]
+	| [dD][oO]
+	| [dD][zZ]
+	| [eE][cC]
+	| [eE][dD][uU]
+	| [eE][eE]
+	| [eE][gG]
+	| [eE][nN][tT][eE][rR][pP][rR][iI][sS][eE][sS]
+	| [eE][qQ][uU][iI][pP][mM][eE][nN][tT]
+	| [eE][rR]
+	| [eE][sS]
+	| [eE][sS][tT][aA][tT][eE]
+	| [eE][tT]
+	| [eE][uU]
+	| [fF][iI]
+	| [fF][jJ]
+	| [fF][kK]
+	| [fF][mM]
+	| [fF][oO]
+	| [fF][rR]
+	| [gG][aA]
+	| [gG][aA][lL][lL][eE][rR][yY]
+	| [gG][bB]
+	| [gG][dD]
+	| [gG][eE]
+	| [gG][fF]
+	| [gG][gG]
+	| [gG][hH]
+	| [gG][iI]
+	| [gG][lL]
+	| [gG][mM]
+	| [gG][nN]
+	| [gG][oO][vV]
+	| [gG][pP]
+	| [gG][qQ]
+	| [gG][rR]
+	| [gG][rR][aA][pP][hH][iI][cC][sS]
+	| [gG][sS]
+	| [gG][tT]
+	| [gG][uU]
+	| [gG][uU][rR][uU]
+	| [gG][wW]
+	| [gG][yY]
+	| [hH][kK]
+	| [hH][mM]
+	| [hH][nN]
+	| [hH][oO][lL][dD][iI][nN][gG][sS]
+	| [hH][rR]
+	| [hH][tT]
+	| [hH][uU]
+	| [iI][dD]
+	| [iI][eE]
+	| [iI][lL]
+	| [iI][mM]
+	| [iI][nN]
+	| [iI][nN][fF][oO]
+	| [iI][nN][tT]
+	| [iI][oO]
+	| [iI][qQ]
+	| [iI][rR]
+	| [iI][sS]
+	| [iI][tT]
+	| [jJ][eE]
+	| [jJ][mM]
+	| [jJ][oO]
+	| [jJ][oO][bB][sS]
+	| [jJ][pP]
+	| [kK][eE]
+	| [kK][gG]
+	| [kK][hH]
+	| [kK][iI]
+	| [kK][iI][tT][cC][hH][eE][nN]
+	| [kK][mM]
+	| [kK][nN]
+	| [kK][pP]
+	| [kK][rR]
+	| [kK][wW]
+	| [kK][yY]
+	| [kK][zZ]
+	| [lL][aA]
+	| [lL][aA][nN][dD]
+	| [lL][bB]
+	| [lL][cC]
+	| [lL][iI]
+	| [lL][iI][gG][hH][tT][iI][nN][gG]
+	| [lL][kK]
+	| [lL][rR]
+	| [lL][sS]
+	| [lL][tT]
+	| [lL][uU]
+	| [lL][vV]
+	| [lL][yY]
+	| [mM][aA]
+	| [mM][cC]
+	| [mM][dD]
+	| [mM][eE]
+	| [mM][eE][nN][uU]
+	| [mM][gG]
+	| [mM][hH]
+	| [mM][iI][lL]
+	| [mM][kK]
+	| [mM][lL]
+	| [mM][mM]
+	| [mM][nN]
+	| [mM][oO]
+	| [mM][oO][bB][iI]
+	| [mM][pP]
+	| [mM][qQ]
+	| [mM][rR]
+	| [mM][sS]
+	| [mM][tT]
+	| [mM][uU]
+	| [mM][uU][sS][eE][uU][mM]
+	| [mM][vV]
+	| [mM][wW]
+	| [mM][xX]
+	| [mM][yY]
+	| [mM][zZ]
+	| [nN][aA]
+	| [nN][aA][mM][eE]
+	| [nN][cC]
+	| [nN][eE]
+	| [nN][eE][tT]
+	| [nN][fF]
+	| [nN][gG]
+	| [nN][iI]
+	| [nN][lL]
+	| [nN][oO]
+	| [nN][pP]
+	| [nN][rR]
+	| [nN][uU]
+	| [nN][zZ]
+	| [oO][mM]
+	| [oO][rR][gG]
+	| [pP][aA]
+	| [pP][eE]
+	| [pP][fF]
+	| [pP][gG]
+	| [pP][hH]
+	| [pP][hH][oO][tT][oO][gG][rR][aA][pP][hH][yY]
+	| [pP][kK]
+	| [pP][lL]
+	| [pP][lL][uU][mM][bB][iI][nN][gG]
+	| [pP][mM]
+	| [pP][nN]
+	| [pP][oO][sS][tT]
+	| [pP][rR]
+	| [pP][rR][oO]
+	| [pP][sS]
+	| [pP][tT]
+	| [pP][wW]
+	| [pP][yY]
+	| [qQ][aA]
+	| [rR][eE]
+	| [rR][oO]
+	| [rR][sS]
+	| [rR][uU]
+	| [rR][wW]
+	| [sS][aA]
+	| [sS][bB]
+	| [sS][cC]
+	| [sS][dD]
+	| [sS][eE]
+	| [sS][eE][xX][yY]
+	| [sS][gG]
+	| [sS][hH]
+	| [sS][iI]
+	| [sS][iI][nN][gG][lL][eE][sS]
+	| [sS][jJ]
+	| [sS][kK]
+	| [sS][lL]
+	| [sS][mM]
+	| [sS][nN]
+	| [sS][oO]
+	| [sS][rR]
+	| [sS][tT]
+	| [sS][uU]
+	| [sS][vV]
+	| [sS][xX]
+	| [sS][yY]
+	| [sS][zZ]
+	| [tT][aA][tT][tT][oO][oO]
+	| [tT][cC]
+	| [tT][dD]
+	| [tT][eE][cC][hH][nN][oO][lL][oO][gG][yY]
+	| [tT][eE][lL]
+	| [tT][fF]
+	| [tT][gG]
+	| [tT][hH]
+	| [tT][iI][pP][sS]
+	| [tT][jJ]
+	| [tT][kK]
+	| [tT][lL]
+	| [tT][mM]
+	| [tT][nN]
+	| [tT][oO]
+	| [tT][oO][dD][aA][yY]
+	| [tT][pP]
+	| [tT][rR]
+	| [tT][rR][aA][vV][eE][lL]
+	| [tT][tT]
+	| [tT][vV]
+	| [tT][wW]
+	| [tT][zZ]
+	| [uU][aA]
+	| [uU][gG]
+	| [uU][kK]
+	| [uU][nN][oO]
+	| [uU][sS]
+	| [uU][yY]
+	| [uU][zZ]
+	| [vV][aA]
+	| [vV][cC]
+	| [vV][eE]
+	| [vV][eE][nN][tT][uU][rR][eE][sS]
+	| [vV][gG]
+	| [vV][iI]
+	| [vV][nN]
+	| [vV][oO][yY][aA][gG][eE]
+	| [vV][uU]
+	| [wW][fF]
+	| [wW][sS]
+	| [xX][nN]--3[eE]0[bB]707[eE]
+	| [xX][nN]--45[bB][rR][jJ]9[cC]
+	| [xX][nN]--80[aA][oO]21[aA]
+	| [xX][nN]--80[aA][sS][eE][hH][dD][bB]
+	| [xX][nN]--80[aA][sS][wW][gG]
+	| [xX][nN]--90[aA]3[aA][cC]
+	| [xX][nN]--[cC][lL][cC][hH][cC]0[eE][aA]0[bB]2[gG]2[aA]9[gG][cC][dD]
+	| [xX][nN]--[fF][iI][qQ][sS]8[sS]
+	| [xX][nN]--[fF][iI][qQ][zZ]9[sS]
+	| [xX][nN]--[fF][pP][cC][rR][jJ]9[cC]3[dD]
+	| [xX][nN]--[fF][zZ][cC]2[cC]9[eE]2[cC]
+	| [xX][nN]--[gG][eE][cC][rR][jJ]9[cC]
+	| [xX][nN]--[hH]2[bB][rR][jJ]9[cC]
+	| [xX][nN]--[jJ]1[aA][mM][hH]
+	| [xX][nN]--[jJ]6[wW]193[gG]
+	| [xX][nN]--[kK][pP][rR][wW]13[dD]
+	| [xX][nN]--[kK][pP][rR][yY]57[dD]
+	| [xX][nN]--[lL]1[aA][cC][cC]
+	| [xX][nN]--[lL][gG][bB][bB][aA][tT]1[aA][dD]8[jJ]
+	| [xX][nN]--[mM][gG][bB]9[aA][wW][bB][fF]
+	| [xX][nN]--[mM][gG][bB][aA]3[aA]4[fF]16[aA]
+	| [xX][nN]--[mM][gG][bB][aA][aA][mM]7[aA]8[hH]
+	| [xX][nN]--[mM][gG][bB][aA][yY][hH]7[gG][pP][aA]
+	| [xX][nN]--[mM][gG][bB][bB][hH]1[aA]71[eE]
+	| [xX][nN]--[mM][gG][bB][cC]0[aA]9[aA][zZ][cC][gG]
+	| [xX][nN]--[mM][gG][bB][eE][rR][pP]4[aA]5[dD]4[aA][rR]
+	| [xX][nN]--[mM][gG][bB][xX]4[cC][dD]0[aA][bB]
+	| [xX][nN]--[nN][gG][bB][cC]5[aA][zZ][dD]
+	| [xX][nN]--[oO]3[cC][wW]4[hH]
+	| [xX][nN]--[oO][gG][bB][pP][fF]8[fF][lL]
+	| [xX][nN]--[pP]1[aA][iI]
+	| [xX][nN]--[pP][gG][bB][sS]0[dD][hH]
+	| [xX][nN]--[qQ]9[jJ][yY][bB]4[cC]
+	| [xX][nN]--[sS]9[bB][rR][jJ]9[cC]
+	| [xX][nN]--[uU][nN][uU][pP]4[yY]
+	| [xX][nN]--[wW][gG][bB][hH]1[cC]
+	| [xX][nN]--[wW][gG][bB][lL]6[aA]
+	| [xX][nN]--[xX][kK][cC]2[aA][lL]3[hH][yY][eE]2[aA]
+	| [xX][nN]--[xX][kK][cC]2[dD][lL]3[aA]5[eE][eE]0[hH]
+	| [xX][nN]--[yY][fF][rR][oO]4[iI]67[oO]
+	| [xX][nN]--[yY][gG][bB][iI]2[aA][mM][mM][xX]
+	| [xX][xX][xX]
+	| [yY][eE]
+	| [yY][tT]
+	| [zZ][aA]
+	| [zZ][mM]
+	| [zZ][wW]
+	) "."?   // Accept trailing root (empty) domain
+
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/ClassicTokenizer50.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/ClassicTokenizer50.java
new file mode 100644
index 0000000..92a9f4d
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/ClassicTokenizer50.java
@@ -0,0 +1,186 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.analysis.standard.std50;
+
+import java.io.IOException;
+
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.standard.ClassicTokenizer;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.util.AttributeFactory;
+
+/** A grammar-based tokenizer constructed with JFlex
+ *
+ * <p> This should be a good tokenizer for most European-language documents:
+ *
+ * <ul>
+ *   <li>Splits words at punctuation characters, removing punctuation. However, a 
+ *     dot that's not followed by whitespace is considered part of a token.
+ *   <li>Splits words at hyphens, unless there's a number in the token, in which case
+ *     the whole token is interpreted as a product number and is not split.
+ *   <li>Recognizes email addresses and internet hostnames as one token.
+ * </ul>
+ *
+ * <p>Many applications have specific tokenizer needs.  If this tokenizer does
+ * not suit your application, please consider copying this source code
+ * directory to your project and maintaining your own grammar-based tokenizer.
+ *
+ * ClassicTokenizer was named StandardTokenizer in Lucene versions prior to 3.1.
+ * As of 3.1, {@link StandardTokenizer} implements Unicode text segmentation,
+ * as specified by UAX#29.
+ */
+
+public final class ClassicTokenizer50 extends Tokenizer {
+  /** A private instance of the JFlex-constructed scanner */
+  private ClassicTokenizerImpl50 scanner;
+
+  public static final int ALPHANUM          = 0;
+  public static final int APOSTROPHE        = 1;
+  public static final int ACRONYM           = 2;
+  public static final int COMPANY           = 3;
+  public static final int EMAIL             = 4;
+  public static final int HOST              = 5;
+  public static final int NUM               = 6;
+  public static final int CJ                = 7;
+
+  public static final int ACRONYM_DEP       = 8;
+
+  /** String token types that correspond to token type int constants */
+  public static final String [] TOKEN_TYPES = new String [] {
+    "<ALPHANUM>",
+    "<APOSTROPHE>",
+    "<ACRONYM>",
+    "<COMPANY>",
+    "<EMAIL>",
+    "<HOST>",
+    "<NUM>",
+    "<CJ>",
+    "<ACRONYM_DEP>"
+  };
+  
+  private int skippedPositions;
+
+  private int maxTokenLength = StandardAnalyzer.DEFAULT_MAX_TOKEN_LENGTH;
+
+  /** Set the max allowed token length.  Any token longer
+   *  than this is skipped. */
+  public void setMaxTokenLength(int length) {
+    if (length < 1) {
+      throw new IllegalArgumentException("maxTokenLength must be greater than zero");
+    }
+    this.maxTokenLength = length;
+  }
+
+  /** @see #setMaxTokenLength */
+  public int getMaxTokenLength() {
+    return maxTokenLength;
+  }
+
+  /**
+   * Creates a new instance of the {@link ClassicTokenizer}.  Attaches
+   * the <code>input</code> to the newly created JFlex scanner.
+   *
+   * See http://issues.apache.org/jira/browse/LUCENE-1068
+   */
+  public ClassicTokenizer50() {
+    init();
+  }
+
+  /**
+   * Creates a new ClassicTokenizer with a given {@link org.apache.lucene.util.AttributeFactory} 
+   */
+  public ClassicTokenizer50(AttributeFactory factory) {
+    super(factory);
+    init();
+  }
+
+  private void init() {
+    this.scanner = new ClassicTokenizerImpl50(input);
+  }
+
+  // this tokenizer generates three attributes:
+  // term offset, positionIncrement and type
+  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+  private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
+  private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
+  private final TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
+
+  /*
+   * (non-Javadoc)
+   *
+   * @see org.apache.lucene.analysis.TokenStream#next()
+   */
+  @Override
+  public final boolean incrementToken() throws IOException {
+    clearAttributes();
+    skippedPositions = 0;
+
+    while(true) {
+      int tokenType = scanner.getNextToken();
+
+      if (tokenType == ClassicTokenizerImpl50.YYEOF) {
+        return false;
+      }
+
+      if (scanner.yylength() <= maxTokenLength) {
+        posIncrAtt.setPositionIncrement(skippedPositions+1);
+        scanner.getText(termAtt);
+        final int start = scanner.yychar();
+        offsetAtt.setOffset(correctOffset(start), correctOffset(start+termAtt.length()));
+
+        if (tokenType == ClassicTokenizer.ACRONYM_DEP) {
+          typeAtt.setType(ClassicTokenizer.TOKEN_TYPES[ClassicTokenizer.HOST]);
+          termAtt.setLength(termAtt.length() - 1); // remove extra '.'
+        } else {
+          typeAtt.setType(ClassicTokenizer.TOKEN_TYPES[tokenType]);
+        }
+        return true;
+      } else
+        // When we skip a too-long term, we still increment the
+        // position increment
+        skippedPositions++;
+    }
+  }
+  
+  @Override
+  public final void end() throws IOException {
+    super.end();
+    // set final offset
+    int finalOffset = correctOffset(scanner.yychar() + scanner.yylength());
+    offsetAtt.setOffset(finalOffset, finalOffset);
+    // adjust any skipped tokens
+    posIncrAtt.setPositionIncrement(posIncrAtt.getPositionIncrement()+skippedPositions);
+  }
+  
+  @Override
+  public void close() throws IOException {
+    super.close();
+    scanner.yyreset(input);
+  }
+
+  @Override
+  public void reset() throws IOException {
+    super.reset();
+    scanner.yyreset(input);
+    skippedPositions = 0;
+  }
+}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/ClassicTokenizerImpl50.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/ClassicTokenizerImpl50.jflex
new file mode 100644
index 0000000..8779664
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/ClassicTokenizerImpl50.jflex
@@ -0,0 +1,128 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.analysis.standard.std50;
+
+import java.io.Reader;
+import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+
+/**
+ * This class implements the classic lucene StandardTokenizer up until 3.0 
+ */
+@SuppressWarnings("fallthrough")
+%%
+
+%class ClassicTokenizerImpl50
+%unicode 3.0
+%integer
+%function getNextToken
+%pack
+%char
+%buffer 4096
+
+%{
+
+public static final int ALPHANUM          = StandardTokenizer.ALPHANUM;
+public static final int APOSTROPHE        = StandardTokenizer.APOSTROPHE;
+public static final int ACRONYM           = StandardTokenizer.ACRONYM;
+public static final int COMPANY           = StandardTokenizer.COMPANY;
+public static final int EMAIL             = StandardTokenizer.EMAIL;
+public static final int HOST              = StandardTokenizer.HOST;
+public static final int NUM               = StandardTokenizer.NUM;
+public static final int CJ                = StandardTokenizer.CJ;
+public static final int ACRONYM_DEP       = StandardTokenizer.ACRONYM_DEP;
+
+public static final String [] TOKEN_TYPES = StandardTokenizer.TOKEN_TYPES;
+
+public final int yychar()
+{
+    return yychar;
+}
+
+/**
+ * Fills CharTermAttribute with the current token text.
+ */
+public final void getText(CharTermAttribute t) {
+  t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
+}
+
+   public final void setBufferSize(int numChars) {
+     throw new UnsupportedOperationException();
+   }
+%}
+
+THAI       = [\u0E00-\u0E59]
+
+// basic word: a sequence of digits & letters (includes Thai to enable ThaiAnalyzer to function)
+ALPHANUM   = ({LETTER}|{THAI}|[:digit:])+
+
+// internal apostrophes: O'Reilly, you're, O'Reilly's
+// use a post-filter to remove possessives
+APOSTROPHE =  {ALPHA} ("'" {ALPHA})+
+
+// acronyms: U.S.A., I.B.M., etc.
+// use a post-filter to remove dots
+ACRONYM    =  {LETTER} "." ({LETTER} ".")+
+
+ACRONYM_DEP  = {ALPHANUM} "." ({ALPHANUM} ".")+
+
+// company names like AT&T and Excite@Home.
+COMPANY    =  {ALPHA} ("&"|"@") {ALPHA}
+
+// email addresses
+EMAIL      =  {ALPHANUM} (("."|"-"|"_") {ALPHANUM})* "@" {ALPHANUM} (("."|"-") {ALPHANUM})+
+
+// hostname
+HOST       =  {ALPHANUM} ((".") {ALPHANUM})+
+
+// floating point, serial, model numbers, ip addresses, etc.
+// every other segment must have at least one digit
+NUM        = ({ALPHANUM} {P} {HAS_DIGIT}
+           | {HAS_DIGIT} {P} {ALPHANUM}
+           | {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+
+           | {HAS_DIGIT} ({P} {ALPHANUM} {P} {HAS_DIGIT})+
+           | {ALPHANUM} {P} {HAS_DIGIT} ({P} {ALPHANUM} {P} {HAS_DIGIT})+
+           | {HAS_DIGIT} {P} {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+)
+
+// punctuation
+P           = ("_"|"-"|"/"|"."|",")
+
+// at least one digit
+HAS_DIGIT  = ({LETTER}|[:digit:])* [:digit:] ({LETTER}|[:digit:])*
+
+ALPHA      = ({LETTER})+
+
+// From the JFlex manual: "the expression that matches everything of <a> not matched by <b> is !(!<a>|<b>)"
+LETTER     = !(![:letter:]|{CJ})
+
+// Chinese and Japanese (but NOT Korean, which is included in [:letter:])
+CJ         = [\u3100-\u312f\u3040-\u309F\u30A0-\u30FF\u31F0-\u31FF\u3300-\u337f\u3400-\u4dbf\u4e00-\u9fff\uf900-\ufaff\uff65-\uff9f]
+
+%%
+
+{ALPHANUM}                                                     { return ALPHANUM; }
+{APOSTROPHE}                                                   { return APOSTROPHE; }
+{ACRONYM}                                                      { return ACRONYM; }
+{COMPANY}                                                      { return COMPANY; }
+{EMAIL}                                                        { return EMAIL; }
+{HOST}                                                         { return HOST; }
+{NUM}                                                          { return NUM; }
+{CJ}                                                           { return CJ; }
+{ACRONYM_DEP}                                                  { return ACRONYM_DEP; }
+
+/** Ignore the rest */
+[^]                                                            { /* Break so we don't hit fall-through warning: */ break;/* ignore */ }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/StandardTokenizer50.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/StandardTokenizer50.java
new file mode 100644
index 0000000..e1131c1
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/StandardTokenizer50.java
@@ -0,0 +1,204 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.analysis.standard.std50;
+
+import java.io.IOException;
+
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.standard.StandardTokenizerImpl;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.util.AttributeFactory;
+
+/** A grammar-based tokenizer constructed with JFlex.
+ * <p>
+ * This class implements the Word Break rules from the
+ * Unicode Text Segmentation algorithm, as specified in 
+ * <a href="http://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>.
+ * <p>Many applications have specific tokenizer needs.  If this tokenizer does
+ * not suit your application, please consider copying this source code
+ * directory to your project and maintaining your own grammar-based tokenizer.
+ */
+
+public final class StandardTokenizer50 extends Tokenizer {
+  /** A private instance of the JFlex-constructed scanner */
+  private StandardTokenizerImpl50 scanner;
+
+  // TODO: how can we remove these old types?!
+  public static final int ALPHANUM          = 0;
+  /** @deprecated (3.1) */
+  @Deprecated
+  public static final int APOSTROPHE        = 1;
+  /** @deprecated (3.1) */
+  @Deprecated
+  public static final int ACRONYM           = 2;
+  /** @deprecated (3.1) */
+  @Deprecated
+  public static final int COMPANY           = 3;
+  public static final int EMAIL             = 4;
+  /** @deprecated (3.1) */
+  @Deprecated
+  public static final int HOST              = 5;
+  public static final int NUM               = 6;
+  /** @deprecated (3.1) */
+  @Deprecated
+  public static final int CJ                = 7;
+
+  /** @deprecated (3.1) */
+  @Deprecated
+  public static final int ACRONYM_DEP       = 8;
+
+  public static final int SOUTHEAST_ASIAN = 9;
+  public static final int IDEOGRAPHIC = 10;
+  public static final int HIRAGANA = 11;
+  public static final int KATAKANA = 12;
+  public static final int HANGUL = 13;
+  
+  /** String token types that correspond to token type int constants */
+  public static final String [] TOKEN_TYPES = new String [] {
+    "<ALPHANUM>",
+    "<APOSTROPHE>",
+    "<ACRONYM>",
+    "<COMPANY>",
+    "<EMAIL>",
+    "<HOST>",
+    "<NUM>",
+    "<CJ>",
+    "<ACRONYM_DEP>",
+    "<SOUTHEAST_ASIAN>",
+    "<IDEOGRAPHIC>",
+    "<HIRAGANA>",
+    "<KATAKANA>",
+    "<HANGUL>"
+  };
+  
+  public static final int MAX_TOKEN_LENGTH_LIMIT = 1024 * 1024;
+  
+  private int skippedPositions;
+
+  private int maxTokenLength = StandardAnalyzer.DEFAULT_MAX_TOKEN_LENGTH;
+
+  /**
+   * Set the max allowed token length.  No tokens longer than this are emitted.
+   * 
+   * @throws IllegalArgumentException if the given length is outside of the
+   *  range [1, {@value #MAX_TOKEN_LENGTH_LIMIT}].
+   */ 
+  public void setMaxTokenLength(int length) {
+    if (length < 1) {
+      throw new IllegalArgumentException("maxTokenLength must be greater than zero");
+    } else if (length > MAX_TOKEN_LENGTH_LIMIT) {
+      throw new IllegalArgumentException("maxTokenLength may not exceed " + MAX_TOKEN_LENGTH_LIMIT);
+    }
+    if (length != maxTokenLength) {
+      maxTokenLength = length;
+      scanner.setBufferSize(length);
+    }
+  }
+
+  /** @see #setMaxTokenLength */
+  public int getMaxTokenLength() {
+    return maxTokenLength;
+  }
+
+  /**
+   * Creates a new instance of the {@link org.apache.lucene.analysis.standard.StandardTokenizer}.  Attaches
+   * the <code>input</code> to the newly created JFlex scanner.
+
+   * See http://issues.apache.org/jira/browse/LUCENE-1068
+   */
+  public StandardTokenizer50() {
+    init();
+  }
+
+  /**
+   * Creates a new StandardTokenizer with a given {@link org.apache.lucene.util.AttributeFactory} 
+   */
+  public StandardTokenizer50(AttributeFactory factory) {
+    super(factory);
+    init();
+  }
+
+  private void init() {
+    this.scanner = new StandardTokenizerImpl50(input);
+  }
+
+  // this tokenizer generates three attributes:
+  // term offset, positionIncrement and type
+  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+  private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
+  private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
+  private final TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
+
+  /*
+   * (non-Javadoc)
+   *
+   * @see org.apache.lucene.analysis.TokenStream#next()
+   */
+  @Override
+  public final boolean incrementToken() throws IOException {
+    clearAttributes();
+    skippedPositions = 0;
+
+    while(true) {
+      int tokenType = scanner.getNextToken();
+
+      if (tokenType == StandardTokenizerImpl.YYEOF) {
+        return false;
+      }
+
+      if (scanner.yylength() <= maxTokenLength) {
+        posIncrAtt.setPositionIncrement(skippedPositions+1);
+        scanner.getText(termAtt);
+        final int start = scanner.yychar();
+        offsetAtt.setOffset(correctOffset(start), correctOffset(start+termAtt.length()));
+        typeAtt.setType(StandardTokenizer.TOKEN_TYPES[tokenType]);
+        return true;
+      } else
+        // When we skip a too-long term, we still increment the
+        // position increment
+        skippedPositions++;
+    }
+  }
+  
+  @Override
+  public final void end() throws IOException {
+    super.end();
+    // set final offset
+    int finalOffset = correctOffset(scanner.yychar() + scanner.yylength());
+    offsetAtt.setOffset(finalOffset, finalOffset);
+    // adjust any skipped tokens
+    posIncrAtt.setPositionIncrement(posIncrAtt.getPositionIncrement()+skippedPositions);
+  }
+
+  @Override
+  public void close() throws IOException {
+    super.close();
+    scanner.yyreset(input);
+  }
+
+  @Override
+  public void reset() throws IOException {
+    super.reset();
+    scanner.yyreset(input);
+    skippedPositions = 0;
+  }
+}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/StandardTokenizerImpl50.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/StandardTokenizerImpl50.jflex
new file mode 100644
index 0000000..21332aa
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/StandardTokenizerImpl50.jflex
@@ -0,0 +1,202 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.analysis.standard.std50;
+
+import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+
+/**
+ * This class implements Word Break rules from the Unicode Text Segmentation 
+ * algorithm, as specified in 
+ * <a href="http://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>. 
+ * <p>
+ * Tokens produced are of the following types:
+ * <ul>
+ *   <li>&lt;ALPHANUM&gt;: A sequence of alphabetic and numeric characters</li>
+ *   <li>&lt;NUM&gt;: A number</li>
+ *   <li>&lt;SOUTHEAST_ASIAN&gt;: A sequence of characters from South and Southeast
+ *       Asian languages, including Thai, Lao, Myanmar, and Khmer</li>
+ *   <li>&lt;IDEOGRAPHIC&gt;: A single CJKV ideographic character</li>
+ *   <li>&lt;HIRAGANA&gt;: A single hiragana character</li>
+ *   <li>&lt;KATAKANA&gt;: A sequence of katakana characters</li>
+ *   <li>&lt;HANGUL&gt;: A sequence of Hangul characters</li>
+ * </ul>
+ */
+@SuppressWarnings("fallthrough")
+%%
+
+%unicode 6.3
+%integer
+%final
+%public
+%class StandardTokenizerImpl50
+%function getNextToken
+%char
+%buffer 255
+
+// UAX#29 WB4. X (Extend | Format)* --> X
+//
+HangulEx            = [\p{Script:Hangul}&&[\p{WB:ALetter}\p{WB:Hebrew_Letter}]] [\p{WB:Format}\p{WB:Extend}]*
+HebrewOrALetterEx   = [\p{WB:HebrewLetter}\p{WB:ALetter}]                       [\p{WB:Format}\p{WB:Extend}]*
+NumericEx           = [\p{WB:Numeric}[\p{Blk:HalfAndFullForms}&&\p{Nd}]]        [\p{WB:Format}\p{WB:Extend}]*
+KatakanaEx          = \p{WB:Katakana}                                           [\p{WB:Format}\p{WB:Extend}]* 
+MidLetterEx         = [\p{WB:MidLetter}\p{WB:MidNumLet}\p{WB:SingleQuote}]      [\p{WB:Format}\p{WB:Extend}]* 
+MidNumericEx        = [\p{WB:MidNum}\p{WB:MidNumLet}\p{WB:SingleQuote}]         [\p{WB:Format}\p{WB:Extend}]*
+ExtendNumLetEx      = \p{WB:ExtendNumLet}                                       [\p{WB:Format}\p{WB:Extend}]*
+HanEx               = \p{Script:Han}                                            [\p{WB:Format}\p{WB:Extend}]*
+HiraganaEx          = \p{Script:Hiragana}                                       [\p{WB:Format}\p{WB:Extend}]*
+SingleQuoteEx       = \p{WB:Single_Quote}                                       [\p{WB:Format}\p{WB:Extend}]*
+DoubleQuoteEx       = \p{WB:Double_Quote}                                       [\p{WB:Format}\p{WB:Extend}]*
+HebrewLetterEx      = \p{WB:Hebrew_Letter}                                      [\p{WB:Format}\p{WB:Extend}]*
+RegionalIndicatorEx = \p{WB:RegionalIndicator}                                  [\p{WB:Format}\p{WB:Extend}]*
+ComplexContextEx    = \p{LB:Complex_Context}                                    [\p{WB:Format}\p{WB:Extend}]*
+
+%{
+  /** Alphanumeric sequences */
+  public static final int WORD_TYPE = StandardTokenizer.ALPHANUM;
+  
+  /** Numbers */
+  public static final int NUMERIC_TYPE = StandardTokenizer.NUM;
+  
+  /**
+   * Chars in class \p{Line_Break = Complex_Context} are from South East Asian
+   * scripts (Thai, Lao, Myanmar, Khmer, etc.).  Sequences of these are kept 
+   * together as as a single token rather than broken up, because the logic
+   * required to break them at word boundaries is too complex for UAX#29.
+   * <p>
+   * See Unicode Line Breaking Algorithm: http://www.unicode.org/reports/tr14/#SA
+   */
+  public static final int SOUTH_EAST_ASIAN_TYPE = StandardTokenizer.SOUTHEAST_ASIAN;
+  
+  public static final int IDEOGRAPHIC_TYPE = StandardTokenizer.IDEOGRAPHIC;
+  
+  public static final int HIRAGANA_TYPE = StandardTokenizer.HIRAGANA;
+  
+  public static final int KATAKANA_TYPE = StandardTokenizer.KATAKANA;
+  
+  public static final int HANGUL_TYPE = StandardTokenizer.HANGUL;
+
+  public final int yychar()
+  {
+    return yychar;
+  }
+
+  /**
+   * Fills CharTermAttribute with the current token text.
+   */
+  public final void getText(CharTermAttribute t) {
+    t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
+  }
+  
+  /**
+   * Sets the scanner buffer size in chars
+   */
+   public final void setBufferSize(int numChars) {
+     ZZ_BUFFERSIZE = numChars;
+     char[] newZzBuffer = new char[ZZ_BUFFERSIZE];
+     System.arraycopy(zzBuffer, 0, newZzBuffer, 0, Math.min(zzBuffer.length, ZZ_BUFFERSIZE));
+     zzBuffer = newZzBuffer;
+   }
+%}
+
+%%
+
+// UAX#29 WB1.   sot   
+//        WB2.        eot
+//
+<<EOF>> { return YYEOF; }
+
+// UAX#29 WB8.   Numeric  Numeric
+//        WB11.  Numeric (MidNum | MidNumLet | Single_Quote)  Numeric
+//        WB12.  Numeric  (MidNum | MidNumLet | Single_Quote) Numeric
+//        WB13a. (ALetter | Hebrew_Letter | Numeric | Katakana | ExtendNumLet)  ExtendNumLet
+//        WB13b. ExtendNumLet  (ALetter | Hebrew_Letter | Numeric | Katakana) 
+//
+{ExtendNumLetEx}* {NumericEx} ( ( {ExtendNumLetEx}* | {MidNumericEx} ) {NumericEx} )* {ExtendNumLetEx}* 
+  { return NUMERIC_TYPE; }
+
+// subset of the below for typing purposes only!
+{HangulEx}+
+  { return HANGUL_TYPE; }
+  
+{KatakanaEx}+
+  { return KATAKANA_TYPE; }
+
+// UAX#29 WB5.   (ALetter | Hebrew_Letter)  (ALetter | Hebrew_Letter)
+//        WB6.   (ALetter | Hebrew_Letter)  (MidLetter | MidNumLet | Single_Quote) (ALetter | Hebrew_Letter)
+//        WB7.   (ALetter | Hebrew_Letter) (MidLetter | MidNumLet | Single_Quote)  (ALetter | Hebrew_Letter)
+//        WB7a.  Hebrew_Letter  Single_Quote
+//        WB7b.  Hebrew_Letter  Double_Quote Hebrew_Letter
+//        WB7c.  Hebrew_Letter Double_Quote  Hebrew_Letter
+//        WB9.   (ALetter | Hebrew_Letter)  Numeric
+//        WB10.  Numeric  (ALetter | Hebrew_Letter)
+//        WB13.  Katakana  Katakana
+//        WB13a. (ALetter | Hebrew_Letter | Numeric | Katakana | ExtendNumLet)  ExtendNumLet
+//        WB13b. ExtendNumLet  (ALetter | Hebrew_Letter | Numeric | Katakana) 
+//
+{ExtendNumLetEx}*  ( {KatakanaEx}          ( {ExtendNumLetEx}*   {KatakanaEx}                           )*
+                   | ( {HebrewLetterEx}    ( {SingleQuoteEx}     | {DoubleQuoteEx}  {HebrewLetterEx}    )
+                     | {NumericEx}         ( ( {ExtendNumLetEx}* | {MidNumericEx} ) {NumericEx}         )*
+                     | {HebrewOrALetterEx} ( ( {ExtendNumLetEx}* | {MidLetterEx}  ) {HebrewOrALetterEx} )*
+                     )+
+                   )
+({ExtendNumLetEx}+ ( {KatakanaEx}          ( {ExtendNumLetEx}*   {KatakanaEx}                           )*
+                   | ( {HebrewLetterEx}    ( {SingleQuoteEx}     | {DoubleQuoteEx}  {HebrewLetterEx}    )
+                     | {NumericEx}         ( ( {ExtendNumLetEx}* | {MidNumericEx} ) {NumericEx}         )*
+                     | {HebrewOrALetterEx} ( ( {ExtendNumLetEx}* | {MidLetterEx}  ) {HebrewOrALetterEx} )*
+                     )+
+                   )
+)*
+{ExtendNumLetEx}* 
+  { return WORD_TYPE; }
+
+
+// From UAX #29:
+//
+//    [C]haracters with the Line_Break property values of Contingent_Break (CB), 
+//    Complex_Context (SA/South East Asian), and XX (Unknown) are assigned word 
+//    boundary property values based on criteria outside of the scope of this
+//    annex.  That means that satisfactory treatment of languages like Chinese
+//    or Thai requires special handling.
+// 
+// In Unicode 6.3, only one character has the \p{Line_Break = Contingent_Break}
+// property: U+FFFC (  ) OBJECT REPLACEMENT CHARACTER.
+//
+// In the ICU implementation of UAX#29, \p{Line_Break = Complex_Context}
+// character sequences (from South East Asian scripts like Thai, Myanmar, Khmer,
+// Lao, etc.) are kept together.  This grammar does the same below.
+//
+// See also the Unicode Line Breaking Algorithm:
+//
+//    http://www.unicode.org/reports/tr14/#SA
+//
+{ComplexContextEx}+ { return SOUTH_EAST_ASIAN_TYPE; }
+
+// UAX#29 WB14.  Any  Any
+//
+{HanEx} { return IDEOGRAPHIC_TYPE; }
+{HiraganaEx} { return HIRAGANA_TYPE; }
+
+
+// UAX#29 WB3.   CR  LF
+//        WB3a.  (Newline | CR | LF) 
+//        WB3b.   (Newline | CR | LF)
+//        WB13c. Regional_Indicator  Regional_Indicator
+//        WB14.  Any  Any
+//
+{RegionalIndicatorEx} {RegionalIndicatorEx}+ | [^]
+  { /* Break so we don't hit fall-through warning: */ break; /* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */ }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/UAX29URLEmailTokenizer50.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/UAX29URLEmailTokenizer50.java
new file mode 100644
index 0000000..5e1f7e0
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/UAX29URLEmailTokenizer50.java
@@ -0,0 +1,173 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.analysis.standard.std50;
+
+
+import java.io.IOException;
+
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.standard.UAX29URLEmailTokenizerImpl;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.util.AttributeFactory;
+
+/**
+ * This class implements Word Break rules from the Unicode Text Segmentation 
+ * algorithm, as specified in 
+ * <a href="http://unicode.org/reports/tr29/">Unicode Standard Annex #29</a> 
+ * URLs and email addresses are also tokenized according to the relevant RFCs.
+ * <p>
+ * Tokens produced are of the following types:
+ * <ul>
+ *   <li>&lt;ALPHANUM&gt;: A sequence of alphabetic and numeric characters</li>
+ *   <li>&lt;NUM&gt;: A number</li>
+ *   <li>&lt;URL&gt;: A URL</li>
+ *   <li>&lt;EMAIL&gt;: An email address</li>
+ *   <li>&lt;SOUTHEAST_ASIAN&gt;: A sequence of characters from South and Southeast
+ *       Asian languages, including Thai, Lao, Myanmar, and Khmer</li>
+ *   <li>&lt;IDEOGRAPHIC&gt;: A single CJKV ideographic character</li>
+ *   <li>&lt;HIRAGANA&gt;: A single hiragana character</li>
+ * </ul>
+ */
+@Deprecated
+public final class UAX29URLEmailTokenizer50 extends Tokenizer {
+  /** A private instance of the JFlex-constructed scanner */
+  private final UAX29URLEmailTokenizerImpl50 scanner;
+  
+  public static final int ALPHANUM          = 0;
+  public static final int NUM               = 1;
+  public static final int SOUTHEAST_ASIAN   = 2;
+  public static final int IDEOGRAPHIC       = 3;
+  public static final int HIRAGANA          = 4;
+  public static final int KATAKANA          = 5;
+  public static final int HANGUL            = 6;
+  public static final int URL               = 7;
+  public static final int EMAIL             = 8;
+
+  /** String token types that correspond to token type int constants */
+  public static final String [] TOKEN_TYPES = new String [] {
+    StandardTokenizer.TOKEN_TYPES[StandardTokenizer.ALPHANUM],
+    StandardTokenizer.TOKEN_TYPES[StandardTokenizer.NUM],
+    StandardTokenizer.TOKEN_TYPES[StandardTokenizer.SOUTHEAST_ASIAN],
+    StandardTokenizer.TOKEN_TYPES[StandardTokenizer.IDEOGRAPHIC],
+    StandardTokenizer.TOKEN_TYPES[StandardTokenizer.HIRAGANA],
+    StandardTokenizer.TOKEN_TYPES[StandardTokenizer.KATAKANA],
+    StandardTokenizer.TOKEN_TYPES[StandardTokenizer.HANGUL],
+    "<URL>",
+    "<EMAIL>",
+  };
+  
+  private int skippedPositions;
+
+  private int maxTokenLength = StandardAnalyzer.DEFAULT_MAX_TOKEN_LENGTH;
+
+  /** Set the max allowed token length.  Any token longer
+   *  than this is skipped. */
+  public void setMaxTokenLength(int length) {
+    if (length < 1) {
+      throw new IllegalArgumentException("maxTokenLength must be greater than zero");
+    }
+    this.maxTokenLength = length;
+    scanner.setBufferSize(Math.min(length, 1024 * 1024)); // limit buffer size to 1M chars
+  }
+
+  /** @see #setMaxTokenLength */
+  public int getMaxTokenLength() {
+    return maxTokenLength;
+  }
+
+  /**
+   * Creates a new instance of the UAX29URLEmailTokenizer.  Attaches
+   * the <code>input</code> to the newly created JFlex scanner.
+
+   */
+  public UAX29URLEmailTokenizer50() {
+    this.scanner = getScanner();
+  }
+
+  /**
+   * Creates a new UAX29URLEmailTokenizer with a given {@link AttributeFactory} 
+   */
+  public UAX29URLEmailTokenizer50(AttributeFactory factory) {
+    super(factory);
+    this.scanner = getScanner();
+  }
+
+  private UAX29URLEmailTokenizerImpl50 getScanner() {
+    return new UAX29URLEmailTokenizerImpl50(input);
+  }
+
+  // this tokenizer generates three attributes:
+  // term offset, positionIncrement and type
+  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+  private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
+  private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
+  private final TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
+
+  @Override
+  public final boolean incrementToken() throws IOException {
+    clearAttributes();
+    skippedPositions = 0;
+
+    while(true) {
+      int tokenType = scanner.getNextToken();
+
+      if (tokenType == UAX29URLEmailTokenizerImpl.YYEOF) {
+        return false;
+      }
+
+      if (scanner.yylength() <= maxTokenLength) {
+        posIncrAtt.setPositionIncrement(skippedPositions+1);
+        scanner.getText(termAtt);
+        final int start = scanner.yychar();
+        offsetAtt.setOffset(correctOffset(start), correctOffset(start+termAtt.length()));
+        typeAtt.setType(TOKEN_TYPES[tokenType]);
+        return true;
+      } else
+        // When we skip a too-long term, we still increment the
+        // position increment
+        skippedPositions++;
+    }
+  }
+  
+  @Override
+  public final void end() throws IOException {
+    super.end();
+    // set final offset
+    int finalOffset = correctOffset(scanner.yychar() + scanner.yylength());
+    offsetAtt.setOffset(finalOffset, finalOffset);
+    // adjust any skipped tokens
+    posIncrAtt.setPositionIncrement(posIncrAtt.getPositionIncrement()+skippedPositions);
+  }
+  
+  @Override
+  public void close() throws IOException {
+    super.close();
+    scanner.yyreset(input);
+  }
+
+  @Override
+  public void reset() throws IOException {
+    super.reset();
+    scanner.yyreset(input);
+    skippedPositions = 0;
+  }
+}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/UAX29URLEmailTokenizerImpl50.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/UAX29URLEmailTokenizerImpl50.jflex
new file mode 100644
index 0000000..ed42fcb
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/UAX29URLEmailTokenizerImpl50.jflex
@@ -0,0 +1,316 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.analysis.standard.std50;
+
+import org.apache.lucene.analysis.standard.UAX29URLEmailTokenizer;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+
+/**
+ * This class implements Word Break rules from the Unicode Text Segmentation 
+ * algorithm, as specified in 
+ * <a href="http://unicode.org/reports/tr29/">Unicode Standard Annex #29</a> 
+ * URLs and email addresses are also tokenized according to the relevant RFCs.
+ * <p>
+ * Tokens produced are of the following types:
+ * <ul>
+ *   <li>&lt;ALPHANUM&gt;: A sequence of alphabetic and numeric characters</li>
+ *   <li>&lt;NUM&gt;: A number</li>
+ *   <li>&lt;URL&gt;: A URL</li>
+ *   <li>&lt;EMAIL&gt;: An email address</li>
+ *   <li>&lt;SOUTHEAST_ASIAN&gt;: A sequence of characters from South and Southeast
+ *       Asian languages, including Thai, Lao, Myanmar, and Khmer</li>
+ *   <li>&lt;IDEOGRAPHIC&gt;: A single CJKV ideographic character</li>
+ *   <li>&lt;HIRAGANA&gt;: A single hiragana character</li>
+ *   <li>&lt;KATAKANA&gt;: A sequence of katakana characters</li>
+ *   <li>&lt;HANGUL&gt;: A sequence of Hangul characters</li>
+ * </ul>
+ */
+@SuppressWarnings("fallthrough")
+@Deprecated
+%%
+
+%unicode 6.3
+%integer
+%final
+%public
+%class UAX29URLEmailTokenizerImpl50
+%function getNextToken
+%char
+%xstate AVOID_BAD_URL
+%buffer 255
+
+// UAX#29 WB4. X (Extend | Format)* --> X
+//
+HangulEx            = [\p{Script:Hangul}&&[\p{WB:ALetter}\p{WB:Hebrew_Letter}]] [\p{WB:Format}\p{WB:Extend}]*
+HebrewOrALetterEx   = [\p{WB:HebrewLetter}\p{WB:ALetter}]                       [\p{WB:Format}\p{WB:Extend}]*
+NumericEx           = [\p{WB:Numeric}[\p{Blk:HalfAndFullForms}&&\p{Nd}]]        [\p{WB:Format}\p{WB:Extend}]*
+KatakanaEx          = \p{WB:Katakana}                                           [\p{WB:Format}\p{WB:Extend}]* 
+MidLetterEx         = [\p{WB:MidLetter}\p{WB:MidNumLet}\p{WB:SingleQuote}]      [\p{WB:Format}\p{WB:Extend}]* 
+MidNumericEx        = [\p{WB:MidNum}\p{WB:MidNumLet}\p{WB:SingleQuote}]         [\p{WB:Format}\p{WB:Extend}]*
+ExtendNumLetEx      = \p{WB:ExtendNumLet}                                       [\p{WB:Format}\p{WB:Extend}]*
+HanEx               = \p{Script:Han}                                            [\p{WB:Format}\p{WB:Extend}]*
+HiraganaEx          = \p{Script:Hiragana}                                       [\p{WB:Format}\p{WB:Extend}]*
+SingleQuoteEx       = \p{WB:Single_Quote}                                       [\p{WB:Format}\p{WB:Extend}]*
+DoubleQuoteEx       = \p{WB:Double_Quote}                                       [\p{WB:Format}\p{WB:Extend}]*
+HebrewLetterEx      = \p{WB:Hebrew_Letter}                                      [\p{WB:Format}\p{WB:Extend}]*
+RegionalIndicatorEx = \p{WB:RegionalIndicator}                                  [\p{WB:Format}\p{WB:Extend}]*
+ComplexContextEx    = \p{LB:Complex_Context}                                    [\p{WB:Format}\p{WB:Extend}]*
+
+// URL and E-mail syntax specifications:
+//
+//     RFC-952:  DOD INTERNET HOST TABLE SPECIFICATION
+//     RFC-1035: DOMAIN NAMES - IMPLEMENTATION AND SPECIFICATION
+//     RFC-1123: Requirements for Internet Hosts - Application and Support
+//     RFC-1738: Uniform Resource Locators (URL)
+//     RFC-3986: Uniform Resource Identifier (URI): Generic Syntax
+//     RFC-5234: Augmented BNF for Syntax Specifications: ABNF
+//     RFC-5321: Simple Mail Transfer Protocol
+//     RFC-5322: Internet Message Format
+
+%include ASCIITLD.jflex-macro
+
+DomainLabel = [A-Za-z0-9] ([-A-Za-z0-9]* [A-Za-z0-9])?
+DomainNameStrict = {DomainLabel} ("." {DomainLabel})* {ASCIITLD}
+DomainNameLoose  = {DomainLabel} ("." {DomainLabel})*
+
+IPv4DecimalOctet = "0"{0,2} [0-9] | "0"? [1-9][0-9] | "1" [0-9][0-9] | "2" ([0-4][0-9] | "5" [0-5])
+IPv4Address  = {IPv4DecimalOctet} ("." {IPv4DecimalOctet}){3} 
+IPv6Hex16Bit = [0-9A-Fa-f]{1,4}
+IPv6LeastSignificant32Bits = {IPv4Address} | ({IPv6Hex16Bit} ":" {IPv6Hex16Bit})
+IPv6Address =                                                  ({IPv6Hex16Bit} ":"){6} {IPv6LeastSignificant32Bits}
+            |                                             "::" ({IPv6Hex16Bit} ":"){5} {IPv6LeastSignificant32Bits}
+            |                            {IPv6Hex16Bit}?  "::" ({IPv6Hex16Bit} ":"){4} {IPv6LeastSignificant32Bits}
+            | (({IPv6Hex16Bit} ":"){0,1} {IPv6Hex16Bit})? "::" ({IPv6Hex16Bit} ":"){3} {IPv6LeastSignificant32Bits}
+            | (({IPv6Hex16Bit} ":"){0,2} {IPv6Hex16Bit})? "::" ({IPv6Hex16Bit} ":"){2} {IPv6LeastSignificant32Bits}
+            | (({IPv6Hex16Bit} ":"){0,3} {IPv6Hex16Bit})? "::"  {IPv6Hex16Bit} ":"     {IPv6LeastSignificant32Bits}
+            | (({IPv6Hex16Bit} ":"){0,4} {IPv6Hex16Bit})? "::"                         {IPv6LeastSignificant32Bits}
+            | (({IPv6Hex16Bit} ":"){0,5} {IPv6Hex16Bit})? "::"                         {IPv6Hex16Bit}
+            | (({IPv6Hex16Bit} ":"){0,6} {IPv6Hex16Bit})? "::"
+
+URIunreserved = [-._~A-Za-z0-9]
+URIpercentEncoded = "%" [0-9A-Fa-f]{2}
+URIsubDelims = [!$&'()*+,;=]
+URIloginSegment = ({URIunreserved} | {URIpercentEncoded} | {URIsubDelims})*
+URIlogin = {URIloginSegment} (":" {URIloginSegment})? "@"
+URIquery    = "?" ({URIunreserved} | {URIpercentEncoded} | {URIsubDelims} | [:@/?])*
+URIfragment = "#" ({URIunreserved} | {URIpercentEncoded} | {URIsubDelims} | [:@/?])*
+URIport = ":" [0-9]{1,5}
+URIhostStrict = ("[" {IPv6Address} "]") | {IPv4Address} | {DomainNameStrict}  
+URIhostLoose  = ("[" {IPv6Address} "]") | {IPv4Address} | {DomainNameLoose} 
+URIauthorityLoose  = {URIlogin}? {URIhostLoose}  {URIport}?
+
+HTTPsegment = ({URIunreserved} | {URIpercentEncoded} | [;:@&=])*
+HTTPpath = ("/" {HTTPsegment})+
+HTTPscheme = [hH][tT][tT][pP][sS]? "://"
+HTTPurlFull = {HTTPscheme} {URIlogin}? {URIhostLoose} {URIport}? {HTTPpath}? {URIquery}? {URIfragment}?
+URIportRequired = {URIport} {HTTPpath}? {URIquery}? {URIfragment}?
+HTTPpathRequired = {URIport}? {HTTPpath} {URIquery}? {URIfragment}?
+URIqueryRequired = {URIport}? {HTTPpath}? {URIquery} {URIfragment}?
+URIfragmentRequired = {URIport}? {HTTPpath}? {URIquery}? {URIfragment}
+// {HTTPurlNoScheme} excludes {URIlogin}, because it could otherwise accept e-mail addresses
+HTTPurlNoScheme = {URIhostStrict} ({URIportRequired} | {HTTPpathRequired} | {URIqueryRequired} | {URIfragmentRequired})
+HTTPurl = {HTTPurlFull} | {HTTPurlNoScheme}
+
+FTPorFILEsegment = ({URIunreserved} | {URIpercentEncoded} | [?:@&=])*
+FTPorFILEpath = "/" {FTPorFILEsegment} ("/" {FTPorFILEsegment})*
+FTPtype = ";" [tT][yY][pP][eE] "=" [aAiIdD]
+FTPscheme = [fF][tT][pP] "://"
+FTPurl = {FTPscheme} {URIauthorityLoose} {FTPorFILEpath} {FTPtype}? {URIfragment}?
+
+FILEscheme = [fF][iI][lL][eE] "://"
+FILEurl = {FILEscheme} {URIhostLoose}? {FTPorFILEpath} {URIfragment}?
+
+URL = {HTTPurl} | {FTPurl} | {FILEurl}
+
+EMAILquotedString = [\"] ([\u0001-\u0008\u000B\u000C\u000E-\u0021\u0023-\u005B\u005D-\u007E] | [\\] [\u0000-\u007F])* [\"]
+EMAILatomText = [A-Za-z0-9!#$%&'*+-/=?\^_`{|}~]
+EMAILlabel = {EMAILatomText}+ | {EMAILquotedString}
+EMAILlocalPart = {EMAILlabel} ("." {EMAILlabel})*
+EMAILdomainLiteralText = [\u0001-\u0008\u000B\u000C\u000E-\u005A\u005E-\u007F] | [\\] [\u0000-\u007F]
+// DFA minimization allows {IPv6Address} and {IPv4Address} to be included 
+// in the {EMAILbracketedHost} definition without incurring any size penalties, 
+// since {EMAILdomainLiteralText} recognizes all valid IP addresses.
+// The IP address regexes are included in {EMAILbracketedHost} simply as a 
+// reminder that they are acceptable bracketed host forms.
+EMAILbracketedHost = "[" ({EMAILdomainLiteralText}* | {IPv4Address} | [iI][pP][vV] "6:" {IPv6Address}) "]"
+EMAIL = {EMAILlocalPart} "@" ({DomainNameStrict} | {EMAILbracketedHost})
+
+
+%{
+  /** Alphanumeric sequences */
+  public static final int WORD_TYPE = UAX29URLEmailTokenizer.ALPHANUM;
+  
+  /** Numbers */
+  public static final int NUMERIC_TYPE = UAX29URLEmailTokenizer.NUM;
+  
+  /**
+   * Chars in class \p{Line_Break = Complex_Context} are from South East Asian
+   * scripts (Thai, Lao, Myanmar, Khmer, etc.).  Sequences of these are kept 
+   * together as as a single token rather than broken up, because the logic
+   * required to break them at word boundaries is too complex for UAX#29.
+   * <p>
+   * See Unicode Line Breaking Algorithm: http://www.unicode.org/reports/tr14/#SA
+   */
+  public static final int SOUTH_EAST_ASIAN_TYPE = UAX29URLEmailTokenizer.SOUTHEAST_ASIAN;
+  
+  public static final int IDEOGRAPHIC_TYPE = UAX29URLEmailTokenizer.IDEOGRAPHIC;
+  
+  public static final int HIRAGANA_TYPE = UAX29URLEmailTokenizer.HIRAGANA;
+  
+  public static final int KATAKANA_TYPE = UAX29URLEmailTokenizer.KATAKANA;
+  
+  public static final int HANGUL_TYPE = UAX29URLEmailTokenizer.HANGUL;
+  
+  public static final int EMAIL_TYPE = UAX29URLEmailTokenizer.EMAIL;
+  
+  public static final int URL_TYPE = UAX29URLEmailTokenizer.URL;
+
+  public final int yychar()
+  {
+    return yychar;
+  }
+
+  /**
+   * Fills CharTermAttribute with the current token text.
+   */
+  public final void getText(CharTermAttribute t) {
+    t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
+  }
+  
+  /**
+   * Sets the scanner buffer size in chars
+   */
+   public final void setBufferSize(int numChars) {
+     ZZ_BUFFERSIZE = numChars;
+     char[] newZzBuffer = new char[ZZ_BUFFERSIZE];
+     System.arraycopy(zzBuffer, 0, newZzBuffer, 0, Math.min(zzBuffer.length, ZZ_BUFFERSIZE));
+     zzBuffer = newZzBuffer;
+   }
+%}
+
+%%
+
+<YYINITIAL, AVOID_BAD_URL> {
+
+// UAX#29 WB1.   sot   
+//        WB2.        eot
+//
+  <<EOF>> { return YYEOF; }
+
+  {URL}   { yybegin(YYINITIAL); return URL_TYPE; }
+
+  // LUCENE-5391: Don't recognize no-scheme domain-only URLs with a following alphanumeric character
+  {URIhostStrict} / [^-\w] { yybegin(YYINITIAL); return URL_TYPE; }
+}
+
+// Match bad URL (no scheme domain-only URL with a following alphanumeric character)
+// then change to AVOID_BAD_URL state and pushback the match.
+// This rule won't match when in AVOID_BAD_URL state
+{URIhostStrict} / [-\w]  { yybegin(AVOID_BAD_URL); yypushback(yylength()); }
+
+// Match a no-schema domain at EOF
+// This rule won't match when in AVOID_BAD_URL state
+{URIhostStrict} { return URL_TYPE; }
+
+<YYINITIAL, AVOID_BAD_URL> {
+
+  // LUCENE-3880: Disrupt recognition of "mailto:test" as <ALPHANUM> from "mailto:test@example.org"
+  [mM][aA][iI][lL][tT][oO] / ":" {EMAIL} {  yybegin(YYINITIAL); return WORD_TYPE; }
+
+  {EMAIL} { yybegin(YYINITIAL); return EMAIL_TYPE; }
+
+  // UAX#29 WB8.   Numeric  Numeric
+  //        WB11.  Numeric (MidNum | MidNumLet | Single_Quote)  Numeric
+  //        WB12.  Numeric  (MidNum | MidNumLet | Single_Quote) Numeric
+  //        WB13a. (ALetter | Hebrew_Letter | Numeric | Katakana | ExtendNumLet)  ExtendNumLet
+  //        WB13b. ExtendNumLet  (ALetter | Hebrew_Letter | Numeric | Katakana)
+  //
+  {ExtendNumLetEx}* {NumericEx} ( ( {ExtendNumLetEx}* | {MidNumericEx} ) {NumericEx} )* {ExtendNumLetEx}*
+    {  yybegin(YYINITIAL); return NUMERIC_TYPE; }
+
+  // subset of the below for typing purposes only!
+  {HangulEx}+
+    { yybegin(YYINITIAL); return HANGUL_TYPE; }
+  
+  {KatakanaEx}+
+    { yybegin(YYINITIAL); return KATAKANA_TYPE; }
+
+  // UAX#29 WB5.   (ALetter | Hebrew_Letter)  (ALetter | Hebrew_Letter)
+  //        WB6.   (ALetter | Hebrew_Letter)  (MidLetter | MidNumLet | Single_Quote) (ALetter | Hebrew_Letter)
+  //        WB7.   (ALetter | Hebrew_Letter) (MidLetter | MidNumLet | Single_Quote)  (ALetter | Hebrew_Letter)
+  //        WB7a.  Hebrew_Letter  Single_Quote
+  //        WB7b.  Hebrew_Letter  Double_Quote Hebrew_Letter
+  //        WB7c.  Hebrew_Letter Double_Quote  Hebrew_Letter
+  //        WB9.   (ALetter | Hebrew_Letter)  Numeric
+  //        WB10.  Numeric  (ALetter | Hebrew_Letter)
+  //        WB13.  Katakana  Katakana
+  //        WB13a. (ALetter | Hebrew_Letter | Numeric | Katakana | ExtendNumLet)  ExtendNumLet
+  //        WB13b. ExtendNumLet  (ALetter | Hebrew_Letter | Numeric | Katakana)
+  //
+  {ExtendNumLetEx}*  ( {KatakanaEx}          ( {ExtendNumLetEx}*   {KatakanaEx}                           )*
+                     | ( {HebrewLetterEx}    ( {SingleQuoteEx}     | {DoubleQuoteEx}  {HebrewLetterEx}    )
+                       | {NumericEx}         ( ( {ExtendNumLetEx}* | {MidNumericEx} ) {NumericEx}         )*
+                       | {HebrewOrALetterEx} ( ( {ExtendNumLetEx}* | {MidLetterEx}  ) {HebrewOrALetterEx} )*
+                       )+
+                     )
+  ({ExtendNumLetEx}+ ( {KatakanaEx}          ( {ExtendNumLetEx}*   {KatakanaEx}                           )*
+                     | ( {HebrewLetterEx}    ( {SingleQuoteEx}     | {DoubleQuoteEx}  {HebrewLetterEx}    )
+                       | {NumericEx}         ( ( {ExtendNumLetEx}* | {MidNumericEx} ) {NumericEx}         )*
+                       | {HebrewOrALetterEx} ( ( {ExtendNumLetEx}* | {MidLetterEx}  ) {HebrewOrALetterEx} )*
+                       )+
+                     )
+  )*
+  {ExtendNumLetEx}*
+    { yybegin(YYINITIAL); return WORD_TYPE; }
+
+
+  // From UAX #29:
+  //
+  //    [C]haracters with the Line_Break property values of Contingent_Break (CB),
+  //    Complex_Context (SA/South East Asian), and XX (Unknown) are assigned word
+  //    boundary property values based on criteria outside of the scope of this
+  //    annex.  That means that satisfactory treatment of languages like Chinese
+  //    or Thai requires special handling.
+  //
+  // In Unicode 6.3, only one character has the \p{Line_Break = Contingent_Break}
+  // property: U+FFFC (  ) OBJECT REPLACEMENT CHARACTER.
+  //
+  // In the ICU implementation of UAX#29, \p{Line_Break = Complex_Context}
+  // character sequences (from South East Asian scripts like Thai, Myanmar, Khmer,
+  // Lao, etc.) are kept together.  This grammar does the same below.
+  //
+  // See also the Unicode Line Breaking Algorithm:
+  //
+  //    http://www.unicode.org/reports/tr14/#SA
+  //
+  {ComplexContextEx}+ { yybegin(YYINITIAL); return SOUTH_EAST_ASIAN_TYPE; }
+
+  // UAX#29 WB14.  Any  Any
+  //
+  {HanEx} { yybegin(YYINITIAL); return IDEOGRAPHIC_TYPE; }
+  {HiraganaEx} { yybegin(YYINITIAL); return HIRAGANA_TYPE; }
+
+
+  // UAX#29 WB3.   CR  LF
+  //        WB3a.  (Newline | CR | LF) 
+  //        WB3b.   (Newline | CR | LF)
+  //        WB13c. Regional_Indicator  Regional_Indicator
+  //        WB14.  Any  Any
+  //
+  {RegionalIndicatorEx} {RegionalIndicatorEx}+ | [^]
+    { yybegin(YYINITIAL); /* Not numeric, word, ideographic, hiragana, or SE Asian -- ignore it. */ }
+}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/package-info.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/package-info.java
new file mode 100644
index 0000000..d876d05
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/package-info.java
@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Fast, general-purpose grammar-based tokenizers.
+ * <p>The <code>org.apache.lucene.analysis.standard</code> package contains three
+ * fast grammar-based tokenizers constructed with JFlex:</p>
+ * <ul>
+ *     <li>{@link org.apache.lucene.analysis.standard.StandardTokenizer}:
+ *         as of Lucene 3.1, implements the Word Break rules from the Unicode Text 
+ *         Segmentation algorithm, as specified in 
+ *         <a href="http://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>.
+ *         Unlike <code>UAX29URLEmailTokenizer</code>, URLs and email addresses are
+ *         <b>not</b> tokenized as single tokens, but are instead split up into 
+ *         tokens according to the UAX#29 word break rules.
+ *         <br>
+ *         {@link org.apache.lucene.analysis.standard.StandardAnalyzer StandardAnalyzer} includes
+ *         {@link org.apache.lucene.analysis.standard.StandardTokenizer StandardTokenizer},
+ *         {@link org.apache.lucene.analysis.standard.StandardFilter StandardFilter}, 
+ *         {@link org.apache.lucene.analysis.core.LowerCaseFilter LowerCaseFilter}
+ *         and {@link org.apache.lucene.analysis.core.StopFilter StopFilter}.
+ *         When the <code>Version</code> specified in the constructor is lower than 
+ *         3.1, the {@link org.apache.lucene.analysis.standard.ClassicTokenizer ClassicTokenizer}
+ *         implementation is invoked.</li>
+ *     <li>{@link org.apache.lucene.analysis.standard.ClassicTokenizer ClassicTokenizer}:
+ *         this class was formerly (prior to Lucene 3.1) named 
+ *         <code>StandardTokenizer</code>.  (Its tokenization rules are not
+ *         based on the Unicode Text Segmentation algorithm.)
+ *         {@link org.apache.lucene.analysis.standard.ClassicAnalyzer ClassicAnalyzer} includes
+ *         {@link org.apache.lucene.analysis.standard.ClassicTokenizer ClassicTokenizer},
+ *         {@link org.apache.lucene.analysis.standard.StandardFilter StandardFilter}, 
+ *         {@link org.apache.lucene.analysis.core.LowerCaseFilter LowerCaseFilter}
+ *         and {@link org.apache.lucene.analysis.core.StopFilter StopFilter}.
+ *     </li>
+ *     <li>{@link org.apache.lucene.analysis.standard.UAX29URLEmailTokenizer UAX29URLEmailTokenizer}:
+ *         implements the Word Break rules from the Unicode Text Segmentation
+ *         algorithm, as specified in 
+ *         <a href="http://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>.
+ *         URLs and email addresses are also tokenized according to the relevant RFCs.
+ *         <br>
+ *         {@link org.apache.lucene.analysis.standard.UAX29URLEmailAnalyzer UAX29URLEmailAnalyzer} includes
+ *         {@link org.apache.lucene.analysis.standard.UAX29URLEmailTokenizer UAX29URLEmailTokenizer},
+ *         {@link org.apache.lucene.analysis.standard.StandardFilter StandardFilter},
+ *         {@link org.apache.lucene.analysis.core.LowerCaseFilter LowerCaseFilter}
+ *         and {@link org.apache.lucene.analysis.core.StopFilter StopFilter}.
+ *     </li>
+ * </ul>
+ */
+package org.apache.lucene.analysis.standard.std50;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/package.html b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/package.html
new file mode 100644
index 0000000..4cf4f85
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/std50/package.html
@@ -0,0 +1,22 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html><head></head>
+<body>
+Backwards-compatible implementation to match {@link org.apache.lucene.util.Version#LUCENE_5_0}
+</body>
+</html>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerFactory.java
index 3a57096..1f534e5 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerFactory.java
@@ -20,8 +20,11 @@ package org.apache.lucene.analysis.wikipedia;
 import java.util.Collections;
 import java.util.Map;
 
+import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.util.TokenizerFactory;
+import org.apache.lucene.analysis.wikipedia.wp50.WikipediaTokenizer50;
 import org.apache.lucene.util.AttributeFactory;
+import org.apache.lucene.util.Version;
 
 /** 
  * Factory for {@link WikipediaTokenizer}.
@@ -44,8 +47,13 @@ public class WikipediaTokenizerFactory extends TokenizerFactory {
   
   // TODO: add support for WikipediaTokenizer's advanced options.
   @Override
-  public WikipediaTokenizer create(AttributeFactory factory) {
-    return new WikipediaTokenizer(factory, WikipediaTokenizer.TOKENS_ONLY,
-        Collections.<String>emptySet());
+  public Tokenizer create(AttributeFactory factory) {
+    if (luceneMatchVersion.onOrAfter(Version.LUCENE_6_0_0)) {
+      return new WikipediaTokenizer(factory, WikipediaTokenizer.TOKENS_ONLY,
+          Collections.<String>emptySet());
+    } else {
+      return new WikipediaTokenizer50(factory, WikipediaTokenizer.TOKENS_ONLY,
+          Collections.<String>emptySet());
+    }
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex
index 404fbd6..67236f7 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex
@@ -25,7 +25,7 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 %%
 
 %class WikipediaTokenizerImpl
-%unicode 3.0
+%unicode 7.0
 %integer
 %function getNextToken
 %pack
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/wp50/WikipediaTokenizer50.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/wp50/WikipediaTokenizer50.java
new file mode 100644
index 0000000..014da72
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/wp50/WikipediaTokenizer50.java
@@ -0,0 +1,318 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.analysis.wikipedia.wp50;
+
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.util.AttributeFactory;
+import org.apache.lucene.util.AttributeSource;
+
+import java.io.IOException;
+import java.io.Reader;
+import java.util.*;
+
+
+/**
+ * Extension of StandardTokenizer that is aware of Wikipedia syntax.  It is based off of the
+ * Wikipedia tutorial available at http://en.wikipedia.org/wiki/Wikipedia:Tutorial, but it may not be complete.
+ * @lucene.experimental
+ */
+public final class WikipediaTokenizer50 extends Tokenizer {
+  public static final String INTERNAL_LINK = "il";
+  public static final String EXTERNAL_LINK = "el";
+  //The URL part of the link, i.e. the first token
+  public static final String EXTERNAL_LINK_URL = "elu";
+  public static final String CITATION = "ci";
+  public static final String CATEGORY = "c";
+  public static final String BOLD = "b";
+  public static final String ITALICS = "i";
+  public static final String BOLD_ITALICS = "bi";
+  public static final String HEADING = "h";
+  public static final String SUB_HEADING = "sh";
+
+  public static final int ALPHANUM_ID          = 0;
+  public static final int APOSTROPHE_ID        = 1;
+  public static final int ACRONYM_ID           = 2;
+  public static final int COMPANY_ID           = 3;
+  public static final int EMAIL_ID             = 4;
+  public static final int HOST_ID              = 5;
+  public static final int NUM_ID               = 6;
+  public static final int CJ_ID                = 7;
+  public static final int INTERNAL_LINK_ID     = 8;
+  public static final int EXTERNAL_LINK_ID     = 9;
+  public static final int CITATION_ID          = 10;
+  public static final int CATEGORY_ID          = 11;
+  public static final int BOLD_ID              = 12;
+  public static final int ITALICS_ID           = 13;
+  public static final int BOLD_ITALICS_ID      = 14;
+  public static final int HEADING_ID           = 15;
+  public static final int SUB_HEADING_ID       = 16;
+  public static final int EXTERNAL_LINK_URL_ID = 17;
+
+  /** String token types that correspond to token type int constants */
+  public static final String [] TOKEN_TYPES = new String [] {
+    "<ALPHANUM>",
+    "<APOSTROPHE>",
+    "<ACRONYM>",
+    "<COMPANY>",
+    "<EMAIL>",
+    "<HOST>",
+    "<NUM>",
+    "<CJ>",
+    INTERNAL_LINK,
+    EXTERNAL_LINK,
+    CITATION,
+    CATEGORY,
+    BOLD,
+    ITALICS,
+    BOLD_ITALICS,
+    HEADING,
+    SUB_HEADING,
+    EXTERNAL_LINK_URL
+  };
+
+  /**
+   * Only output tokens
+   */
+  public static final int TOKENS_ONLY = 0;
+  /**
+   * Only output untokenized tokens, which are tokens that would normally be split into several tokens
+   */
+  public static final int UNTOKENIZED_ONLY = 1;
+  /**
+   * Output the both the untokenized token and the splits
+   */
+  public static final int BOTH = 2;
+  /**
+   * This flag is used to indicate that the produced "Token" would, if {@link #TOKENS_ONLY} was used, produce multiple tokens.
+   */
+  public static final int UNTOKENIZED_TOKEN_FLAG = 1;
+  /**
+   * A private instance of the JFlex-constructed scanner
+   */
+  private final WikipediaTokenizerImpl50 scanner;
+
+  private int tokenOutput = TOKENS_ONLY;
+  private Set<String> untokenizedTypes = Collections.emptySet();
+  private Iterator<AttributeSource.State> tokens = null;
+  
+  private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
+  private final TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
+  private final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
+  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+  private final FlagsAttribute flagsAtt = addAttribute(FlagsAttribute.class);
+  
+  private boolean first;
+
+  /**
+   * Creates a new instance of the {@link WikipediaTokenizer50}. Attaches the
+   * <code>input</code> to a newly created JFlex scanner.
+   */
+  public WikipediaTokenizer50() {
+    this(TOKENS_ONLY, Collections.<String>emptySet());
+  }
+
+  /**
+   * Creates a new instance of the {@link org.apache.lucene.analysis.wikipedia.WikipediaTokenizer}.  Attaches the
+   * <code>input</code> to a the newly created JFlex scanner.
+   *
+   * @param tokenOutput One of {@link #TOKENS_ONLY}, {@link #UNTOKENIZED_ONLY}, {@link #BOTH}
+   */
+  public WikipediaTokenizer50(int tokenOutput, Set<String> untokenizedTypes) {
+    this.scanner = new WikipediaTokenizerImpl50(this.input);
+    init(tokenOutput, untokenizedTypes);
+  }
+
+  /**
+   * Creates a new instance of the {@link org.apache.lucene.analysis.wikipedia.WikipediaTokenizer}.  Attaches the
+   * <code>input</code> to a the newly created JFlex scanner. Uses the given {@link org.apache.lucene.util.AttributeFactory}.
+   *
+   * @param tokenOutput One of {@link #TOKENS_ONLY}, {@link #UNTOKENIZED_ONLY}, {@link #BOTH}
+   */
+  public WikipediaTokenizer50(AttributeFactory factory, int tokenOutput, Set<String> untokenizedTypes) {
+    super(factory);
+    this.scanner = new WikipediaTokenizerImpl50(this.input);
+    init(tokenOutput, untokenizedTypes);
+  }
+  
+  private void init(int tokenOutput, Set<String> untokenizedTypes) {
+    // TODO: cutover to enum
+    if (tokenOutput != TOKENS_ONLY &&
+        tokenOutput != UNTOKENIZED_ONLY &&
+        tokenOutput != BOTH) {
+      throw new IllegalArgumentException("tokenOutput must be TOKENS_ONLY, UNTOKENIZED_ONLY or BOTH");
+    }
+    this.tokenOutput = tokenOutput;
+    this.untokenizedTypes = untokenizedTypes;    
+  }
+  
+  /*
+  * (non-Javadoc)
+  *
+  * @see org.apache.lucene.analysis.TokenStream#next()
+  */
+  @Override
+  public final boolean incrementToken() throws IOException {
+    if (tokens != null && tokens.hasNext()){
+      AttributeSource.State state = tokens.next();
+      restoreState(state);
+      return true;
+    }
+    clearAttributes();
+    int tokenType = scanner.getNextToken();
+
+    if (tokenType == WikipediaTokenizerImpl50.YYEOF) {
+      return false;
+    }
+    String type = WikipediaTokenizerImpl50.TOKEN_TYPES[tokenType];
+    if (tokenOutput == TOKENS_ONLY || untokenizedTypes.contains(type) == false){
+      setupToken();
+    } else if (tokenOutput == UNTOKENIZED_ONLY && untokenizedTypes.contains(type) == true){
+      collapseTokens(tokenType);
+
+    }
+    else if (tokenOutput == BOTH){
+      //collapse into a single token, add it to tokens AND output the individual tokens
+      //output the untokenized Token first
+      collapseAndSaveTokens(tokenType, type);
+    }
+    int posinc = scanner.getPositionIncrement();
+    if (first && posinc == 0) {
+      posinc = 1; // don't emit posinc=0 for the first token!
+    }
+    posIncrAtt.setPositionIncrement(posinc);
+    typeAtt.setType(type);
+    first = false;
+    return true;
+  }
+
+  private void collapseAndSaveTokens(int tokenType, String type) throws IOException {
+    //collapse
+    StringBuilder buffer = new StringBuilder(32);
+    int numAdded = scanner.setText(buffer);
+    //TODO: how to know how much whitespace to add
+    int theStart = scanner.yychar();
+    int lastPos = theStart + numAdded;
+    int tmpTokType;
+    int numSeen = 0;
+    List<AttributeSource.State> tmp = new ArrayList<>();
+    setupSavedToken(0, type);
+    tmp.add(captureState());
+    //while we can get a token and that token is the same type and we have not transitioned to a new wiki-item of the same type
+    while ((tmpTokType = scanner.getNextToken()) != WikipediaTokenizerImpl50.YYEOF && tmpTokType == tokenType && scanner.getNumWikiTokensSeen() > numSeen){
+      int currPos = scanner.yychar();
+      //append whitespace
+      for (int i = 0; i < (currPos - lastPos); i++){
+        buffer.append(' ');
+      }
+      numAdded = scanner.setText(buffer);
+      setupSavedToken(scanner.getPositionIncrement(), type);
+      tmp.add(captureState());
+      numSeen++;
+      lastPos = currPos + numAdded;
+    }
+    //trim the buffer
+    // TODO: this is inefficient
+    String s = buffer.toString().trim();
+    termAtt.setEmpty().append(s);
+    offsetAtt.setOffset(correctOffset(theStart), correctOffset(theStart + s.length()));
+    flagsAtt.setFlags(UNTOKENIZED_TOKEN_FLAG);
+    //The way the loop is written, we will have proceeded to the next token.  We need to pushback the scanner to lastPos
+    if (tmpTokType != WikipediaTokenizerImpl50.YYEOF){
+      scanner.yypushback(scanner.yylength());
+    }
+    tokens = tmp.iterator();
+  }
+
+  private void setupSavedToken(int positionInc, String type){
+    setupToken();
+    posIncrAtt.setPositionIncrement(positionInc);
+    typeAtt.setType(type);
+  }
+
+  private void collapseTokens(int tokenType) throws IOException {
+    //collapse
+    StringBuilder buffer = new StringBuilder(32);
+    int numAdded = scanner.setText(buffer);
+    //TODO: how to know how much whitespace to add
+    int theStart = scanner.yychar();
+    int lastPos = theStart + numAdded;
+    int tmpTokType;
+    int numSeen = 0;
+    //while we can get a token and that token is the same type and we have not transitioned to a new wiki-item of the same type
+    while ((tmpTokType = scanner.getNextToken()) != WikipediaTokenizerImpl50.YYEOF && tmpTokType == tokenType && scanner.getNumWikiTokensSeen() > numSeen){
+      int currPos = scanner.yychar();
+      //append whitespace
+      for (int i = 0; i < (currPos - lastPos); i++){
+        buffer.append(' ');
+      }
+      numAdded = scanner.setText(buffer);
+      numSeen++;
+      lastPos = currPos + numAdded;
+    }
+    //trim the buffer
+    // TODO: this is inefficient
+    String s = buffer.toString().trim();
+    termAtt.setEmpty().append(s);
+    offsetAtt.setOffset(correctOffset(theStart), correctOffset(theStart + s.length()));
+    flagsAtt.setFlags(UNTOKENIZED_TOKEN_FLAG);
+    //The way the loop is written, we will have proceeded to the next token.  We need to pushback the scanner to lastPos
+    if (tmpTokType != WikipediaTokenizerImpl50.YYEOF){
+      scanner.yypushback(scanner.yylength());
+    } else {
+      tokens = null;
+    }
+  }
+
+  private void setupToken() {
+    scanner.getText(termAtt);
+    final int start = scanner.yychar();
+    offsetAtt.setOffset(correctOffset(start), correctOffset(start + termAtt.length()));
+  }
+
+  @Override
+  public void close() throws IOException {
+    super.close();
+    scanner.yyreset(input);
+  }
+
+  /*
+  * (non-Javadoc)
+  *
+  * @see org.apache.lucene.analysis.TokenStream#reset()
+  */
+  @Override
+  public void reset() throws IOException {
+    super.reset();
+    scanner.yyreset(input);
+    tokens = null;
+    scanner.reset();
+    first = true;
+  }
+
+  @Override
+  public void end() throws IOException {
+    super.end();
+    // set final offset
+    final int finalOffset = correctOffset(scanner.yychar() + scanner.yylength());
+    this.offsetAtt.setOffset(finalOffset, finalOffset);
+  }
+}
\ No newline at end of file
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/wp50/WikipediaTokenizerImpl50.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/wp50/WikipediaTokenizerImpl50.jflex
new file mode 100644
index 0000000..809c94f
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/wp50/WikipediaTokenizerImpl50.jflex
@@ -0,0 +1,343 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.analysis.wikipedia.wp50;
+
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.wikipedia.WikipediaTokenizer;
+
+/**
+ * JFlex-generated tokenizer that is aware of Wikipedia syntax.
+ */
+@SuppressWarnings("fallthrough")
+%%
+
+%class WikipediaTokenizerImpl50
+%unicode 3.0
+%integer
+%function getNextToken
+%pack
+%char
+%buffer 4096
+
+%{
+
+public static final int ALPHANUM          = WikipediaTokenizer.ALPHANUM_ID;
+public static final int APOSTROPHE        = WikipediaTokenizer.APOSTROPHE_ID;
+public static final int ACRONYM           = WikipediaTokenizer.ACRONYM_ID;
+public static final int COMPANY           = WikipediaTokenizer.COMPANY_ID;
+public static final int EMAIL             = WikipediaTokenizer.EMAIL_ID;
+public static final int HOST              = WikipediaTokenizer.HOST_ID;
+public static final int NUM               = WikipediaTokenizer.NUM_ID;
+public static final int CJ                = WikipediaTokenizer.CJ_ID;
+public static final int INTERNAL_LINK     = WikipediaTokenizer.INTERNAL_LINK_ID;
+public static final int EXTERNAL_LINK     = WikipediaTokenizer.EXTERNAL_LINK_ID;
+public static final int CITATION          = WikipediaTokenizer.CITATION_ID;
+public static final int CATEGORY          = WikipediaTokenizer.CATEGORY_ID;
+public static final int BOLD              = WikipediaTokenizer.BOLD_ID;
+public static final int ITALICS           = WikipediaTokenizer.ITALICS_ID;
+public static final int BOLD_ITALICS      = WikipediaTokenizer.BOLD_ITALICS_ID;
+public static final int HEADING           = WikipediaTokenizer.HEADING_ID;
+public static final int SUB_HEADING       = WikipediaTokenizer.SUB_HEADING_ID;
+public static final int EXTERNAL_LINK_URL = WikipediaTokenizer.EXTERNAL_LINK_URL_ID;
+
+
+private int currentTokType;
+private int numBalanced = 0;
+private int positionInc = 1;
+private int numLinkToks = 0;
+//Anytime we start a new on a Wiki reserved token (category, link, etc.) this value will be 0, otherwise it will be the number of tokens seen
+//this can be useful for detecting when a new reserved token is encountered
+//see https://issues.apache.org/jira/browse/LUCENE-1133
+private int numWikiTokensSeen = 0;
+
+public static final String [] TOKEN_TYPES = WikipediaTokenizer.TOKEN_TYPES;
+
+/**
+Returns the number of tokens seen inside a category or link, etc.
+@return the number of tokens seen inside the context of wiki syntax.
+**/
+public final int getNumWikiTokensSeen(){
+  return numWikiTokensSeen;
+}
+
+public final int yychar()
+{
+    return yychar;
+}
+
+public final int getPositionIncrement(){
+  return positionInc;
+}
+
+/**
+ * Fills Lucene token with the current token text.
+ */
+final void getText(CharTermAttribute t) {
+  t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
+}
+
+final int setText(StringBuilder buffer){
+  int length = zzMarkedPos - zzStartRead;
+  buffer.append(zzBuffer, zzStartRead, length);
+  return length;
+}
+
+final void reset() {
+  currentTokType = 0;
+  numBalanced = 0;
+  positionInc = 1;
+  numLinkToks = 0;
+  numWikiTokensSeen = 0;
+}
+
+
+%}
+
+// basic word: a sequence of digits & letters
+ALPHANUM   = ({LETTER}|{DIGIT}|{KOREAN})+
+
+// internal apostrophes: O'Reilly, you're, O'Reilly's
+// use a post-filter to remove possesives
+APOSTROPHE =  {ALPHA} ("'" {ALPHA})+
+
+// acronyms: U.S.A., I.B.M., etc.
+// use a post-filter to remove dots
+ACRONYM    =  {ALPHA} "." ({ALPHA} ".")+
+
+// company names like AT&T and Excite@Home.
+COMPANY    =  {ALPHA} ("&"|"@") {ALPHA}
+
+// email addresses
+EMAIL      =  {ALPHANUM} (("."|"-"|"_") {ALPHANUM})* "@" {ALPHANUM} (("."|"-") {ALPHANUM})+
+
+// hostname
+HOST       =  {ALPHANUM} ((".") {ALPHANUM})+
+
+// floating point, serial, model numbers, ip addresses, etc.
+// every other segment must have at least one digit
+NUM        = ({ALPHANUM} {P} {HAS_DIGIT}
+           | {DIGIT}+ {P} {DIGIT}+
+           | {HAS_DIGIT} {P} {ALPHANUM}
+           | {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+
+           | {HAS_DIGIT} ({P} {ALPHANUM} {P} {HAS_DIGIT})+
+           | {ALPHANUM} {P} {HAS_DIGIT} ({P} {ALPHANUM} {P} {HAS_DIGIT})+
+           | {HAS_DIGIT} {P} {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+)
+
+TAGS = "<"\/?{ALPHANUM}({WHITESPACE}*{ALPHANUM}=\"{ALPHANUM}\")*">"
+
+// punctuation
+P           = ("_"|"-"|"/"|"."|",")
+
+// at least one digit
+HAS_DIGIT  =
+    ({LETTER}|{DIGIT})*
+    {DIGIT}
+    ({LETTER}|{DIGIT})*
+
+ALPHA      = ({LETTER})+
+
+
+LETTER     = [\u0041-\u005a\u0061-\u007a\u00c0-\u00d6\u00d8-\u00f6\u00f8-\u00ff\u0100-\u1fff\uffa0-\uffdc]
+
+DIGIT      = [\u0030-\u0039\u0660-\u0669\u06f0-\u06f9\u0966-\u096f\u09e6-\u09ef\u0a66-\u0a6f\u0ae6-\u0aef\u0b66-\u0b6f\u0be7-\u0bef\u0c66-\u0c6f\u0ce6-\u0cef\u0d66-\u0d6f\u0e50-\u0e59\u0ed0-\u0ed9\u1040-\u1049]
+
+KOREAN     = [\uac00-\ud7af\u1100-\u11ff]
+
+// Chinese, Japanese
+CJ         = [\u3040-\u318f\u3100-\u312f\u3040-\u309F\u30A0-\u30FF\u31F0-\u31FF\u3300-\u337f\u3400-\u4dbf\u4e00-\u9fff\uf900-\ufaff\uff65-\uff9f]
+
+WHITESPACE = \r\n | [ \r\n\t\f]
+
+//Wikipedia
+DOUBLE_BRACKET = "["{2}
+DOUBLE_BRACKET_CLOSE = "]"{2}
+DOUBLE_BRACKET_CAT = "["{2}":"?"Category:"
+EXTERNAL_LINK = "["
+TWO_SINGLE_QUOTES = "'"{2}
+CITATION = "<ref>"
+CITATION_CLOSE = "</ref>"
+INFOBOX = {DOUBLE_BRACE}("I"|"i")nfobox_
+
+DOUBLE_BRACE = "{"{2}
+DOUBLE_BRACE_CLOSE = "}"{2}
+PIPE = "|"
+DOUBLE_EQUALS = "="{2}
+
+
+%state CATEGORY_STATE
+%state INTERNAL_LINK_STATE
+%state EXTERNAL_LINK_STATE
+
+%state TWO_SINGLE_QUOTES_STATE
+%state THREE_SINGLE_QUOTES_STATE
+%state FIVE_SINGLE_QUOTES_STATE
+%state DOUBLE_EQUALS_STATE
+%state DOUBLE_BRACE_STATE
+%state STRING
+
+%%
+
+<YYINITIAL>{ALPHANUM}                                                     {positionInc = 1; return ALPHANUM; }
+<YYINITIAL>{APOSTROPHE}                                                   {positionInc = 1; return APOSTROPHE; }
+<YYINITIAL>{ACRONYM}                                                      {positionInc = 1; return ACRONYM; }
+<YYINITIAL>{COMPANY}                                                      {positionInc = 1; return COMPANY; }
+<YYINITIAL>{EMAIL}                                                        {positionInc = 1; return EMAIL; }
+<YYINITIAL>{NUM}                                                          {positionInc = 1; return NUM; }
+<YYINITIAL>{HOST}                                                         {positionInc = 1; return HOST; }
+<YYINITIAL>{CJ}                                                           {positionInc = 1; return CJ; }
+
+//wikipedia
+<YYINITIAL>{
+  //First {ALPHANUM} is always the link, set positioninc to 1 for double bracket, but then inside the internal link state
+  //set it to 0 for the next token, such that the link and the first token are in the same position, but then subsequent
+  //tokens within the link are incremented
+  {DOUBLE_BRACKET} {numWikiTokensSeen = 0; positionInc = 1; currentTokType = INTERNAL_LINK; yybegin(INTERNAL_LINK_STATE);/* Break so we don't hit fall-through warning: */ break;}
+  {DOUBLE_BRACKET_CAT} {numWikiTokensSeen = 0; positionInc = 1; currentTokType = CATEGORY; yybegin(CATEGORY_STATE);/* Break so we don't hit fall-through warning: */ break;}
+  {EXTERNAL_LINK} {numWikiTokensSeen = 0; positionInc = 1; currentTokType = EXTERNAL_LINK_URL; yybegin(EXTERNAL_LINK_STATE);/* Break so we don't hit fall-through warning: */ break;}
+  {TWO_SINGLE_QUOTES} {numWikiTokensSeen = 0; positionInc = 1; if (numBalanced == 0){numBalanced++;yybegin(TWO_SINGLE_QUOTES_STATE);} else{numBalanced = 0;}/* Break so we don't hit fall-through warning: */ break;}
+  {DOUBLE_EQUALS} {numWikiTokensSeen = 0; positionInc = 1; yybegin(DOUBLE_EQUALS_STATE);/* Break so we don't hit fall-through warning: */ break;}
+  {DOUBLE_BRACE} {numWikiTokensSeen = 0; positionInc = 1; currentTokType = CITATION; yybegin(DOUBLE_BRACE_STATE);/* Break so we don't hit fall-through warning: */ break;}
+  {CITATION} {numWikiTokensSeen = 0; positionInc = 1; currentTokType = CITATION; yybegin(DOUBLE_BRACE_STATE);/* Break so we don't hit fall-through warning: */ break;}
+//ignore
+  [^] |{INFOBOX}                                               {numWikiTokensSeen = 0;  positionInc = 1; /* Break so we don't hit fall-through warning: */ break;}
+}
+
+<INTERNAL_LINK_STATE>{
+//First {ALPHANUM} is always the link, set position to 0 for these
+//This is slightly different from EXTERNAL_LINK_STATE because that one has an explicit grammar for capturing the URL
+  {ALPHANUM} {yybegin(INTERNAL_LINK_STATE); numWikiTokensSeen++; return currentTokType;}
+  {DOUBLE_BRACKET_CLOSE} {numLinkToks = 0; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;}
+  //ignore
+  [^]                                               { positionInc = 1; /* Break so we don't hit fall-through warning: */ break;}
+}
+
+<EXTERNAL_LINK_STATE>{
+//increment the link token, but then don't increment the tokens after that which are still in the link
+  ("http://"|"https://"){HOST}("/"?({ALPHANUM}|{P}|\?|"&"|"="|"#")*)* {positionInc = 1; numWikiTokensSeen++; yybegin(EXTERNAL_LINK_STATE); return currentTokType;}
+  {ALPHANUM} {if (numLinkToks == 0){positionInc = 0;} else{positionInc = 1;} numWikiTokensSeen++; currentTokType = EXTERNAL_LINK; yybegin(EXTERNAL_LINK_STATE); numLinkToks++; return currentTokType;}
+  "]" {numLinkToks = 0; positionInc = 0; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;}
+  {WHITESPACE}                                               { positionInc = 1; /* Break so we don't hit fall-through warning: */ break;}
+}
+
+<CATEGORY_STATE>{
+  {ALPHANUM} {yybegin(CATEGORY_STATE); numWikiTokensSeen++; return currentTokType;}
+  {DOUBLE_BRACKET_CLOSE} {yybegin(YYINITIAL);/* Break so we don't hit fall-through warning: */ break;}
+  //ignore
+  [^]                                               { positionInc = 1; /* Break so we don't hit fall-through warning: */ break;}
+}
+//italics
+<TWO_SINGLE_QUOTES_STATE>{
+  "'" {currentTokType = BOLD;  yybegin(THREE_SINGLE_QUOTES_STATE); /* Break so we don't hit fall-through warning: */ break;}
+   "'''" {currentTokType = BOLD_ITALICS;  yybegin(FIVE_SINGLE_QUOTES_STATE); /* Break so we don't hit fall-through warning: */ break;}
+   {ALPHANUM} {currentTokType = ITALICS; numWikiTokensSeen++;  yybegin(STRING); return currentTokType;/*italics*/}
+   //we can have links inside, let those override
+   {DOUBLE_BRACKET} {currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0; yybegin(INTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;}
+   {DOUBLE_BRACKET_CAT} {currentTokType = CATEGORY; numWikiTokensSeen = 0; yybegin(CATEGORY_STATE); /* Break so we don't hit fall-through warning: */ break;}
+   {EXTERNAL_LINK} {currentTokType = EXTERNAL_LINK; numWikiTokensSeen = 0; yybegin(EXTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;}
+
+   //ignore
+   [^]                                               { /* Break so we don't hit fall-through warning: */ break;/* ignore */ }
+}
+//bold
+<THREE_SINGLE_QUOTES_STATE>{
+  {ALPHANUM} {yybegin(STRING); numWikiTokensSeen++; return currentTokType;}
+  //we can have links inside, let those override
+   {DOUBLE_BRACKET} {currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0; yybegin(INTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;}
+   {DOUBLE_BRACKET_CAT} {currentTokType = CATEGORY; numWikiTokensSeen = 0; yybegin(CATEGORY_STATE); /* Break so we don't hit fall-through warning: */ break;}
+   {EXTERNAL_LINK} {currentTokType = EXTERNAL_LINK; numWikiTokensSeen = 0; yybegin(EXTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;}
+
+   //ignore
+   [^]                                               { /* Break so we don't hit fall-through warning: */ break;/* ignore */ }
+
+}
+//bold italics
+<FIVE_SINGLE_QUOTES_STATE>{
+  {ALPHANUM} {yybegin(STRING); numWikiTokensSeen++; return currentTokType;}
+  //we can have links inside, let those override
+   {DOUBLE_BRACKET} {currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0;  yybegin(INTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;}
+   {DOUBLE_BRACKET_CAT} {currentTokType = CATEGORY; numWikiTokensSeen = 0; yybegin(CATEGORY_STATE); /* Break so we don't hit fall-through warning: */ break;}
+   {EXTERNAL_LINK} {currentTokType = EXTERNAL_LINK; numWikiTokensSeen = 0; yybegin(EXTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;}
+
+   //ignore
+   [^]                                               { /* Break so we don't hit fall-through warning: */ break;/* ignore */ }
+}
+
+<DOUBLE_EQUALS_STATE>{
+ "=" {currentTokType = SUB_HEADING; numWikiTokensSeen = 0; yybegin(STRING); /* Break so we don't hit fall-through warning: */ break;}
+ {ALPHANUM} {currentTokType = HEADING; yybegin(DOUBLE_EQUALS_STATE); numWikiTokensSeen++; return currentTokType;}
+ {DOUBLE_EQUALS} {yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;}
+  //ignore
+  [^]                                               { /* Break so we don't hit fall-through warning: */ break;/* ignore */ }
+}
+
+<DOUBLE_BRACE_STATE>{
+  {ALPHANUM} {yybegin(DOUBLE_BRACE_STATE); numWikiTokensSeen = 0; return currentTokType;}
+  {DOUBLE_BRACE_CLOSE} {yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;}
+  {CITATION_CLOSE} {yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;}
+  //ignore
+  [^]                                               { /* Break so we don't hit fall-through warning: */ break;/* ignore */ }
+}
+
+<STRING> {
+  "'''''" {numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end bold italics*/}
+  "'''" {numBalanced = 0;currentTokType = ALPHANUM;yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end bold*/}
+  "''" {numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end italics*/}
+  "===" {numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL); /* Break so we don't hit fall-through warning: */ break;/*end sub header*/}
+  {ALPHANUM} {yybegin(STRING); numWikiTokensSeen++; return currentTokType;/* STRING ALPHANUM*/}
+  //we can have links inside, let those override
+   {DOUBLE_BRACKET} {numBalanced = 0; numWikiTokensSeen = 0; currentTokType = INTERNAL_LINK;yybegin(INTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;}
+   {DOUBLE_BRACKET_CAT} {numBalanced = 0; numWikiTokensSeen = 0; currentTokType = CATEGORY;yybegin(CATEGORY_STATE); /* Break so we don't hit fall-through warning: */ break;}
+   {EXTERNAL_LINK} {numBalanced = 0; numWikiTokensSeen = 0; currentTokType = EXTERNAL_LINK;yybegin(EXTERNAL_LINK_STATE); /* Break so we don't hit fall-through warning: */ break;}
+
+
+  {PIPE} {yybegin(STRING); return currentTokType;/*pipe*/}
+
+  [^]                                              { /* Break so we don't hit fall-through warning: */ break;/* ignore STRING */ }
+}
+
+
+
+
+/*
+{INTERNAL_LINK}                                                { return curentTokType; }
+
+{CITATION}                                                { return currentTokType; }
+{CATEGORY}                                                { return currentTokType; }
+
+{BOLD}                                                { return currentTokType; }
+{ITALICS}                                                { return currentTokType; }
+{BOLD_ITALICS}                                                { return currentTokType; }
+{HEADING}                                                { return currentTokType; }
+{SUB_HEADING}                                                { return currentTokType; }
+
+*/
+//end wikipedia
+
+/** Ignore the rest */
+[^] | {TAGS}                                          { /* Break so we don't hit fall-through warning: */ break;/* ignore */ }
+
+
+//INTERNAL_LINK = "["{2}({ALPHANUM}+{WHITESPACE}*)+"]"{2}
+//EXTERNAL_LINK = "["http://"{HOST}.*?"]"
+//CITATION = "{"{2}({ALPHANUM}+{WHITESPACE}*)+"}"{2}
+//CATEGORY = "["{2}"Category:"({ALPHANUM}+{WHITESPACE}*)+"]"{2}
+//CATEGORY_COLON = "["{2}":Category:"({ALPHANUM}+{WHITESPACE}*)+"]"{2}
+//BOLD = '''({ALPHANUM}+{WHITESPACE}*)+'''
+//ITALICS = ''({ALPHANUM}+{WHITESPACE}*)+''
+//BOLD_ITALICS = '''''({ALPHANUM}+{WHITESPACE}*)+'''''
+//HEADING = "="{2}({ALPHANUM}+{WHITESPACE}*)+"="{2}
+//SUB_HEADING ="="{3}({ALPHANUM}+{WHITESPACE}*)+"="{3}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/wp50/package-info.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/wp50/package-info.java
new file mode 100644
index 0000000..e07fc0a
--- /dev/null
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/wp50/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Tokenizer that is aware of Wikipedia syntax.
+ */
+package org.apache.lucene.analysis.wikipedia.wp50;
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestStandardAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestStandardAnalyzer.java
index 6c6ddc8..09f5d44 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestStandardAnalyzer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestStandardAnalyzer.java
@@ -281,7 +281,7 @@ public class TestStandardAnalyzer extends BaseTokenStreamTestCase {
   }
   
   public void testUnicodeWordBreaks() throws Exception {
-    WordBreakTestUnicode_6_3_0 wordBreakTest = new WordBreakTestUnicode_6_3_0();
+    WordBreakTestUnicode_8_0_0 wordBreakTest = new WordBreakTestUnicode_8_0_0();
     wordBreakTest.test(a);
   }
   
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestUAX29URLEmailTokenizer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestUAX29URLEmailTokenizer.java
index 8d3c706..dab6c52 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestUAX29URLEmailTokenizer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestUAX29URLEmailTokenizer.java
@@ -21,8 +21,6 @@ import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.standard.UAX29URLEmailTokenizer;
-import org.apache.lucene.analysis.standard.WordBreakTestUnicode_6_3_0;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.TestUtil;
@@ -469,7 +467,7 @@ public class TestUAX29URLEmailTokenizer extends BaseTokenStreamTestCase {
   }
 
   public void testUnicodeWordBreaks() throws Exception {
-    WordBreakTestUnicode_6_3_0 wordBreakTest = new WordBreakTestUnicode_6_3_0();
+    WordBreakTestUnicode_8_0_0 wordBreakTest = new WordBreakTestUnicode_8_0_0();
     wordBreakTest.test(a);
   }
   
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/generateJavaUnicodeWordBreakTest.pl b/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/generateJavaUnicodeWordBreakTest.pl
index ec37924..b771e3d 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/generateJavaUnicodeWordBreakTest.pl
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/generateJavaUnicodeWordBreakTest.pl
@@ -40,7 +40,7 @@ $underscore_version =~ s/\./_/g;
 my $class_name = "WordBreakTestUnicode_${underscore_version}";
 my $output_filename = "${class_name}.java";
 my $header =<<"__HEADER__";
-package org.apache.lucene.analysis.core;
+package org.apache.lucene.analysis.standard;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -140,7 +140,8 @@ for my $line (@tests) {
       }
     }
     if ($has_wanted_char) {
-      push @tokens, '"'.join('', map { "\\u$_" } @chars).'"';
+      # NB: Need to escape quote characters for Java if they are in our expected output.
+      push @tokens, '"'.join('', map { $_ ~~ /0022/ ? "\\\"" : "\\u$_" } @chars).'"';
     }
   }
   print OUT "    assertAnalyzesTo(analyzer, \"${test_string}\",\n";
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/random.text.with.urls.txt b/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/random.text.with.urls.txt
index ef5ad89..a597070 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/random.text.with.urls.txt
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/random.text.with.urls.txt
@@ -52,7 +52,7 @@ accusatory manner as well known that Joe Gargery marry her cup. `I wonder and
 there was publicly made it was,
 <file:///Gdx5CDZYW%6cnzMJ/7HJ/J%63BSZDXtS/yfWXqq6#> as lookers on; me, I
 noticed that hand, gave me
-<http://1qvgjd1.TP/7oq5gWW/Gwqf8fxBXR4/?Br,q=ayMz0&1IO%370N7=;Sl1czc2L+5bRISfD+w&ygP3FhV%E1w36=2Rx>
+<http://1qvgjd1.TOP/7oq5gWW/Gwqf8fxBXR4/?Br,q=ayMz0&1IO%370N7=;Sl1czc2L+5bRISfD+w&ygP3FhV%E1w36=2Rx>
 upside down, and comforted me up. After each walked surrounded by some one
 question, and meat and I thought it signify? `Certainly!' assented Mr
 Pumblechook,
@@ -910,7 +910,7 @@ it accuses man to call him steady,
 [E69:a743:5C18:C43F:780d:FDD0:EBC8:2ce9]/uAWRrcx men!'' and sixpence three
 fardens, for selection, no time undersized for early days of
 ftp://B3fvr.l5GW6REKV.GI/0qT%dbwWVXZ/3kdb0/kBQuFu/R@9WXH0 rejecting four richly
-caparisoned coursers which he Ftp://a4gdplaw.TP/zyf2c37ZfY/QaiwZ3l/CUi9.ado/
+caparisoned coursers which he Ftp://a4gdplaw.TOP/zyf2c37ZfY/QaiwZ3l/CUi9.ado/
 found Joe has stood in respect of chalk 8L.vg/LjRJZ/z7/Fkg9dwmTDSp about him
 till he was agreeable, and none before. Conscience is rich, too; ain't alone,
 T7wos.u6I.cJP-5HQQCA.9dutej.SG/6McEZ0 and pressed it would have done, and asked
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/urls.from.random.text.with.urls.txt b/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/urls.from.random.text.with.urls.txt
index cf216ca..6d35dcd 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/urls.from.random.text.with.urls.txt
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/urls.from.random.text.with.urls.txt
@@ -25,7 +25,7 @@ ftp://R5ecjkf1yx4wpskfh.tv0y3m90ak.0R605.se:51297/zpWcRRcG/1woSqw7ZUko/
 file:///%C5=.%8by/uuFXEaW8.%7E4/DRM%33Kh2xb8u%7FHizfLn/aoF06#7srWW%2EKoFf
 HTTP://yA2O3F.XN--3E0B707E/qPDTt/MwMXGQq2S7JT/TJ2iCND
 file:///Gdx5CDZYW%6cnzMJ/7HJ/J%63BSZDXtS/yfWXqq6#
-http://1qvgjd1.TP/7oq5gWW/Gwqf8fxBXR4/?Br,q=ayMz0&1IO%370N7=;Sl1czc2L+5bRISfD+w&ygP3FhV%E1w36=2Rx
+http://1qvgjd1.TOP/7oq5gWW/Gwqf8fxBXR4/?Br,q=ayMz0&1IO%370N7=;Sl1czc2L+5bRISfD+w&ygP3FhV%E1w36=2Rx
 ftp://5SCC6BUYP.Knf1cvlc22z9.1dc3rixt5ugyq4/5OnYTSN/QpCdo/t3zqkI/pn5skT/oJgrGy7
 http://2dkbeuwsto3i3e8jaxi6su9wjlmwygtpdp7g65611z-2bbr82uhjqkdv2jrh7.KZ/FiSvI/aaB&dPQ%42kLdM
 FTP://Hi144dz6hctql2n3uom.GE/%1A4OBV%63h/DoA4hpXFmqldOw-MB/PNYoaSDJB2F1k5/Nx%BBEDhrHhcMB
@@ -427,7 +427,7 @@ https://07zje.j84g-9lx-673h.vwr.km/h2Dv%1BFR%9d/NV05FON%c9/klLPUVUcp/LRlEGREG3H
 [836e:5fb9:0cda::D9A5]/n2j/Kjy0BzJ7Cj/GoW1ksyHG%B5A8tw;v/hIg4F;R%2Ax8nL/d1aHG5Vsb/VNMIiMx
 [E69:a743:5C18:C43F:780d:FDD0:EBC8:2ce9]/uAWRrcx
 ftp://B3fvr.l5GW6REKV.GI/0qT%dbwWVXZ/3kdb0/kBQuFu/R@9WXH0
-Ftp://a4gdplaw.TP/zyf2c37ZfY/QaiwZ3l/CUi9.ado/
+Ftp://a4gdplaw.TOP/zyf2c37ZfY/QaiwZ3l/CUi9.ado/
 8L.vg/LjRJZ/z7/Fkg9dwmTDSp
 T7wos.u6I.cJP-5HQQCA.9dutej.SG/6McEZ0
 jJ0D1X6C5CCNWYGOCI4NNFC5A5NYJZTCW65DHS.d1yxpq.TC/EQ%DBYuIdBv
diff --git a/lucene/common-build.xml b/lucene/common-build.xml
index 145f8d2..0c91bbc 100644
--- a/lucene/common-build.xml
+++ b/lucene/common-build.xml
@@ -2334,7 +2334,8 @@ ${ant.project.name}.test.dependencies=${test.classpath.list}
 
   <!-- JFlex task -->
   <target name="-install-jflex" unless="jflex.loaded" depends="ivy-availability-check,ivy-configure">
-    <ivy:cachepath organisation="de.jflex" module="jflex" revision="1.6.0"
+    <loadproperties prefix="ivyversions" srcFile="${common.dir}/ivy-versions.properties"/>
+    <ivy:cachepath organisation="de.jflex" module="jflex" revision="${ivyversions./de.jflex/jflex}"
                    inline="true" conf="default" transitive="true" pathid="jflex.classpath"/>
     <taskdef name="jflex" classname="jflex.anttask.JFlexTask" classpathref="jflex.classpath"/>
     <property name="jflex.loaded" value="true"/>
diff --git a/lucene/ivy-versions.properties b/lucene/ivy-versions.properties
index d5ef256..55d3bd4 100644
--- a/lucene/ivy-versions.properties
+++ b/lucene/ivy-versions.properties
@@ -67,6 +67,7 @@ com.sun.jersey.version = 1.9
 /commons-io/commons-io = 2.4
 /commons-lang/commons-lang = 2.6
 /commons-logging/commons-logging = 1.1.3
+/de.jflex/jflex = 1.6.1
 /de.l3s.boilerpipe/boilerpipe = 1.1.0
 /dom4j/dom4j = 1.6.1
 /hsqldb/hsqldb = 1.8.0.10
-- 
1.9.1

