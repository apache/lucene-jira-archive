commit 255fc0ba5902b9169bc6c1ade4ca9e4cc1c6b527
Author: Atri Sharma <atri@apache.org>
Date:   Mon Jan 14 12:45:37 2019 +0530

    Add API To Filter Terms From TermVectors
    
    TermVectors can occupy sizeable space when the number of documents
    being indexed is high. However, not all terms might be needed for
    specific workloads and memory can be saved by ignoring those when
    parsing documents.
    
    NOTE: This API breaks CheckIndex since CheckIndex does not understand
    Terms with zero frequency. The fix for that will follow in a separate
    patch

diff --git a/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsFormat.java
index a0a65a64b1..d0d827a15a 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsFormat.java
@@ -18,9 +18,11 @@ package org.apache.lucene.codecs;
 
 
 import java.io.IOException;
+import java.util.List;
 
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.Term;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 
@@ -40,4 +42,10 @@ public abstract class TermVectorsFormat {
   /** Returns a {@link TermVectorsWriter} to write term
    *  vectors. */
   public abstract TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException;
+
+  /** Returns a {@link TermVectorsWriter} with applied filtering list
+   *  vectors. */
+  public TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context, List<Term> filterList) throws IOException {
+    return vectorsWriter(directory, segmentInfo, context);
+  }
 }
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java
index b84065af66..1a646127eb 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java
@@ -28,6 +28,7 @@ import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.MergeState;
 import org.apache.lucene.index.PostingsEnum;
+import org.apache.lucene.index.Term;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -59,12 +60,20 @@ import static org.apache.lucene.search.DocIdSetIterator.NO_MORE_DOCS;
  * @lucene.experimental
  */
 public abstract class TermVectorsWriter implements Closeable {
-  
+
+  /** Filter list for terms that are to be filtered and not stored */
+  protected List<String> filterTermsAsStringList = new ArrayList<>();
+
   /** Sole constructor. (For invocation by subclass 
    *  constructors, typically implicit.) */
   protected TermVectorsWriter() {
   }
 
+  /** Set the filter list */
+  public void setFilterTermsList(List<Term> filterTermsList) {
+    filterTermsList.forEach(currentTerm -> filterTermsAsStringList.add(currentTerm.bytes().utf8ToString()));
+  }
+
   /** Called before writing the term vectors of the document.
    *  {@link #startField(FieldInfo, int, boolean, boolean, boolean)} will 
    *  be called <code>numVectorFields</code> times. Note that if term 
@@ -93,6 +102,9 @@ public abstract class TermVectorsWriter implements Closeable {
   /** Called after a term and all its positions have been added. */
   public void finishTerm() throws IOException {}
 
+  /** Process skipped term if the term was filtered */
+  public void skipTerm() throws IOException {}
+
   /** Adds a term position and offsets */
   public abstract void addPosition(int position, int startOffset, int endOffset, BytesRef payload) throws IOException;
 
@@ -287,7 +299,15 @@ public abstract class TermVectorsWriter implements Closeable {
       termsEnum = terms.iterator();
 
       int termCount = 0;
+      int skippedTermCount = 0;
       while(termsEnum.next() != null) {
+
+        if (filterTermsAsStringList.contains(termsEnum.term().utf8ToString())) {
+          skipTerm();
+          ++skippedTermCount;
+          continue;
+        }
+
         termCount++;
 
         final int freq = (int) termsEnum.totalTermFreq();
@@ -315,7 +335,8 @@ public abstract class TermVectorsWriter implements Closeable {
         }
         finishTerm();
       }
-      assert termCount == numTerms;
+      assert termCount + skippedTermCount == numTerms;
+
       finishField();
     }
     assert fieldCount == numFields;
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsReader.java
index cbe1050bf0..87a19f275f 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsReader.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsReader.java
@@ -448,6 +448,17 @@ public final class CompressingTermVectorsReader extends TermVectorsReader implem
       for (int i = 0; i < numFields; ++i) {
         final int[] fStartOffsets = startOffsets[i];
         final int[] fPositions = positions[i];
+
+        /** If vector is empty */
+        if (lengths[i].length == 0) {
+          if (i == (numFields - 1)) {
+            return null;
+          }
+          else {
+            continue;
+          }
+        }
+
         // patch offsets from positions
         if (fStartOffsets != null && fPositions != null) {
           final float fieldCharsPerTerm = charsPerTerm[fieldNumOffs[i]];
@@ -462,6 +473,7 @@ public final class CompressingTermVectorsReader extends TermVectorsReader implem
           for (int j = 0, end = (int) numTerms.get(skip + i); j < end; ++j) {
             // delta-decode start offsets and  patch lengths using term lengths
             final int termLength = fPrefixLengths[j] + fSuffixLengths[j];
+
             lengths[i][positionIndex[i][j]] += termLength;
             for (int k = positionIndex[i][j] + 1; k < positionIndex[i][j + 1]; ++k) {
               fStartOffsets[k] += fStartOffsets[k - 1];
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java
index 4f8d004e20..31c00b127e 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java
@@ -839,8 +839,10 @@ public final class CompressingTermVectorsWriter extends TermVectorsWriter {
           } else {
             vectors = vectorsReader.get(i);
           }
+
           addAllDocVectors(vectors, mergeState);
           ++docCount;
+
         }
       }
     }
diff --git a/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java b/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
index f9aaf34685..bed75e2feb 100644
--- a/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
@@ -4449,6 +4449,8 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable,
                                                      merge.info.info, infoStream, dirWrapper,
                                                      globalFieldNumberMap, 
                                                      context);
+      merger.setFilteredTerms(config.getFilteredTermsList());
+
       merge.info.setSoftDelCount(Math.toIntExact(softDeleteCount.get()));
       merge.checkAborted();
 
diff --git a/lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java b/lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
index a72aeab007..95b5868d1f 100644
--- a/lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
+++ b/lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
@@ -20,6 +20,7 @@ package org.apache.lucene.index;
 import java.io.PrintStream;
 import java.util.Arrays;
 import java.util.EnumSet;
+import java.util.List;
 import java.util.stream.Collectors;
 
 import org.apache.lucene.analysis.Analyzer;
@@ -473,6 +474,12 @@ public final class IndexWriterConfig extends LiveIndexWriterConfig {
     return (IndexWriterConfig) super.setUseCompoundFile(useCompoundFile);
   }
 
+  @Override
+  public IndexWriterConfig setFilteredTermsList(List<Term> filteredTermsList) {
+    return (IndexWriterConfig) super.setFilteredTermsList(filteredTermsList);
+  }
+
+
   /**
    * Sets if calls {@link IndexWriter#close()} should first commit
    * before closing.  Use <code>true</code> to match behavior of Lucene 4.x.
diff --git a/lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java b/lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java
index 259a020c3a..b970276907 100644
--- a/lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java
+++ b/lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java
@@ -17,7 +17,9 @@
 package org.apache.lucene.index;
 
 
+import java.util.ArrayList;
 import java.util.Collections;
+import java.util.List;
 import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
@@ -113,6 +115,12 @@ public class LiveIndexWriterConfig {
   /** soft deletes field */
   protected String softDeletesField = null;
 
+  /**
+   * Filter list for terms that should not be stored in TermVectors.
+   * NOTE: This API should only be used when the consumer is sure of their usecase
+   */
+  protected List<Term> filteredTermsList = new ArrayList<>();
+
   // used by IndexWriterConfig
   LiveIndexWriterConfig(Analyzer analyzer) {
     this.analyzer = analyzer;
@@ -475,6 +483,22 @@ public class LiveIndexWriterConfig {
     return softDeletesField;
   }
 
+  /**
+   * Set the filtered terms list
+   */
+  public LiveIndexWriterConfig setFilteredTermsList(List<Term> filteredTermsList) {
+    this.filteredTermsList = filteredTermsList;
+
+    return this;
+  }
+
+  /**
+   * Return the list of terms to be filtered during writes to TermVector
+   */
+  public List<Term> getFilteredTermsList() {
+    return filteredTermsList;
+  }
+
   @Override
   public String toString() {
     StringBuilder sb = new StringBuilder();
@@ -499,6 +523,7 @@ public class LiveIndexWriterConfig {
     sb.append("indexSort=").append(getIndexSort()).append("\n");
     sb.append("checkPendingFlushOnUpdate=").append(isCheckPendingFlushOnUpdate()).append("\n");
     sb.append("softDeletesField=").append(getSoftDeletesField()).append("\n");
+    sb.append("filteredTermsList=").append(getFilteredTermsList()).append("\n");
     return sb.toString();
   }
 }
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java b/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
index 6554cc59da..005358b2e7 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -49,6 +49,7 @@ final class SegmentMerger {
   
   final MergeState mergeState;
   private final FieldInfos.Builder fieldInfosBuilder;
+  private List<Term> filteredTerms;
 
   // note, just like in codec apis Directory 'dir' is NOT the same as segmentInfo.dir!!
   SegmentMerger(List<CodecReader> readers, SegmentInfo segmentInfo, InfoStream infoStream, Directory dir,
@@ -217,6 +218,10 @@ final class SegmentMerger {
     mergeState.mergeFieldInfos = fieldInfosBuilder.finish();
   }
 
+  public void setFilteredTerms(List<Term> filteredTerms) {
+    this.filteredTerms = filteredTerms;
+  }
+
   /**
    * Merge stored fields from each of the segments into the new one.
    * @return The number of documents in all of the readers
@@ -235,6 +240,10 @@ final class SegmentMerger {
    */
   private int mergeVectors() throws IOException {
     try (TermVectorsWriter termVectorsWriter = codec.termVectorsFormat().vectorsWriter(directory, mergeState.segmentInfo, context)) {
+      if (filteredTerms != null) {
+        termVectorsWriter.setFilterTermsList(filteredTerms);
+      }
+
       return termVectorsWriter.merge(mergeState);
     }
   }
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter.java b/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter.java
index 1f7b57dcaa..c8bc41dd49 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter.java
@@ -18,6 +18,8 @@ package org.apache.lucene.index;
 
 
 import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CachingTokenFilter;
@@ -366,7 +368,115 @@ public class TestTermVectorsWriter extends LuceneTestCase {
     r.close();
     dir.close();
   }
-  
+
+  public void testFilteredTermVector() throws IOException {
+
+    Directory dir = newDirectory();
+    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))
+        .setMaxBufferedDocs(2)
+        .setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH)
+        .setMergeScheduler(new SerialMergeScheduler())
+        .setMergePolicy(new LogDocMergePolicy()));
+
+    Document document = new Document();
+    FieldType customType = new FieldType();
+    customType.setStored(true);
+
+    List<Term> filteredTermsList = new ArrayList<>();
+
+    filteredTermsList.add(new Term("filteredTerm", "valueOfFilteredTerm"));
+    writer.getConfig().setFilteredTermsList(filteredTermsList);
+
+    Field storedField = newField("stored", "stored", customType);
+    document.add(storedField);
+    writer.addDocument(document);
+
+    document = new Document();
+    document.add(storedField);
+    FieldType customType2 = new FieldType(StringField.TYPE_NOT_STORED);
+    customType2.setStoreTermVectors(true);
+    customType2.setStoreTermVectorPositions(true);
+    customType2.setStoreTermVectorOffsets(true);
+    Field termVectorField = newField("termVector", "termVector", customType2);
+
+    document.add(termVectorField);
+    writer.addDocument(document);
+
+    document = new Document();
+    Field termVectorField2 = newField("filteredTerm", "valueOfFilteredTerm", customType2);
+    document.add(storedField);
+    document.add(termVectorField2);
+    writer.addDocument(document);
+
+    writer.forceMerge(1);
+    writer.close();
+
+    IndexReader reader = DirectoryReader.open(dir);
+    assert(reader.numDocs() == 3);
+    assertNotNull(reader.getTermVectors(1));
+    assertNull(reader.getTermVectors(2));
+    reader.close();
+
+    dir.close();
+  }
+
+  public void testFilteredTermVectorWithMultipleTerms() throws IOException {
+
+    Directory dir = newDirectory();
+    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))
+        .setMaxBufferedDocs(2)
+        .setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH)
+        .setMergeScheduler(new SerialMergeScheduler())
+        .setMergePolicy(new LogDocMergePolicy()));
+
+    Document document = new Document();
+    FieldType customType = new FieldType();
+    customType.setStored(true);
+
+    List<Term> filteredTermsList = new ArrayList<>();
+
+    filteredTermsList.add(new Term("filteredTerm", "valueOfFilteredTerm"));
+    writer.getConfig().setFilteredTermsList(filteredTermsList);
+
+    Field storedField = newField("stored", "stored", customType);
+    document.add(storedField);
+    writer.addDocument(document);
+
+    document = new Document();
+    document.add(storedField);
+    FieldType customType2 = new FieldType(StringField.TYPE_NOT_STORED);
+    customType2.setStoreTermVectors(true);
+    customType2.setStoreTermVectorPositions(true);
+    customType2.setStoreTermVectorOffsets(true);
+    Field termVectorField = newField("termVector", "termVector", customType2);
+
+    document.add(termVectorField);
+    writer.addDocument(document);
+
+    document = new Document();
+    Field storedField2 = newField("stored2", "stored2", customType);
+    document.add(storedField2);
+
+    Field termVectorField2 = newField("filteredTerm", "valueOfFilteredTerm", customType2);
+    Field termVectorField3 = newField("nonFilteredTerm", "valueOfNonFilteredTerm", customType2);
+    document.add(storedField);
+    //document.add(termVectorField2);
+    document.add(termVectorField3);
+    document.add(termVectorField2);
+    writer.addDocument(document);
+
+    writer.forceMerge(1);
+    writer.close();
+
+    IndexReader reader = DirectoryReader.open(dir);
+    assert(reader.numDocs() == 3);
+    assertNotNull(reader.getTermVectors(1));
+    assertNotNull(reader.getTermVectors(2));
+    reader.close();
+
+    dir.close();
+  }
+
   // LUCENE-1168
   public void testTermVectorCorruption() throws IOException {
 
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingTermVectorsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingTermVectorsFormat.java
index 000fd6f3ff..63922d94dc 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingTermVectorsFormat.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingTermVectorsFormat.java
@@ -185,6 +185,17 @@ public class AssertingTermVectorsFormat extends TermVectorsFormat {
       --termCount;
     }
 
+    @Override
+    public void skipTerm() throws IOException {
+      assert positionCount == 0;
+      /** Still check that the document and field statuses are in correct state*/
+      assert docStatus == Status.STARTED;
+      assert fieldStatus == Status.STARTED;
+      in.skipTerm();
+      termStatus = Status.FINISHED;
+      --termCount;
+    }
+
     @Override
     public void addPosition(int position, int startOffset, int endOffset,
         BytesRef payload) throws IOException {
